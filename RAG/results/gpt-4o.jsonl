{"File Name":"libelektra\/ingroup_removal.md","Context":"## Problem\\nCurrently, an error or warning message in elektra causes the following line to be shown:\\n```\\nIngroup: <group>\\n```\\nIts main purpose is to show the user if the error occurred in either `kdb`, `module` or `plugin`.\\nThe problem is that this message is of no value for the user and increases the verbosity of the message.\\n","Decision":"The `ingroup` message will be removed as it does not yield any notable benefit.\\nSee [Error concept](error_codes.md)\\n","tokens":83,"id":1308,"text":"## Problem\\nCurrently, an error or warning message in elektra causes the following line to be shown:\\n```\\nIngroup: <group>\\n```\\nIts main purpose is to show the user if the error occurred in either `kdb`, `module` or `plugin`.\\nThe problem is that this message is of no value for the user and increases the verbosity of the message.\\n\n\n##Decision\nThe `ingroup` message will be removed as it does not yield any notable benefit.\\nSee [Error concept](error_codes.md)\\n","MatchedIds":[1319,1302,1303,1274,2975],"Predicted":"## Decision\nRemove the `Ingroup: <group>` line from all error and warning messages.\n\n* This change will reduce the verbosity of error and warning messages.\n* The distinction between `kdb`, `module`, and `plugin` is not relevant or valuable for end users.\n* If grouping information is needed for debugging purposes, it can be logged internally but not shown in user-facing messages.\n\nThis decision aims to streamline the user experience while retaining necessary debugging information internally.","GenTime":"2024-08-19 23:20:31"}
{"File Name":"knot-documentation\/ARC-003-amqp-exc-name.md","Context":"## Context\\nAs defined by CloudAMQP Blog, \"exchanges are message routing agents, defined by the virtual host within RabbitMQ. An exchange is responsible for routing the messages to different queues with the help of header attributes, bindings, and routing keys.\"\\nThe Exchange name needs to be defined taking into account the architecture defined for the system. Thus, there will be a client that communicates with BabelTower, which has the Connector as one of its services (in this case BabelTower is the Connector client) or the client communicates directly with the Connector.\\nThe goal is to define names that make it clear what this Exchange is used for.\\n","Decision":"* The communication between two components will be done with two queues, where one component is consumer in one and producer in another and the other component follows the opposite.\\n* The reference for choosing the queue name will be the client.\\n* The queue where BabelTower publishes messages will be called 'fogOut'.\\n* The queue where BabelTower consumes messages will be called 'fogIn'.\\n* The queue where Connector publishes messages will be called 'connOut'.\\n* The queue where Connector consumes messages will be called 'connIn'.\\n","tokens":131,"id":3339,"text":"## Context\\nAs defined by CloudAMQP Blog, \"exchanges are message routing agents, defined by the virtual host within RabbitMQ. An exchange is responsible for routing the messages to different queues with the help of header attributes, bindings, and routing keys.\"\\nThe Exchange name needs to be defined taking into account the architecture defined for the system. Thus, there will be a client that communicates with BabelTower, which has the Connector as one of its services (in this case BabelTower is the Connector client) or the client communicates directly with the Connector.\\nThe goal is to define names that make it clear what this Exchange is used for.\\n\n\n##Decision\n* The communication between two components will be done with two queues, where one component is consumer in one and producer in another and the other component follows the opposite.\\n* The reference for choosing the queue name will be the client.\\n* The queue where BabelTower publishes messages will be called 'fogOut'.\\n* The queue where BabelTower consumes messages will be called 'fogIn'.\\n* The queue where Connector publishes messages will be called 'connOut'.\\n* The queue where Connector consumes messages will be called 'connIn'.\\n","MatchedIds":[979,4694,1575,2309,84],"Predicted":"## Decision Drivers\n- Clarity of purpose\n- Consistency with existing naming conventions\n- Scalability for future exchanges\n- Ease of understanding for new team members\n\n## Decision\nWe will adopt the following naming convention for defining the RabbitMQ exchanges:\n\n1. **Client-to-BabelTower Exchange**:\n   Name: `client.babeltower.exchange`\n   Rationale: This name indicates that the exchange is for messages from the client to BabelTower. It makes it clear that BabelTower is acting as the intermediary.\n\n2. **BabelTower-to-Connector Exchange**:\n   Name: `babeltower.connector.exchange`\n   Rationale: This name indicates that the exchange is used for internal communication between BabelTower and the Connector service. It signifies the intermediary role of BabelTower in the message flow.\n\n3. **Client-to-Connector Exchange**:\n   Name: `client.connector.exchange`\n   Rationale: This name indicates that the exchange is for direct communication between the client and the Connector service, bypassing BabelTower.\n\n### Positive Consequences\n- Names are clear and descriptive, making it easier for team members to understand the purpose of each exchange.\n- Consistent naming conventions aid in scalability and future maintenance.\n- New team members can quickly grasp the system's architecture by just looking at the exchange names.\n\n### Negative Consequences\n- May require updating existing configurations and documentation to align with the new naming conventions.\n- Slight increase in initial setup time to ensure naming consistency.\n\nOverall, this naming convention strives to ensure clarity and ease of understanding, which supports efficient communication and system maintenance.","GenTime":"2024-08-19 23:20:38"}
{"File Name":"decodeweb\/0003-store-configuration-into-local-storage.md","Context":"## Context\\nTo provide the required functionality for the pilot we need to store some\\nconfiguration in a persistent way while just operating within a browser\\nenvironment. This state could be written to a backend database, or we could\\njust store into localstorage.\\n","Decision":"We will store the user's configuration into localstorage protected by a PIN\\ncode.\\n### Proposed storage structure\\n```json\\n{\\n\"1234\": {\\n\"keypair\": {},\\n\"blindSignature\": {},\\n\"devices\": {\\n\"abc123\": {\\n\"longitude\": 12.2,\\n\"latitude\": 52.4,\\n\"exposure\": \"INDOOR\",\\n\"communities\": {\\n\"2a745dac-015a-4cde-a348-75bccc21502b\": {\\n\"authorizable_attribute_id\": \"6a1ba9b1-3f20-4e12-ae65-154c5eb7e00f\",\\n\"credential\": {},\\n\"blindproofCredential\": {}\\n}\\n}\\n}\\n}\\n}\\n}\\n```\\nPlease see `.\/localstorage.json` for a complete example including zenroom credentials.\\n","tokens":52,"id":2221,"text":"## Context\\nTo provide the required functionality for the pilot we need to store some\\nconfiguration in a persistent way while just operating within a browser\\nenvironment. This state could be written to a backend database, or we could\\njust store into localstorage.\\n\n\n##Decision\nWe will store the user's configuration into localstorage protected by a PIN\\ncode.\\n### Proposed storage structure\\n```json\\n{\\n\"1234\": {\\n\"keypair\": {},\\n\"blindSignature\": {},\\n\"devices\": {\\n\"abc123\": {\\n\"longitude\": 12.2,\\n\"latitude\": 52.4,\\n\"exposure\": \"INDOOR\",\\n\"communities\": {\\n\"2a745dac-015a-4cde-a348-75bccc21502b\": {\\n\"authorizable_attribute_id\": \"6a1ba9b1-3f20-4e12-ae65-154c5eb7e00f\",\\n\"credential\": {},\\n\"blindproofCredential\": {}\\n}\\n}\\n}\\n}\\n}\\n}\\n```\\nPlease see `.\/localstorage.json` for a complete example including zenroom credentials.\\n","MatchedIds":[5257,4859,4843,2488,1514],"Predicted":"## Decision\nWe will use localStorage to store the configuration data in a persistent way while operating within a browser environment. \n\nLocalStorage provides a simple API for storing key-value pairs and has the following advantages for our use case:\n- **Persistence**: Data remains persistent even after the browser is closed and reopened.\n- **Simplicity**: It is straightforward to implement, reducing development time.\n- **No Additional Infrastructure**: Eliminates the need for a backend database, which is overkill for this pilot functionality and avoids unnecessary complexity.\n- **Browser Support**: Well-supported across modern browsers, ensuring broad compatibility for end users.\n\nFor security considerations, we will ensure that sensitive data is either not stored in local storage or is encrypted before being saved.","GenTime":"2024-08-19 23:20:42"}
{"File Name":"holochain-rust\/0006-splitting-agent-into-front-house-back-house-or-not.md","Context":"## Context\\nFor Holo, we need to have user agent's keys and source chain on the client machine and the rest of the Holochain agent (the DHT shard etc.) be held by HoloPorts.\\nIn February 2018, Arthur, Eric and Nico discussed this during a co-creative session in London and made the assumption to have the Holochain agent be split up into two pieces, called front-house and back-house (prior left and right hemisphere). The front-house was meant to entail the source chain and private key management as well as a ribosome to run the app and provide interfaces for the UI and in the non-Holo case for bridging - everything that is user\/agent facing. The back-house should have been the DHT shard, routing table, metrics, etc. Basically everything that is network facing.\\nWith this separation, the reasoning in February was to rewrite (only) the front-house in JS to be able to run this piece in a web browser, as needed for Holo.\\nEric and Nico continued to map out the specifics of how these two modules would interface. In that process it became apparent that dividing an agent in these two pieces and have them run on different remote machines has a huge problem:\\n* every network communication (including the world model which happens on an ongoing basis) that the back-house is doing has to be signed with the agent's keys\\n* the agent's keys are by definition part of the front-house\\n* **-> the back-house can't live up to its main accountability without communicating with the front-house and requesting a signature for every packet of communication that might be even triggered from the network\/DHT while the user is offline and keys not accessible**\\nFurther conversation including Arthur on May 17th 2018 makes it clear that thinking in terms of two different kinds of agency seems appropriate. We discussed separating the authoring or creative agency from the part that runs validations and holds DHT shards and world model information, and allows the later to proxy for the former, **with separate keys**.\\n","Decision":"We decide to **not** emulate a single agency (as represented by one key) across two remote devices. In other words: we decide to not try to implement distributed agents.\\nInstead, we solve the initial Holo front-end problem by establishing **two sub-agencies with distinct keys**, where the Holo client's authoring agency explicitly grants proxy rights to a HoloPort's DHT agency.\\nIn other words: the Holo user uses their local key to sign a statement that grants another agent (the HoloPort) to act on their behalf for all the cases needed to have the HoloPort carry the DHT weight for this agent. But technically, it is another agent with its own key.\\n","tokens":423,"id":1500,"text":"## Context\\nFor Holo, we need to have user agent's keys and source chain on the client machine and the rest of the Holochain agent (the DHT shard etc.) be held by HoloPorts.\\nIn February 2018, Arthur, Eric and Nico discussed this during a co-creative session in London and made the assumption to have the Holochain agent be split up into two pieces, called front-house and back-house (prior left and right hemisphere). The front-house was meant to entail the source chain and private key management as well as a ribosome to run the app and provide interfaces for the UI and in the non-Holo case for bridging - everything that is user\/agent facing. The back-house should have been the DHT shard, routing table, metrics, etc. Basically everything that is network facing.\\nWith this separation, the reasoning in February was to rewrite (only) the front-house in JS to be able to run this piece in a web browser, as needed for Holo.\\nEric and Nico continued to map out the specifics of how these two modules would interface. In that process it became apparent that dividing an agent in these two pieces and have them run on different remote machines has a huge problem:\\n* every network communication (including the world model which happens on an ongoing basis) that the back-house is doing has to be signed with the agent's keys\\n* the agent's keys are by definition part of the front-house\\n* **-> the back-house can't live up to its main accountability without communicating with the front-house and requesting a signature for every packet of communication that might be even triggered from the network\/DHT while the user is offline and keys not accessible**\\nFurther conversation including Arthur on May 17th 2018 makes it clear that thinking in terms of two different kinds of agency seems appropriate. We discussed separating the authoring or creative agency from the part that runs validations and holds DHT shards and world model information, and allows the later to proxy for the former, **with separate keys**.\\n\n\n##Decision\nWe decide to **not** emulate a single agency (as represented by one key) across two remote devices. In other words: we decide to not try to implement distributed agents.\\nInstead, we solve the initial Holo front-end problem by establishing **two sub-agencies with distinct keys**, where the Holo client's authoring agency explicitly grants proxy rights to a HoloPort's DHT agency.\\nIn other words: the Holo user uses their local key to sign a statement that grants another agent (the HoloPort) to act on their behalf for all the cases needed to have the HoloPort carry the DHT weight for this agent. But technically, it is another agent with its own key.\\n","MatchedIds":[1495,1494,578,527,4539],"Predicted":"## Decision\nThe architecture will be adjusted to implement two distinct types of agents: the \"Authoring Agent\" (front-house) and the \"Proxy Agent\" (back-house). \n\n### Authoring Agent:\n- This will reside on the client machine, handling the user agent\u2019s keys, source chain, and private key management.\n- It will include the ribosome to run the app and provide interfaces for the UI, as well as bridging capabilities in non-Holo scenarios.\n- It will be responsible for signing transactions and authoring entries.\n\n### Proxy Agent:\n- This will run on HoloPorts and be responsible for the DHT shard, routing table, metrics, and other network-facing activities.\n- It will handle validations and maintain world model information.\n- It will have its own separate keys to manage network communications independently, reducing the dependency on the Authoring Agent for signing.\n\n### Inter-Agency Communication:\n- The Proxy Agent will be designed to securely proxy requests from the Authoring Agent.\n- The two agents will have a secure and efficient protocol for synchronized interactions, but the Proxy Agent will use its keys to manage day-to-day network communications and signatures required for those.\n\nThis separation will ensure that the main accountability of the Proxy Agent is upheld without constantly having to communicate with the Authoring Agent for signatures. This makes the system more robust and reduces latency and downtime, especially when the user is offline.","GenTime":"2024-08-19 23:20:47"}
{"File Name":"raster-foundry\/adr-0006-workflow-manager.md","Context":"## Context\\nThis ADR has been superseded by `ADR-0019`.\\nSome actions and features of Raster Foundry require a way to manage asynchronous tasks and workflows.\\nFor instance, user uploads of imagery or tools may start workflows in an ad hoc manner, while in\\ncontrast imports of imagery from NASA or partners may need to happen on a schedule. The nature of\\nthese tasks could vary from bash scripts and python functions to spark jobs and ECS tasks.\\nThe ideal tool will provide some means to monitor task progress, retry on some failures, and\\nnotify personnel if necessary. There are a few options of tools we can use: celery, SWF, Luigi, and Airflow.\\nAzavea has experience working with both celery and SWF; however, due to our past experience with these\\ntools it seemed prudent to explore additional options as well.\\n| Workflow Tool   | Pros | Cons |\\n|-----------------|------|------|\\n| Celery          | Familiar, written in python, flexible | Provides poor primitives for workflows, many open issues, difficult to monitor workflows |\\n| SWF (botoflow)) | Familiar, now written in python, maintaining state is not our responsibility (HA by default), great primitives for workflows and tasks | Difficult to monitor, relatively immature tools and projects, not many others using it |\\n| Luigi           | Mature, seems to be stable, written in python | Unfamiliar execution model, primarily designed for scheduled, recurring task |\\n| Airflow         | Mature, stable, fits into our execution model, written in python, excellent UI | Requires celery (for what we want to do)), requires managing the scheduler and a cache |\\n","Decision":"Raster Foundry will use Airflow as a task manager. There are a number of advantages over some of the\\nalternatives. First, Airflow has a large, active, user base that have used it in production. The\\nproject itself is in the Apache incubator, providing a strong signal that the project is of high quality\\nSecond, Airflow's UI for monitoring task and workflow progress is great. It provides\\nhigh-level relevant information that will enable us to diagnose issues quickly. Additionally, it\\nprovides a means to view log output of tasks directly in the admin interface. Third, Airflow\\nsupports both scheduled and ad hoc tasks. Lastly, Airflow's architecture would re-use many\\ncomponents that will already be a part of Raster Foundry's infrastructure - including a Postgres\\ndatabase and a redis cache.\\n","tokens":353,"id":1688,"text":"## Context\\nThis ADR has been superseded by `ADR-0019`.\\nSome actions and features of Raster Foundry require a way to manage asynchronous tasks and workflows.\\nFor instance, user uploads of imagery or tools may start workflows in an ad hoc manner, while in\\ncontrast imports of imagery from NASA or partners may need to happen on a schedule. The nature of\\nthese tasks could vary from bash scripts and python functions to spark jobs and ECS tasks.\\nThe ideal tool will provide some means to monitor task progress, retry on some failures, and\\nnotify personnel if necessary. There are a few options of tools we can use: celery, SWF, Luigi, and Airflow.\\nAzavea has experience working with both celery and SWF; however, due to our past experience with these\\ntools it seemed prudent to explore additional options as well.\\n| Workflow Tool   | Pros | Cons |\\n|-----------------|------|------|\\n| Celery          | Familiar, written in python, flexible | Provides poor primitives for workflows, many open issues, difficult to monitor workflows |\\n| SWF (botoflow)) | Familiar, now written in python, maintaining state is not our responsibility (HA by default), great primitives for workflows and tasks | Difficult to monitor, relatively immature tools and projects, not many others using it |\\n| Luigi           | Mature, seems to be stable, written in python | Unfamiliar execution model, primarily designed for scheduled, recurring task |\\n| Airflow         | Mature, stable, fits into our execution model, written in python, excellent UI | Requires celery (for what we want to do)), requires managing the scheduler and a cache |\\n\n\n##Decision\nRaster Foundry will use Airflow as a task manager. There are a number of advantages over some of the\\nalternatives. First, Airflow has a large, active, user base that have used it in production. The\\nproject itself is in the Apache incubator, providing a strong signal that the project is of high quality\\nSecond, Airflow's UI for monitoring task and workflow progress is great. It provides\\nhigh-level relevant information that will enable us to diagnose issues quickly. Additionally, it\\nprovides a means to view log output of tasks directly in the admin interface. Third, Airflow\\nsupports both scheduled and ad hoc tasks. Lastly, Airflow's architecture would re-use many\\ncomponents that will already be a part of Raster Foundry's infrastructure - including a Postgres\\ndatabase and a redis cache.\\n","MatchedIds":[1681,1680,4388,1563,4281],"Predicted":"## Decision\n\nWe have decided to use Airflow as the tool to manage asynchronous tasks and workflows for Raster Foundry. Despite its need for Celery to handle certain functionalities and the overhead of managing the scheduler and cache, Airflow stands out due to its maturity, stability, and excellent user interface. Additionally, it fits well into our execution model and provides the capability to monitor task progress, retry on failures, and notify personnel as necessary.\n\nThe decision was based on the following considerations:\n- Airflow's maturity and stability make it a reliable choice for critical workflows.\n- The excellent UI simplifies monitoring and troubleshooting tasks, improving developer efficiency.\n- Airflow's well-defined primitives for workflows and task orchestration align well with the diverse nature of our tasks, from bash scripts to Spark jobs.\n- Its widespread adoption and active community support provide a greater assurance of long-term support and continual improvement.\n\nWhile the requirement to manage the scheduler and cache adds complexity, the advantages offered by Airflow in terms of functionality and usability outweigh these concerns. Therefore, Airflow will be integrated into our system for managing asynchronous tasks and workflows.","GenTime":"2024-08-19 23:20:52"}
{"File Name":"occurrent\/0001-mongodb-database-structure.md","Context":"## Context\\nWe need to record events in MongoDB in a specific structure\/schema.\\n","Decision":"The [CloudEvents](https:\/\/cloudevents.io\/) are persisted like this in the \"events collection\" in the database (1):\\n```json\\n{\\n\"specversion\": \"1.0\",\\n\"id\": \"86282094-5344-4309-932a-129a7774735e\",\\n\"source\": \"http:\/\/name\",\\n\"type\": \"org.occurrent.domain.NameDefined\",\\n\"datacontenttype\": \"application\/json\",\\n\"dataschema\" : \"http:\/\/someschema.com\/schema.json\",\\n\"subject\": \"name1\",\\n\"time\": \"2020-07-10T14:48:23.272Z\",\\n\"data\": {\\n\"timestamp\": 1594392503272,\\n\"name\": \"name1\"\\n},\\n\"streamid\" : \"streamid\"\\n}\\n```\\nNote that \"streamid\" is added as an extension by the MongoDB event stores in order to read all events for a particular stream.\\nIf stream consistency is enabled then another collection, the \"stream consistency\" collection is also written to the database (2):\\n```json\\n{\\n\"_id\" : \"streamid\",\\n\"version\" : 1\\n}\\n```\\nWhen appending cloud events to the stream the consistency of the stream is maintained by comparing the version supplied by the user\\nwith the version present in (2). If they don't match then the cloud events are not written. Also if there are two threads writing to the same\\nstream at once then one of them will run into an error which means it has to retry (optimistic locking). For this to work, transactions are required!\\nAnother previous approach was instead to store the events like this:\\n```json\\n{\\n\"_id\": \"streamid\",\\n\"version\" : 1,\\n\"events\": [{\\n\"specversion\": \"1.0\",\\n\"id\": \"86282094-5344-4309-932a-129a7774735e\",\\n\"source\": \"http:\/\/name\",\\n\"type\": \"org.occurrent.domain.NameDefined\",\\n\"datacontenttype\": \"application\/json\",\\n\"subject\": \"name1\",\\n\"time\": \"2020-07-10T14:48:23.272Z\",\\n\"data\": {\\n\"timestamp\": 1594392503272,\\n\"name\": \"name1\"\\n}\\n}]\\n}\\n```\\nI.e. the events were stored inside a single document. While there are several benefits of using this approach, such as:\\n1. No transactions required, just do;\\n```java\\neventCollection.updateOne(and(eq(\"_id\", streamId), eq(\"version\", expectedStreamVersion)),\\ncombine(pushEach(\"events\", serializedEvents), set(\"version\", expectedStreamVersion + 1)),\\nnew UpdateOptions().upsert(true));\\n```\\n1. Reads could be done in a streaming fashion even though the events were stored as a subarray using aggregations\\n1. Subscriptions could take a `List<CloudEvent>`, i.e. all events written in the same transaction to the event store.\\nWhen not using the approach subscriptions gets notified once for each event and the consumer needs to reassemble\\nthe \"transaction\" somehow. This is a major drawback when not using this approach.\\nThere are however two major drawbacks that lead to not using this approach:\\n1. There's 16Mb document size limit in MongoDB so this approach wouldn't work for large streams\\n1. It's much hard to implement queries\/filters for subscriptions. The aggregation support is\\n[limited](https:\/\/stackoverflow.com\/questions\/62846085\/remove-element-from-subarray-using-an-aggregation-stage-applied-to-a-change-stre)\\nwhen working with subscriptions preventing simple filters (it would have been much simpler if `unwind`\\nwas supported since then we could flatten out the `events` subarray before applying the queries, i.e. something like\\n`(unwind(\"$events\"), replaceRoot(\"$events\"), match(filter.apply(\"type\", item))`).\\nAnother problem with subscriptions is the format, when a document is _created_ the content is specified\\nin the `fullDocument` property but it's a different property when the document is updated. Thus a filter\/query\\nwould not need to consider both these cases which is very difficult. With the new approach a query\/filter is much\\neasier since we only need to care about inserts.\\n","tokens":17,"id":4385,"text":"## Context\\nWe need to record events in MongoDB in a specific structure\/schema.\\n\n\n##Decision\nThe [CloudEvents](https:\/\/cloudevents.io\/) are persisted like this in the \"events collection\" in the database (1):\\n```json\\n{\\n\"specversion\": \"1.0\",\\n\"id\": \"86282094-5344-4309-932a-129a7774735e\",\\n\"source\": \"http:\/\/name\",\\n\"type\": \"org.occurrent.domain.NameDefined\",\\n\"datacontenttype\": \"application\/json\",\\n\"dataschema\" : \"http:\/\/someschema.com\/schema.json\",\\n\"subject\": \"name1\",\\n\"time\": \"2020-07-10T14:48:23.272Z\",\\n\"data\": {\\n\"timestamp\": 1594392503272,\\n\"name\": \"name1\"\\n},\\n\"streamid\" : \"streamid\"\\n}\\n```\\nNote that \"streamid\" is added as an extension by the MongoDB event stores in order to read all events for a particular stream.\\nIf stream consistency is enabled then another collection, the \"stream consistency\" collection is also written to the database (2):\\n```json\\n{\\n\"_id\" : \"streamid\",\\n\"version\" : 1\\n}\\n```\\nWhen appending cloud events to the stream the consistency of the stream is maintained by comparing the version supplied by the user\\nwith the version present in (2). If they don't match then the cloud events are not written. Also if there are two threads writing to the same\\nstream at once then one of them will run into an error which means it has to retry (optimistic locking). For this to work, transactions are required!\\nAnother previous approach was instead to store the events like this:\\n```json\\n{\\n\"_id\": \"streamid\",\\n\"version\" : 1,\\n\"events\": [{\\n\"specversion\": \"1.0\",\\n\"id\": \"86282094-5344-4309-932a-129a7774735e\",\\n\"source\": \"http:\/\/name\",\\n\"type\": \"org.occurrent.domain.NameDefined\",\\n\"datacontenttype\": \"application\/json\",\\n\"subject\": \"name1\",\\n\"time\": \"2020-07-10T14:48:23.272Z\",\\n\"data\": {\\n\"timestamp\": 1594392503272,\\n\"name\": \"name1\"\\n}\\n}]\\n}\\n```\\nI.e. the events were stored inside a single document. While there are several benefits of using this approach, such as:\\n1. No transactions required, just do;\\n```java\\neventCollection.updateOne(and(eq(\"_id\", streamId), eq(\"version\", expectedStreamVersion)),\\ncombine(pushEach(\"events\", serializedEvents), set(\"version\", expectedStreamVersion + 1)),\\nnew UpdateOptions().upsert(true));\\n```\\n1. Reads could be done in a streaming fashion even though the events were stored as a subarray using aggregations\\n1. Subscriptions could take a `List<CloudEvent>`, i.e. all events written in the same transaction to the event store.\\nWhen not using the approach subscriptions gets notified once for each event and the consumer needs to reassemble\\nthe \"transaction\" somehow. This is a major drawback when not using this approach.\\nThere are however two major drawbacks that lead to not using this approach:\\n1. There's 16Mb document size limit in MongoDB so this approach wouldn't work for large streams\\n1. It's much hard to implement queries\/filters for subscriptions. The aggregation support is\\n[limited](https:\/\/stackoverflow.com\/questions\/62846085\/remove-element-from-subarray-using-an-aggregation-stage-applied-to-a-change-stre)\\nwhen working with subscriptions preventing simple filters (it would have been much simpler if `unwind`\\nwas supported since then we could flatten out the `events` subarray before applying the queries, i.e. something like\\n`(unwind(\"$events\"), replaceRoot(\"$events\"), match(filter.apply(\"type\", item))`).\\nAnother problem with subscriptions is the format, when a document is _created_ the content is specified\\nin the `fullDocument` property but it's a different property when the document is updated. Thus a filter\/query\\nwould not need to consider both these cases which is very difficult. With the new approach a query\/filter is much\\neasier since we only need to care about inserts.\\n","MatchedIds":[4428,3119,1436,294,2583],"Predicted":"## Decision\nWe will define and use a consistent JSON schema for recording events in MongoDB, ensuring that each event document adheres to predefined fields and data types.","GenTime":"2024-08-19 23:20:55"}
{"File Name":"adr-demo\/0002-use-microservices-architecture.md","Context":"## Context\\nThe web application we want to develop and offer to our users is a picture service that allows :\\n* Uploading images and gifs with a title and tags\\n* Searching images by title or tag\\n* Downloading an image in various file formats and sizes\\nAs we want to quicly build a first version to be able to quickly go online and answer competitors, we choose\\nto deploy our application to Google Cloud (as we do not have any bare metal servers).\\nThe application should meet the following criteria:\\n* it should be quick enough so the search answers in less than 500ms\\n* it should be scalable so we can meet future usage\\n","Decision":"In order to meet required criteria we choose to develop our application using a microservices approach :\\neach business need will be addressed by a specific microservice, as described in the following schema:\\n![](microservices-architecture.png)\\nEach microservice should take a simple input and produce a simple output, each microservice should act as a black box\\nregarding other microservices, meaning state should not be shared across different instances of same or differents microservices.\\n","tokens":137,"id":2516,"text":"## Context\\nThe web application we want to develop and offer to our users is a picture service that allows :\\n* Uploading images and gifs with a title and tags\\n* Searching images by title or tag\\n* Downloading an image in various file formats and sizes\\nAs we want to quicly build a first version to be able to quickly go online and answer competitors, we choose\\nto deploy our application to Google Cloud (as we do not have any bare metal servers).\\nThe application should meet the following criteria:\\n* it should be quick enough so the search answers in less than 500ms\\n* it should be scalable so we can meet future usage\\n\n\n##Decision\nIn order to meet required criteria we choose to develop our application using a microservices approach :\\neach business need will be addressed by a specific microservice, as described in the following schema:\\n![](microservices-architecture.png)\\nEach microservice should take a simple input and produce a simple output, each microservice should act as a black box\\nregarding other microservices, meaning state should not be shared across different instances of same or differents microservices.\\n","MatchedIds":[2515,583,1797,1182,1239],"Predicted":"## Decision\n\nAfter assessing our requirements and the capabilities of Google Cloud, we will utilize a combination of Google Cloud services to deploy our picture service application. The following decisions have been made:\n\n1. **Google Cloud Storage (GCS)**:\n   * For storing raw images, gifs, and their associated thumbnails.\n   * Provides highly durable and available object storage with low latency.\n\n2. **Google Cloud Firestore**:\n   * As our NoSQL database for storing metadata like titles, tags, and other searchable information.\n   * Offers real-time synchronization and supports complex queries, ensuring that search operations can return results within the required 500ms.\n\n3. **Google Kubernetes Engine (GKE)**:\n   * For hosting the application services including upload, search, and download functionalities.\n   * Allows us to scale the services based on demand automatically, ensuring high availability and scalability.\n\n4. **Google Cloud Functions**:\n   * To handle image processing tasks such as creating thumbnails and converting images to various formats and sizes.\n   * Serverless nature ensures elasticity and cost-effectiveness.\n\n5. **Google Cloud Load Balancer**:\n   * For distributing incoming traffic efficiently across multiple instances of our application running on GKE.\n   * Ensures quick response times and can handle increasing loads as the user base grows.\n\n6. **Google Cloud Operations (formerly Stackdriver)**:\n   * For monitoring, logging, and debugging the application.\n   * Ensures we have comprehensive observability into the system's performance and can quickly address any issues.\n\n### Additional Considerations\n\n- For future-proofing, we will integrate Google Cloud Pub\/Sub for any asynchronous processing needs and to decouple services further for better scalability.\n- We will use Google Cloud Identity and Access Management (IAM) to ensure secure and fine-grained access control to our resources.\n\nThis architecture leverages Google Cloud's native services and capabilities, ensuring we meet our criteria of quick search response times and future scalability, while enabling rapid deployment to stay competitive in the market.","GenTime":"2024-08-19 23:21:05"}
{"File Name":"stamper\/0004-bootsptrap-as-frontend-framework.md","Context":"## Context\\nWe need to choose a frontend framework.\\n","Decision":"We don't have any specific requirements yet, so use Bootstrap because it is the most popular framework, and has a lot of documentation.\\n","tokens":12,"id":1240,"text":"## Context\\nWe need to choose a frontend framework.\\n\n\n##Decision\nWe don't have any specific requirements yet, so use Bootstrap because it is the most popular framework, and has a lot of documentation.\\n","MatchedIds":[1243,3715,3718,469,1239],"Predicted":"## Decision\nWe use React because it has a large community, extensive ecosystem, and provides flexibility and performance enhancements with its component-based architecture.","GenTime":"2024-08-19 23:21:09"}
{"File Name":"tdr-dev-documentation\/0018-govkuk-notify-staging-configuration.md","Context":"## Context\\nIt was decided to set the GovUK Notify Staging service to go live: [0015 GovUK Notify Staging Configuration](0015-govuuk-notify-staging-configuration.md)\\n","Decision":"The GovUK Notify team recommended that usually they do not go live with non-production services.\\nIt was decided to not go live with the GovUK Notify Staging services following this advice from the GovUK Notify team.\\nIt is still possible to send emails to users participating with UAT via adding their email addresses to the service's guest list.\\nThe Staging service is sufficiently consistent with the Production service if it were to remain in \"trial mode\".\\nFor full details of the \"trial mode\" for GovUK Notify services see here: https:\/\/www.notifications.service.gov.uk\/using-notify\/trial-mode\\n","tokens":44,"id":1767,"text":"## Context\\nIt was decided to set the GovUK Notify Staging service to go live: [0015 GovUK Notify Staging Configuration](0015-govuuk-notify-staging-configuration.md)\\n\n\n##Decision\nThe GovUK Notify team recommended that usually they do not go live with non-production services.\\nIt was decided to not go live with the GovUK Notify Staging services following this advice from the GovUK Notify team.\\nIt is still possible to send emails to users participating with UAT via adding their email addresses to the service's guest list.\\nThe Staging service is sufficiently consistent with the Production service if it were to remain in \"trial mode\".\\nFor full details of the \"trial mode\" for GovUK Notify services see here: https:\/\/www.notifications.service.gov.uk\/using-notify\/trial-mode\\n","MatchedIds":[1784,1786,5213,996,2050],"Predicted":"## Decision\nOnce the GovUK Notify staging service is live, we will ensure that it is connected to a subset of real user data for more accurate testing. This subset will mimic the production environment to identify any discrepancies or issues earlier in the development cycle. Additionally, user data policies and GDPR compliance will be rigorously adhered to, ensuring that all user data used in staging is anonymized or consented for this purpose. This setup will allow us to perform end-to-end user testing, including email notifications and SMS alerts, to validate the user experience before deploying changes to the production environment.","GenTime":"2024-08-19 23:21:13"}
{"File Name":"operational-data-hub\/0023-iso-8601-to-specify-date-and-time-with-timezone.md","Context":"## Context\\nJSON does not specify how a date(time) string should be formatted. The ISO 8601 standard is widely used within the JSON community to specify date-time objects. [RFC 3339]([https:\/\/tools.ietf.org\/html\/rfc3339) describes the usage of the ISO-8601 standard.\\n","Decision":"We will use the ISO-8601 (latest version) standard (as described in RFC-3339) for formatting date(time) objects whenever a date(time) object is serialized. This applies (but is not limited) to JSON messages, logging, data-store\/firestore timestamps.\\nAll date objects must have a time-zone included.\\n","tokens":66,"id":2755,"text":"## Context\\nJSON does not specify how a date(time) string should be formatted. The ISO 8601 standard is widely used within the JSON community to specify date-time objects. [RFC 3339]([https:\/\/tools.ietf.org\/html\/rfc3339) describes the usage of the ISO-8601 standard.\\n\n\n##Decision\nWe will use the ISO-8601 (latest version) standard (as described in RFC-3339) for formatting date(time) objects whenever a date(time) object is serialized. This applies (but is not limited) to JSON messages, logging, data-store\/firestore timestamps.\\nAll date objects must have a time-zone included.\\n","MatchedIds":[280,2181,2585,2198,1180],"Predicted":"## Decision\nWe will format all date-time strings in our JSON data according to the ISO 8601 standard, as specified in [RFC 3339](https:\/\/tools.ietf.org\/html\/rfc3339).\n\n### Rationale\n- **Interoperability**: Using the ISO 8601 standard ensures compatibility with a wide range of systems and libraries that also adhere to this standard.\n- **Readability**: The human-readable format of ISO 8601 date-time strings enhances clarity, especially in debugging scenarios.\n- **Consistency**: Adhering to a single, well-defined standard reduces ambiguity and minimizes the risk of errors in date-time handling across different components of the system.\n\n### Implementation\n- All date-time fields in our JSON API responses and requests will use the ISO 8601 format (e.g., `2023-10-31T13:45:30Z` for a UTC timestamp).\n- Documentation will specify that clients and services interacting with our API should expect and generate date-time strings in ISO 8601 format.\n- Validation mechanisms will be put in place to ensure that any date-time strings received from clients conform to this format.","GenTime":"2024-08-19 23:21:21"}
{"File Name":"lcarsde\/display-server-library.md","Context":"## Context and Problem Statement\\nThe window manager needs to use some kind of display management system to get notified about opening and closing windows and generally handle functions on the display and input\/output. XCB was used initially but some additional functionality was needed, which is covered by Xlib libraries. Mixing the two systems became a hazard.\\n","Decision":"Chosen option: \"Xlib\", because it was used already for the additional tooling and there's lots of documentations and examples.\\n### Positive Consequences <!-- optional -->\\n* Direct compatibility with additional tool libraries, e.g. for XPM handling\\n* Lots of resources and with Openbox a very complete WM (in sense of ICCCM and EWMH) as possible template\\n### Negative Consequences <!-- optional -->\\n* Refactoring code\\n* Wayland might be more modern\\n* XCB is faster at times due to asynchronous\\n","tokens":66,"id":142,"text":"## Context and Problem Statement\\nThe window manager needs to use some kind of display management system to get notified about opening and closing windows and generally handle functions on the display and input\/output. XCB was used initially but some additional functionality was needed, which is covered by Xlib libraries. Mixing the two systems became a hazard.\\n\n\n##Decision\nChosen option: \"Xlib\", because it was used already for the additional tooling and there's lots of documentations and examples.\\n### Positive Consequences <!-- optional -->\\n* Direct compatibility with additional tool libraries, e.g. for XPM handling\\n* Lots of resources and with Openbox a very complete WM (in sense of ICCCM and EWMH) as possible template\\n### Negative Consequences <!-- optional -->\\n* Refactoring code\\n* Wayland might be more modern\\n* XCB is faster at times due to asynchronous\\n","MatchedIds":[1545,5076,798,2482,4613],"Predicted":"## Decision\nWe have decided to use Xlib exclusively for the window manager's display management system.\n\n### Status\nAccepted\n\n### Consequences\n- **Positive**: \n  - Simplifies the codebase by using a single API, eliminating the complexities and potential errors associated with mixing XCB and Xlib.\n  - Xlib provides a comprehensive set of functionalities that cover all the necessary needs for display and input\/output management, ensuring stability and reliability.\n  - Extensive documentation and community support available for Xlib will aid in troubleshooting and expanding functionality as needed.\n\n- **Negative**: \n  - Xlib can be slightly less efficient compared to XCB in some scenarios due to its synchronous nature. This might lead to minor performance overheads.\n  - The transition from XCB to Xlib will require rewriting some parts of the existing implementation, which could introduce temporary instability and a short period of increased development effort.\n\n### Alternatives Considered\n1. **Continuing to use both XCB and Xlib**: \n   - While this approach would allow leveraging the strengths of both libraries, it increases the complexity and risk of bugs due to the mixed API usage.\n   \n2. **Using XCB exclusively**:\n   - Although XCB is known for its efficiency and asynchronous nature, it lacks some higher-level functionalities provided by Xlib, which would require additional custom implementations.\n\n### Implementation Plan\n1. Conduct a thorough review of the existing codebase to identify all the instances where XCB is used.\n2. Create mappings of XCB functionalities to their Xlib equivalents.\n3. Incrementally replace XCB functionality with Xlib, ensuring thorough testing after each change to maintain stability.\n4. Execute an extensive testing phase to identify and resolve any unforeseen issues.\n5. Update documentation to reflect the changes and provide guidelines for future development using Xlib.\n\n### In Retrospect\nWhile the choice to move exclusively to Xlib might have some performance trade-offs, the simplification and stability it provides to the codebase outweigh the downsides. This ensures a more maintainable and reliable system in the long term. Keeping an eye on performance metrics post-transition will be crucial, and further optimizations can be carried out if necessary.","GenTime":"2024-08-19 23:21:28"}
{"File Name":"fixcity\/0003-sys-use-modular-monolith.md","Context":"## Context\\n- Constraints\\n- Greenfield project\\n- Quality attributes\\n- Expected fast load\\n","Decision":"We will not separate components in to separate deployment units,\\nwe will use modular monolith approach with single database.\\n","tokens":20,"id":1797,"text":"## Context\\n- Constraints\\n- Greenfield project\\n- Quality attributes\\n- Expected fast load\\n\n\n##Decision\nWe will not separate components in to separate deployment units,\\nwe will use modular monolith approach with single database.\\n","MatchedIds":[1031,1792,2516,4361,3982],"Predicted":"## Decision\nWe will adopt a cloud-native, serverless architecture to ensure scalability and cost-efficiency for our greenfield project. This decision is made to accommodate our constraints and to meet the quality attribute of expected fast load times. Utilizing serverless services, such as AWS Lambda or Google Cloud Functions, will enable automatic scaling based on demand, thereby ensuring rapid response times and optimal performance. Additionally, serverless architectures will allow us to focus on business logic without worrying about server maintenance or capacity planning.\n\n### Advantages\n- **Scalability**: Automatically adapts to varying loads, ensuring consistent performance.\n- **Cost-efficiency**: Pay only for actual usage, minimizing costs related to idle resources.\n- **Rapid Development**: Accelerates the development process by offloading infrastructure management to the cloud provider.\n- **Focus on Business Logic**: Frees the team to concentrate on delivering business value rather than managing infrastructure.\n\n### Disadvantages\n- **Cold Start Latencies**: Initial requests might experience slight delays due to the time taken to spin up the serverless function.\n- **Vendor Lock-in**: Heavy reliance on specific cloud services may lead to vendor lock-in, potentially complicating future migrations.\n\n### Implementation Steps\n1. **Select Cloud Provider**: Choose a serverless platform (AWS Lambda, Google Cloud Functions, Azure Functions) based on team expertise and business needs.\n2. **Design Services**: Architect the application into small, discrete functions or microservices to leverage the serverless model.\n3. **Deploy and Monitor**: Deploy the serverless functions and set up monitoring to track performance and ensure the quality attributes of fast load times are consistently met.\n\nThis approach aligns with our constraints and quality attributes, promising a robust and responsive solution for our greenfield project.","GenTime":"2024-08-19 23:21:36"}
{"File Name":"remultiform\/0008-support-indexeddb-as-a-data-store.md","Context":"## Context\\nWe need to persist the user data to the backend. Rather than building our\\nfrontend to be tightly coupled to an API, we want a layer that can sit between\\nthe application and the API.\\nWe know that we will need to support offline use of these forms. We also want\\nusers to be able to recover unsubmitted forms if they haven't gone online\\nbetween sessions.\\nWe also know that some forms will require the upload of images.\\nIndexedDB is a [well supported](https:\/\/caniuse.com\/#feat=indexeddb) browser\\ndatabase that persists data across sessions. It supports storing blob data (such\\nas images), and has decent storage limits.\\n","Decision":"We will support IndexedDB as a data store layer, sitting between the application\\nand the API.\\n","tokens":144,"id":5257,"text":"## Context\\nWe need to persist the user data to the backend. Rather than building our\\nfrontend to be tightly coupled to an API, we want a layer that can sit between\\nthe application and the API.\\nWe know that we will need to support offline use of these forms. We also want\\nusers to be able to recover unsubmitted forms if they haven't gone online\\nbetween sessions.\\nWe also know that some forms will require the upload of images.\\nIndexedDB is a [well supported](https:\/\/caniuse.com\/#feat=indexeddb) browser\\ndatabase that persists data across sessions. It supports storing blob data (such\\nas images), and has decent storage limits.\\n\n\n##Decision\nWe will support IndexedDB as a data store layer, sitting between the application\\nand the API.\\n","MatchedIds":[2221,4859,2488,2515,4843],"Predicted":"## Decision\nWe will use IndexedDB as the browser storage solution to persist user data. This decision is based on its well-supported functionality, ability to store blob data (including images), and its capability to persist data across sessions.\n\n### Implementation Plan\n1. **Storage Layer**: Implement a storage layer using IndexedDB to manage user forms and related images. This layer will handle:\n   - **Form Data**: Store form input data in JSON format.\n   - **Image Data**: Store images as blobs.\n   - **Session Management**: Ensure data is persisted across user sessions and enable recovery of unsubmitted forms.\n\n2. **Offline Support**: Utilize IndexedDB to cache data locally when offline. This allows:\n   - Users to fill out and save forms while offline.\n   - Automatic synchronization with the backend once the device goes online.\n\n3. **Recovery Mechanism**: Develop a mechanism to recover unsubmitted forms. This ensures:\n   - Users do not lose data if they close the browser or navigate away before submitting.\n   - Partial form submissions can be resumed in subsequent sessions.\n\n4. **API Integration Layer**: Create an integration layer between the application and the backend API. This layer will:\n   - Handle synchronization of data stored in IndexedDB with the backend.\n   - Ensure a seamless experience for users transitioning from offline to online states.\n\n### Benefits\n- **Decoupling Frontend and Backend**: Provides a clear separation of concerns and allows for easier adjustments to either side independently.\n- **Offline Functionality**: Enhances user experience by allowing form completion and data persistence even without internet connectivity.\n- **Data Recovery**: Increases data reliability by recovering unsubmitted forms, reducing potential data loss.\n\nBy leveraging IndexedDB for local storage and implementing a robust synchronization and recovery mechanism, we can ensure a reliable and user-friendly experience for form management in our application.","GenTime":"2024-08-19 23:21:41"}
{"File Name":"TOSCAna\/0018-cloudfoundry-no-deployment-in-runtime.md","Context":"## Problem\\nMost of the credentials\/information of the environment (like a service address) are only available as soon the application is deployed.\\n","Decision":"* Chosen Alternative: creating scripts\\n","tokens":29,"id":1745,"text":"## Problem\\nMost of the credentials\/information of the environment (like a service address) are only available as soon the application is deployed.\\n\n\n##Decision\n* Chosen Alternative: creating scripts\\n","MatchedIds":[4358,4096,1173,2471,1239],"Predicted":"## Decision\nWe will use a combination of environment variables and a configuration management tool like Consul or AWS Systems Manager Parameter Store for dynamically managing and retrieving credentials and environment-specific information post-deployment.\n\n* **Environment Variables**: For initial deployment configurations and environment-specific settings that can be predefined.\n\n* **Consul or AWS Systems Manager Parameter Store**: For credentials and information that become available post-deployment, allowing us to securely store, manage, and retrieve these values dynamically at runtime.\n\nThis approach ensures that sensitive information is managed securely and that the application can be configured with necessary environment-specific data dynamically even after deployment.","GenTime":"2024-08-19 23:21:45"}
{"File Name":"celestia-core\/adr-026-general-merkle-proof.md","Context":"## Context\\nWe are using raw `[]byte` for merkle proofs in `abci.ResponseQuery`. It makes hard to handle multilayer merkle proofs and general cases. Here, new interface `ProofOperator` is defined. The users can defines their own Merkle proof format and layer them easily.\\nGoals:\\n- Layer Merkle proofs without decoding\/reencoding\\n- Provide general way to chain proofs\\n- Make the proof format extensible, allowing thirdparty proof types\\n","Decision":"### ProofOperator\\n`type ProofOperator` is an interface for Merkle proofs. The definition is:\\n```go\\ntype ProofOperator interface {\\nRun([][]byte) ([][]byte, error)\\nGetKey() []byte\\nProofOp() ProofOp\\n}\\n```\\nSince a proof can treat various data type, `Run()` takes `[][]byte` as the argument, not `[]byte`. For example, a range proof's `Run()` can take multiple key-values as its argument. It will then return the root of the tree for the further process, calculated with the input value.\\n`ProofOperator` does not have to be a Merkle proof - it can be a function that transforms the argument for intermediate process e.g. prepending the length to the `[]byte`.\\n### ProofOp\\n`type ProofOp` is a protobuf message which is a triple of `Type string`, `Key []byte`, and `Data []byte`. `ProofOperator` and `ProofOp`are interconvertible, using `ProofOperator.ProofOp()` and `OpDecoder()`, where `OpDecoder` is a function that each proof type can register for their own encoding scheme. For example, we can add an byte for encoding scheme before the serialized proof, supporting JSON decoding.\\n","tokens":99,"id":4130,"text":"## Context\\nWe are using raw `[]byte` for merkle proofs in `abci.ResponseQuery`. It makes hard to handle multilayer merkle proofs and general cases. Here, new interface `ProofOperator` is defined. The users can defines their own Merkle proof format and layer them easily.\\nGoals:\\n- Layer Merkle proofs without decoding\/reencoding\\n- Provide general way to chain proofs\\n- Make the proof format extensible, allowing thirdparty proof types\\n\n\n##Decision\n### ProofOperator\\n`type ProofOperator` is an interface for Merkle proofs. The definition is:\\n```go\\ntype ProofOperator interface {\\nRun([][]byte) ([][]byte, error)\\nGetKey() []byte\\nProofOp() ProofOp\\n}\\n```\\nSince a proof can treat various data type, `Run()` takes `[][]byte` as the argument, not `[]byte`. For example, a range proof's `Run()` can take multiple key-values as its argument. It will then return the root of the tree for the further process, calculated with the input value.\\n`ProofOperator` does not have to be a Merkle proof - it can be a function that transforms the argument for intermediate process e.g. prepending the length to the `[]byte`.\\n### ProofOp\\n`type ProofOp` is a protobuf message which is a triple of `Type string`, `Key []byte`, and `Data []byte`. `ProofOperator` and `ProofOp`are interconvertible, using `ProofOperator.ProofOp()` and `OpDecoder()`, where `OpDecoder` is a function that each proof type can register for their own encoding scheme. For example, we can add an byte for encoding scheme before the serialized proof, supporting JSON decoding.\\n","MatchedIds":[1985,4354,4097,1962,1956],"Predicted":"## Decision\nA new interface `ProofOperator` will be introduced to replace the current use of raw `[]byte` for merkle proofs in `abci.ResponseQuery`. This interface will allow users to define, layer, and chain their own Merkle proof formats in a more efficient and flexible manner. \n\n### ProofOperator Interface\nThe `ProofOperator` interface will include methods for applying the proof and combining multiple proofs without requiring decoding and reencoding. This interface will also be extensible, allowing third-party proof types to be easily integrated.\n\n```go\n\/\/ ProofOperator defines the interface for a Merkle proof operator\ntype ProofOperator interface {\n    \/\/ ApplyProof applies the proof to the provided data and returns the result\n    ApplyProof(data []byte) ([]byte, error)\n    \n    \/\/ CombineProofs combines multiple proofs into a single proof\n    CombineProofs(proofs ...ProofOperator) (ProofOperator, error)\n    \n    \/\/ Serialize serializes the proof operator for storage or transmission\n    Serialize() ([]byte, error)\n\n    \/\/ Deserialize deserializes the proof operator from the provided data\n    Deserialize(data []byte) error\n}\n```\n\n### Implementation Details\n1. **Defining Proof Formats**: Different Merkle proof formats can be defined as implementations of the `ProofOperator` interface. This allows flexibility in the type of proofs that can be used.\n\n2. **Layering Proofs**: The `CombineProofs` method will enable layering of multiple proofs without the need for decoding and reencoding, thereby improving performance and simplicity.\n\n3. **Extensibility**: By providing a method for serialization and deserialization, third-party proof types can be easily integrated into the system.\n\n4. **General Proof Chaining**: The `ProofOperator` interface will provide a general way to chain proofs through the `ApplyProof` method, which ensures that each proof step can be individually verified and combined into a comprehensive proof chain.\n\n5. **Backward Compatibility**: Existing proofs using raw `[]byte` will be retrospectively supported by implementing a basic `ProofOperator` that wraps the raw bytes, allowing a smooth transition.\n\n### Example Usage\nAn example implementation of a basic Merkle proof operator might look like this:\n\n```go\n\/\/ BasicMerkleProofOperator implements the ProofOperator interface for a simple Merkle proof\ntype BasicMerkleProofOperator struct {\n    proof []byte\n}\n\nfunc (op *BasicMerkleProofOperator) ApplyProof(data []byte) ([]byte, error) {\n    \/\/ Apply the proof to the data (implementation would be specific to the proof type)\n    \/\/ ... (Merkle proof application logic)\n}\n\nfunc (op *BasicMerkleProofOperator) CombineProofs(proofs ...ProofOperator) (ProofOperator, error) {\n    \/\/ Combine the provided proofs into a single proof (implementation depends on proof type)\n    \/\/ ... (proof combination logic)\n}\n\nfunc (op *BasicMerkleProofOperator) Serialize() ([]byte, error) {\n    return op.proof, nil\n}\n\nfunc (op *BasicMerkleProofOperator) Deserialize(data []byte) error {\n    op.proof = data\n    return nil\n}\n```\n\nBy adopting this new `ProofOperator` interface, the system will achieve better modularity, performance, and extensibility in handling Merkle proofs, ensuring it meets the goals of layering proofs, providing a general way to chain proofs, and making the proof format extensible.","GenTime":"2024-08-19 23:21:56"}
{"File Name":"corona-hackathon\/0011-postgres.md","Context":"## Context\\nWe have to make a decision on how\/where to host the DB.\\n","Decision":"We use a freely managed [Postgres on Heroku](https:\/\/www.heroku.com\/postgres)\\n","tokens":19,"id":3657,"text":"## Context\\nWe have to make a decision on how\/where to host the DB.\\n\n\n##Decision\nWe use a freely managed [Postgres on Heroku](https:\/\/www.heroku.com\/postgres)\\n","MatchedIds":[1239,469,2488,1740,4428],"Predicted":"## Decision\nWe will host the database on AWS RDS. This decision is driven by our need for a managed database solution that ensures high availability, automatic backups, and easy scaling. AWS RDS supports multiple database engines, including MySQL, PostgreSQL, and Oracle, which gives us flexibility in choosing the right database for our needs. Additionally, using AWS RDS allows us to integrate seamlessly with other AWS services, streamlining our overall infrastructure management.","GenTime":"2024-08-19 23:21:59"}
{"File Name":"monocle\/0011-search-query-language.md","Context":"## Context and Problem Statement\\nTo build custom dashboards we need to define queries that are too complex for the existing filter box form.\\nWe would like to use flexible search expressions based on a proper query language.\\n","Decision":"Chosen option: \"Monocle Query Language\", because it comes out best (see below).\\n### Positive Consequences\\n- We improve the user experience by replacing the clunky filter box with a simpler search bar.\\n- We create a re-usable component.\\n### Negative Consequences\\n- We need to maintain a language toolchain.\\n","tokens":44,"id":4637,"text":"## Context and Problem Statement\\nTo build custom dashboards we need to define queries that are too complex for the existing filter box form.\\nWe would like to use flexible search expressions based on a proper query language.\\n\n\n##Decision\nChosen option: \"Monocle Query Language\", because it comes out best (see below).\\n### Positive Consequences\\n- We improve the user experience by replacing the clunky filter box with a simpler search bar.\\n- We create a re-usable component.\\n### Negative Consequences\\n- We need to maintain a language toolchain.\\n","MatchedIds":[2513,4738,1701,4378,4909],"Predicted":"## Decision\nChosen option: Use SQL (Structured Query Language) for defining complex queries for custom dashboards.\n\n### Rationale\n1. **Existing Knowledge**: SQL is a well-known, industry-standard query language. Many developers and data analysts are already familiar with it.\n2. **Functionality**: SQL supports a wide range of complex query operations, including joins, subqueries, aggregations, and window functions, which can easily accommodate our needs for building custom dashboards.\n3. **Integration**: SQL integrates well with most database systems and analytics tools. This provides flexibility regardless of the underlying data storage.\n4. **Maturity and Stability**: Being a mature technology, SQL is stable and well-supported with extensive documentation and community support.\n5. **Performance**: Advanced database systems have optimized engines for executing SQL queries efficiently, even for complex analytic tasks.\n\n### Positive Consequences\n1. **Powerful Queries**: Ability to define highly complex and specific queries that meet the detailed requirements of custom dashboards.\n2. **Flexibility**: Users can easily update and modify queries to adapt to changing data requirements and analytics needs.\n3. **Reusability**: SQL queries can be saved and reused across different dashboards and reports, ensuring consistency and reducing duplication of effort.\n4. **Standardization**: Using a standardized querying language helps in maintaining uniformity across the organization\u2019s reporting and analytic efforts.\n\n### Negative Consequences\n1. **Learning Curve**: Users not familiar with SQL may need training to make the most out of the query capabilities.\n2. **Security Concerns**: Exposing raw SQL querying capability might require strong access control measures to ensure data security and prevent unauthorized data access.\n3. **Potential for Complex Maintenance**: Very complex SQL queries might become difficult to maintain and debug over time, especially if not properly documented.\n\n### Mitigation Strategies\n1. **Training**: Provide training sessions and resources on SQL for data analysts and other relevant users.\n2. **Access Control**: Implement fine-grained access control and auditing to ensure only authorized personnel can execute and modify SQL queries.\n3. **Documentation**: Encourage comprehensive documentation of complex queries to simplify future maintenance and debugging efforts.\n4. **Query Optimization Tools**: Utilize query optimization tools and performance monitoring to ensure the efficiency of SQL queries and to address any performance issues proactively.\n\nBy leveraging SQL, we will enable the creation of powerful and flexible custom dashboards that meet the sophisticated needs of our users.","GenTime":"2024-08-19 23:22:06"}
{"File Name":"GoStudy\/0002-use-hexagonal-architecture.md","Context":"## Context\\nThe SDARS application consists of 3 independent components that can be communicated in various ways.\\nTo enable different communication ways we need to apply a proper architectural style.\\n","Decision":"Adopt Hexagonal Architecture for project.\\n","tokens":37,"id":428,"text":"## Context\\nThe SDARS application consists of 3 independent components that can be communicated in various ways.\\nTo enable different communication ways we need to apply a proper architectural style.\\n\n\n##Decision\nAdopt Hexagonal Architecture for project.\\n","MatchedIds":[429,1094,2502,786,4896],"Predicted":"## Decision\nWe will implement an Event-Driven Architecture (EDA) style for the SDARS application. This approach will allow the three independent components to communicate asynchronously through events, promoting loose coupling and improving scalability and resilience. An event bus or message broker, such as Apache Kafka or RabbitMQ, will be used to facilitate this communication.\n\n### Positive Consequences\n* Promotes loose coupling between components, making the system more modular and easier to maintain.\n* Enhances scalability, as components can handle events independently and in parallel.\n* Improves resilience, as components do not rely on synchronous communication and can continue operating even if other components fail.\n* Facilitates real-time data processing and responses to events.\n\n### Negative Consequences\n* Requires learning and managing an additional infrastructure component, such as an event bus or message broker.\n* Asynchronous communication can add complexity to debugging and monitoring.\n* Message ordering and eventual consistency need to be carefully managed.\n\n","GenTime":"2024-08-19 23:22:11"}
{"File Name":"form-builder\/0010-testing-the-editor-pipeline.md","Context":"## Context\\nOur pipeline follows a continuous delivery method with the manual gate to release to production.\\n![alt text](0010-release-pipeline-context.png \"Diagram of how the editor is released through the pipeline.\")\\nWe have one Editor in test which causes issues when the rest of the team want to test a feature. Also, while the Editor is tied up in testing the pipeline is blocked for deploying to live-production.\\n","Decision":"![alt text](0010-release-pipeline-proposed.png \"Diagram of how the editor is released through the pipeline to have a testable editor.\")\\nThe main problem to solve is having a feature that is being developed available to the wider team to test. Running locally was not considered as the MOJ Macbooks are locked down to those outside of the technical disciplines.\\nChanging the pipeline to create an Editor (all plugged in to the dev and test environments). This will involve, building and deploying the editor code into a pod and attaching to a unique URL. Not every branch will need to create a new test editor, the developer will need to add \"testable-\" to the beginning of the branch name (i.e. `testable-my-really-awesome-feature`).\\nThe Editor will use the current auth mechanism unless the effort outweighs the investment. At a mimimum, basic auth of a set username and password must be added.\\nTest Editor must be cleanly removed after it has fullfilled it's purpose. This should ideally be automated either triggered by merging the branch or via commandline.\\n","tokens":87,"id":5102,"text":"## Context\\nOur pipeline follows a continuous delivery method with the manual gate to release to production.\\n![alt text](0010-release-pipeline-context.png \"Diagram of how the editor is released through the pipeline.\")\\nWe have one Editor in test which causes issues when the rest of the team want to test a feature. Also, while the Editor is tied up in testing the pipeline is blocked for deploying to live-production.\\n\n\n##Decision\n![alt text](0010-release-pipeline-proposed.png \"Diagram of how the editor is released through the pipeline to have a testable editor.\")\\nThe main problem to solve is having a feature that is being developed available to the wider team to test. Running locally was not considered as the MOJ Macbooks are locked down to those outside of the technical disciplines.\\nChanging the pipeline to create an Editor (all plugged in to the dev and test environments). This will involve, building and deploying the editor code into a pod and attaching to a unique URL. Not every branch will need to create a new test editor, the developer will need to add \"testable-\" to the beginning of the branch name (i.e. `testable-my-really-awesome-feature`).\\nThe Editor will use the current auth mechanism unless the effort outweighs the investment. At a mimimum, basic auth of a set username and password must be added.\\nTest Editor must be cleanly removed after it has fullfilled it's purpose. This should ideally be automated either triggered by merging the branch or via commandline.\\n","MatchedIds":[187,351,1739,3789,3915],"Predicted":"## Decision\nWe will introduce multiple testing environments to alleviate the bottleneck caused by having only one Editor in test. Specifically, we will create separate, isolated test environments for each feature branch. This will allow multiple features to be tested concurrently without interfering with one another.\n\nTo achieve this, we will implement automated environment provisioning within our CI\/CD pipeline. When a new feature branch is pushed, a corresponding test environment will be automatically created. Once testing is complete and the feature is merged, the temporary environment will be automatically destroyed to free up resources.\n\nAdditionally, the master branch will have its dedicated staging environment to ensure that a stable version is always available for testing before the manual gate to release to production. This will ensure that the pipeline remains unblocked and deployments to live-production can proceed smoothly.\n\nBy adding this flexibility, we aim to improve testing efficiency, reduce deployment delays, and maintain a smooth continuous delivery workflow.","GenTime":"2024-08-19 23:22:16"}
{"File Name":"iampeterbanjo.com\/0008-use-ava-test-runner.md","Context":"## Context\\n[Lab][hapi-lab] is misreporting the code coverage stats because I have not set it up to [work with Typescript][lab-ts]. The other issue with Lab is that its community is quite small meaning less plug-and-play with other tools. I thought about using [Ava][ava-typescript] but [this review][dodds-jest] of Ava's performance issues doesn't sound great.\\n","Decision":"Jest seems like the way to go:\\n- Typescript support\\n- Large community\\n- Familiar\\n","tokens":91,"id":1594,"text":"## Context\\n[Lab][hapi-lab] is misreporting the code coverage stats because I have not set it up to [work with Typescript][lab-ts]. The other issue with Lab is that its community is quite small meaning less plug-and-play with other tools. I thought about using [Ava][ava-typescript] but [this review][dodds-jest] of Ava's performance issues doesn't sound great.\\n\n\n##Decision\nJest seems like the way to go:\\n- Typescript support\\n- Large community\\n- Familiar\\n","MatchedIds":[452,2273,3323,4223,4785],"Predicted":"## Decision\nWe will transition to using Jest as our testing framework. It has robust support for TypeScript, a large and active community, and extensive resources for integration with other tools. This should resolve the code coverage reporting issues and provide a more reliable, well-supported testing environment.","GenTime":"2024-08-19 23:22:19"}
{"File Name":"handbook\/0003-css.md","Context":"## Context and Problem Statement\\nWe're building websites and web applications that share a common design system with reusable components. How do we write CSS styles in a way that is performant and safe?\\n## Decision Drivers\\n- Should be performant, with code splitting, caching and minimal runtime overhead.\\n- Needs to have easy access to our design system constants. These should optimally be shared with JS logic.\\n- Should be type-safe to catch issues when refactoring.\\n- Reusable components should be closed, not accepting arbitrary styles\/classes.\\n- We want a pattern for responsive props with atomic layout components.\\n","Decision":"- Should be performant, with code splitting, caching and minimal runtime overhead.\\n- Needs to have easy access to our design system constants. These should optimally be shared with JS logic.\\n- Should be type-safe to catch issues when refactoring.\\n- Reusable components should be closed, not accepting arbitrary styles\/classes.\\n- We want a pattern for responsive props with atomic layout components.\\nChosen option: Treat, because it combines the best of both worlds from CSS-in-JS and CSS modules.\\nWe'll create shared components that have responsive props, but are otherwise closed for modifications. Theme variables are defined in a shared library with TypeScript.\\nExample:\\n```typescript jsx\\n\/\/ Good:\\n<Box padding\"small\" \/>\\n<Box padding={{xs: 'small', md: 'medium'}} \/>\\n<Input large \/>\\n<Text preset=\"heading3\" as=\"p\" \/>\\n\/\/ Bad:\\n<Box className={customLayout} \/>\\n<Input style={{ height: 50, padding: 16 }} \/>\\n<Text className={styles.heading} \/>\\n```\\n### Positive Consequences\\n- Treat is statically extracted at build time, so it has minimal runtime.\\n- Styles load in parallel with JS, also when code splitting.\\n- Styles are written in TypeScript which gives us type safety when referring to shared variables, styles and helpers.\\n- Styles are in special files, separate from markup and components giving us clear separation with good visibility into the rendered markup.\\n- We can pull in responsive layout component patterns from [Braid], which gives us a good base to lay out components and pages.\\n### Negative Consequences\\n- We are choosing a pretty new framework, so it may 1) have bugs or issues, 2) be an obstacle for new developers or 3) be discontinued.\\n- When we're generating responsive styles at build time we need to be mindful at how many variations we allow (eg media queries, columns, whitespace), since they can easily bloat our CSS with unused styles.\\n","tokens":126,"id":1992,"text":"## Context and Problem Statement\\nWe're building websites and web applications that share a common design system with reusable components. How do we write CSS styles in a way that is performant and safe?\\n## Decision Drivers\\n- Should be performant, with code splitting, caching and minimal runtime overhead.\\n- Needs to have easy access to our design system constants. These should optimally be shared with JS logic.\\n- Should be type-safe to catch issues when refactoring.\\n- Reusable components should be closed, not accepting arbitrary styles\/classes.\\n- We want a pattern for responsive props with atomic layout components.\\n\n\n##Decision\n- Should be performant, with code splitting, caching and minimal runtime overhead.\\n- Needs to have easy access to our design system constants. These should optimally be shared with JS logic.\\n- Should be type-safe to catch issues when refactoring.\\n- Reusable components should be closed, not accepting arbitrary styles\/classes.\\n- We want a pattern for responsive props with atomic layout components.\\nChosen option: Treat, because it combines the best of both worlds from CSS-in-JS and CSS modules.\\nWe'll create shared components that have responsive props, but are otherwise closed for modifications. Theme variables are defined in a shared library with TypeScript.\\nExample:\\n```typescript jsx\\n\/\/ Good:\\n<Box padding\"small\" \/>\\n<Box padding={{xs: 'small', md: 'medium'}} \/>\\n<Input large \/>\\n<Text preset=\"heading3\" as=\"p\" \/>\\n\/\/ Bad:\\n<Box className={customLayout} \/>\\n<Input style={{ height: 50, padding: 16 }} \/>\\n<Text className={styles.heading} \/>\\n```\\n### Positive Consequences\\n- Treat is statically extracted at build time, so it has minimal runtime.\\n- Styles load in parallel with JS, also when code splitting.\\n- Styles are written in TypeScript which gives us type safety when referring to shared variables, styles and helpers.\\n- Styles are in special files, separate from markup and components giving us clear separation with good visibility into the rendered markup.\\n- We can pull in responsive layout component patterns from [Braid], which gives us a good base to lay out components and pages.\\n### Negative Consequences\\n- We are choosing a pretty new framework, so it may 1) have bugs or issues, 2) be an obstacle for new developers or 3) be discontinued.\\n- When we're generating responsive styles at build time we need to be mindful at how many variations we allow (eg media queries, columns, whitespace), since they can easily bloat our CSS with unused styles.\\n","MatchedIds":[157,381,1125,2655,802],"Predicted":"## Decision\nWe will use Emotion as our CSS-in-JS library for writing styles in our web applications and websites because:\n\n- **Performance**: Emotion provides a balanced approach between features and performance. It supports critical CSS extraction, code splitting, and caching, which ensures that our applications are performant.\n\n- **Design System Integration**: With Emotion, we can easily access and manipulate our design system constants. The library allows us to share constants and variables between CSS and JS, maintaining consistency across our components.\n\n- **Type Safety**: Emotion has excellent TypeScript support, enabling type-safe styles. This is crucial for catching issues early on during refactoring and ensuring robust code.\n\n- **Component Isolation**: Emotion enables us to create reusable components that do not accept arbitrary styles or classes. We can enforce style consistency and avoid unpredictable style overrides by encapsulating the styles within the components.\n\n- **Responsive Props with Atomic Layout Components**: Emotion allows us to achieve responsive design by defining atomic CSS classes and leveraging responsive props in our layout components. This helps in creating scalable and maintainable responsive designs.\n\nIn summary, Emotion meets our requirements for performance, design system integration, type safety, encapsulation, and responsive design patterns. Therefore, it is the chosen solution for our CSS styling needs.\n\n### Positive Consequences\n- Improved performance with smaller CSS bundles and faster load times.\n- Consistent use of design system constants across styles and JS logic.\n- Enhanced type safety, reducing errors during development and refactoring.\n- Encapsulated styles ensure reusable components behave predictably.\n- Simplified implementation of responsive designs with atomic layout components.\n\n### Negative Consequences\n- Developers need to be familiar with Emotion's API and styling methodology.\n- Transitioning from traditional CSS or other CSS-in-JS libraries might require initial overhead and learning curve.","GenTime":"2024-08-19 23:22:25"}
{"File Name":"hello\/005-test-framework.md","Context":"## Context\\nWe want our code to be well tested. What tools or frameworks can we\\nleverage?\\n","Decision":"We will use the [pytest] framework to test our [Python] code. In\\naddition, we will use the [mock] library to prevent our tests\\nfrom interacting with external services.\\n","tokens":23,"id":4359,"text":"## Context\\nWe want our code to be well tested. What tools or frameworks can we\\nleverage?\\n\n\n##Decision\nWe will use the [pytest] framework to test our [Python] code. In\\naddition, we will use the [mock] library to prevent our tests\\nfrom interacting with external services.\\n","MatchedIds":[4868,400,1387,1258,4179],"Predicted":"## Decision\nWe will leverage the following tools and frameworks for thorough testing:\n\n* **JUnit** - For unit testing in Java, providing a robust framework for writing and running repeatable tests.\n* **Mockito** - For mock testing in Java, allowing us to create mock objects and verify interactions.\n* **PyTest** - For testing Python code, with powerful features such as fixtures and plugins to extend its capabilities.\n* **Selenium** - For browser automation, enabling us to write end-to-end tests for web applications.\n* **Cypress** - For modern web testing, offering fast, reliable, and easy end-to-end testing for anything that runs in a browser.\n* **Jest** - For testing JavaScript code, particularly useful in React applications due to its smooth integration with the ecosystem.\n* **Enzyme** - For testing React components, providing utilities to test the output of components\u2019 render methods.\n* **Simplecov** - For monitoring code coverage in Ruby applications, helping ensure that all code paths are tested.","GenTime":"2024-08-19 23:22:31"}
{"File Name":"lbh-adrs\/Event-Driven-Architecture.md","Context":"## **Context**\\nWhen the microservices need to interact with each other (e.g. to sync common data), the preferable approach is to adopt an event-driven architecture. This approach guarantees loosely coupled services which can be run and deployed in isolation.\\nIn order to implement an events-driven architecture we need to use a message broker which should be responsible for sending the messages.\\nIn AWS, two options can be considered:\\n1. SNS (Simple Notification Service)\\nSNS implements pub\/sub messaging which allow applications to send messages to multiple subscribers through push mechanisms.\\n2. SNS (Simple Notification Service) + SQS (Simple Queue Service)\\nSQS is a message queue service used by distributed applications to exchange messages through a polling model and can be used to decouple sending and receiving components. Using Amazon SNS and Amazon SQS together, messages can be delivered to applications that require immediate notification of an event, and also persisted in an Amazon SQS queue for other applications to process at a later time.\\n","Decision":"**SNS (Simple Notification Service) + SQS (Simple Queue Service)**\\nBy coupling SQS with SNS, the subscriber can receive messages at \"their peace\". It allows subscribers to be offline, tolerant to network and host failures. Although SNS has got a 4-phase retry policy, the message can be lost if the consumer is not available. Instead if the subscriber uses a queue, we are able to achieve guaranteed delivery.\\n","tokens":209,"id":2309,"text":"## **Context**\\nWhen the microservices need to interact with each other (e.g. to sync common data), the preferable approach is to adopt an event-driven architecture. This approach guarantees loosely coupled services which can be run and deployed in isolation.\\nIn order to implement an events-driven architecture we need to use a message broker which should be responsible for sending the messages.\\nIn AWS, two options can be considered:\\n1. SNS (Simple Notification Service)\\nSNS implements pub\/sub messaging which allow applications to send messages to multiple subscribers through push mechanisms.\\n2. SNS (Simple Notification Service) + SQS (Simple Queue Service)\\nSQS is a message queue service used by distributed applications to exchange messages through a polling model and can be used to decouple sending and receiving components. Using Amazon SNS and Amazon SQS together, messages can be delivered to applications that require immediate notification of an event, and also persisted in an Amazon SQS queue for other applications to process at a later time.\\n\n\n##Decision\n**SNS (Simple Notification Service) + SQS (Simple Queue Service)**\\nBy coupling SQS with SNS, the subscriber can receive messages at \"their peace\". It allows subscribers to be offline, tolerant to network and host failures. Although SNS has got a 4-phase retry policy, the message can be lost if the consumer is not available. Instead if the subscriber uses a queue, we are able to achieve guaranteed delivery.\\n","MatchedIds":[786,1575,979,302,2316],"Predicted":"## Decision\nChosen option: **SNS (Simple Notification Service) + SQS (Simple Queue Service)**\n\n### Rationale:\nCombining SNS and SQS offers a flexible and robust solution that leverages the strengths of both services, providing the best of both immediate notification and reliable message queuing.\n\n### Positive Consequences\n* **Decoupling:** Provides strong decoupling between microservices which aids in scaling and maintainability.\n* **Immediate Notification & Persistence:** Ensures that critical messages can be immediately processed by subscribers while still being queued for slower or occasional processing services.\n* **Fault Tolerance:** Enhances fault tolerance as SNS handles immediate message dissemination while SQS ensures messages are not lost if a consumer is temporarily unavailable.\n* **Scalability:** Allows easy scaling of both publishers and subscribers independently.\n\n### Negative Consequences\n* **Complexity:** Introduces additional complexity in setting up and managing both SNS and SQS.\n* **Cost:** Potentially higher costs due to using two separate services.\n* **Message Duplication Handling:** Requires careful handling to deal with potential message duplication.\n\nCombining SNS and SQS provides a balanced solution, enhancing our microservice architecture\u2019s resilience, scalability, and flexibility.","GenTime":"2024-08-19 23:22:36"}
{"File Name":"CICD-pipeline\/002-usage-and-import-of-shared-library.md","Context":"## Context\\nWe need to decide how the shared library should be loaded for usage.\\n- A library version can be a tag, a branch or a commit SHA.\\n- A library can be loaded explicitly or implicitly\\n- A library default version can be configured globally or not\\n### Loading implicitly\\n````groovy\\npipeline {\\nagent any\\nstages {\\nstage {\\nsteps {\\nsomeMagicallyAvailableLibraryFunction()\\n}\\n}\\n}\\n}\\n````\\n- a default version of the library needs to be defined globally\\n- there is no need to import the library, as it will be loaded automatically\\n- you can override the version by importing it manually: `@Library('my-shared-library@my-custom-version') _`\\n### Loading explicitly\\n````groovy\\n@Library('my-shared-library@1.0.0') _\\npipeline {\\nagent any\\nstages {\\nstage {\\nsteps {\\nsomeMagicallyAvailableLibraryFunction()\\n}\\n}\\n}\\n}\\n````\\n- an import statement is always needed\\n- a default version could be defined globally, then the version can be left out when importing\\n### Using a global predefined version\\n- can be overriden in the pipeline\\n- does not force users to select a version\\n### No globel predefined version\\n- import will not work without version\\n- consumers are aware of the version that they use\\n- can be dynamic (auto updating) when using release channels with branches\\n","Decision":"We will use explicit loading without a predefined version and release channels for releasing based on branches.\\n- reduces the magic of the used version\\n- can still be auto updating\\n- consumers know that a library is used and what the version is\\n","tokens":314,"id":2766,"text":"## Context\\nWe need to decide how the shared library should be loaded for usage.\\n- A library version can be a tag, a branch or a commit SHA.\\n- A library can be loaded explicitly or implicitly\\n- A library default version can be configured globally or not\\n### Loading implicitly\\n````groovy\\npipeline {\\nagent any\\nstages {\\nstage {\\nsteps {\\nsomeMagicallyAvailableLibraryFunction()\\n}\\n}\\n}\\n}\\n````\\n- a default version of the library needs to be defined globally\\n- there is no need to import the library, as it will be loaded automatically\\n- you can override the version by importing it manually: `@Library('my-shared-library@my-custom-version') _`\\n### Loading explicitly\\n````groovy\\n@Library('my-shared-library@1.0.0') _\\npipeline {\\nagent any\\nstages {\\nstage {\\nsteps {\\nsomeMagicallyAvailableLibraryFunction()\\n}\\n}\\n}\\n}\\n````\\n- an import statement is always needed\\n- a default version could be defined globally, then the version can be left out when importing\\n### Using a global predefined version\\n- can be overriden in the pipeline\\n- does not force users to select a version\\n### No globel predefined version\\n- import will not work without version\\n- consumers are aware of the version that they use\\n- can be dynamic (auto updating) when using release channels with branches\\n\n\n##Decision\nWe will use explicit loading without a predefined version and release channels for releasing based on branches.\\n- reduces the magic of the used version\\n- can still be auto updating\\n- consumers know that a library is used and what the version is\\n","MatchedIds":[3031,3653,1631,128,701],"Predicted":"## Decision\nWe will use explicit loading with a global predefined version for the shared library. This approach provides a balance between convenience and control.\n\n### Justification:\n\n- **Explicit Loading:**\n  - Ensures that all pipeline scripts explicitly declare their dependencies, increasing transparency and clarity.\n  - Reduces implicit dependencies, making it clear which version of the library is being used.\n  \n- **Global Predefined Version:**\n  - Provides a default library version that simplifies the setup for most users.\n  - Users can still override the global default version when needed, allowing flexibility for testing or using different versions.\n  - Reduces the risk of version conflicts and ensures consistency across different pipelines by default.\n\n### Implementation:\n\n1. **Define a Global Default Version:**\n   - A global default version of the shared library will be configured to streamline the usage across various pipelines.\n\n2. **Explicit Import with Optional Version:**\n   - Pipelines will explicitly import the shared library. If the default version is acceptable, the import statement can exclude the version:\n     ```groovy\n     @Library('my-shared-library') _\n     pipeline {\n         agent any\n         stages {\n             stage {\n                 steps {\n                     someMagicallyAvailableLibraryFunction()\n                 }\n             }\n         }\n     }\n     ```\n   - If a different version is needed, the version can be specified:\n     ```groovy\n     @Library('my-shared-library@my-custom-version') _\n     pipeline {\n         agent any\n         stages {\n             stage {\n                 steps {\n                     someMagicallyAvailableLibraryFunction()\n                 }\n             }\n         }\n     }\n     ```\n\nThis decision ensures that pipelines have a clear and consistent way to declare dependencies while maintaining the flexibility to adapt to different versions as needed.","GenTime":"2024-08-19 23:22:42"}
{"File Name":"ios-architecture-decision-logs\/0006-inject-parameters-asArguments-toPresenter.md","Context":"## Context\\nWe faced a problem while injecting new parameters to presenters' initializer. The list of init params are getting much longer.\\n","Decision":"Each module has it's own `createModule` or `setupModule` static method in its router. If we have 3 or more properties to be injected to any of these methods, we must create `XYZArguments` structure that contains those properties and pass the argument object to router.\\nWe must also inject the same arguments variable to the presenters' initializer and use the variables inside through the arguments object, and not copy each of variables inside arguments to the presenter one by one.\\n**Note**: Argument object must only include the values to be injected from outer module to initialize the target module (e.g. any kind of `Config\/AB flags`, `Managers` and `Helpers` must not be stored inside arguments, and these variables must be injected to the presenters' initializer directly)\\n","tokens":28,"id":4983,"text":"## Context\\nWe faced a problem while injecting new parameters to presenters' initializer. The list of init params are getting much longer.\\n\n\n##Decision\nEach module has it's own `createModule` or `setupModule` static method in its router. If we have 3 or more properties to be injected to any of these methods, we must create `XYZArguments` structure that contains those properties and pass the argument object to router.\\nWe must also inject the same arguments variable to the presenters' initializer and use the variables inside through the arguments object, and not copy each of variables inside arguments to the presenter one by one.\\n**Note**: Argument object must only include the values to be injected from outer module to initialize the target module (e.g. any kind of `Config\/AB flags`, `Managers` and `Helpers` must not be stored inside arguments, and these variables must be injected to the presenters' initializer directly)\\n","MatchedIds":[4980,4556,1758,1545,2502],"Predicted":"## Decision\nWe decided to use the builder pattern for initializing presenters. This pattern will allow us to chain method calls for setting various parameters, improving readability and maintainability. Additionally, it will help manage default values and optional parameters more effectively, making the initializers less cumbersome and more flexible as the requirements evolve.","GenTime":"2024-08-19 23:22:45"}
{"File Name":"form-design-system\/components_ignore_classnames.md","Context":"## Context and Problem Statement\\nSome components in FDS would suffer from accepting a `classNames` prop. `Flex`, for\\nexample, would no longer guarantee it would work as a basic layout utility if we allowed\\narbitrary style overrides on the component.\\nWe want all components in FDS to follow a similar prop interface, so this decision should\\napply to _all_ components.\\n","Decision":"Chosen option: \"No components accept a `className`\", because...\\n- It's too risky to introduce style overrides to low-level layout components.\\n- Style customization should happen via props, where the component can be responsible for\\nstyle changes and variations.\\n### Positive Consequences <!-- optional -->\\n- Prop API consistency across all components.\\n### Negative Consequences <!-- optional -->\\n- Consumers of FDS components must handle the breaking change. Any custom `className`\\nprops on FDS components must be moved to a direct parent or direct child of the\\ncomponent.\\n","tokens":80,"id":802,"text":"## Context and Problem Statement\\nSome components in FDS would suffer from accepting a `classNames` prop. `Flex`, for\\nexample, would no longer guarantee it would work as a basic layout utility if we allowed\\narbitrary style overrides on the component.\\nWe want all components in FDS to follow a similar prop interface, so this decision should\\napply to _all_ components.\\n\n\n##Decision\nChosen option: \"No components accept a `className`\", because...\\n- It's too risky to introduce style overrides to low-level layout components.\\n- Style customization should happen via props, where the component can be responsible for\\nstyle changes and variations.\\n### Positive Consequences <!-- optional -->\\n- Prop API consistency across all components.\\n### Negative Consequences <!-- optional -->\\n- Consumers of FDS components must handle the breaking change. Any custom `className`\\nprops on FDS components must be moved to a direct parent or direct child of the\\ncomponent.\\n","MatchedIds":[1992,157,381,5090,4613],"Predicted":"## Context and Problem Statement\nSome components in FDS would suffer from accepting a `classNames` prop. `Flex`, for example, would no longer guarantee it would work as a basic layout utility if we allowed arbitrary style overrides on the component. We want all components in FDS to follow a similar prop interface, so this decision should apply to _all_ components.\n\n## Decision Drivers\n- Ensuring the consistency and reliability of FDS components\n- Maintaining the integrity of layout utility components like `Flex`\n- Minimizing the risk of style conflicts and bugs\n- Providing a clear pattern for developers using FDS components\n\n## Decision\nWe will not allow FDS components to accept arbitrary `classNames` or `style` props. Instead, we will define a controlled set of props for each component that is aligned with the design system guidelines and intended use cases. Any required variation or customization should be achieved through these predefined props.\n\n### Implementation\n1. **Define Props**: Each FDS component will have clearly defined props that cover common use cases. For example, `Flex` will include props like `direction`, `justifyContent`, `alignItems`, etc.\n2. **Extendability**: For more complex styles or variations, we will extend components by creating new styled variants or by composing them with additional utility components.\n3. **Documentation**: Provide comprehensive documentation and examples showcasing how to use each component and their respective props. Highlight best practices for achieving custom styles using the defined props.\n4. **Uniformity**: Ensure all FDS components follow this pattern, promoting uniformity and preventing arbitrary style overrides.\n\n### Positive Consequences\n- **Consistency**: By removing the ability to apply arbitrary class names, we maintain uniformity and consistency in the styling and behavior of FDS components.\n- **Reliability**: Components like `Flex` will retain their intended utility without unexpected behavior due to overridden styles.\n- **Clarity**: Developers will have a clear and predictable interface for styling and customizing components, reducing the learning curve and potential for errors.\n- **Maintainability**: The design system becomes easier to maintain and update, as styles are controlled and predictable.\n\n### Negative Consequences\n- **Reduced Flexibility**: Developers might find the inability to apply arbitrary class names restrictive, especially in edge cases.\n- **Learning Curve**: Developers must adapt to using predefined props and understand the design system components better, which may initially slow down development.\n\n### Examples\nInstead of:\n```jsx\n<Flex className=\"custom-flex-class\" \/>\n```\nDevelopers should use:\n```jsx\n<Flex direction=\"row\" justifyContent=\"center\" alignItems=\"center\" gap=\"large\" \/>\n```\nFor more complex or edge case styling:\n```jsx\nconst CustomFlex = styled(Flex)`\n  background-color: #f00;\n  padding: 20px;\n`;\n\n\/\/ Usage\n<CustomFlex direction=\"row\" justifyContent=\"center\" alignItems=\"center\" \/>\n```\n\n### Conclusion\nBy not allowing arbitrary `classNames` or `style` props and promoting the use of well-defined component props, we ensure the FDS components remain consistent, reliable, and maintainable. This decision prioritizes the integrity of the design system and helps developers create predictable and cohesive UIs.","GenTime":"2024-08-19 23:22:56"}
{"File Name":"green_log\/0003-decouple-generation-and-handling.md","Context":"## Context\\nWe want the logging API used by applications to be consistent, while allowing for logs to be filed, forwarded, filtered and formatted in a variety of ways.\\n","Decision":"De-couple generation of log message\/entries from how they are handled.\\n* A `logger` object provides an API that can be used to generate log entries.\\n* Log \"entries\" are strongly typed structures.\\n* Log entry \"handlers\" provide a simple, consistent interface.\\n```mermaid\\nsequenceDiagram\\nApp ->> Logger:       info(\"Message\")\\nLogger ->> Handler:   <<(entry)\\n```\\n","tokens":35,"id":1924,"text":"## Context\\nWe want the logging API used by applications to be consistent, while allowing for logs to be filed, forwarded, filtered and formatted in a variety of ways.\\n\n\n##Decision\nDe-couple generation of log message\/entries from how they are handled.\\n* A `logger` object provides an API that can be used to generate log entries.\\n* Log \"entries\" are strongly typed structures.\\n* Log entry \"handlers\" provide a simple, consistent interface.\\n```mermaid\\nsequenceDiagram\\nApp ->> Logger:       info(\"Message\")\\nLogger ->> Handler:   <<(entry)\\n```\\n","MatchedIds":[4852,1927,3290,2181,1180],"Predicted":"## Decision\nWe will standardize on using the [Winston](https:\/\/github.com\/winstonjs\/winston) logging library for our applications due to its flexibility and wide range of features. The decision entails:\n\n1. **Centralized Configuration Management:**\n   - A single configuration file will define log levels, file locations, and formats to ensure consistency across the application.\n   - Configuration can be environment-specific, allowing different settings for development, staging, and production.\n\n2. **Log Levels:**\n   - Adopt a consistent set of log levels across all applications, such as `error`, `warn`, `info`, `verbose`, `debug`, and `silly`.\n\n3. **Logging Transports:**\n   - Utilize Winston's transport mechanism to direct logs to different destinations including:\n     - File-based logging for persistent storage.\n     - Console logging for development and debugging.\n     - HTTP\/S or TCP transports for forwarding logs to centralized systems like Logstash or other log management solutions.\n     - Custom transports if necessary, to fit specific requirements.\n\n4. **Formatting:**\n   - Use Winston\u2019s built-in formatters for JSON and simple text formats.\n   - Implement custom formatters when specific log structure or annotation is necessary.\n   - Ensure log entries include timestamps and context-specific information to aid in traceability and analysis.\n\n5. **Filtering:**\n   - Apply filters to control which log entries are actually output or forwarded based on log levels, modules, or contextual data.\n   - Enable or disable specific loggers or transports as required by the running context or environment.\n\n6. **Integration with Monitoring Tools:**\n   - Ensure compatibility with log aggregation and monitoring tools like ELK stack, Splunk, and CloudWatch.\n   - Use the appropriate Winston transport or integration plugin for seamless connection to these tools.\n\n7. **Error Handling:**\n   - Implement robust error handling mechanisms within the logging framework to prevent log failures from affecting application performance.\n   - Use fallback mechanisms to route logs to alternative sources in case of transport failures.\n\nBy leveraging Winston, we ensure that our logging approach remains consistent across all applications while providing the flexibility needed to meet diverse logging requirements.","GenTime":"2024-08-19 23:23:06"}
{"File Name":"publicservice-registry\/0013-handle-eventual-consistency-with-etags.md","Context":"## Context\\nThe UI might request a resource after it's creation or alteration, while the async projections are not yet up to date with this change.\\nIn that case, the user would see stale data without knowing why, perhaps thinking its changes were not accepted by the application.\\nTo prevent this, the client needs a mechanism to check whether or not its latest update has already been processed by the projections.\\n","Decision":"We will handle eventual consistency concerns using ETags. On succesful POST or PUT to the API, the client will receive a response containing an ETag header. This header will contain the highest event number caused by the client's request.\\nAll Projections will update their ProjectionState with this number.\\nIf a client wants to know if the resource is up to date with their changes, they will have to include the ETag in a `If-Match` header. Depending on whether the Projections is up to date or not, the API will return a `200 OK` or `412 PRECONDITION FAILED`.\\n","tokens":83,"id":3127,"text":"## Context\\nThe UI might request a resource after it's creation or alteration, while the async projections are not yet up to date with this change.\\nIn that case, the user would see stale data without knowing why, perhaps thinking its changes were not accepted by the application.\\nTo prevent this, the client needs a mechanism to check whether or not its latest update has already been processed by the projections.\\n\n\n##Decision\nWe will handle eventual consistency concerns using ETags. On succesful POST or PUT to the API, the client will receive a response containing an ETag header. This header will contain the highest event number caused by the client's request.\\nAll Projections will update their ProjectionState with this number.\\nIf a client wants to know if the resource is up to date with their changes, they will have to include the ETag in a `If-Match` header. Depending on whether the Projections is up to date or not, the API will return a `200 OK` or `412 PRECONDITION FAILED`.\\n","MatchedIds":[1606,3845,1654,1558,2886],"Predicted":"## Decision\nWe will implement a versioning strategy where each update to the resource increments a version number or timestamp. The client will include this version or timestamp when requesting the resource. If the server detects that the requested version is not yet available in the projections, it will return a specific \"pending\" status along with an estimate of when the data will be up-to-date, or it will trigger a synchronous fetch to ensure the latest data is provided.\n\nAdditionally, the client will use a polling mechanism with exponential backoff to recheck the resource until the desired version or timestamp is available. This will ensure that the user is informed of the delay and can be confident that their changes are being processed.\n\nTo implement this:\n\n1. **Server-side:** Modify the resource creation and update endpoints to return the new version number or timestamp.\n2. **Projections:** Ensure that each projection update increments and tracks the version number or timestamp.\n3. **Client-side:** Implement logic to append the version number or timestamp to resource requests and handle the \"pending\" status response by polling until the latest data is available.","GenTime":"2024-08-19 23:23:11"}
{"File Name":"libelektra\/holes.md","Context":"## Problem\\nConfig files ideally do not copy any structure if they only want to\\nset a single key.\\n","Decision":"Support holes and values for non-leaves in a KeySet if the underlying format allows it.\\nIf the underlying format does not support it and there is also not an obvious\\nway how to circumvent it -- e.g., JSON which does not have comments -- holes and\\nvalues in non-leaves can be supported with key names starting with \u00aeelektra.\\n","tokens":23,"id":1307,"text":"## Problem\\nConfig files ideally do not copy any structure if they only want to\\nset a single key.\\n\n\n##Decision\nSupport holes and values for non-leaves in a KeySet if the underlying format allows it.\\nIf the underlying format does not support it and there is also not an obvious\\nway how to circumvent it -- e.g., JSON which does not have comments -- holes and\\nvalues in non-leaves can be supported with key names starting with \u00aeelektra.\\n","MatchedIds":[5227,1289,1292,1318,1291],"Predicted":"## Decision\n- Update configuration file parsers to support setting individual keys without requiring the duplication of the entire structure.\n- Introduce a mechanism where changes to single keys can be specified in isolation, with reference paths to the necessary structures.\n- Modify the internal representation of configuration data to efficiently handle partial updates.\n- Add documentation and usage examples to assist developers and administrators in leveraging the new mechanism for setting single keys.\n- Ensure backward compatibility so existing configuration files and structures remain functional without modification.\n- Conduct thorough testing to validate the robustness and performance of the new mechanism under various scenarios.\n","GenTime":"2024-08-19 23:23:14"}
{"File Name":"Nosedive\/0005-sqlite-database.md","Context":"## Context\\nOne of the final stack canditate for persistence is [cockroachdb](https:\/\/www.cockroachlabs.com\/), but the windows version\\n","Decision":"We remove all complexity at the beginnign using sqlite. As cockorachdb is SQL compatible, we not expect difficults to migrate\\n","tokens":33,"id":109,"text":"## Context\\nOne of the final stack canditate for persistence is [cockroachdb](https:\/\/www.cockroachlabs.com\/), but the windows version\\n\n\n##Decision\nWe remove all complexity at the beginnign using sqlite. As cockorachdb is SQL compatible, we not expect difficults to migrate\\n","MatchedIds":[2520,1167,4859,3657,1740],"Predicted":"## Decision\nWe have evaluated CockroachDB as a candidate for our persistence mechanism. Although CockroachDB is an attractive option due to its horizontal scalability and strong consistency guarantees similar to those of traditional RDBMS, the Windows version presents certain limitations and potential stability issues.\n\nGiven our requirements and the current state of the Windows version of CockroachDB, we will opt against its use at this time. Instead, we will explore other mature and stable options, possibly leveraging other SQL or NoSQL solutions that align well with our existing infrastructure and operational needs.\n\n### Positive Consequences\n* Increase in overall system stability by avoiding the potential issues associated with the Windows version of CockroachDB.\n* Better support and community resources from more mature database solutions.\n* Avoiding unanticipated downtime or performance degradation related to the current limitations of CockroachDB on Windows.\n\n### Negative Consequences\n* Missing out on the horizontal scalability and strong consistency benefits of CockroachDB.\n* Potential need to re-evaluate database options in the future if CockroachDB improves its support for Windows environments.\n* Possible increase in complexity if a different database solution necessitates additional configuration or migration efforts.\n\n### Future Actions\n* Monitor the development and updates related to the Windows version of CockroachDB for future reconsideration.\n* Document the decision outcome and re-evaluate as necessary based on changing project requirements or advancements in CockroachDB support.","GenTime":"2024-08-19 23:23:19"}
{"File Name":"content-publisher\/0008-restricted-deletion.md","Context":"## Context\\nWe use foreign keys to help enforce database integrity e.g.\\n```\\n# ensure creator_id on a document matches a row in the users table\\n# when we try to delete a user, raise an error if its the creator of a document\\nadd_foreign_key \"documents\", \"users\", column: \"creator_id\", on_delete: :restrict\\n```\\nThere are other `on_delete` behaviours instead of `restrict`.\\n```\\n# when we delete a user, set creator_id to NULL on all of its documents\\nadd_foreign_key \"documents\", \"users\", column: \"creator_id\", on_delete: :nullify\\n# when we delete a user, also delete all of its document\\nadd_foreign_key \"documents\", \"users\", column: \"creator_id\", on_delete: :cascade\\n```\\nDifferent options are appropriate depending on the foreign key.\\n* `restrict` is a good choice in a child\/parent scenario, where deleting a child should be restricted if it would make the parent invalid. For example, we should not be able to delete an ActiveStorage blob if its parent image still exists.\\n* `nullify` works well if the relationship is optional. For example, our code doesn't currently require a document to have a creator, so it might be reasonable to `creator_id` on document to NULL if we delete a user for some reason.\\n* `cascade` is a good choice in a parent\/child scenario, where it makes sense to delete the child when we delete the parent. For example, if we delete a document for some reason, it also makes sense to delete the editions.\\n","Decision":"Currently, we don't have any real-world scenarios for deleting stuff from the database. Therefore, we should do the simplest thing and `restrict` on all foreign key delete actions. `restrict` is actually the default, but we should continue to specify it explicitly to indicate a conscious decision.\\n","tokens":341,"id":4872,"text":"## Context\\nWe use foreign keys to help enforce database integrity e.g.\\n```\\n# ensure creator_id on a document matches a row in the users table\\n# when we try to delete a user, raise an error if its the creator of a document\\nadd_foreign_key \"documents\", \"users\", column: \"creator_id\", on_delete: :restrict\\n```\\nThere are other `on_delete` behaviours instead of `restrict`.\\n```\\n# when we delete a user, set creator_id to NULL on all of its documents\\nadd_foreign_key \"documents\", \"users\", column: \"creator_id\", on_delete: :nullify\\n# when we delete a user, also delete all of its document\\nadd_foreign_key \"documents\", \"users\", column: \"creator_id\", on_delete: :cascade\\n```\\nDifferent options are appropriate depending on the foreign key.\\n* `restrict` is a good choice in a child\/parent scenario, where deleting a child should be restricted if it would make the parent invalid. For example, we should not be able to delete an ActiveStorage blob if its parent image still exists.\\n* `nullify` works well if the relationship is optional. For example, our code doesn't currently require a document to have a creator, so it might be reasonable to `creator_id` on document to NULL if we delete a user for some reason.\\n* `cascade` is a good choice in a parent\/child scenario, where it makes sense to delete the child when we delete the parent. For example, if we delete a document for some reason, it also makes sense to delete the editions.\\n\n\n##Decision\nCurrently, we don't have any real-world scenarios for deleting stuff from the database. Therefore, we should do the simplest thing and `restrict` on all foreign key delete actions. `restrict` is actually the default, but we should continue to specify it explicitly to indicate a conscious decision.\\n","MatchedIds":[224,317,4011,1031,2403],"Predicted":"## Decision\nTo enforce database integrity consistently, we have decided to apply different `on_delete` behaviors based on the specific relationship and context of the foreign key:\n\n1. **Restrict (`on_delete: :restrict`)**\n    - Usage: Apply this option in child\/parent scenarios where deleting the child should be disallowed if it would invalidate the parent.\n    - Example: Usage in relationships where the child entity critically depends on the parent (e.g., ActiveStorage blob with an attached image parent).\n    - Rationale: This ensures that important relationships are not broken unintentionally, maintaining data integrity and preventing orphaned records.\n\n2. **Nullify (`on_delete: :nullify`)**\n    - Usage: Apply this option where the relationship is optional, meaning the child can exist without the parent.\n    - Example: Usage in scenarios where the foreign key can be safely set to NULL without breaking the application's logic (e.g., `creator_id` on a document can be NULL if the user is deleted).\n    - Rationale: This provides flexibility in optional relationships, allowing the child to persist even if the parent is removed.\n\n3. **Cascade (`on_delete: :cascade`)**\n    - Usage: Apply this option in parent\/child scenarios where deleting the parent should also delete the child entities.\n    - Example: Usage where the child entities are closely tied to the existence of the parent (e.g., deleting a document should also delete its editions).\n    - Rationale: This simplifies the model by automatically handling cleanup of related child entities, ensuring no orphan data remains and maintaining database integrity.\n\nBy strategically using these options, we can maintain the integrity of our database while balancing the needs of different relationships within our application. Each foreign key should be carefully evaluated to choose the most suitable `on_delete` behavior based on the detailed context and application requirements.","GenTime":"2024-08-19 23:23:24"}
{"File Name":"amf\/0003-new-annotation-removal-stage-present-in-all-webapi-pipelines.md","Context":"## Context\\nWhen referencing external files there are certain cases where AMF inlines the parsed content without creating a links (this is because the targeted elements are not present in the references of the base unit).\\nFor these cases, when a emitting an unresolved model these references are being emitted inlined.\\n","Decision":"In order to avoid emitting these references inlined for an unresolved model, we must make use of annotation to save the original reference.\\nWhen saving this reference, we must make sure that if the model is resolved this annotation is no longer present so that the emitter does not render references for a flattened model.\\nThis leads to the creation of a new resolution stage that removes specific annotations from the model that must not be present in a resolved base unit.\\n","tokens":61,"id":1393,"text":"## Context\\nWhen referencing external files there are certain cases where AMF inlines the parsed content without creating a links (this is because the targeted elements are not present in the references of the base unit).\\nFor these cases, when a emitting an unresolved model these references are being emitted inlined.\\n\n\n##Decision\nIn order to avoid emitting these references inlined for an unresolved model, we must make use of annotation to save the original reference.\\nWhen saving this reference, we must make sure that if the model is resolved this annotation is no longer present so that the emitter does not render references for a flattened model.\\nThis leads to the creation of a new resolution stage that removes specific annotations from the model that must not be present in a resolved base unit.\\n","MatchedIds":[405,2689,3637,1855,4307],"Predicted":"## Decision\nA new mechanism will be introduced to maintain external references as links even if the targeted elements are not present in the references of the base unit.\\nWhen emitting an unresolved model, these external references will be preserved as links rather than inlining the content.\\nTo achieve this, a new flag will be added to the emission process that identifies external references and treats them appropriately, ensuring that the references are maintained as links in both resolved and unresolved states.\\nThis approach minimizes data redundancy and maintains consistency in the representation of external references, leading to easier maintenance and readability.\n\n","GenTime":"2024-08-19 23:23:28"}
{"File Name":"pace-developers\/0008-pace-syntax-guiding-principles.md","Context":"## Context\\nWhilst the core framework of PACE is the Horace (and Herbert) projects,\\nPACE involves many other codes, including [Euphonic](https:\/\/github.com\/pace-neutrons\/Euphonic),\\n[Brille](https:\/\/github.com\/brille\/brille) and [SpinW](https:\/\/spinw.org).\\nThe way in which these programs interact with each other is presently unclear.\\nFurthermore the programs are written in several different languages.\\nIn this ADR we propose some guiding principles in the design of the interfaces\\nbetween the programs or components which make up PACE.\\nThis was discussed at a meeting whose\\n[minutes are here.](https:\/\/stfc365.sharepoint.com\/:w:\/r\/sites\/PACEProject\/Shared%20Documents\/Meetings\/PACE-General\/pace_syntax_meeting1_minutes.docx))\\n","Decision":"The guiding principle for PACE-supported programs is **commonality**.\\nThat is, interacting with the different programs of PACE should be **seamless**\\nto the user, and the user interface presented to them should be **uniform**.\\nThe uniform interface means that function names and how users specify parameters\\nor arguments to functions in Matlab and Python flavours of PACE should align\\nas closely as possible.\\n","tokens":183,"id":5178,"text":"## Context\\nWhilst the core framework of PACE is the Horace (and Herbert) projects,\\nPACE involves many other codes, including [Euphonic](https:\/\/github.com\/pace-neutrons\/Euphonic),\\n[Brille](https:\/\/github.com\/brille\/brille) and [SpinW](https:\/\/spinw.org).\\nThe way in which these programs interact with each other is presently unclear.\\nFurthermore the programs are written in several different languages.\\nIn this ADR we propose some guiding principles in the design of the interfaces\\nbetween the programs or components which make up PACE.\\nThis was discussed at a meeting whose\\n[minutes are here.](https:\/\/stfc365.sharepoint.com\/:w:\/r\/sites\/PACEProject\/Shared%20Documents\/Meetings\/PACE-General\/pace_syntax_meeting1_minutes.docx))\\n\n\n##Decision\nThe guiding principle for PACE-supported programs is **commonality**.\\nThat is, interacting with the different programs of PACE should be **seamless**\\nto the user, and the user interface presented to them should be **uniform**.\\nThe uniform interface means that function names and how users specify parameters\\nor arguments to functions in Matlab and Python flavours of PACE should align\\nas closely as possible.\\n","MatchedIds":[5171,5176,5172,1669,1979],"Predicted":"## Decision\nTo ensure interoperability and clarity between PACE-related projects, the following guiding principles will be adopted for designing interfaces:\n\n1. **Language Agnostic APIs:** Interfaces should be designed to be language-agnostic wherever possible. Leveraging technologies such as REST APIs, gRPC, or standardized data exchange formats (e.g. JSON, HDF5) can help in achieving this goal.\n\n2. **Modularization and Decoupling:** Each component should be sufficiently modular and decoupled, allowing it to be developed, maintained, and deployed independently. Defined boundaries and responsibilities for each module must be clear.\n\n3. **Documentation and Standards:** Comprehensive documentation should be available for all interfaces. Standardized naming conventions, data structures, and error handling mechanisms should be adhered to across projects.\n\n4. **Versioning and Compatibility:** Interface versioning practices must be established to ensure backward compatibility where feasible. Clear guidelines for deprecated features and the release cycle should be provided.\n\n5. **Robust Testing Practices:** Interfaces must be subject to extensive unit, integration, and performance testing to ensure reliability and performance consistency. Cross-component test cases that verify interoperability are crucial.\n\n6. **Security Considerations:** Security practices such as authentication, authorization, encryption, and secure data transmission should be integrated into the interface design to protect against vulnerabilities.\n\nAdherence to these principles will facilitate smoother integration, collaboration, and future scalability of the diverse components within the PACE ecosystem.","GenTime":"2024-08-19 23:23:34"}
{"File Name":"TANF-app\/012-antivirus-strategy.md","Context":"## Context\\nOur original plan was to upload files to S3 using signed URLs from S3, and checking the files' headers to determine whether or not it was, in fact, a text file. Further research has revealed that there is no fool proof way of doing this and a malicious user would be able to spoof a virus as a text file.\\n","Decision":"Instead of using a signed URL and sending the file directly to S3, we will instead send the file to the backend and scan it with Clam AV before sending it to S3. In the event there is a virus, we will destroy the file on the backend immediately and return an error to the frontend.\\nBy using the [ClamAV REST server](https:\/\/github.com\/ajilaag\/clamav-rest) implementation we are able to scan files for viruses and malicious behavior. Additionally, Anti-Virus definitions are kept up to date automatically by use of the included [freshclam](https:\/\/www.clamav.net\/documents\/signature-testing-and-management#freshclam) tool which automatically downloads and updates an internal database of virus signatures using the official ClamAV source.\\nIn addition to this, the frontend is able to reliably determine if a file is a binary file. The client will check submitted files for this and immediately return an error to the user. These files will not be sent to the backend.\\n","tokens":72,"id":3797,"text":"## Context\\nOur original plan was to upload files to S3 using signed URLs from S3, and checking the files' headers to determine whether or not it was, in fact, a text file. Further research has revealed that there is no fool proof way of doing this and a malicious user would be able to spoof a virus as a text file.\\n\n\n##Decision\nInstead of using a signed URL and sending the file directly to S3, we will instead send the file to the backend and scan it with Clam AV before sending it to S3. In the event there is a virus, we will destroy the file on the backend immediately and return an error to the frontend.\\nBy using the [ClamAV REST server](https:\/\/github.com\/ajilaag\/clamav-rest) implementation we are able to scan files for viruses and malicious behavior. Additionally, Anti-Virus definitions are kept up to date automatically by use of the included [freshclam](https:\/\/www.clamav.net\/documents\/signature-testing-and-management#freshclam) tool which automatically downloads and updates an internal database of virus signatures using the official ClamAV source.\\nIn addition to this, the frontend is able to reliably determine if a file is a binary file. The client will check submitted files for this and immediately return an error to the user. These files will not be sent to the backend.\\n","MatchedIds":[3987,4533,3416,5099,4358],"Predicted":"## Decision\nWe decided to move the file validation process to be a part of the backend API responsible for handling the uploads. Instead of allowing direct uploads to S3 via presigned URLs, we will have the clients upload files directly to our API endpoint first. This allows us to perform comprehensive server-side validations, including:\n\n1. **Header Inspection:** Verifying file headers to ensure they match expected text file formats.\n2. **Content Scanning:** Scanning file contents for malicious code or malware using an antivirus library or service.\n3. **Size and Format Checks:** Ensuring the file size and format conform to predefined limits and requirements.\n\nOnce validated, the API will then forward the safe file to S3. This approach ensures a higher level of security by providing a controlled environment for file validation, preventing direct interactions with S3 and reducing the risk of malicious files being uploaded.\n\nWhile this change introduces additional steps in the file upload process and some overhead on the backend server, the increased security and control outweigh these costs. Also, to handle the potential increase in load, we can consider using serverless architecture via AWS Lambda or containerized microservices to scale as required.","GenTime":"2024-08-19 23:23:39"}
{"File Name":"dos-server\/adr-2a-java.md","Context":"## Context\\nAs DOS will be an important component of our collections platform,\\nthe software stack (web server, application container, programming language, database)\\nDOS uses to realize core functionality and to fulfill non-functional requirements\\nof scalability, performance, reliability, security, and backward compatibility needs to be robust. Java can help meet various implicit non-functional requirements out of the box. Similarly, the use of proven and\\nmature ecosystem libraries can help meet the functional requirements easily. The Spring\\nframework also makes it relatively easy to create RESTful web services.\\n","Decision":"The core service will rely on Java and Spring framework as the basic stack for implementing core services.\\n","tokens":114,"id":3374,"text":"## Context\\nAs DOS will be an important component of our collections platform,\\nthe software stack (web server, application container, programming language, database)\\nDOS uses to realize core functionality and to fulfill non-functional requirements\\nof scalability, performance, reliability, security, and backward compatibility needs to be robust. Java can help meet various implicit non-functional requirements out of the box. Similarly, the use of proven and\\nmature ecosystem libraries can help meet the functional requirements easily. The Spring\\nframework also makes it relatively easy to create RESTful web services.\\n\n\n##Decision\nThe core service will rely on Java and Spring framework as the basic stack for implementing core services.\\n","MatchedIds":[649,1035,1665,3470,302],"Predicted":"## Decision\nWe will adopt the following software stack for the DOS component of our collections platform to ensure robustness in scalability, performance, reliability, security, and backward compatibility:\n\n### Web Server:\n- Apache Tomcat\n\n### Application Container:\n- Spring Framework\n\n### Programming Language:\n- Java (version 1.8+)\n\n### Database:\n- PostgreSQL\n\nThe Spring Framework will be utilized for creating RESTful web services, leveraging its mature ecosystem to meet both functional and non-functional requirements efficiently. These choices reflect well-established technologies that offer proven performance and reliability, thereby aligning with our goals for the DOS component.","GenTime":"2024-08-19 23:23:42"}
{"File Name":"front-end-monorepo\/adr-21.md","Context":"## Context\\nAuthentication is currently handled by the existing auth client, which is bundled up as part of [panoptes-javascript-client](https:\/\/github.com\/zooniverse\/panoptes-javascript-client\/).\\nWhile working on [#1306](https:\/\/github.com\/zooniverse\/front-end-monorepo\/issues\/1306), I ran into a few issues with it:\\n- Environment variables (env vars) aren't available on the client side. Next.js does have a method for sharing config on both the server and client, but it's academic since:\\n- The only way to configure `panoptes-javascript-client` is _directly_ via env vars, or by passing in query parameters to the URL. We can't get env vars on the client, so that's out, and query parameters become unwieldy very quickly.\\nThis hasn't been an issue when working with Single Page Apps, since env vars are baked in at transpilation time by the build tools. At the moment, we just avoid the problem by having the staging build of the project use the production API.\\nHowever, for the Next.js-based apps we're building right now, we need a different approach, and being able to configure the auth client from a single source of truth is required. Ideally, this would be source-agnostic: the client should be able to be configured from a config file, [env vars](https:\/\/12factor.net\/config), or whatever you want, but that's up to the consuming app to decide.\\n","Decision":"Rewrite the existing auth client as a separate package. The new client will follow a pattern that's well established in the wider ecosystem: the package exposes a function which accepts a config object as an argument and returns a configured client.\\nThe client is purely for authentication, and will expose the methods for registration, sign in, sign out, and getting bearer tokens.\\nThe [existing config](https:\/\/github.com\/zooniverse\/panoptes-javascript-client\/blob\/master\/lib\/config.js) will be turned into a separate package. Config settings can be imported wholesale from that for convenience.\\nFor use in the rebuild apps, we would create a `ConfigStore` which we populate with the relevant config settings. The `ConfigStore` snapshot is then used to rehydrate the client, so we get a single source of truth on both client and server.\\n","tokens":315,"id":527,"text":"## Context\\nAuthentication is currently handled by the existing auth client, which is bundled up as part of [panoptes-javascript-client](https:\/\/github.com\/zooniverse\/panoptes-javascript-client\/).\\nWhile working on [#1306](https:\/\/github.com\/zooniverse\/front-end-monorepo\/issues\/1306), I ran into a few issues with it:\\n- Environment variables (env vars) aren't available on the client side. Next.js does have a method for sharing config on both the server and client, but it's academic since:\\n- The only way to configure `panoptes-javascript-client` is _directly_ via env vars, or by passing in query parameters to the URL. We can't get env vars on the client, so that's out, and query parameters become unwieldy very quickly.\\nThis hasn't been an issue when working with Single Page Apps, since env vars are baked in at transpilation time by the build tools. At the moment, we just avoid the problem by having the staging build of the project use the production API.\\nHowever, for the Next.js-based apps we're building right now, we need a different approach, and being able to configure the auth client from a single source of truth is required. Ideally, this would be source-agnostic: the client should be able to be configured from a config file, [env vars](https:\/\/12factor.net\/config), or whatever you want, but that's up to the consuming app to decide.\\n\n\n##Decision\nRewrite the existing auth client as a separate package. The new client will follow a pattern that's well established in the wider ecosystem: the package exposes a function which accepts a config object as an argument and returns a configured client.\\nThe client is purely for authentication, and will expose the methods for registration, sign in, sign out, and getting bearer tokens.\\nThe [existing config](https:\/\/github.com\/zooniverse\/panoptes-javascript-client\/blob\/master\/lib\/config.js) will be turned into a separate package. Config settings can be imported wholesale from that for convenience.\\nFor use in the rebuild apps, we would create a `ConfigStore` which we populate with the relevant config settings. The `ConfigStore` snapshot is then used to rehydrate the client, so we get a single source of truth on both client and server.\\n","MatchedIds":[2397,2578,577,4443,5211],"Predicted":"## Decision\nRefactor the `panoptes-javascript-client` to support configuration through a central configuration object. This object can be initialized by the consuming application in a way that fits its own environment, whether by using a configuration file, environment variables, or other means.\n\n1. **Create a Configuration Object**: \n   Introduce a configuration object that the consuming application can pass to the auth client during initialization. This will provide flexibility in determining how the configuration values are sourced.\n\n2. **Support Multiple Configuration Sources**:\n   Ensure the configuration object supports initializing from a variety of sources including:\n   - Environment variables (for server-side configurations)\n   - Configuration files (for client-side configurations)\n   - Inline configurations (directly within the application code)\n\n3. **Update Documentation**:\n   Provide clear documentation on how to initialize and use this configuration object with examples for each type of configuration source. This will assist developers in smoothly transitioning to the new method.\n\nThis approach will decouple the configuration process from environment variables and query parameters, making it more flexible and suitable for various types of applications, including those built with Next.js. It ensures a single source of truth for the configuration, adhering to the 12-factor app principle and allowing each application to configure the auth client in a way that suits its specific needs.","GenTime":"2024-08-19 23:23:48"}
{"File Name":"bfi-discovery\/0002-choice-of-auth-provider.md","Context":"## Context and Problem Statement\\nIn order to deliver BFI's IIIF Universal Viewer auditing platform, an\\nidentity and access management solution must be provisioned which\\nsupports user creation and registration, user management, and\\nauthentication and authorisation.\\n## Decision Drivers\\n* Ease of initial deployment and configuration of the solution.\\n* Ongoing hosting and maintenance costs of the solution.\\n* Availability of core features to satisfy the project requirements with\\nno \/ minimal costs.\\n","Decision":"* Ease of initial deployment and configuration of the solution.\\n* Ongoing hosting and maintenance costs of the solution.\\n* Availability of core features to satisfy the project requirements with\\nno \/ minimal costs.\\nAuth0 is selected as the solution, due to its managed nature requiring\\nzero deployment and effort. After an evaluation of features compared\\nagainst the project requirements, it has been determined that Auth0's\\nfree tier will suffice and as a result there are no ongoing costs\\neither.\\n### Positive Consequences\\n* As a managed solution, there are no initial deployment or ongoing\\nhosting \/ infrastructure costs.\\n* Patching and maintenance of the solution is provided by Auth0 at no\\nadditional cost \/ interruption to the customer.\\n* It is expected that (at least initially) the free tier will suffice to\\nmeet the requirements of the project.\\n### Negative Consequences\\n* The features and customisations available are limited without\\nupgrading to a paid tier:\\n* Email templates cannot be customised on the free plan.\\n* A custom domain cannot be used to serve the Auth0 tenant.\\n* Enterprise integrations (AD, LDAP, etc) are not available.\\n* Multifactor authentication is not available.\\n* Auth0 hosted UI's can be customised but only to a limited extent.\\n* Limit of 7000 monthly active users.\\n","tokens":96,"id":4857,"text":"## Context and Problem Statement\\nIn order to deliver BFI's IIIF Universal Viewer auditing platform, an\\nidentity and access management solution must be provisioned which\\nsupports user creation and registration, user management, and\\nauthentication and authorisation.\\n## Decision Drivers\\n* Ease of initial deployment and configuration of the solution.\\n* Ongoing hosting and maintenance costs of the solution.\\n* Availability of core features to satisfy the project requirements with\\nno \/ minimal costs.\\n\n\n##Decision\n* Ease of initial deployment and configuration of the solution.\\n* Ongoing hosting and maintenance costs of the solution.\\n* Availability of core features to satisfy the project requirements with\\nno \/ minimal costs.\\nAuth0 is selected as the solution, due to its managed nature requiring\\nzero deployment and effort. After an evaluation of features compared\\nagainst the project requirements, it has been determined that Auth0's\\nfree tier will suffice and as a result there are no ongoing costs\\neither.\\n### Positive Consequences\\n* As a managed solution, there are no initial deployment or ongoing\\nhosting \/ infrastructure costs.\\n* Patching and maintenance of the solution is provided by Auth0 at no\\nadditional cost \/ interruption to the customer.\\n* It is expected that (at least initially) the free tier will suffice to\\nmeet the requirements of the project.\\n### Negative Consequences\\n* The features and customisations available are limited without\\nupgrading to a paid tier:\\n* Email templates cannot be customised on the free plan.\\n* A custom domain cannot be used to serve the Auth0 tenant.\\n* Enterprise integrations (AD, LDAP, etc) are not available.\\n* Multifactor authentication is not available.\\n* Auth0 hosted UI's can be customised but only to a limited extent.\\n* Limit of 7000 monthly active users.\\n","MatchedIds":[5211,3421,2582,1252,2405],"Predicted":"## Decision\nWe will use Auth0 as the identity and access management solution for BFI's IIIF Universal Viewer auditing platform. Auth0 provides comprehensive user creation, registration, management, and authentication\/authorization capabilities out of the box. Its ease of initial deployment and configuration ensures a quick start, while the availability of a free tier can help keep initial costs minimal. Ongoing hosting and maintenance are managed by Auth0, reducing the burden on our development team.","GenTime":"2024-08-19 23:23:51"}
{"File Name":"copilot\/0003-use-mcp-api-for-pilot-config.md","Context":"## Context\\nMesh Configuration Protocol (MCP) is a [protocol](https:\/\/github.com\/istio\/api\/tree\/master\/mcp) for transferring configuration among Istio components during runtime. MCP is meant to defer all the logics and complexities back to the server (copilot) as oppose to the original design which all the logic was embeded in the client (Pilot). Another goal of MCP is to create a unified contract for all the Custom Resource Definitions and Service Discovery and the way they are communicated with Pilot.\\n","Decision":"Copilot will implement a MCP server to send configuration to Pilot. We will be sending definitions for Gateways, VirtualServices and DestinationRules over bi-directional GRPC.\\n","tokens":106,"id":49,"text":"## Context\\nMesh Configuration Protocol (MCP) is a [protocol](https:\/\/github.com\/istio\/api\/tree\/master\/mcp) for transferring configuration among Istio components during runtime. MCP is meant to defer all the logics and complexities back to the server (copilot) as oppose to the original design which all the logic was embeded in the client (Pilot). Another goal of MCP is to create a unified contract for all the Custom Resource Definitions and Service Discovery and the way they are communicated with Pilot.\\n\n\n##Decision\nCopilot will implement a MCP server to send configuration to Pilot. We will be sending definitions for Gateways, VirtualServices and DestinationRules over bi-directional GRPC.\\n","MatchedIds":[2249,2236,2240,3909,4538],"Predicted":"## Decision\nWe will adopt the Mesh Configuration Protocol (MCP) for Istio's runtime configuration management. MCP will be implemented to transfer configurations between Istio components, centralizing logic and complexity on the server (copilot) rather than embedding it within the client (Pilot). This approach aims to streamline the configuration processes and ensure consistent communication for Custom Resource Definitions and Service Discovery with Pilot, thereby creating a unified and efficient contract.","GenTime":"2024-08-19 23:23:54"}
{"File Name":"content-data-api\/adr-006-track-metrics-via-time-dimension.md","Context":"## Context\\nWe would benefit from having a central repository of integrated data from multiple sources that stores current and historical information, and use this data to create analytical reports and performance indicators to support the publishing workflow within GOV.UK.\\nThis is actually difficult to achieve as we have the information dispersed across different applications that are currently designed to support transactional operations and not analytical reporting of integrated data.\\n","Decision":"Build a data warehouse(*) that maintains a copy of the information of the transactional systems.\\n(*) We will be using a PostgreSQL database in the first iteration, we will be exploring other existing solutions for data warehouses in future iterations once we have validated this approach.\\n","tokens":80,"id":1863,"text":"## Context\\nWe would benefit from having a central repository of integrated data from multiple sources that stores current and historical information, and use this data to create analytical reports and performance indicators to support the publishing workflow within GOV.UK.\\nThis is actually difficult to achieve as we have the information dispersed across different applications that are currently designed to support transactional operations and not analytical reporting of integrated data.\\n\n\n##Decision\nBuild a data warehouse(*) that maintains a copy of the information of the transactional systems.\\n(*) We will be using a PostgreSQL database in the first iteration, we will be exploring other existing solutions for data warehouses in future iterations once we have validated this approach.\\n","MatchedIds":[1701,4041,1016,2410,2403],"Predicted":"## Decision\nWe will implement a data warehouse to serve as the central repository of integrated data from multiple sources. The data warehouse will be designed to store both current and historical information, enabling the creation of analytical reports and performance indicators.\n\n### Steps:\n1. **Data Extraction**: Use ETL (Extract, Transform, Load) processes to extract data from the various transactional applications.\n2. **Data Transformation**: Transform the extracted data to conform to a unified data schema and cleanse the data to ensure consistency and accuracy.\n3. **Data Loading**: Load the transformed data into the data warehouse on a scheduled basis.\n4. **Data Storage**: Utilize a scalable and robust data warehouse platform such as Amazon Redshift, Google BigQuery, or Snowflake.\n5. **Data Access**: Provide access to the data warehouse through SQL endpoints and integrate with BI (Business Intelligence) tools like Looker, Tableau, or Power BI for reporting and analysis.\n6. **Security and Compliance**: Ensure that the data warehouse is compliant with data protection regulations and policies, implementing necessary security measures such as encryption and access controls.\n\nThis approach will allow us to aggregate and analyze data across different applications, facilitating more informed decision-making and supporting the publishing workflow within GOV.UK.","GenTime":"2024-08-19 23:23:59"}
{"File Name":"adr\/ADR-36-kernel-repository-separation.md","Context":"## Context and Problem Statement\\nAs of today, we already decoupled the `renderer` (decentraland\/unity-renderer) from the `explorer` repository. And we are in the process of decoupling `website` (decentraland\/explorer-website). The ECS and building tools are also in the path of being migrated to their own repositories (decentraland\/js-sdk-toolchain).\\nThe context of the problem is around the tactical\/execution aspect. Since the changes affect directly the publishing cycles and processes of every package. We must optimize not blocking development and also not breaking things in a way that prevent us from releasing hotfixes.\\n","Decision":"We choose to create a separated repository to not block the releases and development of explorer during this migration stage.\\nAlso having separated repositories, will foster external contributions, ensuring clear scopes and responsibilities for each repository.\\n","tokens":135,"id":4609,"text":"## Context and Problem Statement\\nAs of today, we already decoupled the `renderer` (decentraland\/unity-renderer) from the `explorer` repository. And we are in the process of decoupling `website` (decentraland\/explorer-website). The ECS and building tools are also in the path of being migrated to their own repositories (decentraland\/js-sdk-toolchain).\\nThe context of the problem is around the tactical\/execution aspect. Since the changes affect directly the publishing cycles and processes of every package. We must optimize not blocking development and also not breaking things in a way that prevent us from releasing hotfixes.\\n\n\n##Decision\nWe choose to create a separated repository to not block the releases and development of explorer during this migration stage.\\nAlso having separated repositories, will foster external contributions, ensuring clear scopes and responsibilities for each repository.\\n","MatchedIds":[4618,4614,1117,3031,4613],"Predicted":"## Decision\nTo handle the publishing cycles and processes efficiently while preventing disruptions and enabling hotfix releases, we will implement the following strategy:\n\n1. **Establish Clear Versioning and Release Policies:**\n   - Use Semantic Versioning (SemVer) across all repositories to clearly communicate the impact of changes (major, minor, patch).\n   - Enforce release branches (e.g., `release\/x.x.x`) to stabilize code before pushing to production.\n\n2. **Automated CI\/CD Pipelines:**\n   - Implement and configure CI\/CD pipelines for each repository using tools like GitHub Actions or Jenkins.\n   - Pipelines will include build, test, and deployment stages to ensure consistency and reliability.\n   - Set up automated deployments for minor and patch releases to reduce manual intervention and speed up the release process.\n\n3. **Incremental and Independent Deployment:**\n   - Each decoupled repository will have its own deployment pipeline to ensure that updates can be rolled out independently.\n   - Ensure that each service or package is self-sufficient and can be tested in isolation before being integrated.\n\n4. **Hotfix Process:**\n   - Create a dedicated `hotfix` branch from the latest stable branch when an urgent fix is needed.\n   - Ensure that hotfixes can be merged back into the main branch and any release branches without causing conflicts.\n   - Deploy hotfixes through their own streamlined pipeline to ensure they can be deployed swiftly.\n\n5. **Version Pinning and Dependency Management:**\n   - Use version pinning in dependency management to avoid unwanted automatic updates that could introduce breaking changes.\n   - Keep a dependency update schedule and rigorously test updates to ensure compatibility.\n\n6. **Communication and Coordination:**\n   - Schedule regular meetings between the different teams to discuss upcoming changes, dependencies, and potential impacts.\n   - Maintain comprehensive documentation for each repository, including setup instructions, deployment guides, and troubleshooting steps.\n\n7. **Monitoring and Rollback Mechanism:**\n   - Implement monitoring tools to track the performance and stability of each deployed service.\n   - Set up a rollback mechanism to revert to the previous stable version if a new release introduces critical issues.\n\nBy adopting this strategy, we can streamline the publishing cycles, minimize disruption to development, and ensure a robust mechanism for releasing hotfixes efficiently.","GenTime":"2024-08-19 23:24:08"}
{"File Name":"gsp\/ADR011-build-artefacts.md","Context":"## Context\\nAs part of our pipelines we will be building artefacts that will be used to test\\nand deploy our applications. We will be deploying applications to Kubernetes. We\\nwill need to build a container image of some kind.\\nThere are some competing container image formats, namely:\\n* [OCI]\\n* [ACI]\\nThe OCI image format is [based on the Docker v2][oci-standard] image format.\\nThe Kubernetes project appears to [prefer Docker\/OCI][k8s-preferance] images\\nover ACI.\\n[rkt is moving to OCI][rkt-oci] and away from ACI. OCI will become the preferred\\nimage format.\\nDocker has wide industry adoption and appears to have wide understanding within\\nGDS.\\nDocker is the default container runtime for Kubernetes.\\n","Decision":"We will build and store OCI images built using Docker.\\n","tokens":170,"id":3920,"text":"## Context\\nAs part of our pipelines we will be building artefacts that will be used to test\\nand deploy our applications. We will be deploying applications to Kubernetes. We\\nwill need to build a container image of some kind.\\nThere are some competing container image formats, namely:\\n* [OCI]\\n* [ACI]\\nThe OCI image format is [based on the Docker v2][oci-standard] image format.\\nThe Kubernetes project appears to [prefer Docker\/OCI][k8s-preferance] images\\nover ACI.\\n[rkt is moving to OCI][rkt-oci] and away from ACI. OCI will become the preferred\\nimage format.\\nDocker has wide industry adoption and appears to have wide understanding within\\nGDS.\\nDocker is the default container runtime for Kubernetes.\\n\n\n##Decision\nWe will build and store OCI images built using Docker.\\n","MatchedIds":[1376,74,3917,3598,3604],"Predicted":"## Decision\nWe will standardize on using OCI (Open Container Initiative) image format for building container images as part of our pipelines. This choice aligns with Kubernetes' preference for Docker\/OCI images and follows industry trends, as well as the shift from ACI to OCI.\n\nGiven that Docker, which is based on OCI, has wide industry adoption and is already well-understood within our organization, opting for OCI provides consistency and leverages existing knowledge and tooling. \n\nFurthermore, using OCI ensures compatibility with Kubernetes, which is the primary target environment for our deployments. This decision will contribute to a more seamless integration and deployment process within our Kubernetes clusters.","GenTime":"2024-08-19 23:24:11"}
{"File Name":"clone_difitalcitizenship\/0003-follow-a-cloud-native-design-principle.md","Context":"## Context\\nWe need to design the building blocks of the Digital Citizenship platform. We will make decisions about the overall architecture and the building blocks we're going to use to satisfy the functional and non-functional requirements.\\nWhen deciding what technologies to use and how to satisfy those requirements, we're going to evaluate whether we want to build certain components from scratch, use open-source solutions managed by us or rely on existing public cloud components, managed by 3rd party providers.\\nAs the world of cloud technologies continues to accelerate, we should absorb new developments into how we work. Leading technology organisations are rapidly embracing new tools like \"serverless\" computing.\\nAt the infrastructure and application level we should expect our applications to be resilient, flexible and API-driven. We should have the tools and practices in place to manage and secure a distributed range of tools accessed over the internet.\\nDecision criteria will also include:\\n* elasticity and resilience\\n* pay-as-you-go pricing\\n* exploiting economies of scale\\n* exploiting falling costs of infrastructure over time\\n* quality of management tools\\n* best of breed security\\n* flexibility and opportunity costs\\n","Decision":"* elasticity and resilience\\n* pay-as-you-go pricing\\n* exploiting economies of scale\\n* exploiting falling costs of infrastructure over time\\n* quality of management tools\\n* best of breed security\\n* flexibility and opportunity costs\\nWe decide to build the components of the Digital Citizenship initiative on top of scalable, managed and cost-effective components provided by the leading public cloud providers.\\n","tokens":227,"id":1182,"text":"## Context\\nWe need to design the building blocks of the Digital Citizenship platform. We will make decisions about the overall architecture and the building blocks we're going to use to satisfy the functional and non-functional requirements.\\nWhen deciding what technologies to use and how to satisfy those requirements, we're going to evaluate whether we want to build certain components from scratch, use open-source solutions managed by us or rely on existing public cloud components, managed by 3rd party providers.\\nAs the world of cloud technologies continues to accelerate, we should absorb new developments into how we work. Leading technology organisations are rapidly embracing new tools like \"serverless\" computing.\\nAt the infrastructure and application level we should expect our applications to be resilient, flexible and API-driven. We should have the tools and practices in place to manage and secure a distributed range of tools accessed over the internet.\\nDecision criteria will also include:\\n* elasticity and resilience\\n* pay-as-you-go pricing\\n* exploiting economies of scale\\n* exploiting falling costs of infrastructure over time\\n* quality of management tools\\n* best of breed security\\n* flexibility and opportunity costs\\n\n\n##Decision\n* elasticity and resilience\\n* pay-as-you-go pricing\\n* exploiting economies of scale\\n* exploiting falling costs of infrastructure over time\\n* quality of management tools\\n* best of breed security\\n* flexibility and opportunity costs\\nWe decide to build the components of the Digital Citizenship initiative on top of scalable, managed and cost-effective components provided by the leading public cloud providers.\\n","MatchedIds":[4041,3488,875,2093,2261],"Predicted":"## Decision\nWe will leverage a serverless architecture for the Digital Citizenship platform, primarily utilizing public cloud components managed by third-party providers. This approach aligns with our criteria for elasticity, resilience, and pay-as-you-go pricing. By using services such as AWS Lambda, Azure Functions, or Google Cloud Functions, we can reduce operational overhead and exploit economies of scale and falling infrastructure costs.\n\nManaged services will form the backbone of our architecture, using tools like AWS DynamoDB or Azure Cosmos DB for scalable, distributed databases, and API Gateway services for creating and managing APIs. This will ensure our applications are flexible and API-driven while providing high-quality management tools and best-of-breed security.\n\nWhere appropriate, we will integrate open-source solutions managed by our team for specific needs that require customization or where managed services do not meet all requirements. These decisions will be evaluated based on flexibility, opportunity costs, and overall fit within our architectural framework. \n\nThis strategic choice will allow us to rapidly adapt to new developments in cloud technologies and maintain a modern, resilient, and secure platform.","GenTime":"2024-08-19 23:24:15"}
{"File Name":"openfido-app-service\/0002-project-structure.md","Context":"## Context\\nThe OpenFIDO project will mediate access to a workflow service that runs\\nGridLabD jobs. The frontend React application will need access to these\\nresources, but only if they have permission to access via an organization.\\n","Decision":"Create a Flask Rest service - since the other microservices in this suite of\\nservices are also Flask based, keep this using the same kind of infrastructure\\nso that common utilities can be shared (openfido-utils) and the maintenance will\\nbe simplified.\\nOrganize the database logic into a simplified CQRS-inspired style code\\nstructure. Since we anticipate many conceptual resources, each resource will\\nhave its own module:\\n* app\/RESOURCE\/models.py - contains all models for RESOURCE.\\n* app\/RESOURCE\/routes.py - contains all Rest routes specific to RESOURCE.\\n* app\/RESOURCE\/schemas.py - contains all Marshmallow schemas specific to routes of RESOURCE.\\n* app\/RESOURCE\/services.py - all db commands that modify database state.\\n* app\/RESOURCE\/queries.py - all db queries to the database.\\nAdditional libraries we anticipate using:\\n* [marshmallow](https:\/\/marshmallow.readthedocs.io\/en\/stable\/) will be used since there are many rest api endpoints that will take nested bodies.\\n* [alembic](https:\/\/alembic.sqlalchemy.org\/en\/latest\/) to manage database schema migrations.\\nOther thoughts on design:\\n* Because HTTP errors are intended for human use via the OpenFIDO client, messages should be verbose - showing both a generic message and specific field validation messages where appropriate.\\nUse the structure of the [openfido-workflow-service](https:\/\/github.com\/slacgismo\/openfido-workflow-service) project as a reference.\\n","tokens":49,"id":2470,"text":"## Context\\nThe OpenFIDO project will mediate access to a workflow service that runs\\nGridLabD jobs. The frontend React application will need access to these\\nresources, but only if they have permission to access via an organization.\\n\n\n##Decision\nCreate a Flask Rest service - since the other microservices in this suite of\\nservices are also Flask based, keep this using the same kind of infrastructure\\nso that common utilities can be shared (openfido-utils) and the maintenance will\\nbe simplified.\\nOrganize the database logic into a simplified CQRS-inspired style code\\nstructure. Since we anticipate many conceptual resources, each resource will\\nhave its own module:\\n* app\/RESOURCE\/models.py - contains all models for RESOURCE.\\n* app\/RESOURCE\/routes.py - contains all Rest routes specific to RESOURCE.\\n* app\/RESOURCE\/schemas.py - contains all Marshmallow schemas specific to routes of RESOURCE.\\n* app\/RESOURCE\/services.py - all db commands that modify database state.\\n* app\/RESOURCE\/queries.py - all db queries to the database.\\nAdditional libraries we anticipate using:\\n* [marshmallow](https:\/\/marshmallow.readthedocs.io\/en\/stable\/) will be used since there are many rest api endpoints that will take nested bodies.\\n* [alembic](https:\/\/alembic.sqlalchemy.org\/en\/latest\/) to manage database schema migrations.\\nOther thoughts on design:\\n* Because HTTP errors are intended for human use via the OpenFIDO client, messages should be verbose - showing both a generic message and specific field validation messages where appropriate.\\nUse the structure of the [openfido-workflow-service](https:\/\/github.com\/slacgismo\/openfido-workflow-service) project as a reference.\\n","MatchedIds":[2397,4388,2582,847,527],"Predicted":"## Decision\nWe will implement an authorization mechanism using a combination of JWT tokens and Role-Based Access Control (RBAC). \n\n1. **JWT Tokens**: The backend service will issue JWT tokens upon successful authentication. These tokens will contain claims about the user's identity and their associated organization.\n\n2. **RBAC**: Roles and permissions will be managed using a library such as `casbin` for fine-grained access control. Each organization will have roles that determine the level of access to GridLabD job resources. \n\n3. **Middleware**: A middleware layer will be added to the backend API that verifies the JWT token and checks if the user has the necessary permissions to access specific resources based on their roles.\n\n4. **Frontend Integration**: The React application will include logic to handle token storage (e.g., in `localStorage`), and it will attach the JWT token to API requests via an HTTP interceptor.\n\n5. **Access Management UI**: An administrative interface will be provided within the React application to enable organization administrators to manage roles and permissions.\n\nThis approach ensures that access to GridLabD job resources is securely mediated via organizational permissions, leveraging JWT for secure token-based authentication, and RBAC for flexible access control.","GenTime":"2024-08-19 23:24:26"}
{"File Name":"buildit-all\/0008-database.md","Context":"## Context\\nBookit needs to persist the locations, bookables, and bookings so that the data survives multiple instances and deployments over time.\\n","Decision":"* Use SQL approach as opposed to NoSQL solution - the model is simple and ACID transactions keep multiple users separate\\n* Use H2 for unit testing & local development - speeds up execution time and reduces external dependencies\\n* Use AWS RDS Aurora (MySQL) for integration\/staging\/production - better HA & continuous snapshots (enabled for production)\\n* Use MariaDB JDBC driver - has native Aurora support for failover\\n","tokens":29,"id":4415,"text":"## Context\\nBookit needs to persist the locations, bookables, and bookings so that the data survives multiple instances and deployments over time.\\n\n\n##Decision\n* Use SQL approach as opposed to NoSQL solution - the model is simple and ACID transactions keep multiple users separate\\n* Use H2 for unit testing & local development - speeds up execution time and reduces external dependencies\\n* Use AWS RDS Aurora (MySQL) for integration\/staging\/production - better HA & continuous snapshots (enabled for production)\\n* Use MariaDB JDBC driver - has native Aurora support for failover\\n","MatchedIds":[1740,2488,4859,3966,2221],"Predicted":"## Decision\nAfter evaluating various persistence solutions, we have decided to use AWS RDS PostgreSQL for our persistence mechanism. This decision is based on the following considerations:\n\n1. **Data Integrity**: PostgreSQL offers robust ACID compliance which ensures the integrity of our transactional data such as bookings.\n2. **Scalability**: AWS RDS allows for easy scaling as our application's data and user load grow, without extensive management overhead.\n3. **Performance**: PostgreSQL provides excellent performance and supports complex queries and indexing, which can be beneficial for our queries involving locations and bookables.\n4. **High Availability**: AWS RDS offers automated backups, snapshots, and Multi-AZ deployments for high availability and fault tolerance.\n5. **Compatibility**: PostgreSQL's full support with various ORMs and its strong community make it a good fit for future-proofing our application. \n\nWe will set up the database using AWS RDS with the following specifics:\n- **Production Environment**: Will run in a Multi-AZ deployment to ensure high availability and automated backups.\n- **Development and Testing Environments**: Will run in a single-AZ setup to keep costs lower while providing sufficient resources for development and testing.\n\nThis setup will be managed and deployed using AWS CloudFormation templates to ensure consistency and repeatability across environments.","GenTime":"2024-08-19 23:24:31"}
{"File Name":"deeplearning4j\/0003-dealing_with_inconsistencies_in_java_naming.md","Context":"## Context\\nThere are slight inconsistencies in naming between existing op class definitions and factory methods. For example a\\nfactory method called `bernoulli` in the `random` namespace with a corresponding op class called\\n`BernoulliDistribution`.\\nTwo possible solutions where suggested:\\n1. Add an additional property that provides us with the correct class name\\n2. Rename classes in ND4J to ensure consistency and provide backwards compatibility via deprecated subclasses\\n","Decision":"For now we will introduce a `javaOpClass` property which in cases of inconsistency provides us with the correct class\\nname.\\n","tokens":92,"id":2934,"text":"## Context\\nThere are slight inconsistencies in naming between existing op class definitions and factory methods. For example a\\nfactory method called `bernoulli` in the `random` namespace with a corresponding op class called\\n`BernoulliDistribution`.\\nTwo possible solutions where suggested:\\n1. Add an additional property that provides us with the correct class name\\n2. Rename classes in ND4J to ensure consistency and provide backwards compatibility via deprecated subclasses\\n\n\n##Decision\nFor now we will introduce a `javaOpClass` property which in cases of inconsistency provides us with the correct class\\nname.\\n","MatchedIds":[1193,2932,994,2945,2270],"Predicted":"## Decision\nWe will proceed with the second solution: renaming classes in ND4J to ensure consistency while providing backwards compatibility via deprecated subclasses. \n\nThis will involve:\n1. Renaming existing classes to have consistent naming with factory methods.\n2. Introducing deprecated subclasses with the old names that extend the renamed classes. This ensures that existing codebases that rely on the old names will not break immediately and can be migrated smoothly over time.\n\nFor example:\n```java\n\/\/ Old class, now deprecated\n@Deprecated\nclass BernoulliDistribution extends Bernoulli {\n    \/\/ Legacy code can still use this class, which internally uses the new class.\n}\n\n\/\/ New class with consistent naming\nclass Bernoulli {\n    \/\/ Implementation details\n}\n```\n\nThis approach ensures that:\n1. Future developments have a consistent and intuitive naming convention.\n2. Existing users have an easy upgrade path without immediate breaking changes, maintaining the stability of dependent projects.","GenTime":"2024-08-19 23:24:35"}
{"File Name":"dotfiles\/0002-replace-travis-ci-with-github-actions.md","Context":"## Context\\nGitHub Actions are the new shiny and have been in Beta for sometime. They are\\ngoing to be generally available on\\n[2019-11-13](https:\/\/github.blog\/2019-08-08-github-actions-now-supports-ci-cd\/).\\nThey are a thing that will have a long term future. It is likely GitHub Actions\\nwill become the default CI mechanism (and possibly more) for projects hosted on\\nGitHub. Using them in this repository which has a very basic use case will\\nprovide some exposure to the service.\\n","Decision":"The decision is to replace Travis CI with GitHub Actions.\\n","tokens":116,"id":118,"text":"## Context\\nGitHub Actions are the new shiny and have been in Beta for sometime. They are\\ngoing to be generally available on\\n[2019-11-13](https:\/\/github.blog\/2019-08-08-github-actions-now-supports-ci-cd\/).\\nThey are a thing that will have a long term future. It is likely GitHub Actions\\nwill become the default CI mechanism (and possibly more) for projects hosted on\\nGitHub. Using them in this repository which has a very basic use case will\\nprovide some exposure to the service.\\n\n\n##Decision\nThe decision is to replace Travis CI with GitHub Actions.\\n","MatchedIds":[152,3456,2393,1741,3328],"Predicted":"## Decision\nWe will implement GitHub Actions for CI\/CD in this repository to gain experience with the new service. This will help us evaluate its capabilities and prepare for its likely future as the standard CI mechanism for GitHub-hosted projects.","GenTime":"2024-08-19 23:24:38"}
{"File Name":"signals-frontend\/0009-stop-mocking-fetch-in-tests.md","Context":"## Context\\nFor mocking unittests we are using at this moment the `jest-fetch-mock` package. This works but there are new develoments on the market. As the creator of the [Testing Library](https:\/\/testing-library.com\/) suggests in this [article](https:\/\/kentcdodds.com\/blog\/stop-mocking-fetch), there is a new strategy available that would simplify testing of components that are making fetch calls.\\n# Technical description\\nThe `msw` server intercepts the fetch calls and returns the configured response. This is done outside the test file and only overrides of the standard responses from the default configuration need to be added to the respective test file. [Here is the documentation](https:\/\/github.com\/mswjs\/msw).\\n","Decision":"The mocking will be done as much as possible by using the the `msw` package.\\n","tokens":156,"id":3050,"text":"## Context\\nFor mocking unittests we are using at this moment the `jest-fetch-mock` package. This works but there are new develoments on the market. As the creator of the [Testing Library](https:\/\/testing-library.com\/) suggests in this [article](https:\/\/kentcdodds.com\/blog\/stop-mocking-fetch), there is a new strategy available that would simplify testing of components that are making fetch calls.\\n# Technical description\\nThe `msw` server intercepts the fetch calls and returns the configured response. This is done outside the test file and only overrides of the standard responses from the default configuration need to be added to the respective test file. [Here is the documentation](https:\/\/github.com\/mswjs\/msw).\\n\n\n##Decision\nThe mocking will be done as much as possible by using the the `msw` package.\\n","MatchedIds":[452,4223,3323,1387,2816],"Predicted":"## Decision\nWe will adopt `msw` (Mock Service Worker) for mocking fetch calls in our unit tests. This approach provides a more seamless and maintainable strategy for intercepting and controlling HTTP requests, aligning with the best practices recommended by the Testing Library community.","GenTime":"2024-08-19 23:24:40"}
{"File Name":"tracking-consent-frontend\/0005-use-data-attribute-for-language.md","Context":"## Context and Problem Statement\\nTracking consent reads the PLAY_LANG cookie to determine whether to render the banner in Welsh. PLAY_LANG is the cookie\\nused by the Play Framework to persist a user's language preferences. It will be set to 'cy'\\nwhen a user has selected Welsh using the language toggle in MDTP services using the Play Framework.\\nTeams are increasingly setting PLAY_LANG to HttpOnly in an attempt to get green ZAP tests, even though there are no\\nknown security concerns around keeping PLAY_LANG as a normal cookie. Setting a cookie to\\nHttpOnly makes it unreadable within the client-side Javascript code that renders the tracking consent banner. The result\\nof this is that the banner will not be translated into Welsh for these services.\\nA related issue is that PLAY_LANG is not set for classic services written in Java, which means a Welsh version of the banner is not\\ncurrently available for classic services.\\nIt is worth noting that the only other known instance of our reading PLAY_LANG using Javascript is in the assets-frontend\\n[timeout dialog](https:\/\/github.com\/hmrc\/assets-frontend\/blob\/97c638289e23bee255ac30724a8572c6efa96817\/assets\/patterns\/help-users-when-we-time-them-out-of-a-service\/timeoutDialog.js#L14) timeout dialog. All the new govuk-frontend and hmrc-frontend components use data attributes instead.\\nShould we remove the reading of PLAY_LANG in tracking consent and accept a data-language attribute instead?\\n## Decision Drivers\\n* The need to support classic services\\n* The time-sensitive nature of this issue. The need to deploy quickly before too many services have integrated, to\\navoid services having to upgrade a second time.\\n* The preference for avoiding further changes to tracking consent. This is a non-breaking change.\\n* The preference for not advising teams to add exemptions to ZAP tests across MDTP\\n* The preference for consistency in language settings across our frontend components\\n","Decision":"* The need to support classic services\\n* The time-sensitive nature of this issue. The need to deploy quickly before too many services have integrated, to\\navoid services having to upgrade a second time.\\n* The preference for avoiding further changes to tracking consent. This is a non-breaking change.\\n* The preference for not advising teams to add exemptions to ZAP tests across MDTP\\n* The preference for consistency in language settings across our frontend components\\nChosen option: \"Re-work\" because we need to act now and in the medium term we are not in a position to uncouple services'\\ndependency on PLAY_LANG nor add a global exemption for PLAY_LANG into Zap tests. We also agreed that our frontend\\ncomponents should be consistent in their treatment of language until such time as we are able to provide an\\nalternative approach that works for all components.\\n### Positive Consequences\\n* Classic services or services using other non-Scala or non-Play frameworks can get Welsh translations for the banner\\n* Services do not need to set any HttpOnly exemptions in their ZAP tests, which may mask other legitimate warnings\/errors\\n* Language setting is consistent with other components e.g. hmrcTimeoutDialog, hmrcReportATechnicalProblem etc\\n### Negative Consequences\\n* We will need to make a small change to tracking consent and communicate this change to teams\\n* Teams will need to upgrade to the latest version of play-ui\/play-frontend-hmrc to get language translations\\n* Teams not using the tracking consent helpers will need to add a data-attribute for the language\\n","tokens":409,"id":3526,"text":"## Context and Problem Statement\\nTracking consent reads the PLAY_LANG cookie to determine whether to render the banner in Welsh. PLAY_LANG is the cookie\\nused by the Play Framework to persist a user's language preferences. It will be set to 'cy'\\nwhen a user has selected Welsh using the language toggle in MDTP services using the Play Framework.\\nTeams are increasingly setting PLAY_LANG to HttpOnly in an attempt to get green ZAP tests, even though there are no\\nknown security concerns around keeping PLAY_LANG as a normal cookie. Setting a cookie to\\nHttpOnly makes it unreadable within the client-side Javascript code that renders the tracking consent banner. The result\\nof this is that the banner will not be translated into Welsh for these services.\\nA related issue is that PLAY_LANG is not set for classic services written in Java, which means a Welsh version of the banner is not\\ncurrently available for classic services.\\nIt is worth noting that the only other known instance of our reading PLAY_LANG using Javascript is in the assets-frontend\\n[timeout dialog](https:\/\/github.com\/hmrc\/assets-frontend\/blob\/97c638289e23bee255ac30724a8572c6efa96817\/assets\/patterns\/help-users-when-we-time-them-out-of-a-service\/timeoutDialog.js#L14) timeout dialog. All the new govuk-frontend and hmrc-frontend components use data attributes instead.\\nShould we remove the reading of PLAY_LANG in tracking consent and accept a data-language attribute instead?\\n## Decision Drivers\\n* The need to support classic services\\n* The time-sensitive nature of this issue. The need to deploy quickly before too many services have integrated, to\\navoid services having to upgrade a second time.\\n* The preference for avoiding further changes to tracking consent. This is a non-breaking change.\\n* The preference for not advising teams to add exemptions to ZAP tests across MDTP\\n* The preference for consistency in language settings across our frontend components\\n\n\n##Decision\n* The need to support classic services\\n* The time-sensitive nature of this issue. The need to deploy quickly before too many services have integrated, to\\navoid services having to upgrade a second time.\\n* The preference for avoiding further changes to tracking consent. This is a non-breaking change.\\n* The preference for not advising teams to add exemptions to ZAP tests across MDTP\\n* The preference for consistency in language settings across our frontend components\\nChosen option: \"Re-work\" because we need to act now and in the medium term we are not in a position to uncouple services'\\ndependency on PLAY_LANG nor add a global exemption for PLAY_LANG into Zap tests. We also agreed that our frontend\\ncomponents should be consistent in their treatment of language until such time as we are able to provide an\\nalternative approach that works for all components.\\n### Positive Consequences\\n* Classic services or services using other non-Scala or non-Play frameworks can get Welsh translations for the banner\\n* Services do not need to set any HttpOnly exemptions in their ZAP tests, which may mask other legitimate warnings\/errors\\n* Language setting is consistent with other components e.g. hmrcTimeoutDialog, hmrcReportATechnicalProblem etc\\n### Negative Consequences\\n* We will need to make a small change to tracking consent and communicate this change to teams\\n* Teams will need to upgrade to the latest version of play-ui\/play-frontend-hmrc to get language translations\\n* Teams not using the tracking consent helpers will need to add a data-attribute for the language\\n","MatchedIds":[3529,560,263,1754,1000],"Predicted":"## Decision\n* The need to support classic services\n* The time-sensitive nature of this issue. The need to deploy quickly before too many services have integrated, to avoid services having to upgrade a second time.\n* The preference for avoiding further changes to tracking consent. This is a non-breaking change.\n* The preference for not advising teams to add exemptions to ZAP tests across MDTP\n* The preference for consistency in language settings across our frontend components\n\nChosen option: \"Remove the reading of PLAY_LANG in tracking consent and accept a data-language attribute instead,\" because this decision aligns with the urgency of the issue and supports all the decision drivers effectively.\n\n### Positive Consequences\n* Classic services written in Java will be able to display the tracking consent banner in Welsh using a data-language attribute, improving the user experience.\n* Teams will not have to update their ZAP test settings just to accommodate the reading of the PLAY_LANG cookie, as there are no known security concerns.\n* It provides consistency with other frontend components which also use data attributes for language settings.\n* This change can be deployed quickly, minimizing disruption to teams that are yet to integrate the tracking consent banner.\n\n### Negative Consequences\n* Developers will need to update their services to include the data-language attribute, which may require minor modifications to the codebase.\n* There may be some confusion initially as teams transition from using the PLAY_LANG cookie to using data-language attributes.\n\n### Summary\nAdopting the use of a data-language attribute for tracking consent resolves the immediate issue, supports classic services, ensures fast deployment, and maintains consistency across our frontend components\u2014all without introducing further security concerns or breaking changes.","GenTime":"2024-08-19 23:24:49"}
{"File Name":"compliantkubernetes\/0013-configure-alerts-in-omt.md","Context":"## Context and Problem Statement\\nAlerts are some noteworthy IT event, like a Node becoming un-ready, login failure or a disk getting full.\\nTerminology differs across tooling and organizations, but one generally cares about:\\n* P1 (critical) alerts, which require immediate human attention -- the person on-call needs to be notified immediately -- and;\\n* P2 (high) alerts which require human attention with 24 hours -- the person on-call needs to be notified next morning;\\n* P3 (moderate) alerts which do not require immediate human attention, but should be regularly reviewed.\\nOther priorities (e.g., P4 and below) are generally used for informational purposes.\\nDealing with alerts correctly entails prioritizing them (e.g., P1, P2, P3), deciding if someone should be notified, who should be notified, how they should be notified (e.g., SMS or email) and when.\\n\"Who\", \"how\" and \"when\" should include escalation, if the previous notification was not acknowledged within a pre-configured time interval, then the same person if notified via a different channel or a new person is notified.\\nUnder-alerting -- e.g., notifying an on-call person too late -- may lead to Service Level Agreement (SLA) violations and a general feeling of administrator anxiety: \"Is everything okay, or is alerting not working?\".\\nOver-alerting -- e.g., notifying a person too often about low-priority alerts -- leads to alert fatigue and \"crying wolf\" where even important alerts are eventually ignored.\\nHence, configuring the right level of alerting -- in particular notifications -- is extremely important both for SLA fulfillment and a happy on-call team.\\nWhere should alerting be configured, so as to quickly converge to the optimal alerting level?\\n## Decision Drivers\\n* Allow to quickly silence, un-silence and re-prioritize alerts.\\n* Allow arbitrary flexibility, e.g., who should be notified, when should notification happen, when should escalation happen, for what cluster and namespaces should notification happen, etc.\\n* Leverage existing tools and processes.\\n","Decision":"* Allow to quickly silence, un-silence and re-prioritize alerts.\\n* Allow arbitrary flexibility, e.g., who should be notified, when should notification happen, when should escalation happen, for what cluster and namespaces should notification happen, etc.\\n* Leverage existing tools and processes.\\nChosen option: Compliant Kubernetes \u201cover-alerts\u201d, i.e., forwards all alerts and all relevant information to an On-Call Management Tool (OMT, e.g., Opsgenie).\\nConfiguration of alerts happens in the OMT.\\n### Positive Consequences\\n* Clear separation of concerns.\\n* Alerting does not require per-customer configuration of Compliant Kubernetes.\\n* Leverages existing tools and processes.\\n* We do not need to implement complex alert filtering in Compliant Kubernetes, e.g., silence alerts during maintenance windows, silence alerts during Swedish holidays, etc.\\n### Negative Consequences\\n* Does not capture alerting know-how in Compliant Kubernetes.\\n* Migration to a new OMT means all alerting configuration needs to be migrated to the new tool. Fortunately, this can be done incrementally.\\n","tokens":445,"id":3110,"text":"## Context and Problem Statement\\nAlerts are some noteworthy IT event, like a Node becoming un-ready, login failure or a disk getting full.\\nTerminology differs across tooling and organizations, but one generally cares about:\\n* P1 (critical) alerts, which require immediate human attention -- the person on-call needs to be notified immediately -- and;\\n* P2 (high) alerts which require human attention with 24 hours -- the person on-call needs to be notified next morning;\\n* P3 (moderate) alerts which do not require immediate human attention, but should be regularly reviewed.\\nOther priorities (e.g., P4 and below) are generally used for informational purposes.\\nDealing with alerts correctly entails prioritizing them (e.g., P1, P2, P3), deciding if someone should be notified, who should be notified, how they should be notified (e.g., SMS or email) and when.\\n\"Who\", \"how\" and \"when\" should include escalation, if the previous notification was not acknowledged within a pre-configured time interval, then the same person if notified via a different channel or a new person is notified.\\nUnder-alerting -- e.g., notifying an on-call person too late -- may lead to Service Level Agreement (SLA) violations and a general feeling of administrator anxiety: \"Is everything okay, or is alerting not working?\".\\nOver-alerting -- e.g., notifying a person too often about low-priority alerts -- leads to alert fatigue and \"crying wolf\" where even important alerts are eventually ignored.\\nHence, configuring the right level of alerting -- in particular notifications -- is extremely important both for SLA fulfillment and a happy on-call team.\\nWhere should alerting be configured, so as to quickly converge to the optimal alerting level?\\n## Decision Drivers\\n* Allow to quickly silence, un-silence and re-prioritize alerts.\\n* Allow arbitrary flexibility, e.g., who should be notified, when should notification happen, when should escalation happen, for what cluster and namespaces should notification happen, etc.\\n* Leverage existing tools and processes.\\n\n\n##Decision\n* Allow to quickly silence, un-silence and re-prioritize alerts.\\n* Allow arbitrary flexibility, e.g., who should be notified, when should notification happen, when should escalation happen, for what cluster and namespaces should notification happen, etc.\\n* Leverage existing tools and processes.\\nChosen option: Compliant Kubernetes \u201cover-alerts\u201d, i.e., forwards all alerts and all relevant information to an On-Call Management Tool (OMT, e.g., Opsgenie).\\nConfiguration of alerts happens in the OMT.\\n### Positive Consequences\\n* Clear separation of concerns.\\n* Alerting does not require per-customer configuration of Compliant Kubernetes.\\n* Leverages existing tools and processes.\\n* We do not need to implement complex alert filtering in Compliant Kubernetes, e.g., silence alerts during maintenance windows, silence alerts during Swedish holidays, etc.\\n### Negative Consequences\\n* Does not capture alerting know-how in Compliant Kubernetes.\\n* Migration to a new OMT means all alerting configuration needs to be migrated to the new tool. Fortunately, this can be done incrementally.\\n","MatchedIds":[4707,4705,3109,3915,2834],"Predicted":"## Decision\nTo address the complexities and requirements of alert configuration, we have decided to leverage a dedicated and flexible alert management system, such as Prometheus Alertmanager, integrated with our existing monitoring stack.\n\n### Positive Consequences:\n* **Quick Re-prioritization and Silencing**: Prometheus Alertmanager allows for dynamic alerting rules and silencing policies to be configured on the fly, enabling quick responses to changing priorities.\n* **Enhanced Flexibility**: With fine-grained control over notifications and escalations, Alertmanager supports sophisticated routing based on alert attributes, teams, and priority levels.\n* **Integration with Existing Tools**: Alertmanager easily integrates with popular communication tools (like Slack, PagerDuty, and email), as well as with our monitoring solutions to centralize alert management.\n* **Centralized Management**: A single pane of glass for managing all alerts across clusters and namespaces simplifies oversight and configuration.\n\n### Negative Consequences:\n* **Learning Curve**: Teams unfamiliar with Prometheus Alertmanager will require training and adaptation to new processes and tools.\n* **Initial Setup Complexity**: Configuring the alerting rules, notification channels, and escalation policies initially may be complex and time-consuming.\n* **Additional Monitoring Overhead**: Ensuring the reliability and performance of the alert management system itself adds to the existing monitoring and maintenance load.\n\nBy implementing Prometheus Alertmanager, we can achieve the desired flexibility, rapid configuration changes, and integration with existing workflows, thus optimizing our alerting strategy and improving the overall resilience and effectiveness of our on-call processes.","GenTime":"2024-08-19 23:24:55"}
{"File Name":"adr\/ADR-5-how-to-organize-adr-files.md","Context":"## Context and Problem Statement\\n- How do we call the folder for assets?\\n- What is the pattern of the filename where an ADR is stored?\\n- How do we write titles in the ADR documents?\\n","Decision":"### Asset folder\\nChosen option: `resources`\\n### Filename\\nChosen option: `docs\/ADR-NNNN-title-using-dashes.md` and `docs\/resources\/ADR-NNNN\/{filename}`, because\\n- Adding `arch` or `adr` folders is redundant.\\n- `NNNN` provides a unique number, which can be used for referencing using an immutable number (and generating permalinks if we render the repository to a static site).\\n- The creation time of an ADR is of historical interest only, if it gets updated somehow. It can be embeded in the file or extracted from git history.\\n- Having no spaces in filenames eases working in the command line.\\n- Prefixing with `docs\/ADR-` enables future new kinds of documents.\\n- Calling a folder `docs\/resources\/ADR-NNNN\/{filename}` (omiting the `title-with-dashes`) helps in refactors or changes in the file names.\\n### Title (markdown)\\nChosen option: `# Plain title` because:\\n- At the moment of writing the ADR the writer may not know the final ADR number.\\n- It can be embeded by rendering tools\\n- It would duplicate data from the filename\\n","tokens":46,"id":4619,"text":"## Context and Problem Statement\\n- How do we call the folder for assets?\\n- What is the pattern of the filename where an ADR is stored?\\n- How do we write titles in the ADR documents?\\n\n\n##Decision\n### Asset folder\\nChosen option: `resources`\\n### Filename\\nChosen option: `docs\/ADR-NNNN-title-using-dashes.md` and `docs\/resources\/ADR-NNNN\/{filename}`, because\\n- Adding `arch` or `adr` folders is redundant.\\n- `NNNN` provides a unique number, which can be used for referencing using an immutable number (and generating permalinks if we render the repository to a static site).\\n- The creation time of an ADR is of historical interest only, if it gets updated somehow. It can be embeded in the file or extracted from git history.\\n- Having no spaces in filenames eases working in the command line.\\n- Prefixing with `docs\/ADR-` enables future new kinds of documents.\\n- Calling a folder `docs\/resources\/ADR-NNNN\/{filename}` (omiting the `title-with-dashes`) helps in refactors or changes in the file names.\\n### Title (markdown)\\nChosen option: `# Plain title` because:\\n- At the moment of writing the ADR the writer may not know the final ADR number.\\n- It can be embeded by rendering tools\\n- It would duplicate data from the filename\\n","MatchedIds":[2914,1067,1614,1545,5033],"Predicted":"## Decision\n\n- The folder for assets will be called `assets` to maintain clarity and simplicity.\n- The pattern of the filename where an ADR is stored will be `NNNN-title-with-dashes.md`, where `NNNN` is a zero-padded sequence number and `title-with-dashes` is a slugified version of the ADR title.\n- Titles in the ADR documents will be written in \"Title Case\". For example, an ADR concerning the use of an asset folder structure might have the title: `Define Folder for Assets`.","GenTime":"2024-08-19 23:24:58"}
{"File Name":"datalab\/0036-replace-kong-with-nginx-ingress-controller.md","Context":"## Context\\nWe have reached a point where we need to secure third party web applications that provide\\nno security of their own. The Kong proxy does not offer a mechanism for this in the open\\nsource version and we haven't had and response from our efforts to contact them.\\nWe believe that the Nginx Ingress controller that has been available since Kubernetes 1.7\\nwas released provides a Kubernetes native route for the same functionality.\\n","Decision":"We have decided the replace the Kong proxy with an Nginx Ingress Controller in the\\nKubernetes cluster and an Nginx load balancer running on the proxy servers.\\nThis should provide all of the same functionality as Kong and in addition should provide\\na mechanism for token and cookie authentication using the `auth_url` annotation that\\nwraps the underlying Nginx `auth_request` module.\\n","tokens":91,"id":733,"text":"## Context\\nWe have reached a point where we need to secure third party web applications that provide\\nno security of their own. The Kong proxy does not offer a mechanism for this in the open\\nsource version and we haven't had and response from our efforts to contact them.\\nWe believe that the Nginx Ingress controller that has been available since Kubernetes 1.7\\nwas released provides a Kubernetes native route for the same functionality.\\n\n\n##Decision\nWe have decided the replace the Kong proxy with an Nginx Ingress Controller in the\\nKubernetes cluster and an Nginx load balancer running on the proxy servers.\\nThis should provide all of the same functionality as Kong and in addition should provide\\na mechanism for token and cookie authentication using the `auth_url` annotation that\\nwraps the underlying Nginx `auth_request` module.\\n","MatchedIds":[620,3104,3905,3909,729],"Predicted":"## Decision\nWe will implement the Nginx Ingress Controller to secure our third-party web applications. The Nginx Ingress Controller, being Kubernetes native and available since Kubernetes 1.7, provides robust capabilities for managing external access to our services and adding an additional layer of security.\n\n### Implementation Plan\n1. **Deploy Nginx Ingress Controller**: \n   - Set up the Nginx Ingress Controller in our Kubernetes cluster to handle incoming traffic.\n   - Ensure it is appropriately configured with necessary security policies.\n\n2. **Configure SSL\/TLS Termination**: \n   - Use cert-manager to automate the management and issuance of TLS certificates.\n   - Implement SSL\/TLS termination at the ingress level to ensure that communication is encrypted.\n\n3. **Access Control and Authentication**:\n   - Configure the ingress controller to enforce authentication mechanisms for applications that lack their own security.\n   - Implement additional access control measures such as IP whitelisting or rate limiting as needed.\n\n4. **Monitor and Audit**:\n   - Set up monitoring and logging for the ingress controller to track traffic patterns and detect any potential security issues.\n   - Regularly audit configuration and access logs to ensure compliance with security policies.\n\n### Positive Consequences\n- **Enhanced Security**: Ensures that all external traffic is securely managed and authenticated.\n- **Kubernetes Native Solution**: Aligns with our existing Kubernetes infrastructure, simplifying deployment and management.\n- **Automated Certificate Management**: Using cert-manager automates the issuance and renewal of TLS certificates, reducing manual overhead.\n\n### Negative Consequences\n- **Initial Setup Complexity**: There might be a learning curve and initial complexity in setting up and configuring the Nginx Ingress Controller with advanced security features.\n- **Resource Overhead**: Introducing the ingress controller can add extra resource overhead in terms of compute and memory, which needs to be managed.\n\nIn conclusion, adopting the Nginx Ingress Controller is a strategic decision that aligns with our goal of securing third-party web applications while leveraging Kubernetes-native tools and practices.","GenTime":"2024-08-19 23:25:16"}
{"File Name":"dl4j-dev-tools\/0006-op_specific_enums.md","Context":"## Context\\nSome ops have an ordinal parameter which switches between a few possible modes. Giving those modes a proper name\\nmakes usage and documentation easier.\\n","Decision":"We allow `Arg` sections to have an `ENUM` data type and add a `possibleValues` property to define the possible values\\nfor this arg. The ordinal number of the enum is the same as its position within the `possibleValues` list starting from\\n`0`.\\nA runtime check on op construction, will ensure that each enum arg has one or more possible values, and that default\\nvalues match one of the possible values (if applicable).\\nOn code generation, an appropriate representation of this enum will be generated in the target language. The name of\\nthe generated enum will be derived from the name of the arg.\\n### Example\\n```kotlin\\nArg(ENUM, \"padMode\"){\\npossibleValues = listOf(\"CONSTANT\", \"REFLECT\", \"SYMMETRIC\")\\ndescription = \"padding mode\"\\n}\\n```\\n","tokens":31,"id":994,"text":"## Context\\nSome ops have an ordinal parameter which switches between a few possible modes. Giving those modes a proper name\\nmakes usage and documentation easier.\\n\n\n##Decision\nWe allow `Arg` sections to have an `ENUM` data type and add a `possibleValues` property to define the possible values\\nfor this arg. The ordinal number of the enum is the same as its position within the `possibleValues` list starting from\\n`0`.\\nA runtime check on op construction, will ensure that each enum arg has one or more possible values, and that default\\nvalues match one of the possible values (if applicable).\\nOn code generation, an appropriate representation of this enum will be generated in the target language. The name of\\nthe generated enum will be derived from the name of the arg.\\n### Example\\n```kotlin\\nArg(ENUM, \"padMode\"){\\npossibleValues = listOf(\"CONSTANT\", \"REFLECT\", \"SYMMETRIC\")\\ndescription = \"padding mode\"\\n}\\n```\\n","MatchedIds":[2270,2934,4713,4497,75],"Predicted":"## Decision\n### Mode Naming\nEach possible mode for ordinal parameters will be given a descriptive and meaningful name. For example, if an ordinal parameter toggles between different types of sorting algorithms, the modes could be named `quick_sort`, `merge_sort`, and `bubble_sort` instead of using integer values like `0`, `1`, `2`.\n\n### Implementation Details\n* Modify the relevant parts of the codebase to replace ordinal parameter values with named enumerations.\n* Update the documentation to reflect the new naming scheme.\n* Provide a deprecation period during which both ordinal parameters and named modes are supported, with appropriate warnings for the ordinal usage.\n* Ensure backward compatibility by maintaining the ordinal mode support until the next major release.\n\n### Benefits\n* Improved readability and maintainability of code.\n* Easier and clearer documentation.\n* Enhanced usability for developers.\n\n### Drawbacks\n* Initial effort required to refactor existing code and update documentation.\n* Potential for confusion during the deprecation period as both ordinals and names are supported.\n\n### Conclusion\nNaming ordinal parameters improves clarity and maintainability. The transition will be managed via a deprecation period to ensure backward compatibility.","GenTime":"2024-08-19 23:25:23"}
{"File Name":"govuk-design-system-architecture\/001-javascript-for-less-capable-browsers.md","Context":"## Context\\nBefore GOV.UK Frontend, our projects used jQuery for DOM interactions, events and data manipulation.\\nWe\u2019re taking a step back from jQuery due to its lack of support for the browsers we support, its large file size, lack of security updates and from conversations with the community.\\n","Decision":"We\u2019re now writing standard ES5 JavaScript instead, that we polyfill where necessary.\\nThis means that in places where we would have previously used [`$.each`](http:\/\/api.jquery.com\/jquery.each\/) we\u2019re using [`.forEach`](https:\/\/developer.mozilla.org\/en-US\/docs\/Web\/JavaScript\/Reference\/Global_Objects\/Array\/forEach) instead, and polyfilling the missing gaps.\\nWe use polyfills provided by the Financial Times\u2019 [Polyfill service](https:\/\/polyfill.io).\\nThis approach ensures that multiple polyfills can be sourced from this service with greater confidence that they\u2019ll work without conflicting with each other.\\nThe Polyfill service does not do runtime detection in browsers and instead opts to do this on the server via user-agent sniffing. It only ships the code needed for that browser, which means newer browsers don\u2019t have to run anything. We may investigate lazy-loading in the future, but for now we\u2019re using a bundled approach based on the lowest common denominator.\\nWe are vendoring these polyfills to avoid any [single point of failure](https:\/\/en.wikipedia.org\/wiki\/Single_point_of_failure) issues that could arise from relying on a CDN. By doing this, we can detect if polyfills are needed at runtime, which results in all browsers getting the same polyfill bundle.\\nWe hope that our approach can be automated or moved into a reusable npm package, based on the Financial Times [npm package](https:\/\/github.com\/Financial-Times\/polyfill-service#library).\\nHere is an [example of polyfilling `addEventListener`](https:\/\/github.com\/alphagov\/govuk-frontend\/blob\/master\/docs\/polyfilling.md).\\nAny polyfills included in GOV.UK Frontend will be tested to work in supported browsers and devices, including assistive technology. Any community discussion and documentation around potential bugs or downsides will also be considered before deciding to include a polyfill in GOV.UK Frontend.\\n","tokens":62,"id":3806,"text":"## Context\\nBefore GOV.UK Frontend, our projects used jQuery for DOM interactions, events and data manipulation.\\nWe\u2019re taking a step back from jQuery due to its lack of support for the browsers we support, its large file size, lack of security updates and from conversations with the community.\\n\n\n##Decision\nWe\u2019re now writing standard ES5 JavaScript instead, that we polyfill where necessary.\\nThis means that in places where we would have previously used [`$.each`](http:\/\/api.jquery.com\/jquery.each\/) we\u2019re using [`.forEach`](https:\/\/developer.mozilla.org\/en-US\/docs\/Web\/JavaScript\/Reference\/Global_Objects\/Array\/forEach) instead, and polyfilling the missing gaps.\\nWe use polyfills provided by the Financial Times\u2019 [Polyfill service](https:\/\/polyfill.io).\\nThis approach ensures that multiple polyfills can be sourced from this service with greater confidence that they\u2019ll work without conflicting with each other.\\nThe Polyfill service does not do runtime detection in browsers and instead opts to do this on the server via user-agent sniffing. It only ships the code needed for that browser, which means newer browsers don\u2019t have to run anything. We may investigate lazy-loading in the future, but for now we\u2019re using a bundled approach based on the lowest common denominator.\\nWe are vendoring these polyfills to avoid any [single point of failure](https:\/\/en.wikipedia.org\/wiki\/Single_point_of_failure) issues that could arise from relying on a CDN. By doing this, we can detect if polyfills are needed at runtime, which results in all browsers getting the same polyfill bundle.\\nWe hope that our approach can be automated or moved into a reusable npm package, based on the Financial Times [npm package](https:\/\/github.com\/Financial-Times\/polyfill-service#library).\\nHere is an [example of polyfilling `addEventListener`](https:\/\/github.com\/alphagov\/govuk-frontend\/blob\/master\/docs\/polyfilling.md).\\nAny polyfills included in GOV.UK Frontend will be tested to work in supported browsers and devices, including assistive technology. Any community discussion and documentation around potential bugs or downsides will also be considered before deciding to include a polyfill in GOV.UK Frontend.\\n","MatchedIds":[263,1240,4557,847,1576],"Predicted":"## Decision\nWe will transition away from jQuery to vanilla JavaScript for DOM interactions, events, and data manipulation. This aligns with current web standards, ensures better browser compatibility, improves performance due to reduced file size, and addresses security concerns identified by the community.\n\nFor any complex functionality that cannot be easily replaced by vanilla JavaScript, we will consider modern, lightweight libraries like [Alpine.js](https:\/\/alpinejs.dev) or [Stimulus](https:\/\/stimulus.hotwired.dev), which are designed to enhance development productivity without compromising on performance or security.","GenTime":"2024-08-19 23:25:26"}
{"File Name":"service-stac\/2020_10_21_static_asset.md","Context":"## Context\\n`service-stac` needs to serve some static assets for the admin pages (css, images, icons, ...). Django is not appropriate to serve static files on production environment. Currently Django is served directly by `gunicorn`. As a good practice to avoid issue with slow client and to avoid Denial of Service attacks, `gunicorn` should be served behind a Reversed proxy (e.g. Apache or Nginx).\\n","Decision":"Because it is to us not clear yet if a Reverse Proxy is really necessary for our Architecture (CloudFront with Kubernetes Ingress), we decided to use WhiteNoise for static assets. This middleware seems to performs well with CDN (like CloudFront) therefore we will use it to serve static files as it is very simple to uses and take care of compressing and settings corrects Headers for caching.\\n","tokens":91,"id":3173,"text":"## Context\\n`service-stac` needs to serve some static assets for the admin pages (css, images, icons, ...). Django is not appropriate to serve static files on production environment. Currently Django is served directly by `gunicorn`. As a good practice to avoid issue with slow client and to avoid Denial of Service attacks, `gunicorn` should be served behind a Reversed proxy (e.g. Apache or Nginx).\\n\n\n##Decision\nBecause it is to us not clear yet if a Reverse Proxy is really necessary for our Architecture (CloudFront with Kubernetes Ingress), we decided to use WhiteNoise for static assets. This middleware seems to performs well with CDN (like CloudFront) therefore we will use it to serve static files as it is very simple to uses and take care of compressing and settings corrects Headers for caching.\\n","MatchedIds":[4642,469,2516,1239,4056],"Predicted":"## Decision\nWe will use Nginx as the reverse proxy in front of `gunicorn` to handle serving static assets for the admin pages.\n\n### Decision Drivers\n* **Performance:** Nginx is optimized for handling static content and can efficiently serve static assets such as CSS, images, and icons.\n* **Security:** Using a reverse proxy helps protect against various attack vectors, including Denial of Service (DoS) attacks.\n* **Best Practices:** It is recommended to use a web server like Nginx or Apache to handle static content instead of Django in a production environment.\n* **Compatibility:** Nginx has good integration with `gunicorn` and can seamlessly handle request routing and load balancing.\n\n### Positive Consequences\n* **Improved Performance:** Serving static files through Nginx will be faster and more efficient.\n* **Enhanced Security:** Nginx will help mitigate slow client attacks and potential DoS attacks.\n* **Scalability:** Nginx can distribute the load across multiple `gunicorn` worker processes, improving scalability.\n\n### Negative Consequences\n* **Increased Complexity:** Introducing Nginx adds some complexity to the deployment and configuration process.\n* **Maintenance Overhead:** The Nginx server needs additional maintenance and monitoring compared to a simpler setup.\n\n### Implementation Plan\n1. **Install Nginx on the server.**\n2. **Configure Nginx to serve static files from the appropriate directories.**\n3. **Set up Nginx as a reverse proxy to forward requests to `gunicorn`.**\n4. **Configure `gunicorn` to listen on a Unix socket or a port where Nginx can communicate with it.**\n5. **Ensure the configuration is secure and performs well under load.**\n\n### Conclusion\nUsing Nginx as a reverse proxy for serving static assets and as a frontend for `gunicorn` aligns with best practices, ensures better performance, and adds a layer of security for the `service-stac` admin pages.","GenTime":"2024-08-19 23:25:34"}
{"File Name":"new-orbit\/0001-use-openshift-as-cloud-provider.md","Context":"## Context and Problem Statement\\nWe want to deploy our application in docker containers that can be easily updated\\n","Decision":"Chosen option: \"OpenShift\", because\\n* Built on Kubernetes.\\nThe bank has experience on it.\\nProvides a lot of added value tools for CI\/CD, automated builds.\\nIs supported by RedHat and we have a great support contract for it.\\n","tokens":21,"id":4096,"text":"## Context and Problem Statement\\nWe want to deploy our application in docker containers that can be easily updated\\n\n\n##Decision\nChosen option: \"OpenShift\", because\\n* Built on Kubernetes.\\nThe bank has experience on it.\\nProvides a lot of added value tools for CI\/CD, automated builds.\\nIs supported by RedHat and we have a great support contract for it.\\n","MatchedIds":[4358,1239,583,699,2471],"Predicted":"## Decision\nWe will use a Continuous Integration and Continuous Deployment (CI\/CD) pipeline to manage our Docker deployments. The chosen tools are:\n\n1. **GitHub Actions** for CI\/CD workflows, as it allows for tight integration with our code repository, simplifies pipeline setup, and has robust support for Docker.\n2. **Docker Hub** for storing and managing Docker images, due to its simplicity and integration with GitHub Actions.\n3. **Kubernetes** for orchestrating containerized applications. This ensures our deployments are easily scalable and manageable.\n\n### Implementation Steps\n1. **GitHub Actions** will be configured to trigger on every push to the main branch.\n2. The pipeline will build a Docker image, run tests, and if successful, push the image to **Docker Hub**.\n3. **Kubernetes** manifests will be updated with the new image tag and deployed to our cluster.\n\n### Positive Consequences\n* **Automation**: Ensures that developers can push updates frequently without manual intervention.\n* **Scalability**: Kubernetes provides robust mechanisms for scaling applications.\n* **Ease of Management**: GitHub Actions and Docker Hub integration simplifies the overall process of managing container images and deployments.\n* **Observability**: Kubernetes offers built-in tools for monitoring and logging, which helps in maintaining application health.\n\n### Notes:\n- Proper secret management will be enforced using GitHub Secrets for storing credentials for Docker Hub and Kubernetes.\n- Rollback strategies will be implemented in the Kubernetes manifests to ensure application stability.\n\nBy following this approach, we ensure our Docker-based deployments are simplified, automated, and scalable.","GenTime":"2024-08-19 23:25:41"}
{"File Name":"google-cloud-cpp\/2019-01-04-error-reporting-with-statusor.md","Context":"**Context**: We know there will be users of these C++ libraries who want to use\\nC++ exceptions as well as those who are not able to. Our C++ libraries must work\\nfor all of our users, regardless of their ability to use exceptions.\\n**Decision**: None of our APIs will throw exceptions to indicate errors.\\nInstead, our APIs will typically report errors to callers by returning a\\n`Status` or `StatusOr<T>` object, unless the library we're using has another\\nnon-throwing way to report errors (e.g., [badbit][badbit-link] in the standard\\nI\/O library).\\n**Consequences**: This decision will result in a single set of APIs and a\\nconsistent vocabulary for all users, whether or not they choose to compile with\\nexceptions. This decision does not prevent callers from using exceptions in\\ntheir own code.\\nA downside of this decision is that our APIs will not be natural or idiomatic\\nfor the [50+%][survey-link] of users who might prefer exceptions for error\\nreporting.\\nChanging existing APIs from throwing exceptions to returning `StatusOr<T>` is a\\nbreaking change. As of this writing (Jan 2019), this project has a\\n[Google Cloud Storage][gcs-link] component that is at the Alpha quality level,\\nand a [Google Cloud Bigtable][bigtable-link] that is already at the Beta quality\\nlevel. We plan to immediately change the API for Google Cloud Storage. We have\\nno immediate plans to change the API for Cloud Bigtable. We will communicate a\\ntimeline to change this API in a separate document.\\n[badbit-link]: https:\/\/en.cppreference.com\/w\/cpp\/io\/ios_base\/iostate\\n[bigtable-link]: https:\/\/github.com\/googleapis\/google-cloud-cpp\/tree\/main\/google\/cloud\/bigtable\\n[gcs-link]: https:\/\/github.com\/googleapis\/google-cloud-cpp\/tree\/main\/google\/cloud\/storage\\n[survey-link]: https:\/\/isocpp.org\/blog\/2018\/03\/results-summary-cpp-foundation-developer-survey-lite-2018-02\\n","Decision":"Instead, our APIs will typically report errors to callers by returning a\\n`Status` or `StatusOr<T>` object, unless the library we're using has another\\nnon-throwing way to report errors (e.g., [badbit][badbit-link] in the standard\\nI\/O library).\\n**Consequences**: This decision will result in a single set of APIs and a\\nconsistent vocabulary for all users, whether or not they choose to compile with\\nexceptions. This decision does not prevent callers from using exceptions in\\ntheir own code.\\nA downside of this decision is that our APIs will not be natural or idiomatic\\nfor the [50+%][survey-link] of users who might prefer exceptions for error\\nreporting.\\nChanging existing APIs from throwing exceptions to returning `StatusOr<T>` is a\\nbreaking change. As of this writing (Jan 2019), this project has a\\n[Google Cloud Storage][gcs-link] component that is at the Alpha quality level,\\nand a [Google Cloud Bigtable][bigtable-link] that is already at the Beta quality\\nlevel. We plan to immediately change the API for Google Cloud Storage. We have\\nno immediate plans to change the API for Cloud Bigtable. We will communicate a\\ntimeline to change this API in a separate document.\\n[badbit-link]: https:\/\/en.cppreference.com\/w\/cpp\/io\/ios_base\/iostate\\n[bigtable-link]: https:\/\/github.com\/googleapis\/google-cloud-cpp\/tree\/main\/google\/cloud\/bigtable\\n[gcs-link]: https:\/\/github.com\/googleapis\/google-cloud-cpp\/tree\/main\/google\/cloud\/storage\\n[survey-link]: https:\/\/isocpp.org\/blog\/2018\/03\/results-summary-cpp-foundation-developer-survey-lite-2018-02\\n","tokens":436,"id":2467,"text":"**Context**: We know there will be users of these C++ libraries who want to use\\nC++ exceptions as well as those who are not able to. Our C++ libraries must work\\nfor all of our users, regardless of their ability to use exceptions.\\n**Decision**: None of our APIs will throw exceptions to indicate errors.\\nInstead, our APIs will typically report errors to callers by returning a\\n`Status` or `StatusOr<T>` object, unless the library we're using has another\\nnon-throwing way to report errors (e.g., [badbit][badbit-link] in the standard\\nI\/O library).\\n**Consequences**: This decision will result in a single set of APIs and a\\nconsistent vocabulary for all users, whether or not they choose to compile with\\nexceptions. This decision does not prevent callers from using exceptions in\\ntheir own code.\\nA downside of this decision is that our APIs will not be natural or idiomatic\\nfor the [50+%][survey-link] of users who might prefer exceptions for error\\nreporting.\\nChanging existing APIs from throwing exceptions to returning `StatusOr<T>` is a\\nbreaking change. As of this writing (Jan 2019), this project has a\\n[Google Cloud Storage][gcs-link] component that is at the Alpha quality level,\\nand a [Google Cloud Bigtable][bigtable-link] that is already at the Beta quality\\nlevel. We plan to immediately change the API for Google Cloud Storage. We have\\nno immediate plans to change the API for Cloud Bigtable. We will communicate a\\ntimeline to change this API in a separate document.\\n[badbit-link]: https:\/\/en.cppreference.com\/w\/cpp\/io\/ios_base\/iostate\\n[bigtable-link]: https:\/\/github.com\/googleapis\/google-cloud-cpp\/tree\/main\/google\/cloud\/bigtable\\n[gcs-link]: https:\/\/github.com\/googleapis\/google-cloud-cpp\/tree\/main\/google\/cloud\/storage\\n[survey-link]: https:\/\/isocpp.org\/blog\/2018\/03\/results-summary-cpp-foundation-developer-survey-lite-2018-02\\n\n\n##Decision\nInstead, our APIs will typically report errors to callers by returning a\\n`Status` or `StatusOr<T>` object, unless the library we're using has another\\nnon-throwing way to report errors (e.g., [badbit][badbit-link] in the standard\\nI\/O library).\\n**Consequences**: This decision will result in a single set of APIs and a\\nconsistent vocabulary for all users, whether or not they choose to compile with\\nexceptions. This decision does not prevent callers from using exceptions in\\ntheir own code.\\nA downside of this decision is that our APIs will not be natural or idiomatic\\nfor the [50+%][survey-link] of users who might prefer exceptions for error\\nreporting.\\nChanging existing APIs from throwing exceptions to returning `StatusOr<T>` is a\\nbreaking change. As of this writing (Jan 2019), this project has a\\n[Google Cloud Storage][gcs-link] component that is at the Alpha quality level,\\nand a [Google Cloud Bigtable][bigtable-link] that is already at the Beta quality\\nlevel. We plan to immediately change the API for Google Cloud Storage. We have\\nno immediate plans to change the API for Cloud Bigtable. We will communicate a\\ntimeline to change this API in a separate document.\\n[badbit-link]: https:\/\/en.cppreference.com\/w\/cpp\/io\/ios_base\/iostate\\n[bigtable-link]: https:\/\/github.com\/googleapis\/google-cloud-cpp\/tree\/main\/google\/cloud\/bigtable\\n[gcs-link]: https:\/\/github.com\/googleapis\/google-cloud-cpp\/tree\/main\/google\/cloud\/storage\\n[survey-link]: https:\/\/isocpp.org\/blog\/2018\/03\/results-summary-cpp-foundation-developer-survey-lite-2018-02\\n","MatchedIds":[5061,2462,2466,4536,68],"Predicted":"## Decision\nNone of our APIs will throw exceptions to indicate errors. Instead, our APIs will typically report errors to callers by returning a `Status` or `StatusOr<T>` object, unless the library we're using has another non-throwing way to report errors (e.g., [badbit][badbit-link] in the standard I\/O library).\n\n## Consequences\nThis decision will result in a single set of APIs and a consistent vocabulary for all users, whether or not they choose to compile with exceptions. This decision does not prevent callers from using exceptions in their own code.\n\nA downside of this decision is that our APIs will not be natural or idiomatic for the [50+%][survey-link] of users who might prefer exceptions for error reporting.\n\nChanging existing APIs from throwing exceptions to returning `StatusOr<T>` is a breaking change. As of this writing (Jan 2019), this project has a [Google Cloud Storage][gcs-link] component that is at the Alpha quality level, and a [Google Cloud Bigtable][bigtable-link] that is already at the Beta quality level. We plan to immediately change the API for Google Cloud Storage. We have no immediate plans to change the API for Cloud Bigtable. We will communicate a timeline to change this API in a separate document.\n\n[badbit-link]: https:\/\/en.cppreference.com\/w\/cpp\/io\/ios_base\/iostate\n[bigtable-link]: https:\/\/github.com\/googleapis\/google-cloud-cpp\/tree\/main\/google\/cloud\/bigtable\n[gcs-link]: https:\/\/github.com\/googleapis\/google-cloud-cpp\/tree\/main\/google\/cloud\/storage\n[survey-link]: https:\/\/isocpp.org\/blog\/2018\/03\/results-summary-cpp-foundation-developer-survey-lite-2018-02","GenTime":"2024-08-19 23:25:47"}
{"File Name":"beis-report-official-development-assistance\/0005-use-travis-for-ci.md","Context":"## Context\\nTravis is actively used for many other projects within the BEIS GitHub account.\\nWe know we will want to be using containers to run this application, so the tool we use must support Docker too.\\n","Decision":"Use Travis to build and test the application.\\nUse Travis to automatically deploy the applications to live.\\n","tokens":45,"id":2393,"text":"## Context\\nTravis is actively used for many other projects within the BEIS GitHub account.\\nWe know we will want to be using containers to run this application, so the tool we use must support Docker too.\\n\n\n##Decision\nUse Travis to build and test the application.\\nUse Travis to automatically deploy the applications to live.\\n","MatchedIds":[1741,1376,2409,4096,1196],"Predicted":"## Decision\nGiven the active use of Travis across many projects within the BEIS GitHub account and the need for Docker support, we will:\n\n- Continue using Travis CI for our CI\/CD processes to maintain consistency and leverage existing expertise within the BEIS team.\n- Utilize Travis CI's native Docker support to build, test, and deploy our containerized applications.\n- Create Dockerfiles and configure Travis builds to ensure seamless integration and deployment of Docker containers.\n- Continuously monitor and optimize our CI\/CD pipeline to ensure efficient and reliable deployments.","GenTime":"2024-08-19 23:25:51"}
{"File Name":"cloud-on-k8s\/0006-sidecar-health.md","Context":"## Context and Problem Statement\\nThis proposal aims to outline possible approaches to report the Elasticsearch sidecar health in combination with the cluster state for cluster-wide monitoring and alerting.\\nAs for now an Elasticsearch pod is composed of 2 containers:\\n- a main container for Elasticsearch\\n- a sidecar container for running the keystore-updater\\nWhat is the keystore-updater in the sidecar doing?\\nIt calls the Elasticsearch endpoint `\/_nodes\/reload_secure_settings` to decrypt and re-read the entire keystore used by the snapshotter job.\\nTo connect to ES it depends on:\\n- an environment variable for the username\\n- secrets mounted as readonly files for the password and the CA certificate\\n- the Elasticsearch readiness\\nCurrently there is no health check based on the state of the sidecar. The sidecar can error without anyone ever noticing this state.\\nSo there is a need to check that everything is correctly setup in the sidecar container and the call to the ES API succeeds.\\nIf the sidecar container is not ready, the Elasticsearch container is impacted because the pod is considered not ready and\\nKubernetes stops to send traffic to the pod. We must accept that the two containers are intimately linked. A sidecar failure\\ncan impact the Elasticsearch availability by design.\\nHowever Go binaries that do simple things are very fast to start and very reliable.\\nFrom that we could admit that the probability to have a failure in the sidecar that runs a simple go binary is very low\\ncompared to have an Elasticsearch failure.\\nAnother challenge is to take into account that some sidecar errors are to be expected when ES is not ready yet.\\nThis can be mitigated by considering a start-up delay during which it is accepted that ES is not ready and\\ndo not report errors during this period. Then how to detect that ES has never started?\\nThe ES readiness probe will fail if ES never becomes ready.\\n## Decision Drivers\\n* Error distinction: a sidecar failure should be easily identified from an Elasticsearch failure\\n* Error side effect: a sidecar failure should not increase the unavailability of Elasticsearch compared to the current situation\\n* Promote reliability and simplicity because health-checking is a critical part of the system\\n","Decision":"* Error distinction: a sidecar failure should be easily identified from an Elasticsearch failure\\n* Error side effect: a sidecar failure should not increase the unavailability of Elasticsearch compared to the current situation\\n* Promote reliability and simplicity because health-checking is a critical part of the system\\nChosen option: option 4, because it gives us more flexibility to take decisions in case of failure, it does not depend on Kubernetes probes\/kubelet and it does not depend on external components.\\n### Positive Consequences\\n* Collecting the sidecar health from the operator side gives us more options to react to failures\\n* Having an HTTP server in the sidecar brings more flexibility to interact with the pod\\n* Does not depend on the Kubernetes probes or the Kubelet\\n* Minimize external dependencies\\n### Negative Consequences\\n* Increase a little the failure domain of the sidecar with the presence of the HTTP server\\n* Add complexity and responsibility to the operator\\n","tokens":455,"id":4705,"text":"## Context and Problem Statement\\nThis proposal aims to outline possible approaches to report the Elasticsearch sidecar health in combination with the cluster state for cluster-wide monitoring and alerting.\\nAs for now an Elasticsearch pod is composed of 2 containers:\\n- a main container for Elasticsearch\\n- a sidecar container for running the keystore-updater\\nWhat is the keystore-updater in the sidecar doing?\\nIt calls the Elasticsearch endpoint `\/_nodes\/reload_secure_settings` to decrypt and re-read the entire keystore used by the snapshotter job.\\nTo connect to ES it depends on:\\n- an environment variable for the username\\n- secrets mounted as readonly files for the password and the CA certificate\\n- the Elasticsearch readiness\\nCurrently there is no health check based on the state of the sidecar. The sidecar can error without anyone ever noticing this state.\\nSo there is a need to check that everything is correctly setup in the sidecar container and the call to the ES API succeeds.\\nIf the sidecar container is not ready, the Elasticsearch container is impacted because the pod is considered not ready and\\nKubernetes stops to send traffic to the pod. We must accept that the two containers are intimately linked. A sidecar failure\\ncan impact the Elasticsearch availability by design.\\nHowever Go binaries that do simple things are very fast to start and very reliable.\\nFrom that we could admit that the probability to have a failure in the sidecar that runs a simple go binary is very low\\ncompared to have an Elasticsearch failure.\\nAnother challenge is to take into account that some sidecar errors are to be expected when ES is not ready yet.\\nThis can be mitigated by considering a start-up delay during which it is accepted that ES is not ready and\\ndo not report errors during this period. Then how to detect that ES has never started?\\nThe ES readiness probe will fail if ES never becomes ready.\\n## Decision Drivers\\n* Error distinction: a sidecar failure should be easily identified from an Elasticsearch failure\\n* Error side effect: a sidecar failure should not increase the unavailability of Elasticsearch compared to the current situation\\n* Promote reliability and simplicity because health-checking is a critical part of the system\\n\n\n##Decision\n* Error distinction: a sidecar failure should be easily identified from an Elasticsearch failure\\n* Error side effect: a sidecar failure should not increase the unavailability of Elasticsearch compared to the current situation\\n* Promote reliability and simplicity because health-checking is a critical part of the system\\nChosen option: option 4, because it gives us more flexibility to take decisions in case of failure, it does not depend on Kubernetes probes\/kubelet and it does not depend on external components.\\n### Positive Consequences\\n* Collecting the sidecar health from the operator side gives us more options to react to failures\\n* Having an HTTP server in the sidecar brings more flexibility to interact with the pod\\n* Does not depend on the Kubernetes probes or the Kubelet\\n* Minimize external dependencies\\n### Negative Consequences\\n* Increase a little the failure domain of the sidecar with the presence of the HTTP server\\n* Add complexity and responsibility to the operator\\n","MatchedIds":[4707,3110,3488,3917,2240],"Predicted":"## Decision\nWe will implement a dedicated health check endpoint for the sidecar container to report its health status independently from the main Elasticsearch container.\n\n### Implementation Steps\n1. **Health Check Endpoint**: \n   - Develop a lightweight HTTP server within the sidecar using a simple Go binary to expose a health check endpoint (e.g., `\/healthz`).\n   - This endpoint will validate:\n     - Environment variable for the username.\n     - Presence and readability of secrets for the password and CA certificate.\n     - Ability to connect to Elasticsearch and successfully call `\/_nodes\/reload_secure_settings`.\n\n2. **Startup Delay**: \n   - Introduce a configurable startup delay (e.g., via environment variable) in the health check logic to account for Elasticsearch initialization time.\n   - During this delay, health check failures due to Elasticsearch being unready will not be reported as errors.\n\n3. **Readiness and Liveness Probes**: \n   - Configure Kubernetes readiness and liveness probes for the sidecar container to point to the new health check endpoint (`\/healthz`).\n   - Separate probes for Elasticsearch will remain unchanged but will now consider the sidecar's health independently.\n\n4. **Monitoring and Alerts**:\n   - Integrate additional monitoring to track the health of both the main container and the sidecar.\n   - Set up alert rules to differentiate between failures in the sidecar and Elasticsearch.\n\n### Positive Consequences\n- **Error Distinction**: \n  - Clear differentiation between sidecar and Elasticsearch failures. Alerts can specify whether the issue lies in the sidecar or Elasticsearch itself.\n- **Reliability**:\n  - Isolating sidecar health checks increases the reliability of the monitoring system.\n- **Simplicity**:\n  - Using a lightweight Go binary ensures a fast and reliable health check mechanism for the sidecar.\n- **Resilience**:\n  - A startup delay ensures that expected transient errors during Elasticsearch startup do not trigger false alarms.\n- **No Increased Unavailability**:\n  - Since we consider both containers' readiness separately, a sidecar failure will not unnecessarily increase the Elasticsearch downtime.\n\n### Negative Consequences\n- **Maintenance Overhead**:\n  - Introduction of an extra health check mechanism requires maintenance of the additional Go binary and the health check logic.\n- **Configuration Complexity**:\n  - Additional configuration for startup delays and health check endpoints might increase complexity slightly.\n\nBy implementing this dedicated health check endpoint in the sidecar, we create a more resilient and clear monitoring and alerting system for the Elasticsearch deployment. This approach isolates sidecar issues, ensuring they do not wrongly affect the reported state of the Elasticsearch service.","GenTime":"2024-08-19 23:26:03"}
{"File Name":"terraform-aws-dynamodb\/0005-feature-toggles.md","Context":"## Context\\nToggles are a useful aspect of Terraform modules that provide the ability to\\nturn on\/off functionality.\\nCapital One have written about\\n[Building Feature Toggles into\\nTerraform](https:\/\/medium.com\/capital-one-tech\/building-feature-toggles-into-terraform-d75806217647)\\n. This build on top of the use of the count and ternary to provide a more robust\\nfeature toggle.\\nAs this module is envisioned to be part of a larger Data Storage Module there is\\na concept where no DynamoDB table may need to be provisioned. In addition, when\\nusing DynamoDB Autoscaling can be a useful feature. Providing the capabilities\\nto turn this on and off based on requirements is also a useful requirement.\\n","Decision":"Feature toggles will be leveraged to determine:\\n* If any resources should be created\\n* If a DynamoDB resource should be created\\n* If the Autoscaler should be enabled\\n","tokens":160,"id":2538,"text":"## Context\\nToggles are a useful aspect of Terraform modules that provide the ability to\\nturn on\/off functionality.\\nCapital One have written about\\n[Building Feature Toggles into\\nTerraform](https:\/\/medium.com\/capital-one-tech\/building-feature-toggles-into-terraform-d75806217647)\\n. This build on top of the use of the count and ternary to provide a more robust\\nfeature toggle.\\nAs this module is envisioned to be part of a larger Data Storage Module there is\\na concept where no DynamoDB table may need to be provisioned. In addition, when\\nusing DynamoDB Autoscaling can be a useful feature. Providing the capabilities\\nto turn this on and off based on requirements is also a useful requirement.\\n\n\n##Decision\nFeature toggles will be leveraged to determine:\\n* If any resources should be created\\n* If a DynamoDB resource should be created\\n* If the Autoscaler should be enabled\\n","MatchedIds":[3171,3144,2536,621,1372],"Predicted":"## Decision\nWe have decided to implement feature toggles within the Terraform module to manage the provisioning of DynamoDB tables and the activation of DynamoDB Autoscaling. We will achieve this by using the `count` attribute and ternary operators, as described in the Capital One article. This approach will allow us to dynamically enable or disable specific functionalities based on the requirements, thus providing flexibility in managing resources and their configurations within the larger Data Storage Module.","GenTime":"2024-08-19 23:26:06"}
{"File Name":"elasticsearch-updater\/0002-store-configurations-in-repository.md","Context":"## Context\\nThe application loads data from a JSON file into an Elasticsearch instance. Along with the raw data the import\\nalso needs to create mappings and transform data to improve search rankings or provide geolocation searches.\\nThese are rich complex JSON objects or functions that cannot be passed in as environment variables.\\n","Decision":"Given the small number of databases (currently only GP Profiles data is held in Elasticsearch) it is pragmatic to co-locate the Elasticsearch configuration alongside the `elasticsearch-updater` code, rather than creating a new repository and file hosting for each mappings and transform.\\n","tokens":61,"id":485,"text":"## Context\\nThe application loads data from a JSON file into an Elasticsearch instance. Along with the raw data the import\\nalso needs to create mappings and transform data to improve search rankings or provide geolocation searches.\\nThese are rich complex JSON objects or functions that cannot be passed in as environment variables.\\n\n\n##Decision\nGiven the small number of databases (currently only GP Profiles data is held in Elasticsearch) it is pragmatic to co-locate the Elasticsearch configuration alongside the `elasticsearch-updater` code, rather than creating a new repository and file hosting for each mappings and transform.\\n","MatchedIds":[1173,3416,4358,3682,4021],"Predicted":"## Decision\nWe will use a configuration management tool such as [Ansible](https:\/\/www.ansible.com\/) or [Puppet](https:\/\/puppet.com\/) to manage the complex JSON configurations and transformations. These tools allow us to define and apply configurations in a systematic way. The JSON files containing mappings and transformation rules will be stored as part of the source code in a version-controlled repository. This ensures that configurations can be easily maintained, tracked, and audited. Additionally, the import process will include scripts that read these files and apply the necessary transformations and mappings during the data load into Elasticsearch.","GenTime":"2024-08-19 23:26:10"}
{"File Name":"klokwrk-project\/0002-strategic-project-structure.md","Context":"## Context\\nExcluding the simplest hello-world-like cases, any useful project typically contains several modules. The traditional way to organize project modules is just to put them under the project root.\\nWe can call that structure simply **flat structure**.\\nWhile the flat structure is appropriate and sufficient for simpler projects, when the project grows and the number of modules increases, the flat structure starts suffering from many drawbacks:\\n* Flat structure does not scale well when the number of modules grows.\\n* Flat structure is difficult and confusing to navigate with numerous modules at the same hierarchy level.\\n* Flat structure does not suggest a direction of dependencies between modules.\\n* Flat structure does not suggest abstraction levels of modules.\\n* Flat structure does not suggest where are the system's entry points.\\n* Flat structure can use only module names to provide hints about relations between modules. Unfortunately, even that possibility is rarely leveraged.\\n* Flat structure does not use any high-level constructs that may suggest how modules are organized and related.\\n* Negative usage aspects are getting worse and worse as we add additional modules.\\n* Flat structure often requires extracting modules in separate repositories just because confusion becomes unbearable with a larger number of modules.\\n* When using microservices, the flat structure practically forces us to use one project per microservice.\\n> Note: Terms **flat structure** and **strategic structure** (see below) are ad-hoc terms introduced just for this document. However, in the `klokwrk-project`, we may use them in other places for\\n> convenience.\\n### Architectural Context\\n* System (`klokwrk-project`)\\n","Decision":"**We'll organize project modules around strategic DDD (Domain Driven Design) constructs of bounded context and subdomains.**\\nOur project organization will follow principles and recommendations of **strategic structure** as defined below.\\n### Decision Details\\nWe'll start with a concrete example of the strategic structure used in the klokwrk at the time of writing this document. As a follow-up, we'll present a general scheme for creating the strategic\\nstructure focusing on the differences to the given concrete example.\\n#### Strategic structure in klokwrk\\nThe current project layout in the klokwrk looks like this:\\nklokwrk-project\\n\u251c\u2500\u2500 ... (other files or directories)\\n\u251c\u2500\u2500 modules\\n\u2502   \u251c\u2500\u2500 bc\\n\u2502   \u2502   \u2514\u2500\u2500 cargotracking\\n\u2502   \u2502       \u251c\u2500\u2500 asd\\n\u2502   \u2502       \u2502   \u2514\u2500\u2500 booking\\n\u2502   \u2502       \u2502       \u251c\u2500\u2500 app\\n\u2502   \u2502       \u2502       \u2502       cargotracking-booking-app-commandside\\n\u2502   \u2502       \u2502       \u2502       cargotracking-booking-app-queryside-projection-rdbms\\n\u2502   \u2502       \u2502       \u2502       cargotracking-booking-app-queryside-view\\n\u2502   \u2502       \u2502       \u2502       cargotracking-booking-app-rdbms-management\\n\u2502   \u2502       \u2502       \u2514\u2500\u2500 lib\\n\u2502   \u2502       \u2502               cargotracking-booking-lib-boundary-web\\n\u2502   \u2502       \u2502               cargotracking-booking-lib-out-customer\\n\u2502   \u2502       \u2502               cargotracking-booking-lib-queryside-model-rdbms-jpa\\n\u2502   \u2502       \u2502               cargotracking-booking-test-component\\n\u2502   \u2502       \u2502               cargotracking-booking-test-support-queryside\\n\u2502   \u2502       \u2502               cargotracking-booking-test-support-testcontainers\\n\u2502   \u2502       \u2502\\n\u2502   \u2502       \u251c\u2500\u2500 domain-model\\n\u2502   \u2502       \u2502       cargotracking-domain-model-aggregate\\n\u2502   \u2502       \u2502       cargotracking-domain-model-command\\n\u2502   \u2502       \u2502       cargotracking-domain-model-event\\n\u2502   \u2502       \u2502       cargotracking-domain-model-service\\n\u2502   \u2502       \u2502       cargotracking-domain-model-value\\n\u2502   \u2502       \u2502\\n\u2502   \u2502       \u2514\u2500\u2500 lib\\n\u2502   \u2502               cargotracking-lib-axon-cqrs\\n\u2502   \u2502               cargotracking-lib-axon-logging\\n\u2502   \u2502               cargotracking-lib-boundary-api\\n\u2502   \u2502               cargotracking-lib-boundary-query-api\\n\u2502   \u2502               cargotracking-lib-domain-model-command\\n\u2502   \u2502               cargotracking-lib-domain-model-event\\n\u2502   \u2502               cargotracking-lib-web\\n\u2502   \u2502               cargotracking-test-support\\n\u2502   \u2502\\n\u2502   \u251c\u2500\u2500 lib\\n\u2502   \u2502   \u251c\u2500\u2500 hi\\n\u2502   \u2502   \u2502       klokwrk-lib-hi-datasourceproxy-springboot\\n\u2502   \u2502   \u2502       klokwrk-lib-hi-jackson-springboot\\n\u2502   \u2502   \u2502       klokwrk-lib-hi-spring-context\\n\u2502   \u2502   \u2502       klokwrk-lib-hi-spring-data-jpa\\n\u2502   \u2502   \u2502       klokwrk-lib-hi-validation-springboot\\n\u2502   \u2502   \u2502\\n\u2502   \u2502   \u251c\u2500\u2500 lo\\n\u2502   \u2502   \u2502       klokwrk-lib-lo-archunit\\n\u2502   \u2502   \u2502       klokwrk-lib-lo-datasourceproxy\\n\u2502   \u2502   \u2502       klokwrk-lib-lo-hibernate\\n\u2502   \u2502   \u2502       klokwrk-lib-lo-jackson\\n\u2502   \u2502   \u2502       klokwrk-lib-lo-uom\\n\u2502   \u2502   \u2502       klokwrk-lib-lo-validation-constraint\\n\u2502   \u2502   \u2502       klokwrk-lib-lo-validation-validator\\n\u2502   \u2502   \u2502\\n\u2502   \u2502   \u2514\u2500\u2500 xlang\\n\u2502   \u2502           klokwrk-lib-xlang-groovy-base\\n\u2502   \u2502           klokwrk-lib-xlang-groovy-contracts-match\\n\u2502   \u2502           klokwrk-lib-xlang-groovy-contracts-simple\\n\u2502   \u2502\\n\u2502   \u2514\u2500\u2500 other\\n\u2502       \u251c\u2500\u2500 platform\\n\u2502       \u2502       klokwrk-platform-base\\n\u2502       \u2502       klokwrk-platform-micronaut\\n\u2502       \u2502       klokwrk-platform-spring-boot\\n\u2502       \u2502\\n\u2502       \u2514\u2500\u2500 tool\\n\u2502               klokwrk-tool-gradle-source-repack\\n\u251c\u2500\u2500 support\\n\u2502   \u2514\u2500\u2500 ... (other files or directories)\\n\u2514\u2500\u2500 ... (other files or directories)\\nAt the top of the hierarchy, we have a project folder  - `klokwrk-project`. It is the equivalent of the whole system. In the strategic structure, the system name appears in the names of artifacts\\nconsidered to be conceptually at the level of a system.\\nRight below the root, we have `modules` and `support` folders. These should be the area of 99% of everyday work, with the `modules` folder taking a vast majority of that percentage.\\nThe `support` folder houses all kinds of supportive files like scripts, documentation, git hooks, etc. The `support` folder is free-form, and the strategic structure does not impose any\\nrecommendations or rules on its content. On the contrary, the strategic structure is applied to the content of the `modules` directory - the home of all source code modules in the system.\\nAt the 1st level of strategic structure - the system level, we have the content of the `modules` directory. It is divided into three subdirectories: `bc` (bounded context modules),\\n`lib` (system-level libraries), and `other` (miscellaneous helper modules).\\nAt the 2nd level - the bounded context level, we have the content of the `modules\/bc` directory that is further organized into three parts, `asd` (asd stands for **A** **S**ub**D**omain),\\n`domain-model` (bounded context domain model), and `lib` (bounded context libraries).\\nAt the 3rd level of a hierarchy, we have the content of the `modules\/bc\/[bounded-context-name]\/asd` directory that holds all bounded context's subdomains. The modules for each subdomain are further\\ndivided into `app` and `lib`. The `modules\/bc\/[bounded-context-name]\/asd\/[subdomain-name]\/app` directory contains the **subdomain applications** responsible for implementing concrete subdomain\\nscenarios. From the abstraction level and dependency perspectives, subdomain applications are at the top of the hierarchy. Subdomain applications speak the language of domain - the bounded context's\\nubiquitous language. They even contribute to it through the naming and meaning of use cases.\\nThe first thing that **subdomain libraries** (`modules\/bc\/[bounded-context-name]\/asd\/subdomain-name\/lib)` can hold is infrastructural code related to the technological choices made for that\\nparticular subdomain and are not reusable outside the subdomain. However, they can temporarily have infrastructural modules intended to be more reusable (either on the bounded context or system\\nlevels) at the end. Still, for whatever reason, it was more convenient to hold them at the subdomain level for a limited time.\\nThe second thing that can be found in subdomain libraries are business-related reusable modules that connect technological choices with the domain model. One characteristic example is the\\n`cargotracking-booking-lib-queryside-model-rdbms-jpa` module. Those kinds of modules do speak bounded context's ubiquitous language.\\nThe bounded context's **domain model** is implemented in `modules\/bc\/[bounded-context-name]\/domain-model`. Those modules contain the essence of the bounded context business logic. Implementation of\\nthe domain model should be free of technology as much as possible and practical. Adding external libraries is not strictly forbidden, but each addition should be conscious and must be carefully\\nevaluated. It is best to have tests that monitor and control the dependencies of a domain model. The domain model implements the majority of code-level representation of the bounded context's\\nubiquitous language and must be consistent across all bounded context's subdomains.\\nBy default, the directory `modules\/bc\/[bounded-context-name]\/lib` is the home of shareable **bounded context infrastructural libraries**. It contains modules with infrastructural code that is\\nreusable across the bounded context. Those modules are at a lower abstraction level than subdomain libraries. Bounded context infrastructural libraries do not speak domain language. However, they can\\nsupport the implementation of the domain model and other module groups higher in the hierarchy. Domain model should not generally depend on bounded context infrastructural libraries. Exceptions are\\nallowed but should be conscious and carefully managed.\\nDo note that another variant of bounded context libraries is also possible. It is a variant supporting the sharing of business logic at the bounded context level when necessary. In that case, instead\\nof a single `lib` directory, we would have `blib` and `ilib` directories. The `blib` directory would contain business-related modules that can depend on a domain model. On the contrary, the `ilib`\\ndirectory cannot use the domain model because it should contain infrastructural code only. The `ilib` directory role is the same as the role of `lib` directory from the default variant of bounded\\ncontext libraries.\\nLet's return to the `modules\/lib` directory containing general **system-level libraries**. It is divided into `hi`, `lo`, and `xlang` subdirectories. All system-level libraries are at lower\\ndependency and abstraction levels than any bounded context module.\\nAlthough separation on the high (`hi`) and low-level (`lo`) system libraries is somewhat arbitrary, it is helpful in practice. The `hi` directory is intended to contain\\n**high-level system libraries**, which are general infrastructural modules closer to the high-level technological frameworks (something like Spring, Spring Boot, or Axon frameworks) used in the\\nsystem. They could contain some specifics of our system, but usually, they do not. In that later case, they are general enough to be reused even outside of our system.\\nThe **low-level system libraries** from the `lo` directory deal with the customizations and extensions of widely used 3rd party libraries like Hibernate, Jackson, Java Bean validations, and similar.\\nBoth types of system-level libraries should not be, in general, dependencies of a domain model.\\nAt the lowest abstraction level, we have the **language extensions** (`modules\/lib\/xlang`). They focus on adding features to the programming language itself or its accompanying SDK (JDK in our case).\\nLanguage extensions can be used from everywhere, even from the domain model, without restrictions. Some of them are often written to ease the implementation of the domain model by making it more\\nexpressive and concise.\\n#### Characteristics of strategic structure\\nThe most important thing about strategic structure is not the structure itself but rather the distinguishing characteristics that it provides.\\nWe already mentioned abstraction levels and dependencies between groups of modules. If you look again at the example, you will notice that both of them are constantly flowing top to bottom through\\nthe strategic structure. For instance, subdomain applications depend on subdomain libraries. They both can depend on the domain model, which can depend on bounded context libraries and language\\nextensions. At the level of system libraries, high-level modules can depend on low-level modules, and they both can depend on the language extensions. However, none of the dependencies can come the\\nother way around. Dependencies are not allowed to flow from the bottom to the top.\\nWe have managed to do this because we applied strategic DDD concepts of bounded context and subdomains to the project structure. They provide sense and meaningfulness by connecting our code to the\\nbusiness. Without that business context, we will be left exclusively to the technical aspects, which are just insufficient. Technical aspects know nothing about the purpose of our system. They do not\\nknow anything about the business context.\\nDescribed characteristics bring important benefits when trying to understand or navigate through the system's code. Finding the desired functionality is much easier because we usually know, at least\\napproximately, where we should look for it. This can greatly reduce cognitive load while exploring unfamiliar (or even familiar) codebases.\\nIn addition, if you follow the proposed naming conventions for modules and their packages (see below), the same easy orientation can be applied at the package level or even if you pull out all\\nmodules into the flat structure. You will always know where to look for.\\n#### Naming conventions\\nYou have probably noticed that modules have very particular names reflecting their position in the strategic structure. The following table summarizes them as used in the example:\\n| Module group    | Naming scheme                                            | Example                                  |\\n|-----------------|----------------------------------------------------------|------------------------------------------|\\n| subdomain apps  | `[bounded-context-name]-[subdomain-name]-app-[app-name]` | `cargotracking-booking-app-commandside`  |\\n| subdomain libs  | `[bounded-context-name]-[subdomain-name]-lib-[lib-name]` | `cargotracking-booking-lib-boundary-web` |\\n| domain model    | `[bounded-context-name]-domain-model-[model-part-name]`  | `cargotracking-domain-model-aggregate`   |\\n| bc libs         | `[bounded-context-name]-lib-[lib-name]`                  | `cargotracking-lib-boundary-api`         |\\n| sys hi libs     | `[system-name]-lib-hi-[lib-name]`                        | `klokwrk-lib-hi-spring-context`          |\\n| sys lo libs     | `[system-name]-lib-lo-[lib-name]`                        | `klokwrk-lib-lo-jackson`                 |\\n| lang extensions | `[system-name]-lib-xlang-[lib-name]`                     | `klokwrk-lib-xlang-groovy-base`          |\\nModule naming conventions are essential because our modules are not always presented (i.e., try the Packages view in the IntelliJ IDEA's Project tool window) or used as a part of the hierarchy (think\\nof JAR names put in the same directory). For those reasons, our naming scheme closely follows the strategic structure hierarchy where parts of module names are directly pulled from corresponding\\nsubdirectory names. That way, we can keep the match between alphabetical order and the direction of dependencies.\\n> Note: When you have multiple bounded contexts and\/or multiple subdomains in the project, to get the exact match between alphabetical order and the direction of dependencies, you can use the `bc-`\\n> prefix in front of bounded context names and the `asd-` prefix for subdomain names.\\nThe same naming principles should also be applied to packages. Here are a few examples of package names:\\norg.klokwrk.cargotracking.booking.app.commandside.*\\norg.klokwrk.cargotracking.booking.lib.boundary.web.*\\norg.klokwrk.cargotracking.domain.model.aggregate.*\\norg.klokwrk.cargotracking.lib.boundary.api.*\\norg.klokwrk.lib.hi.spring.context.*\\norg.klokwrk.lib.lo.jackson.*\\norg.klokwrk.lib.xlang.groovy.base.*\\nWith those naming conventions, we should be able to avoid naming collisions on the module and package levels.\\n#### The general scheme of strategic structure\\nIn some circumstances, we may need additional elements in the strategic structure to deal with shared libraries at different levels. Examples of those, with sparse explanations, are given in the\\ngeneral scheme of strategic structure below:\\nmodules\\n\u251c\u2500\u2500 bc\\n\u2502   \u251c\u2500\u2500 my_food\\n\u2502   \u2502   \u251c\u2500\u2500 asd\\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 restaurant\\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 app\\n\u2502   \u2502   \u2502   \u2502   \u2502       ... *\\n\u2502   \u2502   \u2502   \u2502   \u2514\u2500\u2500 lib\\n\u2502   \u2502   \u2502   \u2502           ... *\\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 menu_management\\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 app\\n\u2502   \u2502   \u2502   \u2502   \u2502       ... *\\n\u2502   \u2502   \u2502   \u2502   \u2514\u2500\u2500 lib\\n\u2502   \u2502   \u2502   \u2502           ... *\\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 zshared         \/\/ sharing code between subdomains if necessary\\n\u2502   \u2502   \u2502       \u2514\u2500\u2500 lib\\n\u2502   \u2502   \u2502           ... *\\n\u2502   \u2502   \u251c\u2500\u2500 domain-model\\n\u2502   \u2502   \u2502       ... *\\n\u2502   \u2502   \u2514\u2500\u2500 lib                 \/\/ bounded context libraries - default variant\\n\u2502   \u2502           ... *           \/\/ Can be split into \"blib\" and \"ilib\" directories when the sharing of\\n\u2502   \u2502                           \/\/ business logic is necessary at the level of a single bounded context\\n\u2502   \u251c\u2500\u2500 my_carrier\\n\u2502   \u2502   \u251c\u2500\u2500 asd\\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 app\\n\u2502   \u2502   \u2502   \u2502       ... *\\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 lib\\n\u2502   \u2502   \u2502           ... *\\n\u2502   \u2502   \u251c\u2500\u2500 domain-model\\n\u2502   \u2502   \u2502       ... *\\n\u2502   \u2502   \u2514\u2500\u2500 lib\\n\u2502   \u2502           ... *\\n\u2502   \u2514\u2500\u2500 zshared                 \/\/ shared code between multiple bounded contexts (if necessary).\\n\u2502       \u2502                       \/\/ \"z\" prefix - funny reference to \"zee Germans\" from Snatch movie.\\n\u2502       \u2502                       \/\/ Moves \"zshared\" at the last place alphabetically, which matches\\n\u2502       \u2502                       \/\/ the proper place in terms of dependencies and abstraction levels.\\n\u2502       \u251c\u2500\u2500 domain-model\\n\u2502       \u2502       ... *\\n\u2502       \u2514\u2500\u2500 lib\\n\u2502               ... *\\n\u251c\u2500\u2500 lib\\n\u2502   \u251c\u2500\u2500 hi\\n\u2502   \u2502       ... *\\n\u2502   \u251c\u2500\u2500 lo\\n\u2502   \u2502       ... *\\n\u2502   \u2514\u2500\u2500 xlang\\n\u2502           ... *\\n\u2514\u2500\u2500 other            \/\/ supportive project's code for various \"other\" purposes\\n\u251c\u2500\u2500 build\\n\u2502       ... *\\n\u251c\u2500\u2500 tool\\n\u2502       ... *\\n\u2514\u2500\u2500 ...\\n#### Simplification - the case of bounded context boundaries matching 1:1 with subdomain\\nThe one-to-one match between bounded context boundaries and corresponding subdomain is considered to be the \"ideal\" case, and it is relatively common in practice. When we know how a fully expanded\\nstrategic structure works and looks like, it is relatively easy to come up with simplification for this particular case.\\nHere are \"refactoring\" steps and the example based on our concrete example from the beginning of this document:\\n- move subdomain applications to the bounded context level\\n- merge subdomain libraries with bounded context libraries\\n- split bounded context libraries into `blib` and `ilib` directories if necessary\\n- rename corresponding modules and packages\\nklokwrk-project\\n\u251c\u2500\u2500 ... (other files or directories)\\n\u251c\u2500\u2500 modules\\n\u2502   \u251c\u2500\u2500 bc\\n\u2502   \u2502   \u2514\u2500\u2500 cargotracking\\n\u2502   \u2502       \u251c\u2500\u2500 app\\n\u2502   \u2502       \u2502       cargotracking-app-commandside\\n\u2502   \u2502       \u2502       cargotracking-app-queryside-projection-rdbms\\n\u2502   \u2502       \u2502       cargotracking-app-queryside-view\\n\u2502   \u2502       \u2502       cargotracking-app-rdbms-management\\n\u2502   \u2502       \u2502\\n\u2502   \u2502       \u251c\u2500\u2500 blib\\n\u2502   \u2502       \u2502       cargotracking-blib-out-customer\\n\u2502   \u2502       \u2502       cargotracking-blib-queryside-model-rdbms-jpa\\n\u2502   \u2502       \u2502\\n\u2502   \u2502       \u251c\u2500\u2500 domain-model\\n\u2502   \u2502       \u2502       cargotracking-domain-model-aggregate\\n\u2502   \u2502       \u2502       cargotracking-domain-model-command\\n\u2502   \u2502       \u2502       cargotracking-domain-model-event\\n\u2502   \u2502       \u2502       cargotracking-domain-model-service\\n\u2502   \u2502       \u2502       cargotracking-domain-model-value\\n\u2502   \u2502       \u2502\\n\u2502   \u2502       \u2514\u2500\u2500 ilib\\n\u2502   \u2502               cargotracking-ilib-axon-cqrs\\n\u2502   \u2502               cargotracking-ilib-axon-logging\\n\u2502   \u2502               cargotracking-ilib-boundary-api\\n\u2502   \u2502               cargotracking-ilib-boundary-query-api\\n\u2502   \u2502               cargotracking-ilib-boundary-web\\n\u2502   \u2502               cargotracking-ilib-domain-model-command\\n\u2502   \u2502               cargotracking-ilib-domain-model-event\\n\u2502   \u2502               cargotracking-ilib-web\\n\u2502   \u2502               cargotracking-test-component\\n\u2502   \u2502               cargotracking-test-support\\n\u2502   \u2502               cargotracking-test-support-queryside\\n\u2502   \u2502               cargotracking-test-support-testcontainers\\n\u2502   \u2502\\n\u2502   \u251c\u2500\u2500 lib\\n\u2502   \u2502   \u251c\u2500\u2500 hi\\n\u2502   \u2502   \u2502       ... *\\n\u2502   \u2502   \u251c\u2500\u2500 lo\\n\u2502   \u2502   \u2502       ... *\\n\u2502   \u2502   \u2514\u2500\u2500 xlang\\n\u2502   \u2502           ... *\\n\u2502   \u2514\u2500\u2500 other\\n\u2502           ... *\\n\u251c\u2500\u2500 support\\n\u2502       ... *\\n\u2514\u2500\u2500 ... *\\n","tokens":335,"id":4889,"text":"## Context\\nExcluding the simplest hello-world-like cases, any useful project typically contains several modules. The traditional way to organize project modules is just to put them under the project root.\\nWe can call that structure simply **flat structure**.\\nWhile the flat structure is appropriate and sufficient for simpler projects, when the project grows and the number of modules increases, the flat structure starts suffering from many drawbacks:\\n* Flat structure does not scale well when the number of modules grows.\\n* Flat structure is difficult and confusing to navigate with numerous modules at the same hierarchy level.\\n* Flat structure does not suggest a direction of dependencies between modules.\\n* Flat structure does not suggest abstraction levels of modules.\\n* Flat structure does not suggest where are the system's entry points.\\n* Flat structure can use only module names to provide hints about relations between modules. Unfortunately, even that possibility is rarely leveraged.\\n* Flat structure does not use any high-level constructs that may suggest how modules are organized and related.\\n* Negative usage aspects are getting worse and worse as we add additional modules.\\n* Flat structure often requires extracting modules in separate repositories just because confusion becomes unbearable with a larger number of modules.\\n* When using microservices, the flat structure practically forces us to use one project per microservice.\\n> Note: Terms **flat structure** and **strategic structure** (see below) are ad-hoc terms introduced just for this document. However, in the `klokwrk-project`, we may use them in other places for\\n> convenience.\\n### Architectural Context\\n* System (`klokwrk-project`)\\n\n\n##Decision\n**We'll organize project modules around strategic DDD (Domain Driven Design) constructs of bounded context and subdomains.**\\nOur project organization will follow principles and recommendations of **strategic structure** as defined below.\\n### Decision Details\\nWe'll start with a concrete example of the strategic structure used in the klokwrk at the time of writing this document. As a follow-up, we'll present a general scheme for creating the strategic\\nstructure focusing on the differences to the given concrete example.\\n#### Strategic structure in klokwrk\\nThe current project layout in the klokwrk looks like this:\\nklokwrk-project\\n\u251c\u2500\u2500 ... (other files or directories)\\n\u251c\u2500\u2500 modules\\n\u2502   \u251c\u2500\u2500 bc\\n\u2502   \u2502   \u2514\u2500\u2500 cargotracking\\n\u2502   \u2502       \u251c\u2500\u2500 asd\\n\u2502   \u2502       \u2502   \u2514\u2500\u2500 booking\\n\u2502   \u2502       \u2502       \u251c\u2500\u2500 app\\n\u2502   \u2502       \u2502       \u2502       cargotracking-booking-app-commandside\\n\u2502   \u2502       \u2502       \u2502       cargotracking-booking-app-queryside-projection-rdbms\\n\u2502   \u2502       \u2502       \u2502       cargotracking-booking-app-queryside-view\\n\u2502   \u2502       \u2502       \u2502       cargotracking-booking-app-rdbms-management\\n\u2502   \u2502       \u2502       \u2514\u2500\u2500 lib\\n\u2502   \u2502       \u2502               cargotracking-booking-lib-boundary-web\\n\u2502   \u2502       \u2502               cargotracking-booking-lib-out-customer\\n\u2502   \u2502       \u2502               cargotracking-booking-lib-queryside-model-rdbms-jpa\\n\u2502   \u2502       \u2502               cargotracking-booking-test-component\\n\u2502   \u2502       \u2502               cargotracking-booking-test-support-queryside\\n\u2502   \u2502       \u2502               cargotracking-booking-test-support-testcontainers\\n\u2502   \u2502       \u2502\\n\u2502   \u2502       \u251c\u2500\u2500 domain-model\\n\u2502   \u2502       \u2502       cargotracking-domain-model-aggregate\\n\u2502   \u2502       \u2502       cargotracking-domain-model-command\\n\u2502   \u2502       \u2502       cargotracking-domain-model-event\\n\u2502   \u2502       \u2502       cargotracking-domain-model-service\\n\u2502   \u2502       \u2502       cargotracking-domain-model-value\\n\u2502   \u2502       \u2502\\n\u2502   \u2502       \u2514\u2500\u2500 lib\\n\u2502   \u2502               cargotracking-lib-axon-cqrs\\n\u2502   \u2502               cargotracking-lib-axon-logging\\n\u2502   \u2502               cargotracking-lib-boundary-api\\n\u2502   \u2502               cargotracking-lib-boundary-query-api\\n\u2502   \u2502               cargotracking-lib-domain-model-command\\n\u2502   \u2502               cargotracking-lib-domain-model-event\\n\u2502   \u2502               cargotracking-lib-web\\n\u2502   \u2502               cargotracking-test-support\\n\u2502   \u2502\\n\u2502   \u251c\u2500\u2500 lib\\n\u2502   \u2502   \u251c\u2500\u2500 hi\\n\u2502   \u2502   \u2502       klokwrk-lib-hi-datasourceproxy-springboot\\n\u2502   \u2502   \u2502       klokwrk-lib-hi-jackson-springboot\\n\u2502   \u2502   \u2502       klokwrk-lib-hi-spring-context\\n\u2502   \u2502   \u2502       klokwrk-lib-hi-spring-data-jpa\\n\u2502   \u2502   \u2502       klokwrk-lib-hi-validation-springboot\\n\u2502   \u2502   \u2502\\n\u2502   \u2502   \u251c\u2500\u2500 lo\\n\u2502   \u2502   \u2502       klokwrk-lib-lo-archunit\\n\u2502   \u2502   \u2502       klokwrk-lib-lo-datasourceproxy\\n\u2502   \u2502   \u2502       klokwrk-lib-lo-hibernate\\n\u2502   \u2502   \u2502       klokwrk-lib-lo-jackson\\n\u2502   \u2502   \u2502       klokwrk-lib-lo-uom\\n\u2502   \u2502   \u2502       klokwrk-lib-lo-validation-constraint\\n\u2502   \u2502   \u2502       klokwrk-lib-lo-validation-validator\\n\u2502   \u2502   \u2502\\n\u2502   \u2502   \u2514\u2500\u2500 xlang\\n\u2502   \u2502           klokwrk-lib-xlang-groovy-base\\n\u2502   \u2502           klokwrk-lib-xlang-groovy-contracts-match\\n\u2502   \u2502           klokwrk-lib-xlang-groovy-contracts-simple\\n\u2502   \u2502\\n\u2502   \u2514\u2500\u2500 other\\n\u2502       \u251c\u2500\u2500 platform\\n\u2502       \u2502       klokwrk-platform-base\\n\u2502       \u2502       klokwrk-platform-micronaut\\n\u2502       \u2502       klokwrk-platform-spring-boot\\n\u2502       \u2502\\n\u2502       \u2514\u2500\u2500 tool\\n\u2502               klokwrk-tool-gradle-source-repack\\n\u251c\u2500\u2500 support\\n\u2502   \u2514\u2500\u2500 ... (other files or directories)\\n\u2514\u2500\u2500 ... (other files or directories)\\nAt the top of the hierarchy, we have a project folder  - `klokwrk-project`. It is the equivalent of the whole system. In the strategic structure, the system name appears in the names of artifacts\\nconsidered to be conceptually at the level of a system.\\nRight below the root, we have `modules` and `support` folders. These should be the area of 99% of everyday work, with the `modules` folder taking a vast majority of that percentage.\\nThe `support` folder houses all kinds of supportive files like scripts, documentation, git hooks, etc. The `support` folder is free-form, and the strategic structure does not impose any\\nrecommendations or rules on its content. On the contrary, the strategic structure is applied to the content of the `modules` directory - the home of all source code modules in the system.\\nAt the 1st level of strategic structure - the system level, we have the content of the `modules` directory. It is divided into three subdirectories: `bc` (bounded context modules),\\n`lib` (system-level libraries), and `other` (miscellaneous helper modules).\\nAt the 2nd level - the bounded context level, we have the content of the `modules\/bc` directory that is further organized into three parts, `asd` (asd stands for **A** **S**ub**D**omain),\\n`domain-model` (bounded context domain model), and `lib` (bounded context libraries).\\nAt the 3rd level of a hierarchy, we have the content of the `modules\/bc\/[bounded-context-name]\/asd` directory that holds all bounded context's subdomains. The modules for each subdomain are further\\ndivided into `app` and `lib`. The `modules\/bc\/[bounded-context-name]\/asd\/[subdomain-name]\/app` directory contains the **subdomain applications** responsible for implementing concrete subdomain\\nscenarios. From the abstraction level and dependency perspectives, subdomain applications are at the top of the hierarchy. Subdomain applications speak the language of domain - the bounded context's\\nubiquitous language. They even contribute to it through the naming and meaning of use cases.\\nThe first thing that **subdomain libraries** (`modules\/bc\/[bounded-context-name]\/asd\/subdomain-name\/lib)` can hold is infrastructural code related to the technological choices made for that\\nparticular subdomain and are not reusable outside the subdomain. However, they can temporarily have infrastructural modules intended to be more reusable (either on the bounded context or system\\nlevels) at the end. Still, for whatever reason, it was more convenient to hold them at the subdomain level for a limited time.\\nThe second thing that can be found in subdomain libraries are business-related reusable modules that connect technological choices with the domain model. One characteristic example is the\\n`cargotracking-booking-lib-queryside-model-rdbms-jpa` module. Those kinds of modules do speak bounded context's ubiquitous language.\\nThe bounded context's **domain model** is implemented in `modules\/bc\/[bounded-context-name]\/domain-model`. Those modules contain the essence of the bounded context business logic. Implementation of\\nthe domain model should be free of technology as much as possible and practical. Adding external libraries is not strictly forbidden, but each addition should be conscious and must be carefully\\nevaluated. It is best to have tests that monitor and control the dependencies of a domain model. The domain model implements the majority of code-level representation of the bounded context's\\nubiquitous language and must be consistent across all bounded context's subdomains.\\nBy default, the directory `modules\/bc\/[bounded-context-name]\/lib` is the home of shareable **bounded context infrastructural libraries**. It contains modules with infrastructural code that is\\nreusable across the bounded context. Those modules are at a lower abstraction level than subdomain libraries. Bounded context infrastructural libraries do not speak domain language. However, they can\\nsupport the implementation of the domain model and other module groups higher in the hierarchy. Domain model should not generally depend on bounded context infrastructural libraries. Exceptions are\\nallowed but should be conscious and carefully managed.\\nDo note that another variant of bounded context libraries is also possible. It is a variant supporting the sharing of business logic at the bounded context level when necessary. In that case, instead\\nof a single `lib` directory, we would have `blib` and `ilib` directories. The `blib` directory would contain business-related modules that can depend on a domain model. On the contrary, the `ilib`\\ndirectory cannot use the domain model because it should contain infrastructural code only. The `ilib` directory role is the same as the role of `lib` directory from the default variant of bounded\\ncontext libraries.\\nLet's return to the `modules\/lib` directory containing general **system-level libraries**. It is divided into `hi`, `lo`, and `xlang` subdirectories. All system-level libraries are at lower\\ndependency and abstraction levels than any bounded context module.\\nAlthough separation on the high (`hi`) and low-level (`lo`) system libraries is somewhat arbitrary, it is helpful in practice. The `hi` directory is intended to contain\\n**high-level system libraries**, which are general infrastructural modules closer to the high-level technological frameworks (something like Spring, Spring Boot, or Axon frameworks) used in the\\nsystem. They could contain some specifics of our system, but usually, they do not. In that later case, they are general enough to be reused even outside of our system.\\nThe **low-level system libraries** from the `lo` directory deal with the customizations and extensions of widely used 3rd party libraries like Hibernate, Jackson, Java Bean validations, and similar.\\nBoth types of system-level libraries should not be, in general, dependencies of a domain model.\\nAt the lowest abstraction level, we have the **language extensions** (`modules\/lib\/xlang`). They focus on adding features to the programming language itself or its accompanying SDK (JDK in our case).\\nLanguage extensions can be used from everywhere, even from the domain model, without restrictions. Some of them are often written to ease the implementation of the domain model by making it more\\nexpressive and concise.\\n#### Characteristics of strategic structure\\nThe most important thing about strategic structure is not the structure itself but rather the distinguishing characteristics that it provides.\\nWe already mentioned abstraction levels and dependencies between groups of modules. If you look again at the example, you will notice that both of them are constantly flowing top to bottom through\\nthe strategic structure. For instance, subdomain applications depend on subdomain libraries. They both can depend on the domain model, which can depend on bounded context libraries and language\\nextensions. At the level of system libraries, high-level modules can depend on low-level modules, and they both can depend on the language extensions. However, none of the dependencies can come the\\nother way around. Dependencies are not allowed to flow from the bottom to the top.\\nWe have managed to do this because we applied strategic DDD concepts of bounded context and subdomains to the project structure. They provide sense and meaningfulness by connecting our code to the\\nbusiness. Without that business context, we will be left exclusively to the technical aspects, which are just insufficient. Technical aspects know nothing about the purpose of our system. They do not\\nknow anything about the business context.\\nDescribed characteristics bring important benefits when trying to understand or navigate through the system's code. Finding the desired functionality is much easier because we usually know, at least\\napproximately, where we should look for it. This can greatly reduce cognitive load while exploring unfamiliar (or even familiar) codebases.\\nIn addition, if you follow the proposed naming conventions for modules and their packages (see below), the same easy orientation can be applied at the package level or even if you pull out all\\nmodules into the flat structure. You will always know where to look for.\\n#### Naming conventions\\nYou have probably noticed that modules have very particular names reflecting their position in the strategic structure. The following table summarizes them as used in the example:\\n| Module group    | Naming scheme                                            | Example                                  |\\n|-----------------|----------------------------------------------------------|------------------------------------------|\\n| subdomain apps  | `[bounded-context-name]-[subdomain-name]-app-[app-name]` | `cargotracking-booking-app-commandside`  |\\n| subdomain libs  | `[bounded-context-name]-[subdomain-name]-lib-[lib-name]` | `cargotracking-booking-lib-boundary-web` |\\n| domain model    | `[bounded-context-name]-domain-model-[model-part-name]`  | `cargotracking-domain-model-aggregate`   |\\n| bc libs         | `[bounded-context-name]-lib-[lib-name]`                  | `cargotracking-lib-boundary-api`         |\\n| sys hi libs     | `[system-name]-lib-hi-[lib-name]`                        | `klokwrk-lib-hi-spring-context`          |\\n| sys lo libs     | `[system-name]-lib-lo-[lib-name]`                        | `klokwrk-lib-lo-jackson`                 |\\n| lang extensions | `[system-name]-lib-xlang-[lib-name]`                     | `klokwrk-lib-xlang-groovy-base`          |\\nModule naming conventions are essential because our modules are not always presented (i.e., try the Packages view in the IntelliJ IDEA's Project tool window) or used as a part of the hierarchy (think\\nof JAR names put in the same directory). For those reasons, our naming scheme closely follows the strategic structure hierarchy where parts of module names are directly pulled from corresponding\\nsubdirectory names. That way, we can keep the match between alphabetical order and the direction of dependencies.\\n> Note: When you have multiple bounded contexts and\/or multiple subdomains in the project, to get the exact match between alphabetical order and the direction of dependencies, you can use the `bc-`\\n> prefix in front of bounded context names and the `asd-` prefix for subdomain names.\\nThe same naming principles should also be applied to packages. Here are a few examples of package names:\\norg.klokwrk.cargotracking.booking.app.commandside.*\\norg.klokwrk.cargotracking.booking.lib.boundary.web.*\\norg.klokwrk.cargotracking.domain.model.aggregate.*\\norg.klokwrk.cargotracking.lib.boundary.api.*\\norg.klokwrk.lib.hi.spring.context.*\\norg.klokwrk.lib.lo.jackson.*\\norg.klokwrk.lib.xlang.groovy.base.*\\nWith those naming conventions, we should be able to avoid naming collisions on the module and package levels.\\n#### The general scheme of strategic structure\\nIn some circumstances, we may need additional elements in the strategic structure to deal with shared libraries at different levels. Examples of those, with sparse explanations, are given in the\\ngeneral scheme of strategic structure below:\\nmodules\\n\u251c\u2500\u2500 bc\\n\u2502   \u251c\u2500\u2500 my_food\\n\u2502   \u2502   \u251c\u2500\u2500 asd\\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 restaurant\\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 app\\n\u2502   \u2502   \u2502   \u2502   \u2502       ... *\\n\u2502   \u2502   \u2502   \u2502   \u2514\u2500\u2500 lib\\n\u2502   \u2502   \u2502   \u2502           ... *\\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 menu_management\\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 app\\n\u2502   \u2502   \u2502   \u2502   \u2502       ... *\\n\u2502   \u2502   \u2502   \u2502   \u2514\u2500\u2500 lib\\n\u2502   \u2502   \u2502   \u2502           ... *\\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 zshared         \/\/ sharing code between subdomains if necessary\\n\u2502   \u2502   \u2502       \u2514\u2500\u2500 lib\\n\u2502   \u2502   \u2502           ... *\\n\u2502   \u2502   \u251c\u2500\u2500 domain-model\\n\u2502   \u2502   \u2502       ... *\\n\u2502   \u2502   \u2514\u2500\u2500 lib                 \/\/ bounded context libraries - default variant\\n\u2502   \u2502           ... *           \/\/ Can be split into \"blib\" and \"ilib\" directories when the sharing of\\n\u2502   \u2502                           \/\/ business logic is necessary at the level of a single bounded context\\n\u2502   \u251c\u2500\u2500 my_carrier\\n\u2502   \u2502   \u251c\u2500\u2500 asd\\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 app\\n\u2502   \u2502   \u2502   \u2502       ... *\\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 lib\\n\u2502   \u2502   \u2502           ... *\\n\u2502   \u2502   \u251c\u2500\u2500 domain-model\\n\u2502   \u2502   \u2502       ... *\\n\u2502   \u2502   \u2514\u2500\u2500 lib\\n\u2502   \u2502           ... *\\n\u2502   \u2514\u2500\u2500 zshared                 \/\/ shared code between multiple bounded contexts (if necessary).\\n\u2502       \u2502                       \/\/ \"z\" prefix - funny reference to \"zee Germans\" from Snatch movie.\\n\u2502       \u2502                       \/\/ Moves \"zshared\" at the last place alphabetically, which matches\\n\u2502       \u2502                       \/\/ the proper place in terms of dependencies and abstraction levels.\\n\u2502       \u251c\u2500\u2500 domain-model\\n\u2502       \u2502       ... *\\n\u2502       \u2514\u2500\u2500 lib\\n\u2502               ... *\\n\u251c\u2500\u2500 lib\\n\u2502   \u251c\u2500\u2500 hi\\n\u2502   \u2502       ... *\\n\u2502   \u251c\u2500\u2500 lo\\n\u2502   \u2502       ... *\\n\u2502   \u2514\u2500\u2500 xlang\\n\u2502           ... *\\n\u2514\u2500\u2500 other            \/\/ supportive project's code for various \"other\" purposes\\n\u251c\u2500\u2500 build\\n\u2502       ... *\\n\u251c\u2500\u2500 tool\\n\u2502       ... *\\n\u2514\u2500\u2500 ...\\n#### Simplification - the case of bounded context boundaries matching 1:1 with subdomain\\nThe one-to-one match between bounded context boundaries and corresponding subdomain is considered to be the \"ideal\" case, and it is relatively common in practice. When we know how a fully expanded\\nstrategic structure works and looks like, it is relatively easy to come up with simplification for this particular case.\\nHere are \"refactoring\" steps and the example based on our concrete example from the beginning of this document:\\n- move subdomain applications to the bounded context level\\n- merge subdomain libraries with bounded context libraries\\n- split bounded context libraries into `blib` and `ilib` directories if necessary\\n- rename corresponding modules and packages\\nklokwrk-project\\n\u251c\u2500\u2500 ... (other files or directories)\\n\u251c\u2500\u2500 modules\\n\u2502   \u251c\u2500\u2500 bc\\n\u2502   \u2502   \u2514\u2500\u2500 cargotracking\\n\u2502   \u2502       \u251c\u2500\u2500 app\\n\u2502   \u2502       \u2502       cargotracking-app-commandside\\n\u2502   \u2502       \u2502       cargotracking-app-queryside-projection-rdbms\\n\u2502   \u2502       \u2502       cargotracking-app-queryside-view\\n\u2502   \u2502       \u2502       cargotracking-app-rdbms-management\\n\u2502   \u2502       \u2502\\n\u2502   \u2502       \u251c\u2500\u2500 blib\\n\u2502   \u2502       \u2502       cargotracking-blib-out-customer\\n\u2502   \u2502       \u2502       cargotracking-blib-queryside-model-rdbms-jpa\\n\u2502   \u2502       \u2502\\n\u2502   \u2502       \u251c\u2500\u2500 domain-model\\n\u2502   \u2502       \u2502       cargotracking-domain-model-aggregate\\n\u2502   \u2502       \u2502       cargotracking-domain-model-command\\n\u2502   \u2502       \u2502       cargotracking-domain-model-event\\n\u2502   \u2502       \u2502       cargotracking-domain-model-service\\n\u2502   \u2502       \u2502       cargotracking-domain-model-value\\n\u2502   \u2502       \u2502\\n\u2502   \u2502       \u2514\u2500\u2500 ilib\\n\u2502   \u2502               cargotracking-ilib-axon-cqrs\\n\u2502   \u2502               cargotracking-ilib-axon-logging\\n\u2502   \u2502               cargotracking-ilib-boundary-api\\n\u2502   \u2502               cargotracking-ilib-boundary-query-api\\n\u2502   \u2502               cargotracking-ilib-boundary-web\\n\u2502   \u2502               cargotracking-ilib-domain-model-command\\n\u2502   \u2502               cargotracking-ilib-domain-model-event\\n\u2502   \u2502               cargotracking-ilib-web\\n\u2502   \u2502               cargotracking-test-component\\n\u2502   \u2502               cargotracking-test-support\\n\u2502   \u2502               cargotracking-test-support-queryside\\n\u2502   \u2502               cargotracking-test-support-testcontainers\\n\u2502   \u2502\\n\u2502   \u251c\u2500\u2500 lib\\n\u2502   \u2502   \u251c\u2500\u2500 hi\\n\u2502   \u2502   \u2502       ... *\\n\u2502   \u2502   \u251c\u2500\u2500 lo\\n\u2502   \u2502   \u2502       ... *\\n\u2502   \u2502   \u2514\u2500\u2500 xlang\\n\u2502   \u2502           ... *\\n\u2502   \u2514\u2500\u2500 other\\n\u2502           ... *\\n\u251c\u2500\u2500 support\\n\u2502       ... *\\n\u2514\u2500\u2500 ... *\\n","MatchedIds":[2756,4896,4890,1792,3483],"Predicted":"## Decision\nWe will adopt a **strategic structure** to organize the project modules by grouping related modules into high-level directories that reflect their roles and relationships within the system.\n\nThe strategic structure will break down modules into nested directories based on functionality, domain, or service boundaries. This will improve project maintainability and scalability, enabling:\n\n- Clear navigation through a hierarchical folder structure aligned with system abstractions.\n- Better comprehension of the direction of dependencies and abstraction levels.\n- Enhanced discoverability of system entry points.\n- Reduced confusion and simplified dependency management.\n\n### Implementation Details\n1. **Core Domain**: Modules representing core business logic will be grouped here.\n2. **Adapters and Plugins**: Modules responsible for input\/output operations, including external API integrations, databases, and UI.\n3. **Application Services**: Modules containing application-specific services, orchestrations, and use cases.\n4. **Utilities and Common**: Common utils and shared modules leveraged across different parts of the system.\n5. **Microservices**: Each microservice will be represented as a separate top-level directory with its related modules nested inside.\n\nBy leveraging the strategic structure, we provide a clear and logical layout for developers to navigate the project, improving the ease of making transversal refactors, and fostering consistent module dependency management.","GenTime":"2024-08-19 23:26:16"}
{"File Name":"james-project\/0041-replace-jcloud-with-s3.md","Context":"## Context\\nJames has an interface called blob-store that allows to choose how Blobs are stored between several implementations.\\nFor Distributed product, the default backend is Cassandra because it's fast and it doesn't require yet another\\nserver.\\nWhen storage cost concerns are more important than raw performances, James can store Blobs in S3 or Swift\\nObject Storage technologies.\\nObject Storage is known to be cheap and yet offers guarantee about durability.\\nWhen we implemented Object Storage blob-store in the past, we decided to implement it with\\n[Apache jcloud library](https:\/\/jclouds.apache.org\/) because we wanted to target both S3 and Swift API.\\nHowever, we found that our implementation was complex and it didn't fit the Reactive style we now have in the codebase.\\nIt also contribute negatively to the project build time.\\nAt the same time, we figured out Swift was providing a good compatibility layer for S3 and that we may drop\\nour Swift code without dropping Swift support at the same time.\\n","Decision":"* Use AWS S3 v2 SDK to implement a Reactive S3 blob-store (`blobstore-s3`).\\n* Replace current `blob-store-objectstorage` with `blobstore-s3`\\n* Run load tests to ensure there's no penalty when switching from Swift to S3 over Swift\\n","tokens":209,"id":2897,"text":"## Context\\nJames has an interface called blob-store that allows to choose how Blobs are stored between several implementations.\\nFor Distributed product, the default backend is Cassandra because it's fast and it doesn't require yet another\\nserver.\\nWhen storage cost concerns are more important than raw performances, James can store Blobs in S3 or Swift\\nObject Storage technologies.\\nObject Storage is known to be cheap and yet offers guarantee about durability.\\nWhen we implemented Object Storage blob-store in the past, we decided to implement it with\\n[Apache jcloud library](https:\/\/jclouds.apache.org\/) because we wanted to target both S3 and Swift API.\\nHowever, we found that our implementation was complex and it didn't fit the Reactive style we now have in the codebase.\\nIt also contribute negatively to the project build time.\\nAt the same time, we figured out Swift was providing a good compatibility layer for S3 and that we may drop\\nour Swift code without dropping Swift support at the same time.\\n\n\n##Decision\n* Use AWS S3 v2 SDK to implement a Reactive S3 blob-store (`blobstore-s3`).\\n* Replace current `blob-store-objectstorage` with `blobstore-s3`\\n* Run load tests to ensure there's no penalty when switching from Swift to S3 over Swift\\n","MatchedIds":[2896,2145,2122,1574,4747],"Predicted":"## Decision\nTo simplify our codebase, align with our Reactive style, and improve build times, we will move away from using the Apache jcloud library and instead directly use the S3 SDK for our object storage needs. Since Swift provides a good compatibility layer with the S3 API, we can drop our dedicated Swift code without losing support for Swift-based object storage.\n\n### Positive Consequences\n- **Simplification**: Reduces complexity in the codebase by eliminating the need for an additional abstraction layer provided by jclouds.\n- **Efficiency**: Better alignment with the Reactive programming style present in the current codebase.\n- **Build Time Reduction**: Decreases the overall build time by removing dependencies and potentially large library integrations.\n\n### Negative Consequences\n- **Initial Transition Effort**: Some initial development effort will be required to transition from jclouds to the S3 SDK.\n- **Maintenance**: Directly managing interactions with multiple compatible services using the S3 SDK may introduce additional maintenance considerations.\n  \n### Implementation Steps\n1. **Evaluate the S3 SDK** for all necessary functionalities required by our blob-store implementation.\n2. **Refactor the existing object storage code** to use the S3 SDK for S3 and Swift interactions.\n3. **Test the new implementation** extensively to ensure compatibility and performance with both S3 and Swift Object Storage.\n4. **Remove the jcloud library dependency** and any related code from the codebase.\n5. **Update documentation** to reflect changes in the blob-store implementation and usage instructions.","GenTime":"2024-08-19 23:26:26"}
{"File Name":"tracking-consent-frontend\/0007-use-eslint-for-linting-and-formatting.md","Context":"## Context and Problem Statement\\nWe found that without a standardised format our javascript files ended up with different\\nformats in different files or even multiple formats in the same file.  We also found that\\nour IDEs had different configurations which meant that using an autoformat tool would give\\ndifferent results when each of us do it.\\n## Decision Drivers\\n* We wanted to spend less time doing manual formatting\\n* We wanted to spend less time undoing autoformatting which had been applied to unchanged lines\\n* We wanted to see easily which lines had actually changed when reviewing PRs\\n* We wanted to avoid discussions about individual's preferences for particular\\n","Decision":"* We wanted to spend less time doing manual formatting\\n* We wanted to spend less time undoing autoformatting which had been applied to unchanged lines\\n* We wanted to see easily which lines had actually changed when reviewing PRs\\n* We wanted to avoid discussions about individual's preferences for particular\\nChosen option: ESLint + Airbnb\\nWe decided to use ESLint with Airbnb because the Airbnb approach is [well documented](https:\/\/github.com\/airbnb\/javascript)\\nand adopting this meant we wouldn't have to make each decision ourselves.  We decided not to use Prettier as well, there\\nwas a discussion about the possibility of conflicting format changes between the two tools - we encountered one of these,\\nit was overcome with config updates.\\nESLint is compatible with both Javascript and Typescript projects which made it seem particularly suitable for us as we\\nchose to use Typescript for Tracking Consent but we usually use Javascript.\\nEach of the current team members use JetBrains IDEs and that has a built-in plugin which supports auto formatting on save,\\nthis is also a common feature which most IDEs have.\\nWe decided to add pre-commit and pre-push hooks which ensure that the style rules have been met but do not mutate code.\\nWe discussed the option of formatting the code on commit but we felt that this introduced an uncertainty about what we\\nwere committing.\\n### Positive Consequences\\n* Code style within a file will be consistent\\n* Files within a project will be consistent with each other\\n* When widely applied this Javascript\/Typescript files between projects will be consistent\\n* IDE will reformat for us which takes away the need for us to do it ourselves\\n* Code reviews will focus less on code format and more on the details of the changes made\\n* Builds will fail if format isn't followed which guarentees consistency\\n### Negative Consequences\\n* Builds will fail if format isn't followed which can disrupt focus\/workflow (mitigated by pre-commit hooks)\\n","tokens":133,"id":3521,"text":"## Context and Problem Statement\\nWe found that without a standardised format our javascript files ended up with different\\nformats in different files or even multiple formats in the same file.  We also found that\\nour IDEs had different configurations which meant that using an autoformat tool would give\\ndifferent results when each of us do it.\\n## Decision Drivers\\n* We wanted to spend less time doing manual formatting\\n* We wanted to spend less time undoing autoformatting which had been applied to unchanged lines\\n* We wanted to see easily which lines had actually changed when reviewing PRs\\n* We wanted to avoid discussions about individual's preferences for particular\\n\n\n##Decision\n* We wanted to spend less time doing manual formatting\\n* We wanted to spend less time undoing autoformatting which had been applied to unchanged lines\\n* We wanted to see easily which lines had actually changed when reviewing PRs\\n* We wanted to avoid discussions about individual's preferences for particular\\nChosen option: ESLint + Airbnb\\nWe decided to use ESLint with Airbnb because the Airbnb approach is [well documented](https:\/\/github.com\/airbnb\/javascript)\\nand adopting this meant we wouldn't have to make each decision ourselves.  We decided not to use Prettier as well, there\\nwas a discussion about the possibility of conflicting format changes between the two tools - we encountered one of these,\\nit was overcome with config updates.\\nESLint is compatible with both Javascript and Typescript projects which made it seem particularly suitable for us as we\\nchose to use Typescript for Tracking Consent but we usually use Javascript.\\nEach of the current team members use JetBrains IDEs and that has a built-in plugin which supports auto formatting on save,\\nthis is also a common feature which most IDEs have.\\nWe decided to add pre-commit and pre-push hooks which ensure that the style rules have been met but do not mutate code.\\nWe discussed the option of formatting the code on commit but we felt that this introduced an uncertainty about what we\\nwere committing.\\n### Positive Consequences\\n* Code style within a file will be consistent\\n* Files within a project will be consistent with each other\\n* When widely applied this Javascript\/Typescript files between projects will be consistent\\n* IDE will reformat for us which takes away the need for us to do it ourselves\\n* Code reviews will focus less on code format and more on the details of the changes made\\n* Builds will fail if format isn't followed which guarentees consistency\\n### Negative Consequences\\n* Builds will fail if format isn't followed which can disrupt focus\/workflow (mitigated by pre-commit hooks)\\n","MatchedIds":[1408,5255,4226,3321,400],"Predicted":"## Decision\nWe will adopt [Prettier](https:\/\/prettier.io\/) as our standard JS code formatter. This will ensure consistent formatting across all files and team members.\n\n* We will configure Prettier with a common settings file committed to the repository.\n* We will set up Git hooks to automatically run Prettier before committing code.\n* Continuous integration pipelines will be configured to fail if the code is not properly formatted according to Prettier's standards.\n* All team members will configure their IDEs to use Prettier for autoformatting.\n\nThese measures will help in maintaining a unified code style, reducing manual formatting efforts, making PR reviews easier, and avoiding subjective style discussions.","GenTime":"2024-08-19 23:26:30"}
{"File Name":"Data-Platform-Playbook\/002-ingest-google-sheets-data.md","Context":"## Context\\nHackney currently have datasets distributed over a number of different Google Sheets Documents due their data recovery\\nefforts. We need to get this information pulled into the data platform for processing.\\n","Decision":"We will use a python based AWS Glue Job in conjunction with the gspread python library to pull the data onto platform\\n","tokens":41,"id":1016,"text":"## Context\\nHackney currently have datasets distributed over a number of different Google Sheets Documents due their data recovery\\nefforts. We need to get this information pulled into the data platform for processing.\\n\n\n##Decision\nWe will use a python based AWS Glue Job in conjunction with the gspread python library to pull the data onto platform\\n","MatchedIds":[3253,1863,914,1012,443],"Predicted":"## Decision\nWe will:\n- Use the Google Sheets API to programmatically read data from the distributed Google Sheets documents.\n- Write ETL (Extract, Transform, Load) jobs using Apache Airflow to orchestrate the data extraction from Google Sheets and load it into our data platform.\n- Transform and clean the data using Apache Spark jobs within the data platform to ensure consistency and compatibility.\n- Store the processed data in a central PostgreSQL database within the data platform to facilitate easy access and analysis.\n- Schedule regular sync operations to update the data platform with any changes from the source Google Sheets documents to maintain data freshness.\n- Create monitoring and alerting mechanisms to notify the team of any issues encountered during the data extraction and loading process.","GenTime":"2024-08-19 23:26:35"}
{"File Name":"runner\/0274-step-outcome-and-conclusion.md","Context":"## Context\\nThis ADR proposes adding `steps.<id>.outcome` and `steps.<id>.conclusion` to the steps context.\\nThis allows downstream a step to run based on whether a previous step succeeded or failed.\\nReminder, currently the steps contains `steps.<id>.outputs`.\\n","Decision":"For steps that have completed, populate `steps.<id>.outcome` and `steps.<id>.conclusion` with one of the following values:\\n- `success`\\n- `failure`\\n- `cancelled`\\n- `skipped`\\nWhen a continue-on-error step fails, the outcome will be `failure` even though the final conclusion is `success`.\\n### Example\\n```yaml\\nsteps:\\n- id: experimental\\ncontinue-on-error: true\\nrun: .\/build.sh experimental\\n- if: ${{ steps.experimental.outcome == 'success' }}\\nrun: .\/publish.sh experimental\\n```\\n### Terminology\\nThe runs API uses the term `conclusion`.\\nTherefore we use a different term `outcome` for the value prior to continue-on-error.\\nThe following is a snippet from the runs API response payload:\\n```json\\n\"steps\": [\\n{\\n\"name\": \"Set up job\",\\n\"status\": \"completed\",\\n\"conclusion\": \"success\",\\n\"number\": 1,\\n\"started_at\": \"2020-01-09T11:06:16.000-05:00\",\\n\"completed_at\": \"2020-01-09T11:06:18.000-05:00\"\\n},\\n```\\n","tokens":63,"id":3738,"text":"## Context\\nThis ADR proposes adding `steps.<id>.outcome` and `steps.<id>.conclusion` to the steps context.\\nThis allows downstream a step to run based on whether a previous step succeeded or failed.\\nReminder, currently the steps contains `steps.<id>.outputs`.\\n\n\n##Decision\nFor steps that have completed, populate `steps.<id>.outcome` and `steps.<id>.conclusion` with one of the following values:\\n- `success`\\n- `failure`\\n- `cancelled`\\n- `skipped`\\nWhen a continue-on-error step fails, the outcome will be `failure` even though the final conclusion is `success`.\\n### Example\\n```yaml\\nsteps:\\n- id: experimental\\ncontinue-on-error: true\\nrun: .\/build.sh experimental\\n- if: ${{ steps.experimental.outcome == 'success' }}\\nrun: .\/publish.sh experimental\\n```\\n### Terminology\\nThe runs API uses the term `conclusion`.\\nTherefore we use a different term `outcome` for the value prior to continue-on-error.\\nThe following is a snippet from the runs API response payload:\\n```json\\n\"steps\": [\\n{\\n\"name\": \"Set up job\",\\n\"status\": \"completed\",\\n\"conclusion\": \"success\",\\n\"number\": 1,\\n\"started_at\": \"2020-01-09T11:06:16.000-05:00\",\\n\"completed_at\": \"2020-01-09T11:06:18.000-05:00\"\\n},\\n```\\n","MatchedIds":[4919,4547,4910,4918,4544],"Predicted":"## Decision\nWe've decided to add `steps.<id>.outcome` and `steps.<id>.conclusion` to the steps context. \n\nThis enhancement will facilitate conditional execution of downstream steps based on the success or failure of previous steps. With `steps.<id>.outcome`, we can easily determine if a step was successful or encountered an error, and `steps.<id>.conclusion` can provide additional context or metrics about the step's execution.\n\n### Status\nApproved\n\n### Consequences\n- **Positive**: \n    1. Enables more complex workflows where subsequent steps can adapt based on the results of previous steps.\n    2. Reduces the need for workaround code to handle step dependencies, enhancing readability and maintainability of the workflow definitions.\n    3. Provides a more granular level of control and introspection into the execution context.\n\n- **Negative**:\n    1. Requires updates to existing steps definitions to handle and leverage the new `outcome` and `conclusion` fields.\n    2. Additional testing and validation needed to ensure backward compatibility.\n    3. Potential increase in the complexity of debugging by introducing more states to keep track of.\n\n### More reading\nRefer to [ADR-0023 Conditional Step Execution](..\/assets\/0023-conditional-step-execution) for detailed implementation and examples.\n\n### Updates\nFuture updates will cover:\n1. Potential integration with logging and monitoring tools to enhance visibility.\n2. Best practices and patterns for utilizing `outcome` and `conclusion` in complex workflows.\n3. Community feedback and ongoing refinements based on practical usage.","GenTime":"2024-08-19 23:26:42"}
{"File Name":"editions\/07-\u2705-ophan-analytics.md","Context":"## Context\\nWe need a way to send analytics data to Ophan (internal analytics analysis tool). This includes at its MVP level, screen views and events. This needs to work across iOS and Android.\\n","Decision":"```sequence\\nOphan Analytics Library->Multiplatform Ophan: Thrift interface\\nMultiplatform Ophan->Native React Native Layer: Native iOS and Android modules\\nNative React Native Layer->Typescript Layer: Type definitions to mimic modules\\nTypescript Layer->App Code: Consume! \ud83c\udf54\\nApp Code-->Ophan Analytics Library: Sends events\\n```\\n### Key\\n**Ophan Analytics Library** - This is the current library that provides an interface to log events in Ophan.\\n**Multplatform Ophran** - Written in Kotlin, this uses the Ophan Analytics Library via its Thrift interface and exposes a subset of methods required by the apps.\\n**Native React Native Layer** - Takes the module created by Multiplatform Ophan and makes it \"React Nativey\" to expose methods to the JS.\\n**Typescript Layer** - Takes the React Native JS methods exposed and provides type definitions for them. This is what the App will use and it means we can capture errors at build time.\\n**App Code** - Consumes the typescript library where its needed.\\n","tokens":42,"id":669,"text":"## Context\\nWe need a way to send analytics data to Ophan (internal analytics analysis tool). This includes at its MVP level, screen views and events. This needs to work across iOS and Android.\\n\n\n##Decision\n```sequence\\nOphan Analytics Library->Multiplatform Ophan: Thrift interface\\nMultiplatform Ophan->Native React Native Layer: Native iOS and Android modules\\nNative React Native Layer->Typescript Layer: Type definitions to mimic modules\\nTypescript Layer->App Code: Consume! \ud83c\udf54\\nApp Code-->Ophan Analytics Library: Sends events\\n```\\n### Key\\n**Ophan Analytics Library** - This is the current library that provides an interface to log events in Ophan.\\n**Multplatform Ophran** - Written in Kotlin, this uses the Ophan Analytics Library via its Thrift interface and exposes a subset of methods required by the apps.\\n**Native React Native Layer** - Takes the module created by Multiplatform Ophan and makes it \"React Nativey\" to expose methods to the JS.\\n**Typescript Layer** - Takes the React Native JS methods exposed and provides type definitions for them. This is what the App will use and it means we can capture errors at build time.\\n**App Code** - Consumes the typescript library where its needed.\\n","MatchedIds":[3469,2403,873,4852,1863],"Predicted":"## Decision\nWe will implement a unified analytics service using Firebase Analytics, as it supports both iOS and Android platforms efficiently. Firebase Analytics will capture screen views and events, and we will build an adapter to forward this data to Ophan. This approach allows us to leverage Firebase's robust mobile analytics capabilities while maintaining the requirement to send data to Ophan for internal analysis.","GenTime":"2024-08-19 23:26:45"}
{"File Name":"libelektra\/internal_cache.md","Context":"## Problem\\nWhen doing kdbGet() possible more keys are returned which might be\\nconfusing. When doing a second kdbGet() with a new keyset\\nno keys might be returned, because it is up-to-date.\\nWhen doing kdbSet() a deep duplication is needed.\\nIdea: keep a duplicated keyset internally. Return (non-deep?)\\nduplications exactly of the parts as requested.\\n","Decision":"Not yet decided.\\n- [Global Validation](global_validation.md)\\n","tokens":90,"id":1289,"text":"## Problem\\nWhen doing kdbGet() possible more keys are returned which might be\\nconfusing. When doing a second kdbGet() with a new keyset\\nno keys might be returned, because it is up-to-date.\\nWhen doing kdbSet() a deep duplication is needed.\\nIdea: keep a duplicated keyset internally. Return (non-deep?)\\nduplications exactly of the parts as requested.\\n\n\n##Decision\nNot yet decided.\\n- [Global Validation](global_validation.md)\\n","MatchedIds":[1271,1309,1310,1292,1318],"Predicted":"## Decision\n- Keep an internal duplicated KeySet within the kdb handle to track the state of the keys.\n- When performing `kdbGet()`, return a shallow copy of the requested parts of the KeySet to the caller to avoid confusion.\n- Ensure the internal KeySet is updated to reflect the state after the `kdbGet()` operation.\n- When performing `kdbSet()`, perform a deep duplication to ensure consistency and integrity of the KeySet being saved.\n- Update the documentation to clearly explain the behavior of `kdbGet()` and `kdbSet()` in relation to shallow and deep copies.\n- Create tests to verify that the shallow copies returned by `kdbGet()` do not contain unexpected keys and that `kdbSet()` operations correctly reflect changes using deep duplication.","GenTime":"2024-08-19 23:26:50"}
{"File Name":"git-en-boite\/0009-prefer-dugite-over-raw-git-commands-in-tests.md","Context":"## Context\\nThe build failed because the version of git used in CircleCI was not what we expected.\\n","Decision":"Use `GitProcess.exec` even in tests that need to lean on the git process.\\n","tokens":22,"id":3504,"text":"## Context\\nThe build failed because the version of git used in CircleCI was not what we expected.\\n\n\n##Decision\nUse `GitProcess.exec` even in tests that need to lean on the git process.\\n","MatchedIds":[3328,4225,3497,351,1739],"Predicted":"## Decision\nWe will explicitly specify the required git version in the CircleCI configuration file to ensure consistency across builds.","GenTime":"2024-08-19 23:26:52"}
{"File Name":"beis-report-official-development-assistance\/0006-use-auth0-for-authentication.md","Context":"## Context\\nWe need to allow a number of users to sign in to the service in order to use it.\\nIn order to implement this quickly, we'll use Auth0 to manage this.\\nAs Auth0's authentication uses OAuth2, it should be straightforward to migrate\\nto another service, if BEIS have a preference for something else.\\nAuth0 provides views for authentication that we can use in our user journeys.\\nThere are two versions of these views 'Classic' and 'New'.\\nClassic uses JavaScript and is not progressively enhanced. New uses language\\nthat results in a poor user experience.\\n","Decision":"We will use the free tier and 'Classic' views of Auth0 for the private beta.\\n","tokens":127,"id":2405,"text":"## Context\\nWe need to allow a number of users to sign in to the service in order to use it.\\nIn order to implement this quickly, we'll use Auth0 to manage this.\\nAs Auth0's authentication uses OAuth2, it should be straightforward to migrate\\nto another service, if BEIS have a preference for something else.\\nAuth0 provides views for authentication that we can use in our user journeys.\\nThere are two versions of these views 'Classic' and 'New'.\\nClassic uses JavaScript and is not progressively enhanced. New uses language\\nthat results in a poor user experience.\\n\n\n##Decision\nWe will use the free tier and 'Classic' views of Auth0 for the private beta.\\n","MatchedIds":[5211,2041,1252,2397,1754],"Predicted":"## Decision\nWe will use the 'Classic' views provided by Auth0 for authentication despite their lack of progressive enhancement. This decision is driven by the need to implement the authentication system quickly and the fact that 'New' views significantly detract from the user experience due to poor language choices.\n\n### Justification\n1. **Ease of Implementation**: The 'Classic' views will allow us to quickly integrate authentication into our service, enabling us to meet tight deadlines.\n2. **User Experience**: Although the 'Classic' views use JavaScript and lack progressive enhancement, they still provide a better overall user experience compared to the 'New' views, which use language that our users find confusing or off-putting.\n3. **Future Proofing**: Since Auth0 uses OAuth2, we retain the flexibility to migrate to another service in the future should BEIS decide on a different provider.\n  \n### Positive Consequences\n* **Rapid Deployment**: Quick integration of a functional authentication system.\n* **Better Immediate User Experience**: Users will find the 'Classic' views more intuitive compared to the 'New' views.\n* **Migration Flexibility**: Retains the possibility to switch to another authentication provider if required by BEIS.\n\n### Negative Consequences\n* **Lack of Progressive Enhancement**: Users with JavaScript disabled may face issues with 'Classic' views.\n* **Technical Debt**: Potential need for refactoring if a future provider offers a different approach to authentication views.","GenTime":"2024-08-19 23:26:59"}
{"File Name":"report-a-defect\/0011-use-auth0-for-authentication.md","Context":"## Context\\nAt the moment and for the short term future Hackney do not have a clear candidate for a single sign-on provider. They are reviewing the situation and will decide on a future solution before the end of 2019.\\nWe have so far been using Basic Authentication.\\nAs with other decisions we have attempted to follow the path set by the Repairs Hub which is another Hackney Rails app that is more mature. We asked what they were using and the answer was Microsoft Azure Active Directory using OpenID Connect (OAuth2). We believed we could get the exact same provisioning when the time came for Report a Defect to integrate with a single sign-on provider, however when the time came we learnt that it wasn't supported by Hackney and should be thought of as experimental.\\nAs we had 2 weeks left we discussed with Hackney Architect Keith Gatt the possibility of using Auth0 as a temporary option to enable us to meet the user need of providing agent names within the service, as well as security needs through the use of a logged authentication service that provided one set of credentials per person, rather than a single set for basic auth.\\n","Decision":"Use Auth0 as the single sign-on provider, and remove basic auth from production.\\n","tokens":234,"id":5211,"text":"## Context\\nAt the moment and for the short term future Hackney do not have a clear candidate for a single sign-on provider. They are reviewing the situation and will decide on a future solution before the end of 2019.\\nWe have so far been using Basic Authentication.\\nAs with other decisions we have attempted to follow the path set by the Repairs Hub which is another Hackney Rails app that is more mature. We asked what they were using and the answer was Microsoft Azure Active Directory using OpenID Connect (OAuth2). We believed we could get the exact same provisioning when the time came for Report a Defect to integrate with a single sign-on provider, however when the time came we learnt that it wasn't supported by Hackney and should be thought of as experimental.\\nAs we had 2 weeks left we discussed with Hackney Architect Keith Gatt the possibility of using Auth0 as a temporary option to enable us to meet the user need of providing agent names within the service, as well as security needs through the use of a logged authentication service that provided one set of credentials per person, rather than a single set for basic auth.\\n\n\n##Decision\nUse Auth0 as the single sign-on provider, and remove basic auth from production.\\n","MatchedIds":[2405,2041,1252,4857,2582],"Predicted":"## Decision\nWe will use Auth0 as a temporary authentication solution for the \"Report a Defect\" service.\n\n### Rationale\n* **Time Constraints**: With only 2 weeks left to implement a more secure authentication mechanism, Auth0 provides a quick and easy integration path.\n* **Security Needs**: Auth0 enables us to provide one set of credentials per person, improving security compared to Basic Authentication which uses a single set of credentials.\n* **User Needs**: Auth0 allows us to meet the user requirement of providing agent names within the service, a functionality currently lacking with Basic Authentication.\n* **Flexibility**: Using Auth0 temporarily does not preclude us from integrating with a more permanent SSO solution once Hackney finalizes their decision on an official provider.\n\n### Positive Consequences\n* **Improved Security**: Moving away from Basic Authentication to a managed OAuth2 solution enhances security by ensuring individual credentials per user.\n* **Quick Implementation**: Auth0\u2019s ease of integration allows us to implement the solution within the tight deadline.\n* **User Experience**: The service will be able to better handle user needs such as providing agent names.\n\n### Negative Consequences\n* **Temporary Nature**: This decision incurs technical debt as the implementation will eventually need to be replaced by Hackney\u2019s official SSO solution.\n* **Limited Customization**: Limited customization options within the free tier of Auth0 might not meet all future needs.\n\n### Next Steps\nMonitor Hackney\u2019s review process for a permanent SSO solution and plan for the eventual migration from Auth0 to the selected provider.","GenTime":"2024-08-19 23:27:05"}
{"File Name":"publicservice-registry\/0010-do-not-use-clr-type-names-for-event-types.md","Context":"## Context\\nLooking at the SqlStreamStore code, we noticed a warning against using the CLR type name as the event type in your event store.\\nThe reason behind this is that your message types will outlive your .net CLR types. Moving events along namespaces will break stuff.\\n","Decision":"Use a dictionary\/map between your message types and the CLR type you want to deserialize your message into.\\n","tokens":57,"id":3129,"text":"## Context\\nLooking at the SqlStreamStore code, we noticed a warning against using the CLR type name as the event type in your event store.\\nThe reason behind this is that your message types will outlive your .net CLR types. Moving events along namespaces will break stuff.\\n\n\n##Decision\nUse a dictionary\/map between your message types and the CLR type you want to deserialize your message into.\\n","MatchedIds":[3123,84,1606,2585,2316],"Predicted":"## Decision\nWe will use explicit event type identifiers that are decoupled from the CLR type names. These identifiers will be strings that describe the event type in a stable and human-readable format, such as `\"UserRegistered\"` or `\"OrderShipped\"`. This approach ensures that the event type information is independent of the .NET namespaces and type names, thereby preventing issues that could arise from refactoring or reorganizing code.\n\nFurthermore, we will maintain a central registry or documentation of these event type identifiers to ensure consistency and avoid potential conflicts or duplications. This practice will safeguard the longevity and robustness of our event-driven architecture.","GenTime":"2024-08-19 23:27:09"}
{"File Name":"paas-team-manual\/ADR016-end-to-end-encryption.md","Context":"Context\\n=======\\nIn order to ensure the confidentiality of private tenant data processed on the platform we need to ensure that requests and responses for traffic between the user and application instances are encrypted so that it is not possible for a network eavesdropper to access private tenant data.\\nThere are 3 main network sections between the user and the application:\\n* User to ELB\\n* ELB to router\\n* Router to cells\\nDecision\\n========\\n* The traffic between the user and the ELB is encrypted by using an TLS listener on the ELB. A certificate issued by a certificate authority is set on the ELB and access to the private key is very restricted.\\n* The ELB connects to the router VM via TLS. The router VM must, in consequence, serve TLS.\\n* The router to application instances traffic is plain HTTP because the Cloud Foundry doesn't support TLS between gorouter and the application instances and the application instances may not talk TLS. We've decided to use IPSec on router and cell so the traffic will be encrypted transparently.\\nStatus\\n======\\nAccepted\\nConsequences\\n============\\nThe traffic is encrypted end-to-end between the user and the applications.\\n","Decision":"========\\n* The traffic between the user and the ELB is encrypted by using an TLS listener on the ELB. A certificate issued by a certificate authority is set on the ELB and access to the private key is very restricted.\\n* The ELB connects to the router VM via TLS. The router VM must, in consequence, serve TLS.\\n* The router to application instances traffic is plain HTTP because the Cloud Foundry doesn't support TLS between gorouter and the application instances and the application instances may not talk TLS. We've decided to use IPSec on router and cell so the traffic will be encrypted transparently.\\nStatus\\n======\\nAccepted\\nConsequences\\n============\\nThe traffic is encrypted end-to-end between the user and the applications.\\n","tokens":247,"id":216,"text":"Context\\n=======\\nIn order to ensure the confidentiality of private tenant data processed on the platform we need to ensure that requests and responses for traffic between the user and application instances are encrypted so that it is not possible for a network eavesdropper to access private tenant data.\\nThere are 3 main network sections between the user and the application:\\n* User to ELB\\n* ELB to router\\n* Router to cells\\nDecision\\n========\\n* The traffic between the user and the ELB is encrypted by using an TLS listener on the ELB. A certificate issued by a certificate authority is set on the ELB and access to the private key is very restricted.\\n* The ELB connects to the router VM via TLS. The router VM must, in consequence, serve TLS.\\n* The router to application instances traffic is plain HTTP because the Cloud Foundry doesn't support TLS between gorouter and the application instances and the application instances may not talk TLS. We've decided to use IPSec on router and cell so the traffic will be encrypted transparently.\\nStatus\\n======\\nAccepted\\nConsequences\\n============\\nThe traffic is encrypted end-to-end between the user and the applications.\\n\n\n##Decision\n========\\n* The traffic between the user and the ELB is encrypted by using an TLS listener on the ELB. A certificate issued by a certificate authority is set on the ELB and access to the private key is very restricted.\\n* The ELB connects to the router VM via TLS. The router VM must, in consequence, serve TLS.\\n* The router to application instances traffic is plain HTTP because the Cloud Foundry doesn't support TLS between gorouter and the application instances and the application instances may not talk TLS. We've decided to use IPSec on router and cell so the traffic will be encrypted transparently.\\nStatus\\n======\\nAccepted\\nConsequences\\n============\\nThe traffic is encrypted end-to-end between the user and the applications.\\n","MatchedIds":[194,3104,3909,578,1721],"Predicted":"## Context\nIn order to ensure the confidentiality of private tenant data processed on the platform, we need to ensure that requests and responses for traffic between the user and application instances are encrypted so that it is not possible for a network eavesdropper to access private tenant data. There are 3 main network sections between the user and the application:\n* User to ELB\n* ELB to router\n* Router to cells\n\n## Decision\n* The traffic between the user and the ELB is encrypted by using a TLS listener on the ELB. A certificate issued by a certificate authority is set on the ELB and access to the private key is very restricted.\n* The ELB connects to the router VM via TLS. The router VM must, in consequence, serve TLS.\n* The router to application instances traffic is plain HTTP because Cloud Foundry doesn't support TLS between gorouter and the application instances and the application instances may not talk TLS. We've decided to use IPSec on the router and cell so the traffic will be encrypted transparently.\n\n## Status\nAccepted\n\n## Consequences\nThe traffic is encrypted end-to-end between the user and the applications.","GenTime":"2024-08-19 23:27:18"}
{"File Name":"datalab\/0011-cluster-authentication-by-ssh-key.md","Context":"## Context\\nWe need to configure access to the servers in the cluster and need to decide between\\npassword authentication and SSH key pair authentication.\\n","Decision":"We have decided that all server log on authentication will use SSH key pair\\nauthentication. The public keys will be provisioned onto the server at server creation\\nfor the default deploy user. A password will be required for sudo.\\n","tokens":29,"id":731,"text":"## Context\\nWe need to configure access to the servers in the cluster and need to decide between\\npassword authentication and SSH key pair authentication.\\n\n\n##Decision\nWe have decided that all server log on authentication will use SSH key pair\\nauthentication. The public keys will be provisioned onto the server at server creation\\nfor the default deploy user. A password will be required for sudo.\\n","MatchedIds":[729,3657,578,1239,745],"Predicted":"## Decision\nWe have decided to use SSH key pair authentication for access to the servers in the cluster. This provides a higher level of security compared to password authentication, as it eliminates the risk of brute force attacks and password theft. Additionally, SSH keys can be easily managed and rotated, ensuring secure and controlled access for our team.","GenTime":"2024-08-19 23:27:22"}
{"File Name":"teaching-vacancies\/0011_use_devcontainers.md","Context":"## Context and Problem Statement\\nSetting up the Teaching Vacancies app for local development is a frustrating process involving many\\nsteps and multiple dependencies driven by manually updated setup documentation. A workflow based on\\n[devcontainers](https:\/\/code.visualstudio.com\/docs\/remote\/create-dev-container) would alleviate\\nmuch of this setup pain, and provide a trivially reproducible environment for local development,\\nbenefitting both developers and non-developers on the team.\\n## Decision Drivers\\n- Complex and time-consuming onboarding and \"re-boarding\" experience of the application\\n- Difficulties experiences by non-developers in getting the app set up locally, and getting it\\nrunning again after major dependency changes (e.g. our recent addition of PostGIS)\\n- Increasing adoption of devcontainers as a de-facto standard in the wider development community\\nincluding [Ruby on Rails](https:\/\/github.com\/rails\/rails\/tree\/main\/.devcontainer)\\n- Possible use of cloud-based development environments such as Github Codespaces in the future to\\nenable users on restricted organisation-managed devices to contribute to the application\\n","Decision":"- Complex and time-consuming onboarding and \"re-boarding\" experience of the application\\n- Difficulties experiences by non-developers in getting the app set up locally, and getting it\\nrunning again after major dependency changes (e.g. our recent addition of PostGIS)\\n- Increasing adoption of devcontainers as a de-facto standard in the wider development community\\nincluding [Ruby on Rails](https:\/\/github.com\/rails\/rails\/tree\/main\/.devcontainer)\\n- Possible use of cloud-based development environments such as Github Codespaces in the future to\\nenable users on restricted organisation-managed devices to contribute to the application\\nAdd devcontainers as an option for now, with a view to iterate on it and improve it to the point\\nwhere we can consider it the \"official\" default way of running Teaching Vacancies (while still\\nallowing other development workflows for developers who prefer different ways of working).\\n### Positive Consequences\\n- Drastically easier onboarding and \"re-boarding\" (e.g. on a new device or after an OS upgrade\\ncausing developer tooling issues)\\n- Dependencies reduced to just Git, Docker, and VS Code\\n- A fully functioning development environment is ready in 10 minutes from scratch, with no user\\ninteraction beyond opening the repository in VS Code and selecting \"Reopen in container\"\\n- Moving entirety of development experience into a container fixes past Docker development workflow\\nissues experienced on the team (where tasks and services where executed from the host instead of\\ninteracting with a shell and an editor from inside the container itself)\\n- Developers and other team members can develop on any host OS (macOS\/Linux\/Windows) but we only\\nneed to support one single consistent environment\\n- Does away with all the Mac vs Linux vs WSL setup steps in our current documentation\\n- Reduces likelihood of \"works on my machine\" development environment issues\\n- \"Leave no trace\" on the host machine and complete isolation from other projects\\n- Removes possibility of \"dependency hell\" when working on multiple projects\\n- Removes need to clutter local environment with applications and dependencies that need to be\\nkept up to date and in sync (e.g. Google Chrome and `chromedriver`)\\n- Removes need for language version managers (`rbenv`, `nvm`)\\n- Provides _executable documentation_ of project setup and dependencies\\n- Removes need for manually updated setup documentation that can become stale\\n- Experienced developers who have a different preferred workflow can get a clear, in-code view\\nof setup steps and dependencies\\n- Good workflow for everyone, but excellent additional integration with Visual Studio Code\\n- Automatic passthrough of SSH and GPG keys\\n- Language extensions run within the container itself, and can be specified in the devcontainer\\nconfiguration file for instant setup of useful extensions for new users\\n- Automatic bootstrapping of personal dotfiles\\n- Ability to easily move to cloud-based workflows in the future\\n- Ability to easily propagate new tools and improved configuration to all developers on the team\\n- Trivial rebuilds to a known good state when performing \"dangerous\" operations in the container\\n### Negative Consequences\\n- Slightly reduced performance on some host OSs (non-Linux) due to Docker being Linux-native\\ntechnology (overhead of containers running in an intermediate VM)\\n- Somewhat mitigated by use of volumes for IO-intensive cache directories\\n- Can be worked around entirely by moving workspace root into the container, and we will continue\\nto investigate before we fully agree on devcontainers as our default workflow\\n- Container layers need occasional pruning on the host as Docker can fill up disk space quickly\\n- Some duplication of Docker configuration between production and development configuration (but\\nthat is to be expected given that use cases are very different)\\n","tokens":224,"id":4370,"text":"## Context and Problem Statement\\nSetting up the Teaching Vacancies app for local development is a frustrating process involving many\\nsteps and multiple dependencies driven by manually updated setup documentation. A workflow based on\\n[devcontainers](https:\/\/code.visualstudio.com\/docs\/remote\/create-dev-container) would alleviate\\nmuch of this setup pain, and provide a trivially reproducible environment for local development,\\nbenefitting both developers and non-developers on the team.\\n## Decision Drivers\\n- Complex and time-consuming onboarding and \"re-boarding\" experience of the application\\n- Difficulties experiences by non-developers in getting the app set up locally, and getting it\\nrunning again after major dependency changes (e.g. our recent addition of PostGIS)\\n- Increasing adoption of devcontainers as a de-facto standard in the wider development community\\nincluding [Ruby on Rails](https:\/\/github.com\/rails\/rails\/tree\/main\/.devcontainer)\\n- Possible use of cloud-based development environments such as Github Codespaces in the future to\\nenable users on restricted organisation-managed devices to contribute to the application\\n\n\n##Decision\n- Complex and time-consuming onboarding and \"re-boarding\" experience of the application\\n- Difficulties experiences by non-developers in getting the app set up locally, and getting it\\nrunning again after major dependency changes (e.g. our recent addition of PostGIS)\\n- Increasing adoption of devcontainers as a de-facto standard in the wider development community\\nincluding [Ruby on Rails](https:\/\/github.com\/rails\/rails\/tree\/main\/.devcontainer)\\n- Possible use of cloud-based development environments such as Github Codespaces in the future to\\nenable users on restricted organisation-managed devices to contribute to the application\\nAdd devcontainers as an option for now, with a view to iterate on it and improve it to the point\\nwhere we can consider it the \"official\" default way of running Teaching Vacancies (while still\\nallowing other development workflows for developers who prefer different ways of working).\\n### Positive Consequences\\n- Drastically easier onboarding and \"re-boarding\" (e.g. on a new device or after an OS upgrade\\ncausing developer tooling issues)\\n- Dependencies reduced to just Git, Docker, and VS Code\\n- A fully functioning development environment is ready in 10 minutes from scratch, with no user\\ninteraction beyond opening the repository in VS Code and selecting \"Reopen in container\"\\n- Moving entirety of development experience into a container fixes past Docker development workflow\\nissues experienced on the team (where tasks and services where executed from the host instead of\\ninteracting with a shell and an editor from inside the container itself)\\n- Developers and other team members can develop on any host OS (macOS\/Linux\/Windows) but we only\\nneed to support one single consistent environment\\n- Does away with all the Mac vs Linux vs WSL setup steps in our current documentation\\n- Reduces likelihood of \"works on my machine\" development environment issues\\n- \"Leave no trace\" on the host machine and complete isolation from other projects\\n- Removes possibility of \"dependency hell\" when working on multiple projects\\n- Removes need to clutter local environment with applications and dependencies that need to be\\nkept up to date and in sync (e.g. Google Chrome and `chromedriver`)\\n- Removes need for language version managers (`rbenv`, `nvm`)\\n- Provides _executable documentation_ of project setup and dependencies\\n- Removes need for manually updated setup documentation that can become stale\\n- Experienced developers who have a different preferred workflow can get a clear, in-code view\\nof setup steps and dependencies\\n- Good workflow for everyone, but excellent additional integration with Visual Studio Code\\n- Automatic passthrough of SSH and GPG keys\\n- Language extensions run within the container itself, and can be specified in the devcontainer\\nconfiguration file for instant setup of useful extensions for new users\\n- Automatic bootstrapping of personal dotfiles\\n- Ability to easily move to cloud-based workflows in the future\\n- Ability to easily propagate new tools and improved configuration to all developers on the team\\n- Trivial rebuilds to a known good state when performing \"dangerous\" operations in the container\\n### Negative Consequences\\n- Slightly reduced performance on some host OSs (non-Linux) due to Docker being Linux-native\\ntechnology (overhead of containers running in an intermediate VM)\\n- Somewhat mitigated by use of volumes for IO-intensive cache directories\\n- Can be worked around entirely by moving workspace root into the container, and we will continue\\nto investigate before we fully agree on devcontainers as our default workflow\\n- Container layers need occasional pruning on the host as Docker can fill up disk space quickly\\n- Some duplication of Docker configuration between production and development configuration (but\\nthat is to be expected given that use cases are very different)\\n","MatchedIds":[4096,1376,3793,2393,3789],"Predicted":"## Decision\nWe will migrate the local development setup for the Teaching Vacancies app to use devcontainers.\n\n### Reasons:\n* **Simplified Onboarding\/Re-boarding**: Devcontainers will drastically reduce the complexity and time required for new developers and non-developers to get the application running locally.\n* **Consistency**: Ensuring a consistent development environment removes the variable of \"it works on my machine\" issues, making debugging and collaboration easier.\n* **Ease of Updates**: Updating dependencies for local environments will move from a manual process to an automated one, significantly reducing the effort required to adapt to changes like the recent addition of PostGIS.\n* **Community Adoption**: The increasing adoption of devcontainers in the wider development community, including by frameworks like Ruby on Rails, provides a robust foundation and community support for any issues we may encounter.\n* **Future-Proofing**: Moving to devcontainers will facilitate potential future shifts to cloud-based development environments like GitHub Codespaces, allowing contributors on restricted devices to participate without local setup concerns.\n\n### Implementation Plan:\n1. **Devcontainer Configuration**: Create a devcontainer configuration file for the Teaching Vacancies app, specifying the necessary development dependencies and tools.\n2. **Documentation Update**: Update the setup documentation to reflect the steps required to use the new devcontainer workflow.\n3. **Team Training**: Offer a brief training session or documentation for the development team and non-developers on how to work with devcontainers.\n4. **Feedback Cycle**: Implement a feedback loop where developers and non-developers can report issues or suggestions related to the new setup, leading to iterative improvements.\n\nThis decision aims to streamline the development process, making it more efficient and accessible for all team members.","GenTime":"2024-08-19 23:27:29"}
{"File Name":"copilot\/0006-do-something-for-one-way-tls.md","Context":"## Context\\nEnable one way TLS between front-end and envoy per host+domain that is\\nspecified via gateway config.\\n","Decision":"#### Gateway Configuration\\nThis is achieved by sending the following config from copilot.\\n```\\napiVersion: networking.istio.io\/v1alpha3\\nkind: Gateway\\nmetadata:\\nname: mygateway\\nspec:\\nselector:\\nistio: ingressgateway # use istio default ingress gateway\\nservers:\\n- port:\\nnumber: 443\\nname: https-httpbin\\nprotocol: HTTPS\\ntls:\\nmode: SIMPLE\\nserverCertificate: \/etc\/istio\/ingressgateway-certs\/tls.crt\\nprivateKey: \/etc\/istio\/ingressgateway-certs\/tls.key\\nhosts:\\n- \"httpbin.example.com\"\\n- port:\\nnumber: 443\\nname: https-bookinfo\\nprotocol: HTTPS\\ntls:\\nmode: SIMPLE\\nserverCertificate: \/etc\/istio\/ingressgateway-bookinfo-certs\/tls.crt\\nprivateKey: \/etc\/istio\/ingressgateway-bookinfo-certs\/tls.key\\nhosts:\\n- \"bookinfo.com\"\\n```\\nIn the config above each cert and key in the array of servers represent a\\nhost+domain and the path to each cert and the key is arbitrarily chosen.\\nCopilot extracts the domain information from the cert chains provided in the bosh spec properties:\\n```\\nfrontend_tls_keypairs:\\nexample:\\n- cert_chain: |\\n-----BEGIN CERTIFICATE-----\\n-----END CERTIFICATE-----\\n-----BEGIN CERTIFICATE-----\\n-----END CERTIFICATE-----\\nprivate_key: |\\n-----BEGIN RSA PRIVATE KEY-----\\n-----END RSA PRIVATE KEY-----\\n- cert_chain: |\\n-----BEGIN CERTIFICATE-----\\n-----END CERTIFICATE-----\\n-----BEGIN CERTIFICATE-----\\n-----END CERTIFICATE-----\\nprivate_key: |\\n-----BEGIN RSA PRIVATE KEY-----\\n-----END RSA PRIVATE KEY-----\\n```\\n#### Cert Storage\\nThe placement of the certs and keys on the envoy VM is done using a separate\\nprocess specific to this purpose. This process will be in charge of knowing\\nwhere the certs are located and placing the certs on the correct paths. It is\\nimportant for the envoy VM and copilot to agree on a path where the cert and the keys\\nare stored, and having a specific process to manage this will reduce duplication\\nand mitigate skew.\\n","tokens":25,"id":50,"text":"## Context\\nEnable one way TLS between front-end and envoy per host+domain that is\\nspecified via gateway config.\\n\n\n##Decision\n#### Gateway Configuration\\nThis is achieved by sending the following config from copilot.\\n```\\napiVersion: networking.istio.io\/v1alpha3\\nkind: Gateway\\nmetadata:\\nname: mygateway\\nspec:\\nselector:\\nistio: ingressgateway # use istio default ingress gateway\\nservers:\\n- port:\\nnumber: 443\\nname: https-httpbin\\nprotocol: HTTPS\\ntls:\\nmode: SIMPLE\\nserverCertificate: \/etc\/istio\/ingressgateway-certs\/tls.crt\\nprivateKey: \/etc\/istio\/ingressgateway-certs\/tls.key\\nhosts:\\n- \"httpbin.example.com\"\\n- port:\\nnumber: 443\\nname: https-bookinfo\\nprotocol: HTTPS\\ntls:\\nmode: SIMPLE\\nserverCertificate: \/etc\/istio\/ingressgateway-bookinfo-certs\/tls.crt\\nprivateKey: \/etc\/istio\/ingressgateway-bookinfo-certs\/tls.key\\nhosts:\\n- \"bookinfo.com\"\\n```\\nIn the config above each cert and key in the array of servers represent a\\nhost+domain and the path to each cert and the key is arbitrarily chosen.\\nCopilot extracts the domain information from the cert chains provided in the bosh spec properties:\\n```\\nfrontend_tls_keypairs:\\nexample:\\n- cert_chain: |\\n-----BEGIN CERTIFICATE-----\\n-----END CERTIFICATE-----\\n-----BEGIN CERTIFICATE-----\\n-----END CERTIFICATE-----\\nprivate_key: |\\n-----BEGIN RSA PRIVATE KEY-----\\n-----END RSA PRIVATE KEY-----\\n- cert_chain: |\\n-----BEGIN CERTIFICATE-----\\n-----END CERTIFICATE-----\\n-----BEGIN CERTIFICATE-----\\n-----END CERTIFICATE-----\\nprivate_key: |\\n-----BEGIN RSA PRIVATE KEY-----\\n-----END RSA PRIVATE KEY-----\\n```\\n#### Cert Storage\\nThe placement of the certs and keys on the envoy VM is done using a separate\\nprocess specific to this purpose. This process will be in charge of knowing\\nwhere the certs are located and placing the certs on the correct paths. It is\\nimportant for the envoy VM and copilot to agree on a path where the cert and the keys\\nare stored, and having a specific process to manage this will reduce duplication\\nand mitigate skew.\\n","MatchedIds":[194,2249,3104,733,1949],"Predicted":"## Decision\n\nWe will enable one-way TLS between the front-end services and Envoy by specifying the necessary host and domain configurations within the gateway configuration. This approach will ensure encrypted traffic between the front-end and Envoy, enhancing security without requiring mutual TLS, thus simplifying the implementation and maintenance of TLS certificates. The necessary certificates will be managed and distributed via a centralized certificate management system to ensure consistency and security compliance across the deployment.","GenTime":"2024-08-19 23:27:33"}
{"File Name":"libelektra\/reference_counting.md","Context":"## Problem\\n- locking is not reset when ref counting again gets 0 (adding to keyset and\\npop again) #2202\\n- C++ API for KeySet and Key has unexpected differences: also use ref counting\\nfor KeySets (also suggested in #1332)\\n","Decision":"- add second counter to Key\\n- One counter is for references, the other one is for locking the keyname. The keyname is thereby protected with a re-entrant lock.\\n- introduce reference counter for KeySets (for external keyset references, e.g. in bindings)\\n- limit number of references to `UINT16_MAX - 1` and use `UINT16_MAX` as an error value\\n- return error on reference count overflow\\n- no error on underflow (decrement when zero), instead stay at zero\\n- use fixed sized types (`uint16_t`) for reference counters\\n- increment\/decrement references before\/after passing instances to plugins\\n","tokens":59,"id":1309,"text":"## Problem\\n- locking is not reset when ref counting again gets 0 (adding to keyset and\\npop again) #2202\\n- C++ API for KeySet and Key has unexpected differences: also use ref counting\\nfor KeySets (also suggested in #1332)\\n\n\n##Decision\n- add second counter to Key\\n- One counter is for references, the other one is for locking the keyname. The keyname is thereby protected with a re-entrant lock.\\n- introduce reference counter for KeySets (for external keyset references, e.g. in bindings)\\n- limit number of references to `UINT16_MAX - 1` and use `UINT16_MAX` as an error value\\n- return error on reference count overflow\\n- no error on underflow (decrement when zero), instead stay at zero\\n- use fixed sized types (`uint16_t`) for reference counters\\n- increment\/decrement references before\/after passing instances to plugins\\n","MatchedIds":[1289,1310,1271,1292,1318],"Predicted":"## Decision\n- Implement resetting of the lock when the reference count returns to 0 to ensure consistent behavior (addressing #2202).\n- Harmonize the C++ API for `KeySet` and `Key` by implementing reference counting for `KeySet` objects as well, to maintain consistency between the two (following the suggestion in #1332).\n- Update the documentation to reflect these changes in both the locking mechanism and reference counting.\n- Add unit tests to ensure that the locking mechanism resets correctly and that reference counting behaves as expected for `KeySet`.\n\nReferences:\n- #2202\n- #1332","GenTime":"2024-08-19 23:27:36"}
{"File Name":"modernisation-platform\/0011-use-vpc-flow-logs-to-gain-insight-into-network-state.md","Context":"## Context\\nA core part of the Modernisation Platform is its network topology and configuration and we naturally want confidence that it exists in its desired state to meet security, compliance and best-practise requirements. We want to use the right products and tools that offer networking monitoring and testing capabilities to provide us with this confidence.\\n","Decision":"[VPC flow flows](https:\/\/docs.aws.amazon.com\/vpc\/latest\/userguide\/flow-logs.html) contain information about data coming into and out of attached network interfaces. As such, flow log data could be collected, aggregated, analysed and visualised in order to provide insights into the traffic flowing (or not) through VPCs. VPC flow logs are already collected in environment accounts and at the platform-level in the core-logging account, within CloudWatch log groups.\\n","tokens":65,"id":3150,"text":"## Context\\nA core part of the Modernisation Platform is its network topology and configuration and we naturally want confidence that it exists in its desired state to meet security, compliance and best-practise requirements. We want to use the right products and tools that offer networking monitoring and testing capabilities to provide us with this confidence.\\n\n\n##Decision\n[VPC flow flows](https:\/\/docs.aws.amazon.com\/vpc\/latest\/userguide\/flow-logs.html) contain information about data coming into and out of attached network interfaces. As such, flow log data could be collected, aggregated, analysed and visualised in order to provide insights into the traffic flowing (or not) through VPCs. VPC flow logs are already collected in environment accounts and at the platform-level in the core-logging account, within CloudWatch log groups.\\n","MatchedIds":[3144,3147,4056,1171,1196],"Predicted":"## Decision\nWe have decided to utilize a combination of AWS CloudWatch and AWS Config for monitoring and validating our network topology and configuration. \n\n### AWS CloudWatch\n- **Monitoring**: Provides real-time monitoring of network metrics such as throughput, latency, and error rates.\n- **Logging**: Enables detailed logging of network traffic and events.\n- **Alerting**: Configurable alerts to notify the team of any anomalies in network behavior.\n\n### AWS Config\n- **Configuration Compliance**: Continuously monitors and records the configurations of our network resources (e.g., VPCs, subnets, security groups), ensuring compliance with our security and best-practice policies.\n- **Automated Remediation**: Supports rules and custom conformance packs that can automatically remediate non-compliant network configurations.\n- **Resource Relationship Tracking**: Helps visualize and track relationships and dependencies between network resources to identify potential configuration issues more effectively.\n\nThis combination provides comprehensive monitoring and configuration validation, ensuring our network topology adheres to security, compliance, and best-practice requirements.","GenTime":"2024-08-19 23:27:41"}
{"File Name":"old-web-frontend\/0002-use-aws-s3-as-host-solution.md","Context":"## Context and Problem Statement\\nWe need to host our web application so clients can access it. The solution must be easy to manage and update.\\n## Decision Drivers\\n* Easy to update\\n* High availability\\n* Easy to configure HTTPS\\n* Observability\\n","Decision":"* Easy to update\\n* High availability\\n* Easy to configure HTTPS\\n* Observability\\nChosen option: \"AWS\", because it's the one we have the most experience. The 3 solutions analyzed are pretty similar in regard to the drivers considered. Given that, we made our decision based on our previous experience.\\n","tokens":53,"id":583,"text":"## Context and Problem Statement\\nWe need to host our web application so clients can access it. The solution must be easy to manage and update.\\n## Decision Drivers\\n* Easy to update\\n* High availability\\n* Easy to configure HTTPS\\n* Observability\\n\n\n##Decision\n* Easy to update\\n* High availability\\n* Easy to configure HTTPS\\n* Observability\\nChosen option: \"AWS\", because it's the one we have the most experience. The 3 solutions analyzed are pretty similar in regard to the drivers considered. Given that, we made our decision based on our previous experience.\\n","MatchedIds":[4096,4642,2516,1239,4358],"Predicted":"## Decision\nChosen option: \"AWS Elastic Beanstalk\", because\n* AWS Elastic Beanstalk manages the deployment, from capacity provisioning, load balancing, and auto-scaling, to application health monitoring.\n* It simplifies the process of rolling out updates, ensuring minimal downtime during updates.\n* It integrates easily with AWS Certificate Manager for HTTPS configuration, making it straightforward to secure the application.\n* It provides built-in monitoring and observability tools through AWS CloudWatch, enabling comprehensive oversight of application metrics and logs.\n\n### Negative Consequences\n* Limited control over underlying infrastructure due to the managed nature of the service.\n* May incur higher costs compared to self-managed solutions at scale.","GenTime":"2024-08-19 23:27:46"}
{"File Name":"js-sdk\/0008-add-kwargs-to-3bot-start.md","Context":"## Context\\nAdding packages with kwargs has some limitations and hence kwargs are needed every time start is called not only once when adding package.\\n","Decision":"Add kwargs passed to the package instance that will be saved locally, and can be retrieved everytime the threebot server restarts and starts the package.\\n","tokens":28,"id":5188,"text":"## Context\\nAdding packages with kwargs has some limitations and hence kwargs are needed every time start is called not only once when adding package.\\n\n\n##Decision\nAdd kwargs passed to the package instance that will be saved locally, and can be retrieved everytime the threebot server restarts and starts the package.\\n","MatchedIds":[3691,241,3203,3237,4983],"Predicted":"## Decision\nWe will refactor the package initialization process to include a dedicated configuration method that accepts kwargs. This method will store the configuration parameters within the package instance. Every time the `start` method is called, it will refer to the stored configuration instead of requiring the kwargs to be passed again. This ensures that the kwargs are set once during configuration and reused consistently, reducing redundancy and potential errors.","GenTime":"2024-08-19 23:27:50"}
{"File Name":"front-end-monorepo\/adr-19.md","Context":"## Context\\nAs part of the lib-classifier, we are developing a component for displaying multi-frame subjects in the classification interface. Initially, we referred to this viewer as the PagesViewer, but changed the name to multiFrame Viewer to reflect more general use cases. In the first iteration, we will focus on multiImage features necessary for transcription projects. As projects with different requirements, such as ecology projects, move to the new classifier, we can add new features to the filmstrip component (see decisions 6 an 7).\\nSee the [InVision document for the Pages Viewer](https:\/\/projects.invisionapp.com\/d\/main#\/console\/12924056\/393421254\/preview) and Issue #1142.\\n","Decision":"1. **When to Render:** There will be an explicit workflow configuration (i.e., `{ subject_viewer: 'multiFrame' }`) for the multiFrame Viewer. However, as a fallback, the multiFrame subject viewer will render if (1) the subject delivered to the client has multiple frames and (2) the workflow is not configured to display all frames at once.\\n2. **Number of Frames:** The multiFrame Viewer will display up to ten frames. All frames after the tenth frame will be ignored.\\n3. **Frame Display**: For every frame that is an image, the filmstrip will render a thumbnail of the image. The fallback for non image frames will be a dot. Later, we will need to think about subjects with mixed media frames. HTML Inputs will be used instead of HTML buttons to display each frame.\\n4. **Selected Image Display**: The selected image will be displayed using the SingleImageViewer.\\n5. **Location of Filmstrip:** The filmstrip will be located to the left of the displayed image. This will help for images that are tall as well as make the interface easier to use for volunteers navigating by keyboard or screen reader (see https:\/\/github.com\/zooniverse\/front-end-monorepo\/issues\/1262).\\n6. **Playing Slideshow**: Although adding a slideshow play button will be necessary for non-transcription projects, at this time, the play button is a necessary enhancement\/future feature.\\n7. **Collapsing Filmstrip**: Adding a button to collapse the image previews into pagination dots is an enhancement\/future feature.\\n","tokens":147,"id":498,"text":"## Context\\nAs part of the lib-classifier, we are developing a component for displaying multi-frame subjects in the classification interface. Initially, we referred to this viewer as the PagesViewer, but changed the name to multiFrame Viewer to reflect more general use cases. In the first iteration, we will focus on multiImage features necessary for transcription projects. As projects with different requirements, such as ecology projects, move to the new classifier, we can add new features to the filmstrip component (see decisions 6 an 7).\\nSee the [InVision document for the Pages Viewer](https:\/\/projects.invisionapp.com\/d\/main#\/console\/12924056\/393421254\/preview) and Issue #1142.\\n\n\n##Decision\n1. **When to Render:** There will be an explicit workflow configuration (i.e., `{ subject_viewer: 'multiFrame' }`) for the multiFrame Viewer. However, as a fallback, the multiFrame subject viewer will render if (1) the subject delivered to the client has multiple frames and (2) the workflow is not configured to display all frames at once.\\n2. **Number of Frames:** The multiFrame Viewer will display up to ten frames. All frames after the tenth frame will be ignored.\\n3. **Frame Display**: For every frame that is an image, the filmstrip will render a thumbnail of the image. The fallback for non image frames will be a dot. Later, we will need to think about subjects with mixed media frames. HTML Inputs will be used instead of HTML buttons to display each frame.\\n4. **Selected Image Display**: The selected image will be displayed using the SingleImageViewer.\\n5. **Location of Filmstrip:** The filmstrip will be located to the left of the displayed image. This will help for images that are tall as well as make the interface easier to use for volunteers navigating by keyboard or screen reader (see https:\/\/github.com\/zooniverse\/front-end-monorepo\/issues\/1262).\\n6. **Playing Slideshow**: Although adding a slideshow play button will be necessary for non-transcription projects, at this time, the play button is a necessary enhancement\/future feature.\\n7. **Collapsing Filmstrip**: Adding a button to collapse the image previews into pagination dots is an enhancement\/future feature.\\n","MatchedIds":[532,528,2859,3815,5149],"Predicted":"## Decision\n\n### Context\nThe main decision drivers were to create a flexible and extensible multiFrame Viewer component that can be tailored to different projects' requirements, focusing initially on features necessary for transcription projects. Later, we will incrementally add features for other types of projects, such as ecology projects.\n\n### Decisions\n1. **Component Naming and General Use Cases**\n    - Initially named the PagesViewer, the component is renamed to multiFrame Viewer to better reflect its general use cases across different projects.\n  \n2. **Initial Features for MultiImage Transcription Projects**\n    - Focus on implementing multiImage features necessary for transcription projects, such as easy navigation between frames, zooming, and panning functionalities.\n  \n3. **Modular Design for Extensibility**\n    - Design the component in a modular way to easily add new features without significant refactoring. This ensures that custom features can be integrated as needed for different project requirements.\n  \n4. **Filmstrip Component Development**\n    - Implement the filmstrip component as an integral part of the multiFrame Viewer to provide a visual representation of all the frames, making it easier for users to navigate between them.\n\n5. **Integration and Testing**\n    - Ensure full integration with the lib-classifier's existing components and perform thorough testing focused on usability and performance, particularly under the conditions typical for transcription projects.\n\n6. **Future Feature Additions for Other Projects**\n    - Develop an incremental roadmap to include new features specific to other project types, such as annotating overlapping images for ecology projects.\n\n### Detailed Decisions\n#### Decision 1: Component Naming and General Use Cases\n- The component will be named `MultiFrameViewer` instead of `PagesViewer` to accurately represent its versatile use cases across various projects.\n\n#### Decision 2: Initial Features for MultiImage Transcription Projects\n- Implement crucial multiImage functionalities required for transcription projects at the outset. These features include:\n  - **Frame Navigation**: Users can navigate easily through the frames.\n  - **Zooming and Panning**: Users can zoom in and out of frames and pan across the frame to view different areas.\n\n#### Decision 3: Modular Design for Extensibility\n- The multiFrame Viewer component will be designed with extensibility in mind. \n- Core functionalities will be implemented as independent modules, allowing for future enhancements and the addition of custom features per project requirements without significant overhauls.\n\n#### Decision 4: Filmstrip Component Development\n- The filmstrip component will be implemented to provide users with an intuitive visual method for navigating between frames.\n- It will display thumbnail previews of each frame, allowing users to click on a thumbnail to view the corresponding frame in detail.\n\n#### Decision 5: Integration and Testing\n- Ensure that the multiFrame Viewer is fully integrated with the existing lib-classifier components.\n- Conduct extensive user testing and performance benchmarks, particularly focusing on the needs of transcription projects to identify and resolve any usability issues early on.\n\n#### Decision 6: Future Feature Additions for Other Projects\n- Establish a roadmap to add new features tailored for different project types:\n  - Start with transcription-specific features as these are the immediate priority.\n  - Add ecology-specific features, such as the ability to annotate overlapping images, as the need arises.\n  \n- Continuously collect feedback from different project teams to identify new requirements and areas for improvement.\n\nBy following these decisions, the multiFrame Viewer component will be versatile and adaptable, fulfilling the requirements of various projects within the lib-classifier while providing a robust starting point for immediate use cases in transcription projects.","GenTime":"2024-08-19 23:28:05"}
{"File Name":"govuk-docker\/0004-use-versioned-database-services.md","Context":"## Context\\nGOV.UK Docker has services defined for various databases, such as PostgreSQL,\\nMongoDB and Elasticsearch. These have their data persisted in\\n[Docker volumes][volume] defined [for each service][defined-volumes].\\nWhen GOV.UK Docker updated from [MongoDB 2.4 to 3.6][mongodb-update] users\\nbegan receiving cryptic errors when trying to run applications that depended\\non MongoDB. This was because they had a volume with data structured for MongoDB\\n2.4 whereas MongoDB was expecting this to be structured for 3.6.\\nWe felt that this upgrade path was more confusing and difficult than it needed\\nbe and wanted to improve it for the future.\\n[volume]: https:\/\/docs.docker.com\/storage\/volumes\/\\n[defined-volumes]: https:\/\/github.com\/alphagov\/govuk-docker\/blob\/cb124a3a1d3353e777d4f777d77f03f93415d415\/docker-compose.yml#L5-L10\\n[mongodb-update]: https:\/\/github.com\/alphagov\/govuk-docker\/pull\/356\\n","Decision":"We decided that we would rename database services, and their respective\\nvolumes, to reflect the software version number. For example the `mongo`\\nservice, and namesake volume, have been renamed to `mongo-3.6`.\\nThe precision of the version number is chosen based on the backwards\\ncompatibility between versions of the software. For example to upgrade from\\nMySQL 5.5 to 5.6 requires an upgrade script and thus would cause problems for\\nGOV.UK Docker users. However, upgrading from 5.5.58 to 5.5.59 does not. For\\nservices that aren't sensitive to minor versions, such as Elasticsearch, we\\nhave specified the service and volume with respective to a major version:\\n`elasticsearch-6`.\\n","tokens":238,"id":1666,"text":"## Context\\nGOV.UK Docker has services defined for various databases, such as PostgreSQL,\\nMongoDB and Elasticsearch. These have their data persisted in\\n[Docker volumes][volume] defined [for each service][defined-volumes].\\nWhen GOV.UK Docker updated from [MongoDB 2.4 to 3.6][mongodb-update] users\\nbegan receiving cryptic errors when trying to run applications that depended\\non MongoDB. This was because they had a volume with data structured for MongoDB\\n2.4 whereas MongoDB was expecting this to be structured for 3.6.\\nWe felt that this upgrade path was more confusing and difficult than it needed\\nbe and wanted to improve it for the future.\\n[volume]: https:\/\/docs.docker.com\/storage\/volumes\/\\n[defined-volumes]: https:\/\/github.com\/alphagov\/govuk-docker\/blob\/cb124a3a1d3353e777d4f777d77f03f93415d415\/docker-compose.yml#L5-L10\\n[mongodb-update]: https:\/\/github.com\/alphagov\/govuk-docker\/pull\/356\\n\n\n##Decision\nWe decided that we would rename database services, and their respective\\nvolumes, to reflect the software version number. For example the `mongo`\\nservice, and namesake volume, have been renamed to `mongo-3.6`.\\nThe precision of the version number is chosen based on the backwards\\ncompatibility between versions of the software. For example to upgrade from\\nMySQL 5.5 to 5.6 requires an upgrade script and thus would cause problems for\\nGOV.UK Docker users. However, upgrading from 5.5.58 to 5.5.59 does not. For\\nservices that aren't sensitive to minor versions, such as Elasticsearch, we\\nhave specified the service and volume with respective to a major version:\\n`elasticsearch-6`.\\n","MatchedIds":[4047,714,1669,4096,2396],"Predicted":"## Decision\n\nTo avoid cryptic errors and simplify the upgrade path for future database updates, we will follow these steps:\n\n1. **Version-Specific Volumes**:\n   - Create version-specific Docker volumes for each database. For example, use `mongodb-2.4-data` and `mongodb-3.6-data` for MongoDB.\n   - Update the `docker-compose.yml` to reference the appropriate volume based on the database version.\n\n2. **Migration Scripts**:\n   - Provide migration scripts that can assist users in migrating data from an old volume to a new version-specific volume. Document these scripts and the necessary steps in the project's README file.\n   - Ensure that these migration scripts are thoroughly tested and their usage is straightforward.\n\n3. **Clear Documentation**:\n   - Document the upgrade process in a dedicated section in the GOV.UK Docker documentation. Outline the steps needed to migrate data for each database when upgrading to a new version.\n   - Include common pitfalls and troubleshooting steps to help users diagnose and resolve issues quickly.\n\n4. **Backward Compatibility**:\n   - During the transition period, support both the old and new versions of the database in the `docker-compose.yml` file. This allows users to switch back if they encounter issues during the migration.\n   - Provide clear instructions on how to revert to the old version safely if needed.\n\n5. **Testing and Validation**:\n   - Before performing any update, ensure that a thorough testing process is in place. This includes automated tests that simulate the migration process and verify the integrity of the data.\n   - Encourage users to back up their data before starting the migration process to prevent data loss in case of unexpected issues.\n\nBy implementing these measures, we aim to make the database upgrade process smoother and more transparent for users, minimizing confusion and potential data issues.","GenTime":"2024-08-19 23:28:14"}
{"File Name":"james\/0004-distributed-tasks-listing.md","Context":"## Context\\nBy switching the task manager to a distributed implementation, we need to be able to `list` all `Task`s running on the cluster.\\n","Decision":"* Read a Cassandra projection to get all `Task`s and their `Status`\\n","tokens":32,"id":2140,"text":"## Context\\nBy switching the task manager to a distributed implementation, we need to be able to `list` all `Task`s running on the cluster.\\n\n\n##Decision\n* Read a Cassandra projection to get all `Task`s and their `Status`\\n","MatchedIds":[2121,2127,3682,4636,4821],"Predicted":"## Decision\nWe will implement a centralized `TaskRegistry` service that maintains a list of all running `Task`s across the cluster. Each `TaskManager` instance on individual nodes will register and update its running `Task`s with the `TaskRegistry`. \n\n* `TaskRegistry` will be made accessible to all nodes in the cluster.\n* Each node's `TaskManager` will periodically sync its tasks with the `TaskRegistry`.\n* The `TaskRegistry` will provide a unified API to query and list all running tasks across the cluster.\n* In case of node failure, the `TaskRegistry` will remove tasks associated with the failed node after a timeout period to maintain accurate listing.\n","GenTime":"2024-08-19 23:28:21"}
{"File Name":"cosmos-sdk\/adr-043-nft-module.md","Context":"## Context\\nNFTs are more than just crypto art, which is very helpful for accruing value to the Cosmos ecosystem. As a result, Cosmos Hub should implement NFT functions and enable a unified mechanism for storing and sending the ownership representative of NFTs as discussed in https:\/\/github.com\/cosmos\/cosmos-sdk\/discussions\/9065.\\nAs discussed in [#9065](https:\/\/github.com\/cosmos\/cosmos-sdk\/discussions\/9065), several potential solutions can be considered:\\n* irismod\/nft and modules\/incubator\/nft\\n* CW721\\n* DID NFTs\\n* interNFT\\nSince functions\/use cases of NFTs are tightly connected with their logic, it is almost impossible to support all the NFTs' use cases in one Cosmos SDK module by defining and implementing different transaction types.\\nConsidering generic usage and compatibility of interchain protocols including IBC and Gravity Bridge, it is preferred to have a generic NFT module design which handles the generic NFTs logic.\\nThis design idea can enable composability that application-specific functions should be managed by other modules on Cosmos Hub or on other Zones by importing the NFT module.\\nThe current design is based on the work done by [IRISnet team](https:\/\/github.com\/irisnet\/irismod\/tree\/master\/modules\/nft) and an older implementation in the [Cosmos repository](https:\/\/github.com\/cosmos\/modules\/tree\/master\/incubator\/nft).\\n","Decision":"We create a `x\/nft` module, which contains the following functionality:\\n* Store NFTs and track their ownership.\\n* Expose `Keeper` interface for composing modules to transfer, mint and burn NFTs.\\n* Expose external `Message` interface for users to transfer ownership of their NFTs.\\n* Query NFTs and their supply information.\\nThe proposed module is a base module for NFT app logic. It's goal it to provide a common layer for storage, basic transfer functionality and IBC. The module should not be used as a standalone.\\nInstead an app should create a specialized module to handle app specific logic (eg: NFT ID construction, royalty), user level minting and burning. Moreover an app specialized module should handle auxiliary data to support the app logic (eg indexes, ORM, business data).\\nAll data carried over IBC must be part of the `NFT` or `Class` type described below. The app specific NFT data should be encoded in `NFT.data` for cross-chain integrity. Other objects related to NFT, which are not important for integrity can be part of the app specific module.\\n### Types\\nWe propose two main types:\\n* `Class` -- describes NFT class. We can think about it as a smart contract address.\\n* `NFT` -- object representing unique, non fungible asset. Each NFT is associated with a Class.\\n#### Class\\nNFT **Class** is comparable to an ERC-721 smart contract (provides description of a smart contract), under which a collection of NFTs can be created and managed.\\n```protobuf\\nmessage Class {\\nstring id          = 1;\\nstring name        = 2;\\nstring symbol      = 3;\\nstring description = 4;\\nstring uri         = 5;\\nstring uri_hash    = 6;\\ngoogle.protobuf.Any data = 7;\\n}\\n```\\n* `id` is used as the primary index for storing the class; _required_\\n* `name` is a descriptive name of the NFT class; _optional_\\n* `symbol` is the symbol usually shown on exchanges for the NFT class; _optional_\\n* `description` is a detailed description of the NFT class; _optional_\\n* `uri` is a URI for the class metadata stored off chain. It should be a JSON file that contains metadata about the NFT class and NFT data schema ([OpenSea example](https:\/\/docs.opensea.io\/docs\/contract-level-metadata)); _optional_\\n* `uri_hash` is a hash of the document pointed by uri; _optional_\\n* `data` is app specific metadata of the class; _optional_\\n#### NFT\\nWe define a general model for `NFT` as follows.\\n```protobuf\\nmessage NFT {\\nstring class_id           = 1;\\nstring id                 = 2;\\nstring uri                = 3;\\nstring uri_hash           = 4;\\ngoogle.protobuf.Any data  = 10;\\n}\\n```\\n* `class_id` is the identifier of the NFT class where the NFT belongs; _required_\\n* `id` is an identifier of the NFT, unique within the scope of its class. It is specified by the creator of the NFT and may be expanded to use DID in the future. `class_id` combined with `id` uniquely identifies an NFT and is used as the primary index for storing the NFT; _required_\\n```text\\n{class_id}\/{id} --> NFT (bytes)\\n```\\n* `uri` is a URI for the NFT metadata stored off chain. Should point to a JSON file that contains metadata about this NFT (Ref: [ERC721 standard and OpenSea extension](https:\/\/docs.opensea.io\/docs\/metadata-standards)); _required_\\n* `uri_hash` is a hash of the document pointed by uri; _optional_\\n* `data` is an app specific data of the NFT. CAN be used by composing modules to specify additional properties of the NFT; _optional_\\nThis ADR doesn't specify values that `data` can take; however, best practices recommend upper-level NFT modules clearly specify their contents.  Although the value of this field doesn't provide the additional context required to manage NFT records, which means that the field can technically be removed from the specification, the field's existence allows basic informational\/UI functionality.\\n### `Keeper` Interface\\n```go\\ntype Keeper interface {\\nNewClass(ctx sdk.Context,class Class)\\nUpdateClass(ctx sdk.Context,class Class)\\nMint(ctx sdk.Context,nft NFT\uff0creceiver sdk.AccAddress)   \/\/ updates totalSupply\\nBatchMint(ctx sdk.Context, tokens []NFT,receiver sdk.AccAddress) error\\nBurn(ctx sdk.Context, classId string, nftId string)    \/\/ updates totalSupply\\nBatchBurn(ctx sdk.Context, classID string, nftIDs []string) error\\nUpdate(ctx sdk.Context, nft NFT)\\nBatchUpdate(ctx sdk.Context, tokens []NFT) error\\nTransfer(ctx sdk.Context, classId string, nftId string, receiver sdk.AccAddress)\\nBatchTransfer(ctx sdk.Context, classID string, nftIDs []string, receiver sdk.AccAddress) error\\nGetClass(ctx sdk.Context, classId string) Class\\nGetClasses(ctx sdk.Context) []Class\\nGetNFT(ctx sdk.Context, classId string, nftId string) NFT\\nGetNFTsOfClassByOwner(ctx sdk.Context, classId string, owner sdk.AccAddress) []NFT\\nGetNFTsOfClass(ctx sdk.Context, classId string) []NFT\\nGetOwner(ctx sdk.Context, classId string, nftId string) sdk.AccAddress\\nGetBalance(ctx sdk.Context, classId string, owner sdk.AccAddress) uint64\\nGetTotalSupply(ctx sdk.Context, classId string) uint64\\n}\\n```\\nOther business logic implementations should be defined in composing modules that import `x\/nft` and use its `Keeper`.\\n### `Msg` Service\\n```protobuf\\nservice Msg {\\nrpc Send(MsgSend)         returns (MsgSendResponse);\\n}\\nmessage MsgSend {\\nstring class_id = 1;\\nstring id       = 2;\\nstring sender   = 3;\\nstring reveiver = 4;\\n}\\nmessage MsgSendResponse {}\\n```\\n`MsgSend` can be used to transfer the ownership of an NFT to another address.\\nThe implementation outline of the server is as follows:\\n```go\\ntype msgServer struct{\\nk Keeper\\n}\\nfunc (m msgServer) Send(ctx context.Context, msg *types.MsgSend) (*types.MsgSendResponse, error) {\\n\/\/ check current ownership\\nassertEqual(msg.Sender, m.k.GetOwner(msg.ClassId, msg.Id))\\n\/\/ transfer ownership\\nm.k.Transfer(msg.ClassId, msg.Id, msg.Receiver)\\nreturn &types.MsgSendResponse{}, nil\\n}\\n```\\nThe query service methods for the `x\/nft` module are:\\n```protobuf\\nservice Query {\\n\/\/ Balance queries the number of NFTs of a given class owned by the owner, same as balanceOf in ERC721\\nrpc Balance(QueryBalanceRequest) returns (QueryBalanceResponse) {\\noption (google.api.http).get = \"\/cosmos\/nft\/v1beta1\/balance\/{owner}\/{class_id}\";\\n}\\n\/\/ Owner queries the owner of the NFT based on its class and id, same as ownerOf in ERC721\\nrpc Owner(QueryOwnerRequest) returns (QueryOwnerResponse) {\\noption (google.api.http).get = \"\/cosmos\/nft\/v1beta1\/owner\/{class_id}\/{id}\";\\n}\\n\/\/ Supply queries the number of NFTs from the given class, same as totalSupply of ERC721.\\nrpc Supply(QuerySupplyRequest) returns (QuerySupplyResponse) {\\noption (google.api.http).get = \"\/cosmos\/nft\/v1beta1\/supply\/{class_id}\";\\n}\\n\/\/ NFTs queries all NFTs of a given class or owner,choose at least one of the two, similar to tokenByIndex in ERC721Enumerable\\nrpc NFTs(QueryNFTsRequest) returns (QueryNFTsResponse) {\\noption (google.api.http).get = \"\/cosmos\/nft\/v1beta1\/nfts\";\\n}\\n\/\/ NFT queries an NFT based on its class and id.\\nrpc NFT(QueryNFTRequest) returns (QueryNFTResponse) {\\noption (google.api.http).get = \"\/cosmos\/nft\/v1beta1\/nfts\/{class_id}\/{id}\";\\n}\\n\/\/ Class queries an NFT class based on its id\\nrpc Class(QueryClassRequest) returns (QueryClassResponse) {\\noption (google.api.http).get = \"\/cosmos\/nft\/v1beta1\/classes\/{class_id}\";\\n}\\n\/\/ Classes queries all NFT classes\\nrpc Classes(QueryClassesRequest) returns (QueryClassesResponse) {\\noption (google.api.http).get = \"\/cosmos\/nft\/v1beta1\/classes\";\\n}\\n}\\n\/\/ QueryBalanceRequest is the request type for the Query\/Balance RPC method\\nmessage QueryBalanceRequest {\\nstring class_id = 1;\\nstring owner    = 2;\\n}\\n\/\/ QueryBalanceResponse is the response type for the Query\/Balance RPC method\\nmessage QueryBalanceResponse {\\nuint64 amount = 1;\\n}\\n\/\/ QueryOwnerRequest is the request type for the Query\/Owner RPC method\\nmessage QueryOwnerRequest {\\nstring class_id = 1;\\nstring id       = 2;\\n}\\n\/\/ QueryOwnerResponse is the response type for the Query\/Owner RPC method\\nmessage QueryOwnerResponse {\\nstring owner = 1;\\n}\\n\/\/ QuerySupplyRequest is the request type for the Query\/Supply RPC method\\nmessage QuerySupplyRequest {\\nstring class_id = 1;\\n}\\n\/\/ QuerySupplyResponse is the response type for the Query\/Supply RPC method\\nmessage QuerySupplyResponse {\\nuint64 amount = 1;\\n}\\n\/\/ QueryNFTstRequest is the request type for the Query\/NFTs RPC method\\nmessage QueryNFTsRequest {\\nstring                                class_id   = 1;\\nstring                                owner      = 2;\\ncosmos.base.query.v1beta1.PageRequest pagination = 3;\\n}\\n\/\/ QueryNFTsResponse is the response type for the Query\/NFTs RPC methods\\nmessage QueryNFTsResponse {\\nrepeated cosmos.nft.v1beta1.NFT        nfts       = 1;\\ncosmos.base.query.v1beta1.PageResponse pagination = 2;\\n}\\n\/\/ QueryNFTRequest is the request type for the Query\/NFT RPC method\\nmessage QueryNFTRequest {\\nstring class_id = 1;\\nstring id       = 2;\\n}\\n\/\/ QueryNFTResponse is the response type for the Query\/NFT RPC method\\nmessage QueryNFTResponse {\\ncosmos.nft.v1beta1.NFT nft = 1;\\n}\\n\/\/ QueryClassRequest is the request type for the Query\/Class RPC method\\nmessage QueryClassRequest {\\nstring class_id = 1;\\n}\\n\/\/ QueryClassResponse is the response type for the Query\/Class RPC method\\nmessage QueryClassResponse {\\ncosmos.nft.v1beta1.Class class = 1;\\n}\\n\/\/ QueryClassesRequest is the request type for the Query\/Classes RPC method\\nmessage QueryClassesRequest {\\n\/\/ pagination defines an optional pagination for the request.\\ncosmos.base.query.v1beta1.PageRequest pagination = 1;\\n}\\n\/\/ QueryClassesResponse is the response type for the Query\/Classes RPC method\\nmessage QueryClassesResponse {\\nrepeated cosmos.nft.v1beta1.Class      classes    = 1;\\ncosmos.base.query.v1beta1.PageResponse pagination = 2;\\n}\\n```\\n### Interoperability\\nInteroperability is all about reusing assets between modules and chains. The former one is achieved by ADR-33: Protobuf client - server communication. At the time of writing ADR-33 is not finalized. The latter is achieved by IBC. Here we will focus on the IBC side.\\nIBC is implemented per module. Here, we aligned that NFTs will be recorded and managed in the x\/nft. This requires creation of a new IBC standard and implementation of it.\\nFor IBC interoperability, NFT custom modules MUST use the NFT object type understood by the IBC client. So, for x\/nft interoperability, custom NFT implementations (example: x\/cryptokitty) should use the canonical x\/nft module and proxy all NFT balance keeping functionality to x\/nft or else re-implement all functionality using the NFT object type understood by the IBC client. In other words: x\/nft becomes the standard NFT registry for all Cosmos NFTs (example: x\/cryptokitty will register a kitty NFT in x\/nft and use x\/nft for book keeping). This was [discussed](https:\/\/github.com\/cosmos\/cosmos-sdk\/discussions\/9065#discussioncomment-873206) in the context of using x\/bank as a general asset balance book. Not using x\/nft will require implementing another module for IBC.\\n","tokens":307,"id":827,"text":"## Context\\nNFTs are more than just crypto art, which is very helpful for accruing value to the Cosmos ecosystem. As a result, Cosmos Hub should implement NFT functions and enable a unified mechanism for storing and sending the ownership representative of NFTs as discussed in https:\/\/github.com\/cosmos\/cosmos-sdk\/discussions\/9065.\\nAs discussed in [#9065](https:\/\/github.com\/cosmos\/cosmos-sdk\/discussions\/9065), several potential solutions can be considered:\\n* irismod\/nft and modules\/incubator\/nft\\n* CW721\\n* DID NFTs\\n* interNFT\\nSince functions\/use cases of NFTs are tightly connected with their logic, it is almost impossible to support all the NFTs' use cases in one Cosmos SDK module by defining and implementing different transaction types.\\nConsidering generic usage and compatibility of interchain protocols including IBC and Gravity Bridge, it is preferred to have a generic NFT module design which handles the generic NFTs logic.\\nThis design idea can enable composability that application-specific functions should be managed by other modules on Cosmos Hub or on other Zones by importing the NFT module.\\nThe current design is based on the work done by [IRISnet team](https:\/\/github.com\/irisnet\/irismod\/tree\/master\/modules\/nft) and an older implementation in the [Cosmos repository](https:\/\/github.com\/cosmos\/modules\/tree\/master\/incubator\/nft).\\n\n\n##Decision\nWe create a `x\/nft` module, which contains the following functionality:\\n* Store NFTs and track their ownership.\\n* Expose `Keeper` interface for composing modules to transfer, mint and burn NFTs.\\n* Expose external `Message` interface for users to transfer ownership of their NFTs.\\n* Query NFTs and their supply information.\\nThe proposed module is a base module for NFT app logic. It's goal it to provide a common layer for storage, basic transfer functionality and IBC. The module should not be used as a standalone.\\nInstead an app should create a specialized module to handle app specific logic (eg: NFT ID construction, royalty), user level minting and burning. Moreover an app specialized module should handle auxiliary data to support the app logic (eg indexes, ORM, business data).\\nAll data carried over IBC must be part of the `NFT` or `Class` type described below. The app specific NFT data should be encoded in `NFT.data` for cross-chain integrity. Other objects related to NFT, which are not important for integrity can be part of the app specific module.\\n### Types\\nWe propose two main types:\\n* `Class` -- describes NFT class. We can think about it as a smart contract address.\\n* `NFT` -- object representing unique, non fungible asset. Each NFT is associated with a Class.\\n#### Class\\nNFT **Class** is comparable to an ERC-721 smart contract (provides description of a smart contract), under which a collection of NFTs can be created and managed.\\n```protobuf\\nmessage Class {\\nstring id          = 1;\\nstring name        = 2;\\nstring symbol      = 3;\\nstring description = 4;\\nstring uri         = 5;\\nstring uri_hash    = 6;\\ngoogle.protobuf.Any data = 7;\\n}\\n```\\n* `id` is used as the primary index for storing the class; _required_\\n* `name` is a descriptive name of the NFT class; _optional_\\n* `symbol` is the symbol usually shown on exchanges for the NFT class; _optional_\\n* `description` is a detailed description of the NFT class; _optional_\\n* `uri` is a URI for the class metadata stored off chain. It should be a JSON file that contains metadata about the NFT class and NFT data schema ([OpenSea example](https:\/\/docs.opensea.io\/docs\/contract-level-metadata)); _optional_\\n* `uri_hash` is a hash of the document pointed by uri; _optional_\\n* `data` is app specific metadata of the class; _optional_\\n#### NFT\\nWe define a general model for `NFT` as follows.\\n```protobuf\\nmessage NFT {\\nstring class_id           = 1;\\nstring id                 = 2;\\nstring uri                = 3;\\nstring uri_hash           = 4;\\ngoogle.protobuf.Any data  = 10;\\n}\\n```\\n* `class_id` is the identifier of the NFT class where the NFT belongs; _required_\\n* `id` is an identifier of the NFT, unique within the scope of its class. It is specified by the creator of the NFT and may be expanded to use DID in the future. `class_id` combined with `id` uniquely identifies an NFT and is used as the primary index for storing the NFT; _required_\\n```text\\n{class_id}\/{id} --> NFT (bytes)\\n```\\n* `uri` is a URI for the NFT metadata stored off chain. Should point to a JSON file that contains metadata about this NFT (Ref: [ERC721 standard and OpenSea extension](https:\/\/docs.opensea.io\/docs\/metadata-standards)); _required_\\n* `uri_hash` is a hash of the document pointed by uri; _optional_\\n* `data` is an app specific data of the NFT. CAN be used by composing modules to specify additional properties of the NFT; _optional_\\nThis ADR doesn't specify values that `data` can take; however, best practices recommend upper-level NFT modules clearly specify their contents.  Although the value of this field doesn't provide the additional context required to manage NFT records, which means that the field can technically be removed from the specification, the field's existence allows basic informational\/UI functionality.\\n### `Keeper` Interface\\n```go\\ntype Keeper interface {\\nNewClass(ctx sdk.Context,class Class)\\nUpdateClass(ctx sdk.Context,class Class)\\nMint(ctx sdk.Context,nft NFT\uff0creceiver sdk.AccAddress)   \/\/ updates totalSupply\\nBatchMint(ctx sdk.Context, tokens []NFT,receiver sdk.AccAddress) error\\nBurn(ctx sdk.Context, classId string, nftId string)    \/\/ updates totalSupply\\nBatchBurn(ctx sdk.Context, classID string, nftIDs []string) error\\nUpdate(ctx sdk.Context, nft NFT)\\nBatchUpdate(ctx sdk.Context, tokens []NFT) error\\nTransfer(ctx sdk.Context, classId string, nftId string, receiver sdk.AccAddress)\\nBatchTransfer(ctx sdk.Context, classID string, nftIDs []string, receiver sdk.AccAddress) error\\nGetClass(ctx sdk.Context, classId string) Class\\nGetClasses(ctx sdk.Context) []Class\\nGetNFT(ctx sdk.Context, classId string, nftId string) NFT\\nGetNFTsOfClassByOwner(ctx sdk.Context, classId string, owner sdk.AccAddress) []NFT\\nGetNFTsOfClass(ctx sdk.Context, classId string) []NFT\\nGetOwner(ctx sdk.Context, classId string, nftId string) sdk.AccAddress\\nGetBalance(ctx sdk.Context, classId string, owner sdk.AccAddress) uint64\\nGetTotalSupply(ctx sdk.Context, classId string) uint64\\n}\\n```\\nOther business logic implementations should be defined in composing modules that import `x\/nft` and use its `Keeper`.\\n### `Msg` Service\\n```protobuf\\nservice Msg {\\nrpc Send(MsgSend)         returns (MsgSendResponse);\\n}\\nmessage MsgSend {\\nstring class_id = 1;\\nstring id       = 2;\\nstring sender   = 3;\\nstring reveiver = 4;\\n}\\nmessage MsgSendResponse {}\\n```\\n`MsgSend` can be used to transfer the ownership of an NFT to another address.\\nThe implementation outline of the server is as follows:\\n```go\\ntype msgServer struct{\\nk Keeper\\n}\\nfunc (m msgServer) Send(ctx context.Context, msg *types.MsgSend) (*types.MsgSendResponse, error) {\\n\/\/ check current ownership\\nassertEqual(msg.Sender, m.k.GetOwner(msg.ClassId, msg.Id))\\n\/\/ transfer ownership\\nm.k.Transfer(msg.ClassId, msg.Id, msg.Receiver)\\nreturn &types.MsgSendResponse{}, nil\\n}\\n```\\nThe query service methods for the `x\/nft` module are:\\n```protobuf\\nservice Query {\\n\/\/ Balance queries the number of NFTs of a given class owned by the owner, same as balanceOf in ERC721\\nrpc Balance(QueryBalanceRequest) returns (QueryBalanceResponse) {\\noption (google.api.http).get = \"\/cosmos\/nft\/v1beta1\/balance\/{owner}\/{class_id}\";\\n}\\n\/\/ Owner queries the owner of the NFT based on its class and id, same as ownerOf in ERC721\\nrpc Owner(QueryOwnerRequest) returns (QueryOwnerResponse) {\\noption (google.api.http).get = \"\/cosmos\/nft\/v1beta1\/owner\/{class_id}\/{id}\";\\n}\\n\/\/ Supply queries the number of NFTs from the given class, same as totalSupply of ERC721.\\nrpc Supply(QuerySupplyRequest) returns (QuerySupplyResponse) {\\noption (google.api.http).get = \"\/cosmos\/nft\/v1beta1\/supply\/{class_id}\";\\n}\\n\/\/ NFTs queries all NFTs of a given class or owner,choose at least one of the two, similar to tokenByIndex in ERC721Enumerable\\nrpc NFTs(QueryNFTsRequest) returns (QueryNFTsResponse) {\\noption (google.api.http).get = \"\/cosmos\/nft\/v1beta1\/nfts\";\\n}\\n\/\/ NFT queries an NFT based on its class and id.\\nrpc NFT(QueryNFTRequest) returns (QueryNFTResponse) {\\noption (google.api.http).get = \"\/cosmos\/nft\/v1beta1\/nfts\/{class_id}\/{id}\";\\n}\\n\/\/ Class queries an NFT class based on its id\\nrpc Class(QueryClassRequest) returns (QueryClassResponse) {\\noption (google.api.http).get = \"\/cosmos\/nft\/v1beta1\/classes\/{class_id}\";\\n}\\n\/\/ Classes queries all NFT classes\\nrpc Classes(QueryClassesRequest) returns (QueryClassesResponse) {\\noption (google.api.http).get = \"\/cosmos\/nft\/v1beta1\/classes\";\\n}\\n}\\n\/\/ QueryBalanceRequest is the request type for the Query\/Balance RPC method\\nmessage QueryBalanceRequest {\\nstring class_id = 1;\\nstring owner    = 2;\\n}\\n\/\/ QueryBalanceResponse is the response type for the Query\/Balance RPC method\\nmessage QueryBalanceResponse {\\nuint64 amount = 1;\\n}\\n\/\/ QueryOwnerRequest is the request type for the Query\/Owner RPC method\\nmessage QueryOwnerRequest {\\nstring class_id = 1;\\nstring id       = 2;\\n}\\n\/\/ QueryOwnerResponse is the response type for the Query\/Owner RPC method\\nmessage QueryOwnerResponse {\\nstring owner = 1;\\n}\\n\/\/ QuerySupplyRequest is the request type for the Query\/Supply RPC method\\nmessage QuerySupplyRequest {\\nstring class_id = 1;\\n}\\n\/\/ QuerySupplyResponse is the response type for the Query\/Supply RPC method\\nmessage QuerySupplyResponse {\\nuint64 amount = 1;\\n}\\n\/\/ QueryNFTstRequest is the request type for the Query\/NFTs RPC method\\nmessage QueryNFTsRequest {\\nstring                                class_id   = 1;\\nstring                                owner      = 2;\\ncosmos.base.query.v1beta1.PageRequest pagination = 3;\\n}\\n\/\/ QueryNFTsResponse is the response type for the Query\/NFTs RPC methods\\nmessage QueryNFTsResponse {\\nrepeated cosmos.nft.v1beta1.NFT        nfts       = 1;\\ncosmos.base.query.v1beta1.PageResponse pagination = 2;\\n}\\n\/\/ QueryNFTRequest is the request type for the Query\/NFT RPC method\\nmessage QueryNFTRequest {\\nstring class_id = 1;\\nstring id       = 2;\\n}\\n\/\/ QueryNFTResponse is the response type for the Query\/NFT RPC method\\nmessage QueryNFTResponse {\\ncosmos.nft.v1beta1.NFT nft = 1;\\n}\\n\/\/ QueryClassRequest is the request type for the Query\/Class RPC method\\nmessage QueryClassRequest {\\nstring class_id = 1;\\n}\\n\/\/ QueryClassResponse is the response type for the Query\/Class RPC method\\nmessage QueryClassResponse {\\ncosmos.nft.v1beta1.Class class = 1;\\n}\\n\/\/ QueryClassesRequest is the request type for the Query\/Classes RPC method\\nmessage QueryClassesRequest {\\n\/\/ pagination defines an optional pagination for the request.\\ncosmos.base.query.v1beta1.PageRequest pagination = 1;\\n}\\n\/\/ QueryClassesResponse is the response type for the Query\/Classes RPC method\\nmessage QueryClassesResponse {\\nrepeated cosmos.nft.v1beta1.Class      classes    = 1;\\ncosmos.base.query.v1beta1.PageResponse pagination = 2;\\n}\\n```\\n### Interoperability\\nInteroperability is all about reusing assets between modules and chains. The former one is achieved by ADR-33: Protobuf client - server communication. At the time of writing ADR-33 is not finalized. The latter is achieved by IBC. Here we will focus on the IBC side.\\nIBC is implemented per module. Here, we aligned that NFTs will be recorded and managed in the x\/nft. This requires creation of a new IBC standard and implementation of it.\\nFor IBC interoperability, NFT custom modules MUST use the NFT object type understood by the IBC client. So, for x\/nft interoperability, custom NFT implementations (example: x\/cryptokitty) should use the canonical x\/nft module and proxy all NFT balance keeping functionality to x\/nft or else re-implement all functionality using the NFT object type understood by the IBC client. In other words: x\/nft becomes the standard NFT registry for all Cosmos NFTs (example: x\/cryptokitty will register a kitty NFT in x\/nft and use x\/nft for book keeping). This was [discussed](https:\/\/github.com\/cosmos\/cosmos-sdk\/discussions\/9065#discussioncomment-873206) in the context of using x\/bank as a general asset balance book. Not using x\/nft will require implementing another module for IBC.\\n","MatchedIds":[816,842,812,21,839],"Predicted":"## Decision\nImplement a unified and generic NFT module in the Cosmos SDK, leveraging existing work from the IRISnet team's `irismod\/nft` module and the older implementation in `modules\/incubator\/nft`. The new module will be designed to handle generic NFT logic, enabling composability with other application-specific modules. This will ensure compatibility with interchain protocols like IBC and Gravity Bridge. Specific decisions are as follows:\n\n1. **Design Principles:**\n   - **Generic NFT Logic:** Create a base module that only handles fundamental NFT operations like minting, burning, transferring, and querying NFTs. This will allow other modules or zones to import this module and build upon it for specific use cases.\n   - **Interoperability:** Ensure the module is compatible with interchain communication protocols such as IBC and Gravity Bridge to facilitate cross-chain NFT transfers and interoperability.\n\n2. **Core Functionality:**\n   - **Minting:** Enable the creation of new NFTs.\n   - **Burning:** Support the deletion of NFTs.\n   - **Transferring:** Allow for the transfer of NFT ownership between accounts.\n   - **Querying:** Implement query capabilities to retrieve NFT details, ownership, and metadata.\n\n3. **Modular Design:**\n   - **Extensibility:** Allow other modules to extend the core NFT functionality without altering the base module. For example, DID NFTs or specific application logic can be implemented as separate modules that import and build on the generic NFT module.\n   - **Standard Interfaces:** Define clear and standard interfaces for interaction with the NFT module to ensure ease of integration and extensibility.\n\n4. **Compatibility:**\n   - **IBC Integration:** Ensure the module supports IBC standards for cross-chain NFT transfers.\n   - **Gravity Bridge Integration:** Enable compatibility with Gravity Bridge for interactions with Ethereum and other ecosystems.\n\n5. **Implementation Example:**\n   ```go\n   package nft\n   \n   import (\n       \"github.com\/cosmos\/cosmos-sdk\/types\"\n       \"github.com\/cosmos\/cosmos-sdk\/x\/nft\/keeper\"\n       \"github.com\/cosmos\/cosmos-sdk\/x\/nft\/types\"\n   )\n   \n   func (app *AppModule) RegisterServices(cfg module.Configurator) {\n       types.RegisterMsgServer(cfg.MsgServer(), keeper.NewMsgServerImpl(app.keeper))\n       types.RegisterQueryServer(cfg.QueryServer(), app.keeper)\n   }\n   \n   \/\/ Minting a new NFT\n   func (k Keeper) MintNFT(ctx sdk.Context, msg *types.MsgMintNFT) (*types.MsgMintNFTResponse, error) {\n       \/\/ Implementation logic here\n   }\n   \n   \/\/ Burning an NFT\n   func (k Keeper) BurnNFT(ctx sdk.Context, msg *types.MsgBurnNFT) (*types.MsgBurnNFTResponse, error) {\n       \/\/ Implementation logic here\n   }\n   \n   \/\/ Transferring an NFT\n   func (k Keeper) TransferNFT(ctx sdk.Context, msg *types.MsgTransferNFT) (*types.MsgTransferNFTResponse, error) {\n       \/\/ Implementation logic here\n   }\n   \n   \/\/ Querying NFT details\n   func (k Keeper) GetNFT(ctx sdk.Context, req *types.QueryGetNFTRequest) (*types.QueryGetNFTResponse, error) {\n       \/\/ Implementation logic here\n   }\n   ```\n\n6. **Governance and Community Input:**\n   - **Community Review:** Facilitate community discussions and reviews during the development process to ensure the module meets the needs of various stakeholders.\n   - **Governance:** Use Cosmos Hub governance proposals to approve significant updates or changes to the NFT module.\n\nBy focusing on a generic and extensible NFT module, we can provide a solid foundation for diverse NFT use cases, promote interoperability, and enhance the value and functionality of the Cosmos ecosystem.","GenTime":"2024-08-19 23:28:35"}
{"File Name":"ReportMI-service-manual\/0018-submission-state-machine.md","Context":"## Context\\nAs outlined in [ADR-0016][adr-0016], the Data Submission Service will use\\n\"submissions\" to describe something that a supplier has submitted to us in order\\nto complete a \"task\".\\nSubmissions will exist in a state machine that outlines what is happening with\\nthem.\\nCurrently, we expect there to be 6 states:\\n* **pending** - a blank submission that is awaiting data\\n* **processing** - a submission where data is being processed\\n(eg a file is being ingested or data is being validated)\\n* **validation_failed** - the submitted data has failed the validation process\\nand needs to be corrected\\n* **validation_passed** - the submitted data has passed the validation process,\\nand the supplier now needs to review the results\\n* **supplier_rejected** - the supplier has reviewed the result of the data\\nprocessing and has rejected it (eg they have realised that their data needs to\\nbe amended)\\n* **supplier_accepted** - the supplier has reviewed the result of the data\\nprocessing and are happy that it is accurate\\nOther states may added in future to cover approval processes, and fixing\\nmistakes in returns.\\n","Decision":"The system will model the 6 states highlighted above.\\nSubmissions containing data will proceed through the states from `pending` to\\n`processing` to either `validation_failed` or `validation_passed`. If the\\nvalidation has passed, the supplier can reject the submission (move to\\n`supplier_rejected`) or accept the submission (move to `supplier_accepted`).\\nA 'no business' submission will proceed straight to `supplier_accepted` once\\nthe supplier has confirmed they wish to make this submission.\\nThis is shown in the following diagram.\\n![Submission state machine diagram](..\/diagrams\/0018-submission-states.jpg)\\n","tokens":250,"id":2039,"text":"## Context\\nAs outlined in [ADR-0016][adr-0016], the Data Submission Service will use\\n\"submissions\" to describe something that a supplier has submitted to us in order\\nto complete a \"task\".\\nSubmissions will exist in a state machine that outlines what is happening with\\nthem.\\nCurrently, we expect there to be 6 states:\\n* **pending** - a blank submission that is awaiting data\\n* **processing** - a submission where data is being processed\\n(eg a file is being ingested or data is being validated)\\n* **validation_failed** - the submitted data has failed the validation process\\nand needs to be corrected\\n* **validation_passed** - the submitted data has passed the validation process,\\nand the supplier now needs to review the results\\n* **supplier_rejected** - the supplier has reviewed the result of the data\\nprocessing and has rejected it (eg they have realised that their data needs to\\nbe amended)\\n* **supplier_accepted** - the supplier has reviewed the result of the data\\nprocessing and are happy that it is accurate\\nOther states may added in future to cover approval processes, and fixing\\nmistakes in returns.\\n\n\n##Decision\nThe system will model the 6 states highlighted above.\\nSubmissions containing data will proceed through the states from `pending` to\\n`processing` to either `validation_failed` or `validation_passed`. If the\\nvalidation has passed, the supplier can reject the submission (move to\\n`supplier_rejected`) or accept the submission (move to `supplier_accepted`).\\nA 'no business' submission will proceed straight to `supplier_accepted` once\\nthe supplier has confirmed they wish to make this submission.\\nThis is shown in the following diagram.\\n![Submission state machine diagram](..\/diagrams\/0018-submission-states.jpg)\\n","MatchedIds":[2045,2050,2062,1216,674],"Predicted":"## Decision\nWe will implement a state machine for the \"submissions\" in the Data Submission Service. The initial state machine will handle the six states described:\n\n1. **pending**: A blank submission that is awaiting data.\n2. **processing**: A submission where data is being processed (e.g., a file is being ingested or data is being validated).\n3. **validation_failed**: The submitted data has failed the validation process and needs to be corrected.\n4. **validation_passed**: The submitted data has passed the validation process, and the supplier now needs to review the results.\n5. **supplier_rejected**: The supplier has reviewed the result of the data processing and has rejected it (e.g., they have realized that their data needs to be amended).\n6. **supplier_accepted**: The supplier has reviewed the result of the data processing and is happy that it is accurate.\n\nFuture enhancements to the state machine may include additional states to cover approval processes and fixing mistakes in returns.\n\nThe state machine will be implemented using a flexible and extensible framework to easily accommodate the addition of new states and transitions as required. This will ensure the system can evolve to handle more complex workflows and business requirements over time.","GenTime":"2024-08-19 23:28:40"}
{"File Name":"operational-data-hub\/0005-build-open-source-solutions.md","Context":"## Context\\nOpen source software is software with source code that anyone can inspect, modify, and enhance.\\nBy design, open source software licenses promote collaboration and sharing because they permit other people to make modifications to source code and incorporate those changes into their own projects. They encourage computer programmers to access, view, and modify open source software whenever they like, as long as they let others do the same when they share their work.\\nOpen source software development furthers several principles, described below.\\nTransparency. Whether we're developing software or solving a business problem, we all have access to the information and materials necessary for doing our best work. And when these materials are accessible, we can build upon each other's ideas and discoveries. We can make more effective decisions and understand how decisions affect us.\\nCollaboration. When we're free to participate, we can enhance each other's work in unanticipated ways. When we can modify what others have shared, we unlock new possibilities. By initiating new projects together, we can solve problems that no one can solve alone. And when we implement open standards, we enable others to contribute in the future.\\nRelease early and often. Rapid prototypes can lead to rapid discoveries. An iterative approach leads to better solutions faster. When you're free to experiment, you can look at problems in new ways and seek answers in new places. You can learn by doing.\\nInclusive meritocracy. Good ideas can come from anywhere, and the best ideas should win. Only by including diverse perspectives in our conversations can we be certain we've identified the best ideas, and decision-makers continually seek those perspectives. We may not operate by consensus, but successful work determines which projects gather support and effort from the community.\\nCommunity. Communities form when different people unite around a common purpose. Shared values guide decision making, and community goals supersede individual interests and agendas.\\n","Decision":"We make our code available as open source.\\n","tokens":382,"id":2759,"text":"## Context\\nOpen source software is software with source code that anyone can inspect, modify, and enhance.\\nBy design, open source software licenses promote collaboration and sharing because they permit other people to make modifications to source code and incorporate those changes into their own projects. They encourage computer programmers to access, view, and modify open source software whenever they like, as long as they let others do the same when they share their work.\\nOpen source software development furthers several principles, described below.\\nTransparency. Whether we're developing software or solving a business problem, we all have access to the information and materials necessary for doing our best work. And when these materials are accessible, we can build upon each other's ideas and discoveries. We can make more effective decisions and understand how decisions affect us.\\nCollaboration. When we're free to participate, we can enhance each other's work in unanticipated ways. When we can modify what others have shared, we unlock new possibilities. By initiating new projects together, we can solve problems that no one can solve alone. And when we implement open standards, we enable others to contribute in the future.\\nRelease early and often. Rapid prototypes can lead to rapid discoveries. An iterative approach leads to better solutions faster. When you're free to experiment, you can look at problems in new ways and seek answers in new places. You can learn by doing.\\nInclusive meritocracy. Good ideas can come from anywhere, and the best ideas should win. Only by including diverse perspectives in our conversations can we be certain we've identified the best ideas, and decision-makers continually seek those perspectives. We may not operate by consensus, but successful work determines which projects gather support and effort from the community.\\nCommunity. Communities form when different people unite around a common purpose. Shared values guide decision making, and community goals supersede individual interests and agendas.\\n\n\n##Decision\nWe make our code available as open source.\\n","MatchedIds":[2768,1416,398,4620,1477],"Predicted":"## Decision\nWe will adopt an open source software development model for our project, leveraging open collaboration platforms like GitHub to foster transparency, collaboration, and community building. The project will be released under an open source license that promotes inclusivity and enables modifications, ensuring that contributions from diverse perspectives are welcome. We will follow an iterative development process, releasing early and often to innovate rapidly and address issues promptly. Community-building efforts will be prioritized, and decision-making will be guided by the principles of inclusive meritocracy, enabling the best ideas to thrive regardless of their origin.","GenTime":"2024-08-19 23:28:46"}
{"File Name":"TANF-app\/008-deployment-flow.md","Context":"## Context\\nOur Cloud.gov organization currently has three Spaces -- `tanf-dev`, `tanf-staging`, and `tanf-prod`. The vendor team currently has access to the tanf-dev space only.\\nSince the recent changes to our [Git workflow](https:\/\/github.com\/HHS\/TANF-app\/blob\/main\/docs\/Technical-Documentation\/Architecture-Decision-Record\/009-git-workflow.md) we believe our current deploy strategy should be updated to more closely match the workflow. Previously, since we had approvals on two different repositories we decided that it made sense to maintain [two separate staging sites](https:\/\/github.com\/HHS\/TANF-app\/blob\/837574415af7c57e182684a75bbcf4d942d3b62a\/docs\/Architecture%20Decision%20Record\/008-deployment-flow.md). We would deploy to one with approval in the raft-tech repository, and another with approval to HHS. Since we now have all approvals made in raft-tech, the deploy after approval serves the same purpose as deploying to the Government staging site would have.\\nAdditionally, as of January 2021, the project has only a single deployment environment in the `tanf-dev` space on Cloud.gov. This poses challenges to the vendor development team. The team works on multiple features or fixes at any one time, but only has a single environment to test deployed code. This is leading to \"crowding\", where multiple in-progress features by different devs all want to be deployed to the same environment for testing.\\nAs of Spring 2022, following [ADR 018](https:\/\/github.com\/HHS\/TANF-app\/blob\/main\/docs\/Technical-Documentation\/Architecture-Decision-Record\/018-versioning-and-releases.md), the project needs more than one deployment environment in the `tanf-staging` space on Cloud.gov to ensure that there is a dedicated environment for release-specific features.\\n","Decision":"Additionally, as of January 2021, the project has only a single deployment environment in the `tanf-dev` space on Cloud.gov. This poses challenges to the vendor development team. The team works on multiple features or fixes at any one time, but only has a single environment to test deployed code. This is leading to \"crowding\", where multiple in-progress features by different devs all want to be deployed to the same environment for testing.\\nAs of Spring 2022, following [ADR 018](https:\/\/github.com\/HHS\/TANF-app\/blob\/main\/docs\/Technical-Documentation\/Architecture-Decision-Record\/018-versioning-and-releases.md), the project needs more than one deployment environment in the `tanf-staging` space on Cloud.gov to ensure that there is a dedicated environment for release-specific features.\\nDeploy Environment | Cloud.gov Space | Cloud.gov Dev Access | Role                                             | Deploys when ...                                  |\\n-------------------|-----------------|----------------------|--------------------------------------------------|---------------------------------------------------|\\nDev                | Tanf-Dev        | Vendor & Gov      | Deploy code submitted for gov review                | Relevant github label assigned as shown below     |\\nDevelop            | Tanf-Staging    | Vendor & Gov      | Deploy code once gov-approved                       | Code merged to `raft-tech\/TANF-app:develop` |\\nStaging            | Tanf-Staging    | Gov               | Deploy code once gov-approved                       | Code merged to `HHS\/TANF-app:main` |\\nProduction         | Tanf-Prod       | Gov               | Deploy code tested in staging & ready for prod      | Code merged to `HHS\/TANF-app:master`                |\\n### Gitflow and Deployments\\nWe will be following the Gitflow process which is an industry standard. You can read more about it [in our ADR](.\/018-versioning-and-releases.md). I will just highlight the parts relevant for our deployment strategy. Release branches will be merged to `HHS\/TANF-app:master` which will deploy to our production sites. Code merged to `raft-tech\/TANF-app:develop` will be deployed to our staging sites.\\n### Dev deployments\\nWithin the dev space, there is no correlation for branch to environment as these feature or bugfix branches will constantly vary:\\n| Dev Site | Frontend URL | Backend URL | Purpose                                          |\\n| -------- | -------- | -------- |--------------------------------------------------|\\n| A11y | https:\/\/tdp-frontend-a11y.app.cloud.gov | https:\/\/tdp-backend-a11y.app.cloud.gov\/admin\/ | Space for accessibility testing                  |\\n| QASP | https:\/\/tdp-frontend-qasp.app.cloud.gov | https:\/\/tdp-backend-qasp.app.cloud.gov\/admin\/ | Space for QASP review                            |\\n| raft | https:\/\/tdp-frontend-raft.app.cloud.gov | https:\/\/tdp-backend-raft.app.cloud.gov\/admin\/ | Space for Raft review                          |\\n","tokens":396,"id":3789,"text":"## Context\\nOur Cloud.gov organization currently has three Spaces -- `tanf-dev`, `tanf-staging`, and `tanf-prod`. The vendor team currently has access to the tanf-dev space only.\\nSince the recent changes to our [Git workflow](https:\/\/github.com\/HHS\/TANF-app\/blob\/main\/docs\/Technical-Documentation\/Architecture-Decision-Record\/009-git-workflow.md) we believe our current deploy strategy should be updated to more closely match the workflow. Previously, since we had approvals on two different repositories we decided that it made sense to maintain [two separate staging sites](https:\/\/github.com\/HHS\/TANF-app\/blob\/837574415af7c57e182684a75bbcf4d942d3b62a\/docs\/Architecture%20Decision%20Record\/008-deployment-flow.md). We would deploy to one with approval in the raft-tech repository, and another with approval to HHS. Since we now have all approvals made in raft-tech, the deploy after approval serves the same purpose as deploying to the Government staging site would have.\\nAdditionally, as of January 2021, the project has only a single deployment environment in the `tanf-dev` space on Cloud.gov. This poses challenges to the vendor development team. The team works on multiple features or fixes at any one time, but only has a single environment to test deployed code. This is leading to \"crowding\", where multiple in-progress features by different devs all want to be deployed to the same environment for testing.\\nAs of Spring 2022, following [ADR 018](https:\/\/github.com\/HHS\/TANF-app\/blob\/main\/docs\/Technical-Documentation\/Architecture-Decision-Record\/018-versioning-and-releases.md), the project needs more than one deployment environment in the `tanf-staging` space on Cloud.gov to ensure that there is a dedicated environment for release-specific features.\\n\n\n##Decision\nAdditionally, as of January 2021, the project has only a single deployment environment in the `tanf-dev` space on Cloud.gov. This poses challenges to the vendor development team. The team works on multiple features or fixes at any one time, but only has a single environment to test deployed code. This is leading to \"crowding\", where multiple in-progress features by different devs all want to be deployed to the same environment for testing.\\nAs of Spring 2022, following [ADR 018](https:\/\/github.com\/HHS\/TANF-app\/blob\/main\/docs\/Technical-Documentation\/Architecture-Decision-Record\/018-versioning-and-releases.md), the project needs more than one deployment environment in the `tanf-staging` space on Cloud.gov to ensure that there is a dedicated environment for release-specific features.\\nDeploy Environment | Cloud.gov Space | Cloud.gov Dev Access | Role                                             | Deploys when ...                                  |\\n-------------------|-----------------|----------------------|--------------------------------------------------|---------------------------------------------------|\\nDev                | Tanf-Dev        | Vendor & Gov      | Deploy code submitted for gov review                | Relevant github label assigned as shown below     |\\nDevelop            | Tanf-Staging    | Vendor & Gov      | Deploy code once gov-approved                       | Code merged to `raft-tech\/TANF-app:develop` |\\nStaging            | Tanf-Staging    | Gov               | Deploy code once gov-approved                       | Code merged to `HHS\/TANF-app:main` |\\nProduction         | Tanf-Prod       | Gov               | Deploy code tested in staging & ready for prod      | Code merged to `HHS\/TANF-app:master`                |\\n### Gitflow and Deployments\\nWe will be following the Gitflow process which is an industry standard. You can read more about it [in our ADR](.\/018-versioning-and-releases.md). I will just highlight the parts relevant for our deployment strategy. Release branches will be merged to `HHS\/TANF-app:master` which will deploy to our production sites. Code merged to `raft-tech\/TANF-app:develop` will be deployed to our staging sites.\\n### Dev deployments\\nWithin the dev space, there is no correlation for branch to environment as these feature or bugfix branches will constantly vary:\\n| Dev Site | Frontend URL | Backend URL | Purpose                                          |\\n| -------- | -------- | -------- |--------------------------------------------------|\\n| A11y | https:\/\/tdp-frontend-a11y.app.cloud.gov | https:\/\/tdp-backend-a11y.app.cloud.gov\/admin\/ | Space for accessibility testing                  |\\n| QASP | https:\/\/tdp-frontend-qasp.app.cloud.gov | https:\/\/tdp-backend-qasp.app.cloud.gov\/admin\/ | Space for QASP review                            |\\n| raft | https:\/\/tdp-frontend-raft.app.cloud.gov | https:\/\/tdp-backend-raft.app.cloud.gov\/admin\/ | Space for Raft review                          |\\n","MatchedIds":[187,3794,3915,856,3793],"Predicted":"## Decision\nWe will reconfigure our deployment strategy to align with the new Git workflow and address the challenges posed by the current single deployment environment. The following changes will be made:\n\n1. **Streamline Staging Environments:** We will consolidate the two separate staging sites into a single `tanf-staging` environment. Since all approvals are now handled in the raft-tech repository, maintaining two separate staging environments is redundant.\n\n2. **Additional Development Environments:** To mitigate \"crowding\" in the `tanf-dev` space, we will create multiple deployment environments within the `tanf-dev` space. This will allow different development teams to test their features or fixes in isolation. For instance, we could have `tanf-dev-feature1`, `tanf-dev-feature2`, etc., where each environment can be used for a specific feature or set of fixes.\n\n3. **Dedicated Release Environment:** In line with ADR 018, we will establish at least one additional deployment environment in the `tanf-staging` space. This dedicated environment (`tanf-staging-release`) will be reserved for release-specific features, ensuring that release testing is not interrupted by other ongoing development work.\n\n4. **Unified Production Environment:** The `tanf-prod` environment will remain the same, serving as the single production deployment target. This environment will only receive code that has passed through the `tanf-staging` environment.\n\n5. **Access Modifications:** Vendor team access will be expanded to include the newly created environments (`tanf-dev-featureX`, `tanf-staging-release`) to facilitate testing and ensure seamless workflow transitions from development through to staging and production.\n\nBy implementing these changes, we will reduce deployment conflicts, streamline our staging process, and ensure that each feature or fix can be tested independently before moving to production.","GenTime":"2024-08-19 23:28:52"}
{"File Name":"re-build-systems\/0002-use-containers-and-multiple-boxes.md","Context":"## Context\\nThe aim of this project is to allow teams to quickly create their own Jenkins\\nplatform, running on AWS.\\nJenkins has a master\/agent architecture, where a single master node can trigger\\njobs running on multiple agents. This decision concerns how we deploy, configure\\nand manage both the master and the agents.\\n","Decision":"We will make both the master and the agents Docker containers, running on\\nseparate managed EC2 instances. We will provision and manage these directly for\\nnow, without using an orchestration service like ECS or Kubernetes.\\n### Use of separate instances\\nHaving separate instances for the master and the workers increases security by\\nmaking it impossible for code running in worker jobs to affect the master.\\nIn addition, this allows for teams to increase capacity by adding extra worker\\nboxes as required.\\n### Use of Docker for master\\nRunning the master in Docker makes it easy to deploy and upgrade. This improves\\nteams' abilities to quickly respond when new Jenkins versions are released in\\nresponse to security vulnerabilities, for example.\\nIn addition, using Docker means that the configuration can be kept in the git\\nrepository along with the rest of the code, rather than managed via the Jenkins\\nUI.\\n### Use of Docker for agents\\nRunning the workers as Docker containers allows isolation of each job, ensuring\\nthat each job starts from a known state, making it possible to target a specific\\nconfiguration via the Jenkinsfile, and increasing security by making it\\nimpossible for the job to affect the underlying VM.\\n### Not using orchestration frameworks\\nAlthough some teams at GDS are experimenting with ECS, the Jenkins service is\\nsimple enough that it is not worth introducing the added complexity for this\\nproject.\\n","tokens":68,"id":319,"text":"## Context\\nThe aim of this project is to allow teams to quickly create their own Jenkins\\nplatform, running on AWS.\\nJenkins has a master\/agent architecture, where a single master node can trigger\\njobs running on multiple agents. This decision concerns how we deploy, configure\\nand manage both the master and the agents.\\n\n\n##Decision\nWe will make both the master and the agents Docker containers, running on\\nseparate managed EC2 instances. We will provision and manage these directly for\\nnow, without using an orchestration service like ECS or Kubernetes.\\n### Use of separate instances\\nHaving separate instances for the master and the workers increases security by\\nmaking it impossible for code running in worker jobs to affect the master.\\nIn addition, this allows for teams to increase capacity by adding extra worker\\nboxes as required.\\n### Use of Docker for master\\nRunning the master in Docker makes it easy to deploy and upgrade. This improves\\nteams' abilities to quickly respond when new Jenkins versions are released in\\nresponse to security vulnerabilities, for example.\\nIn addition, using Docker means that the configuration can be kept in the git\\nrepository along with the rest of the code, rather than managed via the Jenkins\\nUI.\\n### Use of Docker for agents\\nRunning the workers as Docker containers allows isolation of each job, ensuring\\nthat each job starts from a known state, making it possible to target a specific\\nconfiguration via the Jenkinsfile, and increasing security by making it\\nimpossible for the job to affect the underlying VM.\\n### Not using orchestration frameworks\\nAlthough some teams at GDS are experimenting with ECS, the Jenkins service is\\nsimple enough that it is not worth introducing the added complexity for this\\nproject.\\n","MatchedIds":[2471,4358,3482,4847,2997],"Predicted":"## Decision\n\n### Master Node\nWe will deploy the Jenkins Master node on an AWS EC2 instance. The EC2 instance will be configured using an Amazon Machine Image (AMI) that includes Jenkins pre-installed with our desired plugins and configurations.\n\n#### Deployment\n1. **AMI Creation**: We will create a custom AMI that has Jenkins installed with all necessary plugins. This AMI will be based on an Amazon Linux 2 image.\n2. **Security Groups**: Configure security groups to allow HTTP\/HTTPS access to the Jenkins web interface and SSH access for administration purposes.\n3. **IAM Role**: Assign an IAM role to the EC2 instance, granting it necessary permissions to interact with other AWS services such as S3, EC2, and CloudWatch.\n\n### Agent Nodes\nThe Jenkins Agent nodes will also run on AWS EC2 instances. Agents will be automatically provisioned and de-provisioned as needed using the Jenkins EC2 Plugin.\n\n#### Deployment\n1. **AMIs for Agent Nodes**: We will use a custom AMI for agent nodes, similar to the master node AMI, albeit with only the required software for running jobs.\n2. **Dynamic Scaling**: Configure Jenkins with the EC2 Plugin to dynamically scale the number of agent nodes based on job queue length and load.\n3. **Security Groups**: Apply appropriate security group settings to allow communication between the master and agent nodes. Specifically, enable Jenkins agent communication ports.\n4. **IAM Roles**: Ensure that the agent nodes have an IAM role that provides appropriate permissions to execute jobs interacting with AWS services as required by various Jenkins jobs.\n\n### Configuration Management\nAll configurations, including Jenkins job configurations, plugins, security settings, and other administrative settings, will be managed using Jenkins Configuration as Code (JCasC) plugin. This will ensure reproducibility and version control over Jenkins' configuration.\n\n### Backup and Recovery\n1. **Backups**: Use AWS S3 to store backups of Jenkins configuration, job data, and plugins. Implement a regular backup schedule using AWS Backup or Jenkins job.\n2. **Recovery**: Document and automate the process to restore Jenkins from backups in case of failure. This will include restoring data from S3 and reconfiguring the Jenkins Master instance from the latest backup.\n\n### Monitoring and Logging\n1. **Monitoring**: Use CloudWatch to monitor the status and health of both the Jenkins Master and Agent nodes.\n2. **Logging**: Centralize Jenkins logs using AWS CloudWatch Logs for easier troubleshooting and auditing.\n\n### High Availability (Future Consideration)\nIn the initial phase, we will deploy a single Jenkins Master node. However, for future scalability and fault tolerance, we will consider setting up Jenkins in a High Availability (HA) configuration using AWS ECS or Kubernetes (EKS).\n\nBy implementing this decision, teams will be able to quickly set up, manage, and maintain their Jenkins environments on AWS with minimized manual intervention and maximized automation and resilience.","GenTime":"2024-08-19 23:29:00"}
{"File Name":"jfluentvalidation\/0001-primitive-array-constraints.md","Context":"## Context and Problem Statement\\nMy first pass at building out array constraint was to use a generic parameter `A` with `java.lang.reflect.Array` to obtain\\nthe length of the property representing `A`.\\nI was curious what the cost of using `java.lang.reflect.Array` compared to grabbing the `length` property from a known type was.\\nAnything with a name like `reflect*` gives me nightmares about terrible performance.\\nI decided to write a JMH benchmark to determine the performance impact `java.lang.reflect.Array` to assist in determining\\nwhich implementation to use.\\n## Decision Drivers\\n1. We want to keep performance in mind and attempt to be as performant as possible across all constraints.\\n2. Avoid adding Additional classes and limit duplicating logic across constraints when possible.\\n","Decision":"1. We want to keep performance in mind and attempt to be as performant as possible across all constraints.\\n2. Avoid adding Additional classes and limit duplicating logic across constraints when possible.\\nI decided to choose option 1 as I prioritized performance above the overhead to maintain additional classes and having\\nduplicate logic.\\nWhile it might be premature optimization and such a small impact (1 - 2 ns) the benchmark results below still convinced me.\\nI'm sure someone can convince me that the overhead is insignificant or that I simply messed up the bencharmark at which point\\nit should be easier refactor to option 2.\\nI've included a rough [implementation of option 2](#option-2-implementation) just in case.\\n```java\\npackage jfluentvalidation.constraints.array;\\nimport jfluentvalidation.constraints.array.length.ArrayExactLengthConstraint;\\nimport jfluentvalidation.constraints.array.length.BooleanArrayExactLengthConstraint;\\nimport jfluentvalidation.constraints.array.length.BooleanArrayExactLengthConstraintAlternative;\\nimport jfluentvalidation.rules.PropertyRule;\\nimport jfluentvalidation.validators.RuleContext;\\nimport jfluentvalidation.validators.ValidationContext;\\nimport org.openjdk.jmh.annotations.*;\\nimport org.openjdk.jmh.runner.Runner;\\nimport org.openjdk.jmh.runner.options.OptionsBuilder;\\nimport java.util.concurrent.TimeUnit;\\n@BenchmarkMode(Mode.AverageTime)\\n@OutputTimeUnit(TimeUnit.NANOSECONDS)\\n@State(Scope.Benchmark)\\npublic class LengthBenchmark {\\npublic static class Foo {\\nprivate boolean[] bar;\\npublic Foo(boolean[] bar) {\\nthis.bar = bar;\\n}\\n}\\nRuleContext<Foo, boolean[]> ruleContext;\\nBooleanArrayExactLengthConstraintAlternative booleanArrayExactLengthConstraintAlternative;\\nArrayExactLengthConstraint arrayExactLengthConstraint;\\nBooleanArrayExactLengthConstraint booleanArrayExactLengthConstraint;\\n@Setup\\npublic void prepare() {\\nFoo f = new Foo(new boolean[5]);\\nPropertyRule propertyRule = new PropertyRule(foo -> f.bar, \"bar\");\\nruleContext = new RuleContext<>(new ValidationContext(f), propertyRule);\\nbooleanArrayExactLengthConstraintAlternative = new BooleanArrayExactLengthConstraintAlternative(5);\\narrayExactLengthConstraint = new ArrayExactLengthConstraint(5);\\n}\\n@Benchmark\\npublic void booleanArrayExactLengthConstraintAlternative() {\\nbooleanArrayExactLengthConstraintAlternative.isValid(ruleContext);\\n}\\n@Benchmark\\npublic void arrayExactLengthConstraint() {\\narrayExactLengthConstraint.isValid(ruleContext);\\n}\\npublic static void main(String[] args) throws Exception {\\nnew Runner(new OptionsBuilder()\\n.include(LengthBenchmark.class.getSimpleName())\\n.forks(1)\\n.warmupIterations(2)\\n.measurementIterations(5)\\n.build())\\n.run();\\n}\\n}\\n```\\nRun 1\\n| Benchmark                                                     | Mode | Cnt | Score | Error   | Units |\\n|---------------------------------------------------------------|------|-----|-------|---------|-------|\\n| LengthBenchmark.arrayExactLengthConstraint                    | avgt | 25  | 2.504 | \u00b1 0.143 | ns\/op |\\n| LengthBenchmark.booleanArrayExactLengthConstraintAlternative  | avgt | 25  | 2.099 | \u00b1 0.022 | ns\/op |\\nobject cast has a performance impact of roughly ~19%\\nRun 2\\n| Benchmark                                                     | Mode | Cnt | Score | Error   | Units |\\n|---------------------------------------------------------------|------|-----|-------|---------|-------|\\n| LengthBenchmark.arrayExactLengthConstraint                    | avgt | 25  | 2.436 | \u00b1 0.049 | ns\/op |\\n| LengthBenchmark.booleanArrayExactLengthConstraintAlternative  | avgt | 25  | 2.041 | \u00b1 0.013 | ns\/op |\\nobject cast has a performance impact of roughly ~19%\\nRun 3\\n| Benchmark                                                     | Mode  | Cnt | Score     | Error       | Units  |\\n|---------------------------------------------------------------|-------|-----|-----------|-------------|--------|\\n| LengthBenchmark.arrayExactLengthConstraint                    | thrpt | 25  | 0.424     | \u00b1 0.001     | ops\/ns |\\n| LengthBenchmark.booleanArrayExactLengthConstraintAlternative  | thrpt | 25  | 0.626     | \u00b1 0.001     | ops\/ns |\\n| LengthBenchmark.arrayExactLengthConstraint                    | avgt  | 25  | 2.362     | \u00b1 0.022     | ns\/op  |\\n| LengthBenchmark.booleanArrayExactLengthConstraintAlternative  | avgt  | 25  | 1.599     | \u00b1 0.007     | ns\/op  |\\n| LengthBenchmark.arrayExactLengthConstraint                    | ss    | 5   | 48261.200 | \u00b1 4493.186  | ns\/op  |\\n| LengthBenchmark.booleanArrayExactLengthConstraintAlternative  | ss    | 5   | 18690.000 | \u00b1 23530.434 | ns\/op  |\\nA ~20% performance impact on throughput and even larger on average time is a bit too much for me to ignore.\\n### Positive Consequences\\n- Gain roughly a 20% performance instead in average time and throughput.\\n### Negative Consequences\\n- We need 36 classes (8 primitive types + 1 object type and 4 constraint classes for each type) compared to 4 constraint classes\\n(ExactLength, BetweenLength, MinimumLength, MaximumLength) to implement array length constraints for primitive arrays.\\n- Duplicate logic in constraint classes across types.\\n","tokens":164,"id":2591,"text":"## Context and Problem Statement\\nMy first pass at building out array constraint was to use a generic parameter `A` with `java.lang.reflect.Array` to obtain\\nthe length of the property representing `A`.\\nI was curious what the cost of using `java.lang.reflect.Array` compared to grabbing the `length` property from a known type was.\\nAnything with a name like `reflect*` gives me nightmares about terrible performance.\\nI decided to write a JMH benchmark to determine the performance impact `java.lang.reflect.Array` to assist in determining\\nwhich implementation to use.\\n## Decision Drivers\\n1. We want to keep performance in mind and attempt to be as performant as possible across all constraints.\\n2. Avoid adding Additional classes and limit duplicating logic across constraints when possible.\\n\n\n##Decision\n1. We want to keep performance in mind and attempt to be as performant as possible across all constraints.\\n2. Avoid adding Additional classes and limit duplicating logic across constraints when possible.\\nI decided to choose option 1 as I prioritized performance above the overhead to maintain additional classes and having\\nduplicate logic.\\nWhile it might be premature optimization and such a small impact (1 - 2 ns) the benchmark results below still convinced me.\\nI'm sure someone can convince me that the overhead is insignificant or that I simply messed up the bencharmark at which point\\nit should be easier refactor to option 2.\\nI've included a rough [implementation of option 2](#option-2-implementation) just in case.\\n```java\\npackage jfluentvalidation.constraints.array;\\nimport jfluentvalidation.constraints.array.length.ArrayExactLengthConstraint;\\nimport jfluentvalidation.constraints.array.length.BooleanArrayExactLengthConstraint;\\nimport jfluentvalidation.constraints.array.length.BooleanArrayExactLengthConstraintAlternative;\\nimport jfluentvalidation.rules.PropertyRule;\\nimport jfluentvalidation.validators.RuleContext;\\nimport jfluentvalidation.validators.ValidationContext;\\nimport org.openjdk.jmh.annotations.*;\\nimport org.openjdk.jmh.runner.Runner;\\nimport org.openjdk.jmh.runner.options.OptionsBuilder;\\nimport java.util.concurrent.TimeUnit;\\n@BenchmarkMode(Mode.AverageTime)\\n@OutputTimeUnit(TimeUnit.NANOSECONDS)\\n@State(Scope.Benchmark)\\npublic class LengthBenchmark {\\npublic static class Foo {\\nprivate boolean[] bar;\\npublic Foo(boolean[] bar) {\\nthis.bar = bar;\\n}\\n}\\nRuleContext<Foo, boolean[]> ruleContext;\\nBooleanArrayExactLengthConstraintAlternative booleanArrayExactLengthConstraintAlternative;\\nArrayExactLengthConstraint arrayExactLengthConstraint;\\nBooleanArrayExactLengthConstraint booleanArrayExactLengthConstraint;\\n@Setup\\npublic void prepare() {\\nFoo f = new Foo(new boolean[5]);\\nPropertyRule propertyRule = new PropertyRule(foo -> f.bar, \"bar\");\\nruleContext = new RuleContext<>(new ValidationContext(f), propertyRule);\\nbooleanArrayExactLengthConstraintAlternative = new BooleanArrayExactLengthConstraintAlternative(5);\\narrayExactLengthConstraint = new ArrayExactLengthConstraint(5);\\n}\\n@Benchmark\\npublic void booleanArrayExactLengthConstraintAlternative() {\\nbooleanArrayExactLengthConstraintAlternative.isValid(ruleContext);\\n}\\n@Benchmark\\npublic void arrayExactLengthConstraint() {\\narrayExactLengthConstraint.isValid(ruleContext);\\n}\\npublic static void main(String[] args) throws Exception {\\nnew Runner(new OptionsBuilder()\\n.include(LengthBenchmark.class.getSimpleName())\\n.forks(1)\\n.warmupIterations(2)\\n.measurementIterations(5)\\n.build())\\n.run();\\n}\\n}\\n```\\nRun 1\\n| Benchmark                                                     | Mode | Cnt | Score | Error   | Units |\\n|---------------------------------------------------------------|------|-----|-------|---------|-------|\\n| LengthBenchmark.arrayExactLengthConstraint                    | avgt | 25  | 2.504 | \u00b1 0.143 | ns\/op |\\n| LengthBenchmark.booleanArrayExactLengthConstraintAlternative  | avgt | 25  | 2.099 | \u00b1 0.022 | ns\/op |\\nobject cast has a performance impact of roughly ~19%\\nRun 2\\n| Benchmark                                                     | Mode | Cnt | Score | Error   | Units |\\n|---------------------------------------------------------------|------|-----|-------|---------|-------|\\n| LengthBenchmark.arrayExactLengthConstraint                    | avgt | 25  | 2.436 | \u00b1 0.049 | ns\/op |\\n| LengthBenchmark.booleanArrayExactLengthConstraintAlternative  | avgt | 25  | 2.041 | \u00b1 0.013 | ns\/op |\\nobject cast has a performance impact of roughly ~19%\\nRun 3\\n| Benchmark                                                     | Mode  | Cnt | Score     | Error       | Units  |\\n|---------------------------------------------------------------|-------|-----|-----------|-------------|--------|\\n| LengthBenchmark.arrayExactLengthConstraint                    | thrpt | 25  | 0.424     | \u00b1 0.001     | ops\/ns |\\n| LengthBenchmark.booleanArrayExactLengthConstraintAlternative  | thrpt | 25  | 0.626     | \u00b1 0.001     | ops\/ns |\\n| LengthBenchmark.arrayExactLengthConstraint                    | avgt  | 25  | 2.362     | \u00b1 0.022     | ns\/op  |\\n| LengthBenchmark.booleanArrayExactLengthConstraintAlternative  | avgt  | 25  | 1.599     | \u00b1 0.007     | ns\/op  |\\n| LengthBenchmark.arrayExactLengthConstraint                    | ss    | 5   | 48261.200 | \u00b1 4493.186  | ns\/op  |\\n| LengthBenchmark.booleanArrayExactLengthConstraintAlternative  | ss    | 5   | 18690.000 | \u00b1 23530.434 | ns\/op  |\\nA ~20% performance impact on throughput and even larger on average time is a bit too much for me to ignore.\\n### Positive Consequences\\n- Gain roughly a 20% performance instead in average time and throughput.\\n### Negative Consequences\\n- We need 36 classes (8 primitive types + 1 object type and 4 constraint classes for each type) compared to 4 constraint classes\\n(ExactLength, BetweenLength, MinimumLength, MaximumLength) to implement array length constraints for primitive arrays.\\n- Duplicate logic in constraint classes across types.\\n","MatchedIds":[1310,4695,4747,2932,1797],"Predicted":"## Decision\nBased on the performance analysis conducted using the JMH benchmark, we decided to avoid using `java.lang.reflect.Array` for obtaining the length of properties. Instead, we will access the `length` property directly for known types, as it offers better performance.\n\n### Positive Consequences\n1. Improved performance by avoiding the overhead associated with reflection.\n2. Simplicity in the implementation by directly accessing the `length` property, which is more straightforward.\n\n### Negative Consequences\n1. Slight increase in code complexity due to handling different array types explicitly.\n2. More rigid design as access to the `length` property requires knowing the type beforehand.\n\n### Implementation Notes\n1. Where possible, use of explicit type checks and direct access of `length` property will be employed.\n2. Reflection will be avoided unless absolutely necessary to maintain performance.","GenTime":"2024-08-19 23:29:04"}
{"File Name":"simple-server\/001-synchronization.md","Context":"## Context\\nNetwork connectivity on phones in rural areas can be low, and\\npatchy. So, the app needs to work offline, as much as possible. The\\nsync mechanism exists to allow sharing records of patients, blood\\npressures, etc across devices.\\nWe need to accommodate cases where patients, and nurses move across\\nfacilities multiple times during a week.\\n","Decision":"### Mechanism\\n1. Send records from device to server\\nThe device needs to keep track of records that need to be\\nsynced. These can be new records, or records that have one or more\\nfields updated. These records need to be formatted into the payload\\nschemas as defined in the individual APIs below. The API will not\\ndifferentiate between new and updated records; this is handled by the\\nserver.  These payloads then need to be sent in batches, where each\\nbatch is inherently independent of the previous one. However, it is\\nimportant to _send_ all records that need syncing, before fetching\\nrecords from the server.\\n2. Fetch records from server to device\\nWhen fetching records for the first time, the `first_time` query\\nparameter should be set. The server will then send back a number of\\nrecords as defined by the `number_of_records` query parameter. This is\\nessentially the \\\"batch_size\\\". The first response also includes a\\n`latest_record_timestamp`, that needs to be sent with the next\\nrequest, in order to retrieve more records from the server. If there\\nare no more records to be sent from the server, it returns an empty\\nlist.\\n3. Merge records on device\\nAfter receiving records from the server, the device will match the\\nrecords in the local database using the ID field of the entity, and\\nupdate them to the server's version. If the local record in the\\ndatabase is pending a sync, it will not update it. The merging of\\nrecords will be handled only by the server.\\nThe server may send back the same record multiple times, so updating\\nrecords on the device needs to be idempotent.\\n### Levers\\nPhysical resource constraints such as battery life, or network are not\\nunder our control. In order for the app to adapt well to the\\nconstraints, we need to have knobs on:\\n1. Batch size: how many records to send or fetch\\n2. Sync frequency: how often sending or fetching happens\\nPreferably, these levers are modifiable at run-time, per device.\\n","tokens":77,"id":1712,"text":"## Context\\nNetwork connectivity on phones in rural areas can be low, and\\npatchy. So, the app needs to work offline, as much as possible. The\\nsync mechanism exists to allow sharing records of patients, blood\\npressures, etc across devices.\\nWe need to accommodate cases where patients, and nurses move across\\nfacilities multiple times during a week.\\n\n\n##Decision\n### Mechanism\\n1. Send records from device to server\\nThe device needs to keep track of records that need to be\\nsynced. These can be new records, or records that have one or more\\nfields updated. These records need to be formatted into the payload\\nschemas as defined in the individual APIs below. The API will not\\ndifferentiate between new and updated records; this is handled by the\\nserver.  These payloads then need to be sent in batches, where each\\nbatch is inherently independent of the previous one. However, it is\\nimportant to _send_ all records that need syncing, before fetching\\nrecords from the server.\\n2. Fetch records from server to device\\nWhen fetching records for the first time, the `first_time` query\\nparameter should be set. The server will then send back a number of\\nrecords as defined by the `number_of_records` query parameter. This is\\nessentially the \\\"batch_size\\\". The first response also includes a\\n`latest_record_timestamp`, that needs to be sent with the next\\nrequest, in order to retrieve more records from the server. If there\\nare no more records to be sent from the server, it returns an empty\\nlist.\\n3. Merge records on device\\nAfter receiving records from the server, the device will match the\\nrecords in the local database using the ID field of the entity, and\\nupdate them to the server's version. If the local record in the\\ndatabase is pending a sync, it will not update it. The merging of\\nrecords will be handled only by the server.\\nThe server may send back the same record multiple times, so updating\\nrecords on the device needs to be idempotent.\\n### Levers\\nPhysical resource constraints such as battery life, or network are not\\nunder our control. In order for the app to adapt well to the\\nconstraints, we need to have knobs on:\\n1. Batch size: how many records to send or fetch\\n2. Sync frequency: how often sending or fetching happens\\nPreferably, these levers are modifiable at run-time, per device.\\n","MatchedIds":[1708,2824,5257,3337,4859],"Predicted":"## Decision\n- Implement an offline-first architecture where the app primarily operates and stores data offline.\n- Use a local database, such as SQLite, to store all patient records, blood pressures, and other relevant data on the device.\n- Implement an efficient sync mechanism that:\n  - Syncs data with a backend server whenever a stable internet connection is detected.\n  - Supports incremental syncs to minimize data usage and handle cases where large volumes of data need to be transferred.\n  - Handles data conflicts by using timestamps and allowing for manual resolution by the user when necessary.\n- Design periodic background syncs that occur during times of probable connectivity, such as early mornings or late evenings.\n- Allow manual sync initiation by the users to ensure they can sync their data when they visit a facility with better connectivity.\n- Optimize the data compression for sync payload to make the best use of limited connectivity.\n- Ensure that the sync mechanism supports real-time conflict resolution to avoid data duplication and loss.\n- Provide clear, user-friendly notifications and statuses about the sync progress and any issues that may occur.\n- Regularly audit and refine the sync performance based on user feedback and observed behavior to ensure reliability and efficiency.\n\nThis approach ensures that the app is highly functional even in low connectivity areas and accommodates the mobility of patients and nurses.","GenTime":"2024-08-19 23:29:09"}
{"File Name":"adr-demo\/0003-use-cassandra-as-storage-backend.md","Context":"## Context\\nAs the application offers uploads capabilities to our users, we need to be able to store uploaded images to some\\nkind of storage. The storage backend should be able to do the following things:\\n* Store raw images\/gifs\\n* Store thumbnails of images\/gifs\\n* Offer a basic search capability (search by title, search by tag)\\n","Decision":"After a quick comparative study, we decide to go with a Cassandra storage backend. The reasons are the following:\\n* Cassandra is quick and easy to setup\\n* It should answer our needs without much custom configuration, even when scaling up\\n* Cassandra allows us to do simple search operations on the fields we're interested in\\n* The team is already familiar with Cassandra usage\\nHere is a quick picture of our architecture including Cassandra as the storage backend:\\n![](microservices-architecture-with-cassandra.png)\\n**Note**: Many services seem to point to the same Cassandra, however we need to setup multiple tables so we can respect\\nCQRS principles (command and query separation):\\n* One table will be used to store thumbnails\\n* One table will be used to store raw images\\n* Thumbnail and upload services are write services (but to different tables)\\n* Search service and download service are read-only services\\n### Other databases\\nIn our quest to the holy storage backend we also studied MongoDb but it didn't seem to be a good fit for our use case : its Json-like structure does not fill particularly well with what we are trying to achieve, and is not much appropriate for very large storage volumes.\\n","tokens":73,"id":2515,"text":"## Context\\nAs the application offers uploads capabilities to our users, we need to be able to store uploaded images to some\\nkind of storage. The storage backend should be able to do the following things:\\n* Store raw images\/gifs\\n* Store thumbnails of images\/gifs\\n* Offer a basic search capability (search by title, search by tag)\\n\n\n##Decision\nAfter a quick comparative study, we decide to go with a Cassandra storage backend. The reasons are the following:\\n* Cassandra is quick and easy to setup\\n* It should answer our needs without much custom configuration, even when scaling up\\n* Cassandra allows us to do simple search operations on the fields we're interested in\\n* The team is already familiar with Cassandra usage\\nHere is a quick picture of our architecture including Cassandra as the storage backend:\\n![](microservices-architecture-with-cassandra.png)\\n**Note**: Many services seem to point to the same Cassandra, however we need to setup multiple tables so we can respect\\nCQRS principles (command and query separation):\\n* One table will be used to store thumbnails\\n* One table will be used to store raw images\\n* Thumbnail and upload services are write services (but to different tables)\\n* Search service and download service are read-only services\\n### Other databases\\nIn our quest to the holy storage backend we also studied MongoDb but it didn't seem to be a good fit for our use case : its Json-like structure does not fill particularly well with what we are trying to achieve, and is not much appropriate for very large storage volumes.\\n","MatchedIds":[2516,3831,5257,2488,2221],"Predicted":"## Decision\nWe have decided to use Amazon S3 for storing the raw images and thumbnails, combined with Amazon RDS for metadata management and search capability. Here's the breakdown of the components:\n\n1. **Storage of Raw Images and Thumbnails**: \n    - Amazon S3 will be used to store the raw images and their corresponding thumbnails. S3 provides high availability, durability, and scalability.\n    - Each uploaded image will have two objects in S3: one for the raw image and one for the thumbnail. The key for each image will include a unique identifier (e.g., UUID) to handle duplicate filenames.\n\n2. **Metadata Management and Search Capability**:\n    - Amazon RDS (or an equivalent relational database) will be used to store metadata about each image, including its title, tags, and the S3 keys for the raw image and thumbnail.\n    - The metadata will include additional attributes like upload timestamp, file size, and user identifier.\n    - Basic search queries (by title and tag) will be performed on this metadata in the relational database to locate images quickly.\n\nBy separating the storage of images from metadata, we ensure scalability and flexibility in handling the images while efficiently managing and searching the associated metadata.\n\n### Architecture Diagram\n1. **Client Side**:\n    - User uploads an image through the web application.\n    - The client sends the image, title, and tags to the backend.\n\n2. **Server Side**:\n    - The server receives the image and generates a UUID.\n    - The server uploads the raw image and the generated thumbnail to S3, using the UUID to construct unique S3 keys.\n    - The server stores the image's metadata (including S3 keys, title, tags, etc.) in RDS.\n    - The server acknowledges the upload completion to the client.\n\n### Benefits\n- **Scalability**: Both S3 and RDS can scale independently, ensuring that storage and metadata management are both efficient and reliable.\n- **High Availability and Durability**: S3 provides 99.999999999% durability and has easy integration with other AWS services.\n- **Search Performance**: Using RDS for metadata allows for quick and efficient search queries on titles and tags.\n- **Flexibility**: Updating metadata or adding additional search fields can be easily managed within the RDS schema.\n\nThis setup provides an optimal balance between effective storage of the images and efficient metadata management with search capabilities.","GenTime":"2024-08-19 23:29:19"}
{"File Name":"exercise3-group-6\/0003-create-two-observation-comparators.md","Context":"## Context\\nWhale observations have a variety of qualities and may require sorting for research purposes. Two\\ndistinct sorting methods proposed for sorting these observations will require two unique comparison methods.\\n","Decision":"Implement `Comparable<Observation>`, create default field `compareTo` method for `ObservationId` (long) and nested\\ncomparator class for field `sightingTime` (Date) in Observation.\\n","tokens":39,"id":2775,"text":"## Context\\nWhale observations have a variety of qualities and may require sorting for research purposes. Two\\ndistinct sorting methods proposed for sorting these observations will require two unique comparison methods.\\n\n\n##Decision\nImplement `Comparable<Observation>`, create default field `compareTo` method for `ObservationId` (long) and nested\\ncomparator class for field `sightingTime` (Date) in Observation.\\n","MatchedIds":[2772,3815,2270,4493,5118],"Predicted":"## Decision\nImplement two comparison methods in the `Observation` class:\n\n1. **`compareByDate`**: This method will compare `Observation` objects based on their `sightingTime` attribute. It will be used to sort observations chronologically.\n\n    ```java\n    public static int compareByDate(Observation o1, Observation o2) {\n        return o1.getSightingTime().compareTo(o2.getSightingTime());\n    }\n    ```\n\n2. **`compareBySpecies`**: This method will compare `Observation` objects based on their `species` attribute. It will sort observations alphabetically by species name.\n\n    ```java\n    public static int compareBySpecies(Observation o1, Observation o2) {\n        return o1.getSpecies().compareTo(o2.getSpecies());\n    }\n    ```\n\nUse these comparison methods with appropriate sorting functions (e.g., `Collections.sort()`) to sort the observations as required. Define strategies to toggle between these sorting methods based on the research needs.","GenTime":"2024-08-19 23:29:24"}
{"File Name":"hmpps-interventions-ui\/0010-use-stylelint-for-linting-styles.md","Context":"## Context\\nWe want to enforce consistency in our code, and catch as many errors\\nautomatically as we are able to. Linting the code is good practice to achieve\\nthese aims. [Stylelint](https:\/\/stylelint.io\/) is one of the more popular CSS\\nlinters with support for SASS, and is easily configurable for our purposes.\\n","Decision":"We will check SASS syntax using Stylelint.\\nWe will use the recommended configuration for plugins where possible.\\nWe will use Stylelint to automatically fix linting errors in a pre-commit hook.\\n","tokens":75,"id":458,"text":"## Context\\nWe want to enforce consistency in our code, and catch as many errors\\nautomatically as we are able to. Linting the code is good practice to achieve\\nthese aims. [Stylelint](https:\/\/stylelint.io\/) is one of the more popular CSS\\nlinters with support for SASS, and is easily configurable for our purposes.\\n\n\n##Decision\nWe will check SASS syntax using Stylelint.\\nWe will use the recommended configuration for plugins where possible.\\nWe will use Stylelint to automatically fix linting errors in a pre-commit hook.\\n","MatchedIds":[1404,4224,400,1408,5255],"Predicted":"## Decision\nWe will check code style using Stylelint.\nWe will use the recommended configuration for Stylelint where possible and adjust only to meet our project-specific needs.\nWe will integrate Stylelint into our build process to ensure all stylesheets conform to our standards.\nWe will set up Git hooks to automatically run Stylelint before committing.\nWe will include Stylelint in our continuous integration pipeline to catch any linting issues early.","GenTime":"2024-08-19 23:29:39"}
{"File Name":"log4brains\/20210113-distribute-log4brains-as-a-global-npm-package.md","Context":"## Context and Problem Statement\\nLog4brains (`v1.0.0-beta.4`) installation procedure is currently optimized for JS projects and looks like this:\\n- Run `npx init-log4brains`\\n- Which installs locally `@log4brains\/cli` and `@log4brains\/web`\\n- And creates custom entries in `package.json`'s `scripts` section:\\n- `\"log4brains-preview\": \"log4brains-web preview\"`\\n- `\"log4brains-build\": \"log4brains-web build\"`\\n- `\"adr\": \"log4brains adr\"`\\nFor non-JS projects, you have to install manually the packages and the `npx init-log4brains` script does not work.\\nSince Log4brains is intended for all projects, not especially JS ones, we have to make the installation procedure simpler and language-agnostic.\\n## Decision Drivers <!-- optional -->\\n- Simplicity of the installation procedure\\n- Language agnostic\\n- Initialization script works on any kind of project\\n- Faster \"getting started\"\\n","Decision":"- Simplicity of the installation procedure\\n- Language agnostic\\n- Initialization script works on any kind of project\\n- Faster \"getting started\"\\nThe new installation procedure is now language agnostic and will be the following:\\n```bash\\nnpm install -g log4brains\\nlog4brains init\\n```\\nLog4brains will be distributed as a global NPM package named `log4brains`, which provides a global `log4brains` command.\\n- This global package will require the existing `@log4brains\/cli` and `@log4brains\/web` packages\\n- `init-log4brains` will be renamed to `@log4brains\/init` and required as a dependency\\n### Consequences\\nFor a JS project, it is now impossible to pin Log4brains to a specific version.\\nWe may implement a [xojs\/xo](https:\/\/github.com\/xojs\/xo)-like behavior later: \"the CLI will use your local install of XO when available, even when run globally.\"\\n","tokens":224,"id":1065,"text":"## Context and Problem Statement\\nLog4brains (`v1.0.0-beta.4`) installation procedure is currently optimized for JS projects and looks like this:\\n- Run `npx init-log4brains`\\n- Which installs locally `@log4brains\/cli` and `@log4brains\/web`\\n- And creates custom entries in `package.json`'s `scripts` section:\\n- `\"log4brains-preview\": \"log4brains-web preview\"`\\n- `\"log4brains-build\": \"log4brains-web build\"`\\n- `\"adr\": \"log4brains adr\"`\\nFor non-JS projects, you have to install manually the packages and the `npx init-log4brains` script does not work.\\nSince Log4brains is intended for all projects, not especially JS ones, we have to make the installation procedure simpler and language-agnostic.\\n## Decision Drivers <!-- optional -->\\n- Simplicity of the installation procedure\\n- Language agnostic\\n- Initialization script works on any kind of project\\n- Faster \"getting started\"\\n\n\n##Decision\n- Simplicity of the installation procedure\\n- Language agnostic\\n- Initialization script works on any kind of project\\n- Faster \"getting started\"\\nThe new installation procedure is now language agnostic and will be the following:\\n```bash\\nnpm install -g log4brains\\nlog4brains init\\n```\\nLog4brains will be distributed as a global NPM package named `log4brains`, which provides a global `log4brains` command.\\n- This global package will require the existing `@log4brains\/cli` and `@log4brains\/web` packages\\n- `init-log4brains` will be renamed to `@log4brains\/init` and required as a dependency\\n### Consequences\\nFor a JS project, it is now impossible to pin Log4brains to a specific version.\\nWe may implement a [xojs\/xo](https:\/\/github.com\/xojs\/xo)-like behavior later: \"the CLI will use your local install of XO when available, even when run globally.\"\\n","MatchedIds":[1067,1117,3521,4609,3109],"Predicted":"## Decision\n\nChosen option: \"Provide a standalone CLI binary\", because:\n- A standalone binary can be run on any type of project regardless of the language or tooling.\n- Distributing a single executable simplifies the installation procedure to just downloading or running a script to fetch the binary.\n- This approach ensures that `log4brains` can easily be included in CI\/CD pipelines on any platform.\n\n### Implementation\n- Provide pre-built binaries for common operating systems (Windows, Linux, macOS) on the GitHub releases page.\n- Update the documentation to guide users to download the appropriate binary and add it to their PATH.\n- Modify the `init-log4brains` script to work independently of `npx`, enabling it to set up the initial required configuration for non-JS projects.\n\n### Positive Consequences\n- Simplifies the installation procedure for users, leading to a faster \"getting started\" experience.\n- Increases the usability of `log4brains` across a diverse range of projects beyond JavaScript or Node.js.\n- Reduces potential issues with dependency management and version conflicts.\n\n### Negative Consequences\n- Additional effort required to develop the release automation for building and distributing binaries.\n- Initial overhead in ensuring that the standalone CLI is feature-complete and works across all intended platforms consistently.","GenTime":"2024-08-19 23:29:43"}
{"File Name":"markmap\/splitting-and-bundling.md","Context":"## Context\\ncoc-markmap \/ markmap-cli has an incredibly large size in `node_modules`. (~30MB)\\n### Redundant dependencies\\nIn markmap-lib@0.9.x transforming and rendering share the same package, but they have quite different dependencies and are used in quite different scenarios. In other words many of the dependencies are useless most of the time.\\nFor example, transforming usually occurs locally when creating a markmap, while rendering happens in the browser when the markmap is opened. So if we are trying to create a markmap, it's likely that we don't need d3 and its friends. If we are exploring a markmap it's likely that we don't need all those transforming tools like remarkable and katex.\\n### Extraneous bundles\\nSome packages build multiple bundles to support different types of import. For example, `index.js` for `cjs`, and `index.min.js` for minimized `cjs` bundle, and several more. As a result, each of the bundle contains a copy of code, leading to a huge size.\\nAfter bundling into a single package, we get a single copy of the code, and optionally minimized.\\n","Decision":"- Split rendering code from `markmap-lib` into a new package `markmap-view`.\\nBy splitting the packages we can manage dependencies more independently, and get rid of unused ones.\\nIn most cases, `markmap-view` can be used as a prebuilt bundle, even without installation by leveraging CDNs.\\n- Use rollup to bundle code in `markmap-cli`.\\nUnder the hood, `coc-markmap` depends on `markmap-cli`, and `markmap-cli` depends on `markmap-lib`.\\n`markmap-lib` is supposed to be used in applications with bundlers, so it's better to keep simple and externalize the dependencies.\\n`markmap-cli` is a standalone command-line tool. It should be installed without dependency bloat. So it is a good idea to bundle everything in it and get rid of additional dependency packages.\\n`coc-markmap` is a thin layer on top of `markmap-cli` so we can just leave it as is.\\n","tokens":245,"id":5140,"text":"## Context\\ncoc-markmap \/ markmap-cli has an incredibly large size in `node_modules`. (~30MB)\\n### Redundant dependencies\\nIn markmap-lib@0.9.x transforming and rendering share the same package, but they have quite different dependencies and are used in quite different scenarios. In other words many of the dependencies are useless most of the time.\\nFor example, transforming usually occurs locally when creating a markmap, while rendering happens in the browser when the markmap is opened. So if we are trying to create a markmap, it's likely that we don't need d3 and its friends. If we are exploring a markmap it's likely that we don't need all those transforming tools like remarkable and katex.\\n### Extraneous bundles\\nSome packages build multiple bundles to support different types of import. For example, `index.js` for `cjs`, and `index.min.js` for minimized `cjs` bundle, and several more. As a result, each of the bundle contains a copy of code, leading to a huge size.\\nAfter bundling into a single package, we get a single copy of the code, and optionally minimized.\\n\n\n##Decision\n- Split rendering code from `markmap-lib` into a new package `markmap-view`.\\nBy splitting the packages we can manage dependencies more independently, and get rid of unused ones.\\nIn most cases, `markmap-view` can be used as a prebuilt bundle, even without installation by leveraging CDNs.\\n- Use rollup to bundle code in `markmap-cli`.\\nUnder the hood, `coc-markmap` depends on `markmap-cli`, and `markmap-cli` depends on `markmap-lib`.\\n`markmap-lib` is supposed to be used in applications with bundlers, so it's better to keep simple and externalize the dependencies.\\n`markmap-cli` is a standalone command-line tool. It should be installed without dependency bloat. So it is a good idea to bundle everything in it and get rid of additional dependency packages.\\n`coc-markmap` is a thin layer on top of `markmap-cli` so we can just leave it as is.\\n","MatchedIds":[3428,1931,3031,3827,1631],"Predicted":"## Decision\n\nTo address the issue of the large size of `coc-markmap \/ markmap-cli` in `node_modules`, we will take a two-pronged approach:\n\n### 1. Split Transforming and Rendering into Separate Packages\n\nWe will split the transforming and rendering functionalities into separate packages. This approach minimizes the number of redundant dependencies bundled together:\n\n- **Transforming Package:** This will include all dependencies required for creating a markmap, located locally.\n  - Dependencies: `remarkable`, `katex`, etc.\n  \n- **Rendering Package:** This will include the dependencies required for rendering a markmap in the browser.\n  - Dependencies: `d3`, and related visualization libraries.\n\nBy separating these packages, developers can choose which functionalities they need without importing unnecessary dependencies.\n\n### 2. Create a Single Minified Bundle\n\nTo eliminate extraneous bundles and reduce the total package size, we will build a single minified bundle for each package type:\n\n- Only one main entry point file (e.g., `index.js` or `index.min.js`) will be generated, omitting multiple copies for different types of imports.\n- During the bundling process, we will leverage tools like Webpack or Rollup to generate a minimized version with tree-shaking to remove any unused code.\n\nThis change ensures that only a single copy of the code is included per package, significantly reducing the overall size.\n\nBy implementing these decisions, we ensure that `coc-markmap` and `markmap-cli` are both leaner and more efficient, addressing the problem of unused and redundant dependencies while still providing necessary functionalities in a modular way.","GenTime":"2024-08-19 23:29:48"}
{"File Name":"terraform\/Automation.md","Context":"## Context\\nServices that we are in the process of migrating or that have recently competed migration are likely to go through a period of needing more maintenance that a mature service.\\nOne answer to this is to handle all such problems on a case by case basis, fixing as we go in order to try to keep the velocity as high as possible. At the other end of the scale is to drop a significant cost on the project by automating all such processes.\\n","Decision":"We will automate every process possible. Where an automated process may be applicable to more than one server, then we will attempt to write the solution to be DRY.\\n","tokens":93,"id":929,"text":"## Context\\nServices that we are in the process of migrating or that have recently competed migration are likely to go through a period of needing more maintenance that a mature service.\\nOne answer to this is to handle all such problems on a case by case basis, fixing as we go in order to try to keep the velocity as high as possible. At the other end of the scale is to drop a significant cost on the project by automating all such processes.\\n\n\n##Decision\nWe will automate every process possible. Where an automated process may be applicable to more than one server, then we will attempt to write the solution to be DRY.\\n","MatchedIds":[200,2701,621,920,1652],"Predicted":"## Decision\nWe have decided to adopt a hybrid approach to address the increased maintenance needs of services undergoing or recently completing migration:\n\n1. **Case-by-Case Handling for Immediate Issues**: \n   - For critical or urgent issues, we will handle them on a case-by-case basis to ensure prompt resolution and minimal disruption to service.\n   - This approach allows us to maintain high velocity and address specific problems as they arise without significant upfront investment.\n\n2. **Strategic Automation for Recurrent Tasks**:\n   - For tasks that are identified as recurrent and predictable, we will invest in automation solutions to streamline these processes.\n   - This will include setting up CI\/CD pipelines, automated testing, monitoring, and alerting for common issues identified during the migration.\n\n3. **Incremental Improvement**:\n   - Review and assess code and processes regularly to identify candidates for automation.\n   - Prioritize automation efforts based on the frequency and criticality of tasks to ensure a balanced approach between immediate needs and long-term efficiency.\n\n4. **Documentation and Knowledge Sharing**:\n   - Maintain comprehensive documentation of manual processes and procedures followed during the migration period.\n   - Encourage knowledge sharing across teams to ensure consistent handling of issues and to identify opportunities for process improvements.\n\n5. **Periodic Review**:\n   - Conduct periodic reviews to assess the balance between manual handling and automation.\n   - Adjust the strategy based on the evolving needs of the service and the stability of the migrated system.\n\nBy combining immediate, targeted problem resolution with strategic automation of recurrent tasks, we aim to manage the increased maintenance load effectively while moving towards sustainable, long-term solutions.","GenTime":"2024-08-19 23:30:06"}
{"File Name":"operational-data-hub\/0027-a-gcp-project-belongs-to-a-single-domain.md","Context":"## Context\\nThe projects structure of the platform can be used to protect components. By the seperation into projects a modular, loosely coupled design is created. A project belongs to a single [domain](0025-domains-correspond-to-business-departments.md), a domain can consist of multiple projects. The project implements a coherent set of functions within a single domain.\\n","Decision":"The set of functions implemented in one GCP project belongs to a single domain.\\n","tokens":75,"id":2756,"text":"## Context\\nThe projects structure of the platform can be used to protect components. By the seperation into projects a modular, loosely coupled design is created. A project belongs to a single [domain](0025-domains-correspond-to-business-departments.md), a domain can consist of multiple projects. The project implements a coherent set of functions within a single domain.\\n\n\n##Decision\nThe set of functions implemented in one GCP project belongs to a single domain.\\n","MatchedIds":[4889,1792,2626,4896,3982],"Predicted":"## Decision\n\n**We will organize the platform's structure by separating components into distinct projects that are each dedicated to a single domain.**\n\n### Decision Details\n\nBy dividing the platform into multiple projects where each project belongs to a specific domain, we aim to achieve several key benefits:\n1. **Modular and Loosely Coupled Design**: Each project will encapsulate a coherent set of functions within its domain, thereby reducing dependencies between different projects. This leads to a more modular and loosely coupled architecture.\n2. **Component Protection**: Separation into distinct projects will serve as a natural boundary to protect components. Changes within one project are less likely to affect others, enhancing stability and maintainability.\n3. **Domain Alignment**: By aligning each project with a single domain, we ensure that functionality is implemented in a way that directly corresponds to business requirements. This alignment supports better domain understanding and ownership.\n4. **Scalability and Reusability**: Modular projects make it easier to scale the platform by independently developing, deploying, and maintaining different parts of the system. Components developed within one project can be reused in others if needed, promoting code reuse.\n5. **Focused Development**: Teams can focus their efforts more effectively by working on specific domains, improving productivity and delivering higher-quality code.\n\n### Implementation Strategy\n\nTo implement this decision, we will follow these steps:\n1. **Define Domains**: Clearly identify and document the distinct domains that exist within the platform. Each domain should correspond to a specific business department or area of functionality.\n2. **Project Creation**: Create separate projects for each domain. Each project will serve as a container for all the components and functionalities related to that domain.\n3. **Dependency Management**: Ensure that dependencies between projects are minimal. Any shared functionality should be extracted into common libraries or services that can be shared across projects without creating tight coupling.\n4. **Access Control and Protection**: Implement access control mechanisms to ensure that components within each project are protected and cannot be inadvertently modified by other projects.\n5. **Documentation and Communication**: Maintain clear documentation for each project, detailing the domain, the scope of functionalities, and the interfaces. Ensure that teams communicate effectively to manage dependencies and shared components.\n\nBy adhering to these principles, we will create a robust, modular platform that can evolve efficiently over time while maintaining high levels of component protection and domain alignment.","GenTime":"2024-08-19 23:30:15"}
{"File Name":"libelektra\/boolean.md","Context":"## Problem\\nInconsistent use of booleans in various parts of Elektra.\\n","Decision":"Only the strings `0` and `1` are allowed in the `KeySet` for `type = boolean`, for both values and defaults.\\nEverything else should lead to errors in checkers (in `kdbSet`).\\nA spec with `type = boolean` without a specified default should be interpreted as `default = 0`.\\nExample for an implementation in C in an application:\\n```c\\nif (k != NULL && strcmp(keyString(k), \"1\") == 0) {\/*true*\/} else {\/*false*\/}\\n```\\nStorage plugins are allowed any representation as suitable, e.g., a JSON plugin might render `1` as `true`.\\nThe type checker plugin should allow\\n- non-presence\\n- the string \"0\"\\n- the string \"1\"\\n","tokens":18,"id":1293,"text":"## Problem\\nInconsistent use of booleans in various parts of Elektra.\\n\n\n##Decision\nOnly the strings `0` and `1` are allowed in the `KeySet` for `type = boolean`, for both values and defaults.\\nEverything else should lead to errors in checkers (in `kdbSet`).\\nA spec with `type = boolean` without a specified default should be interpreted as `default = 0`.\\nExample for an implementation in C in an application:\\n```c\\nif (k != NULL && strcmp(keyString(k), \"1\") == 0) {\/*true*\/} else {\/*false*\/}\\n```\\nStorage plugins are allowed any representation as suitable, e.g., a JSON plugin might render `1` as `true`.\\nThe type checker plugin should allow\\n- non-presence\\n- the string \"0\"\\n- the string \"1\"\\n","MatchedIds":[1291,1303,1292,1319,1310],"Predicted":"## Decision\nStandardize the representation and usage of booleans across all components of Elektra to ensure consistency and maintainability.\n\n- Use `bool` as the standardized type for booleans across the codebase.\n- Use `true` and `false` as the standardized values for `bool`.\n- Add a section in the [coding guidelines](coding_guidelines.md) to specify the standard usage of booleans.\n- Update all existing code to replace:\n  - `int` or `char` used to represent booleans with `bool`.\n  - `0` or non-zero values representing `false` and `true` with `false` and `true` respectively.\n- Modify relevant function signatures to use `bool` where appropriate.\n- Ensure that documentation reflects these changes and update any examples or explanations that use booleans.\n- Add test cases to verify the standardized use of booleans in various modules.\n\nAll developers must adhere to these guidelines for any new code or modifications to existing code to maintain consistency.","GenTime":"2024-08-19 23:30:20"}
{"File Name":"ftd-scratch3-offline\/0004-use-multiple-projects.md","Context":"## Context\\nThe software could be developed in one big (Gradle) project.\\nThis would make integration easier.\\nAt the same time this would make re-use of the code outside of this project harder.\\nOne big project would probably lead to worse code since there would not be the need to have defined API boundaries.\\n","Decision":"We will try to modularize the software and will use multiple projects to achieve this goal.\\n","tokens":67,"id":2626,"text":"## Context\\nThe software could be developed in one big (Gradle) project.\\nThis would make integration easier.\\nAt the same time this would make re-use of the code outside of this project harder.\\nOne big project would probably lead to worse code since there would not be the need to have defined API boundaries.\\n\n\n##Decision\nWe will try to modularize the software and will use multiple projects to achieve this goal.\\n","MatchedIds":[1035,1792,2756,3483,4889],"Predicted":"## Decision\nWe will develop the software as multiple Gradle subprojects within a multi-project build setup. This approach will enforce defined API boundaries and facilitate code reuse outside of this project. Although integration might be slightly more complex, the benefits of modularity, clean architecture, and code reusability outweigh the potential integration challenges.","GenTime":"2024-08-19 23:30:32"}
{"File Name":"phpadr\/0005-phpunit-as-testing-framework.md","Context":"## Context\\nEnsure good code quality with ease for change, integration and error correction.\\n","Decision":"It will be used the [PHPUnit](https:\/\/phpunit.de\/) as testing framework.\\n","tokens":18,"id":5070,"text":"## Context\\nEnsure good code quality with ease for change, integration and error correction.\\n\n\n##Decision\nIt will be used the [PHPUnit](https:\/\/phpunit.de\/) as testing framework.\\n","MatchedIds":[3853,1416,400,1418,4868],"Predicted":"## Decision\nImplement Continuous Integration\/Continuous Deployment (CI\/CD) pipelines using Jenkins and incorporate automated tests. Utilize SonarQube for ongoing code quality analysis and reporting. Additionally, enforce code reviews and use Git for version control to facilitate smooth integration and error tracking.","GenTime":"2024-08-19 23:30:34"}
{"File Name":"FindMeFoodTrucks\/Choice of Compute for Ingestion.md","Context":"## :dart: Context\\nAzure offers a number of ways to host your application code. The following are the considerations for choosing a compute option for the ingestion sub-system:\\n* Should support scheduling\\n* Should support running in background\\n* Should be able to connect to backend Cosmos DB\\n* Should support .net core framework\\n* Service will only run once a day\\n* Would prefer a managed service\\n* Does not have portability requirements\\nThe following options for compute are considered for this service:\\n* App Services\\n* Functions\\n* Container Instances\\n* Service Fabric\\n* AKS\\nChoosing the right compute model will help optimize the development experience and operations\\n","Decision":"The recommended approach is to use Azure Functions considering the following points:\\n* Supports consumption plan (Pay-per-use) which is ideal for sparse usage\\n* Supports .net core framework and is well integrated with Visual Studio development experience\\n* Can leverage cosmos SDK (nuget) to connect back to the data store\\n* Has built-in scheduling capabilities\\n* Is a fully managed PaaS service\\n* No overheads related to cluster management or infrastructure provisioning.\\nThe decision is based on the guidance provided by Microsoft here: https:\/\/docs.microsoft.com\/en-us\/azure\/architecture\/guide\/technology-choices\/compute-decision-tree\\n","tokens":135,"id":1095,"text":"## :dart: Context\\nAzure offers a number of ways to host your application code. The following are the considerations for choosing a compute option for the ingestion sub-system:\\n* Should support scheduling\\n* Should support running in background\\n* Should be able to connect to backend Cosmos DB\\n* Should support .net core framework\\n* Service will only run once a day\\n* Would prefer a managed service\\n* Does not have portability requirements\\nThe following options for compute are considered for this service:\\n* App Services\\n* Functions\\n* Container Instances\\n* Service Fabric\\n* AKS\\nChoosing the right compute model will help optimize the development experience and operations\\n\n\n##Decision\nThe recommended approach is to use Azure Functions considering the following points:\\n* Supports consumption plan (Pay-per-use) which is ideal for sparse usage\\n* Supports .net core framework and is well integrated with Visual Studio development experience\\n* Can leverage cosmos SDK (nuget) to connect back to the data store\\n* Has built-in scheduling capabilities\\n* Is a fully managed PaaS service\\n* No overheads related to cluster management or infrastructure provisioning.\\nThe decision is based on the guidance provided by Microsoft here: https:\/\/docs.microsoft.com\/en-us\/azure\/architecture\/guide\/technology-choices\/compute-decision-tree\\n","MatchedIds":[1089,1094,2062,1182,631],"Predicted":"## Decision\nWe will use Azure Functions to host the ingestion sub-system.\n\nAzure Functions meets all the requirements listed:\n* It supports scheduling through Azure Timer Trigger.\n* Azure Functions can run in the background.\n* It has built-in integration with Cosmos DB.\n* It supports the .NET Core framework.\n* Given that the service will only run once a day, Azure Functions' consumption-based pricing model will optimize cost.\n* As a managed service, Azure Functions offloads infrastructure management tasks.\n* Portability is not required in this case, making Azure Functions a highly suitable option without the need for container orchestration.\n\nUsing Azure Functions will streamline development and operations by offering a serverless, scalable, and cost-effective compute model.","GenTime":"2024-08-19 23:30:38"}
{"File Name":"jabref\/0015-support-an-abstract-query-syntax-for-query-conversion.md","Context":"## Context and Problem Statement\\nAll libraries use their own query syntax for advanced search options. To increase usability, users should be able to formulate their (abstract) search queries in a query syntax that can be mapped to the library specific search queries. To achieve this, the query has to be parsed into an AST.\\nWhich query syntax should be used for the abstract queries?\\nWhich features should the syntax support?\\n","Decision":"Chosen option: \"Use a syntax that is derived of the lucene query syntax\", because only option that is already known, and easy to implement.\\nFurthermore parsers for lucene already exist and are tested.\\nFor simplicity, and lack of universal capabilities across fetchers, only basic query features and therefor syntax is supported:\\n* All terms in the query are whitespace separated and will be ANDed\\n* Default and certain fielded terms are supported\\n* Fielded Terms:\\n* `author`\\n* `title`\\n* `journal`\\n* `year` (for single year)\\n* `year-range` (for range e.g. `year-range:2012-2015`)\\n* The `journal`, `year`, and `year-range` fields should only be populated once in each query\\n* The `year` and `year-range` fields are mutually exclusive\\n* Example:\\n* `author:\"Igor Steinmacher\" author:\"Christoph Treude\" year:2017` will be converted to\\n* `author:\"Igor Steinmacher\" AND author:\"Christoph Treude\" AND year:2017`\\nThe supported syntax can be expressed in EBNF as follows:\\nQuery := {Clause} \\\\nClause:= \\[Field\\] Term \\\\nField := author: | title: | journal: | year: | year-range: | default:\\\\nTerm  := Word | Phrase \\\\nWord can be derived to any series of non-whitespace characters.\\nPhrases are multiple words wrapped in quotes and may contain white-space characters within the quotes.\\\\nNote: Even though this EBNF syntactically allows the creation of queries with year and year-range fields,\\nsuch a query does not make sense semantically and therefore will not be executed.\\n### Positive Consequences\\n* Already tested\\n* Well known\\n* Easy to implement\\n* Can use an existing parser\\n","tokens":84,"id":4738,"text":"## Context and Problem Statement\\nAll libraries use their own query syntax for advanced search options. To increase usability, users should be able to formulate their (abstract) search queries in a query syntax that can be mapped to the library specific search queries. To achieve this, the query has to be parsed into an AST.\\nWhich query syntax should be used for the abstract queries?\\nWhich features should the syntax support?\\n\n\n##Decision\nChosen option: \"Use a syntax that is derived of the lucene query syntax\", because only option that is already known, and easy to implement.\\nFurthermore parsers for lucene already exist and are tested.\\nFor simplicity, and lack of universal capabilities across fetchers, only basic query features and therefor syntax is supported:\\n* All terms in the query are whitespace separated and will be ANDed\\n* Default and certain fielded terms are supported\\n* Fielded Terms:\\n* `author`\\n* `title`\\n* `journal`\\n* `year` (for single year)\\n* `year-range` (for range e.g. `year-range:2012-2015`)\\n* The `journal`, `year`, and `year-range` fields should only be populated once in each query\\n* The `year` and `year-range` fields are mutually exclusive\\n* Example:\\n* `author:\"Igor Steinmacher\" author:\"Christoph Treude\" year:2017` will be converted to\\n* `author:\"Igor Steinmacher\" AND author:\"Christoph Treude\" AND year:2017`\\nThe supported syntax can be expressed in EBNF as follows:\\nQuery := {Clause} \\\\nClause:= \\[Field\\] Term \\\\nField := author: | title: | journal: | year: | year-range: | default:\\\\nTerm  := Word | Phrase \\\\nWord can be derived to any series of non-whitespace characters.\\nPhrases are multiple words wrapped in quotes and may contain white-space characters within the quotes.\\\\nNote: Even though this EBNF syntactically allows the creation of queries with year and year-range fields,\\nsuch a query does not make sense semantically and therefore will not be executed.\\n### Positive Consequences\\n* Already tested\\n* Well known\\n* Easy to implement\\n* Can use an existing parser\\n","MatchedIds":[4637,2513,4741,4644,4378],"Predicted":"## Decision\nChosen option: \"SQL-like Query Syntax\", because it comes out best (see below).\n\n### Positive Consequences\n- **Familiarity**: SQL-like syntax is widely known among users, reducing the learning curve.\n- **Expressiveness**: SQL-like syntax is capable of representing complex queries with ease, including joins, sub-queries, and various conditions.\n- **Tooling**: There are numerous tools and libraries available for parsing and managing SQL-like queries, which reduces implementation effort.\n- **Flexibility**: SQL-like queries can be easily mapped to an AST and subsequently translated to the syntax required by each specific library.\n- **Scalability**: The syntax can be extended with custom functionalities if required in the future.\n\n### Negative Consequences\n- **Complexity**: The SQL-like syntax might introduce some complexity for basic users who are unfamiliar with SQL.\n- **Implementation Overhead**: Translating a SQL-like query to the specific query syntax of different libraries will require a sophisticated mapping mechanism.\n- **Performance**: Depending on the complexity of the query and the efficiency of the parsers, there might be performance considerations.\n\n### Features to Support in the Syntax\n1. **Logical Operators**: AND, OR, NOT for combining multiple conditions.\n2. **Comparison Operators**: =, <>, >, <, >=, <= for comparing values.\n3. **Pattern Matching**: LIKE, ILIKE for string pattern matching.\n4. **Aggregations**: COUNT, SUM, AVG, MIN, MAX for aggregating data.\n5. **Sorting**: ORDER BY clause for sorting the results.\n6. **Filtering**: WHERE clause for specifying conditions to filter results.\n7. **Sub-queries**: Support for nested queries to handle complex scenarios.\n8. **Joins**: INNER JOIN, LEFT JOIN, RIGHT JOIN, FULL JOIN to combine data from multiple collections\/tables.\n9. **Functions**: Built-in functions for date-time, string manipulation, and other common operations.\n10. **Limit\/Offset**: LIMIT and OFFSET clauses for pagination.\n\nThe implementation of a SQL-like query syntax with the above features will provide a robust and user-friendly way for users to craft complex search queries, and it can be effectively parsed into an AST for translation to various library-specific query syntaxes.","GenTime":"2024-08-19 23:30:46"}
{"File Name":"dos-server\/adr-11-exception-monitoring.md","Context":"## Context\\nAn application is needed for real time production error discovery and reporting. Sentry is currently being\\nused by DLS for various applications.\\n","Decision":"We will use Sentry for application exception monitoring.\\n","tokens":30,"id":3370,"text":"## Context\\nAn application is needed for real time production error discovery and reporting. Sentry is currently being\\nused by DLS for various applications.\\n\n\n##Decision\nWe will use Sentry for application exception monitoring.\\n","MatchedIds":[3388,2387,3469,4852,1171],"Predicted":"## Decision\nWe will use Sentry to capture and manage real-time production errors and reporting for our applications.\n* Sentry is already being used by DLS for various applications, indicating a familiarity within the organization.\n* It provides comprehensive error monitoring, making it easier to identify and resolve issues promptly.\n* Sentry integrates well with multiple platforms and frameworks, ensuring wide applicability.\n* With Sentry, the team can benefit from existing knowledge and potentially shared configurations, reducing the learning curve and implementation effort.\n","GenTime":"2024-08-19 23:30:51"}
{"File Name":"docspell\/0010_convert_image_files.md","Context":"# Context and Problem Statement\\nHow to convert image files properly to pdf?\\nSince there are thousands of different image formats, there will never\\nbe support for all. The most common containers should be supported,\\nthough:\\n- jpeg (jfif, exif)\\n- png\\n- tiff (baseline, single page)\\nThe focus is on document images, maybe from digital cameras or\\nscanners.\\n# Considered Options\\n* [pdfbox](https:\/\/pdfbox.apache.org\/) library\\n* [imagemagick](https:\/\/www.imagemagick.org\/) external command\\n* [img2pdf](https:\/\/github.com\/josch\/img2pdf) external command\\n* [tesseract](https:\/\/github.com\/tesseract-ocr\/tesseract) external command\\nThere are no screenshots here, because it doesn't make sense since\\nthey all look the same on the screen. Instead we look at the files\\nproperties.\\n**Input File**\\nThe input files are:\\n```\\n$ identify input\/*\\ninput\/jfif.jpg JPEG 2480x3514 2480x3514+0+0 8-bit sRGB 240229B 0.000u 0:00.000\\ninput\/letter-en.jpg JPEG 1695x2378 1695x2378+0+0 8-bit Gray 256c 467341B 0.000u 0:00.000\\ninput\/letter-en.png PNG 1695x2378 1695x2378+0+0 8-bit Gray 256c 191571B 0.000u 0:00.000\\ninput\/letter-en.tiff TIFF 1695x2378 1695x2378+0+0 8-bit Grayscale Gray 4030880B 0.000u 0:00.000\\n```\\nSize:\\n- jfif.jpg 240k\\n- letter-en.jpg 467k\\n- letter-en.png 191k\\n- letter-en.tiff 4.0M\\n","Decision":"Tesseract.\\nTo not use more external tools, imagemagick and img2pdf are not\\nchosen, even though img2pdf shows the best results and is fastest.\\nPdfbox library would be the favorite, because results are good and\\nwith the [twelvemonkeys](https:\/\/github.com\/haraldk\/TwelveMonkeys)\\nlibrary there is support for many images. The priority is to avoid\\nmore external commands if possible.\\nBut since there already is a dependency to tesseract and it can create\\nsearchable pdfs, the decision is to use tesseract for this. Then PDFs\\nwith images can be converted to searchable PDFs with images. And text\\nextraction is required anyways.\\n","tokens":435,"id":273,"text":"# Context and Problem Statement\\nHow to convert image files properly to pdf?\\nSince there are thousands of different image formats, there will never\\nbe support for all. The most common containers should be supported,\\nthough:\\n- jpeg (jfif, exif)\\n- png\\n- tiff (baseline, single page)\\nThe focus is on document images, maybe from digital cameras or\\nscanners.\\n# Considered Options\\n* [pdfbox](https:\/\/pdfbox.apache.org\/) library\\n* [imagemagick](https:\/\/www.imagemagick.org\/) external command\\n* [img2pdf](https:\/\/github.com\/josch\/img2pdf) external command\\n* [tesseract](https:\/\/github.com\/tesseract-ocr\/tesseract) external command\\nThere are no screenshots here, because it doesn't make sense since\\nthey all look the same on the screen. Instead we look at the files\\nproperties.\\n**Input File**\\nThe input files are:\\n```\\n$ identify input\/*\\ninput\/jfif.jpg JPEG 2480x3514 2480x3514+0+0 8-bit sRGB 240229B 0.000u 0:00.000\\ninput\/letter-en.jpg JPEG 1695x2378 1695x2378+0+0 8-bit Gray 256c 467341B 0.000u 0:00.000\\ninput\/letter-en.png PNG 1695x2378 1695x2378+0+0 8-bit Gray 256c 191571B 0.000u 0:00.000\\ninput\/letter-en.tiff TIFF 1695x2378 1695x2378+0+0 8-bit Grayscale Gray 4030880B 0.000u 0:00.000\\n```\\nSize:\\n- jfif.jpg 240k\\n- letter-en.jpg 467k\\n- letter-en.png 191k\\n- letter-en.tiff 4.0M\\n\n\n##Decision\nTesseract.\\nTo not use more external tools, imagemagick and img2pdf are not\\nchosen, even though img2pdf shows the best results and is fastest.\\nPdfbox library would be the favorite, because results are good and\\nwith the [twelvemonkeys](https:\/\/github.com\/haraldk\/TwelveMonkeys)\\nlibrary there is support for many images. The priority is to avoid\\nmore external commands if possible.\\nBut since there already is a dependency to tesseract and it can create\\nsearchable pdfs, the decision is to use tesseract for this. Then PDFs\\nwith images can be converted to searchable PDFs with images. And text\\nextraction is required anyways.\\n","MatchedIds":[284,287,282,5059,4308],"Predicted":"## Decision\nWe will use ImageMagick to convert image files to PDF.\n\n### Rationale:\n1. **Wide Format Support**: ImageMagick supports a vast array of image formats, making it capable of handling not just JPEG, PNG, and TIFF, but also many other lesser-used formats. This ensures flexibility and future-proofing.\n2. **Efficiency and Quality**: ImageMagick is highly efficient in terms of both processing speed and output quality. The results are consistent and reliable across different image types.\n3. **Customization and Control**: ImageMagick offers extensive options and parameters to control the conversion process, including image quality, compression, color profiles, and more. This level of control can be crucial for fine-tuning the output to meet specific requirements.\n4. **Community and Documentation**: Being a well-established tool, ImageMagick boasts a large user community and comprehensive documentation, which can be highly beneficial for troubleshooting and extending functionality.\n5. **Availability**: ImageMagick is readily available on most platforms and can be easily integrated into existing pipelines using command-line tools or programming interfaces.\n\nWhile other options like pdfbox and img2pdf also provide good results, ImageMagick\u2019s versatility and extensive feature set make it the best choice for this use case. Tesseract, on the other hand, is primarily an OCR tool and is not as suitable for straightforward image-to-PDF conversion without text extraction needs.\n\nHence, ImageMagick is chosen as the optimal solution for converting image files to PDF.","GenTime":"2024-08-19 23:30:56"}
{"File Name":"html-diff-service\/0002-implement-using-spring-boot.md","Context":"## Context\\nThe HTML Diff Service is born out of need to increase performance of HTML\\ndiffing by not requiring a command line execution in Contently's web\\napplication. Additionally, we wanted to remove the Java dependency from our\\nRails application container to simplify and reduce the threat surface.\\n","Decision":"Spring Boot is used to allow for the service to be standalone with support for\\nembedded servers. The decision allows portability to deploy the service without\\na J2EE server.\\n","tokens":60,"id":3249,"text":"## Context\\nThe HTML Diff Service is born out of need to increase performance of HTML\\ndiffing by not requiring a command line execution in Contently's web\\napplication. Additionally, we wanted to remove the Java dependency from our\\nRails application container to simplify and reduce the threat surface.\\n\n\n##Decision\nSpring Boot is used to allow for the service to be standalone with support for\\nembedded servers. The decision allows portability to deploy the service without\\na J2EE server.\\n","MatchedIds":[3586,4868,3156,1576,2101],"Predicted":"## Decision\nWe will implement a native Ruby service for HTML diffing, leveraging existing Ruby libraries like Nokogiri for parsing and Diffy for differencing. This will allow us to move the diffing process entirely within the Rails application, thus eliminating the need for command line execution and removing the Java dependency, thereby improving performance and reducing the threat surface.","GenTime":"2024-08-19 23:30:59"}
{"File Name":"modernisation-platform\/0010-terraform-module-strategy.md","Context":"## Context\\nThe Modernisation Platform uses [Terraform](https:\/\/www.terraform.io\/) for its infrastructure as code. To make infrastructure reusable, or to simply tidy up code you can use [Terraform Modules](https:\/\/www.terraform.io\/docs\/language\/modules\/). There are different use cases in the platform for using modules, and this ADR outlines how we plan to use them.\\n","Decision":"Modules used only by the Modernisation Platform core infrastructure will remain in the [terraform\/modules](https:\/\/github.com\/ministryofjustice\/modernisation-platform\/tree\/main\/terraform\/modules) folder where they are currently located. These modules are mainly single use modules but created to keep the code tidier and easier to maintain. Modules used only by the core which currently have their own repository will remain where they are.\\nModules used by users will have their own repository per module which we link to from the main repo. These modules will be versioned with GitHub releases, and tested with [Terratest](https:\/\/terratest.gruntwork.io\/) against a test AWS account.\\n","tokens":82,"id":3144,"text":"## Context\\nThe Modernisation Platform uses [Terraform](https:\/\/www.terraform.io\/) for its infrastructure as code. To make infrastructure reusable, or to simply tidy up code you can use [Terraform Modules](https:\/\/www.terraform.io\/docs\/language\/modules\/). There are different use cases in the platform for using modules, and this ADR outlines how we plan to use them.\\n\n\n##Decision\nModules used only by the Modernisation Platform core infrastructure will remain in the [terraform\/modules](https:\/\/github.com\/ministryofjustice\/modernisation-platform\/tree\/main\/terraform\/modules) folder where they are currently located. These modules are mainly single use modules but created to keep the code tidier and easier to maintain. Modules used only by the core which currently have their own repository will remain where they are.\\nModules used by users will have their own repository per module which we link to from the main repo. These modules will be versioned with GitHub releases, and tested with [Terratest](https:\/\/terratest.gruntwork.io\/) against a test AWS account.\\n","MatchedIds":[621,3147,2062,1372,2793],"Predicted":"## Decision\nWe have decided to standardize the usage of Terraform Modules across the Modernisation Platform by adhering to the following guidelines:\n\n1. **Module Structure**: Each module should have a clear structure, including a `README.md` file that describes its purpose, inputs, outputs, and usage examples.\n2. **Naming Conventions**: Modules should be named using a consistent naming convention that reflects their purpose. For example, `aws-s3-bucket` or `azure-vm`.\n3. **Versioning**: All modules must use semantic versioning (semver) to ensure compatibility and manage updates effectively.\n4. **Documentation**: Each module must include comprehensive documentation that details the variables, outputs, and any dependencies.\n5. **Reusability**: Modules should be designed to be reusable across different projects and environments to promote consistency.\n6. **Testing**: Each module must include automated tests to verify its functionality. We will use tools like `terratest` to ensure the reliability of modules.\n7. **Registry Usage**: We will publish and consume modules via the Terraform Registry to facilitate easy access and version control.\n\nBy following these guidelines, we ensure that our Terraform modules are well-organized, reusable, and maintainable, enhancing the overall efficiency and reliability of our infrastructure as code.","GenTime":"2024-08-19 23:31:04"}
{"File Name":"openlobby-server\/0006-use-flask.md","Context":"## Context\\nWe need to choose webserver.\\n","Decision":"We will use Flask. Server should be simple - pretty much just with a GraphQL\\nendpoint and GraphiQL.\\n","tokens":11,"id":469,"text":"## Context\\nWe need to choose webserver.\\n\n\n##Decision\nWe will use Flask. Server should be simple - pretty much just with a GraphQL\\nendpoint and GraphiQL.\\n","MatchedIds":[1243,1239,1240,3657,472],"Predicted":"## Decision\nWe will use Nginx as the web server because it is well-known for its high performance, stability, and low resource consumption. Additionally, Nginx provides powerful load-balancing capabilities and is widely supported, making it an excellent choice for scalability and reliability.","GenTime":"2024-08-19 23:31:06"}
{"File Name":"CCS-Architecture-Decision-Records\/0012-use-govuk-notify-for-bulk-communication-with-users.md","Context":"## Context\\nMost public-facing digital services have a need to notify its users about something at a critical business process, or just to manage their expectations to prevent incoming contact.\\nThe notifications can, for example, be:\\n* Telling a supplier they have a task to complete,\\n* Telling a supplier that a deadline is approaching,\\n* Telling a supplier that a buyer has been shortlisted for a procurement,\\n* Providing a receipt,\\n* Providing status updates,\\n* Etc...\\nGenerally, the notifications will be email, but may also need to be SMS in some cases.\\nTo reduce the need for each organisation or service team to build its own infrastructure for sending email and SMS based messages, the Government Digital Service (GDS) has built GOV.UK Notify.\\nGOV.UK Notify can send notifications either via an API call, or by uploading a CSV to the website. Sending email is free, and the service provides feedback on what messages have failed to be delivered.\\nThe service is already used in various parts of CCS, including MISO, Report MI, DigiTS etc.\\nUsing GOV.UK Notify instead of other solutions (eg using the BCC field in an email) also reduces the potential for an accidental data breach.\\n","Decision":"We will use GOV.UK Notify to send notifications to users of any newly built CCS digital services.\\nThis will make sure Notify offers the same user experience across all CCS digital services, and it is currently the most cost-effective (having no-cost for emails) bulk messaging 3rd party solution on the market.\\nThe dashboard to create new Notify templates and send bulk emails is very easy, and new Service Teams at CCS can easily be onboarded to use it, with minimal\/no training.\\nThis ADR does not apply to generic Email Newsletter communication, for which other, more market specific tools may be more appropriate.\\n","tokens":261,"id":996,"text":"## Context\\nMost public-facing digital services have a need to notify its users about something at a critical business process, or just to manage their expectations to prevent incoming contact.\\nThe notifications can, for example, be:\\n* Telling a supplier they have a task to complete,\\n* Telling a supplier that a deadline is approaching,\\n* Telling a supplier that a buyer has been shortlisted for a procurement,\\n* Providing a receipt,\\n* Providing status updates,\\n* Etc...\\nGenerally, the notifications will be email, but may also need to be SMS in some cases.\\nTo reduce the need for each organisation or service team to build its own infrastructure for sending email and SMS based messages, the Government Digital Service (GDS) has built GOV.UK Notify.\\nGOV.UK Notify can send notifications either via an API call, or by uploading a CSV to the website. Sending email is free, and the service provides feedback on what messages have failed to be delivered.\\nThe service is already used in various parts of CCS, including MISO, Report MI, DigiTS etc.\\nUsing GOV.UK Notify instead of other solutions (eg using the BCC field in an email) also reduces the potential for an accidental data breach.\\n\n\n##Decision\nWe will use GOV.UK Notify to send notifications to users of any newly built CCS digital services.\\nThis will make sure Notify offers the same user experience across all CCS digital services, and it is currently the most cost-effective (having no-cost for emails) bulk messaging 3rd party solution on the market.\\nThe dashboard to create new Notify templates and send bulk emails is very easy, and new Service Teams at CCS can easily be onboarded to use it, with minimal\/no training.\\nThis ADR does not apply to generic Email Newsletter communication, for which other, more market specific tools may be more appropriate.\\n","MatchedIds":[2050,5213,1786,1784,1767],"Predicted":"## Decision\nWe will adopt GOV.UK Notify for sending notifications in our public-facing digital services. This decision leverages the existing, reliable, and cost-effective infrastructure provided by GOV.UK Notify, which supports both email and SMS notifications. Initially, we can use CSV uploads for manual notifications, but the long-term plan is to automate notifications via API calls to enhance efficiency and scalability. This adoption will also minimize risks related to data breaches compared to using methods like the BCC field in emails.","GenTime":"2024-08-19 23:31:11"}
{"File Name":"FindMeFoodTrucks\/Hosting Model.md","Context":"## :dart: Context\\nThe application can be hosted on a private datacenter or on a managed cloud datacenter.\\nChoosing the right hosting model will impact the scalability cost and technology choices for the application.\\n","Decision":"The application will be hosted on Microsoft Azure Cloud datacenter.\\n","tokens":43,"id":1089,"text":"## :dart: Context\\nThe application can be hosted on a private datacenter or on a managed cloud datacenter.\\nChoosing the right hosting model will impact the scalability cost and technology choices for the application.\\n\n\n##Decision\nThe application will be hosted on Microsoft Azure Cloud datacenter.\\n","MatchedIds":[1094,1095,3657,108,1239],"Predicted":"## Decision\nWe will host the application on a managed cloud datacenter for the following reasons:\n* Scalability: Managed cloud services typically offer automatic scaling options, making it easier to handle varying load without manual intervention.\n* Cost: Managed cloud providers often offer a pay-as-you-go pricing model which can be more cost-effective compared to maintaining a private datacenter.\n* Technology Choices: Cloud providers offer a wide range of services and integrations, granting access to advanced tools and technologies that enhance development and operational efficiencies.\n* Resilience: Managed cloud services usually come with built-in redundancy and disaster recovery options.\n* Management Overhead: Using a managed cloud service reduces the management and maintenance overhead, allowing the team to focus more on developing the application.\n\nThis decision is based on the guidance provided by various cloud best practices, including pricing models and infrastructure management benefits outlined by major managed cloud providers like AWS, Azure, and Google Cloud.","GenTime":"2024-08-19 23:31:15"}
{"File Name":"csc-swr-architecture\/007-Integration-Approaches.html.md.erb","Context":"## Context\\nThe Children's Social Care Alpha Service is seeking to pull together key data assets\\nsuch as Looked After Children and Foster Families. These will be sourced from Local\\nAuthority (LA) and Independent Foster Agency (IFA) Care Management Systems (CMS'),\\nas well as central Departmental data collections such as SSDA903.\\nThe SSDA903 collection and the CMS at LAs highlight the significance of individual\\npathways through the Care System. Each individuals pathway can be regarded as a\\nseries of events and touchpoints that the individual has had with various other actors\\nin the system, such as IFAs and Foster Families.\\nThe existing SSDA903 dataset is analogous to an event stream, with the Local\\nAuthorities (LAs) and Indedependent Fostering Agencies (IFAs) being event producers.\\n","Decision":"* Develop an API Gateway (see [Kong], [NGinx], [AWS API Gateway]) and REST Proxy, available primarily\\nto LAs and IFAs, serving an [Event Architecture] available internally.\\n* Use an Event Backbone (see [Kafka], [AWS Event Bridge]) to store and publish these events internally.\\n* Publish example client code on the Internet to ease LA and IFA adoption.\\n* Events will become the durable and persistent data which can be used to hydrate\\nthe Department's services, such as Diary Log and Matchmaker Assistant,\\n* [Event Sourcing] should be trialled as a solution.\\n","tokens":169,"id":914,"text":"## Context\\nThe Children's Social Care Alpha Service is seeking to pull together key data assets\\nsuch as Looked After Children and Foster Families. These will be sourced from Local\\nAuthority (LA) and Independent Foster Agency (IFA) Care Management Systems (CMS'),\\nas well as central Departmental data collections such as SSDA903.\\nThe SSDA903 collection and the CMS at LAs highlight the significance of individual\\npathways through the Care System. Each individuals pathway can be regarded as a\\nseries of events and touchpoints that the individual has had with various other actors\\nin the system, such as IFAs and Foster Families.\\nThe existing SSDA903 dataset is analogous to an event stream, with the Local\\nAuthorities (LAs) and Indedependent Fostering Agencies (IFAs) being event producers.\\n\n\n##Decision\n* Develop an API Gateway (see [Kong], [NGinx], [AWS API Gateway]) and REST Proxy, available primarily\\nto LAs and IFAs, serving an [Event Architecture] available internally.\\n* Use an Event Backbone (see [Kafka], [AWS Event Bridge]) to store and publish these events internally.\\n* Publish example client code on the Internet to ease LA and IFA adoption.\\n* Events will become the durable and persistent data which can be used to hydrate\\nthe Department's services, such as Diary Log and Matchmaker Assistant,\\n* [Event Sourcing] should be trialled as a solution.\\n","MatchedIds":[915,920,1863,1016,3253],"Predicted":"## Decision\nAdopt an event-driven architecture (EDA) to integrate and process data from diverse sources.\n\n**Reasoning:**\n1. **Real-time Data Processing:** An event-driven architecture enables real-time data processing from multiple sources (LAs, IFAs, SSDA903 collections), allowing for timely updates and actions.\n2. **Scalability:** EDA can handle increasing volumes of data seamlessly, which is essential given the large and possibly growing number of events and actors involved.\n3. **Flexibility:** This architecture supports the flexibility needed for the exploratory phase, as new event producers and consumers can be added with minimal changes to the existing system.\n4. **Decoupling:** By decoupling event producers (e.g., LAs, IFAs) from event consumers (e.g., analytical tools, dashboards), the architecture ensures that changes in one component do not directly impact others.\n5. **Enhanced Data Integration:** The EDA framework will facilitate the integration of heterogeneous data sources by standardizing event formats and handling data transformation centrally.\n\n**Implementation:**\n1. **Event Brokers:** Use robust event brokers such as Apache Kafka or AWS Kinesis to handle the event streams. These platforms support durability, scalability, and high-throughput messaging.\n2. **Data Transformation and Enrichment:** Implement data transformation and enrichment services that consume events, cleanse data, and publish enriched events to subsequent consumers.\n3. **Storage:** Store the events in a centralized event store (e.g., object storage, HDFS) for historical analysis and to support event replay capabilities.\n4. **Analytics:** Develop consumer applications to leverage the event streams for real-time analytics and reporting on individual pathways through the Care System.\n5. **Security and Compliance:** Ensure that the event streams and related data stores comply with data protection regulations (e.g., GDPR), given the sensitive nature of the data involved.\n\nBy adopting an event-driven architecture, the Children's Social Care Alpha Service will be well-positioned to integrate data efficiently, derive insights in real time, and adapt to future needs smoothly.","GenTime":"2024-08-19 23:31:23"}
{"File Name":"dlp-lux\/0002-Blacklight-Version.md","Context":"## Context\\nIn order to facilitate work on dlp-lux, we need to make a decision about which verion of Blacklight to run.\\n","Decision":"We will use Blacklight 7.\\nThis allows us to avoid problems deploying the application, and sets us up for the future when Blacklight 7 is\\nsupported by Hyrax.\\nThis decision accepts that minor styling differences may occur between Lux and Curate.\\n","tokens":31,"id":3215,"text":"## Context\\nIn order to facilitate work on dlp-lux, we need to make a decision about which verion of Blacklight to run.\\n\n\n##Decision\nWe will use Blacklight 7.\\nThis allows us to avoid problems deploying the application, and sets us up for the future when Blacklight 7 is\\nsupported by Hyrax.\\nThis decision accepts that minor styling differences may occur between Lux and Curate.\\n","MatchedIds":[3653,3657,1239,469,4361],"Predicted":"## Decision\nWe will use the latest stable version of [Blacklight](https:\/\/github.com\/projectblacklight\/blacklight) to ensure we are working with the most up-to-date features and security patches, while also receiving the best community support.","GenTime":"2024-08-19 23:31:26"}
{"File Name":"compliantkubernetes\/0004-plan-for-usage-without-wrapper-scripts.md","Context":"## Context and Problem Statement\\nWe frequently write wrapper scripts. They bring the following value:\\n1. They bind together several tools and make them work together as a whole, e.g., `sops` and `kubectl`.\\n2. They encode domain knowledge and standard operating procedures, e.g., how to add a node, how a cluster should look like, where to find configuration files.\\n3. They enforce best practices, e.g., encrypt secrets consumed or produced by tools.\\nUnfortunately, wrapper scripts can also bring disadvantages:\\n1. They make usages that are deviating from the \"good way\" difficult.\\n2. They risk adding opacity and raise the adoption barrier. People used to the underlying tools may find it difficult to follow how those tools are invoked.\\n3. They add overhead when adding new features or supporting new use-cases.\\n4. They raise the learning curve, i.e., newcomers need to learn the wrapper scripts in addition to the underlying tools. Completely abstracting away the underlying tools is unlikely, due to the [Law of Leaky Abstractions](https:\/\/www.joelonsoftware.com\/2002\/11\/11\/the-law-of-leaky-abstractions\/).\\n## Decision Drivers\\n* We want to make operations simple, predictable, resilient to human error and scalable.\\n* We want to have some predictability in how an environment is set up.\\n* We want to make Compliant Kubernetes flexible and agile.\\n","Decision":"* We want to make operations simple, predictable, resilient to human error and scalable.\\n* We want to have some predictability in how an environment is set up.\\n* We want to make Compliant Kubernetes flexible and agile.\\nWe have chosen to keep wrapper scripts in general. However, they need to be written in a way that ensures that our artefacts (e.g., Terraform scripts, Ansible roles, Helmfiles and Helm Charts) are usable without wrapper scripts. Wrapper scripts should also be simple enough so they can be inspected and useful commands can be copy-pasted out. This ensures that said scripts do not need to be \"too\" powerful and \"too\" well documented, but at the same time they do brings the sought after value.\\nThis decision applies for new wrapper scripts. We will not rework old wrapper scripts.\\n### Positive Consequences\\n* The operations team can encode standard operating procedures and scale ways of working.\\n* Customer-facing developers can easily reuse artefacts for new use-cases, without significant development effort.\\n* Newcomers will (hopefully) find the right trade-off of barriers, depending on whether they are looking for flexibility or predictability.\\n### Negative Consequences\\n* There will be a constant temptation to do things outside wrapper scripts, which will complicated knowledge sharing, operations and support. When this becomes a significant issue, we will need to draft clear guidelines on what should belong in a wrapper scripts and what not.\\n","tokens":301,"id":3109,"text":"## Context and Problem Statement\\nWe frequently write wrapper scripts. They bring the following value:\\n1. They bind together several tools and make them work together as a whole, e.g., `sops` and `kubectl`.\\n2. They encode domain knowledge and standard operating procedures, e.g., how to add a node, how a cluster should look like, where to find configuration files.\\n3. They enforce best practices, e.g., encrypt secrets consumed or produced by tools.\\nUnfortunately, wrapper scripts can also bring disadvantages:\\n1. They make usages that are deviating from the \"good way\" difficult.\\n2. They risk adding opacity and raise the adoption barrier. People used to the underlying tools may find it difficult to follow how those tools are invoked.\\n3. They add overhead when adding new features or supporting new use-cases.\\n4. They raise the learning curve, i.e., newcomers need to learn the wrapper scripts in addition to the underlying tools. Completely abstracting away the underlying tools is unlikely, due to the [Law of Leaky Abstractions](https:\/\/www.joelonsoftware.com\/2002\/11\/11\/the-law-of-leaky-abstractions\/).\\n## Decision Drivers\\n* We want to make operations simple, predictable, resilient to human error and scalable.\\n* We want to have some predictability in how an environment is set up.\\n* We want to make Compliant Kubernetes flexible and agile.\\n\n\n##Decision\n* We want to make operations simple, predictable, resilient to human error and scalable.\\n* We want to have some predictability in how an environment is set up.\\n* We want to make Compliant Kubernetes flexible and agile.\\nWe have chosen to keep wrapper scripts in general. However, they need to be written in a way that ensures that our artefacts (e.g., Terraform scripts, Ansible roles, Helmfiles and Helm Charts) are usable without wrapper scripts. Wrapper scripts should also be simple enough so they can be inspected and useful commands can be copy-pasted out. This ensures that said scripts do not need to be \"too\" powerful and \"too\" well documented, but at the same time they do brings the sought after value.\\nThis decision applies for new wrapper scripts. We will not rework old wrapper scripts.\\n### Positive Consequences\\n* The operations team can encode standard operating procedures and scale ways of working.\\n* Customer-facing developers can easily reuse artefacts for new use-cases, without significant development effort.\\n* Newcomers will (hopefully) find the right trade-off of barriers, depending on whether they are looking for flexibility or predictability.\\n### Negative Consequences\\n* There will be a constant temptation to do things outside wrapper scripts, which will complicated knowledge sharing, operations and support. When this becomes a significant issue, we will need to draft clear guidelines on what should belong in a wrapper scripts and what not.\\n","MatchedIds":[4707,856,1376,631,3110],"Predicted":"## Decision\nAdopt a modular and transparent approach for wrapper scripts that balances their advantages with the need for flexibility, transparency, and ease of learning.\n\n### Key Points\n1. **Modular Approach**: Split wrapper scripts into smaller, independent modules. Each script should handle a single responsibility, making it easier to maintain, understand, and extend.\n   \n2. **Transparent Documentation**: Ensure that all wrapper scripts are well-documented. Provide detailed comments and usage examples within the scripts, as well as standalone comprehensive documentation. This reduces the learning curve and makes it easier for new team members to understand how the underlying tools are invoked.\n\n3. **Configurable Behavior**: Allow configurable defaults in the wrapper scripts to support different use cases without modifying the script itself. This includes using environment variables and configuration files.\n\n4. **Logging and Debugging**: Incorporate extensive logging mechanisms so that users can see exactly what commands are being executed and understand any errors that occur. Provide a debug mode that offers more verbosity and insight into script operations.\n\n5. **Compliance and Best Practices**: Enforce best practices and compliance measures through the scripts, but allow for exceptions where necessary. This can be achieved by introducing \"advanced\" or \"expert\" modes that bypass certain checks while warning users of potential risks.\n\n6. **Validation and Testing**: Implement robust validation of inputs and expected states to make scripts resilient to human error. Include automated testing for scripts to ensure reliability and stability.\n\n### Positive Consequences\n* **Simplified Operations**: Operations can be performed more predictably and are resilient to human errors, making everyday tasks simpler for users.\n* **Enhanced Flexibility**: Modular and configurable design allows users to adapt scripts to various scenarios without extensive rewrites.\n* **Transparency and Adoption**: Improved documentation and logging lower the barriers to adoption and make the scripts easier to understand and troubleshoot.\n* **Better Scalability**: A clear, modular approach makes it easier to scale operations and introduce new features.\n\n### Negative Consequences\n* **Initial Investment**: Requires an initial investment in refactoring existing scripts, building modular components, and creating documentation.\n* **Learning Curve**: Despite efforts to reduce complexity, there will still be an inherent learning curve, particularly with understanding modular design and configuration options.\n* **Maintenance Overhead**: The system of modular scripts may introduce more components to maintain, and keeping documentation synchronized with code changes might require continuous effort.","GenTime":"2024-08-19 23:31:35"}
{"File Name":"ReportMI-service-manual\/0019-store-data-validation-rules-in-json-files.md","Context":"## Context\\nEach framework has a specific description of what data must be submitted in the\\nmonthly MI return. This description outlines what fields should be submitted,\\nwhat data types are accepted and any validation rules which should be applied.\\nAt the moment, these rules are encapsulated in the Excel template which\\nsuppliers download, using drop-downs, tool-tips and sometimes macros.\\nA separate copy of these rules is configured for each framework in MISO, which\\nthe service uses to validate templates have been filed in correctly when\\nuploaded.\\n### Longer term approach\\nIn future, we want a single source for the rules for framework data submissions\\nwhich we can use for various tasks including:\\n- generating submission templates in different formats (eg Excel, ODS, CSV)\\n- validating submitted returns\\n- documenting any APIs\\nThese rules should ideally be stored in a central place and made available to\\nany services which require them via an API. This will allow new services to\\nshare the ruleset and description.\\nThe high-level principle is that there should be an artefact that describes the\\nrules that need to be applied.\\nThe artefacts need to be version controlled so they can be changed during the\\nlifetime of the framework. Services may need to access and used outdated\\nversions of the rules.\\n### Interim approach\\nFor the initial waves of on-boarding, we expect only a small number of\\nframeworks to be submitting monthly returns.\\nWhile we develop our approach, and learn more about the varied rules for each\\nframework, we will use an interim approach for storing the rules.\\nWe will create a JSON file for each framework which outlines the data structure\\nrequired and the validation rules for each field. This JSON file will be stored\\nin a GitHub repository, and used to generate templates and validate submitted\\nfiles.\\nEventually, we will develop an API to provide access to this information.\\n","Decision":"We will create a JSON file for each framework and store it in a GitHub\\nrepository.\\nThis JSON file will describe each field, it's data type and associated\\nvalidation rules.\\nWe will migrate these files to a longer-term solution once we have a better\\nunderstanding of the required rules for each framework.\\n","tokens":391,"id":2045,"text":"## Context\\nEach framework has a specific description of what data must be submitted in the\\nmonthly MI return. This description outlines what fields should be submitted,\\nwhat data types are accepted and any validation rules which should be applied.\\nAt the moment, these rules are encapsulated in the Excel template which\\nsuppliers download, using drop-downs, tool-tips and sometimes macros.\\nA separate copy of these rules is configured for each framework in MISO, which\\nthe service uses to validate templates have been filed in correctly when\\nuploaded.\\n### Longer term approach\\nIn future, we want a single source for the rules for framework data submissions\\nwhich we can use for various tasks including:\\n- generating submission templates in different formats (eg Excel, ODS, CSV)\\n- validating submitted returns\\n- documenting any APIs\\nThese rules should ideally be stored in a central place and made available to\\nany services which require them via an API. This will allow new services to\\nshare the ruleset and description.\\nThe high-level principle is that there should be an artefact that describes the\\nrules that need to be applied.\\nThe artefacts need to be version controlled so they can be changed during the\\nlifetime of the framework. Services may need to access and used outdated\\nversions of the rules.\\n### Interim approach\\nFor the initial waves of on-boarding, we expect only a small number of\\nframeworks to be submitting monthly returns.\\nWhile we develop our approach, and learn more about the varied rules for each\\nframework, we will use an interim approach for storing the rules.\\nWe will create a JSON file for each framework which outlines the data structure\\nrequired and the validation rules for each field. This JSON file will be stored\\nin a GitHub repository, and used to generate templates and validate submitted\\nfiles.\\nEventually, we will develop an API to provide access to this information.\\n\n\n##Decision\nWe will create a JSON file for each framework and store it in a GitHub\\nrepository.\\nThis JSON file will describe each field, it's data type and associated\\nvalidation rules.\\nWe will migrate these files to a longer-term solution once we have a better\\nunderstanding of the required rules for each framework.\\n","MatchedIds":[2039,1001,445,1000,1863],"Predicted":"## Decision\nWe will adopt the interim approach of using JSON files for each framework to outline the data structure and validation rules for monthly MI returns. These JSON files will be stored in a version-controlled GitHub repository. This approach will allow us to generate submission templates and validate submissions using a consistent and single source of truth for each framework's rules.\n\n### Key Steps:\n1. **JSON File Creation**: For each framework, create a JSON file that contains the required data fields, accepted data types, and any applicable validation rules.\n2. **Version Control**: Store these JSON files in a dedicated GitHub repository to ensure they are version-controlled and can be updated as the framework rules change.\n3. **Template Generation**: Utilize these JSON files to generate submission templates in different formats (such as Excel, ODS, CSV) as needed.\n4. **Submission Validation**: Validate submitted returns against the rules defined in these JSON files to ensure data integrity and correctness.\n5. **API Development**: In the longer term, develop an API to provide access to these JSON files and their ruleset to facilitate integration with other services and systems. \n\n### Rationale:\n- **Consistency**: Ensures a single source of truth for framework rules, reducing discrepancies and errors.\n- **Flexibility**: Allows easy updates to rules and data structures, which can be versioned and tracked.\n- **Scalability**: Facilitates the eventual creation of an API to share ruleset information more broadly, supporting multiple services.\n- **Interoperability**: Supports the generation of submission templates in various formats and eases integration with other systems through a standardized approach.\n\nBy adopting this iterative approach, we can start with a manageable solution and expand and refine our processes as we scale and onboard more frameworks.","GenTime":"2024-08-19 23:31:42"}
{"File Name":"tul_cob\/0001-use-feature-flags-to-toggle-features.md","Context":"## Context\\nThere are features that exist in the codebase we are not yet ready to release in production. We would like to use feature flags to toggle the availability of certain features, which will help prevent development and production branches from drifting.\\n","Decision":"We've decided to implement very simple feature flags that can be toggled with environment variables.\\n","tokens":49,"id":3171,"text":"## Context\\nThere are features that exist in the codebase we are not yet ready to release in production. We would like to use feature flags to toggle the availability of certain features, which will help prevent development and production branches from drifting.\\n\n\n##Decision\nWe've decided to implement very simple feature flags that can be toggled with environment variables.\\n","MatchedIds":[4508,4868,3653,187,2538],"Predicted":"## Decision\n* Implement a feature flag system to control the availability of specific features within the codebase. This will ensure that features can be developed and tested without being released to production prematurely.\n* Utilize a configuration management tool (e.g., LaunchDarkly, Optimizely, or a custom-built solution) to manage the feature flags.\n* Ensure that the feature flags configuration can be easily updated without requiring code changes and deployments. This allows for quick enablement or disablement of features.\n* Integrate checks for feature flags at critical points in the codebase, such as within controllers, services, and templates.\n* Establish a clear process and guidelines for the lifecycle of feature flags, including creation, testing, activation, and eventual removal once the feature is fully launched.\n* Implement monitoring and logging around feature flags to track their usage and identify any potential issues when toggling features on or off.\n* Ensure all team members are aware of and understand the feature flag system and its usage to ensure consistent application across the project.","GenTime":"2024-08-19 23:31:45"}
{"File Name":"community\/dr-017-Application-Integration-without-Wormhole-Connector.md","Context":"## Context\\nThe Wormhole Connector is a part of the Application Integration. It is used by external systems deployed inside customers' private datacenter to establish a secure connection tunnel to the Kyma platform.\\n","Decision":"The decision is to get rid of the Wormhole Connector at this stage of the project. It can be an optional component of the Application Connector but a secure tunnel. For now, the Application Connector should focus on stability and on providing the core business features.\\n","tokens":42,"id":3448,"text":"## Context\\nThe Wormhole Connector is a part of the Application Integration. It is used by external systems deployed inside customers' private datacenter to establish a secure connection tunnel to the Kyma platform.\\n\n\n##Decision\nThe decision is to get rid of the Wormhole Connector at this stage of the project. It can be an optional component of the Application Connector but a secure tunnel. For now, the Application Connector should focus on stability and on providing the core business features.\\n","MatchedIds":[2236,733,2499,3339,578],"Predicted":"## Decision\nWe decided to enhance the Wormhole Connector to support mutual TLS (mTLS) for establishing secure communication channels between external systems and the Kyma platform. This will ensure that both sides authenticate each other, adding an extra layer of security. Additionally, we will implement automatic certificate renewal and rotation mechanisms to maintain seamless and secure connectivity. Configuration presets will be provided to simplify deployment and integration within customers' private datacenters. Extensive logging and monitoring capabilities will be added for better observability and troubleshooting.","GenTime":"2024-08-19 23:31:50"}
{"File Name":"infrastructure-adrs\/0009-create-provenance-service.md","Context":"## Context and Problem Statement <!-- required -->\\n[Describe the context and problem statement, e.g., in free form using two to three sentences. You may want to articulate the problem in form of a question.]\\n## Decision Drivers <!-- optional -->\\n* [driver 1, e.g., a force, facing concern, \u2026]\\n* [driver 2, e.g., a force, facing concern, \u2026]\\n* ... <!-- numbers of drivers can vary -->\\n","Decision":"* [driver 1, e.g., a force, facing concern, \u2026]\\n* [driver 2, e.g., a force, facing concern, \u2026]\\n* ... <!-- numbers of drivers can vary -->\\nTBD\\n### Positive Consequences <!-- optional -->\\n* [e.g., improvement of quality attribute satisfaction, follow-up decisions required, \u2026]\\n* ...\\n### Negative Consequences <!-- optional -->\\n* [e.g., compromising quality attribute, follow-up decisions required, \u2026]\\n* ...\\n","tokens":100,"id":794,"text":"## Context and Problem Statement <!-- required -->\\n[Describe the context and problem statement, e.g., in free form using two to three sentences. You may want to articulate the problem in form of a question.]\\n## Decision Drivers <!-- optional -->\\n* [driver 1, e.g., a force, facing concern, \u2026]\\n* [driver 2, e.g., a force, facing concern, \u2026]\\n* ... <!-- numbers of drivers can vary -->\\n\n\n##Decision\n* [driver 1, e.g., a force, facing concern, \u2026]\\n* [driver 2, e.g., a force, facing concern, \u2026]\\n* ... <!-- numbers of drivers can vary -->\\nTBD\\n### Positive Consequences <!-- optional -->\\n* [e.g., improvement of quality attribute satisfaction, follow-up decisions required, \u2026]\\n* ...\\n### Negative Consequences <!-- optional -->\\n* [e.g., compromising quality attribute, follow-up decisions required, \u2026]\\n* ...\\n","MatchedIds":[4062,4701,3820,1145,4616],"Predicted":"## Context and Problem Statement\nWe are currently using a custom-built user authentication system, which has become increasingly difficult to maintain and lacks advanced security features. The question is whether we should continue investing in the maintenance and development of this custom solution or switch to an established third-party authentication service.\n\n## Decision Drivers \n* Security: The need for advanced security features such as multi-factor authentication (MFA) and regular security patches.\n* Maintenance: The ongoing effort and cost involved in maintaining a custom-built system.\n* Integration: The ease of integrating the authentication system with existing and future applications.\n* Cost: The potential cost savings or expenditures involved in migrating to a third-party service.\n* Compliance: The need to meet industry standards and regulations for user data protection.\n\n## Decision\nChosen option: \"Switch to an established third-party authentication service,\" because it offers advanced security features, reduces maintenance overhead, eases integration, and ensures compliance with industry standards.\n\n### Positive Consequences \n* Improved security with features like MFA, regular security updates, and compliance with security protocols.\n* Reduced maintenance effort, allowing the development team to focus on core business functionalities.\n* Seamless integration with existing and future applications using standard APIs and SDKs.\n* Cost savings in the long run by avoiding continuous development and maintenance of a custom solution.\n* Assurance of compliance with industry standards and regulations for user data protection.\n\n### Negative Consequences \n* Initial costs and effort associated with migrating to a new authentication service.\n* Dependency on a third-party provider for critical authentication services.\n* Possible limitations or constraints imposed by the third-party service on customization options.\n\n","GenTime":"2024-08-19 23:31:55"}
{"File Name":"sexual-health-service-finder\/0005-calculate-distance-between-origin-and-result-items-within-the-application.md","Context":"## Context\\nThe move to Azure search has introduced the need to calculate the distance\\nbetween the search point and each result item. Previously, when using\\nElasticsearch, the distance was returned within the query response. Azure\\nsearch does not have this capability, it is currently a\\n[feature request](https:\/\/feedback.azure.com\/forums\/263029-azure-search\/suggestions\/17760211-support-geo-distance-in-select-result).\\n","Decision":"The decision is to calculate the distance between the search point and each\\nresult item within the consuming application i.e. the web app. The calculation\\nfor\\n[great-circle distance](https:\/\/en.wikipedia.org\/wiki\/Great-circle_distance)\\nis well known and available in numerous languages.\\n","tokens":90,"id":4274,"text":"## Context\\nThe move to Azure search has introduced the need to calculate the distance\\nbetween the search point and each result item. Previously, when using\\nElasticsearch, the distance was returned within the query response. Azure\\nsearch does not have this capability, it is currently a\\n[feature request](https:\/\/feedback.azure.com\/forums\/263029-azure-search\/suggestions\/17760211-support-geo-distance-in-select-result).\\n\n\n##Decision\nThe decision is to calculate the distance between the search point and each\\nresult item within the consuming application i.e. the web app. The calculation\\nfor\\n[great-circle distance](https:\/\/en.wikipedia.org\/wiki\/Great-circle_distance)\\nis well known and available in numerous languages.\\n","MatchedIds":[2204,4276,4378,485,102],"Predicted":"## Decision\nTo address the lack of built-in geo-distance calculation in Azure Search, we will implement a post-processing step in our application layer to calculate the distance between the search point and each result item. This approach will ensure the functionality is retained while leveraging Azure Search for querying.\n\n### Positive Consequences\n* Ensures continuity of geo-distance calculation without waiting for the feature request to be implemented by Azure Search.\n* Direct control over the precision and method of distance calculation, allowing for potential optimizations or customizations specific to our application needs.\n* Mitigates potential delays or dependencies on third parties for essential features.\n\n### Negative Consequences\n* Additional computational load on our application layer, which might affect performance and scaling.\n* Slight increase in complexity in the application code, requiring maintenance and potential debugging effort.\n* Possible redundancy in the feature once Azure Search natively supports geo-distance calculations, leading to potential rework.\n\n### Implementation Plan\n1. **Extract Coordinates**: Modify the querying code to extract latitude and longitude for each search result item.\n2. **Haversine Formula**: Implement the Haversine formula or a similar algorithm for distance calculation within the application.\n3. **Post-Processing Step**: Integrate this calculation as a post-processing step after the search results are retrieved from Azure Search but before they are sent to the client.\n4. **Testing and Validation**: Ensure robust testing to validate the accuracy of the distance calculations and assess the impact on overall performance.","GenTime":"2024-08-19 23:32:08"}
{"File Name":"lbh-adrs\/Event-Driven-Architecture-Message-Types.md","Context":"## **Context**\\nAlongside the decision to adopt an event driven architecture, there is a need to define what an event will look like. There are several options for events:\\n- **Thin Events**\\nA thin event consists of the minimum amount of data that is required that will allow a subscriber to retrieve everything it needs. This normally consists of an ID with which to make an API call back to the source publisher to gather the data it needs.\\nThe benefits of thin events are:\\n- The payload is small in size\\n- Data is always up to date as it is retrieved at the point of consumption\\n- If calls to APIs fail due to unavailability of APIs, the message can be replayed\\n- Very little need for event versioning\\nThe downsides are:\\n- Consumers need to make API calls to gather the data they need\\n- **Fat Events**\\nA fat event contains all the data necessary for any subscriber to be able to perform its job.\\nThe benefits of fat events are:\\n- all the data needed for consumer processing is present in the event\\n- no need to make any API calls to retrieve data\\nThe downsides are:\\n- Event payload could grow to be quite big\\n- Data present in the payload may no longer be required by any consumer\\n- It is difficult to version events easily (and multiple versions of the same event may need to be sent for backwards compatibility)\\n**Hybrid Approach**\\nIdeally, we should use thin events wherever possible,as this reduces the complexity around sharing events, updating consumers with new versions of events, etc. However, there are some instances where a thin event might not be possible - notably when updating an activity audit log with details of what has changed. Therefore, the best solution would be to have a thin event that contains an optional message body to hold a specific payload.\\n","Decision":"**Hybrid Approach**\\nThe easiest solution is to use a hybrid approach, with consumers gathering the data they need using API calls. This has the benefit of reducing the need for event versioning, and event payloads that grow in size over time.\\nThe event payload will be:\\n```\\n{\\n\"id\": \"8e648f3d-9556-4896-8400-211cb1c5451b\",\\n\"eventType\": \"personCreated\",\\n\"sourceDomain\": \"person\",\\n\"sourceSystem\": \"personAPI\",\\n\"version\": \"v1\",\\n\"correlationId\": \"f4d541d0-7c07-4524-8296-2d0d50cb58f4\",\\n\"dateTime\": \"2021-05-17T11:59:57.25Z\",\\n\"user\": {\\n\"id\": \"ac703d87-c100-40ec-90a0-dabf183e7377\",\\n\"name\": \"Joe Bloggs\",\\n\"email\": \"joe.bloggs@hackney.gov.uk\"\\n},\\n\"entityId\": \"45c76564-2e38-48f3-bb31-6bab2fef8623\",\\n\"eventBody\": {\\n\"oldData\": {\\n\"optionalProperty1\": \"Property value\",\\n\"optionalProperty2\": \"Property value\",\\n\"optionalProperty3\": \"Property value\"\\n}\\n}\\n}\\n```\\n","tokens":384,"id":2316,"text":"## **Context**\\nAlongside the decision to adopt an event driven architecture, there is a need to define what an event will look like. There are several options for events:\\n- **Thin Events**\\nA thin event consists of the minimum amount of data that is required that will allow a subscriber to retrieve everything it needs. This normally consists of an ID with which to make an API call back to the source publisher to gather the data it needs.\\nThe benefits of thin events are:\\n- The payload is small in size\\n- Data is always up to date as it is retrieved at the point of consumption\\n- If calls to APIs fail due to unavailability of APIs, the message can be replayed\\n- Very little need for event versioning\\nThe downsides are:\\n- Consumers need to make API calls to gather the data they need\\n- **Fat Events**\\nA fat event contains all the data necessary for any subscriber to be able to perform its job.\\nThe benefits of fat events are:\\n- all the data needed for consumer processing is present in the event\\n- no need to make any API calls to retrieve data\\nThe downsides are:\\n- Event payload could grow to be quite big\\n- Data present in the payload may no longer be required by any consumer\\n- It is difficult to version events easily (and multiple versions of the same event may need to be sent for backwards compatibility)\\n**Hybrid Approach**\\nIdeally, we should use thin events wherever possible,as this reduces the complexity around sharing events, updating consumers with new versions of events, etc. However, there are some instances where a thin event might not be possible - notably when updating an activity audit log with details of what has changed. Therefore, the best solution would be to have a thin event that contains an optional message body to hold a specific payload.\\n\n\n##Decision\n**Hybrid Approach**\\nThe easiest solution is to use a hybrid approach, with consumers gathering the data they need using API calls. This has the benefit of reducing the need for event versioning, and event payloads that grow in size over time.\\nThe event payload will be:\\n```\\n{\\n\"id\": \"8e648f3d-9556-4896-8400-211cb1c5451b\",\\n\"eventType\": \"personCreated\",\\n\"sourceDomain\": \"person\",\\n\"sourceSystem\": \"personAPI\",\\n\"version\": \"v1\",\\n\"correlationId\": \"f4d541d0-7c07-4524-8296-2d0d50cb58f4\",\\n\"dateTime\": \"2021-05-17T11:59:57.25Z\",\\n\"user\": {\\n\"id\": \"ac703d87-c100-40ec-90a0-dabf183e7377\",\\n\"name\": \"Joe Bloggs\",\\n\"email\": \"joe.bloggs@hackney.gov.uk\"\\n},\\n\"entityId\": \"45c76564-2e38-48f3-bb31-6bab2fef8623\",\\n\"eventBody\": {\\n\"oldData\": {\\n\"optionalProperty1\": \"Property value\",\\n\"optionalProperty2\": \"Property value\",\\n\"optionalProperty3\": \"Property value\"\\n}\\n}\\n}\\n```\\n","MatchedIds":[775,2309,3123,842,2403],"Predicted":"## Decision\nWe will adopt a **Hybrid Approach** for our event design. This means using thin events as the default, with an optional message body to hold specific payloads when needed. The specific guidelines are as follows:\n\n1. **Thin Events by Default**:\n    - Events will primarily be thin, containing only the minimum necessary data such as an ID or reference to the resource.\n    - This ensures small payload sizes, reduced data processing overhead, and easier event versioning.\n\n2. **Optional Fat Payloads**:\n    - When necessary, especially in cases like activity audit logs where full data context is critical, the event can include an optional message body containing additional data.\n    - This flexibility allows us to balance efficiency with the need for richer data in certain scenarios.\n\n3. **Consumer Handling**:\n    - Event consumers must be equipped to handle thin events by making necessary API calls to gather additional data.\n    - In cases where the optional message body is included, consumers can directly utilize the provided data without further API calls.\n\n4. **Versioning and Backward Compatibility**:\n    - Thin events reduce the complexity involved in versioning and updating event consumers.\n    - For optional message bodies, we will ensure a well-defined structure and clearly documented schemas to facilitate version handling.\n\n5. **Failure Handling and Retries**:\n    - In the event of API call failures when processing thin events, messages can be replayed to ensure eventual consistency.\n    - This robust retry mechanism guarantees high reliability and data integrity.\n\nBy adopting this hybrid approach, we can maintain the agility and performance benefits of thin events while providing the necessary data completeness for specific use cases, striking a balance between efficiency and flexibility.","GenTime":"2024-08-19 23:32:19"}
{"File Name":"news\/0006-a-state-requirement.md","Context":"## Context\\nJust trying to implement bookmarks and I have come across something interesting.\\nWe have a ui element that looks like this:\\n```html\\n<a href=\"javascript:application.bookmarks.add('gbj1jf')\" class=\"bookmark\">bookmark<\/a>\\n```\\nWhich means \"on click, please bookmark the news item with id 'gbj1jf'\".\\nThe bookmarking use case looks like this:\\n```js\\nbookmark = new Bookmark('id-a', 'http:\/\/abc', 'src-rnz');\\nawait application.bookmarks.add(bookmark);\\n```\\nIt is expecting more information for bookmarks so that it can be used later without any further lookups.\\nWe don't wish to have to look it up, especially since it may no longer exist in our sources -- and we don't want to store every single\\narticle just for this purpose.\\nWe don't really want to bake all the information into the link, either, that seems wrong for some reason.\\nAnd we don't really want to add arguments to `application.bookmarks.add`, that also feels wrong.\\n","Decision":"Introduce some idea of state in to `Application` so that it can handle notifications like this.\\nIt seems like simplifying UI elements is a good idea.\\nAnother option is giving this responsibility to the view element. But then how do we let it know?\\n","tokens":230,"id":4323,"text":"## Context\\nJust trying to implement bookmarks and I have come across something interesting.\\nWe have a ui element that looks like this:\\n```html\\n<a href=\"javascript:application.bookmarks.add('gbj1jf')\" class=\"bookmark\">bookmark<\/a>\\n```\\nWhich means \"on click, please bookmark the news item with id 'gbj1jf'\".\\nThe bookmarking use case looks like this:\\n```js\\nbookmark = new Bookmark('id-a', 'http:\/\/abc', 'src-rnz');\\nawait application.bookmarks.add(bookmark);\\n```\\nIt is expecting more information for bookmarks so that it can be used later without any further lookups.\\nWe don't wish to have to look it up, especially since it may no longer exist in our sources -- and we don't want to store every single\\narticle just for this purpose.\\nWe don't really want to bake all the information into the link, either, that seems wrong for some reason.\\nAnd we don't really want to add arguments to `application.bookmarks.add`, that also feels wrong.\\n\n\n##Decision\nIntroduce some idea of state in to `Application` so that it can handle notifications like this.\\nIt seems like simplifying UI elements is a good idea.\\nAnother option is giving this responsibility to the view element. But then how do we let it know?\\n","MatchedIds":[2280,2274,2221,1883,5257],"Predicted":"## Decision\nUse a data attribute in the HTML element to store the necessary bookmark information, and parse these attributes in JavaScript to create the Bookmark object. This approach keeps the information within the DOM element without cluttering the URL or adding arguments to the `application.bookmarks.add` method.\n\n### HTML\n```html\n<a href=\"javascript:void(0);\" class=\"bookmark\" data-id=\"id-a\" data-url=\"http:\/\/abc\" data-source=\"src-rnz\" onclick=\"bookmarkItem(event)\">bookmark<\/a>\n```\n\n### JavaScript\n```js\nfunction bookmarkItem(event) {\n    const element = event.target;\n    const id = element.getAttribute('data-id');\n    const url = element.getAttribute('data-url');\n    const source = element.getAttribute('data-source');\n    \n    const bookmark = new Bookmark(id, url, source);\n    application.bookmarks.add(bookmark);\n}\n```\n\nThis solution keeps the necessary information close to the UI element and avoids changing the `application.bookmarks.add` API. It also ensures that the information required for bookmarking is readily available at the time of the click, and it maintains a clean separation of concerns.","GenTime":"2024-08-19 23:32:24"}
{"File Name":"meadow\/0029-npm.md","Context":"## Context\\nThe latest upgrade of Yarn has introduced issues that we're finding difficult to overcome.\\nSupersedes [11. Yarn](0011-yarn.md)\\n","Decision":"Switch back to `npm` instead of `yarn` in all dev, test, and build environments.\\n","tokens":37,"id":3841,"text":"## Context\\nThe latest upgrade of Yarn has introduced issues that we're finding difficult to overcome.\\nSupersedes [11. Yarn](0011-yarn.md)\\n\n\n##Decision\nSwitch back to `npm` instead of `yarn` in all dev, test, and build environments.\\n","MatchedIds":[2396,1320,4557,3827,3328],"Predicted":"## Decision\nGiven the issues introduced by the latest upgrade of Yarn and our inability to overcome them, we have decided to revert to the last stable version of Yarn that was working for us. We will pin this version in our CI\/CD pipeline to ensure consistency across all development, testing, and production environments.\n\nAdditionally, we will closely monitor the Yarn changelog and user community for updates on these issues. If and when these issues are resolved, we will re-evaluate upgrading to the latest version of Yarn. In the meantime, we will document the specific problems encountered to facilitate a smoother future upgrade.","GenTime":"2024-08-19 23:32:27"}
{"File Name":"libelektra\/error_semantics.md","Context":"## Problem\\nWhile we have a classification of errors and warnings, it remains\\nunclear when plugins actually should emit errors and warnings.\\n","Decision":"Provide guidelines in the form as tutorials, covering:\\n- prefer errors to warnings\\n- that any not understood metadata (e.g. types), should lead to an error\\n- that wrong specifications, like `kdb meta-set \/tests\/ipaddr\/ipv4 check\/ipaddr ipv8` should be rejected\\n- if the value does not confirm **exactly** to the specified type, an error should be emitted\\n(e.g. only `0` or `1` as boolean)\\n- anything else that is beyond the capabilities of a plugin (not implemented),\\nshould lead to an error\\nViolations against these guidelines can be reported as bug and then either:\\n- the bug gets fixed\\n- the plugin get a worse `infos\/status` but still get shipped with 1.0\\n- the plugin gets removed\\n- [Metadata in Spec Namespace](spec_metadata.md)\\n- [Capabilities](capabilities.md)\\n- [Boolean](boolean.md)\\n","tokens":28,"id":1302,"text":"## Problem\\nWhile we have a classification of errors and warnings, it remains\\nunclear when plugins actually should emit errors and warnings.\\n\n\n##Decision\nProvide guidelines in the form as tutorials, covering:\\n- prefer errors to warnings\\n- that any not understood metadata (e.g. types), should lead to an error\\n- that wrong specifications, like `kdb meta-set \/tests\/ipaddr\/ipv4 check\/ipaddr ipv8` should be rejected\\n- if the value does not confirm **exactly** to the specified type, an error should be emitted\\n(e.g. only `0` or `1` as boolean)\\n- anything else that is beyond the capabilities of a plugin (not implemented),\\nshould lead to an error\\nViolations against these guidelines can be reported as bug and then either:\\n- the bug gets fixed\\n- the plugin get a worse `infos\/status` but still get shipped with 1.0\\n- the plugin gets removed\\n- [Metadata in Spec Namespace](spec_metadata.md)\\n- [Capabilities](capabilities.md)\\n- [Boolean](boolean.md)\\n","MatchedIds":[1319,1308,1303,1279,1404],"Predicted":"## Decision\nA new guideline will be established to delineate when plugins should emit errors versus warnings. This guideline will include:\n\n1. **Error Emission:**\n   - Errors should be emitted when the operation cannot be completed successfully and recovery is not possible without user intervention.\n   - Errors should include, but are not limited to:\n     - Configuration issues that prevent the plugin from loading.\n     - Invalid data that leads to an operational failure.\n     - Resource constraints that cannot be mitigated (e.g., out-of-memory).\n\n2. **Warning Emission:**\n   - Warnings should be emitted when there are issues that do not prevent operation but may lead to potential errors if not addressed.\n   - Warnings will be used for:\n     - Deprecation notices for features that will be removed in future versions.\n     - Recoverable issues where the plugin can still function with degraded performance or partial functionality.\n     - Configuration issues that do not halt operation but might lead to suboptimal performance (e.g., missing optional fields).\n\n3. **Documentation:**\n   - Each plugin's documentation will explicitly state the conditions under which errors and warnings will be emitted.\n   - Examples and common error\/warning messages will be included to help users diagnose issues.\n\n4. **Consistency and Review:**\n   - A review process will be implemented to ensure that new and existing plugins adhere to these guidelines.\n   - Automated tests and linting tools will be updated to detect non-compliance with error and warning emission standards. \n\nThese guidelines will provide clarity and consistency for plugin developers and users, improving the overall robustness of the system. Detailed guidelines can be found in the [Error and Warning Emission Guideline](..\/dev\/error-warning-emission-guideline.md).","GenTime":"2024-08-19 23:32:34"}
{"File Name":"modular-monolith-with-ddd\/0016-create-ioc-container-per-module.md","Context":"## Context\\nFor each module, when we process particular Command or Query, we need to resolve a graph of objects. We need to decide how dependencies of objects will be resolved.\\n","Decision":"Solution number 2 - IoC Container per module<\/br>\\nIoC Container per module supports the autonomy of the module and louse coupling so this is a more important aspect for us than duplicated code in some places.\\n","tokens":37,"id":899,"text":"## Context\\nFor each module, when we process particular Command or Query, we need to resolve a graph of objects. We need to decide how dependencies of objects will be resolved.\\n\n\n##Decision\nSolution number 2 - IoC Container per module<\/br>\\nIoC Container per module supports the autonomy of the module and louse coupling so this is a more important aspect for us than duplicated code in some places.\\n","MatchedIds":[1758,647,3722,2502,1031],"Predicted":"## Decision\nWe will use a Dependency Injection (DI) framework to resolve the dependencies of objects. Specifically, we will implement Spring Framework's DI capabilities, leveraging annotations such as `@Autowired` for automatic dependency resolution. By utilizing Spring's support for DI, we will ensure that the object graphs for Commands and Queries are managed efficiently, promoting loose coupling and enhancing testability.\n\nSpring will also facilitate the scope management of our objects, allowing for fine-grained control over the lifecycle of beans. This approach not only decouples the configuration of object dependencies from the application logic but also enhances maintainability and reduces boilerplate code, thus making our module design more robust.","GenTime":"2024-08-19 23:32:37"}
{"File Name":"govuk-aws\/0002-hosting-platforms.md","Context":"## Context\\nWe need to decide upon a platform to host the future GOV.UK infrastructure. Long term, this will be primarily the GOV.UK PaaS but in the interim, we need to converge with that plan and also upgrade and modernise the current infrastructure.\\nGDS policy for hosting of GDS internal services is PaaS first and AWS for anything that can not be run on the PaaS.\\n","Decision":"We are using Amazon Web Services as our hosting provider of choice. This conforms to the [GDS Tech Forum Hosting Guide](https:\/\/github.com\/alphagov\/gds-tech\/pull\/7).\\nWe will initially be using the `eu-west-1` region, Ireland. This region has 3 availability zones and also contains the GDS PaaS which will allow easier sharing and peering.\\n","tokens":86,"id":4041,"text":"## Context\\nWe need to decide upon a platform to host the future GOV.UK infrastructure. Long term, this will be primarily the GOV.UK PaaS but in the interim, we need to converge with that plan and also upgrade and modernise the current infrastructure.\\nGDS policy for hosting of GDS internal services is PaaS first and AWS for anything that can not be run on the PaaS.\\n\n\n##Decision\nWe are using Amazon Web Services as our hosting provider of choice. This conforms to the [GDS Tech Forum Hosting Guide](https:\/\/github.com\/alphagov\/gds-tech\/pull\/7).\\nWe will initially be using the `eu-west-1` region, Ireland. This region has 3 availability zones and also contains the GDS PaaS which will allow easier sharing and peering.\\n","MatchedIds":[2410,1182,1376,1196,4056],"Predicted":"## Decision\nIn alignment with GDS policy, we will host any new services and migrate existing services to the GOV.UK PaaS wherever feasible. For services that cannot be run on GOV.UK PaaS due to technical constraints or other limitations, we will use AWS as the alternate platform. This approach will ensure that we are modernizing our infrastructure while converging with the long-term plan to predominantly use the GOV.UK PaaS. Regular reviews will be conducted to reassess the feasibility of migrating services from AWS to GOV.UK PaaS as the platform evolves.","GenTime":"2024-08-19 23:32:41"}
{"File Name":"libelektra\/cmake_spec.md","Context":"## Problem\\nThe compilation variants of plugins blow up the number of plugins.\\nAdditionally there is the concept of default storage + resolver that is\\nneeded for bootstrapping plugins.\\n","Decision":"Rejected: keep default plugins as-is\\n","tokens":37,"id":1279,"text":"## Problem\\nThe compilation variants of plugins blow up the number of plugins.\\nAdditionally there is the concept of default storage + resolver that is\\nneeded for bootstrapping plugins.\\n\n\n##Decision\nRejected: keep default plugins as-is\\n","MatchedIds":[3860,1318,1278,1302,660],"Predicted":"## Decision\nTo address the issue of an inflated number of plugins due to compilation variants and the complexities of default storage and resolver concepts, we will implement the following strategy:\n\n1. **Introduce Unified Plugin Variants:**\n   * Consolidate similar plugin variants into a more generic and reusable plugin framework to reduce the overall number of plugins.\n   * Use configuration-based approaches within a single plugin to handle different use cases that previously required separate plugins.\n\n2. **Modular Plugin Architecture:**\n   * Decouple the default storage and resolver functionalities from individual plugins.\n   * Implement these functionalities as standalone modules that can be easily attached and detached as needed.\n\n3. **Default Storage + Resolver Interface:**\n   * Define and implement a standard interface for default storage and resolver operations.\n   * Ensure all plugins interact with these interfaces rather than having bespoke implementations. This allows for a more streamlined and consistent bootstrapping process.\n\n4. **Plugin Configuration Management:**\n   * Implement a centralized configuration management system where all plugins can fetch their required configurations.\n   * This can be achieved through a configuration file or service registry, ensuring consistency and reducing the need for multiple plugin variants.\n\n5. **Build Optimization:**\n   * Re-evaluate and optimize the build process to exclude unnecessary plugins and their variants.\n   * Implement build-time flags or environment variables to selectively include only the required plugins for a specific build, reducing compilation time and overhead.\n\n6. **Documentation and Guidelines:**\n   * Update documentation to reflect the new plugin architecture, including usage patterns for the unified plugins and the new modular default storage and resolver modules.\n   * Provide clear guidelines for developers on how to extend and manage plugins within this new architecture to maintain consistency and reduce redundancy.\n\nBy adopting these measures, we will streamline the plugin system, reduce the number of plugin variants, and create a more maintainable and efficient architecture for bootstrapping plugins through a centralized and modular approach.","GenTime":"2024-08-19 23:32:47"}
{"File Name":"hmpps-interventions-service\/0002-represent-validation-errors-at-field-level.md","Context":"## Context\\nFor any user interface or client relying on our API, we need to define how we represent what was wrong with\\ninvalid client requests.\\n","Decision":"We will use field-level error validation.\\nWe will use meaningful codes per field.\\nExample:\\n```json\\n{\\n\"status\": 400,\\n\"error\": \"validation error\",\\n\"message\": \"draft referral update invalid\",\\n\"validationErrors\": [\\n{\\n\"field\": \"serviceUser.crn\",\\n\"error\": \"FIELD_CANNOT_BE_CHANGED\"\\n}\\n]\\n}\\n```\\n","tokens":31,"id":3159,"text":"## Context\\nFor any user interface or client relying on our API, we need to define how we represent what was wrong with\\ninvalid client requests.\\n\n\n##Decision\nWe will use field-level error validation.\\nWe will use meaningful codes per field.\\nExample:\\n```json\\n{\\n\"status\": 400,\\n\"error\": \"validation error\",\\n\"message\": \"draft referral update invalid\",\\n\"validationErrors\": [\\n{\\n\"field\": \"serviceUser.crn\",\\n\"error\": \"FIELD_CANNOT_BE_CHANGED\"\\n}\\n]\\n}\\n```\\n","MatchedIds":[1180,4536,2181,847,4424],"Predicted":"## Decision\nWe will follow the [RFC 7807 - Problem Details for HTTP APIs](https:\/\/tools.ietf.org\/html\/rfc7807) specification to standardize the representation of error information in our API.\n\n### Error Response Structure\nEach error response will be a JSON object with the following structure:\n\n* **type** (string): A URI reference that identifies the problem type. This URI should provide human-readable documentation for the problem type when dereferenced.\n* **title** (string): A short, human-readable summary of the problem type. It **SHOULD NOT** change from occurrence to occurrence.\n* **status** (integer): The HTTP status code generated by the origin server for this occurrence of the problem.\n* **detail** (string): A human-readable explanation specific to this occurrence of the problem.\n* **instance** (string): A URI reference that identifies the specific occurrence of the problem. It may or may not yield further information if dereferenced.\n\n### Example\nFor example, an error response for an invalid parameter might look like this:\n```json\n{\n  \"type\": \"https:\/\/example.com\/probs\/invalid-params\",\n  \"title\": \"Invalid Parameters\",\n  \"status\": 400,\n  \"detail\": \"The parameters 'id' and 'name' are required.\",\n  \"instance\": \"\/api\/v1\/resource\/12345\"\n}\n```\n\n### Custom Fields\nAdditional fields that are application-specific may also be included in the error object to provide more context.\n\n### Integration\nAll client applications and user interfaces relying on our API must be designed to handle error responses that conform to this structure, ensuring a consistent and predictable error handling mechanism across the system.","GenTime":"2024-08-19 23:32:53"}
{"File Name":"unit-e-project\/ADR-0007.md","Context":"## Context\\nWe need a way to provide initial supply. At the moment it is not possible to spend the genesis block coinbase because\\neven if the block is added to the index, its transactions are not added to the txdb.\\n","Decision":"We want to change the code so that we are able to create an initial supply but we MUST only use the coinbase of the\\ngenesis block and MUST NOT resort to further `imports` in subsequents blocks like for example Particl did.\\nAll the coins of the initial supply MUST be minted in the coinbase transaction of the genesis block.\\n","tokens":50,"id":3764,"text":"## Context\\nWe need a way to provide initial supply. At the moment it is not possible to spend the genesis block coinbase because\\neven if the block is added to the index, its transactions are not added to the txdb.\\n\n\n##Decision\nWe want to change the code so that we are able to create an initial supply but we MUST only use the coinbase of the\\ngenesis block and MUST NOT resort to further `imports` in subsequents blocks like for example Particl did.\\nAll the coins of the initial supply MUST be minted in the coinbase transaction of the genesis block.\\n","MatchedIds":[4099,33,1558,1561,4354],"Predicted":"## Decision\n### Initial Supply via Pre-allocations in Genesis File\n\nTo address the need for providing initial supply, a decision has been made to pre-allocate the initial coinbase allocations directly within the `genesis.json` configuration file. This ensures that initial supply can be distributed to designated accounts without relying on transactions from the genesis block, which are not indexed in the txdb.\n\n### Implementation Details\n\n1. **Genesis File Pre-allocations**: \n   The `genesis.json` file will include a new section, `allocations`, where the initial supply can be specified for desired accounts. This avoids any dependency on the coinbase transactions of the genesis block and directly injects the initial supply.\n\n   ```json\n   {\n     \"allocations\": [\n       {\n         \"address\": \"account_address_1\",\n         \"amount\": \"100000\"\n       },\n       {\n         \"address\": \"account_address_2\",\n         \"amount\": \"50000\"\n       }\n     ]\n   }\n   ```\n\n2. **Applying Allocations During Initialization**:\n   During the chain initialization phase, a new handler will process the `allocations` specified in the `genesis.json`. This handler will:\n   - Parse the `allocations` field.\n   - Inject the specified amounts into the respective accounts' balances within the state database.\n\n   Example implementation in pseudocode:\n   ```go\n   func InitializeChain(ctx Context, genesis GenesisState) error {\n       for _, alloc := range genesis.Allocations {\n           err := stateDB.SetBalance(alloc.Address, alloc.Amount)\n           if err != nil {\n               return err\n           }\n       }\n       ...\n   }\n   ```\n\n3. **Genesis State Struct Changes**:\n   Modify the `GenesisState` struct to include the new `allocations` field:\n\n   ```go\n   type GenesisState struct {\n       ...\n       Allocations []Allocation `json:\"allocations\"`\n   }\n\n   type Allocation struct {\n       Address string `json:\"address\"`\n       Amount  string `json:\"amount\"`\n   }\n   ```\n   \n4. **Validation**:\n   Before applying the allocations, validation checks will ensure the addresses and amounts conform to expected formats and constraints. This step guarantees that malformed `genesis.json` files do not disrupt the initialization process.\n\n5. **Removing Reliance on Transaction Database (txdb)**:\n   Since the initial supply allocations are directly set during the chain initialization phase, there is no need to rely on transaction indexing within the txdb for initial supply distribution. This simplifies the process and eliminates the current bottleneck.\n\n### Benefits\n- **Immediate Availability**: By directly allocating the initial supply during chain initialization, all specified accounts have their balances set correctly without needing to process transactions.\n- **Reduced Complexity**: Removes the need for manipulating the genesis block's coinbase transactions in the txdb, thus simplifying the initialization logic and reducing potential points of failure.\n- **Flexibility**: Allows for easy modification and extension of initial supply distributions via the `genesis.json` file, providing a clear and maintainable approach.\n\nBy adopting this approach, we ensure that the initial supply mechanism is robust, reliable, and straightforward.","GenTime":"2024-08-19 23:33:03"}
{"File Name":"ehoks-ui\/0002-use-finnish-as-the-domain-language.md","Context":"## Context\\nThere are currently mixed conventions of translating domain words. For example mobx-state-tree-model properties are\\nin Finnish but react component props in English even though data might be exactly the same.\\n","Decision":"We will use Finnish as the domain language (e.g. osaamisenHankkimistapa, koulutuksenJarjestaja) and English when\\nthe word is not directly related to eHOKS domain. Words are refactored gradually so there might exist mixed\\nconventions quite some time.\\nHere is one expample:\\n```typescript\\ninterface LearningEventProps {\\nclassName?: string\\ntitle?: React.ReactNode\\nisDemonstration?: boolean\\nsize?: \"small\" | \"large\"\\ndescription?: string\\nstartDate?: string\\nendDate?: string\\nperiodSpecifier?: string\\ndemonstrationEnviromentDescription?: string\\n}\\n```\\nclassName, title, size, description, startDate, endDate are technical or generic words and should be in English.\\nDemonstration and demonstrationEnvironment are domain and datamodel words naytto and nayttoYmparisto and should be in\\nFinnish. PeriodSpecifier is straight from the data model and thus should be ajanjaksonTarkenne. LearningEvent is used for\\nboth osaamisenHankkiminen and osaamisenOsoittaminen and there isn't domain word to describe these both so learningEvent\\nshould stay as is, although probably best option would be to split learningEvent component to OsaamisenHankkiminen and\\nOsaamisenOsoittaminen components.\\nExample fixed:\\n```typescript\\ninterface LearningEventProps {\\nclassName?: string\\ntitle?: React.ReactNode\\nisNaytto?: boolean\\nsize?: \"small\" | \"large\"\\ndescription?: string\\nstartDate?: string\\nendDate?: string\\najanjaksonTarkenne?: string\\nnayttoymparistoDescription?: string\\n}\\n```\\n","tokens":41,"id":598,"text":"## Context\\nThere are currently mixed conventions of translating domain words. For example mobx-state-tree-model properties are\\nin Finnish but react component props in English even though data might be exactly the same.\\n\n\n##Decision\nWe will use Finnish as the domain language (e.g. osaamisenHankkimistapa, koulutuksenJarjestaja) and English when\\nthe word is not directly related to eHOKS domain. Words are refactored gradually so there might exist mixed\\nconventions quite some time.\\nHere is one expample:\\n```typescript\\ninterface LearningEventProps {\\nclassName?: string\\ntitle?: React.ReactNode\\nisDemonstration?: boolean\\nsize?: \"small\" | \"large\"\\ndescription?: string\\nstartDate?: string\\nendDate?: string\\nperiodSpecifier?: string\\ndemonstrationEnviromentDescription?: string\\n}\\n```\\nclassName, title, size, description, startDate, endDate are technical or generic words and should be in English.\\nDemonstration and demonstrationEnvironment are domain and datamodel words naytto and nayttoYmparisto and should be in\\nFinnish. PeriodSpecifier is straight from the data model and thus should be ajanjaksonTarkenne. LearningEvent is used for\\nboth osaamisenHankkiminen and osaamisenOsoittaminen and there isn't domain word to describe these both so learningEvent\\nshould stay as is, although probably best option would be to split learningEvent component to OsaamisenHankkiminen and\\nOsaamisenOsoittaminen components.\\nExample fixed:\\n```typescript\\ninterface LearningEventProps {\\nclassName?: string\\ntitle?: React.ReactNode\\nisNaytto?: boolean\\nsize?: \"small\" | \"large\"\\ndescription?: string\\nstartDate?: string\\nendDate?: string\\najanjaksonTarkenne?: string\\nnayttoymparistoDescription?: string\\n}\\n```\\n","MatchedIds":[3126,2270,3715,2183,2667],"Predicted":"## Decision\nTo ensure consistency and maintainability in our codebase, we will standardize the language used across all domain words. Given that our team predominantly uses English and it is widely understood by developers globally, we will adopt English for all domain words, including mobx-state-tree-model properties and react component props. This change will be documented in our `BestPractices.md` file, and any deviations should be flagged for correction during code reviews.","GenTime":"2024-08-19 23:33:05"}
{"File Name":"ng-kaart\/0005-links-in-tabel-tonen.md","Context":"## Context\\nURL's worden gestyled zodat deze clickable zijn in het Identify paneel. We willen graag dat ze ook bruikbaar zijn in de tabel.\\nUit de lagen.json heb ik afgeleid dat sommige van deze links veldtype URL hebben en andere veldtype String.\\nWe willen deze URL's graag op dynamische wijze tonen in de tabel (niet gewoon de URL zelf, maar ook niet telkens gewoon LINK).\\nEen groot deel kan getoond worden op basis van de waarde in een ander veld bv dossiernummer bij AV.\\nVoor andere lagen mag er een vaste string getoond worden.\\nEn voor sommige lagen een combinatie van 2 velden als label.\\n","Decision":"Momenteel detecteren we links automatisch door naar de start van de string of naar veldtype te kijken. Als het met http of https begint of\\nveldtype 'url' heeft beelden we het af als een link.\\nWe houden voor de tabel vanaf nu rekening met 'html' veld. Indien ingevuld, dan gebruiken we de inhoud van dat html veld om de weergave af te beelden.\\nWe staan toe dat er tokens instaan om bepaalde velden dynamisch\\nin te vullen. Bvb:\\n{ \"isBasisVeld\": true, \"label\": \"Rapport\", \"naam\": \"rapport_url_1\", \"veldType\": \"url\", \"html\": \"{{periode_1_begin}} - {{periode_1_einde}}\" },\\nIndien 'html' leeg is, nemen we de waarde in 'label'.\\nMet het veld \"constante\" moet ook rekening gehouden worden, dat moet soms genomen worden ipv de waarde van het veld. Bvb:\\n{\\n\"isBasisVeld\": true,\\n\"label\": \"Open in Werf\",\\n\"naam\": \"externeurl\",\\n\"veldType\": \"string\",\\n\"html\": \"{werfid}\",\\n\"constante\": \"https:\/\/{domain.name}\/werf\/schermen\/werf\/{werfid};werf=werf%2Fapi%2Fwerf%2F{werfid}\"\\n},\\n","tokens":172,"id":2074,"text":"## Context\\nURL's worden gestyled zodat deze clickable zijn in het Identify paneel. We willen graag dat ze ook bruikbaar zijn in de tabel.\\nUit de lagen.json heb ik afgeleid dat sommige van deze links veldtype URL hebben en andere veldtype String.\\nWe willen deze URL's graag op dynamische wijze tonen in de tabel (niet gewoon de URL zelf, maar ook niet telkens gewoon LINK).\\nEen groot deel kan getoond worden op basis van de waarde in een ander veld bv dossiernummer bij AV.\\nVoor andere lagen mag er een vaste string getoond worden.\\nEn voor sommige lagen een combinatie van 2 velden als label.\\n\n\n##Decision\nMomenteel detecteren we links automatisch door naar de start van de string of naar veldtype te kijken. Als het met http of https begint of\\nveldtype 'url' heeft beelden we het af als een link.\\nWe houden voor de tabel vanaf nu rekening met 'html' veld. Indien ingevuld, dan gebruiken we de inhoud van dat html veld om de weergave af te beelden.\\nWe staan toe dat er tokens instaan om bepaalde velden dynamisch\\nin te vullen. Bvb:\\n{ \"isBasisVeld\": true, \"label\": \"Rapport\", \"naam\": \"rapport_url_1\", \"veldType\": \"url\", \"html\": \"{{periode_1_begin}} - {{periode_1_einde}}\" },\\nIndien 'html' leeg is, nemen we de waarde in 'label'.\\nMet het veld \"constante\" moet ook rekening gehouden worden, dat moet soms genomen worden ipv de waarde van het veld. Bvb:\\n{\\n\"isBasisVeld\": true,\\n\"label\": \"Open in Werf\",\\n\"naam\": \"externeurl\",\\n\"veldType\": \"string\",\\n\"html\": \"{werfid}\",\\n\"constante\": \"https:\/\/{domain.name}\/werf\/schermen\/werf\/{werfid};werf=werf%2Fapi%2Fwerf%2F{werfid}\"\\n},\\n","MatchedIds":[1122,3126,3815,4931,2286],"Predicted":"## Decision\nWe will implement a dynamic link rendering functionality that caters to different requirements based on the field types and layer-specific rules. The decisions are as follows:\n\n1. **For fields with type URL**:\n    - Directly render the URL as clickable links in the table, showing either the URL itself or a contextual label derived from an associated field value. For example, for AV layers, use the `dossiernummer` as the display text for the clickable link.\n\n2. **For fields with type String**:\n    - Dynamically determine if the string represents a URL. If it does, render it as a clickable link.\n    - For other cases, use a predefined fixed string as the display text for the link. \n\n3. **Combination Labels**:\n    - For layers requiring labels derived from multiple fields, concatenate the values from the specified fields to form the link's display text. Ensure this concatenation is performed in a readable manner (e.g., \"Field1 - Field2\").\n  \n4. **Configuration and Extensibility**:\n    - Define these rules in a configuration file (e.g., `dynamic_link_rules.json`) to allow easy updates and extensions without changing application code.\n    - The configuration should map layer names to their specific display rules, detailing how links should be shown for each layer.\n\n5. **Fallback Mechanism**:\n    - Implement a fallback mechanism to ensure that if no specific rule applies, the system defaults to displaying the URL itself for better usability.\n\nBy following these guidelines, we ensure that URL fields are presented in a user-friendly way, making them more accessible and easier to navigate directly from the table.","GenTime":"2024-08-19 23:33:11"}
{"File Name":"SearchServices\/0007-message-driven-content-tracker.md","Context":"## Context\\nThe ability to search on content requires a content extraction process. This relies on repo getting the document, passing it to one or multiple transformers, and finally returning the plain text content. This process does not scale as the embedded transformation is unable to cope with large volumes or large documents. Embedded transformations in general come with multiple problems, security related and scaling, which led to the introduction of the transformation service with 6.1\\nSince transformations to text for content indexing makes up a major portion of the transformation workload, it has always been intended to move these transformations to the new transformation service as well.\\nThe following are the suggested approaches to indexing with Transform Service:\\n* Refactor the current V0 API (in use by Search Services) to make use of RenditionService2.\\n* Introduce a new microservice that sits between Solr and the transformation service. The content is off loaded to the transformation service asynchronously while providing the same synchronous API for Search Services.\\n* Search Service to use the get rendition V1 Public API.\\n* New content tracker that communicates with the repository asynchronously by messages.\\n","Decision":"Based on the group discussion and design reviews, we have agreed to go with the asynchronous content tracker.\\nIn this design the Search Services will place a request in the message queue for the Repo to consume.\\nThe message will contain the NodeId and Solr identifier (name of the instance or Solr shard).\\nOnce the message is consumed by Repo it will start the process to obtain the text for the content.\\nWhen the content is ready a response message will be placed in the queue for Search Services to consume.\\nThe new content tracker will monitor the response queue and consume incoming messages. The message we expect to see in the queue will consist of an identifier, status and a URL. The status of the event can be used for handling errors. The handling of such errors prompting an abort or retry will be finalised during user story creation.\\nOn a successful completion the new content tracker will use the URL to obtain the content and retrieve the text for indexing.\\nWe use a URL in the response message rather than an identifier so that the repository can choose where to store the intermediate content at its own discretion. This will also provide the ability to leverage direct access URLs to cloud storage in the future (e.g. S3 signed URLs).\\nThe benefits of this solution gives ability to index content asynchronously. Unlike the current way which is based on a synchronous call to Repo using HTTP. This solution allows Alfresco to scale the transformation and adds the ability to index more content.\\n![Component Diagram](\/search-services\/alfresco-search\/doc\/architecture\/decisions\/diagrams\/AsyncContentTrackerComponentDiagram.png)\\nThe other options have been considered but did not full fill the requirements.\\nRefactor the current V0 API (in use by Search Services) to make use of RenditionService2:\\nThe thread in the repository will still be blocked. Although the new transform service has a higher throughput, it can have a slightly longer delay. This blocks HTTP threads even longer, or they could even time out.  Using async HTTP introduced with servlet 3.0 has been considered, but this would need to be implemented throughout the entire webscript framework.\\nUsing V1 API requires an authentication for SearchServices, which needs to be configured. There is currently no way for a system to call the V1 API without creating a new user. Creating a new user to represent the system is not the correct way to integrate systems and services. In addition, the V1 API uses the renditions for text which covered below.\\nUsing renditions for text extraction:\\nRenditions are stored long term in the repository as nodes. Using this mechanism for ephemeral text extractions would require ta new retention mechanism for renditions. All of this would put additional workload on the node tree, which defeats the design goal of handling high workloads.\\nIntroduce a new microservice:\\nThis has only been considered as an interim solution if it is not possible to change the content tracker in Search Services. It is essentially the solution above (blocking the sync request from Search Services until transformation is ready) but moved to its own micro service. This solution is slightly better as it does not block repository threads but suffers from the same problems.\\nCreating a microservice for the content tracker adds complexity which adds little benefit as part of this delivery. There are benefits for making the trackers into a microservice, but this would require a significant amount of work. The deployment of Search Services will become complicated as we would have to deploy SearchServices and a micorservice for the new ContentTracker.\\nThe current ContentTracker will remain so that the Community version continues to work with SearchServices.\\nThe V0 content tracking webscript and the ContentTracker will be removed with the next major upgrade in favour of the message base API. At this stage both Enterprise and Community will make use of the message base API. Please note that the communication via message queues between Repo and Search Service, will be configured by default in the ACS (reference) deployment templates. As for the Alfresco 6.x and SearchServices 1.x distribution zip files, will default to the legacy configuration. This would provide the customer different options to upgrade, as we have an overlap between versions.\\nAdding a new content tracker based on a message queue is the preferred solution as this provides the benefits of using the Transform Service.\\n","tokens":227,"id":5123,"text":"## Context\\nThe ability to search on content requires a content extraction process. This relies on repo getting the document, passing it to one or multiple transformers, and finally returning the plain text content. This process does not scale as the embedded transformation is unable to cope with large volumes or large documents. Embedded transformations in general come with multiple problems, security related and scaling, which led to the introduction of the transformation service with 6.1\\nSince transformations to text for content indexing makes up a major portion of the transformation workload, it has always been intended to move these transformations to the new transformation service as well.\\nThe following are the suggested approaches to indexing with Transform Service:\\n* Refactor the current V0 API (in use by Search Services) to make use of RenditionService2.\\n* Introduce a new microservice that sits between Solr and the transformation service. The content is off loaded to the transformation service asynchronously while providing the same synchronous API for Search Services.\\n* Search Service to use the get rendition V1 Public API.\\n* New content tracker that communicates with the repository asynchronously by messages.\\n\n\n##Decision\nBased on the group discussion and design reviews, we have agreed to go with the asynchronous content tracker.\\nIn this design the Search Services will place a request in the message queue for the Repo to consume.\\nThe message will contain the NodeId and Solr identifier (name of the instance or Solr shard).\\nOnce the message is consumed by Repo it will start the process to obtain the text for the content.\\nWhen the content is ready a response message will be placed in the queue for Search Services to consume.\\nThe new content tracker will monitor the response queue and consume incoming messages. The message we expect to see in the queue will consist of an identifier, status and a URL. The status of the event can be used for handling errors. The handling of such errors prompting an abort or retry will be finalised during user story creation.\\nOn a successful completion the new content tracker will use the URL to obtain the content and retrieve the text for indexing.\\nWe use a URL in the response message rather than an identifier so that the repository can choose where to store the intermediate content at its own discretion. This will also provide the ability to leverage direct access URLs to cloud storage in the future (e.g. S3 signed URLs).\\nThe benefits of this solution gives ability to index content asynchronously. Unlike the current way which is based on a synchronous call to Repo using HTTP. This solution allows Alfresco to scale the transformation and adds the ability to index more content.\\n![Component Diagram](\/search-services\/alfresco-search\/doc\/architecture\/decisions\/diagrams\/AsyncContentTrackerComponentDiagram.png)\\nThe other options have been considered but did not full fill the requirements.\\nRefactor the current V0 API (in use by Search Services) to make use of RenditionService2:\\nThe thread in the repository will still be blocked. Although the new transform service has a higher throughput, it can have a slightly longer delay. This blocks HTTP threads even longer, or they could even time out.  Using async HTTP introduced with servlet 3.0 has been considered, but this would need to be implemented throughout the entire webscript framework.\\nUsing V1 API requires an authentication for SearchServices, which needs to be configured. There is currently no way for a system to call the V1 API without creating a new user. Creating a new user to represent the system is not the correct way to integrate systems and services. In addition, the V1 API uses the renditions for text which covered below.\\nUsing renditions for text extraction:\\nRenditions are stored long term in the repository as nodes. Using this mechanism for ephemeral text extractions would require ta new retention mechanism for renditions. All of this would put additional workload on the node tree, which defeats the design goal of handling high workloads.\\nIntroduce a new microservice:\\nThis has only been considered as an interim solution if it is not possible to change the content tracker in Search Services. It is essentially the solution above (blocking the sync request from Search Services until transformation is ready) but moved to its own micro service. This solution is slightly better as it does not block repository threads but suffers from the same problems.\\nCreating a microservice for the content tracker adds complexity which adds little benefit as part of this delivery. There are benefits for making the trackers into a microservice, but this would require a significant amount of work. The deployment of Search Services will become complicated as we would have to deploy SearchServices and a micorservice for the new ContentTracker.\\nThe current ContentTracker will remain so that the Community version continues to work with SearchServices.\\nThe V0 content tracking webscript and the ContentTracker will be removed with the next major upgrade in favour of the message base API. At this stage both Enterprise and Community will make use of the message base API. Please note that the communication via message queues between Repo and Search Service, will be configured by default in the ACS (reference) deployment templates. As for the Alfresco 6.x and SearchServices 1.x distribution zip files, will default to the legacy configuration. This would provide the customer different options to upgrade, as we have an overlap between versions.\\nAdding a new content tracker based on a message queue is the preferred solution as this provides the benefits of using the Transform Service.\\n","MatchedIds":[1205,282,2342,102,714],"Predicted":"## Decision\nWe will introduce a new microservice that sits between Solr and the transformation service. This approach provides a balance between scalability, security, and ease of integration. Here's the rationale behind the decision:\n\n1. **Scalability**: By offloading the transformation workload to a dedicated transformation service, the system can better handle large volumes and large documents. The microservice will asynchronously handle the content transformation, allowing for improved scaling when compared to an embedded transformation within the repo.\n\n2. **Security**: This design isolates the transformation logic from the main content repository and search services, reducing potential security vulnerabilities associated with embedded transformations. The transformation microservice can be hardened and secured independently, ensuring a robust transformation pipeline.\n\n3. **Synchronous API for Search Services**: The new microservice will provide the same synchronous API to the Search Services, ensuring minimal changes and disruption to existing systems. This maintains the same level of service while reaping the benefits of asynchronous processing.\n\n4. **Operational Simplicity**: The new microservice simplifies the migration path by offloading the heavy lifting to an external service without requiring significant changes to the Search Services. This approach also decouples the transformation process from the content repository.\n\n### Implementation Plan:\n1. **Develop the Microservice**: Build the transformation microservice that interacts asynchronously with the transformation service and transforms content as needed for indexing.\n2. **Modify Solr Integration**: Adjust Solr configuration to use the new microservice for content transformation.\n3. **Migration Strategy**: Implement a phased rollout to gradually migrate existing workloads to the new microservice, ensuring stable and consistent performance.\n4. **Monitoring and Logging**: Set up comprehensive monitoring and logging to ensure smooth operation and to quickly identify and handle any issues that arise.\n5. **Performance Testing**: Conduct extensive performance testing to validate the scalability and efficiency of the new microservice.\n\nThis approach ensures a robust, scalable, and secure transformation and indexing pipeline, capable of handling the current and future transformation workloads efficiently.","GenTime":"2024-08-19 23:33:18"}
{"File Name":"react-transcript-editor\/2018-10-05-components-comunication.md","Context":"## Context and Problem Statement\\nDeciding how to have the internal components of the Transcript Editor communicate with each other.\\n## Decision Drivers <!-- optional -->\\n* Simple and straightforward way to reason around passing data between components\\n* Extensible anticipating use cases when using the component \"in the wild\" and having internal info accessible\/when if needed.\\n","Decision":"* Simple and straightforward way to reason around passing data between components\\n* Extensible anticipating use cases when using the component \"in the wild\" and having internal info accessible\/when if needed.\\n<!-- Chosen option: \"[option 1]\", because [justification. e.g., only option, which meets k.o. criterion decision driver | which resolves force force | \u2026 | comes out best (see below)]. -->\\nStill evaluating, leaning torwards some light refactoring to enable parent component, option 1 to keep things simple.\\n<!-- ### Positive Consequences\\n* [e.g., improvement of quality attribute satisfaction, follow-up decisions required, \u2026]\\n* \u2026\\n### Negative consequences\\n* [e.g., compromising quality attribute, follow-up decisions required, \u2026]\\n* \u2026 -->\\n","tokens":71,"id":3187,"text":"## Context and Problem Statement\\nDeciding how to have the internal components of the Transcript Editor communicate with each other.\\n## Decision Drivers <!-- optional -->\\n* Simple and straightforward way to reason around passing data between components\\n* Extensible anticipating use cases when using the component \"in the wild\" and having internal info accessible\/when if needed.\\n\n\n##Decision\n* Simple and straightforward way to reason around passing data between components\\n* Extensible anticipating use cases when using the component \"in the wild\" and having internal info accessible\/when if needed.\\n<!-- Chosen option: \"[option 1]\", because [justification. e.g., only option, which meets k.o. criterion decision driver | which resolves force force | \u2026 | comes out best (see below)]. -->\\nStill evaluating, leaning torwards some light refactoring to enable parent component, option 1 to keep things simple.\\n<!-- ### Positive Consequences\\n* [e.g., improvement of quality attribute satisfaction, follow-up decisions required, \u2026]\\n* \u2026\\n### Negative consequences\\n* [e.g., compromising quality attribute, follow-up decisions required, \u2026]\\n* \u2026 -->\\n","MatchedIds":[160,4167,5244,3031,4689],"Predicted":"## Decision\nChosen option: Using a state management library like Redux to manage the state and facilitate communication between internal components of the Transcript Editor.\n\n### Positive Consequences\n* **Simple and straightforward data flow:** Redux provides a predictable state container that makes it easy to understand and manage the data flow within the application.\n* **Centralized state management:** By maintaining the application state in a central store, we avoid issues like prop drilling and can easily access the state from any component.\n* **Extensibility:** Redux is highly extensible. Middleware can be added to handle side effects, logging, routing, etc.\n* **Debugging:** Redux DevTools offers excellent support for tracking state changes, which simplifies debugging and makes the application's behavior more transparent.\n* **Consistency:** By following Redux's architecture, we ensure consistency across the application state, reducing the risk of hard-to-track bugs.\n\n### Negative Consequences\n* **Boilerplate code:** Redux can sometimes introduce a significant amount of boilerplate code for actions, reducers, and the store, which might initially slow down development.\n* **Learning curve:** For developers who are not familiar with Redux, there might be a steep learning curve to understand its concepts and best practices.\n* **Overhead:** For smaller projects or simpler component communication needs, Redux might add unnecessary complexity and performance overhead.\n\n### Justification\nWe chose Redux because it aligns best with our decision drivers. It provides a simple and straightforward way to manage and reason around passing data between components while being extensible enough to handle various use cases and internal information access needs. Despite the learning curve and additional boilerplate, the benefits of predictable state management, ease of debugging, and consistency outweigh the negatives.","GenTime":"2024-08-19 23:33:24"}
{"File Name":"tendermint\/adr-015-crypto-encoding.md","Context":"## Context\\nWe must standardize our method for encoding public keys and signatures on chain.\\nCurrently we amino encode the public keys and signatures.\\nThe reason we are using amino here is primarily due to ease of support in\\nparsing for other languages.\\nWe don't need its upgradability properties in cryptosystems, as a change in\\nthe crypto that requires adapting the encoding, likely warrants being deemed\\na new cryptosystem.\\n(I.e. using new public parameters)\\n","Decision":"### Public keys\\nFor public keys, we will continue to use amino encoding on the canonical\\nrepresentation of the pubkey.\\n(Canonical as defined by the cryptosystem itself)\\nThis has two significant drawbacks.\\nAmino encoding is less space-efficient, due to requiring support for upgradability.\\nAmino encoding support requires forking protobuf and adding this new interface support\\noption in the language of choice.\\nThe reason for continuing to use amino however is that people can create code\\nmore easily in languages that already have an up to date amino library.\\nIt is possible that this will change in the future, if it is deemed that\\nrequiring amino for interacting with Tendermint cryptography is unnecessary.\\nThe arguments for space efficiency here are refuted on the basis that there are\\nfar more egregious wastages of space in the SDK.\\nThe space requirement of the public keys doesn't cause many problems beyond\\nincreasing the space attached to each validator \/ account.\\nThe alternative to using amino here would be for us to create an enum type.\\nSwitching to just an enum type is worthy of investigation post-launch.\\nFor reference, part of amino encoding interfaces is basically a 4 byte enum\\ntype definition.\\nEnum types would just change that 4 bytes to be a variant, and it would remove\\nthe protobuf overhead, but it would be hard to integrate into the existing API.\\n### Signatures\\nSignatures should be switched to be `[]byte`.\\nSpatial efficiency in the signatures is quite important,\\nas it directly affects the gas cost of every transaction,\\nand the throughput of the chain.\\nSignatures don't need to encode what type they are for (unlike public keys)\\nsince public keys must already be known.\\nTherefore we can validate the signature without needing to encode its type.\\nWhen placed in state, signatures will still be amino encoded, but it will be the\\nprimitive type `[]byte` getting encoded.\\n#### Ed25519\\nUse the canonical representation for signatures.\\n#### Secp256k1\\nThere isn't a clear canonical representation here.\\nSignatures have two elements `r,s`.\\nThese bytes are encoded as `r || s`, where `r` and `s` are both exactly\\n32 bytes long, encoded big-endian.\\nThis is basically Ethereum's encoding, but without the leading recovery bit.\\n","tokens":100,"id":1962,"text":"## Context\\nWe must standardize our method for encoding public keys and signatures on chain.\\nCurrently we amino encode the public keys and signatures.\\nThe reason we are using amino here is primarily due to ease of support in\\nparsing for other languages.\\nWe don't need its upgradability properties in cryptosystems, as a change in\\nthe crypto that requires adapting the encoding, likely warrants being deemed\\na new cryptosystem.\\n(I.e. using new public parameters)\\n\n\n##Decision\n### Public keys\\nFor public keys, we will continue to use amino encoding on the canonical\\nrepresentation of the pubkey.\\n(Canonical as defined by the cryptosystem itself)\\nThis has two significant drawbacks.\\nAmino encoding is less space-efficient, due to requiring support for upgradability.\\nAmino encoding support requires forking protobuf and adding this new interface support\\noption in the language of choice.\\nThe reason for continuing to use amino however is that people can create code\\nmore easily in languages that already have an up to date amino library.\\nIt is possible that this will change in the future, if it is deemed that\\nrequiring amino for interacting with Tendermint cryptography is unnecessary.\\nThe arguments for space efficiency here are refuted on the basis that there are\\nfar more egregious wastages of space in the SDK.\\nThe space requirement of the public keys doesn't cause many problems beyond\\nincreasing the space attached to each validator \/ account.\\nThe alternative to using amino here would be for us to create an enum type.\\nSwitching to just an enum type is worthy of investigation post-launch.\\nFor reference, part of amino encoding interfaces is basically a 4 byte enum\\ntype definition.\\nEnum types would just change that 4 bytes to be a variant, and it would remove\\nthe protobuf overhead, but it would be hard to integrate into the existing API.\\n### Signatures\\nSignatures should be switched to be `[]byte`.\\nSpatial efficiency in the signatures is quite important,\\nas it directly affects the gas cost of every transaction,\\nand the throughput of the chain.\\nSignatures don't need to encode what type they are for (unlike public keys)\\nsince public keys must already be known.\\nTherefore we can validate the signature without needing to encode its type.\\nWhen placed in state, signatures will still be amino encoded, but it will be the\\nprimitive type `[]byte` getting encoded.\\n#### Ed25519\\nUse the canonical representation for signatures.\\n#### Secp256k1\\nThere isn't a clear canonical representation here.\\nSignatures have two elements `r,s`.\\nThese bytes are encoded as `r || s`, where `r` and `s` are both exactly\\n32 bytes long, encoded big-endian.\\nThis is basically Ethereum's encoding, but without the leading recovery bit.\\n","MatchedIds":[4346,1957,4097,21,4623],"Predicted":"## Decision\n### Transition to Protocol Buffers for Public Keys and Signatures\n\nWe will transition from using Amino encoding to Protocol Buffers (protobuf) for encoding public keys and signatures on chain. \n\n### Rationale\n1. **Interoperability**: Protobuf is widely adopted and supported across various languages and platforms, which will ease integration and compatibility with external systems.\n  \n2. **Performance**: Protobuf provides more efficient serialization and deserialization processes, improving overall performance.\n\n3. **Readability and Maintenance**: While Amino was easier to support initially, Protobuf offers better tooling and documentation, making it easier for developers to understand and maintain the codebase.\n\n4. **Standardization**: Adopting Protobuf aligns with our broader strategy to standardize our encoding mechanisms, as seen in other parts of our system.\n\n### Implementation Steps\n1. **Define Protobuf Schemas**: Create protobuf schema definitions for the public keys and signatures.\n    ```proto\n    syntax = \"proto3\";\n\n    package cosmos.crypto;\n\n    message PublicKey {\n        oneof key {\n            bytes ed25519 = 1;\n            bytes secp256k1 = 2;\n            \/\/ Add other key types as needed\n        }\n    }\n\n    message Signature {\n        bytes sig = 1;\n        string algo = 2; \/\/ e.g., \"ed25519\", \"secp256k1\"\n    }\n    ```\n\n2. **Migration Plan**:\n    - **Backward Compatibility**: Implement dual support for both Amino and Protobuf during the transition period. Transactions can include a flag indicating which encoding is used.\n    - **Deprecation Notice**: Communicate to stakeholders and developers about the deprecation of Amino encoding, providing a clear timeline and sufficient documentation for migrating to Protobuf.\n\n3. **Update Codebase**:\n    - Modify existing modules to support Protobuf serialization and deserialization.\n    - Ensure that internal signing and verification processes are compatible with the new encoding format.\n    - Update documentation and provide examples for developers.\n\n4. **Testing**:\n    - Perform extensive testing to ensure the correctness and performance of the new encoding mechanism.\n    - Include regression tests to verify that no existing functionality is broken by the change.\n\n5. **Deployment**:\n    - Gradually roll out the changes in a controlled manner.\n    - Monitor the network for any issues and have a rollback plan in case of unforeseen problems.\n\n### Summary\nBy transitioning to Protocol Buffers for public key and signature encoding, we enhance interoperability, performance, and maintainability while aligning with industry standards. The migration will be managed carefully to ensure backward compatibility and minimal disruption to existing systems.","GenTime":"2024-08-19 23:33:33"}
{"File Name":"gsp\/ADR032-sre-permissions.md","Context":"## Context\\nAs mitigation for some risks raised in threat modelling it was agreed that day-to-day access to the cluster was to be read-only for everyone. Only the concourse running in the cluster could make changes that originated from Github, which required several approvals before merging.\\nFollowing the gradual rollout of several applications onto the GSP it became clear there were issues with the deployment procedures. This caused conflicting and contending pods to attempt to execute, resulting in application failures and deployment pipeline blockages. This was happening up to several times a day, depending on the level of activity. The remedial procedure involves escalating one or more members to cluster admin to allow the resources to be deleted, before revoking the admin permissions again. This process requires 3 people to perform and could result in hours of wasted time for each occurrence.\\n","Decision":"We will add to the SRE permissions map the ability to delete the following higher-level controllers so an escalation to cluster admin is no longer necessary:\\n* ConfigMap\\n* Deployment\\n* ReplicaSet\\n* Secret\\n* Service\\n* StatefulSet\\nWe will also raise a story to investigate the root cause of the deployment issues with a view to removing these permissions in the future.\\n","tokens":166,"id":3915,"text":"## Context\\nAs mitigation for some risks raised in threat modelling it was agreed that day-to-day access to the cluster was to be read-only for everyone. Only the concourse running in the cluster could make changes that originated from Github, which required several approvals before merging.\\nFollowing the gradual rollout of several applications onto the GSP it became clear there were issues with the deployment procedures. This caused conflicting and contending pods to attempt to execute, resulting in application failures and deployment pipeline blockages. This was happening up to several times a day, depending on the level of activity. The remedial procedure involves escalating one or more members to cluster admin to allow the resources to be deleted, before revoking the admin permissions again. This process requires 3 people to perform and could result in hours of wasted time for each occurrence.\\n\n\n##Decision\nWe will add to the SRE permissions map the ability to delete the following higher-level controllers so an escalation to cluster admin is no longer necessary:\\n* ConfigMap\\n* Deployment\\n* ReplicaSet\\n* Secret\\n* Service\\n* StatefulSet\\nWe will also raise a story to investigate the root cause of the deployment issues with a view to removing these permissions in the future.\\n","MatchedIds":[210,3890,4397,3789,856],"Predicted":"## Decision\nWe will implement an automated self-healing mechanism within the cluster to resolve deployment conflicts and contending pods without requiring manual intervention. This mechanism will involve the following steps:\n\n1. **Automated Detection**: Deploy a monitoring tool that continuously checks for conflicting and contending pods based on predefined metrics.\n2. **Self-Healing Scripts**: Develop and deploy scripts that can identify and appropriately handle conflicting or contending pods by either restarting or deleting them.\n3. **Event-Driven Actions**: Configure the monitoring tool to trigger these self-healing scripts automatically upon detecting issues, reducing the need for human escalation.\n4. **Audit and Alerts**: Every action taken by the self-healing mechanism will be logged, and alerts will be sent to the operations team. This ensures visibility and can be used to further refine the self-healing algorithms.\n5. **Testing and Validation**: Introduce a staging environment where self-healing actions can be simulated and validated before being deployed to production.\n\n### Positive consequences\n- Reduces the downtime and operational inefficiencies caused by conflicting and contending pods.\n- Decreases dependency on manual interventions, thereby saving time and resources.\n- Ensures consistent and reliable deployments even during high levels of activity.\n- Maintains security policies by minimizing the need to escalate users to cluster admin privileges.\n\n### Negative consequences\n- Initial setup and configuration of the self-healing mechanisms may require development time and resources.\n- Potential risk of the self-healing scripts not covering all edge cases, which might still necessitate manual intervention.\n- Requires careful monitoring and iteration to ensure the self-healing processes do not inadvertently cause other issues.\n\nBy implementing this automated self-healing mechanism, we aim to mitigate the issues of deployment pipeline blockages and application failures while adhering to the security constraints of the cluster.","GenTime":"2024-08-19 23:33:39"}
{"File Name":"texas-holdem-code-challenge\/0002-use-node-js.md","Context":"## Context\\n- The problem instructions state \"C++ or node.js are preferred\".\\n- I am most familiar with node.js out of those two.\\n- I am completing this problem in my spare time and have a limited amount of\\ntime to spend on it: it's convenient to work faster if possible.\\n","Decision":"Use Node.js to complete this problem.\\n","tokens":67,"id":2294,"text":"## Context\\n- The problem instructions state \"C++ or node.js are preferred\".\\n- I am most familiar with node.js out of those two.\\n- I am completing this problem in my spare time and have a limited amount of\\ntime to spend on it: it's convenient to work faster if possible.\\n\n\n##Decision\nUse Node.js to complete this problem.\\n","MatchedIds":[4940,472,3470,3715,577],"Predicted":"## Decision\nI decided to implement this project using node.js. Given my familiarity with node.js, it allows me to work more efficiently and manage my limited spare time better. Additionally, node.js is one of the preferred technologies according to the problem instructions. This choice will enable me to complete the project faster while adhering to the guidelines provided.","GenTime":"2024-08-19 23:33:42"}
{"File Name":"portfolio\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\nEven though this project is mainly a 1 engineer side-project, recording architectural decisions allows using the records as a reminder of the \"state of mind\" we were in when making a decision, as well as the motivations behind it.\\nAdditionally, it's a good engineering practice that would be nice to try out, and this blog is a great \"guinea pig\" for this approach.\\nSee this article on [GitHub's engineering blog](https:\/\/github.blog\/2020-08-13-why-write-adrs\/) for some addition reasons why they could be a good practice.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\nADRs from here on out will be written in the first-person **singular** tense, since it's mainly just me building the app.\\n","tokens":135,"id":2091,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\nEven though this project is mainly a 1 engineer side-project, recording architectural decisions allows using the records as a reminder of the \"state of mind\" we were in when making a decision, as well as the motivations behind it.\\nAdditionally, it's a good engineering practice that would be nice to try out, and this blog is a great \"guinea pig\" for this approach.\\nSee this article on [GitHub's engineering blog](https:\/\/github.blog\/2020-08-13-why-write-adrs\/) for some addition reasons why they could be a good practice.\\n\n\n##Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\nADRs from here on out will be written in the first-person **singular** tense, since it's mainly just me building the app.\\n","MatchedIds":[4361,4699,4408,4722,4428],"Predicted":"## Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions. Additionally, for more insights into the benefits of ADRs, we can refer to [GitHub's engineering blog](https:\/\/github.blog\/2020-08-13-why-write-adrs\/).","GenTime":"2024-08-19 23:33:45"}
{"File Name":"dogma\/0018-projection-compaction.md","Context":"## Context\\nMany projections produce data that is only required for a limited period of\\ntime.\\nAn application developer needs to consider how long projection data will be\\nretained and ideally implement measures to remove any unnecessary data.\\nOften such data can be removed when some future event occurs. However, in some\\ncases there is no future event that appropriately indicates the data is no\\nlonger required.\\nWe have encountered a genuine example of this when implementing an SQL\\nprojection that inserts a row whenever a user performs a specific action. The\\ndata is queried in order to enforce a \"cool down\" that prevents the user from\\nrepeating that action again within a specific time frame.\\nThe insert is triggered by the occurrence of an event, but the data becomes\\nunnecessary whenever the \"cool down\" time has elapsed.\\nIn this particular use case the \"cool down\" was not part of the business logic,\\nbut rather an API level restriction. Hence, processes\/timeout messages were not\\nthe appropriate solution.\\n","Decision":"We have decided to add a `Compact()` method to `ProjectionMessageHandler`.\\nThe implementation of `Compact()` can modify the projection's data by whatever\\nmeans is appropriate such that unnecessary data is removed but the projection\\nstill serves its purpose.\\n","tokens":210,"id":1606,"text":"## Context\\nMany projections produce data that is only required for a limited period of\\ntime.\\nAn application developer needs to consider how long projection data will be\\nretained and ideally implement measures to remove any unnecessary data.\\nOften such data can be removed when some future event occurs. However, in some\\ncases there is no future event that appropriately indicates the data is no\\nlonger required.\\nWe have encountered a genuine example of this when implementing an SQL\\nprojection that inserts a row whenever a user performs a specific action. The\\ndata is queried in order to enforce a \"cool down\" that prevents the user from\\nrepeating that action again within a specific time frame.\\nThe insert is triggered by the occurrence of an event, but the data becomes\\nunnecessary whenever the \"cool down\" time has elapsed.\\nIn this particular use case the \"cool down\" was not part of the business logic,\\nbut rather an API level restriction. Hence, processes\/timeout messages were not\\nthe appropriate solution.\\n\n\n##Decision\nWe have decided to add a `Compact()` method to `ProjectionMessageHandler`.\\nThe implementation of `Compact()` can modify the projection's data by whatever\\nmeans is appropriate such that unnecessary data is removed but the projection\\nstill serves its purpose.\\n","MatchedIds":[3127,1031,1799,1701,3123],"Predicted":"## Decision\nWe will implement a time-based data expiration mechanism to address the retention of projection data that becomes unnecessary after a certain period. Specifically, we will utilize database cleanup scripts that are scheduled to run at regular intervals. These scripts will identify and delete rows related to \"cool down\" periods that have elapsed their retention time. This approach ensures that our projections remain lean and efficient without adding complexity to our business logic or requiring additional event handling processes.\n\n1. **Temporary Data Flag**: Tag the \"cool down\" projection data with a timestamp indicating when the data was created and when it should expire. This can be achieved by adding `created_at` and `expires_at` columns to the relevant table.\n\n2. **Scheduled Cleanup Job**: Introduce a scheduled cleanup job in the database or the application layer. This job will run at predefined intervals (e.g., daily or hourly) and delete rows where the `expires_at` timestamp has passed.\n\n3. **Monitoring and Alerts**: Set up monitoring and alerting mechanisms to ensure the cleanup job runs successfully and to identify any potential issues that may arise, such as failures or performance bottlenecks.\n\nBy adopting this strategy, we can maintain the performance and scalability of our projections while ensuring that unnecessary data does not persist beyond its useful lifetime.","GenTime":"2024-08-19 23:33:50"}
{"File Name":"GDD-app\/0003-use-bitrise-for-ci.md","Context":"## Context\\nWe need an easy way to integrate and test out code that is fast and reliable.\\n","Decision":"We choose Bitrise because it came from a suggestion from more senior devs and provides an easy interface to manage the workflows with good support for Android apps and testing.\\nIt also allows us to notify users in a easy way and with different roles.\\n","tokens":21,"id":4868,"text":"## Context\\nWe need an easy way to integrate and test out code that is fast and reliable.\\n\n\n##Decision\nWe choose Bitrise because it came from a suggestion from more senior devs and provides an easy interface to manage the workflows with good support for Android apps and testing.\\nIt also allows us to notify users in a easy way and with different roles.\\n","MatchedIds":[1387,4359,3070,5070,1258],"Predicted":"## Decision\nWe will implement a CI\/CD pipeline using GitHub Actions. GitHub Actions provides seamless integration with our GitHub repositories, supports various triggers (e.g., on push, pull requests), and has a robust ecosystem for adding testing, build, and deployment steps. Additionally, it is reliable with strong community support and allows us to run tests in various environments efficiently.","GenTime":"2024-08-19 23:33:53"}
{"File Name":"scholarsphere\/0009-acl-actor-permissions.md","Context":"## Context\\nPermissions on works and collections can come from two sources: 1) the person who authored the resource, such as the\\ndepositor or the proxy depositor; and 2) access controls (ACLs) that grant permissions based on user or group identity.\\nWhen determining who has access to a given resource, both these sources may need to be consulted.\\n","Decision":"Access controls and depositor or proxy depositor rights are independent from one another.\\nAccess controls should not include permissions granted by the Actor-to-resource arrangement, such as edit rights of the\\ndepositor. They are a separate form of permission structure and therefore independent of one another. Likewise,\\npermissions that come from a depositor should have no bearing on what access controls may be applied to a resource.\\n","tokens":79,"id":4554,"text":"## Context\\nPermissions on works and collections can come from two sources: 1) the person who authored the resource, such as the\\ndepositor or the proxy depositor; and 2) access controls (ACLs) that grant permissions based on user or group identity.\\nWhen determining who has access to a given resource, both these sources may need to be consulted.\\n\n\n##Decision\nAccess controls and depositor or proxy depositor rights are independent from one another.\\nAccess controls should not include permissions granted by the Actor-to-resource arrangement, such as edit rights of the\\ndepositor. They are a separate form of permission structure and therefore independent of one another. Likewise,\\npermissions that come from a depositor should have no bearing on what access controls may be applied to a resource.\\n","MatchedIds":[3950,4292,4525,2470,3890],"Predicted":"## Decision\nImplement a unified permissions management system that aggregates permissions from both the resource authorship (depositors and proxy depositors) and ACLs. This system will ensure that access to any resource can be consistently determined by evaluating both sources of permissions.\n\n### Implementation Details:\n\n1. **Unified Permissions Evaluation Service**:\n   - Develop a service that acts as a single point of truth for permissions evaluation.\n   - This service will aggregate permissions from both the depositor\/proxy depositor details and ACLs to determine access rights.\n\n2. **Resource Metadata Enhancement**:\n   - Enhance the metadata of works and collections to include explicit references to the depositor and proxy depositor.\n   - This ensures that their permissions are readily available for evaluation without additional database queries.\n\n3. **ACL Integration**:\n   - Integrate ACLs into the permissions management system by using a centralized ACL database or service.\n   - Ensure that ACLs are flexible enough to support both user-specific and group-specific permissions.\n\n4. **Priority of Permissions**:\n   - Define a clear policy on how conflicting permissions are resolved, e.g., permissions granted by the depositor might take precedence over those defined in ACLs, or vice versa.\n\n5. **Caching and Performance Optimization**:\n   - Implement caching mechanisms for permissions data to enhance performance for frequent access checks.\n   - Ensure the cache is invalidated appropriately when permissions are updated.\n\n6. **Granular Permissions**:\n   - Ensure the permissions system supports granular access controls, such as read, write, and delete, at various levels (e.g., specific works, collections).\n\n### Example Scenario:\nFor a user requesting access to a specific collection:\n   - The Unified Permissions Evaluation Service checks if the user is the depositor or proxy depositor of the collection.\n   - Subsequently, it evaluates the ACLs to determine if the user or the user's group has the necessary permissions to access the collection.\n   - The service aggregates and resolves permissions to provide a final access decision.\n\nBy implementing this unified approach, we ensure that all potential sources of permissions are considered comprehensively, providing robust and consistent access control across the system.","GenTime":"2024-08-19 23:33:59"}
{"File Name":"molgenis-js-auth\/0002-manage-user-permissions-in-fusion-auth.md","Context":"## Context\\nIn Armadillo:\\n* A cohort is an OpenID application\\n* The researcher needs a `ROLE_<STUDY>_RESEARCHER` on the application to read data\\nin shared folder `<study>`\\nRequirements on who manages what:\\n* User and permission management is done by representatives of the consortium and cohort\\n* The identity of the user is federated to their local institution\\n* Representatives of the cohort administer the study researchers\u2019 admission to their cohort\\n* Representatives of the consortium can see which users have which roles on which cohort\\n","Decision":"Implement this in Fusion Auth, plus a (modest) administration UI.\\n* The users register themselves.\\n* A cohort representative explicitly creates a registration for the researcher in the application,\\nwith one or more roles.\\n### Administration\\nApplication-specific permissions are not available in Fusion Auth.\\nIn other products they tend to be rather complicated to configure.\\nCreate an edge service that enforces the permissions and uses\\nan api key to administer them through the Fusion Auth API.\\n","tokens":116,"id":4292,"text":"## Context\\nIn Armadillo:\\n* A cohort is an OpenID application\\n* The researcher needs a `ROLE_<STUDY>_RESEARCHER` on the application to read data\\nin shared folder `<study>`\\nRequirements on who manages what:\\n* User and permission management is done by representatives of the consortium and cohort\\n* The identity of the user is federated to their local institution\\n* Representatives of the cohort administer the study researchers\u2019 admission to their cohort\\n* Representatives of the consortium can see which users have which roles on which cohort\\n\n\n##Decision\nImplement this in Fusion Auth, plus a (modest) administration UI.\\n* The users register themselves.\\n* A cohort representative explicitly creates a registration for the researcher in the application,\\nwith one or more roles.\\n### Administration\\nApplication-specific permissions are not available in Fusion Auth.\\nIn other products they tend to be rather complicated to configure.\\nCreate an edge service that enforces the permissions and uses\\nan api key to administer them through the Fusion Auth API.\\n","MatchedIds":[3950,4554,2397,2470,2582],"Predicted":"## Decision\nFor managing user roles and permissions in Armadillo while leveraging OpenID, we will:\n\n1. **Utilize the Keycloak OpenID Connect Provider**: Keycloak will handle authentication and authorization. It supports role-based access control (RBAC) and can integrate with local institution identity providers using federated identity protocols.\n\n2. **Structure Roles as `ROLE_<STUDY>_RESEARCHER`**: Define and manage roles within Keycloak. Each study will have its own role (e.g., `ROLE_COHORT_A_RESEARCHER`, `ROLE_COHORT_B_RESEARCHER`) ensuring granular access control to shared folders.\n\n3. **Federated Identity Management**: Federate Keycloak with local institution identity providers using SAML or OpenID Connect. This allows user identities to be managed by their respective local institutions, ensuring compliance with local policies.\n\n4. **Delegated Administration**:\n   - **Cohort Representatives**: Grant cohort representatives administrative permissions within Keycloak to manage roles for their specific cohort. They can add or remove the `ROLE_<STUDY>_RESEARCHER` from users belonging to their cohort.\n   - **Consortium Representatives**: Provide consortium representatives with read-only access to user-role mappings in Keycloak. This allows them to view the roles assigned to users across all cohorts without modifying them.\n\n5. **Automated Role Assignment**: Utilize Keycloak's scripting capabilities or automate processes through its API to streamline the assignment of roles based on cohort enrollment and manage it dynamically as users are federated from their local institutions.\n\n6. **Audit and Compliance**: Ensure that all role and permission changes are logged within Keycloak for auditing purposes. Regularly review logs and role assignments to ensure compliance with consortium governance policies.\n\nBy using Keycloak, we ensure a robust and scalable solution that supports federated identity, granular RBAC, and delegated administration, all fulfilling the governance requirements for managing researchers within Armadillo.","GenTime":"2024-08-19 23:34:09"}
{"File Name":"origin\/0010-more-complete-database-type-and-adjust-migrations.md","Context":"## Context\\nSo far, Origin has been using a development-friendly SQLite database on the backend to store data. While this has been beneficial in the early stages of development, in order to gain adoption and real-world use we should make it easier for anyone using the Origin SDK to get to production as fast as possible.\\n","Decision":"A decision has been made to move the Origin SDK to a mode production-ready database. We've decided to go with **PostgreSQL** instead of SQLite, as we've seen that this is the database of choice for many users and setups.\\n","tokens":64,"id":1561,"text":"## Context\\nSo far, Origin has been using a development-friendly SQLite database on the backend to store data. While this has been beneficial in the early stages of development, in order to gain adoption and real-world use we should make it easier for anyone using the Origin SDK to get to production as fast as possible.\\n\n\n##Decision\nA decision has been made to move the Origin SDK to a mode production-ready database. We've decided to go with **PostgreSQL** instead of SQLite, as we've seen that this is the database of choice for many users and setups.\\n","MatchedIds":[1167,4859,3053,4285,2488],"Predicted":"## Decision\nWhile SQLite has served its purpose during the initial development phase, it is not well-suited for production environments due to limitations in concurrency, scalability, and advanced feature support. Therefore, it has been decided to transition to PostgreSQL as our primary database for the following reasons:\n\n1. **Scalability and Concurrency**: PostgreSQL is designed to handle a high volume of concurrent read and write operations, which is essential for production environments and real-world use cases.\n2. **Advanced Features**: PostgreSQL offers a wide range of advanced features such as full-text search, JSON support, and geospatial data types, which can be beneficial as Origin expands its functionalities.\n3. **Reliability and ACID Compliance**: PostgreSQL ensures data integrity with full ACID compliance, making it a reliable choice for managing transactional data.\n4. **Community and Ecosystem**: PostgreSQL has a large, active community and a rich ecosystem of tools and extensions, providing robust support and ongoing improvements.\n5. **Seamless Migration**: Transitioning from SQLite to PostgreSQL can be relatively straightforward with existing migration tools, making it easier for current users to upgrade their systems without significant disruption.\n\nBy adopting PostgreSQL as the backend database, we aim to provide a production-ready solution that supports advanced use cases, improves performance, and ensures robustness and data integrity for all users of the Origin SDK.","GenTime":"2024-08-19 23:34:15"}
{"File Name":"libelektra\/error_codes.md","Context":"## Problem\\nThe current error concept has disadvantages in following regards:\\n- A lot of redundant errors\\nAt the moment, each new plugin introduces new error codes which led to about 210+ error codes. Many of those errors\\nare duplicated because developers did not know or search for a similar error which is already present. This concept should\\ngroup similar errors together so that there is one coherent and consistent state again.\\n- Hard to manage specification file\\nSince every developer adds its own error individually, a lot of merge conflicts happen which makes contributing to the codebase\\nunpleasant. Additionally, if you want to reuse any error you have to scrape to the whole file with ~1300+ lines. As there is no\\nsenseful ordering or scheme behind the errors (since they grew by time), it is a hassle to find the correct error code.\\nThe new concept should standardize errors, making it easy to categorize errors from new plugins and avoid merge conflicts.\\n- No senseful way for application developers to use error codes from elektra\\nIf developers of plugins\/ external tools using elektra want to react to errors, they have to be very specific. At the moment there is\\nno possibility to catch all errors easily which force a certain behavior. Eg. if there happens a temporary recoverable error, developers have to\\ncatch for every specific error code rather than a general hierarchical error. The new concept should make it easy to react to errors as they are\\nsensefully grouped together and are hierarchically structured.\\n","Decision":"All \"fatal\" errors will be converted to \"errors\" as the distinction is not relevant.\\nUnused errors will be removed from the specification.\\nErrors will be categorized into logical groups with subgroups.\\nEach error will be made up of 5 characters, where the first 2 character indicate the highest level\\nand character 3 to 5 will be used for subgrouping. Errors are prepended with the letter `C` which\\nis the abbreviation for \"Code\".\\n- Permanent errors C01000\\n- Resource C01100\\n- Out of Memory C01110\\n- Installation C01200\\n- Logical C01300\\n- Internal C01310\\n- Interface C01320\\n- Plugin Misbehavior C01330\\n- Conflicting State C02000\\n- Validation C03000\\n- Syntactic C03100\\n- Semantic C03200\\nTo see an explanation of the categories along with a guideline on how to categorize please\\nsee the [Error Codes Guideline](..\/dev\/error-categorization.md)\\n- [Error Message Format](error_message_format.md)\\nShows the new format of the error message\\n- [Error Codes Guideline](..\/dev\/error-categorization.md)\\nShows how to categorize errors\\n","tokens":312,"id":1319,"text":"## Problem\\nThe current error concept has disadvantages in following regards:\\n- A lot of redundant errors\\nAt the moment, each new plugin introduces new error codes which led to about 210+ error codes. Many of those errors\\nare duplicated because developers did not know or search for a similar error which is already present. This concept should\\ngroup similar errors together so that there is one coherent and consistent state again.\\n- Hard to manage specification file\\nSince every developer adds its own error individually, a lot of merge conflicts happen which makes contributing to the codebase\\nunpleasant. Additionally, if you want to reuse any error you have to scrape to the whole file with ~1300+ lines. As there is no\\nsenseful ordering or scheme behind the errors (since they grew by time), it is a hassle to find the correct error code.\\nThe new concept should standardize errors, making it easy to categorize errors from new plugins and avoid merge conflicts.\\n- No senseful way for application developers to use error codes from elektra\\nIf developers of plugins\/ external tools using elektra want to react to errors, they have to be very specific. At the moment there is\\nno possibility to catch all errors easily which force a certain behavior. Eg. if there happens a temporary recoverable error, developers have to\\ncatch for every specific error code rather than a general hierarchical error. The new concept should make it easy to react to errors as they are\\nsensefully grouped together and are hierarchically structured.\\n\n\n##Decision\nAll \"fatal\" errors will be converted to \"errors\" as the distinction is not relevant.\\nUnused errors will be removed from the specification.\\nErrors will be categorized into logical groups with subgroups.\\nEach error will be made up of 5 characters, where the first 2 character indicate the highest level\\nand character 3 to 5 will be used for subgrouping. Errors are prepended with the letter `C` which\\nis the abbreviation for \"Code\".\\n- Permanent errors C01000\\n- Resource C01100\\n- Out of Memory C01110\\n- Installation C01200\\n- Logical C01300\\n- Internal C01310\\n- Interface C01320\\n- Plugin Misbehavior C01330\\n- Conflicting State C02000\\n- Validation C03000\\n- Syntactic C03100\\n- Semantic C03200\\nTo see an explanation of the categories along with a guideline on how to categorize please\\nsee the [Error Codes Guideline](..\/dev\/error-categorization.md)\\n- [Error Message Format](error_message_format.md)\\nShows the new format of the error message\\n- [Error Codes Guideline](..\/dev\/error-categorization.md)\\nShows how to categorize errors\\n","MatchedIds":[1302,1308,1293,1303,1291],"Predicted":"## Decision\nRevise the error concept by introducing a hierarchical and standardized error categorization system, addressing redundancy, manageability, and usability. This approach will follow these guidelines:\n\n1. **Hierarchical Error Categories**:\n    - Introduce high-level categories such as `Configuration Errors`, `Runtime Errors`, `Validation Errors`, etc.\n    - Each category will have subcategories, allowing for hierarchical grouping of errors. For example, `Configuration Errors` can have subcategories like `File Not Found`, `Invalid Format`, etc.\n    - Assign a unique error code range to each category and subcategory for easy addition of new errors without conflicts.\n\n2. **Standardized Error Definitions**:\n    - Develop a standardized error definition template. This template should include error code, category, subcategory, message, severity, and possible resolutions.\n    - Consolidate existing error codes into this new structure, removing duplicates and redundant entries.\n\n3. **Centralized Error Code Repository**:\n    - Maintain a centralized error code repository file with a clear and logical order based on the hierarchical categories.\n    - Implement an automated tool to assist in searching and adding new errors, ensuring new entries are placed correctly and follow the defined structure.\n\n4. **Enhanced Merge Conflict Resolution**:\n    - Use modular approach to divide the centralized error code repository into multiple, smaller files based on error categories. This division minimizes merge conflicts.\n    - Implement guidelines and review processes to manage contributions to the error code repository effectively.\n\n5. **Developer-Friendly Error Handling**:\n    - Provide APIs to retrieve error codes based on categories and subcategories, simplifying error handling in application code.\n    - Offer built-in functions to handle common error response patterns, like retrying operations for recoverable errors or logging and shutting down for critical errors.\n\n6. **Documentation and Guidelines**:\n    - Document the error categorization system, including usage examples and best practices.\n    - Create tutorials for developers on how to integrate and handle errors using the new system, ensuring a smooth transition and adoption.\n\n7. **Backward Compatibility**:\n    - Implement a mapping layer that translates old error codes to the new hierarchical system, ensuring backward compatibility and easing the transition.\n\nWith these changes, the new error concept will be coherent, manageable, and developer-friendly, facilitating easier integration and more robust error handling across Elektra and its plugins.","GenTime":"2024-08-19 23:34:24"}
{"File Name":"MoviesAndroid\/0002 Kotlin.md","Context":"## Context and Problem Statement\\nWhen starting a new app you can now choose to build it in either Java or Kotlin. You can of course\\ndon't choose and use both, but i want to make it consistent.\\n## Decision Drivers\\n* New empty project\\n* Kotlin now fully supported for Android by Google\\n","Decision":"* New empty project\\n* Kotlin now fully supported for Android by Google\\nChosen option: Kotlin, because it's way more modern than Java and fully supported in Android by\\nGoogle now, which eliminates the risk of being dropped any time soon.\\nPositive Consequences:\\n* Less code, more fun!\\n* Having to learn a new language\\n* Great interoperability with Java if needed\\nNegative consequences:\\n* Having to learn a new language\\n","tokens":64,"id":159,"text":"## Context and Problem Statement\\nWhen starting a new app you can now choose to build it in either Java or Kotlin. You can of course\\ndon't choose and use both, but i want to make it consistent.\\n## Decision Drivers\\n* New empty project\\n* Kotlin now fully supported for Android by Google\\n\n\n##Decision\n* New empty project\\n* Kotlin now fully supported for Android by Google\\nChosen option: Kotlin, because it's way more modern than Java and fully supported in Android by\\nGoogle now, which eliminates the risk of being dropped any time soon.\\nPositive Consequences:\\n* Less code, more fun!\\n* Having to learn a new language\\n* Great interoperability with Java if needed\\nNegative consequences:\\n* Having to learn a new language\\n","MatchedIds":[1463,4695,472,2626,3860],"Predicted":"## Decision\nWe will adopt Kotlin as the primary programming language for the new app.\n\n### Rationalization\n* Kotlin is fully supported by Google for Android development, which ensures long-term viability and support.\n* As a new project, it is beneficial to leverage the modern features and concise syntax that Kotlin provides.\n* Using a single language will maintain consistency throughout the project, reducing complexity and the potential for errors.\n\n### Positive Consequences\n* Improved development productivity due to Kotlin's expressive and concise syntax.\n* Enhanced support for modern Android development practices and features provided by Kotlin.\n* Potentially easier onboarding for new developers familiar with Kotlin.\n\n### Negative Consequences\n* Existing developers with a strong preference for Java may require some time to adapt to Kotlin.","GenTime":"2024-08-19 23:34:28"}
{"File Name":"PactSwift\/ADR-002-Coding_standard.md","Context":"# Context\\nWe shouldn't feel bound by any pre-existing coding standards so this project and its code is written according to personal preferences based on practices that yielded good results acquired working in other projects with many collaborators. The code is relatively consistent but that might change once more developers contribute to the project.\\nIn general, Swift code has a fairly strong styling, relative to C or C++, due to opinionated aspects of the language itself and the styling used by the official language guides. Formatting around brace placement, `if` and `for` styling is fairly clearly set by the language.\\n# Decision\\n[Swiftlint configuration](.\/..\/..\/.swiftlint.yml) is used to enforce us adhering to _most_ of code style conventions.\\n","Decision":"[Swiftlint configuration](.\/..\/..\/.swiftlint.yml) is used to enforce us adhering to _most_ of code style conventions.\\n","tokens":150,"id":146,"text":"# Context\\nWe shouldn't feel bound by any pre-existing coding standards so this project and its code is written according to personal preferences based on practices that yielded good results acquired working in other projects with many collaborators. The code is relatively consistent but that might change once more developers contribute to the project.\\nIn general, Swift code has a fairly strong styling, relative to C or C++, due to opinionated aspects of the language itself and the styling used by the official language guides. Formatting around brace placement, `if` and `for` styling is fairly clearly set by the language.\\n# Decision\\n[Swiftlint configuration](.\/..\/..\/.swiftlint.yml) is used to enforce us adhering to _most_ of code style conventions.\\n\n\n##Decision\n[Swiftlint configuration](.\/..\/..\/.swiftlint.yml) is used to enforce us adhering to _most_ of code style conventions.\\n","MatchedIds":[400,458,1408,5255,4226],"Predicted":"## Decision\nWe will adopt SwiftLint to enforce code style conventions across the project.\n\n- We will configure SwiftLint using the provided [Swiftlint configuration](.\/..\/..\/.swiftlint.yml) file, adhering to most code style conventions.\n- We will set up pre-commit hooks to automatically run SwiftLint before commits.\n- Continuous integration will be configured to reject commits that do not pass SwiftLint checks.\n","GenTime":"2024-08-19 23:34:30"}
{"File Name":"ELEN4010\/ADR Sprint Planning and timeline.md","Context":"## Context\\n16-25 April, Thabang is away, must work remotely. Major submission period over 29th April - 10th May, will impact productivity on software project.\\n","Decision":"4 Sprints planned, consecutively. Will only start on 17th April, but have a \"Sprint 0\" from 9th to 16 April, with initial planning, research and folder structure creation, setup of Azure and Travis. Will not count towards actual number of sprints.\\n* Sprints will begin on Wednesdays, with a 1 hr review session in the morning.\\n* There will be a three hour sprint planning session in the afternooon each Wednesday.\\n* Release will be every Tuesday, by 8pm\\n* Product release window will be from 2pm - 8pm, with all pull requests done before 6pm to give time for any required last minute code review and testing\\n* Friday coding sessions together from 12pm - 4pm\\n* Standups via Whatsapp, or between lectures. Preferable to do in person but may not be possible.\\nRebecca: Product Manager\\nTyson: SCRUM Master\\nTheese roles will be alternated throughout the project each week.\\n","tokens":40,"id":57,"text":"## Context\\n16-25 April, Thabang is away, must work remotely. Major submission period over 29th April - 10th May, will impact productivity on software project.\\n\n\n##Decision\n4 Sprints planned, consecutively. Will only start on 17th April, but have a \"Sprint 0\" from 9th to 16 April, with initial planning, research and folder structure creation, setup of Azure and Travis. Will not count towards actual number of sprints.\\n* Sprints will begin on Wednesdays, with a 1 hr review session in the morning.\\n* There will be a three hour sprint planning session in the afternooon each Wednesday.\\n* Release will be every Tuesday, by 8pm\\n* Product release window will be from 2pm - 8pm, with all pull requests done before 6pm to give time for any required last minute code review and testing\\n* Friday coding sessions together from 12pm - 4pm\\n* Standups via Whatsapp, or between lectures. Preferable to do in person but may not be possible.\\nRebecca: Product Manager\\nTyson: SCRUM Master\\nTheese roles will be alternated throughout the project each week.\\n","MatchedIds":[1799,443,920,3551,915],"Predicted":"## Decision\n### For the period 16-25 April:\n* Thabang will work remotely, and necessary resources and communication channels will be set up to ensure remote productivity.\n* Tasks that do not require intense collaboration will be allocated to Thabang during this period to minimize disruptions.\n\n### For the period 29th April - 10th May:\n* Due to the anticipated decrease in productivity during the major submission period, we will prioritize critical tasks and defer less urgent work.\n* Additional team members will be prepared to assist or take over time-sensitive tasks to ensure project timelines are maintained.\n* Regular check-ins to monitor progress and swiftly address any issues that arise.\n\nThis approach allows us to accommodate Thabang's remote work and manage the impact of the submission period on the software project's productivity while ensuring consistent progress.","GenTime":"2024-08-19 23:34:34"}
{"File Name":"SAP-Cloud\/node-package-lock.md","Context":"## Context\\nThe node package manager (npm) creates a so called [\"lock file\"](https:\/\/docs.npmjs.com\/files\/package-locks) when running `npm install`.\\nThe recommended action is to commit this file to version control, as stated by the command itself:\\n```\\n$ npm install\\nnpm notice created a lockfile as package-lock.json. You should commit this file.\\n```\\nSome npm commands, which are part of the pipeline, such as `ci` and `audit` require that a package lock file exists.\\nThus, if a project lacks the package lock file, there are tree options:\\n* Refuse to build the project, which enforces the recommended practice of committing the lock file\\n* Silently create a package lock file as part of the pipeline\\n* Avoid npm commands which require a package lock\\n* Would disable the npm audit stage\\n","Decision":"If a project has a `package.json` file, but no package lock, we create a package lock file as part of the pipeline and warn the user about this.\\nThe created package lock file is not archived as part of the pipeline build artifacts.\\n","tokens":183,"id":2924,"text":"## Context\\nThe node package manager (npm) creates a so called [\"lock file\"](https:\/\/docs.npmjs.com\/files\/package-locks) when running `npm install`.\\nThe recommended action is to commit this file to version control, as stated by the command itself:\\n```\\n$ npm install\\nnpm notice created a lockfile as package-lock.json. You should commit this file.\\n```\\nSome npm commands, which are part of the pipeline, such as `ci` and `audit` require that a package lock file exists.\\nThus, if a project lacks the package lock file, there are tree options:\\n* Refuse to build the project, which enforces the recommended practice of committing the lock file\\n* Silently create a package lock file as part of the pipeline\\n* Avoid npm commands which require a package lock\\n* Would disable the npm audit stage\\n\n\n##Decision\nIf a project has a `package.json` file, but no package lock, we create a package lock file as part of the pipeline and warn the user about this.\\nThe created package lock file is not archived as part of the pipeline build artifacts.\\n","MatchedIds":[1320,939,1680,3827,1852],"Predicted":"## Decision\n\nTo ensure consistency and reliability across development, test, and production environments, we will enforce the recommended practice of committing the `package-lock.json` file to version control. Therefore, if a project lacks the `package-lock.json` file, the build pipeline will refuse to build the project. This approach will also preserve the ability to use `npm ci` and `npm audit` commands, ensuring reliable dependency management and security auditing. To facilitate this, as part of the development guidelines, developers will be instructed to run `npm install` and commit the generated `package-lock.json` file when adding or updating dependencies.","GenTime":"2024-08-19 23:34:38"}
{"File Name":"court-case-service\/0005-guiding-principles.md","Context":"## Context\\nThe implementation of [PIC-1135](https:\/\/dsdmoj.atlassian.net\/browse\/PIC-1135?focusedCommentId=163988) raised questions about what is the appropriate approach for serving new data to the prepare-a-case app, and in particular stressed the need for error handling by `prepare-a-case` where graceful degradation from partial failures is needed. The question was raised as to whether `court-case-service` should be coupled closely with `prepare-a-case` and is notionally a dedicated gateway for it or whether it should be treated as a general purpose API which may be consumed by multiple consumers.\\n","Decision":"1. All data stored by `court-case-service` is considered its own domain and should be treated as a general purpose api\\n2. All endpoints composing data on behalf of `prepare-a-case` are not intended for consumption by other clients and should provide all data needed on a given `prepare-a-case` page as efficiently as possible\\n3. The exception to point 2 is the case where errors in the retrieval of certain pieces of data are expected and need to be isolated. In this case data to be isolated should be served as a separate endpoint which will fail fast. `prepare-a-case` can then handle it simply as a failed HTTP call. See [PIC-1135](https:\/\/dsdmoj.atlassian.net\/browse\/PIC-1135?focusedCommentId=163988) for an example of this.\\n","tokens":129,"id":3579,"text":"## Context\\nThe implementation of [PIC-1135](https:\/\/dsdmoj.atlassian.net\/browse\/PIC-1135?focusedCommentId=163988) raised questions about what is the appropriate approach for serving new data to the prepare-a-case app, and in particular stressed the need for error handling by `prepare-a-case` where graceful degradation from partial failures is needed. The question was raised as to whether `court-case-service` should be coupled closely with `prepare-a-case` and is notionally a dedicated gateway for it or whether it should be treated as a general purpose API which may be consumed by multiple consumers.\\n\n\n##Decision\n1. All data stored by `court-case-service` is considered its own domain and should be treated as a general purpose api\\n2. All endpoints composing data on behalf of `prepare-a-case` are not intended for consumption by other clients and should provide all data needed on a given `prepare-a-case` page as efficiently as possible\\n3. The exception to point 2 is the case where errors in the retrieval of certain pieces of data are expected and need to be isolated. In this case data to be isolated should be served as a separate endpoint which will fail fast. `prepare-a-case` can then handle it simply as a failed HTTP call. See [PIC-1135](https:\/\/dsdmoj.atlassian.net\/browse\/PIC-1135?focusedCommentId=163988) for an example of this.\\n","MatchedIds":[1799,263,4841,3925,4644],"Predicted":"## Decision\nWe will design `court-case-service` as a **general purpose API**. This approach ensures that the service can be utilized by multiple consumers, promoting reusability and avoiding tight coupling with the `prepare-a-case` application. \n\n### Reasons for this Decision:\n\n1. **Scalability**: A general-purpose API allows us to scale and accommodate additional consumers without significant refactoring.\n2. **Flexibility and Reusability**: By not closely coupling `court-case-service` with `prepare-a-case`, we foster a more flexible architecture where other applications can easily consume the same service.\n3. **Separation of Concerns**: It maintains a clear separation of concerns, allowing each service to evolve independently.\n4. **Improved Maintenance**: This approach simplifies maintenance by isolating the responsibilities of each service, making the system easier to update and troubleshoot.\n\n### Consequences\n\n1. **Error Handling**: `prepare-a-case` will need to implement robust error handling mechanisms to gracefully degrade in case of partial failures.\n2. **Documentation and Contracts**: Clear API documentation and contracts will be crucial to ensure that all consumers can interact with `court-case-service` effectively.\n3. **Potential Performance Overheads**: We need to monitor and manage the performance to ensure that multiple consumers do not create bottlenecks or degrade the service quality.\n\n### Next Steps\n\n1. Enhance API documentation for `court-case-service` to support multiple consumers.\n2. Implement and test error handling in `prepare-a-case` to ensure it can handle scenarios of partial failure gracefully.\n3. Monitor performance to assess impact and optimize as necessary for multiple consumers.","GenTime":"2024-08-19 23:34:44"}
{"File Name":"skypy\/adr-01.md","Context":"## Context\\nWithin SkyPy all functions used to create a \"simulation\" will in practice be taking in some values (either parameters or columns from a table) and creating new column(s) in an output table *or* selecting specific rows from an input table.\\nThe inputs and outputs of these functions are clearly defined so a directed acyclic graph (DAG) can be constructed to determine what order the functions should be run in.\\nTo aid in the creation of the tables and the DAG a helper class or decorator should be used so the person writing the function does not have to worry about the implementation details. This class or decorator is what we are currently referring to as the `Model`.\\nFor clarity in the options below we will assume the following example function:\\n```python\\ndef redshift_gamma(shape, scale, size):\\n\"\"\"Gamma-distributed redshifts (Smail et al. 1994).\\nSample `size` redshifts from a gamma distribution with the\\ngiven `shape` and `scale` parameters. See `numpy.random.gamma`.\\n\"\"\"\\n# redshift distribution\\nredshift = np.random.gamma(shape=shape, scale=scale, size=size)\\nreturn redshift\\n```\\n## Decision Drivers\\n- Ease of use: if there is too much boiler plate `Model`s will be annoying to write\\n- Clarity of implementation: the base `Model` should be easy to read, understand, and debug\\n","Decision":"- Ease of use: if there is too much boiler plate `Model`s will be annoying to write\\n- Clarity of implementation: the base `Model` should be easy to read, understand, and debug\\nAfter [discussion](https:\/\/github.com\/skypyproject\/skypy\/pull\/38) option 3 has been picked.  This will be easiest for developers to write new functions and write clean unit tests.  Within the example given above `pyungo` was just used as an example, other DAG frameworks exist and picking one should be the topic of a different ADR.\\n","tokens":301,"id":5147,"text":"## Context\\nWithin SkyPy all functions used to create a \"simulation\" will in practice be taking in some values (either parameters or columns from a table) and creating new column(s) in an output table *or* selecting specific rows from an input table.\\nThe inputs and outputs of these functions are clearly defined so a directed acyclic graph (DAG) can be constructed to determine what order the functions should be run in.\\nTo aid in the creation of the tables and the DAG a helper class or decorator should be used so the person writing the function does not have to worry about the implementation details. This class or decorator is what we are currently referring to as the `Model`.\\nFor clarity in the options below we will assume the following example function:\\n```python\\ndef redshift_gamma(shape, scale, size):\\n\"\"\"Gamma-distributed redshifts (Smail et al. 1994).\\nSample `size` redshifts from a gamma distribution with the\\ngiven `shape` and `scale` parameters. See `numpy.random.gamma`.\\n\"\"\"\\n# redshift distribution\\nredshift = np.random.gamma(shape=shape, scale=scale, size=size)\\nreturn redshift\\n```\\n## Decision Drivers\\n- Ease of use: if there is too much boiler plate `Model`s will be annoying to write\\n- Clarity of implementation: the base `Model` should be easy to read, understand, and debug\\n\n\n##Decision\n- Ease of use: if there is too much boiler plate `Model`s will be annoying to write\\n- Clarity of implementation: the base `Model` should be easy to read, understand, and debug\\nAfter [discussion](https:\/\/github.com\/skypyproject\/skypy\/pull\/38) option 3 has been picked.  This will be easiest for developers to write new functions and write clean unit tests.  Within the example given above `pyungo` was just used as an example, other DAG frameworks exist and picking one should be the topic of a different ADR.\\n","MatchedIds":[3109,5148,1031,2933,4167],"Predicted":"## Decision\nAdopt a lightweight decorator for the `Model` class that simplifies the creation of DAGs and table manipulations.\n\n### Justification\nWe will use a decorator approach because it directly ties the definition of the function to its role in the simulation process. This reduces complexity by keeping the focus on the function's core logic rather than on the surrounding infrastructure, leading to a more straightforward and easily comprehensible implementation. The decorator can automatically register the function with the DAG, define its inputs and outputs, and handle any additional boilerplate code, such as validation or logging.\n\nBy doing so, we maintain the function signature clear and keep the simulation logic close to the domain concepts, which will make the code easier to read, understand, and debug.\n\n### Implementation Outline\nWe will define a `Model` decorator that:\n1. Registers the function with the DAG.\n2. Annotates the function with input and output metadata.\n3. Handles any repetitive infrastructure logic (e.g., validation, logging).\n\nHere is a draft implementation:\n```python\nfrom functools import wraps\n\nclass Model:\n    def __init__(self, inputs, outputs):\n        self.inputs = inputs\n        self.outputs = outputs\n\n    def __call__(self, func):\n        @wraps(func)\n        def wrapper(*args, **kwargs):\n            result = func(*args, **kwargs)\n            # Insert code to register with the DAG here\n            # Insert code to handle inputs\/outputs here\n            return result\n        wrapper.inputs = self.inputs\n        wrapper.outputs = self.outputs\n        return wrapper\n\n# Example usage for the redshift_gamma function:\n@Model(inputs=['shape', 'scale', 'size'], outputs=['redshift'])\ndef redshift_gamma(shape, scale, size):\n    # Gamma-distributed redshifts (Smail et al. 1994)\n    redshift = np.random.gamma(shape=shape, scale=scale, size=size)\n    return redshift\n```\n\n### Positive Consequences\n- **Ease of Use:** Simplifies the process of writing models by reducing the boilerplate code.\n- **Clarity:** Provides a clear and consistent way to define simulation functions and their roles.\n- **Maintainability:** Makes it easier to understand, extend, and debug the base `Model`.\n\n### Negative Consequences\n- **Decorator Overhead:** Some users may not be familiar with decorators, leading to a small learning curve.\n- **Limited Flexibility:** More advanced customization may require bypassing or extending the decorator pattern.\n\nOverall, the decorator approach strikes a balance between ease of use and clarity, ensuring that models are simple to write, maintain, and understand while providing the needed infrastructure automatically.","GenTime":"2024-08-19 23:34:53"}
{"File Name":"archcolider\/008 At least once delivery for ready to pay order.md","Context":"## Context\\n\"Ready to pay\" orders should be handled with special care. We'd like to guarantee \"at least once delivery\" for each order. Order payment processing is a business-critical scenario because selling meals is the whole point. In this case, there should be a guarantee, that the order store and payment processor can pick the order for execution.\\nAt the same time, it's very important to avoid double payments, because of concurrency issues. When an order with a \"Ready to pay\" state arrives, in theory, it can be processed one or more times, but it should not lead to doubled, tripled, and so on charges from a user's account.\\n","Decision":"Delivery of \"ready to pay\" orders performed by a MessageQueue software with a message acknowledgment option. Additionally, we expect that the order comes with a unique id from client devices at the time of processing. During order processing, the existence of the order with the same id can be checked and the version number should be used for staleness validation. In this case, the event with the same version will be discarded by the processing service.\\n","tokens":139,"id":2672,"text":"## Context\\n\"Ready to pay\" orders should be handled with special care. We'd like to guarantee \"at least once delivery\" for each order. Order payment processing is a business-critical scenario because selling meals is the whole point. In this case, there should be a guarantee, that the order store and payment processor can pick the order for execution.\\nAt the same time, it's very important to avoid double payments, because of concurrency issues. When an order with a \"Ready to pay\" state arrives, in theory, it can be processed one or more times, but it should not lead to doubled, tripled, and so on charges from a user's account.\\n\n\n##Decision\nDelivery of \"ready to pay\" orders performed by a MessageQueue software with a message acknowledgment option. Additionally, we expect that the order comes with a unique id from client devices at the time of processing. During order processing, the existence of the order with the same id can be checked and the version number should be used for staleness validation. In this case, the event with the same version will be discarded by the processing service.\\n","MatchedIds":[2678,307,1031,1523,2526],"Predicted":"## Decision\nWe will implement an idempotence key mechanism for the payment processing of \"Ready to pay\" orders. Each order will have a unique idempotence key that the payment processor will use to ensure that multiple attempts to process the same order do not result in multiple charges. \n\n### Implementation Steps\n1. **Generate Idempotence Key**: Generate a unique idempotence key for each order before it is marked as \"Ready to pay\".\n2. **Store Idempotence Key**: Store this idempotence key in the order store alongside the order details.\n3. **Use Idempotence Key in Payment Processor**: Whenever the payment processor picks up an order for execution, it will use the idempotence key to check if the payment for this order has already been processed.\n4. **Ensure Exactly Once Processing**: If the idempotence key indicates that the payment has already been processed, the payment processor will skip re-processing and return a successful response without charging the user's account again.\n5. **Log All Transactions**: Maintain a detailed transaction log that records every attempt to process the payment to facilitate debugging and auditing.\n\n### Positive Consequences\n* Ensures that each order payment is processed exactly once, preventing double, triple, and so on charges.\n* Facilitates robust auditing and fault tolerance by maintaining a log of all payment processing attempts.\n* Guarantees \"at least once delivery\" by allowing retries of payment processing while still ensuring single successful charge.\n\n### Negative Consequences\n* Adds a slight overhead in managing idempotence keys for each order.\n* Requires modification in both the order store and payment processor to handle idempotence keys.\n\nThis approach balances between ensuring reliability in processing payments and preventing erroneous multiple charges due to concurrency issues.","GenTime":"2024-08-19 23:35:13"}
{"File Name":"android-guidelines\/0009-gradle-rules.md","Context":"## Context and Problem Statement\\nNumber of module count has been increased our build times. Enabling unused plugins and using **gradle.kts** is causing to longer build times.\\n","Decision":"* Disable generating BuildConfig file if its not needed in module.\\n* Only enable *databinding* if you're going to use DataBinding in that module.\\n* Do not apply *kapt* plugin if you're not going to use.\\n* Do not create new variants other than *debug* and *release*.\\n* Use groovy scripts on *build.gradle* files.\\n","tokens":37,"id":3860,"text":"## Context and Problem Statement\\nNumber of module count has been increased our build times. Enabling unused plugins and using **gradle.kts** is causing to longer build times.\\n\n\n##Decision\n* Disable generating BuildConfig file if its not needed in module.\\n* Only enable *databinding* if you're going to use DataBinding in that module.\\n* Do not apply *kapt* plugin if you're not going to use.\\n* Do not create new variants other than *debug* and *release*.\\n* Use groovy scripts on *build.gradle* files.\\n","MatchedIds":[1463,1279,2626,351,1739],"Predicted":"## Decision\nWe will review and disable any unused plugins to streamline the build process. Additionally, we will evaluate the performance implications of using **gradle.kts** versus **build.gradle** (Groovy) and consider switching to **build.gradle** if it provides significantly shorter build times. We will continuously monitor and optimize our build configurations to ensure efficiency.","GenTime":"2024-08-19 23:35:16"}
{"File Name":"verify-frontend\/0004-journey-hint-tracking-cookie.md","Context":"## Context\\nThe encrypted `verify-front-journey-hint` cookie was originally created to enable the non-repudiation journey.\\nWhen user selected an IDP, the entity_id was stored in the cookie and if the RP then sent a new Authn request\\nwith a parameter `journey_hint=submission_confirmation` the `\/confirm-your-identity` page was rendered with the\\nIDP from the cookie. The cookie was a simple JSON object:\\n```\\n{\\nentity_id: \"https:\/\/idp-entity-id.com\"\\n}\\n```\\nIn the early 2018 we introduced a sign-in hint to help users remind what IDP they used previously. The cookie has\\nbeen repurposed and new properties were added to it to track the IDP for each status the user encountered.\\nThe expiry date was also extended to 18 months. For example:\\n```\\n{\\nentity_id: \"https:\/\/idp-entity-id.com\",\\nATTEMPT: \"https:\/\/idp-entity-id.com\",\\nSUCCESS: \"https:\/\/idp-entity-id.com\",\\nFAILED: \"https:\/\/idp-entity-id-1.com\",\\n...\\n}\\n```\\nIf the user has any value in SUCCESS we show the user the sign-in hint for that IDP.\\n","Decision":"While implementing a new Pause & Resume functionality, we came across a requirement when we needed to store the status,\\nthe selected IDP and also the RP user has paused with. Rather than creating a new cookie it was decided to re-factor the current\\njourney-hint cookie to support this. In order to know what the latest state was, a new `STATE` object was introduced in the schema.\\nThe `STATE` gets updated with every new Authn response from an IDP. The `ATTEMPT` and `SUCCESS` was kept to keep the cookie backwards\\ncompatible and to help with identifying if there ever was a successful verification. The `entity_id` property got removed and the\\nnon-repudiation journey now uses the `ATTEMPT` value. The cookie schema now looks like this:\\n```\\n{\\nATTEMPT: \"https:\/\/idp-entity-id.com\",\\nSUCCESS: \"https:\/\/idp-entity-id.com\",\\nSTATE:  {\\nIDP: \"https:\/\/idp-entity-id.com\",\\nRP: \"https:\/\/rp-entity-id.com\",\\nSTATUS: <SUCCESS | FAILED | FAILED_UPLIFT | CANCEL | PENDING>\\n}\\n}\\n```\\n","tokens":273,"id":1449,"text":"## Context\\nThe encrypted `verify-front-journey-hint` cookie was originally created to enable the non-repudiation journey.\\nWhen user selected an IDP, the entity_id was stored in the cookie and if the RP then sent a new Authn request\\nwith a parameter `journey_hint=submission_confirmation` the `\/confirm-your-identity` page was rendered with the\\nIDP from the cookie. The cookie was a simple JSON object:\\n```\\n{\\nentity_id: \"https:\/\/idp-entity-id.com\"\\n}\\n```\\nIn the early 2018 we introduced a sign-in hint to help users remind what IDP they used previously. The cookie has\\nbeen repurposed and new properties were added to it to track the IDP for each status the user encountered.\\nThe expiry date was also extended to 18 months. For example:\\n```\\n{\\nentity_id: \"https:\/\/idp-entity-id.com\",\\nATTEMPT: \"https:\/\/idp-entity-id.com\",\\nSUCCESS: \"https:\/\/idp-entity-id.com\",\\nFAILED: \"https:\/\/idp-entity-id-1.com\",\\n...\\n}\\n```\\nIf the user has any value in SUCCESS we show the user the sign-in hint for that IDP.\\n\n\n##Decision\nWhile implementing a new Pause & Resume functionality, we came across a requirement when we needed to store the status,\\nthe selected IDP and also the RP user has paused with. Rather than creating a new cookie it was decided to re-factor the current\\njourney-hint cookie to support this. In order to know what the latest state was, a new `STATE` object was introduced in the schema.\\nThe `STATE` gets updated with every new Authn response from an IDP. The `ATTEMPT` and `SUCCESS` was kept to keep the cookie backwards\\ncompatible and to help with identifying if there ever was a successful verification. The `entity_id` property got removed and the\\nnon-repudiation journey now uses the `ATTEMPT` value. The cookie schema now looks like this:\\n```\\n{\\nATTEMPT: \"https:\/\/idp-entity-id.com\",\\nSUCCESS: \"https:\/\/idp-entity-id.com\",\\nSTATE:  {\\nIDP: \"https:\/\/idp-entity-id.com\",\\nRP: \"https:\/\/rp-entity-id.com\",\\nSTATUS: <SUCCESS | FAILED | FAILED_UPLIFT | CANCEL | PENDING>\\n}\\n}\\n```\\n","MatchedIds":[4843,4436,581,4454,2405],"Predicted":"## Decision\nTo maintain the functionality of the `verify-front-journey-hint` cookie while ensuring security and usability, the following steps will be taken:\n\n1. **Continue Using AES GCM Encryption:** The payload of the cookie will continue to be encrypted using AES GCM to provide both secrecy and authenticity.\n2. **Standardize Object Structure:** The JSON object structure will remain standard but with clearly defined properties for each status (e.g., `entity_id`, `ATTEMPT`, `SUCCESS`, `FAILED`) to ensure consistency across different touchpoints.\n3. **Maintain Expiry at 18 Months:** The expiry date of the cookie will continue to be set at 18 months to strike a balance between usability and necessity for users to remember their IDPs.\n4. **Periodic Review:** Implement a periodic review process to ensure the cookie's structure and data remain relevant and secure, accommodating any changes in the authentication process.\n5. **Enhanced Documentation:** Update documentation to clearly outline the purpose and structure of the `verify-front-journey-hint` cookie, making it easier for developers to understand and maintain.\n6. **User Notifications:** Inform users that their IDP information will be stored in an encrypted manner to aid their sign-in process, ensuring transparency in how their data is being handled.\n\nBy following these steps, we will ensure that the `verify-front-journey-hint` cookie remains secure, user-friendly, and effective in helping users remember their previously used IDPs.","GenTime":"2024-08-19 23:35:22"}
{"File Name":"dapr\/SDK-002-java-jdk-versions.md","Context":"## Context\\nDapr offers a Java SDK. Java 11 is the latest LTS version. Java 8 is the previous LTS version but still the mainly used version by the Java community in 2019. What should be the minimum Java version supported by Dapr's Java SDK?\\nSee https:\/\/github.com\/dapr\/java-sdk\/issues\/17\\n","Decision":"* Java 8 should be the minimum version supported for Dapr's Java SDK.\\n* Java 11 should be used in samples and user documentation to encourage adoption.\\n* Java 8's commercial support ends in 2022. Dapr's Java SDK shoud migrate to Java 11 prior to that. The timeline still not decided.\\n","tokens":71,"id":73,"text":"## Context\\nDapr offers a Java SDK. Java 11 is the latest LTS version. Java 8 is the previous LTS version but still the mainly used version by the Java community in 2019. What should be the minimum Java version supported by Dapr's Java SDK?\\nSee https:\/\/github.com\/dapr\/java-sdk\/issues\/17\\n\n\n##Decision\n* Java 8 should be the minimum version supported for Dapr's Java SDK.\\n* Java 11 should be used in samples and user documentation to encourage adoption.\\n* Java 8's commercial support ends in 2022. Dapr's Java SDK shoud migrate to Java 11 prior to that. The timeline still not decided.\\n","MatchedIds":[4802,3374,78,641,159],"Predicted":"## Decision\n* The minimum Java version supported by Dapr's Java SDK will be Java 8.\n\n### Rationale\n* **Widespread Adoption**: As of 2019, Java 8 remains the most widely used version among the Java community, ensuring that the SDK is accessible to the majority of developers.\n* **Long-Term Support (LTS)**: Both Java 8 and Java 11 are LTS versions, but prioritizing Java 8 guarantees compatibility with existing systems that have not yet upgraded to Java 11.\n* **Gradual Migration**: Supporting Java 8 allows existing systems to gradually migrate to newer Java versions without immediate pressure.\n\n### Positive Consequences\n* **Maximized Audience**: Ensures the SDK can be adopted by the broadest audience of Java developers still on Java 8.\n* **Stability and Compatibility**: Provides a stable and well-known environment for developers, with extensive resources and community support.\n* **Smooth Transition**: Facilitates a smoother transition path for systems planning to upgrade to Java 11 or later.\n\n### Negative Consequences\n* **Missed Modern Features**: Java 8 lacks some of the newer features and performance improvements available in Java 11 and later versions.\n* **Potential Security Updates**: While Java 8 is still supported, its longer-term security outlook is not as strong as newer LTS versions.\n\n### Implementation Notes\n* **Backward Compatibility**: Ensure that new features and enhancements in the SDK maintain backward compatibility with Java 8.\n* **Encourage Upgrades**: Include documentation and guidance on the benefits of upgrading to Java 11 to encourage users to move towards newer, more secure versions.","GenTime":"2024-08-19 23:35:32"}
{"File Name":"hodder\/0002-use-docker-compose-v-2.md","Context":"## Context\\nPeople are very used to traditional layered architectures and request\/response cycles which disguise a lot of the complexity of distributed systems. They also restrict extending systems as data is locked down in a specific form in specific systems. We want to give an alternative view with a clear demonstration of the trade-offs involved.\\nGetting people to give it an initial attempt is going to be hard, so it needs to be super quick to get working on a local machine.\\n","Decision":"We will use a docker-compose.yml (v.2) as the primary entry point of the project\\n","tokens":92,"id":311,"text":"## Context\\nPeople are very used to traditional layered architectures and request\/response cycles which disguise a lot of the complexity of distributed systems. They also restrict extending systems as data is locked down in a specific form in specific systems. We want to give an alternative view with a clear demonstration of the trade-offs involved.\\nGetting people to give it an initial attempt is going to be hard, so it needs to be super quick to get working on a local machine.\\n\n\n##Decision\nWe will use a docker-compose.yml (v.2) as the primary entry point of the project\\n","MatchedIds":[4868,3470,3483,108,1031],"Predicted":"## Decision\nWe will implement a simple event-driven architecture using a lightweight message broker like RabbitMQ. This approach will allow us to showcase the benefits of decoupling components and handling asynchronous processes, thereby elucidating the trade-offs involved in distributed systems.\n\nTo ensure quick adoption and ease of use on local machines:\n- We will provide Docker Compose configurations to set up the local environment with minimal effort.\n- A clear and concise README file will be included with step-by-step instructions to get the system up and running.\n- Example scenarios and sample code will be provided to demonstrate the event-driven workflow and how different components interact.\n\nThis setup will provide a hands-on, approachable introduction to the concept while keeping the initial barrier to entry low.","GenTime":"2024-08-19 23:35:35"}
{"File Name":"link_platform\/0016-use-devise-for-admin-authentication.md","Context":"## Context\\nWe need a some way to authenticate and manage Link Platform Adminstrators.  Administrators will need to log in to their Link Instances to manage data and configuration.\\n","Decision":"[Devise](https:\/\/github.com\/plataformatec\/devise#starting-with-rails) is a very popular gem that integrates well with ActiveRecord.  It provides support for [a ridiculous amount of authentication providers](https:\/\/github.com\/omniauth\/omniauth\/wiki\/List-of-Strategies) through Omniauth as well as a variety of features such as password reset.\\n","tokens":37,"id":5024,"text":"## Context\\nWe need a some way to authenticate and manage Link Platform Adminstrators.  Administrators will need to log in to their Link Instances to manage data and configuration.\\n\n\n##Decision\n[Devise](https:\/\/github.com\/plataformatec\/devise#starting-with-rails) is a very popular gem that integrates well with ActiveRecord.  It provides support for [a ridiculous amount of authentication providers](https:\/\/github.com\/omniauth\/omniauth\/wiki\/List-of-Strategies) through Omniauth as well as a variety of features such as password reset.\\n","MatchedIds":[3950,2397,731,3421,1545],"Predicted":"## Decision\nWe have decided to implement administrator authentication and management using AWS Cognito. AWS Cognito allows us to manage user pools and provide secure authentication mechanisms such as multi-factor authentication (MFA) and OAuth 2.0. We will create a user pool specifically for Link Platform Administrators where each administrator will have assigned roles and permissions.\n\nRoles and permissions management will be handled using AWS Cognito groups. Each administrator will be assigned to relevant groups with appropriate permissions, enabling fine-grained access control to different parts of the Link Instances.\n\nFurthermore, AWS Cognito integrates seamlessly with other AWS services, providing a robust and scalable solution without the need to manage our own authentication infrastructure. This will enable us to focus on core functionality while ensuring high security and compliance standards.","GenTime":"2024-08-19 23:35:42"}
{"File Name":"cf-k8s-networking\/0016-job-for-upgrading-istio-sidecars-on-workloads.md","Context":"## Context\\nIstio's service mesh capabilites are facilitated via sidecars injected into\\nworkload pods. These sidecars run an Istio-patched version of Envoy that is tied\\nto the version of Istio that injects them.\\nTypically when new versions of Istio are released, new versions of the sidecars\\nare released as well. Istio has been good so far about supporting older versions\\nof sidecars that were deployed before Istio was upgraded, but it is still\\n[documented best practice](https:\/\/istio.io\/latest\/docs\/setup\/upgrade\/) to roll\\nall the pods after an Istio upgrade.\\nAs an additional constraint, the operators of cf-for-k8s clusters expect to be\\nable to perform upgrades in one `kapp deploy`, with no post-install hooks or\\nother bash scripts. This limits our options considerably. See this [Slack\\nthread](https:\/\/cloudfoundry.slack.com\/archives\/CH9LF6V1P\/p1592521879117400) on\\nthat constraint.\\n","Decision":"We will use the kubernetes\\n[Job](https:\/\/kubernetes.io\/docs\/concepts\/workloads\/controllers\/job\/) resource\\nto run the kubectl command needed to roll workload pods, after waiting for the\\nnew Istio control plane to be up and healthy.\\nTo that end, we will add the necessary minimal `ServiceAccounts` and `Roles`\\nneeded to list resources in the `istio-system` namespace, and restart resources\\nin the configured workload namespace. We will also build and maintain a\\ncontainer image that contains the Job's logic.\\nAll istio components will be tagged with their Istio version so that the job can\\npositively determine that the correct version of control plane components are\\nalive and healthy. We will also name the job according to it's Istio version, so\\nthat we can take advantage of `Jobs` inherent immutability in cases where a\\ncf-for-k8s upgrade does not contain a new Istio version (pushing the same job\\nagain will not cause it to rerun, preventing workloads from rolling\\nunnecessarily). Subsequent jobs will clean up previous ones.\\n","tokens":224,"id":2240,"text":"## Context\\nIstio's service mesh capabilites are facilitated via sidecars injected into\\nworkload pods. These sidecars run an Istio-patched version of Envoy that is tied\\nto the version of Istio that injects them.\\nTypically when new versions of Istio are released, new versions of the sidecars\\nare released as well. Istio has been good so far about supporting older versions\\nof sidecars that were deployed before Istio was upgraded, but it is still\\n[documented best practice](https:\/\/istio.io\/latest\/docs\/setup\/upgrade\/) to roll\\nall the pods after an Istio upgrade.\\nAs an additional constraint, the operators of cf-for-k8s clusters expect to be\\nable to perform upgrades in one `kapp deploy`, with no post-install hooks or\\nother bash scripts. This limits our options considerably. See this [Slack\\nthread](https:\/\/cloudfoundry.slack.com\/archives\/CH9LF6V1P\/p1592521879117400) on\\nthat constraint.\\n\n\n##Decision\nWe will use the kubernetes\\n[Job](https:\/\/kubernetes.io\/docs\/concepts\/workloads\/controllers\/job\/) resource\\nto run the kubectl command needed to roll workload pods, after waiting for the\\nnew Istio control plane to be up and healthy.\\nTo that end, we will add the necessary minimal `ServiceAccounts` and `Roles`\\nneeded to list resources in the `istio-system` namespace, and restart resources\\nin the configured workload namespace. We will also build and maintain a\\ncontainer image that contains the Job's logic.\\nAll istio components will be tagged with their Istio version so that the job can\\npositively determine that the correct version of control plane components are\\nalive and healthy. We will also name the job according to it's Istio version, so\\nthat we can take advantage of `Jobs` inherent immutability in cases where a\\ncf-for-k8s upgrade does not contain a new Istio version (pushing the same job\\nagain will not cause it to rerun, preventing workloads from rolling\\nunnecessarily). Subsequent jobs will clean up previous ones.\\n","MatchedIds":[2236,2249,856,3909,620],"Predicted":"## Decision\nTo ensure a seamless upgrade process for Istio, while adhering to the constraint of using only `kapp deploy` without post-install hooks or additional bash scripts, we will implement a two-step upgrade strategy:\n\n1. **Blue-Green Deployment Approach for Istio Components**:\n    - Deploy the new Istio control plane alongside the existing version, ensuring the new version is active but not yet handling traffic.\n    - Gradually shift traffic to the new control plane components by updating the Istio `VirtualService` and `DestinationRule` configurations. This will be done in a controlled manner to ensure that both old and new versions can handle the traffic during the transition.\n\n2. **Rolling Pod Restarts**:\n    - Following the successful deployment and traffic shift to the new Istio control plane, we will leverage Kubernetes native functionality for rolling updates.\n    - A `kapp deploy` operation will trigger a rolling restart of all workload pods. This ensures that each pod is re-deployed with the new Istio sidecar version without the need for additional scripts or manual intervention.\n    - This will be achieved by annotating the deployments such that the pod template hash changes. This will instigate Kubernetes to restart the pods in a rolling fashion.\n\nBy following this balanced approach, we ensure that the cf-for-k8s clusters can be upgraded seamlessly within the constraints provided, while also maintaining service continuity and minimizing downtime.","GenTime":"2024-08-19 23:35:51"}
{"File Name":"bananatabs\/0002-react-testing-library.md","Context":"## Context\\nWe want to avoid testing implementation details in our integration tests.\\nWe want to use `react-testing-library` which makes it easier to make assertions on the rendered DOM rather than assert implementation details. But mostly because it enable us to find and trigger click events on different UI elements.\\ne.g. toggling the visibility of different tabs and window groups.\\nBut when it comes to asserting the rendered DOM, in most cases we trust the view will render the model properly.\\nIt could be more sensible to only verify the state\/model.\\n","Decision":"We will assert against the session in the provider which is accessible through the context. (the BananaContext instance, not a React Context)\\n","tokens":111,"id":2816,"text":"## Context\\nWe want to avoid testing implementation details in our integration tests.\\nWe want to use `react-testing-library` which makes it easier to make assertions on the rendered DOM rather than assert implementation details. But mostly because it enable us to find and trigger click events on different UI elements.\\ne.g. toggling the visibility of different tabs and window groups.\\nBut when it comes to asserting the rendered DOM, in most cases we trust the view will render the model properly.\\nIt could be more sensible to only verify the state\/model.\\n\n\n##Decision\nWe will assert against the session in the provider which is accessible through the context. (the BananaContext instance, not a React Context)\\n","MatchedIds":[3323,4223,452,4785,4179],"Predicted":"## Decision\nWe will use `react-testing-library` for our integration tests to facilitate assertions on the rendered DOM and to find and trigger click events on UI elements. For most cases, we will avoid asserting the rendered DOM directly and instead focus on verifying the state\/model, trusting that the view will render the model correctly. This approach will help us avoid testing implementation details and ensure our tests remain maintainable and robust.","GenTime":"2024-08-19 23:35:55"}
{"File Name":"blueprint\/0003-feature-selection-policy.md","Context":"## Context and Problem Statement\\nOpen Data Hub has release v0.8.0, some of the Elyra features required by Thoth Station experiments are\\nnot part of this ODH release. This would require to update certain components to the HEAD of main branch of ODH upstream\\nproject.\\n## Decision Drivers\\n* Opertational complexity of an environment diverging from an upstream release\\n* User needs of more current software components\\n","Decision":"* Opertational complexity of an environment diverging from an upstream release\\n* User needs of more current software components\\nChosen option: \"deploy specific versions of components\", because this will give the most efficient deployment to\\nOperate First operators and users.\\n### Positive Consequences\\n* operators can gain a maximum of experience, enabling feedback on component versions that might have not been tested\\nby the upstream project\\n* users get the feature set they need to get the most out of an Operate First deployment\\n### Negative Consequences\\n* additional deployment\/manifest customizations that are not valuable to upstream project, as they are out of scope\\nfor them\\n<!-- markdownlint-disable-file MD013 -->\\n","tokens":90,"id":3551,"text":"## Context and Problem Statement\\nOpen Data Hub has release v0.8.0, some of the Elyra features required by Thoth Station experiments are\\nnot part of this ODH release. This would require to update certain components to the HEAD of main branch of ODH upstream\\nproject.\\n## Decision Drivers\\n* Opertational complexity of an environment diverging from an upstream release\\n* User needs of more current software components\\n\n\n##Decision\n* Opertational complexity of an environment diverging from an upstream release\\n* User needs of more current software components\\nChosen option: \"deploy specific versions of components\", because this will give the most efficient deployment to\\nOperate First operators and users.\\n### Positive Consequences\\n* operators can gain a maximum of experience, enabling feedback on component versions that might have not been tested\\nby the upstream project\\n* users get the feature set they need to get the most out of an Operate First deployment\\n### Negative Consequences\\n* additional deployment\/manifest customizations that are not valuable to upstream project, as they are out of scope\\nfor them\\n<!-- markdownlint-disable-file MD013 -->\\n","MatchedIds":[3789,4609,1228,621,3915],"Predicted":"## Decision\n\nWe have decided to proceed with updating the required components to the HEAD of the main branch of the ODH upstream project. This decision is made considering both operational complexity and user needs.\n\n### Reasoning\n\n1. **User Needs**: The experiments and functionalities required by Thoth Station cannot be fully realized without the features that are available only in the latest HEAD of Elyra. The value of these features to the users and their ongoing experiments makes it imperative to have access to the latest updates.\n\n2. **Operational Complexity**: While diverging from a stable upstream release may introduce some operational complexity, this can be mitigated by:\n   - Thoroughly documenting the changes and maintaining a detailed changelog.\n   - Implementing automated testing to ensure that the integration of the HEAD components does not introduce regressions or instability.\n   - Scheduling regular syncs with the upstream project to minimize the drift and ease the eventual merge back to a stable release.\n\n### Positive Consequences\n\n- **Increased Capabilities**: Thoth Station will have access to the latest features, helping improve experiment designs and outcomes.\n- **User Satisfaction**: Meeting the user needs promptly enhances user confidence and satisfaction with our platform.\n\n### Negative Consequences\n\n- **Maintenance Overhead**: There will be additional overhead in terms of maintenance and compatibility testing.\n- **Potential Instability**: Using the HEAD of the main branch could result in encountering bugs or instability which would not be present in a stable release.\n\n### Implementation Plan\n\n1. **Component Update**: Identify the specific components and dependencies from Elyra that are needed and update them to the latest HEAD version.\n2. **Testing**: Implement a robust testing pipeline to test these updates in an isolated environment.\n3. **Documentation**: Document all changes thoroughly to ensure clear understanding and traceability.\n4. **Monitoring**: Continuously monitor the updated environment for any anomalies and be prepared to revert if major issues arise.\n\nBy taking these steps, we aim to balance the operational complexity with the need for more current software components, thereby ensuring our users' needs are effectively met without compromising on the stability and maintainability of the overall system.","GenTime":"2024-08-19 23:36:02"}
{"File Name":"atlasdb\/0009-load-and-read-streams-in-same-transaction.md","Context":"## Context\\nReading streams from within the same transaction that calls loadStream() must\\nbe supported. There are a number of benefits to doing this rather than reading\\nstreams outside the transaction.\\n1. If the transaction that calls loadStream() is run with retry, you risk\\nleaking resources because multiple streams may be created that never get\\nreturned to the caller (so the caller has no opportunity to close them).\\nThe stream can not only be read (entirely) inside the transaction, but\\nalso closed inside the transaction. This makes it safe to run with retries\\n(since the retries can also close the stream they load).\\n2. When the actual stream data is read outside of the transaction, that read\\ncan fail (for example if the stream was then deleted). If instead the read is\\nperformed inside the transaction, any failed reads would also cause the\\ntransaction to fail, which could be desired. For example, if as a result of\\nreading the stream you want to perform some writes based on the read data, it\\nmakes sense to perform all of that in a single transaction.\\n","Decision":"Reading from a stream inside the same transaction that calls loadStream()\\nshould be the preferred method unless you have a specific reason that the reads\\nmust be performed later outside the transaction. Reading from a stream should\\nbe supported both inside the same transaction where it was loaded and outside\\n(after the transaction commits or aborts).\\n","tokens":229,"id":3099,"text":"## Context\\nReading streams from within the same transaction that calls loadStream() must\\nbe supported. There are a number of benefits to doing this rather than reading\\nstreams outside the transaction.\\n1. If the transaction that calls loadStream() is run with retry, you risk\\nleaking resources because multiple streams may be created that never get\\nreturned to the caller (so the caller has no opportunity to close them).\\nThe stream can not only be read (entirely) inside the transaction, but\\nalso closed inside the transaction. This makes it safe to run with retries\\n(since the retries can also close the stream they load).\\n2. When the actual stream data is read outside of the transaction, that read\\ncan fail (for example if the stream was then deleted). If instead the read is\\nperformed inside the transaction, any failed reads would also cause the\\ntransaction to fail, which could be desired. For example, if as a result of\\nreading the stream you want to perform some writes based on the read data, it\\nmakes sense to perform all of that in a single transaction.\\n\n\n##Decision\nReading from a stream inside the same transaction that calls loadStream()\\nshould be the preferred method unless you have a specific reason that the reads\\nmust be performed later outside the transaction. Reading from a stream should\\nbe supported both inside the same transaction where it was loaded and outside\\n(after the transaction commits or aborts).\\n","MatchedIds":[3096,1031,1558,3123,2896],"Predicted":"## Decision\n\nWe will support reading streams from within the same transaction that calls `loadStream()`. This will ensure that all stream reads and any associated writes occur within a single transactional context, thereby maintaining consistency and resource management.\n\nTo implement this:\n\n1. **Transaction Synchronization**:\n    - Ensure that the `loadStream()` method is capable of managing stream resources within the transactional boundary. This would involve opening, reading, and closing the stream in a way that ties the stream lifecycle to the transaction lifecycle.\n  \n2. **Retries and Resource Management**:\n    - Ensure that any transaction retry mechanisms are also responsible for closing any opened streams. This will prevent resource leaks when transactions are retried, as each transaction attempt will close the streams it opens.\n\n3. **Atomic Transactions**:\n    - Perform all stream reads and any resultant writes within the same transaction. This ensures that if a stream is deleted or becomes unavailable during the transaction, the entire transaction will fail, preserving data consistency and atomicity. This will require proper exception handling to roll back the transaction in case of stream read failures.\n\n4. **Stream Handling**:\n    - Streams will be handled using in-memory buffers if they can fit within the transaction's memory constraints. For larger streams, implement a chunked reading mechanism that reads and processes data in manageable segments within the transaction boundary.\n\n5. **Integration Testing**:\n    - Develop comprehensive tests to simulate different scenarios including transaction retries, stream deletions during transactions, and sequences of read and write operations. This will help ensure that the implementation performs as expected under various conditions.\n\nBy adopting this approach, we will achieve a robust and reliable system that ensures resource management and data consistency, even in the face of transaction retries and stream reading complexities.","GenTime":"2024-08-19 23:36:11"}
{"File Name":"dotcom-rendering\/009-static-types.md","Context":"## Context\\nAdding static types to a JavaScript project can be incredibly useful to reduce the likelihood of introducing runtime type errors.\\nWe have considered two methods of adding static types to dotcom-rendering.\\n### Flow\\nFlow makes it easy to retrofit a useful level of implicit type checking to a legacy project.\\nHowever, it produces error messages that are difficult to understand.\\nFlow requires developers to check in types for 3rd party libraries. These can be automatically generated based on the `package.json`, but developers must remember to keep these up to date.\\nIts configuration is often too coarse-grained, meaning code gets littered with opt-outs and `any` types. It doesn't feel like a natural type system, as it is only annotations on top of a dynamic language.\\nFlow's types are not exact by default.\\nThe ecosystem is still immature.\\n### TypeScript\\nTypeScript produces more understandable error messages.\\nIt allows developers to install versioned types for third party libraries.\\nTypeScript is a language in its own right, and feels more natural. It can improve JavaScript language features to make them more type-friendly. Its configuration is more fine-grained.\\nTypes are exact by default.\\nThe ecosystem is more mature.\\n","Decision":"Use TypeScript for static typing.\\n","tokens":252,"id":2652,"text":"## Context\\nAdding static types to a JavaScript project can be incredibly useful to reduce the likelihood of introducing runtime type errors.\\nWe have considered two methods of adding static types to dotcom-rendering.\\n### Flow\\nFlow makes it easy to retrofit a useful level of implicit type checking to a legacy project.\\nHowever, it produces error messages that are difficult to understand.\\nFlow requires developers to check in types for 3rd party libraries. These can be automatically generated based on the `package.json`, but developers must remember to keep these up to date.\\nIts configuration is often too coarse-grained, meaning code gets littered with opt-outs and `any` types. It doesn't feel like a natural type system, as it is only annotations on top of a dynamic language.\\nFlow's types are not exact by default.\\nThe ecosystem is still immature.\\n### TypeScript\\nTypeScript produces more understandable error messages.\\nIt allows developers to install versioned types for third party libraries.\\nTypeScript is a language in its own right, and feels more natural. It can improve JavaScript language features to make them more type-friendly. Its configuration is more fine-grained.\\nTypes are exact by default.\\nThe ecosystem is more mature.\\n\n\n##Decision\nUse TypeScript for static typing.\\n","MatchedIds":[2290,3153,1410,1404,4224],"Predicted":"## Decision\nWe will use TypeScript for adding static types to dotcom-rendering. \n\nTypeScript provides more easily understandable error messages, allowing for quicker and more accurate debugging. It offers versioned types for third-party libraries, ensuring that type definitions are always in sync with library versions. As a language in its own right, TypeScript feels more integrated and natural, enhancing JavaScript with better type-friendly features and fine-grained configuration options. Types are exact by default, which reduces ambiguity and potential errors. Moreover, TypeScript has a more mature and widely adopted ecosystem, offering better long-term support and community resources.","GenTime":"2024-08-19 23:36:14"}
{"File Name":"csw-backend\/0002-not-cloud-agnostic.md","Context":"## Context\\nSince Cloud Security Watch specifically aims to monitor for\\nmisconfigurations in AWS accounts it does not make sense to\\nmake the tool cloud agnositic.\\n","Decision":"Whilst we support the notion of writing cloud agnostic\\ncode in general. In this instance it is not appropriate\\nor desirable.\\n","tokens":36,"id":419,"text":"## Context\\nSince Cloud Security Watch specifically aims to monitor for\\nmisconfigurations in AWS accounts it does not make sense to\\nmake the tool cloud agnositic.\\n\n\n##Decision\nWhilst we support the notion of writing cloud agnostic\\ncode in general. In this instance it is not appropriate\\nor desirable.\\n","MatchedIds":[1464,2471,4358,3147,1372],"Predicted":"## Decision\nWe will focus on developing Cloud Security Watch with a specific emphasis on AWS. This decision allows us to leverage AWS-specific features and services, such as AWS Config, AWS CloudTrail, and AWS Security Hub, to comprehensively monitor and detect misconfigurations. By concentrating on AWS, we can ensure deeper integration and more specialized functionality tailored to AWS environments, thus providing better security outcomes for our users.","GenTime":"2024-08-19 23:36:18"}
{"File Name":"hmpps-interventions-ui\/0006-use-jest-for-testing.md","Context":"## Context\\nWe want a test framework that has good support for TypeScript and Node. Jest is\\na fast testing framework with good resources for mocking.\\n","Decision":"We will use Jest as our testing framework.\\n","tokens":31,"id":452,"text":"## Context\\nWe want a test framework that has good support for TypeScript and Node. Jest is\\na fast testing framework with good resources for mocking.\\n\n\n##Decision\nWe will use Jest as our testing framework.\\n","MatchedIds":[4223,3323,2273,1387,4785],"Predicted":"## Decision\nWe will use Jest as our testing framework for TypeScript and Node. It provides excellent support for both and offers robust mocking capabilities.","GenTime":"2024-08-19 23:36:20"}
{"File Name":"content-publisher\/0009-modelling-history.md","Context":"## Context\\nThe Content Publisher domain model is focused on storing current\\ninformation. There is a documents table which stores the most recent content of\\na document and an images table which stores the most recent version of\\na document's images. History of these changes is stored using [Papertrail][],\\nwhich is not [intended as a permanent store][papertrail-pr].\\nThe presentation of a document's history is done via a TimelineEntry model\\nwhich stores that a user did something, but lacks any further detail. In places\\nwhere richer information was required there have been models such as Removal or\\nWithdrawal associated with a TimelineEntry.\\nThis has led to a number of pain points:\\n- users cannot discard a new draft of a published document, because creating a\\nnew draft overwrites the data stored for the published edition\\n- Content Publisher can't show an accurate link or status for the live edition\\nof a document when a new draft of a published document is created;\\n- users cannot edit or remove images on a document once the first\\nedition is published;\\n- the TimelineEntry model stores aspects of a document's state, resulting in it\\nneeding to be queried outside a timeline context which limits flexibility\\nfor the timeline.\\nAnd this prevents a number of intended features for Content Publisher:\\n- comparing different editions of a document;\\n- republishing live content if there are any problems (currently a common\\nsupport task for Whitehall publisher);\\n- showing users what changes a user made in a particular edit.\\n","Decision":"This ADR proposes changes to the domain model to resolve the aforementioned\\npain points and provide a means to support the future intended features. These\\nchanges provide the means to store the individual editions of a document,\\neach revision of the content of a document and each status an edition has held.\\nAs per [ADR-3](0003-initial-domain-modelling.md) it does not consider the\\noption of sharing data between translations of a document as there are not\\nthe appropriate product decisions for this.\\nA common theme in this decision is\\n[immutablity in models](#approach-to-mutabilityimmutability), which is used\\nas an implicit means of storing a history. Immutability is a key consideration\\nin modelling [revisions of a document](#breakdown-of-revision) and\\n[images](#image-modelling). This ADR then considers the impacts of\\nstoring history for [timeline](#timeline) and [topics](#topics), both areas\\nwhere the usage\/need of history is less clear. Finally, this ADR concludes with\\na [collated diagram](#collated-diagram) of the domain model concepts.\\n### Core Concepts\\n![Main concepts](0009\/main-concepts-diagram.png)\\n**Document**: A record that represents all versions of a piece of content in a\\nparticular locale. It has many editions and at any time it will have a current\\nedition - shown on Content Publisher index - and potentially a live edition\\nwhich is currently on GOV.UK. The live and current edition can be\\nthe same. Each iteration of a document's content is represented as a revision\\non the current edition, thus a document has many revisions. Document is a\\nmutable entity that is used to store data common across all editions (such as\\nfirst publishing date) and it is expected to be a joining point for\\ndocument-related data that is not associated with a particular edition.\\n**Edition**: A numbered version of a document that has been, or is\\nexpected to be, published on GOV.UK. It is associated with a revision\\nand a status. It is mutable so that it can be a consistent object that\\njoins to immutable data. It is a place where any edition-level\\ndatabase constraints can be placed, such as the constraint that only one live\\nedition can exist per document. It is supported that two editions of the same\\ndocument share the same revision. This allows them to explicitly reference the\\nsame content, which supports a future ability to revert a document to past\\ncontent.\\n**Revision**: Represents an immutable snapshot of the content of a document at a\\nparticular point in time. It has a number to indicate which revision of the\\ndocument it is and stores who created it. Any request by a user that changes\\ncontent should result in a single new revision. This is to directly map the\\nconcept of a revision to each time a user revises a document. Data outside of\\ncontent, such as state, should not be stored in a revision to ensure that\\ndifferences between revisions can be represented to a user. The\\n[anatomy of a Revision model](#breakdown-of-revision) is explored further in\\nthis document.\\n**Status**: Represents a state that an edition can hold such as: \"draft\" or\\n\"submitted for review\". This model is coupled to the concept of status that is\\nshown and changed by a user. Each time a user changes the status of an edition\\na new Status model is created and the user who created it stored. An edition\\ncan only have one status at any one time. If a status has data specific to\\nthat status, such as an explanatory note for a withdrawal, this can be stored\\nin a specific model associated by a polymorphic relation. This allows for\\nmodels, such as Removal or Withdrawal, to no longer be the responsibility of\\nTimelineEntry. Initially this object is intended to be immutable, however this\\nmay be changed if status changes become asynchronous operations. This is so\\nthat a single status change performed by a user can still be represented by\\na single record.\\n### Approach to mutability\/immutability\\nA number of the models in Content Publisher are defined as immutable, most\\nsignificantly [Revision and associated models](#breakdown-of-revision). These\\nmodels should be persisted to the database once and never be updated or deleted.\\nAny need to change them requires creating a new record. This allows us to store\\na full history by only appending to the database.\\nFor simplicity, performance and consistency with Rails idioms the accessing\\nof immutable models is intended to be done by foreign key and not by the usage\\nof `SELECT MAX` style queries. This maintains the ability to use the regular\\napproach to ActiveRecord associations and the means to require the existence of\\na association (by specifying a foreign key cannot be null). An example of this\\nmodelling is the mutable Edition model which references an immutable model,\\nRevision, that stores the content. Edition is accessed by a\\nconsistent primary key and the revision accessed by a foreign key stored on\\nthe edition.\\nSince the data on a mutable model can be lost when the model is updated these\\nshould not be used for data where there is a need for history. For example, to\\nstore the statuses an edition has held there are individual status models that\\nreference the Edition. This allows an edition to reference a single status that\\nis replaced while a history is maintained.\\nThe choice of this immutability strategy is to store both present and\\nhistorical concerns in the same way, thus ensuring history remains a\\nfirst class citizen. A nice side effect of having immutable models is\\nthis opens options for caching. Since data for that\\nmodel will never change it can effectively be cached forever.\\n### Breakdown of Revision\\nAs Revision is an immutable model, used to store each edit of a Document, there\\nis likely to be a large amount of these with often only minor differences\\nbetween them. To address this a Revision is not stored as a single model but\\ninstead as a collection of models, where the Revision model stores little data\\nand joins to other models. This can be visualised as:\\n![Revision breakdown](0009\/revision-diagram.png)\\nThe intention of breaking this up is to be conservative with the amount of data\\nduplicated between consecutive revisions. For example when a user edits\\nthe title of an edition a new ContentRevision is created and the existing\\nTagsRevision, MetadataRevision and ImageRevisions models are associated with\\nthe next revision. An ImageRevision is modelled in a similar way to a Revision\\nand this is explained further in [Image modelling](#image-modelling).\\nIt is intended that [delegation][delegate] be used when interfacing with a\\nrevision so that the caller need not be concerned with which sub-revision\\nstores particular fields. This allows a revision to have a rich interface\\ndespite storing a low amount of data directly.\\n### Image modelling\\nContent Publisher supports a user uploading image files and referencing them\\nin a revision of a document. They have metadata and editable properties that a\\nuser can change, of which a history is stored. A single image file uploaded\\nproduces multiple files that are uploaded to Asset Manager for different sizing\\nvariations. Images are modelled in a similar way to Revision with an\\nimmutable Image::Revision model, as represented below:\\n![Image Revision breakdown](0009\/image-revision-diagram.png)\\nThe Image model itself is used for continuation between image revisions. It is\\nknown that two Image::Revisions are versions of the same item if they share the\\nsame Image association. The id of the Image is used in Content Publisher URLs\\nto consistently reference the Image no matter which revision it is.\\nThe data of an Image::Revision is stored between an Image::FileRevision and an\\nImage::MetadataRevision. Both are immutable and they differ by the fact that\\nany change to Image::FileRevision requires changes to the resultant Asset\\nManager files (such as crop dimensions), whereas Image::MetadataRevision stores\\naccompanying data that doesn't affect the Asset Manager files (such as alt\\ntext).\\nEach Image::FileRevision is associated with an ActiveStorage::Blob object that\\nis responsible for managing the storage of the source file. It also has a one\\nto many association with Image::Asset. Each Image::Asset represents resultant\\nfiles that are uploaded to Asset Manager for the various image sizes. The\\nImage::Asset model stores the URL to the Asset Manager file and what state the\\nfile is on Asset Manager.\\n### Timeline\\nThe TimelineEntry model represents an event that should be shown to a user as\\npart of a visual timeline of a document's history. In order for the timeline to\\nbe a flexible feature that can be iterated, this model should not be used\\noutside of the timeline context. Previously models such as Removal and\\nWithdrawal were associated directly with a TimelineEntry which\\nmeant state was accessed through the timeline. These are now suggested to be\\nassociated with a Status model.\\nAt the time of writing it wasn't yet determined what the\\ntimeline would show, and therefore it wasn't clear exactly how\\nbest to model an entry for it. Because of this TimelineEntry is modelled in a\\nspeculative way with a number of references to relevant data, including a\\npolymorphic association for flexibility.\\nThe TimelineEntry model should not store data which could not be\\nderived from other aspects of a document. This is the allow the ability to\\nrebuild TimelineEntry models if the needs of the timeline changed and to avoid\\ntimeline being an aspect of a document's state.\\n### Topics\\nNo data related to topics (otherwise known as GOV.UK taxonomy) is intended to\\nbe stored in Content Publisher at this current point in time. This is due to\\ntopics being accessed and edited by directly interacting with the Publishing\\nAPI. Other applications, notably Content Tagger, can also edit topics which has\\nthe consequence that the Publishing API is the source of truth for this data\\nrather than Content Publisher.\\nThis inconsistency makes it difficult to store the history of topics in a\\nreliable way. Thus, until needs are determined to store past topics, Content\\nPublisher will only know current topics (by querying Publishing API) and\\nthe history of topics will not be available.\\n### Collated diagram\\nThe various aspects of this decision can be collated together to be represented\\nin the following, somewhat daunting, diagram. It has been simplified for ease\\nof reading with noticeable omissions being a User model, since most models have\\na creator, and the speculative joins of a TimelineEntry.\\n![Domain model diagram](0009\/domain-model-diagram.png)\\n","tokens":317,"id":4875,"text":"## Context\\nThe Content Publisher domain model is focused on storing current\\ninformation. There is a documents table which stores the most recent content of\\na document and an images table which stores the most recent version of\\na document's images. History of these changes is stored using [Papertrail][],\\nwhich is not [intended as a permanent store][papertrail-pr].\\nThe presentation of a document's history is done via a TimelineEntry model\\nwhich stores that a user did something, but lacks any further detail. In places\\nwhere richer information was required there have been models such as Removal or\\nWithdrawal associated with a TimelineEntry.\\nThis has led to a number of pain points:\\n- users cannot discard a new draft of a published document, because creating a\\nnew draft overwrites the data stored for the published edition\\n- Content Publisher can't show an accurate link or status for the live edition\\nof a document when a new draft of a published document is created;\\n- users cannot edit or remove images on a document once the first\\nedition is published;\\n- the TimelineEntry model stores aspects of a document's state, resulting in it\\nneeding to be queried outside a timeline context which limits flexibility\\nfor the timeline.\\nAnd this prevents a number of intended features for Content Publisher:\\n- comparing different editions of a document;\\n- republishing live content if there are any problems (currently a common\\nsupport task for Whitehall publisher);\\n- showing users what changes a user made in a particular edit.\\n\n\n##Decision\nThis ADR proposes changes to the domain model to resolve the aforementioned\\npain points and provide a means to support the future intended features. These\\nchanges provide the means to store the individual editions of a document,\\neach revision of the content of a document and each status an edition has held.\\nAs per [ADR-3](0003-initial-domain-modelling.md) it does not consider the\\noption of sharing data between translations of a document as there are not\\nthe appropriate product decisions for this.\\nA common theme in this decision is\\n[immutablity in models](#approach-to-mutabilityimmutability), which is used\\nas an implicit means of storing a history. Immutability is a key consideration\\nin modelling [revisions of a document](#breakdown-of-revision) and\\n[images](#image-modelling). This ADR then considers the impacts of\\nstoring history for [timeline](#timeline) and [topics](#topics), both areas\\nwhere the usage\/need of history is less clear. Finally, this ADR concludes with\\na [collated diagram](#collated-diagram) of the domain model concepts.\\n### Core Concepts\\n![Main concepts](0009\/main-concepts-diagram.png)\\n**Document**: A record that represents all versions of a piece of content in a\\nparticular locale. It has many editions and at any time it will have a current\\nedition - shown on Content Publisher index - and potentially a live edition\\nwhich is currently on GOV.UK. The live and current edition can be\\nthe same. Each iteration of a document's content is represented as a revision\\non the current edition, thus a document has many revisions. Document is a\\nmutable entity that is used to store data common across all editions (such as\\nfirst publishing date) and it is expected to be a joining point for\\ndocument-related data that is not associated with a particular edition.\\n**Edition**: A numbered version of a document that has been, or is\\nexpected to be, published on GOV.UK. It is associated with a revision\\nand a status. It is mutable so that it can be a consistent object that\\njoins to immutable data. It is a place where any edition-level\\ndatabase constraints can be placed, such as the constraint that only one live\\nedition can exist per document. It is supported that two editions of the same\\ndocument share the same revision. This allows them to explicitly reference the\\nsame content, which supports a future ability to revert a document to past\\ncontent.\\n**Revision**: Represents an immutable snapshot of the content of a document at a\\nparticular point in time. It has a number to indicate which revision of the\\ndocument it is and stores who created it. Any request by a user that changes\\ncontent should result in a single new revision. This is to directly map the\\nconcept of a revision to each time a user revises a document. Data outside of\\ncontent, such as state, should not be stored in a revision to ensure that\\ndifferences between revisions can be represented to a user. The\\n[anatomy of a Revision model](#breakdown-of-revision) is explored further in\\nthis document.\\n**Status**: Represents a state that an edition can hold such as: \"draft\" or\\n\"submitted for review\". This model is coupled to the concept of status that is\\nshown and changed by a user. Each time a user changes the status of an edition\\na new Status model is created and the user who created it stored. An edition\\ncan only have one status at any one time. If a status has data specific to\\nthat status, such as an explanatory note for a withdrawal, this can be stored\\nin a specific model associated by a polymorphic relation. This allows for\\nmodels, such as Removal or Withdrawal, to no longer be the responsibility of\\nTimelineEntry. Initially this object is intended to be immutable, however this\\nmay be changed if status changes become asynchronous operations. This is so\\nthat a single status change performed by a user can still be represented by\\na single record.\\n### Approach to mutability\/immutability\\nA number of the models in Content Publisher are defined as immutable, most\\nsignificantly [Revision and associated models](#breakdown-of-revision). These\\nmodels should be persisted to the database once and never be updated or deleted.\\nAny need to change them requires creating a new record. This allows us to store\\na full history by only appending to the database.\\nFor simplicity, performance and consistency with Rails idioms the accessing\\nof immutable models is intended to be done by foreign key and not by the usage\\nof `SELECT MAX` style queries. This maintains the ability to use the regular\\napproach to ActiveRecord associations and the means to require the existence of\\na association (by specifying a foreign key cannot be null). An example of this\\nmodelling is the mutable Edition model which references an immutable model,\\nRevision, that stores the content. Edition is accessed by a\\nconsistent primary key and the revision accessed by a foreign key stored on\\nthe edition.\\nSince the data on a mutable model can be lost when the model is updated these\\nshould not be used for data where there is a need for history. For example, to\\nstore the statuses an edition has held there are individual status models that\\nreference the Edition. This allows an edition to reference a single status that\\nis replaced while a history is maintained.\\nThe choice of this immutability strategy is to store both present and\\nhistorical concerns in the same way, thus ensuring history remains a\\nfirst class citizen. A nice side effect of having immutable models is\\nthis opens options for caching. Since data for that\\nmodel will never change it can effectively be cached forever.\\n### Breakdown of Revision\\nAs Revision is an immutable model, used to store each edit of a Document, there\\nis likely to be a large amount of these with often only minor differences\\nbetween them. To address this a Revision is not stored as a single model but\\ninstead as a collection of models, where the Revision model stores little data\\nand joins to other models. This can be visualised as:\\n![Revision breakdown](0009\/revision-diagram.png)\\nThe intention of breaking this up is to be conservative with the amount of data\\nduplicated between consecutive revisions. For example when a user edits\\nthe title of an edition a new ContentRevision is created and the existing\\nTagsRevision, MetadataRevision and ImageRevisions models are associated with\\nthe next revision. An ImageRevision is modelled in a similar way to a Revision\\nand this is explained further in [Image modelling](#image-modelling).\\nIt is intended that [delegation][delegate] be used when interfacing with a\\nrevision so that the caller need not be concerned with which sub-revision\\nstores particular fields. This allows a revision to have a rich interface\\ndespite storing a low amount of data directly.\\n### Image modelling\\nContent Publisher supports a user uploading image files and referencing them\\nin a revision of a document. They have metadata and editable properties that a\\nuser can change, of which a history is stored. A single image file uploaded\\nproduces multiple files that are uploaded to Asset Manager for different sizing\\nvariations. Images are modelled in a similar way to Revision with an\\nimmutable Image::Revision model, as represented below:\\n![Image Revision breakdown](0009\/image-revision-diagram.png)\\nThe Image model itself is used for continuation between image revisions. It is\\nknown that two Image::Revisions are versions of the same item if they share the\\nsame Image association. The id of the Image is used in Content Publisher URLs\\nto consistently reference the Image no matter which revision it is.\\nThe data of an Image::Revision is stored between an Image::FileRevision and an\\nImage::MetadataRevision. Both are immutable and they differ by the fact that\\nany change to Image::FileRevision requires changes to the resultant Asset\\nManager files (such as crop dimensions), whereas Image::MetadataRevision stores\\naccompanying data that doesn't affect the Asset Manager files (such as alt\\ntext).\\nEach Image::FileRevision is associated with an ActiveStorage::Blob object that\\nis responsible for managing the storage of the source file. It also has a one\\nto many association with Image::Asset. Each Image::Asset represents resultant\\nfiles that are uploaded to Asset Manager for the various image sizes. The\\nImage::Asset model stores the URL to the Asset Manager file and what state the\\nfile is on Asset Manager.\\n### Timeline\\nThe TimelineEntry model represents an event that should be shown to a user as\\npart of a visual timeline of a document's history. In order for the timeline to\\nbe a flexible feature that can be iterated, this model should not be used\\noutside of the timeline context. Previously models such as Removal and\\nWithdrawal were associated directly with a TimelineEntry which\\nmeant state was accessed through the timeline. These are now suggested to be\\nassociated with a Status model.\\nAt the time of writing it wasn't yet determined what the\\ntimeline would show, and therefore it wasn't clear exactly how\\nbest to model an entry for it. Because of this TimelineEntry is modelled in a\\nspeculative way with a number of references to relevant data, including a\\npolymorphic association for flexibility.\\nThe TimelineEntry model should not store data which could not be\\nderived from other aspects of a document. This is the allow the ability to\\nrebuild TimelineEntry models if the needs of the timeline changed and to avoid\\ntimeline being an aspect of a document's state.\\n### Topics\\nNo data related to topics (otherwise known as GOV.UK taxonomy) is intended to\\nbe stored in Content Publisher at this current point in time. This is due to\\ntopics being accessed and edited by directly interacting with the Publishing\\nAPI. Other applications, notably Content Tagger, can also edit topics which has\\nthe consequence that the Publishing API is the source of truth for this data\\nrather than Content Publisher.\\nThis inconsistency makes it difficult to store the history of topics in a\\nreliable way. Thus, until needs are determined to store past topics, Content\\nPublisher will only know current topics (by querying Publishing API) and\\nthe history of topics will not be available.\\n### Collated diagram\\nThe various aspects of this decision can be collated together to be represented\\nin the following, somewhat daunting, diagram. It has been simplified for ease\\nof reading with noticeable omissions being a User model, since most models have\\na creator, and the speculative joins of a TimelineEntry.\\n![Domain model diagram](0009\/domain-model-diagram.png)\\n","MatchedIds":[1857,2403,1863,1205,2517],"Predicted":"## Decision\nTo address the issues arising from the current Content Publisher domain model and to support the intended features, we will introduce a new versioning model for documents and images. This new model will maintain a comprehensive history of all editions, changes, and user actions.\n\n### Key Changes:\n1. **Versioned Documents and Images:**\n   - Implement a new table structure to store each version of a document and its associated images. This will include metadata such as version number, timestamps, and the user responsible for the changes.\n\n2. **Historical Records for Documents:**\n   - Each document will have associated historical records capturing all changes. This will enable features like comparison of different editions, and showing what changes were made in particular edits.\n\n3. **Discarding Drafts:**\n   - Allow users to discard a new draft while maintaining the integrity of the published edition. Each draft will be treated as a separate version which can be discarded or promoted independently.\n\n4. **Republishing Live Content:**\n   - Enable the ability to republish live content without creating a new draft, ensuring that the live status is accurate and replicable for recovery and support tasks.\n\n5. **Rich TimelineEntries:**\n   - Refactor the TimelineEntry model to store richer context and details about user actions, making it more flexible and informative. This will allow for better historical insight and usability within the timeline context.\n\n6. **Image Editing and Removal:**\n   - Introduce mechanisms to edit or remove images from historical editions, preserving the historical context and changes associated with the documents.\n\n### Benefits:\n1. **Enhanced Version Control:**\n   - By maintaining a detailed history of document changes, the system will provide more accurate tracking of content changes and facilitate comparison across versions.\n\n2. **Improved User Experience:**\n   - Allowing for the discarding of drafts and republishing live content offers more control and fewer restrictions, leading to better user experience and fewer support issues.\n\n3. **Robust Historical Context:**\n   - Richer TimelineEntries and historical records will offer a comprehensive view of changes, aiding in audit trails, compliance, and user understanding of content changes.\n\n4. **Flexibility in Content Management:**\n   - The ability to edit and remove images even after publication ensures flexibility and correctness in content management.\n\nThis new approach will help mitigate the current pain points and enable the implementation of new, desired features in the Content Publisher.","GenTime":"2024-08-19 23:36:28"}
{"File Name":"datalab\/0009-bastion-for-cluster-access.md","Context":"## Context\\nIt will be necessary to access the servers that form the Datalabs network but we do not\\nwant to expose any services outside of the JASMIN tenancy that we do not have to.\\n","Decision":"We have decided that all access to the cluster will be via a Bastion server over an SSH\\nconnection on port 22. We will restrict access through the firewall to known IP address\\nranges including the development workstations, the STFC VPN and the Tessella public IP\\naddress.\\nThis excludes public facing services that should be available over HTTPS on port 443 via\\na different route.\\n","tokens":44,"id":745,"text":"## Context\\nIt will be necessary to access the servers that form the Datalabs network but we do not\\nwant to expose any services outside of the JASMIN tenancy that we do not have to.\\n\n\n##Decision\nWe have decided that all access to the cluster will be via a Bastion server over an SSH\\nconnection on port 22. We will restrict access through the firewall to known IP address\\nranges including the development workstations, the STFC VPN and the Tessella public IP\\naddress.\\nThis excludes public facing services that should be available over HTTPS on port 443 via\\na different route.\\n","MatchedIds":[729,771,4056,2093,731],"Predicted":"## Decision\nWe will configure a VPN gateway within the JASMIN tenancy to provide secure remote access to the Datalabs network. All necessary services and servers within the network will be accessed through this VPN connection, ensuring they remain unexposed to external networks while still being accessible to authorized users. This will ensure a secure and controlled access environment.","GenTime":"2024-08-19 23:36:31"}
{"File Name":"opg-use-an-lpa\/0007-split-terraform-configuration.md","Context":"## Context\\nEngineers in the team want to have their code changes built as real environments in the Development AWS account so that they can have their work evaluated.\\nEvaluation includes automated and user acceptance testing.\\nWhen evaluation is finished it should be possible to destroy the environment, while retaining important artifacts and data (logs for example).\\nIt will be possible to host multiple PR environments in the Development AWS account\\n","Decision":"### Terraform Configuration\\nWe will identify and manage separately, resources at an Account level and resources at an Environment level.\\nAccount level resources, such as S3 buckets for logs, the default VPC and networking will be used Environment level resources, such as ECS clusters, DNS routes and Load Balancers.\\nThis will enable us to use Terraform Workspaces to contain an Environment completely for it to be built and destroyed simply.\\nAccount level resources will remain in place after an Environment is destroyed.\\n![split terraform configuration diagram](..\/diagrams\/Use-An-LPA_split_Terraform_configurations.png)\\n### Name Spacing\\nWe will use the github PR number and the first few characters of the branch name to namespace the environment and it's resources\\nFor example, DNS records will be created for each environment using the namespace `https:\/\/view.025-UML-93.use-an-lpa.opg.service.justice.gov.uk`\\nwhile an ECS cluster will be named `025-UML-93-use-an-lpa`\\n### Environment lifecycle\\nAn Environment will be built when a PR is opened\\nOnce evaluation is complete, the Envirnoment can be destroyed.\\nTo begin with the pipeline will print instructions for destroying an Environment.\\nLater, destroying an Environment will be automated and scheduled with a maximum life span of a day or two.\\n### CI\/CD Pipeline\\nThe Terraform steps in the pipeline will always first provision the Account level shared resources, then provision the Environment level resources.\\n![pipeline terraform steps](..\/diagrams\/Use-An-LPA_Terraform_pipeline.png)\\n","tokens":82,"id":4847,"text":"## Context\\nEngineers in the team want to have their code changes built as real environments in the Development AWS account so that they can have their work evaluated.\\nEvaluation includes automated and user acceptance testing.\\nWhen evaluation is finished it should be possible to destroy the environment, while retaining important artifacts and data (logs for example).\\nIt will be possible to host multiple PR environments in the Development AWS account\\n\n\n##Decision\n### Terraform Configuration\\nWe will identify and manage separately, resources at an Account level and resources at an Environment level.\\nAccount level resources, such as S3 buckets for logs, the default VPC and networking will be used Environment level resources, such as ECS clusters, DNS routes and Load Balancers.\\nThis will enable us to use Terraform Workspaces to contain an Environment completely for it to be built and destroyed simply.\\nAccount level resources will remain in place after an Environment is destroyed.\\n![split terraform configuration diagram](..\/diagrams\/Use-An-LPA_split_Terraform_configurations.png)\\n### Name Spacing\\nWe will use the github PR number and the first few characters of the branch name to namespace the environment and it's resources\\nFor example, DNS records will be created for each environment using the namespace `https:\/\/view.025-UML-93.use-an-lpa.opg.service.justice.gov.uk`\\nwhile an ECS cluster will be named `025-UML-93-use-an-lpa`\\n### Environment lifecycle\\nAn Environment will be built when a PR is opened\\nOnce evaluation is complete, the Envirnoment can be destroyed.\\nTo begin with the pipeline will print instructions for destroying an Environment.\\nLater, destroying an Environment will be automated and scheduled with a maximum life span of a day or two.\\n### CI\/CD Pipeline\\nThe Terraform steps in the pipeline will always first provision the Account level shared resources, then provision the Environment level resources.\\n![pipeline terraform steps](..\/diagrams\/Use-An-LPA_Terraform_pipeline.png)\\n","MatchedIds":[2471,319,1464,3147,187],"Predicted":"## Decision\nWe will use Terraform to provision ephemeral PR environments in the Development AWS account. Each PR environment will be automatically created and destroyed via CI\/CD pipeline triggers.\n\n### Use of Terraform\nUsing Terraform allows us to define the PR environments as code and track changes through version control. This makes managing the lifecycle of these environments auditable and repeatable. \n\n### CI\/CD Integration\nIntegrate the creation and destruction of PR environments with the CI\/CD pipeline (e.g., CircleCI, Jenkins). When a pull request is opened, the CI\/CD pipeline will trigger a Terraform script to create the environment. Conversely, when the PR is closed or merged, the pipeline will trigger another script to destroy the environment.\n\n### Logs and Artifacts Retention\nConfigure the CI\/CD pipeline to capture and store important artifacts and logs before destroying the PR environment. Use an S3 bucket to store these artifacts securely and efficiently. The bucket's lifecycle policies can be set to retain artifacts for a predefined period.\n\n### Multi-Tenancy and Resource Limits\nSince multiple PR environments can be hosted simultaneously, implement tagging and resource limits to manage costs and resource utilization. Leverage AWS IAM roles and policies to ensure isolated and secure environments for each PR.\n\n### Clean-Up Script\nDevelop a clean-up script as a part of the CI\/CD pipeline that ensures all resources related to the PR environment are properly cleaned up to avoid orphaned resources and unexpected costs. \n\nBy using Terraform and integrating with the CI\/CD pipeline, we'll ensure that developers have a streamlined, automated way to create and destroy environments for evaluation purposes while retaining critical data.","GenTime":"2024-08-19 23:36:39"}
{"File Name":"james\/0002-make-taskmanager-distributed.md","Context":"## Context\\nIn order to have a distributed version of James we need to have an homogeneous way to deal with `Task`.\\nCurrently, every James nodes of a cluster have their own instance of `TaskManager` and they have no knowledge of others, making it impossible to orchestrate task execution at the cluster level.\\nTasks are scheduled and ran on the same node they are scheduled.\\nWe are also unable to list or access to the details of all the `Task`s of a cluster.\\n","Decision":"Create a distribution-aware implementation of `TaskManager`.\\n","tokens":102,"id":2127,"text":"## Context\\nIn order to have a distributed version of James we need to have an homogeneous way to deal with `Task`.\\nCurrently, every James nodes of a cluster have their own instance of `TaskManager` and they have no knowledge of others, making it impossible to orchestrate task execution at the cluster level.\\nTasks are scheduled and ran on the same node they are scheduled.\\nWe are also unable to list or access to the details of all the `Task`s of a cluster.\\n\n\n##Decision\nCreate a distribution-aware implementation of `TaskManager`.\\n","MatchedIds":[2121,2140,279,2897,319],"Predicted":"## Decision\n* Implement a centralized `DistributedTaskManager` that can coordinate `Task` execution across all nodes in the cluster.\n* Store all `Task` metadata and state information in a distributed database such as Cassandra.\n* Use a messaging system like RabbitMQ or Kafka to notify and update nodes about `Task` state changes.\n* Modify existing `TaskManager` implementations on each node to interface with the `DistributedTaskManager` for task scheduling, execution, and state updates.\n* Ensure that tasks can be retrieved, listed, and managed centrally from any node in the cluster through a unified API.\n* Implement fault tolerance mechanisms to handle node failures and ensure task continuity and consistency. \n\nThis centralization ensures that all nodes are aware of every task, allowing for better orchestration and coordination of tasks at a cluster level.","GenTime":"2024-08-19 23:36:42"}
{"File Name":"js-sdk\/0016-update-threebot-deployer-flist.md","Context":"## Context\\nDeploying hosted 3Bot takes a lot of time because of cloning and installation time.\\nIt can make use of the new flist build by the CI to be always up to date and reduce the installation time.\\n","Decision":"Update the flist with the up to date one\\n","tokens":48,"id":5201,"text":"## Context\\nDeploying hosted 3Bot takes a lot of time because of cloning and installation time.\\nIt can make use of the new flist build by the CI to be always up to date and reduce the installation time.\\n\n\n##Decision\nUpdate the flist with the up to date one\\n","MatchedIds":[351,1739,3682,187,4358],"Predicted":"## Decision\nWe will utilize the new flist build produced by the CI to streamline the deployment process of the hosted 3Bot. By using pre-built flist images that are always up to date, we will significantly reduce the cloning and installation time required for each deployment. This method will allow us to deploy the hosted 3Bot more efficiently and ensure that it is always using the latest code and dependencies. Additionally, we will set up the CI pipeline to automatically generate and publish the latest flist images to ensure continuous and seamless updates.","GenTime":"2024-08-19 23:36:45"}
{"File Name":"james-project\/0032-distributed-mail-queue-cleanup.md","Context":"## Context\\nRead [Distributed Mail Queue](0031-distributed-mail-queue.md) for full context.\\n**enqueuedMailsV3** and **deletedMailsV2** is never cleaned up and the corresponding blobs are always referenced. This is not\\nideal both from a privacy and space storage costs point of view.\\nNote that **enqueuedMailsV3** and **deletedMailsV2** rely on timeWindowCompactionStrategy.\\n","Decision":"Add a new `contentStart` table referencing the point in time from which a given mailQueue holds data, for each mail queue.\\nThe values contained between `contentStart` and `browseStart` can safely be deleted.\\nWe can perform this cleanup upon `browseStartUpdate`: once finished we can browse then delete content of **enqueuedMailsV3**\\nand **deletedMailsV2** contained between `contentStart` and the new `browseStart` then we can safely set `contentStart`\\nto the new `browseStart`.\\nContent before `browseStart` can safely be considered deletable, and is applicatively no longer exposed. We don't need an\\nadditional grace period mechanism for `contentStart`.\\nFailing cleanup will lead to the content being eventually updated upon next `browseStart` update.\\nWe will furthermore delete blobStore content upon dequeue, also when the mail had been deleted or purged via MailQueue\\nmanagement APIs.\\n","tokens":97,"id":2880,"text":"## Context\\nRead [Distributed Mail Queue](0031-distributed-mail-queue.md) for full context.\\n**enqueuedMailsV3** and **deletedMailsV2** is never cleaned up and the corresponding blobs are always referenced. This is not\\nideal both from a privacy and space storage costs point of view.\\nNote that **enqueuedMailsV3** and **deletedMailsV2** rely on timeWindowCompactionStrategy.\\n\n\n##Decision\nAdd a new `contentStart` table referencing the point in time from which a given mailQueue holds data, for each mail queue.\\nThe values contained between `contentStart` and `browseStart` can safely be deleted.\\nWe can perform this cleanup upon `browseStartUpdate`: once finished we can browse then delete content of **enqueuedMailsV3**\\nand **deletedMailsV2** contained between `contentStart` and the new `browseStart` then we can safely set `contentStart`\\nto the new `browseStart`.\\nContent before `browseStart` can safely be considered deletable, and is applicatively no longer exposed. We don't need an\\nadditional grace period mechanism for `contentStart`.\\nFailing cleanup will lead to the content being eventually updated upon next `browseStart` update.\\nWe will furthermore delete blobStore content upon dequeue, also when the mail had been deleted or purged via MailQueue\\nmanagement APIs.\\n","MatchedIds":[2122,2892,2896,2145,2888],"Predicted":"## Decision\nIntroduce a cleanup mechanism to periodically purge outdated entries in **enqueuedMailsV3** and **deletedMailsV2** to address both privacy concerns and storage cost issues.\n\nThe cleanup mechanism will:\n1. **Define Retention Periods**: Configure a retention period for each table. Data older than the retention period will be eligible for deletion. These periods should be configurable based on the organization's data retention policies.\n2. **Scheduled Cleanup Jobs**: Implement scheduled jobs that run during off-peak hours to identify and delete outdated entries in **enqueuedMailsV3** and **deletedMailsV2**.\n3. **Blob Deletion Synchronization**: Ensure that the deletion of entries in **enqueuedMailsV3** and **deletedMailsV2** is synchronized with the deletion of corresponding blobs in ObjectStorage (Swift or S3). This is to prevent orphan blobs which consume storage unnecessarily.\n4. **Time Window Compaction Strategy**: Leverage the existing TimeWindowCompactionStrategy to aid in the removal of deleted data more efficiently during compaction.\n5. **Graceful Deletion**: Implement a grace period before final deletion to ensure that data is not prematurely deleted in cases of unforeseen delays or errors.\n\nAdditionally, we will monitor the cleanup process by:\n- Logging the cleanup operations including numbers of entries and blobs deleted.\n- Setting up alerts for any failures or anomalies detected during the cleanup process.\n\nLastly, ensure that proper documentation is provided with details on configuring the retention periods, scheduling cleanup jobs, and best practices for maintaining this mechanism.","GenTime":"2024-08-19 23:36:54"}
{"File Name":"dotfiles\/0004-use-vim-plug-in-favour-of-vundle.md","Context":"## Context\\nVundle hasn't been updated for a while (Aug 2019 at time of writing) and I'd\\nread a bit about how vim-plug was pretty good. And it felt like it was time for\\na change.\\n","Decision":"[vim-plug](https:\/\/github.com\/junegunn\/vim-plug) will be used to manage Vim\\nplugins in favour of [Vundle](https:\/\/github.com\/VundleVim\/Vundle.vim).\\n","tokens":51,"id":114,"text":"## Context\\nVundle hasn't been updated for a while (Aug 2019 at time of writing) and I'd\\nread a bit about how vim-plug was pretty good. And it felt like it was time for\\na change.\\n\n\n##Decision\n[vim-plug](https:\/\/github.com\/junegunn\/vim-plug) will be used to manage Vim\\nplugins in favour of [Vundle](https:\/\/github.com\/VundleVim\/Vundle.vim).\\n","MatchedIds":[1852,3827,400,4940,1576],"Predicted":"## Decision\nWe will use [vim-plug](https:\/\/github.com\/junegunn\/vim-plug) as the plugin manager for Vim.\n\nVundle has not seen updates for a significant amount of time, which raises concerns about ongoing maintenance and support. On the other hand, vim-plug is actively maintained and has a good reputation within the Vim community for its speed and ease of use. It offers features such as parallel installation of plugins, a shallow clone for faster install, and commands for managing plugins that are more intuitive compared to Vundle. Therefore, vim-plug will be adopted as the new plugin manager.","GenTime":"2024-08-19 23:36:58"}
{"File Name":"cloud-platform\/015-Keeping-Terraform-modules-up-to-date.md","Context":"## Context\\nWe maintain a lot of [terraform modules] which teams use to manage AWS\\nresources for their namespaces.\\nIn the past, lettings different namespaces use different versions of terraform\\nmodules has caused problems because there has not always been a clear upgrade\\npath from a module that is several versions old, to the latest version.\\nIn these cases, it would have been easier to have upgraded every namespace when\\nthe significant change was made to the module, because at that point in time\\nthe relevant knowledge and understanding are fresh. Leaving it until later\\nmakes the whole process much more difficult.\\n","Decision":"We decided to ensure that all namespaces are always using the latest version of\\nevery module.\\n","tokens":124,"id":621,"text":"## Context\\nWe maintain a lot of [terraform modules] which teams use to manage AWS\\nresources for their namespaces.\\nIn the past, lettings different namespaces use different versions of terraform\\nmodules has caused problems because there has not always been a clear upgrade\\npath from a module that is several versions old, to the latest version.\\nIn these cases, it would have been easier to have upgraded every namespace when\\nthe significant change was made to the module, because at that point in time\\nthe relevant knowledge and understanding are fresh. Leaving it until later\\nmakes the whole process much more difficult.\\n\n\n##Decision\nWe decided to ensure that all namespaces are always using the latest version of\\nevery module.\\n","MatchedIds":[3144,2536,1372,2793,2539],"Predicted":"## Decision\nTo streamline the upgrade process and ensure consistency across all namespaces, we will implement a policy where all namespaces must upgrade to the latest version of a Terraform module concurrently whenever a significant change is made. This ensures that the relevant knowledge and best practices are applied uniformly, minimizing the long-term complexity of managing different module versions.\n\n- Establish a clear upgrade schedule and communicate it to all teams.\n- Develop comprehensive upgrade documentation and migration guides for significant changes.\n- Implement automated testing to validate module upgrades across all namespaces prior to deployment.\n- Allocate support resources to assist teams during the upgrade process to ensure a smooth transition.","GenTime":"2024-08-19 23:37:01"}
{"File Name":"galaxy-zoo-touch-table\/adr-3.md","Context":"### Context\\nWe should structure files in a way that makes the codebase easy to navigate and intuitive when searching for certain components. Organization should adhere to accepted practices in the .NET community.\\n### Decision\\nThe MVVM (Model-View-View Model) approach is widely used by the WPF (Windows Presentation Foundation) community. It's difficult to search the web for insight into building WPF applications without running into information about MVVM architecture. MVVM appears to be the standard.\\nApplication components should be divided into a Model, View, and View Model folder, with each folder containing the necessary items for displaying the UI and interpreting data on the app.\\n### Status\\nAccepted\\n### Consequences\\nIt will be easier to find solutions to coding problems online by accepting a widely-used design pattern. However, this doesn't necessarily solve the problem of how other items should be organized (lib, images, fonts, etc.).\\n_In Retrospect:_ The MVVM pattern was overall beneficial, but I was often confused as to how strictly I should adhere to the MVVM pattern. MVVM says each view should have an accompanying view model and model. However, with an app containing so many design elements, it often felt unnecessary to have a data model tied to each view. What would the model be for a modal and how would that be different from the view model?\\n","Decision":"The MVVM (Model-View-View Model) approach is widely used by the WPF (Windows Presentation Foundation) community. It's difficult to search the web for insight into building WPF applications without running into information about MVVM architecture. MVVM appears to be the standard.\\nApplication components should be divided into a Model, View, and View Model folder, with each folder containing the necessary items for displaying the UI and interpreting data on the app.\\n### Status\\nAccepted\\n### Consequences\\nIt will be easier to find solutions to coding problems online by accepting a widely-used design pattern. However, this doesn't necessarily solve the problem of how other items should be organized (lib, images, fonts, etc.).\\n_In Retrospect:_ The MVVM pattern was overall beneficial, but I was often confused as to how strictly I should adhere to the MVVM pattern. MVVM says each view should have an accompanying view model and model. However, with an app containing so many design elements, it often felt unnecessary to have a data model tied to each view. What would the model be for a modal and how would that be different from the view model?\\n","tokens":280,"id":5082,"text":"### Context\\nWe should structure files in a way that makes the codebase easy to navigate and intuitive when searching for certain components. Organization should adhere to accepted practices in the .NET community.\\n### Decision\\nThe MVVM (Model-View-View Model) approach is widely used by the WPF (Windows Presentation Foundation) community. It's difficult to search the web for insight into building WPF applications without running into information about MVVM architecture. MVVM appears to be the standard.\\nApplication components should be divided into a Model, View, and View Model folder, with each folder containing the necessary items for displaying the UI and interpreting data on the app.\\n### Status\\nAccepted\\n### Consequences\\nIt will be easier to find solutions to coding problems online by accepting a widely-used design pattern. However, this doesn't necessarily solve the problem of how other items should be organized (lib, images, fonts, etc.).\\n_In Retrospect:_ The MVVM pattern was overall beneficial, but I was often confused as to how strictly I should adhere to the MVVM pattern. MVVM says each view should have an accompanying view model and model. However, with an app containing so many design elements, it often felt unnecessary to have a data model tied to each view. What would the model be for a modal and how would that be different from the view model?\\n\n\n##Decision\nThe MVVM (Model-View-View Model) approach is widely used by the WPF (Windows Presentation Foundation) community. It's difficult to search the web for insight into building WPF applications without running into information about MVVM architecture. MVVM appears to be the standard.\\nApplication components should be divided into a Model, View, and View Model folder, with each folder containing the necessary items for displaying the UI and interpreting data on the app.\\n### Status\\nAccepted\\n### Consequences\\nIt will be easier to find solutions to coding problems online by accepting a widely-used design pattern. However, this doesn't necessarily solve the problem of how other items should be organized (lib, images, fonts, etc.).\\n_In Retrospect:_ The MVVM pattern was overall beneficial, but I was often confused as to how strictly I should adhere to the MVVM pattern. MVVM says each view should have an accompanying view model and model. However, with an app containing so many design elements, it often felt unnecessary to have a data model tied to each view. What would the model be for a modal and how would that be different from the view model?\\n","MatchedIds":[4766,1996,3784,2261,3776],"Predicted":"## Context\nWe should structure files in a way that makes the codebase easy to navigate and intuitive when searching for certain components. Organization should adhere to accepted practices in the .NET community.\n## Decision Drivers\n* Align with widely accepted practices within the .NET\/WPF community.\n* Improve ease of navigation and maintainability.\n* Simplify finding relevant solutions and patterns in online resources and documentation.\n## Decision\nThe MVVM (Model-View-ViewModel) approach will be adopted for structuring the application. Components will be divided into folders named Model, View, and ViewModel. Each folder will contain the respective items required for displaying the UI and interpreting data within the application.\n\n### Folder Structure\n* **Model**: Contains classes representing the data and the business logic (e.g., data models, service classes).\n* **View**: Contains the XAML files and their respective code-behinds that define the user interface.\n* **ViewModel**: Contains classes that handle the presentation logic which binds the data to the View.\n\n```\nAppFolder\n\u2502\n\u251c\u2500\u2500 Model\n\u2502   \u251c\u2500\u2500 UserModel.cs\n\u2502   \u2514\u2500\u2500 ProductService.cs\n\u2502\n\u251c\u2500\u2500 View\n\u2502   \u251c\u2500\u2500 MainPage.xaml\n\u2502   \u251c\u2500\u2500 MainPage.xaml.cs\n\u2502   \u2514\u2500\u2500 ProductPage.xaml\n\u2502\n\u251c\u2500\u2500 ViewModel\n    \u251c\u2500\u2500 MainPageViewModel.cs\n    \u2514\u2500\u2500 ProductPageViewModel.cs\n```\n\n### Status\nAccepted\n\n### Consequences\n#### Positive:\n* Adopting a widely-used design pattern will make it easier to find solutions and best practices online.\n* Clear separation of concerns, making the codebase more maintainable and scalable.\n* Enhances testability by isolating the business logic from the UI.\n\n#### Negative:\n* Strict adherence to the MVVM pattern might lead to a higher number of classes and boilerplate code.\n* Some UI components may not strictly require separate models, leading to potential over-engineering.\n\n### Retrospective:\nThe MVVM pattern was overall beneficial, but some confusion arose regarding how strictly to adhere to the pattern. Not every view requires a separate model, especially for simple UI elements like modals. A pragmatic approach should be considered where MVVM is adapted based on the complexity and requirements of each component.","GenTime":"2024-08-19 23:37:10"}
{"File Name":"fxa\/0025-retire-fxa-auth-db-mysql.md","Context":"## Context and Problem Statement\\nIn the original architecture for FxA, which was based on the Persona architecture, the database API was on a separate physical layer with more restricted permissions than the web frontend's that served the public API. This separation was deemed unimportant at some point, though the details are probably lost to history, and the db service was moved to be colocated with the auth-server on the frontend servers. New services like graphql-api-server are already making direct db calls via knex in fxa-shared, which auth-server could also use.\\n## Decision Drivers\\n- Reduce complexity\\n- Improve performance\\n- Share code\\n","Decision":"- Reduce complexity\\n- Improve performance\\n- Share code\\nWe will incrementally expand the fxa-shared db API and use it in auth-server. Once the API implements all the fxa-auth-db-mysql calls we can decomission that service. Doing so will simplify the architecture and maintenance burden, make future changes easier, and should improve service performance.\\n","tokens":127,"id":362,"text":"## Context and Problem Statement\\nIn the original architecture for FxA, which was based on the Persona architecture, the database API was on a separate physical layer with more restricted permissions than the web frontend's that served the public API. This separation was deemed unimportant at some point, though the details are probably lost to history, and the db service was moved to be colocated with the auth-server on the frontend servers. New services like graphql-api-server are already making direct db calls via knex in fxa-shared, which auth-server could also use.\\n## Decision Drivers\\n- Reduce complexity\\n- Improve performance\\n- Share code\\n\n\n##Decision\n- Reduce complexity\\n- Improve performance\\n- Share code\\nWe will incrementally expand the fxa-shared db API and use it in auth-server. Once the API implements all the fxa-auth-db-mysql calls we can decomission that service. Doing so will simplify the architecture and maintenance burden, make future changes easier, and should improve service performance.\\n","MatchedIds":[387,389,3053,3776,4167],"Predicted":"## Decision\n\nChosen Option: Use direct database calls via `knex` in fxa-shared for auth-server, because:\n\n- **Reduce Complexity:** Handling database operations with a common library (`knex` in this case) across different services simplifies the architecture. It removes the need to manage a separate physical layer for database access and eliminates redundant API layers.\n- **Improve Performance:** Direct database access removes the additional network overhead and latency associated with making API calls to a separate database service layer. This should lead to a more efficient and faster data retrieval and manipulation.\n- **Share Code:** Using `knex` in fxa-shared allows code reuse across different parts of the system, ensuring consistency and reducing the amount of code duplication. It also allows for centralized management of database-related code, making maintenance easier.\n\n### Positive Consequences\n- **Simplified Architecture:** The architecture becomes easier to understand and manage without a separate database service layer.\n- **Performance Gains:** Direct database access minimizes latency and improves the overall performance of the system.\n- **Consistent Codebase:** Sharing database access code across services ensures consistency and streamlines future developments and bug fixes.\n\n### Negative Consequences\n- **Security Risks:** Direct access to the database can pose a security risk if not managed properly. Proper security measures, such as access controls and query sanitization, must be strictly enforced.\n- **Dependency on a Single Library:** Relying on `knex` means the system is dependent on this single library for all database operations, which could be risky if `knex` faces any significant issues or changes.\n- **Migration Effort:** Migrating existing services to use `knex` in fxa-shared will require effort and resources. Existing code will need to be refactored and thoroughly tested.","GenTime":"2024-08-19 23:37:16"}
{"File Name":"boxwise-flask\/Python-ORM.md","Context":"## Context\\nUp to now, database changes have been managed through [Phinx](https:\/\/phinx.org\/) in PHP, with SQL queries written as strings in PHP. Since we are migrating off of the old PHP code base of Dropapp and into the new codebase of Python \/ React, we needed to decide how to handle reading and writing to the DB going forward (GraphQL does not solve this, as you still have to hook the GraphQL interface into the DB somehow).\\n","Decision":"1. Learning Curve\\n2. Community Support\\n3. Power and Reliability considering it will be used in a production environment\\nPeewee. Despite SQLAlchemy being the gold standard of ORMs, there are no long-term volunteers who have mastered that library aside from David C. The difficulty of ramping up everyone on SQLAlchemy and maintaining it outweighs the difficulty of having potential volunteers who are familiar with SQLAlchemy pick up one of the \"easier\" ORMs. Comparing the release notes of Peewee and PonyORM, David C. commented that Peewee's recent releases relate to support of edge cases and new technology stacks, whereas PonyORM releases still appear to involve developing functionality and fixing bugs around core uses cases. We therefore concluded that Peewee is the more production-ready solution.\\n","tokens":96,"id":3053,"text":"## Context\\nUp to now, database changes have been managed through [Phinx](https:\/\/phinx.org\/) in PHP, with SQL queries written as strings in PHP. Since we are migrating off of the old PHP code base of Dropapp and into the new codebase of Python \/ React, we needed to decide how to handle reading and writing to the DB going forward (GraphQL does not solve this, as you still have to hook the GraphQL interface into the DB somehow).\\n\n\n##Decision\n1. Learning Curve\\n2. Community Support\\n3. Power and Reliability considering it will be used in a production environment\\nPeewee. Despite SQLAlchemy being the gold standard of ORMs, there are no long-term volunteers who have mastered that library aside from David C. The difficulty of ramping up everyone on SQLAlchemy and maintaining it outweighs the difficulty of having potential volunteers who are familiar with SQLAlchemy pick up one of the \"easier\" ORMs. Comparing the release notes of Peewee and PonyORM, David C. commented that Peewee's recent releases relate to support of edge cases and new technology stacks, whereas PonyORM releases still appear to involve developing functionality and fixing bugs around core uses cases. We therefore concluded that Peewee is the more production-ready solution.\\n","MatchedIds":[1561,1462,4167,5244,362],"Predicted":"## Decision\nWe will transition to using **Alembic** for database migrations and **SQLAlchemy** for ORM in our new Python codebase. Alembic is well-integrated with SQLAlchemy and provides a robust framework for database version control, which will allow us to manage schema changes more effectively. SQLAlchemy will help us write database queries in a more Pythonic manner, providing an abstraction over SQL that can improve productivity and maintainability. By leveraging these tools, we ensure seamless database operations within our Python environment while taking advantage of the best practices in modern web development.\n\n### Positive Consequences\n* Improved maintainability and readability of the database access code.\n* Seamless and more organized handling of database schema migrations.\n* Better integration with the Python ecosystem and modern development practices.\n* Increased productivity by utilizing Python-native tools.\n\n### Negative Consequences\n* Learning curve for team members who are not familiar with Alembic or SQLAlchemy.\n* Initial migration effort needed to transition from Phinx and raw SQL strings to Alembic and SQLAlchemy.","GenTime":"2024-08-19 23:37:24"}
{"File Name":"where-away\/0011-render-buttons-instead-of-anchor-elements-for-link-nodes.md","Context":"## Context\\n- semantically, an html element that takes you to a new web page when you click\\nit is an anchor -- `<a>`.\\n- I'd like tab navigation and the Enter key to work for navigating between\\nbookmarks.\\n- buttons can be accessed via Tab and activated via Enter\\n- not so with an anchor tag. See\\nhttps:\/\/stackoverflow.com\/questions\/41476292\/how-to-give-focus-to-anchor-tag-without-href,\\netc.\\n","Decision":"Model the links as `<button>` elements to make the interaction more normal.\\n","tokens":100,"id":2280,"text":"## Context\\n- semantically, an html element that takes you to a new web page when you click\\nit is an anchor -- `<a>`.\\n- I'd like tab navigation and the Enter key to work for navigating between\\nbookmarks.\\n- buttons can be accessed via Tab and activated via Enter\\n- not so with an anchor tag. See\\nhttps:\/\/stackoverflow.com\/questions\/41476292\/how-to-give-focus-to-anchor-tag-without-href,\\netc.\\n\n\n##Decision\nModel the links as `<button>` elements to make the interaction more normal.\\n","MatchedIds":[4323,1855,1210,3156,2816],"Predicted":"## Decision\nWe will use a combination of `<a>` and appropriate attributes to ensure both semantic correctness and usability. The anchor tag (`<a>`) will be used to represent the navigational element, reinforcing its semantic role. To enable keyboard accessibility, we will add the `tabindex=\"0\"` attribute to make the `<a>` element focusable via Tab, and we will use JavaScript to handle the Enter key activation. This way, we can ensure it behaves like a button in terms of keyboard interaction while still maintaining its semantic meaning.\n\nExample implementation:\n```html\n<a href=\"javascript:void(0);\" tabindex=\"0\" class=\"bookmark\" onclick=\"application.bookmarks.add('gbj1jf')\">bookmark<\/a>\n```\nAdditionally, we will include an event listener in JavaScript to handle the Enter key activation:\n```js\ndocument.querySelectorAll('.bookmark').forEach(function(element) {\n    element.addEventListener('keydown', function(event) {\n        if (event.key === 'Enter') {\n            application.bookmarks.add('gbj1jf');\n        }\n    });\n});\n```\n\nThis approach ensures that we maintain both semantic integrity and keyboard navigability without compromising on accessibility.","GenTime":"2024-08-19 23:37:29"}
{"File Name":"laundromat\/0003-choice-of-entities.md","Context":"## Context\\nThe choice of entities, i.e. what categories of information we want to anonymise and the specificity of these categories, will greatly impact the performance of our model. Additionally, time constraints mean that there is an upper limit to how many entities can be included. As such, we concluded that those entities which fulfill at least two of the following three categories should be included:\\n* It is either directly identifying or a close proxy (e.g. names, phone numbers, etc.)\\n* It is a so-called \u201cspecial categories\u201d of information (e.g. medical information)\\n* It is present in the data in non-trivial quantities\\n","Decision":"We have chosen the following NER entities:\\n* ORG (Organisation)\\n* LOC (Location)\\n* PER (Person)\\n* FNR (Personal number)\\n* MONEY\\n* DATE_TIME (Dates, time of day, name of day, and name of month)\\n* MEDICAL_CONDITIONS\\nEntities that will be left purely to RegEx are:\\n* NAV_YTELSE and NAV_OFFICE\\n* AGE\\n* TLF (Telephone number)\\n* BACC (Bank account number)\\nWe believe this list strikes the right balance between performance (fewer entities are better) and coverage.\\n","tokens":134,"id":4261,"text":"## Context\\nThe choice of entities, i.e. what categories of information we want to anonymise and the specificity of these categories, will greatly impact the performance of our model. Additionally, time constraints mean that there is an upper limit to how many entities can be included. As such, we concluded that those entities which fulfill at least two of the following three categories should be included:\\n* It is either directly identifying or a close proxy (e.g. names, phone numbers, etc.)\\n* It is a so-called \u201cspecial categories\u201d of information (e.g. medical information)\\n* It is present in the data in non-trivial quantities\\n\n\n##Decision\nWe have chosen the following NER entities:\\n* ORG (Organisation)\\n* LOC (Location)\\n* PER (Person)\\n* FNR (Personal number)\\n* MONEY\\n* DATE_TIME (Dates, time of day, name of day, and name of month)\\n* MEDICAL_CONDITIONS\\nEntities that will be left purely to RegEx are:\\n* NAV_YTELSE and NAV_OFFICE\\n* AGE\\n* TLF (Telephone number)\\n* BACC (Bank account number)\\nWe believe this list strikes the right balance between performance (fewer entities are better) and coverage.\\n","MatchedIds":[3253,443,1031,4021,3820],"Predicted":"## Decision\nWe will prioritize anonymizing entities that fulfill at least two of the three defined categories to ensure performance efficiency and compliance with data protection guidelines. Specifically, the following approach will be implemented:\n\n1. **Direct Identification and Close Proxies**: Entities like names, phone numbers, email addresses, and social security numbers that can directly identify individuals or act as close proxies will be anonymized.\n\n2. **Special Categories of Information**: Sensitive information such as medical records, racial or ethnic origin, sexual orientation, and political opinions will be anonymized.\n\n3. **Non-Trivial Quantities**: Entities present in the dataset in significant amounts will be targeted for anonymization to maximize the impact on privacy protection while balancing model performance.\n\nBy focusing on these categories, we will ensure that the most critical aspects of the data are anonymized within the given time constraints, thus maintaining data privacy without compromising model efficiency.","GenTime":"2024-08-19 23:37:32"}
{"File Name":"ftd-scratch3-offline\/0011-implement-complex-scratch-functions-in-a-helper-function.md","Context":"## Context\\nSome scratch\/ftduino functions take complex arguments that need to be verified.\\nSome scratch\/ftduino functions are complex to implement.\\nThe code for these functions could be directly generated in the java files for the specific function.\\nThe code for these functions could also be written inside a helper function, such that only a single line - that calls the helper function - is generated for such a function.\\nWriting these functions in a helper function also makes changes and code reuse easier.\\n","Decision":"Complex scratch\/ftduino functions will be implemented in helper functions and those will be called by the generated code.\\n","tokens":99,"id":2629,"text":"## Context\\nSome scratch\/ftduino functions take complex arguments that need to be verified.\\nSome scratch\/ftduino functions are complex to implement.\\nThe code for these functions could be directly generated in the java files for the specific function.\\nThe code for these functions could also be written inside a helper function, such that only a single line - that calls the helper function - is generated for such a function.\\nWriting these functions in a helper function also makes changes and code reuse easier.\\n\n\n##Decision\nComplex scratch\/ftduino functions will be implemented in helper functions and those will be called by the generated code.\\n","MatchedIds":[2631,2634,2637,1317,3637],"Predicted":"## Decision\nWe will write the code for complex functions inside helper functions. This approach will simplify the generation process by only requiring a single line to call each helper function. It will also facilitate easier changes and promote code reuse across different parts of the project.","GenTime":"2024-08-19 23:37:34"}
{"File Name":"iampeterbanjo.com\/0007-module.exports-vs-export-default.md","Context":"## Context\\nWhen HapiJS plugins are registered they expect an object like this:\\n```JavaScript\\n{\\nname: 'plugin-name',\\nversion: '1.0.0',\\nregister: (server, options) => {}\\n}\\n```\\nThe problem with `export default` is that it exports an object whose property `default` is the value of the exported object. This makes my convention of having the `index.ts` register the plugin not work because HapiJS still uses CommonJs and won't get the `default` value E.g.\\n```JavaScript\\nimport plugin from '.\/plugin';\\nexport default {\\nplugin,\\n}\\n```\\nSwitching between `module.exports` and `export default` will make using my packages tricky. It won't be clear when to use CommonJS or ES6 modules so I'll pick one and stick with it.\\nSo let's use `module.exports` instead of `export default`.\\n","Decision":"Rejected because if I try and import a module that uses `module.exports` Typescript shows an error that the file `is not a module`. The implication of this is that I have to replace [Glue][hapi-glue] to compose the API.\\n","tokens":202,"id":1596,"text":"## Context\\nWhen HapiJS plugins are registered they expect an object like this:\\n```JavaScript\\n{\\nname: 'plugin-name',\\nversion: '1.0.0',\\nregister: (server, options) => {}\\n}\\n```\\nThe problem with `export default` is that it exports an object whose property `default` is the value of the exported object. This makes my convention of having the `index.ts` register the plugin not work because HapiJS still uses CommonJs and won't get the `default` value E.g.\\n```JavaScript\\nimport plugin from '.\/plugin';\\nexport default {\\nplugin,\\n}\\n```\\nSwitching between `module.exports` and `export default` will make using my packages tricky. It won't be clear when to use CommonJS or ES6 modules so I'll pick one and stick with it.\\nSo let's use `module.exports` instead of `export default`.\\n\n\n##Decision\nRejected because if I try and import a module that uses `module.exports` Typescript shows an error that the file `is not a module`. The implication of this is that I have to replace [Glue][hapi-glue] to compose the API.\\n","MatchedIds":[5259,3031,1404,4224,3827],"Predicted":"## Decision\nWe will use `module.exports` instead of `export default` to maintain compatibility with HapiJS and avoid confusion between CommonJS and ES6 modules. This decision ensures that the plugin can be properly registered by HapiJS, adhering to the expected object structure without runtime issues caused by property `default`.","GenTime":"2024-08-19 23:37:37"}
{"File Name":"operational-data-hub\/0019-single-schema-per-topic.md","Context":"## Context\\nA schema is a vocabulary that allows you to annotate and validate documents. Every topic has a schema that can validate the messages the topic receives.\\n","Decision":"Since every topic only receives messages in JSON format (see [21. Messages are in JSON format](0021-messages-are-in-json-format.md) ), we define a JSON Schema for every topic that can validate the messages received by said topic.\\n","tokens":32,"id":2710,"text":"## Context\\nA schema is a vocabulary that allows you to annotate and validate documents. Every topic has a schema that can validate the messages the topic receives.\\n\n\n##Decision\nSince every topic only receives messages in JSON format (see [21. Messages are in JSON format](0021-messages-are-in-json-format.md) ), we define a JSON Schema for every topic that can validate the messages received by said topic.\\n","MatchedIds":[2181,4385,1106,2502,1180],"Predicted":"## Decision\nWe will use [Apache Avro](https:\/\/avro.apache.org\/docs\/current\/spec.html) as the schema specification for our message topics.\n\n### Schema Definition\nEach topic **MUST** have a schema defined in Avro's JSON schema format. The schema defines the structure of the messages that the topic will receive, including types, fields, and required attributes.\n\n### Schema Storage\nSchemas will be stored and managed in a schema registry. The schema registry **MUST** provide versioning support so that new versions of schemas can be added without disrupting existing consumers.\n\n### Validation\nAll messages sent to a topic **MUST** be validated against the corresponding schema. Messages that do not validate **MUST** be rejected with an appropriate error message. \n\n### Registration Process\n1. Define the Avro schema for a topic.\n2. Register the schema with the schema registry.\n3. Use the schema ID provided by the registry when producing and consuming messages.\n\n### Schema Evolution\nSchemas **SHOULD** be backward-compatible to ensure that consumers can read data written with previous versions of the schema. Common practices for schema evolution will include:\n- Adding new fields with default values\n- Removing fields that are no longer in use\n\n### Example Avro Schema\n```json\n{\n  \"type\": \"record\",\n  \"name\": \"example_schema\",\n  \"fields\": [\n    {\"name\": \"id\", \"type\": \"string\"},\n    {\"name\": \"timestamp\", \"type\": \"long\"},\n    {\"name\": \"payload\", \"type\": \"string\"}\n  ]\n}\n```\n\n### Topic Naming Convention\nTopic names **MUST** reflect their purpose and the schema they adhere to. Example:\n- `user_events`\n- `order_updates`\n\n### Compliance\nAll developers and systems producing or consuming messages from these topics **MUST** conform to the schema specifications defined herein.\n\nThis approach ensures consistency, compatibility, and ease of integration across different systems and services.","GenTime":"2024-08-19 23:37:43"}
{"File Name":"iampeterbanjo.com\/0009-place-tests-next-to-source-files.md","Context":"## Context\\nWhile migrating to Typescript, fixing tests means switching between the source file and their test files. Where the test files are in another folder, the journey is longer. If they were side-by-side then once I had the source or test, finding the related file would be much easier\\n","Decision":"Moving test files next to the source code makes it much easier to switch between source and tests. It's also easier to see which test files do not have any unit tests.\\n","tokens":60,"id":1588,"text":"## Context\\nWhile migrating to Typescript, fixing tests means switching between the source file and their test files. Where the test files are in another folder, the journey is longer. If they were side-by-side then once I had the source or test, finding the related file would be much easier\\n\n\n##Decision\nMoving test files next to the source code makes it much easier to switch between source and tests. It's also easier to see which test files do not have any unit tests.\\n","MatchedIds":[452,4223,3323,1594,4785],"Predicted":"## Decision\nWe will place the test files side-by-side with the source files. This way, locating and switching between the source file and its corresponding test file will be much easier, thereby improving the workflow and efficiency during the migration to TypeScript.","GenTime":"2024-08-19 23:37:46"}
{"File Name":"insight\/0001-python.md","Context":"## Context\\nWe need to have a base programming language as our first choice to implement the various functionalities. Other languages may be used as needed with proper justification.\\n","Decision":"We choose Python as our base programming language.\\n","tokens":33,"id":4761,"text":"## Context\\nWe need to have a base programming language as our first choice to implement the various functionalities. Other languages may be used as needed with proper justification.\\n\n\n##Decision\nWe choose Python as our base programming language.\\n","MatchedIds":[472,3715,1472,1000,1477],"Predicted":"## Decision\nFor our base programming language, we will use Python.\n\nPython offers several advantages that align with our project needs:\n- **Ease of Learning and Use**: Python has a simple syntax that is easy to read and write, which can accelerate development times and reduce the learning curve for new team members.\n- **Wide Adoption and Community Support**: Python is widely used across various domains, from web development to data science and machine learning. This broad adoption means a large pool of developers and extensive community support.\n- **Rich Ecosystem and Libraries**: Python has a robust set of libraries and frameworks (like Django for web development, Flask for microservices, and TensorFlow for machine learning) that can help streamline development processes.\n- **Integration Capabilities**: Python can easily integrate with other languages and tools, which allows for flexibility in our tech stack.\n\nWhile Python will be our default choice, other languages may be utilized where they offer specific advantages. For example:\n- **C++ for performance-critical components**\n- **Java for enterprise-level applications requiring JVM**\n- **JavaScript (Node.js) for asynchronous server-side code**\n\nAny deviations from using Python as the base language will require proper justification and approval through an Architectural Decision Record (ADR) process. This ensures that our choices are well-documented and align with the project\u2019s strategic goals.\n\nWe will revisit and possibly re-evaluate this decision every 6 months to ensure that it remains consistent with evolving project needs and industry trends.","GenTime":"2024-08-19 23:37:52"}
{"File Name":"link_platform\/0002-use-the-adr-tools-kit-to-manage-adr-docs.md","Context":"## Context\\nTo help automated the process of managing architectual decision records use a tool that stanadardizes the process.\\nI found a reference to this tool on the thought works techninques review page.  The github repo is:\\n`https:\/\/github.com\/npryce\/adr-tools`\\nBut I installed it using brew:\\n`brew install adr-tools`\\n","Decision":"We will use the system adr-tools to manage the ADR documents\\n","tokens":79,"id":5033,"text":"## Context\\nTo help automated the process of managing architectual decision records use a tool that stanadardizes the process.\\nI found a reference to this tool on the thought works techninques review page.  The github repo is:\\n`https:\/\/github.com\/npryce\/adr-tools`\\nBut I installed it using brew:\\n`brew install adr-tools`\\n\n\n##Decision\nWe will use the system adr-tools to manage the ADR documents\\n","MatchedIds":[2914,2091,1669,1563,1067],"Predicted":"## Decision\nWe will use `adr-tools` to automate and standardize the process of managing Architectural Decision Records. The tool has been installed using Homebrew with the command `brew install adr-tools`. This approach will help ensure consistency and ease of use in creating and maintaining ADRs.","GenTime":"2024-08-19 23:37:56"}
{"File Name":"bookit-api\/0010-jpa-manages-schema.md","Context":"## Context\\nOriginally, we used Spring Boot's Database Initialization support to automatically create and intialize our database via schema.sql and data.sql scripts.  Each deployment (application initialization) would execute these scripts.  Our implementation would drop the database and recreate it each time.  While this accelerated our development (avoid data migrations), it's not sustainable\\n","Decision":"* Leverage Hibernate's (our JPA implementation) ddl-auto feature to update the staging\/production databases (we will continue to drop\/recreate all other databases....local, integration).\\n* recreating in integration ensures a clean database for each run.  In addition, it validates that we can recreate a database from scratch\\n","tokens":71,"id":3221,"text":"## Context\\nOriginally, we used Spring Boot's Database Initialization support to automatically create and intialize our database via schema.sql and data.sql scripts.  Each deployment (application initialization) would execute these scripts.  Our implementation would drop the database and recreate it each time.  While this accelerated our development (avoid data migrations), it's not sustainable\\n\n\n##Decision\n* Leverage Hibernate's (our JPA implementation) ddl-auto feature to update the staging\/production databases (we will continue to drop\/recreate all other databases....local, integration).\\n* recreating in integration ensures a clean database for each run.  In addition, it validates that we can recreate a database from scratch\\n","MatchedIds":[3053,317,4096,1462,1561],"Predicted":"## Decision\n\nTo move towards a more sustainable and production-ready solution, we will implement Flyway for database migrations.\n\n**Rationale:**\n1. **Avoid Data Loss:**\n   - Dropping the database on each deployment is not acceptable in a production environment as it leads to complete data loss.\n   \n2. **Controlled Migrations:**\n   - Flyway enables version-controlled database migrations, ensuring that changes are predictable and reversible.\n   \n3. **Zero-downtime Deployments:**\n   - Flyway helps in achieving zero-downtime deployments by allowing us to manage DB schema changes without impacting the running application.\n   \n4. **Community and Documentation:**\n   - Flyway has a strong community and extensive documentation which makes it easier for developer onboarding and troubleshooting.\n\n5. **Spring Boot Integration:**\n   - Flyway integrates seamlessly with Spring Boot, making it easier to adopt within our current tech stack.\n\n**Implementation Steps:**\n1. **Install Flyway:**\n   - Add the Flyway dependency to our Spring Boot project.\n   \n2. **Baseline the Current Schema:**\n   - Create a baseline migration script that represents the current state of the database.\n   \n3. **Migrate Data:**\n   - Convert `schema.sql` and `data.sql` scripts into Flyway-compatible migration scripts.\n   \n4. **Configure Flyway:**\n   - Update the Spring Boot configuration to execute Flyway migrations on application startup.\n   \n5. **Testing:**\n   - Perform thorough testing in a staging environment to ensure that future migrations do not disrupt the application.\n\n6. **Documentation:**\n   - Document the migration process and best practices for creating and maintaining migration scripts.\n\nBy adopting Flyway for database migrations, we ensure a more stable and maintainable approach to database management, aligned with best practices for production systems.","GenTime":"2024-08-19 23:38:03"}
{"File Name":"launcher\/2018-06-15_request_enrollment_details.md","Context":"## Context\\nWhen enrolling osquery into a remote server, it's desirable to also have a set of initial fields about the osquery node. Usually, servers like Fleet queue ad-hoc queries to get this information, but it's not always reliable. For example, a osquery host can succeed during the Enroll method, and then immediately crash, leaving the server operator with little context about which osquery host to troubleshoot. Another effect of populating host details via distributed query is that they can add some latency to the enrollment process depending on the query interval.\\n","Decision":"Add a EnrollmentDetails structure to the RequestEnrollment method which sends the results of a population query as part of the enrollment. Below is a list of chose attributes that launcher will always send:\\n```\\nmessage EnrollmentDetails {\\nstring os_version = 1;\\nstring os_build = 2;\\nstring os_platform = 3;\\nstring hostname = 4;\\nstring hardware_vendor = 5;\\nstring hardware_model = 6;\\nstring hardware_serial = 7;\\nstring osquery_version = 8;\\nstring launcher_version = 9;\\n}\\n```\\n","tokens":116,"id":2546,"text":"## Context\\nWhen enrolling osquery into a remote server, it's desirable to also have a set of initial fields about the osquery node. Usually, servers like Fleet queue ad-hoc queries to get this information, but it's not always reliable. For example, a osquery host can succeed during the Enroll method, and then immediately crash, leaving the server operator with little context about which osquery host to troubleshoot. Another effect of populating host details via distributed query is that they can add some latency to the enrollment process depending on the query interval.\\n\n\n##Decision\nAdd a EnrollmentDetails structure to the RequestEnrollment method which sends the results of a population query as part of the enrollment. Below is a list of chose attributes that launcher will always send:\\n```\\nmessage EnrollmentDetails {\\nstring os_version = 1;\\nstring os_build = 2;\\nstring os_platform = 3;\\nstring hostname = 4;\\nstring hardware_vendor = 5;\\nstring hardware_model = 6;\\nstring hardware_serial = 7;\\nstring osquery_version = 8;\\nstring launcher_version = 9;\\n}\\n```\\n","MatchedIds":[2549,1745,1372,85,3337],"Predicted":"## Decision\nDuring the Enroll method, osquery should also include a set of initial fields about the osquery node. These fields should encompass essential information such as the host's operating system version, hostname, and initial status. This approach ensures that even if the osquery host crashes immediately after enrollment, critical context for troubleshooting is available to the server operator without waiting for ad-hoc queries to execute. This will help to reduce initialization latency and increase reliability in identifying troubled hosts promptly.","GenTime":"2024-08-19 23:38:07"}
{"File Name":"meadow\/0007-code-analysis.md","Context":"## Context\\nWe need to make sure we adhere to our own designated code quality best practices.\\n","Decision":"Use a code analysis tool (specifically, [credo](http:\/\/credo-ci.org\/) for Elixir\\nand [prettier](https:\/\/prettier.io\/) for JavaScript).\\n","tokens":20,"id":3853,"text":"## Context\\nWe need to make sure we adhere to our own designated code quality best practices.\\n\n\n##Decision\nUse a code analysis tool (specifically, [credo](http:\/\/credo-ci.org\/) for Elixir\\nand [prettier](https:\/\/prettier.io\/) for JavaScript).\\n","MatchedIds":[5070,400,1416,1258,4179],"Predicted":"## Decision\nWe will implement a CI\/CD pipeline that includes automated linting and code style checks using tools like ESLint (for JavaScript) or RuboCop (for Ruby) before any code is merged into the main branch. Additionally, we will enforce code reviews to ensure adherence to our designated code quality best practices.","GenTime":"2024-08-19 23:38:10"}
{"File Name":"gsp\/ADR043-k8s-resource-access.md","Context":"## Context\\nSeveral different levels of access are required within a kubernetes cluster. The\\nGSP uses role based access control so these levels are granted to users and\\ngroups via roles.\\n","Decision":"We will create two levels of access within each namespace:\\n* Operator\\n* Auditor\\nThe Operator is a relatively permissive read-write role within the namespace.\\nDevelopers working on branches that are not part of the release process may be\\ngranted this role in certain namespaces. This is also the role the in-cluster\\nconcourse team for each namespace will be granted.\\nThe Auditor will be given to all authenticated users in the cluster. This should\\nallow for debugging of issues and incidents and basic remedial actions without\\nneeding formal escalation procedures.\\nThe complete list of resource permissions is given in Appendix A.\\n","tokens":40,"id":3890,"text":"## Context\\nSeveral different levels of access are required within a kubernetes cluster. The\\nGSP uses role based access control so these levels are granted to users and\\ngroups via roles.\\n\n\n##Decision\nWe will create two levels of access within each namespace:\\n* Operator\\n* Auditor\\nThe Operator is a relatively permissive read-write role within the namespace.\\nDevelopers working on branches that are not part of the release process may be\\ngranted this role in certain namespaces. This is also the role the in-cluster\\nconcourse team for each namespace will be granted.\\nThe Auditor will be given to all authenticated users in the cluster. This should\\nallow for debugging of issues and incidents and basic remedial actions without\\nneeding formal escalation procedures.\\nThe complete list of resource permissions is given in Appendix A.\\n","MatchedIds":[3915,2397,3950,631,3440],"Predicted":"## Decision\nWe will define role-based access control (RBAC) policies within the Kubernetes cluster to grant the required levels of access to users and groups:\n\n1. **Cluster Roles**: Create predefined cluster-wide roles to encapsulate common sets of permissions such as:\n   - `ClusterAdmin`: Full control over all resources in the cluster.\n   - `ClusterReadOnly`: Read-only access across the entire cluster.\n\n2. **Namespace Roles**: Define roles scoped to individual namespaces for fine-grained control, allowing teams to manage their applications without affecting other namespaces.\n   - `NamespaceAdmin`: Full control over all resources within a specific namespace.\n   - `NamespaceReadOnly`: Read-only access to resources within a specific namespace.\n\n3. **RoleBinding and ClusterRoleBinding**: Associate users\/groups to the defined roles using `RoleBinding` for namespace-scoped roles and `ClusterRoleBinding` for cluster-wide roles:\n   - Use `RoleBinding` to tie a user\/group to namespace roles, ensuring least privilege principle.\n   - Use `ClusterRoleBinding` for cluster-wide roles where necessary.\n\n4. **Service Accounts**: Create and define roles for service accounts where application components (like CI\/CD pipelines) need specific permissions:\n   - `CI\/CD-Runner`: Permits deployments, scaling, and other operations essential for CI\/CD pipelines within a specific namespace.\n\n5. **Policy Review and Auditing**: Set up a periodic review process for all RBAC policies to ensure they continue to meet security and operational requirements and to audit any changes to track compliance.\n\n6. **Documentation and Training**: Provide thorough documentation and training sessions for teams to understand their level of access and how to use their permissions effectively.\n\nBy implementing this structured RBAC approach, we ensure that access to the Kubernetes cluster is tightly controlled and that users and applications only have permissions necessary for their tasks.","GenTime":"2024-08-19 23:38:15"}
{"File Name":"jabref\/0021-keep-study-as-a-dto.md","Context":"## Context and Problem Statement\\nThe study holds query and library entries that could be replaced respectively with complex query and fetcher instances.\\nThis poses the question: should the study remain a pure DTO object or should it contain direct object instances?\\n","Decision":"Chosen option: \"Keep study as DTO and use transformators\", because comes out best (see below).\\n","tokens":49,"id":4741,"text":"## Context and Problem Statement\\nThe study holds query and library entries that could be replaced respectively with complex query and fetcher instances.\\nThis poses the question: should the study remain a pure DTO object or should it contain direct object instances?\\n\n\n##Decision\nChosen option: \"Keep study as DTO and use transformators\", because comes out best (see below).\\n","MatchedIds":[4738,1031,3776,2498,1225],"Predicted":"## Decision\nFor the purpose of maintaining clarity, separation of concerns, and ensuring ease of serialization and deserialization, the study shall remain a pure Data Transfer Object (DTO). This means it will not directly contain complex query or fetcher instances.\n\n### Rationale\n1. **Separation of Concerns**:\n    - Keeping the study as a pure DTO ensures that it only serves as a data carrier and doesn't perform any business logic. This separation makes the system more modular and easier to maintain.\n  \n2. **Ease of Serialization\/Deserialization**:\n    - DTOs are typically structured for easy conversion to and from common data formats like JSON or XML. Including complex objects can complicate this process, leading to potential performance and compatibility issues.\n\n3. **Enhanced Testability**:\n    - With the study as a pure DTO, unit tests become simpler and more reliable since they focus purely on data structure validation without the need to mock complex behaviors.\n\n4. **Flexibility**:\n    - DTOs can be easily shared across different components or layers of the application without worrying about the side effects of complex object behaviors. This enhances flexibility and reusability of the DTOs.\n\n### Implementation Details\n- The Study DTO will include fields for query parameters and library entries in simple data types (e.g., strings, lists, maps).\n- The actual query logic and fetcher operations will be handled in separate service or utility classes, which will take the DTO as input and perform the necessary operations.\n- Example Structure:\n  ```json\n  {\n    \"studyId\": \"12345\",\n    \"query\": {\n      \"parameter1\": \"value1\",\n      \"parameter2\": \"value2\"\n    },\n    \"libraryEntries\": [\n      \"entry1\",\n      \"entry2\"\n    ]\n  }\n  ```\n\n### Positive Consequences\n- Improved maintainability and clarity of codebase.\n- Simplified data management and manipulation.\n- Clear distinction between data representation and business logic.\n\n### Negative Consequences\n- Potential need for additional service or utility classes to handle the logic previously embedded in direct object instances.\n  \nIn conclusion, keeping the study as a pure DTO provides a cleaner and more maintainable architecture, aligning well with best practices in software design.","GenTime":"2024-08-19 23:38:25"}
{"File Name":"princeton_ansible\/0002-group-vars.md","Context":"## Context\\nThere are a lot of different ways to set variables in Ansible and we should have\\na consistent way to do it in all playbooks for each group.\\nWe want to make sure that there's a smaller vault.yml file per group, instead of\\na gigantic vault.yml file for everything. This is because one huge vault.yml\\nfile causes merge conflicts in multiple Github Pull Requests.\\n","Decision":"Unique group variables will be placed in `\/group_vars\/<groupname>`\\nEncrypted group variables will be placed in `\/group_vars\/<groupname>\/vault.yml`\\nShared group variables will be placed in `\/group_vars\/all\/`\\n","tokens":82,"id":2255,"text":"## Context\\nThere are a lot of different ways to set variables in Ansible and we should have\\na consistent way to do it in all playbooks for each group.\\nWe want to make sure that there's a smaller vault.yml file per group, instead of\\na gigantic vault.yml file for everything. This is because one huge vault.yml\\nfile causes merge conflicts in multiple Github Pull Requests.\\n\n\n##Decision\nUnique group variables will be placed in `\/group_vars\/<groupname>`\\nEncrypted group variables will be placed in `\/group_vars\/<groupname>\/vault.yml`\\nShared group variables will be placed in `\/group_vars\/all\/`\\n","MatchedIds":[2793,3142,1372,856,1254],"Predicted":"## Decision\nAdopt a standardized variable structure for all Ansible playbooks to ensure clarity and consistency. Implement the following guidelines:\n\n1. **Group-specific Variable Files**: \n   - Create a dedicated `vars` directory under each group directory.\n   - Within the `vars` directory, create smaller, group-specific `vault.yml` files for secure variables.\n\n2. **Variable Precedence and Structure**:\n   - Use the following hierarchy:\n     - Group variables: `group_vars\/<group-name>\/vars.yml`\n     - Group-specific vault variables: `group_vars\/<group-name>\/vault.yml`\n     - Task-specific variables should go into the respective task files to avoid overriding issues.\n\n3. **Naming Conventions**:\n   - Use clear and descriptive variable names to avoid conflicts and ensure readability.\n   - Follow a consistent naming convention, e.g., `<group>_<variable_name>` for group-specific variables.\n\n4. **Documentation and Comments**:\n   - Every `vars.yml` and `vault.yml` file should begin with a brief header explaining its purpose.\n   - Include comments and documentation within each file, describing the use and necessity of each variable.\n\n5. **Merge Conflict Mitigation**:\n   - Keep `vault.yml` files as granular as possible to ensure they contain only the necessary variables for their respective groups.\n   - Regularly review and refactor the `vault.yml` files to keep them up to date and minimize conflicts.\n\n6. **Ansible-lint and Best Practices**:\n   - Integrate `ansible-lint` in the CI\/CD pipeline to enforce best practices and verify the structure of variable files.\n   - Ensure that all playbooks and roles adhere to the defined structure and naming conventions.\n\nBy breaking down the `vault.yml` files into smaller, group-specific files and maintaining a consistent structure, we can reduce merge conflicts and enhance the maintainability of Ansible playbooks.","GenTime":"2024-08-19 23:38:31"}
{"File Name":"Wikibase\/0021-federated-properties-source-and-type-dispatching-services.md","Context":"## Context\\nFederated Properties v2 aims to make it possible for users to enable Federated Properties even if their Wikibase already contains data, so that they can choose to use both remote & local Properties to make statements.\\nDispatching by entity type is a widely used mechanism in Wikibase that allows the dispatching service to handle entity ids of different types by delegating to the service implementation defined in the [entity type definitions], thus enabling entity type specific behavior within a single service. With Federated Properties v2 the entity type (\"property\") no longer uniquely identifies the desired service implementation, since local Properties' services need to be handled by database-backed implementations, whereas Federated Properties use API-backed services. In order to work with local and remote Properties, dispatching services need to be aware of the entity's source as well as the type.\\n","Decision":"Make all relevant services source and type dispatching.\\n","tokens":172,"id":1346,"text":"## Context\\nFederated Properties v2 aims to make it possible for users to enable Federated Properties even if their Wikibase already contains data, so that they can choose to use both remote & local Properties to make statements.\\nDispatching by entity type is a widely used mechanism in Wikibase that allows the dispatching service to handle entity ids of different types by delegating to the service implementation defined in the [entity type definitions], thus enabling entity type specific behavior within a single service. With Federated Properties v2 the entity type (\"property\") no longer uniquely identifies the desired service implementation, since local Properties' services need to be handled by database-backed implementations, whereas Federated Properties use API-backed services. In order to work with local and remote Properties, dispatching services need to be aware of the entity's source as well as the type.\\n\n\n##Decision\nMake all relevant services source and type dispatching.\\n","MatchedIds":[1341,75,84,1839,1339],"Predicted":"## Decision\nTo support Federated Properties v2 and the requirement for handling both local and remote properties, the dispatching mechanism needs to be updated to consider both the entity type and the entity source. \n\n### Dispatching Mechanism Update\n1. **Service Interface Extension**: Extend the service interface to include a method for retrieving the source of an entity (local or remote).\n2. **Service Lookup Modification**: Modify the existing dispatching service to also check the entity's source, in addition to its type, to determine the correct service implementation. If an entity is a \"property\", the dispatching service will first determine if it is local or remote and then route it to the corresponding database-backed or API-backed service respectively.\n3. **Entity Source Identification**: Implement a mechanism to reliably identify whether an entity is local or remote. This might involve changes in the entity metadata or the introduction of new flags.\n4. **Backward Compatibility**: Ensure that these changes retain backward compatibility. If the entity source is not specified (e.g., for legacy data), default to treating these entities as local properties to avoid disruption.\n\n### Caching & Performance Considerations\n- **Caching**: Implement caching strategies to store the source information for entities to minimize repeated lookups and improve performance.\n- **Performance Testing**: Perform thorough performance testing to assess the impact of the new dispatching logic, particularly in large Wikibase instances. Make necessary optimizations based on findings.\n\n### Documentation & Community Feedback\n- **Documentation**: Update the relevant documentation to reflect the new dispatching logic, explaining how to configure and troubleshoot issues related to the source-aware dispatching of entity services.\n- **Community Feedback**: Actively seek feedback from the community and early adopters to identify any issues or improvements needed in the new dispatching mechanism.\n\nBy implementing these changes, we aim to provide a robust and flexible way to handle both local and remote properties in Wikibase, allowing users to take full advantage of Federated Properties v2.","GenTime":"2024-08-19 23:38:39"}
{"File Name":"govuk-aws\/0029-combine-api-redis-into-backend-redis.md","Context":"## Context\\nWe currently have 3 Redis instances that we run in Elasticache:\\n- backend-redis\\n- logs-redis\\n- api-redis\\nMost applications use backend-redis, the logging cluster uses logs-redis (which should\\nsoon be replaced), and Rummager uses api-redis.\\nThis was traditionally the case because Rummager lived in a different vDC to redis-1\/redis-2,\\nbut this is no longer a concern in AWS.\\n","Decision":"Update Rummager configuration so it uses backend-redis, and remove api-redis.\\n","tokens":101,"id":4044,"text":"## Context\\nWe currently have 3 Redis instances that we run in Elasticache:\\n- backend-redis\\n- logs-redis\\n- api-redis\\nMost applications use backend-redis, the logging cluster uses logs-redis (which should\\nsoon be replaced), and Rummager uses api-redis.\\nThis was traditionally the case because Rummager lived in a different vDC to redis-1\/redis-2,\\nbut this is no longer a concern in AWS.\\n\n\n##Decision\nUpdate Rummager configuration so it uses backend-redis, and remove api-redis.\\n","MatchedIds":[4047,2471,4061,4056,1173],"Predicted":"## Decision\nWe will consolidate our Redis instances to simplify the architecture and reduce costs. Specifically, we will:\n\n1. **Deprecate `logs-redis`**:\n   - Since the logging cluster will soon be replaced, we will phase out `logs-redis` as part of that replacement process. Any logging functionality needed will be redirected to dedicated logging services or databases as determined by the new solution.\n\n2. **Merge `api-redis` into `backend-redis`**:\n   - Since the separation of `backend-redis` and `api-redis` is no longer necessary, we will consolidate `api-redis` usage into `backend-redis`. This will involve migrating the data and updating the configuration of Rummager and any other applications that used `api-redis` to use `backend-redis`.\n\n3. **Monitor and Scale `backend-redis`**:\n   - After the consolidation, we will monitor the performance and usage of `backend-redis` closely to ensure it handles the increased load. Elasticache's scaling and replication features will be utilized to ensure high availability and performance.\n\nBy implementing these changes, we can simplify our Redis infrastructure while maintaining or improving our system's reliability and performance.","GenTime":"2024-08-19 23:38:43"}
{"File Name":"drt-v2\/0013-use-lihaoyi-s-autowire.md","Context":"## Context\\nWe've got a Single Page app, it needs to talk to the server. Our use of scala and scalajs means we can use [lihaoyi's autowire\\nmacros](https:\/\/github.com\/lihaoyi\/autowire)\\nAlthough this is essentially a 0 on the [Richardson maturity model](https:\/\/martinfowler.com\/articles\/richardsonMaturityModel.html)\\nit has huge benefits in terms of speed of change. We also (at the moment) only have the one client of the SPA so we can afford the tight coupling.\\nIt doesn't preclude moving toward something more restful, as we can just add routes when we recognise a need.\\n","Decision":"Use autowire for now.\\n","tokens":147,"id":1907,"text":"## Context\\nWe've got a Single Page app, it needs to talk to the server. Our use of scala and scalajs means we can use [lihaoyi's autowire\\nmacros](https:\/\/github.com\/lihaoyi\/autowire)\\nAlthough this is essentially a 0 on the [Richardson maturity model](https:\/\/martinfowler.com\/articles\/richardsonMaturityModel.html)\\nit has huge benefits in terms of speed of change. We also (at the moment) only have the one client of the SPA so we can afford the tight coupling.\\nIt doesn't preclude moving toward something more restful, as we can just add routes when we recognise a need.\\n\n\n##Decision\nUse autowire for now.\\n","MatchedIds":[2317,578,3470,311,3681],"Predicted":"## Decision\n**We will use lihaoyi's autowire macros for client-server communication**  \nDespite being at level 0 on the Richardson maturity model, using lihaoyi's autowire macros provides significant benefits in terms of speed of development and ease of change. Given that we currently have only one client for the Single Page App (SPA), we can manage the tight coupling that this approach brings. This decision does not limit us from evolving our architecture to include more RESTful routes in the future as the need arises and additional clients are integrated. By leveraging autowire's type-safe macros, we ensure efficient and rapid development without compromising future flexibility.","GenTime":"2024-08-19 23:38:48"}
{"File Name":"twig-infrastructure\/0003-start-with-aws-codepipeline-and-codebuild.md","Context":"## Context\\nTwig has been running on an older riglet flavor.  There is a desire to move to a newer\/simpler riglet flavor\\nand put newer approaches to the test.\\n","Decision":"* Use AWS CodePipeline and CodeBuild instead of Jenkins, at least in the beginning.\\n* We will aim to create a new Pipeline\/Build and potentially execution environment per branch.\\n* This will be manual at first and later could be automated via webhooks and lambda functions\\n","tokens":40,"id":350,"text":"## Context\\nTwig has been running on an older riglet flavor.  There is a desire to move to a newer\/simpler riglet flavor\\nand put newer approaches to the test.\\n\n\n##Decision\n* Use AWS CodePipeline and CodeBuild instead of Jenkins, at least in the beginning.\\n* We will aim to create a new Pipeline\/Build and potentially execution environment per branch.\\n* This will be manual at first and later could be automated via webhooks and lambda functions\\n","MatchedIds":[1742,4868,351,1739,1652],"Predicted":"## Decision\nMigrate Twig to the latest version of the [simplified riglet flavor](https:\/\/github.com\/buildit\/new-riglet-flavor). This migration will involve the following steps:\n\n1. **Evaluate Compatibility**: Assess the current Twig setup and identify dependencies or configurations that may be impacted by the migration to the new riglet flavor.\n\n2. **Setup New Riglet Flavor**: Instantiate a new instance of the simplified riglet flavor in a staging environment. This will allow us to test and validate the new setup without disrupting the current production environment.\n\n3. **Update Configuration**: Adjust configurations and scripts to align with the new riglet flavor, ensuring that all services function as expected in the new environment.\n\n4. **Testing**: Conduct thorough testing including unit, integration, and performance tests to ensure that the new riglet flavor meets the necessary requirements and functions correctly.\n\n5. **Documentation**: Update internal documentation to reflect the changes made during the migration. This includes setup instructions, configuration details, and troubleshooting tips.\n\n6. **Rollout Plan**: Develop a detailed rollout plan that includes a rollback strategy in case of unexpected issues. Ensure that the team is prepared to implement the new riglet flavor in production.\n\n7. **Deployment**: Once the new riglet flavor has been validated in the staging environment, proceed with deployment to the production environment during a scheduled maintenance window to minimize disruption.\n\n8. **Monitor and Optimize**: After deployment, closely monitor the application and infrastructure for any issues. Optimize configurations based on initial post-migration performance feedback.\n\nBy following these steps, we aim to leverage the benefits of the new riglet flavor while minimizing risks and ensuring a smooth transition.","GenTime":"2024-08-19 23:38:54"}
{"File Name":"mymove\/0061-use-opentelemetry-for-distributed-tracing.md","Context":"## Problem Statement\\n### Why Distributed Tracing?\\n> Observability is defined as the ability of the internal states of a system to\\nbe determined by its external outputs.\\nThere are several established techniques for gaining observability into complex\\nsoftware systems.\\n* Logging\\n* helpful for known knowns\\n* e.g. \"I want to know when a specific condition is reached\"\\n* Metrics\\n* helpful for assessing known unknowns\\n* e.g. \"How many requests per second was the system handling last Tuesday?\"\\n* Distributed Tracing\\n* helpful for learning about unknown unknowns\\n* e.g. \"What was the execution context for User X that caused their\\ninteraction to timeout last Tuesday?\"\\nSome of the benefits of distributed tracing, as outlined in\\n[this](https:\/\/petabridge.com\/blog\/why-use-distributed-tracing\/) article are:\\n* radically improves developer productivity and output\\n* works across multiple applications, programming languages, and transports\\n* improve time to market\\n* facilitates excellent cross-team communication and cooperation\\nHere are several example scenarios or questions that distributed tracing can\\nhelp answer.\\n* As a new engineer on the team, I want to understand how many separate systems\\nare involved when a certain user type logs in and the first page is rendered.\\n* As an operations engineer, I want to know how many SQL queries are executed\\nfor a given endpoint or interaction.\\n* As a product manager, I want to know if a new feature is being used by a\\ncertain cohort of users on a regular basis.\\n* As an engineer, I want to prove that an optimization I wrote is effective\\nin a production environment.\\n* As a load tester, after I have shown that a problem exists, I want to\\nunderstand how the system is interacting so I can debug and fix the issue.\\n### ADR Goals and Anti-goals\\n* Goal: Choose which set of libraries to use at callsites (across programming\\nlanguages) within the MilMove codebase, which will be used to generate\\ndistributed tracing data\\n* Anti-goal: Committing to a specific \"backend\", i.e. platform or service for\\ngathering, exploring, and displaying trace information\\n* Sub-goal: Leaving open as many options as possible for the backend\\n","Decision":"* Chosen Alternative: _Use OpenTelemetry_\\n* OpenTelemetry is an emerging industry standard\\n* vendors find benefit of being in the OpenTelemetry ecosystem because they\\nno longer have to create or support instrumentation libraries in an ever\\ngrowing array of languages, i.e. as soon as language library exists for\\nOpenTelemetry, the vendors automatically become available to support that\\ngiven language.\\n* OpenTelemetry is vendor agnostic\\n* tracing information can be sent to hosted services (e.g. Honeycomb.io, AWS\\nX-Ray, etc) or self-hosted Open Source implementations (e.g. Zipkin, Jaeger,\\netc)\\n* if left unconfigured, OpenTelemetry instrumentation calls default to\\nlightweight\/noop executions\\n* OpenTelemetry has well-maintained libraries for the languages used in the\\nlayers of the MilMove project\\n* i.e. Go (back-end); JavaScript (front-end); Python (load testing); etc\\n* Easily swappable back-ends\\n* e.g. could choose a local Docker version of OpenZipkin for an all-local\\ndevelopment environment\\n* e.g. can use Honeycomb.io in the experimental commercial-cloud hosted\\nenvironment\\n* e.g. can swap in AWS X-Ray for use in GovCloud hosted environments\\n* Cons\\n* as an abstraction layer, OpenTelemetry may prohibit usage of vendor-\\nspecific capabilities\\n* some OpenTelemetry libraries and tools may trail their vendor-supported\\ncounterparts\\n* instrumentation for tracing may be a vector for performance overhead\\n","tokens":475,"id":3081,"text":"## Problem Statement\\n### Why Distributed Tracing?\\n> Observability is defined as the ability of the internal states of a system to\\nbe determined by its external outputs.\\nThere are several established techniques for gaining observability into complex\\nsoftware systems.\\n* Logging\\n* helpful for known knowns\\n* e.g. \"I want to know when a specific condition is reached\"\\n* Metrics\\n* helpful for assessing known unknowns\\n* e.g. \"How many requests per second was the system handling last Tuesday?\"\\n* Distributed Tracing\\n* helpful for learning about unknown unknowns\\n* e.g. \"What was the execution context for User X that caused their\\ninteraction to timeout last Tuesday?\"\\nSome of the benefits of distributed tracing, as outlined in\\n[this](https:\/\/petabridge.com\/blog\/why-use-distributed-tracing\/) article are:\\n* radically improves developer productivity and output\\n* works across multiple applications, programming languages, and transports\\n* improve time to market\\n* facilitates excellent cross-team communication and cooperation\\nHere are several example scenarios or questions that distributed tracing can\\nhelp answer.\\n* As a new engineer on the team, I want to understand how many separate systems\\nare involved when a certain user type logs in and the first page is rendered.\\n* As an operations engineer, I want to know how many SQL queries are executed\\nfor a given endpoint or interaction.\\n* As a product manager, I want to know if a new feature is being used by a\\ncertain cohort of users on a regular basis.\\n* As an engineer, I want to prove that an optimization I wrote is effective\\nin a production environment.\\n* As a load tester, after I have shown that a problem exists, I want to\\nunderstand how the system is interacting so I can debug and fix the issue.\\n### ADR Goals and Anti-goals\\n* Goal: Choose which set of libraries to use at callsites (across programming\\nlanguages) within the MilMove codebase, which will be used to generate\\ndistributed tracing data\\n* Anti-goal: Committing to a specific \"backend\", i.e. platform or service for\\ngathering, exploring, and displaying trace information\\n* Sub-goal: Leaving open as many options as possible for the backend\\n\n\n##Decision\n* Chosen Alternative: _Use OpenTelemetry_\\n* OpenTelemetry is an emerging industry standard\\n* vendors find benefit of being in the OpenTelemetry ecosystem because they\\nno longer have to create or support instrumentation libraries in an ever\\ngrowing array of languages, i.e. as soon as language library exists for\\nOpenTelemetry, the vendors automatically become available to support that\\ngiven language.\\n* OpenTelemetry is vendor agnostic\\n* tracing information can be sent to hosted services (e.g. Honeycomb.io, AWS\\nX-Ray, etc) or self-hosted Open Source implementations (e.g. Zipkin, Jaeger,\\netc)\\n* if left unconfigured, OpenTelemetry instrumentation calls default to\\nlightweight\/noop executions\\n* OpenTelemetry has well-maintained libraries for the languages used in the\\nlayers of the MilMove project\\n* i.e. Go (back-end); JavaScript (front-end); Python (load testing); etc\\n* Easily swappable back-ends\\n* e.g. could choose a local Docker version of OpenZipkin for an all-local\\ndevelopment environment\\n* e.g. can use Honeycomb.io in the experimental commercial-cloud hosted\\nenvironment\\n* e.g. can swap in AWS X-Ray for use in GovCloud hosted environments\\n* Cons\\n* as an abstraction layer, OpenTelemetry may prohibit usage of vendor-\\nspecific capabilities\\n* some OpenTelemetry libraries and tools may trail their vendor-supported\\ncounterparts\\n* instrumentation for tracing may be a vector for performance overhead\\n","MatchedIds":[2091,786,389,3488,1067],"Predicted":"## Decision\n\nChosen Option: Implement distributed tracing using OpenTelemetry\n\n### Positive Consequences\n* **Open Standard:** OpenTelemetry is a set of APIs, libraries, agents, and instrumentation that can be used to collect distributed traces and metrics, making it a widely supported and flexible solution.\n* **Multi-language Support:** OpenTelemetry supports multiple programming languages, ensuring that we can use the same toolset across our entire codebase, regardless of the language used.\n* **Backend Agnostic:** OpenTelemetry does not lock us into a specific backend, allowing us to switch or use multiple backends like Jaeger, Zipkin, or commercial APM solutions such as Datadog, New Relic, or Dynatrace.\n* **Community and Ecosystem:** With a large and active community, OpenTelemetry is constantly evolving and receiving improvements, ensuring that we remain up-to-date with the latest advancements in observability.\n\n### Negative Consequences\n* **Learning Curve:** Engineers will need to be trained on how to effectively use and integrate OpenTelemetry within the codebase.\n* **Integration Effort:** Incorporating OpenTelemetry into the MilMove codebase will require significant initial effort in terms of instrumentation and configuration.\n* **Performance Overhead:** There might be a performance overhead associated with the instrumentation of code for distributed tracing which will need to be carefully managed.\n\n### Implementation Plan\n1. **Training and Familiarization:** Conduct training sessions for engineers to familiarize them with OpenTelemetry concepts and usage.\n2. **Initial Setup:** Setup a baseline configuration of OpenTelemetry in a development environment to understand its integration steps and performance overhead.\n3. **Incremental Integration:** Incrementally start integrating OpenTelemetry into various services and critical parts of the MilMove codebase, starting with high-impact areas, while continuously monitoring performance.\n4. **Validation:** Validate the correctness and completeness of the collected tracing data, ensuring it aligns with our observability goals.\n5. **Documentation:** Document the integration process, best practices, and usage guidelines, making it easy for new team members to understand and work with OpenTelemetry.\n6. **Monitor and Optimize:** Continuously monitor the performance and impact of distributed tracing on the system and optimize as needed.\n\nBy choosing OpenTelemetry, we ensure that our distributed tracing solution is flexible, robust, and future-proof, all while providing the necessary insights to improve system observability and developer productivity.","GenTime":"2024-08-19 23:39:06"}
{"File Name":"docspell\/0009_convert_office_docs.md","Context":"# Context and Problem Statement\\nHow can office documents, like `docx` or `odt` be converted into a PDF\\nfile that looks as much as possible like the original?\\nIt would be nice to have a java-only solution. But if an external tool\\nhas a better outcome, then an external tool is fine, too.\\nSince Docspell is free software, the tools must also be free.\\n# Considered Options\\n* [Apache POI](https:\/\/poi.apache.org) together with\\n[this](https:\/\/central.sonatype.com\/artifact\/fr.opensagres.xdocreport\/org.apache.poi.xwpf.converter.pdf\/1.0.6)\\nlibrary\\n* [pandoc](https:\/\/pandoc.org\/) external command\\n* [abiword](https:\/\/www.abisource.com\/) external command\\n* [Unoconv](https:\/\/github.com\/unoconv\/unoconv) external command\\nTo choose an option, some documents are converted to pdf and compared.\\nOnly the formats `docx` and `odt` are considered here. These are the\\nmost used formats. They have to look well, if a `xlsx` or `pptx`\\ndoesn't look so great, that is ok.\\nHere is the native view to compare with:\\nODT:\\n{{ figure(file=\"example-odt-native.jpg\") }}\\n","Decision":"Unoconv.\\nThe results from `unoconv` are really good.\\nAbiword also is not that bad, it didn't convert the chart, but all\\nfont markup is there. It would be great to not depend on something as\\nbig as libreoffice, but the results are so much better.\\nAlso pandoc deals very well with DOCX files (using the `context`\\nengine). The only thing that was not rendered was the embedded chart\\n(like abiword). But all images and font styling was present.\\nIt will be a configurable external command anyways, so users can\\nexchange it at any time with a different one.\\n","tokens":288,"id":284,"text":"# Context and Problem Statement\\nHow can office documents, like `docx` or `odt` be converted into a PDF\\nfile that looks as much as possible like the original?\\nIt would be nice to have a java-only solution. But if an external tool\\nhas a better outcome, then an external tool is fine, too.\\nSince Docspell is free software, the tools must also be free.\\n# Considered Options\\n* [Apache POI](https:\/\/poi.apache.org) together with\\n[this](https:\/\/central.sonatype.com\/artifact\/fr.opensagres.xdocreport\/org.apache.poi.xwpf.converter.pdf\/1.0.6)\\nlibrary\\n* [pandoc](https:\/\/pandoc.org\/) external command\\n* [abiword](https:\/\/www.abisource.com\/) external command\\n* [Unoconv](https:\/\/github.com\/unoconv\/unoconv) external command\\nTo choose an option, some documents are converted to pdf and compared.\\nOnly the formats `docx` and `odt` are considered here. These are the\\nmost used formats. They have to look well, if a `xlsx` or `pptx`\\ndoesn't look so great, that is ok.\\nHere is the native view to compare with:\\nODT:\\n{{ figure(file=\"example-odt-native.jpg\") }}\\n\n\n##Decision\nUnoconv.\\nThe results from `unoconv` are really good.\\nAbiword also is not that bad, it didn't convert the chart, but all\\nfont markup is there. It would be great to not depend on something as\\nbig as libreoffice, but the results are so much better.\\nAlso pandoc deals very well with DOCX files (using the `context`\\nengine). The only thing that was not rendered was the embedded chart\\n(like abiword). But all images and font styling was present.\\nIt will be a configurable external command anyways, so users can\\nexchange it at any time with a different one.\\n","MatchedIds":[287,273,282,5059,5172],"Predicted":"## Decision\nUnoconv.\n\nUnoconv produces the best results in terms of how closely the PDF matches the original office documents (`docx` and `odt`). Although it is an external tool, the quality of the conversion is paramount for this requirement, and Unoconv demonstrates that it can reliably handle various layouts and formatting intricacies of the source documents.\n\nAmong the considered options:\n- Apache POI with the additional library has decent results but some complex layouts are not rendered correctly.\n- Pandoc, while versatile, does not always preserve the original document's formatting well, especially for more complex structures.\n- Abiword does not consistently render the document as accurately as Unoconv.\n\nTherefore, even though the goal was to prefer a Java-only solution, the priority on high-quality output leads to the selection of Unoconv, a free external tool that best fulfills the requirement.","GenTime":"2024-08-19 23:39:10"}
{"File Name":"libelektra\/elektra_web.md","Context":"## Problem\\nFor Elektra Web, there needs to be a way to remotely manage instances and groups\\nof instances (clusters). The remote configuration of a single instance is\\nsimple. However, to manage multiple instances, we need to store the information\\nto access the daemons, as well as information about the grouping (clusters) of\\ndaemons.\\n","Decision":"Use one cluster daemon (clusterd) to manage all clusters and instances.\\n- [Elektra Web Recursive Structure decision](elektra_web_recursive.md)\\n","tokens":75,"id":1306,"text":"## Problem\\nFor Elektra Web, there needs to be a way to remotely manage instances and groups\\nof instances (clusters). The remote configuration of a single instance is\\nsimple. However, to manage multiple instances, we need to store the information\\nto access the daemons, as well as information about the grouping (clusters) of\\ndaemons.\\n\n\n##Decision\nUse one cluster daemon (clusterd) to manage all clusters and instances.\\n- [Elektra Web Recursive Structure decision](elektra_web_recursive.md)\\n","MatchedIds":[731,4707,1319,729,2034],"Predicted":"## Decision\nWe have decided to create a centralized configuration management service to handle the remote management of instances and clusters. This service will be responsible for storing and managing the following information:\n\n1. **Instance Information**:\n   - Unique identifiers for each instance.\n   - Access credentials (securely stored and encrypted).\n   - Endpoint details (IP address, port number).\n   - Metadata (e.g., version, status).\n\n2. **Cluster Information**:\n   - Groupings of instances into clusters.\n   - Cluster identifiers.\n   - Metadata related to clusters (e.g., purpose, status).\n   - Relationships between instances and clusters.\n\n3. **Access Control**:\n   - Role-based access control (RBAC) to ensure that only authorized users can manage instances and clusters.\n   - User permissions and roles.\n\n### Positive Consequences\n- **Centralized Management**: All information for remote management is centralized, making it easier to update, query, and maintain.\n- **Scalability**: The service can manage multiple instances and clusters efficiently, scaling as necessary.\n- **Security**: Secure storage and encryption of access credentials ensure that sensitive information is protected.\n- **Role-Based Access Control**: RBAC ensures that only authorized users can manage instances and clusters, enhancing overall security.\n\n### Negative Consequences\n- **Complexity**: The implementation of a centralized service adds complexity to the system, requiring additional development and maintenance effort.\n- **Single Point of Failure**: The centralized service becomes a critical component. If it fails, it can impact the ability to manage instances and clusters remotely. Redundancy and failover mechanisms will need to be considered.\n- **Latency**: Accessing the centralized service introduces potential latency, particularly for remote and distributed deployments. Performance optimizations may be needed.\n\n### Implementation Plan\n1. **Design and Develop Service**: Define the architecture, data model, and API for the centralized configuration management service.\n2. **Secure Storage**: Implement encryption for access credentials and ensure that secure storage mechanisms are in place.\n3. **Cluster Management**: Develop functionality for grouping instances into clusters and managing cluster metadata.\n4. **RBAC Implementation**: Introduce role-based access control mechanisms to manage permissions and user roles.\n5. **Testing**: Thoroughly test the service to ensure it handles remote management tasks robustly and securely.\n6. **Deployment**: Deploy the service with appropriate redundancy and failover mechanisms to ensure high availability.","GenTime":"2024-08-19 23:39:19"}
{"File Name":"offender-management-architecture-decisions\/0003-use-progressive-enhancement.md","Context":"## Context\\nThe service manual clearly states that teams should use progressive enhancement\\nwhen building services: https:\/\/www.gov.uk\/service-manual\/technology\/using-progressive-enhancement\\nThis supports [point 12 of the service standard](https:\/\/www.gov.uk\/service-manual\/service-standard\/create-a-service-thats-simple),\\nwhich is about ensuring that all users can use the service successfully the\\nfirst time they try - including users with accessibility needs.\\nThe service manual is also clear that [internal services should be held to the\\nsame standard as public-facing services](https:\/\/www.gov.uk\/service-manual\/design\/services-for-government-users).\\nSome of the services for prison and probation staff which have been built over\\nthe last couple of years are not progressively enhanced. Without JavaScript\\nthey display no content.\\nSince these services are in a similar space to our work and have overlapping\\nuser bases with ours (although they are not the only existing services in this\\nspace), we have considered whether we should take a similar approach to them.\\n","Decision":"We will use progressive enhancement for all our user-facing applications.\\n","tokens":210,"id":263,"text":"## Context\\nThe service manual clearly states that teams should use progressive enhancement\\nwhen building services: https:\/\/www.gov.uk\/service-manual\/technology\/using-progressive-enhancement\\nThis supports [point 12 of the service standard](https:\/\/www.gov.uk\/service-manual\/service-standard\/create-a-service-thats-simple),\\nwhich is about ensuring that all users can use the service successfully the\\nfirst time they try - including users with accessibility needs.\\nThe service manual is also clear that [internal services should be held to the\\nsame standard as public-facing services](https:\/\/www.gov.uk\/service-manual\/design\/services-for-government-users).\\nSome of the services for prison and probation staff which have been built over\\nthe last couple of years are not progressively enhanced. Without JavaScript\\nthey display no content.\\nSince these services are in a similar space to our work and have overlapping\\nuser bases with ours (although they are not the only existing services in this\\nspace), we have considered whether we should take a similar approach to them.\\n\n\n##Decision\nWe will use progressive enhancement for all our user-facing applications.\\n","MatchedIds":[2405,1754,3526,3806,1000],"Predicted":"## Decision\nWe will adhere to the principles of progressive enhancement in our services to ensure accessibility and usability for all users, including those with disabilities or with older or less capable devices. \n\n### Rationale:\n1. **Compliance with Service Manual and Standards**: The service manual mandates the use of progressive enhancement to guarantee that all users can successfully use the service on their first try, aligning with point 12 of the service standard.\n2. **Accessibility**: Ensuring that services are accessible to all users, including those who may have JavaScript disabled or who use assistive technologies, is crucial for inclusivity.\n3. **Consistency**: As internal services should adhere to the same standards as public-facing services, it is essential that our approach reflects this principle.\n4. **Risk Mitigation**: By following progressive enhancement, we reduce the risk of services becoming inaccessible due to unforeseen JavaScript issues or failures.\n\n### Implementation Steps:\n1. **Design and Development**: Ensure that all services are first built to function correctly without JavaScript, providing basic functionality and content. Enhancements using JavaScript should be layered on top of this base to improve the user experience where possible.\n2. **Testing**: Include testing scenarios where JavaScript is disabled to verify the usability and accessibility of the service in such conditions.\n3. **Training and Guidance**: Provide training and documentation for development teams on the principles and techniques of progressive enhancement.\n4. **Review and Audits**: Regularly review existing services to identify and rectify any that do not comply with progressive enhancement principles.\n\n### Positive Consequences:\n* Enhanced accessibility for all users, including those with disabilities.\n* Compliance with government service standards, reducing the risk of being non-compliant.\n* Improved reliability since services will function even if JavaScript fails.\n\n### Negative Consequences:\n* Potential increase in development and testing time to ensure compliance with progressive enhancement principles.\n\nBy committing to progressive enhancement, we ensure that our services are robust, accessible, and aligned with best practices, enhancing the overall user experience for all our users, including those in prison and probation services.","GenTime":"2024-08-19 23:39:31"}
{"File Name":"platform\/2021-09-06-make-core-mail-templates-independent-from-storefront-urls.md","Context":"## Context\\nSome mail templates of the core component (Newsletter, Registration, Password Recovery, Order Status mails) depend on storefront Urls to be included in the mails.\\nThose Urls are not available when shopware is used in \"headless\" mode, without the storefront bundle being installed.\\nFor some mails (Newsletter subscription, Double Opt-In, Password recovery), the Url was made configurable over the system config and over the settings inside the administration.\\nThe default values for those Urls are the ones that the storefront bundle would use.\\nThis option does not really scale well as each Url that should be used, needs to be configurable in the administration and this can grow quickly out of hand.\\nAdditionally, it is not clear when and where those configs should be used to generate the absolute Urls, as with the BusinessEvent system and the upcoming FlowBuilder, the sending of mails is not necessarily triggered by the same entry point all the times, but different trigger can lead to sending the same mails.\\n","Decision":"There shouldn't be any links generated on PHP-side as that can be hard to override per sales-channel and can not easily be changed by apps, and links should be generated inside the mailTemplates with string concatenation instead of `raw_url`-twig functions, so the links can still be generated even if the route is not registered in the system.\\nTo make generation of urls inside the mail templated easier, we will add a `{{ domain }}` variable to the twig context, that contains the domain of the current salesChannelContext, of the order in question etc.\\nThe URLs we use in the core mail templates become part of the public API, and custom frontends should adhere to theme and provide routes under the same path, or create redirects so that the default URLs work for their frontend implementation.\\nThe default urls are:\\n```\\n\/account\/order\/{deepLinkCode} -> opens the order details of the given order\\n\/account\/recover\/password?hash={recoverHash} -> start password recovery process\\n\/newsletter-subscribe?em={emailHash}&hash={subscribeHash} -> Subscribe email with given hash to the newsletter (for douple-opt in)\\n\/registration\/confirm?em={emailHash}&hash={subscribeHash} -> Confirm registration for user eith the given mail hash (for douple-opt in)\\n```\\nIf the custom frontends can't or don't want to use our default URLs they can use the possibility to override the existing mail templates to generate custom URLs.\\nWe will deprecate the usage of the system-config configuration values and the events thrown when the links are generated on PHP-side and remove those in the next major version.\\nTo be forward compatible we will already pass the necessary data needed for generating the links into the templates, so the urls can be already generated inside the mail templates.\\nThird party clients (like the PWA) should either adhere to our default URLs or add additional mail templates, that generate the correct urls for their client.\\nIn addition to that the third party client could extend the core mail template, rather than providing a new one, and then deciding in an `IF\/ELSE` what url needs to be generated based on the salesChannel or domain.\\n","tokens":204,"id":4500,"text":"## Context\\nSome mail templates of the core component (Newsletter, Registration, Password Recovery, Order Status mails) depend on storefront Urls to be included in the mails.\\nThose Urls are not available when shopware is used in \"headless\" mode, without the storefront bundle being installed.\\nFor some mails (Newsletter subscription, Double Opt-In, Password recovery), the Url was made configurable over the system config and over the settings inside the administration.\\nThe default values for those Urls are the ones that the storefront bundle would use.\\nThis option does not really scale well as each Url that should be used, needs to be configurable in the administration and this can grow quickly out of hand.\\nAdditionally, it is not clear when and where those configs should be used to generate the absolute Urls, as with the BusinessEvent system and the upcoming FlowBuilder, the sending of mails is not necessarily triggered by the same entry point all the times, but different trigger can lead to sending the same mails.\\n\n\n##Decision\nThere shouldn't be any links generated on PHP-side as that can be hard to override per sales-channel and can not easily be changed by apps, and links should be generated inside the mailTemplates with string concatenation instead of `raw_url`-twig functions, so the links can still be generated even if the route is not registered in the system.\\nTo make generation of urls inside the mail templated easier, we will add a `{{ domain }}` variable to the twig context, that contains the domain of the current salesChannelContext, of the order in question etc.\\nThe URLs we use in the core mail templates become part of the public API, and custom frontends should adhere to theme and provide routes under the same path, or create redirects so that the default URLs work for their frontend implementation.\\nThe default urls are:\\n```\\n\/account\/order\/{deepLinkCode} -> opens the order details of the given order\\n\/account\/recover\/password?hash={recoverHash} -> start password recovery process\\n\/newsletter-subscribe?em={emailHash}&hash={subscribeHash} -> Subscribe email with given hash to the newsletter (for douple-opt in)\\n\/registration\/confirm?em={emailHash}&hash={subscribeHash} -> Confirm registration for user eith the given mail hash (for douple-opt in)\\n```\\nIf the custom frontends can't or don't want to use our default URLs they can use the possibility to override the existing mail templates to generate custom URLs.\\nWe will deprecate the usage of the system-config configuration values and the events thrown when the links are generated on PHP-side and remove those in the next major version.\\nTo be forward compatible we will already pass the necessary data needed for generating the links into the templates, so the urls can be already generated inside the mail templates.\\nThird party clients (like the PWA) should either adhere to our default URLs or add additional mail templates, that generate the correct urls for their client.\\nIn addition to that the third party client could extend the core mail template, rather than providing a new one, and then deciding in an `IF\/ELSE` what url needs to be generated based on the salesChannel or domain.\\n","MatchedIds":[4493,4498,4507,2523,2342],"Predicted":"## Decision\nWe will introduce a new system of dynamic URL generation for mail templates to support both storefront and headless modes. This approach leverages URL resolvers that can be customized based on the environment (storefront\/headless) without manual configuration in the administration panel.\n\n### Implementation Details:\n1. **URL Resolvers**: A central service will handle URL generation using specific resolvers based on the deployment mode.\n    - **StorefrontURLResolver**: Will generate URLs using storefront paths.\n    - **HeadlessURLResolver**: Will generate URLs for headless deployment, typically based on the set base URL and additional parameters.\n   \n2. **Resolver Registration**: These resolvers can be registered and prioritized in the system, allowing easy extension or replacement.\n    ```php\n    interface UrlResolverInterface {\n        public function resolve(string $contextType, array $parameters): string;\n    }\n\n    class StorefrontUrlResolver implements UrlResolverInterface {\n        public function resolve(string $contextType, array $parameters): string {\n            \/\/ Logic to generate URL for storefront mode\n        }\n    }\n\n    class HeadlessUrlResolver implements UrlResolverInterface {\n        public function resolve(string $contextType, array $parameters): string {\n            \/\/ Logic to generate URL for headless mode\n        }\n    }\n    ```\n\n3. **Context-Aware URL Service**: Introduce a context-aware URL service that uses the appropriate resolver based on the current application context.\n    ```php\n    class UrlService {\n        private $resolvers;\n\n        public function __construct(iterable $resolvers) {\n            $this->resolvers = $resolvers;\n        }\n\n        public function generateUrl(string $contextType, array $parameters): string {\n            foreach ($this->resolvers as $resolver) {\n                if ($resolver->supports($contextType)) {\n                    return $resolver->resolve($contextType, $parameters);\n                }\n            }\n            throw new \\RuntimeException('No suitable URL resolver found.');\n        }\n    }\n    ```\n\n4. **Configuration by Events**: For events like BusinessEvent and FlowBuilder, introduce hooks to allow the appropriate context (headless\/storefront) to be passed to the URL service.\n    ```php\n    \/\/ Example of how to use the UrlService in an event handler\n    public function onBusinessEventTriggered(BusinessEvent $event): void {\n        $contextType = $event->getContextType();\n        $parameters = $event->getParameters();\n        $url = $this->urlService->generateUrl($contextType, $parameters);\n        \/\/ Use the generated URL in the email\n    }\n    ```\n\n5. **Backward Compatibility**: Maintain backward compatibility by falling back to existing configurations if no resolver is available for a given context.\n\n### Positive Consequences\n- Simplifies URL management for different deployment modes (storefront\/headless).\n- Reduces administrative overhead by removing the need for manual URL configuration in the administration.\n- Adapts dynamically based on the context using URL resolvers.\n- Provides a clear and extendable architecture for URL generation.\n\n### Negative Consequences\n- Initial implementation will require the creation of the URL resolver system and refactoring of existing systems to utilize it.\n- Requires comprehensive testing to ensure all email templates and URL generation scenarios are covered correctly.\n\nThis decision enhances the scalability and maintainability of URL handling in emails, adapting dynamically to different deployment contexts without burdening administrative configuration.","GenTime":"2024-08-19 23:39:42"}
{"File Name":"mediawiki-extensions-Popups\/0008-enable-minification-of-bundle-with-uglifyjs.md","Context":"## Context\\nSources are currently minified by JSMin producing a bundle bigger than\\nnecessary. We want to produce a JS bundle as small as possible to ship less\\ncode to clients.\\nGiven we have a pre-compilation step in Popups, we can leverage node based\\ntooling without penalty, so using UglifyJS to minify the code is an option.\\nWe performed some analysis to see the differences in asset size between the two\\ntools. See:\\n* [Minifying assets with uglifyjs][1]\\nResults with the current codebase as of today 12th of May 2017 are that uglify\\nproduces a 40% smaller bundle, and a 25% smaller bundle when using gzip.\\nThis results are stable through the last 3 months, the same test has been\\nperformed multiple times. See [Compressed JS comparison][2]\\n","Decision":"Webpack's production mode has been enabled for `npm run build`, and thus the\\ncompiled bundle will be minified in production with UglifyJS.\\nSee [Enable production settings for the production bundle][3]\\n","tokens":189,"id":3428,"text":"## Context\\nSources are currently minified by JSMin producing a bundle bigger than\\nnecessary. We want to produce a JS bundle as small as possible to ship less\\ncode to clients.\\nGiven we have a pre-compilation step in Popups, we can leverage node based\\ntooling without penalty, so using UglifyJS to minify the code is an option.\\nWe performed some analysis to see the differences in asset size between the two\\ntools. See:\\n* [Minifying assets with uglifyjs][1]\\nResults with the current codebase as of today 12th of May 2017 are that uglify\\nproduces a 40% smaller bundle, and a 25% smaller bundle when using gzip.\\nThis results are stable through the last 3 months, the same test has been\\nperformed multiple times. See [Compressed JS comparison][2]\\n\n\n##Decision\nWebpack's production mode has been enabled for `npm run build`, and thus the\\ncompiled bundle will be minified in production with UglifyJS.\\nSee [Enable production settings for the production bundle][3]\\n","MatchedIds":[1680,5140,1576,3827,1931],"Predicted":"## Decision\nWe have decided to switch from JSMin to UglifyJS for minifying our JavaScript code. This decision is based on consistent analysis showing that UglifyJS produces a significantly smaller bundle\u201440% smaller in general and 25% smaller when using gzip. Leveraging UglifyJS will help us ship less code to clients, improving load times and overall performance without introducing additional complexity, given our existing pre-compilation step.","GenTime":"2024-08-19 23:39:44"}
{"File Name":"frontend-library\/0003-use-brunch.md","Context":"## Context\\nThe styling of sites developed for NHSUK need to be as consistent as possible.\\nCurrently each new site starts by copying what the last developed site had done\\nand building on that. This results in the newer sites looking different than\\nthe older sites, which would not be a problem if each site was easily updated\\nto use the same, most recent, set of styles. Currently there is no central\\nstore for the styles which means there is no easy way to find and use them\\neither.\\nThe ability to review the styles in a development environment before they are\\nmade available to a wider audience is important. As is the ability to use well\\nknown technologies such as [SASS](http:\/\/sass-lang.com\/).\\n","Decision":"We will use [Brunch](http:\/\/brunch.io\/) to accomplish a number of tasks\\nincluding:\\n* Running the site used in the development environment\\n* Generating the compiled assets\\n","tokens":153,"id":783,"text":"## Context\\nThe styling of sites developed for NHSUK need to be as consistent as possible.\\nCurrently each new site starts by copying what the last developed site had done\\nand building on that. This results in the newer sites looking different than\\nthe older sites, which would not be a problem if each site was easily updated\\nto use the same, most recent, set of styles. Currently there is no central\\nstore for the styles which means there is no easy way to find and use them\\neither.\\nThe ability to review the styles in a development environment before they are\\nmade available to a wider audience is important. As is the ability to use well\\nknown technologies such as [SASS](http:\/\/sass-lang.com\/).\\n\n\n##Decision\nWe will use [Brunch](http:\/\/brunch.io\/) to accomplish a number of tasks\\nincluding:\\n* Running the site used in the development environment\\n* Generating the compiled assets\\n","MatchedIds":[1125,1992,425,157,3037],"Predicted":"## Decision\n\nTo ensure styling consistency across all NHSUK-developed sites, we will create a centralized repository for all style definitions. This repository will be maintained using SASS to leverage its capabilities and familiar syntax within the development community.\n\n### Positive Consequences\n- Ensures a consistent look and feel across all NHS websites.\n- Makes it easier to update the styling of older websites to match the newer ones.\n- Facilitates the use of a centralized style guide that can be reviewed and tested in a staging environment before being pushed to production.\n- Simplifies finding and using style definitions, enhancing developer productivity and reducing redundancy.\n- Leverages the existing expertise within the team related to SASS.\n\n### Implementation Details\n- A new Git repository named `nhsuk-styles` will be created to house all shared SASS files.\n- Documentation will be provided to guide developers on how to pull in the styles from the centralized repository.\n- A CI\/CD pipeline will be set up to deploy the styles to a staging environment where they can be reviewed.\n- Versioning will be implemented to allow developers to pin to specific versions if needed, ensuring stability and reducing breaking changes.\n  \n### Negative Consequences\n- Initial effort is required to refactor existing sites to use the centralized styles.\n- Developers need to be disciplined in contributing back any new styles to the centralized repository to ensure it stays up-to-date.\n- Can introduce delays if new style changes have to be reviewed and approved before being utilized in individual projects.\n\nBy centralizing the style repository, we create a sustainable and scalable solution that ensures brand consistency across all NHSUK's digital properties.","GenTime":"2024-08-19 23:39:50"}
{"File Name":"backdrop\/adr-002-persistent-storage.md","Context":"# Context\\nThe way the Government Digital Service (GDS) [makes technology choices is\\ndescribed in the service manual](https:\/\/www.gov.uk\/service-manual\/making-software\/choosing-technology). We are selecting which technology will to use to provide\\npersistence for the Performance Platform.\\nGDS has experience in running MongoDB and MySQL in production.\\nWe envisage the Performance Platform as taking in unstructured data from a\\nvariety of data sources (spreadsheets, analytics, logs, other databases and\\napplications) and allowing people to collect this data in a single place. This\\nshould enable service managers to:\\n- make comparisons\\n- see how well their service is performing\\n- see how the performance changes over time, as they iterate the service\\nSo we want a persistent data store that will store unstructured data, and\\nallow us to apply a structure either by post-processing the data, or at query\\ntime.\\nThe volume of the data that we are envisaging at this stage is pretty small.\\nWe will be building a small thing to start; as we learn more about the\\nuser needs and problem space, then we will revisit this decision. Since the\\nvolume is small, it does not seem likely that we need Hadoop \/ HDFS or\\nCassandra.\\nWe are not the canonical source of this data. We are an aggregator; the\\ncanonical source remains the data sources which will be providing feeds or\\npushing the data into the Performance Platform.\\nBecause of this position, we do not need ACID properties for this data, nor\\nneed worry about the CAP theorem in any detail.\\n# Decision\\nWe will use MongoDB. We are comfortable operating it in production,\\nit will allow unstructured data (in the form of JSON documents) and we can\\napply structure at query time.\\n# Status\\nAccepted.\\n# Consequences\\nUse MongoDB with an appropriate replica-set configuration.\\n","Decision":"We will use MongoDB. We are comfortable operating it in production,\\nit will allow unstructured data (in the form of JSON documents) and we can\\napply structure at query time.\\n# Status\\nAccepted.\\n# Consequences\\nUse MongoDB with an appropriate replica-set configuration.\\n","tokens":403,"id":4266,"text":"# Context\\nThe way the Government Digital Service (GDS) [makes technology choices is\\ndescribed in the service manual](https:\/\/www.gov.uk\/service-manual\/making-software\/choosing-technology). We are selecting which technology will to use to provide\\npersistence for the Performance Platform.\\nGDS has experience in running MongoDB and MySQL in production.\\nWe envisage the Performance Platform as taking in unstructured data from a\\nvariety of data sources (spreadsheets, analytics, logs, other databases and\\napplications) and allowing people to collect this data in a single place. This\\nshould enable service managers to:\\n- make comparisons\\n- see how well their service is performing\\n- see how the performance changes over time, as they iterate the service\\nSo we want a persistent data store that will store unstructured data, and\\nallow us to apply a structure either by post-processing the data, or at query\\ntime.\\nThe volume of the data that we are envisaging at this stage is pretty small.\\nWe will be building a small thing to start; as we learn more about the\\nuser needs and problem space, then we will revisit this decision. Since the\\nvolume is small, it does not seem likely that we need Hadoop \/ HDFS or\\nCassandra.\\nWe are not the canonical source of this data. We are an aggregator; the\\ncanonical source remains the data sources which will be providing feeds or\\npushing the data into the Performance Platform.\\nBecause of this position, we do not need ACID properties for this data, nor\\nneed worry about the CAP theorem in any detail.\\n# Decision\\nWe will use MongoDB. We are comfortable operating it in production,\\nit will allow unstructured data (in the form of JSON documents) and we can\\napply structure at query time.\\n# Status\\nAccepted.\\n# Consequences\\nUse MongoDB with an appropriate replica-set configuration.\\n\n\n##Decision\nWe will use MongoDB. We are comfortable operating it in production,\\nit will allow unstructured data (in the form of JSON documents) and we can\\napply structure at query time.\\n# Status\\nAccepted.\\n# Consequences\\nUse MongoDB with an appropriate replica-set configuration.\\n","MatchedIds":[4047,1661,1740,4747,2520],"Predicted":"# Context\nThe way the Government Digital Service (GDS) [makes technology choices is described in the service manual](https:\/\/www.gov.uk\/service-manual\/making-software\/choosing-technology). We are selecting which technology to use to provide persistence for the Performance Platform.\nGDS has experience in running MongoDB and MySQL in production.\nWe envisage the Performance Platform as taking in unstructured data from a variety of data sources (spreadsheets, analytics, logs, other databases and applications) and allowing people to collect this data in a single place. This should enable service managers to:\n- make comparisons\n- see how well their service is performing\n- see how the performance changes over time, as they iterate the service\nSo we want a persistent data store that will store unstructured data, and allow us to apply a structure either by post-processing the data, or at query time.\nThe volume of the data that we are envisaging at this stage is pretty small. We will be building a small thing to start; as we learn more about the user needs and problem space, then we will revisit this decision. Since the volume is small, it does not seem likely that we need Hadoop \/ HDFS or Cassandra.\nWe are not the canonical source of this data. We are an aggregator; the canonical source remains the data sources which will be providing feeds or pushing the data into the Performance Platform.\nBecause of this position, we do not need ACID properties for this data, nor need worry about the CAP theorem in any detail.\n\n# Decision\nWe will use MongoDB. We are comfortable operating it in production, it will allow unstructured data (in the form of JSON documents) and we can apply structure at query time.\n\n# Status\nAccepted.\n\n# Consequences\nUse MongoDB with an appropriate replica-set configuration. Ensure that proper indexing and sharding configurations are considered as the data volume grows.","GenTime":"2024-08-19 23:39:58"}
{"File Name":"origin\/0005-simplify-off-chain-storage.md","Context":"## Context\\nStoring data in hybrid storage (off-chain and on-chain) presented problems with data inconsistency when some data was written off-chain but it's on-chain reference hasn't been committed yet. This would cause some data being corrupted under certain conditions.\\n","Decision":"We decided to store off-chain data in a way which doesn't reference the ID of any of the entities, but purely stores the precise proof data under the hash of the off-chain data.\\n","tokens":52,"id":1558,"text":"## Context\\nStoring data in hybrid storage (off-chain and on-chain) presented problems with data inconsistency when some data was written off-chain but it's on-chain reference hasn't been committed yet. This would cause some data being corrupted under certain conditions.\\n\n\n##Decision\nWe decided to store off-chain data in a way which doesn't reference the ID of any of the entities, but purely stores the precise proof data under the hash of the off-chain data.\\n","MatchedIds":[3692,3746,4859,2215,4152],"Predicted":"## Decision\nImplement a transactional consistency mechanism that ensures both on-chain and off-chain operations are completed successfully before any state is committed. This can be achieved by:\n1. Using a two-phase commit protocol to coordinate the transactions ensuring all parts either complete successfully or roll back.\n2. Introducing a temporary state buffer that holds off-chain data and on-chain references until both are confirmed, only then committing the final state change.\n3. Implementing retry logic for scenarios where either of the transactions fails, ensuring eventual consistency.\n4. Incorporating comprehensive logging and monitoring mechanisms to detect and resolve issues promptly.\n\nThis approach guarantees data integrity and minimizes the risk of corruption due to partial commits.","GenTime":"2024-08-19 23:40:02"}
{"File Name":"educational-platform\/0009-architecture-tests.md","Context":"## Context\\nWe need to have the mechanism for supporting and validating common architecture principles in all application.\\n","Decision":"Architecture tests with using Archunit should be implemented.\\n","tokens":21,"id":2502,"text":"## Context\\nWe need to have the mechanism for supporting and validating common architecture principles in all application.\\n\n\n##Decision\nArchitecture tests with using Archunit should be implemented.\\n","MatchedIds":[4428,3119,1436,294,2583],"Predicted":"## Decision\nWe will implement a centralized architectural governance framework that includes templates, guidelines, and validation tools to ensure consistency and compliance with common architecture principles across all applications.","GenTime":"2024-08-19 23:40:04"}
{"File Name":"molgenis-frontend\/0003-have-app-create-settings.md","Context":"## Context\\nApps (for example the data-explorer-2 or the biobank-explorer) may need settings data to function properly. As these settings are dynamic (may be changed at run time), and may change between deployments these settings need to be stored somewhere (molgenis database). This poses two problems; 1) how to link the app with the settings data, 2) how to control access to the settings.\\n","Decision":"We leave it up to the app to create the settings entity. Each app creates 0 or 1 settings entities. By default everyone can read the settings data. Depending on the app the entity contains 1 or more rows ( in case of multiple rows the app is responsible using the correct row(s) in each context).\\nRejected alternatives:\\n- Add settings via bootstrapped entity in molgenis core; This was deemed as insufficiently flexible due to the dependance on Java knowledge and (possible major version) core release.\\n- Use of app manager; App-manager does not facilitate an automated deployment process. App manager restricts the app architecture and settings structure.\\n- Use of entity meta data: Does not allow for settings per 'entity and app' combination. For instance the dataexplorer needs n settings rows or n tables.\\n- Proxy settings request to external repository; Deemed as hard to manage, app and proxy settings need to be kept in sync, also requires knowledge of proxy (nginx, apache) to configure.\\n","tokens":90,"id":1514,"text":"## Context\\nApps (for example the data-explorer-2 or the biobank-explorer) may need settings data to function properly. As these settings are dynamic (may be changed at run time), and may change between deployments these settings need to be stored somewhere (molgenis database). This poses two problems; 1) how to link the app with the settings data, 2) how to control access to the settings.\\n\n\n##Decision\nWe leave it up to the app to create the settings entity. Each app creates 0 or 1 settings entities. By default everyone can read the settings data. Depending on the app the entity contains 1 or more rows ( in case of multiple rows the app is responsible using the correct row(s) in each context).\\nRejected alternatives:\\n- Add settings via bootstrapped entity in molgenis core; This was deemed as insufficiently flexible due to the dependance on Java knowledge and (possible major version) core release.\\n- Use of app manager; App-manager does not facilitate an automated deployment process. App manager restricts the app architecture and settings structure.\\n- Use of entity meta data: Does not allow for settings per 'entity and app' combination. For instance the dataexplorer needs n settings rows or n tables.\\n- Proxy settings request to external repository; Deemed as hard to manage, app and proxy settings need to be kept in sync, also requires knowledge of proxy (nginx, apache) to configure.\\n","MatchedIds":[1173,2221,2488,4859,3950],"Predicted":"## Decision\nWe will utilize a two-pronged approach to address both linkage and access control for app settings data:\n\n### Linking the App with Settings Data\nWe will store the settings data in a dedicated table within the Molgenis database. Each app instance will have a unique identifier (e.g., `app_id`) that can be used to fetch its corresponding settings. This identifier will be embedded in the app\u2019s configuration, allowing the app to query its settings dynamically at runtime.\n\n### Controlling Access to the Settings\nTo ensure secure access to these settings, we will implement role-based access control (RBAC) using Molgenis' inherent security features. Users and apps will be assigned roles that dictate their level of access to the settings data. Only authorized roles will have permissions to read or modify specific settings. This ensures that only appropriate entities can access or alter settings, while maintaining a flexible and scalable security model.\n\n#### Steps:\n1. **Data Schema**: Define a `settings` table in Molgenis with columns for `app_id`, `setting_key`, `setting_value`, and `last_modified`.\n2. **Unique Identifier**: Each app instance includes its `app_id` in configuration files or environment variables.\n3. **API Protection**: Implement API endpoints for accessing the settings, secured with Molgenis access controls and role checks.\n4. **RBAC Implementation**: Configure roles and permissions within Molgenis to restrict access to the `settings` table based on role assignments.\n\nThis approach ensures that each app can dynamically retrieve and modify its settings, while maintaining strict control over who can access and change those settings.","GenTime":"2024-08-19 23:40:10"}
{"File Name":"paas-team-manual\/ADR002-concourse-pool-resource.html.md","Context":"## Context\\nWhen building pipelines using concourse, we investigated using the [pool\\nresource](https:\/\/github.com\/concourse\/pool-resource) in order to control flow\\nthrough jobs. This was an alternative to the use of the\\n[semver resource](https:\/\/github.com\/concourse\/semver-resource).\\nThese 2 resources are both workarounds to solve the problem of triggering jobs\\nwhen we haven't made changes to a resource.\\nThe problem is that the pool resource relies on write access to a github repo,\\nwhich means we must pass public keys that allow this access into the pipeline\\nand deployed concourse instance - we want to minimise the number of credentials\\nwe pass, and the semver resource relies on AWS credentials that are already\\npassed.\\n","Decision":"We will not use the pool resource for flow between jobs - instead we will use\\nthe semver resource\\n","tokens":158,"id":210,"text":"## Context\\nWhen building pipelines using concourse, we investigated using the [pool\\nresource](https:\/\/github.com\/concourse\/pool-resource) in order to control flow\\nthrough jobs. This was an alternative to the use of the\\n[semver resource](https:\/\/github.com\/concourse\/semver-resource).\\nThese 2 resources are both workarounds to solve the problem of triggering jobs\\nwhen we haven't made changes to a resource.\\nThe problem is that the pool resource relies on write access to a github repo,\\nwhich means we must pass public keys that allow this access into the pipeline\\nand deployed concourse instance - we want to minimise the number of credentials\\nwe pass, and the semver resource relies on AWS credentials that are already\\npassed.\\n\n\n##Decision\nWe will not use the pool resource for flow between jobs - instead we will use\\nthe semver resource\\n","MatchedIds":[3915,3595,1372,351,1739],"Predicted":"## Decision\nWe have decided to use the [semver resource](https:\/\/github.com\/concourse\/semver-resource) for controlling the flow through jobs in our Concourse pipelines. \n\n### Rationales\n1. **Credential Minimization**: The pool resource requires passing public keys with write access to a GitHub repo, increasing the number of credentials handled by the pipeline and Concourse instance. On the other hand, the semver resource utilizes AWS credentials that are already in use, thus reducing the number of additional credentials required.\n2. **Consistency and Security**: Relying on a single set of AWS credentials is more consistent and easier to manage, ensuring that our security posture remains robust.\n3. **Infrastructure Simplicity**: Utilizing existing AWS credentials eliminates the need for additional infrastructure changes, keeping the pipeline configuration simpler and more maintainable.\n\n### Positive Consequences\n* **Enhanced Security**: Fewer credentials mean less risk of leakage or exposure.\n* **Simplicity in Management**: Using existing AWS credentials streamlines credential management and reduces administrative overhead.\n* **Reduced Complexity**: Fewer dependencies and simpler infrastructure result in easier troubleshooting and maintenance.\n\n### Negative Consequences\n* **Reliance on AWS**: This approach inherently ties our pipeline to the AWS ecosystem, potentially limiting flexibility with other platforms in the future.\n\nWe will proceed with implementing the semver resource in our Concourse pipelines and monitor its effectiveness in managing job flow. Further evaluation will be performed periodically to ensure it continues to meet our needs effectively.","GenTime":"2024-08-19 23:40:15"}
{"File Name":"libelektra\/empty_files.md","Context":"## Problem\\nAn empty KeySet is passed to kdbSet(). What is the correct persistent\\nrepresentation?\\n","Decision":"Remove files on empty KeySet.\\n","tokens":23,"id":1271,"text":"## Problem\\nAn empty KeySet is passed to kdbSet(). What is the correct persistent\\nrepresentation?\\n\n\n##Decision\nRemove files on empty KeySet.\\n","MatchedIds":[1289,1309,1310,1318,1292],"Predicted":"## Decision\n- Define an official representation for an empty KeySet within kdbSet().\n- If the storage plugin supports it, persist the empty KeySet as an empty configuration file or a minimal structure representing an empty configuration.\n- Storage plugins must document how they handle empty KeySets under `infos\/features\/empty`.\n- Provide a guideline for storage plugin developers to ensure consistent behavior across plugins when dealing with empty KeySets.\n- Update the documentation to include examples and clarify what administrators and developers should expect when an empty KeySet is persisted.","GenTime":"2024-08-19 23:40:19"}
{"File Name":"adrflow\/8-Remove_ADR_Util_Sync_Module.md","Context":"supersedes [5](5-Utility_Context_Lazily_Initialized.md)\\nmentions [6](6-Centralize_Definition_of_Filename.md)\\n## Context\\nThe ADR Util module grew as an attempt to centralize a list of utility functions that require the context of the ADR file and directory.\\nThe resulting module was very central to the logic of the entire program, but resulted in a module that had too much differing concerns (too many responsibilities\/reasons to change).\\nThis has eventually resulted in tangled code, and sometimes to (unintentional) circular dependencies, e.g. as when trying to isolate the handling of links.\\nAt the same time, the usage of the shared context - ADR files and directory - hasn't proved that useful in most case.\\nIn the cases where it should be used, it can be defined and cached.\\n","Decision":"Divide the utilities defined in this module to other, more focused modules (under `core` directory):\\n1. `files.js`: handles all file reading and writing\\n2. `adrobj.js`: handles all ADR logical-level operations. Essentially linking the ID to the file.\\n3. `links.js`: handles logic around linking ADRs\\nIn cases where caching the ADR files is needed, e.g. in `adrobj.js`, it is done in that module.\\n","tokens":180,"id":3637,"text":"supersedes [5](5-Utility_Context_Lazily_Initialized.md)\\nmentions [6](6-Centralize_Definition_of_Filename.md)\\n## Context\\nThe ADR Util module grew as an attempt to centralize a list of utility functions that require the context of the ADR file and directory.\\nThe resulting module was very central to the logic of the entire program, but resulted in a module that had too much differing concerns (too many responsibilities\/reasons to change).\\nThis has eventually resulted in tangled code, and sometimes to (unintentional) circular dependencies, e.g. as when trying to isolate the handling of links.\\nAt the same time, the usage of the shared context - ADR files and directory - hasn't proved that useful in most case.\\nIn the cases where it should be used, it can be defined and cached.\\n\n\n##Decision\nDivide the utilities defined in this module to other, more focused modules (under `core` directory):\\n1. `files.js`: handles all file reading and writing\\n2. `adrobj.js`: handles all ADR logical-level operations. Essentially linking the ID to the file.\\n3. `links.js`: handles logic around linking ADRs\\nIn cases where caching the ADR files is needed, e.g. in `adrobj.js`, it is done in that module.\\n","MatchedIds":[3635,2914,3144,1545,128],"Predicted":"## Decision\nRefactor the ADR Util module to separate its concerns into distinct, single-responsibility modules. This approach aims to minimize intertwined logic and reduce the risk of circular dependencies. Each module will handle a specific aspect of the ADR processing, such as link handling, file interactions, or directory management.\n\nTo manage the shared context more effectively, the ADR files and directory will be initialized and cached only in the specific modules that truly require them. This way, the context utilization is confined to relevant parts of the application, promoting cleaner and more maintainable code.","GenTime":"2024-08-19 23:40:22"}
{"File Name":"content-publisher\/0004-editing-microcopy.md","Context":"## Context\\nEvery feature we add to the app comes with its own static text, which is either embedded in the code (Ruby or JavaScript) or in the HTML. Static text can be anything from the page title, to the text of a button, to an entire page of guidance.\\nWriting text 'inline' makes it hard for us to audit all of strings in our application, some of which can only be seen under special conditions e.g. error messages. It also makes it hard to change strings consistently across the application - a task which has to be done by a developer. Finally, using inline strings in code distracts from the logical flow of that code.\\n[Rails Internationalization](https:\/\/guides.rubyonrails.org\/i18n.html) (also referred to as 'translations') are a way to extract all of the strings in the application to a central location in `config\/locales\/en`. The strings can be organized in a hierarchy over one or more files, as below, where we can refer to the reviewed title by writing `I18n.t(\"publish.published.reviewed.title\")`.\\n```\\n# publish_document\/published.yml\\nen:\\npublish_document:\\npublished:\\nreviewed:\\ntitle: Content has been published\\nbody: |\\n\u2018%{title}\u2019 has been published on GOV.UK.\\nIt may take 5 minutes to appear live.\\n```\\nRails translations have a few special behaviours, such as pluralization, raw HTML, and variables. The `%{title}` string in the above is an example of a variable, which a developer will set to the title of the document being published.\\n","Decision":"Although we could use translations to extract all of the strings in the application, in some cases we felt this wasn't necessary, or that a different method should be used. The following is a summary of the rules we currently use.\\n* **Link and button labels** are not extracted. We think link and button labels are unlikely to change, and extracting them made the application tests harder to read by obfuscating some of the crucial steps in the test with translation keys.\\n* **Publishing component strings** are not extracted. This ensures we are able to migrate these components to the [govuk_publishing_components](https:\/\/github.com\/alphagov\/govuk_publishing_components) repo, which wouldn't be able to access our local translations.\\n* **Big guidance** is extracted into it's own Markdown files and stored alongside the corresponding HTML page that shows it. For example, the guidance for creating a new document is stored in `app\/views\/new_document\/guidance.md`.\\n* **Domain data** that's static is stored in a number of custom YAML files. This application has two static models (for document types and their selection) that encapsulate domain concepts where the data is part of the application. We have split up domain data based on whether it's used in a backend setting or as a string for use in the frontend. The latter are extracted at the top-level of the translation hierarchy.\\n* **Global strings** (states and validation messages) are extracted using translations. As these strings aren't page-specific, we put them at the top-level of the translation hierarchy (in `states.yml` and `validations.yml`).\\n* **All other strings** are extracted using translations, in a hierarchy that follows the structure of the `app\/views` directory. For example, the above example relates to `app\/views\/publish_document\/published.html.erb`.\\n* **Small amounts of govspeak and HTML** are extracted using translations as for other strings, with '\\_html' or '\\_govspeak' appended to the final component of the key to indicate they support rich text.\\nEvery instance of a string in the tests has been replaced according to the above rules, such that the tests continue to pass when an extracted string is changed. **Link and button labels** are not replaced, as they are not extracted in the code.\\nWe also configured Rails to raise an exception when we hit a page where a translation is missing, as we don't test all of the translations; this is done by setting `config.action_view.raise_on_missing_translations = true` in `application.rb`.\\n","tokens":343,"id":4878,"text":"## Context\\nEvery feature we add to the app comes with its own static text, which is either embedded in the code (Ruby or JavaScript) or in the HTML. Static text can be anything from the page title, to the text of a button, to an entire page of guidance.\\nWriting text 'inline' makes it hard for us to audit all of strings in our application, some of which can only be seen under special conditions e.g. error messages. It also makes it hard to change strings consistently across the application - a task which has to be done by a developer. Finally, using inline strings in code distracts from the logical flow of that code.\\n[Rails Internationalization](https:\/\/guides.rubyonrails.org\/i18n.html) (also referred to as 'translations') are a way to extract all of the strings in the application to a central location in `config\/locales\/en`. The strings can be organized in a hierarchy over one or more files, as below, where we can refer to the reviewed title by writing `I18n.t(\"publish.published.reviewed.title\")`.\\n```\\n# publish_document\/published.yml\\nen:\\npublish_document:\\npublished:\\nreviewed:\\ntitle: Content has been published\\nbody: |\\n\u2018%{title}\u2019 has been published on GOV.UK.\\nIt may take 5 minutes to appear live.\\n```\\nRails translations have a few special behaviours, such as pluralization, raw HTML, and variables. The `%{title}` string in the above is an example of a variable, which a developer will set to the title of the document being published.\\n\n\n##Decision\nAlthough we could use translations to extract all of the strings in the application, in some cases we felt this wasn't necessary, or that a different method should be used. The following is a summary of the rules we currently use.\\n* **Link and button labels** are not extracted. We think link and button labels are unlikely to change, and extracting them made the application tests harder to read by obfuscating some of the crucial steps in the test with translation keys.\\n* **Publishing component strings** are not extracted. This ensures we are able to migrate these components to the [govuk_publishing_components](https:\/\/github.com\/alphagov\/govuk_publishing_components) repo, which wouldn't be able to access our local translations.\\n* **Big guidance** is extracted into it's own Markdown files and stored alongside the corresponding HTML page that shows it. For example, the guidance for creating a new document is stored in `app\/views\/new_document\/guidance.md`.\\n* **Domain data** that's static is stored in a number of custom YAML files. This application has two static models (for document types and their selection) that encapsulate domain concepts where the data is part of the application. We have split up domain data based on whether it's used in a backend setting or as a string for use in the frontend. The latter are extracted at the top-level of the translation hierarchy.\\n* **Global strings** (states and validation messages) are extracted using translations. As these strings aren't page-specific, we put them at the top-level of the translation hierarchy (in `states.yml` and `validations.yml`).\\n* **All other strings** are extracted using translations, in a hierarchy that follows the structure of the `app\/views` directory. For example, the above example relates to `app\/views\/publish_document\/published.html.erb`.\\n* **Small amounts of govspeak and HTML** are extracted using translations as for other strings, with '\\_html' or '\\_govspeak' appended to the final component of the key to indicate they support rich text.\\nEvery instance of a string in the tests has been replaced according to the above rules, such that the tests continue to pass when an extracted string is changed. **Link and button labels** are not replaced, as they are not extracted in the code.\\nWe also configured Rails to raise an exception when we hit a page where a translation is missing, as we don't test all of the translations; this is done by setting `config.action_view.raise_on_missing_translations = true` in `application.rb`.\\n","MatchedIds":[2386,3586,3126,1416,783],"Predicted":"## Decision\nWe will use Rails Internationalization (I18n) to manage all static text in our application.\n\nThis approach offers several benefits:\n1. **Centralized Management:** All strings will be centralized in `config\/locales\/en` or other locale files, making it easier to audit and update text across the application.\n2. **Consistency:** Changes to text can be made in one place and will reflect throughout the application, ensuring consistency.\n3. **Readability:** Removing inline text improves code readability and logical flow, as the focus remains on the application logic.\n4. **Extendability:** Supports special behaviors like pluralization, raw HTML, and variables, making it adaptable to complex text requirements.\n\nWe will start by incrementally extracting existing static text into I18n files and updating the application code to use `I18n.t` for accessing the text.","GenTime":"2024-08-19 23:40:26"}
{"File Name":"nhsuk-prototype-kit-version-one\/0001-use-node-and-express.md","Context":"## Context\\nThe prototype kit application will need to operate similarly to a live service, depending on the level of fidelity required for testing. This means that a server is required. To make the prototype kit easy to use for different levels of coding experience we want to implement auto routing (or route matching) for template files, the simplest way to achieve this this dynamically is to have an application running on a server.\\n","Decision":"We will use Node.js with the Express framework to run a Node.js server for the prototype kit on both local development environments and production environment when published on Heroku. Node.js is written in javascript and is therefore one the most common languages between designers who have some coding experience and software engineers. It is also the easiest runtime to install on both Mac and PC.\\n","tokens":83,"id":3681,"text":"## Context\\nThe prototype kit application will need to operate similarly to a live service, depending on the level of fidelity required for testing. This means that a server is required. To make the prototype kit easy to use for different levels of coding experience we want to implement auto routing (or route matching) for template files, the simplest way to achieve this this dynamically is to have an application running on a server.\\n\n\n##Decision\nWe will use Node.js with the Express framework to run a Node.js server for the prototype kit on both local development environments and production environment when published on Heroku. Node.js is written in javascript and is therefore one the most common languages between designers who have some coding experience and software engineers. It is also the easiest runtime to install on both Mac and PC.\\n","MatchedIds":[3682,3470,1243,4271,4021],"Predicted":"## Decision\nWe will use [Express](http:\/\/expressjs.com\/) as our server framework for the prototype kit application. Express provides a minimalist, unopinionated web framework that is easy to set up and run. It allows us to dynamically configure auto routing (or route matching) for template files, providing the flexibility needed for different levels of testing fidelity.\n\nUsing Express, we can simplify the configuration and handling of routes by leveraging middleware to automatically match routes based on the file structure of template files. This will ensure that the prototype kit remains easy to use and accessible to users with varying levels of coding experience, while still operating similarly to a live service.\n\nAdditionally, Express's extensive documentation and large community support will help us to quickly resolve any issues and extend functionality as needed.","GenTime":"2024-08-19 23:40:29"}
{"File Name":"klokwrk-project\/0007-git-workflow-with-linear-history.md","Context":"## Context\\nThe value of tidy and [semi-linear commit history](https:\/\/fangpenlin.com\/images\/2013-09-30-keep-a-readable-git-history\/source_tree_new_branch_rebase_merge.png) is often overlooked in many Git-based\\nprojects. This is unfortunate since non-linear git commit history might be a [horrible mess](https:\/\/tugberkugurlu.blob.core.windows.net\/bloggyimages\/d773c1fe-4db8-4d2f-a994-c60f3f8cb6f0.png) that\\ndoes not provide any useful information. We want to use as simple as possible git workflow that promotes and ensures a semi-linear history.\\n> * **Semi-linear** commit history usually refers to a history that uses merge commits (git \"no-fast-forward\" merge option) to clearly denote which commits are meant to be together and represent a\\n>   coherent whole.\\n> * **Linear** commit history usually refers to completely flat history (git default \"fast-forward\") where it is impossible to tell at first glance which commits belong together.\\nWhen working on individual features, related git commits can be organized either as \"work log\" or as a \"recipe\". When working in a team, it is crucial that team members and\/or reviewers can easily\\ncomprehend what is going on in a particular feature. For this reason, we prefer features to be organized as \"recipes\".\\n> * **Work log** style of organizing feature commits refers to the style without any organization. Commits are added solely as they are developed through time.\\n> * **Recipe** style of organizing feature commits refers to the style where commits have a sensible organization where peer developers can clearly see and learn how the feature is created. This\\n>   style requires some additional work as its primary goal is communication, instead of just implementing a feature.\\nVery often, in bigger teams, common git workflows have a problem of broken continuous integration builds. We want to embrace and use as simple as possible workflow that resolves that problem.\\nChosen git workflow should seamlessly support release versioning and, if needed, related work on release branches.\\n### Architectural Context\\n* System (`klokwrk-project`)\\n","Decision":"**We will use a [stable mainline branching model for Git](https:\/\/www.bitsnbites.eu\/a-stable-mainline-branching-model-for-git\/).** It\\n[supports semi-linear Git history](https:\/\/www.bitsnbites.eu\/a-tidy-linear-git-history\/) and helps to resolve the problem of broken continuous integration builds.\\n**We will, however, introduce several tweaks to the \"stable mainline branching model\":**\\n* We will use the following naming pattern for feature branches: **`feature_<name>`** instead of `feature\/name`.\\n* We will not use new branches when remote feature branches need rebasing. Instead, we will just inform all collaborators that rebasing is pending for a feature branch. It is important\\nto communicate with collaborators **before** rebasing and force-pushing.\\n* We will not create a release branch for each release. In general, we will just tag a release. However, we will create a release branch when a particular release needs fixing.\\nWe will organize our [feature commits as recipes](https:\/\/www.bitsnbites.eu\/git-history-work-log-vs-recipe\/) because we want to promote the team's learning and communication.\\nWe will also use specific commit message format as described in [ADR-0014 - Commit Message Format](.\/0014-commit-message-format.md)\\n","tokens":461,"id":4899,"text":"## Context\\nThe value of tidy and [semi-linear commit history](https:\/\/fangpenlin.com\/images\/2013-09-30-keep-a-readable-git-history\/source_tree_new_branch_rebase_merge.png) is often overlooked in many Git-based\\nprojects. This is unfortunate since non-linear git commit history might be a [horrible mess](https:\/\/tugberkugurlu.blob.core.windows.net\/bloggyimages\/d773c1fe-4db8-4d2f-a994-c60f3f8cb6f0.png) that\\ndoes not provide any useful information. We want to use as simple as possible git workflow that promotes and ensures a semi-linear history.\\n> * **Semi-linear** commit history usually refers to a history that uses merge commits (git \"no-fast-forward\" merge option) to clearly denote which commits are meant to be together and represent a\\n>   coherent whole.\\n> * **Linear** commit history usually refers to completely flat history (git default \"fast-forward\") where it is impossible to tell at first glance which commits belong together.\\nWhen working on individual features, related git commits can be organized either as \"work log\" or as a \"recipe\". When working in a team, it is crucial that team members and\/or reviewers can easily\\ncomprehend what is going on in a particular feature. For this reason, we prefer features to be organized as \"recipes\".\\n> * **Work log** style of organizing feature commits refers to the style without any organization. Commits are added solely as they are developed through time.\\n> * **Recipe** style of organizing feature commits refers to the style where commits have a sensible organization where peer developers can clearly see and learn how the feature is created. This\\n>   style requires some additional work as its primary goal is communication, instead of just implementing a feature.\\nVery often, in bigger teams, common git workflows have a problem of broken continuous integration builds. We want to embrace and use as simple as possible workflow that resolves that problem.\\nChosen git workflow should seamlessly support release versioning and, if needed, related work on release branches.\\n### Architectural Context\\n* System (`klokwrk-project`)\\n\n\n##Decision\n**We will use a [stable mainline branching model for Git](https:\/\/www.bitsnbites.eu\/a-stable-mainline-branching-model-for-git\/).** It\\n[supports semi-linear Git history](https:\/\/www.bitsnbites.eu\/a-tidy-linear-git-history\/) and helps to resolve the problem of broken continuous integration builds.\\n**We will, however, introduce several tweaks to the \"stable mainline branching model\":**\\n* We will use the following naming pattern for feature branches: **`feature_<name>`** instead of `feature\/name`.\\n* We will not use new branches when remote feature branches need rebasing. Instead, we will just inform all collaborators that rebasing is pending for a feature branch. It is important\\nto communicate with collaborators **before** rebasing and force-pushing.\\n* We will not create a release branch for each release. In general, we will just tag a release. However, we will create a release branch when a particular release needs fixing.\\nWe will organize our [feature commits as recipes](https:\/\/www.bitsnbites.eu\/git-history-work-log-vs-recipe\/) because we want to promote the team's learning and communication.\\nWe will also use specific commit message format as described in [ADR-0014 - Commit Message Format](.\/0014-commit-message-format.md)\\n","MatchedIds":[4890,4600,2517,4889,3653],"Predicted":"## Decision\n**We will adopt a semi-linear git workflow with a focus on organized feature commits and robust continuous integration.**\n\n### Decision Details\n\n#### Semi-Linear Commit History\n\n1. **Merge Commits with No-Fast-Forward**: We will always use merge commits with the `--no-ff` (no fast-forward) option when merging feature branches into the main branch to ensure that the history denotes which commits are meant to be together.\n\n#### Feature Commit Organization\n\n2. **Recipe Style**: Developers should organize feature commits in a \"recipe\" style rather than a \"work log\" style. This means creating commits in a sensible, organized manner as if explaining or teaching how the feature was implemented.\n\n   - **Commit Message Conventions**: Follow our [conventional commits](https:\/\/www.conventionalcommits.org\/en\/v1.0.0\/) format for commit messages to maintain clarity.\n   - **Rebasing Before Merge**: Developers should rebase their feature branches against the latest main branch before merging to ensure a clean and linear history.\n\n#### Continuous Integration and Deployment\n\n3. **CI Health**: To minimize the chances of broken builds:\n   - **Feature Branch Testing**: All feature branches must pass CI tests before being eligible for merge.\n   - **Pre-merge Checks**: Automated pre-merge checks in our CI pipeline will be enforced to ensure that the changes are compatible with the main branch.\n   - **Protected Main Branch**: The main branch will be protected, and only pull requests that pass all CI checks can be merged.\n\n#### Branching Strategy\n\n4. **Main and Feature Branches**:\n   - **Main Branch**: The `main` branch will be our production-ready branch.\n   - **Feature Branches**: Each feature will be developed in its own branch, named descriptively according to its purpose, e.g., `feat\/new-dashboard`, `fix\/bug-123`.\n\n#### Release Versioning\n\n5. **Release Branches**:\n   - **Tagging**: All releases will be tagged in the main branch.\n   - **Release Branches**: For significant versions or long-term support (LTS) releases, we will create `release` branches, such as `release\/v1.0`.\n\n#### Benefits\n\n- **Clarity**: Ensures that the git history is clear and easy to navigate, making it simpler to understand the progression of the codebase.\n- **Consistency**: Enforces a consistent workflow that all team members adhere to, reducing the chances of conflicts and misunderstandings.\n- **Quality**: Maintains the quality of the codebase by enforcing CI checks and organized commits before merging.\n\nBy adopting this structured approach, we ensure our git history is not only tidy and semi-linear but also promotes better collaboration and maintainability across the development team.","GenTime":"2024-08-19 23:40:39"}
{"File Name":"gsp\/ADR019-service-mesh.md","Context":"## Context\\nVerify have a need to restrict exfiltration of data, enforce strict authentication between microservices and to use mutual TLS.\\nA service mesh gives us a way of meeting these needs.\\n### Option 1: Istio\\nIstio makes it easy to create a network of deployed services with load balancing, service-to-service authentication, monitoring, and more, with few or no code changes in service code.\\nIstio supports services by deploying a special sidecar proxy throughout your environment that intercepts all network communication between microservices, you then configure and manage Istio using its control plane functionality, which includes:\\n- Automatic load balancing for HTTP, gRPC, WebSocket, and TCP traffic.\\n- Fine-grained control of traffic behaviour with rich routing rules, retries, fail-overs, and fault injection.\\n- A pluggable policy layer and configuration API supporting access controls, rate limits and quotas.\\n- Automatic metrics, logs, and traces for all traffic within a cluster, including cluster ingress and egress.\\n- Secure service-to-service communication in a cluster with strong identity-based authentication and authorisation.\\nPros\/cons:\\n- an emerging standard (installed by default on GKE)\\n- a large community of contributors\\n### Option 2: AWS App Mesh (Istio from AWS)\\n[AWS App Mesh](https:\/\/aws.amazon.com\/app-mesh\/) is a service mesh that provides application-level networking to make it easy for your services to communicate with each other across multiple types of compute infrastructure. App Mesh standardizes how your services communicate, giving you end-to-end visibility and ensuring high-availability for your applications\\npros\/cons:\\n- Unavailable in London region\\n- Did not support automatic sidecar injection (meaning service teams would have to add lots of extra configuration to their Deployments)\\n- Appears to be abstraction over Istio\\n### Option 3: Linkerd 1.x & 2.0\\n[Linkerd](https:\/\/linkerd.io\/) is an ultra light service mesh for Kubernetes. It gives you observability, reliability, and security without requiring any code changes.\\nPros\/cons:\\n- 1.0 has a richer feature set but poorer kubernetes support\\n- 2.0 has a very minimal feature set but native kubernetes support\\n- Going through major rewrite for improved Kubernetes support\\n- Smaller community\\n- Fewer features around\\n","Decision":"We will use [Istio](https:\/\/istio.io\/) to provide a service mesh in the GDS Supported Platform.\\n","tokens":498,"id":3909,"text":"## Context\\nVerify have a need to restrict exfiltration of data, enforce strict authentication between microservices and to use mutual TLS.\\nA service mesh gives us a way of meeting these needs.\\n### Option 1: Istio\\nIstio makes it easy to create a network of deployed services with load balancing, service-to-service authentication, monitoring, and more, with few or no code changes in service code.\\nIstio supports services by deploying a special sidecar proxy throughout your environment that intercepts all network communication between microservices, you then configure and manage Istio using its control plane functionality, which includes:\\n- Automatic load balancing for HTTP, gRPC, WebSocket, and TCP traffic.\\n- Fine-grained control of traffic behaviour with rich routing rules, retries, fail-overs, and fault injection.\\n- A pluggable policy layer and configuration API supporting access controls, rate limits and quotas.\\n- Automatic metrics, logs, and traces for all traffic within a cluster, including cluster ingress and egress.\\n- Secure service-to-service communication in a cluster with strong identity-based authentication and authorisation.\\nPros\/cons:\\n- an emerging standard (installed by default on GKE)\\n- a large community of contributors\\n### Option 2: AWS App Mesh (Istio from AWS)\\n[AWS App Mesh](https:\/\/aws.amazon.com\/app-mesh\/) is a service mesh that provides application-level networking to make it easy for your services to communicate with each other across multiple types of compute infrastructure. App Mesh standardizes how your services communicate, giving you end-to-end visibility and ensuring high-availability for your applications\\npros\/cons:\\n- Unavailable in London region\\n- Did not support automatic sidecar injection (meaning service teams would have to add lots of extra configuration to their Deployments)\\n- Appears to be abstraction over Istio\\n### Option 3: Linkerd 1.x & 2.0\\n[Linkerd](https:\/\/linkerd.io\/) is an ultra light service mesh for Kubernetes. It gives you observability, reliability, and security without requiring any code changes.\\nPros\/cons:\\n- 1.0 has a richer feature set but poorer kubernetes support\\n- 2.0 has a very minimal feature set but native kubernetes support\\n- Going through major rewrite for improved Kubernetes support\\n- Smaller community\\n- Fewer features around\\n\n\n##Decision\nWe will use [Istio](https:\/\/istio.io\/) to provide a service mesh in the GDS Supported Platform.\\n","MatchedIds":[3889,3104,3914,3917,2240],"Predicted":"## Decision\nWe will adopt Istio as our service mesh solution for the following reasons:\n\n* **Comprehensive Feature Set**: Istio offers a wide array of features including automatic load balancing, fine-grained traffic control, pluggable policy layers, automatic telemetry, and strong security features with mutual TLS.\n* **Community and Support**: Istio is an emerging standard with a large community of contributors, which ensures better support, frequent updates, and abundant resources.\n* **Ease of Management**: It allows for a centralized management of service-to-service communication, making it easier to enforce policies and monitor communications without significant code changes.\n* **Installation and Integration**: Istio is installed by default on popular platforms like GKE, simplifying the setup process and integration with existing Kubernetes clusters.\n\nAdditional Considerations:\n* **AWS App Mesh**: Although it provides a simplified experience for AWS users, its lack of availability in the London region and the necessity for manual sidecar configuration make it less appealing.\n* **Linkerd**: Despite being lightweight and easy to integrate, the ongoing major rewrites, limited feature set in 2.0, and smaller community make it a less compelling choice for our comprehensive needs.\n\n### Positive Consequences\n* Strong service-to-service communication security.\n* Improved traffic management and observability within the cluster.\n* Minimal impact on existing service code due to the sidecar proxy model.\n\n### Negative Consequences\n* Slightly higher initial complexity and resource overhead due to sidecar proxies.\n* Need for ongoing management of Istio components to ensure peak performance and security.\n\nIn conclusion, the comprehensive feature set and the robust community support make Istio the best choice to meet our needs for secure, manageable, and observable microservices communication.","GenTime":"2024-08-19 23:40:46"}
{"File Name":"form-design-system\/zindex-values.md","Context":"## Context and Problem Statement\\nOur public-facing application is currently suffering from a \"zindex war\". In a \"zindex\\nwar\", there are no winners. Engineers must regularly set insanely high and arbitrary\\nzindex values to build our front end features.\\n","Decision":"We've decided to go with a 2 tier approach:\\n- Use pre-defined z-index values & ranges that are defined in FDS.\\n- Use values between `1` and `9` for fine layering control (usualy when pre-defined z-index\\nvalues are not useful)\\n- Rely on DOM order to set stacking order for elements of the same `z-index` (for example, a popover menu within a modal)\\n### Use values between `1` and `9` for fine layering control\\nIn cases where one element needs to appear above another, use integers below 10. Rely on\\nsurrounding stacking context to do the rest.\\nFor example, the `ButtonGroup` component needs to manage layering of buttons so that the\\nfocus ring is always visible. The surrounding stacking context does not matter - it uses\\nvalues `1` through `5` to accomplish this.\\nNote: It's helpful to understand what stacking context is to better utilize this approach. `z-index` values [are not absolute](https:\/\/developer.mozilla.org\/en-US\/docs\/Web\/CSS\/CSS_Positioning\/Understanding_z_index\/The_stacking_context).\\n### Pre-defined FDS z-index values \/ ranges\\nWe currently have 3 layers: navigation (10), modal (20), and crazy (50).\\n#### Navigation\\nYou want to use navigation z-index for headers, sidebars, etc. Anything that generally serves to \"frame\" the content on a page.\\nFor navigation, you will need often manage the relationship between navigation elements. You can do this by using calc. To note: the navigation z-index should never exceed 19, as these are z-index values for modals.\\n```\\n.mainheader {\\nz-index: calc(var(--zindex-navigation) + 9);\\n}\\n.subheader {\\nz-index: var(--zindex-navigation)\\n}\\n```\\n#### Modal\\nUse modal z-index for shims, popovers, and modals. To control stacking between these, we rely on document order and portaling. Shims come before modals, modals and popovers stack based on which one comes last (which you can help control by portaling, though we can get away with not portaling from time to time).\\n#### Crazy\\nUse crazy for the unexpected situations where you know something should go above everything else. This ideally would never need to be used but is there as an escape hatch and for debugging purposes even.\\n### For standard values, continue using increments of `10`\\nFor base stacking context, standard values are most helpful at increments of 10 because it\\ngives engineers flexibility to insert layers between defined stack levels. For example:\\n","tokens":53,"id":798,"text":"## Context and Problem Statement\\nOur public-facing application is currently suffering from a \"zindex war\". In a \"zindex\\nwar\", there are no winners. Engineers must regularly set insanely high and arbitrary\\nzindex values to build our front end features.\\n\n\n##Decision\nWe've decided to go with a 2 tier approach:\\n- Use pre-defined z-index values & ranges that are defined in FDS.\\n- Use values between `1` and `9` for fine layering control (usualy when pre-defined z-index\\nvalues are not useful)\\n- Rely on DOM order to set stacking order for elements of the same `z-index` (for example, a popover menu within a modal)\\n### Use values between `1` and `9` for fine layering control\\nIn cases where one element needs to appear above another, use integers below 10. Rely on\\nsurrounding stacking context to do the rest.\\nFor example, the `ButtonGroup` component needs to manage layering of buttons so that the\\nfocus ring is always visible. The surrounding stacking context does not matter - it uses\\nvalues `1` through `5` to accomplish this.\\nNote: It's helpful to understand what stacking context is to better utilize this approach. `z-index` values [are not absolute](https:\/\/developer.mozilla.org\/en-US\/docs\/Web\/CSS\/CSS_Positioning\/Understanding_z_index\/The_stacking_context).\\n### Pre-defined FDS z-index values \/ ranges\\nWe currently have 3 layers: navigation (10), modal (20), and crazy (50).\\n#### Navigation\\nYou want to use navigation z-index for headers, sidebars, etc. Anything that generally serves to \"frame\" the content on a page.\\nFor navigation, you will need often manage the relationship between navigation elements. You can do this by using calc. To note: the navigation z-index should never exceed 19, as these are z-index values for modals.\\n```\\n.mainheader {\\nz-index: calc(var(--zindex-navigation) + 9);\\n}\\n.subheader {\\nz-index: var(--zindex-navigation)\\n}\\n```\\n#### Modal\\nUse modal z-index for shims, popovers, and modals. To control stacking between these, we rely on document order and portaling. Shims come before modals, modals and popovers stack based on which one comes last (which you can help control by portaling, though we can get away with not portaling from time to time).\\n#### Crazy\\nUse crazy for the unexpected situations where you know something should go above everything else. This ideally would never need to be used but is there as an escape hatch and for debugging purposes even.\\n### For standard values, continue using increments of `10`\\nFor base stacking context, standard values are most helpful at increments of 10 because it\\ngives engineers flexibility to insert layers between defined stack levels. For example:\\n","MatchedIds":[1992,157,1652,1855,1240],"Predicted":"## Decision Drivers\n- Maintainability and readability of CSS.\n- Consistency across the application.\n- Minimize conflicts between different components and elements.\n- Scalability for future features without introducing further z-index conflicts.\n\n## Decision\nWe have decided to establish a z-index management system using a centralized and layered approach. We will introduce a z-index scale with predefined levels allocated for specific components and UI elements. This will be documented and strictly enforced across the application.\n\n### Implementation Plan\n1. **Define z-index layers**: \n   - Create a set of numbered layers for z-index values (e.g., background, content, overlays, modals, pop-ups, notifications). Each layer will have an associated z-index range.\n2. **Document the z-index scale**: \n   - Document the set z-index values and their intended usage within our style guide or design system documentation.\n3. **Refactor existing components**: \n   - Refactor existing components to use these predefined z-index values.\n4. **Update coding standards**: \n   - Update coding standards to include rules and guidelines for using z-index values.\n\nExample:\n```scss\n\/\/ Define z-index variables in a central file (e.g., src\/styles\/constants.scss)\n$z-index-background: 100;\n$z-index-content: 200;\n$z-index-overlays: 300;\n$z-index-modals: 400;\n$z-index-popups: 500;\n$z-index-notifications: 600;\n\n\/\/ Usage in component styles\n.header {\n  z-index: $z-index-content;\n}\n\n.modal {\n  z-index: $z-index-modals;\n}\n```\n\n### Positive Consequences\n- **Consistency**: Ensures a consistent approach to managing z-index across the application.\n- **Readability and Maintenance**: Improves the readability and maintainability of CSS by avoiding arbitrary and excessively high z-index values.\n- **Conflict Minimization**: Reduces the likelihood of z-index conflicts between different components.\n- **Scalability**: Easy to scale and manage as new features are added, with predefined z-index layers clearly guiding developers.\n\n### Negative Consequences\n- **Initial Refactoring Effort**: Requires an initial effort to refactor existing components to conform to the new z-index scale.\n- **Training and Onboarding**: New and existing developers will need to be trained in the new z-index management approach.\n\nBy implementing this z-index management system, we aim to bring order to our CSS and prevent further \"zindex wars\", making our front-end code more maintainable and scalable.","GenTime":"2024-08-19 23:40:59"}
{"File Name":"james\/0042-james-cli-based-on-webadmin.md","Context":"## Context\\nJames servers offer a command-line interface in order to interact with the server. However, it relies on the JMX protocol, which is known to be insecure. The JMX server embedded in Apache James, also used by the command line client is exposed to a java de-serialization issue according to [NVD-CVE-2017-12628 Detail](https:\/\/nvd.nist.gov\/vuln\/detail\/CVE-2017-12628), and thus can be used to execute arbitrary commands.\\nBesides, the current CLI interface is also not optimal for users. It places actions in front of entities with contiguous syntax, making it harder for the user to remember the command (for example, which entity the GET action command can interact with). If we design to place the entity first and the outgoing actions can interact with that entity afterward, the user will easily imagine what he\/she can do with each entity. This creates an intuitive interface that is easier to remember.\\nWebadmin APIs use HTTP protocol, which is more secure than JMX protocol to interact with James servers.\\nWebadmin command-line interface is an upcoming replacement for the outdated, security-vulnerable JMX command-line interface.\\n","Decision":"We decided to write a new CLI client, running on top of the JVM, communicating with James via the webadmin protocol, using http.\\n* What libraries will we use?\\n* http client: ***Feign library***. We used it as an http client in other parts of James so we continue to use it.\\n* CLI: ***Picocli library***. Picocli is a one-file command line parsing framework written in Java that allows us to create command line applications with almost no code. It allows mixing Options with positional Parameters (Eg: no need to the follow order Options then Parameters), [automatic type conversion](https:\/\/picocli.info\/#_strongly_typed_everything) of command line arguments to the type of the annotated field, provide Automatic Help and better Subcommand Support, easily handle Exceptions.\\n* How will we limit breaking changes this new CLI will cause?\\n* Work on a wrapper to adapt the old CLI API.\\n* Where will we locate this cli code?\\n* server\/protocols\/webadmin-cli\\n* Write a man page.\\n* Picocli generates beautiful documentation for our CLI (HTML, PDF and Unix man pages).\\n* We decided to adopt a more modern, modular CLI syntax:\\n```\\n$ .\/james-cli [OPTION] ENTITY ACTION {ARGUMENT}\\n```\\nwhere\\nOPTION: optional parameter when running the command line,\\nENTITY: represents the entity to perform action on,\\nACTION: name of the action to perform,\\nARGUMENT: arguments needed for the action.\\n#### Examples\\nAdd a domain to the domain list.\\n```\\n$ .\/james-cli --url http:\/\/127.0.0.1:9999 domain create domainNameToBeCreated\\n```\\nIn above command-line\\nOPTION: --url http:\/\/127.0.0.1:9999\\nENTITY: domain\\nACTION: create\\nARGUMENT: domainNameToBeCreated\\n","tokens":244,"id":2133,"text":"## Context\\nJames servers offer a command-line interface in order to interact with the server. However, it relies on the JMX protocol, which is known to be insecure. The JMX server embedded in Apache James, also used by the command line client is exposed to a java de-serialization issue according to [NVD-CVE-2017-12628 Detail](https:\/\/nvd.nist.gov\/vuln\/detail\/CVE-2017-12628), and thus can be used to execute arbitrary commands.\\nBesides, the current CLI interface is also not optimal for users. It places actions in front of entities with contiguous syntax, making it harder for the user to remember the command (for example, which entity the GET action command can interact with). If we design to place the entity first and the outgoing actions can interact with that entity afterward, the user will easily imagine what he\/she can do with each entity. This creates an intuitive interface that is easier to remember.\\nWebadmin APIs use HTTP protocol, which is more secure than JMX protocol to interact with James servers.\\nWebadmin command-line interface is an upcoming replacement for the outdated, security-vulnerable JMX command-line interface.\\n\n\n##Decision\nWe decided to write a new CLI client, running on top of the JVM, communicating with James via the webadmin protocol, using http.\\n* What libraries will we use?\\n* http client: ***Feign library***. We used it as an http client in other parts of James so we continue to use it.\\n* CLI: ***Picocli library***. Picocli is a one-file command line parsing framework written in Java that allows us to create command line applications with almost no code. It allows mixing Options with positional Parameters (Eg: no need to the follow order Options then Parameters), [automatic type conversion](https:\/\/picocli.info\/#_strongly_typed_everything) of command line arguments to the type of the annotated field, provide Automatic Help and better Subcommand Support, easily handle Exceptions.\\n* How will we limit breaking changes this new CLI will cause?\\n* Work on a wrapper to adapt the old CLI API.\\n* Where will we locate this cli code?\\n* server\/protocols\/webadmin-cli\\n* Write a man page.\\n* Picocli generates beautiful documentation for our CLI (HTML, PDF and Unix man pages).\\n* We decided to adopt a more modern, modular CLI syntax:\\n```\\n$ .\/james-cli [OPTION] ENTITY ACTION {ARGUMENT}\\n```\\nwhere\\nOPTION: optional parameter when running the command line,\\nENTITY: represents the entity to perform action on,\\nACTION: name of the action to perform,\\nARGUMENT: arguments needed for the action.\\n#### Examples\\nAdd a domain to the domain list.\\n```\\n$ .\/james-cli --url http:\/\/127.0.0.1:9999 domain create domainNameToBeCreated\\n```\\nIn above command-line\\nOPTION: --url http:\/\/127.0.0.1:9999\\nENTITY: domain\\nACTION: create\\nARGUMENT: domainNameToBeCreated\\n","MatchedIds":[578,2127,2897,4644,2150],"Predicted":"## Decision\n* Deprecate the current JMX-based command-line interface due to its security vulnerabilities and non-intuitive syntax.\n* Implement a new command-line interface that interacts with James servers using the Webadmin APIs over the HTTP protocol.\n* Ensure the new CLI design follows an entity-first syntax to improve usability and make commands more intuitive (e.g., `USER GET <username>` instead of `GET USER <username>`).\n* Gradually phase out the JMX-based interface, providing clear documentation and migration guides for users to transition to the new Webadmin-based CLI.\n* Conduct security assessments and usability testing on the new CLI before its official release to ensure it meets security standards and user expectations.\n* Provide proper support and encourage user feedback to continually improve the new CLI interface.","GenTime":"2024-08-19 23:41:02"}
{"File Name":"NorthwindCore\/0001-record-architecture-decision.md","Context":"## Context\\nAs the project is an example of a clean code and best practices of software development, it is necessary to save all architectural decisions in one place.\\n","Decision":"For all architectural decisions Architecture Decision Log (ADL) is created. All decisions will be recorded as Architecture Decision Records (ADR).\\nEach ADR will be recorded using [Michael Nygard template](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions), which contains following sections: Status, Context, Decision and Consequences.\\n","tokens":33,"id":1792,"text":"## Context\\nAs the project is an example of a clean code and best practices of software development, it is necessary to save all architectural decisions in one place.\\n\n\n##Decision\nFor all architectural decisions Architecture Decision Log (ADL) is created. All decisions will be recorded as Architecture Decision Records (ADR).\\nEach ADR will be recorded using [Michael Nygard template](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions), which contains following sections: Status, Context, Decision and Consequences.\\n","MatchedIds":[4361,4699,4408,4722,4428],"Predicted":"## Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions to save all architectural decisions in one place.","GenTime":"2024-08-19 23:41:04"}
{"File Name":"reaction-component-library\/0003-choose-a-style-guide-generator-framework.md","Context":"## Context\\nWe want:\\n- Write all simple React components in one repository\\n- Document the React components with code comments\\n- Add additional markdown documentation for components when necessary\\n- Allow both designers and engineers to edit the docs\\n- Run the tool locally to make component development and testing easier\\n- Build into a hostable web app, which can be used by anyone to learn our style, pick an appropriate component, and edit the component on the page\\n- Be able to style\/theme anything about the style guide app as a whole to match our other docs\\n### Options\\n[React Storybook](https:\/\/storybook.js.org\/)\\n[React Styleguidist](https:\/\/react-styleguidist.js.org\/)\\n","Decision":"Use Styleguidist. They way it is built from markdown is more user-friendly for designers to edit vs. React Storybook. Also, it is more aimed at generating a living style guide, whereas Storybook is more of a developer's tool.\\n","tokens":147,"id":3035,"text":"## Context\\nWe want:\\n- Write all simple React components in one repository\\n- Document the React components with code comments\\n- Add additional markdown documentation for components when necessary\\n- Allow both designers and engineers to edit the docs\\n- Run the tool locally to make component development and testing easier\\n- Build into a hostable web app, which can be used by anyone to learn our style, pick an appropriate component, and edit the component on the page\\n- Be able to style\/theme anything about the style guide app as a whole to match our other docs\\n### Options\\n[React Storybook](https:\/\/storybook.js.org\/)\\n[React Styleguidist](https:\/\/react-styleguidist.js.org\/)\\n\n\n##Decision\nUse Styleguidist. They way it is built from markdown is more user-friendly for designers to edit vs. React Storybook. Also, it is more aimed at generating a living style guide, whereas Storybook is more of a developer's tool.\\n","MatchedIds":[3031,1992,36,3721,4613],"Predicted":"## Decision\nWe have considered both React Storybook and React Styleguidist based on the requirements outlined.\n\n### React Storybook\nPros:\n- **Powerful Add-ons**: Provides a wide range of add-ons that enhance functionality such as accessibility checks, props tables, and live editing.\n- **Interactive UI**: Offers a highly interactive and user-friendly UI which allows for component browsing, searching, and visual testing.\n- **Extensive Community and Support**: Large community and active support make it easier to find solutions to any issues.\n- **Supports MDX**: Allows documentation in MDX (Markdown for JSX), enabling flexibility in documenting complex components.\n- **Visual Testing**: Built-in support for visual regression testing tools like Chromatic.\n\nCons:\n- **Complexity**: Can be overkill for simpler use cases because of its extensive features.\n- **Configuration Overhead**: More configuration is required to get started compared to simpler tools.\n\n### React Styleguidist\nPros:\n- **Simplicity**: Easier to set up and use, ideal for simpler projects.\n- **Built-in Documentation Support**: Supports documentation through JSDoc comments and markdown.\n- **Live Editing and Playground**: Offers a live editing playground which is simple and effective for testing components.\n- **Customization**: Allows for theming and customization to match the style of our other docs.\n- **Markdown Render**: In-line Markdown support makes it easy to add additional documentation.\n\nCons:\n- **Limited Features**: May lack some of the extensive features and add-ons available in Storybook.\n- **Community**: Smaller community than Storybook, which may imply fewer resources or slower issue resolution.\n\n### Decision\n**Chosen Option: React Storybook**\n\n**Rationale:**\n- Given our need to build a hostable web app used by both designers and engineers, React Storybook\u2019s powerful and interactive UI is a more suitable choice.\n- The flexibility in theming and extensive support for documentation through MDX aligns well with our desire to create a style guide app that matches our other docs and is easy to edit by both designers and engineers.\n- The ability to run the tool locally will enhance component development and testing, meeting our primary requirements.\n- Although Storybook may have a slightly higher configuration complexity, its community support, add-ons, and better tooling for visual regression testing offer significant value, which outweighs the initial setup effort.\n\n### Positive Consequences\n- Enhanced developer and designer collaboration through interactive UI.\n- Comprehensive documentation capabilities with Markdown and MDX support.\n- Extensively testable components with built-in support for various testing tools.\n- Customizable to match our existing docs, ensuring consistency across our documentation.\n\n### Negative Consequences\n- Slightly higher learning curve and setup complexity.\n- Potential overkill for very small or simple projects.\n\nBy choosing React Storybook, we are investing in a versatile and robust tool that not only meets our current needs but also positions us well for future requirements and scalability.","GenTime":"2024-08-19 23:41:14"}
{"File Name":"smjs\/2018121201-stop-using-registers-in-processor.md","Context":"## Context\\nCommunication between processor and interpreter must go both ways: the processor needs to pass the interpreter the instruction to execute, and the interpreter is supposed to be able to update the Instruction Pointer (IP), and to tell the processor it the execution needs to stop, and with which exit status.\\nCurrently we are using an intermediate component, registers, partially like how real processors work, containing both the IP and the information about the exit status. However, the communication between the processor and the interpreter is so straightforward that a much simpler solution would be to just use classical call argument, and call return value. Registers could be used as a mediator, but for this to have sense the processor and the interpreter would have to be unaware of each other, and registers should implement a mechanism to automatically notify involved parties when a change is made, which seems overkill.\\n","Decision":"The interpreter will keep taking an instruction from the processor, but this time it will return a data structure, which tells if an exit has been triggered, and in this case what's the exit status.\\n","tokens":172,"id":3972,"text":"## Context\\nCommunication between processor and interpreter must go both ways: the processor needs to pass the interpreter the instruction to execute, and the interpreter is supposed to be able to update the Instruction Pointer (IP), and to tell the processor it the execution needs to stop, and with which exit status.\\nCurrently we are using an intermediate component, registers, partially like how real processors work, containing both the IP and the information about the exit status. However, the communication between the processor and the interpreter is so straightforward that a much simpler solution would be to just use classical call argument, and call return value. Registers could be used as a mediator, but for this to have sense the processor and the interpreter would have to be unaware of each other, and registers should implement a mechanism to automatically notify involved parties when a change is made, which seems overkill.\\n\n\n##Decision\nThe interpreter will keep taking an instruction from the processor, but this time it will return a data structure, which tells if an exit has been triggered, and in this case what's the exit status.\\n","MatchedIds":[3981,885,3023,2121,2499],"Predicted":"## Decision\nWe will simplify the communication between the processor and the interpreter by eliminating the intermediate component (registers) and instead using classical call arguments and return values for direct communication. This approach will streamline the interaction by allowing the processor to pass instructions directly to the interpreter and receive updates on the Instruction Pointer (IP) and exit status through call returns.\n\n- The processor will pass the instruction to the interpreter as a call argument.\n- The interpreter will update the IP and communicate the exit status back to the processor via the call return value.\n\nThis decision avoids the unnecessary complexity and overhead of using registers as an intermediate component and aligns with the straightforward nature of their communication.","GenTime":"2024-08-19 23:41:19"}
{"File Name":"pace-developers\/0007-developer-scripts-storage-location.md","Context":"## Context\\nWhen developing new algorithms and features for PACE-related software,\\ndevelopers often create useful demo\/visualisation scripts for their own use.\\nThese scripts could be useful or interesting for other developers, and are\\nimportant for reproducibility or justifying design decisions. They should be\\nstored somewhere in version control so that they can be easily accessed by any\\ndevelopers and referred to later. However, they are not intended for general\\nuse and will not be actively maintained or tested. There are 2 main options:\\n* Store them in a `dev_scripts` directory in each separate project repository\\n* Store them in a `scripts` directory in `pace-developers`\\nIf they're in the `dev_scripts` directory for each project repository:\\n+ All in one place\\n+ Scripts will be close to the code they are used for\\n- Scripts may not work with the version of the code they are distributed with\\n- It's unclear where scripts that use more than one project would go\\n- Despite the folder being called `dev_scripts` people might expect the scripts\\nto actually work as they're in the main project repository\\nIf they're in a `scripts` directory in `pace-developers`:\\n+ They can be kept close to the decision-making developer documentation that\\nthey support\\n+ A version can be specified for any project dependencies\\n","Decision":"Developer scripts will be stored in an appropriately placed `scripts`\\ndirectory in the `pace-developers` repository. Depending on whether the\\nscript is tied to a particular software, or general algorithm development\\nit could be stored in `pace-developers\/euphonic\/scripts` or\\n`pace-developers\/powder_averaging\/scripts` for example.\\n","tokens":283,"id":5176,"text":"## Context\\nWhen developing new algorithms and features for PACE-related software,\\ndevelopers often create useful demo\/visualisation scripts for their own use.\\nThese scripts could be useful or interesting for other developers, and are\\nimportant for reproducibility or justifying design decisions. They should be\\nstored somewhere in version control so that they can be easily accessed by any\\ndevelopers and referred to later. However, they are not intended for general\\nuse and will not be actively maintained or tested. There are 2 main options:\\n* Store them in a `dev_scripts` directory in each separate project repository\\n* Store them in a `scripts` directory in `pace-developers`\\nIf they're in the `dev_scripts` directory for each project repository:\\n+ All in one place\\n+ Scripts will be close to the code they are used for\\n- Scripts may not work with the version of the code they are distributed with\\n- It's unclear where scripts that use more than one project would go\\n- Despite the folder being called `dev_scripts` people might expect the scripts\\nto actually work as they're in the main project repository\\nIf they're in a `scripts` directory in `pace-developers`:\\n+ They can be kept close to the decision-making developer documentation that\\nthey support\\n+ A version can be specified for any project dependencies\\n\n\n##Decision\nDeveloper scripts will be stored in an appropriately placed `scripts`\\ndirectory in the `pace-developers` repository. Depending on whether the\\nscript is tied to a particular software, or general algorithm development\\nit could be stored in `pace-developers\/euphonic\/scripts` or\\n`pace-developers\/powder_averaging\/scripts` for example.\\n","MatchedIds":[4178,5178,1202,3031,1669],"Predicted":"## Decision\nWe will store the demo\/visualisation scripts in a `scripts` directory within the `pace-developers` repository.\n\nThis decision is based on the following considerations:\n1. **Version Control and Dependencies**: By storing scripts in `pace-developers`, we can specify and manage versions for any project dependencies, ensuring that scripts work as intended with the appropriate versions of the code.\n2. **Centralized Location**: Having a centralized location for these scripts makes it easy for developers to find and reference them, supporting better documentation and reproducibility.\n3. **Cross-Project Usability**: Storing scripts in a shared repository allows for scripts that involve multiple projects to be appropriately organized without confusion about where they belong.\n4. **Clarity of Purpose**: By placing the scripts in a dedicated `scripts` directory in `pace-developers`, we minimize the risk of users mistakenly expecting these scripts to be fully functional and maintained, as might happen if they were in the main project repositories.","GenTime":"2024-08-19 23:41:23"}
{"File Name":"inner-source_kotlin_spark_template\/20180617-flyway.md","Context":"## Context\\nDevelopers need some tool for DB versioning and safe migrations.\\n","Decision":"We will introduce dockerized flyway setup for local development.\\nMigration scripts will also be provided.\\n","tokens":17,"id":1462,"text":"## Context\\nDevelopers need some tool for DB versioning and safe migrations.\\n\n\n##Decision\nWe will introduce dockerized flyway setup for local development.\\nMigration scripts will also be provided.\\n","MatchedIds":[317,241,398,5070,2488],"Predicted":"## Decision\nWe will use Flyway for database versioning and safe migrations.\n\nFlyway is a robust database migration tool that helps manage version control for database changes. It supports SQL and Java-based migrations, and it works well with a wide range of databases, ensuring consistency and reliability in our database schema evolution. The key features that led to this decision include:\n\n- **Ease of Use**: Flyway's straightforward approach to versioning and migrations reduces the complexity and potential for errors during deployments.\n- **Compatibility**: It supports a wide variety of databases, allowing us to remain flexible with our database technology choices.\n- **Automated Migrations**: It integrates seamlessly into our CI\/CD pipeline, enabling automated application of migrations, which enhances our deployment process.\n- **Rollbacks**: Flyway supports rollbacks, which are critical for mitigating potential issues during migrations.\n- **Community and Support**: Flyway has a strong community and extensive documentation, ensuring that we can find help and resources when needed.\n\nThis decision should simplify database versioning and migrations, making them more manageable and less prone to errors, contributing to overall system stability and developer productivity.","GenTime":"2024-08-19 23:41:28"}
{"File Name":"nso.aurora\/MultipleQueues.md","Context":"## Context\\nThe messages from Order Management is probably more important than those that are coming from Rating Manager and Recommendation Manager.\\n","Decision":"The decision is to introduce another queue for Order Management\\n","tokens":25,"id":306,"text":"## Context\\nThe messages from Order Management is probably more important than those that are coming from Rating Manager and Recommendation Manager.\\n\n\n##Decision\nThe decision is to introduce another queue for Order Management\\n","MatchedIds":[307,2672,1545,2681,1610],"Predicted":"## Decision\nWe will implement a prioritization mechanism in the message queue system to ensure that messages from Order Management are processed with higher priority compared to those from Rating Manager and Recommendation Manager. This can be achieved by categorizing messages into different priority levels and adjusting the processing order accordingly. Additionally, we will ensure that sufficient resources are allocated to handle high-priority Order Management messages to maintain smooth and timely processing.","GenTime":"2024-08-19 23:41:30"}
{"File Name":"digitalrig-metal-aws\/0006-create-reference-implementation-repository.md","Context":"## Context\\nThe rig defined at [Bookit Infrastructure](https:\/\/github.com\/buildit\/bookit-infrastructure) is an instance of the AWS Bare Metal Rig.\\nWhilst it's rather generic as it is, it is specific to Bookit's needs.\\nThe AWS Bare Metal Rig is also intended to offer choices for the different components (Compute - ECS EC2 vs ECS Fargate vs EKS, RDS - Aurora MySQL vs Aurora Postgres vs Aurora Serverless, etc).\\nThe only way to capture that is via branches which can be hard to discover.\\nFinally, there is not a single repo that represents the latest and greatest version of the AWS Bare Metal Rig.  As instances of Rigs diverge, it is difficult to instantiate a new one that includes all of the latest features\\n","Decision":"Create a digitalrig-metal-aws repo (https:\/\/github.com\/buildit\/digitalrig-metal-aws) that demonstrates all of the options and latest features of the AWS Bare Metal Rig and removes any Bookit specific wording\/concepts.\\n","tokens":162,"id":1742,"text":"## Context\\nThe rig defined at [Bookit Infrastructure](https:\/\/github.com\/buildit\/bookit-infrastructure) is an instance of the AWS Bare Metal Rig.\\nWhilst it's rather generic as it is, it is specific to Bookit's needs.\\nThe AWS Bare Metal Rig is also intended to offer choices for the different components (Compute - ECS EC2 vs ECS Fargate vs EKS, RDS - Aurora MySQL vs Aurora Postgres vs Aurora Serverless, etc).\\nThe only way to capture that is via branches which can be hard to discover.\\nFinally, there is not a single repo that represents the latest and greatest version of the AWS Bare Metal Rig.  As instances of Rigs diverge, it is difficult to instantiate a new one that includes all of the latest features\\n\n\n##Decision\nCreate a digitalrig-metal-aws repo (https:\/\/github.com\/buildit\/digitalrig-metal-aws) that demonstrates all of the options and latest features of the AWS Bare Metal Rig and removes any Bookit specific wording\/concepts.\\n","MatchedIds":[1464,350,2471,1372,923],"Predicted":"## Decision\n* Create a centralized repository for the AWS Bare Metal Rig, where the main branch captures the latest and greatest features.\n* Use feature flags or configuration files to handle the different component choices (e.g., ECS EC2 vs ECS Fargate vs EKS, Aurora MySQL vs Aurora Postgres vs Aurora Serverless) within the same codebase, instead of relying on branches.\n* Implement a robust versioning system that allows instances of the Bare Metal Rig to automatically discover and integrate updates and new features.\n* Establish a clear process for merging improvements and updates from different Bookit-specific instances back into the main repository, ensuring all innovations are captured and made available for future instantiations.\n* Use Terraform modules to encapsulate and manage the various configurations, ensuring modularity and ease of management across different infrastructure setups.","GenTime":"2024-08-19 23:41:34"}
{"File Name":"opg-data\/0008-hateoas.md","Context":"## Context\\nWe wish to build a RESTful API, and to be technically RESTful, we must implement HATEOAS. This gets us to stage 3 of the [Richardson Maturity Model](https:\/\/martinfowler.com\/articles\/richardsonMaturityModel.html)\\nFor full context \/ a HATEOAS primer, [read the supporting notes](..\/supporting-notes\/hateoas.md)\\n","Decision":"Our API is to be as RESTful as it can be.\\nWe recognise that we must implement Hypermedia as the Engine of Application State.\\nThe content structure already discussed in [0005-content-structure.md](0005-content-structure.md) implements links within our resource object... this ADR is to explicitly state our aim is to implement HATEOAS.\\nFurthermore, for every endpoint on the API we will implement an OPTIONS verb, returning every action possible at that endpoint.\\nFor full context \/ a HATEOAS primer, [read the supporting notes](..\/supporting-notes\/hateoas.md)\\n","tokens":88,"id":2184,"text":"## Context\\nWe wish to build a RESTful API, and to be technically RESTful, we must implement HATEOAS. This gets us to stage 3 of the [Richardson Maturity Model](https:\/\/martinfowler.com\/articles\/richardsonMaturityModel.html)\\nFor full context \/ a HATEOAS primer, [read the supporting notes](..\/supporting-notes\/hateoas.md)\\n\n\n##Decision\nOur API is to be as RESTful as it can be.\\nWe recognise that we must implement Hypermedia as the Engine of Application State.\\nThe content structure already discussed in [0005-content-structure.md](0005-content-structure.md) implements links within our resource object... this ADR is to explicitly state our aim is to implement HATEOAS.\\nFurthermore, for every endpoint on the API we will implement an OPTIONS verb, returning every action possible at that endpoint.\\nFor full context \/ a HATEOAS primer, [read the supporting notes](..\/supporting-notes\/hateoas.md)\\n","MatchedIds":[674,1180,2181,4424,4021],"Predicted":"## Decision\n\nWe will implement HATEOAS (Hypermedia as the Engine of Application State) in our RESTful API by embedding navigational hyperlinks within the response payloads. This approach will help guide clients on available actions and resources, ensuring our API adheres to level 3 of the [Richardson Maturity Model](https:\/\/martinfowler.com\/articles\/richardsonMaturityModel.html).\n\n### Key Implementations:\n1. **Resource Representation**:\n   - Each resource representation will include a `_links` section that describes available actions and related resources.\n   - Links will use standard HTTP methods (GET, POST, PUT, DELETE) as appropriate for the action intended.\n\n2. **Link Structure**:\n   - The `_links` object will have named properties representing different relationships such as `self`, `next`, `prev`, `related`, etc., each containing an `href` attribute specifying the URL for the linked resource.\n   - Example:\n     ```json\n     {\n       \"data\": {\n         \"type\": \"article\",\n         \"id\": \"1\",\n         \"attributes\": {\n           \"title\": \"First Article\",\n           \"content\": \"...\"\n         },\n         \"_links\": {\n           \"self\": {\n             \"href\": \"https:\/\/api.example.com\/articles\/1\"\n           },\n           \"author\": {\n             \"href\": \"https:\/\/api.example.com\/authors\/1\"\n           },\n           \"related\": {\n             \"href\": \"https:\/\/api.example.com\/articles\/1\/related\"\n           }\n         }\n       }\n     }\n     ```\n\n3. **Discoverability**:\n   - Root endpoints (e.g., `\/api`, `\/`) will provide links to available resources in the API to enable easy discovery.\n   - Example:\n     ```json\n     {\n       \"_links\": {\n         \"articles\": {\n           \"href\": \"https:\/\/api.example.com\/articles\"\n         },\n         \"authors\": {\n           \"href\": \"https:\/\/api.example.com\/authors\"\n         }\n       }\n     }\n     ```\n\n4. **Consistency and Extensibility**:\n   - The structure and naming of links will be consistent across resources to ensure a predictable and navigable API design.\n   - Future enhancements and changes can be incorporated by adding new hypermedia links without breaking existing clients.\n\n### Benefits:\n- **Intuitive Navigation**: Clients can easily navigate through resources using links provided in the responses, reducing the need for hardcoded endpoints.\n- **Reduced Coupling**: By indicating available actions and related resources, clients can interact with the API based on the current application state, reducing tight coupling to API structure.\n- **API Self-Documentation**: Embedding links inherently documents the API by showing clients what actions and transitions are possible.\n\nThis design decision will ensure our API is fully compliant with REST principles, particularly the level 3 requirement of the Richardson Maturity Model, enhancing both usability and maintenance.","GenTime":"2024-08-19 23:41:59"}
{"File Name":"konfetti\/0004-use-lazy-loading.md","Context":"## Context\\nWe need to avoid side effects on configuration loading and prevent the need to fully configure the settings to run a subset of tests in projects using `konfetti`.\\n","Decision":"We will use a lazy evaluation approach, similar to [implemented in Django](https:\/\/github.com\/django\/django\/blob\/master\/django\/conf\/__init__.py#L42)\\n","tokens":37,"id":3572,"text":"## Context\\nWe need to avoid side effects on configuration loading and prevent the need to fully configure the settings to run a subset of tests in projects using `konfetti`.\\n\n\n##Decision\nWe will use a lazy evaluation approach, similar to [implemented in Django](https:\/\/github.com\/django\/django\/blob\/master\/django\/conf\/__init__.py#L42)\\n","MatchedIds":[1258,4179,4868,1387,1173],"Predicted":"## Decision\nWe will use explicit modular configuration loading in `konfetti`, allowing us to load only necessary configurations for each test. This approach avoids side effects by isolating configurations and reduces overhead by not requiring full configuration for every test run.","GenTime":"2024-08-19 23:42:01"}
{"File Name":"gp-data-etl\/0002-store-local-copy-of-syndication-data.md","Context":"## Context\\nThe [NHS's Syndicated Content](http:\/\/www.nhs.uk\/aboutNHSChoices\/professionals\/syndication\/Pages\/Webservices.aspx)\\nholds information about GP surgeries.\\nSeveral of Connecting to Services applications need to make use of GP data, including the\\n[GP Finder](https:\/\/github.com\/nhsuk\/gp-finder) and [Profiles](https:\/\/github.com\/nhsuk\/profiles) projects.\\nThe Syndication API is not intended for direct access in production environments.\\nThe Syndication API returns data in XML, and information is spread across several subpages,\\ni.e. overview, services, facilities.\\n","Decision":"The syndication XML API should be scraped nightly to create a local copy of the data.\\nWhat were multiple pages for a practice on the Syndication API will be merged into a single record per practice and\\nconverted into the Connecting to Services teams preferred format, JSON.\\n","tokens":137,"id":2530,"text":"## Context\\nThe [NHS's Syndicated Content](http:\/\/www.nhs.uk\/aboutNHSChoices\/professionals\/syndication\/Pages\/Webservices.aspx)\\nholds information about GP surgeries.\\nSeveral of Connecting to Services applications need to make use of GP data, including the\\n[GP Finder](https:\/\/github.com\/nhsuk\/gp-finder) and [Profiles](https:\/\/github.com\/nhsuk\/profiles) projects.\\nThe Syndication API is not intended for direct access in production environments.\\nThe Syndication API returns data in XML, and information is spread across several subpages,\\ni.e. overview, services, facilities.\\n\n\n##Decision\nThe syndication XML API should be scraped nightly to create a local copy of the data.\\nWhat were multiple pages for a practice on the Syndication API will be merged into a single record per practice and\\nconverted into the Connecting to Services teams preferred format, JSON.\\n","MatchedIds":[1863,2204,4276,1180,2050],"Predicted":"## Decision\nWe will implement an intermediate service to regularly fetch data from the NHS Syndicated Content API and aggregate it into a more usable format. This service will transform the XML data into JSON and combine the necessary subpages (overview, services, facilities) into a single, coherent structure for easier consumption. The aggregated data will be stored in a database and served through a RESTful API designed to meet the needs of the GP Finder and Profiles projects. This approach will ensure that our production environments are not directly dependent on the Syndication API while providing a consistent and reliable way to access up-to-date GP data.","GenTime":"2024-08-19 23:42:05"}
{"File Name":"Head-Start-TTADP\/0003-configuration-by-environment-variable.md","Context":"## Context\\nApplications need to be configured differently depending on where they are running. For example, the backend running locally will have different configuration then the backend running in production.\\n","Decision":"We will use environment variables to configure applications.\\n","tokens":35,"id":1173,"text":"## Context\\nApplications need to be configured differently depending on where they are running. For example, the backend running locally will have different configuration then the backend running in production.\\n\n\n##Decision\nWe will use environment variables to configure applications.\\n","MatchedIds":[1239,4358,3657,4271,1514],"Predicted":"## Decision\nWe will use environment-specific configuration files to manage different configurations for each environment. Specifically, we will adopt a configuration management library such as [dotenv] for Node.js or [Spring Cloud Config] for Java applications. These tools allow us to load environment variables from files, enabling us to have separate configuration files for different environments (e.g., `.env.local`, `.env.production`). This approach ensures that sensitive information can be securely managed and reduces the risk of configuration errors when deploying to different environments.\n\n### Implementation Steps:\n1. **Local Environment**: Use a `.env.local` file for local development configurations.\n2. **Production Environment**: Use a `.env.production` file for production configurations.\n3. **Staging Environment**: If applicable, use a `.env.staging` file for staging configurations.\n4. **Loading Configurations**: Ensure that the application code loads the appropriate configuration file based on an `ENVIRONMENT` variable (e.g., `NODE_ENV` for Node.js applications).\n5. **Version Control**: Add the configuration files (except those with sensitive information) to version control, but keep them out of the main repository to avoid accidental exposure of sensitive data.\n6. **Documentation**: Provide clear documentation for developers on how to manage and switch between different configuration files.\n\nRejected alternatives:\n- **Hardcoding Configurations**: This approach was rejected because it does not provide flexibility and increases the risk of mistakes when switching environments.\n- **Using a Single Configuration File with Conditionals**: This was rejected because it can make the configuration file complex and harder to manage.\n\nBy using this approach, we ensure that our application can be easily configured for different environments, which is crucial for local development, testing, and production deployment.","GenTime":"2024-08-19 23:42:14"}
{"File Name":"verify-service-provider\/0025-we-will-only-release-one-configuration-file.md","Context":"## Context\\nHistorically we have had \"two ways\" of configuring Verify Service Provider:\\n- Using environment variables\\n- Using a YAML file\\nWhen using environment variables the application used the verify-service-provider-env.yml\\nfile from the resources directory (so inside the jar). When using the YAML file you would\\npass the path to a different file as a command line parameter - usually people\\nwould use the example one that's contained in the repo.\\nThere were a couple of reasons for the extra complexity of managing two files, both due to\\nrestrictions with the java buildpack used by cloudfoundry:\\n- It's not possible to specify command line arguments through the java buildpack,\\nso you can't specify a path to your config file\\n- We weren't confident in the way cloudfoundry manages static files, so we didn't want\\nto rely on one.\\nThere was also a philosophical point that 12 factor applications should be configured through\\ntheir environment. This made the \"hide the configuration in the .jar and do everything through\\nenv vars\" way appealing.\\n","Decision":"We will remove the verify-service-provider-env.yml file from src\/main\/resources\\nThe application will default to the verify-service-provider.yml\\nfile that's included in the .zip if no command line arguments are provided.\\nIf the application is started without command line arguments specifying a yml file\\nAND no environment variables have been set, startup should error gracefully and tell\\nthe user that the configuration fields have not been specified for example:\\n\"ERROR - no configuration fields found, either set environment variables or specify\\na configuration file using command line arguments ```server <path\/to\/verify-service-provider.yml>```\"\\nWe will establish the path to verify-service-provider.yml by asking java for the\\npath to the .jar file containing the Application class and looking in the parent\\nfolder.\\n","tokens":228,"id":4448,"text":"## Context\\nHistorically we have had \"two ways\" of configuring Verify Service Provider:\\n- Using environment variables\\n- Using a YAML file\\nWhen using environment variables the application used the verify-service-provider-env.yml\\nfile from the resources directory (so inside the jar). When using the YAML file you would\\npass the path to a different file as a command line parameter - usually people\\nwould use the example one that's contained in the repo.\\nThere were a couple of reasons for the extra complexity of managing two files, both due to\\nrestrictions with the java buildpack used by cloudfoundry:\\n- It's not possible to specify command line arguments through the java buildpack,\\nso you can't specify a path to your config file\\n- We weren't confident in the way cloudfoundry manages static files, so we didn't want\\nto rely on one.\\nThere was also a philosophical point that 12 factor applications should be configured through\\ntheir environment. This made the \"hide the configuration in the .jar and do everything through\\nenv vars\" way appealing.\\n\n\n##Decision\nWe will remove the verify-service-provider-env.yml file from src\/main\/resources\\nThe application will default to the verify-service-provider.yml\\nfile that's included in the .zip if no command line arguments are provided.\\nIf the application is started without command line arguments specifying a yml file\\nAND no environment variables have been set, startup should error gracefully and tell\\nthe user that the configuration fields have not been specified for example:\\n\"ERROR - no configuration fields found, either set environment variables or specify\\na configuration file using command line arguments ```server <path\/to\/verify-service-provider.yml>```\"\\nWe will establish the path to verify-service-provider.yml by asking java for the\\npath to the .jar file containing the Application class and looking in the parent\\nfolder.\\n","MatchedIds":[4436,1254,3389,2409,856],"Predicted":"## Decision\nWe decided to consolidate our configuration approach by exclusively using environment variables for configuring the Verify Service Provider. This aligns with the 12-factor app principles, facilitating smoother deployments and management, especially within the constraints of cloudfoundry's java buildpack. Environment variables will be used to populate configurations, eliminating the need for passing command line parameters or relying on cloudfoundry's handling of static files. This approach also simplifies the configuration management by having a single source of truth, thus reducing complexity and potential inconsistencies.","GenTime":"2024-08-19 23:42:17"}
{"File Name":"latis3\/0001-Scala-programming-language.md","Context":"## Context and Problem Statement\\nThe hybrid object-oriented\/functional language Scala is ideally suited for developing LaTiS.\\n## Decision Drivers <!-- optional -->\\n* merits of functional programming languages\\n* merits of strongly typed languages\\n* availability of developers\\n* consensus of community of scientific programmers\\n* suitability for processing large data sets\\n* concepts involving code correctnes\\n* transformations to data are explicit and can be reasoned about\\n* support for DSLs (Domain Specific Languages)\\n","Decision":"* merits of functional programming languages\\n* merits of strongly typed languages\\n* availability of developers\\n* consensus of community of scientific programmers\\n* suitability for processing large data sets\\n* concepts involving code correctnes\\n* transformations to data are explicit and can be reasoned about\\n* support for DSLs (Domain Specific Languages)\\nChosen option: Scala is the language-of-choice for LaTiS development.  The only decision driver above that is not in alignment with this decision is the fact that Python is the preferred language for most scientific programmers.\\n### Positive Consequences <!-- optional -->\\n* immutable data types in Scala reduce the challenges of introducing parallelism\\n* for an extensible library like LaTiS a functional programming language like Scala provides better abstractions and tools to reason about transformations of large data sets\\n* a strongly typed language like Scala helps to prevent code rot and reduces the chance that defects will slip into the code base\\n* well designed Scala applications are based on sound software engineering principles even though they may take more effort to build than similar Java or Python programs\\n* that being said, Scala programs tend to be easier to maintain and refactor which actually minimizes effort over the life of a project\\n* LaTiS is primarily a framework for transforming large datasets in a performant manner, this meshes nicely with the functional approach that emphasizes creating abstractions that do things to data rather than creating specific recipes for specific use cases\\n* Apache Spark, written in Scala, is the leading framework for processing large amounts of data and is becoming on of the primary framework for implementing machine learning algorithms\\n* Scala supports the creation of DSLs which will allow users of LaTiS to specify processing instructions for datasets without themselves nedding to learn Scala\\n* the full gamut of Java libraies is available to Scala developers\\n* many successful Java libraries eventually get re-written in Scala\\n* Scala encourages software craft-people to follow sound engineering principles instead of just developing something that just gets the job done, this is probably the most important factor in choosing Scala, but the hardest to document\\n### Negative Consequences\\n* Scala developers are not as common as Python developers, especially in the field of scientific computing\\n* Python is clearly the dominant language used by scientific programmers\\n","tokens":97,"id":4716,"text":"## Context and Problem Statement\\nThe hybrid object-oriented\/functional language Scala is ideally suited for developing LaTiS.\\n## Decision Drivers <!-- optional -->\\n* merits of functional programming languages\\n* merits of strongly typed languages\\n* availability of developers\\n* consensus of community of scientific programmers\\n* suitability for processing large data sets\\n* concepts involving code correctnes\\n* transformations to data are explicit and can be reasoned about\\n* support for DSLs (Domain Specific Languages)\\n\n\n##Decision\n* merits of functional programming languages\\n* merits of strongly typed languages\\n* availability of developers\\n* consensus of community of scientific programmers\\n* suitability for processing large data sets\\n* concepts involving code correctnes\\n* transformations to data are explicit and can be reasoned about\\n* support for DSLs (Domain Specific Languages)\\nChosen option: Scala is the language-of-choice for LaTiS development.  The only decision driver above that is not in alignment with this decision is the fact that Python is the preferred language for most scientific programmers.\\n### Positive Consequences <!-- optional -->\\n* immutable data types in Scala reduce the challenges of introducing parallelism\\n* for an extensible library like LaTiS a functional programming language like Scala provides better abstractions and tools to reason about transformations of large data sets\\n* a strongly typed language like Scala helps to prevent code rot and reduces the chance that defects will slip into the code base\\n* well designed Scala applications are based on sound software engineering principles even though they may take more effort to build than similar Java or Python programs\\n* that being said, Scala programs tend to be easier to maintain and refactor which actually minimizes effort over the life of a project\\n* LaTiS is primarily a framework for transforming large datasets in a performant manner, this meshes nicely with the functional approach that emphasizes creating abstractions that do things to data rather than creating specific recipes for specific use cases\\n* Apache Spark, written in Scala, is the leading framework for processing large amounts of data and is becoming on of the primary framework for implementing machine learning algorithms\\n* Scala supports the creation of DSLs which will allow users of LaTiS to specify processing instructions for datasets without themselves nedding to learn Scala\\n* the full gamut of Java libraies is available to Scala developers\\n* many successful Java libraries eventually get re-written in Scala\\n* Scala encourages software craft-people to follow sound engineering principles instead of just developing something that just gets the job done, this is probably the most important factor in choosing Scala, but the hardest to document\\n### Negative Consequences\\n* Scala developers are not as common as Python developers, especially in the field of scientific computing\\n* Python is clearly the dominant language used by scientific programmers\\n","MatchedIds":[4761,1000,472,3153,2290],"Predicted":"## Decision\nWe will use Scala for developing LaTiS. Scala's hybrid object-oriented and functional programming paradigms provide a robust and flexible development environment. Its strong type system ensures code correctness, which is critical for processing large data sets. The language's support for Domain-Specific Languages (DSLs) allows for expressive and explicit data transformations, which can be reasoned about effectively. Additionally, Scala is well-regarded within the scientific programming community, ensuring that we can find experienced developers to support the project.","GenTime":"2024-08-19 23:42:24"}
{"File Name":"cf-k8s-networking\/0013-rename-master-branch.md","Context":"## Context\\nA [proposal](https:\/\/lists.cloudfoundry.org\/g\/cf-dev\/topic\/75070528#9059) was\\nmade on the cf-dev mailing list to rename our `master` branch to `main` to make\\nthe CF community a more welcoming and inclusive environment.\\n","Decision":"We are going to rename our `master` branch to `develop`. `develop` was chosen\\ninstead of `main` by team consensus because it better describes the use of the\\nbranch.\\n","tokens":63,"id":2251,"text":"## Context\\nA [proposal](https:\/\/lists.cloudfoundry.org\/g\/cf-dev\/topic\/75070528#9059) was\\nmade on the cf-dev mailing list to rename our `master` branch to `main` to make\\nthe CF community a more welcoming and inclusive environment.\\n\n\n##Decision\nWe are going to rename our `master` branch to `develop`. `develop` was chosen\\ninstead of `main` by team consensus because it better describes the use of the\\nbranch.\\n","MatchedIds":[3653,2378,2964,3789,2703],"Predicted":"## Decision\n### Rename `master` branch to `main`\n\n1. **Preparation:**\n   - Notify all team members and stakeholders about the upcoming change.\n   - Update all relevant documentation to replace references from `master` to `main`. This includes README files, CI\/CD configurations, deployment scripts, and any other project-specific configurations.\n\n2. **Implementation Steps:**\n   - Create a new `main` branch from the current `master` branch. Execute the following commands:\n     ```\n     git branch -m master main\n     git push -u origin main\n     git push origin --delete master\n     ```\n   - Update the default branch setting in the repository settings to `main`.\n\n3. **Post-Renaming Actions:**\n   - Monitor the repository to ensure all workflows and integrations function correctly with the new branch name.\n   - Communicate the successful transition to the team and stakeholders, ensuring everyone updates their local repository references:\n     ```\n     git fetch origin\n     git checkout main\n     git branch -u origin\/main\n     git remote prune origin\n     ```\n   \n4. **Inclusivity and Accessibility Improvements:**\n   - Continue to seek and implement ways to make the community and project more inclusive and welcoming.\n\nThis change is aimed at fostering a more inclusive environment within the CF community.","GenTime":"2024-08-19 23:42:34"}
{"File Name":"road-registry\/002-track-municipalities.md","Context":"## Context\\nWe need the geometries (boundaries) of a municipality to derive the municipality of road segments that do not have an associated left or right street.\\nWe can't rely on the geometries used in the municipality registry because they are (1) not exposed and (2) they do not match the boundary along which road segments have been split up.\\nWhen the boundaries in the municipality registry change, one can't expect all segments to change as well.\\n","Decision":"We keep track of municipalities as defined in the legacy road registry database because those are the ones along which road segments have been split up.\\nWe use `ImportedMunicipality` as the main event to track this. It contains all the data we need for now. This event is assembled while extracting events and imported during deployment.\\n","tokens":96,"id":4930,"text":"## Context\\nWe need the geometries (boundaries) of a municipality to derive the municipality of road segments that do not have an associated left or right street.\\nWe can't rely on the geometries used in the municipality registry because they are (1) not exposed and (2) they do not match the boundary along which road segments have been split up.\\nWhen the boundaries in the municipality registry change, one can't expect all segments to change as well.\\n\n\n##Decision\nWe keep track of municipalities as defined in the legacy road registry database because those are the ones along which road segments have been split up.\\nWe use `ImportedMunicipality` as the main event to track this. It contains all the data we need for now. This event is assembled while extracting events and imported during deployment.\\n","MatchedIds":[4931,1122,4276,3968,3286],"Predicted":"## Decision\nTo resolve the issue of deriving the municipality for road segments without an associated left or right street, we will integrate a custom boundary dataset specifically designed for our needs. This custom dataset will:\n\n1. Be derived from the most accurate and up-to-date external GIS boundary data sources, ensuring it aligns with how our road segments are split.\n2. Be maintained and versioned internally to provide consistency and control over boundary changes.\n\nBy doing this, we will avoid the issues related to the municipality registry boundaries not being exposed and not matching our segmentation. Additionally, this approach will give us the ability to manage and update the boundary data independently, ensuring that road segments align with the appropriate municipality without dependency on registry changes.","GenTime":"2024-08-19 23:42:38"}
{"File Name":"external-service-operator\/0004-ports-are-never-stored-as-named-ports.md","Context":"## Context\\nActually it does not make sense to store ports as strings, as those Backends are not in the cluster so they don't have names and can only be referenced by a port number. Nevertheless the Kuberenetes API makes it possible to store ports as string which will not be used at the endpoints generated by the External Service Operator.\\n","Decision":"The change that we're proposing or have agreed to implement.\\n","tokens":70,"id":1735,"text":"## Context\\nActually it does not make sense to store ports as strings, as those Backends are not in the cluster so they don't have names and can only be referenced by a port number. Nevertheless the Kuberenetes API makes it possible to store ports as string which will not be used at the endpoints generated by the External Service Operator.\\n\n\n##Decision\nThe change that we're proposing or have agreed to implement.\\n","MatchedIds":[1912,3440,2236,631,3905],"Predicted":"## Decision\nStore ports strictly as numbers in our configurations. This aligns with the practical use of ports as numerical identifiers outside the cluster and ensures consistency, eliminating potential issues from string representations of ports that are not usable by the External Service Operator.","GenTime":"2024-08-19 23:42:41"}
{"File Name":"fxa\/0005-minimize-password-entry.md","Context":"## Context and Problem Statement\\nSee [Github Issue 1371][#gh-issue-1371]. The FxA authorization flow sometimes asks already authenticated users to enter their password, sometimes it does not. Password entry, especially on mobile devices, is difficult and a source of user dropoff. Minimizing the need for a password in an authorization flow should increase flow completion rates.\\nWhen and where passwords are asked for has been a repeated source of confusion amongst both users and Firefox Accounts developers. If a user is signed into Sync, passwords are only _supposed_ to be required for authorization flows for RPs that require encryption keys. However, there is a bug in the state management logic that forces users to enter their password more often than expected.\\nTechnically, we _must always_ ask the user to enter their password any time encryption keys are needed by an RP, e.g., Sync, Lockwise, and Send. For RPs that do not require encryption keys, e.g., Monitor and AMO, there is no technical reason why authenticated users must enter their password again, the existing sessionToken is capable of requesting new OAuth tokens.\\n## Decision Drivers\\n- User happiness via fewer keystrokes, less confusion\\n- Improved signin rates\\n","Decision":"- User happiness via fewer keystrokes, less confusion\\n- Improved signin rates\\nChosen option: \"option 2\", because it minimizes the number of places the user must enter their password.\\n### Positive Consequences\\n- User will need to type their password in fewer places.\\n- Signin completion rates should increase.\\n### Negative Consequences\\n- There may be user confusion around what it means to sign out.\\n### [option 1] Keep the existing flow\\nIf a user signs in to Sync first and is not signing into an OAuth\\nRP that requires encryption keys, then no password is required.\\nIf a user does not sign into Sync and instead signs into an\\nOAuth RP, e.g., Send, and then visits a 2nd OAuth RP that does not\\nrequire encryption keys, e.g., Monitor, then they must enter their password.\\n**example 1** User performs the initial authorization flow for an OAuth RP, e.g., Send, and then visits a 2nd OAuth RP that does not require encryption keys, e.g., Monitor, then _ask_ for the password.\\n**example 2** User performs the initial authorization flow for Sync, then a subsequent authorization flow for an OAuth RP that does not require encryption keys, e.g., Monitor, _do not_ ask for the password.\\n**example 3** User performs the initial authorization flow for an OAuth RP, e.g., Monitor, and then a subsequent authorization flow for an OAuth RP that _does_ require encryption keys, e.g., Send, then _ask_ for the password.\\n**example 4** User performs the initial authorization flow for Sync, then a subsequent authorization flow for an OAuth RP that _does_ require encryption keys, e.g., Send, then _ask_ for the password.\\n**example 5** User performs the initial authorization flow for an OAuth RP that does not require keys, e.g., Monitor, and then performs an authorization flow for Sync, then _ask_ for the password.\\n**example 6** User performs the initial authorization flow for an OAuth RP that does does require keys, e.g., Send, and then performs an authorization flow for Sync, then _ask_ for the password.\\n- Good, because we already have it and no effort is required to keep it.\\n- Bad because there is no technical reason why we cannot re-use existing sessionTokens created when signing into OAuth RPs to generate OAuth tokens for other non-key requesting OAuth RPs.\\n- Bad, because users need to enter their password more than they need to.\\n- Bad, because due to a bug in the code, users that are currently signed into Sync are sometimes asked for their password to sign into services such as Monitor that do not require keys.\\n### [option 2] Only ask authenticated users for a password if encryption keys are required\\n**example 1** User performs the initial authorization flow for an OAuth RP, e.g., Send, and then visits a 2nd OAuth RP that does not require encryption keys, e.g., Monitor, then _do not_ ask for the password.\\n**example 2** User performs the initial authorization flow for Sync, then a subsequent authorization flow for an OAuth RP that does not require encryption keys, e.g., Monitor, _do not_ ask for the password.\\n**example 3** User performs the initial authorization flow for an OAuth RP, e.g., Monitor, and then a subsequent authorization flow for an OAuth RP that _does_ require encryption keys, e.g., Send, then _ask_ for the password.\\n**example 4** User performs the initial authorization flow for Sync, then a subsequent authorization flow for an OAuth RP that _does_ require encryption keys, e.g., Send, then _ask_ for the password.\\n**example 5** User performs the initial authorization flow for an OAuth RP that does not require keys, e.g., Monitor, and then performs an authorization flow for Sync, then _ask_ for the password.\\n**example 6** User performs the initial authorization flow for an OAuth RP that does does require keys, e.g., Send, and then performs an authorization flow for Sync, then _ask_ for the password.\\n- Good, because case 1 _does not_ ask for a password whereas it _does_ with option 1.\\n- Bad, because there is potential for user confusion about expected behavior when destroying the sessionToken - should destroying the sessionToken sign the user out of the RP too? See [Github issue 640][#gh-issue-640].\\n- Support for [RP initiated logout][#gh-issue-1979] will largely mitigate this.\\n","tokens":254,"id":377,"text":"## Context and Problem Statement\\nSee [Github Issue 1371][#gh-issue-1371]. The FxA authorization flow sometimes asks already authenticated users to enter their password, sometimes it does not. Password entry, especially on mobile devices, is difficult and a source of user dropoff. Minimizing the need for a password in an authorization flow should increase flow completion rates.\\nWhen and where passwords are asked for has been a repeated source of confusion amongst both users and Firefox Accounts developers. If a user is signed into Sync, passwords are only _supposed_ to be required for authorization flows for RPs that require encryption keys. However, there is a bug in the state management logic that forces users to enter their password more often than expected.\\nTechnically, we _must always_ ask the user to enter their password any time encryption keys are needed by an RP, e.g., Sync, Lockwise, and Send. For RPs that do not require encryption keys, e.g., Monitor and AMO, there is no technical reason why authenticated users must enter their password again, the existing sessionToken is capable of requesting new OAuth tokens.\\n## Decision Drivers\\n- User happiness via fewer keystrokes, less confusion\\n- Improved signin rates\\n\n\n##Decision\n- User happiness via fewer keystrokes, less confusion\\n- Improved signin rates\\nChosen option: \"option 2\", because it minimizes the number of places the user must enter their password.\\n### Positive Consequences\\n- User will need to type their password in fewer places.\\n- Signin completion rates should increase.\\n### Negative Consequences\\n- There may be user confusion around what it means to sign out.\\n### [option 1] Keep the existing flow\\nIf a user signs in to Sync first and is not signing into an OAuth\\nRP that requires encryption keys, then no password is required.\\nIf a user does not sign into Sync and instead signs into an\\nOAuth RP, e.g., Send, and then visits a 2nd OAuth RP that does not\\nrequire encryption keys, e.g., Monitor, then they must enter their password.\\n**example 1** User performs the initial authorization flow for an OAuth RP, e.g., Send, and then visits a 2nd OAuth RP that does not require encryption keys, e.g., Monitor, then _ask_ for the password.\\n**example 2** User performs the initial authorization flow for Sync, then a subsequent authorization flow for an OAuth RP that does not require encryption keys, e.g., Monitor, _do not_ ask for the password.\\n**example 3** User performs the initial authorization flow for an OAuth RP, e.g., Monitor, and then a subsequent authorization flow for an OAuth RP that _does_ require encryption keys, e.g., Send, then _ask_ for the password.\\n**example 4** User performs the initial authorization flow for Sync, then a subsequent authorization flow for an OAuth RP that _does_ require encryption keys, e.g., Send, then _ask_ for the password.\\n**example 5** User performs the initial authorization flow for an OAuth RP that does not require keys, e.g., Monitor, and then performs an authorization flow for Sync, then _ask_ for the password.\\n**example 6** User performs the initial authorization flow for an OAuth RP that does does require keys, e.g., Send, and then performs an authorization flow for Sync, then _ask_ for the password.\\n- Good, because we already have it and no effort is required to keep it.\\n- Bad because there is no technical reason why we cannot re-use existing sessionTokens created when signing into OAuth RPs to generate OAuth tokens for other non-key requesting OAuth RPs.\\n- Bad, because users need to enter their password more than they need to.\\n- Bad, because due to a bug in the code, users that are currently signed into Sync are sometimes asked for their password to sign into services such as Monitor that do not require keys.\\n### [option 2] Only ask authenticated users for a password if encryption keys are required\\n**example 1** User performs the initial authorization flow for an OAuth RP, e.g., Send, and then visits a 2nd OAuth RP that does not require encryption keys, e.g., Monitor, then _do not_ ask for the password.\\n**example 2** User performs the initial authorization flow for Sync, then a subsequent authorization flow for an OAuth RP that does not require encryption keys, e.g., Monitor, _do not_ ask for the password.\\n**example 3** User performs the initial authorization flow for an OAuth RP, e.g., Monitor, and then a subsequent authorization flow for an OAuth RP that _does_ require encryption keys, e.g., Send, then _ask_ for the password.\\n**example 4** User performs the initial authorization flow for Sync, then a subsequent authorization flow for an OAuth RP that _does_ require encryption keys, e.g., Send, then _ask_ for the password.\\n**example 5** User performs the initial authorization flow for an OAuth RP that does not require keys, e.g., Monitor, and then performs an authorization flow for Sync, then _ask_ for the password.\\n**example 6** User performs the initial authorization flow for an OAuth RP that does does require keys, e.g., Send, and then performs an authorization flow for Sync, then _ask_ for the password.\\n- Good, because case 1 _does not_ ask for a password whereas it _does_ with option 1.\\n- Bad, because there is potential for user confusion about expected behavior when destroying the sessionToken - should destroying the sessionToken sign the user out of the RP too? See [Github issue 640][#gh-issue-640].\\n- Support for [RP initiated logout][#gh-issue-1979] will largely mitigate this.\\n","MatchedIds":[1754,2405,560,1252,5211],"Predicted":"## Decision\nWe will modify the FxA authorization flow to ensure that authenticated users are only prompted for their password when encryption keys are required by the requesting party (RP). Specifically, we will:\n\n1. **Fix the State Management Bug:** Address the bug in the state management logic that erroneously requires passwords more often than necessary.\n2. **Leverage Session Tokens:** For RPs that do not require encryption keys (e.g., Monitor and AMO), utilize the existing sessionToken to request new OAuth tokens without additional password prompts.\n\n### Positive Consequences\n- **Increased User Happiness:** Users will experience fewer interruptions and less confusion, especially when using services that do not require encryption keys.\n- **Improved Signin Rates:** By minimizing the need for password entry, particularly on mobile devices, we expect to see higher flow completion rates.\n\n### Negative Consequences\n- **Initial Development Effort:** Fixing the state management bug and ensuring the correct logic for password prompts will require developer resources.\n- **Security Considerations:** Care must be taken to ensure that sessionTokens are handled securely to avoid potential misuse.\n\n### Implementation Plan\n1. **Bug Fixing:** Assign developers to investigate and fix the bug causing unnecessary password prompts.\n2. **Session Management:** Update the session management logic to ensure that passwords are only required when encryption keys are needed.\n3. **Testing:** Conduct thorough testing to ensure the changes correctly address the problem without introducing new issues.\n4. **Monitoring and Feedback:** Once deployed, monitor user feedback and signin rates to verify improvements.\n\nBy implementing these changes, we aim to streamline the authorization flow for Firefox Accounts, enhancing the user experience while maintaining necessary security measures.","GenTime":"2024-08-19 23:42:48"}
{"File Name":"unfinished-design-system\/006-yarn-workspaces.md","Context":"* [Context](#context)\\n* [Decision](#decision)\\n* [Status](#status)\\n* [Consequences](#consequences)\\n* [Experience Report](#experience-report)\\n* [More reading](#more-reading)\\n* [Updates](#updates)\\n","Decision":"* [Status](#status)\\n* [Consequences](#consequences)\\n* [Experience Report](#experience-report)\\n* [More reading](#more-reading)\\n* [Updates](#updates)\\nWe've decided to use [Yarn Workspaces](https:\/\/classic.yarnpkg.com\/en\/docs\/workspaces\/) integrated with Lerna to solve those problems. It has some promising features that can improve our experience and create more sustainable development environments.\\n","tokens":63,"id":4549,"text":"* [Context](#context)\\n* [Decision](#decision)\\n* [Status](#status)\\n* [Consequences](#consequences)\\n* [Experience Report](#experience-report)\\n* [More reading](#more-reading)\\n* [Updates](#updates)\\n\n\n##Decision\n* [Status](#status)\\n* [Consequences](#consequences)\\n* [Experience Report](#experience-report)\\n* [More reading](#more-reading)\\n* [Updates](#updates)\\nWe've decided to use [Yarn Workspaces](https:\/\/classic.yarnpkg.com\/en\/docs\/workspaces\/) integrated with Lerna to solve those problems. It has some promising features that can improve our experience and create more sustainable development environments.\\n","MatchedIds":[4547,4910,4918,4544,4550],"Predicted":"## Decision\n* [Status](#status)\n* [Consequences](#consequences)\n* [Experience Report](#experience-report)\n* [More reading](#more-reading)\n* [Updates](#updates)\n\nWe have decided to adopt a GraphQL API for our new application. GraphQL allows clients to request exactly the data they need, avoiding over-fetching or under-fetching data. This flexibility is beneficial for optimizing our application's performance and providing a better user experience.\n\n### Status\nAccepted\n\n### Consequences\n- **Positive**:\n  - Client applications can request only the data they need, which can reduce the payload size and network usage.\n  - Simplifies the client-side codebase by consolidating multiple REST calls into a single query.\n  - Facilitates better documentation and introspection due to the schema-centric design of GraphQL.\n  \n- **Negative**:\n  - Steeper learning curve for team members who are unfamiliar with GraphQL.\n  - Increased complexity in setting up and maintaining the GraphQL server.\n  - Potential over-fetching on the server side, which needs to be managed carefully to avoid performance bottlenecks.\n\n### Experience Report\nTeams that have worked with GraphQL in the past reported significant improvements in the efficiency of data fetching and reduced time spent on creating and maintaining individual REST endpoints. However, initial setup and learning the new paradigm were time-consuming.\n\n### More reading\n- [Official GraphQL Documentation](https:\/\/graphql.org\/learn\/)\n- [Apollo GraphQL](https:\/\/www.apollographql.com\/)\n- [GraphQL Best Practices](https:\/\/graphql.org\/learn\/best-practices\/)\n\n### Updates\nAny updates or changes to this decision will be documented here as we gather more experience and feedback from the implementation process.","GenTime":"2024-08-19 23:42:55"}
{"File Name":"beis-report-official-development-assistance\/0030-run-data-migrations-manually.md","Context":"## Context\\nThe Data Migrate gem has caused us a number of issues in the past, it runs\\nsilently as part of a deploy, and this can result in surprising errors\\nduring a deploy. We've also had issues with the gem itself - most recently\\na bug in a new version causing strange errors in deployment.\\n","Decision":"With this in mind, we've decided to drop the use of the Data Migrate gem,\\nand instead run any data migrations manually. We have easy access to the\\nconsole via GOV.UK PaaS, so this is relatively painless.\\nWe have added a generator which creates an empty Ruby file with a timestamp and the name of the migration as the filename. Once the code is\\ndeployed, we run the migration on the production server.\\nAs the code is plain old Ruby, it's also easier for us to write tests, if\\nwe think it's worth doing (for example, the migrations code is suitably\\ncomplex).\\n","tokens":69,"id":2396,"text":"## Context\\nThe Data Migrate gem has caused us a number of issues in the past, it runs\\nsilently as part of a deploy, and this can result in surprising errors\\nduring a deploy. We've also had issues with the gem itself - most recently\\na bug in a new version causing strange errors in deployment.\\n\n\n##Decision\nWith this in mind, we've decided to drop the use of the Data Migrate gem,\\nand instead run any data migrations manually. We have easy access to the\\nconsole via GOV.UK PaaS, so this is relatively painless.\\nWe have added a generator which creates an empty Ruby file with a timestamp and the name of the migration as the filename. Once the code is\\ndeployed, we run the migration on the production server.\\nAs the code is plain old Ruby, it's also easier for us to write tests, if\\nwe think it's worth doing (for example, the migrations code is suitably\\ncomplex).\\n","MatchedIds":[317,1666,3841,351,1739],"Predicted":"## Decision\nWe have decided to remove the Data Migrate gem from our deployment pipeline. Instead, we will handle database migrations using a dedicated migration step that runs separately from the application deploy process. This will ensure better visibility into potential issues and allow for smoother rollbacks if necessary.\n\nTo replace the Data Migrate gem, we will take the following steps:\n1. **Manual Trigger:** Introduce a manual step in our CI\/CD pipeline specifically for running database migrations. This step will be clearly logged and monitored.\n2. **Pre-Deployment Script:** Develop pre-deployment scripts to check for pending migrations and alert the team if any exist.\n3. **Versioning Strategy:** Implement a strict versioning strategy to ensure that all necessary migrations are applied in a controlled manner before the application is updated.\n4. **Fallback Mechanism:** Keep a rollback plan in place for migrations, such as down scripts, to quickly revert changes if an issue is detected post-deployment.\n\nBy decoupling database migrations from the main deploy process and improving our visibility into these operations, we aim to reduce the number of surprising errors and improve our overall deployment reliability.","GenTime":"2024-08-19 23:43:09"}
{"File Name":"nozama\/adr-001-simulator-webserver.md","Context":"## Context\\nSince we decided to design the simulator as a separated component we didn't thought how it would communicate with the WebApp. So, the simulator is a simple java program that can't do anything to communicate with Nozama since it is a web application and has a different ecosystem (spring framework).\\n","Decision":"As of now we decided to create a simple webserver to expose simulator to our main WebApp. As it main functionality is to just pass some data when required and send notifications to Nozama's backend when some task is done.\\n","tokens":63,"id":97,"text":"## Context\\nSince we decided to design the simulator as a separated component we didn't thought how it would communicate with the WebApp. So, the simulator is a simple java program that can't do anything to communicate with Nozama since it is a web application and has a different ecosystem (spring framework).\\n\n\n##Decision\nAs of now we decided to create a simple webserver to expose simulator to our main WebApp. As it main functionality is to just pass some data when required and send notifications to Nozama's backend when some task is done.\\n","MatchedIds":[2634,1907,428,3982,578],"Predicted":"## Decision\nWe will introduce a REST API in the WebApp (Nozama) to facilitate communication with the simulator. The simulator, being a Java program, can use HTTP clients to interact with these endpoints. This approach will decouple the simulator from the WebApp\u2019s internal implementation details and leverage the Spring framework's capabilities to expose necessary functionalities. Additionally, this allows for future extensions and potential integration with other tools or applications.","GenTime":"2024-08-19 23:43:12"}
{"File Name":"godspeed-you-blocked-developer\/001. TypeScript, React, Redux, and Webpack.md","Context":"## Context\\nI was once a web developer when jQuery and CSS 2 were the new hotness. I did a lot of web 1.0, single-server work. Then I spent many years building iOS apps. These days I'm interested to be more of a generalist.\\nIn the fall of 2017, I learned some TypeScript and thought it was great. JavaScript is JavaScript, and I like strong types, and TypeScript makes that happen, so it was an instant favorite.\\nI decided to learn some more web technologies. For browser apps that means React and Redux, which is commonly used with Node, NPM, and Webpack. Learning the common stuff is a good way to get my bearings in the field.\\nWhen the idea for this project came to mind, it was a good chance to learn that web stack and catch up my knowledge.\\n","Decision":"I decided to build this site in TypeScript, with React and Redux, on Node, NPM, and Webpack.\\n","tokens":177,"id":4940,"text":"## Context\\nI was once a web developer when jQuery and CSS 2 were the new hotness. I did a lot of web 1.0, single-server work. Then I spent many years building iOS apps. These days I'm interested to be more of a generalist.\\nIn the fall of 2017, I learned some TypeScript and thought it was great. JavaScript is JavaScript, and I like strong types, and TypeScript makes that happen, so it was an instant favorite.\\nI decided to learn some more web technologies. For browser apps that means React and Redux, which is commonly used with Node, NPM, and Webpack. Learning the common stuff is a good way to get my bearings in the field.\\nWhen the idea for this project came to mind, it was a good chance to learn that web stack and catch up my knowledge.\\n\n\n##Decision\nI decided to build this site in TypeScript, with React and Redux, on Node, NPM, and Webpack.\\n","MatchedIds":[3982,3721,1576,1931,1599],"Predicted":"## Decision\nTo explore and get up-to-date with modern web development, I will build this project using React, Redux, Node.js, NPM, and Webpack. This will allow me to leverage my knowledge of TypeScript and learn widely-adopted technologies in the field, thereby broadening my skill set and keeping my knowledge current with industry standards.","GenTime":"2024-08-19 23:43:15"}
{"File Name":"skypy\/adr-02.md","Context":"## Context\\nWe need to decide on a unit convention as to whether units include the factor \/h or not (for instance Mpc or Mpc\/h as a unit of distance). For further discussion see e.g. 10.1017\/pasa.2013.31\\n## Decision Drivers\\n- Flexibility: Mpc\/h allows results to be easily propagated across the 'unknown' value of h (0.68 or 0.74 or something else).\\n- Consistency \/ least surprise: the default for astropy is Mpc\\n","Decision":"- Flexibility: Mpc\/h allows results to be easily propagated across the 'unknown' value of h (0.68 or 0.74 or something else).\\n- Consistency \/ least surprise: the default for astropy is Mpc\\nAfter [discussion](https:\/\/github.com\/skypyproject\/skypy\/issues\/23) and offline, Mpc has been chosen to ensure the closest integration and least surprise for astropy.\\n","tokens":112,"id":5148,"text":"## Context\\nWe need to decide on a unit convention as to whether units include the factor \/h or not (for instance Mpc or Mpc\/h as a unit of distance). For further discussion see e.g. 10.1017\/pasa.2013.31\\n## Decision Drivers\\n- Flexibility: Mpc\/h allows results to be easily propagated across the 'unknown' value of h (0.68 or 0.74 or something else).\\n- Consistency \/ least surprise: the default for astropy is Mpc\\n\n\n##Decision\n- Flexibility: Mpc\/h allows results to be easily propagated across the 'unknown' value of h (0.68 or 0.74 or something else).\\n- Consistency \/ least surprise: the default for astropy is Mpc\\nAfter [discussion](https:\/\/github.com\/skypyproject\/skypy\/issues\/23) and offline, Mpc has been chosen to ensure the closest integration and least surprise for astropy.\\n","MatchedIds":[801,5147,5178,1193,4804],"Predicted":"## Decision\n\nAfter considering the decision drivers, the unit convention chosen is to **include the factor \/h**, thereby using units like Mpc\/h for distance measurements.\n\n### Reasoning:\n- **Flexibility**: Using Mpc\/h units allows for greater flexibility when dealing with cosmological calculations where the Hubble constant (h) is not precisely known or may vary based on different observational datasets. This convention ensures results can be easily adjusted to the correct value of h without requiring additional conversions.\n  \n- **Consistency \/ Least Surprise**: While the default unit in astropy is Mpc, using Mpc\/h is more aligned with the flexibility needed in our domain. It is important to clearly document the use of Mpc\/h units in our project to mitigate any potential confusion for new users who may expect the default astropy behavior.\n\nOverall, the choice of Mpc\/h helps maintain flexibility without unduly compromising on consistency, as long as appropriate documentation and communication regarding this decision are provided to the users.","GenTime":"2024-08-19 23:43:23"}
{"File Name":"verify-self-service\/0003-use-pundit-for-rbac.md","Context":"## Context\\nAs with any modern web system there is a need to secure the application with\\nstrong authentication and Role Based Access Control (RBAC). RBAC allows us to\\ncreate permissions, apply them to roles and associates roles with users. From\\nthis we can restrict what users can do and see within the application based on\\ntheir role. In addition to this we also had to investigate where to store role\\ninformation and define teams to support our use of RBAC.\\n","Decision":"After some investigation and discussion into various ways of implementing RBAC\\nwithin Ruby we have decided to implement the Pundit Ruby gem within the Verify\\nSelf Service application. This gem provides us with a way of creating policies\\nwhich apply to the app as a whole and to individual classes within the\\napplication.\\nWe have chosen pundit for a number of reasons. The first is that it seems\\nrelatively easy to implement within our application. A proof of concept was\\ncreated in a few hours which restricted creating new components within the\\napplication to those who hold the `gds` or `dev` roles. Policies and permissions\\nare defined in code as plain Ruby objects which are easy to understand and\\nsimple to implement. This ease of implementation and an appearance of providing\\nthe fuctionality we are seeking we believe makes pundit a good candidate for\\nimplementing RBAC within our application.\\nIn addition to this we are aware of prior-art use within GDS on the GOV.UK\\nproject. Finally we have found a lot of documentation and articles about how to\\nimplement and use pundit.\\nRole information we have decided to store in a custom field within our cognito\\ninstance. This will take the form of a comma seperated list which will be split\\ninto an array of roles for use within the application. This method allows us to\\nkeep all use information together within AWS Cognito and means that the only way\\nrole information can be changed is via SDK calls to AWS Cognito.\\nFinally we understand there is a need to define teams within our application. We\\nhave decided that a team is analogous to an AWS Cognito group. This allows a\\nuser to be part of one more groups and for a group to hold one or more users. We\\ncan easily query group membership using the AWS Cognito SDK and check for a\\nusers membership to a specific group.\\n","tokens":97,"id":3950,"text":"## Context\\nAs with any modern web system there is a need to secure the application with\\nstrong authentication and Role Based Access Control (RBAC). RBAC allows us to\\ncreate permissions, apply them to roles and associates roles with users. From\\nthis we can restrict what users can do and see within the application based on\\ntheir role. In addition to this we also had to investigate where to store role\\ninformation and define teams to support our use of RBAC.\\n\n\n##Decision\nAfter some investigation and discussion into various ways of implementing RBAC\\nwithin Ruby we have decided to implement the Pundit Ruby gem within the Verify\\nSelf Service application. This gem provides us with a way of creating policies\\nwhich apply to the app as a whole and to individual classes within the\\napplication.\\nWe have chosen pundit for a number of reasons. The first is that it seems\\nrelatively easy to implement within our application. A proof of concept was\\ncreated in a few hours which restricted creating new components within the\\napplication to those who hold the `gds` or `dev` roles. Policies and permissions\\nare defined in code as plain Ruby objects which are easy to understand and\\nsimple to implement. This ease of implementation and an appearance of providing\\nthe fuctionality we are seeking we believe makes pundit a good candidate for\\nimplementing RBAC within our application.\\nIn addition to this we are aware of prior-art use within GDS on the GOV.UK\\nproject. Finally we have found a lot of documentation and articles about how to\\nimplement and use pundit.\\nRole information we have decided to store in a custom field within our cognito\\ninstance. This will take the form of a comma seperated list which will be split\\ninto an array of roles for use within the application. This method allows us to\\nkeep all use information together within AWS Cognito and means that the only way\\nrole information can be changed is via SDK calls to AWS Cognito.\\nFinally we understand there is a need to define teams within our application. We\\nhave decided that a team is analogous to an AWS Cognito group. This allows a\\nuser to be part of one more groups and for a group to hold one or more users. We\\ncan easily query group membership using the AWS Cognito SDK and check for a\\nusers membership to a specific group.\\n","MatchedIds":[2397,4021,2261,4699,4408],"Predicted":"## Decision\nWe will implement Role Based Access Control (RBAC) by using the \"Devise\" gem for authentication and \"Pundit\" gem for authorization. Role information will be stored in a dedicated `roles` table, linked to the users via a many-to-many relationship through a join table `user_roles`. Teams will be defined in a separate `teams` table and each user can be associated with a team via a foreign key relationship. This allows for a clean and maintainable way to manage user permissions and team assignments, facilitating scalable RBAC support in our application.","GenTime":"2024-08-19 23:43:26"}
{"File Name":"TANF-app\/010-assign-superuser.md","Context":"## Context\\nUsually in Django we would assign the first superuser through the CLI, but because this will not\\nbe available in production, we will need another method.\\n","Decision":"The Django Admin provides the easiest way to assign superuser status, so for most users that will\\nbe the method employed. However, we still need a method for creating the first superuser, so that\\nuser will be able to assign others as needed. We will assign this with a data migration using a\\nusername defined in environment variables.\\n","tokens":35,"id":3792,"text":"## Context\\nUsually in Django we would assign the first superuser through the CLI, but because this will not\\nbe available in production, we will need another method.\\n\n\n##Decision\nThe Django Admin provides the easiest way to assign superuser status, so for most users that will\\nbe the method employed. However, we still need a method for creating the first superuser, so that\\nuser will be able to assign others as needed. We will assign this with a data migration using a\\nusername defined in environment variables.\\n","MatchedIds":[2488,5024,2041,1239,3950],"Predicted":"## Decision\nWe will use Django's database migrations and custom management commands to create the first superuser in a production environment. Specifically, we will create a custom Django management command that checks if a superuser exists, and if not, creates one with predefined credentials from environment variables. This ensures that we do not expose sensitive information in our codebase and can securely manage superuser creation in production. The steps are as follows:\n\n1. **Create a custom management command**: We'll create a new Django management command that will handle checking for and creating the superuser.\n   \n2. **Read credentials from environment variables**: This command will read the superuser credentials (username, email, password) from environment variables. \n\n3. **Check if a superuser exists**: The command will query the User model to check if a superuser already exists.\n\n4. **Create the superuser if none exists**: If no superuser is found, the command will create one using the credentials extracted from the environment variables.\n\nThis approach ensures secure and automated creation of the superuser, avoiding manual intervention and potential security risks of hardcoding credentials in the codebase. \n\nHere is a sample implementation of the custom management command:\n\n```python\nfrom django.core.management.base import BaseCommand\nfrom django.contrib.auth.models import User\nimport os\n\nclass Command(BaseCommand):\n    help = 'Create a superuser if none exists'\n\n    def handle(self, *args, **kwargs):\n        if not User.objects.filter(is_superuser=True).exists():\n            username = os.getenv('DJANGO_SUPERUSER_USERNAME')\n            email = os.getenv('DJANGO_SUPERUSER_EMAIL')\n            password = os.getenv('DJANGO_SUPERUSER_PASSWORD')\n            \n            if username and email and password:\n                User.objects.create_superuser(username=username, email=email, password=password)\n                self.stdout.write(self.style.SUCCESS(f'Superuser {username} created successfully.'))\n            else:\n                self.stdout.write(self.style.ERROR('Superuser credentials not provided in environment variables.'))\n        else:\n            self.stdout.write(self.style.NOTICE('Superuser already exists, no action taken.'))\n```\n\nThis management command can be added to the deployment scripts to ensure the superuser is created when the application is deployed to production.","GenTime":"2024-08-19 23:43:33"}
{"File Name":"govuk-aws\/0032-transfer-artefact-binary.md","Context":"## Context\\nWe create binary artifacts for applications when we execute the **Deploy_App**\\nJenkins job. The binary file (artifact) gets stored in an AWS S3 bucket\\n(For example **govuk-integration-artefact**). We need the binary file to deploy\\nthe application to the next environment. For example, we will use the binary\\nfile from **Integration** to deploy the application in the **Staging**\\nenvironment.\\n","Decision":"We decided to use a mechanism that is located outside the environments involved\\nand pass the contents to the correct locations when the package becomes\\navailable.\\n![Transfer Mechanism](.\/0032-transfer-artefact-binary-img01.png?raw=true \"Transfer Mechanism\")\\n### Example\\nWe will use the **Integration** and **Staging** environments as examples.\\n1. We execute the **Deploy_Apps -> router** Jenkins job in the Integration environment.\\n2. A binary file is created in the **govuk-integration-artefact** AWS S3 bucket.\\n3. We have an AWS SNS Topic called **govuk-integration-artefact**.\\n4. We have an AWS S3 **govuk-integration-artefact** bucket event notification. This event sends a notification to the AWS SNS Topic when an **ObjectCreate** event is triggered.\\n5. The **Staging** environment has an AWS S3 bucket called **govuk-staging-artefact**.\\n6. We have an AWS SNS Subscription that subscribes to the **govuk-integration-artefact** AWS SNS Topic. This subscription will enable Staging to know when a new object gets created in the **govuk-integration-artefact** AWS S3 bucket.\\n7. We have an AWS Lambda function called **govuk-staging-artefact**. This function copies objects from the destination bucket to the target bucket. The trigger for the function is the AWS SNS Subscription.\\n","tokens":95,"id":4057,"text":"## Context\\nWe create binary artifacts for applications when we execute the **Deploy_App**\\nJenkins job. The binary file (artifact) gets stored in an AWS S3 bucket\\n(For example **govuk-integration-artefact**). We need the binary file to deploy\\nthe application to the next environment. For example, we will use the binary\\nfile from **Integration** to deploy the application in the **Staging**\\nenvironment.\\n\n\n##Decision\nWe decided to use a mechanism that is located outside the environments involved\\nand pass the contents to the correct locations when the package becomes\\navailable.\\n![Transfer Mechanism](.\/0032-transfer-artefact-binary-img01.png?raw=true \"Transfer Mechanism\")\\n### Example\\nWe will use the **Integration** and **Staging** environments as examples.\\n1. We execute the **Deploy_Apps -> router** Jenkins job in the Integration environment.\\n2. A binary file is created in the **govuk-integration-artefact** AWS S3 bucket.\\n3. We have an AWS SNS Topic called **govuk-integration-artefact**.\\n4. We have an AWS S3 **govuk-integration-artefact** bucket event notification. This event sends a notification to the AWS SNS Topic when an **ObjectCreate** event is triggered.\\n5. The **Staging** environment has an AWS S3 bucket called **govuk-staging-artefact**.\\n6. We have an AWS SNS Subscription that subscribes to the **govuk-integration-artefact** AWS SNS Topic. This subscription will enable Staging to know when a new object gets created in the **govuk-integration-artefact** AWS S3 bucket.\\n7. We have an AWS Lambda function called **govuk-staging-artefact**. This function copies objects from the destination bucket to the target bucket. The trigger for the function is the AWS SNS Subscription.\\n","MatchedIds":[4358,319,3722,351,1739],"Predicted":"## Decision\nWe will standardize our pipeline so that the binary artifact created during the **Deploy_App** Jenkins job is tagged with a unique version identifier and stored in an S3 bucket (e.g., **govuk-integration-artefact**). \n\n### Using Version Identifiers\nEach artifact will be given a version identifier that aligns with our versioning strategy, making it easy to identify and retrieve the correct artifact for deployment to subsequent environments like **Staging**.\n\n### Pipeline Automation\nThe deployment pipelines for subsequent environments (such as **Staging**, **Production**, etc.) will be configured to retrieve the necessary binary artifact from the specified S3 bucket using the version identifier. This ensures that the same version of the artifact that passed through **Integration** testing is deployed across all environments.\n\n### Security and Access Management \nTo ensure secure access, IAM roles and policies will be configured to allow the necessary Jenkins jobs to read from and write to the S3 bucket, while restricting access where not needed.\n\n### Retries and Error Handling\nImplementing error handling and retry mechanisms in the Jenkins pipeline will ensure that transient errors during artifact retrieval do not fail the entire deployment process.\n\n### Logging & Monitoring\nLogging and monitoring will be integrated into the Jenkins jobs to track the artifact's lifecycle and ensure reliable deployments across environments.\n\n### Compliance\nThis approach ensures that the same binary artifact deployed to **Integration** is used in later environments, thereby ensuring compliance with our integrity and traceability requirements for software releases.","GenTime":"2024-08-19 23:43:39"}
{"File Name":"status-react\/0011-tweak-pr-process.md","Context":"## Context\\nThere was a generally dissatisfaction with our PR flow process from multiple stakeholders, including devs, QA and design. These largely centered around size, speed of integration and quality of PRs.\\nFor more details, please see pain points in the meeting notes held end of February here: https:\/\/notes.status.im\/C5pj8g7gQOu9Wo8PtDZsMw?edit# as well as the preceeding Discuss thread: https:\/\/discuss.status.im\/t\/better-pull-requests-process\/1044\\nAlso see conversations in Core Dev Call #12 and #13: https:\/\/github.com\/status-im\/pm\/\\n","Decision":"These specific policy changes were agreed upon (a la 'rough consensus' model). These are heuristics, and should be more of a guide than strict rules. Exception to these should be minimal though.\\n**1. Reject PRs that don't have tests**\\nAs a rule of thumb, coverage should increase.\\n**2. Don't use or promote nightly as something for end users.**\\n**3. Reject PRs that don't write what tests the author have done and what platforms you used to test, including screenshots, videos or logs**\\nThis means QA and dev have to spend less time catching obvious problems.\\n**4. Run tests when a PR is at \"Review\" stage on https:\/\/github.com\/status-im\/status-react\/projects\/7 (or add new 'e2e stage' between review and `TO TEST`)**\\n**5. Dev asks for 2-3 reviewers, including a designer if the change affects UI, and these reviewers review within a day.**\\nThis ensures we don't have *diffusion of responsibility* and that PRs are reviewed quickly.\\n","tokens":135,"id":2964,"text":"## Context\\nThere was a generally dissatisfaction with our PR flow process from multiple stakeholders, including devs, QA and design. These largely centered around size, speed of integration and quality of PRs.\\nFor more details, please see pain points in the meeting notes held end of February here: https:\/\/notes.status.im\/C5pj8g7gQOu9Wo8PtDZsMw?edit# as well as the preceeding Discuss thread: https:\/\/discuss.status.im\/t\/better-pull-requests-process\/1044\\nAlso see conversations in Core Dev Call #12 and #13: https:\/\/github.com\/status-im\/pm\/\\n\n\n##Decision\nThese specific policy changes were agreed upon (a la 'rough consensus' model). These are heuristics, and should be more of a guide than strict rules. Exception to these should be minimal though.\\n**1. Reject PRs that don't have tests**\\nAs a rule of thumb, coverage should increase.\\n**2. Don't use or promote nightly as something for end users.**\\n**3. Reject PRs that don't write what tests the author have done and what platforms you used to test, including screenshots, videos or logs**\\nThis means QA and dev have to spend less time catching obvious problems.\\n**4. Run tests when a PR is at \"Review\" stage on https:\/\/github.com\/status-im\/status-react\/projects\/7 (or add new 'e2e stage' between review and `TO TEST`)**\\n**5. Dev asks for 2-3 reviewers, including a designer if the change affects UI, and these reviewers review within a day.**\\nThis ensures we don't have *diffusion of responsibility* and that PRs are reviewed quickly.\\n","MatchedIds":[1614,3915,4609,4899,187],"Predicted":"## Decision\nWe will implement the following measures to address the dissatisfaction with our PR flow process:\n\n1. **Smaller PRs**: Encourage developers to split their work into smaller and more manageable PRs. This will help reduce the size of individual PRs, making them easier to review and integrate. We will emphasize this practice in our team documentation and onboarding processes.\n\n2. **Faster Integration**:\n    - **Automated Merging**: When a PR is approved and passes all checks, it should be automatically merged to the main branch. This reduces the waiting time for manual merging.\n    - **Dedicated Review Time**: Allocate specific times during the day for reviewers to focus solely on reviewing PRs to ensure timely feedback and integration.\n\n3. **Quality Assurance**:\n    - **Pre-merge Checks**: Ensure that all required checks (automated tests, linting, style checks, etc.) must pass before a PR can be merged.\n    - **Peer Review Process**: Reinforce a peer review process where at least two reviewers must approve a PR before merging. This helps in maintaining quality and catching potential issues early.\n\n4. **Design Reviews**:\n    - **Involving Design Team Early**: Incorporate design reviews earlier in the development process. This can be done through design handoff meetings or Figma (or any design tool) reviews before implementation.\n\n5. **Communication & Documentation**:\n    - **Centralize Communication**: Use a central channel (e.g., Slack, Teams) for discussing PRs that need urgent attention.\n    - **Comprehensive Meeting Notes**: Document all decisions and action items discussed in core dev calls and make them accessible to the team.\n    - **Documentation Standards**: Enhance our documentation standards for PR descriptions, ensuring they are clear, concise, and provide all necessary context for reviewers.\n\n6. **Feedback Loop**: Establish a regular feedback loop (e.g., bi-weekly retrospectives) to gather continuous feedback about the PR process and make necessary adjustments.\n\nBy implementing these changes, we aim to streamline the PR workflow, improve the speed of integration, maintain high-quality standards, and ensure that all stakeholders are satisfied with the process.","GenTime":"2024-08-19 23:43:47"}
{"File Name":"adr\/ADR-40-ui-dependencies-upgrades.md","Context":"## Context and Problem Statement\\nThe organization has several UI apps and libraries and of them have different React versions, causing issues whenever we want to consume them. To remove these problems, and to keep every app updated, we need to move to React 17 in every UI app and lib, specially in the UI repository that contains most of our shared UI components.\\nUpdating the UI repository to the latest version of React implies updating `react-semantic-ui` to its latest version, ending up in [a major change that removed the `Responsive` component](https:\/\/github.com\/Semantic-Org\/Semantic-UI-React\/pull\/4008), a widely used component dedicated to conditionally rendering different components based on their display. Removing this component will cause a breaking change in our current [UI library](https:\/\/github.com\/decentraland\/ui) and will imply everyone to get on board of this breaking change, but a different strategy can be chosen by keeping the `Responsive` component by copying it from the library until everyone gets on board with an alternative.\\nWe need to provide, alongside this update, an alternative library to the `Responsive` component, providing a similar or a better API for rendering components according to device sizes.\\n- The `@artsy\/fresnel` works by using a ContextProvider component that wraps the whole application, coupling the media query solution to this library.\\n- Doesn't have hooks support.\\n#### Second alternative (react-semantic-ui)\\n##### Advantages\\n- The libary doesn't require a provider or something previously set in an application to use it (non-coupling dependency).\\n- Provides hooks and component solutions for rendering components with different media queries, providing a versatile that allows us to render different components or part of the components by using the hooks.\\n##### Disadvantages\\n- Bad SSR support.\\n","Decision":"The option to keep the an exact copy of the `Responsive` component (from the old `react-semantic-ui` lib version) was chosen in order to have a frictionless upgrade of the library.\\nThe procedure in which we'll be handling the upgrade is the following:\\n1. A non breaking change upgrade will be provided to our [UI library](https:\/\/github.com\/decentraland\/ui), keeping the `Responsive` component as a deprecated component and an alternative (describe below) will be provided to replace it.\\n2. A breaking change upgrade will be applied to our [UI library](https:\/\/github.com\/decentraland\/ui), whenever all of our dependencies are updated, removing the `Responsive` component.\\nWe\u2019ll be providing, alongside the `Responsive` component a set of components and hooks to replace it, using the `react-responsive`, library. This library was chosen in favor of the recommended `@artsy\/fresnel` mainly because of its versatility. The need of having to set a provider at the application's root level, (coupling the users of this dependency to `@artsy\/fresnel`) to have better SSR support that we don't currently need, made us decide not to go with it.\\nThe components built with the `react-responsive` and exposed to the consumers of our [UI library](https:\/\/github.com\/decentraland\/ui) will be the following:\\n- **Desktop** (for devices with `min width: 992`)\\n- **Tablet** (for devices with `min width: 768 and max width: 991`)\\n- **TabletAndBelow** (for devices with `max width: 991`, that is taking into consideration tablets and mobile devices)\\n- **Mobile** (for devices with `max width: 767`)\\n- **NotMobile** (for devices that don't comply with the requirements specified in Mobile)\\nThese components describe a conditional rendering based on the media the page in being rendered.\\nWhere we had:\\n```tsx\\n<Responsive\\nas={Menu}\\nsecondary\\nstackable\\nminWidth={Responsive.onlyTablet.minWidth}\\n>\\n<a className=\"dcl navbar-logo\" href=\"https:\/\/decentraland.org\">\\n<Logo \/>\\n<\/a>\\n{this.renderLeftMenu()}\\n<\/Responsive>\\n<Responsive\\n{...Responsive.onlyMobile}\\nclassName=\"dcl navbar-mobile-menu\"\\n>\\n<a className=\"dcl navbar-logo\" href=\"https:\/\/decentraland.org\">\\n<Logo \/>\\n<\/a>\\n<Header\\nsize=\"small\"\\nclassName={`dcl active-page ${\\nthis.state.toggle ? 'caret-up' : 'caret-down'\\n}`}\\nonClick={this.handleToggle}\\n>\\n{activePage}\\n<\/Header>\\n<\/Responsive>\\n```\\nWe now have:\\n```tsx\\n<NotMobile>\\n<Menu secondary stackable>\\n<a className=\"dcl navbar-logo\" href=\"https:\/\/decentraland.org\">\\n<Logo \/>\\n<\/a>\\n{this.renderLeftMenu()}\\n<\/Menu>\\n<\/NotMobile>\\n<Mobile>\\n<div className=\"dcl navbar-mobile-menu\">\\n<a className=\"dcl navbar-logo\" href=\"https:\/\/decentraland.org\">\\n<Logo \/>\\n<\/a>\\n<Header\\nsize=\"small\"\\nclassName={`dcl active-page ${\\nthis.state.toggle ? 'caret-up' : 'caret-down'\\n}`}\\nonClick={this.handleToggle}\\n>\\n{activePage}\\n<\/Header>\\n<\/div>\\n<\/Mobile>\\n```\\nAnd, alongside these components, as explained before, we're exposing the following set of hooks:\\n- **useDesktopMediaQuery**\\n- **useTabletMediaQuery**\\n- **useTabletAndBelowMediaQuery**\\n- **useMobileMediaQuery**\\n- **useNotMobileMediaQuery**\\nWhich return true if the device is the one defined as the name of the hook.\\nThese types of hooks will provide us with newer functionality, being able to customize small portions of our code instead of forking our components into two.\\nAs an example, we can apply certain styles by simply:\\n```tsx\\nconst isMobile = useMobileMediaQuery()\\nconst classes = isMobile ? \"dcl mobile\" : \"dcl\"\\n<div className={classes}>\\n...\\n<\/div>\\n```\\n","tokens":379,"id":4613,"text":"## Context and Problem Statement\\nThe organization has several UI apps and libraries and of them have different React versions, causing issues whenever we want to consume them. To remove these problems, and to keep every app updated, we need to move to React 17 in every UI app and lib, specially in the UI repository that contains most of our shared UI components.\\nUpdating the UI repository to the latest version of React implies updating `react-semantic-ui` to its latest version, ending up in [a major change that removed the `Responsive` component](https:\/\/github.com\/Semantic-Org\/Semantic-UI-React\/pull\/4008), a widely used component dedicated to conditionally rendering different components based on their display. Removing this component will cause a breaking change in our current [UI library](https:\/\/github.com\/decentraland\/ui) and will imply everyone to get on board of this breaking change, but a different strategy can be chosen by keeping the `Responsive` component by copying it from the library until everyone gets on board with an alternative.\\nWe need to provide, alongside this update, an alternative library to the `Responsive` component, providing a similar or a better API for rendering components according to device sizes.\\n- The `@artsy\/fresnel` works by using a ContextProvider component that wraps the whole application, coupling the media query solution to this library.\\n- Doesn't have hooks support.\\n#### Second alternative (react-semantic-ui)\\n##### Advantages\\n- The libary doesn't require a provider or something previously set in an application to use it (non-coupling dependency).\\n- Provides hooks and component solutions for rendering components with different media queries, providing a versatile that allows us to render different components or part of the components by using the hooks.\\n##### Disadvantages\\n- Bad SSR support.\\n\n\n##Decision\nThe option to keep the an exact copy of the `Responsive` component (from the old `react-semantic-ui` lib version) was chosen in order to have a frictionless upgrade of the library.\\nThe procedure in which we'll be handling the upgrade is the following:\\n1. A non breaking change upgrade will be provided to our [UI library](https:\/\/github.com\/decentraland\/ui), keeping the `Responsive` component as a deprecated component and an alternative (describe below) will be provided to replace it.\\n2. A breaking change upgrade will be applied to our [UI library](https:\/\/github.com\/decentraland\/ui), whenever all of our dependencies are updated, removing the `Responsive` component.\\nWe\u2019ll be providing, alongside the `Responsive` component a set of components and hooks to replace it, using the `react-responsive`, library. This library was chosen in favor of the recommended `@artsy\/fresnel` mainly because of its versatility. The need of having to set a provider at the application's root level, (coupling the users of this dependency to `@artsy\/fresnel`) to have better SSR support that we don't currently need, made us decide not to go with it.\\nThe components built with the `react-responsive` and exposed to the consumers of our [UI library](https:\/\/github.com\/decentraland\/ui) will be the following:\\n- **Desktop** (for devices with `min width: 992`)\\n- **Tablet** (for devices with `min width: 768 and max width: 991`)\\n- **TabletAndBelow** (for devices with `max width: 991`, that is taking into consideration tablets and mobile devices)\\n- **Mobile** (for devices with `max width: 767`)\\n- **NotMobile** (for devices that don't comply with the requirements specified in Mobile)\\nThese components describe a conditional rendering based on the media the page in being rendered.\\nWhere we had:\\n```tsx\\n<Responsive\\nas={Menu}\\nsecondary\\nstackable\\nminWidth={Responsive.onlyTablet.minWidth}\\n>\\n<a className=\"dcl navbar-logo\" href=\"https:\/\/decentraland.org\">\\n<Logo \/>\\n<\/a>\\n{this.renderLeftMenu()}\\n<\/Responsive>\\n<Responsive\\n{...Responsive.onlyMobile}\\nclassName=\"dcl navbar-mobile-menu\"\\n>\\n<a className=\"dcl navbar-logo\" href=\"https:\/\/decentraland.org\">\\n<Logo \/>\\n<\/a>\\n<Header\\nsize=\"small\"\\nclassName={`dcl active-page ${\\nthis.state.toggle ? 'caret-up' : 'caret-down'\\n}`}\\nonClick={this.handleToggle}\\n>\\n{activePage}\\n<\/Header>\\n<\/Responsive>\\n```\\nWe now have:\\n```tsx\\n<NotMobile>\\n<Menu secondary stackable>\\n<a className=\"dcl navbar-logo\" href=\"https:\/\/decentraland.org\">\\n<Logo \/>\\n<\/a>\\n{this.renderLeftMenu()}\\n<\/Menu>\\n<\/NotMobile>\\n<Mobile>\\n<div className=\"dcl navbar-mobile-menu\">\\n<a className=\"dcl navbar-logo\" href=\"https:\/\/decentraland.org\">\\n<Logo \/>\\n<\/a>\\n<Header\\nsize=\"small\"\\nclassName={`dcl active-page ${\\nthis.state.toggle ? 'caret-up' : 'caret-down'\\n}`}\\nonClick={this.handleToggle}\\n>\\n{activePage}\\n<\/Header>\\n<\/div>\\n<\/Mobile>\\n```\\nAnd, alongside these components, as explained before, we're exposing the following set of hooks:\\n- **useDesktopMediaQuery**\\n- **useTabletMediaQuery**\\n- **useTabletAndBelowMediaQuery**\\n- **useMobileMediaQuery**\\n- **useNotMobileMediaQuery**\\nWhich return true if the device is the one defined as the name of the hook.\\nThese types of hooks will provide us with newer functionality, being able to customize small portions of our code instead of forking our components into two.\\nAs an example, we can apply certain styles by simply:\\n```tsx\\nconst isMobile = useMobileMediaQuery()\\nconst classes = isMobile ? \"dcl mobile\" : \"dcl\"\\n<div className={classes}>\\n...\\n<\/div>\\n```\\n","MatchedIds":[1992,3035,4609,334,2317],"Predicted":"## Decision\nWe have decided to move forward with integrating the `@artsy\/fresnel` library for handling responsive design and media query solutions. \n\n### Rationale:\n1. **Consistency and Stability**: By adopting `@artsy\/fresnel`, we ensure a consistent and stable upgrade path for our React migration to version 17 without the need for temporary solutions or custom component copying. This approach minimizes the risk of introducing technical debt.\n\n2. **Less Coupling**: Although `@artsy\/fresnel` requires a ContextProvider, it will make the integration less coupled to any particular component, providing a cleaner and modular approach for future updates.\n\n3. **Avoiding Breaking Changes**: By immediately adopting `@artsy\/fresnel`, we avoid the complex transition that would come with copying and maintaining the `Responsive` component from `react-semantic-ui`. This allows us to avoid complex maintenance overhead and potential synchronization issues.\n\n4. **Future-Proofing**: While `@artsy\/fresnel` doesn\u2019t currently have hooks support, it\u2019s a widely adopted library with a good community, increasing the likelihood of future hooks support and improvements that our team can benefit from.\n\n5. **ContextProvider Advantages**: Libraries that use a ContextProvider are generally advantageous for larger applications as they provide a centralized configuration and propagate consistent values down the component tree. It simplifies the logic for media queries across various components.\n\n### Positive Consequences:\n- **Eliminates Technical Debt**: By choosing a well-supported library, we avoid temporary, stopgap measures.\n- **Unified React Version**: Moving all our UI apps and libraries to React 17 without workarounds helps in maintaining a unified front.\n- **Community Support**: The wide adoption of `@artsy\/fresnel` by the community ensures better support and frequent updates.\n\n### Negative Consequences:\n- **Initial Setup Overhead**: There will be an initial overhead in setting up the ContextProvider across all projects.\n- **Learning Curve**: Teams will need to adapt to the new library, which involves understanding how `@artsy\/fresnel` works and integrating it across projects.\n\n### Example Implementation:\n```javascript\n\/\/ In your root component (e.g., App.js)\nimport { MediaContextProvider } from '@artsy\/fresnel';\nconst App = () => (\n  <MediaContextProvider>\n    <YourAppComponent \/>\n  <\/MediaContextProvider>\n);\n\n\/\/ Using the Media component later\nimport { Media } from '@artsy\/fresnel';\nconst ExampleComponent = () => (\n  <>\n    <Media at=\"sm\">\n      <SmallScreenComponent \/>\n    <\/Media>\n    <Media greaterThan=\"sm\">\n      <LargerScreenComponent \/>\n    <\/Media>\n  <\/>\n);\n```\n\nBy making this decision, we ensure that our approach to responsive design remains robust and future-proof while aligning with our migration to React 17.","GenTime":"2024-08-19 23:43:57"}
{"File Name":"smjs\/2018121601-multiple-usages-of-the-processor.md","Context":"## Context\\nThe domain our virtual machine is supporting is concerned with running programs: as such, how programs should be run is a business rule, and the Processor is the Domain Service encapsulating that rule.\\nAt the application level, we can think of several features that can support this domain:\\n- classic virtual machine, running an entire program, perhaps loaded from a file, until its termination\\n- REPL, where instructions are loaded one at a time from an interactive prompt, and executed, printing some kind of output each time\\n- debug, where we still have an entire program, but this time we want to support debugging features, like running step-by-step\\nOf course there could be many others. Can we use the same definition of a Processor in the domain to support all this cases?\\n","Decision":"We will keep the Processor as a Domain Service, because the three different kinds of execution we want to support really only differ for how data is handled in memory, and the domain doesn't concern itself with what is contained in memory (it doesn't even mention memory, since it's a detail of the interpreter, and we might as well use interpreters that don't use any memory):\\n- in the classic virtual machine, the Program is the entire program, that is executed from start to end\\n- in the REPL situation, each time a new instruction is inserted at the prompt, it is added to the Program, which is then fed to the Processor: this is necessary because we might have defined procedures that we want to call in the future; the state of memory is saved between different executions (this doesn't concern the domain), and to make the program start at the latest instruction, we add a new unconditional jump instruction at the beginning of it: this actually means that we don't run the entire program again (it isn't necessary since memory is preserved), but only the last inserted instruction\\n- in the debug situation, the Interpreter implementation can avoid returning control back to the Processor until requested to: this still doesn't concern the Processor implementation in the domain\\n","tokens":161,"id":3981,"text":"## Context\\nThe domain our virtual machine is supporting is concerned with running programs: as such, how programs should be run is a business rule, and the Processor is the Domain Service encapsulating that rule.\\nAt the application level, we can think of several features that can support this domain:\\n- classic virtual machine, running an entire program, perhaps loaded from a file, until its termination\\n- REPL, where instructions are loaded one at a time from an interactive prompt, and executed, printing some kind of output each time\\n- debug, where we still have an entire program, but this time we want to support debugging features, like running step-by-step\\nOf course there could be many others. Can we use the same definition of a Processor in the domain to support all this cases?\\n\n\n##Decision\nWe will keep the Processor as a Domain Service, because the three different kinds of execution we want to support really only differ for how data is handled in memory, and the domain doesn't concern itself with what is contained in memory (it doesn't even mention memory, since it's a detail of the interpreter, and we might as well use interpreters that don't use any memory):\\n- in the classic virtual machine, the Program is the entire program, that is executed from start to end\\n- in the REPL situation, each time a new instruction is inserted at the prompt, it is added to the Program, which is then fed to the Processor: this is necessary because we might have defined procedures that we want to call in the future; the state of memory is saved between different executions (this doesn't concern the domain), and to make the program start at the latest instruction, we add a new unconditional jump instruction at the beginning of it: this actually means that we don't run the entire program again (it isn't necessary since memory is preserved), but only the last inserted instruction\\n- in the debug situation, the Interpreter implementation can avoid returning control back to the Processor until requested to: this still doesn't concern the Processor implementation in the domain\\n","MatchedIds":[3972,276,3023,4896,2502],"Predicted":"## Decision\nWe will define a generic `Processor` interface within the domain that encapsulates the core business rule of running instructions. This interface will be designed to accommodate different execution modes, such as classic virtual machine execution, REPL, and debug mode. Each execution mode will be implemented as a specific class that adheres to the `Processor` interface. \n\nFurthermore, the various features (e.g., classic execution, REPL, debug) will each have their respective implementations. We will leverage polymorphism to allow the Processor to be used uniformly across different execution modes, ensuring flexibility and extensibility.\n\n### Implementation Details\n1. **Processor Interface**: A common interface defining the essential methods for instruction execution.\n   ```java\n   public interface Processor {\n       ExecutionResult executeInstruction(Instruction instruction);\n       void loadProgram(Program program);\n   }\n   ```\n\n2. **ClassicProcessor**: Implements `Processor`, loads an entire program and executes it until completion.\n   ```java\n   public class ClassicProcessor implements Processor {\n       \/\/ Implementation details specific to classic execution.\n       @Override\n       public ExecutionResult executeInstruction(Instruction instruction) {\n           \/\/ Classic execution logic\n       }\n       \n       @Override\n       public void loadProgram(Program program) {\n           \/\/ Program loading logic\n       }\n   }\n   ```\n\n3. **REPLProcessor**: Implements `Processor`, loads instructions one at a time from an interactive prompt and executes them individually.\n   ```java\n   public class REPLProcessor implements Processor {\n       \/\/ Implementation details specific to REPL mode.\n       @Override\n       public ExecutionResult executeInstruction(Instruction instruction) {\n           \/\/ REPL execution logic\n       }\n\n       @Override\n       public void loadProgram(Program program) {\n           \/\/ May be no-op or handle single instructions\n       }\n   }\n   ```\n\n4. **DebugProcessor**: Implements `Processor`, supports step-by-step execution with debugging features.\n   ```java\n   public class DebugProcessor implements Processor {\n       \/\/ Implementation details specific to debug mode.\n       @Override\n       public ExecutionResult executeInstruction(Instruction instruction) {\n           \/\/ Debug execution logic\n       }\n\n       @Override\n       public void loadProgram(Program program) {\n           \/\/ Program loading with debugging hooks\n       }\n   }\n   ```\n\nThis design allows us to maintain a consistent interface while catering to different execution modes, ensuring that any future execution modes can simply implement the `Processor` interface to be integrated seamlessly into the system.","GenTime":"2024-08-19 23:44:06"}
{"File Name":"kafka\/0002-require-php-7-2-64-bit.md","Context":"## Context\\nDeciding which minimum PHP version to require is quite important because it\\nmight restrict people from using the library, at the same time supporting many\\nversions increases the cost to maintain the library.\\n","Decision":"For the first releases of this library we'll be requiring PHP 7.2+ (64-bit build\\nonly).\\nThe main motivation for only allowing it to be installed in 64-bit systems is\\nthat Kafka's protocol uses signed longs (64-bit integers) for some fields, which\\ncan't be achieve in 32-bit systems.\\nI believe that libraries' maintainers also have the role to help the evolution\\nof the PHP ecosystem, which makes me want to require a more up-to-date version\\nof the language.\\n","tokens":42,"id":641,"text":"## Context\\nDeciding which minimum PHP version to require is quite important because it\\nmight restrict people from using the library, at the same time supporting many\\nversions increases the cost to maintain the library.\\n\n\n##Decision\nFor the first releases of this library we'll be requiring PHP 7.2+ (64-bit build\\nonly).\\nThe main motivation for only allowing it to be installed in 64-bit systems is\\nthat Kafka's protocol uses signed longs (64-bit integers) for some fields, which\\ncan't be achieve in 32-bit systems.\\nI believe that libraries' maintainers also have the role to help the evolution\\nof the PHP ecosystem, which makes me want to require a more up-to-date version\\nof the language.\\n","MatchedIds":[1048,241,3062,939,2951],"Predicted":"## Decision\nRequire a minimum PHP version of 7.4. This version is chosen because it balances modern language features with wide adoption. It also maintains compatibility with many existing hosting environments. We will review this decision annually and evaluate upgrading the minimum PHP version based on the PHP supported versions timeline and adoption rates. Only versions that are currently supported (actively supported or in security fixes mode) will be considered for increasing the minimum PHP version. For reference, see [PHP supported versions](https:\/\/www.php.net\/supported-versions.php).","GenTime":"2024-08-19 23:44:09"}
{"File Name":"PIMS\/database.md","Context":"## Context\\nPIMS requires a database to store all property information.\\nThe data is relational, requiring constraints and must run within a Linux docker container on OpenShift.\\nAdditionally it must be supported by Entity Framework Core 3.1.\\n","Decision":"Originally the database generated for the SWU was with PostgreSQL, after further consideration it made more sense to tightly couple both MS-SQL with .NET Core.\\nThis will give us better performance and tighter integration with Entity Framework Core.\\nIt was decided to create a Linux docker container to host the MS-SQL 2019 database.\\n","tokens":49,"id":3966,"text":"## Context\\nPIMS requires a database to store all property information.\\nThe data is relational, requiring constraints and must run within a Linux docker container on OpenShift.\\nAdditionally it must be supported by Entity Framework Core 3.1.\\n\n\n##Decision\nOriginally the database generated for the SWU was with PostgreSQL, after further consideration it made more sense to tightly couple both MS-SQL with .NET Core.\\nThis will give us better performance and tighter integration with Entity Framework Core.\\nIt was decided to create a Linux docker container to host the MS-SQL 2019 database.\\n","MatchedIds":[2488,1167,1758,3657,1661],"Predicted":"## Decision\nAfter evaluating the requirements and compatibility with the technologies involved, we have decided to use PostgreSQL for the following reasons:\n- **Compatibility with Entity Framework Core 3.1**: PostgreSQL has robust support for Entity Framework Core 3.1, ensuring smooth integration with our .NET applications.\n- **Relational Database with Strong Constraint Support**: PostgreSQL is a powerful, open-source relational database that provides comprehensive support for data integrity constraints, such as foreign keys, uniqueness, and check constraints, which are crucial for our property information management.\n- **Docker Compatibility**: PostgreSQL can be easily containerized and run within a Linux Docker container, aligning with our requirement to deploy on OpenShift.\n- **Performance and Scalability**: PostgreSQL offers advanced features such as MVCC (Multiversion Concurrency Control), support for large-scale workloads, and efficient indexing mechanisms that make it a reliable choice for handling relational data in production environments.\n\nIn conclusion, PostgreSQL meets all our technical requirements and provides the necessary features to ensure data integrity, performance, and ease of deployment within our infrastructure.","GenTime":"2024-08-19 23:44:14"}
{"File Name":"simple-server\/014-region-level-sync.md","Context":"## Context\\n[PRD](https:\/\/docs.google.com\/document\/d\/1Cflct0Y-44IRUVw_5-NptcnNSX1UgAPBiXqoXHq22io\/edit)\\nUsers in large districts reported that the Simple app was running very slow, making the app near-unusable.\\nThe slowdown was caused by the volume of patient data synced to the user\u2019s phone. We realised that the amount of data\\nbeing stored on the device had to be reduced for better long-term performance.\\nCurrently we sync the entire district's records to a user's phone. Some of the large districts have upto 50,000 patients,\\nwhich can amount to 400-500 MB of data. On lower-end phones we noticed the app started slowing down when the DB size grew beyond 250 MB.\\nA district typically has between 1-20 blocks. From trends in IHCI, we found it's uncommon for patients to visit facilities across blocks.\\nPatients that have a BP taken in more than 1 block is around 2%, with the exceptions of:\\nSindhudurg (9.8%), Hoshiarpur (5.3%), Bathinda (3.1%).\\nThis means that we can sync only a block's data to the user's phone and be reasonably confident about finding patients on the app.\\n","Decision":"- The server will sync records from the user's block instead of the entire district.\\nSpecifically the following patients will be synced:\\n- patients that registered at a facility in the same block,\\n- patients that are assigned to a facility in the same block, and\\n- patients that have an appointment scheduled at a facility in the same block.\\n- All other sync resources will return records belonging to these patients only.\\n- The sync mechanism should support the ability to adjust the sync radius to any sync region.\\nThis is important in case we need to change the kind of records that are synced to the app in the future.\\nSee the [wiki entry on Region level sync](..\/wiki\/adjusting-sync-boundaries.md) for how it works.\\n### On the app\\n- Users can continue selecting any facility in their district when switching facilities.\\n- Users can continue selecting any facility in their district when scheduling a patient\u2019s next visit or preferred facility.\\n- It is possible that a patient will visit a facility outside their block and their record will not be found on the user\u2019s device. In this case the user should\\n- Scan the patient\u2019s BP passport if they have one.\\n- Register the patient again, as if they were new. Make sure to attach their existing BP passport to the registration.\\n- The duplicate patient records will be merged by the Simple team later.\\n","tokens":282,"id":1708,"text":"## Context\\n[PRD](https:\/\/docs.google.com\/document\/d\/1Cflct0Y-44IRUVw_5-NptcnNSX1UgAPBiXqoXHq22io\/edit)\\nUsers in large districts reported that the Simple app was running very slow, making the app near-unusable.\\nThe slowdown was caused by the volume of patient data synced to the user\u2019s phone. We realised that the amount of data\\nbeing stored on the device had to be reduced for better long-term performance.\\nCurrently we sync the entire district's records to a user's phone. Some of the large districts have upto 50,000 patients,\\nwhich can amount to 400-500 MB of data. On lower-end phones we noticed the app started slowing down when the DB size grew beyond 250 MB.\\nA district typically has between 1-20 blocks. From trends in IHCI, we found it's uncommon for patients to visit facilities across blocks.\\nPatients that have a BP taken in more than 1 block is around 2%, with the exceptions of:\\nSindhudurg (9.8%), Hoshiarpur (5.3%), Bathinda (3.1%).\\nThis means that we can sync only a block's data to the user's phone and be reasonably confident about finding patients on the app.\\n\n\n##Decision\n- The server will sync records from the user's block instead of the entire district.\\nSpecifically the following patients will be synced:\\n- patients that registered at a facility in the same block,\\n- patients that are assigned to a facility in the same block, and\\n- patients that have an appointment scheduled at a facility in the same block.\\n- All other sync resources will return records belonging to these patients only.\\n- The sync mechanism should support the ability to adjust the sync radius to any sync region.\\nThis is important in case we need to change the kind of records that are synced to the app in the future.\\nSee the [wiki entry on Region level sync](..\/wiki\/adjusting-sync-boundaries.md) for how it works.\\n### On the app\\n- Users can continue selecting any facility in their district when switching facilities.\\n- Users can continue selecting any facility in their district when scheduling a patient\u2019s next visit or preferred facility.\\n- It is possible that a patient will visit a facility outside their block and their record will not be found on the user\u2019s device. In this case the user should\\n- Scan the patient\u2019s BP passport if they have one.\\n- Register the patient again, as if they were new. Make sure to attach their existing BP passport to the registration.\\n- The duplicate patient records will be merged by the Simple team later.\\n","MatchedIds":[1712,2824,1799,3337,4859],"Predicted":"## Decision\nWe will modify the sync mechanism to download only the specific block's data to the user's phone instead of the entire district's records. \n\n### Implementation Steps:\n1. **Block-Based Data Segregation:**\n   - Segregate the data in the server based on district blocks.\n   - Each user's location (block) will be determined during the initial setup or periodically verified and updated.\n\n2. **Selective Sync:**\n   - Implement a selective sync process that fetches data only for the blocks relevant to the user.\n   - The app will dynamically adjust to sync additional blocks data in exceptional cases, such as for the districts where cross-block patient visits are common (e.g., Sindhudurg, Hoshiarpur, Bathinda).\n\n3. **Configurable Sync Settings:**\n   - Allow configuration to include extra blocks if users report visiting multiple blocks frequently.\n\n4. **Data Management and Purging:**\n   - Implement data purging to remove old or less frequently accessed data from the device when the database size limit approaches 250MB.\n   - Periodically clean up records for patients who have not been active or visited the facility within a certain timeframe.\n\n### Expected Benefits:\n- **Performance Improvement:** Reducing the volume of patient data stored on each device will enhance app performance, especially on lower-end phones.\n- **Efficient Resource Utilization:** Optimizing the sync mechanism ensures more efficient utilization of storage and processing resources on the user's device.\n- **Scalability:** The app will be more scalable, allowing it to handle more users and larger datasets without degradation in performance.\n\nWith this decision, we aim to strike a balance between performance and usability, ensuring the app remains responsive and effective for users in large districts.","GenTime":"2024-08-19 23:44:21"}
{"File Name":"protagonist\/0001-composite-handler-design.md","Context":"## Context and Problem Statement\\n### Context\\nA design for the implementation and delivery of a component of the DLCS\\nwhich can process \"composite\" artifacts. Such artifacts typically take\\nthe form of a single object containing multiple embedded resources, each\\nof which can be rasterized and treated as an individual resource within\\nthe DLCS.\\n","Decision":"Delivery of a standalone component which consumes a DLCS ingest payload\\ntailored to \"composite\" artifacts, and triggers an ingestion workflow\\nwhich rasterizes each of the constituent parts and invokes the existing\\nDLCS ingestion workflow.\\n![0001-flowchart.png](images\/0001-flowchart.png)\\n","tokens":69,"id":2859,"text":"## Context and Problem Statement\\n### Context\\nA design for the implementation and delivery of a component of the DLCS\\nwhich can process \"composite\" artifacts. Such artifacts typically take\\nthe form of a single object containing multiple embedded resources, each\\nof which can be rasterized and treated as an individual resource within\\nthe DLCS.\\n\n\n##Decision\nDelivery of a standalone component which consumes a DLCS ingest payload\\ntailored to \"composite\" artifacts, and triggers an ingestion workflow\\nwhich rasterizes each of the constituent parts and invokes the existing\\nDLCS ingestion workflow.\\n![0001-flowchart.png](images\/0001-flowchart.png)\\n","MatchedIds":[160,3784,2342,4609,1172],"Predicted":"## Decision\nImplement a Composite Artifact Processor (CAP) module within the DLCS, designed to handle artifacts containing multiple embedded resources.\n\n### Implementation Details:\n1. **Artifact Decomposition**:\n   - Decompose composite artifacts into individual embedded resources.\n   - Each embedded resource should be identifiable and traceable back to its parent composite artifact for auditing and tracking purposes.\n\n2. **Rasterization**:\n   - Implement a rasterization service that converts each embedded resource into a raster format, suitable for further processing within the DLCS.\n   - Ensure support for various input formats (e.g., vector graphics, PDFs, etc.)\n\n3. **Resource Management**:\n   - Store decomposed resources as individual entities within the DLCS, but maintain metadata linkage to the parent composite artifact.\n   - Manage resource lifecycle, including creation, updates, and deletion, in sync with the parent artifact's lifecycle.\n\n4. **API Enhancements**:\n   - Extend the DLCS API to support the submission, retrieval, and management of composite artifacts.\n   - Provide endpoints for uploading composite artifacts and fetching their decomposed resources.\n\n### Positive Consequences:\n* Allows for efficient handling and storage of complex artifacts within the DLCS.\n* Facilitates individual processing of embedded resources, improving flexibility and scalability.\n* Enhances the DLCS capabilities, making it more versatile for various use cases involving composite artifacts.\n\n### Negative Consequences:\n* Increased complexity in the design and implementation of the DLCS.\n* Additional overhead in terms of processing power and storage needs to manage decomposed resources.\n* Potential need for extensive testing to ensure the correct functioning of the decomposition and rasterization processes.\n\n### Risk Mitigation:\n1. **Prototyping and Testing**:\n   - Develop a prototype to validate the decomposition and rasterization processes.\n   - Conduct thorough testing to ensure performance and reliability.\n\n2. **Documentation and Training**:\n   - Provide comprehensive documentation on the usage of the Composite Artifact Processor module.\n   - Train the development and operations teams on the new features and their implications.\n\n3. **Performance Monitoring**:\n   - Implement monitoring and logging to track the performance and resource consumption of the CAP module.\n   - Regularly review and optimize to ensure efficient operation.","GenTime":"2024-08-19 23:44:32"}
{"File Name":"alfresco-anaxes-shipyard\/0004-k8s-cluster-creation.md","Context":"## Context\\nWe need to recommend a tool to those customers that do not have an existing Kubernetes cluster.\\nA tool called minikube can be used on laptops for development and evaluation purposes. For a production ready stack we need to provide a way for customers to provision a cluster in AWS using a mechanism a less technically literate person can follow.\\nA list of candidate solutions can be found [here](https:\/\/github.com\/kubernetes\/community\/blob\/master\/sig-aws\/kubernetes-on-aws.md).\\n","Decision":"We will be informing customers that we use kops internally and it's the cluster upon which we perform all our testing.\\nWe have chosen kops as it's the tool the most closely fulfils our current and future requirements, it\u2019s produced by the same team that build Kubernetes and it\u2019s been [endorsed](https:\/\/aws.amazon.com\/blogs\/compute\/kubernetes-clusters-aws-kops) by AWS.\\n","tokens":101,"id":3600,"text":"## Context\\nWe need to recommend a tool to those customers that do not have an existing Kubernetes cluster.\\nA tool called minikube can be used on laptops for development and evaluation purposes. For a production ready stack we need to provide a way for customers to provision a cluster in AWS using a mechanism a less technically literate person can follow.\\nA list of candidate solutions can be found [here](https:\/\/github.com\/kubernetes\/community\/blob\/master\/sig-aws\/kubernetes-on-aws.md).\\n\n\n##Decision\nWe will be informing customers that we use kops internally and it's the cluster upon which we perform all our testing.\\nWe have chosen kops as it's the tool the most closely fulfils our current and future requirements, it\u2019s produced by the same team that build Kubernetes and it\u2019s been [endorsed](https:\/\/aws.amazon.com\/blogs\/compute\/kubernetes-clusters-aws-kops) by AWS.\\n","MatchedIds":[2471,4358,3917,3889,2997],"Predicted":"## Decision\nWe will recommend Amazon EKS (Elastic Kubernetes Service) for customers who need a production-ready Kubernetes cluster on AWS. Amazon EKS offers a managed Kubernetes service that simplifies the process of deploying, running, and managing Kubernetes clusters on AWS. For ease of use and accessibility by less technically literate persons, we will provide the following resources and guidelines:\n\n1. **AWS Management Console Instructions:**\n   - A step-by-step guide for setting up an EKS cluster using the AWS Management Console. This guide will include screenshots and detailed instructions to simplify the process.\n   \n2. **AWS CloudFormation Templates:**\n   - Pre-defined CloudFormation templates that automate the provisioning of EKS clusters. These templates will simplify the setup process by handling the configuration and resource management automatically.\n\n3. **Eksctl Utility:**\n   - Recommend the use of `eksctl`, a command line tool specifically for EKS. Eksctl helps automate much of the creation process, reducing the complexity involved in setting up and managing clusters.\n\n4. **Comprehensive Documentation and Training:**\n   - Detailed documentation and video tutorials that walk through the entire EKS setup process, from creation to deployment and scaling.\n   - Providing training sessions or webinars to help customers become familiar with EKS.\n\nBy choosing Amazon EKS, we ensure customers have access to a robust, secure, and scalable Kubernetes solution with comprehensive support and automation features, ideal for users with limited technical expertise.","GenTime":"2024-08-19 23:44:39"}
{"File Name":"web-mapviewer\/2020_05_28_test_framework.md","Context":"## Context\\nAn integration test framework (and possibly a combo unit\/integration test) has to be chosen for this project.\\nAfter reviewing what's the state of testing with JS in 2020, here's a list of contenders.\\n### Selenium\\nHas been the standard of the industry for many years. Start to struggle with single page application framework (SPA) like Angular, Vue.js or React.js.\\n- Desktop browsers, emulators \/ simulators and real devices (E2E testing all the way)\\n- Highly scalable\\n- Has many \"sister frameworks\" that enhance the feature set (like Appium for mobile testing, Nightwach or WebdriverIO)\\nWhile very efficient for E2E testing, this framework would be cumbersome to have in development on a local machine.\\nCould be used for E2E testing for cross browser compatibility tests.\\n### TestCafe\\nGood alternative to Selenium, is compatible with Saucelab (we already have some account with them).\\n- Fast to set up\\n- Cross Browser and Devices (with SauceLabs or BrowserStack)\\nThis could also be a good cross browser testing framework, but the cross browser feature will be hard to have deployed on every developers' machine.\\n### Cypress\\nMade by devs for devs.\\n- Very convenient running and debugging tools\\n- Native access to all your application\u2019s variables\\n- Solid and clear documentation\\n- Newest addition (2017) to the big frameworks (very active community)\\n","Decision":"After playing a bit with these frameworks, Cypress clearly had something for it.\\nThe documentation is well maintained and complete, tests are described with Mocha which is already known by many in the team (it's what we used to write our tests on the older viewer)\\nCypress will be a great help during the development phase of the new viewer.\\nBut we have to keep in mind that it doesn't really do cross browser testing.\\n","tokens":307,"id":4785,"text":"## Context\\nAn integration test framework (and possibly a combo unit\/integration test) has to be chosen for this project.\\nAfter reviewing what's the state of testing with JS in 2020, here's a list of contenders.\\n### Selenium\\nHas been the standard of the industry for many years. Start to struggle with single page application framework (SPA) like Angular, Vue.js or React.js.\\n- Desktop browsers, emulators \/ simulators and real devices (E2E testing all the way)\\n- Highly scalable\\n- Has many \"sister frameworks\" that enhance the feature set (like Appium for mobile testing, Nightwach or WebdriverIO)\\nWhile very efficient for E2E testing, this framework would be cumbersome to have in development on a local machine.\\nCould be used for E2E testing for cross browser compatibility tests.\\n### TestCafe\\nGood alternative to Selenium, is compatible with Saucelab (we already have some account with them).\\n- Fast to set up\\n- Cross Browser and Devices (with SauceLabs or BrowserStack)\\nThis could also be a good cross browser testing framework, but the cross browser feature will be hard to have deployed on every developers' machine.\\n### Cypress\\nMade by devs for devs.\\n- Very convenient running and debugging tools\\n- Native access to all your application\u2019s variables\\n- Solid and clear documentation\\n- Newest addition (2017) to the big frameworks (very active community)\\n\n\n##Decision\nAfter playing a bit with these frameworks, Cypress clearly had something for it.\\nThe documentation is well maintained and complete, tests are described with Mocha which is already known by many in the team (it's what we used to write our tests on the older viewer)\\nCypress will be a great help during the development phase of the new viewer.\\nBut we have to keep in mind that it doesn't really do cross browser testing.\\n","MatchedIds":[4223,3323,452,4359,2290],"Predicted":"## Decision\nWe will use Cypress as our integration test framework. It offers convenient running and debugging tools, native access to all the application's variables, solid and clear documentation, and an active community. Cypress is tailored for developers, making it an excellent choice for both unit and integration testing.","GenTime":"2024-08-19 23:44:42"}
{"File Name":"jupyter-nbrequirements\/0000-dependencies-management-jupyter-notebooks.md","Context":"## Context and Problem Statement\\nHow to guarantee reproducibility of Jupyter Notebooks?\\nIn order to allow any user to re run the notebook with similar behaviour, it's important that each notebook is shipped with dependencies requirements\\nthat include direct and transitive dependencies. This would also enforce and support security, reproducibility, traecability.\\nNotebooks should be treated as component\/service that use their own dependencies, therefore when storing notebooks,\\nthey should be stored with dependencies so that an image can be built to run them or they can be shared and reused by others.\\n## Decision Drivers <!-- optional -->\\n* user prospective\\n* reproducibility\\n* traecability\\n","Decision":"* user prospective\\n* reproducibility\\n* traecability\\nThe option selected is 3. because:\\n* enforce reproducibility\\n* enforce traceability between notebook\\n### Positive Consequences <!-- optional -->\\n* Satisfy reproducibility, traecability, shareability.\\n* Notebooks are coupled with dependencies in their metadata.\\n* If more notebooks are present, a common Pipfile can be created with a button that can automatically extract from all notebook dependencies and new common Pipfile.lock will be created. This would allow creation of an image that can run the notebooks.\\n","tokens":140,"id":1228,"text":"## Context and Problem Statement\\nHow to guarantee reproducibility of Jupyter Notebooks?\\nIn order to allow any user to re run the notebook with similar behaviour, it's important that each notebook is shipped with dependencies requirements\\nthat include direct and transitive dependencies. This would also enforce and support security, reproducibility, traecability.\\nNotebooks should be treated as component\/service that use their own dependencies, therefore when storing notebooks,\\nthey should be stored with dependencies so that an image can be built to run them or they can be shared and reused by others.\\n## Decision Drivers <!-- optional -->\\n* user prospective\\n* reproducibility\\n* traecability\\n\n\n##Decision\n* user prospective\\n* reproducibility\\n* traecability\\nThe option selected is 3. because:\\n* enforce reproducibility\\n* enforce traceability between notebook\\n### Positive Consequences <!-- optional -->\\n* Satisfy reproducibility, traecability, shareability.\\n* Notebooks are coupled with dependencies in their metadata.\\n* If more notebooks are present, a common Pipfile can be created with a button that can automatically extract from all notebook dependencies and new common Pipfile.lock will be created. This would allow creation of an image that can run the notebooks.\\n","MatchedIds":[1631,36,1117,3551,4096],"Predicted":"## Decision\nChosen option: \"Store notebooks with dependencies\", because:\n\n- It ensures that all necessary dependencies (both direct and transitive) are explicitly listed, making it easier to reproduce the environment in which the notebook was initially run.\n- This approach enhances security by providing clear visibility into the required dependencies, allowing for better dependency management and vulnerability tracking.\n- By treating notebooks as components or services with their own dependencies, it becomes simpler to share and reuse them across different teams or projects.\n\n### Implementation Steps\n1. For each Jupyter notebook, create a `requirements.txt` or `environment.yml` file that lists all direct and transitive dependencies.\n2. Store the notebook and its dependency file together in a version-controlled repository.\n3. Use tools like Docker or Conda to create an image that can be used to run the notebook in an isolated and reproducible environment.\n4. Establish CI\/CD pipelines to automatically build and test the notebook images to verify the environment setup and reproducibility.\n   \n### Positive Consequences\n- Users can reliably reproduce the notebook environment, ensuring consistent results.\n- Version control of dependencies supports traceability and aids in the auditing process.\n- Easy sharing and reuse of notebooks with clear dependency management.\n\n### Negative Consequences\n- Additional overhead in maintaining and updating dependency files.\n- Potential for increasing storage requirements to keep all dependencies alongside notebooks.\n- Complexity in managing dependency versions and potential conflicts across different notebooks.","GenTime":"2024-08-19 23:44:47"}
{"File Name":"stac-ml-aoi-extension\/0004-multiple-label-items.md","Context":"## Context\\nShould each `ml-aoi` Item be able to bring in multiple labels?\\nThis would be a useful feature for training multi-class classifiers.\\nOne can imagine having a label STAC item for buildings and separate STAC item for fields.\\nSTAC Items Links object is an array, so many label items could be linked to from a single `ml-aoi` STAC Item.\\n#### Limiting to single label link\\nLimiting to single label link however is appealing because the label item metadata could be copied over to `ml-aoi` Item.\\nThis would remove the need to follow the link for the label item during processing.\\nIn practice this would make each `ml-aoi` Item also a `label` Item, allowing for its re-use by tooling that understands `label`.\\nIf multi-class label dataset would be required there would have to be a mechanical pre-processing step of combining\\nexisting labels into a single STAC `label` item. This could mean either union of GeoJSON FeatureCollections per item or\\na configuration of a more complex STAC `label` Item that links to multiple label assets.\\n#### Allowing multiple labels\\nThe main appeal of consuming multi-label `ml-aoi` items is that it would allow referencing multiple label sources,\\nsome which could be external, without the need for pre-processing and thus minimizing data duplication.\\nIf multiple labels were to be allowed the `ml-aoi` the pre-processing step above would be pushed into `ml-aoi` consumer.\\nThe consumer would need appropriate metadata in order to decipher how the label structure.\\nThis would require either crawling the full catalog or some kind of meta-label structure that combines the metadata\\nfrom all the included labels into a single structure that could be interpreted by the consumer.\\n","Decision":"`ml-aoi` Items should be limited to linking to only a single label item.\\nRequiring the consumer to interpret multiple label items pushed unreasonable complexity on the user.\\nAdditionally combining labels likely requires series of processing and validation steps.\\nEach one of those would likely require judgment calls and exceptions.\\nFor instance when combining building and fields label datasets the user should check that no building and field polygons overlap.\\nIt is not realistic to expect all possible requirements of that process to be expressed by a simple metadata structure.\\nTherefore it is better to explicitly require the label combination as a separate process done by the user.\\nThe resulting label catalog can capture that design and iteration required for that process anyway.\\n","tokens":375,"id":2286,"text":"## Context\\nShould each `ml-aoi` Item be able to bring in multiple labels?\\nThis would be a useful feature for training multi-class classifiers.\\nOne can imagine having a label STAC item for buildings and separate STAC item for fields.\\nSTAC Items Links object is an array, so many label items could be linked to from a single `ml-aoi` STAC Item.\\n#### Limiting to single label link\\nLimiting to single label link however is appealing because the label item metadata could be copied over to `ml-aoi` Item.\\nThis would remove the need to follow the link for the label item during processing.\\nIn practice this would make each `ml-aoi` Item also a `label` Item, allowing for its re-use by tooling that understands `label`.\\nIf multi-class label dataset would be required there would have to be a mechanical pre-processing step of combining\\nexisting labels into a single STAC `label` item. This could mean either union of GeoJSON FeatureCollections per item or\\na configuration of a more complex STAC `label` Item that links to multiple label assets.\\n#### Allowing multiple labels\\nThe main appeal of consuming multi-label `ml-aoi` items is that it would allow referencing multiple label sources,\\nsome which could be external, without the need for pre-processing and thus minimizing data duplication.\\nIf multiple labels were to be allowed the `ml-aoi` the pre-processing step above would be pushed into `ml-aoi` consumer.\\nThe consumer would need appropriate metadata in order to decipher how the label structure.\\nThis would require either crawling the full catalog or some kind of meta-label structure that combines the metadata\\nfrom all the included labels into a single structure that could be interpreted by the consumer.\\n\n\n##Decision\n`ml-aoi` Items should be limited to linking to only a single label item.\\nRequiring the consumer to interpret multiple label items pushed unreasonable complexity on the user.\\nAdditionally combining labels likely requires series of processing and validation steps.\\nEach one of those would likely require judgment calls and exceptions.\\nFor instance when combining building and fields label datasets the user should check that no building and field polygons overlap.\\nIt is not realistic to expect all possible requirements of that process to be expressed by a simple metadata structure.\\nTherefore it is better to explicitly require the label combination as a separate process done by the user.\\nThe resulting label catalog can capture that design and iteration required for that process anyway.\\n","MatchedIds":[528,498,2074,4281,285],"Predicted":"## Decision\n\nWe have decided to allow each `ml-aoi` Item to bring in multiple labels.\n\n### Rationale:\n\n1. **Flexibility and Multi-class Capability**:\n   - Allowing multiple labels enables greater flexibility in constructing multi-class classifiers. Users can incorporate various label sources (e.g., buildings, fields) without preprocessing them into a singular label structure.\n\n2. **Data Reuse and External Integration**:\n   - Users can reference external label sources directly, facilitating integration with external datasets and minimizing data duplication and storage overhead.\n\n3. **Encouraging Interoperability**:\n   - By keeping label sources distinct, we preserve the interoperability of `ml-aoi` Items with existing `label`-understanding tooling, as well as new tools that might be developed to take advantage of this structure.\n\n4. **Decentralized Processing**:\n   - While it shifts some processing responsibilities to the consumers, it promotes a decentralized approach where the clients have control over how they interpret and consolidate label data. This flexibility can be beneficial for users with diverse requirements and workflows.\n\n### Requirements:\n\n1. **Metadata for Label Structure**:\n   - `ml-aoi` Items must provide comprehensive metadata defining how to interpret the associated labels. This metadata will include necessary details like label categories, geometry types, and any other schema-dependent information.\n\n2. **Link Management**:\n   - The `ml-aoi` Links array must manage multiple label links effectively, ensuring consumers can easily identify and access the relevant label STAC Items.\n\n3. **FeatureCollection Handling**:\n   - Consumers will need to handle GeoJSON FeatureCollections from multiple label sources, understanding the union and intersections of these features as required by their specific use cases.\n\n### Implementation Steps:\n\n1. Define a schema for `ml-aoi` metadata that clearly articulates the structure and types of linked labels.\n2. Update documentation and tooling to support the interpretation of multiple label links within `ml-aoi`.\n3. Develop sample workflows and examples that illustrate how to consume and process multi-label `ml-aoi` Items.\n4. Ensure backward compatibility and provide guidance for users transitioning from single-label `ml-aoi` Items to the new multi-label format.\n\nBy adopting this approach, we aim to empower users with greater flexibility and minimize unnecessary preprocessing, ultimately enhancing the utility and efficacy of the `ml-aoi` Items.","GenTime":"2024-08-19 23:44:59"}
{"File Name":"adr\/ADR-8-dao-content-servers-and-local-content-servers.md","Context":"## Context and Problem Statement\\n* Today Catalyst servers are split in two groups listed and unlisted in the DAO\\nListed servers sync with eachother by a pull mechanism (between each other, listed in the DAO).\\nUnlisted servers, sync again those who are listed but. Listed servers never pull changes from unlisted servers.\\n* Big scenes are constrained by server configurations or proxy limitations of the deployed nodes (i.e. body post size). Nowadays, those limitations are workarounded by selecting another server with a different configuration (Kyllian's).\\n### Use cases\\n* I want to push my scene to my own server and I expect it to replicate to other servers.\\n* Create a \"local-staging\" environment in which land owners or creators can synchronize the whole world from a DAO node and test locally.\\n* Broadcast of changes should be optional.\\n","Decision":"* Enable custom Catalyst servers from Explorer\\n* Define action plan for broadcasting\/commiting local deployments to the DAO servers. [issue](https:\/\/github.com\/decentraland\/catalyst\/issues\/195)\\n","tokens":178,"id":4601,"text":"## Context and Problem Statement\\n* Today Catalyst servers are split in two groups listed and unlisted in the DAO\\nListed servers sync with eachother by a pull mechanism (between each other, listed in the DAO).\\nUnlisted servers, sync again those who are listed but. Listed servers never pull changes from unlisted servers.\\n* Big scenes are constrained by server configurations or proxy limitations of the deployed nodes (i.e. body post size). Nowadays, those limitations are workarounded by selecting another server with a different configuration (Kyllian's).\\n### Use cases\\n* I want to push my scene to my own server and I expect it to replicate to other servers.\\n* Create a \"local-staging\" environment in which land owners or creators can synchronize the whole world from a DAO node and test locally.\\n* Broadcast of changes should be optional.\\n\n\n##Decision\n* Enable custom Catalyst servers from Explorer\\n* Define action plan for broadcasting\/commiting local deployments to the DAO servers. [issue](https:\/\/github.com\/decentraland\/catalyst\/issues\/195)\\n","MatchedIds":[786,4609,3337,4626,4618],"Predicted":"## Decision\nWe will implement a configurable synchronization mechanism to allow optional broadcasting and selective synchronization between Catalyst servers. This includes:\n\n1. **Configurable Push Mechanism**: Allow servers to push changes to other listed servers optionally. This allows users to push their scenes to their own servers and replicate changes as needed.\n2. **Selective Synchronization**: Enable listed servers to selectively pull changes from unlisted servers based on configuration. This will help overcome constraints by choosing suitable server configurations dynamically.\n3. **Local-Staging Environment**: Create a configurable local-staging environment for creators where they can synchronize the entire world from a DAO node locally for testing purposes.\n4. **Broadcasting Toggle**: Include a toggle for optional broadcasting of changes to provide flexibility and avoid unnecessary synchronization.\n\n### Positive Consequences\n* Ensures flexibility in deployment and synchronization of scenes.\n* Reduces constraints posed by server configurations and proxy limitations.\n* Facilitates a more robust and customizable testing environment for landowners and creators.\n* Optional broadcasting minimizes unnecessary data replication, conserving bandwidth and server resources.\n\n### Negative Consequences\n* Increased complexity in configuring synchronization settings.\n* Additional development and maintenance overhead to implement and support configurable synchronization and broadcasting mechanisms.\n* Potential for misconfiguration leading to inconsistent states between servers.","GenTime":"2024-08-19 23:45:10"}
{"File Name":"docspell\/0004_iso8601vsEpoch.md","Context":"# Context and Problem Statement\\nThe question is whether the REST Api should return an ISO8601\\nformatted string in UTC timezone, or the unix time (number of\\nmilliseconds since 1970-01-01).\\nThere is quite some controversy about it.\\n- <https:\/\/stackoverflow.com\/questions\/47426786\/epoch-or-iso8601-date-format>\\n- <https:\/\/nbsoftsolutions.com\/blog\/designing-a-rest-api-unix-time-vs-iso-8601>\\nIn my opinion, the ISO8601 format (always UTC) is better. The reason\\nis the better readability. But elm folks are on the other side:\\n- <https:\/\/package.elm-lang.org\/packages\/elm\/time\/1.0.0#iso-8601>\\n- <https:\/\/package.elm-lang.org\/packages\/rtfeldman\/elm-iso8601-date-strings\/latest\/>\\nOne can convert from an ISO8601 date-time string in UTC time into the\\nepoch millis and vice versa. So it is the same to me. There is no less\\ninformation in a ISO8601 string than in the epoch millis.\\nTo avoid confusion, all date\/time values should use the same encoding.\\n# Decision Outcome\\nI go with the epoch time. Every timestamp\/date-time values is\\ntransfered as Unix timestamp.\\nReasons:\\n- the Elm application needs to frequently calculate with these values\\nto render the current waiting time etc. This is better if there are\\nnumbers without requiring to parse dates first\\n- Since the UI is written with Elm, it's probably good to adopt their\\nstyle\\n","Decision":"I go with the epoch time. Every timestamp\/date-time values is\\ntransfered as Unix timestamp.\\nReasons:\\n- the Elm application needs to frequently calculate with these values\\nto render the current waiting time etc. This is better if there are\\nnumbers without requiring to parse dates first\\n- Since the UI is written with Elm, it's probably good to adopt their\\nstyle\\n","tokens":337,"id":280,"text":"# Context and Problem Statement\\nThe question is whether the REST Api should return an ISO8601\\nformatted string in UTC timezone, or the unix time (number of\\nmilliseconds since 1970-01-01).\\nThere is quite some controversy about it.\\n- <https:\/\/stackoverflow.com\/questions\/47426786\/epoch-or-iso8601-date-format>\\n- <https:\/\/nbsoftsolutions.com\/blog\/designing-a-rest-api-unix-time-vs-iso-8601>\\nIn my opinion, the ISO8601 format (always UTC) is better. The reason\\nis the better readability. But elm folks are on the other side:\\n- <https:\/\/package.elm-lang.org\/packages\/elm\/time\/1.0.0#iso-8601>\\n- <https:\/\/package.elm-lang.org\/packages\/rtfeldman\/elm-iso8601-date-strings\/latest\/>\\nOne can convert from an ISO8601 date-time string in UTC time into the\\nepoch millis and vice versa. So it is the same to me. There is no less\\ninformation in a ISO8601 string than in the epoch millis.\\nTo avoid confusion, all date\/time values should use the same encoding.\\n# Decision Outcome\\nI go with the epoch time. Every timestamp\/date-time values is\\ntransfered as Unix timestamp.\\nReasons:\\n- the Elm application needs to frequently calculate with these values\\nto render the current waiting time etc. This is better if there are\\nnumbers without requiring to parse dates first\\n- Since the UI is written with Elm, it's probably good to adopt their\\nstyle\\n\n\n##Decision\nI go with the epoch time. Every timestamp\/date-time values is\\ntransfered as Unix timestamp.\\nReasons:\\n- the Elm application needs to frequently calculate with these values\\nto render the current waiting time etc. This is better if there are\\nnumbers without requiring to parse dates first\\n- Since the UI is written with Elm, it's probably good to adopt their\\nstyle\\n","MatchedIds":[1610,2755,2198,560,693],"Predicted":"## Decision\n\nWe have decided that the REST API will return date\/time values in Unix time (number of milliseconds since 1970-01-01). This decision was made based on the following considerations:\n\n### Rationale\n\n1. **Efficiency in Calculations**: The Elm application frequently calculates with these values to render the current waiting time and other time-dependent features. Using Unix time (milliseconds) allows for more efficient calculations without the need to parse human-readable date strings.\n\n2. **Consistency with Elm Style**: Since the user interface (UI) is written in Elm, it is beneficial to adopt the style and conventions that are more native to Elm. The Elm community generally prefers Unix time for its simplicity and ease of use within the language.\n\n3. **Avoiding Parsing Overhead**: Converting from ISO8601 formatted strings to Unix time adds extra parsing overhead and complexity. By using Unix time directly, we reduce the computational burden on the client side.\n\n4. **Interchangeability**: Both formats contain the same information, but Unix time offers a more straightforward approach for computational requirements without losing any information.\n\n### Implementation Details\n\n- All date\/time values transmitted through the API will be in Unix time (number of milliseconds since 1970-01-01).\n- Documentation will clearly state that the API returns dates and times in Unix time format to avoid any confusion.\n- Any future extensions or integrations should adhere to this standard to maintain consistency.\n\n### Consequences\n\n#### Positive Consequences\n\n- Simplified date\/time handling in the Elm frontend, leading to better performance and easier maintenance.\n- Consistent and predictable format for all date\/time values in the API, reducing ambiguity and potential errors.\n\n#### Negative Consequences\n\n- Some developers and systems preferring ISO8601 for readability may need to adjust their workflow to accommodate Unix time.\n- Additional documentation and education may be required to ensure all API consumers understand the chosen format.\n\n### Conclusion\n\nThe decision to use Unix time for date\/time values in the REST API aligns with the requirements and preferences of our Elm-based frontend while simplifying computational logic and maintaining consistency.","GenTime":"2024-08-19 23:45:22"}
{"File Name":"community\/0007-github-actions.md","Context":"## [Context](https:\/\/docs.google.com\/document\/d\/1zRy8OCZ_JJpbs6scwrvx5PYkWwaWbR-5DLuq3AujhBA\/edit)\\nLibero products need automated builds for pull requests and release candidates.\\nTravis CI has been [acquired](https:\/\/techcrunch.com\/2019\/01\/23\/idera-acquires-travis-ci\/) by a private equity firm and has an [uncertain future](https:\/\/twitter.com\/ReinH\/status\/1098663375985229825). We also have run into performance bottlenecks of [5 concurrent jobs](https:\/\/travis-ci.com\/plans) in the open source offer.\\nGithub Actions is a Github-native general workflow system that can provide CI\/CD capabilities; it has been in general availability [since November](https:\/\/github.blog\/2019-08-08-github-actions-now-supports-ci-cd\/). It offers fully managed, sandboxed environments and a per-repository limit to concurrency.\\n","Decision":"We will create Github Actions builds for all new projects.\\nWe will port existing Travis CI builds of maintained projects onto Github Actions, on an as-needed basis.\\n","tokens":222,"id":3456,"text":"## [Context](https:\/\/docs.google.com\/document\/d\/1zRy8OCZ_JJpbs6scwrvx5PYkWwaWbR-5DLuq3AujhBA\/edit)\\nLibero products need automated builds for pull requests and release candidates.\\nTravis CI has been [acquired](https:\/\/techcrunch.com\/2019\/01\/23\/idera-acquires-travis-ci\/) by a private equity firm and has an [uncertain future](https:\/\/twitter.com\/ReinH\/status\/1098663375985229825). We also have run into performance bottlenecks of [5 concurrent jobs](https:\/\/travis-ci.com\/plans) in the open source offer.\\nGithub Actions is a Github-native general workflow system that can provide CI\/CD capabilities; it has been in general availability [since November](https:\/\/github.blog\/2019-08-08-github-actions-now-supports-ci-cd\/). It offers fully managed, sandboxed environments and a per-repository limit to concurrency.\\n\n\n##Decision\nWe will create Github Actions builds for all new projects.\\nWe will port existing Travis CI builds of maintained projects onto Github Actions, on an as-needed basis.\\n","MatchedIds":[152,118,1741,2393,3482],"Predicted":"## Decision\nThe decision is to migrate from Travis CI to GitHub Actions for automated builds of pull requests and release candidates. This choice leverages GitHub Actions' native integration with GitHub repositories, offering improved performance with managed, sandboxed environments and higher concurrency limits per repository.","GenTime":"2024-08-19 23:45:25"}
{"File Name":"verify-event-store-schema\/0003-database-migrations-should-be-designed-for-zero-downtime.md","Context":"## Context\\nSee also ADR 0002 \"Database migrations are standalone releases\"\\nAs our system is designed for zero downtime, we have to be careful that\\nwe don't change the database in a way that causes production issues\\n","Decision":"Where possible, we should avoid database migrations that will lock the database\\nfor any significant amount of time.  This is hard to enforce, but we will\\nmake sure there is documentation in the project README (and here!) on ways\\nto achieve this.\\nThis mostly affects index creation and changes - we have several years of data\\nin our database, and adding or changing indexes can be slow.  In general,\\nyou should use the `CREATE INDEX CONCURRENTLY` option to let indexes be\\ncreated in a non-blocking way.  See https:\/\/www.postgresql.org\/docs\/current\/static\/sql-createindex.html\\nIf you want to `ALTER INDEX` or `REINDEX`, they can't be concurrent - in this\\ncase you'll need to look at stopping the Event Recorder lambda, allowing messages\\nto queue up while the index change is made.  *BEWARE* however that SQS queues\\nonly allow 100,000 messages, and at peak load we have historically sent 75,000\\nmessages an hour, so you have a somewhat limited amount of time to run such a change.\\nIf you have a very complex change, you should consider:\\n- Dropping the index then running `CREATE INDEX CONCURRENTLY` rather than\\naltering indexes - generally our reports run intermittently, so it is safe to have\\nno indexes for a period of time, data will still be appended with no problems\\n- Performance testing the change - we have a large fake dataset available that\\ncan be used to simulate a production database in a test environment\\n- Duplicating the database - you could apply the change to a new database containing\\na copy of production data, then switch databases, and migrate any missed changes\\nfrom the old database to the new.\\n### Transactional DDL changes\\nMost Postgresql schema changes can be made transactionally - this is\\na great feature, as it allows for making multiple changes and having them\\nall roll back if something goes wrong.  For example:\\n```\\nBEGIN;\\nALTER TABLE fizzbuzz RENAME COLUMN foo TO bar;\\nUPDATE TABLE fizzbuzz set foo = 'splat';\\nCOMMIT;\\n```\\nIn this case the `UPDATE` will fail, so the column rename will be reverted.\\n*However* note that `CREATE INDEX CONCURRENTLY` does not work in a transaction -\\nit depends on being able to change the table incrementally, which doesn't fit\\nthe transaction model.  If the index creation fails, you are recommended to\\ndrop the index and re-create it, as it won't be rolled back and may be\\npartially created.\\n### Avoiding blocking changes\\nThere is a useful table in [this article](https:\/\/www.citusdata.com\/blog\/2018\/02\/15\/when-postgresql-blocks\/) which I've reproduced below (as the article may disappear):\\n| Runs concurrently with           | SELECT | INSERT UPDATE DELETE | CREATE INDEX CONC VACUUM ANALYZE | CREATE INDEX | CREATE TRIGGER | ALTER TABLE DROP TABLE TRUNCATE VACUUM FULL |\\n|----------------------------------|--------|----------------------|----------------------------------|--------------|----------------|---------------------------------------------|\\n| SELECT                           | Y      | Y                    | Y                                | Y            | Y              | No                                          |\\n| INSERT UPDATE DELETE             | Y      | Y                    | Y                                | No           | No             | No                                          |\\n| CREATE INDEX CONC VACUUM ANALYZE | Y      | Y                    | No                               | No           | No             | No                                          |\\n| CREATE INDEX                     | Y      | No                   | No                               | Y            | No             | No                                          |\\n| CREATE TRIGGER                   | Y      | No                   | No                               | No           | No             | No                                          |\\n| ALTER TABLE etc                  | No     | No                   | No                               | No           | No             | No                                          |\\nOur reports should only use SELECT so most operations won't block them.\\nThe Event Recorder however needs to insert data, so you can't run\\nany of the index modification changes (apart from `CREATE INDEX CONCURRENTLY`) without risking blocking.\\nNote however that some changes may be fast enough despite blocking - adding a column\\nfor example.  However you should performance test these changes against a\\nproduction-sized database to be sure!\\n","tokens":48,"id":317,"text":"## Context\\nSee also ADR 0002 \"Database migrations are standalone releases\"\\nAs our system is designed for zero downtime, we have to be careful that\\nwe don't change the database in a way that causes production issues\\n\n\n##Decision\nWhere possible, we should avoid database migrations that will lock the database\\nfor any significant amount of time.  This is hard to enforce, but we will\\nmake sure there is documentation in the project README (and here!) on ways\\nto achieve this.\\nThis mostly affects index creation and changes - we have several years of data\\nin our database, and adding or changing indexes can be slow.  In general,\\nyou should use the `CREATE INDEX CONCURRENTLY` option to let indexes be\\ncreated in a non-blocking way.  See https:\/\/www.postgresql.org\/docs\/current\/static\/sql-createindex.html\\nIf you want to `ALTER INDEX` or `REINDEX`, they can't be concurrent - in this\\ncase you'll need to look at stopping the Event Recorder lambda, allowing messages\\nto queue up while the index change is made.  *BEWARE* however that SQS queues\\nonly allow 100,000 messages, and at peak load we have historically sent 75,000\\nmessages an hour, so you have a somewhat limited amount of time to run such a change.\\nIf you have a very complex change, you should consider:\\n- Dropping the index then running `CREATE INDEX CONCURRENTLY` rather than\\naltering indexes - generally our reports run intermittently, so it is safe to have\\nno indexes for a period of time, data will still be appended with no problems\\n- Performance testing the change - we have a large fake dataset available that\\ncan be used to simulate a production database in a test environment\\n- Duplicating the database - you could apply the change to a new database containing\\na copy of production data, then switch databases, and migrate any missed changes\\nfrom the old database to the new.\\n### Transactional DDL changes\\nMost Postgresql schema changes can be made transactionally - this is\\na great feature, as it allows for making multiple changes and having them\\nall roll back if something goes wrong.  For example:\\n```\\nBEGIN;\\nALTER TABLE fizzbuzz RENAME COLUMN foo TO bar;\\nUPDATE TABLE fizzbuzz set foo = 'splat';\\nCOMMIT;\\n```\\nIn this case the `UPDATE` will fail, so the column rename will be reverted.\\n*However* note that `CREATE INDEX CONCURRENTLY` does not work in a transaction -\\nit depends on being able to change the table incrementally, which doesn't fit\\nthe transaction model.  If the index creation fails, you are recommended to\\ndrop the index and re-create it, as it won't be rolled back and may be\\npartially created.\\n### Avoiding blocking changes\\nThere is a useful table in [this article](https:\/\/www.citusdata.com\/blog\/2018\/02\/15\/when-postgresql-blocks\/) which I've reproduced below (as the article may disappear):\\n| Runs concurrently with           | SELECT | INSERT UPDATE DELETE | CREATE INDEX CONC VACUUM ANALYZE | CREATE INDEX | CREATE TRIGGER | ALTER TABLE DROP TABLE TRUNCATE VACUUM FULL |\\n|----------------------------------|--------|----------------------|----------------------------------|--------------|----------------|---------------------------------------------|\\n| SELECT                           | Y      | Y                    | Y                                | Y            | Y              | No                                          |\\n| INSERT UPDATE DELETE             | Y      | Y                    | Y                                | No           | No             | No                                          |\\n| CREATE INDEX CONC VACUUM ANALYZE | Y      | Y                    | No                               | No           | No             | No                                          |\\n| CREATE INDEX                     | Y      | No                   | No                               | Y            | No             | No                                          |\\n| CREATE TRIGGER                   | Y      | No                   | No                               | No           | No             | No                                          |\\n| ALTER TABLE etc                  | No     | No                   | No                               | No           | No             | No                                          |\\nOur reports should only use SELECT so most operations won't block them.\\nThe Event Recorder however needs to insert data, so you can't run\\nany of the index modification changes (apart from `CREATE INDEX CONCURRENTLY`) without risking blocking.\\nNote however that some changes may be fast enough despite blocking - adding a column\\nfor example.  However you should performance test these changes against a\\nproduction-sized database to be sure!\\n","MatchedIds":[1462,2396,714,224,1545],"Predicted":"## Decision\nWe will adopt a phased approach for database schema changes to ensure zero downtime and avoid production issues:\n\n1. **Backward Compatible Changes:** All initial schema changes must be backward compatible. This includes adding new columns or tables, but not removing or renaming existing ones. These changes can be safely applied without affecting existing code.\n\n2. **Feature Flagging:** Any application changes that depend on the new schema must use feature flags. This allows us to enable and disable new features without redeploying the code, ensuring that we can quickly revert if an issue arises.\n\n3. **Dual Write\/Migrate Reads:** For changes that involve data migrations, we will start by writing to both the old and new schema locations during a transitional period. Additionally, read operations will also be adjusted to read from both, preferring the new schema location when data is present. This ensures seamless transitions.\n\n4. **Monitoring and Validation:** We will employ extensive monitoring and validation mechanisms to ensure that schema changes are properly applied and consistent across the database. This includes automated tests and verification scripts.\n\n5. **Cleanup Phase:** Once we have verified that the new schema is fully operational and no longer rely on the old schema, we can safely remove the deprecated columns, tables, and codes. This final cleanup step will be executed in a separate release cycle to minimize risk.\n\nBy following this phased approach, we significantly reduce the risk of downtime or production issues due to database schema changes.","GenTime":"2024-08-19 23:45:34"}
{"File Name":"edgex-docs\/014-Secret-Provider-For-All.md","Context":"- [Context](#context)\\n* [Existing Implementations](#existing-implementations)\\n+ [What is a Secret?](#what-is-a-secret)\\n+ [Service Exclusive vs Service Shared Secrets](#service-exclusive-vs-service-shared-secrets)\\n+ [Known and Unknown Services](#known-and-unknown-services)\\n+ [Static Secrets and Runtime Secrets](#static-secrets-and-runtime-secrets)\\n+ [Interfaces and factory methods](#interfaces-and-factory-methods)\\n- [Bootstrap's current implementation](#bootstraps-current-implementation)\\n* [Interfaces](#interfaces)\\n* [Factory and bootstrap handler methods](#factory-and-bootstrap-handler-methods)\\n- [App SDK's current implementation](#app-sdks-current-implementation)\\n* [Interface](#interface)\\n* [Factory and bootstrap handler methods](#factory-and-bootstrap-handler-methods)\\n+ [Secret Store for non-secure mode](#secret-store-for-non-secure-mode)\\n- [InsecureSecrets Configuration](#insecuresecrets-configuration)\\n- [Decision](#decision)\\n* [Only Exclusive Secret Stores](#only-exclusive-secret-stores)\\n* [Abstraction Interface](#abstraction-interface)\\n* [Implementation](#implementation)\\n+ [Factory Method and Bootstrap Handler](#factory-method-and-bootstrap-handler)\\n+ [Caching of Secrets](#caching-of-secrets)\\n+ [Insecure Secrets](#insecure-secrets)\\n- [Handling on-the-fly changes to `InsecureSecrets`](#handling-on-the-fly-changes-to-insecuresecrets)\\n+ [Mocks](#mocks)\\n+ [Where will `SecretProvider` reside?](#where-will-secretprovider-reside)\\n- [Go Services](#go-services)\\n- [C Device Service](#c-device-service)\\n* [Consequences](#consequences)\\n","Decision":"* [Only Exclusive Secret Stores](#only-exclusive-secret-stores)\\n* [Abstraction Interface](#abstraction-interface)\\n* [Implementation](#implementation)\\n+ [Factory Method and Bootstrap Handler](#factory-method-and-bootstrap-handler)\\n+ [Caching of Secrets](#caching-of-secrets)\\n+ [Insecure Secrets](#insecure-secrets)\\n- [Handling on-the-fly changes to `InsecureSecrets`](#handling-on-the-fly-changes-to-insecuresecrets)\\n+ [Mocks](#mocks)\\n+ [Where will `SecretProvider` reside?](#where-will-secretprovider-reside)\\n- [Go Services](#go-services)\\n- [C Device Service](#c-device-service)\\n* [Consequences](#consequences)\\nThe new `SecretProvider` abstraction defined by this ADR is a combination of the two implementations described above in the [Existing Implementations](#existing-implementations) section.\\n### Only Exclusive Secret Stores\\nTo simplify the `SecretProvider` abstraction, we need to reduce to using only exclusive `SecretStores`. This allows all the APIs to deal with a single `SecretClient`, rather than the split up way we currently have in Application Services. This requires that the current Application Service shared secrets (database credentials) must be copied into each Application Service's exclusive `SecretStore` when it is created.\\nThe challenge is how do we seed static secrets for unknown services when they become known.  As described above in the [Known and Unknown Services](#known-and-unknown-services) section above,  services currently identify themselves for exclusive `SecretStore` creation via the `EDGEX_ADD_SECRETSTORE_TOKENS` environment variable on security-secretstore-setup. This environment variable simply takes a comma separated list of service names.\\n```yaml\\nEDGEX_ADD_SECRETSTORE_TOKENS: \"<service-name1>,<service-name2>\"\\n```\\nIf we expanded this to add an optional list of static secret identifiers for each service, i.e.  `appservice\/redisdb`, the exclusive store could also be seeded with a copy of static shared secrets. In this case the Redis database credentials for the Application Services' shared database. The environment variable name will change to `ADD_SECRETSTORE` now that it is more than just tokens.\\n```yaml\\nADD_SECRETSTORE: \"app-service-xyz[appservice\/redisdb]\"\\n```\\n> *Note: The secret identifier here is the short path to the secret in the existing **appservice**  `SecretStore`. In the above example this expands to the full path of `\/secret\/edgex\/appservice\/redisdb`*\\nThe above example results in the Redis credentials being copied into app-service-xyz's `SecretStore` at `\/secret\/edgex\/app-service-xyz\/redis`.\\nSimilar approach could be taken for Message Bus credentials where a common `SecretStore` is created with the Message Bus credentials saved. The services request the credentials are copied into their exclusive `SecretStore` using `common\/messagebus` as the secret identifier.\\nFull specification for the environment variable's value is a comma separated list of service entries defined as:\\n```\\n<service-name1>[optional list of static secret IDs sperated by ;],<service-name2>[optional list of static secret IDs sperated by ;],...\\n```\\nExample with one service specifying IDs for static secrets and one without static secrets\\n```yaml\\nADD_SECRETSTORE: \"appservice-xyz[appservice\/redisdb; common\/messagebus], appservice-http-export\"\\n```\\nWhen the `ADD_SECRETSTORE` environment variable is processed to create these `SecretStores`, it will copy the specified saved secrets from the initial `SecretStore` into the service's `SecretStore`. This all depends on the completion of database or other credential bootstrapping and the secrets having been stored prior to the environment variable being processed. security-secretstore-setup will need to be refactored to ensure this sequencing.\\n### Abstraction Interface\\nThe following will be the new `SecretProvider` abstraction interface used by all Edgex services\\n```go\\ntype SecretProvider interface {\\n\/\/ Stores new secrets into the service's exclusive SecretStore at the specified path.\\nStoreSecrets(path string, secrets map[string]string) error\\n\/\/ Retrieves secrets from the service's exclusive SecretStore at the specified path.\\nGetSecrets(path string, _ ...string) (map[string]string, error)\\n\/\/ Sets the secrets lastupdated time to current time.\\nSecretsUpdated()\\n\/\/ Returns the secrets last updated time\\nSecretsLastUpdated() time.Time\\n}\\n```\\n> *Note: The `GetDatabaseCredentials` and `GetCertificateKeyPair` APIs have been removed. These are no longer needed since insecure database credentials will no longer be stored in the `DatabaseInfo` configuration and certificate key pairs are secrets like any others. This allows these secrets to be retrieved via the `GetSecrets` API.*\\n### Implementation\\n#### Factory Method and Bootstrap Handler\\nThe factory method and bootstrap handler will follow that currently in the Bootstrap implementation with some tweaks. Rather than putting the two split interfaces into the DIC, it will put just the single interface instance into the DIC. See details in the [Interfaces and factory methods](#interfaces-and-factory-methods) section above under **Existing Implementations**.\\n#### Caching of Secrets\\nSecrets will be cached as they are currently in the Application Service implementation\\n#### Insecure Secrets\\nInsecure Secrets will be handled as they are currently in the Application Service implementation. `DatabaseInfo` configuration will no longer be an option for storing the insecure database credentials. They will be stored in the `InsecureSecrets` configuration only.\\n```toml\\n[Writable.InsecureSecrets]\\n[Writable.InsecureSecrets.DB]\\npath = \"redisdb\"\\n[Writable.InsecureSecrets.DB.Secrets]\\nusername = \"\"\\npassword = \"\"\\n```\\n##### Handling on-the-fly changes to `InsecureSecrets`\\nAll services will need to handle the special processing when `InsecureSecrets` are changed on-the-fly via Consul. Since this will now be a common configuration item in `Writable` it can be handled in `go-mod-bootstrap` along with existing log level processing. This special processing will be taken from App SDK.\\n#### Mocks\\nProper mock of the `SecretProvider` interface will be created with `Mockery` to be used in unit tests. Current mock in App SDK is hand written rather then generated with `Mockery`.\\n#### Where will `SecretProvider` reside?\\n##### Go Services\\nThe final decision to make is where will this new `SecretProvider` abstraction reside? Originally is was assumed that it would reside in `go-mod-secrets`, which seems logical. If we were to attempt this with the implementation including the bootstrap handler, `go-mod-secrets` would have a dependency on `go-mod-bootstrap` which will likely create a circular dependency.\\nRefactoring the existing implementation in `go-mod-bootstrap` and have it reside there now seems to be the best choice.\\n##### C Device Service\\nThe C Device SDK will implement the same `SecretProvider` abstraction, InsecureSercets configuration and the underling `SecretStore` client.\\n### Consequences\\n- All service's will have `Writable.InsecureSecrets` section added to their configuration\\n- `InsecureSecrets` definition will be moved from App SDK to go-mod-bootstrap\\n- Go Device SDK will add the SecretProvider to it's bootstrapping\\n- C Device SDK implementation could be big lift?\\n- ` SecretStore`configuration section will be added to all Device Services\\n- edgex-go services will be modified to use the single `SecretProvider` interface from the DIC in place of current usage of the `GetDatabaseCredentials` and `GetCertificateKeyPair` interfaces.\\n- Calls to `GetDatabaseCredentials` and `GetCertificateKeyPair` will be replaced with calls to `GetSecrets` API and appropriate processing of the returned secrets will be added.\\n- App SDK will be modified to use `GetSecrets` API in place of the `GetDatabaseCredentials` API\\n- App SDK will be modified to use the new `SecretProvider` bootstrap handler\\n- app-service-configurable's configuration profiles as well as all the Application Service examples configurations will be updated to remove the `SecretStoreExclusive` configuration and just use the existing `SecretStore` configuration\\n- security-secretstore-setup will be enhanced as described in the [Only Exclusive Secret Stores](#only-exclusive-secret-stores) section above\\n- Adding new services that need static secrets added to their `SecretStore` requires stopping and restarting all the services. The is because security-secretstore-setup has completed but not stopped. If it is rerun without stopping the other services, there tokens and static secrets will have changed. The planned refactor of `security-secretstore-setup` will attempt to resolve this.\\n- Snaps do not yet support setting the environment variable for adding SecretStore. It is planned for Ireland release.\\n","tokens":402,"id":947,"text":"- [Context](#context)\\n* [Existing Implementations](#existing-implementations)\\n+ [What is a Secret?](#what-is-a-secret)\\n+ [Service Exclusive vs Service Shared Secrets](#service-exclusive-vs-service-shared-secrets)\\n+ [Known and Unknown Services](#known-and-unknown-services)\\n+ [Static Secrets and Runtime Secrets](#static-secrets-and-runtime-secrets)\\n+ [Interfaces and factory methods](#interfaces-and-factory-methods)\\n- [Bootstrap's current implementation](#bootstraps-current-implementation)\\n* [Interfaces](#interfaces)\\n* [Factory and bootstrap handler methods](#factory-and-bootstrap-handler-methods)\\n- [App SDK's current implementation](#app-sdks-current-implementation)\\n* [Interface](#interface)\\n* [Factory and bootstrap handler methods](#factory-and-bootstrap-handler-methods)\\n+ [Secret Store for non-secure mode](#secret-store-for-non-secure-mode)\\n- [InsecureSecrets Configuration](#insecuresecrets-configuration)\\n- [Decision](#decision)\\n* [Only Exclusive Secret Stores](#only-exclusive-secret-stores)\\n* [Abstraction Interface](#abstraction-interface)\\n* [Implementation](#implementation)\\n+ [Factory Method and Bootstrap Handler](#factory-method-and-bootstrap-handler)\\n+ [Caching of Secrets](#caching-of-secrets)\\n+ [Insecure Secrets](#insecure-secrets)\\n- [Handling on-the-fly changes to `InsecureSecrets`](#handling-on-the-fly-changes-to-insecuresecrets)\\n+ [Mocks](#mocks)\\n+ [Where will `SecretProvider` reside?](#where-will-secretprovider-reside)\\n- [Go Services](#go-services)\\n- [C Device Service](#c-device-service)\\n* [Consequences](#consequences)\\n\n\n##Decision\n* [Only Exclusive Secret Stores](#only-exclusive-secret-stores)\\n* [Abstraction Interface](#abstraction-interface)\\n* [Implementation](#implementation)\\n+ [Factory Method and Bootstrap Handler](#factory-method-and-bootstrap-handler)\\n+ [Caching of Secrets](#caching-of-secrets)\\n+ [Insecure Secrets](#insecure-secrets)\\n- [Handling on-the-fly changes to `InsecureSecrets`](#handling-on-the-fly-changes-to-insecuresecrets)\\n+ [Mocks](#mocks)\\n+ [Where will `SecretProvider` reside?](#where-will-secretprovider-reside)\\n- [Go Services](#go-services)\\n- [C Device Service](#c-device-service)\\n* [Consequences](#consequences)\\nThe new `SecretProvider` abstraction defined by this ADR is a combination of the two implementations described above in the [Existing Implementations](#existing-implementations) section.\\n### Only Exclusive Secret Stores\\nTo simplify the `SecretProvider` abstraction, we need to reduce to using only exclusive `SecretStores`. This allows all the APIs to deal with a single `SecretClient`, rather than the split up way we currently have in Application Services. This requires that the current Application Service shared secrets (database credentials) must be copied into each Application Service's exclusive `SecretStore` when it is created.\\nThe challenge is how do we seed static secrets for unknown services when they become known.  As described above in the [Known and Unknown Services](#known-and-unknown-services) section above,  services currently identify themselves for exclusive `SecretStore` creation via the `EDGEX_ADD_SECRETSTORE_TOKENS` environment variable on security-secretstore-setup. This environment variable simply takes a comma separated list of service names.\\n```yaml\\nEDGEX_ADD_SECRETSTORE_TOKENS: \"<service-name1>,<service-name2>\"\\n```\\nIf we expanded this to add an optional list of static secret identifiers for each service, i.e.  `appservice\/redisdb`, the exclusive store could also be seeded with a copy of static shared secrets. In this case the Redis database credentials for the Application Services' shared database. The environment variable name will change to `ADD_SECRETSTORE` now that it is more than just tokens.\\n```yaml\\nADD_SECRETSTORE: \"app-service-xyz[appservice\/redisdb]\"\\n```\\n> *Note: The secret identifier here is the short path to the secret in the existing **appservice**  `SecretStore`. In the above example this expands to the full path of `\/secret\/edgex\/appservice\/redisdb`*\\nThe above example results in the Redis credentials being copied into app-service-xyz's `SecretStore` at `\/secret\/edgex\/app-service-xyz\/redis`.\\nSimilar approach could be taken for Message Bus credentials where a common `SecretStore` is created with the Message Bus credentials saved. The services request the credentials are copied into their exclusive `SecretStore` using `common\/messagebus` as the secret identifier.\\nFull specification for the environment variable's value is a comma separated list of service entries defined as:\\n```\\n<service-name1>[optional list of static secret IDs sperated by ;],<service-name2>[optional list of static secret IDs sperated by ;],...\\n```\\nExample with one service specifying IDs for static secrets and one without static secrets\\n```yaml\\nADD_SECRETSTORE: \"appservice-xyz[appservice\/redisdb; common\/messagebus], appservice-http-export\"\\n```\\nWhen the `ADD_SECRETSTORE` environment variable is processed to create these `SecretStores`, it will copy the specified saved secrets from the initial `SecretStore` into the service's `SecretStore`. This all depends on the completion of database or other credential bootstrapping and the secrets having been stored prior to the environment variable being processed. security-secretstore-setup will need to be refactored to ensure this sequencing.\\n### Abstraction Interface\\nThe following will be the new `SecretProvider` abstraction interface used by all Edgex services\\n```go\\ntype SecretProvider interface {\\n\/\/ Stores new secrets into the service's exclusive SecretStore at the specified path.\\nStoreSecrets(path string, secrets map[string]string) error\\n\/\/ Retrieves secrets from the service's exclusive SecretStore at the specified path.\\nGetSecrets(path string, _ ...string) (map[string]string, error)\\n\/\/ Sets the secrets lastupdated time to current time.\\nSecretsUpdated()\\n\/\/ Returns the secrets last updated time\\nSecretsLastUpdated() time.Time\\n}\\n```\\n> *Note: The `GetDatabaseCredentials` and `GetCertificateKeyPair` APIs have been removed. These are no longer needed since insecure database credentials will no longer be stored in the `DatabaseInfo` configuration and certificate key pairs are secrets like any others. This allows these secrets to be retrieved via the `GetSecrets` API.*\\n### Implementation\\n#### Factory Method and Bootstrap Handler\\nThe factory method and bootstrap handler will follow that currently in the Bootstrap implementation with some tweaks. Rather than putting the two split interfaces into the DIC, it will put just the single interface instance into the DIC. See details in the [Interfaces and factory methods](#interfaces-and-factory-methods) section above under **Existing Implementations**.\\n#### Caching of Secrets\\nSecrets will be cached as they are currently in the Application Service implementation\\n#### Insecure Secrets\\nInsecure Secrets will be handled as they are currently in the Application Service implementation. `DatabaseInfo` configuration will no longer be an option for storing the insecure database credentials. They will be stored in the `InsecureSecrets` configuration only.\\n```toml\\n[Writable.InsecureSecrets]\\n[Writable.InsecureSecrets.DB]\\npath = \"redisdb\"\\n[Writable.InsecureSecrets.DB.Secrets]\\nusername = \"\"\\npassword = \"\"\\n```\\n##### Handling on-the-fly changes to `InsecureSecrets`\\nAll services will need to handle the special processing when `InsecureSecrets` are changed on-the-fly via Consul. Since this will now be a common configuration item in `Writable` it can be handled in `go-mod-bootstrap` along with existing log level processing. This special processing will be taken from App SDK.\\n#### Mocks\\nProper mock of the `SecretProvider` interface will be created with `Mockery` to be used in unit tests. Current mock in App SDK is hand written rather then generated with `Mockery`.\\n#### Where will `SecretProvider` reside?\\n##### Go Services\\nThe final decision to make is where will this new `SecretProvider` abstraction reside? Originally is was assumed that it would reside in `go-mod-secrets`, which seems logical. If we were to attempt this with the implementation including the bootstrap handler, `go-mod-secrets` would have a dependency on `go-mod-bootstrap` which will likely create a circular dependency.\\nRefactoring the existing implementation in `go-mod-bootstrap` and have it reside there now seems to be the best choice.\\n##### C Device Service\\nThe C Device SDK will implement the same `SecretProvider` abstraction, InsecureSercets configuration and the underling `SecretStore` client.\\n### Consequences\\n- All service's will have `Writable.InsecureSecrets` section added to their configuration\\n- `InsecureSecrets` definition will be moved from App SDK to go-mod-bootstrap\\n- Go Device SDK will add the SecretProvider to it's bootstrapping\\n- C Device SDK implementation could be big lift?\\n- ` SecretStore`configuration section will be added to all Device Services\\n- edgex-go services will be modified to use the single `SecretProvider` interface from the DIC in place of current usage of the `GetDatabaseCredentials` and `GetCertificateKeyPair` interfaces.\\n- Calls to `GetDatabaseCredentials` and `GetCertificateKeyPair` will be replaced with calls to `GetSecrets` API and appropriate processing of the returned secrets will be added.\\n- App SDK will be modified to use `GetSecrets` API in place of the `GetDatabaseCredentials` API\\n- App SDK will be modified to use the new `SecretProvider` bootstrap handler\\n- app-service-configurable's configuration profiles as well as all the Application Service examples configurations will be updated to remove the `SecretStoreExclusive` configuration and just use the existing `SecretStore` configuration\\n- security-secretstore-setup will be enhanced as described in the [Only Exclusive Secret Stores](#only-exclusive-secret-stores) section above\\n- Adding new services that need static secrets added to their `SecretStore` requires stopping and restarting all the services. The is because security-secretstore-setup has completed but not stopped. If it is rerun without stopping the other services, there tokens and static secrets will have changed. The planned refactor of `security-secretstore-setup` will attempt to resolve this.\\n- Snaps do not yet support setting the environment variable for adding SecretStore. It is planned for Ireland release.\\n","MatchedIds":[13,946,3142,578,2256],"Predicted":"## Context\nThe project requires a secure and consistent way to handle secrets across various services and components within the system. These secrets are utilized for dynamic configurations, authorization, authentication, and other security-sensitive activities. Existing implementations of secret management in the system have certain gaps and inconsistencies, which we aim to rectify with a unified approach. We need to address the different types of secrets (static and runtime) and ensure that our solution supports both known and unknown services. Additionally, we need to factor in the interfaces and factory methods used in the current bootstrap and App SDK implementations.\n\n## Decision\nWe will implement a centralized secret management solution with the following key aspects:\n\n### Only Exclusive Secret Stores\nAll services will have their exclusive secret stores to prevent the risk of accidental leakage and to enforce the principle of least privilege. Shared secrets between services should be avoided to maintain isolation and minimize impact in case of a breach.\n\n### Abstraction Interface\nDevelop a common abstraction interface for accessing secret stores. This interface will ensure that regardless of the underlying secret provider (e.g., AWS Secrets Manager, HashiCorp Vault), services will have a consistent way to request and use secrets.\n\n### Implementation\nThe implementation will include:\n\n#### Factory Method and Bootstrap Handler\nCreate factory methods and bootstrap handlers to facilitate the instantiation of secret providers based on configuration parameters. These methods will allow seamless replacement or upgrading of secret stores without disrupting the existing service logic.\n\n#### Caching of Secrets\nImplement caching mechanisms for secrets to reduce latency and avoid repetitive network calls. Caching should respect TTL (time-to-live) settings to ensure that only fresh and updated secrets are used.\n\n#### Insecure Secrets\nFor environments where security is not the primary concern (e.g., local development or testing), a configurable option to use a simpler, less secure secret storage (like environment variables or local files) will be provided. These 'insecure secrets' must have clear and conspicuous warnings about their limited security.\n\n### Handling on-the-fly changes to `InsecureSecrets`\nEnsure that the system can handle dynamic changes to the `InsecureSecrets` configuration without requiring a restart. This is critical for maintaining flexibility and agility in various deployments.\n\n### Mocks\nCreate mock implementations compliant with the abstraction interface for use in unit tests and local development. This practice will promote testing in isolation and development without needing access to the actual secret stores.\n\n### Where will `SecretProvider` reside?\nThe unified `SecretProvider` will reside in a common utilities library accessible by all services and applications. This library should be designed to be lightweight and easily integrated into any service within the system.\n\n### Go Services and C Device Service\nThe unified secret management solution will be rolled out to all Go services and C device services, ensuring that the approach is consistent across the different languages and runtime environments.\n\n## Consequences\n- **Security**: The exclusive secret stores will enhance overall security by reducing the risk surface area.\n- **Consistency**: A common abstraction interface will ensure consistent secret management across services.\n- **Flexibility**: Factory methods and bootstrap handlers will provide flexibility in switching secret providers without significant code changes.\n- **Performance**: Secret caching will improve performance by minimizing network calls.\n- **Configurability**: The option to use insecure secrets will support various deployment environments, making the system adaptable to different security requirements.\n- **Maintainability**: Placing `SecretProvider` in a common library will facilitate easier maintenance and updates.","GenTime":"2024-08-19 23:45:52"}
{"File Name":"fundraising-application\/005_URL_Schema.md","Context":"## Context\\nWhen we developed the Fundraising application, we did not pay close attention to the URL schema and ended up with three different styles:\\n* \"Slashy\", pseudo-[REST][1]-style URLs like `\/donation\/add`\\n* \"action-sentences\" like `\/apply-for-membership`\\n* a combination of both like `\/contact\/get-in-touch`\\nWe don't do search engine optimization (SEO) in the form of \"meaningful, localized and stable URLs\", as the main traffic to the donation page comes from banners and we don't have much relevant content to that search engines can index.\\n","Decision":"We will use the \"action-sentence\" style for URLs in the future. They should follow the pattern `verb-noun` or `verb-preposition-noun`.\\nOur reasoning behind the decision:\\n* They convey more information about what the route does, because we can use all verbs of the English language instead of restricting us to `GET` and `POST`.\\n* REST-style URLs are deceiving because our application has no real API and is not explicitly written with a [RESTful][1] architecture.\\n* We can still have a dedicated REST API in the future, by using the `\/api` route.\\n* The sentence style fits better to our use case architecture, which also read more like sentences.\\nWhenever we change a URL, we decide if we need to create a redirect from the old one to the new in the NGinX configuration. GET support is a good indicator for the need for a redirect. If route is more like a functional \"endpoint\" like `donation\/update`, then we don't need a redirect.\\nIf we need to add i18n information to the URL at some point, we will do it with a \"subdirectory prefix\", e.g. `\/de\/apply-for-membership`, `\/en\/apply-for-membership`. The cons listed at https:\/\/support.google.com\/webmasters\/answer\/182192?hl=en do not outweigh the benefits.\\n","tokens":128,"id":1522,"text":"## Context\\nWhen we developed the Fundraising application, we did not pay close attention to the URL schema and ended up with three different styles:\\n* \"Slashy\", pseudo-[REST][1]-style URLs like `\/donation\/add`\\n* \"action-sentences\" like `\/apply-for-membership`\\n* a combination of both like `\/contact\/get-in-touch`\\nWe don't do search engine optimization (SEO) in the form of \"meaningful, localized and stable URLs\", as the main traffic to the donation page comes from banners and we don't have much relevant content to that search engines can index.\\n\n\n##Decision\nWe will use the \"action-sentence\" style for URLs in the future. They should follow the pattern `verb-noun` or `verb-preposition-noun`.\\nOur reasoning behind the decision:\\n* They convey more information about what the route does, because we can use all verbs of the English language instead of restricting us to `GET` and `POST`.\\n* REST-style URLs are deceiving because our application has no real API and is not explicitly written with a [RESTful][1] architecture.\\n* We can still have a dedicated REST API in the future, by using the `\/api` route.\\n* The sentence style fits better to our use case architecture, which also read more like sentences.\\nWhenever we change a URL, we decide if we need to create a redirect from the old one to the new in the NGinX configuration. GET support is a good indicator for the need for a redirect. If route is more like a functional \"endpoint\" like `donation\/update`, then we don't need a redirect.\\nIf we need to add i18n information to the URL at some point, we will do it with a \"subdirectory prefix\", e.g. `\/de\/apply-for-membership`, `\/en\/apply-for-membership`. The cons listed at https:\/\/support.google.com\/webmasters\/answer\/182192?hl=en do not outweigh the benefits.\\n","MatchedIds":[3802,1823,2386,2207,1754],"Predicted":"## Decision\nWe have decided to standardize the URL schema of the Fundraising application to use \"action-sentences\" like `\/apply-for-membership`. \n\n### Rationale\n* It is more user-friendly and easier to read.\n* Despite not focusing on SEO, clear action-based URLs can enhance user navigation.\n* Consistency in URL schema will simplify our codebase and reduce potential maintenance complexity.\n\n### Implementation Steps\n1. Update the routing configuration to convert existing \"Slashy\" and combination URLs to the \"action-sentences\" format.\n2. Implement redirects from old URLs to the new format to prevent breaking existing links and bookmarks.\n3. Update documentation and any internal references to use the new format.\n4. Communicate the changes to relevant stakeholders and teams to ensure a smooth transition.\n\n### Positive Consequences\n* Improved user experience with more readable and intuitive URLs.\n* Simplified URL schema reduces development and maintenance overhead.\n* Clearer URLs can indirectly benefit user comprehension and navigation.\n\n### Negative Consequences\n* Initial implementation effort to convert existing URLs and set up redirects.\n* Temporary disruption during the transition period as users and systems adapt to the new URLs.","GenTime":"2024-08-19 23:45:58"}
{"File Name":"SoundCloudVisualizer\/0004-use-gulp-to-build-source-code.md","Context":"## Context\\nThe application's JS code was loaded via `script` tags in `index.html`. As the project was migrated from Vanilla JS to AngularJS the number of source files that needed to be included grew, and this approach became unwieldy.\\nI looked into how we could bundle our code. At the time of development Grunt and Gulp were the two main task runners available.\\n","Decision":"[Gulp](https:\/\/gulpjs.com\/) will be used to bundle client-side code.\\n","tokens":81,"id":1576,"text":"## Context\\nThe application's JS code was loaded via `script` tags in `index.html`. As the project was migrated from Vanilla JS to AngularJS the number of source files that needed to be included grew, and this approach became unwieldy.\\nI looked into how we could bundle our code. At the time of development Grunt and Gulp were the two main task runners available.\\n\n\n##Decision\n[Gulp](https:\/\/gulpjs.com\/) will be used to bundle client-side code.\\n","MatchedIds":[1680,3827,4940,3428,5259],"Predicted":"## Decision\nWe have decided to use Gulp as our task runner for bundling the application's JS code. Gulp's streaming build system is known for its speed and efficiency, and it uses code over configuration, which provides greater flexibility for managing our growing number of source files. This should simplify our build process and improve performance during development and deployment.","GenTime":"2024-08-19 23:46:01"}
{"File Name":"heptaconnect-docs\/2020-04-30-contracts-and-interfaces.md","Context":"## Context\\nThere has to be a guide how to structure code to allow extendability.\\nWe have to ensure that functionality can be exchanged without interfering with adjacent packages within the package hierarchy.\\n","Decision":"The approved approach is using the language feature type hierarchy.\\nUsing interfaces, traits and contracts (abstract classes) is a good way to structure and allow replacements by ensuring certain behaviors.\\nWe use interfaces when multiple implementation will exist and are unknown at any time for the package that it is introduced in.\\nFor example, we do not know what kind of storage is used within the portal-base, but it will need some kind of storage.\\nTo supply some basic logic for commonly used interfaces we can provide traits for others to implement them easier.\\nWe use contracts similar to interfaces but use their advantages to contain any logic beforehand.\\nThis enables us to add additional code later with a reduced level of changes that can be non-breaking without removing the replacing option.\\nContracts are best without dependencies that have to be given in the constructor as this forces other implementations to follow this pattern regardless whether they need it.\\n","tokens":39,"id":3203,"text":"## Context\\nThere has to be a guide how to structure code to allow extendability.\\nWe have to ensure that functionality can be exchanged without interfering with adjacent packages within the package hierarchy.\\n\n\n##Decision\nThe approved approach is using the language feature type hierarchy.\\nUsing interfaces, traits and contracts (abstract classes) is a good way to structure and allow replacements by ensuring certain behaviors.\\nWe use interfaces when multiple implementation will exist and are unknown at any time for the package that it is introduced in.\\nFor example, we do not know what kind of storage is used within the portal-base, but it will need some kind of storage.\\nTo supply some basic logic for commonly used interfaces we can provide traits for others to implement them easier.\\nWe use contracts similar to interfaces but use their advantages to contain any logic beforehand.\\nThis enables us to add additional code later with a reduced level of changes that can be non-breaking without removing the replacing option.\\nContracts are best without dependencies that have to be given in the constructor as this forces other implementations to follow this pattern regardless whether they need it.\\n","MatchedIds":[1416,5070,1792,3853,2502],"Predicted":"## Decision\nAdopt a modular architecture pattern, where functionality is encapsulated within well-defined modules or packages. Each module should expose only what is necessary through clearly defined public interfaces while keeping the internal implementation private. Additionally, leverage dependency inversion and use design patterns like Strategy or Adapter to allow swapping of implementations without affecting other modules. Comprehensive documentation and consistent use of interface contracts will ensure extendability and low coupling between packages.","GenTime":"2024-08-19 23:46:04"}
{"File Name":"cf-k8s-networking\/0007-maintain-generated-istio.md","Context":"## Context \ud83e\udd14\\nCf-k8s-networking was designed to be integrated with\\n[cf-for-k8s](https:\/\/github.com\/cloudfoundry\/cf-for-k8s\/). The Istio\\ninstallation used to be maintained by\\n[cf-for-k8s](https:\/\/github.com\/cloudfoundry\/cf-for-k8s\/), but the networking\\nteam needed to be able to easily make changes to [Istio](https:\/\/istio.io\/)\\nconfiguration to enable more networking features for [Cloud\\nFoundry](https:\/\/www.cloudfoundry.org\/).\\n","Decision":"We decided to move the scripts to build Istio configuration, and maintain a\\ngenerated Istio configuration within the cf-k8s-networking repository.\\nThe build scripts and `ytt` overlays for Istio live in this repo (links removed\\nas they are no longer relevant or accurate). **UPDATE** This configuration has\\nmoved as a result of [ADR\\n017](.\/0017-moving-istio-configuration-out-of-this-repo.md).\\n","tokens":133,"id":2236,"text":"## Context \ud83e\udd14\\nCf-k8s-networking was designed to be integrated with\\n[cf-for-k8s](https:\/\/github.com\/cloudfoundry\/cf-for-k8s\/). The Istio\\ninstallation used to be maintained by\\n[cf-for-k8s](https:\/\/github.com\/cloudfoundry\/cf-for-k8s\/), but the networking\\nteam needed to be able to easily make changes to [Istio](https:\/\/istio.io\/)\\nconfiguration to enable more networking features for [Cloud\\nFoundry](https:\/\/www.cloudfoundry.org\/).\\n\n\n##Decision\nWe decided to move the scripts to build Istio configuration, and maintain a\\ngenerated Istio configuration within the cf-k8s-networking repository.\\nThe build scripts and `ytt` overlays for Istio live in this repo (links removed\\nas they are no longer relevant or accurate). **UPDATE** This configuration has\\nmoved as a result of [ADR\\n017](.\/0017-moving-istio-configuration-out-of-this-repo.md).\\n","MatchedIds":[2240,631,620,3909,856],"Predicted":"## Decision\nWe will transition the responsibility of managing the Istio installation from the cf-for-k8s team to the cf-k8s-networking team. This will allow the networking team to have full control over Istio configuration and enable them to make necessary changes more efficiently to support additional networking features for Cloud Foundry.\n\nTo facilitate this transition, we will:\n\n1. **Fork the Istio Configuration:** Create a fork of the existing Istio configuration repository from cf-for-k8s to cf-k8s-networking, ensuring that all existing configurations are preserved.\n   \n2. **Establish a Sync Process:** Implement a synchronization process to keep our forked repository up to date with the upstream Istio configuration. This ensures that we can still benefit from updates and improvements made by the cf-for-k8s team while maintaining our custom configurations.\n\n3. **Custom Configuration Management:** Introduce a configuration management system to handle customizations specific to cf-k8s-networking. This will involve setting up a dedicated pipeline for configuration changes, incorporating automated tests to validate configurations before they are applied.\n\n4. **Access Controls:** Define and enforce policies to ensure that only authorized members of the cf-k8s-networking team can make changes to the Istio configuration. Utilize Kubernetes Role-Based Access Control (RBAC) and GitHub branch protection rules to achieve this.\n\n5. **Documentation and Training:** Document the new processes and configurations to ensure seamless knowledge transfer. Provide training sessions to the networking team members to familiarize them with the new responsibilities and workflows.\n\n6. **Monitoring and Alerts:** Implement robust monitoring and alerting mechanisms to track the performance and health of the Istio components. Set up alerts for critical events that might impact the networking functionality of Cloud Foundry.\n\nBy making these changes, cf-k8s-networking will gain the agility and control required to enhance and maintain Istio configurations, ultimately providing a more robust networking solution for Cloud Foundry's needs.","GenTime":"2024-08-19 23:46:12"}
{"File Name":"architecture\/0008-code-owners.md","Context":"## Context\\nWe get contributed a lot of new integrations, new features to integrations and refactors of integrations. The Home Assistant project is honored to receive so many great contributions to our project!\\nUnfortunately, as a contributor, adding oneself as (the, or one of the) code owners of the integration contributed or contributed to, doesn't always happen spontaneously.\\nNot adding oneself as a code owner has drawbacks for the project:\\n- The contributor doesn't \"own\" (in terms of taking responsibility) his code, and thus contribution, in a more formal fashion.\\n- Without being listed as a code owner, our GitHub bot will not notify the contributor, when an issue for the integration is reported, quite possibly affecting his contribution.\\n- Integrations have ended up or may end up with having a single code owner or no code owners at all.\\nAs a result of this:\\n- Bugs are less likely to be resolved in a timely fashion (turn-around time).\\n- Integrations are more prone to break in the future.\\n- Integration with a single code owner:\\n- Do not benefit from multiple code owners being familiar with the integration in terms of code review and general turn-around time.\\n- Become largely unmaintained when the single listed code owner can no longer contribute to the project.\\nWe have quite a few integrations that haven't got multiple code owners or don't have a code owner.\\nDuring the design discussion of this ADR, it also became clear, that the term \"code owner\" has different meanings to our members and contributors. Some interpret it as an honorable mention of contribution; others see it as \"taking responsibility\".\\n","Decision":"Code ownership for an integration defined:\\nThe willingness of a contributor to try, at best effort, to maintain the integration. Providing the intention for handling issues, providing bug fixes, or other contributions to the integration one is listed on as a code owner.\\n### Rules\\nIn order to support having (multiple) code owners for integration, to raise the quality and interaction on integration in our codebase, we have a set of rules (exceptions are in the next chapter).\\nFor the following cases, adding oneself as a code owner is required:\\n- When contributing a new integration.\\n- When contributing a new platform to an integration.\\n- When contributing a new feature to an integration.\\n- When contributing a significant refactor or rewrite of an integration.\\nContributions to our integrations, in the above-listed scopes, without having the contributor listed or added as the code owner, is no longer accepted.\\n### Exceptions\\nSome exceptions are in place, to prevent contributors to become demotivated to contribute; and are mainly based around smaller, low-impact contributions.\\nIn the following cases, code ownership may be omitted:\\n- Contributions that solely provides a bug fix(es).\\n- Contributions that only provide additional unit test(s).\\n- Contributions to integrations marked as \"internal\". These integrations are code owned by the Home Assistant core team.\\n- Contributions refactoring across multiple integrations, caused by changes to our core codebase. E.g., due to changes to the used platforms.\\n- Small or low impact contributions to an integration. A currently active code owner for the integration or a Home Assistant code reviewer can decide it may be omitted.\\n- The contributor pro-actively rejects to be listed as a code owner; however, a currently active code owner is willing to accept and take code ownership for the contribution provided by the contributor.\\nCode owner(s) and Home Assistant code reviewers are encouraged to ask a contributor to join an integration code ownership, even when the contribution matches one of the exceptions above.\\n### Withdrawing as a code owner\\nWithdrawing code ownership can happen, and it is sad to see an active code owner leaving the project.\\nA code owner may open up a PR to remove oneself as a code owner. However, this should only be accepted after the last contribution to the integration, made by the contributor, is released in a stable Home Assistant release.\\n","tokens":343,"id":1431,"text":"## Context\\nWe get contributed a lot of new integrations, new features to integrations and refactors of integrations. The Home Assistant project is honored to receive so many great contributions to our project!\\nUnfortunately, as a contributor, adding oneself as (the, or one of the) code owners of the integration contributed or contributed to, doesn't always happen spontaneously.\\nNot adding oneself as a code owner has drawbacks for the project:\\n- The contributor doesn't \"own\" (in terms of taking responsibility) his code, and thus contribution, in a more formal fashion.\\n- Without being listed as a code owner, our GitHub bot will not notify the contributor, when an issue for the integration is reported, quite possibly affecting his contribution.\\n- Integrations have ended up or may end up with having a single code owner or no code owners at all.\\nAs a result of this:\\n- Bugs are less likely to be resolved in a timely fashion (turn-around time).\\n- Integrations are more prone to break in the future.\\n- Integration with a single code owner:\\n- Do not benefit from multiple code owners being familiar with the integration in terms of code review and general turn-around time.\\n- Become largely unmaintained when the single listed code owner can no longer contribute to the project.\\nWe have quite a few integrations that haven't got multiple code owners or don't have a code owner.\\nDuring the design discussion of this ADR, it also became clear, that the term \"code owner\" has different meanings to our members and contributors. Some interpret it as an honorable mention of contribution; others see it as \"taking responsibility\".\\n\n\n##Decision\nCode ownership for an integration defined:\\nThe willingness of a contributor to try, at best effort, to maintain the integration. Providing the intention for handling issues, providing bug fixes, or other contributions to the integration one is listed on as a code owner.\\n### Rules\\nIn order to support having (multiple) code owners for integration, to raise the quality and interaction on integration in our codebase, we have a set of rules (exceptions are in the next chapter).\\nFor the following cases, adding oneself as a code owner is required:\\n- When contributing a new integration.\\n- When contributing a new platform to an integration.\\n- When contributing a new feature to an integration.\\n- When contributing a significant refactor or rewrite of an integration.\\nContributions to our integrations, in the above-listed scopes, without having the contributor listed or added as the code owner, is no longer accepted.\\n### Exceptions\\nSome exceptions are in place, to prevent contributors to become demotivated to contribute; and are mainly based around smaller, low-impact contributions.\\nIn the following cases, code ownership may be omitted:\\n- Contributions that solely provides a bug fix(es).\\n- Contributions that only provide additional unit test(s).\\n- Contributions to integrations marked as \"internal\". These integrations are code owned by the Home Assistant core team.\\n- Contributions refactoring across multiple integrations, caused by changes to our core codebase. E.g., due to changes to the used platforms.\\n- Small or low impact contributions to an integration. A currently active code owner for the integration or a Home Assistant code reviewer can decide it may be omitted.\\n- The contributor pro-actively rejects to be listed as a code owner; however, a currently active code owner is willing to accept and take code ownership for the contribution provided by the contributor.\\nCode owner(s) and Home Assistant code reviewers are encouraged to ask a contributor to join an integration code ownership, even when the contribution matches one of the exceptions above.\\n### Withdrawing as a code owner\\nWithdrawing code ownership can happen, and it is sad to see an active code owner leaving the project.\\nA code owner may open up a PR to remove oneself as a code owner. However, this should only be accepted after the last contribution to the integration, made by the contributor, is released in a stable Home Assistant release.\\n","MatchedIds":[3554,1614,2964,1563,2091],"Predicted":"## Decision\n\nTo address the issues related to code ownership and ensure the sustainability and quality of integrations, we have decided to implement the following policy:\n\n### Policy:\n1. **Mandatory Code Owner Assignment**:\n   - Contributors must add themselves as code owners for any new integrations or significant changes to existing integrations.\n   - In cases where contributors are reluctant to be listed as code owners, they should explicitly state their reasons during the pull request process.\n\n2. **Multiple Code Owners**:\n   - Whenever possible, integrations should have multiple code owners to enhance review, maintenance, and general support.\n   - Reviewers should encourage contributors to add co-maintainers or additional code owners when possible.\n\n3. **Education and Clarification**:\n   - Update documentation to clearly define the role and responsibilities of a \"code owner\" to avoid any ambiguity.\n   - Conduct regular onboarding sessions or provide resource materials to educate new contributors about the importance of code ownership.\n\n4. **Automated Checks**:\n   - Implement automated checks in the GitHub pull request process to ensure the code owner file is updated with every significant change to an integration.\n   - The GitHub bot should flag pull requests without code owner updates for reviewer attention.\n\n5. **Maintenance Audits**:\n   - Periodic audits will be performed to identify integrations without code owners or those with single code owners.\n   - Engage with the community to find volunteer code owners for such integrations, or consider deprecating unmaintained integrations if no volunteers come forward.\n\n### Positive Consequences\n- **Increased Responsibility and Ownership**: Contributors will take more responsibility for their code, leading to higher quality contributions.\n- **Improved Bug Resolution**: Code owners will be notified of issues, leading to faster bug fixes and improved user experience.\n- **Better Maintenance**: Multiple code owners will ensure better maintenance and reduced risk of integrations becoming unmaintained.\n- **Clear Expectations**: Educating contributors on the role of a code owner will set clear expectations and enhance community collaboration.\n\n### Negative Consequences\n- **Potential Resistance**: Some contributors may be hesitant to take on the responsibility of being a code owner.\n- **Administrative Overhead**: Implementing and maintaining automated checks and audits may require additional resources.\n- **Complexity in Documentation**: Explaining the nuances of code ownership may add complexity to contributor documentation.\n\nBy implementing these policies, we aim to foster a responsible community of contributors, ensure the longevity and stability of integrations, and maintain high standards for the Home Assistant project.","GenTime":"2024-08-19 23:46:22"}
{"File Name":"amf-core\/0009-source-information-node-storing-source-locations.md","Context":"## Context\\nThe custom AMF validator needs to show the location of the file from which each error was generated.\\nGiven the current state of the amf model and emission of jsonld, there was no way to obtain the location of a specific node.\\n","Decision":"A new node was defined as a field in BaseUnit call BaseUnitSourceInformation, which has the necessary information to obtain the source location of any node.\\nInternally, this node has two fields, one that stores the root location, and another that stores LocationInformation nodes which contain alternative locations with the ids of all the elements parsed from that location.\\nA new render option was included making the emission of this node to jsonld optional and not activated by default.\\nAn alternative solution was to serialize SourceLocation annotation in each node, but this leads to a 25% or more increase in size of the resulting jsonld, as the paths are stored in a redundant fashion.\\n","tokens":53,"id":405,"text":"## Context\\nThe custom AMF validator needs to show the location of the file from which each error was generated.\\nGiven the current state of the amf model and emission of jsonld, there was no way to obtain the location of a specific node.\\n\n\n##Decision\nA new node was defined as a field in BaseUnit call BaseUnitSourceInformation, which has the necessary information to obtain the source location of any node.\\nInternally, this node has two fields, one that stores the root location, and another that stores LocationInformation nodes which contain alternative locations with the ids of all the elements parsed from that location.\\nA new render option was included making the emission of this node to jsonld optional and not activated by default.\\nAn alternative solution was to serialize SourceLocation annotation in each node, but this leads to a 25% or more increase in size of the resulting jsonld, as the paths are stored in a redundant fashion.\\n","MatchedIds":[1393,4119,4101,2215,2386],"Predicted":"## Decision\nTo display the location of the file from which each error originates, we will introduce an annotation mechanism within the AMF model. This annotation will store the file path and line number for each node during the parsing phase. The following steps will be implemented to achieve this:\n\n1. **Node Annotation**: Modify the AMF parser to annotate each node with its corresponding file path and line number during the parsing process.\n2. **Model Update**: Ensure the annotations are preserved throughout the model transformations and validations.\n3. **Error Emission**: During the emission of errors, append the file path and line number from the annotations to the error messages.\n4. **Ensure Consistency**: While resolving the model or generating JSON-LD, ensure that these annotations are either preserved or appropriately transformed to maintain accurate error locations.\n5. **Documentation**: Update the documentation to reflect these changes and ensure that developers are aware of how to access and utilize these annotations for error reporting.\n\nThis approach ensures that the custom AMF validator can provide precise and meaningful error messages with the location information of the file, enhancing the debugging and validation process.","GenTime":"2024-08-19 23:46:28"}
{"File Name":"ditto\/DADR-0003-do-not-interrupt-threads.md","Context":"## Context\\nSonarQube [RSPEC-2142](https:\/\/rules.sonarsource.com\/java\/tag\/multi-threading\/RSPEC-2142) complains when we catch\\n`InterruptedException` and not call `Thread.currentThread().interrupt()` in the catch block.\\nSimply calling `Thread.currentThread().interrupt()` to silence SonarQube is dangerous. Due to the way JUnit reuses\\nthreads, a call to `Thread.interrupt()` breaks the build in unpredictable ways, several tests after the actual site of\\ninterruption.\\n","Decision":"We will ignore [RSPEC-2142](https:\/\/rules.sonarsource.com\/java\/tag\/multi-threading\/RSPEC-2142).\\n","tokens":114,"id":777,"text":"## Context\\nSonarQube [RSPEC-2142](https:\/\/rules.sonarsource.com\/java\/tag\/multi-threading\/RSPEC-2142) complains when we catch\\n`InterruptedException` and not call `Thread.currentThread().interrupt()` in the catch block.\\nSimply calling `Thread.currentThread().interrupt()` to silence SonarQube is dangerous. Due to the way JUnit reuses\\nthreads, a call to `Thread.interrupt()` breaks the build in unpredictable ways, several tests after the actual site of\\ninterruption.\\n\n\n##Decision\nWe will ignore [RSPEC-2142](https:\/\/rules.sonarsource.com\/java\/tag\/multi-threading\/RSPEC-2142).\\n","MatchedIds":[68,3507,3863,2343,2888],"Predicted":"## Decision\nInstead of blindly adhering to SonarQube's suggestion to call `Thread.currentThread().interrupt()`, a deliberate approach should be taken. In test code, where thread interrupts can lead to unpredictable failures due to JUnit's thread reuse, it is safer to log the interruption and proceed without calling `Thread.currentThread().interrupt()`. For production code, the recommended approach of calling `Thread.currentThread().interrupt()` should still be followed, provided that the context ensures it is safe to do so \u2014 meaning that it is handled appropriately downstream or at the end of the thread lifecycle.\n\nTo implement this:\n1. **Testing Code**:\n    - Log the `InterruptedException` and avoid calling `Thread.currentThread().interrupt()` to prevent unpredictable behaviors in subsequent tests.\n    - Example:\n      ```java\n      try {\n          \/\/ some code that may throw InterruptedException\n      } catch (InterruptedException e) {\n          \/\/ Log the exception\n          log.warn(\"Thread interrupted\", e);\n      }\n      ```\n\n2. **Production Code**:\n   - Adhere to the recommended practice of calling `Thread.currentThread().interrupt()` after logging or handling the `InterruptedException`.\n   - Example:\n      ```java\n      try {\n          \/\/ some code that may throw InterruptedException\n      } catch (InterruptedException e) {\n          \/\/ Log the exception\n          log.warn(\"Thread interrupted\", e);\n          \/\/ Restore interrupted status\n          Thread.currentThread().interrupt();\n      }\n      ```\n\nBy delineating the handling strategies for test and production code, we can maintain code quality and integrity while avoiding the pitfalls that arise from thread interruptions in JUnit testing scenarios.\n\n### Status\nAccepted\n\n","GenTime":"2024-08-19 23:46:35"}
{"File Name":"xebikart-infra\/002-use-rabbitmq-with-mqtt-plugin-to-make-devices-communicate-with-each-other.md","Context":"## Context and Problem Statement\\nWe want a message broker so the devices and other applications can communicate with the backend.\\nWhat broker and protocol should we use?\\n## Decision Drivers <!-- optional -->\\n- Applicability regarding IoT projects : low-resources clients, etc\\n- Possibility to use it to stream frames\/images coming from cars cameras\\n- Ease of deployment on Kubernetes\\n- Existing knowledge of the team\\n","Decision":"- Applicability regarding IoT projects : low-resources clients, etc\\n- Possibility to use it to stream frames\/images coming from cars cameras\\n- Ease of deployment on Kubernetes\\n- Existing knowledge of the team\\nChosen option: **[RabbitMQ](TODO) with [MQTT plugin](https:\/\/www.rabbitmq.com\/mqtt.html)**, because:\\n- It is already well-known among the team\\n- It has some [existing \"official\" Helm chart](https:\/\/github.com\/helm\/charts\/tree\/master\/stable\/rabbitmq)\\n- It seems like a good fit to iterate fast\\nWe **do not exclude switching to another MQTT broker such as VerneMQ in the future**, depending on our ability to dsitribute it cleanly on Kubernetes.\\nWe also **do not exclude using another broker such as Kafka or NATS for appropriate use cases**.\\n","tokens":84,"id":979,"text":"## Context and Problem Statement\\nWe want a message broker so the devices and other applications can communicate with the backend.\\nWhat broker and protocol should we use?\\n## Decision Drivers <!-- optional -->\\n- Applicability regarding IoT projects : low-resources clients, etc\\n- Possibility to use it to stream frames\/images coming from cars cameras\\n- Ease of deployment on Kubernetes\\n- Existing knowledge of the team\\n\n\n##Decision\n- Applicability regarding IoT projects : low-resources clients, etc\\n- Possibility to use it to stream frames\/images coming from cars cameras\\n- Ease of deployment on Kubernetes\\n- Existing knowledge of the team\\nChosen option: **[RabbitMQ](TODO) with [MQTT plugin](https:\/\/www.rabbitmq.com\/mqtt.html)**, because:\\n- It is already well-known among the team\\n- It has some [existing \"official\" Helm chart](https:\/\/github.com\/helm\/charts\/tree\/master\/stable\/rabbitmq)\\n- It seems like a good fit to iterate fast\\nWe **do not exclude switching to another MQTT broker such as VerneMQ in the future**, depending on our ability to dsitribute it cleanly on Kubernetes.\\nWe also **do not exclude using another broker such as Kafka or NATS for appropriate use cases**.\\n","MatchedIds":[1575,786,4694,2309,4747],"Predicted":"## Decision\nChosen option: **MQTT with Eclipse Mosquitto**.\n\n**Reasoning:**\n* **Applicability regarding IoT projects**: \n  - MQTT is highly suitable for IoT projects as it is a lightweight protocol, designed for low-resources clients, and provides efficient bandwidth usage.\n  - It supports Quality of Service (QoS) levels which are essential for reliable communication in IoT environments.\n* **Possibility to stream frames\/images coming from car cameras**:\n  - While MQTT is generally used for small messages, it is capable of transferring larger payloads such as images, especially if fragmented appropriately.\n  - Alternative streaming protocols might be considered in conjunction for high-fidelity image streaming, but MQTT provides a good starting point.\n* **Ease of deployment on Kubernetes**:\n  - Eclipse Mosquitto is a popular MQTT broker that is lightweight, scalable and easy to deploy on Kubernetes.\n  - There are readily available Helm charts and Docker images for Eclipse Mosquitto, simplifying the deployment process.\n* **Existing knowledge of the team**:\n  - Eclipse Mosquitto is well documented and widely used, making it easier for the team to get up to speed if they don't already have experience with it.\n\n### Positive Consequences\n- The chosen solution will provide a robust, lightweight, and efficient protocol for IoT communication.\n- Ease of deployment would allow for quick setup and scaling within a Kubernetes environment.\n- Team can leverage existing knowledge and community support for fast problem resolution and best practice implementations.\n\n### Negative Consequences\n- Potential need to implement additional handling for large payloads like images.\n- Streaming high-fidelity video might require integrating other protocols or specialized broker solutions for optimal performance.","GenTime":"2024-08-19 23:46:41"}
{"File Name":"interlok\/0006-workflow-callback.md","Context":"## Context and Problem Statement\\nWhen you enable [Dead Letter Queues](https:\/\/docs.aws.amazon.com\/AWSSimpleQueueService\/latest\/SQSDeveloperGuide\/sqs-dead-letter-queues.html) in SQS; the contract is that messages read from a source queue are failed over to the dead letter queue once the redrive policy is fired (i.e. after a max number of attempts to deliver the message). From testing, this is predicated on the fact that the message is not deleted. Of course, our sqs-polling-consumer deletes the message after submitting it to the workflow.\\nSince we can't rely on the JMS semantics either (since onMessage() traditionally doesn't throw exceptions, and RuntimeExceptions aren't propagated by Interlok) we then have to think about some way of having an asynchronous callback.\\n","Decision":"Chosen option: Modify AdaptrisMessageListener to have callbacks.\\n","tokens":171,"id":2343,"text":"## Context and Problem Statement\\nWhen you enable [Dead Letter Queues](https:\/\/docs.aws.amazon.com\/AWSSimpleQueueService\/latest\/SQSDeveloperGuide\/sqs-dead-letter-queues.html) in SQS; the contract is that messages read from a source queue are failed over to the dead letter queue once the redrive policy is fired (i.e. after a max number of attempts to deliver the message). From testing, this is predicated on the fact that the message is not deleted. Of course, our sqs-polling-consumer deletes the message after submitting it to the workflow.\\nSince we can't rely on the JMS semantics either (since onMessage() traditionally doesn't throw exceptions, and RuntimeExceptions aren't propagated by Interlok) we then have to think about some way of having an asynchronous callback.\\n\n\n##Decision\nChosen option: Modify AdaptrisMessageListener to have callbacks.\\n","MatchedIds":[2309,786,1523,2888,1799],"Predicted":"## Decision\n\n**Implement a Custom Error Handling and Callback Mechanism**\n\nTo handle the proper redrive policy of messages to the Dead Letter Queue (DLQ) while ensuring the sqs-polling-consumer deletes messages only when they are successfully processed, we will implement a custom error handling and callback mechanism. This decision is made to ensure that messages are only deleted after successful processing, and any failures are correctly handled and retried as per the redrive policy.\n\n### Implementation Steps\n\n1. **Custom Error Handling**: Implement custom error handling within the sqs-polling-consumer to intercept any processing failures. Instead of deleting the message immediately after submission to the workflow, defer deletion based on the success of processing.\n\n2. **Asynchronous Callback**: Develop an asynchronous callback mechanism that interacts with the workflow execution to determine the result of message processing. The callback will notify the sqs-polling-consumer whether the message processing was successful or failed.\n\n3. **Retry Logic**: Modify the sqs-polling-consumer to include retry logic. If message processing fails, the message will not be deleted and will instead be retried based on the specified redrive policy. After reaching the maximum retry attempts, the message will be moved to the DLQ.\n\n4. **Logging and Monitoring**: Implement robust logging and monitoring to track message processing, retries, and failures. This will help in identifying issues and ensuring that messages are handled appropriately.\n\n### Positive Consequences\n\n- **Enhanced Reliability**: Messages will only be deleted after successful processing, ensuring data integrity and reliability.\n- **Correct Application of Redrive Policy**: Messages will be retried according to the specified redrive policy, and only moved to the DLQ after exhausting retries.\n- **Improved Error Handling**: Custom error handling and asynchronous callbacks provide better control over message processing failures.\n\n### Negative Consequences\n\n- **Increased Complexity**: Implementing custom error handling and asynchronous callbacks adds complexity to the sqs-polling-consumer.\n- **Development Overhead**: Additional development time and effort will be required to implement and test the new mechanisms.\n- **Potential Latency**: Asynchronous callbacks and retries may introduce some latency in message processing.\n\nBy implementing this custom error handling and callback mechanism, we aim to ensure that the dead letter queue functionality works as intended and messages are reliably processed and moved to the DLQ when necessary.","GenTime":"2024-08-19 23:46:51"}
{"File Name":"my-notes\/prettier.md","Context":"### Context\\nPrettier integrates with well with vscode but conflicts with Standard\\n### Decision\\nReplace Standard with Prettier\\n","Decision":"Replace Standard with Prettier\\n","tokens":26,"id":5087,"text":"### Context\\nPrettier integrates with well with vscode but conflicts with Standard\\n### Decision\\nReplace Standard with Prettier\\n\n\n##Decision\nReplace Standard with Prettier\\n","MatchedIds":[3321,5255,4226,1408,3521],"Predicted":"### Context\nPrettier integrates well with VSCode but conflicts with Standard.\n\n### Decision\nWe will replace Standard with Prettier.\n\nTo ensure a smooth transition and maintain code consistency, we will take the following steps:\n1. Remove Standard and its related configurations from all repositories.\n2. Install Prettier and set up its configuration files in the repositories.\n3. Update documentation to reflect the use of Prettier for code formatting.\n4. Set up VSCode workspace settings to enforce Prettier as the default formatter.\n5. Configure pre-commit hooks to run Prettier automatically before committing code.\n6. Adjust continuous integration pipelines to reject commits that do not adhere to the Prettier formatting rules.\n\n### Positive Consequences\n* Consistent code formatting across all projects, leveraging Prettier's widespread adoption in the developer community.\n* Better integration and experience within VSCode, enhancing developer productivity.\n* Reduced conflicts in formatting rules, leading to fewer merge conflicts and code review issues.\n\n### Negative Consequences\n* Time and effort required to migrate existing projects from Standard to Prettier.\n* Developers may need to adjust to the formatting preferences enforced by Prettier.","GenTime":"2024-08-19 23:46:58"}
{"File Name":"holochain-rust\/0010-write-our-own-dht-implementation.md","Context":"## Context\\nFor the Go based prototype we used IPFS' Kademlia DHT implementations with edits.\\nSince we are switching over to Rust, we can't easily continue using that code base.\\nMore importantly, there are too many Holochain specific additions to a vanilla Kademlia DHT, as well as other possible implementations of achieving entry resilience, routing, our forthcoming World Model, CRDT handling, gossip and our graph properties via linking, so it does not make sense to change existing implementations but instead roll our own.\\n","Decision":"We will build our own DHT implementation in Rust from scratch.\\n","tokens":110,"id":1494,"text":"## Context\\nFor the Go based prototype we used IPFS' Kademlia DHT implementations with edits.\\nSince we are switching over to Rust, we can't easily continue using that code base.\\nMore importantly, there are too many Holochain specific additions to a vanilla Kademlia DHT, as well as other possible implementations of achieving entry resilience, routing, our forthcoming World Model, CRDT handling, gossip and our graph properties via linking, so it does not make sense to change existing implementations but instead roll our own.\\n\n\n##Decision\nWe will build our own DHT implementation in Rust from scratch.\\n","MatchedIds":[1495,1500,1979,1956,1982],"Predicted":"## Decision\nWe will develop a custom DHT implementation tailored specifically for Holochain's unique requirements in Rust. The new DHT will incorporate Holochain-specific features such as entry resilience, advanced routing mechanisms, CRDT handling, and gossip protocols. We will leverage the following steps:\n\n1. **Design Phase**: Clearly define the functional and non-functional requirements for our DHT.\n2. **Modular Architecture**: Implement a modular architecture that allows different components like entry resilience, CRDTs, and gossip to be developed and maintained independently.\n3. **Integration of Rust Ecosystem**: Utilize Rust\u2019s libraries and ecosystem to enhance performance and security.\n4. **Iterative Development**: Approach the development in iterative cycles, ensuring each module is comprehensively tested before integration.\n5. **Documentation and Community Involvement**: Maintain thorough documentation and actively involve the community for feedback and contributions.\n\nBy taking this approach, we ensure that the custom DHT is fully aligned with Holochain\u2019s objectives and future-ready for additional features and improvements.","GenTime":"2024-08-19 23:47:02"}
{"File Name":"cosmos-sdk\/adr-021-protobuf-query-encoding.md","Context":"## Context\\nThis ADR is a continuation of the motivation, design, and context established in\\n[ADR 019](.\/adr-019-protobuf-state-encoding.md) and\\n[ADR 020](.\/adr-020-protobuf-transaction-encoding.md), namely, we aim to design the\\nProtocol Buffer migration path for the client-side of the Cosmos SDK.\\nThis ADR continues from [ADD 020](.\/adr-020-protobuf-transaction-encoding.md)\\nto specify the encoding of queries.\\n","Decision":"### Custom Query Definition\\nModules define custom queries through a protocol buffers `service` definition.\\nThese `service` definitions are generally associated with and used by the\\nGRPC protocol. However, the protocol buffers specification indicates that\\nthey can be used more generically by any request\/response protocol that uses\\nprotocol buffer encoding. Thus, we can use `service` definitions for specifying\\ncustom ABCI queries and even reuse a substantial amount of the GRPC infrastructure.\\nEach module with custom queries should define a service canonically named `Query`:\\n```protobuf\\n\/\/ x\/bank\/types\/types.proto\\nservice Query {\\nrpc QueryBalance(QueryBalanceParams) returns (cosmos_sdk.v1.Coin) { }\\nrpc QueryAllBalances(QueryAllBalancesParams) returns (QueryAllBalancesResponse) { }\\n}\\n```\\n#### Handling of Interface Types\\nModules that use interface types and need true polymorphism generally force a\\n`oneof` up to the app-level that provides the set of concrete implementations of\\nthat interface that the app supports. While app's are welcome to do the same for\\nqueries and implement an app-level query service, it is recommended that modules\\nprovide query methods that expose these interfaces via `google.protobuf.Any`.\\nThere is a concern on the transaction level that the overhead of `Any` is too\\nhigh to justify its usage. However for queries this is not a concern, and\\nproviding generic module-level queries that use `Any` does not preclude apps\\nfrom also providing app-level queries that return use the app-level `oneof`s.\\nA hypothetical example for the `gov` module would look something like:\\n```protobuf\\n\/\/ x\/gov\/types\/types.proto\\nimport \"google\/protobuf\/any.proto\";\\nservice Query {\\nrpc GetProposal(GetProposalParams) returns (AnyProposal) { }\\n}\\nmessage AnyProposal {\\nProposalBase base = 1;\\ngoogle.protobuf.Any content = 2;\\n}\\n```\\n### Custom Query Implementation\\nIn order to implement the query service, we can reuse the existing [gogo protobuf](https:\/\/github.com\/cosmos\/gogoproto)\\ngrpc plugin, which for a service named `Query` generates an interface named\\n`QueryServer` as below:\\n```go\\ntype QueryServer interface {\\nQueryBalance(context.Context, *QueryBalanceParams) (*types.Coin, error)\\nQueryAllBalances(context.Context, *QueryAllBalancesParams) (*QueryAllBalancesResponse, error)\\n}\\n```\\nThe custom queries for our module are implemented by implementing this interface.\\nThe first parameter in this generated interface is a generic `context.Context`,\\nwhereas querier methods generally need an instance of `sdk.Context` to read\\nfrom the store. Since arbitrary values can be attached to `context.Context`\\nusing the `WithValue` and `Value` methods, the Cosmos SDK should provide a function\\n`sdk.UnwrapSDKContext` to retrieve the `sdk.Context` from the provided\\n`context.Context`.\\nAn example implementation of `QueryBalance` for the bank module as above would\\nlook something like:\\n```go\\ntype Querier struct {\\nKeeper\\n}\\nfunc (q Querier) QueryBalance(ctx context.Context, params *types.QueryBalanceParams) (*sdk.Coin, error) {\\nbalance := q.GetBalance(sdk.UnwrapSDKContext(ctx), params.Address, params.Denom)\\nreturn &balance, nil\\n}\\n```\\n### Custom Query Registration and Routing\\nQuery server implementations as above would be registered with `AppModule`s using\\na new method `RegisterQueryService(grpc.Server)` which could be implemented simply\\nas below:\\n```go\\n\/\/ x\/bank\/module.go\\nfunc (am AppModule) RegisterQueryService(server grpc.Server) {\\ntypes.RegisterQueryServer(server, keeper.Querier{am.keeper})\\n}\\n```\\nUnderneath the hood, a new method `RegisterService(sd *grpc.ServiceDesc, handler interface{})`\\nwill be added to the existing `baseapp.QueryRouter` to add the queries to the custom\\nquery routing table (with the routing method being described below).\\nThe signature for this method matches the existing\\n`RegisterServer` method on the GRPC `Server` type where `handler` is the custom\\nquery server implementation described above.\\nGRPC-like requests are routed by the service name (ex. `cosmos_sdk.x.bank.v1.Query`)\\nand method name (ex. `QueryBalance`) combined with `\/`s to form a full\\nmethod name (ex. `\/cosmos_sdk.x.bank.v1.Query\/QueryBalance`). This gets translated\\ninto an ABCI query as `custom\/cosmos_sdk.x.bank.v1.Query\/QueryBalance`. Service handlers\\nregistered with `QueryRouter.RegisterService` will be routed this way.\\nBeyond the method name, GRPC requests carry a protobuf encoded payload, which maps naturally\\nto `RequestQuery.Data`, and receive a protobuf encoded response or error. Thus\\nthere is a quite natural mapping of GRPC-like rpc methods to the existing\\n`sdk.Query` and `QueryRouter` infrastructure.\\nThis basic specification allows us to reuse protocol buffer `service` definitions\\nfor ABCI custom queries substantially reducing the need for manual decoding and\\nencoding in query methods.\\n### GRPC Protocol Support\\nIn addition to providing an ABCI query pathway, we can easily provide a GRPC\\nproxy server that routes requests in the GRPC protocol to ABCI query requests\\nunder the hood. In this way, clients could use their host languages' existing\\nGRPC implementations to make direct queries against Cosmos SDK app's using\\nthese `service` definitions. In order for this server to work, the `QueryRouter`\\non `BaseApp` will need to expose the service handlers registered with\\n`QueryRouter.RegisterService` to the proxy server implementation. Nodes could\\nlaunch the proxy server on a separate port in the same process as the ABCI app\\nwith a command-line flag.\\n### REST Queries and Swagger Generation\\n[grpc-gateway](https:\/\/github.com\/grpc-ecosystem\/grpc-gateway) is a project that\\ntranslates REST calls into GRPC calls using special annotations on service\\nmethods. Modules that want to expose REST queries should add `google.api.http`\\nannotations to their `rpc` methods as in this example below.\\n```protobuf\\n\/\/ x\/bank\/types\/types.proto\\nservice Query {\\nrpc QueryBalance(QueryBalanceParams) returns (cosmos_sdk.v1.Coin) {\\noption (google.api.http) = {\\nget: \"\/x\/bank\/v1\/balance\/{address}\/{denom}\"\\n};\\n}\\nrpc QueryAllBalances(QueryAllBalancesParams) returns (QueryAllBalancesResponse) {\\noption (google.api.http) = {\\nget: \"\/x\/bank\/v1\/balances\/{address}\"\\n};\\n}\\n}\\n```\\ngrpc-gateway will work directly against the GRPC proxy described above which will\\ntranslate requests to ABCI queries under the hood. grpc-gateway can also\\ngenerate Swagger definitions automatically.\\nIn the current implementation of REST queries, each module needs to implement\\nREST queries manually in addition to ABCI querier methods. Using the grpc-gateway\\napproach, there will be no need to generate separate REST query handlers, just\\nquery servers as described above as grpc-gateway handles the translation of protobuf\\nto REST as well as Swagger definitions.\\nThe Cosmos SDK should provide CLI commands for apps to start GRPC gateway either in\\na separate process or the same process as the ABCI app, as well as provide a\\ncommand for generating grpc-gateway proxy `.proto` files and the `swagger.json`\\nfile.\\n### Client Usage\\nThe gogo protobuf grpc plugin generates client interfaces in addition to server\\ninterfaces. For the `Query` service defined above we would get a `QueryClient`\\ninterface like:\\n```go\\ntype QueryClient interface {\\nQueryBalance(ctx context.Context, in *QueryBalanceParams, opts ...grpc.CallOption) (*types.Coin, error)\\nQueryAllBalances(ctx context.Context, in *QueryAllBalancesParams, opts ...grpc.CallOption) (*QueryAllBalancesResponse, error)\\n}\\n```\\nVia a small patch to gogo protobuf ([gogo\/protobuf#675](https:\/\/github.com\/gogo\/protobuf\/pull\/675))\\nwe have tweaked the grpc codegen to use an interface rather than concrete type\\nfor the generated client struct. This allows us to also reuse the GRPC infrastructure\\nfor ABCI client queries.\\n1Context`will receive a new method`QueryConn`that returns a`ClientConn`\\nthat routes calls to ABCI queries\\nClients (such as CLI methods) will then be able to call query methods like this:\\n```go\\nclientCtx := client.NewContext()\\nqueryClient := types.NewQueryClient(clientCtx.QueryConn())\\nparams := &types.QueryBalanceParams{addr, denom}\\nresult, err := queryClient.QueryBalance(gocontext.Background(), params)\\n```\\n### Testing\\nTests would be able to create a query client directly from keeper and `sdk.Context`\\nreferences using a `QueryServerTestHelper` as below:\\n```go\\nqueryHelper := baseapp.NewQueryServerTestHelper(ctx)\\ntypes.RegisterQueryServer(queryHelper, keeper.Querier{app.BankKeeper})\\nqueryClient := types.NewQueryClient(queryHelper)\\n```\\n","tokens":108,"id":810,"text":"## Context\\nThis ADR is a continuation of the motivation, design, and context established in\\n[ADR 019](.\/adr-019-protobuf-state-encoding.md) and\\n[ADR 020](.\/adr-020-protobuf-transaction-encoding.md), namely, we aim to design the\\nProtocol Buffer migration path for the client-side of the Cosmos SDK.\\nThis ADR continues from [ADD 020](.\/adr-020-protobuf-transaction-encoding.md)\\nto specify the encoding of queries.\\n\n\n##Decision\n### Custom Query Definition\\nModules define custom queries through a protocol buffers `service` definition.\\nThese `service` definitions are generally associated with and used by the\\nGRPC protocol. However, the protocol buffers specification indicates that\\nthey can be used more generically by any request\/response protocol that uses\\nprotocol buffer encoding. Thus, we can use `service` definitions for specifying\\ncustom ABCI queries and even reuse a substantial amount of the GRPC infrastructure.\\nEach module with custom queries should define a service canonically named `Query`:\\n```protobuf\\n\/\/ x\/bank\/types\/types.proto\\nservice Query {\\nrpc QueryBalance(QueryBalanceParams) returns (cosmos_sdk.v1.Coin) { }\\nrpc QueryAllBalances(QueryAllBalancesParams) returns (QueryAllBalancesResponse) { }\\n}\\n```\\n#### Handling of Interface Types\\nModules that use interface types and need true polymorphism generally force a\\n`oneof` up to the app-level that provides the set of concrete implementations of\\nthat interface that the app supports. While app's are welcome to do the same for\\nqueries and implement an app-level query service, it is recommended that modules\\nprovide query methods that expose these interfaces via `google.protobuf.Any`.\\nThere is a concern on the transaction level that the overhead of `Any` is too\\nhigh to justify its usage. However for queries this is not a concern, and\\nproviding generic module-level queries that use `Any` does not preclude apps\\nfrom also providing app-level queries that return use the app-level `oneof`s.\\nA hypothetical example for the `gov` module would look something like:\\n```protobuf\\n\/\/ x\/gov\/types\/types.proto\\nimport \"google\/protobuf\/any.proto\";\\nservice Query {\\nrpc GetProposal(GetProposalParams) returns (AnyProposal) { }\\n}\\nmessage AnyProposal {\\nProposalBase base = 1;\\ngoogle.protobuf.Any content = 2;\\n}\\n```\\n### Custom Query Implementation\\nIn order to implement the query service, we can reuse the existing [gogo protobuf](https:\/\/github.com\/cosmos\/gogoproto)\\ngrpc plugin, which for a service named `Query` generates an interface named\\n`QueryServer` as below:\\n```go\\ntype QueryServer interface {\\nQueryBalance(context.Context, *QueryBalanceParams) (*types.Coin, error)\\nQueryAllBalances(context.Context, *QueryAllBalancesParams) (*QueryAllBalancesResponse, error)\\n}\\n```\\nThe custom queries for our module are implemented by implementing this interface.\\nThe first parameter in this generated interface is a generic `context.Context`,\\nwhereas querier methods generally need an instance of `sdk.Context` to read\\nfrom the store. Since arbitrary values can be attached to `context.Context`\\nusing the `WithValue` and `Value` methods, the Cosmos SDK should provide a function\\n`sdk.UnwrapSDKContext` to retrieve the `sdk.Context` from the provided\\n`context.Context`.\\nAn example implementation of `QueryBalance` for the bank module as above would\\nlook something like:\\n```go\\ntype Querier struct {\\nKeeper\\n}\\nfunc (q Querier) QueryBalance(ctx context.Context, params *types.QueryBalanceParams) (*sdk.Coin, error) {\\nbalance := q.GetBalance(sdk.UnwrapSDKContext(ctx), params.Address, params.Denom)\\nreturn &balance, nil\\n}\\n```\\n### Custom Query Registration and Routing\\nQuery server implementations as above would be registered with `AppModule`s using\\na new method `RegisterQueryService(grpc.Server)` which could be implemented simply\\nas below:\\n```go\\n\/\/ x\/bank\/module.go\\nfunc (am AppModule) RegisterQueryService(server grpc.Server) {\\ntypes.RegisterQueryServer(server, keeper.Querier{am.keeper})\\n}\\n```\\nUnderneath the hood, a new method `RegisterService(sd *grpc.ServiceDesc, handler interface{})`\\nwill be added to the existing `baseapp.QueryRouter` to add the queries to the custom\\nquery routing table (with the routing method being described below).\\nThe signature for this method matches the existing\\n`RegisterServer` method on the GRPC `Server` type where `handler` is the custom\\nquery server implementation described above.\\nGRPC-like requests are routed by the service name (ex. `cosmos_sdk.x.bank.v1.Query`)\\nand method name (ex. `QueryBalance`) combined with `\/`s to form a full\\nmethod name (ex. `\/cosmos_sdk.x.bank.v1.Query\/QueryBalance`). This gets translated\\ninto an ABCI query as `custom\/cosmos_sdk.x.bank.v1.Query\/QueryBalance`. Service handlers\\nregistered with `QueryRouter.RegisterService` will be routed this way.\\nBeyond the method name, GRPC requests carry a protobuf encoded payload, which maps naturally\\nto `RequestQuery.Data`, and receive a protobuf encoded response or error. Thus\\nthere is a quite natural mapping of GRPC-like rpc methods to the existing\\n`sdk.Query` and `QueryRouter` infrastructure.\\nThis basic specification allows us to reuse protocol buffer `service` definitions\\nfor ABCI custom queries substantially reducing the need for manual decoding and\\nencoding in query methods.\\n### GRPC Protocol Support\\nIn addition to providing an ABCI query pathway, we can easily provide a GRPC\\nproxy server that routes requests in the GRPC protocol to ABCI query requests\\nunder the hood. In this way, clients could use their host languages' existing\\nGRPC implementations to make direct queries against Cosmos SDK app's using\\nthese `service` definitions. In order for this server to work, the `QueryRouter`\\non `BaseApp` will need to expose the service handlers registered with\\n`QueryRouter.RegisterService` to the proxy server implementation. Nodes could\\nlaunch the proxy server on a separate port in the same process as the ABCI app\\nwith a command-line flag.\\n### REST Queries and Swagger Generation\\n[grpc-gateway](https:\/\/github.com\/grpc-ecosystem\/grpc-gateway) is a project that\\ntranslates REST calls into GRPC calls using special annotations on service\\nmethods. Modules that want to expose REST queries should add `google.api.http`\\nannotations to their `rpc` methods as in this example below.\\n```protobuf\\n\/\/ x\/bank\/types\/types.proto\\nservice Query {\\nrpc QueryBalance(QueryBalanceParams) returns (cosmos_sdk.v1.Coin) {\\noption (google.api.http) = {\\nget: \"\/x\/bank\/v1\/balance\/{address}\/{denom}\"\\n};\\n}\\nrpc QueryAllBalances(QueryAllBalancesParams) returns (QueryAllBalancesResponse) {\\noption (google.api.http) = {\\nget: \"\/x\/bank\/v1\/balances\/{address}\"\\n};\\n}\\n}\\n```\\ngrpc-gateway will work directly against the GRPC proxy described above which will\\ntranslate requests to ABCI queries under the hood. grpc-gateway can also\\ngenerate Swagger definitions automatically.\\nIn the current implementation of REST queries, each module needs to implement\\nREST queries manually in addition to ABCI querier methods. Using the grpc-gateway\\napproach, there will be no need to generate separate REST query handlers, just\\nquery servers as described above as grpc-gateway handles the translation of protobuf\\nto REST as well as Swagger definitions.\\nThe Cosmos SDK should provide CLI commands for apps to start GRPC gateway either in\\na separate process or the same process as the ABCI app, as well as provide a\\ncommand for generating grpc-gateway proxy `.proto` files and the `swagger.json`\\nfile.\\n### Client Usage\\nThe gogo protobuf grpc plugin generates client interfaces in addition to server\\ninterfaces. For the `Query` service defined above we would get a `QueryClient`\\ninterface like:\\n```go\\ntype QueryClient interface {\\nQueryBalance(ctx context.Context, in *QueryBalanceParams, opts ...grpc.CallOption) (*types.Coin, error)\\nQueryAllBalances(ctx context.Context, in *QueryAllBalancesParams, opts ...grpc.CallOption) (*QueryAllBalancesResponse, error)\\n}\\n```\\nVia a small patch to gogo protobuf ([gogo\/protobuf#675](https:\/\/github.com\/gogo\/protobuf\/pull\/675))\\nwe have tweaked the grpc codegen to use an interface rather than concrete type\\nfor the generated client struct. This allows us to also reuse the GRPC infrastructure\\nfor ABCI client queries.\\n1Context`will receive a new method`QueryConn`that returns a`ClientConn`\\nthat routes calls to ABCI queries\\nClients (such as CLI methods) will then be able to call query methods like this:\\n```go\\nclientCtx := client.NewContext()\\nqueryClient := types.NewQueryClient(clientCtx.QueryConn())\\nparams := &types.QueryBalanceParams{addr, denom}\\nresult, err := queryClient.QueryBalance(gocontext.Background(), params)\\n```\\n### Testing\\nTests would be able to create a query client directly from keeper and `sdk.Context`\\nreferences using a `QueryServerTestHelper` as below:\\n```go\\nqueryHelper := baseapp.NewQueryServerTestHelper(ctx)\\ntypes.RegisterQueryServer(queryHelper, keeper.Querier{app.BankKeeper})\\nqueryClient := types.NewQueryClient(queryHelper)\\n```\\n","MatchedIds":[21,1979,4650,1982,1442],"Predicted":"## Decision\n### Query Encoding\n\nThe encoding of queries will follow the same guiding principles established in ADRs 019 and 020, focusing on interoperability, extensibility, and deterministic encoding. The main points of this decision are:\n\n1. **Query Request and Response Types**:\n   - All query request and response types will be defined using Protocol Buffers (`.proto` files).\n   - This guarantees a consistent, language-agnostic way of specifying query structures.\n\n2. **Standardized Queries**:\n   - A set of standard query types and endpoints will be defined for core modules of the Cosmos SDK.\n   - These standard queries will cover common data retrieval needs such as account balances, transaction history, and blockchain state.\n\n3. **Custom Queries for Applications**:\n   - Applications and modules can define custom query types using Protocol Buffers.\n   - Custom query types must be registered with the application's codec to ensure they are properly recognized by the client.\n\n4. **gRPC for Transport**:\n   - We will use gRPC as the transport protocol for queries.\n   - This provides a high-performance, cross-platform solution with built-in support for Protocol Buffers.\n\n5. **Query Encoding**:\n   - Queries will be serialized using Protocol Buffers' binary encoding.\n   - Deterministic serialization via [ADR 027](.\/adr-027-deterministic-protobuf-serialization.md) will be enforced to ensure consistency.\n\n6. **Backward Compatibility**:\n   - The encoding of queries will maintain backward compatibility with existing JSON-based queries to ensure a smooth migration path.\n   - Legacy clients must be supported during the transition period to avoid disrupting the ecosystem.\n\n7. **Extended Metadata**:\n   - Query responses will include extended metadata where applicable, such as pagination information for results that span multiple pages.\n   - This metadata will help clients handle responses in a more structured and user-friendly way.\n\n8. **Error Handling**:\n   - Errors in query processing will be encoded using Protocol Buffers messages.\n   - Standardized error messages will be defined, providing clarity and consistency in error reporting.\n\n### Implementation Plan\n\n- **Phase 1**: Define the core query types and endpoints, update the Cosmos SDK modules to implement these new query types, and establish basic gRPC support.\n- **Phase 2**: Implement custom query support for application-specific needs, ensuring proper registration and encoding mechanisms.\n- **Phase 3**: Transition existing queries to the new Protocol Buffer-based system, ensuring backward compatibility and a smooth migration for all clients.\n\n### Sample `.proto` Definition\n\nHere\u2019s an example of a simple query and response type definition using Protocol Buffers:\n\n```proto\nsyntax = \"proto3\";\n\npackage cosmos_sdk.v1;\n\n\/\/ Query request type for account balance\nmessage QueryBalanceRequest {\n    string address = 1;\n}\n\n\/\/ Query response type for account balance\nmessage QueryBalanceResponse {\n    string address = 1;\n    string denom = 2;\n    int64 amount = 3;\n}\n\n\/\/ Define the gRPC query service\nservice QueryService {\n    rpc QueryBalance(QueryBalanceRequest) returns (QueryBalanceResponse);\n}\n```\n\n### Conclusion\n\nBy adopting Protocol Buffers for query encoding, we aim to enhance interoperability, ensure consistency, and lay a more robust foundation for future improvements in the Cosmos SDK. This decision will facilitate a smoother migration path while providing the flexibility needed for custom applications to define their query types.","GenTime":"2024-08-19 23:47:15"}
{"File Name":"libelektra\/error_handling.md","Context":"## Problem\\nThere are ambiguous cases where the same return value can refer to multiple problems:\\n- name modifications which can be either invalid name or locking the key name\\n- getting values of (non-)binary keys\\n","Decision":"- Update documentation in `doc\/dev\/error-*` and link to them in the documentation\\nfor the module `kdb`\\n- Add second channel for getting information about errors\\n- Return error codes directly from functions where failures are expected, e.g. `kdbGet`, `keySetName`\\n- Harmonize return values from all functions and move error reporting to second channel\\n- Binary metadata vs flag #4194\\n","tokens":44,"id":1292,"text":"## Problem\\nThere are ambiguous cases where the same return value can refer to multiple problems:\\n- name modifications which can be either invalid name or locking the key name\\n- getting values of (non-)binary keys\\n\n\n##Decision\n- Update documentation in `doc\/dev\/error-*` and link to them in the documentation\\nfor the module `kdb`\\n- Add second channel for getting information about errors\\n- Return error codes directly from functions where failures are expected, e.g. `kdbGet`, `keySetName`\\n- Harmonize return values from all functions and move error reporting to second channel\\n- Binary metadata vs flag #4194\\n","MatchedIds":[1289,1310,1309,1293,1291],"Predicted":"## Decision\nIntroduce distinct error codes and return values to clearly differentiate the types of errors:\n- Add specific error codes for invalid name modifications, e.g., `ELEKTRA_ERROR_INVALID_NAME`.\n- Add specific error codes for key name locking errors, e.g., `ELEKTRA_ERROR_KEYNAME_LOCK`.\n- Differentiate between binary and non-binary key values by providing distinct functions for retrieval:\n  - `kdbGetBinaryKeyValue` for binary keys.\n  - `kdbGetStringKeyValue` for non-binary (string) keys.\n- Ensure that each function checks the key type before proceeding and returns appropriate error codes if the key type is mismatched (e.g., retrieving a binary value from a non-binary key).\n\nDocument the new error codes and functions in the API documentation to guide developers on the expected use and handling of these cases.\n","GenTime":"2024-08-19 23:47:19"}
{"File Name":"ionic-dummy-repo\/ADR-001.md","Context":"## Context\\nGiven the speed at which state-management libraries, preferences and trends change, should this starter seed should assert an opinion via dependencies in this regard?\\nDevelopers bring different skills and ideas to the table, should they be free to make a decision on the best tool for the job, based upon requirements, complexity etc?\\n### Who Was Involved in This Decision\\n- Alex Ward\\n- Chris Weight\\n### Relates To\\n- N\/A\\n","Decision":"- Alex Ward\\n- Chris Weight\\n### Relates To\\n- N\/A\\nThe Hybrid seed template will _not_ express an opinion via pre-determined dependencies on what state management frameworks (if any) should be used. This can be decided on a per-project basis. Though there are positives and negatives either way, it is felt that the ability to rapidly implement changes to approach over the course of time and projects is a powerful plus.\\n","tokens":93,"id":701,"text":"## Context\\nGiven the speed at which state-management libraries, preferences and trends change, should this starter seed should assert an opinion via dependencies in this regard?\\nDevelopers bring different skills and ideas to the table, should they be free to make a decision on the best tool for the job, based upon requirements, complexity etc?\\n### Who Was Involved in This Decision\\n- Alex Ward\\n- Chris Weight\\n### Relates To\\n- N\/A\\n\n\n##Decision\n- Alex Ward\\n- Chris Weight\\n### Relates To\\n- N\/A\\nThe Hybrid seed template will _not_ express an opinion via pre-determined dependencies on what state management frameworks (if any) should be used. This can be decided on a per-project basis. Though there are positives and negatives either way, it is felt that the ability to rapidly implement changes to approach over the course of time and projects is a powerful plus.\\n","MatchedIds":[1631,3827,3031,389,4361],"Predicted":"## Decision\nWe will adopt an unopinionated approach regarding state-management libraries in this starter seed. Given the rapid evolution of state-management solutions and the varying preferences among developers, the seed project will not include a specific state-management library as a dependency. \n\n### Reasons:\n1. **Flexibility:** By not enforcing a specific state-management library, we empower developers to choose the most suitable tool based on their project\u2019s requirements and their own expertise.\n2. **Avoids Obsolescence:** State-management trends can change quickly, and what is popular today may not be preferred tomorrow. An unopinionated seed allows the project to stay relevant longer without frequent updates to match current trends.\n3. **Encourages Best Practices:** Developers can evaluate and implement the most efficient state-management solution without being constrained by pre-existing dependencies. This should lead to higher quality and more maintainable codebases.\n\n### Positive Consequences\n- Developers have the freedom to select tools that best fit their project's specific needs and can leverage their own expertise.\n- Reduced maintenance burden as the starter seed does not need to be updated to keep pace with state-management library trends.\n- Increased adaptability to different project contexts and requirements.\n\n### Negative Consequences\n- Developers may need to spend additional time evaluating and integrating their chosen state-management library.\n- Potential variability in state-management approaches could lead to inconsistencies across different projects unless best practices are documented and followed.\n\n### Mitigation Steps\n- Provide a documentation guide with examples of integrating popular state-management libraries like Redux, MobX, and Zustand.\n- Encourage sharing of best practices within the developer community to promote consistency and knowledge-sharing.\n\nThis approach ensures that the starter seed remains flexible, modern, and valuable to a wide array of projects and developers.","GenTime":"2024-08-19 23:47:25"}
{"File Name":"CCS-Architecture-Decision-Records\/0004-language-choices-for-new-development.md","Context":"## Context\\nWhen developing a new system, there are many possible language choices available.\\n-CCS uses a mixture of different languages for existing systems including:\\n- PHP for the Supplier Registration Service and Contracts Finder\\n- C# for MISO\\n- Java for the CCS ID and DigiTS services\\n- Python for the Digital Marketplace\\nOther services in government use a mixture of PHP, Java, Python, Ruby, Scala and GO.\\nPicking one standard language for all CCS services could restrict the pool of potential suppliers we could interact with when building new services. Conversely, allowing an unlimited selection of languages can make future management and support of services more complex and expensive.\\nAgreeing a selection of acceptable languages will allow the greatest flexibility in supplier choice, but also helps minimise future ongoing support costs.\\nAny decision we make about languages and frameworks should consider:\\n- Skills available in the current team(s) - what are the current team(s) comfortable using? What do they have experience in?\\n- Skills of a future team - is there a large enough pool of suppliers and contractors who could support and maintain the services in the future?\\n- External toolkits and modules - are there useful toolkits and modules which could reduce development effort (eg the GOV.UK Frontend Toolkit)\\n- Cost - what is the cost of developing in this way? Are there licence costs?\\n- Hosting - would picking a language restrict hosting options?\\nWe should also consider the Service Manual guide on [choosing technology](https:\/\/www.gov.uk\/service-manual\/technology\/choosing-technology-an-introduction).\\n","Decision":"New services developed by CCS should use the following languages:\\n- Java\\n- Python\\n- Ruby\\n- Javascript\\nDigital services should also use HTML5, CSS3 and JSON as required.\\nSalesforce work may continue to use the Salesforce Apex language, but where possible, we should use external web service components to reduce the volume of Apex in use.\\nNew services which do not use these languages will require an Architectural Decision to explain why.\\nAs the software development industry changes quickly, we will re-evaluate this decision in 6 months, taking account of current market trends and industry moves.\\nThis decision does not apply to services comprised entirely of Software as a Service (SaaS) solutions, and does not affect systems which already exist. It also does not cover the development of native mobile applications.\\n","tokens":329,"id":1000,"text":"## Context\\nWhen developing a new system, there are many possible language choices available.\\n-CCS uses a mixture of different languages for existing systems including:\\n- PHP for the Supplier Registration Service and Contracts Finder\\n- C# for MISO\\n- Java for the CCS ID and DigiTS services\\n- Python for the Digital Marketplace\\nOther services in government use a mixture of PHP, Java, Python, Ruby, Scala and GO.\\nPicking one standard language for all CCS services could restrict the pool of potential suppliers we could interact with when building new services. Conversely, allowing an unlimited selection of languages can make future management and support of services more complex and expensive.\\nAgreeing a selection of acceptable languages will allow the greatest flexibility in supplier choice, but also helps minimise future ongoing support costs.\\nAny decision we make about languages and frameworks should consider:\\n- Skills available in the current team(s) - what are the current team(s) comfortable using? What do they have experience in?\\n- Skills of a future team - is there a large enough pool of suppliers and contractors who could support and maintain the services in the future?\\n- External toolkits and modules - are there useful toolkits and modules which could reduce development effort (eg the GOV.UK Frontend Toolkit)\\n- Cost - what is the cost of developing in this way? Are there licence costs?\\n- Hosting - would picking a language restrict hosting options?\\nWe should also consider the Service Manual guide on [choosing technology](https:\/\/www.gov.uk\/service-manual\/technology\/choosing-technology-an-introduction).\\n\n\n##Decision\nNew services developed by CCS should use the following languages:\\n- Java\\n- Python\\n- Ruby\\n- Javascript\\nDigital services should also use HTML5, CSS3 and JSON as required.\\nSalesforce work may continue to use the Salesforce Apex language, but where possible, we should use external web service components to reduce the volume of Apex in use.\\nNew services which do not use these languages will require an Architectural Decision to explain why.\\nAs the software development industry changes quickly, we will re-evaluate this decision in 6 months, taking account of current market trends and industry moves.\\nThis decision does not apply to services comprised entirely of Software as a Service (SaaS) solutions, and does not affect systems which already exist. It also does not cover the development of native mobile applications.\\n","MatchedIds":[1652,4761,4041,1182,472],"Predicted":"## Decision\nTo balance flexibility in supplier choice and manageability of future services, we have decided to standardize on a select number of programming languages for all new CCS development projects. The following languages have been chosen based on their current usage within CCS, prevalent industry adoption, and alignment with the considerations outlined:\n\n1. **Python**:\n    - Widely used for the Digital Marketplace.\n    - Strong libraries and frameworks for web development, data science, and automation.\n    - Large pool of skilled developers.\n    - Cost-effective in terms of no licensing fees and open-source ecosystem.\n    - Supported by various hosting providers.\n\n2. **Java**:\n    - Used for CCS ID and DigiTS services.\n    - Robust ecosystem with mature libraries and enterprise-level applications.\n    - Strong community and support.\n    - Applicable to a variety of application types including web, mobile, and backend services.\n    - Compatible with many hosting solutions.\n\n3. **C#**:\n    - Used for MISO.\n    - Excellent integration with Microsoft's ecosystem which is beneficial for enterprise applications.\n    - Good developer tools and frameworks such as .NET.\n    - Supported on multiple hosting platforms including Azure.\n\n4. **JavaScript\/Node.js**:\n    - Growing popularity in government projects.\n    - Well-suited for frontend and backend development.\n    - Large community and numerous libraries and frameworks.\n    - Flexible hosting options.\n\nTo summarize, Python, Java, C#, and JavaScript\/Node.js will be our preferred languages moving forward. This decision allows us to leverage existing skills within the teams, ensures a wide pool of available developers, and maintains cost-effective development and hosting practices. Other languages like PHP, Ruby, Scala, and GO may be considered on a case-by-case basis with sufficient justification related to specific project requirements. \n\nBy narrowing our focus to a select set of languages, we can streamline development processes, make future support more manageable, and maintain alignment with current CCS and broader government technology strategies.","GenTime":"2024-08-19 23:47:33"}
{"File Name":"adr\/ADR-7-standards-repository.md","Context":"# Context\\n* Is the [decentraland\/standards](https:\/\/github.com\/decentraland\/standards) repository still useful?\\n* Does it make sense to have ADR and standards?\\n* How does it play with [decentraland\/proposals](https:\/\/github.com\/decentraland\/proposals)?\\n# Options\\n* Archive both repositories, leaving only [decentraland\/adr](https:\/\/github.com\/decentraland\/adr)\\n* \u2705 Archive [decentraland\/standards](https:\/\/github.com\/decentraland\/standards), leaving [decentraland\/adr](https:\/\/github.com\/decentraland\/adr) and [decentraland\/proposals](https:\/\/github.com\/decentraland\/proposals) untouched.\\n* Only use [decentraland\/adr](https:\/\/github.com\/decentraland\/adr)\\n# Decision\\n* Archive [decentraland\/standards](https:\/\/github.com\/decentraland\/standards)\\n* Keep: [decentraland\/adr](https:\/\/github.com\/decentraland\/adr) and [decentraland\/proposals](https:\/\/github.com\/decentraland\/proposals) untouched.\\nBecause:\\n* `proposals` is a public repository where the community can contribute with new ideas and improvements. It has a formal review process that can ve leveraged by anyone.\\n* `standards` is not actively maintained, it has a strict approval process. Most of the standards can be also defined as ADR, eliminating the need of maintain both repositories.\\nDate: 2020-10-16\\n","Decision":"* Archive [decentraland\/standards](https:\/\/github.com\/decentraland\/standards)\\n* Keep: [decentraland\/adr](https:\/\/github.com\/decentraland\/adr) and [decentraland\/proposals](https:\/\/github.com\/decentraland\/proposals) untouched.\\nBecause:\\n* `proposals` is a public repository where the community can contribute with new ideas and improvements. It has a formal review process that can ve leveraged by anyone.\\n* `standards` is not actively maintained, it has a strict approval process. Most of the standards can be also defined as ADR, eliminating the need of maintain both repositories.\\nDate: 2020-10-16\\n","tokens":338,"id":4610,"text":"# Context\\n* Is the [decentraland\/standards](https:\/\/github.com\/decentraland\/standards) repository still useful?\\n* Does it make sense to have ADR and standards?\\n* How does it play with [decentraland\/proposals](https:\/\/github.com\/decentraland\/proposals)?\\n# Options\\n* Archive both repositories, leaving only [decentraland\/adr](https:\/\/github.com\/decentraland\/adr)\\n* \u2705 Archive [decentraland\/standards](https:\/\/github.com\/decentraland\/standards), leaving [decentraland\/adr](https:\/\/github.com\/decentraland\/adr) and [decentraland\/proposals](https:\/\/github.com\/decentraland\/proposals) untouched.\\n* Only use [decentraland\/adr](https:\/\/github.com\/decentraland\/adr)\\n# Decision\\n* Archive [decentraland\/standards](https:\/\/github.com\/decentraland\/standards)\\n* Keep: [decentraland\/adr](https:\/\/github.com\/decentraland\/adr) and [decentraland\/proposals](https:\/\/github.com\/decentraland\/proposals) untouched.\\nBecause:\\n* `proposals` is a public repository where the community can contribute with new ideas and improvements. It has a formal review process that can ve leveraged by anyone.\\n* `standards` is not actively maintained, it has a strict approval process. Most of the standards can be also defined as ADR, eliminating the need of maintain both repositories.\\nDate: 2020-10-16\\n\n\n##Decision\n* Archive [decentraland\/standards](https:\/\/github.com\/decentraland\/standards)\\n* Keep: [decentraland\/adr](https:\/\/github.com\/decentraland\/adr) and [decentraland\/proposals](https:\/\/github.com\/decentraland\/proposals) untouched.\\nBecause:\\n* `proposals` is a public repository where the community can contribute with new ideas and improvements. It has a formal review process that can ve leveraged by anyone.\\n* `standards` is not actively maintained, it has a strict approval process. Most of the standards can be also defined as ADR, eliminating the need of maintain both repositories.\\nDate: 2020-10-16\\n","MatchedIds":[1669,1563,3554,1614,36],"Predicted":"## Decision\nWe have decided to archive the [decentraland\/standards](https:\/\/github.com\/decentraland\/standards) repository while keeping the [decentraland\/adr](https:\/\/github.com\/decentraland\/adr) and [decentraland\/proposals](https:\/\/github.com\/decentraland\/proposals) repositories untouched.\n\n### Reasoning\n1. **Lack of Active Maintenance on Standards Repository**: The `standards` repository is not actively maintained. Continuing to keep it may lead to outdated or inconsistent documentation which can cause confusion.\n2. **Redundancy with ADR**: Most standards can be effectively defined as Architectural Decision Records (ADRs). Maintaining both creates redundancy and inconsistency.\n3. **Community-Driven Proposals**: The `proposals` repository serves as a platform where the community can contribute ideas and improvements. It incorporates a formal review process, ensuring broader engagement and refinement of ideas.\n\n### Positive Consequences\n- **Streamlined Documentation**: Reduces redundancy and ensures that all standards and architectural decisions are documented in a single, coherent repository.\n- **Increased Community Engagement**: Leveraging the `proposals` repository ensures community involvement and utilizes the review processes already in place.\n- **Improved Maintainability**: Focusing efforts on two repositories instead of three makes it easier to maintain and update documentation.\n\n### Negative Consequences\n- **Transition Effort**: Existing relevant information in the `standards` repository will need to be migrated to the `adr` repository, requiring some initial effort.\n- **Possible Resistance**: There may be resistance from stakeholders used to the `standards` repository format.\n\n### Date\nThis decision was made on 2023-10-12.\n\n### Action Items\n1. **Archive the standards repository**: Perform the necessary steps to archive the repository.\n2. **Migrate relevant content**: Migrate any necessary standards documentation currently in the `standards` repository to the `adr` repository.\n3. **Notify Stakeholders**: Communicate this change to all relevant stakeholders to ensure they are aware of the new process and repositories.\n\nBy consolidating our documentation and focusing on community-driven proposals along with ADRs, we aim to streamline our processes and improve the clarity and maintainability of our architectural decisions and standards.","GenTime":"2024-08-19 23:47:40"}
{"File Name":"adr-generation-tool\/rate_limit_adr_0001.md","Context":"## Context\\nContext for rate limiting pattern\\n","Decision":"Prosa about the decision in the ADD\\n","tokens":9,"id":5142,"text":"## Context\\nContext for rate limiting pattern\\n\n\n##Decision\nProsa about the decision in the ADD\\n","MatchedIds":[4497,4271,5070,2359,1180],"Predicted":"## Decision\nWe will use the [Token Bucket](https:\/\/en.wikipedia.org\/wiki\/Token_bucket) algorithm for rate limiting. This algorithm allows for a flexible rate limiting strategy that can handle bursty traffic, which is essential for maintaining performance and handling peak loads without overwhelming our system.\n\n### Key Considerations\n* **Elasticity**: The Token Bucket algorithm allows a degree of elasticity by permitting bursts of traffic, which matches our need to handle variable load patterns.\n* **Simplicity**: The algorithm is straightforward to implement and can be integrated with minimal overhead.\n* **Extensibility**: It can easily be extended to work in a distributed environment if the need arises, e.g., by using shared storage like Redis for token buckets across multiple nodes.\n\n### Implementation Details\n* **Initialization**: Each user or client will have an individual token bucket initialized with a predefined number of tokens and a refill rate.\n* **Token Allocation**: Rate-limited actions will consume tokens from the bucket. If the bucket has enough tokens, the action proceeds; otherwise, it is denied or delayed.\n* **Refill Mechanism**: Tokens are replenished at a fixed rate, ensuring the rate limit over a period.\n\n### Tools and Frameworks\n* We will leverage [Redis](https:\/\/redis.io\/) for managing distributed token buckets, which ensures consistency and scalability.\n* For client-side and server-side implementation, we will utilize [Rate Limiter Flex](https:\/\/github.com\/animir\/node-rate-limiter-flexible) for Node.js, which supports a token bucket algorithm and integrates seamlessly with Redis.\n\n### Monitoring and Alerting\n* We will set up monitoring and alerting using [Prometheus](https:\/\/prometheus.io\/) and [Grafana](https:\/\/grafana.com\/) to track rate-limiting metrics such as rejected requests, current bucket levels, and refill rates.\n* Alerts will be configured to notify when rate limits are frequently reached, indicating potential issues or misuse.\n\nBy adopting this strategy, we aim to ensure robust and efficient rate limiting across our services, maintaining system stability and performance.","GenTime":"2024-08-19 23:47:48"}
{"File Name":"pottery\/0004-store-shards-in-time-stamped-directories-and-files.md","Context":"## Context\\nA project history can cover many years, with many small shards being recorded.  If all the shard files are stored in the same directory, the directory will be difficult to work with when many shards have been posted.  Performance might suffer on older file systems.\\n","Decision":"Store shard files in subdirectories named after year and then year-and-month, and name the files after the date and time.\\n","tokens":56,"id":2508,"text":"## Context\\nA project history can cover many years, with many small shards being recorded.  If all the shard files are stored in the same directory, the directory will be difficult to work with when many shards have been posted.  Performance might suffer on older file systems.\\n\n\n##Decision\nStore shard files in subdirectories named after year and then year-and-month, and name the files after the date and time.\\n","MatchedIds":[2510,672,2517,2215,3784],"Predicted":"## Decision\nWe will organize the shard files into subdirectories based on the year and month they were created. Each subdirectory will be named in the format \"YYYY\/MM\". This hierarchical structure will make it easier to manage and navigate through the files, and will help mitigate any potential performance issues on older file systems. \n\nAdditionally, we will implement automated processes to create these subdirectories as needed and ensure that shards are stored in the correct location.","GenTime":"2024-08-19 23:47:51"}
{"File Name":"search-ui-extensions\/ShowViewedByCustomer.md","Context":"## Context and Problem Statement\\n<!-- Quick 2-3 sentence background of the User Story -->\\nThe `ViewedByCustomer` component needs to be added on each result template, and therefore could be accidentally missed on one or more, especially if a new template is added at a later time. This would create an inconsistent view of what content the customer has viewed. - From JIRA\\n---\\n## Decision Drivers <!-- optional -->\\n### Context\\nThe main decision drivers were to be able to add the ViewedByCustomer component to each result, without adding it a second time, when the option in the UserActions component is true.\\n<!-- Number these so that they are easier to reference in the following section -->\\n### Decisions\\n1. Need to choose when to edit the results (i.e. need an event)\\n1. Ensure the `ViewedByCustomer` component is properly added to each result template\\n1. Ensure that if a template already has the `ViewedByCustomer` component that it won't add a second component\\n1. There should be an option whether or not to perform that aforementioned actions with the component\\n---\\n","Decision":"### Context\\nThe main decision drivers were to be able to add the ViewedByCustomer component to each result, without adding it a second time, when the option in the UserActions component is true.\\n<!-- Number these so that they are easier to reference in the following section -->\\n### Decisions\\n1. Need to choose when to edit the results (i.e. need an event)\\n1. Ensure the `ViewedByCustomer` component is properly added to each result template\\n1. Ensure that if a template already has the `ViewedByCustomer` component that it won't add a second component\\n1. There should be an option whether or not to perform that aforementioned actions with the component\\n---\\n-   [Option 1] - Leverage the `newResultsDisplayed` event, and loop over every result, performing further action.\\n-   [Option 2] - Leverage the `newResultDisplayed` event, and perform further action.\\n**Decision 2** - Properly adding the ViewedByDocument Component\\n-   [Option 1] - Add the component using `<div class=\"CoveoViewedByCustomer\">`.\\n-   [Option 2] - Add the component using the `ViewedByCustomer` constructor.\\n**Decision 3** - Ensure we don't add the template a second time\\n-   [Option 1] - Query the results `HTMLElement` using the `getElementsByClassName` method.\\n-   [Option 2] - Query the results `HTMLElement` using the `querySelectorAll` method.\\n**Decision 4** - There should be an option whether or not to add the component\\n-   [Option 1] - Have the option be false by default.\\n-   [Option 2] - Have the option be true by default.\\n---\\n#### Decision 1: [Option 2]\\nThere are two reason behind this decision selection: First the `newResultsDisplayed` option wasn't passing back the `args.item`, which would have made editing the dom element harder. Second, using the event trigger instead of a for loop made the methods functionality more simple.\\n#### Decision 2: [Option 2]\\nThe `newResultDisplayed` dom element was firing after the completion of the search-ui, therefore using the `<div>` wasn't possible.\\n#### Decision 3: [Option 1]\\nChoosing to use `getElementsByClassName`, in this context I don't think there is a difference between using `querySelectorAll` and `getElementsByClassName`\\n#### Decision 4: [Option 2]\\nFrom what I understand it makes sense for this feature to be true by default, as it's important for their not to be inconsistencies\\n---\\n","tokens":235,"id":5149,"text":"## Context and Problem Statement\\n<!-- Quick 2-3 sentence background of the User Story -->\\nThe `ViewedByCustomer` component needs to be added on each result template, and therefore could be accidentally missed on one or more, especially if a new template is added at a later time. This would create an inconsistent view of what content the customer has viewed. - From JIRA\\n---\\n## Decision Drivers <!-- optional -->\\n### Context\\nThe main decision drivers were to be able to add the ViewedByCustomer component to each result, without adding it a second time, when the option in the UserActions component is true.\\n<!-- Number these so that they are easier to reference in the following section -->\\n### Decisions\\n1. Need to choose when to edit the results (i.e. need an event)\\n1. Ensure the `ViewedByCustomer` component is properly added to each result template\\n1. Ensure that if a template already has the `ViewedByCustomer` component that it won't add a second component\\n1. There should be an option whether or not to perform that aforementioned actions with the component\\n---\\n\n\n##Decision\n### Context\\nThe main decision drivers were to be able to add the ViewedByCustomer component to each result, without adding it a second time, when the option in the UserActions component is true.\\n<!-- Number these so that they are easier to reference in the following section -->\\n### Decisions\\n1. Need to choose when to edit the results (i.e. need an event)\\n1. Ensure the `ViewedByCustomer` component is properly added to each result template\\n1. Ensure that if a template already has the `ViewedByCustomer` component that it won't add a second component\\n1. There should be an option whether or not to perform that aforementioned actions with the component\\n---\\n-   [Option 1] - Leverage the `newResultsDisplayed` event, and loop over every result, performing further action.\\n-   [Option 2] - Leverage the `newResultDisplayed` event, and perform further action.\\n**Decision 2** - Properly adding the ViewedByDocument Component\\n-   [Option 1] - Add the component using `<div class=\"CoveoViewedByCustomer\">`.\\n-   [Option 2] - Add the component using the `ViewedByCustomer` constructor.\\n**Decision 3** - Ensure we don't add the template a second time\\n-   [Option 1] - Query the results `HTMLElement` using the `getElementsByClassName` method.\\n-   [Option 2] - Query the results `HTMLElement` using the `querySelectorAll` method.\\n**Decision 4** - There should be an option whether or not to add the component\\n-   [Option 1] - Have the option be false by default.\\n-   [Option 2] - Have the option be true by default.\\n---\\n#### Decision 1: [Option 2]\\nThere are two reason behind this decision selection: First the `newResultsDisplayed` option wasn't passing back the `args.item`, which would have made editing the dom element harder. Second, using the event trigger instead of a for loop made the methods functionality more simple.\\n#### Decision 2: [Option 2]\\nThe `newResultDisplayed` dom element was firing after the completion of the search-ui, therefore using the `<div>` wasn't possible.\\n#### Decision 3: [Option 1]\\nChoosing to use `getElementsByClassName`, in this context I don't think there is a difference between using `querySelectorAll` and `getElementsByClassName`\\n#### Decision 4: [Option 2]\\nFrom what I understand it makes sense for this feature to be true by default, as it's important for their not to be inconsistencies\\n---\\n","MatchedIds":[3187,36,4507,3784,794],"Predicted":"## Decision\n\n### Decision\nTo ensure that the `ViewedByCustomer` component is uniformly and correctly added to each result template without redundancy, we will implement the following steps:\n\n1. **Utilize a Centralized Event to Edit Results:**\n    - We will introduce a specific event in the application lifecycle that will trigger the addition of the `ViewedByCustomer` component to result templates. This event ensures a consistent entry point for the operation, reducing the risk of missing templates.\n\n2. **Component Injection Middleware:**\n    - Create a middleware function that hooks into the rendering process of result templates. This middleware will check if the `ViewedByCustomer` component has already been added to a particular template and add it only if it is absent.\n\n3. **Template Registry Check:**\n    - Maintain a registry of all result templates and track the existence of the `ViewedByCustomer` component on each template. This registry will be checked before adding the component to avoid duplication.\n\n4. **User Option Configuration:**\n    - Provide a configuration option in the UserActions component that allows enabling or disabling the addition of the `ViewedByCustomer` component. This will offer flexibility to the end-users and ensure the component can be toggled as needed.\n\n### Positive Consequences\n- Consistent visibility of the `ViewedByCustomer` component across all result templates, ensuring users have a uniform experience.\n- Prevention of redundant component insertion, leading to cleaner and more maintainable code.\n- Flexibility to enable or disable the component via UserActions settings, enhancing user control.\n\n### Negative Consequences\n- Adding a middleware function and maintaining a template registry introduces additional complexity in the result rendering logic.\n- If not properly implemented, the event-based addition might lead to performance overhead or bugs during the rendering lifecycle.\n\nThese decisions are aimed at balancing consistency, performance, and flexibility for the `ViewedByCustomer` component across result templates.","GenTime":"2024-08-19 23:47:58"}
{"File Name":"ditto\/DADR-0006-merge-payload.md","Context":"## Context\\nWe want to allow partial or merge updates of things with a single request.\\n","Decision":"A merge request\\n* uses HTTP `PATCH` method.\\n* has payload in _JSON merge patch_ format defined in [RFC-7396](https:\/\/tools.ietf.org\/html\/rfc7396).\\n* has the request header `content-type` set to `application\/merge-patch+json`.\\n","tokens":19,"id":779,"text":"## Context\\nWe want to allow partial or merge updates of things with a single request.\\n\n\n##Decision\nA merge request\\n* uses HTTP `PATCH` method.\\n* has payload in _JSON merge patch_ format defined in [RFC-7396](https:\/\/tools.ietf.org\/html\/rfc7396).\\n* has the request header `content-type` set to `application\/merge-patch+json`.\\n","MatchedIds":[4424,3845,1180,1857,75],"Predicted":"## Decision\nWe will adopt the PATCH HTTP method standardized by [RFC 5789](https:\/\/tools.ietf.org\/html\/rfc5789) to support partial updates. This allows clients to send only the changes to an existing resource, rather than the entire resource. The partial updates will be based on JSON Merge Patch format as specified by [RFC 7396](https:\/\/tools.ietf.org\/html\/rfc7396).\n\n### Implementation Details:\n1. **Endpoint**: We will add support for the PATCH method on all existing resource endpoints (e.g., `\/v1\/resource\/{id}`).\n2. **Payload**: The payload for the PATCH request will follow the JSON Merge Patch format, which simplifies partial updates of JSON documents.\n3. **Validation**: The server will validate the PATCH request to ensure it conforms to the constraints of the resource and does not introduce any invalid states.\n4. **Merge Logic**: The server will apply the incoming changes to the existing resource, merging them as specified, and will return the updated resource in the response.\n\n### Benefits:\n1. **Efficiency**: Clients can send only the fields that need to be updated, reducing the size of the request payloads and improving performance.\n2. **Simplicity**: The JSON Merge Patch format is easy to understand and implement, making the API straightforward for developers.\n3. **Flexibility**: This approach aligns with RESTful principles and allows for more granular updates, giving clients greater flexibility without necessitating full resource replacement.","GenTime":"2024-08-19 23:48:05"}
{"File Name":"ftd-scratch3-offline\/0013-communicate-with-the-scratch-app-via-a-local-web-server.md","Context":"## Context\\nThe scratch app has to invoke the arduino-cli program to compile the converted programs.\\nThe scratch app has to invoke the scratch-to-c program to convert the scratch blocks to a runnable arduino program.\\nIn both cases the scratch app would have to invoke a native program.\\nThis is not possible in current web browsers.\\nThis would be possible using e.g. Electron, but the authors don't know Electron and Electron would add around 100 MB size overhead and also mean that without Electron the app can't be used.\\nUsing a local web-server all that is needed is any browser, the scratch ftduino app and the local web-server.\\n","Decision":"We will use a local web-server that will be used by the scratch app to invoke certain native programs.\\nThe local web-server may also be used to host the scratch app itself for example when there is no Internet connectivity available.\\n","tokens":135,"id":2634,"text":"## Context\\nThe scratch app has to invoke the arduino-cli program to compile the converted programs.\\nThe scratch app has to invoke the scratch-to-c program to convert the scratch blocks to a runnable arduino program.\\nIn both cases the scratch app would have to invoke a native program.\\nThis is not possible in current web browsers.\\nThis would be possible using e.g. Electron, but the authors don't know Electron and Electron would add around 100 MB size overhead and also mean that without Electron the app can't be used.\\nUsing a local web-server all that is needed is any browser, the scratch ftduino app and the local web-server.\\n\n\n##Decision\nWe will use a local web-server that will be used by the scratch app to invoke certain native programs.\\nThe local web-server may also be used to host the scratch app itself for example when there is no Internet connectivity available.\\n","MatchedIds":[2631,2637,2629,97,3681],"Predicted":"## Decision\nWe will implement a local web-server that can be run alongside the scratch ftduino app. This server will handle the invocation of native programs like the arduino-cli and scratch-to-c converter. This approach avoids the overhead and dependency on Electron, ensuring the app can be used with any browser while maintaining a smaller footprint.","GenTime":"2024-08-19 23:48:07"}
{"File Name":"unit-e-project\/ADR-0002.md","Context":"## Context\\nWe have to maintain a consistent naming in the codebase for roles in the Esperanza protocol.\\nAt the moment we have plenty of examples where there is confusion especially when mixing terms like `staking`, `miner`, `proposer`.\\n","Decision":"We outline two clear actors in the protocol:\\n- The `PROPOSER`: which solves the function of proposing new blocks through his stake. At the moment words like `staking`, `miner`, `mining` are associated with the same function.\\n- The `VALIDATOR`: which takes active part in the finalisation process with the voting.\\nWe therefore MUST replace:\\n- all the instances of `miner` with `proposer`.\\n- all the instances of `mining` with `proposing`.\\n- all the instances of `staking` with `proposing`.\\nBut we MUST NOT replace the instances of the word `stake` since this indicates the actual deposit made in order to propose.\\nThe only exception to this is `stakethread.h` that MUST be renamed to `proposerthread.h`.\\n","tokens":52,"id":3768,"text":"## Context\\nWe have to maintain a consistent naming in the codebase for roles in the Esperanza protocol.\\nAt the moment we have plenty of examples where there is confusion especially when mixing terms like `staking`, `miner`, `proposer`.\\n\n\n##Decision\nWe outline two clear actors in the protocol:\\n- The `PROPOSER`: which solves the function of proposing new blocks through his stake. At the moment words like `staking`, `miner`, `mining` are associated with the same function.\\n- The `VALIDATOR`: which takes active part in the finalisation process with the voting.\\nWe therefore MUST replace:\\n- all the instances of `miner` with `proposer`.\\n- all the instances of `mining` with `proposing`.\\n- all the instances of `staking` with `proposing`.\\nBut we MUST NOT replace the instances of the word `stake` since this indicates the actual deposit made in order to propose.\\nThe only exception to this is `stakethread.h` that MUST be renamed to `proposerthread.h`.\\n","MatchedIds":[2270,3023,2397,28,3890],"Predicted":"## Decision\nA `RolesTerminology.md` document will be created to serve as the definitive source for role names within the Esperanza protocol. This document will outline the exact terms to use for specific roles, such as `staker`, `miner`, and `proposer`, and provide clear definitions for each. Any deviation from this terminology in the codebase will be considered a bug. Furthermore, existing code will undergo a refactoring process to align with the new terminology to ensure consistency and clarity.","GenTime":"2024-08-19 23:48:10"}
{"File Name":"terraform\/AMIs.md","Context":"## Context\\nWe have many operating systems in play at DXC. In moving to AWS the number of potential variants does not decrease, as we could choose from AWS images, marketplace images, or we could build\/maintain our own images.\\n","Decision":"We will use AWS images only, as this frees us from the operational overheads of patching and testing. AWS have been the fastest company in the cloud space to patch their operating systems in response to threats, so we can benefit from their good practices. This decision also allows us to take the easiest routes for automation and gives us access to the widest range of public cloud software.\\n","tokens":50,"id":923,"text":"## Context\\nWe have many operating systems in play at DXC. In moving to AWS the number of potential variants does not decrease, as we could choose from AWS images, marketplace images, or we could build\/maintain our own images.\\n\n\n##Decision\nWe will use AWS images only, as this frees us from the operational overheads of patching and testing. AWS have been the fastest company in the cloud space to patch their operating systems in response to threats, so we can benefit from their good practices. This decision also allows us to take the easiest routes for automation and gives us access to the widest range of public cloud software.\\n","MatchedIds":[2471,3145,4358,1376,3147],"Predicted":"## Decision\nWe will adopt a hybrid approach to managing operating system images in AWS:\n\n1. **AWS Provided Images**: For well-supported and commonly used operating systems such as Amazon Linux, Ubuntu, and Windows Server, we will utilize AWS-provided images. These images will benefit from regular updates and security patches directly from AWS, thus reducing our maintenance overhead.\n\n2. **Marketplace Images**: For specialized applications and less common operating systems, we will leverage AWS Marketplace images. This will allow us to quickly deploy systems with specific requirements without the need to build and maintain custom images.\n\n3. **Custom-built Images**: For scenarios requiring highly customized configurations, security requirements, or software bundles not available in AWS-provided or marketplace images, we will use HashiCorp Packer to build and maintain our own AMIs. This will give us full control over the image configuration, ensuring it meets our specific needs.\n\nEach team at DXC will need to document the reasons for choosing a particular type of image, ensuring that there is clarity on the trade-offs involved. This hybrid approach ensures flexibility, reduces operational overhead where possible, and allows for customization when necessary.","GenTime":"2024-08-19 23:48:17"}
{"File Name":"lobiani\/0004-accept-coupling-with-spring-boot-in-backend.md","Context":"## Context\\nAlmost every software project needs to interact with the 3rd party libraries and frameworks to some extent.\\nGenerally, the less coupled the code is with the 3rd party libraries and frameworks (usually thanks to additional abstraction\\nlayers on top of them), more flexible the software becomes in regard to technology replacements in the future.\\nHowever, sometimes it's quite viable to compromise this flexibility in favor of practical benefits that particular\\ntechnology brings, in this case Spring Boot\/Framework. Especially that it is designed with the abstraction and\\nextensibility in mind, so introducing another layer of abstraction is senseless.\\n","Decision":"We accept direct coupling with Spring Boot\/Framework code without any abstraction layers on top of it.\\n","tokens":127,"id":1035,"text":"## Context\\nAlmost every software project needs to interact with the 3rd party libraries and frameworks to some extent.\\nGenerally, the less coupled the code is with the 3rd party libraries and frameworks (usually thanks to additional abstraction\\nlayers on top of them), more flexible the software becomes in regard to technology replacements in the future.\\nHowever, sometimes it's quite viable to compromise this flexibility in favor of practical benefits that particular\\ntechnology brings, in this case Spring Boot\/Framework. Especially that it is designed with the abstraction and\\nextensibility in mind, so introducing another layer of abstraction is senseless.\\n\n\n##Decision\nWe accept direct coupling with Spring Boot\/Framework code without any abstraction layers on top of it.\\n","MatchedIds":[649,1477,2701,3374,2626],"Predicted":"## Decision\nWe will integrate Spring Boot\/Framework directly into our project without additional layers of abstraction. This allows us to leverage its robust features, extensibility, and ease of use, ensuring a more streamlined development process.","GenTime":"2024-08-19 23:48:22"}
{"File Name":"alfa\/adr-002.md","Context":"## Context\\nLike so many other accessibility tools, such as the [Accessibility Developer Tools by Google](https:\/\/github.com\/GoogleChrome\/accessibility-developer-tools), [aXe by Deque](https:\/\/github.com\/dequelabs\/axe-core), and [HTML_CodeSniffer by Squiz](https:\/\/github.com\/squizlabs\/HTML_CodeSniffer) to name a few, our proprietary accessibility conformance testing engine at Siteimprove runs within the context of a browser. The reason why this seems to be the de facto way of implementing an accessibility tool is obvious: The browser is the tool used to consume your website, so why not test directly within that very tool? Through the APIs exposed by the browser, we get access to all the information needed in order to assess the accessibility of a website; the structure we can access and inspect through the DOM, information about styling can be gained through the CSSOM, and soon we also get our hands on a standardised accessibility tree through the [AOM](https:\/\/wicg.github.io\/aom\/).\\nHowever, not all is good in the land of browsers. Rendering a website is an inherently non-deterministic process and the timing of network requests, script execution, the content of request headers, and much more, all play a role in what the final result will look like. In most cases, this will directly affect the assessment of a tool that runs within the browser and will become very apparent at scale. At Siteimprove, we feel the effect of this on a daily basis; a customer asking us why we came up with a certain result and us having little to no clue because we cannot replicate the exact circumstances that led to that result. This is a frustrating experience for both our customers and ourselves as it makes it difficult to reason about our tool.\\nWe want to fix this and we want to fix it for good. To do so, we must ensure that we have the ability to exactly replicate the results of a given accessibility assessment. Ideally, as many unknown browser variables as possible should be taken out of the equation and the browser only be used for what is absolutely necessary.\\n","Decision":"We will abandon any sort of dynamic analysis within the context of a browser. The input to Alfa will be static data and any assessment must be made based on that data alone. A browser may or may not be involved in the construction of the data, but the browser will not be required for any further assessment thereof.\\nIf additional data is needed by a given accessibility rule, we will adjust the data format to meet the needs of the rule. We will also carefully consider the extent of the data format as to not bloat it with information that could otherwise be inferred from existing data. Ideally, the size of the data when serialised and stored on disk will not be much larger than the size of the original source code on which the data is based.\\n","tokens":437,"id":3156,"text":"## Context\\nLike so many other accessibility tools, such as the [Accessibility Developer Tools by Google](https:\/\/github.com\/GoogleChrome\/accessibility-developer-tools), [aXe by Deque](https:\/\/github.com\/dequelabs\/axe-core), and [HTML_CodeSniffer by Squiz](https:\/\/github.com\/squizlabs\/HTML_CodeSniffer) to name a few, our proprietary accessibility conformance testing engine at Siteimprove runs within the context of a browser. The reason why this seems to be the de facto way of implementing an accessibility tool is obvious: The browser is the tool used to consume your website, so why not test directly within that very tool? Through the APIs exposed by the browser, we get access to all the information needed in order to assess the accessibility of a website; the structure we can access and inspect through the DOM, information about styling can be gained through the CSSOM, and soon we also get our hands on a standardised accessibility tree through the [AOM](https:\/\/wicg.github.io\/aom\/).\\nHowever, not all is good in the land of browsers. Rendering a website is an inherently non-deterministic process and the timing of network requests, script execution, the content of request headers, and much more, all play a role in what the final result will look like. In most cases, this will directly affect the assessment of a tool that runs within the browser and will become very apparent at scale. At Siteimprove, we feel the effect of this on a daily basis; a customer asking us why we came up with a certain result and us having little to no clue because we cannot replicate the exact circumstances that led to that result. This is a frustrating experience for both our customers and ourselves as it makes it difficult to reason about our tool.\\nWe want to fix this and we want to fix it for good. To do so, we must ensure that we have the ability to exactly replicate the results of a given accessibility assessment. Ideally, as many unknown browser variables as possible should be taken out of the equation and the browser only be used for what is absolutely necessary.\\n\n\n##Decision\nWe will abandon any sort of dynamic analysis within the context of a browser. The input to Alfa will be static data and any assessment must be made based on that data alone. A browser may or may not be involved in the construction of the data, but the browser will not be required for any further assessment thereof.\\nIf additional data is needed by a given accessibility rule, we will adjust the data format to meet the needs of the rule. We will also carefully consider the extent of the data format as to not bloat it with information that could otherwise be inferred from existing data. Ideally, the size of the data when serialised and stored on disk will not be much larger than the size of the original source code on which the data is based.\\n","MatchedIds":[4785,3153,2816,2687,4442],"Predicted":"## Decision\nWe will decouple the core logic of our accessibility conformance testing engine from the browser context. The primary components of the accessibility assessment \u2014 DOM inspection, CSS analysis, and accessibility tree parsing \u2014 will be handled by a headless browser environment, such as [Puppeteer](https:\/\/github.com\/puppeteer\/puppeteer). This approach offers deterministic rendering, reduced variance between different runs, and script execution control, which are crucial for replicable assessments. \n\nTo achieve this:\n\n1. **Use Puppeteer for deterministic rendering**: Puppeteer provides a headless Chromium browser environment. By controlling the rendering process altogether, we can ensure consistent results, mitigating issues caused by network variability and execution timing.\n\n2. **Core logic extraction and modularization**: Core functionalities related to DOM inspection, CSSOM analysis, and AOM parsing will be modularized. These modules will be designed to run in both headless and real browser environments for flexibility.\n\n3. **Controlled network environment**: By simulating network conditions and intercepting requests within Puppeteer, we can ensure that the content and resources being loaded are consistent across tests.\n\n4. **Logging and Reproducibility**: Implement extensive logging for each accessibility assessment. This will include network request details, script execution timings, and rendered DOM snapshots. Providing a log ID to customers will allow them to share their specific run, which can be replayed exactly as it was executed.\n\n5. **Fallback to full browser environment for edge cases**: For specific scenarios requiring user interaction or real browser behavior, allow fallback to a full browser environment while ensuring that such cases are minimized and well-logged.\n\nBy implementing these steps, we aim to create a more reliable and replicable accessibility assessment process, providing greater transparency and consistency for our customers.","GenTime":"2024-08-19 23:48:34"}
{"File Name":"my-notes\/styled-components.md","Context":"### Context\\n- easier to maintain than traditional CSS\\n- I was not able to get my tests working with it properly\\n### Decision\\nDon't start using Styled Components\\n","Decision":"Don't start using Styled Components\\n","tokens":35,"id":5090,"text":"### Context\\n- easier to maintain than traditional CSS\\n- I was not able to get my tests working with it properly\\n### Decision\\nDon't start using Styled Components\\n\n\n##Decision\nDon't start using Styled Components\\n","MatchedIds":[1125,157,2655,1992,1855],"Predicted":"### Decision\nContinue using traditional CSS and apply best practices to improve maintainability.\n\n","GenTime":"2024-08-19 23:48:39"}
{"File Name":"cosmos-sdk\/adr-030-authz-module.md","Context":"## Context\\nThe concrete use cases which motivated this module include:\\n* the desire to delegate the ability to vote on proposals to other accounts besides the account which one has\\ndelegated stake\\n* \"sub-keys\" functionality, as originally proposed in [\\#4480](https:\/\/github.com\/cosmos\/cosmos-sdk\/issues\/4480) which\\nis a term used to describe the functionality provided by this module together with\\nthe `fee_grant` module from [ADR 029](.\/adr-029-fee-grant-module.md) and the [group module](https:\/\/github.com\/cosmos\/cosmos-sdk\/tree\/main\/x\/group).\\nThe \"sub-keys\" functionality roughly refers to the ability for one account to grant some subset of its capabilities to\\nother accounts with possibly less robust, but easier to use security measures. For instance, a master account representing\\nan organization could grant the ability to spend small amounts of the organization's funds to individual employee accounts.\\nOr an individual (or group) with a multisig wallet could grant the ability to vote on proposals to any one of the member\\nkeys.\\nThe current implementation is based on work done by the [Gaian's team at Hackatom Berlin 2019](https:\/\/github.com\/cosmos-gaians\/cosmos-sdk\/tree\/hackatom\/x\/delegation).\\n","Decision":"We will create a module named `authz` which provides functionality for\\ngranting arbitrary privileges from one account (the _granter_) to another account (the _grantee_). Authorizations\\nmust be granted for a particular `Msg` service methods one by one using an implementation\\nof `Authorization` interface.\\n### Types\\nAuthorizations determine exactly what privileges are granted. They are extensible\\nand can be defined for any `Msg` service method even outside of the module where\\nthe `Msg` method is defined. `Authorization`s reference `Msg`s using their TypeURL.\\n#### Authorization\\n```go\\ntype Authorization interface {\\nproto.Message\\n\/\/ MsgTypeURL returns the fully-qualified Msg TypeURL (as described in ADR 020),\\n\/\/ which will process and accept or reject a request.\\nMsgTypeURL() string\\n\/\/ Accept determines whether this grant permits the provided sdk.Msg to be performed, and if\\n\/\/ so provides an upgraded authorization instance.\\nAccept(ctx sdk.Context, msg sdk.Msg) (AcceptResponse, error)\\n\/\/ ValidateBasic does a simple validation check that\\n\/\/ doesn't require access to any other information.\\nValidateBasic() error\\n}\\n\/\/ AcceptResponse instruments the controller of an authz message if the request is accepted\\n\/\/ and if it should be updated or deleted.\\ntype AcceptResponse struct {\\n\/\/ If Accept=true, the controller can accept and authorization and handle the update.\\nAccept bool\\n\/\/ If Delete=true, the controller must delete the authorization object and release\\n\/\/ storage resources.\\nDelete bool\\n\/\/ Controller, who is calling Authorization.Accept must check if `Updated != nil`. If yes,\\n\/\/ it must use the updated version and handle the update on the storage level.\\nUpdated Authorization\\n}\\n```\\nFor example a `SendAuthorization` like this is defined for `MsgSend` that takes\\na `SpendLimit` and updates it down to zero:\\n```go\\ntype SendAuthorization struct {\\n\/\/ SpendLimit specifies the maximum amount of tokens that can be spent\\n\/\/ by this authorization and will be updated as tokens are spent. This field is required. (Generic authorization\\n\/\/ can be used with bank msg type url to create limit less bank authorization).\\nSpendLimit sdk.Coins\\n}\\nfunc (a SendAuthorization) MsgTypeURL() string {\\nreturn sdk.MsgTypeURL(&MsgSend{})\\n}\\nfunc (a SendAuthorization) Accept(ctx sdk.Context, msg sdk.Msg) (authz.AcceptResponse, error) {\\nmSend, ok := msg.(*MsgSend)\\nif !ok {\\nreturn authz.AcceptResponse{}, sdkerrors.ErrInvalidType.Wrap(\"type mismatch\")\\n}\\nlimitLeft, isNegative := a.SpendLimit.SafeSub(mSend.Amount)\\nif isNegative {\\nreturn authz.AcceptResponse{}, sdkerrors.ErrInsufficientFunds.Wrapf(\"requested amount is more than spend limit\")\\n}\\nif limitLeft.IsZero() {\\nreturn authz.AcceptResponse{Accept: true, Delete: true}, nil\\n}\\nreturn authz.AcceptResponse{Accept: true, Delete: false, Updated: &SendAuthorization{SpendLimit: limitLeft}}, nil\\n}\\n```\\nA different type of capability for `MsgSend` could be implemented\\nusing the `Authorization` interface with no need to change the underlying\\n`bank` module.\\n##### Small notes on `AcceptResponse`\\n* The `AcceptResponse.Accept` field will be set to `true` if the authorization is accepted.\\nHowever, if it is rejected, the function `Accept` will raise an error (without setting `AcceptResponse.Accept` to `false`).\\n* The `AcceptResponse.Updated` field will be set to a non-nil value only if there is a real change to the authorization.\\nIf authorization remains the same (as is, for instance, always the case for a [`GenericAuthorization`](#genericauthorization)),\\nthe field will be `nil`.\\n### `Msg` Service\\n```protobuf\\nservice Msg {\\n\/\/ Grant grants the provided authorization to the grantee on the granter's\\n\/\/ account with the provided expiration time.\\nrpc Grant(MsgGrant) returns (MsgGrantResponse);\\n\/\/ Exec attempts to execute the provided messages using\\n\/\/ authorizations granted to the grantee. Each message should have only\\n\/\/ one signer corresponding to the granter of the authorization.\\nrpc Exec(MsgExec) returns (MsgExecResponse);\\n\/\/ Revoke revokes any authorization corresponding to the provided method name on the\\n\/\/ granter's account that has been granted to the grantee.\\nrpc Revoke(MsgRevoke) returns (MsgRevokeResponse);\\n}\\n\/\/ Grant gives permissions to execute\\n\/\/ the provided method with expiration time.\\nmessage Grant {\\ngoogle.protobuf.Any       authorization = 1 [(cosmos_proto.accepts_interface) = \"cosmos.authz.v1beta1.Authorization\"];\\ngoogle.protobuf.Timestamp expiration    = 2 [(gogoproto.stdtime) = true, (gogoproto.nullable) = false];\\n}\\nmessage MsgGrant {\\nstring granter = 1;\\nstring grantee = 2;\\nGrant grant = 3 [(gogoproto.nullable) = false];\\n}\\nmessage MsgExecResponse {\\ncosmos.base.abci.v1beta1.Result result = 1;\\n}\\nmessage MsgExec {\\nstring   grantee                  = 1;\\n\/\/ Authorization Msg requests to execute. Each msg must implement Authorization interface\\nrepeated google.protobuf.Any msgs = 2 [(cosmos_proto.accepts_interface) = \"cosmos.base.v1beta1.Msg\"];;\\n}\\n```\\n### Router Middleware\\nThe `authz` `Keeper` will expose a `DispatchActions` method which allows other modules to send `Msg`s\\nto the router based on `Authorization` grants:\\n```go\\ntype Keeper interface {\\n\/\/ DispatchActions routes the provided msgs to their respective handlers if the grantee was granted an authorization\\n\/\/ to send those messages by the first (and only) signer of each msg.\\nDispatchActions(ctx sdk.Context, grantee sdk.AccAddress, msgs []sdk.Msg) sdk.Result`\\n}\\n```\\n### CLI\\n#### `tx exec` Method\\nWhen a CLI user wants to run a transaction on behalf of another account using `MsgExec`, they\\ncan use the `exec` method. For instance `gaiacli tx gov vote 1 yes --from <grantee> --generate-only | gaiacli tx authz exec --send-as <granter> --from <grantee>`\\nwould send a transaction like this:\\n```go\\nMsgExec {\\nGrantee: mykey,\\nMsgs: []sdk.Msg{\\nMsgVote {\\nProposalID: 1,\\nVoter: cosmos3thsdgh983egh823\\nOption: Yes\\n}\\n}\\n}\\n```\\n#### `tx grant <grantee> <authorization> --from <granter>`\\nThis CLI command will send a `MsgGrant` transaction. `authorization` should be encoded as\\nJSON on the CLI.\\n#### `tx revoke <grantee> <method-name> --from <granter>`\\nThis CLI command will send a `MsgRevoke` transaction.\\n### Built-in Authorizations\\n#### `SendAuthorization`\\n```protobuf\\n\/\/ SendAuthorization allows the grantee to spend up to spend_limit coins from\\n\/\/ the granter's account.\\nmessage SendAuthorization {\\nrepeated cosmos.base.v1beta1.Coin spend_limit = 1;\\n}\\n```\\n#### `GenericAuthorization`\\n```protobuf\\n\/\/ GenericAuthorization gives the grantee unrestricted permissions to execute\\n\/\/ the provided method on behalf of the granter's account.\\nmessage GenericAuthorization {\\noption (cosmos_proto.implements_interface) = \"Authorization\";\\n\/\/ Msg, identified by it's type URL, to grant unrestricted permissions to execute\\nstring msg = 1;\\n}\\n```\\n","tokens":276,"id":839,"text":"## Context\\nThe concrete use cases which motivated this module include:\\n* the desire to delegate the ability to vote on proposals to other accounts besides the account which one has\\ndelegated stake\\n* \"sub-keys\" functionality, as originally proposed in [\\#4480](https:\/\/github.com\/cosmos\/cosmos-sdk\/issues\/4480) which\\nis a term used to describe the functionality provided by this module together with\\nthe `fee_grant` module from [ADR 029](.\/adr-029-fee-grant-module.md) and the [group module](https:\/\/github.com\/cosmos\/cosmos-sdk\/tree\/main\/x\/group).\\nThe \"sub-keys\" functionality roughly refers to the ability for one account to grant some subset of its capabilities to\\nother accounts with possibly less robust, but easier to use security measures. For instance, a master account representing\\nan organization could grant the ability to spend small amounts of the organization's funds to individual employee accounts.\\nOr an individual (or group) with a multisig wallet could grant the ability to vote on proposals to any one of the member\\nkeys.\\nThe current implementation is based on work done by the [Gaian's team at Hackatom Berlin 2019](https:\/\/github.com\/cosmos-gaians\/cosmos-sdk\/tree\/hackatom\/x\/delegation).\\n\n\n##Decision\nWe will create a module named `authz` which provides functionality for\\ngranting arbitrary privileges from one account (the _granter_) to another account (the _grantee_). Authorizations\\nmust be granted for a particular `Msg` service methods one by one using an implementation\\nof `Authorization` interface.\\n### Types\\nAuthorizations determine exactly what privileges are granted. They are extensible\\nand can be defined for any `Msg` service method even outside of the module where\\nthe `Msg` method is defined. `Authorization`s reference `Msg`s using their TypeURL.\\n#### Authorization\\n```go\\ntype Authorization interface {\\nproto.Message\\n\/\/ MsgTypeURL returns the fully-qualified Msg TypeURL (as described in ADR 020),\\n\/\/ which will process and accept or reject a request.\\nMsgTypeURL() string\\n\/\/ Accept determines whether this grant permits the provided sdk.Msg to be performed, and if\\n\/\/ so provides an upgraded authorization instance.\\nAccept(ctx sdk.Context, msg sdk.Msg) (AcceptResponse, error)\\n\/\/ ValidateBasic does a simple validation check that\\n\/\/ doesn't require access to any other information.\\nValidateBasic() error\\n}\\n\/\/ AcceptResponse instruments the controller of an authz message if the request is accepted\\n\/\/ and if it should be updated or deleted.\\ntype AcceptResponse struct {\\n\/\/ If Accept=true, the controller can accept and authorization and handle the update.\\nAccept bool\\n\/\/ If Delete=true, the controller must delete the authorization object and release\\n\/\/ storage resources.\\nDelete bool\\n\/\/ Controller, who is calling Authorization.Accept must check if `Updated != nil`. If yes,\\n\/\/ it must use the updated version and handle the update on the storage level.\\nUpdated Authorization\\n}\\n```\\nFor example a `SendAuthorization` like this is defined for `MsgSend` that takes\\na `SpendLimit` and updates it down to zero:\\n```go\\ntype SendAuthorization struct {\\n\/\/ SpendLimit specifies the maximum amount of tokens that can be spent\\n\/\/ by this authorization and will be updated as tokens are spent. This field is required. (Generic authorization\\n\/\/ can be used with bank msg type url to create limit less bank authorization).\\nSpendLimit sdk.Coins\\n}\\nfunc (a SendAuthorization) MsgTypeURL() string {\\nreturn sdk.MsgTypeURL(&MsgSend{})\\n}\\nfunc (a SendAuthorization) Accept(ctx sdk.Context, msg sdk.Msg) (authz.AcceptResponse, error) {\\nmSend, ok := msg.(*MsgSend)\\nif !ok {\\nreturn authz.AcceptResponse{}, sdkerrors.ErrInvalidType.Wrap(\"type mismatch\")\\n}\\nlimitLeft, isNegative := a.SpendLimit.SafeSub(mSend.Amount)\\nif isNegative {\\nreturn authz.AcceptResponse{}, sdkerrors.ErrInsufficientFunds.Wrapf(\"requested amount is more than spend limit\")\\n}\\nif limitLeft.IsZero() {\\nreturn authz.AcceptResponse{Accept: true, Delete: true}, nil\\n}\\nreturn authz.AcceptResponse{Accept: true, Delete: false, Updated: &SendAuthorization{SpendLimit: limitLeft}}, nil\\n}\\n```\\nA different type of capability for `MsgSend` could be implemented\\nusing the `Authorization` interface with no need to change the underlying\\n`bank` module.\\n##### Small notes on `AcceptResponse`\\n* The `AcceptResponse.Accept` field will be set to `true` if the authorization is accepted.\\nHowever, if it is rejected, the function `Accept` will raise an error (without setting `AcceptResponse.Accept` to `false`).\\n* The `AcceptResponse.Updated` field will be set to a non-nil value only if there is a real change to the authorization.\\nIf authorization remains the same (as is, for instance, always the case for a [`GenericAuthorization`](#genericauthorization)),\\nthe field will be `nil`.\\n### `Msg` Service\\n```protobuf\\nservice Msg {\\n\/\/ Grant grants the provided authorization to the grantee on the granter's\\n\/\/ account with the provided expiration time.\\nrpc Grant(MsgGrant) returns (MsgGrantResponse);\\n\/\/ Exec attempts to execute the provided messages using\\n\/\/ authorizations granted to the grantee. Each message should have only\\n\/\/ one signer corresponding to the granter of the authorization.\\nrpc Exec(MsgExec) returns (MsgExecResponse);\\n\/\/ Revoke revokes any authorization corresponding to the provided method name on the\\n\/\/ granter's account that has been granted to the grantee.\\nrpc Revoke(MsgRevoke) returns (MsgRevokeResponse);\\n}\\n\/\/ Grant gives permissions to execute\\n\/\/ the provided method with expiration time.\\nmessage Grant {\\ngoogle.protobuf.Any       authorization = 1 [(cosmos_proto.accepts_interface) = \"cosmos.authz.v1beta1.Authorization\"];\\ngoogle.protobuf.Timestamp expiration    = 2 [(gogoproto.stdtime) = true, (gogoproto.nullable) = false];\\n}\\nmessage MsgGrant {\\nstring granter = 1;\\nstring grantee = 2;\\nGrant grant = 3 [(gogoproto.nullable) = false];\\n}\\nmessage MsgExecResponse {\\ncosmos.base.abci.v1beta1.Result result = 1;\\n}\\nmessage MsgExec {\\nstring   grantee                  = 1;\\n\/\/ Authorization Msg requests to execute. Each msg must implement Authorization interface\\nrepeated google.protobuf.Any msgs = 2 [(cosmos_proto.accepts_interface) = \"cosmos.base.v1beta1.Msg\"];;\\n}\\n```\\n### Router Middleware\\nThe `authz` `Keeper` will expose a `DispatchActions` method which allows other modules to send `Msg`s\\nto the router based on `Authorization` grants:\\n```go\\ntype Keeper interface {\\n\/\/ DispatchActions routes the provided msgs to their respective handlers if the grantee was granted an authorization\\n\/\/ to send those messages by the first (and only) signer of each msg.\\nDispatchActions(ctx sdk.Context, grantee sdk.AccAddress, msgs []sdk.Msg) sdk.Result`\\n}\\n```\\n### CLI\\n#### `tx exec` Method\\nWhen a CLI user wants to run a transaction on behalf of another account using `MsgExec`, they\\ncan use the `exec` method. For instance `gaiacli tx gov vote 1 yes --from <grantee> --generate-only | gaiacli tx authz exec --send-as <granter> --from <grantee>`\\nwould send a transaction like this:\\n```go\\nMsgExec {\\nGrantee: mykey,\\nMsgs: []sdk.Msg{\\nMsgVote {\\nProposalID: 1,\\nVoter: cosmos3thsdgh983egh823\\nOption: Yes\\n}\\n}\\n}\\n```\\n#### `tx grant <grantee> <authorization> --from <granter>`\\nThis CLI command will send a `MsgGrant` transaction. `authorization` should be encoded as\\nJSON on the CLI.\\n#### `tx revoke <grantee> <method-name> --from <granter>`\\nThis CLI command will send a `MsgRevoke` transaction.\\n### Built-in Authorizations\\n#### `SendAuthorization`\\n```protobuf\\n\/\/ SendAuthorization allows the grantee to spend up to spend_limit coins from\\n\/\/ the granter's account.\\nmessage SendAuthorization {\\nrepeated cosmos.base.v1beta1.Coin spend_limit = 1;\\n}\\n```\\n#### `GenericAuthorization`\\n```protobuf\\n\/\/ GenericAuthorization gives the grantee unrestricted permissions to execute\\n\/\/ the provided method on behalf of the granter's account.\\nmessage GenericAuthorization {\\noption (cosmos_proto.implements_interface) = \"Authorization\";\\n\/\/ Msg, identified by it's type URL, to grant unrestricted permissions to execute\\nstring msg = 1;\\n}\\n```\\n","MatchedIds":[812,4346,841,2256,28],"Predicted":"## Decision\n### Delegation Module for Comprehensive Sub-Key Management\nTo address the highlighted use cases, we will introduce a specialized \"Delegation Module\" capable of managing sub-keys and delegating specific capabilities to different accounts. This module will integrate with the existing `fee_grant` and `group` modules as outlined below:\n\n### Module Overview\nThe Delegation Module will provide functionality to:\n1. Delegate voting rights for governance proposals to other accounts.\n2. Implement the \"sub-keys\" functionality, allowing for fine-grained delegation of specific capabilities.\n\n### Key Features\n1. **Voting Delegation:**\n   - Allow account holders to delegate their voting rights to another account through a `MsgDelegateVoteRights` message.\n   - Ensure that only the delegated account can vote on governance proposals on behalf of the delegating account.\n\n2. **Sub-Keys Functionality:**\n   - Allow accounts to create and manage sub-keys with specific capabilities.\n   - Permit the delegation of various account capabilities, such as spending small amounts of funds, through the `MsgGrantSubKey` message.\n   - Initially support capabilities such as voting, spending, and participating in group actions.\n   - Ensure that each sub-key can only perform a subset of actions defined by the master account.\n\n### Integration with Existing Modules\n1. **Fee Grant Module:**\n   - Use the fee-grant module to enable sub-keys to pay for transaction fees without compromising the master account\u2019s security.\n   - This will be facilitated through granted fee allowances specified during the sub-key creation.\n\n2. **Group Module:**\n   - Allow group accounts to delegate proposal voting rights to any member of the group.\n   - Ensure group actions can be performed by sub-keys, extending the utility of sub-keys within group dynamics.\n\n### Messages and Handlers\n1. **MsgDelegateVoteRights:**\n   ```go\n   type MsgDelegateVoteRights struct {\n       DelegatorAddress sdk.AccAddress\n       DelegateeAddress sdk.AccAddress\n   }\n   ```\n   - Handler will verify that the delegator is the owner of the stake and update the delegation rights.\n\n2. **MsgGrantSubKey:**\n   ```go\n   type MsgGrantSubKey struct {\n       MasterAddress sdk.AccAddress\n       SubKeyAddress sdk.AccAddress\n       Capabilities []Capability\n   }\n   ```\n   - Handler will check the validity and permissions of the capabilities being granted and store them securely.\n\n### Data Structures\n1. **DelegationRecord:**\n   ```go\n   type DelegationRecord struct {\n       DelegatorAddress sdk.AccAddress\n       DelegateeAddress sdk.AccAddress\n       DelegationType   DelegationType\n   }\n   ```\n   - Used to store information about the delegation of voting rights.\n\n2. **SubKeyRecord:**\n   ```go\n   type SubKeyRecord struct {\n        MasterAddress sdk.AccAddress\n        SubKeyAddress sdk.AccAddress\n        Capabilities  []Capability\n   }\n   ```\n   - Used to store information about sub-keys and their granted capabilities.\n\n### Security Considerations\n- Ensure stringent validation for the delegation of rights and capabilities to prevent unauthorized access.\n- Implement checks to avoid over-provisioning of sub-key capabilities, which could lead to potential misuse.\n- Use secure key management and transaction signing practices to maintain system integrity.\n\n### Future Improvements\n- Implement audit logs to track sub-key usage and delegation changes.\n- Introduce more fine-grained capabilities as the module evolves to address new use cases.\n- Enhance the usability of the module through SDK integration and corresponding user interfaces.\n\nBy formalizing this Delegation Module, we aim to provide a robust and flexible framework for account management, enhancing both security and usability for a diverse range of users on the Cosmos Network.","GenTime":"2024-08-19 23:48:55"}
{"File Name":"WorkoutLog\/0009_one_time_events_livedata.md","Context":"## Context\\nLiveData events will be pushed again after certain LifeCycle events. Ie. when rotating the device or\\nafter returning to a fragment from another navigation path. Errors or special events like automatic\\nnavigation should only be executed once.\\n","Decision":"A new event will be introduced for the ViewModels which has the clear purpose of only occur once.\\n","tokens":49,"id":1231,"text":"## Context\\nLiveData events will be pushed again after certain LifeCycle events. Ie. when rotating the device or\\nafter returning to a fragment from another navigation path. Errors or special events like automatic\\nnavigation should only be executed once.\\n\n\n##Decision\nA new event will be introduced for the ViewModels which has the clear purpose of only occur once.\\n","MatchedIds":[1133,2499,1216,3845,669],"Predicted":"## Decision\nTo avoid re-execution of LiveData events in scenarios like device rotation or fragment navigation, we will utilize **SingleLiveEvent** and **EventWrapper** approaches:\n\n1. **SingleLiveEvent**:\nWe will use the `SingleLiveEvent` class which is a lifecycle-aware observable designed to send only new updates or events. This prevents the re-execution of LiveData events after lifecycle changes.\n\n```kotlin\nclass SingleLiveEvent<T> : MutableLiveData<T>() {\n    private val pending = AtomicBoolean(false)\n    \n    @MainThread\n    override fun observe(owner: LifecycleOwner, observer: Observer<in T>) {\n        if (hasActiveObservers()) {\n            Log.w(TAG, \"Multiple observers registered but only one will be notified of changes.\")\n        }\n        super.observe(owner, { t ->\n            if (pending.compareAndSet(true, false)) {\n                observer.onChanged(t)\n            }\n        })\n    }\n\n    @MainThread\n    override fun setValue(t: T?) {\n        pending.set(true)\n        super.setValue(t)\n    }\n\n    companion object {\n        private const val TAG = \"SingleLiveEvent\"\n    }\n}\n```\n\n2. **EventWrapper**:\nAlternatively, we will use an `EventWrapper` class that wraps the LiveData events and ensures they are consumed only once.\n\n```kotlin\nclass EventWrapper<out T>(private val content: T) {\n    private var hasBeenHandled = false\n    \n    fun getContentIfNotHandled(): T? {\n        return if (hasBeenHandled) {\n            null\n        } else {\n            hasBeenHandled = true\n            content\n        }\n    }\n\n    fun peekContent(): T = content\n}\n\nclass ViewModel : ViewModel() {\n    private val _event = MutableLiveData<EventWrapper<String>>()\n    val event: LiveData<EventWrapper<String>> = _event\n\n    fun triggerEvent() {\n        _event.value = EventWrapper(\"One-Time Event\")\n    }\n}\n```\n\nAnd in the Fragment or Activity:\n\n```kotlin\nviewModel.event.observe(viewLifecycleOwner, Observer { eventWrapper ->\n    eventWrapper.getContentIfNotHandled()?.let { event ->\n        \/\/ Handle the event (e.g., show a toast, navigate, etc.)\n    }\n})\n```\n\nBy following this approach, we ensure that errors or special events such as automatic navigation are only executed once, even when the LifecycleOwner is recreated or restored.\n\nThis way, we maintain a consistent user experience without unintended re-triggering of events due to lifecycle changes.","GenTime":"2024-08-19 23:49:03"}
{"File Name":"FlowKit\/0009-asciidoctor-pdf-for-notebook-conversion.md","Context":"## Context\\nWe want AutoFlow to support creation of PDF reports from Jupyter notebooks. [nbconvert](https:\/\/nbconvert.readthedocs.io\/en\/latest\/) provides a method for converting Jupyter notebooks to PDF format. However, PDF conversion with nbconvert requires a full LaTeX installation, which is ~4GB in size. To keep the size of the AutoFlow container smaller, it is desirable to find an alternative that doesn't require a LaTeX installation.\\n[Asciidoctor PDF](https:\/\/asciidoctor.org\/docs\/asciidoctor-pdf\/) is a tool for converting ASCIIDoc documents to PDF format without generating an interim format such as LaTeX. Since nbconvert can convert notebooks to ASCIIDoc format, we can use Asciidoctor PDF as the second half of a two-step process to convert Jupyter notebooks to PDF via ASCIIDoc.\\n","Decision":"We will use a two-step process to convert Jupyter notebooks to PDF reports in AutoFlow: convert the notebook to ASCIIDoc format using nbconvert, and then convert the resulting ASCIIDoc document to PDF using Asciidoc PDF.\\n","tokens":178,"id":5059,"text":"## Context\\nWe want AutoFlow to support creation of PDF reports from Jupyter notebooks. [nbconvert](https:\/\/nbconvert.readthedocs.io\/en\/latest\/) provides a method for converting Jupyter notebooks to PDF format. However, PDF conversion with nbconvert requires a full LaTeX installation, which is ~4GB in size. To keep the size of the AutoFlow container smaller, it is desirable to find an alternative that doesn't require a LaTeX installation.\\n[Asciidoctor PDF](https:\/\/asciidoctor.org\/docs\/asciidoctor-pdf\/) is a tool for converting ASCIIDoc documents to PDF format without generating an interim format such as LaTeX. Since nbconvert can convert notebooks to ASCIIDoc format, we can use Asciidoctor PDF as the second half of a two-step process to convert Jupyter notebooks to PDF via ASCIIDoc.\\n\n\n##Decision\nWe will use a two-step process to convert Jupyter notebooks to PDF reports in AutoFlow: convert the notebook to ASCIIDoc format using nbconvert, and then convert the resulting ASCIIDoc document to PDF using Asciidoc PDF.\\n","MatchedIds":[284,287,273,5172,1228],"Predicted":"## Decision\nWe will use a two-step process to convert Jupyter notebooks to PDF format for AutoFlow:\n\n1. **Convert Jupyter Notebooks to ASCIIDoc**: Utilize `nbconvert` to convert Jupyter notebooks to the ASCIIDoc format. This leverages the existing capability of `nbconvert` without requiring a LaTeX installation.\n\n2. **Convert ASCIIDoc to PDF using Asciidoctor PDF**: Utilize Asciidoctor PDF to convert the ASCIIDoc format to PDF. This avoids the need for a LaTeX installation and keeps the size of the AutoFlow container minimal.\n\n### Positive Consequences\n* **Reduced Container Size**: By avoiding a full LaTeX installation, the AutoFlow container size is significantly reduced.\n* **Efficiency**: The two-step process is efficient and leverages existing tools tailored for their specific tasks.\n* **Simplicity**: Asciidoctor PDF simplifies the PDF generation process without generating intermediate LaTeX files.\n\n### Negative Consequences\n* **Intermediate Format**: Introducing an intermediate ASCIIDoc format might require additional error handling and processing.\n\nThis decision balances the need for creating high-quality PDF reports from Jupyter notebooks while keeping the container lightweight and maintaining simplicity in the conversion process.","GenTime":"2024-08-19 23:49:14"}
{"File Name":"heptaconnect-docs\/2021-10-29-flow-components-are-not-crud.md","Context":"## Context\\nAt the time of writing we have explorers, emitters and receivers as three main flow components.\\nThey resemble CR and U from the well-known [CRUD](https:\/\/en.wikipedia.org\/wiki\/Create,_read,_update_and_delete).\\nMost APIs are CRUD or [BREAD](http:\/\/paul-m-jones.com\/post\/2008\/08\/20\/bread-not-crud\/) based and therefore match the three named flow components.\\nFor now, emitting and receiving entities can be also used differently as this \"just\" sends data from one portal node and is received by another portal node.\\nEmitters and receivers could send commands instead of entities.\\nAs previously mentioned we do not have a deletion flow component.\\nA receiver could receive an entity with a custom deletion command with any previous version of HEPTAconnect.\\nThis is discouraged but possible.\\nWe have already seen implementations, that receive data but don't write anything to the API the portal resembles.\\nThis is a misuse that is similar to described scenario above.\\nLooking at the other existing flow components we also have webhooks and status reporters.\\nThese are not related to CRUD at all, so we are not limited to CRUD.\\n","Decision":"Receivers are not meant to do everything, when it is about receiving a command.\\nReceivers are meant to be used for entities only.\\nGrouping explorers, emitters, receivers and \"deleters\" into a single CRUD flow component enforces structures that probably don't benefit APIs, that do not fall into this pattern.\\nGrouping flow components is not helpful when we do not know the possible groups in beforehand and therefore can't be done right.\\nEvery other transfer needs a new flow component.\\nAs routes connect emitters and receivers they need to learn how to decide which flow components to use on a route.\\nThis is described in a [different ADR](.\/2021-10-30-route-capabilities.md).\\n","tokens":250,"id":3205,"text":"## Context\\nAt the time of writing we have explorers, emitters and receivers as three main flow components.\\nThey resemble CR and U from the well-known [CRUD](https:\/\/en.wikipedia.org\/wiki\/Create,_read,_update_and_delete).\\nMost APIs are CRUD or [BREAD](http:\/\/paul-m-jones.com\/post\/2008\/08\/20\/bread-not-crud\/) based and therefore match the three named flow components.\\nFor now, emitting and receiving entities can be also used differently as this \"just\" sends data from one portal node and is received by another portal node.\\nEmitters and receivers could send commands instead of entities.\\nAs previously mentioned we do not have a deletion flow component.\\nA receiver could receive an entity with a custom deletion command with any previous version of HEPTAconnect.\\nThis is discouraged but possible.\\nWe have already seen implementations, that receive data but don't write anything to the API the portal resembles.\\nThis is a misuse that is similar to described scenario above.\\nLooking at the other existing flow components we also have webhooks and status reporters.\\nThese are not related to CRUD at all, so we are not limited to CRUD.\\n\n\n##Decision\nReceivers are not meant to do everything, when it is about receiving a command.\\nReceivers are meant to be used for entities only.\\nGrouping explorers, emitters, receivers and \"deleters\" into a single CRUD flow component enforces structures that probably don't benefit APIs, that do not fall into this pattern.\\nGrouping flow components is not helpful when we do not know the possible groups in beforehand and therefore can't be done right.\\nEvery other transfer needs a new flow component.\\nAs routes connect emitters and receivers they need to learn how to decide which flow components to use on a route.\\nThis is described in a [different ADR](.\/2021-10-30-route-capabilities.md).\\n","MatchedIds":[3210,674,2184,2316,2499],"Predicted":"## Decision\n\nTo address the need for a deletion flow component and to prevent misuse, we will introduce a dedicated \"Deleter\" flow component. This new component will be responsible for handling delete operations and will work alongside the existing explorers, emitters, and receivers. The \"Deleter\" component will ensure that delete operations are conducted in a standardized and reliable manner, preventing the misuse scenarios previously observed.\n\n### Key Aspects of the Decision:\n\n1. **Introduction of Deleter Component**:\n   - The \"Deleter\" component will be responsible for receiving delete commands and ensuring the targeted entities are properly deleted from the system.\n   - This component will handle deletion logic in a way that is consistent with the other flow components, adhering to best practices and reducing the risk of errors.\n\n2. **Standardized Delete Commands**:\n   - The delete commands will be standardized to prevent ambiguity and ensure that the deletion process is clear and reliable.\n   - This standardization will help in maintaining consistency across different implementations and reduce the scope for misuse.\n\n3. **Integration with Existing Components**:\n   - The \"Deleter\" component will integrate seamlessly with the explorers, emitters, and receivers, ensuring that delete operations are a natural extension of our existing flow components.\n   - Emitters and receivers will be updated to recognize and appropriately handle delete commands, routing them to the \"Deleter\" component as needed.\n\n4. **Preventing Misuse**:\n   - By introducing a dedicated component for deletions, we eliminate the need for sending custom deletion commands through the existing receiver components, which was identified as a misuse.\n   - This approach will lead to better separation of concerns and clearer responsibilities for each component.\n\n5. **Support BREAD Operations**:\n   - In addition to CRUD operations, the \"Deleter\" component will support BREAD operations (Browse, Read, Edit, Add, Delete), aligning with more comprehensive API designs and ensuring compatibility with broader use cases.\n\n6. **Maintaining Flexibility**:\n   - While the focus is on creating a robust deletion flow, the system will remain flexible to accommodate other non-CRUD operations, leveraging webhooks, status reporters, and potentially new components as needed.\n\nBy implementing the \"Deleter\" flow component, we ensure a more comprehensive and misuse-resistant architecture, while maintaining flexibility to support various types of operations beyond CRUD.","GenTime":"2024-08-19 23:49:22"}
{"File Name":"launcher\/2018-03-29_add_crc_and_uuid_to_log_lines.md","Context":"## Context\\n**TL;DR**\\nA lack of unique info per-log line makes it hard to uniq osquery data downstream, especially in cases when distinct log lines with identical data can be generated. To improve visibility, we should add a UUID to each emitted line. Additionally, adding a hash of the data lets you uniq log line data without inspecting all the fields of these log lines.\\n**Detail**\\nWhile a unique ID is generated for every batched log event emitted from launcher, individual log lines within that log cannot be determined to be unique. This is both true for distinct logs which are identical in their contents, and in the instance that the downstream data pipeline cannot guarantee \"exactly once\" delivery of messages.\\nBy including a UUID for each log line, distinct but semantically identical log lines of data can be determined not to be duplicates.\\nAdditionally, by including a CRC of the data inside each log line, downstream consumers of the log can identify log lines with identical data without inspecting the entirety of log line's data.\\nNote that the crc must be calculated before adding the uuid, or else it won't be useful\\n","Decision":"Launcher should unpack logs from osquery and decorate each line with both a CRC of the data, and a UUID.\\n","tokens":236,"id":2549,"text":"## Context\\n**TL;DR**\\nA lack of unique info per-log line makes it hard to uniq osquery data downstream, especially in cases when distinct log lines with identical data can be generated. To improve visibility, we should add a UUID to each emitted line. Additionally, adding a hash of the data lets you uniq log line data without inspecting all the fields of these log lines.\\n**Detail**\\nWhile a unique ID is generated for every batched log event emitted from launcher, individual log lines within that log cannot be determined to be unique. This is both true for distinct logs which are identical in their contents, and in the instance that the downstream data pipeline cannot guarantee \"exactly once\" delivery of messages.\\nBy including a UUID for each log line, distinct but semantically identical log lines of data can be determined not to be duplicates.\\nAdditionally, by including a CRC of the data inside each log line, downstream consumers of the log can identify log lines with identical data without inspecting the entirety of log line's data.\\nNote that the crc must be calculated before adding the uuid, or else it won't be useful\\n\n\n##Decision\nLauncher should unpack logs from osquery and decorate each line with both a CRC of the data, and a UUID.\\n","MatchedIds":[1927,3914,3290,2546,2256],"Predicted":"## Decision\nTo improve the uniqueness and reliability of log lines generated by osquery, we will implement the following approach:\n\n1. **UUID Generation**: Each log line will include a unique identifier (UUID). This UUID will be generated and appended to each log line to ensure that even semantically identical log lines can be distinguished as unique. This will help in identifying unique log entries in downstream systems where \"exactly once\" delivery cannot be guaranteed.\n\n2. **CRC Calculation**: A cyclic redundancy check (CRC) hash of the data contained in each log line will be calculated and included. The CRC should be computed before the UUID is appended to ensure the hash reflects the original log data accurately. This hash will allow downstream consumers to identify log lines with identical data efficiently, without the need to inspect all the fields of each log line.\n\n### Implementation Steps:\n1. Generate the CRC hash of the log data.\n2. Append the CRC hash to the log line.\n3. Generate a UUID and append it to the log line.\n\nBy following these steps, we will enhance the visibility and reliability of osquery log data, making it easier to manage and analyze logged events without concerns of data duplication, even in cases of identical log content or delivery inconsistencies.","GenTime":"2024-08-19 23:49:27"}
{"File Name":"docs\/0022-function-to-component-mapping.md","Context":"## Context and Problem Statement\\nTo implement the EAI patterns we use a combination of a generic component\\nwhich handles the communication with Kafka and a FaaS solution. The business logic of the EAI patterns (message splitting\/aggregation or transformation) is provided via functions which are hosted on the FaaS solution. The generic component communicates with Kafka and de\/serializes the messages. We need a means to wire the instances of the generic component with the functions. E.g. A user wants to insert a message splitter between two message-based components. To realize this an instance of the generic component in combination with a splitting FaaS function will be used. The generic component needs the address of the FaaS gateway and the function name (e.g. http:\/\/address:8080\/function\/msg-payload-splitter) to call the function. To provide the necessary information to instances of the generic component we considered the following techniques.\\n## Decision Drivers <!-- optional -->\\n* MUST be supported by the language\/technology which is used to implement the generic component\\n* MUST be easy to integrate into MICO\\n* SHOULD be a well known and proven solution\\n","Decision":"* MUST be supported by the language\/technology which is used to implement the generic component\\n* MUST be easy to integrate into MICO\\n* SHOULD be a well known and proven solution\\nChosen option: \"Environment variables\", because MICO already supports this and it is easy to implement in the generic component.\\n","tokens":231,"id":4694,"text":"## Context and Problem Statement\\nTo implement the EAI patterns we use a combination of a generic component\\nwhich handles the communication with Kafka and a FaaS solution. The business logic of the EAI patterns (message splitting\/aggregation or transformation) is provided via functions which are hosted on the FaaS solution. The generic component communicates with Kafka and de\/serializes the messages. We need a means to wire the instances of the generic component with the functions. E.g. A user wants to insert a message splitter between two message-based components. To realize this an instance of the generic component in combination with a splitting FaaS function will be used. The generic component needs the address of the FaaS gateway and the function name (e.g. http:\/\/address:8080\/function\/msg-payload-splitter) to call the function. To provide the necessary information to instances of the generic component we considered the following techniques.\\n## Decision Drivers <!-- optional -->\\n* MUST be supported by the language\/technology which is used to implement the generic component\\n* MUST be easy to integrate into MICO\\n* SHOULD be a well known and proven solution\\n\n\n##Decision\n* MUST be supported by the language\/technology which is used to implement the generic component\\n* MUST be easy to integrate into MICO\\n* SHOULD be a well known and proven solution\\nChosen option: \"Environment variables\", because MICO already supports this and it is easy to implement in the generic component.\\n","MatchedIds":[979,1575,2342,389,4896],"Predicted":"## Decision\nWe have decided to use **environment variables** to provide the necessary information (FaaS gateway address and function name) to instances of the generic component. This decision is based on the following considerations:\n\n### Positive Consequences\n- **Language\/Technology Support**: Environment variables are universally supported across different programming languages and technologies, including those used to implement the generic component.\n- **Ease of Integration**: Using environment variables is straightforward and easy to integrate into MICO, as it does not require complex configurations or additional setup.\n- **Proven Solution**: Environment variables are a well-known and proven solution for passing configuration parameters to applications, making them reliable for our use case.\n- **Flexibility**: Environment variables can be easily modified without changing the code, providing flexibility during deployment and scaling.\n\n### Negative Consequences\n- **Environment Management**: Proper management of environment variables is necessary to ensure they are securely stored and correctly set in different environments (development, testing, production).\n- **Runtime Changes**: Changes to environment variables require a restart of the application or container, which might be less convenient for dynamic configurations.\n\nThis approach offers a balanced trade-off between simplicity, integration ease, and flexibility, making it suitable for our requirements.","GenTime":"2024-08-19 23:49:32"}
{"File Name":"openchs-adr\/0009-dos-and-donts-for-building-health-module-api-responses.md","Context":"## Context\\nSome rules regarding usage of health-module apis.\\n","Decision":"Input for health-modules can be anything from openchs-models\\nOutput of health-modules is loose right now. There is no reason at present to fix this.\\nIf a method returns an array of decisions, it has to return the same array everytime. For example, if it has [{\"highRisk\": ['overweight']}], even if there are no high risks detected, decisions have to return ['highRisk': []], and not an empty array.\\n","tokens":13,"id":2823,"text":"## Context\\nSome rules regarding usage of health-module apis.\\n\n\n##Decision\nInput for health-modules can be anything from openchs-models\\nOutput of health-modules is loose right now. There is no reason at present to fix this.\\nIf a method returns an array of decisions, it has to return the same array everytime. For example, if it has [{\"highRisk\": ['overweight']}], even if there are no high risks detected, decisions have to return ['highRisk': []], and not an empty array.\\n","MatchedIds":[1180,5070,3853,4525,2181],"Predicted":"## Decision\nTo ensure consistency and reliability in the use of health-module APIs, the following rules will be implemented:\n\n1. **Authorization and Authentication**: \n    - All health-module API requests must include a valid OAuth 2.0 bearer token in the `Authorization` header.\n    - Tokens will be issued by our identity provider and must be refreshed prior to expiration to maintain access.\n\n2. **Rate Limiting**:\n    - API calls to the health-module endpoints will be limited to 1000 requests per hour per client. Exceeding this limit will result in HTTP 429 (Too Many Requests) responses.\n\n3. **Request and Response Format**:\n    - All API requests must use the JSON format for request payloads.\n    - API responses will be formatted according to the [JSON-API](https:\/\/jsonapi.org\/format\/#document-structure) specification.\n\n4. **Error Handling**:\n    - API responses must include appropriate HTTP status codes.\n    - Error responses will include a consistent structure with `code`, `message`, and `details` attributes as specified in [our error documentation](0007-error-handling-and-status-codes.md).\n    \n5. **Versioning**:\n    - The health-module API will use URI versioning. Current and future versions will be denoted by a `v` prefix in the URL (e.g., `\/api\/v1\/health`).\n\n6. **Data Integrity**:\n    - All POST, PUT, and PATCH requests must include validation to ensure data consistency and integrity.\n    - Data schema will be enforced, and invalid data submissions will result in a HTTP 400 (Bad Request) response.\n\n7. **Logging and Monitoring**:\n    - All interactions with the health-module API will be logged for audit and monitoring purposes.\n    - Logs will include the timestamp, request method, endpoint, and response status.\n\n8. **Caching**:\n    - Responses from GET requests may be cached for up to 300 seconds unless otherwise specified by cache-control headers.\n    - Responses involving sensitive patient data will not be cached to ensure confidentiality.\n\nThese rules aim to ensure secure, consistent, and high-performance interactions with our health-module APIs.","GenTime":"2024-08-19 23:49:49"}
{"File Name":"operational-data-hub\/0017-ddd-defines-ubiquitous-language.md","Context":"## Context\\nDomain-driven design (DDD) is the concept that the structure and language of software code (class names, class methods, class variables) should match the business domain. For example, if a software processes loan applications, it might have classes such as LoanApplication and Customer, and methods such as AcceptOffer and Withdraw.\\nDDD connects the implementation to an evolving model.\\nDomain-driven design is predicated on the following goals:\\n- placing the project's primary focus on the core domain and domain logic;\\n- basing complex designs on a model of the domain;\\n- initiating a creative collaboration between technical and domain experts to iteratively refine a conceptual model that addresses particular domain problems.\\nConcepts of the model include:\\n### Context\\nThe setting in which a word or statement appears that determines its meaning;\\n### Domain\\nA sphere of knowledge (ontology), influence, or activity. The subject area to which the user applies a program is the domain of the software;\\n### Model\\nA system of abstractions that describes selected aspects of a domain and can be used to solve problems related to that domain;\\n### Ubiquitous Language\\nA language structured around the domain model and used by all team members to connect all the activities of the team with the software.\\n","Decision":"We In the context of the ODH we will use the pricniples of Domain Driven Design and use the language which is related to the business where it is operated.\\n","tokens":261,"id":2696,"text":"## Context\\nDomain-driven design (DDD) is the concept that the structure and language of software code (class names, class methods, class variables) should match the business domain. For example, if a software processes loan applications, it might have classes such as LoanApplication and Customer, and methods such as AcceptOffer and Withdraw.\\nDDD connects the implementation to an evolving model.\\nDomain-driven design is predicated on the following goals:\\n- placing the project's primary focus on the core domain and domain logic;\\n- basing complex designs on a model of the domain;\\n- initiating a creative collaboration between technical and domain experts to iteratively refine a conceptual model that addresses particular domain problems.\\nConcepts of the model include:\\n### Context\\nThe setting in which a word or statement appears that determines its meaning;\\n### Domain\\nA sphere of knowledge (ontology), influence, or activity. The subject area to which the user applies a program is the domain of the software;\\n### Model\\nA system of abstractions that describes selected aspects of a domain and can be used to solve problems related to that domain;\\n### Ubiquitous Language\\nA language structured around the domain model and used by all team members to connect all the activities of the team with the software.\\n\n\n##Decision\nWe In the context of the ODH we will use the pricniples of Domain Driven Design and use the language which is related to the business where it is operated.\\n","MatchedIds":[2498,4896,4271,2756,1416],"Predicted":"## Decision\nWe will adopt Domain-Driven Design (DDD) principles to guide the structure and language of our software code. This will include defining our classes, methods, and variables to match the business domain closely. For instance, if we are processing loan applications, we will have classes such as `LoanApplication` and `Customer`, and methods such as `AcceptOffer` and `Withdraw`.\n\nOur approach will ensure:\n1. **Primary focus on the core domain and domain logic:** Prioritize the core business problems and their solutions in our codebase.\n2. **Complex designs based on a domain model:** Develop a robust domain model that reflects the real-world business and addresses domain-specific problems.\n3. **Creative collaboration:** Foster an environment where technical and domain experts collaborate iteratively to refine a conceptual model and solve domain-related issues effectively.\n\nTo support these goals, we will establish a **Ubiquitous Language** that will be used by all team members, ensuring consistency and clarity across all activities within the team, thereby bridging the gap between technical and domain experts.","GenTime":"2024-08-19 23:49:53"}
{"File Name":"meadow\/0009-tailwind-css-framework.md","Context":"## Context\\nWe have used opinionated CSS frameworks such as Twitter Bootstrap in the past, and have found that we spend too much effort and time working around those opinions. Tailwind CSS offers an alternative approach that allows us to iterate quickly with minimal interference from the framework by allowing us to add layout and styles directly in our HTML rather than CSS.\\n","Decision":"Use the Tailwind CSS framework for design and layout.\\n","tokens":70,"id":3829,"text":"## Context\\nWe have used opinionated CSS frameworks such as Twitter Bootstrap in the past, and have found that we spend too much effort and time working around those opinions. Tailwind CSS offers an alternative approach that allows us to iterate quickly with minimal interference from the framework by allowing us to add layout and styles directly in our HTML rather than CSS.\\n\n\n##Decision\nUse the Tailwind CSS framework for design and layout.\\n","MatchedIds":[1992,425,157,1240,2655],"Predicted":"## Decision\nWe will adopt Tailwind CSS as our primary styling framework.\n\n### Positive Consequences\n- **Increased Flexibility:** Tailwind's utility-first approach allows us to design directly in HTML without fighting against pre-defined framework styles.\n- **Rapid Development:** By leveraging utility classes, we can achieve faster prototyping and iterative development cycles.\n- **Consistency:** Tailwind encourages the reuse of utility classes, which promotes consistency in our design system without heavy reliance on external stylesheets.\n- **Customizability:** Tailwind allows for extensive configuration and theming, providing us with the ability to tailor the framework to our specific needs.\n- **Size Reduction:** Unused styles can be purged from the production build, resulting in a smaller CSS footprint.\n\n### Negative Consequences\n- **Steeper Learning Curve:** Developers unfamiliar with utility-first CSS might need some time to get accustomed to this new approach.\n- **Markup Clutter:** Utility classes can make HTML markup appear more verbose and cluttered, potentially impacting readability.\n- **Initial Setup:** Although Tailwind is highly configurable, setting up a comprehensive configuration to meet all our specific needs may require an initial investment of time and effort.\n- **Transition Effort:** Migrating existing components and styles to Tailwind will require considerable effort and coordination, especially for a large and established codebase.\n\n### Action Items\n- Integrate Tailwind CSS into our build process.\n- Provide training and documentation to help the development team get up to speed with using Tailwind.\n- Incrementally migrate existing styles to Tailwind while maintaining a consistent look and feel.\n- Regularly revisit and optimize our Tailwind configuration to ensure it remains aligned with our evolving requirements.","GenTime":"2024-08-19 23:50:00"}
{"File Name":"cosmos-sdk\/adr-016-validator-consensus-key-rotation.md","Context":"## Context\\nValidator consensus key rotation feature has been discussed and requested for a long time, for the sake of safer validator key management policy (e.g. https:\/\/github.com\/tendermint\/tendermint\/issues\/1136). So, we suggest one of the simplest form of validator consensus key rotation implementation mostly onto Cosmos SDK.\\nWe don't need to make any update on consensus logic in Tendermint because Tendermint does not have any mapping information of consensus key and validator operator key, meaning that from Tendermint point of view, a consensus key rotation of a validator is simply a replacement of a consensus key to another.\\nAlso, it should be noted that this ADR includes only the simplest form of consensus key rotation without considering multiple consensus keys concept. Such multiple consensus keys concept shall remain a long term goal of Tendermint and Cosmos SDK.\\n","Decision":"### Pseudo procedure for consensus key rotation\\n* create new random consensus key.\\n* create and broadcast a transaction with a `MsgRotateConsPubKey` that states the new consensus key is now coupled with the validator operator with signature from the validator's operator key.\\n* old consensus key becomes unable to participate on consensus immediately after the update of key mapping state on-chain.\\n* start validating with new consensus key.\\n* validators using HSM and KMS should update the consensus key in HSM to use the new rotated key after the height `h` when `MsgRotateConsPubKey` committed to the blockchain.\\n### Considerations\\n* consensus key mapping information management strategy\\n* store history of each key mapping changes in the kvstore.\\n* the state machine can search corresponding consensus key paired with given validator operator for any arbitrary height in a recent unbonding period.\\n* the state machine does not need any historical mapping information which is past more than unbonding period.\\n* key rotation costs related to LCD and IBC\\n* LCD and IBC will have traffic\/computation burden when there exists frequent power changes\\n* In current Tendermint design, consensus key rotations are seen as power changes from LCD or IBC perspective\\n* Therefore, to minimize unnecessary frequent key rotation behavior, we limited maximum number of rotation in recent unbonding period and also applied exponentially increasing rotation fee\\n* limits\\n* rotations are limited to 1 time in an unbonding window. In future rewrites of the staking module it could be made to happen more times than 1\\n* parameters can be decided by governance and stored in genesis file.\\n* key rotation fee\\n* a validator should pay `KeyRotationFee` to rotate the consensus key which is calculated as below\\n* `KeyRotationFee` = (max(`VotingPowerPercentage`, 1)* `InitialKeyRotationFee`) * 2^(number of rotations in `ConsPubKeyRotationHistory` in recent unbonding period)\\n* evidence module\\n* evidence module can search corresponding consensus key for any height from slashing keeper so that it can decide which consensus key is supposed to be used for given height.\\n* abci.ValidatorUpdate\\n* tendermint already has ability to change a consensus key by ABCI communication(`ValidatorUpdate`).\\n* validator consensus key update can be done via creating new + delete old by change the power to zero.\\n* therefore, we expect we even do not need to change tendermint codebase at all to implement this feature.\\n* new genesis parameters in `staking` module\\n* `MaxConsPubKeyRotations` : maximum number of rotation can be executed by a validator in recent unbonding period. default value 10 is suggested(11th key rotation will be rejected)\\n* `InitialKeyRotationFee` : the initial key rotation fee when no key rotation has happened in recent unbonding period. default value 1atom is suggested(1atom fee for the first key rotation in recent unbonding period)\\n### Workflow\\n1. The validator generates a new consensus keypair.\\n2. The validator generates and signs a `MsgRotateConsPubKey` tx with their operator key and new ConsPubKey\\n```go\\ntype MsgRotateConsPubKey struct {\\nValidatorAddress  sdk.ValAddress\\nNewPubKey         crypto.PubKey\\n}\\n```\\n3. `handleMsgRotateConsPubKey` gets `MsgRotateConsPubKey`, calls `RotateConsPubKey` with emits event\\n4. `RotateConsPubKey`\\n* checks if `NewPubKey` is not duplicated on `ValidatorsByConsAddr`\\n* checks if the validator is does not exceed parameter `MaxConsPubKeyRotations` by iterating `ConsPubKeyRotationHistory`\\n* checks if the signing account has enough balance to pay `KeyRotationFee`\\n* pays `KeyRotationFee` to community fund\\n* overwrites `NewPubKey` in `validator.ConsPubKey`\\n* deletes old `ValidatorByConsAddr`\\n* `SetValidatorByConsAddr` for `NewPubKey`\\n* Add `ConsPubKeyRotationHistory` for tracking rotation\\n```go\\ntype ConsPubKeyRotationHistory struct {\\nOperatorAddress         sdk.ValAddress\\nOldConsPubKey           crypto.PubKey\\nNewConsPubKey           crypto.PubKey\\nRotatedHeight           int64\\n}\\n```\\n5. `ApplyAndReturnValidatorSetUpdates` checks if there is `ConsPubKeyRotationHistory` with `ConsPubKeyRotationHistory.RotatedHeight == ctx.BlockHeight()` and if so, generates 2 `ValidatorUpdate` , one for a remove validator and one for create new validator\\n```go\\nabci.ValidatorUpdate{\\nPubKey: cmttypes.TM2PB.PubKey(OldConsPubKey),\\nPower:  0,\\n}\\nabci.ValidatorUpdate{\\nPubKey: cmttypes.TM2PB.PubKey(NewConsPubKey),\\nPower:  v.ConsensusPower(),\\n}\\n```\\n6. at `previousVotes` Iteration logic of `AllocateTokens`,  `previousVote` using `OldConsPubKey` match up with `ConsPubKeyRotationHistory`, and replace validator for token allocation\\n7. Migrate `ValidatorSigningInfo` and `ValidatorMissedBlockBitArray` from `OldConsPubKey` to `NewConsPubKey`\\n* Note : All above features shall be implemented in `staking` module.\\n","tokens":171,"id":841,"text":"## Context\\nValidator consensus key rotation feature has been discussed and requested for a long time, for the sake of safer validator key management policy (e.g. https:\/\/github.com\/tendermint\/tendermint\/issues\/1136). So, we suggest one of the simplest form of validator consensus key rotation implementation mostly onto Cosmos SDK.\\nWe don't need to make any update on consensus logic in Tendermint because Tendermint does not have any mapping information of consensus key and validator operator key, meaning that from Tendermint point of view, a consensus key rotation of a validator is simply a replacement of a consensus key to another.\\nAlso, it should be noted that this ADR includes only the simplest form of consensus key rotation without considering multiple consensus keys concept. Such multiple consensus keys concept shall remain a long term goal of Tendermint and Cosmos SDK.\\n\n\n##Decision\n### Pseudo procedure for consensus key rotation\\n* create new random consensus key.\\n* create and broadcast a transaction with a `MsgRotateConsPubKey` that states the new consensus key is now coupled with the validator operator with signature from the validator's operator key.\\n* old consensus key becomes unable to participate on consensus immediately after the update of key mapping state on-chain.\\n* start validating with new consensus key.\\n* validators using HSM and KMS should update the consensus key in HSM to use the new rotated key after the height `h` when `MsgRotateConsPubKey` committed to the blockchain.\\n### Considerations\\n* consensus key mapping information management strategy\\n* store history of each key mapping changes in the kvstore.\\n* the state machine can search corresponding consensus key paired with given validator operator for any arbitrary height in a recent unbonding period.\\n* the state machine does not need any historical mapping information which is past more than unbonding period.\\n* key rotation costs related to LCD and IBC\\n* LCD and IBC will have traffic\/computation burden when there exists frequent power changes\\n* In current Tendermint design, consensus key rotations are seen as power changes from LCD or IBC perspective\\n* Therefore, to minimize unnecessary frequent key rotation behavior, we limited maximum number of rotation in recent unbonding period and also applied exponentially increasing rotation fee\\n* limits\\n* rotations are limited to 1 time in an unbonding window. In future rewrites of the staking module it could be made to happen more times than 1\\n* parameters can be decided by governance and stored in genesis file.\\n* key rotation fee\\n* a validator should pay `KeyRotationFee` to rotate the consensus key which is calculated as below\\n* `KeyRotationFee` = (max(`VotingPowerPercentage`, 1)* `InitialKeyRotationFee`) * 2^(number of rotations in `ConsPubKeyRotationHistory` in recent unbonding period)\\n* evidence module\\n* evidence module can search corresponding consensus key for any height from slashing keeper so that it can decide which consensus key is supposed to be used for given height.\\n* abci.ValidatorUpdate\\n* tendermint already has ability to change a consensus key by ABCI communication(`ValidatorUpdate`).\\n* validator consensus key update can be done via creating new + delete old by change the power to zero.\\n* therefore, we expect we even do not need to change tendermint codebase at all to implement this feature.\\n* new genesis parameters in `staking` module\\n* `MaxConsPubKeyRotations` : maximum number of rotation can be executed by a validator in recent unbonding period. default value 10 is suggested(11th key rotation will be rejected)\\n* `InitialKeyRotationFee` : the initial key rotation fee when no key rotation has happened in recent unbonding period. default value 1atom is suggested(1atom fee for the first key rotation in recent unbonding period)\\n### Workflow\\n1. The validator generates a new consensus keypair.\\n2. The validator generates and signs a `MsgRotateConsPubKey` tx with their operator key and new ConsPubKey\\n```go\\ntype MsgRotateConsPubKey struct {\\nValidatorAddress  sdk.ValAddress\\nNewPubKey         crypto.PubKey\\n}\\n```\\n3. `handleMsgRotateConsPubKey` gets `MsgRotateConsPubKey`, calls `RotateConsPubKey` with emits event\\n4. `RotateConsPubKey`\\n* checks if `NewPubKey` is not duplicated on `ValidatorsByConsAddr`\\n* checks if the validator is does not exceed parameter `MaxConsPubKeyRotations` by iterating `ConsPubKeyRotationHistory`\\n* checks if the signing account has enough balance to pay `KeyRotationFee`\\n* pays `KeyRotationFee` to community fund\\n* overwrites `NewPubKey` in `validator.ConsPubKey`\\n* deletes old `ValidatorByConsAddr`\\n* `SetValidatorByConsAddr` for `NewPubKey`\\n* Add `ConsPubKeyRotationHistory` for tracking rotation\\n```go\\ntype ConsPubKeyRotationHistory struct {\\nOperatorAddress         sdk.ValAddress\\nOldConsPubKey           crypto.PubKey\\nNewConsPubKey           crypto.PubKey\\nRotatedHeight           int64\\n}\\n```\\n5. `ApplyAndReturnValidatorSetUpdates` checks if there is `ConsPubKeyRotationHistory` with `ConsPubKeyRotationHistory.RotatedHeight == ctx.BlockHeight()` and if so, generates 2 `ValidatorUpdate` , one for a remove validator and one for create new validator\\n```go\\nabci.ValidatorUpdate{\\nPubKey: cmttypes.TM2PB.PubKey(OldConsPubKey),\\nPower:  0,\\n}\\nabci.ValidatorUpdate{\\nPubKey: cmttypes.TM2PB.PubKey(NewConsPubKey),\\nPower:  v.ConsensusPower(),\\n}\\n```\\n6. at `previousVotes` Iteration logic of `AllocateTokens`,  `previousVote` using `OldConsPubKey` match up with `ConsPubKeyRotationHistory`, and replace validator for token allocation\\n7. Migrate `ValidatorSigningInfo` and `ValidatorMissedBlockBitArray` from `OldConsPubKey` to `NewConsPubKey`\\n* Note : All above features shall be implemented in `staking` module.\\n","MatchedIds":[180,1949,21,4848,839],"Predicted":"## Decision\nWe will implement a simple validator consensus key rotation mechanism within the Cosmos SDK. This implementation will allow validators to replace their existing consensus keys with new ones without requiring any changes to the Tendermint consensus logic. \n\n### Key Details:\n1. **Validator Key Replacement**: A new `MsgRotateConsKey` message type will be introduced. Validators can submit this message to propose the replacement of their current consensus key with a new one.\n2. **Authorization and Validation**: The `MsgRotateConsKey` message must be signed by the validator operator key (conforming to the validator's current account in the state). The new consensus key must also be valid and unique.\n3. **State Update**: Upon successful validation and authorization, the Cosmos SDK will update the validator's consensus key in the staking module's state.\n4. **No Consensus Logic Change in Tendermint**: This ADR strictly avoids modifications to Tendermint's core consensus mechanism. For Tendermint, the key rotation will appear as a simple key replacement without any additional complexity.\n5. **Grace Period**: A grace period will be incorporated to ensure network stability during the key rotation. This ensures that the replacement doesn't affect ongoing consensus processes.\n6. **Transaction Lifecycle**:\n    - Validator submits `MsgRotateConsKey` with the new consensus key.\n    - The message is authenticated using the validator's operator key.\n    - Upon successful validation, the new key replaces the old key in the state.\n    - The network respects the grace period before the new key takes effect.\n\nThis decision aligns our implementation with the long-term goal of possibly supporting multiple consensus keys in the future, while focusing on a straightforward and minimally disruptive approach to key rotation now.\n\n### Example `MsgRotateConsKey` Protobuf Definition:\n```proto\nmessage MsgRotateConsKey {\n    string validator_address = 1; \/\/ Address of the validator operator\n    string new_consensus_pubkey = 2; \/\/ New consensus public key\n}\n```\n\n### CLI\/REST Interface:\n- **CLI Command**: `gaiacli tx staking rotate-cons-key [validator-addr] [new-consensus-pubkey] --from [operator-addr]`\n- **REST Endpoint**: `POST \/staking\/rotate-cons-key`\n\nThis decision aims to enhance validator key management safety while maintaining compatibility and simplicity within the current ecosystem.","GenTime":"2024-08-19 23:50:17"}
{"File Name":"heroes-board-game\/0002-tests-definition-and-scope.md","Context":"## Context and Problem Statement\\nAcceptance tests are embedded into game server application.\\nShould they be written one layer above (UI)?\\n## Decision Drivers\\n* Acceptance tests should be meaningful for end users\\n","Decision":"* Acceptance tests should be meaningful for end users\\nRedefine current acceptance tests and treat them like _component_ tests according to [this definition](https:\/\/www.simpleorientedarchitecture.com\/defining-test-boundaries\/).\\n* Good, because it promotes different tests for different audiences\\n* Good, because it sets a clear terminology\\n* Good, because end users don't care about layers below UI\\n","tokens":44,"id":3166,"text":"## Context and Problem Statement\\nAcceptance tests are embedded into game server application.\\nShould they be written one layer above (UI)?\\n## Decision Drivers\\n* Acceptance tests should be meaningful for end users\\n\n\n##Decision\n* Acceptance tests should be meaningful for end users\\nRedefine current acceptance tests and treat them like _component_ tests according to [this definition](https:\/\/www.simpleorientedarchitecture.com\/defining-test-boundaries\/).\\n* Good, because it promotes different tests for different audiences\\n* Good, because it sets a clear terminology\\n* Good, because end users don't care about layers below UI\\n","MatchedIds":[650,2816,4695,4359,4785],"Predicted":"## Decision\nAcceptance tests will be written at the UI layer to ensure they are meaningful and understandable for end users. \n\n## Justification\nBy placing acceptance tests one layer above the game server application, we can better simulate end-user interactions and scenarios. This approach ensures that the tests are more aligned with the user perspective and capture the complete flow of functionality from the user interface to the backend.\n\n### Positive Consequences\n* Tests will be more representative of actual user interactions, increasing their relevance and coverage.\n* Enhances the clarity and understanding of acceptance tests for all stakeholders, including non-technical team members.\n* Helps in identifying issues at the interface level which might be missed when testing solely at the server layer.\n\n### Negative Consequences\n* May require additional setup and maintenance for UI testing environments and tools.\n* Could lead to longer test execution times compared to running tests at the server layer only.\n\n### Implementation\nWe will use a framework such as [Cypress](https:\/\/www.cypress.io\/) or [Selenium](https:\/\/www.selenium.dev\/) to write and execute these acceptance tests at the UI layer.","GenTime":"2024-08-19 23:50:23"}
{"File Name":"CCS-Architecture-Decision-Records\/0008-cmp-use-shared-store-for-capturing-buyer-needs.md","Context":"## Context\\nEach commercial agreement will ask the buyer questions in order to ascertain what outcomes and supplier offers are applicable to the buyer. Some of these questions will be the same across agreements. Many will have common patterns and data, even if they differ in detail.\\nIn the future we will want to more automatically match buyers\u2019 articulations of need so as to work out which products CCS can offer to meet their need.\\nThe buyer needs data needs to be stored during a buying journey, but final agreement details, when the buyer agrees a call-off for instance, will need to be stored in the agreement records.\\n","Decision":"Use a shared component to store user tasks in a question and answer format during buying journeys\\nThe Q&A definitions should be defined from a common repository defining the agreements\\nAgreed outcomes must be stored in the agreement records\\n","tokens":123,"id":1001,"text":"## Context\\nEach commercial agreement will ask the buyer questions in order to ascertain what outcomes and supplier offers are applicable to the buyer. Some of these questions will be the same across agreements. Many will have common patterns and data, even if they differ in detail.\\nIn the future we will want to more automatically match buyers\u2019 articulations of need so as to work out which products CCS can offer to meet their need.\\nThe buyer needs data needs to be stored during a buying journey, but final agreement details, when the buyer agrees a call-off for instance, will need to be stored in the agreement records.\\n\n\n##Decision\nUse a shared component to store user tasks in a question and answer format during buying journeys\\nThe Q&A definitions should be defined from a common repository defining the agreements\\nAgreed outcomes must be stored in the agreement records\\n","MatchedIds":[2183,1863,2045,4841,914],"Predicted":"## Decision\nWe will implement a two-tier storage solution:\n1. **Transient Storage for Buying Journey Data**: We will use a NoSQL database (e.g., MongoDB or DynamoDB) to store buyer needs data during the buying journey. This will allow us to store the data in a flexible, schema-less manner, accommodating the varying questions and common patterns easily.\n  \n2. **Persistent Storage for Final Agreement Records**: Once the buyer agrees to a call-off, the final agreement details will be moved to a relational database (e.g., PostgreSQL or MySQL). This ensures that we maintain a well-structured and referentially intact record of all agreements.\n\nTo support future automation and matching:\n- Develop a mapping service that uses Machine Learning models to classify and match buyers' needs with relevant CCS products. This service will continuously learn and improve its accuracy over time.\n- Store common question patterns and data in a shared repository to be reused across different commercial agreements, thus maintaining consistency and reducing redundancy.\n  \n### Implementation Steps\n1. Design the schema for storing transient buyer journey data in a NoSQL database.\n2. Design the schema for storing final agreements in a relational database.\n3. Develop APIs to facilitate the seamless transfer of data from transient storage to persistent storage upon agreement completion.\n4. Implement a mapping service to enable automated matching of buyer needs to products.\n5. Develop a shared repository for common question patterns and data.\n6. Perform regular audits to ensure data consistency and integrity between the NoSQL and relational databases.","GenTime":"2024-08-19 23:50:30"}
{"File Name":"structurizr-python\/0005-code-quality-assurance.md","Context":"## Context\\nWriting code that adheres to style guides and other best practices can be\\nannoying. We want to standardize on some best-in-class tools.\\n","Decision":"We will use isort, black, and flake8.\\n","tokens":35,"id":400,"text":"## Context\\nWriting code that adheres to style guides and other best practices can be\\nannoying. We want to standardize on some best-in-class tools.\\n\n\n##Decision\nWe will use isort, black, and flake8.\\n","MatchedIds":[3853,1408,5255,4226,3321],"Predicted":"## Decision\nWe will adopt a set of standardized tools to enforce code style and best practices across all projects:\n\n1. **Code Formatter**: Use [Prettier](https:\/\/prettier.io\/) for formatting JavaScript and other supported languages to ensure a consistent code style.\n2. **Linting**: Use [ESLint](https:\/\/eslint.org\/) for JavaScript to catch syntax and style guide violations. For other languages, appropriate linting tools will be chosen (e.g., [Rubocop](https:\/\/rubocop.org\/) for Ruby).\n3. **Git Hooks**: Implement Git hooks using [Husky](https:\/\/github.com\/typicode\/husky) to automatically run the above tools before committing code.\n4. **Continuous Integration**: Integrate these tools into our CI pipeline to reject commits that do not adhere to the enforced code style or best practices.\n\nBy standardizing on these tools, we aim to reduce the cognitive load on developers and ensure high code quality across all projects.","GenTime":"2024-08-19 23:50:34"}
{"File Name":"stamper\/0003-spring-boot-as-web-framework.md","Context":"## Context\\nWe need to choose a web framework for the app.\\n","Decision":"We use Spring Boot because it allows us starting fast, and concentrating on business logic of the app, rather than working on infrastructure tasks.\\n","tokens":15,"id":1243,"text":"## Context\\nWe need to choose a web framework for the app.\\n\n\n##Decision\nWe use Spring Boot because it allows us starting fast, and concentrating on business logic of the app, rather than working on infrastructure tasks.\\n","MatchedIds":[1240,4271,469,1239,3470],"Predicted":"## Decision\nWe will use the [Django](https:\/\/www.djangoproject.com\/) web framework. This will provide a high level of abstraction and comes with many built-in features like authentication, admin interface, and ORM. Django is a reliable choice because of its \"batteries-included\" philosophy, extensive documentation, and wide community support. It is particularly suitable for rapidly developing secure and maintainable web applications.","GenTime":"2024-08-19 23:50:37"}
{"File Name":"snippets-service\/0005-frequency-capping.md","Context":"## Context\\nFrequency Capping allows Content Managers to limit the number of\\nimpressions or interactions users have with content. Is a widely\\navailable tool in Publishing Platforms.\\nIt's usually developed on the server side where the system can decide\\nhow many times to serve the content to the requesting users which we\\ncall \"Global Frequency Capping\". Additionally the system may be able\\nto limit the number of impressions per user which we call \"Local\" or\\n\"User Frequency Capping\".\\nFor example a Content Piece can be set to 1,000,000 Global Impressions\\nand 1 Impression per User, thus indirectly driving 1,000,000 different\\nusers to this Content.\\nThis functionality has been lacking from the Snippet Service due to\\ntechnical limitations imposed by the way metrics were collected and\\ncontent selection was handled on the client side. The latest\\ndevelopments in Firefox Messaging Center and the Firefox Telemetry\\nPipeline unblock this capability. [0]\\n","Decision":"We decide to implement the Frequency Capping functionality into our\\nplatform to allow Content Managers to limit the number of Impressions,\\nClicks and Blocks per Job.\\nLocal or User Frequency Capping will be handled on the Browser level\\nby the Firefox Messaging Platform. The later supports only Impression\\nFrequency Capping.\\nThe Snippets Service will provide an interface (UI) for the Content\\nManagers to set upper limits on the number of Impressions a Job gets\\nper Hour, Day, Week, Fortnight, Month or for the complete Browser\\nProfile Lifetime. This information is included in the JSON generated\\nfor each Job.\\nFor Global Frequency Capping the Snippets Service will provide an\\ninterface (UI) for the Content Managers to set the limits on total\\nworldwide number of Impressions, Clicks and Blocks per Job.\\nSnippets Service will query Mozilla's Redash for Telemetry data every\\nten minutes and will fetch current impressions, clicks, blocks for\\neach Job with set limits.\\nWhen the reported numbers exceed the set limits then, the Job will be\\nmarked COMPLETE and will be pulled out of the Bundles on the next run\\nof `update_jobs` cron job.\\nThe Frequency Capping functionality is additional to the Date\\nPublishing controls, therefore a Job can end on a specific Date and\\nTime or when its Global Frequency Capping Limits are met.\\n### Monitoring and Handling of Errors\\nSince Global Frequency Capping depends on an external system for\\nMetrics (Redash \/ Telemetry) it is possible that the latest numbers are\\nnot always available to the Snippets Service to make a decision. Such\\ncases include scheduled or unplanned service interruptions or network\\nerrors.\\nIn co-ordination with Snippet Content Owner we decided that for cases\\nwhere the Snippets Service cannot get the latest numbers for more than\\n24 hours, Jobs with Global Frequency Capping will get canceled. The\\ncancellation reason will state that the Jobs where prematurely\\nterminated due to missing metrics.\\nThe cron job responsible for fetching the Data from Telemetry is\\nmonitored by a Dead Man's Snitch.\\n","tokens":208,"id":3229,"text":"## Context\\nFrequency Capping allows Content Managers to limit the number of\\nimpressions or interactions users have with content. Is a widely\\navailable tool in Publishing Platforms.\\nIt's usually developed on the server side where the system can decide\\nhow many times to serve the content to the requesting users which we\\ncall \"Global Frequency Capping\". Additionally the system may be able\\nto limit the number of impressions per user which we call \"Local\" or\\n\"User Frequency Capping\".\\nFor example a Content Piece can be set to 1,000,000 Global Impressions\\nand 1 Impression per User, thus indirectly driving 1,000,000 different\\nusers to this Content.\\nThis functionality has been lacking from the Snippet Service due to\\ntechnical limitations imposed by the way metrics were collected and\\ncontent selection was handled on the client side. The latest\\ndevelopments in Firefox Messaging Center and the Firefox Telemetry\\nPipeline unblock this capability. [0]\\n\n\n##Decision\nWe decide to implement the Frequency Capping functionality into our\\nplatform to allow Content Managers to limit the number of Impressions,\\nClicks and Blocks per Job.\\nLocal or User Frequency Capping will be handled on the Browser level\\nby the Firefox Messaging Platform. The later supports only Impression\\nFrequency Capping.\\nThe Snippets Service will provide an interface (UI) for the Content\\nManagers to set upper limits on the number of Impressions a Job gets\\nper Hour, Day, Week, Fortnight, Month or for the complete Browser\\nProfile Lifetime. This information is included in the JSON generated\\nfor each Job.\\nFor Global Frequency Capping the Snippets Service will provide an\\ninterface (UI) for the Content Managers to set the limits on total\\nworldwide number of Impressions, Clicks and Blocks per Job.\\nSnippets Service will query Mozilla's Redash for Telemetry data every\\nten minutes and will fetch current impressions, clicks, blocks for\\neach Job with set limits.\\nWhen the reported numbers exceed the set limits then, the Job will be\\nmarked COMPLETE and will be pulled out of the Bundles on the next run\\nof `update_jobs` cron job.\\nThe Frequency Capping functionality is additional to the Date\\nPublishing controls, therefore a Job can end on a specific Date and\\nTime or when its Global Frequency Capping Limits are met.\\n### Monitoring and Handling of Errors\\nSince Global Frequency Capping depends on an external system for\\nMetrics (Redash \/ Telemetry) it is possible that the latest numbers are\\nnot always available to the Snippets Service to make a decision. Such\\ncases include scheduled or unplanned service interruptions or network\\nerrors.\\nIn co-ordination with Snippet Content Owner we decided that for cases\\nwhere the Snippets Service cannot get the latest numbers for more than\\n24 hours, Jobs with Global Frequency Capping will get canceled. The\\ncancellation reason will state that the Jobs where prematurely\\nterminated due to missing metrics.\\nThe cron job responsible for fetching the Data from Telemetry is\\nmonitored by a Dead Man's Snitch.\\n","MatchedIds":[1866,2067,1857,1894,3171],"Predicted":"## Decision\nImplement Frequency Capping for the Snippet Service utilizing the Firefox Messaging Center and Firefox Telemetry Pipeline. \n\n### Key Changes:\n1. **Global Frequency Capping**: Utilize the server-side capabilities to limit the total number of impressions or interactions globally. This will ensure content does not exceed the predefined global impression limits.\n\n2. **User Frequency Capping**: Enable per-user frequency capping leveraging the user-specific data from the Firefox Telemetry Pipeline to limit the number of impressions per individual user.\n\n### Benefits:\n1. **Enhanced Content Management**: Allows content managers to have finer control over the reach and exposure of content.\n2. **Improved User Experience**: Prevents users from being overwhelmed by repetitive content, improving overall satisfaction.\n3. **Data-Driven Decisions**: Utilizes reliable data from the Firefox Telemetry Pipeline to inform capping decisions.\n\n### Implementation Steps:\n1. **Telemetry Data Integration**: Integrate the Snippet Service with the Firefox Telemetry Pipeline to access user-specific and global interaction data.\n2. **Server-Side Logic Development**: Develop server-side logic to calculate and enforce both global and user-level impression limits.\n3. **Content Management Interface Update**: Update the content management interface to allow content managers to set global and user frequency caps for content pieces.\n4. **Testing and Validation**: Conduct thorough testing to ensure the accuracy and reliability of the frequency capping mechanisms.\n5. **Documentation and Training**: Document the new functionality and provide training to content managers on how to effectively use frequency capping.\n\nBy leveraging the latest developments in Firefox Messaging Center and Telemetry Pipeline, we can now support robust frequency capping in the Snippet Service, aligning with industry standards and improving content delivery.","GenTime":"2024-08-19 23:50:43"}
{"File Name":"adr\/ADR-1-documenting-architecture-decisions.md","Context":"## Context\\nArchitecture for agile projects has to be described and defined differently. Not all decisions will be made at once, nor will all of them be done when the project begins.\\nAgile methods are not opposed to documentation, only to valueless documentation. Documents that assist the team itself can have value, but only if they are kept up to date. Large documents are never kept up to date. Small, modular documents have at least a chance at being updated.\\nNobody ever reads large documents, either. Most developers have been on at least one project where the specification document was larger (in bytes) than the total source code size. Those documents are too large to open, read, or update. Bite sized pieces are easier for for all stakeholders to consume.\\nOne of the hardest things to track during the life of a project is the motivation behind certain decisions. A new person coming on to a project may be perplexed, baffled, delighted, or infuriated by some past decision. Without understanding the rationale or consequences, this person has only two choices:\\n1. **Blindly accept the decision.**\\nThis response may be OK, if the decision is still valid. It may not be good, however, if the context has changed and the decision should really be revisited. If the project accumulates too many decisions accepted without understanding, then the development team becomes afraid to change anything and the project collapses under its own weight.\\n2. **Blindly change it.**\\nAgain, this may be OK if the decision needs to be reversed. On the other hand, changing the decision without understanding its motivation or consequences could mean damaging the project's overall value without realizing it. (E.g., the decision supported a non-functional requirement that hasn't been tested yet.)\\nIt's better to avoid either blind acceptance or blind reversal.\\n","Decision":"We will keep a collection of records for \"architecturally significant\" decisions: those that affect the structure, non-functional characteristics, dependencies, interfaces, or construction techniques.\\nAn architecture decision record is a short text file in a format similar to an Alexandrian pattern. (Though the decisions themselves are not necessarily patterns, they share the characteristic balancing of forces.) Each record describes a set of forces and a single decision in response to those forces. Note that the decision is the central piece here, so specific forces may appear in multiple ADRs.\\nWe will keep ADRs in the project repository under `docs\/ADR-####-title.md`\\nWe should use a lightweight text formatting language like Markdown or Textile.\\nADRs will be numbered sequentially and monotonically. Numbers will not be reused.\\nIf a decision is reversed, we will keep the old one around, but mark it as superseded. (It's still relevant to know that it _was_ the decision, but is _no longer_ the decision.)\\nWe will use a format with just a few parts, so each document is easy to digest. The format has just a few parts.\\n**Title** These documents have names that are short noun phrases. For example, \"ADR 1: Deployment on Ruby on Rails 3.0.10\" or \"ADR 9: LDAP for Multitenant Integration\"\\n**Context** This section describes the forces at play, including technological, political, social, and project local. These forces are probably in tension, and should be called out as such. The language in this section is value-neutral. It is simply describing facts.\\n**Decision** This section describes our response to these forces. It is stated in full sentences, with active voice. \"We will \u2026\"\\n**Status** A decision may be \"proposed\" if the project stakeholders haven't agreed with it yet, or \"accepted\" once it is agreed. If a later ADR changes or reverses a decision, it may be marked as \"deprecated\" or \"superseded\" with a reference to its replacement.\\n**Consequences** This section describes the resulting context, after applying the decision. All consequences should be listed here, not just the \"positive\" ones. A particular decision may have positive, negative, and neutral consequences, but all of them affect the team and project in the future.\\nThe whole document should be one or two pages long. We will write each ADR as if it is a conversation with a future developer. This requires good writing style, with full sentences organized into paragraphs. Bullets are acceptable only for visual style, not as an excuse for writing sentence fragments. (Bullets kill people, even PowerPoint bullets.)\\n# Status\\nAccepted.\\n# Consequences\\nOne ADR describes one significant decision for a specific project. It should be something that has an effect on how the rest of the project will run.\\nThe consequences of one ADR are very likely to become the context for subsequent ADRs. This is also similar to Alexander's idea of a pattern language: the large-scale responses create spaces for the smaller scale to fit into.\\nDevelopers and project stakeholders can see the ADRs, even as the team composition changes over time.\\nThe motivation behind previous decisions is visible for everyone, present and future. Nobody is left scratching their heads to understand, \"What were they thinking?\" and the time to change old decisions will be clear from changes in the project's context.\\n---\\nOriginal document: [Documenting Architecture Decisions](https:\/\/cognitect.com\/blog\/2011\/11\/15\/documenting-architecture-decisions) by Michael Nygard\\nMore reading:\\n1. [Importance of architecture decisions](http:\/\/www.computer.org\/portal\/web\/csdl\/doi\/10.1109\/MS.2009.52)\\n2. [Documenting software architectures](http:\/\/www.sei.cmu.edu\/library\/abstracts\/books\/0321552687.cfm)\\n","tokens":380,"id":4620,"text":"## Context\\nArchitecture for agile projects has to be described and defined differently. Not all decisions will be made at once, nor will all of them be done when the project begins.\\nAgile methods are not opposed to documentation, only to valueless documentation. Documents that assist the team itself can have value, but only if they are kept up to date. Large documents are never kept up to date. Small, modular documents have at least a chance at being updated.\\nNobody ever reads large documents, either. Most developers have been on at least one project where the specification document was larger (in bytes) than the total source code size. Those documents are too large to open, read, or update. Bite sized pieces are easier for for all stakeholders to consume.\\nOne of the hardest things to track during the life of a project is the motivation behind certain decisions. A new person coming on to a project may be perplexed, baffled, delighted, or infuriated by some past decision. Without understanding the rationale or consequences, this person has only two choices:\\n1. **Blindly accept the decision.**\\nThis response may be OK, if the decision is still valid. It may not be good, however, if the context has changed and the decision should really be revisited. If the project accumulates too many decisions accepted without understanding, then the development team becomes afraid to change anything and the project collapses under its own weight.\\n2. **Blindly change it.**\\nAgain, this may be OK if the decision needs to be reversed. On the other hand, changing the decision without understanding its motivation or consequences could mean damaging the project's overall value without realizing it. (E.g., the decision supported a non-functional requirement that hasn't been tested yet.)\\nIt's better to avoid either blind acceptance or blind reversal.\\n\n\n##Decision\nWe will keep a collection of records for \"architecturally significant\" decisions: those that affect the structure, non-functional characteristics, dependencies, interfaces, or construction techniques.\\nAn architecture decision record is a short text file in a format similar to an Alexandrian pattern. (Though the decisions themselves are not necessarily patterns, they share the characteristic balancing of forces.) Each record describes a set of forces and a single decision in response to those forces. Note that the decision is the central piece here, so specific forces may appear in multiple ADRs.\\nWe will keep ADRs in the project repository under `docs\/ADR-####-title.md`\\nWe should use a lightweight text formatting language like Markdown or Textile.\\nADRs will be numbered sequentially and monotonically. Numbers will not be reused.\\nIf a decision is reversed, we will keep the old one around, but mark it as superseded. (It's still relevant to know that it _was_ the decision, but is _no longer_ the decision.)\\nWe will use a format with just a few parts, so each document is easy to digest. The format has just a few parts.\\n**Title** These documents have names that are short noun phrases. For example, \"ADR 1: Deployment on Ruby on Rails 3.0.10\" or \"ADR 9: LDAP for Multitenant Integration\"\\n**Context** This section describes the forces at play, including technological, political, social, and project local. These forces are probably in tension, and should be called out as such. The language in this section is value-neutral. It is simply describing facts.\\n**Decision** This section describes our response to these forces. It is stated in full sentences, with active voice. \"We will \u2026\"\\n**Status** A decision may be \"proposed\" if the project stakeholders haven't agreed with it yet, or \"accepted\" once it is agreed. If a later ADR changes or reverses a decision, it may be marked as \"deprecated\" or \"superseded\" with a reference to its replacement.\\n**Consequences** This section describes the resulting context, after applying the decision. All consequences should be listed here, not just the \"positive\" ones. A particular decision may have positive, negative, and neutral consequences, but all of them affect the team and project in the future.\\nThe whole document should be one or two pages long. We will write each ADR as if it is a conversation with a future developer. This requires good writing style, with full sentences organized into paragraphs. Bullets are acceptable only for visual style, not as an excuse for writing sentence fragments. (Bullets kill people, even PowerPoint bullets.)\\n# Status\\nAccepted.\\n# Consequences\\nOne ADR describes one significant decision for a specific project. It should be something that has an effect on how the rest of the project will run.\\nThe consequences of one ADR are very likely to become the context for subsequent ADRs. This is also similar to Alexander's idea of a pattern language: the large-scale responses create spaces for the smaller scale to fit into.\\nDevelopers and project stakeholders can see the ADRs, even as the team composition changes over time.\\nThe motivation behind previous decisions is visible for everyone, present and future. Nobody is left scratching their heads to understand, \"What were they thinking?\" and the time to change old decisions will be clear from changes in the project's context.\\n---\\nOriginal document: [Documenting Architecture Decisions](https:\/\/cognitect.com\/blog\/2011\/11\/15\/documenting-architecture-decisions) by Michael Nygard\\nMore reading:\\n1. [Importance of architecture decisions](http:\/\/www.computer.org\/portal\/web\/csdl\/doi\/10.1109\/MS.2009.52)\\n2. [Documenting software architectures](http:\/\/www.sei.cmu.edu\/library\/abstracts\/books\/0321552687.cfm)\\n","MatchedIds":[2517,3784,2261,160,4699],"Predicted":"## Decision\nWe will use Lightweight Architecture Decision Records (ADRs) to document architectural decisions in a modular, bite-sized format. Each ADR will be kept short, focused, and up to date, adhering to the following guidelines:\n\n1. **Version Control**: All ADRs will be stored in a Git repository to leverage version control.\n2. **Structure**: Each ADR will be a separate Markdown file prefixed by an ordered, sequential number.\n3. **Length**: ADRs will be limited to 1-2 pages to ensure they remain concise and easy to read.\n4. **Template**: We will use a standard template for all ADRs, including Context, Decision, and Consequences sections.\n5. **Lifecycle**: ADRs will remain in the repository indefinitely but will be marked as superseded if a decision is no longer valid.\n6. **Collaboration**: Team members will be encouraged to contribute to and update ADRs to reflect the current state and rationale behind decisions.\n\nBy employing these practices, we can ensure that our documentation remains valuable, understandable, and easy to maintain, thereby supporting agile methodologies effectively.","GenTime":"2024-08-19 23:50:49"}
{"File Name":"TANF-app\/015-terraform-service-provisioning.md","Context":"## Context\\nIn [this issue](https:\/\/github.com\/raft-tech\/TANF-app\/issues\/609) we propose tasks to automate the provisioning of Cloud.gov-brokered services via our CI pipeline.\\nThis project plans to use Cloud.gov-brokered S3 buckets for TANF data file storage (sensitive PII data) and for Django Admin static assets (public), and a Cloud.gov-brokered RDS PostgreSQL service. Currently, there are no scripts to provision and secure these S3 buckets and RDS instance, and no dev documentation on how to deploy them, secure them, and verify that they are configured correctly. Additionally, this same initial provisioning must be done again for all additional target environments.\\n","Decision":"[Terraform](https:\/\/www.terraform.io\/) is a tool for building, changing, and versioning infrastructure safely and efficiently, and was proposed as a solution for managing our persistent Cloud.gov-brokered infrastructure. We can closely model what is done in [another HHS project](https:\/\/github.com\/HHS\/Head-Start-TTADP) and create per-environment infrastructure configurations which are leveraged by Terraform in CircleCI, with environment-specific settings read directly from Cloud.gov during our CI process. Note that this workflow was a [recommendation from Cloud.gov](https:\/\/www.youtube.com\/watch?v=86wfgNK_0o4), and they themselves use Terraform to provision their own infrastructure.\\n","tokens":148,"id":3794,"text":"## Context\\nIn [this issue](https:\/\/github.com\/raft-tech\/TANF-app\/issues\/609) we propose tasks to automate the provisioning of Cloud.gov-brokered services via our CI pipeline.\\nThis project plans to use Cloud.gov-brokered S3 buckets for TANF data file storage (sensitive PII data) and for Django Admin static assets (public), and a Cloud.gov-brokered RDS PostgreSQL service. Currently, there are no scripts to provision and secure these S3 buckets and RDS instance, and no dev documentation on how to deploy them, secure them, and verify that they are configured correctly. Additionally, this same initial provisioning must be done again for all additional target environments.\\n\n\n##Decision\n[Terraform](https:\/\/www.terraform.io\/) is a tool for building, changing, and versioning infrastructure safely and efficiently, and was proposed as a solution for managing our persistent Cloud.gov-brokered infrastructure. We can closely model what is done in [another HHS project](https:\/\/github.com\/HHS\/Head-Start-TTADP) and create per-environment infrastructure configurations which are leveraged by Terraform in CircleCI, with environment-specific settings read directly from Cloud.gov during our CI process. Note that this workflow was a [recommendation from Cloud.gov](https:\/\/www.youtube.com\/watch?v=86wfgNK_0o4), and they themselves use Terraform to provision their own infrastructure.\\n","MatchedIds":[4025,3789,2471,3793,771],"Predicted":"## Decision\nWe will implement the automation of provisioning Cloud.gov-brokered services through our CI pipeline using Terraform and CircleCI. This approach will ensure consistency and security across all environments and facilitate repeatable deployments. The decision details are as follows:\n\n1. **Terraform Configuration**:\n    - Develop Terraform scripts to provision Cloud.gov-brokered S3 buckets and RDS PostgreSQL services.\n    - Ensure that the S3 bucket for TANF data file storage is configured with appropriate security settings to handle sensitive PII data.\n    - Ensure that the S3 bucket for Django Admin static assets is correctly configured to serve public assets securely.\n    - Configure the RDS PostgreSQL instance with appropriate security groups, encryption, and backups.\n\n2. **CircleCI Integration**:\n    - Add a new job in the CircleCI pipeline to execute the Terraform scripts for provisioning.\n    - Configure CircleCI with the necessary environment variables for Cloud.gov authentication and Terraform state management.\n    - Ensure that the CircleCI job includes steps for both provisioning new resources and updating existing resources.\n\n3. **Documentation**:\n    - Create comprehensive documentation for the Terraform scripts, detailing how to deploy, secure, and verify the configuration of the provisioned resources.\n    - Include instructions for setting up local development environments to mimic the CI pipeline for testing purposes.\n    - Document the process for initial provisioning and subsequent updates for each environment.\n\n4. **Environment Configuration**:\n    - The provisioning process will be executed initially for the development environment.\n    - Once verified, the same Terraform scripts will be used, with appropriate parameterization, to provision resources in staging and production environments.\n\nBy automating the provisioning of Cloud.gov services through a CI pipeline and documenting the process thoroughly, we ensure a secure, consistent, and repeatable deployment strategy for TANF data storage and application services.","GenTime":"2024-08-19 23:51:00"}
{"File Name":"moneycount-api\/002-Choose_Persist_Currency_In_Json.md","Context":"## Context\\nI have to decide how to persist data related to supported currencies in the project. Choices are a relational database or text files such as txt, xml or json. Even NoSQL databases could be used.\\n","Decision":"I decided to use json files to persist supported currencies in the project because this kind of data doesn't change all the time. JSON files are easy to store, to write, and to read in Java Objects. Each time a new currency is added it's just a matter of update the file.\\n","tokens":44,"id":3071,"text":"## Context\\nI have to decide how to persist data related to supported currencies in the project. Choices are a relational database or text files such as txt, xml or json. Even NoSQL databases could be used.\\n\n\n##Decision\nI decided to use json files to persist supported currencies in the project because this kind of data doesn't change all the time. JSON files are easy to store, to write, and to read in Java Objects. Each time a new currency is added it's just a matter of update the file.\\n","MatchedIds":[2520,3657,1167,4859,4747],"Predicted":"## Decision\nGiven that the data related to supported currencies is likely to be structured, relational, and possibly require frequent updates or lookups, a relational database is the most appropriate choice.\n\nChosen option: \"Relational Database (PostgreSQL)\", for the following reasons:\n\n### Positive Consequences\n* **Ease of Queries**: Complex queries and aggregations can be easily performed using SQL.\n* **Data Integrity**: Referential integrity, constraints, and transactions are natively supported.\n* **Scalability and Performance**: PostgreSQL provides excellent performance for read and write operations and can scale as the data grows.\n* **Widely Understood**: PostgreSQL is a widely used and understood technology in the development community, ensuring good support and resources.\n\n### Negative Consequences\n* **Setup and Maintenance**: Setting up and maintaining a PostgreSQL instance might require more initial effort compared to storing data in simple text files.\n* **Cost**: Depending on the deployment environment, there might be additional costs compared to using plain text files or even some NoSQL solutions.\n\nBackup solutions, indexing, and transactional support provided by PostgreSQL enhance the overall robustness and reliability of the system, making it an optimal choice for persisting data related to supported currencies.","GenTime":"2024-08-19 23:51:06"}
{"File Name":"edgex-docs\/0018-Service-Registry.md","Context":"- [Context](#context)\\n- [Existing Behavior](#existing-behavior)\\n* [Device Services](#device-services)\\n+ [Registry Client Interface Usage](#registry-client-interface-usage)\\n* [Core and Support Services](#core-and-support-services)\\n* [Security Proxy Setup](#security-proxy-setup)\\n- [History](#history)\\n- [Problem Statement](#problem-statement)\\n- [Decision](#decision)\\n- [References](#references)\\n<!--te-->\\n","Decision":"- [References](#references)\\n<!--te-->\\nUpdate the core, support, and security-proxy-setup services to use go-mod-registry's ```Client.GetServiceEndpoint``` method (if started with the\\n```--registry``` option) to determine (a) if a service dependency is available and (b) use the returned address information to initialize client endpoints\\n(or setup the correct route in the case of proxy-setup). The same changes also need to be applied to the App Functions SDK and Go Device SDK, with\\nonly minor changes required in the C Device SDK (see previous commments re: the current implementation).\\n**Note** - this design only works if service registration occurs _before_ the service initializes its clients. For instance, Core Data and Core Metadata\\nboth depend on the other, and thus if both defer service registration till after client initialization, neither will be able to successfully lookup\\nthe address of the other service.\\n","tokens":111,"id":946,"text":"- [Context](#context)\\n- [Existing Behavior](#existing-behavior)\\n* [Device Services](#device-services)\\n+ [Registry Client Interface Usage](#registry-client-interface-usage)\\n* [Core and Support Services](#core-and-support-services)\\n* [Security Proxy Setup](#security-proxy-setup)\\n- [History](#history)\\n- [Problem Statement](#problem-statement)\\n- [Decision](#decision)\\n- [References](#references)\\n<!--te-->\\n\n\n##Decision\n- [References](#references)\\n<!--te-->\\nUpdate the core, support, and security-proxy-setup services to use go-mod-registry's ```Client.GetServiceEndpoint``` method (if started with the\\n```--registry``` option) to determine (a) if a service dependency is available and (b) use the returned address information to initialize client endpoints\\n(or setup the correct route in the case of proxy-setup). The same changes also need to be applied to the App Functions SDK and Go Device SDK, with\\nonly minor changes required in the C Device SDK (see previous commments re: the current implementation).\\n**Note** - this design only works if service registration occurs _before_ the service initializes its clients. For instance, Core Data and Core Metadata\\nboth depend on the other, and thus if both defer service registration till after client initialization, neither will be able to successfully lookup\\nthe address of the other service.\\n","MatchedIds":[947,3784,4549,4547,4910],"Predicted":"## Decision\n\n### Context\n\nOur project involves multiple device services, core services, and support services which rely on a centralized registry for service discovery. The current implementation of registry client interfaces across different services varies, leading to inconsistencies, maintenance challenges, and potential security risks. Additionally, setting up security proxies adds another layer of complexity that demands a standardized approach.\n\n### Existing Behavior\n\n#### Device Services\n\nDevice services use the registry client interface for service discovery and configuration. However, different device services have been observed to implement registry client interfaces differently, causing a lack of uniformity.\n\n##### Registry Client Interface Usage\n\nSome device services employ direct calls to the registry, while others use intermediary layers or libraries that abstract registry interactions. This disparity introduces points of failure and complicated debugging processes.\n\n#### Core and Support Services\n\nCore and support services face similar issues with inconsistent registry client interfaces. These inconsistencies can lead to fragmented service discovery patterns and make coordinated updates across services difficult.\n\n#### Security Proxy Setup\n\nSetting up security proxies for these services is also inconsistent. Each service may use different mechanisms and security layers, leading to increased attack surfaces and operational overhead.\n\n### History\n\nInitially, the registry client interfaces were developed on a per-service basis, leading to fragmented approaches tailored to each services' immediate requirements. As the number of services grew, these inconsistencies became more pronounced. Similarly, the ad hoc approach to security proxy setups has resulted in a patchwork of configurations.\n\n### Problem Statement\n\nThe lack of a unified registry client interface and standardized security proxy setups leads to:\n- Increased maintenance burden.\n- Difficulty in performing coordinated updates.\n- Higher probability of security vulnerabilities.\n- Inefficiencies in service discovery and configuration management.\n\n### Decision\n\n**Unify Registry Client Interfaces and Security Proxy Setup**\n\n1. **Registry Client Interface Standardization**:\n    - Develop a common registry client library that all device, core, and support services must use.\n    - This library will offer standardized methods for service registration, discovery, and configuration fetching.\n    - It will include a robust error-handling mechanism and well-defined interfaces for extension if service-specific customizations are needed.\n\n2. **Centralized Security Proxy Setup**:\n    - Create a centralized configuration for security proxies that all services must adhere to.\n    - Use a common security library that integrates with the registry client library to provide consistent security mechanisms.\n    - Ensure the security proxy setup follows best practices in security architecture and minimizes configuration overhead for individual services.\n\n### Implementation Steps\n\n1. **Design and Develop the Common Registry Client Library**: \n    - Identify common registry operations across all services.\n    - Define and document the standardized API for these operations.\n    - Implement the library and thoroughly test it across a representative set of services.\n\n2. **Refactor Existing Services**:\n    - Refactor device, core, and support services to replace existing registry client interface implementations with the new library.\n    - Ensure backward compatibility and incremental adoption where feasible.\n\n3. **Security Proxy Setup Standardization**:\n    - Develop a common security library that integrates with the registry client.\n    - Create centralized configuration templates and guidelines for proxy setup.\n    - Conduct security reviews to ensure adherence to best practices.\n\n4. **Documentation and Training**:\n    - Document the usage of the new registry client and security libraries.\n    - Conduct training sessions for development and operations teams to ensure smooth adoption.\n\n5. **Monitoring and Feedback**:\n    - Implement monitoring to track the adoption of the new libraries.\n    - Collect feedback from teams to identify any issues or areas for improvement.\n\n### References\n\n- [Registry Client Interface Documentation](#)\n- [Security Best Practices for Microservices](#)\n- [Service Discovery and Configuration Management](#)\n- [Centralized Security Proxy Configuration Examples](#)\n\nBy unifying the registry client interfaces and standardizing security proxy setups, we aim to reduce complexity, improve maintainability, and enhance the overall security posture of our services.","GenTime":"2024-08-19 23:51:19"}
{"File Name":"knot-documentation\/ARC-002-fog-cloud-sync.md","Context":"## Context\\nOne of the main questions that appear when discussing the new fog service development\/integration was how the device contextual representation and data should be synchronized between fog and cloud. Before, when we were using the Meshblu we decided to send all the messages (device registration, schema, and data) to the cloud independently of the message type, data relevance at time or cloud connectivity state. This solution was simple for that moment but was increasing the internal message queues size and obviously isn't optimal mainly when dealing with a constrained environment. Therefore, our goal is to start with a simple but optimal approach in the first stage of the new fog service integration.\\n","Decision":"* Retry to register and update the device's schema on the cloud five times and if the error persists retry with ten minutes interval.\\n* Retry to publish only the last device's data.\\n* Retry to execute only the last commands sent to the device, avoiding to store a lot of pending commands on the message queue (which could lead to inconsistencies).\\n","tokens":132,"id":3337,"text":"## Context\\nOne of the main questions that appear when discussing the new fog service development\/integration was how the device contextual representation and data should be synchronized between fog and cloud. Before, when we were using the Meshblu we decided to send all the messages (device registration, schema, and data) to the cloud independently of the message type, data relevance at time or cloud connectivity state. This solution was simple for that moment but was increasing the internal message queues size and obviously isn't optimal mainly when dealing with a constrained environment. Therefore, our goal is to start with a simple but optimal approach in the first stage of the new fog service integration.\\n\n\n##Decision\n* Retry to register and update the device's schema on the cloud five times and if the error persists retry with ten minutes interval.\\n* Retry to publish only the last device's data.\\n* Retry to execute only the last commands sent to the device, avoiding to store a lot of pending commands on the message queue (which could lead to inconsistencies).\\n","MatchedIds":[979,2824,786,2309,3488],"Predicted":"## Decision\nWe will adopt a selective synchronization approach for the new fog service integration.\n\n### Key Aspects of the Approach:\n1. **Message Filtering**: Implement message filtering to only send high-priority and time-sensitive messages to the cloud.\n2. **Batch Processing**: Utilize batch processing to group less critical messages and send them periodically, reducing the frequency and size of message transfers.\n3. **Local Caching**: Store non-critical data locally on the fog nodes and synchronize with the cloud when connectivity is optimal.\n4. **Priority-based Queues**: Introduce priority-based message queues to ensure that critical data is transmitted first.\n5. **Connectivity Aware Synchronization**: Develop mechanisms to adapt synchronization frequency based on the current network connectivity status.\n\n### Positive Consequences\n- **Reduced Queue Sizes**: By filtering and batching messages, the internal message queues will have fewer items.\n- **Bandwidth Optimization**: Selective synchronization ensures that only essential data is transmitted, saving bandwidth, especially important in constrained environments.\n- **Improved Performance**: By minimizing unnecessary data transfer, the system\u2019s overall performance will be enhanced.\n\n### Negative Consequences\n- **Increased Complexity**: Implementing filtering and batching mechanisms adds to the complexity of the fog service.\n- **Potential Data Staleness**: There is a potential for less critical data to become slightly outdated if not synchronized in real-time.\n\nBy taking these steps, we aim to optimize our approach without overloading the system or sacrificing critical data synchronization needs.","GenTime":"2024-08-19 23:51:27"}
{"File Name":"Marain.Instance\/0001-marain-service-list-and-instance-manifests.md","Context":"## Context\\nThe desire to define formally and unambiguously what are the component parts of a Marain instance. (An 'instance' is one deployed set of services operating in isolation from any other instance. We maintain a dev instance separate from any production use. Where customers use Marain services but are not using endjin's hosted production instance, they have their own instance.)\\n","Decision":"The `Marain.Instance` repo (this repo) includes a master service list, `Solutions\/MarainServices.jsonc`. This JSON (with comments) file contains an entry for each service that can be part of a Marain instance. This gives a name to the service (e.g. `Marain.Tenancy`) and identifies the GitHub project in which the service is defined. It also defines an API prefix, for use in scenarios where all services are made available behind a single API management layer\u2014the API prefix indicates what the first part of the URL should be at the API gateway for accessing the relevant service.\\nWhereas `MarainServices.jsonc` is common to all instances, each instance also defines a manifest. This determines whether particular services are deployed to a particular instance, and if so which version.\\n","tokens":79,"id":357,"text":"## Context\\nThe desire to define formally and unambiguously what are the component parts of a Marain instance. (An 'instance' is one deployed set of services operating in isolation from any other instance. We maintain a dev instance separate from any production use. Where customers use Marain services but are not using endjin's hosted production instance, they have their own instance.)\\n\n\n##Decision\nThe `Marain.Instance` repo (this repo) includes a master service list, `Solutions\/MarainServices.jsonc`. This JSON (with comments) file contains an entry for each service that can be part of a Marain instance. This gives a name to the service (e.g. `Marain.Tenancy`) and identifies the GitHub project in which the service is defined. It also defines an API prefix, for use in scenarios where all services are made available behind a single API management layer\u2014the API prefix indicates what the first part of the URL should be at the API gateway for accessing the relevant service.\\nWhereas `MarainServices.jsonc` is common to all instances, each instance also defines a manifest. This determines whether particular services are deployed to a particular instance, and if so which version.\\n","MatchedIds":[359,4056,1180,1742,1172],"Predicted":"## Decision\n\nTo provide a clear and formal definition of what constitutes a Marain instance, we have made the following decisions:\n\n1. **Core Services**:\n   - Each Marain instance will be composed of a predefined set of core services. These core services are required for the basic functioning of the system and include, but are not limited to:\n     - Identity Service\n     - API Gateway\n     - Workflow Engine\n     - Operations Control\n     - Data Storage (e.g., Cosmos DB or equivalent)\n     - Logging and Monitoring Services\n     - Configuration Management\n\n2. **Environment Specific Services**:\n   - There will be two primary environments for each instance: Development and Production.\n   - Development instances will include additional services and tools necessary for testing, debugging, and development, such as:\n     - Mock Services\n     - Debugging Tools\n     - Sandbox Data Storage\n   - Production instances will be optimized for performance, security, and reliability. They will exclude development-specific tools and services.\n\n3. **Infrastructure Components**:\n   - Each instance will include the necessary infrastructure components to support deployment, networking, and security:\n     - Virtual Networks and Subnets\n     - Load Balancers\n     - Firewalls and Security Groups\n     - Resource Managers (e.g., ARM Templates for Azure)\n\n4. **Tenant Isolation**:\n   - Each instance will ensure multi-tenancy support, where each tenant (client) has isolated resources as defined by Marain's tenancy model.\n   - Resource isolation will be enforced using hierarchical tenant IDs specific to each instance\u2019s core, services, and client-specific sub-tenants.\n\n5. **Configuration and Secrets Management**:\n   - Each instance will include mechanisms for handling configuration settings and secrets specific to that instance:\n     - Configuration settings will be centralized and versioned.\n     - Secrets will be stored securely using tools such as Azure Key Vault or AWS Secrets Manager.\n\n6. **Dependency and Integration Services**:\n   - Each Marain instance will include services necessary for integrating with other systems and third-party services where required:\n     - External API integrations\n     - Data Pipelines and ETL processes\n     - Messaging and Event Systems\n\n7. **Monitoring and Maintenance**:\n   - Each instance will be monitored and maintained independently:\n     - Health check endpoints for core services\n     - Centralized logging and analytics\n     - Automated backup and recovery processes\n     - Maintenance windows and update policies\n\nBy formally defining and documenting these components, we ensure that every Marain instance is consistent, maintainable, and operates in isolation from other instances while meeting the specific needs of its environment (Development or Production).","GenTime":"2024-08-19 23:51:35"}
{"File Name":"gsp\/ADR028-container-tools.md","Context":"## Context\\nWe are currently using Docker as our container runtime.\\nThere are needs for public docker images:\\n* so that master builds can be easily pulled and tested in the local development environments\\n* so that images can be easily shared between different teams\\nThere are needs for digitally verifying the provenance of images:\\n* so that production systems can trust that an image has not been tampered with\\n* so that production systems can authenticate the origin of a build\\nThere are needs for vulnerability scanning:\\n* so that production systems can warn or prevent exploitable software running in production\\nThe docker ecosystem provides tooling that can help us meet these needs:\\n* [Docker Content Trust] (Notary) can be used to sign images and prove provenance\\n* [Docker Registries][Docker Registry] can expose images publicly\\n* Scanning tools like [Clair] can periodically or at pull\/push time perform CVE scanning.\\nUnfortunately AWS [ECR] does not currently support public images or [Docker Content Trust], and there is no managed solution to image scanning from AWS as yet.\\n### Option 1: Wrap AWS ECR\\nWe could write\/manage a proxy to allow exposing [ECR] publicly and integrate the missing features.\\n* Potentially fragile implementation tied to the underlying AWS services\\n* Another thing to have to maintain\\n* Might offer ability to remove functionality as AWS support more features in future\\n### Option 2: Use an external SaaS offering\\nUse a SaaS service like [Quay] which offers most of these features.\\n* Additional configuration for cluster to pull from external source\\n* Reduces ability to automate provisioning (requires additional credential management)\\n* Procurement\\n### Option 3: Self-hosted Docker Tools in cluster\\nWe could deploy Docker Distribution, Notary & Clair into the cluster backed by a managed storage backend like S3\\n* Well integrated with the platform\\n* Would work for local GSP instance\\n","Decision":"We will run a self hosted set of Docker tools\\n","tokens":406,"id":3917,"text":"## Context\\nWe are currently using Docker as our container runtime.\\nThere are needs for public docker images:\\n* so that master builds can be easily pulled and tested in the local development environments\\n* so that images can be easily shared between different teams\\nThere are needs for digitally verifying the provenance of images:\\n* so that production systems can trust that an image has not been tampered with\\n* so that production systems can authenticate the origin of a build\\nThere are needs for vulnerability scanning:\\n* so that production systems can warn or prevent exploitable software running in production\\nThe docker ecosystem provides tooling that can help us meet these needs:\\n* [Docker Content Trust] (Notary) can be used to sign images and prove provenance\\n* [Docker Registries][Docker Registry] can expose images publicly\\n* Scanning tools like [Clair] can periodically or at pull\/push time perform CVE scanning.\\nUnfortunately AWS [ECR] does not currently support public images or [Docker Content Trust], and there is no managed solution to image scanning from AWS as yet.\\n### Option 1: Wrap AWS ECR\\nWe could write\/manage a proxy to allow exposing [ECR] publicly and integrate the missing features.\\n* Potentially fragile implementation tied to the underlying AWS services\\n* Another thing to have to maintain\\n* Might offer ability to remove functionality as AWS support more features in future\\n### Option 2: Use an external SaaS offering\\nUse a SaaS service like [Quay] which offers most of these features.\\n* Additional configuration for cluster to pull from external source\\n* Reduces ability to automate provisioning (requires additional credential management)\\n* Procurement\\n### Option 3: Self-hosted Docker Tools in cluster\\nWe could deploy Docker Distribution, Notary & Clair into the cluster backed by a managed storage backend like S3\\n* Well integrated with the platform\\n* Would work for local GSP instance\\n\n\n##Decision\nWe will run a self hosted set of Docker tools\\n","MatchedIds":[3598,3602,3920,1376,3914],"Predicted":"## Decision\nWe will use an external SaaS offering, specifically [Quay.io], to meet our requirements for public docker images, digital verification, and vulnerability scanning.\n\n### Justification:\n\n1. **Features Availability**:\n    - **Public Docker Images**: Quay.io supports public docker images which facilitate easy sharing between different teams and testing in local development environments.\n    - **Digital Verification**: Quay.io integrates Docker Content Trust (Notary) for signing images and proving provenance, ensuring that production systems can trust and authenticate the origin of builds.\n    - **Vulnerability Scanning**: Quay.io has built-in support for vulnerability scanning upon push\/pull operations, addressing the need for security in production environments.\n\n2. **Maintenance Overhead**:\n    - **Reduction in Maintenance**: By using a SaaS offering, we avoid the complexity and overhead associated with managing and maintaining additional infrastructure ourselves, such as implementing a proxy for ECR or deploying Docker tools in the cluster.\n    - **Updates and Upgrades**: Quay.io, being a managed service, will take care of infrastructure updates, feature upgrades, and security patches, thus lightening the load on our internal IT and devops teams.\n\n3. **Integration Challenges**:\n    - **Minimal Configuration**: While using Quay.io will require some initial setup and configuration to handle credential management for cluster pulls, this is a one-time activity and can be streamlined.\n    - **Automated Provisioning**: Even though there may be some challenges in automating provisioning due to additional credential management, these can be mitigated with proper scripting and documentation.\n\n4. **Scalability and Future-Proofing**:\n    - **Adaptability**: Should our requirements evolve in the future, Quay.io offers a robust and adaptable platform that can scale with our needs.\n    - **Consistency and Reliability**: Utilization of an external, well-supported, and mature service ensures that our container image registry is reliable and consistently available.\n\nThis decision balances feature completeness, maintenance overhead, and future scalability, making Quay.io the most appropriate choice for our container image management needs.","GenTime":"2024-08-19 23:51:43"}
{"File Name":"beis-report-official-development-assistance\/0009-use-pundit-to-manage-permissions.md","Context":"## Context\\nThe service will be used by a variety of different types of users, and we need\\nto ensure that those users are only able to access the parts of the service that\\nwe want them to.\\nOur current chosen authentication provider, Auth0, has support for assigning\\nroles to users, but this couples the service tightly to their service, so we\\nshould avoid this.\\n","Decision":"We will use the 'Pundit' gem to manage users' access to the service. The\\npermissions will be grouped into roles that can then be assigned to users\\nrequiring a particular level of access.\\n","tokens":81,"id":2397,"text":"## Context\\nThe service will be used by a variety of different types of users, and we need\\nto ensure that those users are only able to access the parts of the service that\\nwe want them to.\\nOur current chosen authentication provider, Auth0, has support for assigning\\nroles to users, but this couples the service tightly to their service, so we\\nshould avoid this.\\n\n\n##Decision\nWe will use the 'Pundit' gem to manage users' access to the service. The\\npermissions will be grouped into roles that can then be assigned to users\\nrequiring a particular level of access.\\n","MatchedIds":[3950,578,2405,527,2041],"Predicted":"## Decision\nTo ensure that our service maintains flexibility in user management and is not tightly coupled to Auth0, we will implement a custom role-based access control (RBAC) system within our application. This system will manage user roles and permissions independently of Auth0, allowing us to switch authentication providers in the future without a significant overhaul of our role management system.\n\nWe will store user roles and permissions in our own database, and these will be associated with user profiles maintained by Auth0. Upon user authentication via Auth0, our application will fetch and apply the appropriate roles and permissions from our database to control access to different parts of the service.\n\nBy separating concerns between authentication and authorization, we ensure that we are not vendor-locked into Auth0 for role management while maintaining flexibility to adapt to future changes. This also provides us with a clearer and more granular control over user access levels within our service.","GenTime":"2024-08-19 23:51:47"}
{"File Name":"Wikibase\/0009-refactor-hooks-for-testability.md","Context":"## Context\\nCurrently, the `RepoHooks` class remains largely untested due to a combination of two factors:\\n1. The methods in this class are static, and we do not own the contract under which they should be called, as they are\\ndefined as hooks in `extension.json` or as global variables in the entrypoints e.g. [extensions\/Wikibase\/repo\/Wikibase.php:1020](https:\/\/github.com\/wikimedia\/mediawiki-extensions-Wikibase\/blob\/7b20d22b3c0bbc37ad23f63e38fadc9b1f2ca057\/repo\/Wikibase.php#L1020), which means we cannot easily refactor the methods to increase testability\\n2. Methods rely heavily on the `WikibaseRepo` singleton and it's store, which make it harder to test, as there is no\\nway to to inject a mock of `WikibaseRepo` without dependency injection.\\nA [RFC for enabling dependency injection](https:\/\/phabricator.wikimedia.org\/T240307) in hooks is currently under way.\\nHowever, an interim solution is needed in order to mitigate the amount of untested logic that exists in that file\\nand other places in the codebase.\\nWhile reviewing this issue, two initial solutions were considered:\\n- Refactor `RepoHooks` into a singleton itself, so that when instantiated, we can inject a Mock of `WikibaseRepo`\\ninstead of using the real deal.\\n- Adopt a pattern used in `WikibaseClient` Which enables us to mock several parts of it (namely the store), and replace\\nthe real store by creating an `overrideStore` method. See in following:\\n- [`client\/tests\/phpunit\/includes\/MockClientStore.php`](https:\/\/github.com\/wikimedia\/mediawiki-extensions-Wikibase\/blob\/master\/client\/tests\/phpunit\/includes\/MockClientStore.php)\\n- [`client\/tests\/phpunit\/includes\/DataAccess\/ParserFunctions\/PropertyParserFunctionIntegrationTest.php:42`](https:\/\/github.com\/wikimedia\/mediawiki-extensions-Wikibase\/blob\/master\/client\/tests\/phpunit\/includes\/DataAccess\/ParserFunctions\/PropertyParserFunctionIntegrationTest.php#L42)\\nHowever, after a discussion, it was decided to implement a middle ground, that would enable us to gradually refactor\\nhooks, rather than a one time big change.\\n","Decision":"It was decided to adopt an existing pattern in Wikibase repo where each hook handler gets its own singleton class and\\nprovides at least four methods:\\n- A constructor to make dependency injection easier.\\n- A public static method to bootstrap an instance from global settings. For consistency, this would typically be named\\n`newFromGlobalSettings`.\\n- (optional) A public static method to get a cached instance of the handler object (rather than instantiate it each time):\\nThis is useful for hooks handlers which are called several times.\\n- A public static method to wire up functionality into the hooks system and should contain little to no logic (as it is\\nhard to test without complete integration or e2e tests).\\n- A public method to perform the actual handler logic in a testable way.\\nThis class should be placed under the `includes\/Hooks` directory. An example of this pattern can be seen in:\\nhttps:\/\/gerrit.wikimedia.org\/r\/c\/mediawiki\/extensions\/Wikibase\/+\/574495\\n","tokens":496,"id":1339,"text":"## Context\\nCurrently, the `RepoHooks` class remains largely untested due to a combination of two factors:\\n1. The methods in this class are static, and we do not own the contract under which they should be called, as they are\\ndefined as hooks in `extension.json` or as global variables in the entrypoints e.g. [extensions\/Wikibase\/repo\/Wikibase.php:1020](https:\/\/github.com\/wikimedia\/mediawiki-extensions-Wikibase\/blob\/7b20d22b3c0bbc37ad23f63e38fadc9b1f2ca057\/repo\/Wikibase.php#L1020), which means we cannot easily refactor the methods to increase testability\\n2. Methods rely heavily on the `WikibaseRepo` singleton and it's store, which make it harder to test, as there is no\\nway to to inject a mock of `WikibaseRepo` without dependency injection.\\nA [RFC for enabling dependency injection](https:\/\/phabricator.wikimedia.org\/T240307) in hooks is currently under way.\\nHowever, an interim solution is needed in order to mitigate the amount of untested logic that exists in that file\\nand other places in the codebase.\\nWhile reviewing this issue, two initial solutions were considered:\\n- Refactor `RepoHooks` into a singleton itself, so that when instantiated, we can inject a Mock of `WikibaseRepo`\\ninstead of using the real deal.\\n- Adopt a pattern used in `WikibaseClient` Which enables us to mock several parts of it (namely the store), and replace\\nthe real store by creating an `overrideStore` method. See in following:\\n- [`client\/tests\/phpunit\/includes\/MockClientStore.php`](https:\/\/github.com\/wikimedia\/mediawiki-extensions-Wikibase\/blob\/master\/client\/tests\/phpunit\/includes\/MockClientStore.php)\\n- [`client\/tests\/phpunit\/includes\/DataAccess\/ParserFunctions\/PropertyParserFunctionIntegrationTest.php:42`](https:\/\/github.com\/wikimedia\/mediawiki-extensions-Wikibase\/blob\/master\/client\/tests\/phpunit\/includes\/DataAccess\/ParserFunctions\/PropertyParserFunctionIntegrationTest.php#L42)\\nHowever, after a discussion, it was decided to implement a middle ground, that would enable us to gradually refactor\\nhooks, rather than a one time big change.\\n\n\n##Decision\nIt was decided to adopt an existing pattern in Wikibase repo where each hook handler gets its own singleton class and\\nprovides at least four methods:\\n- A constructor to make dependency injection easier.\\n- A public static method to bootstrap an instance from global settings. For consistency, this would typically be named\\n`newFromGlobalSettings`.\\n- (optional) A public static method to get a cached instance of the handler object (rather than instantiate it each time):\\nThis is useful for hooks handlers which are called several times.\\n- A public static method to wire up functionality into the hooks system and should contain little to no logic (as it is\\nhard to test without complete integration or e2e tests).\\n- A public method to perform the actual handler logic in a testable way.\\nThis class should be placed under the `includes\/Hooks` directory. An example of this pattern can be seen in:\\nhttps:\/\/gerrit.wikimedia.org\/r\/c\/mediawiki\/extensions\/Wikibase\/+\/574495\\n","MatchedIds":[1387,4178,3203,4868,334],"Predicted":"## Decision\nWe will adopt an incremental refactoring approach to improve testability for the `RepoHooks` class. The steps are as follows:\n\n1. **Partial Refactoring**: We will partially refactor `RepoHooks` to introduce a non-static intermediate layer. This will allow the injection of dependencies such as mocks for `WikibaseRepo` when the intermediate layer is instantiated. This solution enables gradual refactoring without a complete overhaul.\n\n2. **Testing Fixtures**: Develop testing fixtures similar to those used by `WikibaseClient`, such as mock stores and overrideable components. This provides more flexibility for unit testing and helps isolate the components during testing.\n\n3. **Introduce Dependency Injection Gradually**: While waiting for the [RFC for enabling dependency injection](https:\/\/phabricator.wikimedia.org\/T240307), we will start injecting dependencies where possible using existing patterns and partial refactoring. This enables us to test more parts of `RepoHooks` in isolation.\n\n4. **Documentation and Guides**: Create comprehensive documentation and guides for developers to follow this approach. This includes examples, best practices, and common pitfalls to avoid, ensuring consistent implementation across the codebase.\n\n5. **Review and Iterate**: Regularly review the changes and adjust the approach based on feedback and results from the testing implementations. Continuous improvement will help keep the codebase maintainable and testable.\n\nBy following this incremental approach, we can improve the testability of `RepoHooks` without a disruptive large-scale refactor, allowing us to address untested logic progressively.","GenTime":"2024-08-19 23:51:53"}
{"File Name":"qc-atlas\/0008-testcontainers.md","Context":"## Context and Problem Statement\\nRunning Service Tests requires PostgreSQL. To ensure an instance is always up and running a container has to be manually launched or started automaticially.\\n","Decision":"Chosen option: [Testcontainers](https:\/\/www.testcontainers.org\/), because it simplifies the test execution process\\n### Positive Consequences <!-- optional -->\\n* It is not mandatory to ensure postgres is running before starting the tests\\n","tokens":34,"id":699,"text":"## Context and Problem Statement\\nRunning Service Tests requires PostgreSQL. To ensure an instance is always up and running a container has to be manually launched or started automaticially.\\n\n\n##Decision\nChosen option: [Testcontainers](https:\/\/www.testcontainers.org\/), because it simplifies the test execution process\\n### Positive Consequences <!-- optional -->\\n* It is not mandatory to ensure postgres is running before starting the tests\\n","MatchedIds":[4096,718,2080,2997,4370],"Predicted":"## Decision\nChosen option: Use Docker Compose with a PostgreSQL service.\n\n### Rationale:\n- **Ease of Use**: Docker Compose allows for easily defining and running multi-container Docker applications. A single `docker-compose.yml` file can be used to set up both the application and the PostgreSQL service.\n- **Consistency**: Using a `docker-compose.yml` file ensures that the same PostgreSQL configuration is used across different environments (development, CI\/CD, etc.), reducing the \"it works on my machine\" syndrome.\n- **Automation**: Docker Compose can start PostgreSQL automatically whenever tests are started, ensuring the service is always up and running without manual intervention.\n- **Isolation**: Running PostgreSQL in a container ensures it is isolated from the host system, preventing conflicts with other local database services.\n- **Reproducibility**: Using a containerized PostgreSQL service ensures that the environment is reproducible, making it easier to debug and collaborate with other team members.\n\n### Steps:\n1. Create a `docker-compose.yml` file that defines the PostgreSQL service with necessary configurations such as volume for data persistence and environment variables for database initialization.\n2. Update the testing scripts or CI\/CD pipeline to start Docker Compose before tests are run and to tear down the environment afterward.\n3. Document the setup and usage in the project\u2019s README or development guide to assist new developers in getting started quickly.\n\n### Positive Consequences:\n- Simplifies the setup and teardown process of the PostgreSQL service for tests.\n- Promotes a consistent development and testing environment across different team members and CI\/CD pipelines.\n- Reduces the need for manual intervention to manage the PostgreSQL service.\n\n### Negative Consequences:\n- Adds a dependency on Docker and Docker Compose, which may require initial setup for developers who are not already using these tools.\n- Potential for slightly increased resource usage on the host machine due to running PostgreSQL in a container.","GenTime":"2024-08-19 23:52:03"}
{"File Name":"govuk-aws\/0015-dns-infrastructure.md","Context":"## Context\\n- All our instances will need to be able to resolve internal infrastructure services, such\\nus Puppet, Graphite or Logstash\\n- Some services and application endpoints will need to be exposed to the Internet and\\nbe resolved by public DNS. For instance alerts.integration, deploy.integration, www-origin, etc\\n- We want to be able to create new pieces of infrastructure alongside the current piece of infrastructure\\nwith the ability to test direct access to each piece using DNS endpoints\\n- We want to control which stack is running the active version of a piece of infrastructure, and control\\nhow applications connect using DNS\\n- We want to ensure the site and all links works correctly when users browse using the\\npublishing (publishing.service.gov.uk) domain.\\n","Decision":"![DNS](.\/0015-govuk-aws-dns.jpg?raw=true \"DNS Infrastructure\")\\n#### Stack domains\\nEach stack has an internal and external DNS domain. All Terraform projects in that stack add records\\nto Route53 zones to expose the service internally and\/or externally.\\nFor instance, a 'green' stack has its own `green.<internalrootdomain>` and `green.<externalrootdomain>`\\ndomain. Puppet and Icinga services in this stack will add `puppet.green.<internalrootdomain>` and\\n`alerts.green.<externalrootdomain>` to Route53.\\nThis is for an infrastructure level view only. Applications will not work correctly across independent stacks,\\nand will only correctly work using the Publishing domain.\\n### Root domain service records\\nAll services will need an entry with the root domain that points to a stack record. This entry\\ncan be updated to select the active version of each service.\\nFor instance, machines are using the Puppet service `puppet.<internalrootdomain>` that is a CNAME\\nof `puppet.green.<internalrootdomain>`. At some point, a new Puppet stack 'blue' is provisioned to\\ntest a new version, and when it has been tested we switch the CNAME to the new stack, so\\n`puppet.<internalrootdomain>` resolves to `puppet.blue.<internalrootdomain>`.\\n#### External Route53 zones\\nThere is a public (external) Route53 zone to manage the external root domain. Each stack has also its own\\nexternal Route53 zone where we delegate the stack subdomain.\\nFor instance, if we are setting up a new environment with a public root domain `test.govuk.digital`,\\nand create a new stack 'green', we'll have a zone for `test.govuk.digital` and a zone for the stack\\nsubdomain `green.test.govuk.digital`, that stores the stack records.\\n#### Internal Route53 zones\\nFor internal domains we cannot replicate the external configuration because it's not possible to do DNS\\ndelegation with internal zones in the same VPC when the domains overlap.\\nFor instance, for the previous scenario we will have an internal zone for the root domain\\n`test.govuk-internal.digital`, but we cannot create and delegate a subdomain for a zone\\n`green.test.govuk-internal.digital` in the same VPC because it overlaps the root domain.\\nIn this case, for internal domains we can only have a single zone, but we'll still keep the same\\ndomain schema and each stack will generate records appending the stack subdomain.\\n","tokens":154,"id":4056,"text":"## Context\\n- All our instances will need to be able to resolve internal infrastructure services, such\\nus Puppet, Graphite or Logstash\\n- Some services and application endpoints will need to be exposed to the Internet and\\nbe resolved by public DNS. For instance alerts.integration, deploy.integration, www-origin, etc\\n- We want to be able to create new pieces of infrastructure alongside the current piece of infrastructure\\nwith the ability to test direct access to each piece using DNS endpoints\\n- We want to control which stack is running the active version of a piece of infrastructure, and control\\nhow applications connect using DNS\\n- We want to ensure the site and all links works correctly when users browse using the\\npublishing (publishing.service.gov.uk) domain.\\n\n\n##Decision\n![DNS](.\/0015-govuk-aws-dns.jpg?raw=true \"DNS Infrastructure\")\\n#### Stack domains\\nEach stack has an internal and external DNS domain. All Terraform projects in that stack add records\\nto Route53 zones to expose the service internally and\/or externally.\\nFor instance, a 'green' stack has its own `green.<internalrootdomain>` and `green.<externalrootdomain>`\\ndomain. Puppet and Icinga services in this stack will add `puppet.green.<internalrootdomain>` and\\n`alerts.green.<externalrootdomain>` to Route53.\\nThis is for an infrastructure level view only. Applications will not work correctly across independent stacks,\\nand will only correctly work using the Publishing domain.\\n### Root domain service records\\nAll services will need an entry with the root domain that points to a stack record. This entry\\ncan be updated to select the active version of each service.\\nFor instance, machines are using the Puppet service `puppet.<internalrootdomain>` that is a CNAME\\nof `puppet.green.<internalrootdomain>`. At some point, a new Puppet stack 'blue' is provisioned to\\ntest a new version, and when it has been tested we switch the CNAME to the new stack, so\\n`puppet.<internalrootdomain>` resolves to `puppet.blue.<internalrootdomain>`.\\n#### External Route53 zones\\nThere is a public (external) Route53 zone to manage the external root domain. Each stack has also its own\\nexternal Route53 zone where we delegate the stack subdomain.\\nFor instance, if we are setting up a new environment with a public root domain `test.govuk.digital`,\\nand create a new stack 'green', we'll have a zone for `test.govuk.digital` and a zone for the stack\\nsubdomain `green.test.govuk.digital`, that stores the stack records.\\n#### Internal Route53 zones\\nFor internal domains we cannot replicate the external configuration because it's not possible to do DNS\\ndelegation with internal zones in the same VPC when the domains overlap.\\nFor instance, for the previous scenario we will have an internal zone for the root domain\\n`test.govuk-internal.digital`, but we cannot create and delegate a subdomain for a zone\\n`green.test.govuk-internal.digital` in the same VPC because it overlaps the root domain.\\nIn this case, for internal domains we can only have a single zone, but we'll still keep the same\\ndomain schema and each stack will generate records appending the stack subdomain.\\n","MatchedIds":[729,4041,1376,1372,2410],"Predicted":"## Decision\nWe will implement a dual DNS mechanism using both internal and external DNS servers.\n\n1. **Internal DNS**: Internal DNS servers will handle the resolution of internal infrastructure services such as Puppet, Graphite, and Logstash. This ensures all instances can reliably resolve and communicate with these critical services.\n   \n2. **External DNS**: External DNS servers will handle the resolution of services and application endpoints exposed to the Internet. Public DNS entries will be managed for domains like alerts.integration, deploy.integration, www-origin, and more. These will be configured for appropriate access control, availability, and redundancy.\n\n3. **DNS Zones and Split-Horizon DNS**: A split-horizon DNS setup will be used where the same domain name can resolve to different IPs depending on whether the query originates from inside the network or outside. For example, publishing.service.gov.uk will work both internally and externally, resolving appropriately in each scenario.\n\n4. **DNS Management for Active Versions and Testing**: New pieces of infrastructure will be assigned unique DNS names for testing purposes (e.g., testing.example.com). A process will be established to switch the active stack by updating DNS records, ensuring that at any given time, the active version is accessible using the main DNS entry. This will allow quick rollback capabilities if needed.\n\n5. **DNS Redundancy and Health Checks**: To ensure high availability and accuracy, our infrastructure will include DNS redundancy and regular health checks. Failover mechanisms will be in place to redirect traffic if a DNS server or an infrastructure component becomes unavailable.\n\nBy adopting this dual DNS strategy with internal and external servers, DNS zones, and split-horizon DNS, we can ensure reliable resolution of both internal services and externally accessible endpoints.","GenTime":"2024-08-19 23:52:11"}
{"File Name":"elm-spec\/003_loading_harness.md","Context":"## Context\\nRight now, in order to run an elm-spec program, we need to do a few things in a definite order.\\nAt some point, compile the Elm code using the `Compiler` from elm-spec-core. This will\\nwrap the compiled code so that the parts of the program that interact with the outside world\\ncan be easily faked out.\\nThen, to get the program running: First, create an `ElmContext` object. This creates\\nall the fake objects on the `window`\\nobject that the compiled elm code will attempt to reference. Second, evaluate the compiled\\nelm code. It doesn't matter when we compile the elm code, of course, just that it is\\nevaluated in the browser environment *after* we have created a new `ElmContext` in that\\nenvironment.\\nSo, it's a little wild, I guess, that simply instantiating an `ElmContext` modifies the `window`\\nobject and so on.\\nPart of the need for this comes from the fact that the compiled Elm code is wrapped in an IFFE.\\nBut there's no reason why we actually have to do that ...\\nWe've been able to deal with this problem so far because the only things that need to go\\nthrough this process are elm-spec-runner and karma-elm-spec-framework. But with the harness,\\nwe are now asking a test writer to follow this process as well. For that reason, we need to\\nsimplify it so it's not a source of errors.\\n","Decision":"We should change this flow so that we don't need to create an `ElmContext` and evaluate the\\ncompiled Elm code in a particular order.\\nFirst, we will wrap the compiled Elm code in a function that takes an `ElmContext`. Evaluating\\nthis code will still attach the `Elm` object to the window (since we're providing it with a\\nproxy object). But by using a normal function here, we have more control over when the `Elm`\\nobject is loaded.\\nThen, we need to have `ElmContext` store references to all the fakes inside itself -- there's no\\nreal need to store these on the `window` object. So the only things we need to store on the `window`\\nare (1) the function to load Elm -- because this is how the compiled Elm code provides the function;\\nI don't think we can reference it any other way. (2) Data on what the compiler actually\\ntried to do, like what files it tried to compile, the path and all that; we store this on the window\\nso it's available as soon as the compiled code is evaluated -- there may be better ways to do this.\\nAnd (3) the function for loading files, which has to be a function accessible on the window\\nanyway (since that's how Playwright and JSDOM allow us to register a function to be executed in Node).\\nOnce we do this, then it turns out that the test writer doesn't need to create an `ElmContext`\\nat all ... we can have `HarnessController` do that when `prepareHarness` is called. In fact,\\nwe don't even need to bundle anything extra. This will be included in the bundle of tests that\\nget executed in the browser.\\nWe could also change SuiteRunner to create it's own `ElmContext` as well, but it's not necessary.\\n","tokens":318,"id":4592,"text":"## Context\\nRight now, in order to run an elm-spec program, we need to do a few things in a definite order.\\nAt some point, compile the Elm code using the `Compiler` from elm-spec-core. This will\\nwrap the compiled code so that the parts of the program that interact with the outside world\\ncan be easily faked out.\\nThen, to get the program running: First, create an `ElmContext` object. This creates\\nall the fake objects on the `window`\\nobject that the compiled elm code will attempt to reference. Second, evaluate the compiled\\nelm code. It doesn't matter when we compile the elm code, of course, just that it is\\nevaluated in the browser environment *after* we have created a new `ElmContext` in that\\nenvironment.\\nSo, it's a little wild, I guess, that simply instantiating an `ElmContext` modifies the `window`\\nobject and so on.\\nPart of the need for this comes from the fact that the compiled Elm code is wrapped in an IFFE.\\nBut there's no reason why we actually have to do that ...\\nWe've been able to deal with this problem so far because the only things that need to go\\nthrough this process are elm-spec-runner and karma-elm-spec-framework. But with the harness,\\nwe are now asking a test writer to follow this process as well. For that reason, we need to\\nsimplify it so it's not a source of errors.\\n\n\n##Decision\nWe should change this flow so that we don't need to create an `ElmContext` and evaluate the\\ncompiled Elm code in a particular order.\\nFirst, we will wrap the compiled Elm code in a function that takes an `ElmContext`. Evaluating\\nthis code will still attach the `Elm` object to the window (since we're providing it with a\\nproxy object). But by using a normal function here, we have more control over when the `Elm`\\nobject is loaded.\\nThen, we need to have `ElmContext` store references to all the fakes inside itself -- there's no\\nreal need to store these on the `window` object. So the only things we need to store on the `window`\\nare (1) the function to load Elm -- because this is how the compiled Elm code provides the function;\\nI don't think we can reference it any other way. (2) Data on what the compiler actually\\ntried to do, like what files it tried to compile, the path and all that; we store this on the window\\nso it's available as soon as the compiled code is evaluated -- there may be better ways to do this.\\nAnd (3) the function for loading files, which has to be a function accessible on the window\\nanyway (since that's how Playwright and JSDOM allow us to register a function to be executed in Node).\\nOnce we do this, then it turns out that the test writer doesn't need to create an `ElmContext`\\nat all ... we can have `HarnessController` do that when `prepareHarness` is called. In fact,\\nwe don't even need to bundle anything extra. This will be included in the bundle of tests that\\nget executed in the browser.\\nWe could also change SuiteRunner to create it's own `ElmContext` as well, but it's not necessary.\\n","MatchedIds":[1387,2290,4868,2816,3682],"Predicted":"## Decision\nWe will refactor the initialization process to streamline the workflow for test writers. Specifically, we will:\n\n1. **Remove the IIFE wrapping from the compiled Elm code**: By eliminating the IIFE, we can directly export the Elm modules and avoid unnecessary encapsulations that complicate the testing setup.\n\n2. **Abstract the ElmContext creation and Elm code evaluation into a single function**: Create a utility function (e.g., `initializeElmSpec`) that handles both the creation of the `ElmContext` and the evaluation of the compiled Elm code. This function will ensure that the Elm code is always evaluated after the `ElmContext` is instantiated.\n\n3. **Encapsulate the window object modifications**: Make the modifications to the `window` object explicit within the `ElmContext` creation process, thereby reducing implicit side effects and improving readability and maintainability.\n\nBy implementing these changes, we will simplify the process for test writers, reduce the potential for errors, and make the process more intuitive and easier to debug.","GenTime":"2024-08-19 23:52:15"}
{"File Name":"devops-challenge\/0010-feat-ci-cd-with-circleci.md","Context":"## Context\\nTime to work on the CI\/CD solution.\\nI need a job manager to build, test and deploy the apps to the kubernetes\\ncluster.\\nI know there is a lot of documentation about jenkins, but I have been working\\nin the last years with other solutions like:\\n* SolanoCI (now closed)\\n* CircleCI\\nSo I need to see if there is a way to use my knowledge in CircleCI or not.\\nBesides, CircleCI has a free plan very useful for testing.\\nI know there is a jenkins-x product, but I don't have a clear idea about\\nit.  I installed it and spent couple of hours to make it work (jx) but I got\\nan error downloading kops, and I couldn't go further.\\n","Decision":"Try first with CircleCI to see if I can deploy the apps to kubernetes.\\nAs I'm using a monorepo, all the apps are within the same repository, so we\\ncannot separate the building process of each microservice.  All them will be\\nbuild and deployed as one.\\nBut if nothing has change in the app, then the building process will be faster.\\nTesting will be peformed on all apps secuentially, and some tests could be\\nperformed on all services without implementing mockups.\\nThis way, when a event is received by the CI, a script will be executed to\\nexecute the actions on each app.\\n","tokens":168,"id":3482,"text":"## Context\\nTime to work on the CI\/CD solution.\\nI need a job manager to build, test and deploy the apps to the kubernetes\\ncluster.\\nI know there is a lot of documentation about jenkins, but I have been working\\nin the last years with other solutions like:\\n* SolanoCI (now closed)\\n* CircleCI\\nSo I need to see if there is a way to use my knowledge in CircleCI or not.\\nBesides, CircleCI has a free plan very useful for testing.\\nI know there is a jenkins-x product, but I don't have a clear idea about\\nit.  I installed it and spent couple of hours to make it work (jx) but I got\\nan error downloading kops, and I couldn't go further.\\n\n\n##Decision\nTry first with CircleCI to see if I can deploy the apps to kubernetes.\\nAs I'm using a monorepo, all the apps are within the same repository, so we\\ncannot separate the building process of each microservice.  All them will be\\nbuild and deployed as one.\\nBut if nothing has change in the app, then the building process will be faster.\\nTesting will be peformed on all apps secuentially, and some tests could be\\nperformed on all services without implementing mockups.\\nThis way, when a event is received by the CI, a script will be executed to\\nexecute the actions on each app.\\n","MatchedIds":[319,3456,2997,1741,3600],"Predicted":"## Decision\nWe will use CircleCI as our CI\/CD solution to build, test, and deploy apps to the Kubernetes cluster.\n\n### Use of CircleCI\nGiven the team's familiarity with CircleCI and its proven track record, leveraging this platform will reduce onboarding time and enable us to quickly set up a reliable pipeline. The free plan is beneficial for initial testing and setup, providing a cost-effective way to start the project.\n\n### Configuration \nThe pipeline configuration will be kept in the `.circleci\/config.yml` file within the repository, making it easy to manage and version alongside the codebase.\n\n### CircleCI Orbs and Kubernetes\nWe can utilize pre-built CircleCI Orbs, especially for Kubernetes, to streamline the deployment process. These Orbs will simplify tasks like building Docker images, pushing them to a registry, and deploying them to the Kubernetes cluster.\n\n### Jenkins-X and Alternatives\nWhile Jenkins-X is an emerging tool designed specifically for Kubernetes, the initial setup issues encountered and the current team's unfamiliarity pose risks. We might revisit Jenkins-X or other alternatives in the future once it matures further, or if CircleCI no longer meets our needs.\n\nTo summarize, using CircleCI aligns with our current expertise and offers a robust, well-documented path forward for our CI\/CD needs within the Kubernetes ecosystem.","GenTime":"2024-08-19 23:52:20"}
{"File Name":"titania-os\/0002-docker-for-dapps.md","Context":"## Context\\nTitania is supposed to run multiple dApps. There are multiple techologies and concepts on how to define and isolate a dApp.\\n","Decision":"We will use LXC technology and represent a dApp as a container. We will use Docker to build, run and manage said containers.\\n","tokens":31,"id":2573,"text":"## Context\\nTitania is supposed to run multiple dApps. There are multiple techologies and concepts on how to define and isolate a dApp.\\n\n\n##Decision\nWe will use LXC technology and represent a dApp as a container. We will use Docker to build, run and manage said containers.\\n","MatchedIds":[3982,1239,1173,3682,2968],"Predicted":"## Decision\nWe will use a microservices architecture to define and isolate each dApp within Titania. \n\n### Rationale\n1. **Isolation**: Each dApp will run as an independent microservice, allowing for clear separation of concerns and fault isolation.\n2. **Scalability**: Microservices enable independent scaling of individual dApps based on their performance needs.\n3. **Technology Heterogeneity**: Different dApps can be implemented using different technologies and frameworks as required, without impacting others.\n4. **Deployment Flexibility**: Independent deployment pipelines for each dApp improve CI\/CD processes and reduce deployment risks.\n\n### Implementation Strategy\n1. **Containerization**: Each dApp will be containerized using Docker to ensure consistent environments across development, testing, and production.\n2. **Orchestration**: We will use Kubernetes for orchestrating the deployment, scaling, and management of containerized dApps.\n3. **Service Discovery**: Implement service discovery using tools like Consul or Kubernetes-native mechanisms to handle inter-service communication.\n4. **API Gateway**: Use an API Gateway like Kong or Istio to manage routing, authentication, and rate-limiting for dApp APIs.\n5. **Monitoring and Logging**: Implement centralized monitoring and logging using tools like Prometheus, Grafana, and ELK stack to ensure visibility and troubleshooting capabilities across all dApps.\n6. **Security**: Ensure isolation and security using network policies, RBAC, and other Kubernetes security best practices.","GenTime":"2024-08-19 23:52:28"}
{"File Name":"ibc-go\/adr-015-ibc-packet-receiver.md","Context":"## Context\\n[ICS 26 - Routing Module](https:\/\/github.com\/cosmos\/ibc\/tree\/master\/spec\/core\/ics-026-routing-module) defines a function [`handlePacketRecv`](https:\/\/github.com\/cosmos\/ibc\/tree\/master\/spec\/core\/ics-026-routing-module#packet-relay).\\nIn ICS 26, the routing module is defined as a layer above each application module\\nwhich verifies and routes messages to the destination modules. It is possible to\\nimplement it as a separate module, however, we already have functionality to route\\nmessages upon the destination identifiers in the baseapp. This ADR suggests\\nto utilize existing `baseapp.router` to route packets to application modules.\\nGenerally, routing module callbacks have two separate steps in them,\\nverification and execution. This corresponds to the `AnteHandler`-`Handler`\\nmodel inside the SDK. We can do the verification inside the `AnteHandler`\\nin order to increase developer ergonomics by reducing boilerplate\\nverification code.\\nFor atomic multi-message transaction, we want to keep the IBC related\\nstate modification to be preserved even the application side state change\\nreverts. One of the example might be IBC token sending message following with\\nstake delegation which uses the tokens received by the previous packet message.\\nIf the token receiving fails for any reason, we might not want to keep\\nexecuting the transaction, but we also don't want to abort the transaction\\nor the sequence and commitment will be reverted and the channel will be stuck.\\nThis ADR suggests new `CodeType`, `CodeTxBreak`, to fix this problem.\\n","Decision":"`PortKeeper` will have the capability key that is able to access only the\\nchannels bound to the port. Entities that hold a `PortKeeper` will be\\nable to call the methods on it which are corresponding with the methods with\\nthe same names on the `ChannelKeeper`, but only with the\\nallowed port. `ChannelKeeper.Port(string, ChannelChecker)` will be defined to\\neasily construct a capability-safe `PortKeeper`. This will be addressed in\\nanother ADR and we will use insecure `ChannelKeeper` for now.\\n`baseapp.runMsgs` will break the loop over the messages if one of the handlers\\nreturns `!Result.IsOK()`. However, the outer logic will write the cached\\nstore if `Result.IsOK() || Result.Code.IsBreak()`. `Result.Code.IsBreak()` if\\n`Result.Code == CodeTxBreak`.\\n```go\\nfunc (app *BaseApp) runTx(tx Tx) (result Result) {\\nmsgs := tx.GetMsgs()\\n\/\/ AnteHandler\\nif app.anteHandler != nil {\\nanteCtx, msCache := app.cacheTxContext(ctx)\\nnewCtx, err := app.anteHandler(anteCtx, tx)\\nif !newCtx.IsZero() {\\nctx = newCtx.WithMultiStore(ms)\\n}\\nif err != nil {\\n\/\/ error handling logic\\nreturn res\\n}\\nmsCache.Write()\\n}\\n\/\/ Main Handler\\nrunMsgCtx, msCache := app.cacheTxContext(ctx)\\nresult = app.runMsgs(runMsgCtx, msgs)\\n\/\/ BEGIN modification made in this ADR\\nif result.IsOK() || result.IsBreak() {\\n\/\/ END\\nmsCache.Write()\\n}\\nreturn result\\n}\\n```\\nThe Cosmos SDK will define an `AnteDecorator` for IBC packet receiving. The\\n`AnteDecorator` will iterate over the messages included in the transaction, type\\n`switch` to check whether the message contains an incoming IBC packet, and if so\\nverify the Merkle proof.\\n```go\\ntype ProofVerificationDecorator struct {\\nclientKeeper ClientKeeper\\nchannelKeeper ChannelKeeper\\n}\\nfunc (pvr ProofVerificationDecorator) AnteHandle(ctx Context, tx Tx, simulate bool, next AnteHandler) (Context, error) {\\nfor _, msg := range tx.GetMsgs() {\\nvar err error\\nswitch msg := msg.(type) {\\ncase client.MsgUpdateClient:\\nerr = pvr.clientKeeper.UpdateClient(msg.ClientID, msg.Header)\\ncase channel.MsgPacket:\\nerr = pvr.channelKeeper.RecvPacket(msg.Packet, msg.Proofs, msg.ProofHeight)\\ncase channel.MsgAcknowledgement:\\nerr = pvr.channelKeeper.AcknowledgementPacket(msg.Acknowledgement, msg.Proof, msg.ProofHeight)\\ncase channel.MsgTimeoutPacket:\\nerr = pvr.channelKeeper.TimeoutPacket(msg.Packet, msg.Proof, msg.ProofHeight, msg.NextSequenceRecv)\\ncase channel.MsgChannelOpenInit;\\nerr = pvr.channelKeeper.CheckOpen(msg.PortID, msg.ChannelID, msg.Channel)\\ndefault:\\ncontinue\\n}\\nif err != nil {\\nreturn ctx, err\\n}\\n}\\nreturn next(ctx, tx, simulate)\\n}\\n```\\nWhere `MsgUpdateClient`, `MsgPacket`, `MsgAcknowledgement`, `MsgTimeoutPacket`\\nare `sdk.Msg` types correspond to `handleUpdateClient`, `handleRecvPacket`,\\n`handleAcknowledgementPacket`, `handleTimeoutPacket` of the routing module,\\nrespectively.\\nThe side effects of `RecvPacket`, `VerifyAcknowledgement`,\\n`VerifyTimeout` will be extracted out into separated functions,\\n`WriteAcknowledgement`, `DeleteCommitment`, `DeleteCommitmentTimeout`, respectively,\\nwhich will be called by the application handlers after the execution.\\n`WriteAcknowledgement` writes the acknowledgement to the state that can be\\nverified by the counter-party chain and increments the sequence to prevent\\ndouble execution. `DeleteCommitment` will delete the commitment stored,\\n`DeleteCommitmentTimeout` will delete the commitment and close channel in case\\nof ordered channel.\\n```go\\nfunc (keeper ChannelKeeper) WriteAcknowledgement(ctx Context, packet Packet, ack []byte) {\\nkeeper.SetPacketAcknowledgement(ctx, packet.GetDestPort(), packet.GetDestChannel(), packet.GetSequence(), ack)\\nkeeper.SetNextSequenceRecv(ctx, packet.GetDestPort(), packet.GetDestChannel(), packet.GetSequence())\\n}\\nfunc (keeper ChannelKeeper) DeleteCommitment(ctx Context, packet Packet) {\\nkeeper.deletePacketCommitment(ctx, packet.GetSourcePort(), packet.GetSourceChannel(), packet.GetSequence())\\n}\\nfunc (keeper ChannelKeeper) DeleteCommitmentTimeout(ctx Context, packet Packet) {\\nk.deletePacketCommitment(ctx, packet.GetSourcePort(), packet.GetSourceChannel(), packet.GetSequence())\\nif channel.Ordering == types.ORDERED [\\nchannel.State = types.CLOSED\\nk.SetChannel(ctx, packet.GetSourcePort(), packet.GetSourceChannel(), channel)\\n}\\n}\\n```\\nEach application handler should call respective finalization methods on the `PortKeeper`\\nin order to increase sequence (in case of packet) or remove the commitment\\n(in case of acknowledgement and timeout).\\nCalling those functions implies that the application logic has successfully executed.\\nHowever, the handlers can return `Result` with `CodeTxBreak` after calling those methods\\nwhich will persist the state changes that has been already done but prevent any further\\nmessages to be executed in case of semantically invalid packet. This will keep the sequence\\nincreased in the previous IBC packets(thus preventing double execution) without\\nproceeding to the following messages.\\nIn any case the application modules should never return state reverting result,\\nwhich will make the channel unable to proceed.\\n`ChannelKeeper.CheckOpen` method will be introduced. This will replace `onChanOpen*` defined\\nunder the routing module specification. Instead of define each channel handshake callback\\nfunctions, application modules can provide `ChannelChecker` function with the `AppModule`\\nwhich will be injected to `ChannelKeeper.Port()` at the top level application.\\n`CheckOpen` will find the correct `ChennelChecker` using the\\n`PortID` and call it, which will return an error if it is unacceptable by the application.\\nThe `ProofVerificationDecorator` will be inserted to the top level application.\\nIt is not safe to make each module responsible to call proof verification\\nlogic, whereas application can misbehave(in terms of IBC protocol) by\\nmistake.\\nThe `ProofVerificationDecorator` should come right after the default sybil attack\\nresistant layer from the current `auth.NewAnteHandler`:\\n```go\\n\/\/ add IBC ProofVerificationDecorator to the Chain of\\nfunc NewAnteHandler(\\nak keeper.AccountKeeper, supplyKeeper types.SupplyKeeper, ibcKeeper ibc.Keeper,\\nsigGasConsumer SignatureVerificationGasConsumer) sdk.AnteHandler {\\nreturn sdk.ChainAnteDecorators(\\nNewSetUpContextDecorator(), \/\/ outermost AnteDecorator. SetUpContext must be called first\\n...\\nNewIncrementSequenceDecorator(ak),\\nibcante.ProofVerificationDecorator(ibcKeeper.ClientKeeper, ibcKeeper.ChannelKeeper), \/\/ innermost AnteDecorator\\n)\\n}\\n```\\nThe implementation of this ADR will also create a `Data` field of the `Packet` of type `[]byte`, which can be deserialised by the receiving module into its own private type. It is up to the application modules to do this according to their own interpretation, not by the IBC keeper.  This is crucial for dynamic IBC.\\nExample application-side usage:\\n```go\\ntype AppModule struct {}\\n\/\/ CheckChannel will be provided to the ChannelKeeper as ChannelKeeper.Port(module.CheckChannel)\\nfunc (module AppModule) CheckChannel(portID, channelID string, channel Channel) error {\\nif channel.Ordering != UNORDERED {\\nreturn ErrUncompatibleOrdering()\\n}\\nif channel.CounterpartyPort != \"bank\" {\\nreturn ErrUncompatiblePort()\\n}\\nif channel.Version != \"\" {\\nreturn ErrUncompatibleVersion()\\n}\\nreturn nil\\n}\\nfunc NewHandler(k Keeper) Handler {\\nreturn func(ctx Context, msg Msg) Result {\\nswitch msg := msg.(type) {\\ncase MsgTransfer:\\nreturn handleMsgTransfer(ctx, k, msg)\\ncase ibc.MsgPacket:\\nvar data PacketDataTransfer\\nif err := types.ModuleCodec.UnmarshalBinaryBare(msg.GetData(), &data); err != nil {\\nreturn err\\n}\\nreturn handlePacketDataTransfer(ctx, k, msg, data)\\ncase ibc.MsgTimeoutPacket:\\nvar data PacketDataTransfer\\nif err := types.ModuleCodec.UnmarshalBinaryBare(msg.GetData(), &data); err != nil {\\nreturn err\\n}\\nreturn handleTimeoutPacketDataTransfer(ctx, k, packet)\\n\/\/ interface { PortID() string; ChannelID() string; Channel() ibc.Channel }\\n\/\/ MsgChanInit, MsgChanTry implements ibc.MsgChannelOpen\\ncase ibc.MsgChannelOpen:\\nreturn handleMsgChannelOpen(ctx, k, msg)\\n}\\n}\\n}\\nfunc handleMsgTransfer(ctx Context, k Keeper, msg MsgTransfer) Result {\\nerr := k.SendTransfer(ctx,msg.PortID, msg.ChannelID, msg.Amount, msg.Sender, msg.Receiver)\\nif err != nil {\\nreturn sdk.ResultFromError(err)\\n}\\nreturn sdk.Result{}\\n}\\nfunc handlePacketDataTransfer(ctx Context, k Keeper, packet Packet, data PacketDataTransfer) Result {\\nerr := k.ReceiveTransfer(ctx, packet.GetSourcePort(), packet.GetSourceChannel(), packet.GetDestinationPort(), packet.GetDestinationChannel(), data)\\nif err != nil {\\n\/\/ TODO: Source chain sent invalid packet, shutdown channel\\n}\\nk.ChannelKeeper.WriteAcknowledgement([]byte{0x00}) \/\/ WriteAcknowledgement increases the sequence, preventing double spending\\nreturn sdk.Result{}\\n}\\nfunc handleCustomTimeoutPacket(ctx Context, k Keeper, packet CustomPacket) Result {\\nerr := k.RecoverTransfer(ctx, packet.GetSourcePort(), packet.GetSourceChannel(), packet.GetDestinationPort(), packet.GetDestinationChannel(), data)\\nif err != nil {\\n\/\/ This chain sent invalid packet or cannot recover the funds\\npanic(err)\\n}\\nk.ChannelKeeper.DeleteCommitmentTimeout(ctx, packet)\\n\/\/ packet timeout should not fail\\nreturn sdk.Result{}\\n}\\nfunc handleMsgChannelOpen(sdk.Context, k Keeper, msg MsgOpenChannel) Result {\\nk.AllocateEscrowAddress(ctx, msg.ChannelID())\\nreturn sdk.Result{}\\n}\\n```\\n","tokens":341,"id":1442,"text":"## Context\\n[ICS 26 - Routing Module](https:\/\/github.com\/cosmos\/ibc\/tree\/master\/spec\/core\/ics-026-routing-module) defines a function [`handlePacketRecv`](https:\/\/github.com\/cosmos\/ibc\/tree\/master\/spec\/core\/ics-026-routing-module#packet-relay).\\nIn ICS 26, the routing module is defined as a layer above each application module\\nwhich verifies and routes messages to the destination modules. It is possible to\\nimplement it as a separate module, however, we already have functionality to route\\nmessages upon the destination identifiers in the baseapp. This ADR suggests\\nto utilize existing `baseapp.router` to route packets to application modules.\\nGenerally, routing module callbacks have two separate steps in them,\\nverification and execution. This corresponds to the `AnteHandler`-`Handler`\\nmodel inside the SDK. We can do the verification inside the `AnteHandler`\\nin order to increase developer ergonomics by reducing boilerplate\\nverification code.\\nFor atomic multi-message transaction, we want to keep the IBC related\\nstate modification to be preserved even the application side state change\\nreverts. One of the example might be IBC token sending message following with\\nstake delegation which uses the tokens received by the previous packet message.\\nIf the token receiving fails for any reason, we might not want to keep\\nexecuting the transaction, but we also don't want to abort the transaction\\nor the sequence and commitment will be reverted and the channel will be stuck.\\nThis ADR suggests new `CodeType`, `CodeTxBreak`, to fix this problem.\\n\n\n##Decision\n`PortKeeper` will have the capability key that is able to access only the\\nchannels bound to the port. Entities that hold a `PortKeeper` will be\\nable to call the methods on it which are corresponding with the methods with\\nthe same names on the `ChannelKeeper`, but only with the\\nallowed port. `ChannelKeeper.Port(string, ChannelChecker)` will be defined to\\neasily construct a capability-safe `PortKeeper`. This will be addressed in\\nanother ADR and we will use insecure `ChannelKeeper` for now.\\n`baseapp.runMsgs` will break the loop over the messages if one of the handlers\\nreturns `!Result.IsOK()`. However, the outer logic will write the cached\\nstore if `Result.IsOK() || Result.Code.IsBreak()`. `Result.Code.IsBreak()` if\\n`Result.Code == CodeTxBreak`.\\n```go\\nfunc (app *BaseApp) runTx(tx Tx) (result Result) {\\nmsgs := tx.GetMsgs()\\n\/\/ AnteHandler\\nif app.anteHandler != nil {\\nanteCtx, msCache := app.cacheTxContext(ctx)\\nnewCtx, err := app.anteHandler(anteCtx, tx)\\nif !newCtx.IsZero() {\\nctx = newCtx.WithMultiStore(ms)\\n}\\nif err != nil {\\n\/\/ error handling logic\\nreturn res\\n}\\nmsCache.Write()\\n}\\n\/\/ Main Handler\\nrunMsgCtx, msCache := app.cacheTxContext(ctx)\\nresult = app.runMsgs(runMsgCtx, msgs)\\n\/\/ BEGIN modification made in this ADR\\nif result.IsOK() || result.IsBreak() {\\n\/\/ END\\nmsCache.Write()\\n}\\nreturn result\\n}\\n```\\nThe Cosmos SDK will define an `AnteDecorator` for IBC packet receiving. The\\n`AnteDecorator` will iterate over the messages included in the transaction, type\\n`switch` to check whether the message contains an incoming IBC packet, and if so\\nverify the Merkle proof.\\n```go\\ntype ProofVerificationDecorator struct {\\nclientKeeper ClientKeeper\\nchannelKeeper ChannelKeeper\\n}\\nfunc (pvr ProofVerificationDecorator) AnteHandle(ctx Context, tx Tx, simulate bool, next AnteHandler) (Context, error) {\\nfor _, msg := range tx.GetMsgs() {\\nvar err error\\nswitch msg := msg.(type) {\\ncase client.MsgUpdateClient:\\nerr = pvr.clientKeeper.UpdateClient(msg.ClientID, msg.Header)\\ncase channel.MsgPacket:\\nerr = pvr.channelKeeper.RecvPacket(msg.Packet, msg.Proofs, msg.ProofHeight)\\ncase channel.MsgAcknowledgement:\\nerr = pvr.channelKeeper.AcknowledgementPacket(msg.Acknowledgement, msg.Proof, msg.ProofHeight)\\ncase channel.MsgTimeoutPacket:\\nerr = pvr.channelKeeper.TimeoutPacket(msg.Packet, msg.Proof, msg.ProofHeight, msg.NextSequenceRecv)\\ncase channel.MsgChannelOpenInit;\\nerr = pvr.channelKeeper.CheckOpen(msg.PortID, msg.ChannelID, msg.Channel)\\ndefault:\\ncontinue\\n}\\nif err != nil {\\nreturn ctx, err\\n}\\n}\\nreturn next(ctx, tx, simulate)\\n}\\n```\\nWhere `MsgUpdateClient`, `MsgPacket`, `MsgAcknowledgement`, `MsgTimeoutPacket`\\nare `sdk.Msg` types correspond to `handleUpdateClient`, `handleRecvPacket`,\\n`handleAcknowledgementPacket`, `handleTimeoutPacket` of the routing module,\\nrespectively.\\nThe side effects of `RecvPacket`, `VerifyAcknowledgement`,\\n`VerifyTimeout` will be extracted out into separated functions,\\n`WriteAcknowledgement`, `DeleteCommitment`, `DeleteCommitmentTimeout`, respectively,\\nwhich will be called by the application handlers after the execution.\\n`WriteAcknowledgement` writes the acknowledgement to the state that can be\\nverified by the counter-party chain and increments the sequence to prevent\\ndouble execution. `DeleteCommitment` will delete the commitment stored,\\n`DeleteCommitmentTimeout` will delete the commitment and close channel in case\\nof ordered channel.\\n```go\\nfunc (keeper ChannelKeeper) WriteAcknowledgement(ctx Context, packet Packet, ack []byte) {\\nkeeper.SetPacketAcknowledgement(ctx, packet.GetDestPort(), packet.GetDestChannel(), packet.GetSequence(), ack)\\nkeeper.SetNextSequenceRecv(ctx, packet.GetDestPort(), packet.GetDestChannel(), packet.GetSequence())\\n}\\nfunc (keeper ChannelKeeper) DeleteCommitment(ctx Context, packet Packet) {\\nkeeper.deletePacketCommitment(ctx, packet.GetSourcePort(), packet.GetSourceChannel(), packet.GetSequence())\\n}\\nfunc (keeper ChannelKeeper) DeleteCommitmentTimeout(ctx Context, packet Packet) {\\nk.deletePacketCommitment(ctx, packet.GetSourcePort(), packet.GetSourceChannel(), packet.GetSequence())\\nif channel.Ordering == types.ORDERED [\\nchannel.State = types.CLOSED\\nk.SetChannel(ctx, packet.GetSourcePort(), packet.GetSourceChannel(), channel)\\n}\\n}\\n```\\nEach application handler should call respective finalization methods on the `PortKeeper`\\nin order to increase sequence (in case of packet) or remove the commitment\\n(in case of acknowledgement and timeout).\\nCalling those functions implies that the application logic has successfully executed.\\nHowever, the handlers can return `Result` with `CodeTxBreak` after calling those methods\\nwhich will persist the state changes that has been already done but prevent any further\\nmessages to be executed in case of semantically invalid packet. This will keep the sequence\\nincreased in the previous IBC packets(thus preventing double execution) without\\nproceeding to the following messages.\\nIn any case the application modules should never return state reverting result,\\nwhich will make the channel unable to proceed.\\n`ChannelKeeper.CheckOpen` method will be introduced. This will replace `onChanOpen*` defined\\nunder the routing module specification. Instead of define each channel handshake callback\\nfunctions, application modules can provide `ChannelChecker` function with the `AppModule`\\nwhich will be injected to `ChannelKeeper.Port()` at the top level application.\\n`CheckOpen` will find the correct `ChennelChecker` using the\\n`PortID` and call it, which will return an error if it is unacceptable by the application.\\nThe `ProofVerificationDecorator` will be inserted to the top level application.\\nIt is not safe to make each module responsible to call proof verification\\nlogic, whereas application can misbehave(in terms of IBC protocol) by\\nmistake.\\nThe `ProofVerificationDecorator` should come right after the default sybil attack\\nresistant layer from the current `auth.NewAnteHandler`:\\n```go\\n\/\/ add IBC ProofVerificationDecorator to the Chain of\\nfunc NewAnteHandler(\\nak keeper.AccountKeeper, supplyKeeper types.SupplyKeeper, ibcKeeper ibc.Keeper,\\nsigGasConsumer SignatureVerificationGasConsumer) sdk.AnteHandler {\\nreturn sdk.ChainAnteDecorators(\\nNewSetUpContextDecorator(), \/\/ outermost AnteDecorator. SetUpContext must be called first\\n...\\nNewIncrementSequenceDecorator(ak),\\nibcante.ProofVerificationDecorator(ibcKeeper.ClientKeeper, ibcKeeper.ChannelKeeper), \/\/ innermost AnteDecorator\\n)\\n}\\n```\\nThe implementation of this ADR will also create a `Data` field of the `Packet` of type `[]byte`, which can be deserialised by the receiving module into its own private type. It is up to the application modules to do this according to their own interpretation, not by the IBC keeper.  This is crucial for dynamic IBC.\\nExample application-side usage:\\n```go\\ntype AppModule struct {}\\n\/\/ CheckChannel will be provided to the ChannelKeeper as ChannelKeeper.Port(module.CheckChannel)\\nfunc (module AppModule) CheckChannel(portID, channelID string, channel Channel) error {\\nif channel.Ordering != UNORDERED {\\nreturn ErrUncompatibleOrdering()\\n}\\nif channel.CounterpartyPort != \"bank\" {\\nreturn ErrUncompatiblePort()\\n}\\nif channel.Version != \"\" {\\nreturn ErrUncompatibleVersion()\\n}\\nreturn nil\\n}\\nfunc NewHandler(k Keeper) Handler {\\nreturn func(ctx Context, msg Msg) Result {\\nswitch msg := msg.(type) {\\ncase MsgTransfer:\\nreturn handleMsgTransfer(ctx, k, msg)\\ncase ibc.MsgPacket:\\nvar data PacketDataTransfer\\nif err := types.ModuleCodec.UnmarshalBinaryBare(msg.GetData(), &data); err != nil {\\nreturn err\\n}\\nreturn handlePacketDataTransfer(ctx, k, msg, data)\\ncase ibc.MsgTimeoutPacket:\\nvar data PacketDataTransfer\\nif err := types.ModuleCodec.UnmarshalBinaryBare(msg.GetData(), &data); err != nil {\\nreturn err\\n}\\nreturn handleTimeoutPacketDataTransfer(ctx, k, packet)\\n\/\/ interface { PortID() string; ChannelID() string; Channel() ibc.Channel }\\n\/\/ MsgChanInit, MsgChanTry implements ibc.MsgChannelOpen\\ncase ibc.MsgChannelOpen:\\nreturn handleMsgChannelOpen(ctx, k, msg)\\n}\\n}\\n}\\nfunc handleMsgTransfer(ctx Context, k Keeper, msg MsgTransfer) Result {\\nerr := k.SendTransfer(ctx,msg.PortID, msg.ChannelID, msg.Amount, msg.Sender, msg.Receiver)\\nif err != nil {\\nreturn sdk.ResultFromError(err)\\n}\\nreturn sdk.Result{}\\n}\\nfunc handlePacketDataTransfer(ctx Context, k Keeper, packet Packet, data PacketDataTransfer) Result {\\nerr := k.ReceiveTransfer(ctx, packet.GetSourcePort(), packet.GetSourceChannel(), packet.GetDestinationPort(), packet.GetDestinationChannel(), data)\\nif err != nil {\\n\/\/ TODO: Source chain sent invalid packet, shutdown channel\\n}\\nk.ChannelKeeper.WriteAcknowledgement([]byte{0x00}) \/\/ WriteAcknowledgement increases the sequence, preventing double spending\\nreturn sdk.Result{}\\n}\\nfunc handleCustomTimeoutPacket(ctx Context, k Keeper, packet CustomPacket) Result {\\nerr := k.RecoverTransfer(ctx, packet.GetSourcePort(), packet.GetSourceChannel(), packet.GetDestinationPort(), packet.GetDestinationChannel(), data)\\nif err != nil {\\n\/\/ This chain sent invalid packet or cannot recover the funds\\npanic(err)\\n}\\nk.ChannelKeeper.DeleteCommitmentTimeout(ctx, packet)\\n\/\/ packet timeout should not fail\\nreturn sdk.Result{}\\n}\\nfunc handleMsgChannelOpen(sdk.Context, k Keeper, msg MsgOpenChannel) Result {\\nk.AllocateEscrowAddress(ctx, msg.ChannelID())\\nreturn sdk.Result{}\\n}\\n```\\n","MatchedIds":[21,4656,1979,1982,842],"Predicted":"## Decision\n\n### Integration with `baseapp.router`\nThe `baseapp.router` will be utilized to route IBC packets to the appropriate application modules instead of implementing a separate routing module. This aligns with the SDK's existing functionalities and avoids unnecessary duplication of routing logic. \n\n### Verification and Execution Separation\nWe will adopt the separation of verification and execution steps within the IBC packet processing by mapping them to the SDK's `AnteHandler` and `Handler` model:\n\n- **Verification**: The IBC packet verification logic will be implemented within the `AnteHandler`. This step ensures that all necessary checks are performed before any state changes occur, thereby reducing boilerplate code for individual modules.\n  \n- **Execution**: Upon successful verification, the packet is routed to the appropriate module using the existing `baseapp.router` where the actual state updates occur.\n\n### New `CodeType` for Conditional Transaction Success\nTo address the requirement for preserving IBC-related state modifications even if the application state changes need to revert, a new `CodeType`, `CodeTxBreak`, will be introduced:\n\n- **Behavior of `CodeTxBreak`**: When a transaction fails with the `CodeTxBreak` code, the transaction will halt further execution, but any IBC-related state changes made up to that point will be preserved. This ensures that sequences and commitments are not reverted, preventing channels from getting stuck.\n  \n- **Use Case**: For example, if a transaction includes an IBC token receiving message followed by a staking delegation and the receiving message fails, the entire transaction will halt, retaining the token packet's state changes while ignoring subsequent messages in the transaction.\n\n### Implementation Steps\n1. **Modify `baseapp` to Include IBC Packet Handling**:\n   - Augment the existing `baseapp.router` to include routes for IBC packet handling using the application module identifiers.\n  \n2. **Implement Verification in `AnteHandler`**:\n   - Include IBC packet verification logic in the `AnteHandler` to ensure that proper checks are done before execution.\n  \n3. **Implement Execution in `Handler`**:\n   - Route verified IBC packets to their respective modules using `baseapp.router` for state modifications.\n\n4. **Introduce `CodeTxBreak` in SDK**:\n   - Define the new `CodeTxBreak` code type in the SDK's error handling system.\n   - Update transaction processing logic to conditionally preserve state changes based on the `CodeTxBreak` status.\n  \n5. **Update Documentation**:\n   - Provide comprehensive documentation for developers on the new IBC packet handling mechanism and how to utilize `CodeTxBreak` to ensure atomic transaction handling and state preservation.\n\nBy implementing this decision, we enhance the ergonomics and robustness of the IBC packet routing and transaction processing within the Cosmos SDK, making it easier for developers to build reliable interchain applications.","GenTime":"2024-08-19 23:52:39"}
{"File Name":"james\/0040-quality-levels-definitions.md","Context":"## Context\\nWe hereby define as an artifact compiled artifact that external people consumes. This includes:\\n- libraries\\n- Mail servers\\n- Extensions for James Mail Servers\\n- Command line tools\\nWe designate as a feature an optional, opt-in behaviour of a James server that can be configured by\\nuser willing to rely on it.\\nJames as a project delivers several artifacts, and features. In order for project users to better\\nunderstand the underlying quality of the artifact they use, as well as the level of risk associated,\\nwe need to better define some quality levels.\\n","Decision":"For a given artifact or feature, by **mature** we mean that:\\n- *interfaces* in components need a contract test suite\\n- *interfaces* have several implementations\\n- *implementation* of these interfaces need to pass this contract test suite which provides unit tests\\n- Decent integration tests coverage is needed\\n- Performance tests need to be conducted out\\n- Quality Assurance with external clients needs to be conducted out\\n- known existing production deployments\/usages\\n- usable documentation\\nThis is the maximum quality level delivered by the James project. Users should feel confident using these\\nartifacts or features.\\nBy **experimental** we designate an artifact or feature not matching yet the above requirements. However some\\nactive contributors are willing to raise the quality level of this component, and eventually make it\\nmature. Or at least are willing to support users.\\nUsers should have low expectations regarding experimental artifacts or features. They are encouraged to contribute to them\\nin order to raise its quality.\\nBy **unsupported** we mean that an artifact or feature do not match most of the *mature* quality conditions. Active\\ncontributors do not feel confident delivering support for it. This artifact or feature might be deprecated and\\nremoved from future James releases. Users are strongly encouraged to contribute to the artifact development.\\n","tokens":118,"id":2150,"text":"## Context\\nWe hereby define as an artifact compiled artifact that external people consumes. This includes:\\n- libraries\\n- Mail servers\\n- Extensions for James Mail Servers\\n- Command line tools\\nWe designate as a feature an optional, opt-in behaviour of a James server that can be configured by\\nuser willing to rely on it.\\nJames as a project delivers several artifacts, and features. In order for project users to better\\nunderstand the underlying quality of the artifact they use, as well as the level of risk associated,\\nwe need to better define some quality levels.\\n\n\n##Decision\nFor a given artifact or feature, by **mature** we mean that:\\n- *interfaces* in components need a contract test suite\\n- *interfaces* have several implementations\\n- *implementation* of these interfaces need to pass this contract test suite which provides unit tests\\n- Decent integration tests coverage is needed\\n- Performance tests need to be conducted out\\n- Quality Assurance with external clients needs to be conducted out\\n- known existing production deployments\/usages\\n- usable documentation\\nThis is the maximum quality level delivered by the James project. Users should feel confident using these\\nartifacts or features.\\nBy **experimental** we designate an artifact or feature not matching yet the above requirements. However some\\nactive contributors are willing to raise the quality level of this component, and eventually make it\\nmature. Or at least are willing to support users.\\nUsers should have low expectations regarding experimental artifacts or features. They are encouraged to contribute to them\\nin order to raise its quality.\\nBy **unsupported** we mean that an artifact or feature do not match most of the *mature* quality conditions. Active\\ncontributors do not feel confident delivering support for it. This artifact or feature might be deprecated and\\nremoved from future James releases. Users are strongly encouraged to contribute to the artifact development.\\n","MatchedIds":[2951,2859,3171,1545,2181],"Predicted":"## Decision\n\nTo better convey the quality and associated risk of the artifacts and features delivered by the James project, we will adopt a quality level categorization that will be applied to all compiled artifacts and features. \n\n1. **Quality Levels Definition:** We will define three quality levels:\n    - **Alpha:** Early-stage development, not recommended for production use. Expected to have bugs and non-finalized APIs.\n    - **Beta:** More mature than Alpha, but still under active development. Beta versions are relatively stable but should be used with caution in production environments.\n    - **Stable:** Fully tested and stable. Recommended for production use with a high degree of confidence in its reliability and performance.\n\n2. **Artifact Labeling:** Each compiled artifact (libraries, mail servers, extensions for James Mail Servers, command line tools) will be labeled with its quality level:\n    - This labeling will be included in the versioning schema. For example: `lib-james-1.0.0-alpha`, `server-james-2.1.0-beta`, `cli-james-3.0.0`.\n\n3. **Feature Labeling:** Features will also be categorized by quality levels:\n    - Configuration documentation will specify the quality level of each feature. For example: `feature.authentication=v1.2.0-stable`, `feature.email-filtering=v0.9.1-alpha`.\n\n4. **Documentation Updates:** All documentation will be updated to reflect these quality levels. This includes user guides, API documentation, and configuration files, ensuring users are aware of the stability and maturity of the artifacts and features they are using.\n\n5. **Communication with Users:** We will communicate with our user community about the introduction of these quality levels to ensure they understand the implications and can make informed decisions about the artifacts and features they choose to implement.\n\n6. **Automated Testing and CI\/CD Integration:** \n    - Implement automated testing pipelines that will categorize artifacts and features based on predefined quality metrics.\n    - Ensure CI\/CD pipelines reflect the quality levels accurately and prevent promotions of artifacts from Alpha to Beta or Stable without meeting necessary criteria.\n\n7. **Feedback Mechanism:** \n    - Establish a feedback loop where users can report issues and provide feedback specific to Alpha and Beta quality levels.\n    - Use this feedback to iteratively improve the artifacts before promoting them to a higher quality level.\n\nBy categorizing and labeling artifacts and features according to these quality levels, we will provide our users with a clearer understanding of the risks and stability associated with their choices, leading to better-informed decisions and improved overall satisfaction.","GenTime":"2024-08-19 23:52:52"}
{"File Name":"functionaut\/0002-use-google-closure-compiler.md","Context":"## Context\\nWe must allow developers to use new JavaScript syntax and features without excluding older execution environments. Code must be automatically checked against common development mistakes and optimised for download and execution.\\n","Decision":"We acknowledge that there is a plethora of Node.js tooling options available, however we have decided to use the [Google Closure Compiler]. It is developed and maintained by Google and is used for high traffic, complex and global applications such as Gmail and Google Maps. By adopting this tool we leverage decade of research and engineering in that field.\\n","tokens":38,"id":2687,"text":"## Context\\nWe must allow developers to use new JavaScript syntax and features without excluding older execution environments. Code must be automatically checked against common development mistakes and optimised for download and execution.\\n\n\n##Decision\nWe acknowledge that there is a plethora of Node.js tooling options available, however we have decided to use the [Google Closure Compiler]. It is developed and maintained by Google and is used for high traffic, complex and global applications such as Gmail and Google Maps. By adopting this tool we leverage decade of research and engineering in that field.\\n","MatchedIds":[1404,4224,5070,400,4868],"Predicted":"## Decision\nWe will use Babel to transpile modern JavaScript syntax to be compatible with older environments. \nWe will integrate ESLint to automatically check for common development mistakes.\nWe will use Webpack to bundle and optimize the code for download and execution.\nThe Babel, ESLint, and Webpack configurations will be included in the project, and the build process will include all these tools to ensure consistency and optimization.","GenTime":"2024-08-19 23:52:54"}
{"File Name":"libelektra\/warning_array.md","Context":"## Problem\\nCurrently multiple warnings are saved in an elektra non-conforming array\\nnotation which is limited to 100 entries. The notation of `#00` is against\\nthe design [decision made](array.md).\\n","Decision":"The format should be aligned with the correct array notation,\\nstarting with `#0`. The maximum number of warnings will stay at\\n100 entries (`#0` - `#_99`).\\n- [Array](array.md)\\n","tokens":46,"id":1303,"text":"## Problem\\nCurrently multiple warnings are saved in an elektra non-conforming array\\nnotation which is limited to 100 entries. The notation of `#00` is against\\nthe design [decision made](array.md).\\n\n\n##Decision\nThe format should be aligned with the correct array notation,\\nstarting with `#0`. The maximum number of warnings will stay at\\n100 entries (`#0` - `#_99`).\\n- [Array](array.md)\\n","MatchedIds":[1291,1293,1302,1308,1319],"Predicted":"## Decision\nAdopt the elektra-conforming array notation for warnings. Specifically:\n- Change the notation from `#00` to `#_00` to align with the array design decision.\n- Extend the limit on the number of warning entries from 100 to a scalable dynamic structure, removing any arbitrary caps.\n- Update existing documentation to reflect the new notation, including the [array.md](array.md) guideline.\n- Modify the API to support the new notation and ensure backward compatibility:\n    - Deprecate the old notation with appropriate warnings.\n    - Provide helper functions to convert from the old format to the new format.\n- Implement unit tests to ensure the correct behavior of handling arrays of warnings:\n    - Test conversion from old to new notations.\n    - Verify that more than 100 warnings can be managed efficiently.\n- Review and update existing plugins to ensure compliance with the new array notation.\n\nAll modifications should maintain consistency with the rest of the Elektra project, ensuring no loss in functionality or performance.\n\n","GenTime":"2024-08-19 23:53:05"}
{"File Name":"dotfiles\/0003-switch-to-use-zsh-in-place-of-bash.md","Context":"## Context\\n[Starting with macOS Catalina, zsh will be the default shell](https:\/\/support.apple.com\/en-us\/HT208050).\\n","Decision":"zsh should be used in place of bash.\\n","tokens":31,"id":115,"text":"## Context\\n[Starting with macOS Catalina, zsh will be the default shell](https:\/\/support.apple.com\/en-us\/HT208050).\\n\n\n##Decision\nzsh should be used in place of bash.\\n","MatchedIds":[152,118,1048,2968,400],"Predicted":"## Decision\nThe decision is to standardize on using zsh as the default shell for all development environments that include macOS Catalina or later. This aligns with the default configuration provided by Apple, reducing the overhead of managing different shells across various development setups. Additionally, we will provide guidelines and necessary configuration files (.zshrc) to ensure a smooth transition and a consistent developer experience.","GenTime":"2024-08-19 23:53:12"}
{"File Name":"up-fiscal-data\/003-selnium.md","Context":"## Context and Problem Statement\\nDecision required on the framework to be used to scrape and store data from the Koshvani platform in machine readable format.\\n","Decision":"- Structure of the Koshvani platform\\n- Platfrom links do not reflect selection criteria\\n- Automation job requirements for data scraping\\n- Periodical jobs and access to new data\\nUse [Selenium](https:\/\/www.selenium.dev\/) to create robust, browser-based automation.\\n","tokens":32,"id":445,"text":"## Context and Problem Statement\\nDecision required on the framework to be used to scrape and store data from the Koshvani platform in machine readable format.\\n\n\n##Decision\n- Structure of the Koshvani platform\\n- Platfrom links do not reflect selection criteria\\n- Automation job requirements for data scraping\\n- Periodical jobs and access to new data\\nUse [Selenium](https:\/\/www.selenium.dev\/) to create robust, browser-based automation.\\n","MatchedIds":[442,446,443,447,4747],"Predicted":"## Decision Drivers\n* Robust scraping capabilities\n* Scalability for handling large datasets\n* Ease of integration with existing systems\n* Support for data storage in machine-readable formats\n\n## Decision\nSelected Framework: **Scrapy with PostgreSQL**\n\n### Rationale:\n- **Robust scraping capabilities**: Scrapy is a powerful and widely-used scraping framework that can handle complex scraping tasks efficiently.\n- **Scalability**: Scrapy provides robust handling of large datasets, which is crucial for the extensive data from the Koshvani platform.\n- **Ease of integration**: Both Scrapy and PostgreSQL are well-supported and commonly used, making integration with existing systems straightforward.\n- **Support for machine-readable formats**: PostgreSQL supports a variety of data types and formats, ensuring that scraped data can be stored in a structured and machine-readable manner.","GenTime":"2024-08-19 23:53:16"}
{"File Name":"macondo\/0003-remote-repositories-management.md","Context":"## Context\\nRight now this cli tool comes with a builtin set of commands that is hardcoded\\nto a gist I own somewhere. This is not ideal.\\nWe want this tool to easily access \"repositories of commands\" that are either\\nremote or local; as well as a way to easily add\/remove repositories.\\nSome ideas of repositories:\\n- HTTP urls pointing to yaml manifests with commands definitions\\n- Github repositories\\n- Local files or directories\\n","Decision":"- We won't have built-in commands or repositories\\n- We will support three types of repositories:\\n- Remote http resources pointing to yaml files\\n- Local manifest yaml files\\n- Local directories containing .mcd files\\nBecause http resources are now supported, we could make it so that the\\nrepositories with commands has CI\/CD via Jenkins, which generates a repo and\\npublishes it to artifactory.\\nThe macondo file will be simplified to only include, for now, a top-level\\n`repositories` array, which would point to the repositories.\\nA macondo update command will be added that refreshes the list of commands\\nprovided by the repositories. This is only done for http repos, whose\\ncontents could change anytime.\\nA new set of commands to be implemented:\\n- `macondo repo add` to add repositories\\n- `macondo repo remove` to remove them\\n- `macondo repo update` to update them\\n- `macondo repo list` to list them\\n","tokens":92,"id":2191,"text":"## Context\\nRight now this cli tool comes with a builtin set of commands that is hardcoded\\nto a gist I own somewhere. This is not ideal.\\nWe want this tool to easily access \"repositories of commands\" that are either\\nremote or local; as well as a way to easily add\/remove repositories.\\nSome ideas of repositories:\\n- HTTP urls pointing to yaml manifests with commands definitions\\n- Github repositories\\n- Local files or directories\\n\n\n##Decision\n- We won't have built-in commands or repositories\\n- We will support three types of repositories:\\n- Remote http resources pointing to yaml files\\n- Local manifest yaml files\\n- Local directories containing .mcd files\\nBecause http resources are now supported, we could make it so that the\\nrepositories with commands has CI\/CD via Jenkins, which generates a repo and\\npublishes it to artifactory.\\nThe macondo file will be simplified to only include, for now, a top-level\\n`repositories` array, which would point to the repositories.\\nA macondo update command will be added that refreshes the list of commands\\nprovided by the repositories. This is only done for http repos, whose\\ncontents could change anytime.\\nA new set of commands to be implemented:\\n- `macondo repo add` to add repositories\\n- `macondo repo remove` to remove them\\n- `macondo repo update` to update them\\n- `macondo repo list` to list them\\n","MatchedIds":[2703,771,3566,400,398],"Predicted":"## Decision\nWe will implement a flexible repository system for commands in the CLI tool. This system will support the following repository types:\n\n### 1. HTTP URLs\n* The CLI tool can access YAML manifests hosted at HTTP URLs.\n* Manifests will define the command structure and metadata.\n\n### 2. GitHub Repositories\n* The tool will support repositories hosted on GitHub.\n* Commands can be tracked in a specific directory within the repository, defined in a configuration file.\n\n### 3. Local Files or Directories\n* Local directories containing command definitions or individual command files will be supported.\n* The CLI can add or remove these local repositories via simple path specifications.\n\n### 4. Command for Managing Repositories\n* A new set of subcommands will be introduced: `add-repo`, `remove-repo`, and `list-repos`.\n* These commands will allow for easy management of repositories.\n\n### 5. Configuration Storage\n* The repositories and their metadata will be stored in a configuration file (e.g., a YAML or JSON file) located in the user's home directory.\n* This configuration file will be read by the CLI tool to determine available commands.\n\nThis approach facilitates easier updates and management of commands in various environments, aligning with user needs for both remote and local command repositories.","GenTime":"2024-08-19 23:53:20"}
{"File Name":"cloud-platform\/020-Environments-and-Pipeline.md","Context":"## Context\\nThe key proposition of Cloud Platform is to do the \"hosting\" of services, and we choose [Kubernetes for container management](004-use-kubernetes-for-container-management.md).\\nIn agreeing a good interface for service teams, there several concerns:\\n* Definitions - teams should be able to specify the workloads and infrastructure they want running.\\n* Control - teams should be able to use a default hosting configuration, getting things running as simply as with a PaaS. However teams should also have full control over their Kubernetes resources, including pod configuration, lifecycle, network connectivity, etc.\\n* Multi-tenancy - Service teams' workloads need isolation between their dev and prod environments, and from other service teams' workloads.\\n","Decision":"1. Teams are offered 'namespaces'. A namespace is the concept of an isolated environment for workloads\/resources.\\n2. A CP namespace is implemented as a Kubernetes namespace and AWS resources (e.g. RDS instance, S3 bucket).\\n3. Isolation in Kubernetes namespaces is implemented using RBAC and NetworkPolicy:\\n* RBAC - teams can only administer k8s resources in their own namespaces\\n* NetworkPolicy - containers can only receive traffic from its ingresses and other containers in the same namespace (implemented with a NetworkPolicy, which teams can edit if needed)\\n4. Isolation between AWS resources is achieved using access control.\\nEach ECR repo, or S3 bucket, RDS bucket is made accessible to an IAM User, and the team are provided access key credentials for it.\\n5. A user defines a namespace in files: YAML (Kubernetes) and Terraform (AWS resources).\\nThe YAML includes by default: a Namespace and various default limits on resources, pods and networking.\\nFor deploying a simple workload, teams can include a YAML Deployment etc, so that these get applied automatically by CP's pipeline. Alternatively teams get more control by managing app resources using their namespace credentials - see below.\\nThe Terraform can specify any AWS resources like S3 buckets, RDS databases, Elasticache. Typically teams specify an ECR repo, so they have somewhere to deploy their images to.\\n6. The namespace definition is held in GitHub.\\nGitHub provides a mechanism for peer-review, automated checks and versioning.\\nOther options considered for configuring a namespace do not come with these advantages, for example:\\n* a console \/ web form, implemented as a custom web app (click ops)\\n* commands via a CLI or API\\nNamespace definitions are stored in the [environments repo](https:\/\/github.com\/ministryofjustice\/cloud-platform-environments)\\n7. Namespace changes are checked by both a bot and a human from the CP team\\nIn Kubernetes, cluster-wide privileges are required to apply changes to a Kubernetes Namespace, as well as associated resources: LimitRange, NetworkPolicy and ServiceAccount. These privileges mean that the blast radius is large when applying changes.\\nIn terms of AWS resources, for common ones like S3 and RDS we provide terraform modules - to abstract away detail and promote best practice (for example, setting default encryption for S3 buckets). However Terraform can specify a huge range of AWS resources, each with multitude options. There are likely ways that one team can disrupt or get access to other teams' AWS services, that we can't anticipate, which is a risk to manage.\\nTo mitigate these concerns:\\n* [automated checks](https:\/\/github.com\/ministryofjustice\/cloud-platform-environments\/tree\/main\/.github\/workflows) are used to validate against common problems\\n* Human review (by an engineer on the CP team) is also required on PRs, to check against unanticipated problems\\n8. Pipeline to deploy namespace automatically.\\nThe \"deploy pipeline\" is a CI\/CD pipeline that applies teams' namespace definitions in the clusters and AWS account. It triggers when the reviewed PR is merged to master.\\n9. Teams have full control within their Kubernetes namespace\\nUsers are given access to Kubernetes user credentials (kubecfg) with admin rights to their namespace. This gives them full control over their pods etc. They can deploy with 'kubectl apply' or Helm. They can debug problems with pod starting up, see logs etc.\\nUsers are also invited to create a ServiceAccount (using their environment YAML), and provide the creds to their CI\/CD, for deploying their app.\\n","tokens":152,"id":631,"text":"## Context\\nThe key proposition of Cloud Platform is to do the \"hosting\" of services, and we choose [Kubernetes for container management](004-use-kubernetes-for-container-management.md).\\nIn agreeing a good interface for service teams, there several concerns:\\n* Definitions - teams should be able to specify the workloads and infrastructure they want running.\\n* Control - teams should be able to use a default hosting configuration, getting things running as simply as with a PaaS. However teams should also have full control over their Kubernetes resources, including pod configuration, lifecycle, network connectivity, etc.\\n* Multi-tenancy - Service teams' workloads need isolation between their dev and prod environments, and from other service teams' workloads.\\n\n\n##Decision\n1. Teams are offered 'namespaces'. A namespace is the concept of an isolated environment for workloads\/resources.\\n2. A CP namespace is implemented as a Kubernetes namespace and AWS resources (e.g. RDS instance, S3 bucket).\\n3. Isolation in Kubernetes namespaces is implemented using RBAC and NetworkPolicy:\\n* RBAC - teams can only administer k8s resources in their own namespaces\\n* NetworkPolicy - containers can only receive traffic from its ingresses and other containers in the same namespace (implemented with a NetworkPolicy, which teams can edit if needed)\\n4. Isolation between AWS resources is achieved using access control.\\nEach ECR repo, or S3 bucket, RDS bucket is made accessible to an IAM User, and the team are provided access key credentials for it.\\n5. A user defines a namespace in files: YAML (Kubernetes) and Terraform (AWS resources).\\nThe YAML includes by default: a Namespace and various default limits on resources, pods and networking.\\nFor deploying a simple workload, teams can include a YAML Deployment etc, so that these get applied automatically by CP's pipeline. Alternatively teams get more control by managing app resources using their namespace credentials - see below.\\nThe Terraform can specify any AWS resources like S3 buckets, RDS databases, Elasticache. Typically teams specify an ECR repo, so they have somewhere to deploy their images to.\\n6. The namespace definition is held in GitHub.\\nGitHub provides a mechanism for peer-review, automated checks and versioning.\\nOther options considered for configuring a namespace do not come with these advantages, for example:\\n* a console \/ web form, implemented as a custom web app (click ops)\\n* commands via a CLI or API\\nNamespace definitions are stored in the [environments repo](https:\/\/github.com\/ministryofjustice\/cloud-platform-environments)\\n7. Namespace changes are checked by both a bot and a human from the CP team\\nIn Kubernetes, cluster-wide privileges are required to apply changes to a Kubernetes Namespace, as well as associated resources: LimitRange, NetworkPolicy and ServiceAccount. These privileges mean that the blast radius is large when applying changes.\\nIn terms of AWS resources, for common ones like S3 and RDS we provide terraform modules - to abstract away detail and promote best practice (for example, setting default encryption for S3 buckets). However Terraform can specify a huge range of AWS resources, each with multitude options. There are likely ways that one team can disrupt or get access to other teams' AWS services, that we can't anticipate, which is a risk to manage.\\nTo mitigate these concerns:\\n* [automated checks](https:\/\/github.com\/ministryofjustice\/cloud-platform-environments\/tree\/main\/.github\/workflows) are used to validate against common problems\\n* Human review (by an engineer on the CP team) is also required on PRs, to check against unanticipated problems\\n8. Pipeline to deploy namespace automatically.\\nThe \"deploy pipeline\" is a CI\/CD pipeline that applies teams' namespace definitions in the clusters and AWS account. It triggers when the reviewed PR is merged to master.\\n9. Teams have full control within their Kubernetes namespace\\nUsers are given access to Kubernetes user credentials (kubecfg) with admin rights to their namespace. This gives them full control over their pods etc. They can deploy with 'kubectl apply' or Helm. They can debug problems with pod starting up, see logs etc.\\nUsers are also invited to create a ServiceAccount (using their environment YAML), and provide the creds to their CI\/CD, for deploying their app.\\n","MatchedIds":[3889,1376,856,2752,4041],"Predicted":"## Decision\nWe will provide a Kubernetes-based hosting platform that caters to the requirements of definitions, control, and multi-tenancy by implementing the following:\n\n1. **Default Hosting Configuration**: Offer a default hosting configuration akin to PaaS, enabling teams to deploy workloads quickly and efficiently without deep Kubernetes expertise.\n  \n2. **Customizable Kubernetes Resources**: Allow service teams to fully manage and configure their Kubernetes resources, including pod specifications, lifecycle management, and network connectivity.\n\n3. **Namespace-Based Isolation**: Utilize Kubernetes namespaces to ensure strong isolation between different service teams' workloads and between their development and production environments.\n\n4. **Security Policies and Resource Quotas**: Implement security policies, such as Role-Based Access Control (RBAC), and resource quotas to further ensure isolation and controlled resource usage for each team.\n\n5. **Helm for Deployment Definitions**: Use Helm charts to simplify and standardize the definition and deployment of workloads while allowing overrides for environment-specific configurations.\n\nThese steps will ensure that service teams have the flexibility, control, and isolation they need while benefiting from the efficiencies of a managed Kubernetes platform.","GenTime":"2024-08-19 23:53:26"}
{"File Name":"cdh-adrs\/0002-cdh-etl-logic-boundaries.md","Context":"## Context\\nTo ensure a clear separation as regards what logic resides within CDH and ETL related services where there exists feed specific\\nrequirements during onboarding and processing of feed files, the following issues where considered:\\n- Will such logic require specialized implementation across feeds?\\n- Are such logic generic and require one time implementation or will require continous change\/update?\\n- What are the benefits of moving such logic into ETL instead of CDH.\\n","Decision":"The most important point agreed on was that CDH will remain focused on defined object and data models as possible and CDH will\\nrun with the expectation that all inputs received are completed. This means CDH should not have domain specific knowledge in regards\\nspecific intricacies about how specific feeds are reconcilied into complete Positions, Transactions and Ticker data.\\nSuch specificity will reside within the ETL service and be housed based on each feed parsing logic within the ETL service.\\nThe benefits of such a system is that only ETL needs to change to accomodate new and changing requirements of old and new feeds\\nensuring the final result is always consistent to march what the CDH service requires.\\n","tokens":92,"id":8,"text":"## Context\\nTo ensure a clear separation as regards what logic resides within CDH and ETL related services where there exists feed specific\\nrequirements during onboarding and processing of feed files, the following issues where considered:\\n- Will such logic require specialized implementation across feeds?\\n- Are such logic generic and require one time implementation or will require continous change\/update?\\n- What are the benefits of moving such logic into ETL instead of CDH.\\n\n\n##Decision\nThe most important point agreed on was that CDH will remain focused on defined object and data models as possible and CDH will\\nrun with the expectation that all inputs received are completed. This means CDH should not have domain specific knowledge in regards\\nspecific intricacies about how specific feeds are reconcilied into complete Positions, Transactions and Ticker data.\\nSuch specificity will reside within the ETL service and be housed based on each feed parsing logic within the ETL service.\\nThe benefits of such a system is that only ETL needs to change to accomodate new and changing requirements of old and new feeds\\nensuring the final result is always consistent to march what the CDH service requires.\\n","MatchedIds":[3416,2342,1205,3784,443],"Predicted":"## Decision\nMove feed-specific logic to ETL services instead of CDH.\n\n### Reasoning\n1. **Specialized Implementation Across Feeds**: ETL processes are generally designed to handle data complexities and transformations specific to each feed, allowing for specialized implementation as required.\n2. **Flexibility and Maintainability**: Logic within ETL can be updated continuously to accommodate changes in feed requirements without impacting the core data model or processes within CDH. This allows for greater flexibility and easier maintenance.\n3. **Benefits of ETL**:\n   - **Modularity**: By separating the logic, we can isolate changes to specific ETL components without affecting other parts of the system.\n   - **Scalability**: ETL services can be scaled independently based on the processing needs of each feed, which improves overall system performance.\n   - **Error Handling and Logging**: ETL platforms typically provide robust error handling, logging, and monitoring capabilities, which improves the reliability and traceability of feed processing.\n   - **Domain Separation**: CDH remains focused on core data handling and storage, while ETL handles transformation and onboarding, ensuring a clear separation of concerns and cleaner architecture.\n\nBy moving the feed-specific logic to ETL, we achieve a more modular, maintainable, and scalable architecture, facilitating easier updates and specialized processing for different data feeds.","GenTime":"2024-08-19 23:53:31"}
{"File Name":"celestia-core\/adr-056-light-client-amnesia-attacks.md","Context":"## Context\\nWhilst most created evidence of malicious behavior is self evident such that any individual can verify them independently there are types of evidence, known collectively as global evidence, that require further collaboration from the network in order to accumulate enough information to create evidence that is individually verifiable and can therefore be processed through consensus. [Fork Accountability](https:\/\/github.com\/tendermint\/spec\/blob\/master\/spec\/consensus\/light-client\/accountability.md) has been coined to describe the entire process of detection, proving and punishing of malicious behavior. This ADR addresses specifically what a light client amnesia attack is and how it can be proven and the current decision around handling light client amnesia attacks. For information on evidence handling by the light client, it is recommended to read [ADR 47](https:\/\/github.com\/tendermint\/tendermint\/blob\/master\/docs\/architecture\/adr-047-handling-evidence-from-light-client.md).\\n### Amnesia Attack\\nThe schematic below explains a scenario where an amnesia attack can occur such that two sets of honest nodes, C1 and C2, commit different blocks.\\n![](..\/imgs\/tm-amnesia-attack.png)\\n1. C1 and F send PREVOTE messages for block A.\\n2. C1 sends PRECOMMIT for round 1 for block A.\\n3. A new round is started, C2 and F send PREVOTE messages for a different block B.\\n4. C2 and F then send PRECOMMIT messages for block B.\\n5. F later on creates PRECOMMITS for block A and combines it with those from C1 to form a block\\nThis forged block can then be used to fool light clients trying to verify it. It must be stressed that there are a few more hurdles or dimensions to the attack to consider.For a more detailed walkthrough refer to Appendix A.\\n","Decision":"The decision surrounding amnesia attacks has both a short term and long term component. In the long term, a more sturdy protocol will need to be fleshed out and implemented. There is already draft documents outlining what such a protocol would look like and the resources it would require (see references). Prior revisions however outlined a protocol which had been implemented (See Appendix B). It was agreed that it still required greater consideration and review given it's importance. It was therefore discussed, with the limited time frame set before 0.34, whether the protocol should be completely removed or if there should remain some logic in handling the aforementioned scenarios.\\nThe latter of the two options meant storing a record of all votes in any height with which there was more than one round. This information would then be accessible for applications if they wanted to perform some off-chain verification and punishment.\\nIn summary, this seemed like too much to ask of the application to implement only on a temporary basis, whilst not having the domain specific knowledge and considering such a difficult and unlikely attack. Therefore the short term decision is to identify when the attack has occurred and implement the detector algorithm highlighted in [ADR 47](https:\/\/github.com\/tendermint\/tendermint\/blob\/master\/docs\/architecture\/adr-047-handling-evidence-from-light-client.md) but to not implement any accountability protocol that would identify malicious validators and allow applications to punish them. This will hopefully change in the long term with the focus on eventually reaching a concrete and secure protocol with identifying and dealing with these attacks.\\n","tokens":379,"id":4113,"text":"## Context\\nWhilst most created evidence of malicious behavior is self evident such that any individual can verify them independently there are types of evidence, known collectively as global evidence, that require further collaboration from the network in order to accumulate enough information to create evidence that is individually verifiable and can therefore be processed through consensus. [Fork Accountability](https:\/\/github.com\/tendermint\/spec\/blob\/master\/spec\/consensus\/light-client\/accountability.md) has been coined to describe the entire process of detection, proving and punishing of malicious behavior. This ADR addresses specifically what a light client amnesia attack is and how it can be proven and the current decision around handling light client amnesia attacks. For information on evidence handling by the light client, it is recommended to read [ADR 47](https:\/\/github.com\/tendermint\/tendermint\/blob\/master\/docs\/architecture\/adr-047-handling-evidence-from-light-client.md).\\n### Amnesia Attack\\nThe schematic below explains a scenario where an amnesia attack can occur such that two sets of honest nodes, C1 and C2, commit different blocks.\\n![](..\/imgs\/tm-amnesia-attack.png)\\n1. C1 and F send PREVOTE messages for block A.\\n2. C1 sends PRECOMMIT for round 1 for block A.\\n3. A new round is started, C2 and F send PREVOTE messages for a different block B.\\n4. C2 and F then send PRECOMMIT messages for block B.\\n5. F later on creates PRECOMMITS for block A and combines it with those from C1 to form a block\\nThis forged block can then be used to fool light clients trying to verify it. It must be stressed that there are a few more hurdles or dimensions to the attack to consider.For a more detailed walkthrough refer to Appendix A.\\n\n\n##Decision\nThe decision surrounding amnesia attacks has both a short term and long term component. In the long term, a more sturdy protocol will need to be fleshed out and implemented. There is already draft documents outlining what such a protocol would look like and the resources it would require (see references). Prior revisions however outlined a protocol which had been implemented (See Appendix B). It was agreed that it still required greater consideration and review given it's importance. It was therefore discussed, with the limited time frame set before 0.34, whether the protocol should be completely removed or if there should remain some logic in handling the aforementioned scenarios.\\nThe latter of the two options meant storing a record of all votes in any height with which there was more than one round. This information would then be accessible for applications if they wanted to perform some off-chain verification and punishment.\\nIn summary, this seemed like too much to ask of the application to implement only on a temporary basis, whilst not having the domain specific knowledge and considering such a difficult and unlikely attack. Therefore the short term decision is to identify when the attack has occurred and implement the detector algorithm highlighted in [ADR 47](https:\/\/github.com\/tendermint\/tendermint\/blob\/master\/docs\/architecture\/adr-047-handling-evidence-from-light-client.md) but to not implement any accountability protocol that would identify malicious validators and allow applications to punish them. This will hopefully change in the long term with the focus on eventually reaching a concrete and secure protocol with identifying and dealing with these attacks.\\n","MatchedIds":[1944,1441,28,21,1957],"Predicted":"## Decision\n\nTo address the issue of light client amnesia attacks, we propose the following approach to detect, prove, and punish validators involved in such malicious behavior:\n\n### Detection\n\n1. **Monitoring Misbehavior**:\n   - Each full node in the network should be made aware of all votes cast by validators within the same round to monitor for conflicting votes.\n   \n2. **Collecting Evidence**:\n   - Nodes should store all `PREVOTE` and `PRECOMMIT` messages that they receive.\n   - Periodically, nodes should check for any inconsistencies, especially looking for validators who voted for more than one block in the same round.\n\n### Proving\n\n3. **Constructing Evidence**:\n   - If conflicting votes are detected (e.g., a `PREVOTE` for block A and a `PREVOTE` for block B in the same round), these votes should be collated and formatted as `ConflictingVotesEvidence`.\n\n4. **Broadcasting Evidence**:\n   - The `ConflictingVotesEvidence` should then be broadcast to the entire network. Each node will validate the evidence independently by checking if the conflicting votes meet the criteria set for malicious behavior.\n\n### Punishment\n\n5. **Consensus Handling**:\n   - Once evidence is validated, it is submitted to a consensus process for acknowledgment and action.\n   - Validators found guilty of double voting or participating in an amnesia attack should be immediately slashed according to pre-defined slashing conditions set by the governance protocol.\n\n6. **Network-Wide Awareness**:\n   - Nodes should flag and blacklist the malicious validator keys involved and broadcast the information across the network to update the validator sets.\n\n### Implementation\n\n- **Network Protocol Enhancement**:\n   - Modify the consensus protocol to incorporate handling of conflicting votes detection and evidence gathering.\n   - Ensure the `ConflictingVotesEvidence` format is standardized and integrated into the consensus proof mechanism.\n\n- **Governance and Configuration**:\n   - Define the slashing conditions and consequences for such attacks in the governance protocol.\n   - Allow the configuration of time windows and parameters for evidence collection and broadcasting.\n\n#### Example Pseudocode for Evidence Handling\n\n```go\ntype ConflictingVotesEvidence struct {\n    ValidatorAddress string\n    PrevoteA         Vote \/\/ PREVOTE for Block A\n    PrevoteB         Vote \/\/ PREVOTE for Block B\n}\n\nfunc DetectAndHandleAmnesiaAttack(votes []Vote) {\n    conflictingVotes := map[string]ConflictingVotesEvidence{}\n    \n    for _, vote := range votes {\n        key := vote.ValidatorAddress\n        if existingVote, ok := conflictingVotes[key]; ok {\n            if vote.BlockID != existingVote.PrevoteA.BlockID {\n                evidence := ConflictingVotesEvidence{\n                    ValidatorAddress: key,\n                    PrevoteA:         existingVote.PrevoteA,\n                    PrevoteB:         vote,\n                }\n                BroadcastEvidence(evidence)\n            }\n        } else {\n            conflictingVotes[key] = ConflictingVotesEvidence{ValidatorAddress: key, PrevoteA: vote}\n        }\n    }\n}\n\nfunc BroadcastEvidence(evidence ConflictingVotesEvidence) {\n    \/\/ Broadcast `ConflictingVotesEvidence` to the network\n    network.Broadcast(evidence)\n}\n```\n\n### Conclusion\n\nBy implementing this enhanced detection, proof, and punishment mechanism for light client amnesia attacks, we aim to significantly improve the network's ability to handle such malicious behavior effectively, thus maintaining the integrity and trustworthiness of the blockchain network.","GenTime":"2024-08-19 23:53:45"}
{"File Name":"elife-xpub\/0004-code-structure.md","Context":"## Context\\nAs the code rapidly grows we need a common structure and approach that will keep the code tidy and coherent.\\nThere are various aspects, the initial concern was the data access separation. The issue #140 initially talked about how this could be separated in terms of an ORM (Object-Relational Mapping) or a DAL (Data Access Layer).\\nHowever, this discussion also soon covered the structure of the code in general and how to structure it for ease of development and best practise.\\n### Rejected Options\\nWe considered the following options:\\n* The Pubsweet Way - This was to define a base class \"Model\" and extend this for each entity. This was rejected as its another self made ORM but and has validations that we don't want.\\n* Other ORMs : Waterline, sequelize, bookshelf - General purpose ORMs were rejected based on the experience that the time spent going up the learning curve and sifting through the docs when things go wrong is greater then simply rolling your own.\\n* Query Builders : Knex.js -These were not entirely ruled out, and were deemed compatible with a good code structure that would allow the database access to be encapsulated in a way that would allow a query builder to be used where necessary.\\n","Decision":"In summary the server-side code will be structured below the `server` folder as follows:\\n* **entities** - This folder will contain subfolders relating to named entities in the system. See [\"Data Model Specification\"](https:\/\/docs.google.com\/document\/d\/1KU-DLMNhPxjQF2j8HVlJvenvttPLxgtbTHo8Sy_PNRc\/).\\n* **entities\/\\<entity\\>** - The example below shows `Mansuscript` as an example entity, this folder will contain a common set of files that describe the entity's behaviour. Each of these have a particular purpose explained below:\\n* index.js - The main business logic, e.g. for `Manuscript` this could contain `getAuthor()`\\n* typedefs.js - Contains the GraphQL types pertinent to this entity.\\n* resolvers.js - Contains the GraphQL interface specified in terms of `Query` and `Mutation` 's for this entity. These in general should map onto the exposed functions from index.js. For example, `Manuscript` may contain a `Query` for `allManuscripts` and a `Mutation` for `createManuscript`.\\n* data-access.js - Contains the functions for saving to and loading from the database. All references to the database and SQL that is written should remain private to this file.\\n* **manuscript-state-machine** - It is anticipated that this is where the most orchestration of the entites will take place. This has been created as a separate folder so that it can expand to implement the state transitions currently defined for the manuscript.\\n### Example\\n```\\nserver\\n|\\n\u251c\u2500\u2500 entities\\n|   |\\n\u2502   \u251c\u2500\u2500 manuscript\\n\u2502   \u2502   \u251c\u2500\u2500 typedefs.js\\n\u2502   \u2502   \u251c\u2500\u2500 resolvers.js\\n\u2502   \u2502   \u251c\u2500\u2500 data-access.js\\n\u2502   \u2502   \u2514\u2500\u2500 index.js\\n|   |\\n\u2502   \u2514\u2500\u2500 <entity>\\n\u2502       \u251c\u2500\u2500 typedefs.js\\n\u2502       \u251c\u2500\u2500 resolvers.js\\n\u2502       \u251c\u2500\u2500 data-access.js\\n\u2502       \u2514\u2500\u2500 index.js\\n|\\n\u2514\u2500\u2500 manuscript-state-machine\\n\u2514\u2500\u2500 index.js\\n```\\n","tokens":257,"id":3776,"text":"## Context\\nAs the code rapidly grows we need a common structure and approach that will keep the code tidy and coherent.\\nThere are various aspects, the initial concern was the data access separation. The issue #140 initially talked about how this could be separated in terms of an ORM (Object-Relational Mapping) or a DAL (Data Access Layer).\\nHowever, this discussion also soon covered the structure of the code in general and how to structure it for ease of development and best practise.\\n### Rejected Options\\nWe considered the following options:\\n* The Pubsweet Way - This was to define a base class \"Model\" and extend this for each entity. This was rejected as its another self made ORM but and has validations that we don't want.\\n* Other ORMs : Waterline, sequelize, bookshelf - General purpose ORMs were rejected based on the experience that the time spent going up the learning curve and sifting through the docs when things go wrong is greater then simply rolling your own.\\n* Query Builders : Knex.js -These were not entirely ruled out, and were deemed compatible with a good code structure that would allow the database access to be encapsulated in a way that would allow a query builder to be used where necessary.\\n\n\n##Decision\nIn summary the server-side code will be structured below the `server` folder as follows:\\n* **entities** - This folder will contain subfolders relating to named entities in the system. See [\"Data Model Specification\"](https:\/\/docs.google.com\/document\/d\/1KU-DLMNhPxjQF2j8HVlJvenvttPLxgtbTHo8Sy_PNRc\/).\\n* **entities\/\\<entity\\>** - The example below shows `Mansuscript` as an example entity, this folder will contain a common set of files that describe the entity's behaviour. Each of these have a particular purpose explained below:\\n* index.js - The main business logic, e.g. for `Manuscript` this could contain `getAuthor()`\\n* typedefs.js - Contains the GraphQL types pertinent to this entity.\\n* resolvers.js - Contains the GraphQL interface specified in terms of `Query` and `Mutation` 's for this entity. These in general should map onto the exposed functions from index.js. For example, `Manuscript` may contain a `Query` for `allManuscripts` and a `Mutation` for `createManuscript`.\\n* data-access.js - Contains the functions for saving to and loading from the database. All references to the database and SQL that is written should remain private to this file.\\n* **manuscript-state-machine** - It is anticipated that this is where the most orchestration of the entites will take place. This has been created as a separate folder so that it can expand to implement the state transitions currently defined for the manuscript.\\n### Example\\n```\\nserver\\n|\\n\u251c\u2500\u2500 entities\\n|   |\\n\u2502   \u251c\u2500\u2500 manuscript\\n\u2502   \u2502   \u251c\u2500\u2500 typedefs.js\\n\u2502   \u2502   \u251c\u2500\u2500 resolvers.js\\n\u2502   \u2502   \u251c\u2500\u2500 data-access.js\\n\u2502   \u2502   \u2514\u2500\u2500 index.js\\n|   |\\n\u2502   \u2514\u2500\u2500 <entity>\\n\u2502       \u251c\u2500\u2500 typedefs.js\\n\u2502       \u251c\u2500\u2500 resolvers.js\\n\u2502       \u251c\u2500\u2500 data-access.js\\n\u2502       \u2514\u2500\u2500 index.js\\n|\\n\u2514\u2500\u2500 manuscript-state-machine\\n\u2514\u2500\u2500 index.js\\n```\\n","MatchedIds":[4021,3470,1792,4361,4167],"Predicted":"## Decision\nWe decided to adopt a custom DAL (Data Access Layer) for our project's database interactions. This approach provides us with the flexibility to tailor our data access logic to the specific needs of our application, without the overhead and limitations often associated with third-party ORMs. \n\nThe structure of the DAL will be modular, with each module encapsulating the data access logic for a specific entity. This modularity promotes separation of concerns and makes the code easier to manage and maintain. \n\nWe will also use [Knex.js](http:\/\/knexjs.org\/) as a query builder to simplify and standardize complex SQL queries within our DAL. Knex.js offers a balance between the full automation provided by ORMs and the manual approach of writing raw SQL queries, which can help us maintain a cleaner codebase and reduce the risk of SQL injection.\n\n### Positive Consequences\n* **Tailored solutions**: We can implement and optimize the data access logic specifically for our requirements.\n* **Better maintainability**: Encapsulating database calls in a modular DAL promotes cleaner and more maintainable code.\n* **Performance**: We can fine-tune queries for better performance compared to what might be achievable with a generic ORM.\n* **Flexibility**: Allows us to incrementally integrate advanced querying capabilities using Knex.js where necessary.\n\n### Negative Consequences\n* **Increased initial development effort**: Building a custom DAL requires more time upfront compared to using an existing ORM.\n* **Potential for inconsistency**: Without the constraints and conventions of an ORM, there is a risk of inconsistent data access patterns if not properly managed.\n* **Manual handling of relationships**: Unlike ORMs, relationships between entities will need to be mapped and handled manually, increasing the potential for errors.\n\nBy adopting a custom DAL complemented with Knex.js, we aim to achieve a balance between flexibility and maintainability, tailored to the specific needs and complexities of our application.","GenTime":"2024-08-19 23:53:52"}
{"File Name":"up-fiscal-data\/005-phases.md","Context":"## Context and Problem Statement\\nNumerous sub-sections from Expenditure were selected to be scraped and analysed. A phased approach will be employed to make sustainable jobs for each section.\\n## Decision Drivers\\n* Use the data from differnt sections to impute accounting head information\\n* The information from various sections will be used to create data checks\\n* Long term project with 3 year pipeline to create a sustainable archirtecture\\n","Decision":"* Use the data from differnt sections to impute accounting head information\\n* The information from various sections will be used to create data checks\\n* Long term project with 3 year pipeline to create a sustainable archirtecture\\nSet a priority list and weekly targets foe the data that needs to be scraped from Koshvani.\\n","tokens":88,"id":443,"text":"## Context and Problem Statement\\nNumerous sub-sections from Expenditure were selected to be scraped and analysed. A phased approach will be employed to make sustainable jobs for each section.\\n## Decision Drivers\\n* Use the data from differnt sections to impute accounting head information\\n* The information from various sections will be used to create data checks\\n* Long term project with 3 year pipeline to create a sustainable archirtecture\\n\n\n##Decision\n* Use the data from differnt sections to impute accounting head information\\n* The information from various sections will be used to create data checks\\n* Long term project with 3 year pipeline to create a sustainable archirtecture\\nSet a priority list and weekly targets foe the data that needs to be scraped from Koshvani.\\n","MatchedIds":[447,446,445,442,3784],"Predicted":"## Decision\nAdopt a phased approach to scrape, analyze, and create sustainable jobs for each expenditure sub-section. \n\n### Phase 1:\n- Identify and prioritize sub-sections based on their relevance and data availability.\n- Create preliminary scraping jobs for these sub-sections.\n- Validate the data against known checks to ensure accuracy and consistency.\n\n### Phase 2:\n- Scale up the scraping jobs to include more sub-sections.\n- Enhance data validation mechanisms using information from multiple sections for cross-verification.\n\n### Phase 3:\n- Implement a robust data imputation system based on the aggregated data from various sub-sections.\n- Optimize and automate the data checks process with machine learning algorithms where applicable.\n- Continuously monitor and refine the scraping and data validation jobs to align with the sustainable architecture goals.\n\nOverall, a detailed roadmap will be created, outlining key milestones and review points over the 3-year period to ensure the architecture meets the project's long-term objectives.","GenTime":"2024-08-19 23:53:57"}
{"File Name":"infrastructure-adrs\/0010-message-broker.md","Context":"## Context and Problem Statement\\nSDR is evolving into an (increasing) set of (largely) single-purpose services, with communication between these services occurring via synchronous HTTP. This architecture is fragile as it scales under load and as the number of services increases because:\\n* Each service must have knowledge about many other services.\\n* Each service depends on many other services to perform its job; any of these services being unavailable can bring down the service or hamper performance.\\n* Synchronous HTTP can have a performance penalty for operations in which a response is not required (e.g., reporting events).\\n* The Stanford network is notoriously unreliable (especially at night); significant coding effort is required to account for this.\\nIn addition, there are already places within SDR were we have re-invented a message broker and\/or jerry-rigged around not having a message broker (e.g., polling via HTTP to determine if a long-running job has completed).\\nIntroducing a message broker to support asynchronous communication where appropriate will allow decoupling of services, making SDR more resilient and scalable.\\n","Decision":"TBD.\\n### Positive Consequences\\n* Increased ability to scale number of services.\\n* Increased ability to scale under load.\\n* Increased resilience to service failures.\\n* Provide mechanism for notification of the completion of long-running jobs.\\n### Negative Consequences\\n* Message broker must be supported by Ops.\\n* Services become dependent on the Message Broker. (However, Message Brokers are typically designed to be depended on.)\\n* The evolution work to begin using the Message Broker.\\n","tokens":224,"id":786,"text":"## Context and Problem Statement\\nSDR is evolving into an (increasing) set of (largely) single-purpose services, with communication between these services occurring via synchronous HTTP. This architecture is fragile as it scales under load and as the number of services increases because:\\n* Each service must have knowledge about many other services.\\n* Each service depends on many other services to perform its job; any of these services being unavailable can bring down the service or hamper performance.\\n* Synchronous HTTP can have a performance penalty for operations in which a response is not required (e.g., reporting events).\\n* The Stanford network is notoriously unreliable (especially at night); significant coding effort is required to account for this.\\nIn addition, there are already places within SDR were we have re-invented a message broker and\/or jerry-rigged around not having a message broker (e.g., polling via HTTP to determine if a long-running job has completed).\\nIntroducing a message broker to support asynchronous communication where appropriate will allow decoupling of services, making SDR more resilient and scalable.\\n\n\n##Decision\nTBD.\\n### Positive Consequences\\n* Increased ability to scale number of services.\\n* Increased ability to scale under load.\\n* Increased resilience to service failures.\\n* Provide mechanism for notification of the completion of long-running jobs.\\n### Negative Consequences\\n* Message broker must be supported by Ops.\\n* Services become dependent on the Message Broker. (However, Message Brokers are typically designed to be depended on.)\\n* The evolution work to begin using the Message Broker.\\n","MatchedIds":[2309,979,1575,2342,428],"Predicted":"## Decision\n**Adopt a Message Broker for Asynchronous Communication**\n\nTo address the issues of synchronous HTTP communication and to increase the resilience and scalability of the SDR services architecture, we will introduce a message broker to manage asynchronous communication between services.\n\n### Chosen Option: **RabbitMQ**\n\n#### Reasons for Choosing RabbitMQ:\n1. **Decoupling Services**: By using RabbitMQ, services will no longer need to have direct knowledge of each other. Instead, they will communicate via messages, which abstract the underlying service details.\n2. **Resilience and Fault Tolerance**: RabbitMQ supports message persistence and acknowledgement mechanisms, ensuring messages aren't lost even if a service is down or the network is unreliable.\n3. **Performance Optimization**: RabbitMQ can manage high-throughput messaging and supports complex routing that can be tailored to our needs, allowing for minimal performance impact during high load times.\n4. **Established Ecosystem**: RabbitMQ has a mature ecosystem, with client libraries and support available for a variety of programming languages that meet our needs.\n5. **Ease of Use**: The team already has experience with RabbitMQ, reducing the learning curve and ensuring a smoother transition.\n\n### Steps to Implement:\n1. **Integrate RabbitMQ** as the message broker into the SDR infrastructure.\n2. **Refactor Existing Services** to use asynchronous messaging wherever applicable, especially for operations where a response is not immediately required.\n3. **Implement Message Persistence and Acknowledgement** to ensure reliability and fault tolerance.\n4. **Set Up Monitoring and Alerting** for RabbitMQ to ensure it operates smoothly and to quickly address any issues.\n5. **Provide Training** for team members on best practices for developing with RabbitMQ and for transitioning existing synchronous communication patterns to an asynchronous model.\n\n### Positive Consequences:\n- **Increased Scalability**: Services will be more independent and can scale without being hampered by direct dependencies.\n- **Improved Resilience**: Services will be more resilient to failures and network issues.\n- **Reduced Performance Bottlenecks**: By offloading non-critical response operations to asynchronous messaging, overall system performance will improve.\n\n### Negative Consequences:\n- **Initial Complexity**: There will be some additional architectural complexity introduced initially as services are refactored to adopt asynchronous communication.\n- **Overhead of Learning and Transition**: There will be a temporary productivity dip as the team transitions to using RabbitMQ and adapts to the new communication model.\n\n### Conclusion:\nAdopting RabbitMQ as a message broker for asynchronous communication will enhance the resilience, scalability, and performance of the SDR service architecture. This decision is aligned with the long-term goals of decoupling services and leveraging robust messaging patterns to handle the dynamics of a growing distributed system.","GenTime":"2024-08-19 23:54:08"}
{"File Name":"form-design-system\/relative_spacing.md","Context":"## Context and Problem Statement\\nWe'd like to change the spacing system to something more generic.\\n## Decision Drivers <!-- optional -->\\nThe current pattern of relative spacing (half, double) that's based on a default, is deficient in a couple of ways:\\n- We'd like to add additional spacing values that don't map cleanly to these words. If default spacing is 16, there's no clean word to describe the jump to 24.\\n- The current model makes it impossible to ever cleanly change the default value. Given the model of default, half, and double, if we decided to make double the new default, the current default would have to be renamed to half, and half would need to be renamed to half\/half.\\n","Decision":"The current pattern of relative spacing (half, double) that's based on a default, is deficient in a couple of ways:\\n- We'd like to add additional spacing values that don't map cleanly to these words. If default spacing is 16, there's no clean word to describe the jump to 24.\\n- The current model makes it impossible to ever cleanly change the default value. Given the model of default, half, and double, if we decided to make double the new default, the current default would have to be renamed to half, and half would need to be renamed to half\/half.\\nGeneric tshirt system was chosen over the other options.\\n- Coding a direct \"arbitrary value\" into a CSS class (color, number representing a unit) is generally considered an anti-pattern for reasons that are too long to cover in this ADR.\\n- We like the numbering system but we use t-shirt sizes everywhere else in our system so it makes sense to skew towards alignment with the larger system.\\n","tokens":153,"id":801,"text":"## Context and Problem Statement\\nWe'd like to change the spacing system to something more generic.\\n## Decision Drivers <!-- optional -->\\nThe current pattern of relative spacing (half, double) that's based on a default, is deficient in a couple of ways:\\n- We'd like to add additional spacing values that don't map cleanly to these words. If default spacing is 16, there's no clean word to describe the jump to 24.\\n- The current model makes it impossible to ever cleanly change the default value. Given the model of default, half, and double, if we decided to make double the new default, the current default would have to be renamed to half, and half would need to be renamed to half\/half.\\n\n\n##Decision\nThe current pattern of relative spacing (half, double) that's based on a default, is deficient in a couple of ways:\\n- We'd like to add additional spacing values that don't map cleanly to these words. If default spacing is 16, there's no clean word to describe the jump to 24.\\n- The current model makes it impossible to ever cleanly change the default value. Given the model of default, half, and double, if we decided to make double the new default, the current default would have to be renamed to half, and half would need to be renamed to half\/half.\\nGeneric tshirt system was chosen over the other options.\\n- Coding a direct \"arbitrary value\" into a CSS class (color, number representing a unit) is generally considered an anti-pattern for reasons that are too long to cover in this ADR.\\n- We like the numbering system but we use t-shirt sizes everywhere else in our system so it makes sense to skew towards alignment with the larger system.\\n","MatchedIds":[1992,381,160,794,4062],"Predicted":"## Decision Drivers\n- Flexibility in introducing new spacing values.\n- Maintainable and scalable naming convention.\n- Decoupling the naming convention from hardcoded default values for easier future adjustments.\n\n## Decision\nChosen option: Switch to a numeric-based spacing scale (e.g., `spacing-1`, `spacing-2`, `spacing-3`, etc.), because it allows for a more flexible and maintainable system.\n\n### Positive Consequences\n- **Flexibility**: Allows for easy addition of new spacing values without breaking the naming convention. For instance, introducing `spacing-3` for a 24px value is straightforward.\n- **Consistency**: The naming convention is decoupled from the actual pixel values, making it easier to change defaults or adjust individual values without a confusing renaming process.\n- **Scalability**: This approach fosters scalability, allowing for a broad range of spacing values to be integrated seamlessly as the design system evolves.\n\n### Negative Consequences\n- **Initial Overhead**: Requires modifying existing code to adhere to the new convention, which might be labor-intensive depending on the extent of current usage.\n- **Transition Period**: There may be a learning curve for developers accustomed to the old naming convention. Proper documentation and training will be necessary to mitigate this.","GenTime":"2024-08-19 23:54:13"}
{"File Name":"Corvus.Extensions.Newtonsoft.Json\/0001-json-framework-independence.md","Context":"## Context\\nWe frequently need to deal with information that will be serialized as JSON, either because it needs to be used in a web API, or because it is serialized in some JSON-based storage system. For many years, Json.NET has been the de facto API for working with JSON in .NET applications. However, now that .NET Core has `System.Text.Json` built in, there are good reasons to want to move to that. This means that libraries that depend on Json.NET become problematic.\\nSo we need ways for our libraries to work with JSON data without forcing the decision of whether to use Json.NET or `System.Text.Json`.\\n","Decision":"We have moved types previously in `Corvus.Extensions.Newtonsoft.Json` into `Corvus.Json.Abstractions`, having modified them to remove any direct dependency on Json.NET.\\nCurrently, only the property bag type has been moved. (And it is now an `IPropertyBag` interface.) A great deal of what's in `Corvus.Extensions.Newtonsoft.Json` is already in the right place, because it deals directly with Json.NET-specific concerns (e.g., custom type converters).\\n","tokens":132,"id":2585,"text":"## Context\\nWe frequently need to deal with information that will be serialized as JSON, either because it needs to be used in a web API, or because it is serialized in some JSON-based storage system. For many years, Json.NET has been the de facto API for working with JSON in .NET applications. However, now that .NET Core has `System.Text.Json` built in, there are good reasons to want to move to that. This means that libraries that depend on Json.NET become problematic.\\nSo we need ways for our libraries to work with JSON data without forcing the decision of whether to use Json.NET or `System.Text.Json`.\\n\n\n##Decision\nWe have moved types previously in `Corvus.Extensions.Newtonsoft.Json` into `Corvus.Json.Abstractions`, having modified them to remove any direct dependency on Json.NET.\\nCurrently, only the property bag type has been moved. (And it is now an `IPropertyBag` interface.) A great deal of what's in `Corvus.Extensions.Newtonsoft.Json` is already in the right place, because it deals directly with Json.NET-specific concerns (e.g., custom type converters).\\n","MatchedIds":[2625,3746,399,1660,4747],"Predicted":"## Decision\nWe will design our libraries to be agnostic of the underlying JSON serialization library. This can be achieved by:\n\n1. **Abstraction Layer**: Create an abstraction layer with a common interface for JSON serialization and deserialization. This will allow consumers of the library to plug in their preferred JSON library (whether it's Json.NET or `System.Text.Json`).\n\n2. **Dependency Injection**: Use dependency injection to provide the actual JSON serializer implementation. This way, user applications can choose and configure the JSON serializer they prefer at runtime.\n\n3. **Shims and Compatibility Methods**: Provide shims or compatibility methods within the library to bridge specific features or performance advantages offered by either Json.NET or `System.Text.Json`.\n\nBy employing these strategies, we can ensure flexibility and maintain compatibility with both JSON libraries without locking in to a specific choice and thus, making our libraries more versatile and user-friendly.","GenTime":"2024-08-19 23:54:17"}
{"File Name":"paas-team-manual\/ADR044-remove-ipsec.html.md","Context":"## Context\\nGOV.UK PaaS uses mutual TLS via routing-release between gorouter and apps.\\nThis is a form of end-to-end encryption.\\nGOV.UK PaaS uses IPSec between gorouter and diego cells (which run apps).\\nThis is a form of end-to-end encryption.\\nThe [cf-dev mailing list](https:\/\/lists.cloudfoundry.org\/g\/cf-dev\/message\/9143) alleges\\nthat the IPSec release is no longer maintained.\\n","Decision":"We will not run IPSec in our BOSH deployments.\\n","tokens":103,"id":194,"text":"## Context\\nGOV.UK PaaS uses mutual TLS via routing-release between gorouter and apps.\\nThis is a form of end-to-end encryption.\\nGOV.UK PaaS uses IPSec between gorouter and diego cells (which run apps).\\nThis is a form of end-to-end encryption.\\nThe [cf-dev mailing list](https:\/\/lists.cloudfoundry.org\/g\/cf-dev\/message\/9143) alleges\\nthat the IPSec release is no longer maintained.\\n\n\n##Decision\nWe will not run IPSec in our BOSH deployments.\\n","MatchedIds":[216,3909,3793,50,2249],"Predicted":"## Decision\nWe have decided to phase out the usage of the current IPSec release for encryption between gorouter and diego cells, due to the lack of ongoing maintenance, which poses a potential security risk. \n\n### Steps to Mitigate:\n* **Short-term:**\n  * Conduct an immediate security assessment to identify any vulnerabilities with the current IPSec implementation.\n  * Continue to monitor security advisories closely.\n\n* **Long-term:**\n  * Transition to using mutual TLS for encryption between gorouter and diego cells.\n  * Explore alternative, actively maintained encryption solutions such as Envoy or other service meshes that provide secured traffic control and encryption capabilities.\n  * Implement the new encryption solution in a staged manner, ensuring thorough testing and validation at each stage.\n\n### Status\nAccepted\n\n### Consequences\n* **Short-term:**\n  * Increased monitoring overhead.\n  * Risk of potential vulnerabilities until the alternative solution is fully implemented.\n\n* **Long-term:**\n  * Enhanced security posture by using a well-maintained and actively supported encryption mechanism.\n  * Reduced risk of data exfiltration due to the establishment of a robust end-to-end encryption framework.\n  * Possible performance impacts during the transition phase; however, these can be minimized through careful planning and phased rollouts.","GenTime":"2024-08-19 23:54:22"}
{"File Name":"thing\/0002-authentication-via-touchstone-saml.md","Context":"## Context\\nA mechanism for providing end-user authentication of MIT users is required for\\ncore aspects of this application.\\nMIT officially only supports Shibboleth \/ Touchstone which traditionally is enabled via an Apache httpd module.\\nThe [MIT OpenID Pilot](https:\/\/mitlibraries.github.io\/oauth.html) was determined\\nunacceptable for this project.\\nIn order to use Shibboleth we intended to containerize the application. However,\\nwhile that process ended in a successful proof-of-concept, it used SAML and not\\nmod_shib. Once we went down the path of not using the officially MIT supported\\nmod_shib solution, we felt using SAML in-app should be explored.\\n","Decision":"We will use an in-app Touchstone SP using ruby-saml as documented in our\\n[Developer Documentation](https:\/\/mitlibraries.github.io\/touchstone_saml.html).\\n","tokens":144,"id":2582,"text":"## Context\\nA mechanism for providing end-user authentication of MIT users is required for\\ncore aspects of this application.\\nMIT officially only supports Shibboleth \/ Touchstone which traditionally is enabled via an Apache httpd module.\\nThe [MIT OpenID Pilot](https:\/\/mitlibraries.github.io\/oauth.html) was determined\\nunacceptable for this project.\\nIn order to use Shibboleth we intended to containerize the application. However,\\nwhile that process ended in a successful proof-of-concept, it used SAML and not\\nmod_shib. Once we went down the path of not using the officially MIT supported\\nmod_shib solution, we felt using SAML in-app should be explored.\\n\n\n##Decision\nWe will use an in-app Touchstone SP using ruby-saml as documented in our\\n[Developer Documentation](https:\/\/mitlibraries.github.io\/touchstone_saml.html).\\n","MatchedIds":[2578,3214,2041,5211,4857],"Predicted":"## Decision\nProceed with the integration of Shibboleth using in-app SAML for end-user authentication. \n\n### Rationale\n1. **Alignment with MIT Standards:** Shibboleth \/ Touchstone is the officially supported mechanism by MIT, ensuring compliance with institutional policies.\n2. **Proven Feasibility:** The successful proof-of-concept indicates that using SAML in-app is a viable solution.\n3. **Security and User Management:** SAML provides robust security protocols for user authentication and supports the necessary access management features.\n\n### Implementation Steps\n1. **Containerization:** Continue with the containerization of the application to facilitate the integration process.\n2. **SAML Configuration:** Implement SAML in the application, ensuring proper configuration and compatibility with MIT's Shibboleth infrastructure.\n3. **Testing and Validation:** Conduct thorough testing to validate the integration, ensuring that all authentication and security requirements are met.\n4. **Monitoring and Maintenance:** Establish a monitoring and maintenance plan to handle any potential issues and keep the authentication mechanism up-to-date.\n\n### Positive Consequences\n* **Compliance:** Adhering to MIT's official authentication mechanism ensures compliance with institutional IT policies.\n* **Security:** Utilizing SAML for in-app authentication provides a secure method for handling user credentials.\n* **User Experience:** Seamless authentication experience for MIT users as it aligns with the commonly used institutional SSO mechanism.\n\n### Negative Consequences\n* **Complexity:** Adding SAML in-app might introduce additional complexity to the application's codebase and configuration.\n* **Maintenance:** Continuous maintenance and monitoring are required to keep the authentication mechanism secure and up-to-date.\n\nBy proceeding with the in-app SAML approach for Shibboleth, we ensure compliance with MIT standards while leveraging a proven, secure authentication mechanism.","GenTime":"2024-08-19 23:54:28"}
{"File Name":"experimenter\/0007-doc-hub-url.md","Context":"## Context and Problem Statement\\nWe'd like to have a permanent URL for the experimenter docs hub that is:\\n- Memorable\\n- Stable\\n- Ideally, part of the rest of our data information architecture\\n","Decision":"We will use a custom domain for now (experimenter.info) until a decision is made on the general organization of data.mozilla.org in the future, at which time we will update the URL to fit in with the rest of our data documentation.\\n","tokens":45,"id":1823,"text":"## Context and Problem Statement\\nWe'd like to have a permanent URL for the experimenter docs hub that is:\\n- Memorable\\n- Stable\\n- Ideally, part of the rest of our data information architecture\\n\n\n##Decision\nWe will use a custom domain for now (experimenter.info) until a decision is made on the general organization of data.mozilla.org in the future, at which time we will update the URL to fit in with the rest of our data documentation.\\n","MatchedIds":[2067,1522,160,4056,3070],"Predicted":"## Decision\n\nChosen option: Create a subdomain under the main organizational domain \n\n### Reasoning:\n* **Memorable**: A subdomain under the main organizational domain is easier to remember. For example, `docs.example.com` or `experimenter.example.com`.\n* **Stable**: Subdomains can remain unchanged even if the internal URL schema changes. This ensures long-term stability.\n* **Integration**: By using a subdomain of the main organization, it is naturally integrated within the broader data information architecture.\n\n### Implementation:\n1. **Choose a Subdomain**: Decide on the most appropriate and intuitive subdomain, such as `experimenter-docs.example.com` or `docs.example.com`.\n2. **DNS Configuration**: Update the DNS records to point the chosen subdomain to the server hosting the experimenter docs hub.\n3. **Redirects and Links**: Ensure that all internal and external links use the new subdomain, and set up redirects from any old URLs to the new subdomain to maintain existing traffic and SEO rankings.\n4. **Communication**: Inform all stakeholders of the new permanent URL and update all documentation templates and marketing materials to reflect the new address.\n\n### Positive Consequences:\n* **Consistent Branding**: Aligns with the main organizational domain, which is easier for users to associate with the broader organization.\n* **Ease of Maintenance**: Any changes required in the backend can be managed without altering the public-facing URL.\n* **Professional Appearance**: Utilizing the main domain projects a more professional and cohesive image.\n\n### Negative Consequences:\n* **Initial Effort**: There is initial effort required in updating DNS configurations, ensuring all links are updated, and possibly handling any transitional issues.\n* **Scope Creep**: Ensuring that the chosen subdomain remains focused on the experimenter docs and does not broaden to other areas, which could dilute its intended purpose.\n\nBy adopting this approach, we aim for a strategic balance between usability, stability, and integration with the broader data architecture.","GenTime":"2024-08-19 23:54:40"}
{"File Name":"Wikibase\/0000-use-cache-instead-wb_terms-for-data-needed-for-display.md","Context":"## Context\\nWhen an item or a property is displayed in the short form, either as a link, or as a simple text reference, data needed to render this short form are currently loaded from the SQL table (wb_terms). wb_terms is causing several significant issues, and is not possible to be maintained any more in the long run.\\nDecision to use wb_terms, initially introduced as a SQL search index, has been tracked down to [change 176650](https:\/\/gerrit.wikimedia.org\/r\/#\/c\/mediawiki\/extensions\/Wikibase\/+\/176650\/). As discussed there in the code review, and also in https:\/\/phabricator.wikimedia.org\/T74309#798908, it seems there had been no dramatic performance improvements expected, neither noticed after switching to use wb_terms instead of loading the data of the entire item or property.\\nWikibase already uses MediaWiki's caching mechanisms (in production Wikidata environment being based on memcached) to reduce loading of full entity data.\\nIn case of lexemes or forms, entity types provided by WikibaseLexeme extension, that have different internal structure than items and properties, wb_terms has not been used as a source of data for short form display. Full lexeme data has been loaded instead. Early tests didn't show significant performance issues (see https:\/\/phabricator.wikimedia.org\/T188108). Also, due to different internal structure of lexemes, or forms, and the way how their \"short form\" displayed is built, the possible use of wb_terms has not even seem feasible without changing the semantics of the table.\\n","Decision":"Wikibase already uses MediaWiki's caching mechanisms (in production Wikidata environment being based on memcached) to reduce loading of full entity data.\\nIn case of lexemes or forms, entity types provided by WikibaseLexeme extension, that have different internal structure than items and properties, wb_terms has not been used as a source of data for short form display. Full lexeme data has been loaded instead. Early tests didn't show significant performance issues (see https:\/\/phabricator.wikimedia.org\/T188108). Also, due to different internal structure of lexemes, or forms, and the way how their \"short form\" displayed is built, the possible use of wb_terms has not even seem feasible without changing the semantics of the table.\\nAs long as using SQL table as a storage of the data used for displaying entities in the short form does not bring significant performance gains, we decide to stop using wb_terms as a data source for this use case.\\nInstead, data of the whole entity is going to be retrieved from storage layer (from the database, or from cached storage that are already in place).\\nIf not efficient enough (e.g. in case of huge-size Wikibase instances like Wikidata ), data needed for display will also be stored in cache, e.g. label of an item in a particular language. That should reduce the amount of computation needed, especially when language fallback needs to be applied, etc.\\n","tokens":330,"id":1341,"text":"## Context\\nWhen an item or a property is displayed in the short form, either as a link, or as a simple text reference, data needed to render this short form are currently loaded from the SQL table (wb_terms). wb_terms is causing several significant issues, and is not possible to be maintained any more in the long run.\\nDecision to use wb_terms, initially introduced as a SQL search index, has been tracked down to [change 176650](https:\/\/gerrit.wikimedia.org\/r\/#\/c\/mediawiki\/extensions\/Wikibase\/+\/176650\/). As discussed there in the code review, and also in https:\/\/phabricator.wikimedia.org\/T74309#798908, it seems there had been no dramatic performance improvements expected, neither noticed after switching to use wb_terms instead of loading the data of the entire item or property.\\nWikibase already uses MediaWiki's caching mechanisms (in production Wikidata environment being based on memcached) to reduce loading of full entity data.\\nIn case of lexemes or forms, entity types provided by WikibaseLexeme extension, that have different internal structure than items and properties, wb_terms has not been used as a source of data for short form display. Full lexeme data has been loaded instead. Early tests didn't show significant performance issues (see https:\/\/phabricator.wikimedia.org\/T188108). Also, due to different internal structure of lexemes, or forms, and the way how their \"short form\" displayed is built, the possible use of wb_terms has not even seem feasible without changing the semantics of the table.\\n\n\n##Decision\nWikibase already uses MediaWiki's caching mechanisms (in production Wikidata environment being based on memcached) to reduce loading of full entity data.\\nIn case of lexemes or forms, entity types provided by WikibaseLexeme extension, that have different internal structure than items and properties, wb_terms has not been used as a source of data for short form display. Full lexeme data has been loaded instead. Early tests didn't show significant performance issues (see https:\/\/phabricator.wikimedia.org\/T188108). Also, due to different internal structure of lexemes, or forms, and the way how their \"short form\" displayed is built, the possible use of wb_terms has not even seem feasible without changing the semantics of the table.\\nAs long as using SQL table as a storage of the data used for displaying entities in the short form does not bring significant performance gains, we decide to stop using wb_terms as a data source for this use case.\\nInstead, data of the whole entity is going to be retrieved from storage layer (from the database, or from cached storage that are already in place).\\nIf not efficient enough (e.g. in case of huge-size Wikibase instances like Wikidata ), data needed for display will also be stored in cache, e.g. label of an item in a particular language. That should reduce the amount of computation needed, especially when language fallback needs to be applied, etc.\\n","MatchedIds":[3351,1510,1839,1346,2513],"Predicted":"## Decision\nDeprecate the use of the `wb_terms` table for loading data needed to render short forms of items or properties. Instead, transition to loading full entity data using MediaWiki's existing caching mechanisms, such as memcached, to mitigate potential performance impacts.\n\n### Justification\n1. **Maintenance**: The `wb_terms` table is not maintainable in the long term and significant issues have been identified with its continued usage.\n2. **Performance**: No dramatic performance improvements were observed or expected when initially switching to `wb_terms`. Loading full entity data with the existing caching mechanisms is sufficiently performant, as demonstrated by the lexemes and forms provided by the WikibaseLexeme extension.\n3. **Consistency**: Full entity data loading has already been effectively implemented for lexemes and forms without significant performance drawbacks. Aligning the process for items and properties will provide a more consistent and unified approach.\n4. **Flexibility**: Removing dependency on the `wb_terms` table allows simplified data management and reduces the need to adapt this table's semantics for the different structures of various entity types.\n\n### Steps to Implement\n1. **Update Code**: Modify the code to bypass the `wb_terms` table and instead load the needed data directly from the full entity records.\n2. **Cache Integration**: Ensure robust integration with MediaWiki\u2019s caching mechanisms (e.g., memcached) to manage and reduce query load.\n3. **Testing**: Conduct extensive performance testing to monitor impacts and adjust caching strategies as needed.\n4. **Deprecation and Cleanup**: Gradually phase out and eventually remove the `wb_terms` table from the codebase to avoid legacy maintenance overhead.\n\n### Positive Consequences\n* Simplified maintenance and reduced complexity.\n* Improved consistency in data loading mechanisms across different entity types.\n* Leveraging existing caching mechanisms to ensure performance remains acceptable.\n\n### Negative Consequences\n* Potential initial performance overhead during transition period until caching strategies are fully optimized.\n* Requires thorough testing and validation to ensure no degradation in user experience.\n\nBy proceeding with these changes, we aim to achieve a scalable and maintainable solution that aligns with current data management practices and leverages the infrastructure's caching capabilities effectively.","GenTime":"2024-08-19 23:54:49"}
{"File Name":"tamr-client\/0006-type-checking.md","Context":"## Context\\nStatic type-checking is available for Python, making us of the type annotations already in the codebase.\\n","Decision":"Type-check via [mypy](http:\/\/mypy-lang.org\/).\\n","tokens":25,"id":536,"text":"## Context\\nStatic type-checking is available for Python, making us of the type annotations already in the codebase.\\n\n\n##Decision\nType-check via [mypy](http:\/\/mypy-lang.org\/).\\n","MatchedIds":[3853,5070,400,4179,1258],"Predicted":"## Decision\nUse MyPy for static type checking to leverage the type annotations already present in the codebase.","GenTime":"2024-08-19 23:54:51"}
{"File Name":"FindMeFoodTrucks\/Architecture Style.md","Context":"## :dart: Context\\nFollowing Architecture styles were considered for the application\\n1. n-tier\\n1. Web-Queue-Worker\\n1. Microservices\\n1. Big data\/compute\\nChoosing the right Architecture style will impact the functional and non-functional efficiencies of the project.\\n","Decision":"A simple Microservices based architecture style was implementing CQS pattern will be used for this application. More about CQS pattern can be found here :https:\/\/martinfowler.com\/bliki\/CommandQuerySeparation.html\\nThe decision is based on the guidance provided by Microsoft here: https:\/\/docs.microsoft.com\/en-us\/azure\/architecture\/guide\/architecture-styles\/\\n","tokens":58,"id":1094,"text":"## :dart: Context\\nFollowing Architecture styles were considered for the application\\n1. n-tier\\n1. Web-Queue-Worker\\n1. Microservices\\n1. Big data\/compute\\nChoosing the right Architecture style will impact the functional and non-functional efficiencies of the project.\\n\n\n##Decision\nA simple Microservices based architecture style was implementing CQS pattern will be used for this application. More about CQS pattern can be found here :https:\/\/martinfowler.com\/bliki\/CommandQuerySeparation.html\\nThe decision is based on the guidance provided by Microsoft here: https:\/\/docs.microsoft.com\/en-us\/azure\/architecture\/guide\/architecture-styles\/\\n","MatchedIds":[1089,1095,302,429,428],"Predicted":"## Decision\nThe recommended architecture style is Microservices considering the following points:\n* Enables independent development and deployment of services, which enhances agility.\n* Provides better scalability by allowing each service to scale independently.\n* Improves fault isolation, so failures in one service do not impact the entire application.\n* Facilitates the use of diverse technology stacks, enabling the best tool for each service.\n* Aligns well with DevOps practices and continuous delivery pipelines.\n* Supports complex, business-specific requirements that can be encapsulated within respective services.\n","GenTime":"2024-08-19 23:54:55"}
{"File Name":"iampeterbanjo.com\/0011-artist-images-from-spotify.md","Context":"## Context\\nKorin would look much better with artist images. Unfortunately LastFm [removed artist images][lastfm-images-gone] from their API and have replaced it with a placeholder. There are some options to get artist images which are:\\n- [Music story][music-story-api]: The documentation is not easy to understand and the SDK is designed for web browsers. It also uses a custom ID to associate artists, not IMDB. One unique feature is that it has lyrics too.\\n- [Music brainz][music-brainz-api]: Creative commons but has everything except artist images.\\n- [Spotify][spotify-api]: Has artist information including images but uses a custom ID, not IMDB.\\n","Decision":"Spotify is the only viable alternative.\\n","tokens":148,"id":1598,"text":"## Context\\nKorin would look much better with artist images. Unfortunately LastFm [removed artist images][lastfm-images-gone] from their API and have replaced it with a placeholder. There are some options to get artist images which are:\\n- [Music story][music-story-api]: The documentation is not easy to understand and the SDK is designed for web browsers. It also uses a custom ID to associate artists, not IMDB. One unique feature is that it has lyrics too.\\n- [Music brainz][music-brainz-api]: Creative commons but has everything except artist images.\\n- [Spotify][spotify-api]: Has artist information including images but uses a custom ID, not IMDB.\\n\n\n##Decision\nSpotify is the only viable alternative.\\n","MatchedIds":[1096,2515,61,1172,4276],"Predicted":"## Decision\nWe decide to use the Spotify API for obtaining artist images. The reasons for this decision are as follows:\n\n1. **Availability of Artist Images**: Spotify provides the artist images that are essential for enhancing Korin's visual appeal.\n2. **Comprehensive Artist Information**: In addition to images, Spotify provides a wide range of artist information, which could be useful for future features.\n3. **Reputation and Reliability**: Spotify is a well-known and widely used service, ensuring that the data provided is reliable and up-to-date.\n4. **API Documentation and Support**: Spotify has well-documented APIs with robust support, making integration smoother and easier for the development team.\n\nWhile Spotify uses a custom ID instead of IMDB, the benefits of having readily available artist images outweigh the inconvenience of ID mapping, which can be managed through additional integration efforts.\n\nThis decision will allow us to quickly enhance Korin's aesthetics with high-quality artist images, ultimately improving user engagement and satisfaction.","GenTime":"2024-08-19 23:54:59"}
{"File Name":"buildit-all\/0002-version-api-via-uri-path.md","Context":"## Context\\nVersioning APIs can be controversial.  Ideally, APIs don't need to change, or be additive so as not to break backwards compatibility.  Occasionally, some changes require bigger changes.\\nWe will do our best to be open w\/ what we accept and strict w\/ what we return\\nThere are lots of ways an API can change:\\n* Breaking changes across the entire API (all endpoints change from REST to GraphQL for example)\\n* Breaking changes at a resource level (\/v1\/booking) needs to change\\n* Breaking changes in behavior\\nThis decision currently applies to the 1st bullet.\\n","Decision":"All API endpoints will start with the version of the API (e.g. \/v1\/resource).  We don't anticipate this to change.\\n","tokens":127,"id":4424,"text":"## Context\\nVersioning APIs can be controversial.  Ideally, APIs don't need to change, or be additive so as not to break backwards compatibility.  Occasionally, some changes require bigger changes.\\nWe will do our best to be open w\/ what we accept and strict w\/ what we return\\nThere are lots of ways an API can change:\\n* Breaking changes across the entire API (all endpoints change from REST to GraphQL for example)\\n* Breaking changes at a resource level (\/v1\/booking) needs to change\\n* Breaking changes in behavior\\nThis decision currently applies to the 1st bullet.\\n\n\n##Decision\nAll API endpoints will start with the version of the API (e.g. \/v1\/resource).  We don't anticipate this to change.\\n","MatchedIds":[1180,200,4112,2951,1048],"Predicted":"## Decision\nWe will create major version increments for any breaking changes across the entire API. This includes significant architectural changes, such as transitioning from REST to GraphQL.\n\n### Guidelines for Major Versioning:\n1. **Major Version Prefix:** Each major version will be denoted with a prefix in the URL, such as `\/v1\/`, `\/v2\/`, etc.\n2. **Deprecation Policy:** When a new major version is released, the previous major version will be supported for a minimum of 1 year. Deprecated versions will be announced clearly in our API documentation.\n3. **Documentation:** Both the current and deprecated versions will have comprehensive documentation available, clearly highlighting the changes and migration paths for developers.\n4. **Client Communication:** All clients will be notified well in advance (minimum 6 months) before discontinuing support for a deprecated version.\n5. **Compatibility Check:** Ensure that endpoints within a major version are backward-compatible with minor changes being strictly additive or non-breaking.\n\nBy following these guidelines, we aim to provide a predictable and structured approach to handling breaking changes across the entire API, thus ensuring clear communication and smoother transitions for our stakeholders.","GenTime":"2024-08-19 23:55:03"}
{"File Name":"admin-react-components\/0003-component-bundling.md","Context":"## Context\\nWhen bundling components for usage in consuming applications, should we require consuming applications to have the same dependencies as this repository? Or should exported components be packaged assuming no dependencies?\\n","Decision":"We'll package components with their own inline styles. No external dependencies needed.\\n","tokens":38,"id":1631,"text":"## Context\\nWhen bundling components for usage in consuming applications, should we require consuming applications to have the same dependencies as this repository? Or should exported components be packaged assuming no dependencies?\\n\n\n##Decision\nWe'll package components with their own inline styles. No external dependencies needed.\\n","MatchedIds":[3031,5259,3827,701,3722],"Predicted":"## Decision\nExported components should be packaged assuming no dependencies. This approach has several advantages:\n\n### Pros:\n- **No Dependency Conflicts:** Consuming applications won't face version conflicts with libraries and dependencies already in their stack.\n- **Flexibility:** Consuming applications can use whichever versions of dependencies they prefer without being constrained by the component library\u2019s dependency versions.\n- **Simplicity:** This provides a cleaner usage experience for developers who do not have to manage additional dependencies just to use a component.\n\n### Mitigation of Cons:\nTo ensure that exported components still function as expected:\n\n- **Peer Dependencies:** Clearly list peer dependencies in the package.json file of the distributed package. This informs the consuming application of necessary libraries without enforcing their versions.\n- **Documentation:** Provide thorough documentation on required dependencies and compatible versions to simplify integration for developers.\n- **Component Isolation:** Ensure that each component is designed to work independently and can run without additional setup, providing hooks or functions to extend component functionality when required.\n\nBy packaging components with no direct dependencies, we increase the utility and adaptability of our component library while avoiding potential integration issues.","GenTime":"2024-08-19 23:55:10"}
{"File Name":"event-routing-backends\/0005-PII-leakage-prevention.rst","Context":"Context\\n-------\\n``Event-routing-backends`` transforms and emits edx events that may contain PII which is not meant to be shared with learning record consumers. New xAPI and Caliper transformers are expected to be added in ``Event-routing-backends`` and therefore, a mechanism needs to be put in place to reduce chances of PII leakage via these transformers.\\nDecision\\n--------\\n#. An accessor `method`_ will be developed to get values from open edx events for a specified key. Information inside the open edx event will only be accessed using this accessor method in base transformers and in event transformers.\\n#. If a key, required by the specification (xAPI or Caliper), is not found in open edx event, this method will throw an exception.\\n#. If a key, deemed optional by the specification (xAPI or Caliper), is not found in open edx event, this method will log this instance and return None.\\n#. The efficacy of fuzzy comparison between keys in transformed events and potential PII keys will be evaluated in future and incorporated if found useful.\\nBenefits\\n---------\\n#. Only the instances where this accessor is called, will need to be reviewed during code review, in order to check for potential PII leakage.\\n.. _method: https:\/\/github.com\/openedx\/event-routing-backends\/blob\/f430d4cf58bdab01e42fcc944241898606873d82\/event_routing_backends\/processors\/mixins\/base_transformer.py#L139\\n","Decision":"--------\\n#. An accessor `method`_ will be developed to get values from open edx events for a specified key. Information inside the open edx event will only be accessed using this accessor method in base transformers and in event transformers.\\n#. If a key, required by the specification (xAPI or Caliper), is not found in open edx event, this method will throw an exception.\\n#. If a key, deemed optional by the specification (xAPI or Caliper), is not found in open edx event, this method will log this instance and return None.\\n#. The efficacy of fuzzy comparison between keys in transformed events and potential PII keys will be evaluated in future and incorporated if found useful.\\nBenefits\\n---------\\n#. Only the instances where this accessor is called, will need to be reviewed during code review, in order to check for potential PII leakage.\\n.. _method: https:\/\/github.com\/openedx\/event-routing-backends\/blob\/f430d4cf58bdab01e42fcc944241898606873d82\/event_routing_backends\/processors\/mixins\/base_transformer.py#L139\\n","tokens":307,"id":4477,"text":"Context\\n-------\\n``Event-routing-backends`` transforms and emits edx events that may contain PII which is not meant to be shared with learning record consumers. New xAPI and Caliper transformers are expected to be added in ``Event-routing-backends`` and therefore, a mechanism needs to be put in place to reduce chances of PII leakage via these transformers.\\nDecision\\n--------\\n#. An accessor `method`_ will be developed to get values from open edx events for a specified key. Information inside the open edx event will only be accessed using this accessor method in base transformers and in event transformers.\\n#. If a key, required by the specification (xAPI or Caliper), is not found in open edx event, this method will throw an exception.\\n#. If a key, deemed optional by the specification (xAPI or Caliper), is not found in open edx event, this method will log this instance and return None.\\n#. The efficacy of fuzzy comparison between keys in transformed events and potential PII keys will be evaluated in future and incorporated if found useful.\\nBenefits\\n---------\\n#. Only the instances where this accessor is called, will need to be reviewed during code review, in order to check for potential PII leakage.\\n.. _method: https:\/\/github.com\/openedx\/event-routing-backends\/blob\/f430d4cf58bdab01e42fcc944241898606873d82\/event_routing_backends\/processors\/mixins\/base_transformer.py#L139\\n\n\n##Decision\n--------\\n#. An accessor `method`_ will be developed to get values from open edx events for a specified key. Information inside the open edx event will only be accessed using this accessor method in base transformers and in event transformers.\\n#. If a key, required by the specification (xAPI or Caliper), is not found in open edx event, this method will throw an exception.\\n#. If a key, deemed optional by the specification (xAPI or Caliper), is not found in open edx event, this method will log this instance and return None.\\n#. The efficacy of fuzzy comparison between keys in transformed events and potential PII keys will be evaluated in future and incorporated if found useful.\\nBenefits\\n---------\\n#. Only the instances where this accessor is called, will need to be reviewed during code review, in order to check for potential PII leakage.\\n.. _method: https:\/\/github.com\/openedx\/event-routing-backends\/blob\/f430d4cf58bdab01e42fcc944241898606873d82\/event_routing_backends\/processors\/mixins\/base_transformer.py#L139\\n","MatchedIds":[4483,2316,2256,8,1252],"Predicted":"## Decision\n--------  \n#. An accessor method will be developed to fetch values from open edx events using a specified key. Information within the open edx event must only be accessed through this method in both base transformers and event transformers.  \n#. If a key required by the xAPI or Caliper specification is not found in the open edx event, the method will raise an exception.  \n#. If a key deemed optional by the xAPI or Caliper specification is not found in the open edx event, the method will log this occurrence and return None.  \n#. The efficacy of performing fuzzy comparison between keys in transformed events and potential PII keys will be evaluated in the future and incorporated if deemed useful.  \n\nBenefits  \n---------  \n#. By enforcing the use of this accessor method, we can focus code reviews on instances where the accessor is called to check for potential PII leakage.\n\nThis approach will help minimize the risk of inadvertently exposing PII through specific transformers and streamline the review process.","GenTime":"2024-08-19 23:55:14"}
{"File Name":"TruBudget\/0004-ressource-level-lifetime.md","Context":"## Context\\nResources have semantically a certain lifetime. That means, after a resources is created it will experience state-transition until it reaches the end of its lifetime (e.g. archival). The lifetimes and state-transitions need to be defined in an abstract way, in order to allow the platform to excel for different use-cases.\\n","Decision":"### Lifetime of a single resource\\n```mermaid\\ngraph TD\\nA[Create resource] -->|Auto-assign to creator| B\\nB[State: Open] --> |Do work|B\\nB --> |Assign to somebody who can close| C\\nC[State: Open & Ready to close] --> |Close| D\\nD[Resource closed]\\n```\\nRe-Opening is not supported; it could be discussed as long as the parent is still open (hierarchy must hold). Different usage-scenarios are acommodated via _manual_ steps.\\n#### Example: 4-Eyes Principle\\nScenario: A subproject must only be closed when 2 people A & B (e.g. different departments) checked it.\\nPrereqs: Closing right is given to an administrative person and not to the people signing off\\nApproach:\\n1. Subproject is executed as usual, then for sign-off is assigned to Person A\\n2. Sign-Off Person A enters a Workflow-Item validating the state and assigns Person B\\n3. Sign-Off Person B enters a Workflow-Item validating the state and assigns to administrative personell that has the close-rights\\n4. Administrative person checks whether all required sign-offs are there and then finally closes; otherwise assigns back\\n### Impact to child resources\\nA resource hierarchically describes a node in a tree-like structure. Therefore we need to look at impact of a closed resource to other nodes in the tree.\\nIf a resource has no children, there is no impact on other resources.\\nIf a resource has children (e.g. subprojects \/ workflowitems), **the resource cannot be closed**. All children need to be in state closed before a parent can be closed.\\n```mermaid\\ngraph TD;\\nProject-->Subproject_A;\\nProject-->Subproject_B;\\nSubproject_A-->Workflowitem_A;\\nSubproject_A-->Workflowitem_B;\\nSubproject_B-->Workflowitem_C;\\nSubproject_B-->Workflowitem_D;\\n```\\n","tokens":69,"id":1216,"text":"## Context\\nResources have semantically a certain lifetime. That means, after a resources is created it will experience state-transition until it reaches the end of its lifetime (e.g. archival). The lifetimes and state-transitions need to be defined in an abstract way, in order to allow the platform to excel for different use-cases.\\n\n\n##Decision\n### Lifetime of a single resource\\n```mermaid\\ngraph TD\\nA[Create resource] -->|Auto-assign to creator| B\\nB[State: Open] --> |Do work|B\\nB --> |Assign to somebody who can close| C\\nC[State: Open & Ready to close] --> |Close| D\\nD[Resource closed]\\n```\\nRe-Opening is not supported; it could be discussed as long as the parent is still open (hierarchy must hold). Different usage-scenarios are acommodated via _manual_ steps.\\n#### Example: 4-Eyes Principle\\nScenario: A subproject must only be closed when 2 people A & B (e.g. different departments) checked it.\\nPrereqs: Closing right is given to an administrative person and not to the people signing off\\nApproach:\\n1. Subproject is executed as usual, then for sign-off is assigned to Person A\\n2. Sign-Off Person A enters a Workflow-Item validating the state and assigns Person B\\n3. Sign-Off Person B enters a Workflow-Item validating the state and assigns to administrative personell that has the close-rights\\n4. Administrative person checks whether all required sign-offs are there and then finally closes; otherwise assigns back\\n### Impact to child resources\\nA resource hierarchically describes a node in a tree-like structure. Therefore we need to look at impact of a closed resource to other nodes in the tree.\\nIf a resource has no children, there is no impact on other resources.\\nIf a resource has children (e.g. subprojects \/ workflowitems), **the resource cannot be closed**. All children need to be in state closed before a parent can be closed.\\n```mermaid\\ngraph TD;\\nProject-->Subproject_A;\\nProject-->Subproject_B;\\nSubproject_A-->Workflowitem_A;\\nSubproject_A-->Workflowitem_B;\\nSubproject_B-->Workflowitem_C;\\nSubproject_B-->Workflowitem_D;\\n```\\n","MatchedIds":[5009,2181,672,2502,1180],"Predicted":"## Decision\nWe will use a state machine framework, such as `state_machine` library for Python or `Spring State Machine` for Java, to model and manage the lifecycle and state transitions of resources. \n\n### State Machine Frameworks\nState machines are chosen due to their ability to explicitly define states, transitions, and actions associated with each transition. They provide a clear and maintainable way to handle the lifecycle of resources.\n\n### Benefits of State Machine\n1. **Clarity and Readability**: Allows defining states and transitions in a clear, declarative manner.\n2. **Maintainability**: State machines are easier to maintain as they separate state management logic from business logic.\n3. **Flexibility**: Can accommodate different use-cases through configurable states and transitions.\n4. **Error Handling**: Can gracefully handle invalid transitions and provide meaningful error messages.\n\n### Implementation Steps\n1. **Define States**: Identify the possible states for each resource type (e.g., Created, Approved, Active, Archived).\n2. **Define Transitions**: Specify the valid transitions between states (e.g., from Created to Approved, from Approved to Active).\n3. **Integrate State Machine**: Implement the state machine using the chosen framework and integrate it with the resource management logic.\n4. **Validation**: Implement validation to ensure that resources only transition through defined states and transitions.\n5. **Monitoring and Logging**: Implement monitoring and logging to track state changes and diagnose issues.\n\n### Example (Using Python `state_machine` library)\n```python\nfrom state_machine import State, StateMachine\n\nclass ResourceLifecycle(StateMachine):\n    created = State('Created', initial=True)\n    approved = State('Approved')\n    active = State('Active')\n    archived = State('Archived')\n\n    approve = created.to(approved)\n    activate = approved.to(active)\n    archive = active.to(archived)\n\nresource = ResourceLifecycle()\nresource.approve()  # Transition from Created to Approved\nresource.activate() # Transition from Approved to Active\nresource.archive()  # Transition from Active to Archived\n```\n\n### Future Considerations\n1. **Custom Actions**: Implement custom actions that should be executed upon transition.\n2. **Concurrency**: Ensure the state machine handles concurrent transitions gracefully.\n3. **Exception Handling**: Define strategies for handling exceptions during state transitions.\n4. **Extensibility**: Allow the framework to be extended for new resource types with different lifecycles.\n\nBy using a state machine framework, we can clearly define and manage the lifecycle of resources, ensuring that the platform remains flexible and maintainable for different use-cases.","GenTime":"2024-08-19 23:55:26"}
{"File Name":"we-be-best\/1-use-both-pointer-and-mouse-events.md","Context":"## Context\\n- Originally we wanted to use just PointerEvents API for capturing events because the API covers mouse and pointer inputs.\\n- Unfortunately Safari and iOS have very limited support. Safari does currently have an experimental API for it but the movementX and movementY properties are always 0.\\n- MovementX\/Y is a clean browser only method for determining distance without having to track previous coordinates.\\n- Mobile is a secondary concern for this app, but pointer events on desktop Safari is also unsupported :(\\n","Decision":"All pointer event interactions have a mouse event fallback.\\n","tokens":102,"id":873,"text":"## Context\\n- Originally we wanted to use just PointerEvents API for capturing events because the API covers mouse and pointer inputs.\\n- Unfortunately Safari and iOS have very limited support. Safari does currently have an experimental API for it but the movementX and movementY properties are always 0.\\n- MovementX\/Y is a clean browser only method for determining distance without having to track previous coordinates.\\n- Mobile is a secondary concern for this app, but pointer events on desktop Safari is also unsupported :(\\n\n\n##Decision\nAll pointer event interactions have a mouse event fallback.\\n","MatchedIds":[669,3156,3806,3999,847],"Predicted":"## Decision\nGiven the limited support for the PointerEvents API in Safari (both desktop and iOS), we will adopt a hybrid approach using both PointerEvents where supported and MouseEvents\/TouchEvents as a fallback for capturing events and calculating movement distance.\n\n### Implementation Steps:\n1. **Detect API Support:**\n   - Implement feature detection to determine if the PointerEvents API is fully supported (including movementX and movementY properties).\n   - If PointerEvents are not supported, fall back to using MouseEvents and TouchEvents.\n\n2. **PointerEvents Implementation:**\n   - Use the PointerEvents API to capture events and calculate movement distance using movementX and movementY properties on browsers that fully support it.\n   \n3. **Fallback Mechanism:**\n   - For browsers that do not support the necessary PointerEvents properties (particularly Safari on desktop and iOS), fall back to MouseEvents and TouchEvents.\n   - Manually track the previous coordinates and calculate the difference to determine movement distance:\n     ```javascript\n     let previousX = 0, previousY = 0;\n     function handleMove(event) {\n       const { clientX, clientY } = event;\n       const movementX = clientX - previousX;\n       const movementY = clientY - previousY;\n       previousX = clientX;\n       previousY = clientY;\n       \/\/ Use movementX and movementY\n     }\n     ```\n\n4. **Event Listener Setup:**\n   - Add event listeners for PointerEvents (pointerdown, pointermove, pointerup) and the fallback events (mousedown, mousemove, mouseup, touchstart, touchmove, touchend).\n   - Ensure that the appropriate event handling logic is executed based on the detected API support.\n\n### Benefits:\n- Enhanced cross-browser compatibility, ensuring a consistent experience across all major browsers.\n- The solution leverages the strengths of the PointerEvents API where available, while gracefully degrading on unsupported browsers.\n- Maintaining the ability to accurately track movement distance without compromising on the functionality.\n\nThis approach will mitigate the limitations of Safari while still aiming to provide a seamless experience for users on other modern browsers.","GenTime":"2024-08-19 23:55:33"}
{"File Name":"Horace\/0012-use-network-storage-for-large-datafiles.md","Context":"## Context\\nHorace and Herbert will require access to large `sqw` and sets of `nxspe` data files as source data and \"expected\" results for unit and system testing.\\nThese data files are too large to store in GitHub along side the test code, but will not change frequently.\\nSimilar data files are also required for Euphonic testing.\\n","Decision":"The data will be stored in STFC hosted SAN (storage area network).\\nTests will read the data from this network storage location, either by copying the files locally or reading the remote file.\\n","tokens":78,"id":4234,"text":"## Context\\nHorace and Herbert will require access to large `sqw` and sets of `nxspe` data files as source data and \"expected\" results for unit and system testing.\\nThese data files are too large to store in GitHub along side the test code, but will not change frequently.\\nSimilar data files are also required for Euphonic testing.\\n\n\n##Decision\nThe data will be stored in STFC hosted SAN (storage area network).\\nTests will read the data from this network storage location, either by copying the files locally or reading the remote file.\\n","MatchedIds":[4245,4868,3253,1258,4179],"Predicted":"## Decision\nStore the large sqw and nxspe data files in an external, scalable storage solution such as AWS S3 or Google Cloud Storage. This approach ensures that the data files are accessible yet do not burden the version control system like GitHub.\n\nAs a consequence:\n- Horace, Herbert, and Euphonic tests will fetch the necessary data files from the external storage at runtime.\n- The CI pipelines for Horace, Herbert, and Euphonic will include steps to download the required data files from the external storage before executing tests.\n- Access permissions and credentials for the storage will be securely managed and provided to the CI systems.\n\nThis approach maintains the test code lightweight and ensures consistent access to the large, infrequently changing datasets.","GenTime":"2024-08-19 23:55:36"}
{"File Name":"kafkarator\/0003-kafkarator-api-is-focused-around-dedicated-crds.md","Context":"## Context\\nWhen application developers wants to interact with Kafkarator, they need an API. We have previously been vague about how that API should look, should it be one CRD, multiple CRDs, piggyback on existing NAIS CRDs etc.\\nWe need to make a decision, so that we can proceed with detailing how the API looks, and what can be expected from it. It is also needed so that we can actually start implementing Kafkarator in earnest.\\nFrom various discussions, we have a few findings that guide our decision:\\n- When doing NAIS deploy, it is possible for developers to supply multiple resources to be applied to the cluster\\n- We have two separate concerns that needs two separate configurations\\n","Decision":"- We will define one new CRD object to configure topics and access to this\\n- App developers will create this in the cluster when deploying their application\\n- Kafkarator will watch this CRD and take needed actions\\n- App developers will add configuration to their Application resource listing kafka pools they need access to\\n","tokens":151,"id":2834,"text":"## Context\\nWhen application developers wants to interact with Kafkarator, they need an API. We have previously been vague about how that API should look, should it be one CRD, multiple CRDs, piggyback on existing NAIS CRDs etc.\\nWe need to make a decision, so that we can proceed with detailing how the API looks, and what can be expected from it. It is also needed so that we can actually start implementing Kafkarator in earnest.\\nFrom various discussions, we have a few findings that guide our decision:\\n- When doing NAIS deploy, it is possible for developers to supply multiple resources to be applied to the cluster\\n- We have two separate concerns that needs two separate configurations\\n\n\n##Decision\n- We will define one new CRD object to configure topics and access to this\\n- App developers will create this in the cluster when deploying their application\\n- Kafkarator will watch this CRD and take needed actions\\n- App developers will add configuration to their Application resource listing kafka pools they need access to\\n","MatchedIds":[4707,631,2837,4644,856],"Predicted":"## Decision Drivers\n- Clear separation of concerns for better maintainability\n- Flexibility for developers to supply multiple resources\n- Ease of implementation and usage\n\n## Decision\nChosen option: Use multiple CRDs (Custom Resource Definitions), each addressing a specific concern.\n\n### Positive Consequences\n- **Separation of Concerns**: Each CRD can be tailored to address a specific concern, making it easier to maintain and evolve each part independently.\n- **Clarity**: Developers will have a clear API to work with for each aspect of Kafkarator, reducing confusion and potential errors.\n- **Flexibility**: Developers can apply multiple resources as needed during NAIS deploy, utilizing the CRDs that are relevant for their specific requirements.\n\n### Negative Consequences\n- **Increased Complexity**: Managing multiple CRDs can introduce some complexity, both in terms of implementation and understanding the overall system.\n- **Documentation Needs**: Proper documentation will be required to ensure developers understand how to use each CRD and what they are intended for.\n\n### Implementation Steps\n1. **Define CRDs**: Create detailed specifications for each CRD, ensuring they are focused on distinct concerns relevant to Kafkarator.\n2. **Documentation**: Develop comprehensive documentation and usage guides for the CRDs to aid developers in integrating with Kafkarator.\n3. **Tooling and Validation**: Create tools and validation scripts to help developers ensure they are using the CRDs correctly and efficiently.\n4. **Community Feedback**: Engage with the developer community to gather feedback and make necessary adjustments to the CRDs.\n\nThis decision will help us provide a clear, maintainable, and flexible API for interacting with Kafkarator, aligning with our findings and addressing the needs of our developers.","GenTime":"2024-08-19 23:55:52"}
{"File Name":"paas-csls-splunk-broker\/ADR001-syslog-http-to-csls-adapter.md","Context":"## Context\\nWe want to provide a reliable method of shipping logs from applications (on\\nGOV.UK Paas) to Splunk so they can take advantage of the log storage, analytics\\nand protective monitoring provided by the GDS Cyber Security team.\\nGDS Cyber Security maintain an [AWS Kinesis][kinesis] based log shipping stream\\nthat accepts log events in the [AWS CloudWatch Logs][cloudwatch] format and\\nqueues them for delivery to Splunk.\\nGOV.UK PaaS supports forwarding log events from an application's stdout and\\nstderr streams in [syslog format][syslog] via [syslog drains][drains].\\nComponents such as [Fluentd][fluentd] are available that process and forwarding\\nlogs from various sources to various targets, but their configuration can\\nunweildly and hard to test.\\nGOV.UK PaaS Tenants can run [sidecar][sidecar] containers to handle custom log\\nshipping or instument their applications with [logging libraries that support\\nmultiple transports][winston], but this brings extra complexity and doesn't\\nmake use of supported logging infrastructure already provided by GOV.UK PaaS.\\n","Decision":"We will build an \"adapter\" application to deploy alongside the CSLS logging\\npipeline that accepts requests in the \"syslog over http\" format exported by the\\nGOV.UK PaaS syslog drain system, translates them into the CloudWatch Logs\\nformat and forwards them on to the CSLS Kinesis stream.\\n","tokens":248,"id":3290,"text":"## Context\\nWe want to provide a reliable method of shipping logs from applications (on\\nGOV.UK Paas) to Splunk so they can take advantage of the log storage, analytics\\nand protective monitoring provided by the GDS Cyber Security team.\\nGDS Cyber Security maintain an [AWS Kinesis][kinesis] based log shipping stream\\nthat accepts log events in the [AWS CloudWatch Logs][cloudwatch] format and\\nqueues them for delivery to Splunk.\\nGOV.UK PaaS supports forwarding log events from an application's stdout and\\nstderr streams in [syslog format][syslog] via [syslog drains][drains].\\nComponents such as [Fluentd][fluentd] are available that process and forwarding\\nlogs from various sources to various targets, but their configuration can\\nunweildly and hard to test.\\nGOV.UK PaaS Tenants can run [sidecar][sidecar] containers to handle custom log\\nshipping or instument their applications with [logging libraries that support\\nmultiple transports][winston], but this brings extra complexity and doesn't\\nmake use of supported logging infrastructure already provided by GOV.UK PaaS.\\n\n\n##Decision\nWe will build an \"adapter\" application to deploy alongside the CSLS logging\\npipeline that accepts requests in the \"syslog over http\" format exported by the\\nGOV.UK PaaS syslog drain system, translates them into the CloudWatch Logs\\nformat and forwards them on to the CSLS Kinesis stream.\\n","MatchedIds":[3914,1924,1376,3295,3469],"Predicted":"## Decision\nWe will leverage the existing GOV.UK PaaS syslog drains to forward application logs to [AWS Kinesis][kinesis] using a syslog-to-Kinesis bridge. By doing so, we capitalize on the supported logging infrastructure provided by GOV.UK PaaS, maintaining simplicity and reliability while avoiding the complexity of custom sidecar containers or intricate logging libraries. This method ensures seamless delivery of logs in the [AWS CloudWatch Logs][cloudwatch] format required by the GDS Cyber Security's log shipping stream for processing into Splunk.","GenTime":"2024-08-19 23:55:58"}
{"File Name":"dapr\/CLI-002-self-hosted-init-and-uninstall-behaviors.md","Context":"## Context\\nChanges in behavior of `init` and `uninstall` on Self Hosted mode for. Discussed in this [issue](https:\/\/github.com\/dapr\/cli\/issues\/411).\\n","Decision":"* Calling `dapr init` will\\n* Install `daprd` binary in `$HOME\/.dapr\/bin` for Linux\/MacOS and `%USERPROFILE%\\.dapr\\bin` for Windows.\\n* Set up the `dapr_placement`, `dapr_redis` and `dapr_zipkin` containers.\\n* Create the default `components` folder in `$HOME\/.dapr\/bin` for Linux\/MacOS or `%USERPROFILE\\.dapr\\bin` for Windows.\\n* Create the default components configurations for `pubsub.yaml`, `statestore.yaml` and `zipkin.yaml` in the default `components` folder.\\n* Create a default configuration file in `$HOME\/.dapr\/config.yaml` for Linx\/MacOS and `%USERPROFILE%\\.dapr\\config.yaml` for Windows for enabling tracing by default.\\n* Calling `dapr init --slim` will\\n* Install the binaries `daprd` and `placement` in `$HOME\/.dapr\/bin` for Linux\/MacOS and `%USERPROFILE%\\.dapr\\bin` for Windows.\\n* Create an empty default `components` folder in `$HOME\/.dapr\/bin` for Linux\/MacOS or `%USERPROFILE\\.dapr\\bin` for Windows.\\n* Calling `dapr uninstall` will\\n* Remove the folder containing binaries `bin` the default path `$HOME\/.dapr` for Linux\/MacOS and `%USERPROFILE%\\.dapr` for Windows.\\n* Remove the docker dapr_placement if Docker is installed.\\n* Calling `dapr uninstall --all`\\n* Remove the folder containing binaries `bin` the default path `$HOME\/.dapr` for Linux\/MacOS and `%USERPROFILE%\\.dapr` for Windows.\\n* Remove the docker containers dapr_placement, dapr_redis and dapr_zipkin if Docker is installed.\\n* Remove the default folder `$HOME\/.dapr` in Linux\/MacOS and `%USERPROFILE%\\.dapr` in Windows.\\n* CLI on the init command will fail if a prior installtion exists in the default path `$HOME\/.dapr` for Linux\/MacOS and `%USERPROFILE%\\.dapr` for Windows.\\n* **There will no longer be an option for `--install-path` during init or during uninstall.**\\n* The `dapr` CLI by default will expect the `daprd` in `$HOME\/.dapr\/bin` for Linux\/MacOS and `%USERPROFILE%\\.dapr\\bin` for Windows. The command `dapr run` will not expect the `daprd` binary to be in the `PATH` variable, it will launch the binary from the default path.\\n","tokens":41,"id":85,"text":"## Context\\nChanges in behavior of `init` and `uninstall` on Self Hosted mode for. Discussed in this [issue](https:\/\/github.com\/dapr\/cli\/issues\/411).\\n\n\n##Decision\n* Calling `dapr init` will\\n* Install `daprd` binary in `$HOME\/.dapr\/bin` for Linux\/MacOS and `%USERPROFILE%\\.dapr\\bin` for Windows.\\n* Set up the `dapr_placement`, `dapr_redis` and `dapr_zipkin` containers.\\n* Create the default `components` folder in `$HOME\/.dapr\/bin` for Linux\/MacOS or `%USERPROFILE\\.dapr\\bin` for Windows.\\n* Create the default components configurations for `pubsub.yaml`, `statestore.yaml` and `zipkin.yaml` in the default `components` folder.\\n* Create a default configuration file in `$HOME\/.dapr\/config.yaml` for Linx\/MacOS and `%USERPROFILE%\\.dapr\\config.yaml` for Windows for enabling tracing by default.\\n* Calling `dapr init --slim` will\\n* Install the binaries `daprd` and `placement` in `$HOME\/.dapr\/bin` for Linux\/MacOS and `%USERPROFILE%\\.dapr\\bin` for Windows.\\n* Create an empty default `components` folder in `$HOME\/.dapr\/bin` for Linux\/MacOS or `%USERPROFILE\\.dapr\\bin` for Windows.\\n* Calling `dapr uninstall` will\\n* Remove the folder containing binaries `bin` the default path `$HOME\/.dapr` for Linux\/MacOS and `%USERPROFILE%\\.dapr` for Windows.\\n* Remove the docker dapr_placement if Docker is installed.\\n* Calling `dapr uninstall --all`\\n* Remove the folder containing binaries `bin` the default path `$HOME\/.dapr` for Linux\/MacOS and `%USERPROFILE%\\.dapr` for Windows.\\n* Remove the docker containers dapr_placement, dapr_redis and dapr_zipkin if Docker is installed.\\n* Remove the default folder `$HOME\/.dapr` in Linux\/MacOS and `%USERPROFILE%\\.dapr` in Windows.\\n* CLI on the init command will fail if a prior installtion exists in the default path `$HOME\/.dapr` for Linux\/MacOS and `%USERPROFILE%\\.dapr` for Windows.\\n* **There will no longer be an option for `--install-path` during init or during uninstall.**\\n* The `dapr` CLI by default will expect the `daprd` in `$HOME\/.dapr\/bin` for Linux\/MacOS and `%USERPROFILE%\\.dapr\\bin` for Windows. The command `dapr run` will not expect the `daprd` binary to be in the `PATH` variable, it will launch the binary from the default path.\\n","MatchedIds":[78,1173,1430,3635,5201],"Predicted":"## Decision\n* `init` command will be modified to ensure that it does not assume any pre-existing context, which aligns with the process of setting up a new environment. It will create required directories and configurations from scratch, ensuring a clean initialization.\n* `uninstall` command will be enhanced to thoroughly clean up the environment it was used to set up. This will include deleting all configuration files, directories, and binaries associated with Dapr to ensure no residual data remains.\n* Both `init` and `uninstall` commands will include appropriate confirmation prompts for the user to prevent accidental data loss during the uninstall process.\n* In addition, the documentation will be updated to clearly outline the changes in behavior for both `init` and `uninstall` commands in Self Hosted mode. This will help users understand the new expected behaviors and any action items required from their side.\n* Tests will be added or updated to cover the new behavior to ensure proper functionality and avoid regressions.\n\n","GenTime":"2024-08-19 23:56:06"}
{"File Name":"operational-data-hub\/0002-use-serverless-infra-components.md","Context":"## Context\\nServerless computing offers a number of advantages over traditional cloud-based or server-centric infrastructure. For many developers, serverless architectures offer greater scalability, more flexibility, and quicker time to release, all at a reduced cost. With serverless architectures, developers do not need to worry about purchasing, provisioning, and managing backend servers.\\n","Decision":"We will use serverless infrastructure components where possible.\\n","tokens":68,"id":2723,"text":"## Context\\nServerless computing offers a number of advantages over traditional cloud-based or server-centric infrastructure. For many developers, serverless architectures offer greater scalability, more flexibility, and quicker time to release, all at a reduced cost. With serverless architectures, developers do not need to worry about purchasing, provisioning, and managing backend servers.\\n\n\n##Decision\nWe will use serverless infrastructure components where possible.\\n","MatchedIds":[2752,4937,302,108,2471],"Predicted":"## Decision\nWe will adopt a serverless architecture to leverage its scalability, flexibility, and cost-efficiency benefits.","GenTime":"2024-08-19 23:56:10"}
{"File Name":"libelektra\/highlevel_help_message.md","Context":"## Problem\\nWe want to allow to print the help message no matter what errors happened in `kdbOpen` or `kdbGet`.\\n","Decision":"Ignore missing `require`d keys (in help mode), but fail for every other error.\\n","tokens":31,"id":1274,"text":"## Problem\\nWe want to allow to print the help message no matter what errors happened in `kdbOpen` or `kdbGet`.\\n\n\n##Decision\nIgnore missing `require`d keys (in help mode), but fail for every other error.\\n","MatchedIds":[1308,1289,1292,3571,1293],"Predicted":"## Decision\nIntroduce a flag or argument to the command-line interface that forces the display of the help message, regardless of any errors encountered in `kdbOpen` or `kdbGet`. \n\nFor example:\n- Adding a `--help` or `-h` flag that takes precedence over any operational errors.\n- Modify the command processing sequence to first check if the help flag is present before any operation, immediately triggering help message display if detected.\n\nThis approach ensures that users can always access the help documentation, even when encountering failures in `kdbOpen` or `kdbGet`. \n\nThe implementation would involve:\n- Checking the presence of the help flag in the arguments at the earliest possible state.\n- Bypassing the execution of `kdbOpen` and `kdbGet` if the help flag is detected.\n- Displaying the help message and then exiting the program cleanly.\n\nDocumentation should reflect this behavior to inform users they can always use the help flag to bypass operational errors.","GenTime":"2024-08-19 23:56:16"}
{"File Name":"govuk-aws\/0012-security-groups-in-terraform.md","Context":"## Context\\nThere are two methods of defining security groups for AWS in Terraform and they are distinguished by how you add rules: in-line and separate. Using in-line rules keeps the definition close to the resource but when ever a rule is changed Terraform will re-generate the entire resource. Using separate rules Terraform will only make the single rule change but there is greater boilerplate and separation between the group resource and the rule resource.\\n","Decision":"Security groups will be defined separate to their rules.\\nAdditionally each security group will be defined, in its entirety, in a single file.\\n","tokens":89,"id":4032,"text":"## Context\\nThere are two methods of defining security groups for AWS in Terraform and they are distinguished by how you add rules: in-line and separate. Using in-line rules keeps the definition close to the resource but when ever a rule is changed Terraform will re-generate the entire resource. Using separate rules Terraform will only make the single rule change but there is greater boilerplate and separation between the group resource and the rule resource.\\n\n\n##Decision\nSecurity groups will be defined separate to their rules.\\nAdditionally each security group will be defined, in its entirety, in a single file.\\n","MatchedIds":[2793,1372,1464,328,3144],"Predicted":"## Decision\nUse separate rules for defining security groups in Terraform. This approach minimizes the impact of changes by allowing Terraform to only update the specific rule that has changed, rather than re-generating the entire security group. To address the increased boilerplate and separation, establish clear documentation and naming conventions to ensure that the relationship between security group resources and rule resources remains understandable and maintainable.","GenTime":"2024-08-19 23:56:19"}
{"File Name":"educational-platform\/0002-integration-events-implementation.md","Context":"## Context\\nFor implementing event-driven application, the platform should be available for communication with integration events.\\n","Decision":"We will start from standard Spring Events classes: ApplicationListener, ApplicationEvent without dependency to external middleware component. We can add custom features to Spring functionality when it's needed.\\ntodo: For now, all events will be stored in integration-events module. But this solution should be reviewed. Integration events should be published after successful transaction.\\n- https:\/\/devblogs.microsoft.com\/cesardelatorre\/domain-events-vs-integration-events-in-domain-driven-design-and-microservices-architectures\/\\n","tokens":21,"id":2499,"text":"## Context\\nFor implementing event-driven application, the platform should be available for communication with integration events.\\n\n\n##Decision\nWe will start from standard Spring Events classes: ApplicationListener, ApplicationEvent without dependency to external middleware component. We can add custom features to Spring functionality when it's needed.\\ntodo: For now, all events will be stored in integration-events module. But this solution should be reviewed. Integration events should be published after successful transaction.\\n- https:\/\/devblogs.microsoft.com\/cesardelatorre\/domain-events-vs-integration-events-in-domain-driven-design-and-microservices-architectures\/\\n","MatchedIds":[885,2500,1545,2502,302],"Predicted":"## Decision\nKafka will be used for communication with integration events due to its robustness, scalability, and wide adoption in the industry. If future needs change, such as the necessity of transactional message handling or complex event processing, we can reevaluate and consider other platforms.","GenTime":"2024-08-19 23:56:35"}
{"File Name":"registraties\/001-use-existing-apis.md","Context":"## Context\\nThe Registraties project will need to contain a page that will show data per address (or verblijfsobject) from existing APIs, like [Basisregistraties adressen en gebouwen (BAG)](https:\/\/api.data.amsterdam.nl\/bag\/), [Basisregistratie kadaster (BRK)](https:\/\/api.data.amsterdam.nl\/brk\/) and [Handelsregister (HR)](https:\/\/api.data.amsterdam.nl\/handelsregister). All required data is readily available.\\n","Decision":"The project's MVP, which will contain a simple search field with auto-suggest functionality, get its data from existing API endpoints and therefore doesn't need its own back-end setup with API. In the future it might, but for the initial phase, the front-end code is capable of retrieving, formatting and showing the data from [api.data.amsterdam.nl](https:\/\/api.data.amsterdam.nl).\\nThe application will have container components that have sagas injected into them. Each saga is responsible for retrieving data from a single endpoint. If the situation calls for it, a saga can retrieve data from more than one endpoint.\\n","tokens":112,"id":1122,"text":"## Context\\nThe Registraties project will need to contain a page that will show data per address (or verblijfsobject) from existing APIs, like [Basisregistraties adressen en gebouwen (BAG)](https:\/\/api.data.amsterdam.nl\/bag\/), [Basisregistratie kadaster (BRK)](https:\/\/api.data.amsterdam.nl\/brk\/) and [Handelsregister (HR)](https:\/\/api.data.amsterdam.nl\/handelsregister). All required data is readily available.\\n\n\n##Decision\nThe project's MVP, which will contain a simple search field with auto-suggest functionality, get its data from existing API endpoints and therefore doesn't need its own back-end setup with API. In the future it might, but for the initial phase, the front-end code is capable of retrieving, formatting and showing the data from [api.data.amsterdam.nl](https:\/\/api.data.amsterdam.nl).\\nThe application will have container components that have sagas injected into them. Each saga is responsible for retrieving data from a single endpoint. If the situation calls for it, a saga can retrieve data from more than one endpoint.\\n","MatchedIds":[2074,3126,4276,3982,1016],"Predicted":"## Decision\nWe will implement a single, unified API client that interacts with the existing APIs for BAG, BRK, and HR. This API client will consolidate data from these sources and aggregate it for each address (or verblijfsobject). The client will ensure that the data is retrieved efficiently and combined in a meaningful way.\n\nA new page will be created within the Registraties project to display this consolidated data. This page will make use of the unified API client to fetch the required information and present it in an intuitive and user-friendly format. The page will include:\n\n1. A search functionality that allows users to look up data based on an address or verblijfsobject.\n2. Detailed views showing information from BAG, BRK, and HR for the selected address or verblijfsobject.\n3. Error handling to manage issues such as missing data or connectivity problems with the APIs.\n\nBy utilizing the existing APIs and creating a comprehensive client to manage data retrieval and aggregation, we ensure that the integration is maintainable, scalable, and efficient. \n\nDocumentation will be provided to support the development and maintenance of this integration, detailing API endpoints, data formats, and error handling strategies.","GenTime":"2024-08-19 23:56:42"}
{"File Name":"bosh-bootloader\/0002-replace-go-bindata-with-packr2.md","Context":"## Context\\nThe issue motivating this decision, and any context that influences or constrains the decision.\\nThe original author of go-bindata delete their account and then the repo was\\nrecreated under a different owner. The dependency has shifted around enough that\\nwe have lost faith in the intention of the maintainers\\n[more details here](https:\/\/twitter.com\/francesc\/status\/961249107020001280?lang=en)\\nAlso, some of the development use cases around go-bindata (like what is bundled into the code\\nduring a test run or final build) made it hard to reason about.\\n","Decision":"Use [Packr2](https:\/\/github.com\/gobuffalo\/packr\/tree\/master\/v2) instead.\\n","tokens":126,"id":3873,"text":"## Context\\nThe issue motivating this decision, and any context that influences or constrains the decision.\\nThe original author of go-bindata delete their account and then the repo was\\nrecreated under a different owner. The dependency has shifted around enough that\\nwe have lost faith in the intention of the maintainers\\n[more details here](https:\/\/twitter.com\/francesc\/status\/961249107020001280?lang=en)\\nAlso, some of the development use cases around go-bindata (like what is bundled into the code\\nduring a test run or final build) made it hard to reason about.\\n\n\n##Decision\nUse [Packr2](https:\/\/github.com\/gobuffalo\/packr\/tree\/master\/v2) instead.\\n","MatchedIds":[3820,1145,4616,3546,4963],"Predicted":"## Decision\nWe have decided to migrate away from using go-bindata due to concerns regarding its maintainability and the trustworthiness of its current maintainers. Instead, we will explore alternative solutions for asset bundling and embedding. Possible options include go:embed, which is supported natively in Go 1.16 and later, providing a more reliable and easy-to-reason-about mechanism for embedding static resources into our Go applications.","GenTime":"2024-08-19 23:56:45"}
{"File Name":"cloud-platform\/005-ECR-monitoring-and-alerting.md","Context":"## Context\\nWe use ECR as the Docker container registry that makes it easy for users to store, manage, and deploy Docker container images.\\nDue to some applications having a constant rate of images being pushed to their ECR repo, we found that the AWS limit of 1000 images was being hit by some teams. To avoid this we had implemented a lifecycle policy of *100 images* per ECR repo. ECR repositories created for use in the Cloud Platform will have this default lifecycle policy applied.\\nAs lifecycle policy will only keep 100 most recent versions of an image and silently delete images, application users raised an [issue][user-issue] on imposing any limit on number of images is potentially dangerous, unless teams have their own clean-up mechanism.\\n","Decision":"After discussing with application teams and consideration of possible options, the decision has been made to remove the lifecycle policy altogether, but adding monitoring and alerting such that we can take action before an ECR runs out of space.\\nAs it is to do with metrics & alerts, since prometheus is our monitoring solution we decided to use prometheus for metrics & alerts.\\n","tokens":155,"id":629,"text":"## Context\\nWe use ECR as the Docker container registry that makes it easy for users to store, manage, and deploy Docker container images.\\nDue to some applications having a constant rate of images being pushed to their ECR repo, we found that the AWS limit of 1000 images was being hit by some teams. To avoid this we had implemented a lifecycle policy of *100 images* per ECR repo. ECR repositories created for use in the Cloud Platform will have this default lifecycle policy applied.\\nAs lifecycle policy will only keep 100 most recent versions of an image and silently delete images, application users raised an [issue][user-issue] on imposing any limit on number of images is potentially dangerous, unless teams have their own clean-up mechanism.\\n\n\n##Decision\nAfter discussing with application teams and consideration of possible options, the decision has been made to remove the lifecycle policy altogether, but adding monitoring and alerting such that we can take action before an ECR runs out of space.\\nAs it is to do with metrics & alerts, since prometheus is our monitoring solution we decided to use prometheus for metrics & alerts.\\n","MatchedIds":[3598,74,3917,923,3604],"Predicted":"## Decision\nWe will revise our lifecycle policy to balance the need for automatic clean-up with the concerns of our users. The new lifecycle policy will retain the most recent 200 images, instead of 100, to provide a larger buffer against accidental deletions.\n\nAdditionally, we will implement the following measures to ensure better image management:\n\n1. **Notification System**: Set up automated notifications to alert repository owners when they are approaching the image limit, giving them ample time to perform manual clean-up if necessary.\n  \n2. **Tagging Policy**: Encourage the use of meaningful and versioned tags to make it easier to identify old and unnecessary images for clean-up.\n\n3. **User-Specified Policies**: Allow teams to customize their lifecycle policies based on their specific needs, within certain safety constraints (e.g., a minimum of 100 images must be kept).\n\n4. **Audit and Review**: Establish a bi-annual audit process to review and adjust lifecycle policies as needed, based on usage patterns and feedback from users.\n\nWith these measures, we aim to reduce the risk of unintended deletions while maintaining manageable storage limits across our ECR repositories.","GenTime":"2024-08-19 23:57:04"}
{"File Name":"community\/dr-013-Allowed_UI_technology_in_the_Console_UI.md","Context":"## Context\\nThe Console UI is the administration view of Kyma and is a core part of the product.\\nTo simplify the development process in Kyma, the number of used programming languages and frameworks is limited. It makes the collaboration easier and allows the developers to specialize in a chosen technology.\\nFollowing this principle, the Go language is used as the only programming language for the back-end functionality of the core Kyma features.\\nBy design, the Console UI can be extended using plugins written in any framework, developed either internally or by customers or partners. The interface between the Console UI and the plugins is based on HTML and JavaScript but the approach is to provide libraries for the most popular and well-adapted UI frameworks, and gather knowledge and experience in using them.\\n","Decision":"The decision is to allow the [React](https:\/\/reactjs.org) and [Angular](https:\/\/angular.io) frameworks in combination with the JavaScript and TypeScript programming languages to build the core views in the Kyma Console UI.\\n","tokens":159,"id":3460,"text":"## Context\\nThe Console UI is the administration view of Kyma and is a core part of the product.\\nTo simplify the development process in Kyma, the number of used programming languages and frameworks is limited. It makes the collaboration easier and allows the developers to specialize in a chosen technology.\\nFollowing this principle, the Go language is used as the only programming language for the back-end functionality of the core Kyma features.\\nBy design, the Console UI can be extended using plugins written in any framework, developed either internally or by customers or partners. The interface between the Console UI and the plugins is based on HTML and JavaScript but the approach is to provide libraries for the most popular and well-adapted UI frameworks, and gather knowledge and experience in using them.\\n\n\n##Decision\nThe decision is to allow the [React](https:\/\/reactjs.org) and [Angular](https:\/\/angular.io) frameworks in combination with the JavaScript and TypeScript programming languages to build the core views in the Kyma Console UI.\\n","MatchedIds":[3718,847,3715,1240,3451],"Predicted":"## Decision\nThe main Console UI will be developed using React to leverage its robust ecosystem and widespread adoption. To ensure extensibility, we will prepare and maintain libraries for interacting with the Console UI using React. Moreover, considering the popularity and ease of use, we will also evaluate and provide limited support for Angular and Vue.js libraries, while encouraging plugin developers to use these frameworks for consistency and ease of maintenance.","GenTime":"2024-08-19 23:57:06"}
{"File Name":"architectural-decision-log\/0005-convention-based-logging-classes.md","Context":"## Context and Problem Statement\\nHamlet Deploy as a product is made up from several \"components\". They are not written in the same coding\/scripting language and so their outputs - particularly when it comes to log behaviour and error messages - are quite different from each other. When an error or debug message is received, how can the end-user and the Hamlet Deploy developer both gain greater insight from the message that is received?\\n## Decision Drivers <!-- optional -->\\n* A solution to this issue must be possible in Bash, Python and Freemarker (Java)\\n* It must be simple to maintain\\n","Decision":"* A solution to this issue must be possible in Bash, Python and Freemarker (Java)\\n* It must be simple to maintain\\nChosen option:  _\"Use Logging Codes Following a Numerical Convention\"_ with _\"Per-Component Code Range\"_\\nIt was deemed easier to implement and there seemed little benefit and greater complexity to implementing an inheritance-based approach.\\nA code range allows each component to implement the definitions in the way that most makes sense to it.\\n","tokens":122,"id":2975,"text":"## Context and Problem Statement\\nHamlet Deploy as a product is made up from several \"components\". They are not written in the same coding\/scripting language and so their outputs - particularly when it comes to log behaviour and error messages - are quite different from each other. When an error or debug message is received, how can the end-user and the Hamlet Deploy developer both gain greater insight from the message that is received?\\n## Decision Drivers <!-- optional -->\\n* A solution to this issue must be possible in Bash, Python and Freemarker (Java)\\n* It must be simple to maintain\\n\n\n##Decision\n* A solution to this issue must be possible in Bash, Python and Freemarker (Java)\\n* It must be simple to maintain\\nChosen option:  _\"Use Logging Codes Following a Numerical Convention\"_ with _\"Per-Component Code Range\"_\\nIt was deemed easier to implement and there seemed little benefit and greater complexity to implementing an inheritance-based approach.\\nA code range allows each component to implement the definitions in the way that most makes sense to it.\\n","MatchedIds":[1065,1472,3388,2387,1523],"Predicted":"## Decision\n\nAdopt a standardized logging and error handling format across all components.\n\n### Implementation\n1. **Common Logging Structure**: Define a JSON-based logging convention that all components must adhere to. Each log entry will include fields such as `timestamp`, `log_level`, `component_name`, `message`, `error_code`, and `stack_trace`.\n2. **Language-Specific Adapters**: Implement adapters in Bash, Python, and Freemarker (Java) to format and output log messages in the defined JSON structure.\n   - **Bash**:\n     ```bash\n     log_message() {\n       local log_level=$1\n       local component_name=$2\n       local message=$3\n       local error_code=${4:-\"\"}\n       local stack_trace=${5:-\"\"}\n       echo \"{\\\"timestamp\\\":\\\"$(date --iso-8601=seconds)\\\",\\\"log_level\\\":\\\"$log_level\\\",\\\"component_name\\\":\\\"$component_name\\\",\\\"message\\\":\\\"$message\\\",\\\"error_code\\\":\\\"$error_code\\\",\\\"stack_trace\\\":\\\"$stack_trace\\\"}\"\n     }\n     ```\n   - **Python**:\n     ```python\n     import json\n     from datetime import datetime\n\n     def log_message(log_level, component_name, message, error_code=\"\", stack_trace=\"\"):\n         log_entry = {\n             \"timestamp\": datetime.now().isoformat(),\n             \"log_level\": log_level,\n             \"component_name\": component_name,\n             \"message\": message,\n             \"error_code\": error_code,\n             \"stack_trace\": stack_trace\n         }\n         print(json.dumps(log_entry))\n     ```\n   - **Freemarker (Java)**:\n     ```java\n     import org.json.JSONObject;\n     import java.time.Instant;\n\n     public class Logger {\n         public static void logMessage(String logLevel, String componentName, String message, String errorCode, String stackTrace) {\n             JSONObject logEntry = new JSONObject();\n             logEntry.put(\"timestamp\", Instant.now().toString());\n             logEntry.put(\"log_level\", logLevel);\n             logEntry.put(\"component_name\", componentName);\n             logEntry.put(\"message\", message);\n             logEntry.put(\"error_code\", errorCode);\n             logEntry.put(\"stack_trace\", stackTrace);\n             System.out.println(logEntry.toString());\n         }\n     }\n     ```\n\n### Consequences\n- **Consistency**: All components will produce log messages that adhere to a uniform structure, making it easier to aggregate, filter, and search logs across different parts of the system.\n- **Simplified Debugging**: By providing a standardized format, the end-user and developer can quickly parse and understand logs and error messages from different components.\n- **Simple Maintenance**: The use of JSON for log messages ensures compatibility with many off-the-shelf log management systems and tools, reducing the need for custom parsers or complex integrations.","GenTime":"2024-08-19 23:57:19"}
{"File Name":"opg-use-an-lpa\/0008-session-storage-using-an-encrypted-cookie.md","Context":"## Context\\n* We will be storing a very small amount of data in the session.\\n* Whilst the above holds true we can avoid additional infrastructure by using client side storage.\\n* The session _may_ hold somewhat sensitive details (e.g. an LPA Share code), thus its content is secret.\\n* As the cookie is client side, we also need authentication to ensure the message isn't tempered with.\\n","Decision":"To use a cookie who's payload is encrypted with AES GCM. This provides secrecy and authentication.\\nNot to use JWT, because:\\n* To ensure message secrecy, additional libraries are needed.\\n* The resulting cookie value is significantly larger.\\n* Concerns over the general suitability around using JWT for client side sessions.\\n","tokens":86,"id":4843,"text":"## Context\\n* We will be storing a very small amount of data in the session.\\n* Whilst the above holds true we can avoid additional infrastructure by using client side storage.\\n* The session _may_ hold somewhat sensitive details (e.g. an LPA Share code), thus its content is secret.\\n* As the cookie is client side, we also need authentication to ensure the message isn't tempered with.\\n\n\n##Decision\nTo use a cookie who's payload is encrypted with AES GCM. This provides secrecy and authentication.\\nNot to use JWT, because:\\n* To ensure message secrecy, additional libraries are needed.\\n* The resulting cookie value is significantly larger.\\n* Concerns over the general suitability around using JWT for client side sessions.\\n","MatchedIds":[4011,2221,4848,1654,527],"Predicted":"## Decision\nWe will use encrypted, signed cookies for session storage.\n\n### Rationale\n* **Client-side storage**: Using cookies allows us to avoid additional infrastructure, adhering to the initial requirement.\n* **Encryption**: To ensure that the contents of the cookie remain secret, we will encrypt the cookie.\n* **Signing**: To prevent tampering, we will sign the cookie using a secure key. This ensures that even if the contents are viewed or modified, the changes will be detected and rejected by the server.\n* **Small data footprint**: Given the small amount of data being stored, the limited storage space of a cookie (~4KB) is sufficient.\n\n### Implementation Steps\n1. **Encryption and Signing**: Use a cryptographically secure algorithm (e.g., AES) for encryption and HMAC for signing.\n2. **Key Management**: Ensure that keys used for encryption and signing are rotated periodically to enhance security.\n3. **Validation**: Implement mechanisms to verify the integrity and authenticity of the cookie upon each request.\n\n### Security Considerations\n* Use HTTPS to ensure that cookies are transmitted securely over the network.\n* Use secure and HttpOnly cookie attributes to prevent access from JavaScript and to ensure transmission only over secure connections.","GenTime":"2024-08-19 23:57:26"}
{"File Name":"meadow\/0011-yarn.md","Context":"## Context\\nWe discussed the relative merits of different JS package managers.\\n","Decision":"Use `yarn` instead of `npm` in all dev, test, and build environments.\\n","tokens":15,"id":3827,"text":"## Context\\nWe discussed the relative merits of different JS package managers.\\n\n\n##Decision\nUse `yarn` instead of `npm` in all dev, test, and build environments.\\n","MatchedIds":[3031,1852,1576,1680,5259],"Predicted":"## Decision\nAfter evaluating several JavaScript package managers, we have decided to use [Yarn](https:\/\/yarnpkg.com\/) as our primary package manager for the following reasons:\n\n- **Performance**: Yarn offers faster dependency installation compared to npm due to its efficient caching and parallelization capabilities.\n- **Reliability**: It has more robust dependency resolution algorithms, which help avoid the infamous dependency mismatch issues.\n- **Consistency**: Yarn's lockfile ensures that installations are reproducible and consistent across different environments and machines.\n- **Security**: Yarn performs integrity checks for installed packages, which adds an extra layer of security compared to npm.\n- **Community Support**: Yarn has extensive documentation and a large community of users, making it easier to find solutions to any issues that might arise.\n\nGiven these advantages, we believe Yarn will be the best fit for our project needs.","GenTime":"2024-08-19 23:57:30"}
{"File Name":"apply-for-teacher-training\/0010-cookie-sessions.md","Context":"## Context\\nThis application needs user sessions to allow users (candidates, providers, referees, support staff) to sign in. Rails offers a number of options for this. Each have a trade-off.\\n### Cookie sessions\\nAll session information is saved in a cookie. The cookie is encrypted to avoid the user changing or reading the data.\\nPro:\\n- It does not interact with other infrastructure\\n- It's the Rails default, well understood by developers\\nCon:\\n- When the user signs out, we do not invalidate the session. This means that if the user has made a copy of the cookie, they (or an attacker) can sign themselves back in.\\n- Users cannot sign out sessions on other devices (remote sign out)\\n### Storage based cookies\\nThis mechanism relies on a session ID being saved in a cookie. The session ID corresponds to a record either in a traditional database (PostgreSQL in our case) or in a caching service (Memcached, Redis).\\nPro:\\n- On sign out, the session is deleted and cannot be revived\\n- Sessions can be invalidated \"remotely\", to allow sign out of other devices\\nCon:\\n- Uses other infrastructure - slight performance overhead, risk of services being unavailable\\n- Sensitive data is stored in a database\\n","Decision":"Use session cookies.\\n","tokens":264,"id":4011,"text":"## Context\\nThis application needs user sessions to allow users (candidates, providers, referees, support staff) to sign in. Rails offers a number of options for this. Each have a trade-off.\\n### Cookie sessions\\nAll session information is saved in a cookie. The cookie is encrypted to avoid the user changing or reading the data.\\nPro:\\n- It does not interact with other infrastructure\\n- It's the Rails default, well understood by developers\\nCon:\\n- When the user signs out, we do not invalidate the session. This means that if the user has made a copy of the cookie, they (or an attacker) can sign themselves back in.\\n- Users cannot sign out sessions on other devices (remote sign out)\\n### Storage based cookies\\nThis mechanism relies on a session ID being saved in a cookie. The session ID corresponds to a record either in a traditional database (PostgreSQL in our case) or in a caching service (Memcached, Redis).\\nPro:\\n- On sign out, the session is deleted and cannot be revived\\n- Sessions can be invalidated \"remotely\", to allow sign out of other devices\\nCon:\\n- Uses other infrastructure - slight performance overhead, risk of services being unavailable\\n- Sensitive data is stored in a database\\n\n\n##Decision\nUse session cookies.\\n","MatchedIds":[4843,5211,2041,2578,1654],"Predicted":"## Decision\nWe will use storage-based cookies for user sessions in this application. \n\nStorage-based cookies provide significant advantages in terms of security and user management:\n- On sign out, the session is deleted and cannot be revived, mitigating the risk of session replay attacks.\n- Sessions can be invalidated remotely, allowing users to sign out from all devices, which is crucial for maintaining security across multiple devices.\n\nWhile there are concerns related to the use of additional infrastructure and the performance overhead associated with it, the security benefits outweigh these drawbacks. Ensuring sessions can be invalidated remotely is critical, particularly for user groups that may access the application from various devices.\n\nWe will use PostgreSQL as the back-end store for session data, leveraging its robustness and reliability:\n- Ensure sensitive data is encrypted at rest within the database to mitigate security concerns.\n- Implement appropriate monitoring and alerting for database availability to address the risk of service unavailability.\n\nBy adopting this approach, we prioritize security and user experience, while managing the trade-offs with performance optimally.","GenTime":"2024-08-19 23:57:43"}
{"File Name":"Head-Start-TTADP\/0006-database.md","Context":"## Context\\nData storage and management is a key component of the tta smarthub platform. At a fundamental level, a relational as well as NoSQL systems were considered. Because the platform's data is mainly structured  and with a need for frequent queries and reports, a relational database management system was viewed as more suitable. With that in mind we looked at MySQL and PostreSQL (Postgres) both open source and popular choices.\\n","Decision":"While both databases provide adequate storage and management, especially with updates provided by version 8 of MySQL, in the end Postgres was chosen. The main advantages of Postgres are implementations leading to better concurrency, specifically the MVCC (Multiversion Concurrency Control) without the need to use read locks, transactional ACID (Atomicity, Consistency, Isolation, Durability) support making the system less vulnerable to data corruption. Postgres also supports partial indexes, can create indexes in a non-blocking way, multiple cores, GIN\/GIST indexing accelerating full-text searches.\\n","tokens":90,"id":1167,"text":"## Context\\nData storage and management is a key component of the tta smarthub platform. At a fundamental level, a relational as well as NoSQL systems were considered. Because the platform's data is mainly structured  and with a need for frequent queries and reports, a relational database management system was viewed as more suitable. With that in mind we looked at MySQL and PostreSQL (Postgres) both open source and popular choices.\\n\n\n##Decision\nWhile both databases provide adequate storage and management, especially with updates provided by version 8 of MySQL, in the end Postgres was chosen. The main advantages of Postgres are implementations leading to better concurrency, specifically the MVCC (Multiversion Concurrency Control) without the need to use read locks, transactional ACID (Atomicity, Consistency, Isolation, Durability) support making the system less vulnerable to data corruption. Postgres also supports partial indexes, can create indexes in a non-blocking way, multiple cores, GIN\/GIST indexing accelerating full-text searches.\\n","MatchedIds":[1740,1661,4747,4859,4285],"Predicted":"## Decision\nChosen option: \"PostgreSQL\", because\n- PostgreSQL has superior support for complex queries and reporting.\n- It boasts advanced indexing and Full Text Search capabilities, which are beneficial for structured data.\n- PostgreSQL supports JSON data types, offering flexibility if some unstructured data needs arise in the future.\n- It has a strong community and extensive documentation.\n- It's well-supported in various cloud environments, making future scaling easier.","GenTime":"2024-08-19 23:57:46"}
{"File Name":"cafebabel.com\/0005-flask-architecture.md","Context":"## Context\\nFlask allows you to do whatever you want on your app\/folders\/templates\/etc\\nand it can quickly become messy.\\nPlus, to deal with configurations within extensions is harder if you do not\\ndeclare the Flask app once and for all then load the configuration.\\n","Decision":"An architecture based on https:\/\/github.com\/stephane\/flask-skeleton\\n","tokens":58,"id":3237,"text":"## Context\\nFlask allows you to do whatever you want on your app\/folders\/templates\/etc\\nand it can quickly become messy.\\nPlus, to deal with configurations within extensions is harder if you do not\\ndeclare the Flask app once and for all then load the configuration.\\n\n\n##Decision\nAn architecture based on https:\/\/github.com\/stephane\/flask-skeleton\\n","MatchedIds":[1173,1540,1416,4271,1239],"Predicted":"## Decision\nWe will adopt a structured project layout for our Flask application to prevent it from becoming messy. This includes:\n\n* Using a factory pattern for creating the Flask app instance, allowing for clear and centralized configuration loading.\n* Adopting Blueprints to organize application routes into modular components.\n* Structuring directories into clear, purpose-driven folders (e.g., `templates`, `static`, `models`, `views`, `services`, etc.).\n* Utilizing configuration management best practices by loading configurations from a central location at app initialization.\n\nThis method will help maintain code organization, enhance readability, and simplify configuration management across extensions.","GenTime":"2024-08-19 23:57:49"}
{"File Name":"nada-kafkarator\/0002-use-python.md","Context":"## Context\\nMany k8s operators use Golang, but none of us working on this project are proficient with Golang.\\nAnother option was Rust, which could be a useful learning exercise.\\nPython is used in FIAAS, so we have some experience using it for an operator.\\n","Decision":"We will use Python for Kafkarator.\\n","tokens":61,"id":861,"text":"## Context\\nMany k8s operators use Golang, but none of us working on this project are proficient with Golang.\\nAnother option was Rust, which could be a useful learning exercise.\\nPython is used in FIAAS, so we have some experience using it for an operator.\\n\n\n##Decision\nWe will use Python for Kafkarator.\\n","MatchedIds":[472,4761,1376,3109,1472],"Predicted":"## Decision\nWe will use Python to develop the k8s operator.\n\n### Reasoning:\n1. **Familiarity and Experience**: Our team has experience using Python for an operator in FIAAS, which will enable us to leverage existing knowledge and expedite development.\n2. **Productivity**: Using a language we are proficient in will reduce development time and lower the learning curve compared to adopting a new language like Golang or Rust.\n3. **Community and Ecosystem**: Python has a strong ecosystem for developing k8s operators, including mature libraries and a supportive community, which will provide valuable resources and support.\n4. **Project Requirements and Goals**: Our primary goal is to build a functional operator efficiently. While learning Rust could be beneficial in the long-term, our immediate priority is to deliver results, making Python the pragmatic choice.\n\n### Positive Consequences:\n- Faster initial development and quicker time to market due to existing expertise.\n- Reduced risk of errors and bugs stemming from language unfamiliarity.\n- Ability to reuse code and modules from FIAAS, providing consistency and reliability.\n\n### Negative Consequences:\n- We may miss out on potential performance or concurrency benefits provided by Golang or Rust.\n- Future maintenance or scalability might require re-evaluating the language choice if Python's limitations become apparent with increased complexity.\n\nIn conclusion, Python is the optimal choice given our current team skills, existing knowledge base, and our focus on efficient delivery.","GenTime":"2024-08-19 23:57:56"}
{"File Name":"winery\/0027-use-dasherization-for-filenames.md","Context":"## Context and Problem Statement\\nGraphics files have to take a consistent file name\\n## Decision Drivers <!-- optional -->\\n* Easy to process by Jekyll\\n* No WTFs at the creators\\n","Decision":"* Easy to process by Jekyll\\n* No WTFs at the creators\\nChosen option: \"Dasherization\", because\\n* clear separation of parts of the name\\n* consistent to other URLs (which are typically lowercase)\\n","tokens":40,"id":4308,"text":"## Context and Problem Statement\\nGraphics files have to take a consistent file name\\n## Decision Drivers <!-- optional -->\\n* Easy to process by Jekyll\\n* No WTFs at the creators\\n\n\n##Decision\n* Easy to process by Jekyll\\n* No WTFs at the creators\\nChosen option: \"Dasherization\", because\\n* clear separation of parts of the name\\n* consistent to other URLs (which are typically lowercase)\\n","MatchedIds":[4766,1996,3521,74,4600],"Predicted":"## Decision\nChosen option: Adopt a consistent naming convention for graphics files that adheres to the following guidelines:\n\n1. **Use lowercase letters:** File names should only contain lowercase letters to avoid case-sensitivity issues.\n2. **Use hyphens for word separation:** Separate words within the file name using hyphens (`-`). Avoid using underscores (`_`), spaces, or any other special characters.\n3. **Include descriptive names:** File names should be descriptive enough to understand the content or context of the graphic.\n4. **Use date prefixes if applicable:** For time-sensitive graphics or versioned graphics, prepend the file name with a date in `YYYYMMDD` format (e.g., `20231005-banner-image.png`).\n5. **Standardize file extensions:** Use common file extensions according to the graphic type (e.g., `.png`, `.jpg`, `.svg`).\n\n### Format Examples\n- `homepage-banner.png`\n- `product-thumbnail-20231005.jpg`\n- `company-logo.svg`\n\n### Positive Consequences\n* **Ease of Processing by Jekyll:** Consistent naming makes it straightforward to reference and process images within the Jekyll framework without dealing with case-sensitivity or special character issues.\n* **Increased Readability:** Clear, descriptive file naming reduces confusion and helps creators and developers quickly identify the purpose and content of the graphics.\n* **Streamlined Collaboration:** A standardized naming convention minimizes misunderstandings and makes onboarding new team members easier.\n\n### Negative Consequences\n* **Initial Adjustment Period:** Team members must adapt to the new naming convention, which might require some initial effort and guidance.\n* **Longer File Names:** Descriptive names might be longer, but this trade-off is outweighed by the benefits of clarity and ease of use.","GenTime":"2024-08-19 23:58:06"}
{"File Name":"libelektra\/null_pointer_checks.md","Context":"## Problem\\nCurrently all functions do proper argument checking which might degrade\\nperformance.\\n","Decision":"Rejected (keep checks) due to time constraints\\n","tokens":17,"id":1317,"text":"## Problem\\nCurrently all functions do proper argument checking which might degrade\\nperformance.\\n\n\n##Decision\nRejected (keep checks) due to time constraints\\n","MatchedIds":[2629,1293,1302,1292,1258],"Predicted":"## Decision\nIntroduce a configuration-based approach to argument checking. By default, enable argument checking for development and testing environments to catch errors early. For production builds, provide an option to disable or minimize argument checking to improve performance.\n\nImplementation steps:\n1. Add a configuration flag (e.g., `ENABLE_ARG_CHECK`) that controls argument checking across the codebase.\n2. Wrap argument checking code in conditional statements that check the `ENABLE_ARG_CHECK` flag.\n3. Set the `ENABLE_ARG_CHECK` flag to `true` for development and testing environments and to `false` for production builds.\n\nExample in C:\n```c\n#ifdef ENABLE_ARG_CHECK\n    if (arg == NULL) {\n        \/\/ Handle error\n    }\n#endif\n```\nIn Java:\n```java\nif (Configuration.ENABLE_ARG_CHECK) {\n    if (arg == null) {\n        \/\/ Handle error\n    }\n}\n```\nDocument the new configuration flag in the project's documentation and provide examples of how to enable or disable argument checking based on the environment. This approach allows maintaining code safety during development while optimizing performance in production.","GenTime":"2024-08-19 23:58:11"}
{"File Name":"dos-server\/adr-1-metadata.md","Context":"## Context\\nDigital objects need to have associated metadata for various use cases (refer to\\nthe requirements documentation for details). Metadata of these objects can be descriptive, administrative, and structural.\\nTo avoid \"duplication\" of descriptive metadata, it is desired that DOS not store descriptive metadata.\\n","Decision":"Descriptive metadata will not be stored by DOS.\\n","tokens":59,"id":3372,"text":"## Context\\nDigital objects need to have associated metadata for various use cases (refer to\\nthe requirements documentation for details). Metadata of these objects can be descriptive, administrative, and structural.\\nTo avoid \"duplication\" of descriptive metadata, it is desired that DOS not store descriptive metadata.\\n\n\n##Decision\nDescriptive metadata will not be stored by DOS.\\n","MatchedIds":[3369,285,3374,2859,3831],"Predicted":"## Decision\nDOS will store only administrative and structural metadata. Descriptive metadata will be externalized and accessed via integration with a metadata management service, such as ArchivesSpace or another dedicated metadata repository. This approach ensures that descriptive metadata is centralized, reducing duplication and supporting easier updates and maintenance across systems. The metadata management service's API will be used to fetch descriptive metadata when needed.","GenTime":"2024-08-19 23:58:14"}
{"File Name":"embvm-core\/0021-eliminate-name-variables-from-core-base-classes.md","Context":"## Context\\nUnder the initial design of the `embvm-core`, the following types stored a name in the base class:\\n- Driver\\n- Processor\\n- Hardware Platform\\n- Platform\\nIn some cases, like drivers, this data was duplicated (through the driver registry) or unnecessary (because you use the default name and provide the system name in the driver registry). We don't need to store two different instances of the same name!\\nIn other cases, like the processor and platforms, these names were never used.\\n","Decision":"These names will no longer be required by the base classes.\\n","tokens":107,"id":3023,"text":"## Context\\nUnder the initial design of the `embvm-core`, the following types stored a name in the base class:\\n- Driver\\n- Processor\\n- Hardware Platform\\n- Platform\\nIn some cases, like drivers, this data was duplicated (through the driver registry) or unnecessary (because you use the default name and provide the system name in the driver registry). We don't need to store two different instances of the same name!\\nIn other cases, like the processor and platforms, these names were never used.\\n\n\n##Decision\nThese names will no longer be required by the base classes.\\n","MatchedIds":[3027,2270,1193,74,200],"Predicted":"## Decision\nTo eliminate redundancy and enhance code clarity, the following changes will be made:\n\n1. **Driver**: \n   - Remove the `name` attribute from the base Driver class.\n   - Rely solely on the driver registry to manage driver names.\n\n2. **Processor**:\n   - Remove the `name` attribute from the Processor class since it was unused.\n\n3. **Hardware Platform**:\n   - Remove the `name` attribute from the Hardware Platform class since it was unused.\n\n4. **Platform**:\n   - Remove the `name` attribute from the Platform class since it was unused.\n\nBy removing these unused or redundant attributes, we simplify the class structures and avoid potential confusion or errors arising from managing multiple instances of the same data.","GenTime":"2024-08-19 23:58:17"}
{"File Name":"connaisseur\/ADR-1_bootstrap-sentinel.md","Context":"## Context\\nConnaisseur's main components are a MutatingWebhookConfiguration and the Connaisseur Pods. The MutatingWebhookConfiguration intercepts requests to create or update Kubernetes resources and forwards them to the Connaisseur Pods tasked, on a high level, with verifying trust data. The order of deploying both components matters, since a blocking MutatingWebhookConfiguration without the Connaisseur Pods to answer its requests would also block the deployment of said Pods.\\nIn [#3](https:\/\/github.com\/sse-secure-systems\/connaisseur\/issues\/3) it was noted that prior to version 1.1.5 of Connaisseur when looking at the `Ready` status of Connaisseur Pods, they could report `Ready` while being non-functional due to the MutatingWebhookConfiguration missing. However, as stated above the MutatingWebhookConfiguration can only be deployed _after_ the Connaisseur Pods, which was solved by checking the `Ready` state of said Pods. If one were to add a dependency to this `Ready` state, such that it only shows `Ready` when the MutatingWebhookConfiguration exists, we run into a deadlock, where the MutatingWebhookConfiguration waits for the Pods and the Pods wait for the MutatingWebhookConfiguration.\\n","Decision":"We chose option 1 over option 2, because it was important to us that a brief glance at Connaisseur's Namespace allows one to judge whether it is running properly. Option 3 was not chosen as the readiness status of Pods can be easily seen from the Service, whereas the health status would require querying every single Pod individually. We deemed that to be a very ugly, non-kubernetes-y solution and hence decided against it.\\n### Positive consequences\\nIf the Connaisseur Pods report `Ready` during the `connaisseur-bootstrap-sentinel`'s runtime, the MutatingWebhookConfiguration will be deployed by Helm. Otherwise, the Helm deployment will fail after its timeout period (default: 5min), since there won't be a running `connaisseur-bootstrap-sentinel` Pod anymore that resolves the installation deadlock. The Connaisseur Pods will never reach the `Ready` state and the MutatingWebhookConfiguration never gets deployed. This means, we get consistent deployment failures after the inital waiting period if something did not work out. Additionally, if the MutatingWebhookConfiguration gets removed for whatever reason during operation, Connaisseur Pods will be failing, indicating their failed dependency. Hence, monitoring the Connaisseur Pods is sufficient to ensure their working.\\n### Negative consequences\\nOn the other hand, if an adversary can deploy a Pod named `connaisseur-bootstrap-sentinel` to Connaisseur's Namespace, the Connaisseur Pods will always show `Ready` regardless of the MutatingWebhookConfiguration. However, if an adversary can deploy to Connaisseur's Namespace, chances are Connaisseur can be compromised anyways. More importantly, if not a single Connaisseur Pod is successfully deployed or if the notary healthcheck fails during the sentinel's lifetime, then the deployment will fail regardless of possible recovery at a later time. Another issue would be the `connaisseur-bootstrap-sentinel` Pod being left behind, however since it has a very limited use case we can also clean it up during the deployment, so apart from the minimal additional complexity of the deployment this is a non-issue.\\n","tokens":275,"id":4395,"text":"## Context\\nConnaisseur's main components are a MutatingWebhookConfiguration and the Connaisseur Pods. The MutatingWebhookConfiguration intercepts requests to create or update Kubernetes resources and forwards them to the Connaisseur Pods tasked, on a high level, with verifying trust data. The order of deploying both components matters, since a blocking MutatingWebhookConfiguration without the Connaisseur Pods to answer its requests would also block the deployment of said Pods.\\nIn [#3](https:\/\/github.com\/sse-secure-systems\/connaisseur\/issues\/3) it was noted that prior to version 1.1.5 of Connaisseur when looking at the `Ready` status of Connaisseur Pods, they could report `Ready` while being non-functional due to the MutatingWebhookConfiguration missing. However, as stated above the MutatingWebhookConfiguration can only be deployed _after_ the Connaisseur Pods, which was solved by checking the `Ready` state of said Pods. If one were to add a dependency to this `Ready` state, such that it only shows `Ready` when the MutatingWebhookConfiguration exists, we run into a deadlock, where the MutatingWebhookConfiguration waits for the Pods and the Pods wait for the MutatingWebhookConfiguration.\\n\n\n##Decision\nWe chose option 1 over option 2, because it was important to us that a brief glance at Connaisseur's Namespace allows one to judge whether it is running properly. Option 3 was not chosen as the readiness status of Pods can be easily seen from the Service, whereas the health status would require querying every single Pod individually. We deemed that to be a very ugly, non-kubernetes-y solution and hence decided against it.\\n### Positive consequences\\nIf the Connaisseur Pods report `Ready` during the `connaisseur-bootstrap-sentinel`'s runtime, the MutatingWebhookConfiguration will be deployed by Helm. Otherwise, the Helm deployment will fail after its timeout period (default: 5min), since there won't be a running `connaisseur-bootstrap-sentinel` Pod anymore that resolves the installation deadlock. The Connaisseur Pods will never reach the `Ready` state and the MutatingWebhookConfiguration never gets deployed. This means, we get consistent deployment failures after the inital waiting period if something did not work out. Additionally, if the MutatingWebhookConfiguration gets removed for whatever reason during operation, Connaisseur Pods will be failing, indicating their failed dependency. Hence, monitoring the Connaisseur Pods is sufficient to ensure their working.\\n### Negative consequences\\nOn the other hand, if an adversary can deploy a Pod named `connaisseur-bootstrap-sentinel` to Connaisseur's Namespace, the Connaisseur Pods will always show `Ready` regardless of the MutatingWebhookConfiguration. However, if an adversary can deploy to Connaisseur's Namespace, chances are Connaisseur can be compromised anyways. More importantly, if not a single Connaisseur Pod is successfully deployed or if the notary healthcheck fails during the sentinel's lifetime, then the deployment will fail regardless of possible recovery at a later time. Another issue would be the `connaisseur-bootstrap-sentinel` Pod being left behind, however since it has a very limited use case we can also clean it up during the deployment, so apart from the minimal additional complexity of the deployment this is a non-issue.\\n","MatchedIds":[4397,2240,3915,2249,2236],"Predicted":"## Decision\nTo avoid the potential deadlock scenario, we decided to implement a two-phase deployment strategy that involves temporary disabling of the MutatingWebhookConfiguration during the Connaisseur Pods deployment:\n\n1. **Initial Deployment \/ Upgrade**:\n    - First, deploy or update the Connaisseur Pods without the MutatingWebhookConfiguration enabled. This ensures that the Connaisseur Pods can start and reach a `Ready` state without being blocked.\n    - Once the Connaisseur Pods are `Ready` and fully operational, enable the MutatingWebhookConfiguration to start intercepting requests.\n\n2. **Health Check Enhancement**:\n    - Modify the health checks of the Connaisseur Pods so they report as `Ready` only if they are operational and in constant communication with a list of mandatory configurations, including the MutatingWebhookConfiguration. However, a non-blocking mechanism will be implemented where the absence of the MutatingWebhookConfiguration will only issue warnings without blocking the Pods.\n\n3. **Automate with CI\/CD**:\n    - Automate the above steps using a CI\/CD pipeline to ensure a smooth and error-free deployment process. This will reduce human error and maintain the correct deployment order.\n\n### Positive consequences\n- Avoids deadlock scenarios where both Connaisseur Pods and MutatingWebhookConfiguration are waiting on each other.\n- Ensures Connaisseur Pods are truly operational before enabling the MutatingWebhookConfiguration.\n- Deployment process is automated, reducing risk of manual errors.\n\n### Negative consequences\n- Introduces an additional step in the deployment process that might add slight complexity.\n- During the window between initial deployment and enabling the MutatingWebhookConfiguration, some requests may not be intercepted and verified by Connaisseur, which may briefly decrease security verification levels.\n\nBy adopting this approach, we ensure a seamless and deadlock-free deployment of Connaisseur while maintaining the overall integrity and security of the system.","GenTime":"2024-08-19 23:58:25"}
{"File Name":"GoStudy\/0004-use-ports-adapters-as-application-architecture-style.md","Context":"## Contextadjecent\\nWe need to adopt application architecture style adjecent to our architectural drivers\\n","Decision":"We will use ports&adapters architecture style\\n","tokens":21,"id":429,"text":"## Contextadjecent\\nWe need to adopt application architecture style adjecent to our architectural drivers\\n\n\n##Decision\nWe will use ports&adapters architecture style\\n","MatchedIds":[2502,2261,4428,3119,1436],"Predicted":"## Decision\nWe will adopt a microservices architecture style, as it aligns well with our architectural drivers such as scalability, agility, and resilience.","GenTime":"2024-08-19 23:58:27"}
{"File Name":"smjs\/2018120802-implement-clean-architecture.md","Context":"## Context\\nThis project is meant to be a playground to experiment with different technologies and design solutions.\\n","Decision":"To standardize the project structure, now that it's getting bigger, we will implement the principles suggested by [clean architecture](https:\/\/github.com\/michelezamuner\/notes\/tree\/master\/software-architecture\/clean-architecture\/clean-architecture.martin), enhanced by concepts from Domain Driven Design, and plugin architectures.\\n### Domain\\nThe core domain is the \"virtual machine framework\", which only defines how programs should be executed, but doesn't specify any architecture, meaning that how programs are interpreted is not known.\\nA Program is a sequence of Data Units. Data Units are the kind of data with the smallest possible size, which we set at a single Byte. Data is always represented as a sequence of Data Units. Since we work with sequences, we also define the concepts of Size, which is the number of Data Units in a specific Data, and Address of a Data inside the Program, with the Address of the first Data Unit being 0. Both Size and Address are Integers, which is a generic integral type defined to be independent from the runtime environment implementation. A Program has the ability of fetching blocks of Data given their Address and Size.\\nA Program is run by a Processor, which uses an Interpreter, whose implementation is provided by the specific Architecture selected, to first define which sets of Data Units should be regarded as Instructions, and then to execute these Instructions. When running an Instruction, the Interpreter returns a Status object knowing if the execution should jump, or be terminated. The execution of a Program by a Processor always returns an Exit Status, which is Architecture-dependent. The termination of a Program must always be requested explicitly, via a dedicated instruction, otherwise an error is raised.\\nAn Interpreter must use the System to perform I\/O operations, and ultimately to allow a Program to communicate with the users; however, the implementation of the System depends on the actual application where the Processor and Interpreter are running, so it's left to be specified.\\nAdditional domains are defined for each architecture, so that a virtual machine can support many different architectures.\\nAssemblers and compilers also define their own domains.\\nThe following domains could thus be defined:\\n- `smf`: the core virtual machine framework domain\\n- `sma`: definitions for the SMA architecture domain\\n- `basm`: definitions for the BASM assembler domain\\n- `php`: definitions for the PHP compiler domain\\n### Application\\nThe application layer may define the following primary ports:\\n- the `vm` port allows to execute programs, according to the configured architecture\\n- the `repl` port allows to execute programs interactively, and uses the functionality of `vm`\\n- the `dbg` port allows to execute programs step by step for debugging, and uses the functionality of `vm`\\n- the `asm` port allows to run an assembler on some assembly code to produce executable object code\\n- the `cmp` port allows to run a compiler on some high level language to produce assembly code\\nAs far as secondary ports, we need the following:\\n- a `arcl` port allows the application to load an architecture definition\\n- a `pl` port allows the application to load a program\\n- a `asml` port allows the application to load assembly code, to be assembled\\n- a `cl` port allows the application to load high level code, to be compiled\\n- a `sys` port allows the application to interact with the underlying operating system\\n### Adapters\\nPrimary adapters might be defined to create command line applications, or Web applications. Secondary adapters might be defined to read data from files or from memory. See below for more concrete examples.\\n### Plugin architecture\\nWe want to support building different types of applications by composing together sets of different available plugins. For example:\\n**sloth machine (CLI)**\\nAD_sm + AD_larcl + AD_fpl + AD_ossys + AP_vm + AP_arcl + AP_pl + AP_sys + D_smf + D_sma (or others)\\n**sloth machine assembler (CLI)**\\nAD_asm + AD_fasml + AP_asm + AP_asml + D_basm (or others)\\n**sloth machine compiler (CLI)**\\nAD_cmp + AD_fcl + AD_masml + AP_cmp + AP_asm + AP_cl + AP_asml + D_basm (or others) + D_php (or others)\\n**sloth machine runner (CLI)**\\nAD_run + AD_larcl + AD_fcl + AD_masml + AD_mpl + AD_ossys + AP_vm + AP_cmp + AP_asm + AP_arcl + AP_cl + AP_asml + AP_pl + AP_sys + D_smf + D_sma (or others) + D_basm (or others) + D_php (or others)\\n**sloth machine REPL (CLI)**\\nAD_repl + AD_larcl + AD_mcl + AD_masml + AD_mpl + AD_ossys + AP_repl + AP_cmp + AP_asm + AP_arcl + AP_cl + AP_asml + AP_pl + AP_sys + D_smf + D_sma (or others) + D_basm (or others) + D_php (or others)\\n**sloth machine debugger (CLI)**\\nAD_dbg + AD_larcl + AD_fcl + AD_masml + AD_mpl + AD_ossys + AP_dbg + AP_cmp + AP_asm + AP_arcl + AP_cl + AP_asml + AP_pl + AP_sys + D_smf + D_sma (or others) + D_basm (or others) + D_php (or others)\\n**sloth machine Web (Web)**\\nAD_web + AD_warcl + AD_wcl + AD_masml + AD_mpl + AD_wsys + AP_vm + AP_cmp + AP_asm + AP_repl + AP_dbg + AP_arcl + AP_cl + AP_asml + AP_pl + AP_sys + D_smf + D_sma (or others) + D_basm (or others) + D_php (or others)\\n- `D_smf`: Domain Sloth Machine Framework\\n- `D_sma`: Domain Sloth Machine Architecture\\n- `D_basm`: Domain Basic Assembly for Sloth Machine\\n- `D_php`: Domain PHP\\n- `AP_vm`: Application Virtual Machine (primary port)\\n- `AP_cmp`: Application Compiler (primary port)\\n- `AP_asm`: Application Assembler (primary port)\\n- `AP_repl`: Application REPL (primary port)\\n- `AP_dbg`: Application Debugger (primary port)\\n- `AP_arcl`: Application Architecture Loader (secondary port)\\n- `AP_pl`: Application Program Loader (secondary port)\\n- `AP_asml`: Application Assembly Loader (secondary port)\\n- `AP_cl`: Application Code Loader (secondary port)\\n- `AP_sys`: Application System (secondary port)\\n- `AD_sm`: Adapter Sloth Machine (primary adapter)\\n- `AD_cmp`: Adapter Compiler (primary adapter)\\n- `AD_run`: Adapter Runner (primary adapter)\\n- `AD_repl`: Adapter REPL (primary adapter)\\n- `AD_dbg`: Adapter Debugger (primary adapter)\\n- `AD_web`: Adapter Web (primary adapter)\\n- `AD_larcl`: Adapter Local Architecture Loader (secondary adapter)\\n- `AD_warcl`: Adapter Web Architecture Loader (secondary adapter)\\n- `AD_fpl`: Adapter File Program Loader (secondary adapter)\\n- `AD_mpl`: Adapter Memory Program Loader (secondary adapter)\\n- `AD_fasml`: Adapter File Assembly Loader (secondary adapter)\\n- `AD_masml`: Adapter Memory Assembly Loader (secondary adapter)\\n- `AD_fcl`: Adapter File Code Loader (secondary adapter)\\n- `AD_mcl`: Adapter Memory Code Loader (secondary adapter)\\n- `AD_wcl`: Adapter Web Code Loader (secondary adapter)\\n- `AD_ossys`: Adapter OS System (secondary adapter)\\n- `AD_wsys`: Adapter Web System (secondary adapter)\\n### Example modules\\n```\\ndomain\\nsmf\\ndata\\nDataUnit: (Byte)\\nData: DataUnit[]\\nSize: (Integer)\\nAddress: (Integer)\\nprogram [data]\\nProgram\\nProgram(data.Data)\\nread(data.Address, data.Size): data.Data\\ninterpreter [data]\\nOpcode: data.Data\\nOperands: data.Data\\nExitStatus: (Integer)\\nInstruction\\nInstruction(Address, Opcode, Operands)\\ngetAddress(): Address\\ngetOpcode(): Opcode\\ngetOperands(): Operands\\nStatus\\nshouldJump(): (Boolean)\\ngetJumpAddress(): data.Address\\nshouldExit(): (Boolean)\\ngetExitStatus(): ExitStatus\\n<Interpreter>\\ngetOpcodeSize(): data.Size\\ngetOperandsSize(Opcode): data.Size\\nexec(Instruction): Status\\nprocessor [program, interpreter]\\nProcessor\\nProcessor(interpreter.<Interpreter>)\\nrun(program.Program): interpreter.ExitStatus\\narchitecture [data, interpreter]\\n<System>\\nread(data.Integer fd, data.Size size): data.Data\\nwrite(data.Integer fd, data.Data data, data.Size size): data.Size\\n<Architecture>\\ngetInterpreter(<System>): interpreter.<Interpreter>\\nsma [smf]\\nInterpreter: smf.interpreter.<Interpreter>\\nInterpreter(smf.architecture.<System>)\\napplication\\nvm\\nrun_program [domain.smf, application.arcl, application.pl, application.sys]\\n<Request>\\ngetArchitectureName(): String\\ngetProgramReference(): String\\nResponse\\ngetExitStatus(): domain.smf.interpreter.ExitStatus\\n<Presenter>\\npresent(Response)\\nRunProgram\\nRunProgram(ProcessorFactory, <Presenter>, application.arcl.<ArchitectureLoader>, application.pl.<ProgramLoader>, application.sys.<System>)\\nexec(<Request>)\\nProcessorFactory\\ncreate(domain.smf.interpreter.<Interpreter>): domain.smf.processor.Processor\\narcl [domain.smf]\\n<ArchitectureLoader>\\nload(architectureName: String): domain.smf.architecture.<Architecture>\\npl [domain.smf]\\n<ProgramLoader>\\nload(programReference: String): domain.smf.program.Program\\nsys [domain.smf]\\n<System>: domain.smf.architecture.<System>\\nadapters\\nsm [application.vm, domain.smf]\\nrun_program [application.vm, domain.smf]\\nRequest: application.vm.run_program.<Request>\\nController\\nController(application.vm.run_program.RunProgram)\\nrunProgram(Request)\\nViewModel\\nViewModel(domain.smf.interpreter.<ExitStatus>)\\ngetExitStatus(): <native-integer>\\n<View>\\nrender(ViewModel)\\nExitStatusView: <View>\\nrender(ViewModel)\\ngetExitStatus(): <native-integer>\\nPresenter: application.vm.run_program.<Presenter>\\nPresenter(<View>)\\npresent(application.vm.run_program.Response)\\nlarcl [application.arcl]\\nLocalArchitectureLoader: application.arcl.<ArchitectureLoader>\\nfpl [application.pl]\\nFileProgramLoader: application.pl.<ProgramLoader>\\nossys [application.sys]\\nOSSystem: application.sys.<System>\\n```\\n### Examples of main implementations\\n```\\nmodule domain.smf.processor\\nimport domain.smf.interpreter.<Interpreter>\\nimport domain.smf.program.Program\\nimport domain.smf.interpreter.ExitStatus\\nimport domain.smf.data.Size\\nimport domain.smf.data.Address\\nimport domain.smf.interpreter.Opcode\\nimport domain.smf.interpreter.Operands\\nimport domain.smf.interpreter.Instruction\\nimport domain.smf.interpreter.Status\\nclass Processor\\nProcessor(<Interpreter> interpreter)\\nthis.interpreter = interpreter\\nrun(Program program): ExitStatus\\nSize opcodeSize = interpreter.getOpcodeSize()\\nAddress address = 0\\nwhile (true)\\nOpcode opcode = program.read(address, opcodeSize)\\nSize operandsSize = interpreter.getOperandsSize(opcode)\\nAddress operandsAddress = address + opcodeSize\\nOperands operands = program.read(operandsAddress, operandsSize)\\nInstruction instruction = new Instruction(address, opcode, operands)\\nStatus status = interpreter.exec(instruction)\\nif (status.shouldExit())\\nreturn status.getExitStatus()\\naddress = status.shouldJump() ? status.getJumpAddress() : operandsAddress + operandsSize\\n\/\/ @todo: handle missing exit\\n```\\n```\\nmodule domain.sma.interpreter\\nimport domain.smf.interpreter.<Interpreter>\\nimport domain.smf.interpreter.ExitStatus\\nimport domain.smf.interpreter.Status\\nimport domain.smf.data.Size\\nimport domain.smf.data.Address\\nimport domain.smf.interpreter.Opcode\\nimport domain.smf.interpreter.Instruction\\nimport domain.smf.architecture.<System>\\nimport domain.sma.InstructionSet\\nimport domain.sma.Result\\nimport domain.sma.JumpResult\\nimport domain.sma.ExitResult\\nimport domain.sma.InstructionDefinition\\nclass Interpreter: <Interpreter>\\nInterpreter(InstructionSet instructionSet, <System> system)\\nthis.instructionSet = instructionSet\\nthis.system = system\\ngetOpcodeSize(): Size\\nreturn new Integer(1)\\ngetOperandsSize(Opcode opcode): Size\\nreturn instructionSet.getInstructionDefinition(opcode).getOperandsSize()\\nexec(Instruction instruction): Status\\nAddress jumpAddress = null\\nAddress exitStatus = null\\nInstructionDefinition definition = instructionSet.getInstructionDefinition(instruction.getOpcode())\\nResult result = definition.exec(instruction.getOperands())\\nif (result instanceof JumpResult)\\njumpAddress = result.getJumpAddress()\\nif (result instanceof ExitResult)\\nexitStatus = result.getExitStatus()\\nreturn new Status(jumpAddress, exitStatus)\\n```\\n```\\nmodule application.vm.run_program\\nimport application.vm.run_program.ProcessorFactory\\nimport application.vm.run_program.<Presenter>\\nimport application.arcl.<ArchitectureLoader>\\nimport application.pl.<ProgramLoader>\\nimport application.sys.<System>\\nimport application.vm.run_program.<Request>\\nimport domain.smf.architecture.<Architecture>\\nimport domain.smf.interpreter.<Interpreter>\\nimport domain.smf.processor.Processor\\nimport domain.smf.program.Program\\nimport domain.smf.interpreter.ExitStatus\\nimport application.vm.run_program.Response\\nclass RunProgram\\nRunProgram(\\nProcessorFactory processorFactory,\\n<Presenter> presenter,\\n<ArchitectureLoader> architectureLoader,\\n<ProgramLoader> programLoader,\\n<System> system\\n)\\nthis.processorFactory = processorFactory\\nthis.presenter = presenter\\nthis.architectureLoader = architectureLoader\\nthis.programLoader = programLoader\\nthis.system = system\\nexec(<Request> request)\\n<Architecture> architecture = architectureLoader.load(request.getArchitectureName())\\n<Interpreter> interpreter = architecture.getInterpreter(this.system)\\nProcessor processor = processorFactory.create(interpreter)\\nProgram program = programLoader.load(request.getProgramReference())\\nExitStatus exitStatus = processor.run(program)\\npresenter.present(new Response(exitStatus))\\n```\\n```\\nmodule adapters.sm.run_program\\nimport application.vm.run_program.RunProgram\\nimport application.vm.run_program.<Request>\\nclass Controller\\nController(RunProgram service)\\nthis.service = service\\nrunProgram(<Request> request)\\nservice.run(request)\\n```\\n```\\nmodule adapters.sm.run_program\\nimport adapters.sm.run_program.<View>\\nimport application.vm.run_program.Response\\nimport adapters.sm.run_program.ViewModel\\nclass Presenter\\nPresenter(<View> view)\\nthis.view = view\\npresent(Response response)\\nViewModel viewModel = new ViewModel(response.getExitStatus())\\nview.render(viewModel)\\n```\\n","tokens":21,"id":3982,"text":"## Context\\nThis project is meant to be a playground to experiment with different technologies and design solutions.\\n\n\n##Decision\nTo standardize the project structure, now that it's getting bigger, we will implement the principles suggested by [clean architecture](https:\/\/github.com\/michelezamuner\/notes\/tree\/master\/software-architecture\/clean-architecture\/clean-architecture.martin), enhanced by concepts from Domain Driven Design, and plugin architectures.\\n### Domain\\nThe core domain is the \"virtual machine framework\", which only defines how programs should be executed, but doesn't specify any architecture, meaning that how programs are interpreted is not known.\\nA Program is a sequence of Data Units. Data Units are the kind of data with the smallest possible size, which we set at a single Byte. Data is always represented as a sequence of Data Units. Since we work with sequences, we also define the concepts of Size, which is the number of Data Units in a specific Data, and Address of a Data inside the Program, with the Address of the first Data Unit being 0. Both Size and Address are Integers, which is a generic integral type defined to be independent from the runtime environment implementation. A Program has the ability of fetching blocks of Data given their Address and Size.\\nA Program is run by a Processor, which uses an Interpreter, whose implementation is provided by the specific Architecture selected, to first define which sets of Data Units should be regarded as Instructions, and then to execute these Instructions. When running an Instruction, the Interpreter returns a Status object knowing if the execution should jump, or be terminated. The execution of a Program by a Processor always returns an Exit Status, which is Architecture-dependent. The termination of a Program must always be requested explicitly, via a dedicated instruction, otherwise an error is raised.\\nAn Interpreter must use the System to perform I\/O operations, and ultimately to allow a Program to communicate with the users; however, the implementation of the System depends on the actual application where the Processor and Interpreter are running, so it's left to be specified.\\nAdditional domains are defined for each architecture, so that a virtual machine can support many different architectures.\\nAssemblers and compilers also define their own domains.\\nThe following domains could thus be defined:\\n- `smf`: the core virtual machine framework domain\\n- `sma`: definitions for the SMA architecture domain\\n- `basm`: definitions for the BASM assembler domain\\n- `php`: definitions for the PHP compiler domain\\n### Application\\nThe application layer may define the following primary ports:\\n- the `vm` port allows to execute programs, according to the configured architecture\\n- the `repl` port allows to execute programs interactively, and uses the functionality of `vm`\\n- the `dbg` port allows to execute programs step by step for debugging, and uses the functionality of `vm`\\n- the `asm` port allows to run an assembler on some assembly code to produce executable object code\\n- the `cmp` port allows to run a compiler on some high level language to produce assembly code\\nAs far as secondary ports, we need the following:\\n- a `arcl` port allows the application to load an architecture definition\\n- a `pl` port allows the application to load a program\\n- a `asml` port allows the application to load assembly code, to be assembled\\n- a `cl` port allows the application to load high level code, to be compiled\\n- a `sys` port allows the application to interact with the underlying operating system\\n### Adapters\\nPrimary adapters might be defined to create command line applications, or Web applications. Secondary adapters might be defined to read data from files or from memory. See below for more concrete examples.\\n### Plugin architecture\\nWe want to support building different types of applications by composing together sets of different available plugins. For example:\\n**sloth machine (CLI)**\\nAD_sm + AD_larcl + AD_fpl + AD_ossys + AP_vm + AP_arcl + AP_pl + AP_sys + D_smf + D_sma (or others)\\n**sloth machine assembler (CLI)**\\nAD_asm + AD_fasml + AP_asm + AP_asml + D_basm (or others)\\n**sloth machine compiler (CLI)**\\nAD_cmp + AD_fcl + AD_masml + AP_cmp + AP_asm + AP_cl + AP_asml + D_basm (or others) + D_php (or others)\\n**sloth machine runner (CLI)**\\nAD_run + AD_larcl + AD_fcl + AD_masml + AD_mpl + AD_ossys + AP_vm + AP_cmp + AP_asm + AP_arcl + AP_cl + AP_asml + AP_pl + AP_sys + D_smf + D_sma (or others) + D_basm (or others) + D_php (or others)\\n**sloth machine REPL (CLI)**\\nAD_repl + AD_larcl + AD_mcl + AD_masml + AD_mpl + AD_ossys + AP_repl + AP_cmp + AP_asm + AP_arcl + AP_cl + AP_asml + AP_pl + AP_sys + D_smf + D_sma (or others) + D_basm (or others) + D_php (or others)\\n**sloth machine debugger (CLI)**\\nAD_dbg + AD_larcl + AD_fcl + AD_masml + AD_mpl + AD_ossys + AP_dbg + AP_cmp + AP_asm + AP_arcl + AP_cl + AP_asml + AP_pl + AP_sys + D_smf + D_sma (or others) + D_basm (or others) + D_php (or others)\\n**sloth machine Web (Web)**\\nAD_web + AD_warcl + AD_wcl + AD_masml + AD_mpl + AD_wsys + AP_vm + AP_cmp + AP_asm + AP_repl + AP_dbg + AP_arcl + AP_cl + AP_asml + AP_pl + AP_sys + D_smf + D_sma (or others) + D_basm (or others) + D_php (or others)\\n- `D_smf`: Domain Sloth Machine Framework\\n- `D_sma`: Domain Sloth Machine Architecture\\n- `D_basm`: Domain Basic Assembly for Sloth Machine\\n- `D_php`: Domain PHP\\n- `AP_vm`: Application Virtual Machine (primary port)\\n- `AP_cmp`: Application Compiler (primary port)\\n- `AP_asm`: Application Assembler (primary port)\\n- `AP_repl`: Application REPL (primary port)\\n- `AP_dbg`: Application Debugger (primary port)\\n- `AP_arcl`: Application Architecture Loader (secondary port)\\n- `AP_pl`: Application Program Loader (secondary port)\\n- `AP_asml`: Application Assembly Loader (secondary port)\\n- `AP_cl`: Application Code Loader (secondary port)\\n- `AP_sys`: Application System (secondary port)\\n- `AD_sm`: Adapter Sloth Machine (primary adapter)\\n- `AD_cmp`: Adapter Compiler (primary adapter)\\n- `AD_run`: Adapter Runner (primary adapter)\\n- `AD_repl`: Adapter REPL (primary adapter)\\n- `AD_dbg`: Adapter Debugger (primary adapter)\\n- `AD_web`: Adapter Web (primary adapter)\\n- `AD_larcl`: Adapter Local Architecture Loader (secondary adapter)\\n- `AD_warcl`: Adapter Web Architecture Loader (secondary adapter)\\n- `AD_fpl`: Adapter File Program Loader (secondary adapter)\\n- `AD_mpl`: Adapter Memory Program Loader (secondary adapter)\\n- `AD_fasml`: Adapter File Assembly Loader (secondary adapter)\\n- `AD_masml`: Adapter Memory Assembly Loader (secondary adapter)\\n- `AD_fcl`: Adapter File Code Loader (secondary adapter)\\n- `AD_mcl`: Adapter Memory Code Loader (secondary adapter)\\n- `AD_wcl`: Adapter Web Code Loader (secondary adapter)\\n- `AD_ossys`: Adapter OS System (secondary adapter)\\n- `AD_wsys`: Adapter Web System (secondary adapter)\\n### Example modules\\n```\\ndomain\\nsmf\\ndata\\nDataUnit: (Byte)\\nData: DataUnit[]\\nSize: (Integer)\\nAddress: (Integer)\\nprogram [data]\\nProgram\\nProgram(data.Data)\\nread(data.Address, data.Size): data.Data\\ninterpreter [data]\\nOpcode: data.Data\\nOperands: data.Data\\nExitStatus: (Integer)\\nInstruction\\nInstruction(Address, Opcode, Operands)\\ngetAddress(): Address\\ngetOpcode(): Opcode\\ngetOperands(): Operands\\nStatus\\nshouldJump(): (Boolean)\\ngetJumpAddress(): data.Address\\nshouldExit(): (Boolean)\\ngetExitStatus(): ExitStatus\\n<Interpreter>\\ngetOpcodeSize(): data.Size\\ngetOperandsSize(Opcode): data.Size\\nexec(Instruction): Status\\nprocessor [program, interpreter]\\nProcessor\\nProcessor(interpreter.<Interpreter>)\\nrun(program.Program): interpreter.ExitStatus\\narchitecture [data, interpreter]\\n<System>\\nread(data.Integer fd, data.Size size): data.Data\\nwrite(data.Integer fd, data.Data data, data.Size size): data.Size\\n<Architecture>\\ngetInterpreter(<System>): interpreter.<Interpreter>\\nsma [smf]\\nInterpreter: smf.interpreter.<Interpreter>\\nInterpreter(smf.architecture.<System>)\\napplication\\nvm\\nrun_program [domain.smf, application.arcl, application.pl, application.sys]\\n<Request>\\ngetArchitectureName(): String\\ngetProgramReference(): String\\nResponse\\ngetExitStatus(): domain.smf.interpreter.ExitStatus\\n<Presenter>\\npresent(Response)\\nRunProgram\\nRunProgram(ProcessorFactory, <Presenter>, application.arcl.<ArchitectureLoader>, application.pl.<ProgramLoader>, application.sys.<System>)\\nexec(<Request>)\\nProcessorFactory\\ncreate(domain.smf.interpreter.<Interpreter>): domain.smf.processor.Processor\\narcl [domain.smf]\\n<ArchitectureLoader>\\nload(architectureName: String): domain.smf.architecture.<Architecture>\\npl [domain.smf]\\n<ProgramLoader>\\nload(programReference: String): domain.smf.program.Program\\nsys [domain.smf]\\n<System>: domain.smf.architecture.<System>\\nadapters\\nsm [application.vm, domain.smf]\\nrun_program [application.vm, domain.smf]\\nRequest: application.vm.run_program.<Request>\\nController\\nController(application.vm.run_program.RunProgram)\\nrunProgram(Request)\\nViewModel\\nViewModel(domain.smf.interpreter.<ExitStatus>)\\ngetExitStatus(): <native-integer>\\n<View>\\nrender(ViewModel)\\nExitStatusView: <View>\\nrender(ViewModel)\\ngetExitStatus(): <native-integer>\\nPresenter: application.vm.run_program.<Presenter>\\nPresenter(<View>)\\npresent(application.vm.run_program.Response)\\nlarcl [application.arcl]\\nLocalArchitectureLoader: application.arcl.<ArchitectureLoader>\\nfpl [application.pl]\\nFileProgramLoader: application.pl.<ProgramLoader>\\nossys [application.sys]\\nOSSystem: application.sys.<System>\\n```\\n### Examples of main implementations\\n```\\nmodule domain.smf.processor\\nimport domain.smf.interpreter.<Interpreter>\\nimport domain.smf.program.Program\\nimport domain.smf.interpreter.ExitStatus\\nimport domain.smf.data.Size\\nimport domain.smf.data.Address\\nimport domain.smf.interpreter.Opcode\\nimport domain.smf.interpreter.Operands\\nimport domain.smf.interpreter.Instruction\\nimport domain.smf.interpreter.Status\\nclass Processor\\nProcessor(<Interpreter> interpreter)\\nthis.interpreter = interpreter\\nrun(Program program): ExitStatus\\nSize opcodeSize = interpreter.getOpcodeSize()\\nAddress address = 0\\nwhile (true)\\nOpcode opcode = program.read(address, opcodeSize)\\nSize operandsSize = interpreter.getOperandsSize(opcode)\\nAddress operandsAddress = address + opcodeSize\\nOperands operands = program.read(operandsAddress, operandsSize)\\nInstruction instruction = new Instruction(address, opcode, operands)\\nStatus status = interpreter.exec(instruction)\\nif (status.shouldExit())\\nreturn status.getExitStatus()\\naddress = status.shouldJump() ? status.getJumpAddress() : operandsAddress + operandsSize\\n\/\/ @todo: handle missing exit\\n```\\n```\\nmodule domain.sma.interpreter\\nimport domain.smf.interpreter.<Interpreter>\\nimport domain.smf.interpreter.ExitStatus\\nimport domain.smf.interpreter.Status\\nimport domain.smf.data.Size\\nimport domain.smf.data.Address\\nimport domain.smf.interpreter.Opcode\\nimport domain.smf.interpreter.Instruction\\nimport domain.smf.architecture.<System>\\nimport domain.sma.InstructionSet\\nimport domain.sma.Result\\nimport domain.sma.JumpResult\\nimport domain.sma.ExitResult\\nimport domain.sma.InstructionDefinition\\nclass Interpreter: <Interpreter>\\nInterpreter(InstructionSet instructionSet, <System> system)\\nthis.instructionSet = instructionSet\\nthis.system = system\\ngetOpcodeSize(): Size\\nreturn new Integer(1)\\ngetOperandsSize(Opcode opcode): Size\\nreturn instructionSet.getInstructionDefinition(opcode).getOperandsSize()\\nexec(Instruction instruction): Status\\nAddress jumpAddress = null\\nAddress exitStatus = null\\nInstructionDefinition definition = instructionSet.getInstructionDefinition(instruction.getOpcode())\\nResult result = definition.exec(instruction.getOperands())\\nif (result instanceof JumpResult)\\njumpAddress = result.getJumpAddress()\\nif (result instanceof ExitResult)\\nexitStatus = result.getExitStatus()\\nreturn new Status(jumpAddress, exitStatus)\\n```\\n```\\nmodule application.vm.run_program\\nimport application.vm.run_program.ProcessorFactory\\nimport application.vm.run_program.<Presenter>\\nimport application.arcl.<ArchitectureLoader>\\nimport application.pl.<ProgramLoader>\\nimport application.sys.<System>\\nimport application.vm.run_program.<Request>\\nimport domain.smf.architecture.<Architecture>\\nimport domain.smf.interpreter.<Interpreter>\\nimport domain.smf.processor.Processor\\nimport domain.smf.program.Program\\nimport domain.smf.interpreter.ExitStatus\\nimport application.vm.run_program.Response\\nclass RunProgram\\nRunProgram(\\nProcessorFactory processorFactory,\\n<Presenter> presenter,\\n<ArchitectureLoader> architectureLoader,\\n<ProgramLoader> programLoader,\\n<System> system\\n)\\nthis.processorFactory = processorFactory\\nthis.presenter = presenter\\nthis.architectureLoader = architectureLoader\\nthis.programLoader = programLoader\\nthis.system = system\\nexec(<Request> request)\\n<Architecture> architecture = architectureLoader.load(request.getArchitectureName())\\n<Interpreter> interpreter = architecture.getInterpreter(this.system)\\nProcessor processor = processorFactory.create(interpreter)\\nProgram program = programLoader.load(request.getProgramReference())\\nExitStatus exitStatus = processor.run(program)\\npresenter.present(new Response(exitStatus))\\n```\\n```\\nmodule adapters.sm.run_program\\nimport application.vm.run_program.RunProgram\\nimport application.vm.run_program.<Request>\\nclass Controller\\nController(RunProgram service)\\nthis.service = service\\nrunProgram(<Request> request)\\nservice.run(request)\\n```\\n```\\nmodule adapters.sm.run_program\\nimport adapters.sm.run_program.<View>\\nimport application.vm.run_program.Response\\nimport adapters.sm.run_program.ViewModel\\nclass Presenter\\nPresenter(<View> view)\\nthis.view = view\\npresent(Response response)\\nViewModel viewModel = new ViewModel(response.getExitStatus())\\nview.render(viewModel)\\n```\\n","MatchedIds":[1792,4361,4699,4408,4722],"Predicted":"## Decision\nWe will document our architectural decisions using Architecture Decision Records (ADR). This will ensure that even in an experimental environment, we have a record of the decisions made, the contexts in which they were made, and the reasons behind them. We will follow Michael Nygard's template as described in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions.","GenTime":"2024-08-19 23:58:31"}
{"File Name":"dilay\/0003-use-compile-api-to-transfter-class.md","Context":"## Context\\nhttps:\/\/github.com\/microsoft\/TypeScript\/wiki\/Using-the-Compiler-API\\nhttps:\/\/github.com\/dsherret\/ts-morph#readme\\n> TypeScript Compiler API wrapper. Provides an easier way to navigate and manipulate TypeScript and JavaScript code.\\n","Decision":"Decision here...\\n","tokens":54,"id":1337,"text":"## Context\\nhttps:\/\/github.com\/microsoft\/TypeScript\/wiki\/Using-the-Compiler-API\\nhttps:\/\/github.com\/dsherret\/ts-morph#readme\\n> TypeScript Compiler API wrapper. Provides an easier way to navigate and manipulate TypeScript and JavaScript code.\\n\n\n##Decision\nDecision here...\\n","MatchedIds":[5259,1410,452,1588,1404],"Predicted":"## Decision\nWe will use `ts-morph` as a wrapper around the TypeScript Compiler API for navigating and manipulating TypeScript and JavaScript code.\\nWe will leverage its simplified and convenient API to improve development efficiency and reduce boilerplate code.\\nWe will ensure that our team members are familiarized with `ts-morph` through internal documentation and training sessions.\\n","GenTime":"2024-08-19 23:58:36"}
{"File Name":"meadow\/0012-websockets.md","Context":"## Context\\nWe need a way to provide live updates to the front-end for ingest sheet validation,\\ningest status, etc.\\n","Decision":"We will use the [WebSocket API](https:\/\/www.w3.org\/TR\/websockets\/) via [Phoenix Channels](https:\/\/hexdocs.pm\/phoenix\/channels.html) to enable real-time communication between the client and server.\\n","tokens":27,"id":3845,"text":"## Context\\nWe need a way to provide live updates to the front-end for ingest sheet validation,\\ningest status, etc.\\n\n\n##Decision\nWe will use the [WebSocket API](https:\/\/www.w3.org\/TR\/websockets\/) via [Phoenix Channels](https:\/\/hexdocs.pm\/phoenix\/channels.html) to enable real-time communication between the client and server.\\n","MatchedIds":[847,3469,779,1701,3127],"Predicted":"## Decision\nWe will implement live updates to the front-end using WebSockets. \n\n* The server will establish a WebSocket connection with the client upon initial connection.\n* The server will push updates for ingest sheet validation and ingest status through this WebSocket connection in real-time.\n* We will use [Socket.IO](https:\/\/socket.io\/) for both server and client implementations due to its robustness and support for fallbacks in case WebSockets are not available.\n* This solution ensures low latency and efficient bi-directional communication between the server and the front-end.\n\nWe also considered the following alternative solutions:\n* Long Polling: While it can provide real-time updates, it is less efficient and more resource-intensive compared to WebSockets.\n* Server-Sent Events (SSE): Although simpler to implement, SSE is uni-directional and would not support bi-directional communication needs.","GenTime":"2024-08-19 23:58:41"}
{"File Name":"ftd-scratch3-offline\/0015-use-floats-for-storing-numbers-not-doubles-as-scratch-does.md","Context":"## Context\\nScratch uses Javascript's numbers type to store all numbers it uses. The Javascript number type is effectively a 64 bit double.\\nThe ftduino only supports 32-bit floats.\\n64-bit double emulation might be possible but slow and likely exhaust the available memory and program space.\\n","Decision":"We will use 32-bit floats to store any numbers.\\n","tokens":60,"id":2631,"text":"## Context\\nScratch uses Javascript's numbers type to store all numbers it uses. The Javascript number type is effectively a 64 bit double.\\nThe ftduino only supports 32-bit floats.\\n64-bit double emulation might be possible but slow and likely exhaust the available memory and program space.\\n\n\n##Decision\nWe will use 32-bit floats to store any numbers.\\n","MatchedIds":[2637,2634,2629,2687,5198],"Predicted":"## Decision\nWe will convert all numbers used in Scratch to 32-bit floats when interfacing with the ftduino. This approach ensures compatibility with the hardware constraints and avoids the overhead of 64-bit double emulation.","GenTime":"2024-08-19 23:58:44"}
{"File Name":"android-guidelines\/0007-use-deprecated-annotation-with-description.md","Context":"## Context and Problem Statement\\nDeprecated annotation usages without any comment or suggestion makes no sense when using existing classes.\\n","Decision":"When we decide to use deprecated annotation in classes, we need to comment reason and alternative to that implementation.\\n","tokens":24,"id":3863,"text":"## Context and Problem Statement\\nDeprecated annotation usages without any comment or suggestion makes no sense when using existing classes.\\n\n\n##Decision\nWhen we decide to use deprecated annotation in classes, we need to comment reason and alternative to that implementation.\\n","MatchedIds":[1193,4497,160,4556,4695],"Predicted":"## Decision\n### Strategy for Deprecated Annotations\nTo ensure deprecated annotations are clear and helpful, follow these rules:\n\n1. **Mandatory Comment**: Every deprecated annotation must include a comment explaining why the item is deprecated.\n    - :white_check_mark: Valid\n    ```java\n    \/**\n     * @deprecated use NewClass instead because this class has performance issues.\n     *\/\n    @Deprecated\n    public class OldClass {\n        \/\/...\n    }\n    ```\n    - :x: Invalid\n    ```java\n    @Deprecated\n    public class OldClass {\n        \/\/...\n    }\n    ```\n\n2. **Suggested Alternatives**: Always provide an alternative solution or class to use.\n    - :white_check_mark: Valid\n    ```java\n    \/**\n     * @deprecated use NewMethod() instead due to better performance.\n     *\/\n    @Deprecated\n    public void oldMethod() {\n        \/\/old implementation\n    }\n    ```\n    - :x: Invalid\n    ```java\n    \/**\n     * @deprecated\n     *\/\n    @Deprecated\n    public void oldMethod() {\n        \/\/old implementation\n    }\n    ```\n\n3. **Deprecation Timeline**: If applicable, include a date or version when the item was deprecated.\n    - :white_check_mark: Valid\n    ```java\n    \/**\n     * @deprecated since version 2.0, use NewFeature()\n     *\/\n    @Deprecated\n    public void deprecatedFeature() {\n        \/\/...\n    }\n    ```\n    - :x: Invalid\n    ```java\n    \/**\n     * @deprecated use NewFeature()\n     *\/\n    @Deprecated\n    public void deprecatedFeature() {\n        \/\/...\n    }\n    ```\n\n4. **Reasons for Deprecation**: Include detailed explanations about the reasons for deprecation.\n    - :white_check_mark: Valid\n    ```java\n    \/**\n     * @deprecated because this method can cause memory leaks. Use updatedSafeMethod() instead.\n     *\/\n    @Deprecated\n    public void riskyMethod() {\n        \/\/...\n    }\n    ```\n    - :x: Invalid\n    ```java\n    \/**\n     * @deprecated\n     *\/\n    @Deprecated\n    public void riskyMethod() {\n        \/\/...\n    }\n    ```\n\n### Synopsys\nBy following these guidelines, the usage of deprecated annotations will be informative and provide clear guidance to developers on how to transition away from deprecated classes, methods, or fields. This practice ensures better maintenance and evolution of the codebase.","GenTime":"2024-08-19 23:58:57"}
{"File Name":"umbrella\/0003-component-configuration-via-context.md","Context":"## Context\\nAn alternative configuration procedure to ADR-0002, possibly better\\nsuited for dynamic theming, theme changes and separating the component\\nconfiguration between behavioral and stylistic aspects. This new\\napproach utilizes the hdom context object to retrieve theme attributes,\\nwhereas the previous solution ignored the context object entirely.\\nA live demo of the code discussed here is available at:\\n[demo.thi.ng\/umbrella\/hdom-theme-adr-0003](https:\/\/demo.thi.ng\/umbrella\/hdom-theme-adr-0003)\\n","Decision":"### Split component configuration\\n#### Behavioral aspects\\nComponent pre-configuration options SHOULD purely consist of behavioral\\nsettings and NOT include any aesthetic \/ theme oriented options. To\\nbetter express this intention, it's recommended to suffix these\\ninterface names with `Behavior`, e.g. `ButtonBehavior`.\\n```ts\\ninterface ButtonBehavior {\\n\/**\\n* Element name to use for enabled buttons.\\n* Default: \"a\"\\n*\/\\ntag: string;\\n\/**\\n* Element name to use for disabled buttons.\\n* Default: \"span\"\\n*\/\\ntagDisabled: string;\\n\/**\\n* Default attribs, always injected for active button states\\n* and overridable at runtime.\\n* Default: `{ href: \"#\", role: \"button\" }`\\n*\/\\nattribs: IObjectOf<any>;\\n}\\n```\\n#### Theme stored in hdom context\\nEven though there's work underway to develop a flexble theming system\\nfor hdom components, the components themselves SHOULD be agnostic to\\nthis and only expect to somehow obtain styling attributes from the hdom\\ncontext object passed to each component function. How is shown further\\nbelow.\\nIn this example we define a `theme` key in the context object, under\\nwhich theme options for all participating components are stored.\\n```ts\\nconst ctx = {\\n...\\ntheme: {\\nprimaryButton: {\\ndefault: { class: ... },\\ndisabled: { class: ... },\\nselected: { class: ... },\\n},\\nsecondaryButton: {\\ndefault: { class: ... },\\ndisabled: { class: ... },\\nselected: { class: ... },\\n},\\n...\\n}\\n};\\n```\\n### Component definition\\n```ts\\nimport { getIn, Path } from \"@thi.ng\/paths\";\\n\/**\\n* Instance specific runtime args. All optional.\\n*\/\\ninterface ButtonArgs {\\n\/**\\n* Click event handler to be wrapped with preventDefault() call\\n*\/\\nonclick: EventListener;\\n\/**\\n* Disabled flag. Used to determine themed version.\\n*\/\\ndisabled: boolean;\\n\/**\\n* Selected flag. Used to determine themed version.\\n*\/\\nselected: boolean;\\n\/**\\n* Link target.\\n*\/\\nhref: string;\\n}\\nconst button = (themeCtxPath: Path, behavior?: Partial<ButtonBehavior>) => {\\n\/\/ init with defaults\\nbehavior = {\\ntag: \"a\",\\ntagDisabled: \"span\",\\n...behavior\\n};\\nbehavior.attribs = { href: \"#\", role: \"button\", ...behavior.attribs };\\n\/\/ return component function as closure\\nreturn (ctx: any, args: Partial<ButtonArgs>, ...body: any[]) => {\\n\/\/ lookup component theme config in context\\nconst theme = getIn(ctx, themeCtxPath);\\nif (args.disabled) {\\nreturn [behavior.tagDisabled, {\\n...behavior.attribs,\\n...theme.disabled,\\n...args,\\n}, ...body];\\n} else {\\nconst attribs = {\\n...behavior.attribs,\\n...theme[args.selected ? \"selected\" : \"default\"],\\n...args\\n};\\nif (args && args.onclick && (args.href == null || args.href === \"#\")) {\\nattribs.onclick = (e) => (e.preventDefault(), args.onclick(e));\\n}\\nreturn [behavior.tag, attribs, ...body];\\n}\\n};\\n};\\n```\\n### Component usage\\n```ts\\nconst darkTheme = {\\nid: \"dark\",\\nbody: {\\nclass: \"vh-100 bg-black moon-gray pa3 sans-serif\"\\n},\\nlink: {\\nclass: \"link dim b light-silver\"\\n},\\nbutton: {\\ndefault: {\\nclass: \"dib link mr2 ph3 pv2 blue hover-lightest-blue hover-b--current br3 ba b--blue\"\\n},\\nselected: {\\nclass: \"dib link mr2 ph3 pv2 red hover-gold hover-b--current br3 ba b--red\"\\n},\\ndisabled: {\\nclass: \"dib mr2 ph3 pv2 mid-gray br3 ba b--mid-gray\"\\n}\\n}\\n};\\nconst bt = button(\"theme.button\");\\nconst btFixed = button(\"theme.button\", { attribs: { style: { width: \"8rem\" } } });\\nconst app = (ctx) =>\\n[\"div\", ctx.theme.body,\\n[bt, { onclick: () => alert(\"toggle\") }, \"Toggle\"],\\n[bt, { href: \"https:\/\/github.com\/thi-ng\/umbrella\" }, \"External\"],\\n[btFixed, { onclick: () => alert(\"hi\"), selected: true }, \"Selected\"],\\n[btFixed, { disabled: true }, \"Disabled\"] ];\\n\/\/ start app with theme in context\\nstart(\"app\", app, { theme: darkTheme })\\n```\\n","tokens":117,"id":5185,"text":"## Context\\nAn alternative configuration procedure to ADR-0002, possibly better\\nsuited for dynamic theming, theme changes and separating the component\\nconfiguration between behavioral and stylistic aspects. This new\\napproach utilizes the hdom context object to retrieve theme attributes,\\nwhereas the previous solution ignored the context object entirely.\\nA live demo of the code discussed here is available at:\\n[demo.thi.ng\/umbrella\/hdom-theme-adr-0003](https:\/\/demo.thi.ng\/umbrella\/hdom-theme-adr-0003)\\n\n\n##Decision\n### Split component configuration\\n#### Behavioral aspects\\nComponent pre-configuration options SHOULD purely consist of behavioral\\nsettings and NOT include any aesthetic \/ theme oriented options. To\\nbetter express this intention, it's recommended to suffix these\\ninterface names with `Behavior`, e.g. `ButtonBehavior`.\\n```ts\\ninterface ButtonBehavior {\\n\/**\\n* Element name to use for enabled buttons.\\n* Default: \"a\"\\n*\/\\ntag: string;\\n\/**\\n* Element name to use for disabled buttons.\\n* Default: \"span\"\\n*\/\\ntagDisabled: string;\\n\/**\\n* Default attribs, always injected for active button states\\n* and overridable at runtime.\\n* Default: `{ href: \"#\", role: \"button\" }`\\n*\/\\nattribs: IObjectOf<any>;\\n}\\n```\\n#### Theme stored in hdom context\\nEven though there's work underway to develop a flexble theming system\\nfor hdom components, the components themselves SHOULD be agnostic to\\nthis and only expect to somehow obtain styling attributes from the hdom\\ncontext object passed to each component function. How is shown further\\nbelow.\\nIn this example we define a `theme` key in the context object, under\\nwhich theme options for all participating components are stored.\\n```ts\\nconst ctx = {\\n...\\ntheme: {\\nprimaryButton: {\\ndefault: { class: ... },\\ndisabled: { class: ... },\\nselected: { class: ... },\\n},\\nsecondaryButton: {\\ndefault: { class: ... },\\ndisabled: { class: ... },\\nselected: { class: ... },\\n},\\n...\\n}\\n};\\n```\\n### Component definition\\n```ts\\nimport { getIn, Path } from \"@thi.ng\/paths\";\\n\/**\\n* Instance specific runtime args. All optional.\\n*\/\\ninterface ButtonArgs {\\n\/**\\n* Click event handler to be wrapped with preventDefault() call\\n*\/\\nonclick: EventListener;\\n\/**\\n* Disabled flag. Used to determine themed version.\\n*\/\\ndisabled: boolean;\\n\/**\\n* Selected flag. Used to determine themed version.\\n*\/\\nselected: boolean;\\n\/**\\n* Link target.\\n*\/\\nhref: string;\\n}\\nconst button = (themeCtxPath: Path, behavior?: Partial<ButtonBehavior>) => {\\n\/\/ init with defaults\\nbehavior = {\\ntag: \"a\",\\ntagDisabled: \"span\",\\n...behavior\\n};\\nbehavior.attribs = { href: \"#\", role: \"button\", ...behavior.attribs };\\n\/\/ return component function as closure\\nreturn (ctx: any, args: Partial<ButtonArgs>, ...body: any[]) => {\\n\/\/ lookup component theme config in context\\nconst theme = getIn(ctx, themeCtxPath);\\nif (args.disabled) {\\nreturn [behavior.tagDisabled, {\\n...behavior.attribs,\\n...theme.disabled,\\n...args,\\n}, ...body];\\n} else {\\nconst attribs = {\\n...behavior.attribs,\\n...theme[args.selected ? \"selected\" : \"default\"],\\n...args\\n};\\nif (args && args.onclick && (args.href == null || args.href === \"#\")) {\\nattribs.onclick = (e) => (e.preventDefault(), args.onclick(e));\\n}\\nreturn [behavior.tag, attribs, ...body];\\n}\\n};\\n};\\n```\\n### Component usage\\n```ts\\nconst darkTheme = {\\nid: \"dark\",\\nbody: {\\nclass: \"vh-100 bg-black moon-gray pa3 sans-serif\"\\n},\\nlink: {\\nclass: \"link dim b light-silver\"\\n},\\nbutton: {\\ndefault: {\\nclass: \"dib link mr2 ph3 pv2 blue hover-lightest-blue hover-b--current br3 ba b--blue\"\\n},\\nselected: {\\nclass: \"dib link mr2 ph3 pv2 red hover-gold hover-b--current br3 ba b--red\"\\n},\\ndisabled: {\\nclass: \"dib mr2 ph3 pv2 mid-gray br3 ba b--mid-gray\"\\n}\\n}\\n};\\nconst bt = button(\"theme.button\");\\nconst btFixed = button(\"theme.button\", { attribs: { style: { width: \"8rem\" } } });\\nconst app = (ctx) =>\\n[\"div\", ctx.theme.body,\\n[bt, { onclick: () => alert(\"toggle\") }, \"Toggle\"],\\n[bt, { href: \"https:\/\/github.com\/thi-ng\/umbrella\" }, \"External\"],\\n[btFixed, { onclick: () => alert(\"hi\"), selected: true }, \"Selected\"],\\n[btFixed, { disabled: true }, \"Disabled\"] ];\\n\/\/ start app with theme in context\\nstart(\"app\", app, { theme: darkTheme })\\n```\\n","MatchedIds":[157,1855,1125,4498,381],"Predicted":"## Decision\nConsidering that the new approach leverages the hdom context object to retrieve theme attributes, which offers improved options for dynamic theming and better separation between behavioral and stylistic aspects, we have decided to adopt the new configuration procedure outlined in ADR-0003. \n\n**Justification:**\n\n- **Dynamic Theming**: Utilizing the hdom context object allows for more dynamic theme changes at runtime without the need to reinitialize components.\n- **Separation of Concerns**: This approach clearly separates behavioral attributes from stylistic ones, enhancing maintainability and readability of the codebase.\n- **Consistency**: By fetching theme attributes directly from the context, consistency in theming can be ensured across the application.\n- **Future-Proofing**: Adopting a more flexible and modern approach will help future-proof the codebase against upcoming features and changes.\n\nWe will transition to this new procedure and deprecate the previous ADR-0002 configuration process. Detailed migration steps and examples will be documented and provided to the development team to ensure a smooth transition.","GenTime":"2024-08-19 23:59:01"}
{"File Name":"buy-for-your-school\/0013-use-dfe-sign-in-as-auth-provider.md","Context":"## Context\\nThe service needs a way to authenticate trusted school buying professionals and to restrict the majority of access to the public.\\nWe believe a simpler password-less authentication mechanism would be all that's required. This service does not need any of the school and user information held within DfE Sign-in (DSI). DfE governance has reviewed our concern and decided this service should use DSI.\\nThere is currently no formal recommendation for a tool of choice in the technical guidance https:\/\/github.com\/DFE-Digital\/technical-guidance.\\nWe want a tool that provides an open and modern security standard.\\n","Decision":"We are going to use DSI as our single sign-on provider using the OIDC standard.\\n","tokens":126,"id":1252,"text":"## Context\\nThe service needs a way to authenticate trusted school buying professionals and to restrict the majority of access to the public.\\nWe believe a simpler password-less authentication mechanism would be all that's required. This service does not need any of the school and user information held within DfE Sign-in (DSI). DfE governance has reviewed our concern and decided this service should use DSI.\\nThere is currently no formal recommendation for a tool of choice in the technical guidance https:\/\/github.com\/DFE-Digital\/technical-guidance.\\nWe want a tool that provides an open and modern security standard.\\n\n\n##Decision\nWe are going to use DSI as our single sign-on provider using the OIDC standard.\\n","MatchedIds":[2041,2405,5211,4857,2582],"Predicted":"## Decision\n\nWe will use the DfE Sign-in (DSI) service for authenticating trusted school buying professionals as per DfE governance review. While the decision to use DSI was mandated, we acknowledge that DSI aligns with our requirement of using an open and modern security standard.\n\n### Positive Consequences\n\n* **Compliance with Governance:** Using DSI ensures compliance with DfE governance, avoiding any regulatory or policy-related complications.\n* **Security Standards:** DSI adheres to modern security standards, ensuring our authentication mechanism is secure and robust.\n* **Centralized Management:** Utilizing DSI enables centralized management of authentication, simplifying administration and potentially integrating with other DfE services in the future.\n* **No Need for User Information:** Since we do not require specific school and user information from DSI, we can leverage the existing DSI infrastructure without unnecessary data requirements.\n\n### Negative Consequences\n\n* **Limited Customization:** As we will be using a mandated service, our ability to customize the user experience, including potential password-less authentication features, may be limited.\n* **Dependency on External Service:** Relying on DSI means our service's authentication mechanism is dependent on the availability and uptime of the DSI service.\n* **Initial Integration Effort:** There may be some initial effort and potential delay involved in integrating our service with DSI, depending on the existing DSI infrastructure and APIs.\n","GenTime":"2024-08-19 23:59:09"}
{"File Name":"application-services\/0004-early-startup-experiments.md","Context":"## Context and Problem Statement\\nAs an experimenter, I would like to run experiments early on a user's first run of the application. However, the experiment data is only available on the second run. We would like to have that experiment data available before the user's first run.\\nFor more information: https:\/\/docs.google.com\/document\/d\/1Qw36_7G6XyHvJZdM-Hxh4nqYZyCsYajG0L5mO33Yd5M\/edit\\n## Decision Drivers\\n* Availability of experiments early on the first run\\n* No impact on experimentation data analysis\\n* Flexibility in creating experiments\\n* Ability to quickly disable experiments\\n* Simplicity of releases\\n* Mobile's expectations of Nimbus (The SDK should be idempotent)\\n","Decision":"* Availability of experiments early on the first run\\n* No impact on experimentation data analysis\\n* Flexibility in creating experiments\\n* Ability to quickly disable experiments\\n* Simplicity of releases\\n* Mobile's expectations of Nimbus (The SDK should be idempotent)\\nNone of the options were feasible, so for now we are sticking with option **(A) Do Nothing** until there are experiments planned that are expected to run on early startup on the first run, then we will revaluate our options.\\nThe **(B) Bundle Experiment data with app on release** option was rejected mainly due to difficulty in disabling experiments and pausing enrollments. This can create a negative user experience as it prevents us from disabling any problematic experiments. Additionally, it ties experiment creation with application release cycles.\\nThe **(C) Retrieve Experiment data on first run, and deal with delay** option was rejected due to the fact it changes the Nimbus SDK will no longer be idempotent,and the possibility of introducing undesirable UI.\\n","tokens":170,"id":2067,"text":"## Context and Problem Statement\\nAs an experimenter, I would like to run experiments early on a user's first run of the application. However, the experiment data is only available on the second run. We would like to have that experiment data available before the user's first run.\\nFor more information: https:\/\/docs.google.com\/document\/d\/1Qw36_7G6XyHvJZdM-Hxh4nqYZyCsYajG0L5mO33Yd5M\/edit\\n## Decision Drivers\\n* Availability of experiments early on the first run\\n* No impact on experimentation data analysis\\n* Flexibility in creating experiments\\n* Ability to quickly disable experiments\\n* Simplicity of releases\\n* Mobile's expectations of Nimbus (The SDK should be idempotent)\\n\n\n##Decision\n* Availability of experiments early on the first run\\n* No impact on experimentation data analysis\\n* Flexibility in creating experiments\\n* Ability to quickly disable experiments\\n* Simplicity of releases\\n* Mobile's expectations of Nimbus (The SDK should be idempotent)\\nNone of the options were feasible, so for now we are sticking with option **(A) Do Nothing** until there are experiments planned that are expected to run on early startup on the first run, then we will revaluate our options.\\nThe **(B) Bundle Experiment data with app on release** option was rejected mainly due to difficulty in disabling experiments and pausing enrollments. This can create a negative user experience as it prevents us from disabling any problematic experiments. Additionally, it ties experiment creation with application release cycles.\\nThe **(C) Retrieve Experiment data on first run, and deal with delay** option was rejected due to the fact it changes the Nimbus SDK will no longer be idempotent,and the possibility of introducing undesirable UI.\\n","MatchedIds":[1819,1823,3529,1799,669],"Predicted":"## Decision\nWe will implement a method to pre-fetch experiment data during the app installation phase, ensuring that the experiment data is available before the user's first run.\n\n### Approach\n1. **Pre-Fetch Experiment Data**: During the app installation process, leverage background services or initial setup routines to fetch and store experiment data. This ensures that when the user launches the app for the first time, the experiment data is already available.\n2. **Idempotent SDK Design**: Ensure that the Nimbus SDK remains idempotent by designing the fetching process in a way that it neither disrupts nor duplicates experiment-related operations. The SDK should gracefully handle re-fetching the same data if necessary.\n3. **Configurable Fetch Timing**: Allow configuration to define when and how often the experiment data should be fetched, providing flexibility in experimentation schedules.\n4. **Quick Experiment Disabling**: Introduce a mechanism to quickly update or disable experiments on the server-side, ensuring real-time updates without requiring a new app release.\n\n### Positive Consequences\n* Experiment data will be available on the first run, meeting the primary requirement.\n* No additional releases are needed for experiments, simplifying experiment management.\n* Maintains the integrity of experimentation data as it does not involve client-side manipulations after installation.\n* Provides developers with the flexibility to create and manage experiments without waiting for app updates.\n\n### Negative Consequences\n* Initial app installation may take slightly longer due to the fetching process, although this will typically be imperceptible to the user.\n* Requires robust error handling and retry mechanisms to ensure that data fetching does not fail silently.\n* Possible initial increase in server load due to pre-fetching activities during the installation phase, which will need to be managed.\n\n### Implementation Considerations\n* Ensure efficient and lightweight data fetching algorithms to minimize performance impacts.\n* Monitor initial fetching errors and adapt retry logic as necessary.\n* Include logging and analytics to track the success rate of pre-fetch operations and any potential impacts on user experience.\n\nBy adopting this approach, we can achieve the availability of experiment data on the first run without compromising on the flexibility and analysis aspects of our experiments.","GenTime":"2024-08-19 23:59:18"}
{"File Name":"adr-viewer\/0006-accessibility-as-a-first-class-concern.md","Context":"## Context\\nThis tool had, up until this point, made assumptions about how its users might interpret the information it presents (for example, using colour as the main mechanism of distinguishing record types)\\n","Decision":"Accessibility will now be a first-class concern of this project. All future design decisions should bear this in mind.\\n","tokens":40,"id":3815,"text":"## Context\\nThis tool had, up until this point, made assumptions about how its users might interpret the information it presents (for example, using colour as the main mechanism of distinguishing record types)\\n\n\n##Decision\nAccessibility will now be a first-class concern of this project. All future design decisions should bear this in mind.\\n","MatchedIds":[3784,4428,3119,1436,294],"Predicted":"## Decision\nWe will implement user accessibility guidelines to ensure that information is clearly conveyed through multiple means, such as adding text labels and icons, in addition to using color. This approach aligns with best practices as outlined in the Web Content Accessibility Guidelines (WCAG).","GenTime":"2024-08-19 23:59:20"}
{"File Name":"documents\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the future architectural decisions made.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":14,"id":138,"text":"## Context\\nWe need to record the future architectural decisions made.\\n\n\n##Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","MatchedIds":[2261,4428,3119,1436,294],"Predicted":"## Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions.","GenTime":"2024-08-19 23:59:22"}
{"File Name":"containers\/2-.md","Context":"## Context\\n======\\nFedora has minimal base image too, and it will typically have more current versions of any software we need compared to the Ubuntu images.\\n","Decision":"======\\nChange all Dockerfiles to use the latest `fedora-minimal` image as the base and update any necessary commands to work with Fedora system tools like `dnf`.\\n","tokens":34,"id":2619,"text":"## Context\\n======\\nFedora has minimal base image too, and it will typically have more current versions of any software we need compared to the Ubuntu images.\\n\n\n##Decision\n======\\nChange all Dockerfiles to use the latest `fedora-minimal` image as the base and update any necessary commands to work with Fedora system tools like `dnf`.\\n","MatchedIds":[923,3868,2393,3920,4096],"Predicted":"## Decision\nWe will use Fedora minimal base images for our container deployments. This decision is driven by the need for more current software versions, which Fedora typically provides compared to Ubuntu images. Using Fedora will ensure that our applications benefit from the latest features and security updates, providing a more up-to-date environment.","GenTime":"2024-08-19 23:59:25"}
{"File Name":"TANF-app\/001-ADR-template.md","Context":"## Context\\nWe need to record the architectural decisions made on this project. This context section should include the history and driving reason(s) on why a decision needed to be made.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":37,"id":3784,"text":"## Context\\nWe need to record the architectural decisions made on this project. This context section should include the history and driving reason(s) on why a decision needed to be made.\\n\n\n##Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","MatchedIds":[4428,3119,1436,294,2583],"Predicted":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions). \nThis will help us maintain a clear history and rationale behind each architectural decision.","GenTime":"2024-08-19 23:59:26"}
{"File Name":"Horace\/0017-separate-absolute-and-relative-indexing-APIs-in-pixel-array.md","Context":"## Context\\nThe pixel array within an SQW object can be too large to fit into memory.\\nTo avoid running out of memory, the object holding the pixel array can be\\nfile-backed.\\nThis means that only a \"page\" of the pixel array is loaded into memory at any\\none time.\\nTherefore two possible ways to index into the pixel array exist:\\n1. **Absolute index**:\\n_The position of the pixel in the full, file-backed, pixel array_.\\n2. **Relative index**:\\n_The position of the pixel in the currently loaded page of pixel data._\\n","Decision":"There will be two separate APIs for accessing data,\\nthese APIs distinguish between the two types of indexing.\\n1. **Perform absolute indexing  using `get_` methods:**\\nObtaining a subset of pixels or pixel data by absolute index will be\\npossible using a `get_` method.\\nFor example, the following will retrieve pixels 100-200 by absolute index:\\n```matlab\\npixels.get_pixels(100:200)\\n```\\nSimilarly, to retrieve a range of data from particular pixel array fields:\\n```matlab\\npixels.get_data({'signal', 'variance'}, 100:200)\\n```\\nAt time of writing, there are no plans to implement similar `set_`\\nmethods with absolute indexing.\\nIf there becomes a requirement for setters with absolute indexing,\\nthese should follow a similar syntax.\\nFor example, the following should set signal and variance from absolute\\nindex 100 to 200, to zero:\\n```matlab\\nset_data({'signal', 'variance'}, 100:200, zeros(2, 101));\\n```\\n2. **Perform relative indexing using attribute:**\\nObtaining pixel data using an attribute will return just the data for the\\ncurrently cached page.\\nHence, indexing into these attributes will be relative.\\nFor example, the following will retrieve the signal values of pixels 10-20\\nin the currently cached page:\\n```matlab\\npixels.signal(10:20)\\n```\\nSetting of pixel data will be possible using attributes and relative\\nindexing.\\nFor example the following will set pixels 1 to 20 on the current page to\\nzero:\\n```matlab\\npixels.signal(1:20) = 0\\n```\\n","tokens":128,"id":4236,"text":"## Context\\nThe pixel array within an SQW object can be too large to fit into memory.\\nTo avoid running out of memory, the object holding the pixel array can be\\nfile-backed.\\nThis means that only a \"page\" of the pixel array is loaded into memory at any\\none time.\\nTherefore two possible ways to index into the pixel array exist:\\n1. **Absolute index**:\\n_The position of the pixel in the full, file-backed, pixel array_.\\n2. **Relative index**:\\n_The position of the pixel in the currently loaded page of pixel data._\\n\n\n##Decision\nThere will be two separate APIs for accessing data,\\nthese APIs distinguish between the two types of indexing.\\n1. **Perform absolute indexing  using `get_` methods:**\\nObtaining a subset of pixels or pixel data by absolute index will be\\npossible using a `get_` method.\\nFor example, the following will retrieve pixels 100-200 by absolute index:\\n```matlab\\npixels.get_pixels(100:200)\\n```\\nSimilarly, to retrieve a range of data from particular pixel array fields:\\n```matlab\\npixels.get_data({'signal', 'variance'}, 100:200)\\n```\\nAt time of writing, there are no plans to implement similar `set_`\\nmethods with absolute indexing.\\nIf there becomes a requirement for setters with absolute indexing,\\nthese should follow a similar syntax.\\nFor example, the following should set signal and variance from absolute\\nindex 100 to 200, to zero:\\n```matlab\\nset_data({'signal', 'variance'}, 100:200, zeros(2, 101));\\n```\\n2. **Perform relative indexing using attribute:**\\nObtaining pixel data using an attribute will return just the data for the\\ncurrently cached page.\\nHence, indexing into these attributes will be relative.\\nFor example, the following will retrieve the signal values of pixels 10-20\\nin the currently cached page:\\n```matlab\\npixels.signal(10:20)\\n```\\nSetting of pixel data will be possible using attributes and relative\\nindexing.\\nFor example the following will set pixels 1 to 20 on the current page to\\nzero:\\n```matlab\\npixels.signal(1:20) = 0\\n```\\n","MatchedIds":[3090,2508,2896,2145,4234],"Predicted":"## Decision\nWe decided to implement the **absolute index** approach for indexing into the pixel array. This decision was made based on the following considerations:\n\n1. **Consistency**: Using an absolute index provides a consistent way to reference any pixel in the entire array, which simplifies operations that need to access multiple pages of data.\n\n2. **Ease of Use**: Absolute indexing is more intuitive for developers and users, as it directly relates to the original data set and does not require additional computations to translate between relative page indexes and absolute positions.\n\n3. **Performance**: While relative indexing might seem to offer performance benefits within a page, the cost of translating between relative and absolute indexes in higher-level operations could negate these benefits. Absolute indexing avoids this overhead.\n\n4. **Scalability**: As the size of the data grows and more pages are needed, absolute indexing scales more naturally because operations can always refer to a single indexing system without concern for page boundaries.\n\n5. **Simplicity in Implementation**: Absolute indexing simplifies the underlying implementation as it requires fewer adjustments when loading different pages into memory, thus reducing potential errors and improving maintainability.\n\nTo handle the loading of pages efficiently, we will incorporate a paging mechanism that can dynamically load and unload pages based on the absolute index being accessed. This ensures that memory usage remains within limits while providing seamless access to any part of the pixel array.","GenTime":"2024-08-19 23:59:33"}
{"File Name":"sre-challenge\/0004-using-aws-ecs-as-platform.md","Context":"## Context\\nI want to use docker at AWS but I don't want to provision a docker\\ninfrastructure myself.  So I'm going to use AWS ECS managed service and create\\na cluster.\\n","Decision":"Done.\\n","tokens":42,"id":2997,"text":"## Context\\nI want to use docker at AWS but I don't want to provision a docker\\ninfrastructure myself.  So I'm going to use AWS ECS managed service and create\\na cluster.\\n\n\n##Decision\nDone.\\n","MatchedIds":[2471,4358,4937,1464,319],"Predicted":"## Decision\nWe will use Amazon ECS (Elastic Container Service) in conjunction with Fargate to manage our containerized applications. This decision allows us to leverage a fully managed container orchestration service without the need to provision or manage the underlying infrastructure.\n\n### Details:\n1. **Cluster Management:**\n   - Create an ECS cluster that will host our containerized applications.\n   - Use Fargate launch type to run containers directly without provisioning or managing EC2 instances.\n   \n2. **Container Definitions:**\n   - Define our application containers using ECS Task Definitions. Each task definition will describe one or more containers that constitute our application.\n\n3. **Service Configuration:**\n   - Configure ECS services to manage and maintain the desired number of task containers. This ensures high availability and resilience.\n\n4. **Networking:**\n   - Utilize AWS VPC for network isolation and security.\n   - Employ Application Load Balancer (ALB) where needed to distribute traffic among containers.\n\n5. **Monitoring and Logging:**\n   - Use AWS CloudWatch for monitoring container performance and logging container output.\n\n6. **Deployment Pipeline:**\n   - Integrate with AWS CodePipeline and AWS CodeBuild to automate the build and deployment process of Docker images to the ECS cluster.\n   - Store Docker images in Amazon ECR (Elastic Container Registry) for versioning and management.\n\n### Configuration Requirements:\n- ECS cluster creation via the AWS Management Console, AWS CLI, or IaC tools like Terraform or CloudFormation.\n- Proper configuration of IAM roles and policies to allow ECS tasks to interact with AWS services.\n\nThis approach simplifies infrastructure management while leveraging AWS\u2019s managed services for scalability, reliability, and cost efficiency.","GenTime":"2024-08-19 23:59:40"}
{"File Name":"lobiani\/0005-use-cqrs-architectural-style.md","Context":"## Context\\nIn non-trivial projects with complex business rules, it is often a challenge to come up with the model that scales well\\nfor reads and writes at the same time in regard to performance and maintainability.\\n","Decision":"We will adopt Command Query Responsibility Segregation architectural style where there may exist 2 models for same\\ndomain entities each respectively on the Command (write) and Query (read) sides\\n","tokens":45,"id":1031,"text":"## Context\\nIn non-trivial projects with complex business rules, it is often a challenge to come up with the model that scales well\\nfor reads and writes at the same time in regard to performance and maintainability.\\n\n\n##Decision\nWe will adopt Command Query Responsibility Segregation architectural style where there may exist 2 models for same\\ndomain entities each respectively on the Command (write) and Query (read) sides\\n","MatchedIds":[1797,1792,4021,1246,2383],"Predicted":"## Decision\nTo address the need for scalability, performance, and maintainability in non-trivial projects with complex business rules, we will adopt the Command Query Responsibility Segregation (CQRS) pattern. This approach allows us to separate the read and write operations into different models, each optimized for its specific operation.\n\n- **Command Model**: This model handles the write operations. It will ensure that all business rules and validations are enforced during data modifications.\n- **Query Model**: This model handles the read operations. It will be optimized for querying and retrieving data efficiently, without the overhead of write-related concerns.\n\nBy separating the responsibilities of read and write operations, the CQRS pattern will enable us to scale each part independently, enhancing performance and maintainability. Additionally, we will utilize event sourcing where necessary to capture all changes to the application's state as a sequence of events, ensuring a robust mechanism for auditing and historical data reconstruction.","GenTime":"2024-08-19 23:59:44"}
{"File Name":"testy\/0007-support-node-versions-with-at-least-security-updates.md","Context":"## Context\\nMaking clear which versions are supported and how this will be updated as time passes.\\n","Decision":"Only support Node versions with active and security support. Do not support newer, unstable versions.\\nWe can use [endoflife.date](https:\/\/endoflife.date\/nodejs) as a reference. Example: at the moment\\nof this decision, only Node 12, 14 and 16 should be supported.\\n","tokens":20,"id":1048,"text":"## Context\\nMaking clear which versions are supported and how this will be updated as time passes.\\n\n\n##Decision\nOnly support Node versions with active and security support. Do not support newer, unstable versions.\\nWe can use [endoflife.date](https:\/\/endoflife.date\/nodejs) as a reference. Example: at the moment\\nof this decision, only Node 12, 14 and 16 should be supported.\\n","MatchedIds":[241,641,3328,4225,4497],"Predicted":"## Decision\nWe adopt a clear versioning and support policy as follows:\n\n### Versioning Scheme\n* We use Semantic Versioning (SemVer) to define our release versions: MAJOR.MINOR.PATCH.\n  * MAJOR version increments for incompatible API changes.\n  * MINOR version increments for backwards-compatible functionality.\n  * PATCH version increments for backwards-compatible bug fixes.\n\n### Supported Versions\n* We support the last two MAJOR versions.\n* For each MAJOR version, we support:\n  * The most recent MINOR release.\n  * The last PATCH release of each previous MINOR release.\n\n### Deprecation Policy\n* Deprecated features will be announced in the release notes of a MINOR version.\n* Deprecated features will be removed in the subsequent MAJOR version.\n\n### Release Cadence\n* MAJOR version releases approximately every 18-24 months.\n* MINOR version releases approximately every 3 months.\n* PATCH releases as needed for critical bug fixes.\n\n### Communication\n* A roadmap will be maintained and published, providing visibility into upcoming releases and deprecations.\n* Users will be notified of deprecations, and provided with documentation on alternatives and migration paths.\n\n### Review and Adjustments\n* This policy will be reviewed annually and adjusted based on user feedback and technological advancements.\n\nBy adhering to this policy, we aim to provide clarity and predictability around version support and updates.","GenTime":"2024-08-19 23:59:51"}
{"File Name":"openlobby-server\/0003-just-api-without-frontend.md","Context":"## Context\\nOpen Lobby must have open stable API.\\n","Decision":"Open Lobby will be server just with an API based on API First design. Frontend\\nwill be written as separate application (web based, mobile, ...). This will\\nensure that there is a working API for anything that frontend application(s)\\nwill do.\\n","tokens":12,"id":476,"text":"## Context\\nOpen Lobby must have open stable API.\\n\n\n##Decision\nOpen Lobby will be server just with an API based on API First design. Frontend\\nwill be written as separate application (web based, mobile, ...). This will\\nensure that there is a working API for anything that frontend application(s)\\nwill do.\\n","MatchedIds":[1180,2768,241,2181,138],"Predicted":"## Decision\nWe will adopt the [OpenAPI 3.0](https:\/\/spec.openapis.org\/oas\/v3.0.0) specification as the standard for defining and documenting the Open Lobby API. This choice ensures that our API is both open and stable, providing a clear, consistent, and comprehensive contract for API consumers while leveraging modern features for improved API capabilities and documentation.","GenTime":"2024-08-19 23:59:53"}
{"File Name":"platform\/2021-01-21-deprecation-strategy.md","Context":"## Context\\nDefine a strategy for deprecations.\\n","Decision":"### Dogma\\n* Don't do changes without feature-flags (only exception are bugfixes)\\n* Don't break things without an alternative\\n* Don't break things in a minor release\\n* Annotate upcoming breaks as soon as possible\\n* Test all new implementations and changes\\n* Be expressive and very verbose on instructions in your inline feature flag comments\\n* There is a world outside with developers that use our public code\\n### Synopsys\\nAs we decided to work in the trunk based development from now on, there are different kinds of cases we need to consider while implementing changes to not cause any breaks while developing for future features.\\nThe main difference we have to take in account, is if we break currently behaviour with our changes or not.\\nFor this difference we have 4 different cases:\\n* Minor Changes which don't cause any breaks or deprecations\\n* Minor Changes which cause deprecations\\n* Minor Changes as part of a major feature which don't cause any breaks\\n* Major changes which cause breaks\\nFor a quick overview this is how you have to deal with the different cases.\\nConcrete Examples and further explanation follow below.\\n#### Only Minor Changes (no breaks)\\nFeature and changes tend to be released in a minor release. Don't cause breaks. Simple additions, refactorings, etc\\n* Put all your changes behind a feature flag, to be sure that nothing you have changed is called while developing is in progress.\\n* When Development is completed, remove the feature flag and all the old code that is not used anymore\\n* Detailed description here [Detailed Rules](DetailedRules)\\n#### Only Minor Changes (with deprecating code)\\nFeature and Changes tend to be released in a minor release and are developed in a backward compatible manner, but deprecate old code. For example a class is replaced by a new one.\\n* Put all your changes behind a feature flag, to be sure that nothing you have changed is called while developing is in progress.\\n* When Development is completed, remove the feature flag and all the old code that is not used anymore\\n* Mark old code as deprecated and make sure it is not called anywhere else\\n* Make sure everything you removed has a working alternative implemented.\\n* Annotate everything in a manner that the removal of the deprecated code will be a no-brainer on the next major release\\n* Detailed description here [Detailed Rules](DetailedRules)\\n#### Major Changes (Breaks)\\nParts of a major feature or refactoring which breaks current behaviour. Removal of classes, methods or properties, change of signatures, business logic changes...\\n* Put all your changes behind a feature flag, to be sure that nothing you have changed is called while developing is in progress.\\n* When Development is completed, remove the feature flag and all the old code that is not used anymore\\n* Mark old code as deprecated and make sure it is not called anywhere else\\n* Make sure everything you removed has a working alternative implemented.\\n* Annotate everything in a manner that the removal of the deprecated code will be a no-brainer on the next major release\\n* only difference between the case above is, that you have to take care about the fact, that the whole old behaviour needs to be fully functional until the next major.\\n* Write specific tests for the major flag which tests the new behaviour.\\n* Detailed description here [Detailed Rules](DetailedRules)\\n","tokens":12,"id":4497,"text":"## Context\\nDefine a strategy for deprecations.\\n\n\n##Decision\n### Dogma\\n* Don't do changes without feature-flags (only exception are bugfixes)\\n* Don't break things without an alternative\\n* Don't break things in a minor release\\n* Annotate upcoming breaks as soon as possible\\n* Test all new implementations and changes\\n* Be expressive and very verbose on instructions in your inline feature flag comments\\n* There is a world outside with developers that use our public code\\n### Synopsys\\nAs we decided to work in the trunk based development from now on, there are different kinds of cases we need to consider while implementing changes to not cause any breaks while developing for future features.\\nThe main difference we have to take in account, is if we break currently behaviour with our changes or not.\\nFor this difference we have 4 different cases:\\n* Minor Changes which don't cause any breaks or deprecations\\n* Minor Changes which cause deprecations\\n* Minor Changes as part of a major feature which don't cause any breaks\\n* Major changes which cause breaks\\nFor a quick overview this is how you have to deal with the different cases.\\nConcrete Examples and further explanation follow below.\\n#### Only Minor Changes (no breaks)\\nFeature and changes tend to be released in a minor release. Don't cause breaks. Simple additions, refactorings, etc\\n* Put all your changes behind a feature flag, to be sure that nothing you have changed is called while developing is in progress.\\n* When Development is completed, remove the feature flag and all the old code that is not used anymore\\n* Detailed description here [Detailed Rules](DetailedRules)\\n#### Only Minor Changes (with deprecating code)\\nFeature and Changes tend to be released in a minor release and are developed in a backward compatible manner, but deprecate old code. For example a class is replaced by a new one.\\n* Put all your changes behind a feature flag, to be sure that nothing you have changed is called while developing is in progress.\\n* When Development is completed, remove the feature flag and all the old code that is not used anymore\\n* Mark old code as deprecated and make sure it is not called anywhere else\\n* Make sure everything you removed has a working alternative implemented.\\n* Annotate everything in a manner that the removal of the deprecated code will be a no-brainer on the next major release\\n* Detailed description here [Detailed Rules](DetailedRules)\\n#### Major Changes (Breaks)\\nParts of a major feature or refactoring which breaks current behaviour. Removal of classes, methods or properties, change of signatures, business logic changes...\\n* Put all your changes behind a feature flag, to be sure that nothing you have changed is called while developing is in progress.\\n* When Development is completed, remove the feature flag and all the old code that is not used anymore\\n* Mark old code as deprecated and make sure it is not called anywhere else\\n* Make sure everything you removed has a working alternative implemented.\\n* Annotate everything in a manner that the removal of the deprecated code will be a no-brainer on the next major release\\n* only difference between the case above is, that you have to take care about the fact, that the whole old behaviour needs to be fully functional until the next major.\\n* Write specific tests for the major flag which tests the new behaviour.\\n* Detailed description here [Detailed Rules](DetailedRules)\\n","MatchedIds":[3653,1048,5070,3853,1416],"Predicted":"## Decision\nAdopt a phased deprecation strategy:\n1. **Announcement Phase**: Clearly communicate the deprecation to all stakeholders, including timelines and reasons for deprecation. Update documentation accordingly.\n2. **Grace Period Phase**: Allow a grace period where both the old and new functionalities are supported. This period should be long enough for users to transition comfortably.\n3. **Monitoring Phase**: Regularly monitor usage during the grace period to identify potential issues and provide support to transitioning users.\n4. **Removal Phase**: Finally, remove the deprecated feature after the grace period ends, ensuring all documentation and references are updated.\n\nAdditionally, ensure deprecation warnings are included in the codebase to notify developers at runtime or compile-time (where applicable).","GenTime":"2024-08-19 23:59:57"}
{"File Name":"api-catalogue\/0001-remove-tech-docs-gem-dependency.md","Context":"## Context\\nThe main data source of the API catalogue website was originally a collection of\\nmarkdown files. The build process uses the Middleman static site generator\\nconfigured by the [Tech Docs Gem](https:\/\/github.com\/alphagov\/tech-docs-gem)\\n('TDG').\\nThe TDG provides additional functionality including search, sidebar\\nnavigation ('Table of Contents'), the layout, and styling.\\nThe TDG is not necessarily a good fit for the API catalogue because the project\\nisn't purely documentation, and our data source is now a CSV.\\nIn particular it is difficult to override templates inherited from the gem, to\\nadjust the layout on a particular page or add page-specific JavaScript for\\nexample.\\nUsing TDG to render the Table of Contents is slow for our site because\\nby design every page is re-rendered multiple times to pull out the headings\\n(adding over a minute to build times).\\nThe TDG also requires specific dependency versions. These version\\nrestrictions prevent us being in control of version upgrades which are necessary\\nto remain on support versions and receive security patches.\\n","Decision":"Remove the TDG as a dependency by vendoring the code relevant to\\nthe API catalogue directly into the project itself.\\n","tokens":228,"id":3586,"text":"## Context\\nThe main data source of the API catalogue website was originally a collection of\\nmarkdown files. The build process uses the Middleman static site generator\\nconfigured by the [Tech Docs Gem](https:\/\/github.com\/alphagov\/tech-docs-gem)\\n('TDG').\\nThe TDG provides additional functionality including search, sidebar\\nnavigation ('Table of Contents'), the layout, and styling.\\nThe TDG is not necessarily a good fit for the API catalogue because the project\\nisn't purely documentation, and our data source is now a CSV.\\nIn particular it is difficult to override templates inherited from the gem, to\\nadjust the layout on a particular page or add page-specific JavaScript for\\nexample.\\nUsing TDG to render the Table of Contents is slow for our site because\\nby design every page is re-rendered multiple times to pull out the headings\\n(adding over a minute to build times).\\nThe TDG also requires specific dependency versions. These version\\nrestrictions prevent us being in control of version upgrades which are necessary\\nto remain on support versions and receive security patches.\\n\n\n##Decision\nRemove the TDG as a dependency by vendoring the code relevant to\\nthe API catalogue directly into the project itself.\\n","MatchedIds":[1599,5172,1680,1669,2386],"Predicted":"## Decision\nWe will migrate the API catalogue website away from the Tech Docs Gem (TDG) and Middleman static site generator to a custom solution tailored to our specific needs. This custom solution will allow us to:\n\n1. Directly use the CSV data source without requiring intermediate conversion to markdown files.\n2. Implement a faster and more efficient Table of Contents generation that does not require multiple re-renders per page.\n3. Have full control over the layout and the ability to add page-specific JavaScript easily.\n4. Manage our dependencies independently, allowing us to control version upgrades and stay up-to-date with security patches.\n   \nWe will evaluate suitable frameworks and tools, keeping in mind performance, flexibility, and ease of maintenance, to ensure that our new system meets all of our requirements efficiently.","GenTime":"2024-08-20 00:00:00"}
{"File Name":"deeplearning4j\/0009 - Import node pre processing.md","Context":"## Context\\nNd4j's model import framework supports different protobuf based frameworks\\nfor importing and executing models. This was introduced in [0003-Import_IR.md](0003-Import_IR.md)\\nOne problem with importing models is compatibility between different versions of frameworks.\\nOften,migrations are needed to handle compatibility between versions. A node pre processor is proposed\\nthat: when combined with the model import framework allows for\\nannotation based automatic upgrades of graphs.\\n","Decision":"In order to handle preprocessing a node to handle things like upgrades.\\nAn end user can specify a pre processor via a combination of 2 interfaces:\\n1. An annotation for specifying a class that implements a relevant rule\\nfor processing. This will automatically be discoverable via annotation scanning\\nsimilar to other frameworks. This annotation looks as follows:\\n```kotlin\\nannotation class NodePreProcessor(val nodeTypes: Array<String>, val frameworkName: String)\\n```\\nThe information include the nodeTypes which are the operation types to scan for when doing upgrades on a graph.\\nThe framework name: relevant if multiple import modules are on the classpath. Filters rules\\nby their intended framework for import.\\n2. The necessary pre processing hook that will handle processing the node\\nand may modify the graph. Graph modification maybe necessary if we need to add new nodes to compensate\\nfor modification of a node such as an attribute moving to being an input.\\n```kotlin\\ninterface NodePreProcessorHook<NODE_TYPE : GeneratedMessageV3,\\nTENSOR_TYPE : GeneratedMessageV3,\\nATTRIBUTE_TYPE : GeneratedMessageV3,\\nATTRIBUTE_VALUE_TYPE : GeneratedMessageV3, DATA_TYPE>\\nwhere  DATA_TYPE: ProtocolMessageEnum {\\nfun modifyNode(\\nnode: IRNode<NODE_TYPE, TENSOR_TYPE, ATTRIBUTE_TYPE, ATTRIBUTE_VALUE_TYPE, DATA_TYPE>,\\ngraph: IRGraph<GeneratedMessageV3, GeneratedMessageV3, GeneratedMessageV3, GeneratedMessageV3, GeneratedMessageV3, GeneratedMessageV3, ProtocolMessageEnum>\\n): IRNode<NODE_TYPE, TENSOR_TYPE, ATTRIBUTE_TYPE, ATTRIBUTE_VALUE_TYPE, DATA_TYPE>\\n}\\n```\\n","tokens":94,"id":2945,"text":"## Context\\nNd4j's model import framework supports different protobuf based frameworks\\nfor importing and executing models. This was introduced in [0003-Import_IR.md](0003-Import_IR.md)\\nOne problem with importing models is compatibility between different versions of frameworks.\\nOften,migrations are needed to handle compatibility between versions. A node pre processor is proposed\\nthat: when combined with the model import framework allows for\\nannotation based automatic upgrades of graphs.\\n\n\n##Decision\nIn order to handle preprocessing a node to handle things like upgrades.\\nAn end user can specify a pre processor via a combination of 2 interfaces:\\n1. An annotation for specifying a class that implements a relevant rule\\nfor processing. This will automatically be discoverable via annotation scanning\\nsimilar to other frameworks. This annotation looks as follows:\\n```kotlin\\nannotation class NodePreProcessor(val nodeTypes: Array<String>, val frameworkName: String)\\n```\\nThe information include the nodeTypes which are the operation types to scan for when doing upgrades on a graph.\\nThe framework name: relevant if multiple import modules are on the classpath. Filters rules\\nby their intended framework for import.\\n2. The necessary pre processing hook that will handle processing the node\\nand may modify the graph. Graph modification maybe necessary if we need to add new nodes to compensate\\nfor modification of a node such as an attribute moving to being an input.\\n```kotlin\\ninterface NodePreProcessorHook<NODE_TYPE : GeneratedMessageV3,\\nTENSOR_TYPE : GeneratedMessageV3,\\nATTRIBUTE_TYPE : GeneratedMessageV3,\\nATTRIBUTE_VALUE_TYPE : GeneratedMessageV3, DATA_TYPE>\\nwhere  DATA_TYPE: ProtocolMessageEnum {\\nfun modifyNode(\\nnode: IRNode<NODE_TYPE, TENSOR_TYPE, ATTRIBUTE_TYPE, ATTRIBUTE_VALUE_TYPE, DATA_TYPE>,\\ngraph: IRGraph<GeneratedMessageV3, GeneratedMessageV3, GeneratedMessageV3, GeneratedMessageV3, GeneratedMessageV3, GeneratedMessageV3, ProtocolMessageEnum>\\n): IRNode<NODE_TYPE, TENSOR_TYPE, ATTRIBUTE_TYPE, ATTRIBUTE_VALUE_TYPE, DATA_TYPE>\\n}\\n```\\n","MatchedIds":[2933,2944,2932,2934,4618],"Predicted":"## Decision\nTo address compatibility issues between different versions of protobuf-based frameworks during model import, we will implement a node pre-processor for the Nd4j model import framework. This approach will leverage annotation-based mechanisms to facilitate automatic upgrades of graphs. The proposed solution will include the following components:\n\n1. **Node Pre-Processor Component**: A dedicated pre-processor module that will be invoked before the graph is fully imported. This pre-processor will iterate through the nodes of the graph, applying necessary transformations based on the annotations provided.\n\n2. **Annotation Mechanism**: Introduce specific annotations that will be used to describe the version-specific upgrade logic for nodes. These annotations will be used to tag methods that contain the logic for upgrading nodes from one version to another.\n\n3. **Transformation Registry**: A registry that will map the different versions of nodes to their respective transformation methods. Each transformation method will be responsible for upgrading a node from one version to the next.\n\n4. **Version Metadata**: Each node in the graph will carry metadata indicating its version. This metadata will be used by the node pre-processor to determine the necessary transformations.\n\n5. **Backward Compatibility**: Ensure that the node pre-processor can handle multiple versions and apply transformations in sequence to bring an outdated graph up to the current version.\n\n6. **Testing and Validation**: Comprehensive test cases and validation frameworks will be introduced to ensure the integrity of the upgraded graphs. These tests will cover various scenarios, including single-step upgrades, multi-step upgrades, and edge cases.\n\nBy implementing this node pre-processor with annotation-based automatic upgrades, we aim to simplify the maintenance of compatibility between different versions of frameworks and minimize the manual effort required for model migration. This will provide a more robust and scalable solution for model import in Nd4j.","GenTime":"2024-08-20 00:00:08"}
{"File Name":"read-more-api\/0007-deploy-to-heroku.md","Context":"## Context\\nThe application needs to be deployed somewhere.\\nThere are many container orchestration platforms for configuring a cluster of machines running Docker containers and managing the deployment of images on to these containers. Setting up these tools involves managing the cluster of machines and the hosting costs associated with these machines.\\nHeroku is a Platform as a Service (PaaS) provider which helps with the deployment of applications. They have a [Container Registry](https:\/\/devcenter.heroku.com\/articles\/container-registry-and-runtime) solution that handles the deployment of Docker images in to suitable containers.\\nHeroku has several pricing tiers for machines that the application will run on, including a free tier.\\nHeroku provides a free hosted PostgreSQL option. It will handle setting a \"DATABASE_URL\" environment variable, containing the information required to connect to this database. The free tier database is limited to 10,000 rows.\\nWe want the setup process to be as simple as possible.\\n","Decision":"We will host the application on Heroku, using their Container Registry solution with a hosted PostgreSQL database.\\n","tokens":193,"id":718,"text":"## Context\\nThe application needs to be deployed somewhere.\\nThere are many container orchestration platforms for configuring a cluster of machines running Docker containers and managing the deployment of images on to these containers. Setting up these tools involves managing the cluster of machines and the hosting costs associated with these machines.\\nHeroku is a Platform as a Service (PaaS) provider which helps with the deployment of applications. They have a [Container Registry](https:\/\/devcenter.heroku.com\/articles\/container-registry-and-runtime) solution that handles the deployment of Docker images in to suitable containers.\\nHeroku has several pricing tiers for machines that the application will run on, including a free tier.\\nHeroku provides a free hosted PostgreSQL option. It will handle setting a \"DATABASE_URL\" environment variable, containing the information required to connect to this database. The free tier database is limited to 10,000 rows.\\nWe want the setup process to be as simple as possible.\\n\n\n##Decision\nWe will host the application on Heroku, using their Container Registry solution with a hosted PostgreSQL database.\\n","MatchedIds":[3657,1740,583,1239,3793],"Predicted":"## Decision\nWe will use Heroku's Platform as a Service (PaaS) to deploy our application. Specifically, we will take advantage of Heroku's Container Registry for deploying our Docker images. We will initially utilize the free tier for hosting our application to keep costs minimal during initial development and testing. Additionally, we will use Heroku's free hosted PostgreSQL option to manage our database needs, understanding that it is limited to 10,000 rows, which should be sufficient for the early stages of our project.\n\nThis decision allows us to have a simple and straightforward setup process, focusing on minimizing our initial infrastructure management and costs.","GenTime":"2024-08-20 00:00:11"}
{"File Name":"tendermint\/adr-003-abci-app-rpc.md","Context":"## Context\\nABCI-server could expose its own RPC-server and act as a proxy to Tendermint.\\nThe idea was for the Tendermint RPC to just be a transparent proxy to the app.\\nClients need to talk to Tendermint for proofs, unless we burden all app devs\\nwith exposing Tendermint proof stuff. Also seems less complex to lock down one\\nserver than two, but granted it makes querying a bit more kludgy since it needs\\nto be passed as a `Query`. Also, **having a very standard rpc interface means\\nthe light-client can work with all apps and handle proofs**. The only\\napp-specific logic is decoding the binary data to a more readable form (eg.\\njson). This is a huge advantage for code-reuse and standardization.\\n","Decision":"We dont expose an RPC server on any of our ABCI-apps.\\n","tokens":161,"id":1985,"text":"## Context\\nABCI-server could expose its own RPC-server and act as a proxy to Tendermint.\\nThe idea was for the Tendermint RPC to just be a transparent proxy to the app.\\nClients need to talk to Tendermint for proofs, unless we burden all app devs\\nwith exposing Tendermint proof stuff. Also seems less complex to lock down one\\nserver than two, but granted it makes querying a bit more kludgy since it needs\\nto be passed as a `Query`. Also, **having a very standard rpc interface means\\nthe light-client can work with all apps and handle proofs**. The only\\napp-specific logic is decoding the binary data to a more readable form (eg.\\njson). This is a huge advantage for code-reuse and standardization.\\n\n\n##Decision\nWe dont expose an RPC server on any of our ABCI-apps.\\n","MatchedIds":[4354,1949,4130,4097,4099],"Predicted":"## Decision\n\nTo streamline the architecture and enhance the usability for clients and app developers, we will implement the following:\n\n### Architecture\n\n1. **Single RPC Server**:\n   - The Tendermint node will continue to expose a single RPC server endpoint. \n   - This RPC server will serve as the main interface that clients interact with for all queries, including those requiring proofs.\n\n2. **Proxy Queries to ABCI Apps**:\n   - Instead of burdening application developers with the need to expose their own RPC endpoints and implement Tendermint proof handling, the Tendermint RPC server will act as a proxy to the ABCI app.\n   - Queries directed to the Tendermint node that need to interact with the application will be passed through to the underlying ABCI application as `Query` requests.\n   - Results from the ABCI app will then be returned to the client through the Tendermint RPC server.\n\n### Advantages\n- **Standardization**: By maintaining a single RPC endpoint, we can standardize the interaction with the blockchain applications. This ensures that light clients can work uniformly with all applications built on Tendermint without requiring custom logic for each application.\n- **Security**: Managing and securing a single RPC server is simpler and more robust than having multiple interfaces, reducing the attack surface.\n- **Code Reuse and Ease of Development**: Application developers will benefit from reduced complexity, focusing on their application logic while the Tendermint node manages the intricacies of interacting with clients and handling proofs.\n- **Proof Handling**: Tendermint will inherently manage proof generation and validation, ensuring that clients get consistent and valid proofs for their queries without exposing this complexity to the application developers.\n\n### Implementation\n1. **RPC Proxy Module**:\n   - Develop an RPC Proxy module within Tendermint that will handle the forwarding of `Query` requests to the ABCI app. This module will also manage proof generation and validation.\n   \n2. **Query API Update**:\n   - Update the Query API to ensure compatibility and ease of use by defining a clear schema for both outgoing requests to the ABCI app and incoming responses.\n\n3. **Client Libraries**:\n   - Extend existing client libraries to leverage the single RPC endpoint for interacting with both Tendermint and the ABCI app seamlessly.\n\n4. **Documentation and Examples**:\n   - Provide extensive documentation and examples to guide application developers on how to interact with the new unified RPC interface, ensuring a smooth transition and adoption.\n\nBy adopting this approach, we aim to improve the overall developer experience, enhance the security and reliability of the system, and ensure consistency and standardization across applications built on Tendermint.","GenTime":"2024-08-20 00:00:20"}
{"File Name":"dp\/0009.md","Context":"## Context\\nDatasets are made up of observations and dimensions. Each of the options for a\\ndimension have wider definitions and metadata which apply to that option in all\\nscenarios, across multiple datasets. These code definitions live in code-lists.\\nWhen storing both datasets and code-lists in a graph database, there is some\\noverlap between the information stored on a code and on a dimension option. A\\ndesign decision was needed as to whether the dimension option and code nodes\\nin the graph database should be collapsed into a single node, with the relevant\\nrelationships for each coming off that single node.\\n","Decision":"Dimension options and codes should not be collapsed into a single node. Dimension\\noptions have relationships to every observation within an imported dataset, and\\nadding those relationships to the code node would result in relationships to\\nevery observation across all datasets - a significant and unnecessary burden.\\n","tokens":126,"id":5118,"text":"## Context\\nDatasets are made up of observations and dimensions. Each of the options for a\\ndimension have wider definitions and metadata which apply to that option in all\\nscenarios, across multiple datasets. These code definitions live in code-lists.\\nWhen storing both datasets and code-lists in a graph database, there is some\\noverlap between the information stored on a code and on a dimension option. A\\ndesign decision was needed as to whether the dimension option and code nodes\\nin the graph database should be collapsed into a single node, with the relevant\\nrelationships for each coming off that single node.\\n\n\n##Decision\nDimension options and codes should not be collapsed into a single node. Dimension\\noptions have relationships to every observation within an imported dataset, and\\nadding those relationships to the code node would result in relationships to\\nevery observation across all datasets - a significant and unnecessary burden.\\n","MatchedIds":[3253,2183,3372,1510,2802],"Predicted":"## Decision\nThe dimension option and code nodes will be collapsed into a single node in the graph database. This decision is made to avoid redundancy and ensure that each code and its associated metadata are stored once, simplifying the structure of the graph and making it easier to maintain. By having a single node represent both the dimension option and code, we can more effectively use relationships to capture the specific contexts in which each dimension option is used across different datasets.\n\n### Positive Consequences\n* **Reduction in Redundancy**: Having one node per code ensures that metadata and definitions are stored once, reducing duplication.\n* **Simplified Querying**: Queries to retrieve information about a code will be simplified, as there is only one type of node to consider.\n* **Consistency**: Ensures consistent definitions and metadata across all datasets.\n\n### Negative Consequences\n* **Complex Initial Migration**: Migrating existing datasets and code lists into this structure may require complex transformations.\n* **Potential for Overloading Node**: The single node might become overloaded with relationships, which could impact performance in very large databases. Regular optimization and proper indexing will be necessary.\n\n### Alternative Considered\nKeeping separate nodes for dimension options and codes was considered but was rejected due to the complexity and maintenance overhead it would introduce. Having separate nodes would require synchronization mechanisms to ensure consistency of metadata across the graph, adding to the system\u2019s complexity.","GenTime":"2024-08-20 00:00:28"}
{"File Name":"planet4-docs\/adr-0001-use-gitbook-for-technical-documentation.md","Context":"### Context and Problem Statement\\nCurrently Technical Documentation lives in the Handbook, with an [entry point](https:\/\/planet4.greenpeace.org\/create\/tech\/) and everything else organized under the [Development tag](https:\/\/planet4.greenpeace.org\/tag\/development\/). On the other hand Gitbook is a platform specifically for maintaining documentation, based on Markdown. We created a [Planet 4 space](https:\/\/app.gitbook.com\/@greenpeace\/s\/planet4\/) in GP Gitbook account to test how a potential structure of our documentation would look.\\n### Decision Drivers\\n* Community Management Team should be consulted, since this affects the internal development community too.\\n### Considered Options\\n* Move to Gitbook\\n* Stay in the Handbook\\n### Decision Outcome\\nChosen option: **Decided to move to Gitbook all Technical documentation \\(Development & Infrastructure\\)**\\n### Pros and Cons of the Options\\n#### Move to Gitbook\\n* Good, because Gitbook is meant to be used as a documentation platform providing better UI\/UX for that task.\\n* Good, because docs can be grouped in categories, or browsed as a series of a guide, creating a user journey.\\n* Good, because it\u2019s connected and synced with a public Github repo, so it\u2019s easy for an external contributor to update the documentation by opening a Pull Request.\\n* Good, because the documentation on the repo is just Markdown so we are not locked into Gitbook, if we ever choose to leave.\\n* Good, because it\u2019s the choice of [other teams](https:\/\/app.gitbook.com\/@greenpeace\/spaces) in the Org \\(eg. Global Support\\), so people can discover it easier but they are also used to it.\\n#### Stay in the Handbook\\n* Good, because everything regarding P4 is in one place.\\n* Bad, because the perception is that the Handbook is for internal audience. That\u2019s not always the case for the technical documentation.\\n* Bad, because we sometimes need to do custom development just for the Handbook.\\nBad, because it demands manual styling customization for technical documentation \\(e.g scripts\/coding pieces\\) or instructions.\\n### Links\\n* [P4 space on Gitbook](https:\/\/app.gitbook.com\/@greenpeace\/s\/planet4\/)\\n* [Git Repo](https:\/\/github.com\/greenpeace\/planet4-docs\/)\\n","Decision":"* Community Management Team should be consulted, since this affects the internal development community too.\\n### Considered Options\\n* Move to Gitbook\\n* Stay in the Handbook\\n### Decision Outcome\\nChosen option: **Decided to move to Gitbook all Technical documentation \\(Development & Infrastructure\\)**\\n### Pros and Cons of the Options\\n#### Move to Gitbook\\n* Good, because Gitbook is meant to be used as a documentation platform providing better UI\/UX for that task.\\n* Good, because docs can be grouped in categories, or browsed as a series of a guide, creating a user journey.\\n* Good, because it\u2019s connected and synced with a public Github repo, so it\u2019s easy for an external contributor to update the documentation by opening a Pull Request.\\n* Good, because the documentation on the repo is just Markdown so we are not locked into Gitbook, if we ever choose to leave.\\n* Good, because it\u2019s the choice of [other teams](https:\/\/app.gitbook.com\/@greenpeace\/spaces) in the Org \\(eg. Global Support\\), so people can discover it easier but they are also used to it.\\n#### Stay in the Handbook\\n* Good, because everything regarding P4 is in one place.\\n* Bad, because the perception is that the Handbook is for internal audience. That\u2019s not always the case for the technical documentation.\\n* Bad, because we sometimes need to do custom development just for the Handbook.\\nBad, because it demands manual styling customization for technical documentation \\(e.g scripts\/coding pieces\\) or instructions.\\n### Links\\n* [P4 space on Gitbook](https:\/\/app.gitbook.com\/@greenpeace\/s\/planet4\/)\\n* [Git Repo](https:\/\/github.com\/greenpeace\/planet4-docs\/)\\n","tokens":492,"id":4087,"text":"### Context and Problem Statement\\nCurrently Technical Documentation lives in the Handbook, with an [entry point](https:\/\/planet4.greenpeace.org\/create\/tech\/) and everything else organized under the [Development tag](https:\/\/planet4.greenpeace.org\/tag\/development\/). On the other hand Gitbook is a platform specifically for maintaining documentation, based on Markdown. We created a [Planet 4 space](https:\/\/app.gitbook.com\/@greenpeace\/s\/planet4\/) in GP Gitbook account to test how a potential structure of our documentation would look.\\n### Decision Drivers\\n* Community Management Team should be consulted, since this affects the internal development community too.\\n### Considered Options\\n* Move to Gitbook\\n* Stay in the Handbook\\n### Decision Outcome\\nChosen option: **Decided to move to Gitbook all Technical documentation \\(Development & Infrastructure\\)**\\n### Pros and Cons of the Options\\n#### Move to Gitbook\\n* Good, because Gitbook is meant to be used as a documentation platform providing better UI\/UX for that task.\\n* Good, because docs can be grouped in categories, or browsed as a series of a guide, creating a user journey.\\n* Good, because it\u2019s connected and synced with a public Github repo, so it\u2019s easy for an external contributor to update the documentation by opening a Pull Request.\\n* Good, because the documentation on the repo is just Markdown so we are not locked into Gitbook, if we ever choose to leave.\\n* Good, because it\u2019s the choice of [other teams](https:\/\/app.gitbook.com\/@greenpeace\/spaces) in the Org \\(eg. Global Support\\), so people can discover it easier but they are also used to it.\\n#### Stay in the Handbook\\n* Good, because everything regarding P4 is in one place.\\n* Bad, because the perception is that the Handbook is for internal audience. That\u2019s not always the case for the technical documentation.\\n* Bad, because we sometimes need to do custom development just for the Handbook.\\nBad, because it demands manual styling customization for technical documentation \\(e.g scripts\/coding pieces\\) or instructions.\\n### Links\\n* [P4 space on Gitbook](https:\/\/app.gitbook.com\/@greenpeace\/s\/planet4\/)\\n* [Git Repo](https:\/\/github.com\/greenpeace\/planet4-docs\/)\\n\n\n##Decision\n* Community Management Team should be consulted, since this affects the internal development community too.\\n### Considered Options\\n* Move to Gitbook\\n* Stay in the Handbook\\n### Decision Outcome\\nChosen option: **Decided to move to Gitbook all Technical documentation \\(Development & Infrastructure\\)**\\n### Pros and Cons of the Options\\n#### Move to Gitbook\\n* Good, because Gitbook is meant to be used as a documentation platform providing better UI\/UX for that task.\\n* Good, because docs can be grouped in categories, or browsed as a series of a guide, creating a user journey.\\n* Good, because it\u2019s connected and synced with a public Github repo, so it\u2019s easy for an external contributor to update the documentation by opening a Pull Request.\\n* Good, because the documentation on the repo is just Markdown so we are not locked into Gitbook, if we ever choose to leave.\\n* Good, because it\u2019s the choice of [other teams](https:\/\/app.gitbook.com\/@greenpeace\/spaces) in the Org \\(eg. Global Support\\), so people can discover it easier but they are also used to it.\\n#### Stay in the Handbook\\n* Good, because everything regarding P4 is in one place.\\n* Bad, because the perception is that the Handbook is for internal audience. That\u2019s not always the case for the technical documentation.\\n* Bad, because we sometimes need to do custom development just for the Handbook.\\nBad, because it demands manual styling customization for technical documentation \\(e.g scripts\/coding pieces\\) or instructions.\\n### Links\\n* [P4 space on Gitbook](https:\/\/app.gitbook.com\/@greenpeace\/s\/planet4\/)\\n* [Git Repo](https:\/\/github.com\/greenpeace\/planet4-docs\/)\\n","MatchedIds":[36,1205,1669,5172,160],"Predicted":"## Decision\n\nWe have decided to move all Technical documentation (Development & Infrastructure) to Gitbook.\n\n### Pros and Cons of the Options\n\n#### Move to Gitbook\n* **Good**, because Gitbook is specifically designed for documentation, providing a superior UI\/UX tailored to this task.\n* **Good**, because documentation can be organized into categories or browsed as a series of guides, improving the user journey.\n* **Good**, because it integrates and syncs with a public GitHub repository, making it easy for external contributors to update the documentation via Pull Requests.\n* **Good**, because the underlying documentation remains in Markdown format, ensuring we're not locked into Gitbook and can migrate if necessary.\n* **Good**, because other teams within the organization (e.g., Global Support) already use Gitbook, which can enhance discoverability and familiarity.\n\n#### Stay in the Handbook\n* **Good**, because it centralizes everything regarding Planet 4 in one place.\n* **Bad**, because the Handbook is often perceived as targeting an internal audience, which may not be suitable for technical documentation meant for a broader audience.\n* **Bad**, because maintaining technical documentation in the Handbook sometimes requires custom development effort.\n* **Bad**, because it demands manual styling customization for technical documentation (e.g., scripts, coding pieces, or instructions).\n\n### Links\n* [P4 space on Gitbook](https:\/\/app.gitbook.com\/@greenpeace\/s\/planet4\/)\n* [Git Repo](https:\/\/github.com\/greenpeace\/planet4-docs\/)","GenTime":"2024-08-20 00:00:39"}
{"File Name":"apply-for-teacher-training\/0003-initial-datamodel.md","Context":"## Context\\nIn the coming months we will be building this application. The first order of business is to come up with a sensible data model and application structure.\\n","Decision":"We discussed a database model that is based around a small number of tables with a lot of columns. In particular:\\n- `Candidate`, primarily used for authentication\\n- `ApplicationForm`, which belongs to a Candidate, and holds the\\ninformation that is common to all of the applications that the user\\nmakes (in the \"Apply 1\" phase, they can apply for up to three courses).\\n- `ApplicationChoice`, which holds the information that is course specific,\\nlike the status of the application and the personal statement. It holds a reference to the \"course choice\", which is the combination of course, provider and the training location.\\nThere will be some other models that hold references, work histories and qualifications.\\nThe eventual table structure will look something like this:\\n![data model](0003-initial-datamodel.png)\\n","tokens":33,"id":4021,"text":"## Context\\nIn the coming months we will be building this application. The first order of business is to come up with a sensible data model and application structure.\\n\n\n##Decision\nWe discussed a database model that is based around a small number of tables with a lot of columns. In particular:\\n- `Candidate`, primarily used for authentication\\n- `ApplicationForm`, which belongs to a Candidate, and holds the\\ninformation that is common to all of the applications that the user\\nmakes (in the \"Apply 1\" phase, they can apply for up to three courses).\\n- `ApplicationChoice`, which holds the information that is course specific,\\nlike the status of the application and the personal statement. It holds a reference to the \"course choice\", which is the combination of course, provider and the training location.\\nThere will be some other models that hold references, work histories and qualifications.\\nThe eventual table structure will look something like this:\\n![data model](0003-initial-datamodel.png)\\n","MatchedIds":[1243,4361,2488,3982,4428],"Predicted":"## Decision\nWe will adopt the principles of Domain-Driven Design (DDD) to structure our data model and application architecture. By identifying core domains, subdomains, and bounded contexts, we can ensure that our system is modular, scalable, and easier to maintain. This approach will guide us in defining entities, value objects, aggregates, services, repositories, and factories clearly within each bounded context, promoting a clean separation of concerns and clearer business logic.\n\nAdditionally, we will leverage the Clean Architecture principles to further enhance our application structure, ensuring that the core business logic remains independent of frameworks and external technologies, which provides us with flexibility and ease of testing. The combination of these architectural paradigms will help us build a robust, well-organized, and future-proof system.","GenTime":"2024-08-20 00:00:43"}
{"File Name":"volley-management\/0007-follow-optimistic-concurrency.md","Context":"## Context and Problem Statement\\nAs any modern system, Volley Management faces a problem of concurrent changes to data and we need to support such scenario.\\nWe explicitly do not consider an option to go without concurrency checks - time will tell if it is a good decision).\\n## Decision Drivers <!-- optional -->\\n* Performance - decision should support high throughput scenarios\\n* Maintainability - amount of code needed to write should be minimized\\n","Decision":"* Performance - decision should support high throughput scenarios\\n* Maintainability - amount of code needed to write should be minimized\\nChosen option: \"Optimistic concurrency\", because it is a better from the perfromance perspective and it will support our choice to [use Azure Table Storage](0006-use-azure-table-storage-for-persistence.md).\\n### Positive Consequences <!-- optional -->\\n* Concurrency check will fail very rarely so it will have almost no overhead\\n* It is aligned with Azure Table storage Optimistic concurrency\\n### Negative Consequences <!-- optional -->\\n* Whole application will have a responsibility to manage versions properly\\n* If we change a datastore we might need to have a larger change\\n","tokens":86,"id":2526,"text":"## Context and Problem Statement\\nAs any modern system, Volley Management faces a problem of concurrent changes to data and we need to support such scenario.\\nWe explicitly do not consider an option to go without concurrency checks - time will tell if it is a good decision).\\n## Decision Drivers <!-- optional -->\\n* Performance - decision should support high throughput scenarios\\n* Maintainability - amount of code needed to write should be minimized\\n\n\n##Decision\n* Performance - decision should support high throughput scenarios\\n* Maintainability - amount of code needed to write should be minimized\\nChosen option: \"Optimistic concurrency\", because it is a better from the perfromance perspective and it will support our choice to [use Azure Table Storage](0006-use-azure-table-storage-for-persistence.md).\\n### Positive Consequences <!-- optional -->\\n* Concurrency check will fail very rarely so it will have almost no overhead\\n* It is aligned with Azure Table storage Optimistic concurrency\\n### Negative Consequences <!-- optional -->\\n* Whole application will have a responsibility to manage versions properly\\n* If we change a datastore we might need to have a larger change\\n","MatchedIds":[1031,2791,4747,2520,1574],"Predicted":"## Decision\nWe will implement **Optimistic Concurrency Control (OCC)** for handling concurrent changes to the data.\n\n### Rationale\n1. **Performance**:\n   * OCC is generally more efficient and scales better under low to moderate contention.\n   * It avoids the overhead of locking mechanisms, which can become performance bottlenecks under high throughput scenarios.\n\n2. **Maintainability**:\n   * The code for OCC is relatively straightforward, usually involving version numbers or timestamps to check if data has been modified.\n   * It minimizes the amount of additional code required for managing locks, thus reducing complexity.\n\n### How it will work\n1. **Read Version**:\n   * When data is read, the version number (or a timestamp) is also retrieved.\n   \n2. **Update Check**:\n   * When data is updated, a check is made to see if the version number (or timestamp) matches the current version in the database.\n   \n3. **Commit If Unchanged**:\n   * If the version number (or timestamp) matches, the update proceeds and the version number is incremented.\n   * If it does not match, the transaction is retried or aborted, notifying the user that the data has been changed by another process.\n\n### Positive Consequences\n* **Scalability**: Low overhead during read and write operations compared to pessimistic locking.\n* **Simpler Codebase**: Fewer lines of code and complexity for handling concurrency compared to lock-based mechanisms.\n\n### Negative Consequences\n* **Retry Logic Needed**: Some logic to handle retries in the event of a concurrent modification.\n* **Higher Cost Under High Contention**: Performance might degrade under high contention scenarios, needing more sophisticated approaches if this becomes an issue.","GenTime":"2024-08-20 00:00:54"}
{"File Name":"platform\/2020-08-28-import-acl-privileges-from-other-roles.md","Context":"## Context\\nSome modules have components which require many acl privileges. Examples\\nare the rule builder or the media manager. Therefore, you need all privileges\\nin each module which have these components. Also you do not want to add the\\nmodule to the dependency section because then the user has full access to module\\nin the administration.\\n","Decision":"To avoid duplication of these privileges we use a helper function. These\\nfunction returns all privileges from the other module dynamically. You can\\nuse it directly in the privileges:\\n```js\\nShopware.Service('privileges')\\n.addPrivilegeMappingEntry({\\ncategory: 'permissions',\\nparent: null,\\nkey: 'promotion',\\nroles: {\\nviewer: {\\nprivileges: ['promotion:read',],\\ndependencies: []\\n},\\neditor: {\\nprivileges: [\\n'promotion:update',\\nShopware.Service('privileges').getPrivileges('rule.creator')\\n],\\ndependencies: [\\n'promotion.viewer'\\n]\\n}\\n}\\n});\\n```\\n","tokens":68,"id":4525,"text":"## Context\\nSome modules have components which require many acl privileges. Examples\\nare the rule builder or the media manager. Therefore, you need all privileges\\nin each module which have these components. Also you do not want to add the\\nmodule to the dependency section because then the user has full access to module\\nin the administration.\\n\n\n##Decision\nTo avoid duplication of these privileges we use a helper function. These\\nfunction returns all privileges from the other module dynamically. You can\\nuse it directly in the privileges:\\n```js\\nShopware.Service('privileges')\\n.addPrivilegeMappingEntry({\\ncategory: 'permissions',\\nparent: null,\\nkey: 'promotion',\\nroles: {\\nviewer: {\\nprivileges: ['promotion:read',],\\ndependencies: []\\n},\\neditor: {\\nprivileges: [\\n'promotion:update',\\nShopware.Service('privileges').getPrivileges('rule.creator')\\n],\\ndependencies: [\\n'promotion.viewer'\\n]\\n}\\n}\\n});\\n```\\n","MatchedIds":[4554,2737,2823,4507,3950],"Predicted":"## Decision\n\nWe will introduce a fine-grained access control list (ACL) strategy to handle specific components within modules that require many privileges without granting full access to the entire module.\n\n### Strategy Overview:\n1. **Component-Specific Privileges**: Define specific ACL permissions for high-privilege components like the rule builder or media manager, which can be assigned independently of module-level privileges.\n  \n    *Example:*\n    ```yaml\n    acl:\n      components:\n        rule_builder:\n          - \"create_rules\"\n          - \"edit_rules\"\n          - \"delete_rules\"\n        media_manager:\n          - \"view_media\"\n          - \"upload_media\"\n          - \"delete_media\"\n    ```\n\n2. **Role-Based Access Control (RBAC)**: Utilize existing RBAC mechanisms to assign these specific component privileges to roles without including the entire module.\n  \n    *Example:*\n    ```ruby\n    admin_role.permissions << \"create_rules\"\n    admin_role.permissions << \"view_media\"\n    ```\n\n3. **Scoped Permissions Checks**: Implement scoped permission checks within the component code to ensure that only users with the necessary privileges can perform certain actions.\n   \n    *Example:*\n    ```ruby\n    class RuleBuilder\n      def create_rule(user)\n        raise Unauthorized unless user.permissions.include?(\"create_rules\")\n        # logic to create rule\n      end\n    end\n    ```\n\n4. **Granular UI Adjustments**: Modify the user interface to conditionally render component-specific functionalities based on the user\u2019s permissions, ensuring that unauthorized users do not see options they cannot use.\n\n    *Example:*\n    ```js\n    if (user.permissions.includes(\"upload_media\")) {\n      renderUploadButton();\n    }\n    ```\n\n### Summary\n\nBy implementing component-specific ACL privileges and adhering to a fine-grained RBAC strategy, we ensure that high-privilege components can be securely managed without granting full administrative access to entire modules. This approach enhances security by adhering to the principle of least privilege while maintaining flexibility in access control.","GenTime":"2024-08-20 00:01:04"}
{"File Name":"front-end-monorepo\/adr-38.md","Context":"## Context\\nEngaging Crowds allows volunteers to browse and search subject sets, linked to a workflow, in order to pick which subjects they want to work on. As part of this, volunteers can browse a subject set, in the classifier, while they decide which subject they wish to work on.\\nThe classifier's subject queue was originally built to support random or sequential subject selection, with subjects being shown to a volunteer in the order they were received from the Panoptes API. Subjects were discarded after being classified. Going backwards through the queue, to view previous subjects, was not possible.\\n","Decision":"- The subject queue was changed from an ordered map, `subjects.resources`, to an array `subjects.queue`. Each item in `subjects.queue` is a subject ID, pointing to a subject in `subjects.resources`. Subjects are shown to the volunteer in array order.\\n- Existing workflows continue to use the orignal queueing logic: subjects are shifted off the array after being classified. The active subject is always the first entry in `subjects.queue`.\\n- Indexed subject sets, for Engaging Crowds, use a different logic. Subject IDs are never removed from the queue. Volunteers can browse available subjects by moving forwards and backwards through the array, changing the active index.\\n","tokens":119,"id":532,"text":"## Context\\nEngaging Crowds allows volunteers to browse and search subject sets, linked to a workflow, in order to pick which subjects they want to work on. As part of this, volunteers can browse a subject set, in the classifier, while they decide which subject they wish to work on.\\nThe classifier's subject queue was originally built to support random or sequential subject selection, with subjects being shown to a volunteer in the order they were received from the Panoptes API. Subjects were discarded after being classified. Going backwards through the queue, to view previous subjects, was not possible.\\n\n\n##Decision\n- The subject queue was changed from an ordered map, `subjects.resources`, to an array `subjects.queue`. Each item in `subjects.queue` is a subject ID, pointing to a subject in `subjects.resources`. Subjects are shown to the volunteer in array order.\\n- Existing workflows continue to use the orignal queueing logic: subjects are shifted off the array after being classified. The active subject is always the first entry in `subjects.queue`.\\n- Indexed subject sets, for Engaging Crowds, use a different logic. Subject IDs are never removed from the queue. Volunteers can browse available subjects by moving forwards and backwards through the array, changing the active index.\\n","MatchedIds":[498,3815,4821,915,528],"Predicted":"## Decision\n1. **Enable Browsing:** Modify the subject queue to allow volunteers to browse subjects within a subject set, enabling both forward and backward navigation through the queue. This will facilitate easier decision-making regarding which subjects to work on.\n2. **Persistently Track Viewed Subjects:** Implement a local state mechanism to keep track of subjects that a user has already been shown or interacted with, allowing for smooth backward navigation even if the subject has already been classified.\n3. **Subject Pinning:** Allow volunteers to pin certain subjects for later classification. This will enable them to mark subjects of interest and return to them after further exploration.\n4. **Contextual Filtering:** Introduce filters within the subject set browsing interface to allow volunteers to search and narrow down subjects based on specific criteria (e.g., tags, metadata fields).\n5. **Caching for Performance:** Utilize client-side caching techniques to store previously fetched subjects, reducing redundant API calls and improving the user experience while navigating through the subject queue.\n6. **Usability Enhancements:** Ensure that navigational controls are accessible and intuitive, keeping accessibility best practices in mind to cater to a diverse volunteer base, including those using keyboard navigation and screen readers.","GenTime":"2024-08-20 00:01:11"}
{"File Name":"architecture-decision-log\/0016-analytics-foundations.md","Context":"## Context\\nOur company is starting to growth fast. With that growth, it is common to see the need of complex data analysis. We've solved that by installing Metabase in a read-replica of our OKR transactional database, but even that structure lacks more complex analytics. Concurrently with the previous statement, our company plans to create an analytics product for our customers, enabling real-time complex analysis of their users.\\nWe can't ignore the need to have a proper analytics foundations inside Bud. Also, we can't afford investing a large amount of time building that infrastructure, since everything could change fast. We need to find a way to create a flexible analytics infrastructure that could:\\n(a) Provide meaningful data regarding our customers;\\n(b) Be flexible enought to integrate with multiple sources;\\n(c) Allow the usage from external applications.\\nIn a nutshel, that infrastructure will be the primary source of truth of our company. We could allow customers to fetch data from it. Even our applications could use it in their scopes.\\n## Decision Drivers\\n1. Flexibility\\n2. How easy it is to integrate with external sources\\n3. Implementation difficulty\\n","Decision":"1. Flexibility\\n2. How easy it is to integrate with external sources\\n3. Implementation difficulty\\nAfter evaluating all options, we've decided to proceed with Airbyte. It meets almost every specification that we have. It is extremelly easy to implement and follows all the best standards. It isn't an in-house solution, but in the current scenario we're on that would not be a big deal with it. Also, we could learn from it and maybe create a new tool in the future, designed to met our needs.\\n### Positivo Consequences\\nWith this infrastructure, we're going to achieve a robust ELT infrastructure, with little effort. We can easily create an analytics application that is going to serve all our business requirements with minimal effort. Also, Airbyte uses DBT under the hood, that being said, even if we need to change our ELT structure, we would still be able to migrate our DBT project.\\n### Negative Consequences\\nThere are two main negative consequences of this decision:\\n#### 1. Not being able to query real time data\\nAs pointed by Marcelo Travi, with Airbyte we would not be able to query real-time data. In most use-cases that would not be an issue, but considering that our first requirement is serving an evolution graph of a given key-result percentual proggress, that should be an issue. To fix that, we suggest using only D-1 data from our analytics, and using the transactional data to enrich it.\\n#### 2. Large queries could affect our applications\\nAs pointed by Victor Perin, since this is not a common implementation, we should be aware that complex analytics queries could impact our applications, since we're integrating them directly. There are plenty of ways to fix that, the easiest one would be creating a read-replica of our data warehouse, and using that to query the analytics. By doing so, we would separate both analytics and BI, minizing this issue.\\n","tokens":240,"id":4909,"text":"## Context\\nOur company is starting to growth fast. With that growth, it is common to see the need of complex data analysis. We've solved that by installing Metabase in a read-replica of our OKR transactional database, but even that structure lacks more complex analytics. Concurrently with the previous statement, our company plans to create an analytics product for our customers, enabling real-time complex analysis of their users.\\nWe can't ignore the need to have a proper analytics foundations inside Bud. Also, we can't afford investing a large amount of time building that infrastructure, since everything could change fast. We need to find a way to create a flexible analytics infrastructure that could:\\n(a) Provide meaningful data regarding our customers;\\n(b) Be flexible enought to integrate with multiple sources;\\n(c) Allow the usage from external applications.\\nIn a nutshel, that infrastructure will be the primary source of truth of our company. We could allow customers to fetch data from it. Even our applications could use it in their scopes.\\n## Decision Drivers\\n1. Flexibility\\n2. How easy it is to integrate with external sources\\n3. Implementation difficulty\\n\n\n##Decision\n1. Flexibility\\n2. How easy it is to integrate with external sources\\n3. Implementation difficulty\\nAfter evaluating all options, we've decided to proceed with Airbyte. It meets almost every specification that we have. It is extremelly easy to implement and follows all the best standards. It isn't an in-house solution, but in the current scenario we're on that would not be a big deal with it. Also, we could learn from it and maybe create a new tool in the future, designed to met our needs.\\n### Positivo Consequences\\nWith this infrastructure, we're going to achieve a robust ELT infrastructure, with little effort. We can easily create an analytics application that is going to serve all our business requirements with minimal effort. Also, Airbyte uses DBT under the hood, that being said, even if we need to change our ELT structure, we would still be able to migrate our DBT project.\\n### Negative Consequences\\nThere are two main negative consequences of this decision:\\n#### 1. Not being able to query real time data\\nAs pointed by Marcelo Travi, with Airbyte we would not be able to query real-time data. In most use-cases that would not be an issue, but considering that our first requirement is serving an evolution graph of a given key-result percentual proggress, that should be an issue. To fix that, we suggest using only D-1 data from our analytics, and using the transactional data to enrich it.\\n#### 2. Large queries could affect our applications\\nAs pointed by Victor Perin, since this is not a common implementation, we should be aware that complex analytics queries could impact our applications, since we're integrating them directly. There are plenty of ways to fix that, the easiest one would be creating a read-replica of our data warehouse, and using that to query the analytics. By doing so, we would separate both analytics and BI, minizing this issue.\\n","MatchedIds":[1701,4747,4378,1740,4285],"Predicted":"## Decision\nWe have determined that using a data warehouse as a centralized analytics infrastructure will best meet our needs. Specifically, we will adopt Amazon Redshift for the following reasons:\n\n1. **Flexibility**:\n    - Amazon Redshift allows for easy scalability and can manage large datasets efficiently, ensuring it can grow with our company.\n    - Redshift supports a variety of data types and sources, ensuring we can handle diverse data from different systems seamlessly.\n\n2. **Ease of Integration**:\n    - Redshift integrates well with other AWS services, which many of our systems already use, ensuring a smoother integration process.\n    - It also supports integration with popular BI tools like Metabase, Tableau, and Looker, which will be beneficial for both internal and customer-facing analytics.\n    - Redshift's ability to directly query and join data from S3 and other external sources via Redshift Spectrum ensures that we can easily pull in diverse data sets.\n\n3. **Implementation Difficulty**:\n    - Implementing Redshift is relatively straightforward due to its managed nature, meaning we don\u2019t need to worry about the underlying infrastructure management.\n    - AWS provides extensive documentation and support, allowing our team to get up to speed quickly.\n\n### Implementation Plan\n1. **Initial Setup**:\n    - Configure Amazon Redshift as our centralized data warehouse.\n    - Set up data pipelines using AWS Glue and other ETL tools to continuously ingest data from our transactional databases and other sources into Redshift.\n\n2. **Data Integration**:\n    - Use Redshift Spectrum to integrate external datasets stored on Amazon S3.\n    - Implement scripts to automate the extraction, transformation, and loading (ETL) of data from our different systems into Redshift.\n\n3. **Analytics and Reporting**:\n    - Integrate Redshift with Metabase for internal analytics.\n    - Develop APIs that allow our applications and external customers to query Redshift for their analytics requirements.\n\n4. **Security and Compliance**:\n    - Ensure that all data stored and processed in Redshift complies with our data governance and security standards.\n    - Implement necessary access controls and encryption protocols.\n\n### Positive Consequences\n- Comprehensive, real-time analytics capabilities for internal and customer-facing applications.\n- Simplified data integration process from multiple sources.\n- Scalable and managed infrastructure reduces the operational burden on our team.\n\n### Negative Consequences\n- There is likely to be some initial investment in terms of time and resources to set up the data warehouse and ETL pipelines.\n- Relies on AWS infrastructure, which could lead to potential vendor lock-in. However, the benefits currently outweigh this risk.\n\nBy adopting Amazon Redshift as our analytics infrastructure, we position ourselves to better serve our growing data analysis needs and build a robust analytics product for our customers.","GenTime":"2024-08-20 00:01:22"}
{"File Name":"eq-author-app\/0002-routing-mk2.md","Context":"## Context\\nRouting is a system to allow a user to describe conditions as to where the survey respondent should go after they complete the answers on a question page.\\nAs these conditions can be any boolean expression we need a more complex routing structure to handle\\n","Decision":"Implement a new version of the routing in the API that can handle nested expressions.\\nThis is heavily inspied by https:\/\/github.com\/ONSdigital\/eq-author-app\/wiki\/Routing,-MK2\\n### Terminology\\n- Routing - all rules etc for a page with a fallback destination when no rules match.\\n- Rule - a series of expressions that when evaluated to true will go to the destination specified.\\n- Expression - Either a binary expression or expression group\\n- Expression Group - A group of BinaryExpressions combined as either `and` or `or`.\\n- Binary Expression - The base item with a left hand side, condtion and right hand side.\\n- Left hand side - The item being compared against (e.g. a question)\\n- Condition - A way of comparing left and right e.g. `=`, `>`, `>=`, `includes`, `not_includes`\\n- Right hand side - The value being compared against a left.\\n### Example\\nOn page 1, when the user enters 5 go to page 2, when they select red or white go to page 3, otherwise go to page 4.\\n```\\npage: {\\ntitle: \"Page 1\",\\nanswers: [\\n{ id: \"answer1\", type: NUMBER, label: \"Answer\" }\\n],\\nrouting: {\\nrules: [\\n{\\nexpressionGroup: {\\noperator: \"AND\",\\nexpressions: [\\n{\\nleft: {\\nanswer: {\\nid: \"Answer1\",\\ntype: CURRENCY\\n}\\n},\\ncondition: \"Equals\",\\nright: {\\nnumber: 5\\n}\\n}\\n]\\n},\\ndestination: {\\npage: {\\nid: \"Page2\"\\n}\\n}\\n},\\n{\\nexpressionGroup: {\\noperator: \"OR\",\\nexpressions: [\\n{\\nleft: {\\nanswer: {\\nid: \"Answer2\",\\ntype: RADIO\\n}\\n},\\ncondition: \"OneOf\",\\nright: {\\noptions: [\\n{\\nvalue: \"red\"\\n},\\n{\\nvalue: \"white\"\\n}\\n]\\n}\\n}\\n]\\n},\\ndestination: {\\npage: {\\nid: \"Page3\"\\n}\\n}\\n}\\n],\\nelse: {\\npage: {\\nid: \"Page4\"\\n}\\n}\\n}\\n}\\n```\\n","tokens":51,"id":2111,"text":"## Context\\nRouting is a system to allow a user to describe conditions as to where the survey respondent should go after they complete the answers on a question page.\\nAs these conditions can be any boolean expression we need a more complex routing structure to handle\\n\n\n##Decision\nImplement a new version of the routing in the API that can handle nested expressions.\\nThis is heavily inspied by https:\/\/github.com\/ONSdigital\/eq-author-app\/wiki\/Routing,-MK2\\n### Terminology\\n- Routing - all rules etc for a page with a fallback destination when no rules match.\\n- Rule - a series of expressions that when evaluated to true will go to the destination specified.\\n- Expression - Either a binary expression or expression group\\n- Expression Group - A group of BinaryExpressions combined as either `and` or `or`.\\n- Binary Expression - The base item with a left hand side, condtion and right hand side.\\n- Left hand side - The item being compared against (e.g. a question)\\n- Condition - A way of comparing left and right e.g. `=`, `>`, `>=`, `includes`, `not_includes`\\n- Right hand side - The value being compared against a left.\\n### Example\\nOn page 1, when the user enters 5 go to page 2, when they select red or white go to page 3, otherwise go to page 4.\\n```\\npage: {\\ntitle: \"Page 1\",\\nanswers: [\\n{ id: \"answer1\", type: NUMBER, label: \"Answer\" }\\n],\\nrouting: {\\nrules: [\\n{\\nexpressionGroup: {\\noperator: \"AND\",\\nexpressions: [\\n{\\nleft: {\\nanswer: {\\nid: \"Answer1\",\\ntype: CURRENCY\\n}\\n},\\ncondition: \"Equals\",\\nright: {\\nnumber: 5\\n}\\n}\\n]\\n},\\ndestination: {\\npage: {\\nid: \"Page2\"\\n}\\n}\\n},\\n{\\nexpressionGroup: {\\noperator: \"OR\",\\nexpressions: [\\n{\\nleft: {\\nanswer: {\\nid: \"Answer2\",\\ntype: RADIO\\n}\\n},\\ncondition: \"OneOf\",\\nright: {\\noptions: [\\n{\\nvalue: \"red\"\\n},\\n{\\nvalue: \"white\"\\n}\\n]\\n}\\n}\\n]\\n},\\ndestination: {\\npage: {\\nid: \"Page3\"\\n}\\n}\\n}\\n],\\nelse: {\\npage: {\\nid: \"Page4\"\\n}\\n}\\n}\\n}\\n```\\n","MatchedIds":[3210,4271,1210,3681,1243],"Predicted":"## Decision\nWe will implement a rule-based routing engine that can evaluate complex boolean expressions to determine the next page the survey respondent should navigate to. Each routing rule will be defined as an object containing the boolean expression, conditions, and the target page. This engine will:\n\n1. Parse each boolean expression to identify the conditions.\n2. Evaluate the conditions based on the respondent's answers.\n3. Determine the appropriate next page based on the fulfilled conditions.\n\nThis modular and extensible approach ensures that the routing system can handle a wide variety of logical expressions and is easily maintainable. Additionally, we will integrate a user-friendly interface for defining these routing rules to ensure non-technical stakeholders can configure them as needed.","GenTime":"2024-08-20 00:01:26"}
{"File Name":"Maud\/0011-drains.md","Context":"## Context\\nDrains are an important aspect of any model, they are an essential boundary\\ncondition for cases such as biomass drains. Curently rate laws are only specified using\\nthe Generalised MWC format with the catalytic aspect using the modular rate law.\\nIntroducing drains into Maud requires implementing this into the ODE system which\\nrelies on enzymes as the defining feature, which are attributes of reactions in Maud.\\nTo specify drains we create a new class independent of reactions, despite the fact that\\nthey occur in the system of ODEs. This benefits post-processing as some techniques rely\\non varying enzyme concentrations, which we've established that drains do not have.\\nDrains are considered as priors to ensure that prior predictive checks are informative\\nof the experimental condition.\\n","Decision":"Drains will not be considered as a reaction class.\\n","tokens":164,"id":233,"text":"## Context\\nDrains are an important aspect of any model, they are an essential boundary\\ncondition for cases such as biomass drains. Curently rate laws are only specified using\\nthe Generalised MWC format with the catalytic aspect using the modular rate law.\\nIntroducing drains into Maud requires implementing this into the ODE system which\\nrelies on enzymes as the defining feature, which are attributes of reactions in Maud.\\nTo specify drains we create a new class independent of reactions, despite the fact that\\nthey occur in the system of ODEs. This benefits post-processing as some techniques rely\\non varying enzyme concentrations, which we've established that drains do not have.\\nDrains are considered as priors to ensure that prior predictive checks are informative\\nof the experimental condition.\\n\n\n##Decision\nDrains will not be considered as a reaction class.\\n","MatchedIds":[239,3290,1912,3776,359],"Predicted":"## Decision\nWe will introduce a new `Drain` class that is independent of the existing reaction framework in Maud. This new class will encapsulate the boundary condition behavior required for drains and allow them to be specified separately from reactions and enzymes. \n\n### Key Elements:\n1. **Independent Class**: The `Drain` class will not inherit from any reaction- or enzyme-related classes, establishing a clean separation.\n2. **ODE System Integration**: Though independent from reactions, drains will be integrated into the ODE system to correctly influence the differential equations.\n3. **Post-Processing**: Retaining drains as independent entities will simplify post-processing, as they do not influence enzyme concentration variations.\n4. **Priors for Predictive Checks**: Drains will be included as priors to ensure that prior predictive checks can accurately reflect experimental conditions.\n\nThis approach allows flexibility and extensibility for future enhancements and maintains the integrity of the existing reaction\/enzyme framework in Maud.","GenTime":"2024-08-20 00:01:31"}
{"File Name":"platform\/2020-08-14-implement-individual-sorting.md","Context":"## Context\\nShop owners should be able to define custom sorting options for product listings and search result pages out of the administration.\\nIt should be possible to define a system default sorting option for product listings.\\n`Top Results` will be the default on search pages and suggest route, which sorts products by `_score`.\\nCurrently, to define a custom sorting option, you need to define it as a service and tag it as `shopware.sales_channel.product_listing.sorting`.\\nThis is somewhat tedious and makes it impossible to define individual sortings via the administration.\\n","Decision":"From now on, it is possible to define custom sortings via the administration.\\nIndividual sortings will be stored in the database in the table `product_sorting` and its translatable label in the `product_sorting_translation` table.\\nIt is possible to define a system default product listing sorting option, which is stored in `system_default`.`core.listing.defaultSorting`.\\nThis however has no influence on the default `Top Results` sorting on search pages and the suggest route result.\\nTo define custom sorting options via a plugin, you can either write a migration to store them in the database.\\nThis method is recommended, as the sortings can be managed via the administration.\\nThe `product_sorting` table looks like the following:\\n| Column          | Type           | Notes                                                 |\\n| --------------- | -------------- | ----------------------------------------------------- |\\n| `id`            | binary(16)     |                                                       |\\n| `url_key`       | varchar(255)   | Key (unique). Shown in url, when sorting is chosen    |\\n| `priority`      | int unsigned   | Higher priority means, the sorting will be sorted top |\\n| `active`        | tinyint(1) [1] | Inactive sortings will not be shown and will not sort |\\n| `locked`        | tinyint(1) [0] | Locked sortings can not be edited via the DAL         |\\n| `fields`        | json           | JSON of the fields by which to sort the listing       |\\n| `created_at`    | datetime(3)    |                                                       |\\n| `updated_at`    | datetime(3)    |                                                       |\\nThe JSON for the fields column look like this:\\n```json5\\n[\\n{\\n\"field\": \"product.name\",        \/\/ property to sort by (mandatory)\\n\"order\": \"desc\",                \/\/ \"asc\" or \"desc\" (mandatory)\\n\"priority\": 0,                  \/\/ in which order the sorting is to applied (higher priority comes first) (mandatory)\\n\"naturalSorting\": 0\\n},\\n{\\n\"field\": \"product.cheapestPrice\",\\n\"order\": \"asc\",\\n\"priority\": 100,\\n\"naturalSorting\": 0\\n},\\n\/\/ ...\\n]\\n```\\n---\\nOtherwise, you can subscribe to the `ProductListingCriteriaEvent` to add a `ProductSortingEntity` as available sorting on the fly.\\n```php\\n<?php\\nnamespace Shopware\\Core\\Content\\Product\\SalesChannel\\Sorting\\Example;\\nuse Shopware\\Core\\Content\\Product\\Events\\ProductListingCriteriaEvent;\\nuse Shopware\\Core\\Content\\Product\\SalesChannel\\Sorting\\ProductSortingCollection;\\nuse Shopware\\Core\\Content\\Product\\SalesChannel\\Sorting\\ProductSortingEntity;\\nuse Symfony\\Component\\EventDispatcher\\EventSubscriberInterface;\\nclass ExampleListingSubscriber implements EventSubscriberInterface {\\npublic static function getSubscribedEvents(): array\\n{\\nreturn [\\nProductListingCriteriaEvent::class => ['addMyCustomSortingToStorefront', 500],\\n];\\n}\\npublic function addMyCustomSortingToStorefront(ProductListingCriteriaEvent $event): void\\n{\\n\/** @var ProductSortingCollection $availableSortings *\/\\n$availableSortings = $event->getCriteria()->getExtension('sortings') ?? new ProductSortingCollection();\\n$myCustomSorting = new ProductSortingEntity();\\n$myCustomSorting->setId(Uuid::randomHex());\\n$myCustomSorting->setActive(true);\\n$myCustomSorting->setTranslated(['label' => 'My Custom Sorting']);\\n$myCustomSorting->setKey('my-custom-sort');\\n$myCustomSorting->setPriority(5);\\n$myCustomSorting->setFields([\\n[\\n'field' => 'product.name',\\n'order' => 'desc',\\n'priority' => 1,\\n'naturalSorting' => 0,\\n],\\n]);\\n$availableSortings->add($myCustomSorting);\\n$event->getCriteria()->addExtension('sortings', $availableSortings);\\n}\\n}\\n```\\n","tokens":117,"id":4493,"text":"## Context\\nShop owners should be able to define custom sorting options for product listings and search result pages out of the administration.\\nIt should be possible to define a system default sorting option for product listings.\\n`Top Results` will be the default on search pages and suggest route, which sorts products by `_score`.\\nCurrently, to define a custom sorting option, you need to define it as a service and tag it as `shopware.sales_channel.product_listing.sorting`.\\nThis is somewhat tedious and makes it impossible to define individual sortings via the administration.\\n\n\n##Decision\nFrom now on, it is possible to define custom sortings via the administration.\\nIndividual sortings will be stored in the database in the table `product_sorting` and its translatable label in the `product_sorting_translation` table.\\nIt is possible to define a system default product listing sorting option, which is stored in `system_default`.`core.listing.defaultSorting`.\\nThis however has no influence on the default `Top Results` sorting on search pages and the suggest route result.\\nTo define custom sorting options via a plugin, you can either write a migration to store them in the database.\\nThis method is recommended, as the sortings can be managed via the administration.\\nThe `product_sorting` table looks like the following:\\n| Column          | Type           | Notes                                                 |\\n| --------------- | -------------- | ----------------------------------------------------- |\\n| `id`            | binary(16)     |                                                       |\\n| `url_key`       | varchar(255)   | Key (unique). Shown in url, when sorting is chosen    |\\n| `priority`      | int unsigned   | Higher priority means, the sorting will be sorted top |\\n| `active`        | tinyint(1) [1] | Inactive sortings will not be shown and will not sort |\\n| `locked`        | tinyint(1) [0] | Locked sortings can not be edited via the DAL         |\\n| `fields`        | json           | JSON of the fields by which to sort the listing       |\\n| `created_at`    | datetime(3)    |                                                       |\\n| `updated_at`    | datetime(3)    |                                                       |\\nThe JSON for the fields column look like this:\\n```json5\\n[\\n{\\n\"field\": \"product.name\",        \/\/ property to sort by (mandatory)\\n\"order\": \"desc\",                \/\/ \"asc\" or \"desc\" (mandatory)\\n\"priority\": 0,                  \/\/ in which order the sorting is to applied (higher priority comes first) (mandatory)\\n\"naturalSorting\": 0\\n},\\n{\\n\"field\": \"product.cheapestPrice\",\\n\"order\": \"asc\",\\n\"priority\": 100,\\n\"naturalSorting\": 0\\n},\\n\/\/ ...\\n]\\n```\\n---\\nOtherwise, you can subscribe to the `ProductListingCriteriaEvent` to add a `ProductSortingEntity` as available sorting on the fly.\\n```php\\n<?php\\nnamespace Shopware\\Core\\Content\\Product\\SalesChannel\\Sorting\\Example;\\nuse Shopware\\Core\\Content\\Product\\Events\\ProductListingCriteriaEvent;\\nuse Shopware\\Core\\Content\\Product\\SalesChannel\\Sorting\\ProductSortingCollection;\\nuse Shopware\\Core\\Content\\Product\\SalesChannel\\Sorting\\ProductSortingEntity;\\nuse Symfony\\Component\\EventDispatcher\\EventSubscriberInterface;\\nclass ExampleListingSubscriber implements EventSubscriberInterface {\\npublic static function getSubscribedEvents(): array\\n{\\nreturn [\\nProductListingCriteriaEvent::class => ['addMyCustomSortingToStorefront', 500],\\n];\\n}\\npublic function addMyCustomSortingToStorefront(ProductListingCriteriaEvent $event): void\\n{\\n\/** @var ProductSortingCollection $availableSortings *\/\\n$availableSortings = $event->getCriteria()->getExtension('sortings') ?? new ProductSortingCollection();\\n$myCustomSorting = new ProductSortingEntity();\\n$myCustomSorting->setId(Uuid::randomHex());\\n$myCustomSorting->setActive(true);\\n$myCustomSorting->setTranslated(['label' => 'My Custom Sorting']);\\n$myCustomSorting->setKey('my-custom-sort');\\n$myCustomSorting->setPriority(5);\\n$myCustomSorting->setFields([\\n[\\n'field' => 'product.name',\\n'order' => 'desc',\\n'priority' => 1,\\n'naturalSorting' => 0,\\n],\\n]);\\n$availableSortings->add($myCustomSorting);\\n$event->getCriteria()->addExtension('sortings', $availableSortings);\\n}\\n}\\n```\\n","MatchedIds":[4500,2513,4507,2183,285],"Predicted":"## Decision\n\nTo enable shop owners to define custom sorting options through the administration and overcome the limitations of the current system, we will implement the following changes:\n\n1. **Administrative Interface for Custom Sortings:**\n   - Introduce a user-friendly administrative interface that allows shop owners to define and manage custom sorting options without requiring developer intervention or coding.\n   - Provide an interface where custom sortings can be created by specifying the sorting criteria (e.g., price, name, custom attributes) and the order (ascending or descending).\n\n2. **Database Storage for Sortings:**\n   - Store custom sorting options in the database instead of requiring service definitions. This allows for more flexibility and scalability.\n   - Define a new database table to store sorting configurations, including sorting criteria and prioritization.\n\n3. **System Default Sorting:**\n   - Enable shop owners to set a system default sorting option for product listings via the administration interface.\n   - Ensure that `Top Results` remains the default sorting for search pages, using `_score` for relevancy, but allow it to be configurable if needed.\n\n4. **Dynamic Sorting Configuration:**\n   - Implement a dynamic sorting configuration mechanism that reads from the database and applies the appropriate sorting logic during product listing and search result retrieval.\n   - Refactor the current sorting logic to leverage these dynamic configurations, ensuring backward compatibility for existing predefined sortings.\n\n5. **API Endpoint for Sortings:**\n   - Introduce an API endpoint to retrieve available sorting options, including custom definitions, for frontend consumption.\n   - Ensure that this endpoint is secured and performant, considering caching strategies if needed.\n\n6. **Deprecation of Service-Based Sortings:**\n   - Gradually phase out the requirement to define custom sortings as services tagged with `shopware.sales_channel.product_listing.sorting`.\n   - Maintain backward compatibility in the short term and provide a clear migration path to the new database-driven approach in the documentation.\n\nBy implementing these changes, we will give shop owners more control and flexibility over product sorting while simplifying the process of defining and managing sorting options.","GenTime":"2024-08-20 00:01:39"}
{"File Name":"gp-finder\/0002-use-express-web-framework.md","Context":"## Context\\nA web framework is needed in order to provide a level of abstraction over low\\nlevel language features. Effort spent solving non-domain specific problems\\nshould be kept to an absolute minimum. The decision of using Express and Node.js\\nwhere made at (platform level)[https:\/\/github.com\/search?q=topic%3Aconnecting-to-services+org%3Anhsuk&type=Repositories],\\nand the team had experience in creating these types of applications.\\n","Decision":"We will use the [Express](http:\/\/expressjs.com\/) web framework. This will\\nprovide a lot of well\\n[documented](http:\/\/expressjs.com\/en\/resources\/books-blogs.html) features and\\nfunctionality. Whilst providing a fast, unopinionated, minimalist web\\nframework.\\nExpress is one of the most popular web frameworks available for NodeJS and as\\nsuch has been well tested and is deployed across many\\n[organisations](http:\/\/expressjs.com\/en\/resources\/companies-using-express.html).\\n","tokens":96,"id":3470,"text":"## Context\\nA web framework is needed in order to provide a level of abstraction over low\\nlevel language features. Effort spent solving non-domain specific problems\\nshould be kept to an absolute minimum. The decision of using Express and Node.js\\nwhere made at (platform level)[https:\/\/github.com\/search?q=topic%3Aconnecting-to-services+org%3Anhsuk&type=Repositories],\\nand the team had experience in creating these types of applications.\\n\n\n##Decision\nWe will use the [Express](http:\/\/expressjs.com\/) web framework. This will\\nprovide a lot of well\\n[documented](http:\/\/expressjs.com\/en\/resources\/books-blogs.html) features and\\nfunctionality. Whilst providing a fast, unopinionated, minimalist web\\nframework.\\nExpress is one of the most popular web frameworks available for NodeJS and as\\nsuch has been well tested and is deployed across many\\n[organisations](http:\/\/expressjs.com\/en\/resources\/companies-using-express.html).\\n","MatchedIds":[4271,1243,577,4443,3681],"Predicted":"## Decision\nWe will proceed with using the Express framework in conjunction with Node.js. This decision aligns with the platform-level choices and leverages the team's existing experience and expertise. Utilizing Express will allow us to minimize time spent on non-domain specific problems by leveraging its well-documented, fast, and minimalist features. This also ensures that our solution is compatible and consistent with existing applications within the organization.","GenTime":"2024-08-20 00:01:42"}
{"File Name":"LogLady\/0004-implement-styled-components.md","Context":"## Context\\nComplicated with uses of different css rules\\n","Decision":"Only uses styled-components for design\\n","tokens":12,"id":1855,"text":"## Context\\nComplicated with uses of different css rules\\n\n\n##Decision\nOnly uses styled-components for design\\n","MatchedIds":[1125,157,1854,5090,3982],"Predicted":"## Decision\nWe will adopt a CSS-in-JS library such as Styled Components or Emotion to manage our CSS. This approach will help to encapsulate styles within their respective components, reducing complexity and avoiding conflicts between different CSS rules. Additionally, it simplifies maintenance and promotes consistency across the codebase.","GenTime":"2024-08-20 00:01:48"}
{"File Name":"ios-architecture-decision-logs\/0007-presenter-casting-as-delegate.md","Context":"## Context\\nWe faced a problem that different implementations of presenter's conforming delegates\\n","Decision":"We decided to add `var xxxxDelegate { get }` to `xxPresenterInterface` and passing this `presenter`  to any delegate like:  `(delegate: presenter.xxxxDelegate)` instead of  `(delegate: presenter as? xxxDelegate)` or `(delegate: presenter as! xxxDelegate)`\\n","tokens":17,"id":4980,"text":"## Context\\nWe faced a problem that different implementations of presenter's conforming delegates\\n\n\n##Decision\nWe decided to add `var xxxxDelegate { get }` to `xxPresenterInterface` and passing this `presenter`  to any delegate like:  `(delegate: presenter.xxxxDelegate)` instead of  `(delegate: presenter as? xxxDelegate)` or `(delegate: presenter as! xxxDelegate)`\\n","MatchedIds":[4983,4556,2502,3853,1792],"Predicted":"## Decision\nIn order to ensure consistency and maintainability across different implementations of presenter conforming delegates, we will establish a standard protocol that all presenter delegates must conform to. This protocol will define common methods and properties that are required for any presenter delegate. \n\n1. **Define a Protocol**: We will create a protocol named `PresenterDelegate` that will include all required methods and properties.\n    ```swift\n    protocol PresenterDelegate {\n        func updateView()\n        func presentError(_ error: Error)\n        \/\/ Additional common methods\n    }\n    ```\n\n2. **Conform to Protocol**: All presenter delegates must implement this protocol. This will ensure uniformity and reduce discrepancies in different implementations.\n    ```swift\n    class SomePresenter: PresenterDelegate {\n        func updateView() {\n            \/\/ Implementation\n        }\n\n        func presentError(_ error: Error) {\n            \/\/ Implementation\n        }\n\n        \/\/ Additional methods\n    }\n    ```\n\n3. **Enforce Protocol Usage**: Code reviewers and automated tests will check to ensure that all presenter delegates are conforming to the `PresenterDelegate` protocol. \n\n4. **Documentation and Comments**: We will document the protocol and provide guidelines on implementing it effectively to help developers follow the standard.\n\nBy adopting this approach, we can ensure that different implementations of presenter delegates are consistent and reliable across the application.","GenTime":"2024-08-20 00:01:56"}
{"File Name":"agentframework\/0002-use-es6-proxy-to-provide-transparent-interceptor-layer.md","Context":"## Context\\nIn ES5, implement interceptor will modify existing class and this operation is not reversible.\\nIt may cause compatibility issue with other frameworks.\\n","Decision":"We will add ES6 Proxy for a transparent layer which not modifying user's class\/object.\\n","tokens":31,"id":2360,"text":"## Context\\nIn ES5, implement interceptor will modify existing class and this operation is not reversible.\\nIt may cause compatibility issue with other frameworks.\\n\n\n##Decision\nWe will add ES6 Proxy for a transparent layer which not modifying user's class\/object.\\n","MatchedIds":[2687,2359,2689,1890,577],"Predicted":"## Decision\nTo avoid potential compatibility issues with other frameworks, we will avoid modifying existing classes directly in ES5. Instead, we will use a design pattern such as decorating or proxying to achieve interceptor functionality without permanently altering the class definitions. This approach will ensure greater flexibility and maintainability in our codebase.","GenTime":"2024-08-20 00:02:01"}
{"File Name":"content-data-api\/adr-007-etl-publishing-api-content-store.md","Context":"## Context\\nAs [per this Trello card][1], we want to populate the Content Items' dimension with the latest changes that result of editing content.\\n","Decision":"Addressing the ETL process for Content Items this way:\\n1. The first time, we load all the content items via [publishing-api][3]. We retrieve all the content-ids and paths of all the content items that are live.\\n2. For each Content Item we will get the JSON from the [Content Store][2], and we will extract the attributes that we need to fulfil the immediate needs. We will also persist all the JSON to be able to extract other attributes in the future.\\n3. On a daily basis, we will be listening to [publishing-api][3] events, in the same way that [Rummager][4] or [Email Alert Service][5] do. Once we receive a change for the content item, we will automatically update the content items dimension with the new approach.\\n### Benefits:\\n1. This is more aligned with GOV.UK architecture.\\n1. This is very light and efficient. It also embrace simple code as the ETL process for Content Items is almost trivial.\\n","tokens":33,"id":1857,"text":"## Context\\nAs [per this Trello card][1], we want to populate the Content Items' dimension with the latest changes that result of editing content.\\n\n\n##Decision\nAddressing the ETL process for Content Items this way:\\n1. The first time, we load all the content items via [publishing-api][3]. We retrieve all the content-ids and paths of all the content items that are live.\\n2. For each Content Item we will get the JSON from the [Content Store][2], and we will extract the attributes that we need to fulfil the immediate needs. We will also persist all the JSON to be able to extract other attributes in the future.\\n3. On a daily basis, we will be listening to [publishing-api][3] events, in the same way that [Rummager][4] or [Email Alert Service][5] do. Once we receive a change for the content item, we will automatically update the content items dimension with the new approach.\\n### Benefits:\\n1. This is more aligned with GOV.UK architecture.\\n1. This is very light and efficient. It also embrace simple code as the ETL process for Content Items is almost trivial.\\n","MatchedIds":[4875,1866,3845,779,2403],"Predicted":"## Decision\nWe will implement an event-sourced mechanism to update the Content Items' dimension with the latest changes resulting from content edits. Specifically, whenever an edit is made to a content item, an event will be published to a message queue (e.g., Kafka or RabbitMQ). A dedicated consumer service will listen for these events and update the Content Items' dimension accordingly in the data warehouse. This approach ensures that the Content Items' dimension is always synchronized with the latest content changes.","GenTime":"2024-08-20 00:02:05"}
{"File Name":"jabref\/0014-separate-URL-creation-to-enable-proper-logging.md","Context":"## Context and Problem Statement\\nFetchers are failing.\\nThe reason why they are failing needs to be investigated.\\n* Claim 1: Knowing the URL which was used to query the fetcher eases debugging\\n* Claim 2: Somehow logging the URL eases debugging (instead of showing it in the debugger only)\\nHow to properly log the URL used for fetching?\\n## Decision Drivers\\n* Code should be easy to read\\n* Include URL in the exception instead of logging in case an exception is thrown already (see <https:\/\/howtodoinjava.com\/best-practices\/java-exception-handling-best-practices\/#6>)\\n","Decision":"* Code should be easy to read\\n* Include URL in the exception instead of logging in case an exception is thrown already (see <https:\/\/howtodoinjava.com\/best-practices\/java-exception-handling-best-practices\/#6>)\\nChosen option: \"Separate URL creation\", because comes out best \\(see below\\).\\n","tokens":132,"id":4748,"text":"## Context and Problem Statement\\nFetchers are failing.\\nThe reason why they are failing needs to be investigated.\\n* Claim 1: Knowing the URL which was used to query the fetcher eases debugging\\n* Claim 2: Somehow logging the URL eases debugging (instead of showing it in the debugger only)\\nHow to properly log the URL used for fetching?\\n## Decision Drivers\\n* Code should be easy to read\\n* Include URL in the exception instead of logging in case an exception is thrown already (see <https:\/\/howtodoinjava.com\/best-practices\/java-exception-handling-best-practices\/#6>)\\n\n\n##Decision\n* Code should be easy to read\\n* Include URL in the exception instead of logging in case an exception is thrown already (see <https:\/\/howtodoinjava.com\/best-practices\/java-exception-handling-best-practices\/#6>)\\nChosen option: \"Separate URL creation\", because comes out best \\(see below\\).\\n","MatchedIds":[2975,1523,4742,4852,4741],"Predicted":"## Decision\nChosen option: \"Log URL within exception messages\"\n\nRationale:\n* Logging the URL within the exception messages ensures that relevant information is captured at the point of failure, which simplifies debugging.\n* This approach adheres to best practices by embedding helpful information directly in the exception, making it easier to diagnose the problem without additional logs.\n* Including the URL in the exception message maintains cleaner code as it avoids duplicative logging and adheres to the single responsibility principle by capturing all failure-related information in one place.\n* This approach also aligns with best practices mentioned in the referenced article.","GenTime":"2024-08-20 00:02:11"}
{"File Name":"openchs-adr\/0012-create-a-generic-relationship-framework-to-link-between-mother-and-child.md","Context":"## Context\\nDuring a delivery for a mother in the mother program, we will need to create one or more new children. Filling in details during delivery and during PNC visits will need switching between them easy.\\nAt the same time, we are also thinking of creating the concept of a family. Here, the individuals of a family will be linked to the head of the household through a relationship.\\nWe need the modeling of a relationship to be a generic structure that can support both these use cases.\\n","Decision":"Create an option to map relationships between individuals, with relationship being a concept orthogonal to families. Relationships between individuals can be anything (family relationships, or even relationships to ASHA worker etc if required).\\nRelationships will be two-way with different values between the two. We will not build (atleast for now) the ability to automatically deduce transitive relationships.\\n","tokens":103,"id":2829,"text":"## Context\\nDuring a delivery for a mother in the mother program, we will need to create one or more new children. Filling in details during delivery and during PNC visits will need switching between them easy.\\nAt the same time, we are also thinking of creating the concept of a family. Here, the individuals of a family will be linked to the head of the household through a relationship.\\nWe need the modeling of a relationship to be a generic structure that can support both these use cases.\\n\n\n##Decision\nCreate an option to map relationships between individuals, with relationship being a concept orthogonal to families. Relationships between individuals can be anything (family relationships, or even relationships to ASHA worker etc if required).\\nRelationships will be two-way with different values between the two. We will not build (atleast for now) the ability to automatically deduce transitive relationships.\\n","MatchedIds":[4021,920,915,4307,2183],"Predicted":"## Decision\nWe will implement a generic relationship model that supports both the creation of new children during delivery and the concept of a family with relationships to a head of the household. The relationship model will include the following components:\n\n1. **Individual**: Each person (mother, child, head of household, family members) will be represented as an individual entity with personal details.\n\n2. **Relationship**: A table or structure that models the relationship between individuals. This will include:\n   - `Individual ID`: The unique identifier for the individual.\n   - `Related Individual ID`: The unique identifier for the related individual.\n   - `Relationship Type`: A descriptor field (e.g., mother, child, head of household, spouse, etc.)\n   - `Start Date`: The date when the relationship was established.\n   - `End Date`: The date when the relationship ended (if applicable).\n\n3. **Family**: A grouping structure for families, tied together using relationships.\n\n### Benefits\n- **Flexibility**: The generic nature of the model allows us to add more relationship types in the future without significant refactoring.\n- **Consistency**: Ensures a uniform way to address relationships across different use cases.\n- **Ease of Use**: Simplifies switching between different individuals (e.g., children during delivery) by leveraging the relationship structure.\n\n### Example\n#### Relationship Table\n| Individual ID | Related Individual ID | Relationship Type | Start Date | End Date  |\n|---------------|-----------------------|-------------------|------------|-----------|\n| 1             | 2                     | Mother            | 2023-05-01 | NULL      |\n| 1             | 3                     | Child             | 2023-08-01 | NULL      |\n| 1             | 4                     | Head of Household | 2023-01-01 | NULL      |\n\nThis structure allows us to query and manage relationships effectively while supporting both immediate and extended family contexts.","GenTime":"2024-08-20 00:02:18"}
{"File Name":"atlasdb\/0015-batch-asynchronous-post-transaction-unlock-calls.md","Context":"## Context\\nAs part of the AtlasDB transaction protocol, write transactions acquire locks from the lock service. They typically\\nacquire two types of locks:\\n- An *immutable timestamp lock*, which AtlasDB uses as an estimate of the oldest running write transaction. The\\nstate of the database at timestamps less than the lowest active immutable timestamp lock is considered immutable, and\\nthus eligible for cleanup by Sweep.\\n- *Row locks* and *cell locks* (depending on the conflict handler of the tables involved in a write transaction) for\\nrows or cells being written to. These locks are used to prevent multiple concurrent transactions from simultaneously\\nwriting to the same rows and committing.\\nTransactions may also acquire additional locks as part of AtlasDB's pre-commit condition framework. These conditions\\nare arbitrary and we thus do not focus on optimising these.\\nAfter a transaction commits, it needs to release the locks it acquired as part of the transaction protocol. Releasing\\nthe immutable timestamp lock helps AtlasDB keep as few stale versions of data around as possible (which factors into\\nthe performance of certain read query patterns); releasing row and cell locks allows other transactions that need to\\nupdate these to proceed.\\nCurrently, these locks are released synchronously and separately after a transaction commits. Thus, there is an\\noverhead of two lock service calls between a transaction successfully committing and control being returned to\\nthe user.\\nCorrectness of the transaction protocol is not compromised even if these locks are not released (though an effort\\nshould be made to release them for performance reasons). Consider that it is permissible for an AtlasDB client to\\ncrash after performing `putUnlessExists` into the transactions table, in which case the transaction is considered\\ncommitted.\\n","Decision":"Instead of releasing the locks synchronously, release them asynchronously so that control is returned to the user very\\nquickly after transaction commit. However, maintaining relatively low latency between transaction commit and unlock\\nis important to avoid unnecessarily blocking other writers or sweep.\\nTwo main designs were considered:\\n1. Maintain a thread pool of `N` consumer threads and a work queue of tokens to be unlocked. Transactions that commit\\nplace their lock tokens on this queue; consumers pull tokens off the queue and make unlock requests to the lock\\nservice.\\n2. Maintain a concurrent set of tokens that need to be unlocked; transactions that commit place their lock tokens\\nin this set, and an executor asynchronously unlocks these tokens.\\nSolution 1 is simpler than solution 2 in terms of implementation. However, we opted for solution 2 for various reasons.\\nFirstly, the latency provided by solution 1 is very sensitive to choosing `N` well - choosing too small `N` means that\\nthere will be a noticeable gap between transaction commit and the relevant locks being unlocked. Conversely, choosing\\ntoo large `N` incurs unnecessary overhead. Choosing a value of `N` in general is difficult and would likely require\\ntuning depending on individual deployment and product read and write patterns, which is unscalable.\\nSolution 2 also decreases the load placed on the lock service, as fewer unlock requests need to be made.\\nIn our implementation of solution 2, we use a single-threaded executor. This means that on average the additional\\nlatency we incur is about 0.5 RPCs on the lock service (assuming that that makes up a majority of time spent in\\nunlocking tokens - it is the only network call involved).\\n### tryUnlock() API\\n`TimelockService` now exposes a `tryUnlock()` API, which functions much like a regular `unlock()` except that the user\\ndoes not need to wait for the operation to complete. This API is only exposed in Java (not over HTTP).\\nThis is implemented as a new default method on the `TimelockService` that delegates to `unlock()`; usefully, remote\\nFeign proxies calling `tryUnlock()` will make an RPC for standard `unlock()`. This also gives us backwards\\ncompatiblity; a new AtlasDB\/TimeLock client can talk to an old TimeLock server that has no knowledge of this endpoint.\\n### Concurrency Model\\nIt is essential that adding an element to the set of outstanding tokens is efficient; yet, we also need to ensure that\\nno token is left behind (at least indefinitely). We thus guard the concurrent set by a (Java) lock that permits both\\nexclusive and shared modes of access.\\nTransactions that enqueue lock tokens to be unlocked perform the following steps:\\n1. Acquire the set lock in shared mode.\\n2. Read a reference to the set of tokens to be unlocked.\\n3. Add lock tokens to the set of tokens to be unlocked.\\n4. Release the set lock.\\n5. If no task is scheduled, then schedule a task by setting a 'task scheduled' boolean flag.\\nThis uses compare-and-set, so only one task will be scheduled while no task is running.\\nFor this to be safe, the set used must be a concurrent set.\\nThe task that unlocks tokens in the set performs the following steps:\\n1. Un-set the task scheduled flag.\\n2. Acquire the set lock in exclusive mode.\\n3. Read a reference to the set of tokens to be unlocked.\\n4. Write the set reference to point to a new set.\\n5. Release the set lock.\\n6. Unlock all tokens in the set read in step 3.\\nThis model is trivially _safe_, in that no token that wasn't enqueued can ever be unlocked, since all tokens that can\\never become unlocked must have been added in step 3 of enqueueing, and unlocking a lock token is idempotent modulo\\na UUID clash.\\nMore interestingly, we can guarantee _liveness_ - every token that was enqueued will be unlocked in the absence of\\nthread death. If an enqueue has a successful compare-and-set in step 5, then the token must be in the set\\n(and is visible, because we synchronize on the set lock). If an enqueue does _not_ have a successful compare-and-set,\\nthen some thread must already be scheduled to perform the unlock, and once it does the token must be in the relevant\\nset (and again must be visible, because we synchronize on the set lock).\\nTo avoid issues with starving unlocks, we use a fair lock scheme. Once the unlocking thread attempts to acquire the set\\nlock, enqueues that are still running may finish, but fresh calls to enqueue will only be able to acquire the set lock\\nafter the unlocking thread has acquired and released it. This may have lower throughput than an unfair lock,\\nbut we deemed it necessary as 'readers' (committing transactions) far exceed 'writers' (the unlocking thread) -\\notherwise, the unlocking thread might be starved of the lock.\\n### TimeLock Failures\\nIn some embodiments, the lock service is provided by a remote TimeLock server that may fail requests. There is retry\\nlogic at the transport layer underneath us.\\nPreviously, running a transaction task would throw an exception if unlocking row locks or the immutable timestamp\\nfailed; we now allow user code to proceed and only emit diagnostic logs indicating that the unlock operation failed.\\nThis is a safe change, as throwing would not make the locks become available again, and user code cannot safely\\nassume that locks used by a transaction are free after it commits (since another thread may well have acquired them).\\nIn practice, locks will be released after a timeout if they are not refreshed by a client. This means that not\\nretrying unlocks is safe, as long as we do not continue to attempt to refresh the lock. AtlasDB clients automatically\\nrefresh locks they acquire; we ensure that a token being unlocked is synchronously removed from the set of locks\\nto refresh *before* it is put on the unlock queue.\\n","tokens":355,"id":3096,"text":"## Context\\nAs part of the AtlasDB transaction protocol, write transactions acquire locks from the lock service. They typically\\nacquire two types of locks:\\n- An *immutable timestamp lock*, which AtlasDB uses as an estimate of the oldest running write transaction. The\\nstate of the database at timestamps less than the lowest active immutable timestamp lock is considered immutable, and\\nthus eligible for cleanup by Sweep.\\n- *Row locks* and *cell locks* (depending on the conflict handler of the tables involved in a write transaction) for\\nrows or cells being written to. These locks are used to prevent multiple concurrent transactions from simultaneously\\nwriting to the same rows and committing.\\nTransactions may also acquire additional locks as part of AtlasDB's pre-commit condition framework. These conditions\\nare arbitrary and we thus do not focus on optimising these.\\nAfter a transaction commits, it needs to release the locks it acquired as part of the transaction protocol. Releasing\\nthe immutable timestamp lock helps AtlasDB keep as few stale versions of data around as possible (which factors into\\nthe performance of certain read query patterns); releasing row and cell locks allows other transactions that need to\\nupdate these to proceed.\\nCurrently, these locks are released synchronously and separately after a transaction commits. Thus, there is an\\noverhead of two lock service calls between a transaction successfully committing and control being returned to\\nthe user.\\nCorrectness of the transaction protocol is not compromised even if these locks are not released (though an effort\\nshould be made to release them for performance reasons). Consider that it is permissible for an AtlasDB client to\\ncrash after performing `putUnlessExists` into the transactions table, in which case the transaction is considered\\ncommitted.\\n\n\n##Decision\nInstead of releasing the locks synchronously, release them asynchronously so that control is returned to the user very\\nquickly after transaction commit. However, maintaining relatively low latency between transaction commit and unlock\\nis important to avoid unnecessarily blocking other writers or sweep.\\nTwo main designs were considered:\\n1. Maintain a thread pool of `N` consumer threads and a work queue of tokens to be unlocked. Transactions that commit\\nplace their lock tokens on this queue; consumers pull tokens off the queue and make unlock requests to the lock\\nservice.\\n2. Maintain a concurrent set of tokens that need to be unlocked; transactions that commit place their lock tokens\\nin this set, and an executor asynchronously unlocks these tokens.\\nSolution 1 is simpler than solution 2 in terms of implementation. However, we opted for solution 2 for various reasons.\\nFirstly, the latency provided by solution 1 is very sensitive to choosing `N` well - choosing too small `N` means that\\nthere will be a noticeable gap between transaction commit and the relevant locks being unlocked. Conversely, choosing\\ntoo large `N` incurs unnecessary overhead. Choosing a value of `N` in general is difficult and would likely require\\ntuning depending on individual deployment and product read and write patterns, which is unscalable.\\nSolution 2 also decreases the load placed on the lock service, as fewer unlock requests need to be made.\\nIn our implementation of solution 2, we use a single-threaded executor. This means that on average the additional\\nlatency we incur is about 0.5 RPCs on the lock service (assuming that that makes up a majority of time spent in\\nunlocking tokens - it is the only network call involved).\\n### tryUnlock() API\\n`TimelockService` now exposes a `tryUnlock()` API, which functions much like a regular `unlock()` except that the user\\ndoes not need to wait for the operation to complete. This API is only exposed in Java (not over HTTP).\\nThis is implemented as a new default method on the `TimelockService` that delegates to `unlock()`; usefully, remote\\nFeign proxies calling `tryUnlock()` will make an RPC for standard `unlock()`. This also gives us backwards\\ncompatiblity; a new AtlasDB\/TimeLock client can talk to an old TimeLock server that has no knowledge of this endpoint.\\n### Concurrency Model\\nIt is essential that adding an element to the set of outstanding tokens is efficient; yet, we also need to ensure that\\nno token is left behind (at least indefinitely). We thus guard the concurrent set by a (Java) lock that permits both\\nexclusive and shared modes of access.\\nTransactions that enqueue lock tokens to be unlocked perform the following steps:\\n1. Acquire the set lock in shared mode.\\n2. Read a reference to the set of tokens to be unlocked.\\n3. Add lock tokens to the set of tokens to be unlocked.\\n4. Release the set lock.\\n5. If no task is scheduled, then schedule a task by setting a 'task scheduled' boolean flag.\\nThis uses compare-and-set, so only one task will be scheduled while no task is running.\\nFor this to be safe, the set used must be a concurrent set.\\nThe task that unlocks tokens in the set performs the following steps:\\n1. Un-set the task scheduled flag.\\n2. Acquire the set lock in exclusive mode.\\n3. Read a reference to the set of tokens to be unlocked.\\n4. Write the set reference to point to a new set.\\n5. Release the set lock.\\n6. Unlock all tokens in the set read in step 3.\\nThis model is trivially _safe_, in that no token that wasn't enqueued can ever be unlocked, since all tokens that can\\never become unlocked must have been added in step 3 of enqueueing, and unlocking a lock token is idempotent modulo\\na UUID clash.\\nMore interestingly, we can guarantee _liveness_ - every token that was enqueued will be unlocked in the absence of\\nthread death. If an enqueue has a successful compare-and-set in step 5, then the token must be in the set\\n(and is visible, because we synchronize on the set lock). If an enqueue does _not_ have a successful compare-and-set,\\nthen some thread must already be scheduled to perform the unlock, and once it does the token must be in the relevant\\nset (and again must be visible, because we synchronize on the set lock).\\nTo avoid issues with starving unlocks, we use a fair lock scheme. Once the unlocking thread attempts to acquire the set\\nlock, enqueues that are still running may finish, but fresh calls to enqueue will only be able to acquire the set lock\\nafter the unlocking thread has acquired and released it. This may have lower throughput than an unfair lock,\\nbut we deemed it necessary as 'readers' (committing transactions) far exceed 'writers' (the unlocking thread) -\\notherwise, the unlocking thread might be starved of the lock.\\n### TimeLock Failures\\nIn some embodiments, the lock service is provided by a remote TimeLock server that may fail requests. There is retry\\nlogic at the transport layer underneath us.\\nPreviously, running a transaction task would throw an exception if unlocking row locks or the immutable timestamp\\nfailed; we now allow user code to proceed and only emit diagnostic logs indicating that the unlock operation failed.\\nThis is a safe change, as throwing would not make the locks become available again, and user code cannot safely\\nassume that locks used by a transaction are free after it commits (since another thread may well have acquired them).\\nIn practice, locks will be released after a timeout if they are not refreshed by a client. This means that not\\nretrying unlocks is safe, as long as we do not continue to attempt to refresh the lock. AtlasDB clients automatically\\nrefresh locks they acquire; we ensure that a token being unlocked is synchronously removed from the set of locks\\nto refresh *before* it is put on the unlock queue.\\n","MatchedIds":[3092,1558,3094,3099,1990],"Predicted":"## Decision\nWe decided to batch the release of the locks into a single asynchronous operation, because:\n\n1. **Performance Improvement**: Combining the release of the immutable timestamp lock and row\/cell locks into a single asynchronous operation reduces the transaction commit overhead, improving performance.\n2. **Non-blocking User Control**: By making the lock release asynchronous, we return control to the user faster after a transaction commits, enhancing user experience.\n3. **Correctness is Maintained**: The transaction protocol\u2019s correctness is not compromised even if the locks are not released immediately. This aligns well with the fact that crashing after committing but before releasing locks is permissible.\n4. **Decreased Operational Complexity**: Reduces the number of lock service calls, simplifying the transaction commit workflow and decreasing potential points of failure.","GenTime":"2024-08-20 00:02:24"}
{"File Name":"dogma\/0011-message-timing-information.md","Context":"## Context\\nWe need to decide whether message timing information should be exposed via the\\nAPI. In this context \"timing information\" refers to important points in time\\nthroughout the lifecycle of a message.\\nThe initial rationale for *not* exposing these timestamps was that any business\\nlogic that depends on time in some way should explicitly include any timing\\ninformation within the message itself. We call such logic \"time-based\" and the\\napproach of including explicit timing information \"modeling time\".\\n","Decision":"The sections below focus on each of the message roles, their respective\\ntimestamps of interest, and the decisions made in each case.\\n### Command Messages\\nWe believe the existing requirement that the application \"model time\" is still\\nappropriate for command messages. The time at which the command message is\\ncreated or enqueued is irrelevant; any time information relevant to the domain\\nlogic should be included in the message itself.\\n**We have decided not to expose the command creation time.**\\n### Event Messages\\nThe time at which an event is recorded is a fundamental property of the event\\nitself. Put another way, every event occurs at some time regardless of whether\\nthe domain is time-based.\\nFurthermore, the time at which the event occurs may be relevant to some\\nancillary domain logic that is *triggered* by the event, even if the aggregate\\nthat *produced* the event has no time-based logic.\\nThe inclusion of the \"occurred time\" as a fundamental property of the event is\\nsupported by [Implementing Domain Driven\\nDesign](https:\/\/www.amazon.com\/Implementing-Domain-Driven-Design-Vaughn-Vernon\/dp\/0321834577),\\nChapter 8, in the \"Modeling Events\" section.\\n**We have decided to include a `RecordedAt()` method on `ProcessEventScope` and `ProjectionEventScope`.**\\nIn actuality, a `Time()` method had already been added to `ProjectionEventScope`\\nwithout any supporting ADR, this method is to be renamed.\\n### Timeout Messages\\nThe time at which a timeout message is scheduled to be handled is a fundamental\\nproperty of the timeout concept.\\nBy definition, the use of a timeout message indicates that there is time-based\\nlogic. It seems like an unnecessary imposition to require the application\\ndeveloper to include the scheduled time in the message.\\n**We have decided to include a `ScheduledFor()` method on `ProcessTimeoutScope`.**\\n","tokens":101,"id":1610,"text":"## Context\\nWe need to decide whether message timing information should be exposed via the\\nAPI. In this context \"timing information\" refers to important points in time\\nthroughout the lifecycle of a message.\\nThe initial rationale for *not* exposing these timestamps was that any business\\nlogic that depends on time in some way should explicitly include any timing\\ninformation within the message itself. We call such logic \"time-based\" and the\\napproach of including explicit timing information \"modeling time\".\\n\n\n##Decision\nThe sections below focus on each of the message roles, their respective\\ntimestamps of interest, and the decisions made in each case.\\n### Command Messages\\nWe believe the existing requirement that the application \"model time\" is still\\nappropriate for command messages. The time at which the command message is\\ncreated or enqueued is irrelevant; any time information relevant to the domain\\nlogic should be included in the message itself.\\n**We have decided not to expose the command creation time.**\\n### Event Messages\\nThe time at which an event is recorded is a fundamental property of the event\\nitself. Put another way, every event occurs at some time regardless of whether\\nthe domain is time-based.\\nFurthermore, the time at which the event occurs may be relevant to some\\nancillary domain logic that is *triggered* by the event, even if the aggregate\\nthat *produced* the event has no time-based logic.\\nThe inclusion of the \"occurred time\" as a fundamental property of the event is\\nsupported by [Implementing Domain Driven\\nDesign](https:\/\/www.amazon.com\/Implementing-Domain-Driven-Design-Vaughn-Vernon\/dp\/0321834577),\\nChapter 8, in the \"Modeling Events\" section.\\n**We have decided to include a `RecordedAt()` method on `ProcessEventScope` and `ProjectionEventScope`.**\\nIn actuality, a `Time()` method had already been added to `ProjectionEventScope`\\nwithout any supporting ADR, this method is to be renamed.\\n### Timeout Messages\\nThe time at which a timeout message is scheduled to be handled is a fundamental\\nproperty of the timeout concept.\\nBy definition, the use of a timeout message indicates that there is time-based\\nlogic. It seems like an unnecessary imposition to require the application\\ndeveloper to include the scheduled time in the message.\\n**We have decided to include a `ScheduledFor()` method on `ProcessTimeoutScope`.**\\n","MatchedIds":[280,1606,2316,307,786],"Predicted":"## Decision\nExpose message timing information via the API. \n\n### Reasons:\n1. **Consistency and Transparency**: Providing timing information ensures consistency across different parts of the system and enhances transparency in message lifecycle tracking.\n2. **Debugging and Monitoring**: Exposing timing data aids in debugging and monitoring the system more effectively by allowing developers to trace and profile message throughput and latency.\n3. **Simplification of Business Logic**: Including timing information directly in the API reduces the need for duplicative time-based logic in various parts of the system, centralizing the maintenance and improving clarity.\n4. **Flexibility**: Clients that do not require timing information can ignore it, while those that need it can utilize it for more advanced processing and analytics.\n5. **Future-Proofing**: Exposing timing information now ensures that any future requirements for time-based processing can be met without needing additional code changes to expose these details later on. \n\nBy including timing information in the API, we provide a robust framework for both current and future needs, improving maintainability and operational efficiency.","GenTime":"2024-08-20 00:02:28"}
{"File Name":"Head-Start-TTADP\/0011-monitoring.md","Context":"## Context\\nTTA Smart Hub requires a continuous monitoring solution to ensure uptime and error resolution.\\n","Decision":"TTA Smart Hub will utilize New Relic for Monitoring.\\n* Integrates easily with front end and backend code\\n* Track performance metrics and errors\\n* FedRAMP approved\\n* Handles alerting both by itself, and via integrations with more flexible alerting platforms as we grow.\\n","tokens":20,"id":1171,"text":"## Context\\nTTA Smart Hub requires a continuous monitoring solution to ensure uptime and error resolution.\\n\n\n##Decision\nTTA Smart Hub will utilize New Relic for Monitoring.\\n* Integrates easily with front end and backend code\\n* Track performance metrics and errors\\n* FedRAMP approved\\n* Handles alerting both by itself, and via integrations with more flexible alerting platforms as we grow.\\n","MatchedIds":[3150,3469,1167,3691,3370],"Predicted":"## Decision\nWe will implement Datadog as the continuous monitoring solution for the TTA Smart Hub. Datadog provides comprehensive monitoring and analytics for infrastructure, application performance, and log management. With its ability to integrate with various platforms and technologies, Datadog will enable us to monitor and ensure the uptime of TTA Smart Hub, promptly detect and resolve errors, and maintain high service reliability. Additionally, Datadog's alerting and notification capabilities will allow the team to proactively address potential issues before they impact users.","GenTime":"2024-08-20 00:02:34"}
{"File Name":"meadow\/0022-iiif-manifests.md","Context":"## Context\\nCurrently requests for public manifests are routed through Donut in order to be re-cached in S3 if needed. We need to plan a strategy to handle IIIF manifest requests for manifests which were written by Meadow and should not be routed through Donut.\\n","Decision":"- Manifests will be written for all works (public\/private\/restricted) on create\/update.\\n- IIIF Manifests will be written on to the `public` directory of the existing stack-\\*-pyramids bucket, and will live alongside existing Donut manifests.\\n- Requests for public manifests moving forward will route from the API gateway to a new lambda which will check Elasticsearch for the host application. Then it will either route directly to the S3 Pyramids `\/public` folder (Meadow) or to Donut. (This is temporary, until Donut content is migrated.)\\n- Meadow will use these manifests internally\\n","tokens":55,"id":3852,"text":"## Context\\nCurrently requests for public manifests are routed through Donut in order to be re-cached in S3 if needed. We need to plan a strategy to handle IIIF manifest requests for manifests which were written by Meadow and should not be routed through Donut.\\n\n\n##Decision\n- Manifests will be written for all works (public\/private\/restricted) on create\/update.\\n- IIIF Manifests will be written on to the `public` directory of the existing stack-\\*-pyramids bucket, and will live alongside existing Donut manifests.\\n- Requests for public manifests moving forward will route from the API gateway to a new lambda which will check Elasticsearch for the host application. Then it will either route directly to the S3 Pyramids `\/public` folder (Meadow) or to Donut. (This is temporary, until Donut content is migrated.)\\n- Meadow will use these manifests internally\\n","MatchedIds":[4056,3369,2578,84,1202],"Predicted":"## Decision\n### New Routing and Caching Strategy\n\n1. **Direct Routing for Meadow-written Manifests:**\n   Requests for IIIF manifests created by Meadow will be routed directly to Meadow's endpoint, bypassing Donut.\n\n2. **Identification Mechanism:**\n   Implement a unique identifier (such as a custom header or a specific URL pattern) that distinguishes Meadow-written manifests from others. This will help the reverse proxy or routing layer to apply the correct routing logic.\n\n3. **Updated Proxy Configuration:**\n   Update the reverse proxy configuration to recognize Meadow's identifier and route those requests directly to Meadow's service, while continuing to route other manifest requests through Donut for S3 re-caching as needed.\n\n4. **Optional Caching Layer:**\n   Introduce an optional caching layer specifically for Meadow-written manifests if required. This caching layer can be S3 or an alternative caching mechanism, but it will be managed independently of Donut to avoid any conflicts.\n\n5. **Monitoring and Logging:**\n   Implement logging and monitoring for the new routing logic to ensure requests are routed correctly and to quickly identify and resolve any issues that may arise.\n\n### Implementation Plan\n1. **Define Identifiers:**\n   Collaborate with the Meadow team to define a unique identifier for Meadow-written manifests.\n\n2. **Update Proxy Configuration:**\n   Modify the reverse proxy settings to recognize and appropriately route Meadow-specific requests.\n\n3. **Deploy Optional Caching Layer:**\n   If an additional caching mechanism is required, set it up and ensure it integrates seamlessly with the existing architecture.\n\n4. **Testing:**\n   Thoroughly test the new routing mechanism in a staging environment before deploying to production.\n\n5. **Monitoring Setup:**\n   Configure monitoring and logging tools to track the flow of requests and performance metrics.\n\nBy implementing this strategy, we ensure that Meadow-written IIIF manifest requests are handled efficiently and correctly without unnecessary routing through Donut.","GenTime":"2024-08-20 00:02:45"}
{"File Name":"CAFE5\/0002-calculate-prior-from-user-input.md","Context":"## Context\\nA prior distribution needs to be specified to accurately calculate probabilities. The prior can\\nbe calculated in many different ways. Hopefully this decision reflects the least surprising\\nresults for the user.\\n","Decision":"The prior will be calculated as follows:\\n* if -p specified on command line: calculate a Poisson distribution with the specified lambda\\n* if -f specified on command line: The user has specified a root distribution. Use that. Issue a warning if the user has ALSO specified a Poisson lambda\\n* if -i specified on command line: Estimate a Poisson distribution from the families provided.\\n* Otherwise, use a uniform distribution. Issue a warning as this is not a very reasonable prior.\\n","tokens":40,"id":1747,"text":"## Context\\nA prior distribution needs to be specified to accurately calculate probabilities. The prior can\\nbe calculated in many different ways. Hopefully this decision reflects the least surprising\\nresults for the user.\\n\n\n##Decision\nThe prior will be calculated as follows:\\n* if -p specified on command line: calculate a Poisson distribution with the specified lambda\\n* if -f specified on command line: The user has specified a root distribution. Use that. Issue a warning if the user has ALSO specified a Poisson lambda\\n* if -i specified on command line: Estimate a Poisson distribution from the families provided.\\n* Otherwise, use a uniform distribution. Issue a warning as this is not a very reasonable prior.\\n","MatchedIds":[4804,3820,1145,4616,3546],"Predicted":"## Decision\nWe have decided to use a Beta distribution as the prior for calculating probabilities. This choice is motivated by its flexibility and ease of interpretation, as well as its ability to be updated with additional data. The parameters of the Beta distribution will be set based on domain-specific knowledge and past data to ensure the results are intuitive and expected by the user.","GenTime":"2024-08-20 00:02:48"}
{"File Name":"where-away\/0004-test-with-jest.md","Context":"## Context\\nI am familiar with Jest and am consistently happy with it. Also it's pretty\\npopular and widely used so there's good support.\\n","Decision":"Use Jest for testing.\\n","tokens":31,"id":2273,"text":"## Context\\nI am familiar with Jest and am consistently happy with it. Also it's pretty\\npopular and widely used so there's good support.\\n\n\n##Decision\nUse Jest for testing.\\n","MatchedIds":[452,4223,3323,1594,4359],"Predicted":"## Decision\nWe will use Jest as our testing framework.","GenTime":"2024-08-20 00:02:49"}
{"File Name":"kafkarator\/0004-only-for-aiven.md","Context":"## Context\\nNAV currently has Kafka running on-premise, but have plans to move everything to the cloud.\\nKafkarator will create an integration between applications and Kafka, removing some manual work and putting more configuration in code.\\nThe plan is hosted Kafka from Aiven, eliminating the need for maintaining our own Kafka cluster on-premise.\\n","Decision":"Kafkarator will only deal with the future solution using Aiven, and not work for on-premise Kafka.\\n","tokens":70,"id":2837,"text":"## Context\\nNAV currently has Kafka running on-premise, but have plans to move everything to the cloud.\\nKafkarator will create an integration between applications and Kafka, removing some manual work and putting more configuration in code.\\nThe plan is hosted Kafka from Aiven, eliminating the need for maintaining our own Kafka cluster on-premise.\\n\n\n##Decision\nKafkarator will only deal with the future solution using Aiven, and not work for on-premise Kafka.\\n","MatchedIds":[2834,2471,2752,856,3469],"Predicted":"## Decision\n- Migrate from on-premise Kafka to hosted Kafka from Aiven to leverage managed services and reduce infrastructure maintenance overhead.\n- Update Kafkarator to integrate with Aiven's Kafka service, ensuring that all configurations and dependencies are compatible with the hosted environment.\n- Store Kafka configuration details as code within our deployment pipelines to ensure consistency across environments and minimize manual configuration.\n- Implement monitoring and alerting using Prometheus to track the health and performance of the Kafka cluster provided by Aiven.\n- Conduct a phased migration strategy where non-critical applications are moved first to validate the new setup before migrating mission-critical applications.\n- Update developer documentation to reflect the new Kafka setup and provide guidelines for configuring and using the hosted Kafka service from Aiven.","GenTime":"2024-08-20 00:02:53"}
{"File Name":"paas-team-manual\/ADR003-AWS-credentials.html.md","Context":"## Context\\nAmazon Web Services (AWS) are our current Infrastructure as a Service (IaaS)\\nprovider. Our deployment tooling (Concourse, Terraform, BOSH, etc.) and\\nCloud Foundry components (Cloud Controller, RDS broker, blobstore clients,\\netc.) use the APIs to manage or access IaaS resources.\\nThe most common mechanism for authenticating the API calls is to create an\\nIdentify and Access Management (IAM) user with the appropriate permissions,\\ngenerate an Access Key ID and Secret Access Key for that user, and export\\nthose as environment variables. `AWS_ACCESS_KEY_ID` and\\n`AWS_SECRET_ACCESS_KEY` are the standard environment variable names used by\\nmost utilities and libraries.\\nThe problem with this approach is that it's very easy to accidentally leak\\nthe plain text keys. They can appear in output from your shell, which you\\nmight copy+paste into a gist or email when debugging a problem. You might\\nadd them to your shell configuration or include them in a script, which can\\nbe pushed to a public code repository.\\nOur team have leaked keys like this on more than one occasion. It's worth\\nnoting that even if you realise that you've done this, delete the commit and\\nrevoke the keys, they may have already been used maliciously because\\nautomated bots monitor sites like GitHub using the [events firehose][] to\\ndetect any credentials.\\n[events firehose]: https:\/\/developer.github.com\/v3\/activity\/events\/\\nAs an alternative to using pre-generated keys, AWS recommends that you use\\n[IAM roles and instance profiles][] when accessing the API from EC2\\ninstances. You delegate permissions to the EC2 instance and temporary\\ncredentials are made available from the instance metadata service. Most\\ntools and libraries automatically support this. The credentials are\\nregularly rotated and never need to be stored in configuration files.\\n[IAM roles and instance profiles]: http:\/\/docs.aws.amazon.com\/IAM\/latest\/UserGuide\/best-practices.html#use-roles-with-ec2\\n","Decision":"To reduce the likelihood of us leaking AWS keys we will use IAM roles and\\ninstance profiles for all operations that run from EC2 instances. This\\nincludes everything that happens within Concourse and Cloud Foundry.\\nTo reduce the impact of us leaking AWS keys we will use an IAM policy with\\nan [`aws:SourceIp` condition][condition] to\\nenforce that IAM accounts for team members are only used from the office IP\\naddresses.\\n[condition]: http:\/\/docs.aws.amazon.com\/IAM\/latest\/UserGuide\/access_policies_examples.html#iam-policy-example-deny-source-ip-address\\nThe IAM roles, profiles, and policies will be managed by our\\n[aws-account-wide-terraform][] repo.\\n[aws-account-wide-terraform]: https:\/\/github.digital.cabinet-office.gov.uk\/government-paas\/aws-account-wide-terraform\\n","tokens":427,"id":211,"text":"## Context\\nAmazon Web Services (AWS) are our current Infrastructure as a Service (IaaS)\\nprovider. Our deployment tooling (Concourse, Terraform, BOSH, etc.) and\\nCloud Foundry components (Cloud Controller, RDS broker, blobstore clients,\\netc.) use the APIs to manage or access IaaS resources.\\nThe most common mechanism for authenticating the API calls is to create an\\nIdentify and Access Management (IAM) user with the appropriate permissions,\\ngenerate an Access Key ID and Secret Access Key for that user, and export\\nthose as environment variables. `AWS_ACCESS_KEY_ID` and\\n`AWS_SECRET_ACCESS_KEY` are the standard environment variable names used by\\nmost utilities and libraries.\\nThe problem with this approach is that it's very easy to accidentally leak\\nthe plain text keys. They can appear in output from your shell, which you\\nmight copy+paste into a gist or email when debugging a problem. You might\\nadd them to your shell configuration or include them in a script, which can\\nbe pushed to a public code repository.\\nOur team have leaked keys like this on more than one occasion. It's worth\\nnoting that even if you realise that you've done this, delete the commit and\\nrevoke the keys, they may have already been used maliciously because\\nautomated bots monitor sites like GitHub using the [events firehose][] to\\ndetect any credentials.\\n[events firehose]: https:\/\/developer.github.com\/v3\/activity\/events\/\\nAs an alternative to using pre-generated keys, AWS recommends that you use\\n[IAM roles and instance profiles][] when accessing the API from EC2\\ninstances. You delegate permissions to the EC2 instance and temporary\\ncredentials are made available from the instance metadata service. Most\\ntools and libraries automatically support this. The credentials are\\nregularly rotated and never need to be stored in configuration files.\\n[IAM roles and instance profiles]: http:\/\/docs.aws.amazon.com\/IAM\/latest\/UserGuide\/best-practices.html#use-roles-with-ec2\\n\n\n##Decision\nTo reduce the likelihood of us leaking AWS keys we will use IAM roles and\\ninstance profiles for all operations that run from EC2 instances. This\\nincludes everything that happens within Concourse and Cloud Foundry.\\nTo reduce the impact of us leaking AWS keys we will use an IAM policy with\\nan [`aws:SourceIp` condition][condition] to\\nenforce that IAM accounts for team members are only used from the office IP\\naddresses.\\n[condition]: http:\/\/docs.aws.amazon.com\/IAM\/latest\/UserGuide\/access_policies_examples.html#iam-policy-example-deny-source-ip-address\\nThe IAM roles, profiles, and policies will be managed by our\\n[aws-account-wide-terraform][] repo.\\n[aws-account-wide-terraform]: https:\/\/github.digital.cabinet-office.gov.uk\/government-paas\/aws-account-wide-terraform\\n","MatchedIds":[2256,1464,2471,3907,4358],"Predicted":"## Decision\nWe will transition to using IAM roles and instance profiles for authenticating API calls from EC2 instances, instead of generating and using static AWS Access Key IDs and Secret Access Keys. This approach eliminates the risk of accidental leakage of credentials through environmental variables or configuration files, and ensures that credentials are automatically rotated and securely managed via AWS's instance metadata service. All deployment tooling and Cloud Foundry components will be updated to support this new authentication mechanism.","GenTime":"2024-08-20 00:02:58"}
{"File Name":"marsha\/0002-videos-languages.md","Context":"## Context\\nWe want to think Marsha as accessible from the beginning. At least from\\nthe point of view of the videos, which are the main content available.\\nWe can think about a video as a main content, with many auxiliary\\ncontents.\\n### Auxiliary contents\\n#### Audio\\nWe have a main video, with an audio track included. The author could\\npropose many other audio tracks, as audio files, and in the player the\\nviewer can change the one to use.\\n#### Timed texts\\nIn addition to audio tracks, many timed text tracks can be available.\\n#### Sign language\\nSome people with disabilities could want a video with the sign language\\ntranscript. For this it can be a video incorporated in the original one,\\nor an other video displayed on the site.\\nAs sign languages are not the same for every spoken language, there can\\nbe several sign languages videos for a single video.\\n","Decision":"We decided to take all these elements into account right from the\\nbeginning.\\nSo we have a main Django model named `Video`, from an author, with the\\nlink to the main video file, including the default audio track.\\nFor the other audio tracks, we have an `AudioTrack` Django model, with a\\n`ForeignKey` to the `Video` instance, named `video`, and a `language`\\nfield (with only one audio track for each video+language)\\nIt's the same for timed text tracks, we have a `TimedTextTrack` Django model,\\nwith the same `video` and `language` fields, but with an additional `mode`\\nfield to indicate that this timed text track is either a simple subtitle,\\na \"[closed\\ncaptioning](https:\/\/en.wikipedia.org\/wiki\/Closed_captioning)\" ie subtitles\\nfor deaf or hard of hearing viewers, or a transcript. So there can be up to\\n3 timed text tracks for each video+language: one for each mode.\\nAnd finally, for sign-languages videos, it's the same as for audio\\ntracks: a Django model named `SignTrack` with the same `video` and\\n`language` field.\\n","tokens":191,"id":2843,"text":"## Context\\nWe want to think Marsha as accessible from the beginning. At least from\\nthe point of view of the videos, which are the main content available.\\nWe can think about a video as a main content, with many auxiliary\\ncontents.\\n### Auxiliary contents\\n#### Audio\\nWe have a main video, with an audio track included. The author could\\npropose many other audio tracks, as audio files, and in the player the\\nviewer can change the one to use.\\n#### Timed texts\\nIn addition to audio tracks, many timed text tracks can be available.\\n#### Sign language\\nSome people with disabilities could want a video with the sign language\\ntranscript. For this it can be a video incorporated in the original one,\\nor an other video displayed on the site.\\nAs sign languages are not the same for every spoken language, there can\\nbe several sign languages videos for a single video.\\n\n\n##Decision\nWe decided to take all these elements into account right from the\\nbeginning.\\nSo we have a main Django model named `Video`, from an author, with the\\nlink to the main video file, including the default audio track.\\nFor the other audio tracks, we have an `AudioTrack` Django model, with a\\n`ForeignKey` to the `Video` instance, named `video`, and a `language`\\nfield (with only one audio track for each video+language)\\nIt's the same for timed text tracks, we have a `TimedTextTrack` Django model,\\nwith the same `video` and `language` fields, but with an additional `mode`\\nfield to indicate that this timed text track is either a simple subtitle,\\na \"[closed\\ncaptioning](https:\/\/en.wikipedia.org\/wiki\/Closed_captioning)\" ie subtitles\\nfor deaf or hard of hearing viewers, or a transcript. So there can be up to\\n3 timed text tracks for each video+language: one for each mode.\\nAnd finally, for sign-languages videos, it's the same as for audio\\ntracks: a Django model named `SignTrack` with the same `video` and\\n`language` field.\\n","MatchedIds":[1096,498,3351,4553,3187],"Predicted":"## Decision\n1. **Audio Tracks:** The video player will support multiple audio tracks. The user will be able to select from the available audio tracks via a dedicated menu in the video player interface.\n\n2. **Timed Texts:** The video player will support multiple timed text tracks (such as captions and subtitles). The user will be able to select from the available timed texts through a dropdown menu in the player. The player will also support standard caption controls such as font size, color, and background customization to enhance accessibility.\n\n3. **Sign Language:** The video player will support one or more sign language videos. These sign language videos can be overlaid on the primary video or displayed alongside it. Users will have the option to select their preferred sign language track through a menu in the video player.\n\n4. **User Preferences:** The video player will remember user preferences for audio tracks, timed texts, and sign language selections across sessions to enhance the user experience.\n\n5. **Configurability:** All auxiliary content (audio tracks, timed texts, sign language videos) will be configurable through the Marsha backend to allow authors to easily manage and update content.\n\n6. **Compliance:** All features will comply with accessibility standards such as WCAG 2.1 to ensure a wide range of accessibility needs are met.\n\nBy implementing these features from the beginning, Marsha will provide a highly accessible and customizable video viewing experience.","GenTime":"2024-08-20 00:03:04"}
{"File Name":"google-cloud-cpp-common\/2019-03-26-binary-distributions.md","Context":"**Context**: Compiling google-cloud-cpp from source is not always possible nor\\ndesired by all users. Some users may want to install a binary package that\\ncontains our public headers and pre-compiled libraries so that they can link\\nagainst our library. There is no single standard binary package manager in C++,\\ninstead there are a variety of others that users might want to use (e.g., dpkg,\\nrpm, vcpkg).\\n**Decision**: We will not directly support nor provide any binary packages. We\\nwill not test binary distributions of our code. And we will not host configs\\nfor binary distributions, since that would involve hosting files which we do\\nnot test. However, we will do our best to make our code easily packageable by\\nothers in a wide variety of formats. For more context, see\\n[#333](https:\/\/github.com\/googleapis\/google-cloud-cpp\/issues\/333).\\n**Consequences**: This decision will shield from endorsing any particular\\nbinary package management system. Other individuals who want to build and\\nmaintain a binary distribution of our code may do so without our permission or\\nknowledge. If they need small and reasonable tweaks from us to support their\\nbinary distributions, they may file an issue and we'll do our best to\\naccommodate them within reason.\\n","Decision":"will not test binary distributions of our code. And we will not host configs\\nfor binary distributions, since that would involve hosting files which we do\\nnot test. However, we will do our best to make our code easily packageable by\\nothers in a wide variety of formats. For more context, see\\n[#333](https:\/\/github.com\/googleapis\/google-cloud-cpp\/issues\/333).\\n**Consequences**: This decision will shield from endorsing any particular\\nbinary package management system. Other individuals who want to build and\\nmaintain a binary distribution of our code may do so without our permission or\\nknowledge. If they need small and reasonable tweaks from us to support their\\nbinary distributions, they may file an issue and we'll do our best to\\naccommodate them within reason.\\n","tokens":273,"id":5061,"text":"**Context**: Compiling google-cloud-cpp from source is not always possible nor\\ndesired by all users. Some users may want to install a binary package that\\ncontains our public headers and pre-compiled libraries so that they can link\\nagainst our library. There is no single standard binary package manager in C++,\\ninstead there are a variety of others that users might want to use (e.g., dpkg,\\nrpm, vcpkg).\\n**Decision**: We will not directly support nor provide any binary packages. We\\nwill not test binary distributions of our code. And we will not host configs\\nfor binary distributions, since that would involve hosting files which we do\\nnot test. However, we will do our best to make our code easily packageable by\\nothers in a wide variety of formats. For more context, see\\n[#333](https:\/\/github.com\/googleapis\/google-cloud-cpp\/issues\/333).\\n**Consequences**: This decision will shield from endorsing any particular\\nbinary package management system. Other individuals who want to build and\\nmaintain a binary distribution of our code may do so without our permission or\\nknowledge. If they need small and reasonable tweaks from us to support their\\nbinary distributions, they may file an issue and we'll do our best to\\naccommodate them within reason.\\n\n\n##Decision\nwill not test binary distributions of our code. And we will not host configs\\nfor binary distributions, since that would involve hosting files which we do\\nnot test. However, we will do our best to make our code easily packageable by\\nothers in a wide variety of formats. For more context, see\\n[#333](https:\/\/github.com\/googleapis\/google-cloud-cpp\/issues\/333).\\n**Consequences**: This decision will shield from endorsing any particular\\nbinary package management system. Other individuals who want to build and\\nmaintain a binary distribution of our code may do so without our permission or\\nknowledge. If they need small and reasonable tweaks from us to support their\\nbinary distributions, they may file an issue and we'll do our best to\\naccommodate them within reason.\\n","MatchedIds":[2462,2467,3873,1631,1117],"Predicted":"## Decision\nWe will not directly support nor provide any binary packages. We will not test binary distributions of our code. And we will not host configs for binary distributions, since that would involve hosting files which we do not test. However, we will do our best to make our code easily packageable by others in a wide variety of formats. For more context, see [#333](https:\/\/github.com\/googleapis\/google-cloud-cpp\/issues\/333).\n\n**Consequences**: This decision will shield us from endorsing any particular binary package management system. Other individuals who want to build and maintain a binary distribution of our code may do so without our permission or knowledge. If they need small and reasonable tweaks from us to support their binary distributions, they may file an issue and we'll do our best to accommodate them within reason.","GenTime":"2024-08-20 00:03:08"}
{"File Name":"remultiform\/0003-use-rollup-to-build-distributables.md","Context":"## Context\\nWe want to be able to distribute this library to me ingested by TypeScript or\\nplain JavaScript (both commonJS and module) applications.\\n[Rollup](https:\/\/rollupjs.org\/guide\/en\/) is a popular JavaScript bundler with\\nsupport for TypeScript and simple configuration.\\n","Decision":"We will build distributables using Rollup.js.\\n","tokens":62,"id":5259,"text":"## Context\\nWe want to be able to distribute this library to me ingested by TypeScript or\\nplain JavaScript (both commonJS and module) applications.\\n[Rollup](https:\/\/rollupjs.org\/guide\/en\/) is a popular JavaScript bundler with\\nsupport for TypeScript and simple configuration.\\n\n\n##Decision\nWe will build distributables using Rollup.js.\\n","MatchedIds":[1631,3827,1576,3323,4223],"Predicted":"## Decision\nWe will use Rollup as our JavaScript bundler to generate distributions that support both TypeScript and plain JavaScript (commonJS and module) applications.","GenTime":"2024-08-20 00:03:10"}
{"File Name":"paas-csls-splunk-broker\/ADR007-paas-taking-ownership.md","Context":"## Context\\nThe RE Autom8 team originally wrote and maintained this broker, in collaboration with Cyber Security. It is configured as a service broker in the Ireland and London regions of GOV.UK PaaS, and enabled for a few GDS-internal tenants. The code lives in a subdirectory of the `alphagov\/tech-ops` repository, and the pipeline which builds and deploys it lives in the Tech Ops multi-tenant Concourse.\\n","Decision":"The GOV.UK PaaS team decided that they were happy to take ownership of the broker, because it requires knowledge of the platform to maintain, and they maintain all the other brokers on the platform.\\n","tokens":94,"id":3291,"text":"## Context\\nThe RE Autom8 team originally wrote and maintained this broker, in collaboration with Cyber Security. It is configured as a service broker in the Ireland and London regions of GOV.UK PaaS, and enabled for a few GDS-internal tenants. The code lives in a subdirectory of the `alphagov\/tech-ops` repository, and the pipeline which builds and deploys it lives in the Tech Ops multi-tenant Concourse.\\n\n\n##Decision\nThe GOV.UK PaaS team decided that they were happy to take ownership of the broker, because it requires knowledge of the platform to maintain, and they maintain all the other brokers on the platform.\\n","MatchedIds":[3295,2410,3794,4047,4041],"Predicted":"## Decision\nWe will continue to maintain the broker in its current setup within the `alphagov\/tech-ops` repository and the Tech Ops multi-tenant Concourse pipeline. This decision ensures continuity and leverages the existing infrastructure and expertise of the RE Autom8 team and Cyber Security. Furthermore, we will establish a regular review process to evaluate the broker's performance, security updates, and potential enhancements to ensure it remains robust and up-to-date for the GDS-internal tenants using it on the GOV.UK PaaS in the Ireland and London regions.","GenTime":"2024-08-20 00:03:14"}
{"File Name":"structurizr-python\/0009-use-pydantic-for-json-de-serialization.md","Context":"## Context\\nIn order to interact with a remote workspace, for example, at structurizr.com.\\nThe remote or local workspace has to be (de-)serialized from or to JSON.\\n","Decision":"In order to perform these operations we choose\\n[pydantic](https:\/\/pydantic-docs.helpmanual.io\/) which has a nice API, active\\ncommunity, good data validation, helpful documentation, and good performance.\\n","tokens":39,"id":399,"text":"## Context\\nIn order to interact with a remote workspace, for example, at structurizr.com.\\nThe remote or local workspace has to be (de-)serialized from or to JSON.\\n\n\n##Decision\nIn order to perform these operations we choose\\n[pydantic](https:\/\/pydantic-docs.helpmanual.io\/) which has a nice API, active\\ncommunity, good data validation, helpful documentation, and good performance.\\n","MatchedIds":[2585,2625,3746,1180,3722],"Predicted":"## Decision\nWe will use `System.Text.Json` for (de-)serializing the remote or local workspace to and from JSON. This decision is based on the fact that `System.Text.Json` is built into .NET Core, offers good performance, and eliminates the need for reliance on third-party libraries.","GenTime":"2024-08-20 00:03:17"}
{"File Name":"celestia-core\/adr-060-go-api-stability.md","Context":"## Context\\nWith the release of Tendermint 1.0 we will adopt [semantic versioning](https:\/\/semver.org). One major implication is a guarantee that we will not make backwards-incompatible changes until Tendermint 2.0 (except in pre-release versions). In order to provide this guarantee for our Go API, we must clearly define which of our APIs are public, and what changes are considered backwards-compatible.\\nCurrently, we list packages that we consider public in our [README](https:\/\/github.com\/tendermint\/tendermint#versioning), but since we are still at version 0.x we do not provide any backwards compatiblity guarantees at all.\\n### Glossary\\n* **External project:** a different Git\/VCS repository or code base.\\n* **External package:** a different Go package, can be a child or sibling package in the same project.\\n* **Internal code:** code not intended for use in external projects.\\n* **Internal directory:** code under `internal\/` which cannot be imported in external projects.\\n* **Exported:** a Go identifier starting with an uppercase letter, which can therefore be accessed by an external package.\\n* **Private:** a Go identifier starting with a lowercase letter, which therefore cannot be accessed by an external package unless via an exported field, variable, or function\/method return value.\\n* **Public API:** any Go identifier that can be imported or accessed by an external project, except test code in `_test.go` files.\\n* **Private API:** any Go identifier that is not accessible via a public API, including all code in the internal directory.\\n","Decision":"From Tendermint 1.0, all internal code (except private APIs) will be placed in a root-level [`internal` directory](https:\/\/golang.org\/cmd\/go\/#hdr-Internal_Directories), which the Go compiler will block for use by external projects. All exported items outside of the `internal` directory are considered a public API and subject to backwards compatibility guarantees, except files ending in `_test.go`.\\nThe `crypto` package may be split out to a separate module in a separate repo. This is the main general-purpose package used by external projects, and is the only Tendermint dependency in e.g. IAVL which can cause some problems for projects depending on both IAVL and Tendermint. This will be decided after further discussion.\\nThe `tm-db` package will remain a separate module in a separate repo. The `crypto` package may possibly be split out, pending further discussion, as this is the main general-purpose package used by other projects.\\n","tokens":336,"id":4112,"text":"## Context\\nWith the release of Tendermint 1.0 we will adopt [semantic versioning](https:\/\/semver.org). One major implication is a guarantee that we will not make backwards-incompatible changes until Tendermint 2.0 (except in pre-release versions). In order to provide this guarantee for our Go API, we must clearly define which of our APIs are public, and what changes are considered backwards-compatible.\\nCurrently, we list packages that we consider public in our [README](https:\/\/github.com\/tendermint\/tendermint#versioning), but since we are still at version 0.x we do not provide any backwards compatiblity guarantees at all.\\n### Glossary\\n* **External project:** a different Git\/VCS repository or code base.\\n* **External package:** a different Go package, can be a child or sibling package in the same project.\\n* **Internal code:** code not intended for use in external projects.\\n* **Internal directory:** code under `internal\/` which cannot be imported in external projects.\\n* **Exported:** a Go identifier starting with an uppercase letter, which can therefore be accessed by an external package.\\n* **Private:** a Go identifier starting with a lowercase letter, which therefore cannot be accessed by an external package unless via an exported field, variable, or function\/method return value.\\n* **Public API:** any Go identifier that can be imported or accessed by an external project, except test code in `_test.go` files.\\n* **Private API:** any Go identifier that is not accessible via a public API, including all code in the internal directory.\\n\n\n##Decision\nFrom Tendermint 1.0, all internal code (except private APIs) will be placed in a root-level [`internal` directory](https:\/\/golang.org\/cmd\/go\/#hdr-Internal_Directories), which the Go compiler will block for use by external projects. All exported items outside of the `internal` directory are considered a public API and subject to backwards compatibility guarantees, except files ending in `_test.go`.\\nThe `crypto` package may be split out to a separate module in a separate repo. This is the main general-purpose package used by external projects, and is the only Tendermint dependency in e.g. IAVL which can cause some problems for projects depending on both IAVL and Tendermint. This will be decided after further discussion.\\nThe `tm-db` package will remain a separate module in a separate repo. The `crypto` package may possibly be split out, pending further discussion, as this is the main general-purpose package used by other projects.\\n","MatchedIds":[4424,2951,812,2181,5177],"Predicted":"## Decision\n1. **Public API Identification**: Any code under packages explicitly listed in our repository\u2019s README as public will be considered part of the public API. This includes all `Exported` identifiers within these packages, excluding those contained in `_test.go` files.\n\n2. **Backwards Compatibility**: We will adhere to the semantic versioning guidelines:\n   - **Major** version increments for incompatible API changes.\n   - **Minor** version increments when adding functionality in a backwards-compatible manner.\n   - **Patch** version increments for backwards-compatible bug fixes.\n   \n3. **Non-Public Code**: Any code not listed as public in the README, including all code under the `internal\/` directory, will be considered private and not subject to backwards compatibility guarantees.\n\n4. **Stable Contracts**: Existing public APIs will not be altered in a way that breaks backward compatibility until the release of Tendermint 2.0. This includes:\n   - Changing function signatures\n   - Removing public methods or fields\n   - Altering behavior that external projects depend on\n\n5. **Deprecation Policy**: If a public API is to be deprecated, we will first introduce equivalent functionality under a new API with proper documentation. The deprecated API will be marked accordingly and maintained for at least one major version to give users time to migrate.\n\n6. **Documentation and Communication**: All public APIs will be documented thoroughly, and any changes will be communicated through release notes and the changelog.\n\n7. **Pre-release Versions**: Changes in pre-release versions (e.g., 1.1.0-alpha, 1.2.0-beta) do not guarantee backward compatibility and may include breaking changes. These versions are for testing and feedback before the stable release.\n\nBy following these decisions, we aim to ensure a stable and consistent experience for users of Tendermint's Go API while still being able to evolve and improve the software.","GenTime":"2024-08-20 00:03:25"}
{"File Name":"operational-data-hub\/0059-runtime-dependency-only-on-gcp.md","Context":"## Context\\nAvailability of systems can be improved by reducing the number of dependencies. Each additional dependency comes with the risk of that service breaking, causing issues to our system. On the other hand, 3rd party services can be leveraged to quickly build and apply managed services at low cost, compared to creating and running these services by ourselves. Therefore, the right balance between dependency on and benefitting from external services is important.\\nDuring run the changes in 3rd party dependencies do not change. Therefore, caching or copying the functionality of these dependencies is possible in many cases. In build and development environments, the dependencies on 3rd party components and services change more often. Therefore, it is oftentimes not opportune to invest in becoming independent on these components and services.\\n","Decision":"During runtime, we will only depend on resources services from the Google Cloud Platform.\\n","tokens":158,"id":2701,"text":"## Context\\nAvailability of systems can be improved by reducing the number of dependencies. Each additional dependency comes with the risk of that service breaking, causing issues to our system. On the other hand, 3rd party services can be leveraged to quickly build and apply managed services at low cost, compared to creating and running these services by ourselves. Therefore, the right balance between dependency on and benefitting from external services is important.\\nDuring run the changes in 3rd party dependencies do not change. Therefore, caching or copying the functionality of these dependencies is possible in many cases. In build and development environments, the dependencies on 3rd party components and services change more often. Therefore, it is oftentimes not opportune to invest in becoming independent on these components and services.\\n\n\n##Decision\nDuring runtime, we will only depend on resources services from the Google Cloud Platform.\\n","MatchedIds":[1035,3483,929,302,1631],"Predicted":"## Decision\nWe will implement caching or local copies of 3rd party dependencies in production environments to minimize runtime issues and improve system availability. However, in build and development environments, we will directly use 3rd party services to maintain agility and reduce the overhead of maintaining local copies, acknowledging the higher frequency of changes in these environments.","GenTime":"2024-08-20 00:03:28"}
{"File Name":"govuk-aws\/0038-mongo_replacement_by_documentdb.md","Context":"## Context\\nGOV.UK uses MongoDB as the backing database for a number of apps. In a generic\\nGOV.UK environment, there are 5 MongoDB clusters: one for Licensify (a legacy\\nservice), one for performance monitoring, one for router and another per cloud\\nprovider (i.e. AWS and Carrenza) for all other apps (e.g. assets, imminence,\\nmaslow, content, publisher and short url manager).\\nOne architectural strategy principle of TechOps is to make use as much as\\npossible AWS managed services in order to reduce the burden on GOV.UK teams to\\nmanage Commercial off-the-shelf (COTS) functionality\/services.\\n","Decision":"The approaches taken for migrating from MongoDB to AWS DocumentDB are:\\n1. new apps that are being migrated from legacy cloud providers (e.g UKCloud and\\nCarrenza) to AWS and use MongoDB should be evaluated for compatibility with\\nAWS DocumentDB during the migration phase. Based on this evaluation, a\\ndecision should be made based on cost benefit whether a particular migrating\\napp should be using DocumentDB or MongoDB.\\n2. there are some apps (e.g. content store and imminence) that have already been\\nmigrated to AWS and uses the MongoDB cluster there. These apps should be\\nevaluated for compatibility with AWS DocumentDB and prioritised accordingly\\nfor migration to AWS DocumentDB.\\n","tokens":145,"id":4047,"text":"## Context\\nGOV.UK uses MongoDB as the backing database for a number of apps. In a generic\\nGOV.UK environment, there are 5 MongoDB clusters: one for Licensify (a legacy\\nservice), one for performance monitoring, one for router and another per cloud\\nprovider (i.e. AWS and Carrenza) for all other apps (e.g. assets, imminence,\\nmaslow, content, publisher and short url manager).\\nOne architectural strategy principle of TechOps is to make use as much as\\npossible AWS managed services in order to reduce the burden on GOV.UK teams to\\nmanage Commercial off-the-shelf (COTS) functionality\/services.\\n\n\n##Decision\nThe approaches taken for migrating from MongoDB to AWS DocumentDB are:\\n1. new apps that are being migrated from legacy cloud providers (e.g UKCloud and\\nCarrenza) to AWS and use MongoDB should be evaluated for compatibility with\\nAWS DocumentDB during the migration phase. Based on this evaluation, a\\ndecision should be made based on cost benefit whether a particular migrating\\napp should be using DocumentDB or MongoDB.\\n2. there are some apps (e.g. content store and imminence) that have already been\\nmigrated to AWS and uses the MongoDB cluster there. These apps should be\\nevaluated for compatibility with AWS DocumentDB and prioritised accordingly\\nfor migration to AWS DocumentDB.\\n","MatchedIds":[4266,4044,1666,4841,4041],"Predicted":"## Decision\nWe will migrate our MongoDB clusters to Amazon Managed Services for MongoDB (Amazon DocumentDB). This will streamline our operations by reducing the burden on GOV.UK teams to manage and maintain MongoDB clusters, adhering to our architectural strategy of leveraging AWS managed services where possible.\n\n## Status\nAccepted.\n\n## Consequences\n- Transition all MongoDB clusters used in various services (Licensify, performance monitoring, router, etc.) to Amazon DocumentDB.\n- Establish a migration plan to ensure minimal downtime and data integrity during the transition.\n- Update all application configurations to connect to the new Amazon DocumentDB instances.\n- Monitor and validate performance post-transition to ensure the new managed services meet our operational requirements.\n- Decommission the old MongoDB clusters once the migration is confirmed successful.","GenTime":"2024-08-20 00:03:33"}
{"File Name":"oasis-core\/0001-tm-multi-root-apphash.md","Context":"## Context\\nCurrently the Tendermint ABCI application hash is equal to the consensus state\\nroot for a specific height. In order to allow additional uses, like proving to\\nlight clients that specific events have been emitted in a block, we should make\\nthe application hash be derivable from potentially different kinds of roots.\\n","Decision":"The proposed design is to derive the Tendermint ABCI application hash by hashing\\nall the different roots as follows:\\n```\\nAppHash := H(Context || Root_0 || ... || Root_n)\\n```\\nWhere:\\n- `H` is the SHA-512\/256 hash function.\\n- `Context` is the string `oasis-core\/tendermint: roots`.\\n- `Root_i` is the fixed-size SHA-512\/256 root hash of the specified root.\\nCurrently, the only root would be the existing consensus state root at index 0.\\nTo implement this change the following modifications would be required:\\n- Update the ABCI multiplexer's `Commit` method to calculate and return the\\napplication hash using the scheme specified above.\\n- Update the consensus API `SignedHeader` response to include the\\n`UntrustedStateRoot` (the untrusted prefix denotes that the user must verify\\nthat the state root corresponds to `AppHash` provided in the signed header in\\n`Meta`).\\nWhen new roots will be added in the future, both `Block` and `SignedHeader`\\nwill need to include them all.\\n","tokens":65,"id":4354,"text":"## Context\\nCurrently the Tendermint ABCI application hash is equal to the consensus state\\nroot for a specific height. In order to allow additional uses, like proving to\\nlight clients that specific events have been emitted in a block, we should make\\nthe application hash be derivable from potentially different kinds of roots.\\n\n\n##Decision\nThe proposed design is to derive the Tendermint ABCI application hash by hashing\\nall the different roots as follows:\\n```\\nAppHash := H(Context || Root_0 || ... || Root_n)\\n```\\nWhere:\\n- `H` is the SHA-512\/256 hash function.\\n- `Context` is the string `oasis-core\/tendermint: roots`.\\n- `Root_i` is the fixed-size SHA-512\/256 root hash of the specified root.\\nCurrently, the only root would be the existing consensus state root at index 0.\\nTo implement this change the following modifications would be required:\\n- Update the ABCI multiplexer's `Commit` method to calculate and return the\\napplication hash using the scheme specified above.\\n- Update the consensus API `SignedHeader` response to include the\\n`UntrustedStateRoot` (the untrusted prefix denotes that the user must verify\\nthat the state root corresponds to `AppHash` provided in the signed header in\\n`Meta`).\\nWhen new roots will be added in the future, both `Block` and `SignedHeader`\\nwill need to include them all.\\n","MatchedIds":[1985,4099,4152,1962,1558],"Predicted":"## Decision\nTo provide greater flexibility and functionality, the application hash will be modified to support derivation from multiple roots. Specifically, we will design the application hash to be a commitment to a Merkle root that can include various sub-roots, representing different types of data (e.g., consensus state root, event root, etc.).\n\n### Implementation Details\n1. **Composite Root Structure**: \n   The application hash will be derived from a composite Merkle tree structure, where each leaf represents different data categories such as state root, event root, and any future categories.\n   \n   ```go\n   type CompositeRoot struct {\n       StateRoot  []byte\n       EventRoot  []byte\n       OtherRoots map[string][]byte\n   }\n   ```\n\n2. **Merkleization Process**: \n   Each sub-root (e.g., StateRoot, EventRoot) will itself be a Merkle root of its respective data. These sub-roots are then combined into a single Merkle tree to produce the composite application hash.\n\n3. **Proofs**:\n   Light clients will be able to obtain Merkle proofs for each sub-root within the composite Merkle tree, enabling verification of specific events or state changes without requiring the entire application state.\n\n4. **Backward Compatibility**:\n   For a transition period, we will maintain backward compatibility by allowing the application hash to remain as the consensus state root when no additional roots are provided. New clients can begin using the composite structure immediately.\n\n5. **ABCI Modifications**:\n   The ABCI interface will be extended to allow applications to provide multiple roots during `EndBlock` and transport them in the same structure for consistency.\n\n### Example\n```go\ntype ABCIResponse struct {\n    StateRoot   []byte `json:\"state_root\"`\n    EventRoot   []byte `json:\"event_root\"`\n    CompositeRoot CompositeRoot `json:\"composite_root\"`\n}\n```\n\n6. **Documentation and Tooling**:\n   Detailed documentation and tooling will be provided to assist developers in generating and verifying composite application hashes, including example implementations.\n\nBy adopting this approach, we significantly enhance the utility and versatility of the application hash, enabling new types of proofs and interactions to be efficiently supported.","GenTime":"2024-08-20 00:03:40"}
{"File Name":"structurizr-python\/0002-version-control-our-code.md","Context":"## Context\\nWe need to version control our code in order to avoid disasters and maintain\\nsanity. We also want to collaborate online with a wider community.\\n","Decision":"We use git for version control and GitHub for collaboration.\\n","tokens":33,"id":398,"text":"## Context\\nWe need to version control our code in order to avoid disasters and maintain\\nsanity. We also want to collaborate online with a wider community.\\n\n\n##Decision\nWe use git for version control and GitHub for collaboration.\\n","MatchedIds":[241,4868,1416,400,2703],"Predicted":"## Decision\nWe will use Git for version control due to its robustness, widespread use, and strong community support. To facilitate online collaboration, we will host our repositories on GitHub, which provides an excellent platform for open-source projects with features for issue tracking, pull requests, and continuous integration. This setup will help us manage code versions effectively and collaborate seamlessly with the broader community.","GenTime":"2024-08-20 00:03:43"}
{"File Name":"adr-tools\/0005-help-comments.md","Context":"## Context\\nThe tool will have a `help` subcommand to provide documentation\\nfor users.\\nIt's nice to have usage documentation in the script files\\nthemselves, in comments.  When reading the code, that's the first\\nplace to look for information about how to run a script.\\n","Decision":"Write usage documentation in comments in the source file.\\nDistinguish between documentation comments and normal comments.\\nDocumentation comments have two hash characters at the start of\\nthe line.\\nThe `adr help` command can parse comments out from the script\\nusing the standard Unix tools `grep` and `cut`.\\n","tokens":64,"id":3566,"text":"## Context\\nThe tool will have a `help` subcommand to provide documentation\\nfor users.\\nIt's nice to have usage documentation in the script files\\nthemselves, in comments.  When reading the code, that's the first\\nplace to look for information about how to run a script.\\n\n\n##Decision\nWrite usage documentation in comments in the source file.\\nDistinguish between documentation comments and normal comments.\\nDocumentation comments have two hash characters at the start of\\nthe line.\\nThe `adr help` command can parse comments out from the script\\nusing the standard Unix tools `grep` and `cut`.\\n","MatchedIds":[3571,2191,400,1416,2914],"Predicted":"## Decision\nThe `help` subcommand will extract usage documentation directly from specially formatted comments within the script files. This ensures that the documentation is always located in close proximity to the code it describes, making it easier for developers to maintain and understand the usage of the script.","GenTime":"2024-08-20 00:03:46"}
{"File Name":"connecting-to-services\/0003-use-postcodes-io-for-postcode-lookup.md","Context":"## Context\\nThe application is about finding services closest to the search point. All\\nservices have a co-ordinate in lat\/lon format. Currently the application\\nrequests either an out-code or a postcode as the search point. The submitted\\npostcode needs to be resolved to a lat\/lon that can be used to query against.\\n","Decision":"[Postcodes.io](https:\/\/postcodes.io\/) provides both an out-code and a postcode\\nlookup. The results of which return latitude and longitude. The service is\\nfree, [supported](https:\/\/postcodes.io\/about),\\n[monitored](https:\/\/status.ideal-postcodes.co.uk\/) and contains the full set of\\nactive postcodes for the UK. Supplied by the\\n[ONS](https:\/\/data.gov.uk\/dataset\/national-statistics-postcode-lookup-uk).\\n","tokens":68,"id":2204,"text":"## Context\\nThe application is about finding services closest to the search point. All\\nservices have a co-ordinate in lat\/lon format. Currently the application\\nrequests either an out-code or a postcode as the search point. The submitted\\npostcode needs to be resolved to a lat\/lon that can be used to query against.\\n\n\n##Decision\n[Postcodes.io](https:\/\/postcodes.io\/) provides both an out-code and a postcode\\nlookup. The results of which return latitude and longitude. The service is\\nfree, [supported](https:\/\/postcodes.io\/about),\\n[monitored](https:\/\/status.ideal-postcodes.co.uk\/) and contains the full set of\\nactive postcodes for the UK. Supplied by the\\n[ONS](https:\/\/data.gov.uk\/dataset\/national-statistics-postcode-lookup-uk).\\n","MatchedIds":[4276,4274,3968,1180,485],"Predicted":"## Decision\nThe decision is to integrate a postcode-to-lat\/lon geocoding service into the application. This service will convert the submitted postcode or out-code into its corresponding latitude and longitude coordinates. This geocoded information will then be used for querying and finding the closest services to the search point. A reliable external geocoding API, such as the Google Maps Geocoding API or the Data BC Geocoder API, can be utilized for this purpose to ensure accuracy and efficiency.","GenTime":"2024-08-20 00:03:50"}
{"File Name":"connaisseur\/ADR-2_release-management.md","Context":"## Context\\nDuring its initial development Connaisseur was more or less maintained by a single person and not released frequently. Hence, the easiest option was to just have the maintainer build and push at certain stages of development. With the influx of more team members, the number of contributions and hence the number of needed\/reasonable releases went up. Also since publication, it is more important that the uploaded Connaisseur image corresponds to the most recent version referenced in the Helm chart.\\nA single person having to build, sign and push the images whenever a new pull request is accepted is hence unpractical for both development and agility.\\n","Decision":"For choice 1, we decided to go for two branches. On the one hand, `master` being the branch that contains the code of the latest release and will be tagged with release versions. On the other hand, there will be a `develop` branch that hosts the current state of development and will be merged to `master` whenever we want to create a new release.\\nThis way we get rid of the current pain of releasing with every pull request at the cost a some overhead during release.\\nIn the process of automating most of the release process, we will run an integration test with locally built images for pull requests to `master`. Regarding choice 2, whenever a pull request is merged, whoever merged the PR has to tag this commit on the `master` branch with the most recent version. Right after the merge, whoever merged the PR builds, signs and pushes the new Connaisseur release and creates a tag on the `master` branch referencing the new release version.\\nAfter the image is pushed and the new commit tagged, the pipeline will run the integration test with the image pulled from Docker Hub to ensure that the released version is working.\\nWe decided for this option as it does not expose credentials to GitHub Actions, which we wanted to avoid especially in light of the [recent GitHub Actions injection attacks](https:\/\/bugs.chromium.org\/p\/project-zero\/issues\/detail?id=2070) and as it would also prevent us from opening up the repository to Pull Requests. To alleviate the work required for doing the steps outside the pipeline we use a shell script that will automate these steps given suitable environment, i.e. Docker context and DCT keys.\\n### Positive consequences\\n- We can develop without having to ship changes immediatly.\\n- Release process does not expose credentials to GitHub Actions.\\n- Code gets Git tags.\\n### Negative consequences\\n- Process from code to release for a single change is more cumbersome than right now.\\n- Release still requires human intervention.\\n","tokens":131,"id":4397,"text":"## Context\\nDuring its initial development Connaisseur was more or less maintained by a single person and not released frequently. Hence, the easiest option was to just have the maintainer build and push at certain stages of development. With the influx of more team members, the number of contributions and hence the number of needed\/reasonable releases went up. Also since publication, it is more important that the uploaded Connaisseur image corresponds to the most recent version referenced in the Helm chart.\\nA single person having to build, sign and push the images whenever a new pull request is accepted is hence unpractical for both development and agility.\\n\n\n##Decision\nFor choice 1, we decided to go for two branches. On the one hand, `master` being the branch that contains the code of the latest release and will be tagged with release versions. On the other hand, there will be a `develop` branch that hosts the current state of development and will be merged to `master` whenever we want to create a new release.\\nThis way we get rid of the current pain of releasing with every pull request at the cost a some overhead during release.\\nIn the process of automating most of the release process, we will run an integration test with locally built images for pull requests to `master`. Regarding choice 2, whenever a pull request is merged, whoever merged the PR has to tag this commit on the `master` branch with the most recent version. Right after the merge, whoever merged the PR builds, signs and pushes the new Connaisseur release and creates a tag on the `master` branch referencing the new release version.\\nAfter the image is pushed and the new commit tagged, the pipeline will run the integration test with the image pulled from Docker Hub to ensure that the released version is working.\\nWe decided for this option as it does not expose credentials to GitHub Actions, which we wanted to avoid especially in light of the [recent GitHub Actions injection attacks](https:\/\/bugs.chromium.org\/p\/project-zero\/issues\/detail?id=2070) and as it would also prevent us from opening up the repository to Pull Requests. To alleviate the work required for doing the steps outside the pipeline we use a shell script that will automate these steps given suitable environment, i.e. Docker context and DCT keys.\\n### Positive consequences\\n- We can develop without having to ship changes immediatly.\\n- Release process does not expose credentials to GitHub Actions.\\n- Code gets Git tags.\\n### Negative consequences\\n- Process from code to release for a single change is more cumbersome than right now.\\n- Release still requires human intervention.\\n","MatchedIds":[3915,4395,351,1739,5102],"Predicted":"## Decision\nWe will implement an automated CI\/CD pipeline for building, signing, and pushing Connaisseur images to streamline the release process and ensure consistency between the published image and the Helm chart. The pipeline will be triggered upon merging a pull request into the main branch, ensuring that new contributions are promptly incorporated and distributed.\n\n### Steps Taken\n1. **CI\/CD Setup**: Incorporate CI\/CD tools (e.g., GitHub Actions, GitLab CI, or Jenkins) to automate the build, test, sign, and push processes.\n2. **Automated Build and Test**: The pipeline will automatically build and test the Connaisseur image to verify that new changes do not introduce any issues.\n3. **Image Signing**: Integrate automated image signing to guarantee the integrity and authenticity of the images.\n4. **Push to Registry**: Upon successful build, test, and sign steps, the pipeline will push the new image to the container registry.\n5. **Helm Chart Update**: Ensure the Helm chart is updated with the latest image version and automatically published or indicated to be updated manually as part of a defined process.\n\n### Positive Consequences\n- **Increased Agility**: Contributions can be merged and released more efficiently, supporting a higher velocity of development.\n- **Consistency**: Automated processes ensure that the published image always corresponds to the Helm chart, reducing discrepancies.\n- **Scalability**: The team can scale contributions without being bottlenecked by manual release processes.\n\n### Negative Consequences\n- **Initial Setup and Maintenance**: Setting up and maintaining the CI\/CD pipeline requires initial time and effort.\n- **Resource Usage**: Automated pipelines consume resources and might require attention to optimize build and test stages for efficiency.\n- **Error Propagation**: Any errors in the CI\/CD configuration might lead to unintended releases or failed deployments, necessitating diligent monitoring and testing of the pipeline itself.\n\nOverall, implementing an automated CI\/CD pipeline will significantly enhance the reliability and efficiency of Connaisseur's release process, accommodating the increasing number of contributions and ensuring up-to-date releases.","GenTime":"2024-08-20 00:04:00"}
{"File Name":"opensmarthouse-core\/adr0000.md","Context":"## Context\\nOpenHAB uses OSGi as a runtime.\\nThis runtime promotes clear separation between implementation and contract (API\/implementation\/SPI) packages.\\nOnce program gets launched OSGi framework such Apache Felix or Eclipse Equinox makes sure that \"implementation\" packages stay hidden.\\nOn the build tool side we do not have such strong separation because many parts of project are co-developed.\\nInternal packages and API are in the same source root, and often functionally different elements of code are included in the same bundle.\\nFor example, this means that the `org.openhab.core.items` package is in the same module as `org.openhab.core.items.internal`.\\nAs a result, during compile time we have all of the dependencies together - ones which are required by `core.items` and ones used by `core.items.internal` package.\\nWhile it might not cause major issues for this module, it might have devastating influence over callers who depend on public parts of the API.\\nDuring compilation phase they will get polluted by internal package dependencies and quite often use them.\\nSuch approach promotes tight coupling between contract and implementation.\\nMore over, it also promotes exposure of specific implementation classes via public API.\\nThe natural way to deal with such things is to address them with a build tool that includes an appropriate includes\/excludes mechanism for dependencies.\\nIt would work properly, but openHAB core is a single jar which makes things even harder.\\nThis means that quite many dependencies get unnecessarily propagated to all callers of public APIs.\\nopenHAB utilizes Apache Karaf for provisioning of the application.\\nKaraf provisioning itself is capable of verifying its \"features\" based on declared modules, bundles, JAR files, etc.\\nCurrently, most of the project features depend on one of two root features, `openhab-core-base` or `openhab-runtime-base`, making no distinction on how particular parts of the framework interact with each other.\\nA tiny extension (SPI) bundle that is targeted at a specific framework feature would need to resolve one of the above features, which would then lead to the activation of the entire openhab framework.\\nPresent structure of modules \/ bundles is as follows:\\n```\\n[openhab thing core] <--- [openhab rest core]\\n```\\n","Decision":"Since openHAB core is a framework, we need to keep strong and clear separation of API and implementation packages.\\nThis will clarify module responsibilities and streamline development of extensions through smaller and easier to maintain modules.\\nThere are also significant opportunities to speed up the build processes and make better use of resolution and graph analysis between framework parts.\\nWe decided that a clear separation between API and implementation packages should be made.\\nThis means that each service intended to be interacted with through an OSGi services API should form its own contract bundle.\\nNew framework modules will separate `internal` package under a `core.<area>.core` bundle and have a distinct interface in a separate module.\\nAdditionally, functionally separate components should be placed in separate bundles to ensure that unnecessary dependencies are not pulled in to a build.\\nThis is a big change which involves a lot of code relocations and also cleans up the project dependencies which were built up over time.\\nBy preserving public package names we will be able to keep backward compatibility at an affordable level.\\n```\\n[openhab thing api] <--- [openhab rest thing]\\n^\\n|\\n[openhab thing core]\\n```\\nIdeally, such a pattern should evolve further to include a common structure used for keeping contracts and implementations separate.\\n","tokens":471,"id":660,"text":"## Context\\nOpenHAB uses OSGi as a runtime.\\nThis runtime promotes clear separation between implementation and contract (API\/implementation\/SPI) packages.\\nOnce program gets launched OSGi framework such Apache Felix or Eclipse Equinox makes sure that \"implementation\" packages stay hidden.\\nOn the build tool side we do not have such strong separation because many parts of project are co-developed.\\nInternal packages and API are in the same source root, and often functionally different elements of code are included in the same bundle.\\nFor example, this means that the `org.openhab.core.items` package is in the same module as `org.openhab.core.items.internal`.\\nAs a result, during compile time we have all of the dependencies together - ones which are required by `core.items` and ones used by `core.items.internal` package.\\nWhile it might not cause major issues for this module, it might have devastating influence over callers who depend on public parts of the API.\\nDuring compilation phase they will get polluted by internal package dependencies and quite often use them.\\nSuch approach promotes tight coupling between contract and implementation.\\nMore over, it also promotes exposure of specific implementation classes via public API.\\nThe natural way to deal with such things is to address them with a build tool that includes an appropriate includes\/excludes mechanism for dependencies.\\nIt would work properly, but openHAB core is a single jar which makes things even harder.\\nThis means that quite many dependencies get unnecessarily propagated to all callers of public APIs.\\nopenHAB utilizes Apache Karaf for provisioning of the application.\\nKaraf provisioning itself is capable of verifying its \"features\" based on declared modules, bundles, JAR files, etc.\\nCurrently, most of the project features depend on one of two root features, `openhab-core-base` or `openhab-runtime-base`, making no distinction on how particular parts of the framework interact with each other.\\nA tiny extension (SPI) bundle that is targeted at a specific framework feature would need to resolve one of the above features, which would then lead to the activation of the entire openhab framework.\\nPresent structure of modules \/ bundles is as follows:\\n```\\n[openhab thing core] <--- [openhab rest core]\\n```\\n\n\n##Decision\nSince openHAB core is a framework, we need to keep strong and clear separation of API and implementation packages.\\nThis will clarify module responsibilities and streamline development of extensions through smaller and easier to maintain modules.\\nThere are also significant opportunities to speed up the build processes and make better use of resolution and graph analysis between framework parts.\\nWe decided that a clear separation between API and implementation packages should be made.\\nThis means that each service intended to be interacted with through an OSGi services API should form its own contract bundle.\\nNew framework modules will separate `internal` package under a `core.<area>.core` bundle and have a distinct interface in a separate module.\\nAdditionally, functionally separate components should be placed in separate bundles to ensure that unnecessary dependencies are not pulled in to a build.\\nThis is a big change which involves a lot of code relocations and also cleans up the project dependencies which were built up over time.\\nBy preserving public package names we will be able to keep backward compatibility at an affordable level.\\n```\\n[openhab thing api] <--- [openhab rest thing]\\n^\\n|\\n[openhab thing core]\\n```\\nIdeally, such a pattern should evolve further to include a common structure used for keeping contracts and implementations separate.\\n","MatchedIds":[4889,649,3604,1035,2701],"Predicted":"## Decision\nWe will refactor the openHAB core modules to achieve a clear separation between API, implementation, and SPI packages, aligning better with OSGi runtime principles.\n\n### Decision Details\n1. **Modularization**:\n    - **Separate API and Implementation Modules**: Split existing core modules into separate API and implementation modules. For instance, `org.openhab.core.items` and `org.openhab.core.items.internal` will be moved into distinct modules. This will prevent the exposure of internal implementation details in public APIs.\n    \n2. **Clear Package Naming**:\n    - Implement clear naming conventions for packages to discern between API and internal packages easily. For instance,:\n        - API: `org.openhab.core.items.api`\n        - Internal: `org.openhab.core.items.internal`\n\n3. **Build Tool Enhancement**:\n    - Enhance the build tool configuration to include\/exclude specific packages and dependencies appropriately. This ensures that internal dependencies do not leak into publicly exposed APIs.\n    \n4. **Feature Definition in Karaf**:\n    - Define more granular Karaf features to be aligned with the new modular structure. Instead of depending on broad `openhab-core-base` or `openhab-runtime-base` features, define features specific to functionalities provided by the module.\n    - For example, the feature for `org.openhab.core.items` could be `openhab-core-items-api`, while the internal features could be `openhab-core-items-impl`.\n\n5. **Gradual Refactoring**:\n    - Implement these changes gradually, starting with the most critical modules. Ensure backward compatibility where possible by providing transitional APIs or modules during the refactoring process.\n\n### Benefits\n- **Improved Modularity**: This change will ensure a clear dependency hierarchy and improve modularity in the project.\n- **Reduced Coupling**: By separating APIs from implementation, calling components will not inadvertently depend on implementation details, thus reducing tight coupling.\n- **Better Maintainability**: With clear distinctions between API and implementation, it will be easier to maintain and upgrade individual parts of the system.\n- **More Efficient Runtime**: Smaller, more focused modules and features will reduce the overhead during the provisioning and activation phase.\n\n### Next Steps\n- Design a detailed refactoring plan, complete with timelines and milestones.\n- Start refactoring high-priority modules as a proof of concept.\n- Iterate based on feedback and gradually refactor the remaining modules.\n- Update the Karaf feature definitions and ensure the build system aligns with the new modular structure.\n\nBy adopting this approach, we can align more closely with OSGi principles and improve the dependability and maintainability of the openHAB framework.","GenTime":"2024-08-20 00:04:12"}
{"File Name":"drt-v2\/0006-physical-deployment-one-jvm-per-port.md","Context":"## Context\\nDRTv1 tried to normalise all ports into a single model. We frequently had requests for port specific features\\nor structures. Also it\\n","Decision":"Use a jvm per airport\\n","tokens":33,"id":1912,"text":"## Context\\nDRTv1 tried to normalise all ports into a single model. We frequently had requests for port specific features\\nor structures. Also it\\n\n\n##Decision\nUse a jvm per airport\\n","MatchedIds":[1735,1545,3150,428,4361],"Predicted":"## Decision\nWe will adopt a more flexible approach in DRTv2 by designing port-specific models that allow for the unique features and structures required by different ports. This will ensure that each port can leverage its own set of capabilities while maintaining compatibility with the overall system architecture.","GenTime":"2024-08-20 00:04:14"}
{"File Name":"atlasdb\/0002-prevent-tables-from-being-creating-simultaneously-in-cassandra-via-a-locks-table.md","Context":"## Context\\nCassandra [has an issue](https:\/\/issues.apache.org\/jira\/browse\/CASSANDRA-10699) which can cause data loss in the situation:\\n1. Node A and node B concurrently create table \"some.table\"\\n2. Both table creations succeed, creating tables with column family ids \"123\" and \"456\"\\n3. Cassandra picks \"123\" to be the correct table for \"some.table\"\\n4. Cassandra is restarted\\n5. After restart Cassandra gossips and decides that \"456\" is the correct table for \"some.table\"\\n6. All data that was written to \"123\" is now lost\\nTo fix this we must prevent tables from being created at the same time.\\n","Decision":"All schema mutations will globally synchronise via a check-and-set operation on a specific cell in a _lock table.\\n","tokens":151,"id":3094,"text":"## Context\\nCassandra [has an issue](https:\/\/issues.apache.org\/jira\/browse\/CASSANDRA-10699) which can cause data loss in the situation:\\n1. Node A and node B concurrently create table \"some.table\"\\n2. Both table creations succeed, creating tables with column family ids \"123\" and \"456\"\\n3. Cassandra picks \"123\" to be the correct table for \"some.table\"\\n4. Cassandra is restarted\\n5. After restart Cassandra gossips and decides that \"456\" is the correct table for \"some.table\"\\n6. All data that was written to \"123\" is now lost\\nTo fix this we must prevent tables from being created at the same time.\\n\n\n##Decision\nAll schema mutations will globally synchronise via a check-and-set operation on a specific cell in a _lock table.\\n","MatchedIds":[3092,224,1558,317,1654],"Predicted":"## Decision\nWe decided to implement a shared lock mechanism to ensure that table creation requests are serialized across nodes. This approach involves the following steps:\n\n1. **Single Source of Truth**: Designate one node as the \"Table Creation Leader\" responsible for managing table creation locks. Other nodes must obtain permission from this leader before proceeding with table creation.\n2. **Distributed Lock Service**: Use a distributed lock service such as Apache ZooKeeper or a dedicated lock table to enforce mutual exclusion. When a node wants to create a table, it must first acquire a lock from this service.\n3. **Timeout and Retry Mechanism**: Implement timeouts and retries for acquiring locks to handle scenarios where the \"Table Creation Leader\" is unavailable or the lock service is under heavy load.\n4. **Auditing and Monitoring**: Maintain logs and monitoring tools to track table creation requests and ensure they are being serialized properly.\n\nThis solution ensures that only one node can create a table at any given time, thereby preventing the concurrent creation issue and avoiding potential data loss.","GenTime":"2024-08-20 00:04:19"}
{"File Name":"saas-platform-frontend\/0006-use-antdesign-as-the-ui-framework.md","Context":"## Context\\nTo speed up the UI development we need to select a UI Framework that has a good community as well as good functionality.\\n","Decision":"We use Ant Design as the UI Framework.\\n","tokens":28,"id":3718,"text":"## Context\\nTo speed up the UI development we need to select a UI Framework that has a good community as well as good functionality.\\n\n\n##Decision\nWe use Ant Design as the UI Framework.\\n","MatchedIds":[1240,1243,3715,847,1477],"Predicted":"## Decision\nWe will use [React](https:\/\/reactjs.org\/) as our UI framework. React is widely adopted and has a large, active community, which ensures ample support and resources. Additionally, React provides efficient component-based architecture and high performance, which aligns with our goal to speed up UI development. For styling, we will use [Material-UI](https:\/\/material-ui.com\/), which offers a rich set of pre-designed components and follows Material Design guidelines to ensure a consistent and modern user interface.","GenTime":"2024-08-20 00:04:22"}
{"File Name":"docspell\/0012_periodic_tasks.md","Context":"# Context and Problem Statement\\nCurrently there is a `Scheduler` that consumes tasks off a queue in\\nthe database. This allows multiple job executors running in parallel\\nracing for the next job to execute. This is for executing tasks\\nimmediately \u2013 as long as there are enough resource.\\nWhat is missing, is a component that maintains periodic tasks. The\\nreason for this is to have house keeping tasks that run regularily and\\nclean up stale or unused data. Later, users should be able to create\\nperiodic tasks, for example to read e-mails from an inbox or to be\\nnotified of due items.\\nThe problem is again, that it must work with multiple job executor\\ninstances running at the same time. This is the same pattern as with\\nthe `Scheduler`: it must be ensured that only one task is used at a\\ntime. Multiple job exectuors must not schedule a perdiodic task more\\nthan once. If a periodic tasks takes longer than the time between\\nruns, it must wait for the next interval.\\n# Considered Options\\n1. Adding a `timer` and `nextrun` field to the current `job` table\\n2. Creating a separate table for periodic tasks\\n","Decision":"The 2. option.\\nFor internal housekeeping tasks, it may suffice to reuse the existing\\n`job` queue by adding more fields such that a job may be considered\\nperiodic. But this conflates with what the `Scheduler` is doing now\\n(executing tasks as soon as possible while being bound to some\\nresource limits) with a completely different subject.\\nThere will be a new `PeriodicScheduler` that works on a new table in\\nthe database that is representing periodic tasks. This table will\\nshare fields with the `job` table to be able to create `RJob` records.\\nThis new component is only taking care of periodically submitting jobs\\nto the job queue such that the `Scheduler` will eventually pick it up\\nand run it. If the tasks cannot run (for example due to resource\\nlimitation), the periodic scheduler can't do nothing but wait and try\\nnext time.\\n```sql\\nCREATE TABLE \"periodic_task\" (\\n\"id\" varchar(254) not null primary key,\\n\"enabled\" boolean not null,\\n\"task\" varchar(254) not null,\\n\"group_\" varchar(254) not null,\\n\"args\" text not null,\\n\"subject\" varchar(254) not null,\\n\"submitter\" varchar(254) not null,\\n\"priority\" int not null,\\n\"worker\" varchar(254),\\n\"marked\" timestamp,\\n\"timer\" varchar(254) not null,\\n\"nextrun\" timestamp not null,\\n\"created\" timestamp not null\\n);\\n```\\nPreparing for other features, at some point periodic tasks will be\\ncreated by users. It should be possible to disable\/enable them. The\\nnext 6 properties are needed to insert jobs into the `job` table. The\\n`worker` field (and `marked`) are used to mark a periodic job as\\n\"being worked on by a job executor\".\\nThe `timer` is the schedule, which is a\\n[systemd-like](https:\/\/man7.org\/linux\/man-pages\/man7\/systemd.time.7.html#CALENDAR_EVENTS)\\ncalendar event string. This is parsed by [this\\nlibrary](https:\/\/github.com\/eikek\/calev). The `nextrun` field will\\nstore the timestamp of the next time the task would need to be\\nexecuted. This is needed to query this table for the newest task.\\nThe `PeriodicScheduler` works roughly like this:\\nOn startup:\\n- Remove stale worker values. If the process has been killed, there\\nmay be marked tasks which must be cleared now.\\nMain-Loop:\\n0. Cancel current scheduled notify (see 4. below)\\n1. get next (= earliest & enabled) periodic job\\n2. if none: stop\\n3. if triggered (= `nextrun <= 'now'`):\\n- Mark periodic task. On fail: goto 1.\\n- Submit new job into the jobqueue:\\n- Update `nextrun` field\\n- Check for non-final jobs of that name. This is required to not\\nrun the same periodic task multiple times concurrently.\\n- if exist: goto 4.\\n- if not exist: submit job\\n- Unmark periodic task\\n4. if future\\n- schedule notify: notify self to run again next time the task\\nschedule triggers\\n","tokens":257,"id":279,"text":"# Context and Problem Statement\\nCurrently there is a `Scheduler` that consumes tasks off a queue in\\nthe database. This allows multiple job executors running in parallel\\nracing for the next job to execute. This is for executing tasks\\nimmediately \u2013 as long as there are enough resource.\\nWhat is missing, is a component that maintains periodic tasks. The\\nreason for this is to have house keeping tasks that run regularily and\\nclean up stale or unused data. Later, users should be able to create\\nperiodic tasks, for example to read e-mails from an inbox or to be\\nnotified of due items.\\nThe problem is again, that it must work with multiple job executor\\ninstances running at the same time. This is the same pattern as with\\nthe `Scheduler`: it must be ensured that only one task is used at a\\ntime. Multiple job exectuors must not schedule a perdiodic task more\\nthan once. If a periodic tasks takes longer than the time between\\nruns, it must wait for the next interval.\\n# Considered Options\\n1. Adding a `timer` and `nextrun` field to the current `job` table\\n2. Creating a separate table for periodic tasks\\n\n\n##Decision\nThe 2. option.\\nFor internal housekeeping tasks, it may suffice to reuse the existing\\n`job` queue by adding more fields such that a job may be considered\\nperiodic. But this conflates with what the `Scheduler` is doing now\\n(executing tasks as soon as possible while being bound to some\\nresource limits) with a completely different subject.\\nThere will be a new `PeriodicScheduler` that works on a new table in\\nthe database that is representing periodic tasks. This table will\\nshare fields with the `job` table to be able to create `RJob` records.\\nThis new component is only taking care of periodically submitting jobs\\nto the job queue such that the `Scheduler` will eventually pick it up\\nand run it. If the tasks cannot run (for example due to resource\\nlimitation), the periodic scheduler can't do nothing but wait and try\\nnext time.\\n```sql\\nCREATE TABLE \"periodic_task\" (\\n\"id\" varchar(254) not null primary key,\\n\"enabled\" boolean not null,\\n\"task\" varchar(254) not null,\\n\"group_\" varchar(254) not null,\\n\"args\" text not null,\\n\"subject\" varchar(254) not null,\\n\"submitter\" varchar(254) not null,\\n\"priority\" int not null,\\n\"worker\" varchar(254),\\n\"marked\" timestamp,\\n\"timer\" varchar(254) not null,\\n\"nextrun\" timestamp not null,\\n\"created\" timestamp not null\\n);\\n```\\nPreparing for other features, at some point periodic tasks will be\\ncreated by users. It should be possible to disable\/enable them. The\\nnext 6 properties are needed to insert jobs into the `job` table. The\\n`worker` field (and `marked`) are used to mark a periodic job as\\n\"being worked on by a job executor\".\\nThe `timer` is the schedule, which is a\\n[systemd-like](https:\/\/man7.org\/linux\/man-pages\/man7\/systemd.time.7.html#CALENDAR_EVENTS)\\ncalendar event string. This is parsed by [this\\nlibrary](https:\/\/github.com\/eikek\/calev). The `nextrun` field will\\nstore the timestamp of the next time the task would need to be\\nexecuted. This is needed to query this table for the newest task.\\nThe `PeriodicScheduler` works roughly like this:\\nOn startup:\\n- Remove stale worker values. If the process has been killed, there\\nmay be marked tasks which must be cleared now.\\nMain-Loop:\\n0. Cancel current scheduled notify (see 4. below)\\n1. get next (= earliest & enabled) periodic job\\n2. if none: stop\\n3. if triggered (= `nextrun <= 'now'`):\\n- Mark periodic task. On fail: goto 1.\\n- Submit new job into the jobqueue:\\n- Update `nextrun` field\\n- Check for non-final jobs of that name. This is required to not\\nrun the same periodic task multiple times concurrently.\\n- if exist: goto 4.\\n- if not exist: submit job\\n- Unmark periodic task\\n4. if future\\n- schedule notify: notify self to run again next time the task\\nschedule triggers\\n","MatchedIds":[2791,3361,2127,4821,1799],"Predicted":"## Decision\n\nWe will create a separate table for periodic tasks.\n\n### Rationale\n\n1. **Separation of Concerns**: Maintaining a separate table for periodic tasks allows a clear distinction between immediate jobs and periodic tasks. This makes it easier to manage and maintain each type of job without additional complexity to the existing `job` table.\n\n2. **Scalability**: As the number of periodic tasks grows, segregating them into a separate table will help keep the database operations efficient. It's easier to index, search, and manage tasks when they are logically separated by their nature.\n\n3. **Concurrency Control**: Periodic tasks will require additional fields and logic to ensure they are not scheduled more than once by multiple job executors. A dedicated table can house these specific fields and constraints without affecting the immediate job execution logic.\n\n4. **Future Flexibility**: Users will eventually be able to create periodic tasks, and having them in a separate table allows for more extensibility. Additional fields specific to user-created periodic tasks can be easily added without affecting the existing job structure.\n\n### How it should work\n\n1. **Table Structure**: The new `periodic_tasks` table will contain fields such as `id`, `task_name`, `interval`, `next_run`, `last_run`, `status`, and any additional metadata required for periodic tasks.\n\n2. **Scheduling Logic**: A periodic task scheduler will check the `periodic_tasks` table at regular intervals to determine which tasks are due for execution. It will update the `next_run` field after each execution to ensure that the task is scheduled for its next interval.\n\n3. **Concurrency Handling**: When a periodic task is picked up for execution, its status will be updated to ensure no other executor picks it up again until the current execution is completed. \n\n4. **Integration**: The periodic task scheduler will integrate with the existing job executors, ensuring that periodic tasks are handled seamlessly alongside the immediate jobs.\n\n### Pros and Cons\n\n**Pros:**\n- Clear separation of immediate and periodic tasks\n- Easier database maintenance and scalability\n- Reduced risk of complex and error-prone queries\n- Flexibility for future enhancements\n\n**Cons:**\n- Initial development required to set up and integrate the new table\n- Slight increase in overall system complexity due to the addition of a new component\n\nBy adopting this decision, we ensure a robust, scalable, and maintainable solution for managing periodic tasks alongside immediate job executions.","GenTime":"2024-08-20 00:04:30"}
{"File Name":"buildit-all\/0005-use-id-token-from-microsoft-as-bearer-token.md","Context":"## Context\\nIn the interest of time and getting something to work, we are going to break up the steps further\\n","Decision":"* Instead of exchanging id_token for opaque access_token, the client will always send the id_token as the Bearer token\\n* Proper validation of the id_token will still occur\\n","tokens":24,"id":4416,"text":"## Context\\nIn the interest of time and getting something to work, we are going to break up the steps further\\n\n\n##Decision\n* Instead of exchanging id_token for opaque access_token, the client will always send the id_token as the Bearer token\\n* Proper validation of the id_token will still occur\\n","MatchedIds":[3722,3682,4868,4021,3982],"Predicted":"## Decision\nWe will adopt an incremental delivery approach by breaking down the project into smaller, manageable tasks. Each task will be focused on delivering a specific, functional aspect of the application. This methodology allows us to prioritize essential features and deploy them progressively. \n\nKey actions:\n- Define the minimum viable product (MVP) and focus initial efforts on core functionalities.\n- Create detailed task lists for each feature, ensuring they are small and deliverable within short time frames.\n- Implement continuous integration and continuous deployment (CI\/CD) pipelines to facilitate frequent releases and quick feedback.\n- Regularly review and adjust priorities based on feedback and evolving requirements.\n\nThis iterative process will help ensure that we make consistent progress, identify issues early, and demonstrate working aspects of the application incrementally.","GenTime":"2024-08-20 00:04:34"}
{"File Name":"FindMeFoodTrucks\/Messaging choice.md","Context":"## :dart: Context\\nThe Web APIs can be implemented as Synchronous or Asynchronous.\\n","Decision":"Considering the fact that the APIs does a datastore look up which can take some time, making the services asynchronous is the recommendation.\\nThe decision is based on the guidance provided by Microsoft here: https:\/\/azure.microsoft.com\/mediahandler\/files\/resourcefiles\/api-design\/Azure_API-Design_Guide_eBook.pdf\\n","tokens":20,"id":1091,"text":"## :dart: Context\\nThe Web APIs can be implemented as Synchronous or Asynchronous.\\n\n\n##Decision\nConsidering the fact that the APIs does a datastore look up which can take some time, making the services asynchronous is the recommendation.\\nThe decision is based on the guidance provided by Microsoft here: https:\/\/azure.microsoft.com\/mediahandler\/files\/resourcefiles\/api-design\/Azure_API-Design_Guide_eBook.pdf\\n","MatchedIds":[1094,1089,1180,307,2359],"Predicted":"## Decision\nThe Web APIs will be implemented as Asynchronous. This approach enhances scalability and responsiveness by allowing the server to handle multiple requests concurrently without blocking operations.","GenTime":"2024-08-20 00:04:38"}
{"File Name":"datalab\/0002-ansible-for-provisioning-tool.md","Context":"## Context\\nWe need a tool to provision servers and software for the datalabs project.\\n","Decision":"We will use Ansible as our provisioning tool as both the JASMIN DevOps team and Tessella\\nteam have experience using it.\\n","tokens":20,"id":771,"text":"## Context\\nWe need a tool to provision servers and software for the datalabs project.\\n\n\n##Decision\nWe will use Ansible as our provisioning tool as both the JASMIN DevOps team and Tessella\\nteam have experience using it.\\n","MatchedIds":[4358,4025,745,2409,2062],"Predicted":"## Decision\nWe have decided to use Ansible for provisioning servers and software for the datalabs project. Ansible is a powerful and simple automation tool that supports multi-tier deployments and can configure both servers and software in a consistent manner. \n\n### Justifications:\n- **Agentless**: Ansible operates over SSH without needing an agent, simplifying management and improving security.\n- **Readable YAML Syntax**: Ansible playbooks use YAML, which is easy to read and write, facilitating collaboration and maintenance.\n- **Extensible and Scalable**: Ansible allows us to define roles and playbooks, making it extensible and scalable for multiple environments.\n- **Existing Ecosystem**: Ansible has a rich ecosystem of modules and community-contributed roles available on Ansible Galaxy, reducing the need to start from scratch.\n\n### Next Steps:\n1. Create a repository for Ansible playbooks and roles.\n2. Define the inventory of servers.\n3. Develop playbooks for provisioning servers, installing required software, and configuring environments.\n4. Integrate with CI\/CD pipelines to enable automated deployments.\n5. Collaborate with the team to refine and validate the playbooks in different environments.","GenTime":"2024-08-20 00:04:44"}
{"File Name":"govuk-aws\/0008-postgres-on-puppetmaster.md","Context":"## Context\\nWe need to decide whether to run PostgreSQL on the Puppetmaster itself or use\\nRDS from the off.\\nCurrently our Puppetmaster uses a local PostgreSQL instance as its database. In moving to AWS we have the opportunity to replace this with an RDS instance.\\nThis may remove a reasonable amount of puppet code used to configure the database and associated tasks (e.g. backups). It would also require an amount of Terraform work to provision the RDS instance and some updates to the Puppetmaster code to use that instance.\\n","Decision":"The Puppetmaster will continue to use a local instance of PostgreSQL.\\n","tokens":110,"id":4061,"text":"## Context\\nWe need to decide whether to run PostgreSQL on the Puppetmaster itself or use\\nRDS from the off.\\nCurrently our Puppetmaster uses a local PostgreSQL instance as its database. In moving to AWS we have the opportunity to replace this with an RDS instance.\\nThis may remove a reasonable amount of puppet code used to configure the database and associated tasks (e.g. backups). It would also require an amount of Terraform work to provision the RDS instance and some updates to the Puppetmaster code to use that instance.\\n\n\n##Decision\nThe Puppetmaster will continue to use a local instance of PostgreSQL.\\n","MatchedIds":[3657,1740,224,207,4041],"Predicted":"## Decision\nWe will use Amazon RDS for PostgreSQL instead of running PostgreSQL on the Puppetmaster itself. This helps us achieve several goals:\n\n1. **Reduce Puppet Code Complexity**: By offloading database management to RDS, we can eliminate a significant portion of Puppet code related to database configuration and maintenance, making our Puppetmaster manifests simpler and easier to manage.\n\n2. **Automated Backups and Maintenance**: RDS offers automated backups, patching, and maintenance, which reduces the operational burden on our team and ensures higher availability and reliability.\n\n3. **Scalability and Performance**: RDS provides built-in scalability options and performance optimization features such as read replicas, which can be leveraged as our needs grow.\n\n4. **Terraform Integration**: While some Terraform work is required to provision the RDS instance, this is a one-time setup cost. It also aligns well with our infrastructure-as-code practices, ensuring that the provisioning of resources is consistent and repeatable.\n\n5. **Security**: Utilizing RDS allows for enhanced security features such as network isolation, encryption at rest, and fine-grained IAM controls, which are more robust compared to a locally managed PostgreSQL instance on the Puppetmaster.\n\nWe will update the Puppetmaster configuration to connect to the RDS instance and ensure that all necessary Terraform scripts are in place to automate the provisioning and configuration of the RDS instance.","GenTime":"2024-08-20 00:04:50"}
{"File Name":"saas-platform-frontend\/0002-use-the-stack-from-react-the-complete-guide-to-keep-up-to-date-with-react.md","Context":"## Context\\nI need to decided with which tools I build my react app.\\n","Decision":"I build the react app using [`React - The Complete Guide (incl Hooks, React Router, Redux)`](https:\/\/www.udemy.com\/react-the-complete-guide-incl-redux\/).\\n","tokens":17,"id":3721,"text":"## Context\\nI need to decided with which tools I build my react app.\\n\n\n##Decision\nI build the react app using [`React - The Complete Guide (incl Hooks, React Router, Redux)`](https:\/\/www.udemy.com\/react-the-complete-guide-incl-redux\/).\\n","MatchedIds":[3715,1240,1239,1243,4940],"Predicted":"## Decision\nI use Create React App to bootstrap the project because it provides a well-structured starter kit with minimal configuration. Additionally, I will use ESLint for linting, Prettier for code formatting, and Jest for testing to ensure code quality and maintainability. For state management, I will integrate Redux.","GenTime":"2024-08-20 00:04:52"}
{"File Name":"ansible-docker\/choose_webserver.md","Context":"### Context: A webserver is a necessary requirment to complete the assignment\\n* A webserver is needed to host the web application.\\n* There is a PHP-apache bundle offered by docker (see Dockerfile in php directory), thus apache was chosen purely out of convenience.\\n* I do not have much experience with Web Servers and there is a lot of documentation on apache. Again, convenience was a major factor in making the decision to use apache.\\n### Decision: The change proposed to the current implementation is to add an apache web server to host the php web application created in the php directory\\n### Consequences: No forseen consequences or drawbacks in using apache.\\n","Decision":"### Consequences: No forseen consequences or drawbacks in using apache.\\n","tokens":139,"id":4833,"text":"### Context: A webserver is a necessary requirment to complete the assignment\\n* A webserver is needed to host the web application.\\n* There is a PHP-apache bundle offered by docker (see Dockerfile in php directory), thus apache was chosen purely out of convenience.\\n* I do not have much experience with Web Servers and there is a lot of documentation on apache. Again, convenience was a major factor in making the decision to use apache.\\n### Decision: The change proposed to the current implementation is to add an apache web server to host the php web application created in the php directory\\n### Consequences: No forseen consequences or drawbacks in using apache.\\n\n\n##Decision\n### Consequences: No forseen consequences or drawbacks in using apache.\\n","MatchedIds":[469,4642,4832,583,3681],"Predicted":"## Context and Problem Statement\nA webserver is needed to host the web application. The solution should be convenient and easy to manage due to limited experience with web servers.\n\n## Decision Drivers\n* Convenience\n* Availability of PHP-apache bundle in Docker\n* Extensive documentation available for Apache\n* Ease of use\n\n## Considered Options\n1. Apache\n2. Nginx\n3. Lighttpd\n\n## Decision\nChosen option: \"Apache,\" because it is conveniently available as a bundle with PHP in Docker and there is extensive documentation to support it. This decision simplifies setup and management due to the existing Dockerfile configuration and ease of use.\n\n### Consequences\n* Positive: Convenience and ease of setup owing to the PHP-apache Docker bundle.\n* Positive: Abundant documentation available for troubleshooting and configuring Apache.\n* Negative: Apache might not be as performant as other modern web servers like Nginx, but for the purposes of this assignment, performance is not a critical concern.\n\n### Recommendations\n* As familiarity with web servers grows, consider exploring other web servers like Nginx for potential performance benefits.\n* Monitor the Apache instance during the development of the web application to ensure it meets all necessary requirements.","GenTime":"2024-08-20 00:04:57"}
{"File Name":"celestia-core\/adr-005-decouple-blockid-and-partsetheader.md","Context":"## Context\\nCelestia has multiple commits to the block data via the `DataHash` and the `PartSetHeader` in the `BlockID`. As stated in the [#184](https:\/\/github.com\/celestiaorg\/lazyledger-core\/issues\/184), we no longer need the `PartSetHeader` for this additional commitment to the block's data. However, we are still planning to use the `PartSetHeader` for block propagation during consensus in the short-medium term. This means that we will remove the `PartSetHeader` from as many places as possible, but keep it in the `Proposal` struct.\\n","Decision":"While we build other better designs to experiment with, we will continue to implement the design specified here as it is not orthogonal. https:\/\/github.com\/celestiaorg\/lazyledger-core\/pull\/434#issuecomment-869158788\\n","tokens":132,"id":4152,"text":"## Context\\nCelestia has multiple commits to the block data via the `DataHash` and the `PartSetHeader` in the `BlockID`. As stated in the [#184](https:\/\/github.com\/celestiaorg\/lazyledger-core\/issues\/184), we no longer need the `PartSetHeader` for this additional commitment to the block's data. However, we are still planning to use the `PartSetHeader` for block propagation during consensus in the short-medium term. This means that we will remove the `PartSetHeader` from as many places as possible, but keep it in the `Proposal` struct.\\n\n\n##Decision\nWhile we build other better designs to experiment with, we will continue to implement the design specified here as it is not orthogonal. https:\/\/github.com\/celestiaorg\/lazyledger-core\/pull\/434#issuecomment-869158788\\n","MatchedIds":[1558,1956,4354,21,4099],"Predicted":"## Decision\nWe will proceed with the removal of the `PartSetHeader` from all locations except the `Proposal` struct to streamline the multiple commits to the block data via the `DataHash`. This decision aims to reduce redundancy and simplify the commitment process while maintaining the necessary functionality for block propagation during consensus.\n\n### Changes to be Implemented\n1. **Remove `PartSetHeader` from BlockID:**\n   - Update the `BlockID` struct to eliminate the `PartSetHeader`.\n   - Adjust any related logic and tests to no longer rely on the presence of `PartSetHeader` in `BlockID`.\n\n2. **Retain `PartSetHeader` in Proposal Struct:**\n   - Ensure the `PartSetHeader` continues to exist and be utilized within the `Proposal` struct for block propagation.\n   - Validate that block propagation logic during the short to medium term remains functional and efficient with this configuration.\n\n3. **Update Documentation and Code References:**\n   - Modify corresponding documentation to reflect the removal of `PartSetHeader` from the affected areas.\n   - Refactor code to eliminate dependencies on `PartSetHeader` where it is no longer required.\n\n4. **Testing and Verification:**\n   - Rigorously test the updated implementation to ensure that there are no regressions or issues caused by the removal of `PartSetHeader`.\n   - Validate that block propagation using `PartSetHeader` in the `Proposal` struct continues to work as expected.\n\nBy implementing these changes, we maintain the integrity of block propagation during the consensus process while eliminating unnecessary redundancy in the block data commitments. This approach balances short-term needs with the goal of simplifying the data commitment structure.","GenTime":"2024-08-20 00:05:03"}
{"File Name":"dogma\/0012-identifier-comparison.md","Context":"## Context\\nIdentifiers (the names and keys used to identify applications and handlers) must\\nbe compared by engines to determine if two such entities are to be considered\\nequivalent.\\nThe documentation specifies that such keys must be non-empty UTF-8 strings\\nconsisting of printable characters without whitespace, but it did not previously\\nspecify how such strings would be compared.\\nThese identifiers are either mostly or entirely immutable and generated as part\\nof the source code. They do not need to be parsed and validated from user input.\\n","Decision":"In keeping with current behavior, we've decided to specify byte-wise comparison\\nsemantics for identifiers.\\n","tokens":109,"id":1618,"text":"## Context\\nIdentifiers (the names and keys used to identify applications and handlers) must\\nbe compared by engines to determine if two such entities are to be considered\\nequivalent.\\nThe documentation specifies that such keys must be non-empty UTF-8 strings\\nconsisting of printable characters without whitespace, but it did not previously\\nspecify how such strings would be compared.\\nThese identifiers are either mostly or entirely immutable and generated as part\\nof the source code. They do not need to be parsed and validated from user input.\\n\n\n##Decision\nIn keeping with current behavior, we've decided to specify byte-wise comparison\\nsemantics for identifiers.\\n","MatchedIds":[2181,2256,2270,1416,4119],"Predicted":"## Decision\nIdentifiers should be compared using a case-sensitive, byte-by-byte comparison method.\n\n### Rationale\n- **Case-Sensitivity**: Since identifiers are generated as part of the source code and are not meant to be parsed or validated from user input, case-sensitive comparisons ensure that `IdentifierA` and `identifiera` are treated as distinct values.\n- **Byte-by-Byte Comparison**: This method ensures that comparisons are straightforward and unambiguous, focusing purely on the binary representation of the strings without any locale-specific or encoding-specific transformations.\n\n### Implementation\n- Ensure that all comparison operations in the codebase employ a case-sensitive, byte-by-byte approach.\n- Update relevant documentation to indicate that identifiers must adhere to the non-empty UTF-8 string requirement and that they should be compared in a case-sensitive, byte-by-byte manner.\n\n```plaintext\nfunction compareIdentifiers(id1, id2) {\n    return id1 === id2;\n}\n```\n\nBy adopting this approach, we ensure that identifier comparison is consistent, unambiguous, and efficient.","GenTime":"2024-08-20 00:05:14"}
{"File Name":"road-registry\/006-provisional-changes.md","Context":"## Context\\nAs a result of running the feature-compare process a bunch of files are produced. These files are essentially a _diff_ between what the operator started out with, that is the dump \/ extract he \/ she started working from, and what they ended up with, after having edited shapes and associated data in their editor. Some of those files have a `_ALL.DBF|_ALL.SHP` suffix. These files are the files we base our logic on to validate and translate to an _internal_ change request. Most notably, these files have an extra column called `RECORDTYPE` which, as the name suggests, classifies a record as either being `IDENTICAL`, `ADDED`, `MODIFIED` or `REMOVED`. Next to that most of the files contain, per record, the equivalent of a primary key that identifies a row. This primary key is used in other files as a foreign key to reference a record sitting in another file. Records with a `RECORDTYPE` of `ADDED` will use a temporary identifier (a really big number that we assume is free to use) to make this work (see [004-temporary-and-permanent-identifiers.md](004-temporary-and-permanent-identifiers.md) for how that works). One gotcha is that the primary keys are not always unique, that is, they can appear multiple times in the `*_ALL.DBF|*_ALL.SHP` files, once for each record type. A common scenario is a modification represented as a removal and an addition record.\\nFor the `WEGSEGMENT_ALL.DBF` file, things are more complicated ... next to having a `WS_OIDN` column act as primary key it has a `EVENTIDN` column acting as an alternative primary key in some cases. In case the `RECORDTYPE` is `ADDED` and the `EVENTIDN` has a value differing from `0`, the `WS_OIDN` column refers to an existing road segment and the `EVENTIDN` column refers to its new representation. In such a case, other files refer to a road segment by the value found in the `EVENTIDN`, not by the value in `WS_OIDN`. Alas, such is life ...\\n","Decision":"Modifying a road segment involves data from `WEGSEGMENT_ALL.DBF`, `WEGSEGMENT_ALL.SHP` and `ATTRIJSTROKEN_ALL.DBF`, `ATTWEGBREEDTE_ALL.DBF`, `ATTWEGVERHARDING_ALL.DBF` - that is, when it is represented as an _internal_ change request command. Each of those `.DBF` files contains a `RECORDTYPE` column. As such, a road segment could be marked as identical in `WEGSEGMENT_ALL.DB`, yet it's lanes, width and \/ or surfaces could be marked as a mixture of modified, removed, added, identical. This is the reason why a road segment that is identical is appended as a `provisional` change to the list of translated changes. Because we're not sure, just yet, that it is an actual change. The order in which these files are translated causes a `provisional` change, if warranted, to be promoted to an actual change.\\nWhy go thru all this trouble? Well, lanes, widths, and surfaces are tightly coupled to the geometry of a road segment such that it makes sense to capture them as a holistic change rather than as individual fragmented changes. There's still a bigger debate to be had about which pieces of data change together ...\\n","tokens":467,"id":4931,"text":"## Context\\nAs a result of running the feature-compare process a bunch of files are produced. These files are essentially a _diff_ between what the operator started out with, that is the dump \/ extract he \/ she started working from, and what they ended up with, after having edited shapes and associated data in their editor. Some of those files have a `_ALL.DBF|_ALL.SHP` suffix. These files are the files we base our logic on to validate and translate to an _internal_ change request. Most notably, these files have an extra column called `RECORDTYPE` which, as the name suggests, classifies a record as either being `IDENTICAL`, `ADDED`, `MODIFIED` or `REMOVED`. Next to that most of the files contain, per record, the equivalent of a primary key that identifies a row. This primary key is used in other files as a foreign key to reference a record sitting in another file. Records with a `RECORDTYPE` of `ADDED` will use a temporary identifier (a really big number that we assume is free to use) to make this work (see [004-temporary-and-permanent-identifiers.md](004-temporary-and-permanent-identifiers.md) for how that works). One gotcha is that the primary keys are not always unique, that is, they can appear multiple times in the `*_ALL.DBF|*_ALL.SHP` files, once for each record type. A common scenario is a modification represented as a removal and an addition record.\\nFor the `WEGSEGMENT_ALL.DBF` file, things are more complicated ... next to having a `WS_OIDN` column act as primary key it has a `EVENTIDN` column acting as an alternative primary key in some cases. In case the `RECORDTYPE` is `ADDED` and the `EVENTIDN` has a value differing from `0`, the `WS_OIDN` column refers to an existing road segment and the `EVENTIDN` column refers to its new representation. In such a case, other files refer to a road segment by the value found in the `EVENTIDN`, not by the value in `WS_OIDN`. Alas, such is life ...\\n\n\n##Decision\nModifying a road segment involves data from `WEGSEGMENT_ALL.DBF`, `WEGSEGMENT_ALL.SHP` and `ATTRIJSTROKEN_ALL.DBF`, `ATTWEGBREEDTE_ALL.DBF`, `ATTWEGVERHARDING_ALL.DBF` - that is, when it is represented as an _internal_ change request command. Each of those `.DBF` files contains a `RECORDTYPE` column. As such, a road segment could be marked as identical in `WEGSEGMENT_ALL.DB`, yet it's lanes, width and \/ or surfaces could be marked as a mixture of modified, removed, added, identical. This is the reason why a road segment that is identical is appended as a `provisional` change to the list of translated changes. Because we're not sure, just yet, that it is an actual change. The order in which these files are translated causes a `provisional` change, if warranted, to be promoted to an actual change.\\nWhy go thru all this trouble? Well, lanes, widths, and surfaces are tightly coupled to the geometry of a road segment such that it makes sense to capture them as a holistic change rather than as individual fragmented changes. There's still a bigger debate to be had about which pieces of data change together ...\\n","MatchedIds":[2215,4930,3815,8,317],"Predicted":"## Decision\nTo handle the complexity of the `WEGSEGMENT_ALL.DBF` file and other similar files, we will implement a multi-step validation and translation process with the following key components:\n\n1. **Unique Primary Key Validation**:\n    - Implement a validation step that ensures all primary keys (e.g., `WS_OIDN`, `EVENTIDN`) are unique within their respective `RECORDTYPE` categories. This validation will catch any inconsistencies early in the process.\n\n2. **Event-Based Referencing**:\n    - When processing records with `RECORDTYPE` set to `ADDED`, if `EVENTIDN` has a non-zero value, treat `EVENTIDN` as the primary identifier for all subsequent references to that road segment. Implement a mapping layer that translates `WS_OIDN` to `EVENTIDN` where applicable.\n\n3. **Temporary Identifier Management**:\n    - For records with `RECORDTYPE` of `ADDED`, ensure that temporary identifiers are correctly assigned and tracked. Implement a mechanism to resolve these temporary identifiers to permanent ones once they are confirmed free to use.\n\n4. **Record State Management**:\n    - Implement a state management system that keeps track of the different states (IDENTICAL, ADDED, MODIFIED, REMOVED) of each record. This system will handle scenarios where multiple records with the same primary key appear in different states.\n\n5. **Cross-File Foreign Key Handling**:\n    - Develop a cross-file reference handling mechanism that respects the identified primary and foreign keys. This mechanism will ensure consistency across different related files.\n\n6. **Logging and Error Reporting**:\n    - Implement detailed logging and error reporting throughout the validation and translation process to help identify and troubleshoot issues easily.\n\n7. **Documentation and Training**:\n    - Provide comprehensive documentation and training for operators on how the system handles these files, including edge cases and common pitfalls.\n\nBy following this structured approach, we can ensure that the complexities inherent in the `WEGSEGMENT_ALL.DBF` file and others are managed effectively, leading to reliable and accurate internal change requests.","GenTime":"2024-08-20 00:05:22"}
{"File Name":"sepa-customer-platform\/0004-authenticate-backstage-users-azure-ad.md","Context":"## Context\\nAzure Cloud Services and Microsoft Dynamics 365 have been proposed to support the CCP case & contact management, therefore, to maximise vendor reuse\\n& interoperability, Azure's identity management system will be used to provide authentication for the frontend application (back stage\/SEPA users).\\n## Decision Drivers\\nSEPA uses Active Directory for it's existing internal account management so providing one unified single sign on login would improve security and usability.\\n","Decision":"SEPA uses Active Directory for it's existing internal account management so providing one unified single sign on login would improve security and usability.\\n[Option 1] SEPA should look to integrate Azure Active Directory with Dynamics to provide a shared identity 'federation', where backstage (SEPA) users are able to access Dynamics data via the WebAPI & perform CRUD operations using their existing single sign on login.\\nDynamics users would therefore match the users within the Active Directory, and SEPA would require a Dynamics license for each SEPA user.\\nIf SEPA's Active Directory is managed on premises, Azure AD connect can be used to auto sync existing Active Directory accounts to the Azure setup.\\n### Positive Consequences\\n* Reuse of existing SEPA logins\\n* Better backstage user experience\\n### Negative Consequences\\n* If Azure Active Directory connect is used to sync accounts, this would need to be monitored & managed by SEPA.\\n","tokens":90,"id":5157,"text":"## Context\\nAzure Cloud Services and Microsoft Dynamics 365 have been proposed to support the CCP case & contact management, therefore, to maximise vendor reuse\\n& interoperability, Azure's identity management system will be used to provide authentication for the frontend application (back stage\/SEPA users).\\n## Decision Drivers\\nSEPA uses Active Directory for it's existing internal account management so providing one unified single sign on login would improve security and usability.\\n\n\n##Decision\nSEPA uses Active Directory for it's existing internal account management so providing one unified single sign on login would improve security and usability.\\n[Option 1] SEPA should look to integrate Azure Active Directory with Dynamics to provide a shared identity 'federation', where backstage (SEPA) users are able to access Dynamics data via the WebAPI & perform CRUD operations using their existing single sign on login.\\nDynamics users would therefore match the users within the Active Directory, and SEPA would require a Dynamics license for each SEPA user.\\nIf SEPA's Active Directory is managed on premises, Azure AD connect can be used to auto sync existing Active Directory accounts to the Azure setup.\\n### Positive Consequences\\n* Reuse of existing SEPA logins\\n* Better backstage user experience\\n### Negative Consequences\\n* If Azure Active Directory connect is used to sync accounts, this would need to be monitored & managed by SEPA.\\n","MatchedIds":[2405,5211,2041,3421,1252],"Predicted":"## Decision\nWe will use Azure Active Directory (Azure AD) as the identity management system for the frontend application for backstage\/SEPA users.\n\n## Rationale\n1. **Vendor Reuse and Interoperability**: Since Azure Cloud Services and Microsoft Dynamics 365 are being used, Azure AD will seamlessly integrate, maximizing interoperability and reducing potential issues with compatibility.\n2. **Unified Single Sign-On (SSO)**: SEPA's existing use of Active Directory for internal account management aligns with Azure AD, providing a unified SSO experience. This will significantly enhance security by centralizing authentication and simplify the user experience.\n3. **Security and Usability**: Azure AD offers robust security features which will improve the overall security posture. Additionally, providing a single set of credentials for various services will improve usability for SEPA users.\n4. **Scalability and Maintenance**: Using Azure AD will leverage Microsoft's infrastructure, which is scalable and easier to maintain compared to self-hosted solutions.\n\nBy choosing Azure AD, we ensure a secure, efficient, and user-friendly authentication mechanism that aligns with SEPA's existing infrastructure.","GenTime":"2024-08-20 00:05:28"}
{"File Name":"claim-additional-payments-for-teaching\/0003-use-rubocop-for-linting.md","Context":"## Context\\nWe need to lint our Ruby code, the team has previous experience working with\\nRuboCop.\\n","Decision":"We will use RuboCop for linting our Ruby code\\n","tokens":24,"id":2101,"text":"## Context\\nWe need to lint our Ruby code, the team has previous experience working with\\nRuboCop.\\n\n\n##Decision\nWe will use RuboCop for linting our Ruby code\\n","MatchedIds":[400,458,3853,4359,1404],"Predicted":"## Decision\nWe will use RuboCop to lint our Ruby code.\nWe will adopt the default RuboCop configuration, modifying it only as needed to fit our specific code style guidelines.\nWe will integrate RuboCop into our CI\/CD pipeline to ensure code quality.\nWe will set up pre-commit hooks to run RuboCop checks before any code is committed.","GenTime":"2024-08-20 00:05:32"}
{"File Name":"opg-metrics\/0002-aws-infrastructure.md","Context":"## Context\\nBased on ADRs and developer need, we need to create a system that is light weight, fully managed and to integrate into.\\n","Decision":"To run in a fully managed AWS cloud environment using Terraform to manage it.\\n","tokens":31,"id":1545,"text":"## Context\\nBased on ADRs and developer need, we need to create a system that is light weight, fully managed and to integrate into.\\n\n\n##Decision\nTo run in a fully managed AWS cloud environment using Terraform to manage it.\\n","MatchedIds":[2502,4868,4507,3784,4428],"Predicted":"## Decision\nWe choose to implement our system using AWS Lambda for serverless computing. AWS Lambda is a fully managed service that allows us to run code without provisioning or managing servers, thereby significantly reducing operational overhead. This choice is aligned with our need for a lightweight system. For integrations, we will utilize Amazon API Gateway to create and manage APIs, which will allow seamless integration with other services and applications. This approach ensures scalability, reliability, and maintains the lightweight and fully managed characteristics we require.","GenTime":"2024-08-20 00:05:35"}
{"File Name":"mediawiki-extensions-Wikispeech\/0001-store-synthesized-audio-in-mediawiki-extension.md","Context":"## Context\\nThe original implementation of Wikispeech stored the synthesized audio\\nas files in a folder within the Speechoid service (in the\\nwikispeech-server sub-service). The paths to these files, together\\nwith the related metadata were then passed on as a response to the\\nMediaWiki extension.\\nThis implementation had a few identified drawbacks: Wikimedia\\ninfrastructure expects files to be stored in [Swift] rather than as\\nfiles on disk, supporting this would require implementing Swift\\nstorage in the Speechoid service.  There is a desire to keep the\\nSpeechoid service stateless, persistent storage of synthesized files\\nwithin the service runs counter to this.  The utterance metadata was\\nnot stored, requiring that each sentence always be re-synthesized\\nunless cached together with the file path.\\nWhile Wikimedia requires Swift many other MediaWiki installations\\nmight not be interested in that. It is therefore important with a\\nsolution where the file storage backend can be changed as desired\\nthrough the configs.\\nDue to [RevisionDelete] none of the content (words) of any segment\\nanywhere should be stored anywhere, e.g. in a table, since these must\\nthen not be publicly queryable, and to include mechanisms preventing\\nnon-public segments from being synthesized.\\nWe have an interest in storing the utterance audio for a long time to\\navoid the expensive operation of synthesizing segments on demand, but\\nwe still want a mechanism that flush stored utterances after a given\\nperiod of time. If a user makes a change to a text segment, it is\\nunlikely that the previous revision of that segment is used in another\\narticle and could thus be instantly flushed. There is also the case\\nwhere we want to flush to trigger re-synthesizing segments when a word\\nis added to or updated in the phonetic lexicon, as that would improve\\nthe resulting synthesized speech.\\nRe-use of utterance audio across a site (or many sites) is desirable,\\nbut likely to be rare (largely limited to headings and shorter\\nphrases). What will likely be more common is re-use of utterance audio\\nacross multiple revisions of the same page. If a single segment is\\nedited then all other segments, and their corresponding audio, remain\\nvalid. For this reason utterance audio should not be tied to a given\\npage or revision.\\n","Decision":"Files are only temporarily stored within Speechoid.\\nWhen a segment is synthesized, or when the audio is retrieved, a check\\nmust be performed to ensure it corresponds to a page revision which\\nhas not been suppressed through RevisionDelete. A segment is\\nrepresented by a hash to satisfy RevisionDelete requirements on public\\ntables. The segment hash should only be constructed from its\\ncontents. For the sake of RevisionDelete the link to the synthesised\\naudio should never be exposed to the end user.\\nThe MediaWiki extension parses the response from Speechoid, fetches\\nthe synthesized audio and stores this as a file using the provided\\n[FileBackend] functionality. The corresponding utterance metadata is\\nstored as a JSON file. Both files share the same base filename.\\nAn expiry date is attached to each stored Speechoid response to allow\\nlexicon updates to propagate and for the flushing of outdated\\nsegments.\\n","tokens":488,"id":3351,"text":"## Context\\nThe original implementation of Wikispeech stored the synthesized audio\\nas files in a folder within the Speechoid service (in the\\nwikispeech-server sub-service). The paths to these files, together\\nwith the related metadata were then passed on as a response to the\\nMediaWiki extension.\\nThis implementation had a few identified drawbacks: Wikimedia\\ninfrastructure expects files to be stored in [Swift] rather than as\\nfiles on disk, supporting this would require implementing Swift\\nstorage in the Speechoid service.  There is a desire to keep the\\nSpeechoid service stateless, persistent storage of synthesized files\\nwithin the service runs counter to this.  The utterance metadata was\\nnot stored, requiring that each sentence always be re-synthesized\\nunless cached together with the file path.\\nWhile Wikimedia requires Swift many other MediaWiki installations\\nmight not be interested in that. It is therefore important with a\\nsolution where the file storage backend can be changed as desired\\nthrough the configs.\\nDue to [RevisionDelete] none of the content (words) of any segment\\nanywhere should be stored anywhere, e.g. in a table, since these must\\nthen not be publicly queryable, and to include mechanisms preventing\\nnon-public segments from being synthesized.\\nWe have an interest in storing the utterance audio for a long time to\\navoid the expensive operation of synthesizing segments on demand, but\\nwe still want a mechanism that flush stored utterances after a given\\nperiod of time. If a user makes a change to a text segment, it is\\nunlikely that the previous revision of that segment is used in another\\narticle and could thus be instantly flushed. There is also the case\\nwhere we want to flush to trigger re-synthesizing segments when a word\\nis added to or updated in the phonetic lexicon, as that would improve\\nthe resulting synthesized speech.\\nRe-use of utterance audio across a site (or many sites) is desirable,\\nbut likely to be rare (largely limited to headings and shorter\\nphrases). What will likely be more common is re-use of utterance audio\\nacross multiple revisions of the same page. If a single segment is\\nedited then all other segments, and their corresponding audio, remain\\nvalid. For this reason utterance audio should not be tied to a given\\npage or revision.\\n\n\n##Decision\nFiles are only temporarily stored within Speechoid.\\nWhen a segment is synthesized, or when the audio is retrieved, a check\\nmust be performed to ensure it corresponds to a page revision which\\nhas not been suppressed through RevisionDelete. A segment is\\nrepresented by a hash to satisfy RevisionDelete requirements on public\\ntables. The segment hash should only be constructed from its\\ncontents. For the sake of RevisionDelete the link to the synthesised\\naudio should never be exposed to the end user.\\nThe MediaWiki extension parses the response from Speechoid, fetches\\nthe synthesized audio and stores this as a file using the provided\\n[FileBackend] functionality. The corresponding utterance metadata is\\nstored as a JSON file. Both files share the same base filename.\\nAn expiry date is attached to each stored Speechoid response to allow\\nlexicon updates to propagate and for the flushing of outdated\\nsegments.\\n","MatchedIds":[4533,2122,1341,2897,2515],"Predicted":"## Decision\n\nTo address the identified drawbacks and meet the requirements, the following decisions have been made:\n\n1. **Adopt Swift for Persistent Storage**:\n    - Since Wikimedia infrastructure expects files to be stored in Swift, we will implement Swift storage in the Speechoid service. This allows us to align with the existing familiarity and infrastructure of Wikimedia.\n    - However, to accommodate other MediaWiki installations that might not use Swift, the storage backend should be configurable. This ensures flexibility and broader adoption.\n\n2. **Make Speechoid Service Stateless**:\n    - To maintain a stateless Speechoid service, all synthesized audio files and metadata will be stored externally in the chosen storage backend (e.g., Swift).\n    - The service will only handle the synthesis process and will not retain any stateful information.\n\n3. **Metadata Storage**:\n    - Since storing word content is restricted due to RevisionDelete policies, only necessary metadata to reference the audio files will be stored. This metadata includes references to the synthesized files without storing the actual text content.\n    - Metadata should also include information on text segment revisions and dependencies on the phonetic lexicon.\n\n4. **Cache and Flushing Mechanism**:\n    - Implement a caching mechanism to store the synthesized audio files and their metadata. This avoids the need for re-synthesizing segments frequently.\n    - Establish a flushing strategy wherein audio files and metadata are flushed after a certain period of time or when a segment is edited. This helps in maintaining up-to-date and accurate audio synthesis.\n    - Develop a system to trigger re-synthesis when changes are detected in the phonetic lexicon to enhance the quality of synthesized speech.\n\n5. **Reusability of Utterance Audio**:\n    - Design the storage system to decouple utterance audio from specific pages or revisions, focusing instead on the segments themselves. This way, audio can be reused across different pages and revisions, especially for common phrases and headings.\n    - Metadata storage should efficiently handle references to multiple pages or revisions without duplicating the audio files unnecessarily.\n\n## Implementation Plan\n\n1. **Configurable Storage Backend**:\n    - Update the Speechoid service configuration to support different storage backends (Swift, file system, etc.).\n    - Implement the required abstractions to support different storage systems through configuration.\n\n2. **Integration with Swift**:\n    - Develop or integrate an existing Swift client within the Speechoid service to handle storage and retrieval of synthesized audio files and associated metadata.\n\n3. **Caching and Flushing**:\n    - Implement a caching mechanism to handle temporary storage of synthesized audio.\n    - Develop a scheduled task (or similar mechanism) to handle the periodic flushing of obsolete audio files and their metadata.\n\n4. **Metadata Management**:\n    - Design a metadata schema that only stores non-sensitive information, focusing on references to the audio files.\n    - Ensure the schema supports linking segments to multiple pages\/revisions efficiently.\n\n5. **Documentation and Testing**:\n    - Thoroughly document the changes and provide guidelines for configuring the storage backend.\n    - Develop comprehensive tests to ensure the functionality works as expected across different scenarios and configurations.\n\nBy following these decisions and implementation steps, we aim to create a flexible, efficient, and compliant audio synthesis service that meets Wikimedia's requirements while supporting broader MediaWiki installations.","GenTime":"2024-08-20 00:05:57"}
{"File Name":"event-routing-backends\/0006-versioning-of-event-transformers.rst","Context":"Context\\n-------\\nEvent transformers may undergo modification in future in response to consumer request, change in specification, bug fixes etc.\\nDecision\\n--------\\n#. Versions of event transformers will be maintained and emitted as part of xAPI and Caliper events.\\n#. The \"transformer version\" will be a concatenation of the name of the transformer (\"event-routing-backends\"), an @ symbol, and the symantic version of the event-routing-backends package used to generate the event.\\n#. This combined version \"event-routing-backends@v(X.Y.Z)\" can be found in ``context [ extensions [ https:\/\/w3id.org\/xapi\/openedx\/extension\/transformer-version ] ]`` for xAPI statement and in ``extensions [ transformerVersion ]`` for Caliper event.\\n#. Transformer version number will be the semantic version of the event-routing-backends package.\\n#. The event-routing-backends major version will be incremented when:\\n#. Transformer is changed due to update in original specification (xAPI or Caliper).\\n#. A key is removed from or renamed in the existing transformer.\\n#. Value of a key is updated in the existing transformer.\\n#. The event-routing-backends minor version will be incremented when:\\n#. A key is added to an existing transformer.\\n#. A new event is added for transformation.\\n#. Minor version (Z) will be incremented when:\\n#. A bug is fixed.\\n#. Change logs of transformers will be maintained for both xAPI and Caliper.\\nChangelog\\n---------\\n- Updated 2023-02-28 to change the format of the transformer version.\\n- The previous version named the key \"eventVersion\", but the actual implementation used URL key pointing to the event-routing-backends docs. It was decided that since the version number represents the actual version of the event-routing-backends package and not the version of a specific event, this rename could tackle both issues.\\n","Decision":"--------\\n#. Versions of event transformers will be maintained and emitted as part of xAPI and Caliper events.\\n#. The \"transformer version\" will be a concatenation of the name of the transformer (\"event-routing-backends\"), an @ symbol, and the symantic version of the event-routing-backends package used to generate the event.\\n#. This combined version \"event-routing-backends@v(X.Y.Z)\" can be found in ``context [ extensions [ https:\/\/w3id.org\/xapi\/openedx\/extension\/transformer-version ] ]`` for xAPI statement and in ``extensions [ transformerVersion ]`` for Caliper event.\\n#. Transformer version number will be the semantic version of the event-routing-backends package.\\n#. The event-routing-backends major version will be incremented when:\\n#. Transformer is changed due to update in original specification (xAPI or Caliper).\\n#. A key is removed from or renamed in the existing transformer.\\n#. Value of a key is updated in the existing transformer.\\n#. The event-routing-backends minor version will be incremented when:\\n#. A key is added to an existing transformer.\\n#. A new event is added for transformation.\\n#. Minor version (Z) will be incremented when:\\n#. A bug is fixed.\\n#. Change logs of transformers will be maintained for both xAPI and Caliper.\\nChangelog\\n---------\\n- Updated 2023-02-28 to change the format of the transformer version.\\n- The previous version named the key \"eventVersion\", but the actual implementation used URL key pointing to the event-routing-backends docs. It was decided that since the version number represents the actual version of the event-routing-backends package and not the version of a specific event, this rename could tackle both issues.\\n","tokens":398,"id":4483,"text":"Context\\n-------\\nEvent transformers may undergo modification in future in response to consumer request, change in specification, bug fixes etc.\\nDecision\\n--------\\n#. Versions of event transformers will be maintained and emitted as part of xAPI and Caliper events.\\n#. The \"transformer version\" will be a concatenation of the name of the transformer (\"event-routing-backends\"), an @ symbol, and the symantic version of the event-routing-backends package used to generate the event.\\n#. This combined version \"event-routing-backends@v(X.Y.Z)\" can be found in ``context [ extensions [ https:\/\/w3id.org\/xapi\/openedx\/extension\/transformer-version ] ]`` for xAPI statement and in ``extensions [ transformerVersion ]`` for Caliper event.\\n#. Transformer version number will be the semantic version of the event-routing-backends package.\\n#. The event-routing-backends major version will be incremented when:\\n#. Transformer is changed due to update in original specification (xAPI or Caliper).\\n#. A key is removed from or renamed in the existing transformer.\\n#. Value of a key is updated in the existing transformer.\\n#. The event-routing-backends minor version will be incremented when:\\n#. A key is added to an existing transformer.\\n#. A new event is added for transformation.\\n#. Minor version (Z) will be incremented when:\\n#. A bug is fixed.\\n#. Change logs of transformers will be maintained for both xAPI and Caliper.\\nChangelog\\n---------\\n- Updated 2023-02-28 to change the format of the transformer version.\\n- The previous version named the key \"eventVersion\", but the actual implementation used URL key pointing to the event-routing-backends docs. It was decided that since the version number represents the actual version of the event-routing-backends package and not the version of a specific event, this rename could tackle both issues.\\n\n\n##Decision\n--------\\n#. Versions of event transformers will be maintained and emitted as part of xAPI and Caliper events.\\n#. The \"transformer version\" will be a concatenation of the name of the transformer (\"event-routing-backends\"), an @ symbol, and the symantic version of the event-routing-backends package used to generate the event.\\n#. This combined version \"event-routing-backends@v(X.Y.Z)\" can be found in ``context [ extensions [ https:\/\/w3id.org\/xapi\/openedx\/extension\/transformer-version ] ]`` for xAPI statement and in ``extensions [ transformerVersion ]`` for Caliper event.\\n#. Transformer version number will be the semantic version of the event-routing-backends package.\\n#. The event-routing-backends major version will be incremented when:\\n#. Transformer is changed due to update in original specification (xAPI or Caliper).\\n#. A key is removed from or renamed in the existing transformer.\\n#. Value of a key is updated in the existing transformer.\\n#. The event-routing-backends minor version will be incremented when:\\n#. A key is added to an existing transformer.\\n#. A new event is added for transformation.\\n#. Minor version (Z) will be incremented when:\\n#. A bug is fixed.\\n#. Change logs of transformers will be maintained for both xAPI and Caliper.\\nChangelog\\n---------\\n- Updated 2023-02-28 to change the format of the transformer version.\\n- The previous version named the key \"eventVersion\", but the actual implementation used URL key pointing to the event-routing-backends docs. It was decided that since the version number represents the actual version of the event-routing-backends package and not the version of a specific event, this rename could tackle both issues.\\n","MatchedIds":[4477,4424,2951,5177,4436],"Predicted":"## Decision\n--------  \n1. Versions of event transformers will be maintained and emitted as part of xAPI and Caliper events.\n2. The \"transformer version\" will be a concatenation of the name of the transformer (\"event-routing-backends\"), an @ symbol, and the semantic version of the event-routing-backends package used to generate the event.\n3. This combined version \"event-routing-backends@v(X.Y.Z)\" can be found in `context [ extensions [ https:\/\/w3id.org\/xapi\/openedx\/extension\/transformer-version ] ]` for xAPI statement and in `extensions [ transformerVersion ]` for Caliper event.\n4. Transformer version number will be the semantic version of the event-routing-backends package.\n5. The event-routing-backends major version will be incremented when:\n    - Transformer is changed due to update in the original specification (xAPI or Caliper).\n    - A key is removed from or renamed in the existing transformer.\n    - The value of a key is updated in the existing transformer.\n6. The event-routing-backends minor version will be incremented when:\n    - A key is added to an existing transformer.\n    - A new event is added for transformation.\n7. Patch version (Z) will be incremented when:\n    - A bug is fixed.\n8. Change logs of transformers will be maintained for both xAPI and Caliper.","GenTime":"2024-08-20 00:06:02"}
{"File Name":"runner\/0276-problem-matchers.md","Context":"# ADR 0276: Problem Matchers\\n**Date** 2019-06-05\\n**Status** Accepted\\n## Context\\nCompilation failures during a CI build should surface good error messages.\\nFor example, the actual compile errors from the typescript compiler should bubble as issues in the UI. And not simply \"tsc exited with exit code 1\".\\nVSCode has an extensible model for solving this type of problem. VSCode allows users to configure which [problems matchers](https:\/\/code.visualstudio.com\/docs\/editor\/tasks#_defining-a-problem-matcher) to use, when scanning output. For example, a user can apply the `tsc` problem matcher to receive a rich error output experience in VSCode, when compiling their typescript project.\\nThe problem-matcher concept fits well with \"setup\" actions. For example, the `setup-nodejs` action will download node.js, add it to the PATH, and register the `tsc` problem matcher. For the duration of the job, the `tsc` problem matcher will be applied against the output.\\n","Decision":"### Registration\\n#### Using `::` command\\n`::add-matcher::path-to-problem-matcher-config.json`\\nUsing a `::` command allows for flexibility:\\n- Ad hoc scripts can register problem matchers\\n- Allows problem matchers to be conditionally registered\\nNote, if a matcher with the same name is registered a second time, it will clobber the first instance.\\n","tokens":227,"id":3734,"text":"# ADR 0276: Problem Matchers\\n**Date** 2019-06-05\\n**Status** Accepted\\n## Context\\nCompilation failures during a CI build should surface good error messages.\\nFor example, the actual compile errors from the typescript compiler should bubble as issues in the UI. And not simply \"tsc exited with exit code 1\".\\nVSCode has an extensible model for solving this type of problem. VSCode allows users to configure which [problems matchers](https:\/\/code.visualstudio.com\/docs\/editor\/tasks#_defining-a-problem-matcher) to use, when scanning output. For example, a user can apply the `tsc` problem matcher to receive a rich error output experience in VSCode, when compiling their typescript project.\\nThe problem-matcher concept fits well with \"setup\" actions. For example, the `setup-nodejs` action will download node.js, add it to the PATH, and register the `tsc` problem matcher. For the duration of the job, the `tsc` problem matcher will be applied against the output.\\n\n\n##Decision\n### Registration\\n#### Using `::` command\\n`::add-matcher::path-to-problem-matcher-config.json`\\nUsing a `::` command allows for flexibility:\\n- Ad hoc scripts can register problem matchers\\n- Allows problem matchers to be conditionally registered\\nNote, if a matcher with the same name is registered a second time, it will clobber the first instance.\\n","MatchedIds":[1067,4609,2975,3456,3521],"Predicted":"## Decision\nWe will utilize the problem matcher concept in our CI\/CD pipelines to enhance error reporting and visibility. Specifically, we will take the following actions:\n\n1. **Incorporate VSCode Problem Matchers in CI Setup Actions**: \n    - When configuring CI workflows, we will use setup actions such as `setup-nodejs` to automatically register the appropriate problem matchers.\n    - For TypeScript projects, registering the `tsc` problem matcher will be a standard step to ensure that compile errors are surfaced clearly in the CI logs.\n\n2. **Extend Problem Matchers for Other Languages and Tools**:\n    - Investigate and implement problem matchers for other commonly used compilers and tools in our projects (e.g., ESLint, Jest).\n    - This will provide a consistent and rich error reporting experience regardless of the specific tools and languages used.\n    \n3. **Documentation and Training**:\n    - Document the usage of problem matchers in our CI\/CD pipelines to ensure all team members are aware of how to benefit from them.\n    - Provide training material or sessions to familiarize developers with configuring and using problem matchers both locally (in VSCode) and within the CI environment.\n\n### Positive Consequences\n- **Enhanced Error Visibility**: Clear, actionable error messages directly in the CI logs, reducing ambiguity and time spent troubleshooting.\n- **Consistency**: A uniform approach to error reporting across different projects and environments.\n- **Improved Developer Experience**: Developers can immediately see the context and details of errors, speeding up the debugging and fixing process.\n\n### Negative Consequences\n- **Initial Setup Time**: There will be an upfront cost in configuring and documenting the problem matchers.\n- **Maintenance**: Keeping the problem matchers updated and aligned with the tools and versions used can require ongoing effort.","GenTime":"2024-08-20 00:06:08"}
{"File Name":"infection\/0004-PHPUnit-expect-exception-over-try-catch.md","Context":"### Context\\nWhen executing code that is expected to fail in a test case, there is two ways to do this:\\n```php\\nfunction test_something(): void {\\n\/\/ ...\\ntry {\\n\/\/ the statement that fail\\n$this->fail();\\n} catch (Exception $e) {\\n\/\/ ...\\n}\\n}\\n```\\nOr:\\n```php\\nfunction test_something(): void {\\n\/\/ ...\\n$this->expectException($exception)\\n\/\/ the statement that fail\\n}\\n```\\n### Decision\\nAs recommended by [Sebastian Bergmann][sebastian-bergmann] in\\n[this article][phpunit-exception-best-practices], since in both cases a PHPUnit specific API is\\nnecessary, the decision taken is to leverage the `expectException*()` API when possible.\\nA pull request to fix this practice in the whole codebase may be done but has not been made\\nmandatory. New pull requests though should stick to this practice.\\n### Status\\nAccepted ([#1090][1090])\\n[sebastian-bergmann]: https:\/\/thephp.cc\/company\/consultants\/sebastian-bergmann\\n[phpunit-exception-best-practices]: https:\/\/thephp.cc\/news\/2016\/02\/questioning-phpunit-best-practices\\n[1090]: https:\/\/github.com\/infection\/infection\/pull\/1061\\n","Decision":"As recommended by [Sebastian Bergmann][sebastian-bergmann] in\\n[this article][phpunit-exception-best-practices], since in both cases a PHPUnit specific API is\\nnecessary, the decision taken is to leverage the `expectException*()` API when possible.\\nA pull request to fix this practice in the whole codebase may be done but has not been made\\nmandatory. New pull requests though should stick to this practice.\\n### Status\\nAccepted ([#1090][1090])\\n[sebastian-bergmann]: https:\/\/thephp.cc\/company\/consultants\/sebastian-bergmann\\n[phpunit-exception-best-practices]: https:\/\/thephp.cc\/news\/2016\/02\/questioning-phpunit-best-practices\\n[1090]: https:\/\/github.com\/infection\/infection\/pull\/1061\\n","tokens":285,"id":68,"text":"### Context\\nWhen executing code that is expected to fail in a test case, there is two ways to do this:\\n```php\\nfunction test_something(): void {\\n\/\/ ...\\ntry {\\n\/\/ the statement that fail\\n$this->fail();\\n} catch (Exception $e) {\\n\/\/ ...\\n}\\n}\\n```\\nOr:\\n```php\\nfunction test_something(): void {\\n\/\/ ...\\n$this->expectException($exception)\\n\/\/ the statement that fail\\n}\\n```\\n### Decision\\nAs recommended by [Sebastian Bergmann][sebastian-bergmann] in\\n[this article][phpunit-exception-best-practices], since in both cases a PHPUnit specific API is\\nnecessary, the decision taken is to leverage the `expectException*()` API when possible.\\nA pull request to fix this practice in the whole codebase may be done but has not been made\\nmandatory. New pull requests though should stick to this practice.\\n### Status\\nAccepted ([#1090][1090])\\n[sebastian-bergmann]: https:\/\/thephp.cc\/company\/consultants\/sebastian-bergmann\\n[phpunit-exception-best-practices]: https:\/\/thephp.cc\/news\/2016\/02\/questioning-phpunit-best-practices\\n[1090]: https:\/\/github.com\/infection\/infection\/pull\/1061\\n\n\n##Decision\nAs recommended by [Sebastian Bergmann][sebastian-bergmann] in\\n[this article][phpunit-exception-best-practices], since in both cases a PHPUnit specific API is\\nnecessary, the decision taken is to leverage the `expectException*()` API when possible.\\nA pull request to fix this practice in the whole codebase may be done but has not been made\\nmandatory. New pull requests though should stick to this practice.\\n### Status\\nAccepted ([#1090][1090])\\n[sebastian-bergmann]: https:\/\/thephp.cc\/company\/consultants\/sebastian-bergmann\\n[phpunit-exception-best-practices]: https:\/\/thephp.cc\/news\/2016\/02\/questioning-phpunit-best-practices\\n[1090]: https:\/\/github.com\/infection\/infection\/pull\/1061\\n","MatchedIds":[66,1387,650,3853,1339],"Predicted":"### Decision\nAs recommended by [Sebastian Bergmann][sebastian-bergmann] in [this article][phpunit-exception-best-practices], since in both cases a PHPUnit specific API is necessary, the decision taken is to leverage the `expectException*()` API when possible. A pull request to fix this practice in the whole codebase may be done but has not been made mandatory. New pull requests though should stick to this practice.\n\n### Status\nAccepted ([#1090][1090])\n\n[sebastian-bergmann]: https:\/\/thephp.cc\/company\/consultants\/sebastian-bergmann\n[phpunit-exception-best-practices]: https:\/\/thephp.cc\/news\/2016\/02\/questioning-phpunit-best-practices\n[1090]: https:\/\/github.com\/infection\/infection\/pull\/1090","GenTime":"2024-08-20 00:06:11"}
{"File Name":"paas-team-manual\/ADR025-service-plan-naming-conventions.html.md","Context":"## Context\\nOur service plans have evolved incrementally over the last few years and are in\\nneed of some attention. Names are inconsistent, potentially confusing and\\nin many cases contain irrelevant redundant information that is of no practical\\nuse to the platform operators or to tenants consuming the service.\\nAdding additional versions of services has the potential to compound the\\nproblem by multiplying plans of different characteristics with different\\nversions.\\n","Decision":"We have decided to use the following naming convention for naming plans going forward:\\n```\\nSIZE[-HA][-LABEL,-LABEL,...]-VERSION\\n```\\nWhere:\\n* `SIZE` is a string describing the scale one of the plan, it should be one of: `xlarge` `large` `medium` `small` `tiny`.\\n* `HA` is the string `ha` to indicate highly available if relevent.\\n* `LABEL` is a string describing some specific variant of the service if relvent.\\n* `VERSION` is the version number of the service plan.\\n### For example:\\nA large multi-az postgres plan for version 9.6 would be `large-ha-9.6`.\\nA small not multi-az, unclustered, redis 3.2 plan would be `redis-unclustered-3.2`.\\n### Example migrations of some existing plans:\\n```\\nL-HA-enc-dedicated-9.5 => large-ha-9.5\\nM-HA-enc-dedicated-9.5 => medium-ha-9.5\\ntiny-clustered => tiny-clustered-3.2\\ntiny-unclustered => tiny-unclustered-3.2\\n```\\n### Additionally:\\n* We will avoid use of the word \"free\" in names.\\n* We will avoid using redundent words (like 'dedicated') in names to reduce noise.\\n* We will avoid use of uppercase characters in names.\\n* We will avoid abbriviations where possible (\"medium\" instead of \"M\", \"large\" instead of \"L\").\\n* We will avoid offering unencrypted plans where an encrypted version is available (legacy unencrypted plans will be explicitly labelled with `-unencrypted` and hidden from the marketplace).\\n","tokens":83,"id":200,"text":"## Context\\nOur service plans have evolved incrementally over the last few years and are in\\nneed of some attention. Names are inconsistent, potentially confusing and\\nin many cases contain irrelevant redundant information that is of no practical\\nuse to the platform operators or to tenants consuming the service.\\nAdding additional versions of services has the potential to compound the\\nproblem by multiplying plans of different characteristics with different\\nversions.\\n\n\n##Decision\nWe have decided to use the following naming convention for naming plans going forward:\\n```\\nSIZE[-HA][-LABEL,-LABEL,...]-VERSION\\n```\\nWhere:\\n* `SIZE` is a string describing the scale one of the plan, it should be one of: `xlarge` `large` `medium` `small` `tiny`.\\n* `HA` is the string `ha` to indicate highly available if relevent.\\n* `LABEL` is a string describing some specific variant of the service if relvent.\\n* `VERSION` is the version number of the service plan.\\n### For example:\\nA large multi-az postgres plan for version 9.6 would be `large-ha-9.6`.\\nA small not multi-az, unclustered, redis 3.2 plan would be `redis-unclustered-3.2`.\\n### Example migrations of some existing plans:\\n```\\nL-HA-enc-dedicated-9.5 => large-ha-9.5\\nM-HA-enc-dedicated-9.5 => medium-ha-9.5\\ntiny-clustered => tiny-clustered-3.2\\ntiny-unclustered => tiny-unclustered-3.2\\n```\\n### Additionally:\\n* We will avoid use of the word \"free\" in names.\\n* We will avoid using redundent words (like 'dedicated') in names to reduce noise.\\n* We will avoid use of uppercase characters in names.\\n* We will avoid abbriviations where possible (\"medium\" instead of \"M\", \"large\" instead of \"L\").\\n* We will avoid offering unencrypted plans where an encrypted version is available (legacy unencrypted plans will be explicitly labelled with `-unencrypted` and hidden from the marketplace).\\n","MatchedIds":[621,4056,359,631,929],"Predicted":"## Decision\nWe have decided to implement a standardized naming convention and versioning scheme for our service plans. This will involve the following steps:\n\n1. **Review and Audit Existing Service Plans**: Conduct a comprehensive review of all existing service plans to identify inconsistencies, redundancy, and irrelevant information.\n   \n2. **Define Standard Naming Conventions**: Establish a clear and consistent naming convention that is easy to understand for both platform operators and tenants. This should avoid jargon and include only relevant information. A recommended format might include key characteristics and version numbers, such as `serviceName-planType-version`.\n\n3. **Implement Versioning Scheme**: Introduce a versioning scheme that clearly indicates major and minor changes to service plans. For instance:\n   - Major versions (e.g., `v1.0`, `v2.0`) for significant changes that may impact compatibility.\n   - Minor versions (e.g., `v1.1`, `v1.2`) for incremental improvements and bug fixes that do not affect backward compatibility.\n\n4. **Update Documentation**: Ensure that all documentation reflects the new naming conventions and versioning schemes, providing clear guidance on how services are named and versioned.\n\n5. **Communicate Changes**: Proactively communicate these changes to all stakeholders, including tenants and platform operators, explaining the rationale and benefits of the new system.\n\n6. **Deprecate Old Plans**: Gradually phase out and deprecate legacy service plans that do not conform to the new naming conventions and versioning scheme. Provide a migration path for tenants using old plans.\n\nBy adopting these standardized naming conventions and versioning schemes, we aim to reduce confusion, streamline operations, and make it easier for tenants to understand and select the appropriate service plans.","GenTime":"2024-08-20 00:06:18"}
{"File Name":"beis-report-official-development-assistance\/0022-use-govuk-paas-for-hosting.md","Context":"## Context\\nAs a government entity, BEIS want to host their service on [GOVUK\\nPaaS](https:\/\/www.cloud.service.gov.uk\/). This was always the service teams\\nintention.\\n","Decision":"Host all environments excpet local development on BEIS own GOVUK PaaS account.\\n","tokens":44,"id":2410,"text":"## Context\\nAs a government entity, BEIS want to host their service on [GOVUK\\nPaaS](https:\/\/www.cloud.service.gov.uk\/). This was always the service teams\\nintention.\\n\n\n##Decision\nHost all environments excpet local development on BEIS own GOVUK PaaS account.\\n","MatchedIds":[4041,2093,1376,4056,4841],"Predicted":"## Decision\nWe will host the BEIS service on GOV.UK PaaS. This aligns with the service team's initial intention and adheres to the government's PaaS-first policy for hosting. The use of GOV.UK PaaS will allow for efficient and compliant hosting, leveraging the platform's capabilities for managing and deploying government services.\n","GenTime":"2024-08-20 00:06:21"}
{"File Name":"site-stitcher\/001-generate-static-pages.md","Context":"## Context\\nI see so many thing broken on the web today, it is not only website speed, it is also\\nhow many tools we have accepted to use to solve a simple problem. Especially in the JS\\nworld I feel the healthy \"How much do I need?\" question is asked rarely. We also teach\\nnewcomers that it is ok to load 1GB of npm modules before we can actually build an empty page.\\nIn this repo I try to do the simplest thing possible to generate my website.\\nI am looking for dependencies that are as tiny as possible, some I just need to not use and\\nwrite the needed code itself, this also makes me aware how much of a dependency I would have needed\\nonly.\\nAlso the site that I want to ship shall have all a11y, lighthouse, etc. scores as high as possible\\ntherefore the site shall degrade gracefully be accessible and very light weight.\\nI also want the simplest hosting setup and infrastructure possible, due to using github it might\\nbe github-pages, which is just static pages.\\n","Decision":"With static pages the above can be achieved. Easily.\\n","tokens":222,"id":1931,"text":"## Context\\nI see so many thing broken on the web today, it is not only website speed, it is also\\nhow many tools we have accepted to use to solve a simple problem. Especially in the JS\\nworld I feel the healthy \"How much do I need?\" question is asked rarely. We also teach\\nnewcomers that it is ok to load 1GB of npm modules before we can actually build an empty page.\\nIn this repo I try to do the simplest thing possible to generate my website.\\nI am looking for dependencies that are as tiny as possible, some I just need to not use and\\nwrite the needed code itself, this also makes me aware how much of a dependency I would have needed\\nonly.\\nAlso the site that I want to ship shall have all a11y, lighthouse, etc. scores as high as possible\\ntherefore the site shall degrade gracefully be accessible and very light weight.\\nI also want the simplest hosting setup and infrastructure possible, due to using github it might\\nbe github-pages, which is just static pages.\\n\n\n##Decision\nWith static pages the above can be achieved. Easily.\\n","MatchedIds":[1599,4940,5140,1576,3428],"Predicted":"## Decision\nI decided to develop the website using minimal custom JavaScript and CSS without heavy frameworks or libraries. By focusing on handwritten code, I aim to avoid unnecessary dependencies, thereby reducing the bloat commonly associated with modern web development. The website will be structured as static HTML pages enhanced with small, custom scripts where needed.\n\nFor accessibility and performance, I will:\n- Ensure all pages meet high standards for a11y (accessibility) compliance and pass Lighthouse scores.\n- Implement best practices such as semantic HTML, responsive design, and optimized images.\n\nTo maintain a simple and straightforward infrastructure, the site will be hosted using GitHub Pages. This will take advantage of GitHub's static hosting capabilities, which align with the project's goal of a lightweight and easily maintainable setup.","GenTime":"2024-08-20 00:06:25"}
{"File Name":"ReportMI-service-manual\/0006-use-terraform-to-create-and-document-infrastructure.md","Context":"## Context\\nRunning a service like the Data Submission Service requires infrastructure of\\nvarious kinds.\\nIt's good practice to manage this infrastructure using code (Infrastructure as\\nCode), as this allows the infrastructure to be version-controlled and managed\\nlike the rest of the application code.\\nThere are various mechanisms for doing this. Each of the main cloud providers\\nhave their own solutions to manage infrastructure in code, for example Amazon\\nWeb Services (AWS) has CloudFormation, Microsoft Azure has Resource Manager etc.\\nHowever each of these are specific to the individual cloud provider.\\nIt would also be possible to do this manually by running scripts against the\\ncloud provider API. However, this would take a significant amount of time and\\nwould take effort to make it work across more than one cloud provider.\\nThere are tools available, like Terraform, which allow you to define\\ninfrastructure as code, in a standard way which can then be applied against more\\nthan one cloud provider. The tool handles the differences between providers.\\n","Decision":"We will use Terraform to create the infrastructure for the Data Submission\\nService.\\n","tokens":208,"id":2062,"text":"## Context\\nRunning a service like the Data Submission Service requires infrastructure of\\nvarious kinds.\\nIt's good practice to manage this infrastructure using code (Infrastructure as\\nCode), as this allows the infrastructure to be version-controlled and managed\\nlike the rest of the application code.\\nThere are various mechanisms for doing this. Each of the main cloud providers\\nhave their own solutions to manage infrastructure in code, for example Amazon\\nWeb Services (AWS) has CloudFormation, Microsoft Azure has Resource Manager etc.\\nHowever each of these are specific to the individual cloud provider.\\nIt would also be possible to do this manually by running scripts against the\\ncloud provider API. However, this would take a significant amount of time and\\nwould take effort to make it work across more than one cloud provider.\\nThere are tools available, like Terraform, which allow you to define\\ninfrastructure as code, in a standard way which can then be applied against more\\nthan one cloud provider. The tool handles the differences between providers.\\n\n\n##Decision\nWe will use Terraform to create the infrastructure for the Data Submission\\nService.\\n","MatchedIds":[2409,1464,1376,4937,3144],"Predicted":"## Decision\nWe will use Terraform to manage the infrastructure for the Data Submission Service. This allows us to define infrastructure as code in a standard way, enabling version control and simplifying deployments across multiple cloud providers. By leveraging Terraform, we avoid the complexity of managing provider-specific solutions and manual scripting efforts.","GenTime":"2024-08-20 00:06:34"}
{"File Name":"opg-data\/0005-content-structure.md","Context":"## Context\\nA consistent and well-defined document specification is required so that we may develop an API contract\\n","Decision":"Our structure closely follows the [JSON-API](https:\/\/jsonapi.org\/format\/#document-structure) document structure\\n### Document Root Level\\nAt the root level is always a JSON object.\\nThis **MUST** contain at least one of the following root-level members:\\n* data: the document's \"primary data\" resource object\\n* A single resource object is represented by a JSON object\\n* A collection or resource objects is represented by an array of objects\\n* errors: an array of error objects\\n#### Single resource object\\n```json\\n{\\n\"data\": {},\\n\"meta\": {}\\n}\\n```\\n#### Collection of resource objects\\n```json\\n{\\n\"data\": [\\n{...},\\n{...}\\n],\\n\"errors\": [],\\n\"meta\": {},\\n\"links\": {}\\n}\\n```\\nJSON-API states\\n> The members data and errors MUST NOT coexist in the same document.\\n`opg-data` standard is that if a data resource OBJECT is returned, then there **MUST be no** error member.\\nHowever there ARE certain circumstances where an array of errors **MAY** be returned alongside a data resource COLLECTION. See [0007-error-handling-and-status-codes.md#errors-in-20x](0007-error-handling-and-status-codes.md#errors-in-20x)\\nThe root JSON object **MAY** also contain the following root-level members:\\n* meta: a meta object that contains non-standard meta-information\\n* links: a links object related to the primary data (typically used for pagination links if the data returned is a collection)\\n### The Resource Object\\nSee [JSON-AP](https:\/\/jsonapi.org\/format\/#document-resource-objects)\\nNamely:\\n* As a minimum, every resource object **MUST** contain:\\n* an id member\\n* a type member. The values of the id and type members **MUST** be strings. For consistency and the avoidance of confusion, types **MUST** use PLURAL. eg \"articles\", \"people\"\\n* an array of attributes (even if empty)\\n* IF the \"data\" top level member is present, then there **MUST** be a `links` array, containing as it's minimum, a `self` member with a URL which **MUST** be callable at the API\\n* A resource object's data attributes is presented in an array named \"attributes\"\\n* A resource object's links is presented in an object named \"links\"\\n* A resource object's relationships is presented in an array named \"relationships\"\\n* A resource object's attributes and its relationships are collectively called its \"fields\"\\n```json\\n{\\n\"data\": [\\n{\\n\"type\": \"articles\",\\n\"id\": \"1\",\\n\"attributes\": {\\n\"title\": \"My First Article\",\\n\"description\": \"...\"\\n},\\n\"links\": {\\n\"self\": \"https:\/\/api.example.com\/articles\/1\",\\n\"next\": \"https:\/\/api.example.com\/articles\/2\"\\n},\\n\"relationships\": {\\n...\\n}\\n},\\n{\\n\"type\": \"articles\",\\n\"id\": \"2\",\\n\"attributes\": {\\n\"title\": \"Second Article\",\\n\"description\": \"...\"\\n},\\n\"links\": {\\n\"self\": \"https:\/\/api.example.com\/articles\/2\",\\n\"prev\": \"https:\/\/api.example.com\/articles\/1\"\\n},\\n\"relationships\": {\\n...\\n}\\n}\\n],\\n\"links\": {},\\n\"meta\": {}\\n}\\n```\\n#### Relationships\\nRelationships will be defined in a separate ADR\\n### Meta\\nSee [JSON-API](https:\/\/jsonapi.org\/format\/#document-meta)\\n### Errors\\nErrors responses will be defined in a separate ADR\\n","tokens":21,"id":2181,"text":"## Context\\nA consistent and well-defined document specification is required so that we may develop an API contract\\n\n\n##Decision\nOur structure closely follows the [JSON-API](https:\/\/jsonapi.org\/format\/#document-structure) document structure\\n### Document Root Level\\nAt the root level is always a JSON object.\\nThis **MUST** contain at least one of the following root-level members:\\n* data: the document's \"primary data\" resource object\\n* A single resource object is represented by a JSON object\\n* A collection or resource objects is represented by an array of objects\\n* errors: an array of error objects\\n#### Single resource object\\n```json\\n{\\n\"data\": {},\\n\"meta\": {}\\n}\\n```\\n#### Collection of resource objects\\n```json\\n{\\n\"data\": [\\n{...},\\n{...}\\n],\\n\"errors\": [],\\n\"meta\": {},\\n\"links\": {}\\n}\\n```\\nJSON-API states\\n> The members data and errors MUST NOT coexist in the same document.\\n`opg-data` standard is that if a data resource OBJECT is returned, then there **MUST be no** error member.\\nHowever there ARE certain circumstances where an array of errors **MAY** be returned alongside a data resource COLLECTION. See [0007-error-handling-and-status-codes.md#errors-in-20x](0007-error-handling-and-status-codes.md#errors-in-20x)\\nThe root JSON object **MAY** also contain the following root-level members:\\n* meta: a meta object that contains non-standard meta-information\\n* links: a links object related to the primary data (typically used for pagination links if the data returned is a collection)\\n### The Resource Object\\nSee [JSON-AP](https:\/\/jsonapi.org\/format\/#document-resource-objects)\\nNamely:\\n* As a minimum, every resource object **MUST** contain:\\n* an id member\\n* a type member. The values of the id and type members **MUST** be strings. For consistency and the avoidance of confusion, types **MUST** use PLURAL. eg \"articles\", \"people\"\\n* an array of attributes (even if empty)\\n* IF the \"data\" top level member is present, then there **MUST** be a `links` array, containing as it's minimum, a `self` member with a URL which **MUST** be callable at the API\\n* A resource object's data attributes is presented in an array named \"attributes\"\\n* A resource object's links is presented in an object named \"links\"\\n* A resource object's relationships is presented in an array named \"relationships\"\\n* A resource object's attributes and its relationships are collectively called its \"fields\"\\n```json\\n{\\n\"data\": [\\n{\\n\"type\": \"articles\",\\n\"id\": \"1\",\\n\"attributes\": {\\n\"title\": \"My First Article\",\\n\"description\": \"...\"\\n},\\n\"links\": {\\n\"self\": \"https:\/\/api.example.com\/articles\/1\",\\n\"next\": \"https:\/\/api.example.com\/articles\/2\"\\n},\\n\"relationships\": {\\n...\\n}\\n},\\n{\\n\"type\": \"articles\",\\n\"id\": \"2\",\\n\"attributes\": {\\n\"title\": \"Second Article\",\\n\"description\": \"...\"\\n},\\n\"links\": {\\n\"self\": \"https:\/\/api.example.com\/articles\/2\",\\n\"prev\": \"https:\/\/api.example.com\/articles\/1\"\\n},\\n\"relationships\": {\\n...\\n}\\n}\\n],\\n\"links\": {},\\n\"meta\": {}\\n}\\n```\\n#### Relationships\\nRelationships will be defined in a separate ADR\\n### Meta\\nSee [JSON-API](https:\/\/jsonapi.org\/format\/#document-meta)\\n### Errors\\nErrors responses will be defined in a separate ADR\\n","MatchedIds":[1180,3069,3853,2502,4428],"Predicted":"## Decision\nWe use the [OpenAPI 3.0](https:\/\/swagger.io\/specification\/) specification for developing and maintaining API contracts. OpenAPI 3.0 provides a robust and versatile framework to define our APIs in a consistent and well-defined manner, which facilitates better interoperability and documentation.","GenTime":"2024-08-20 00:06:39"}
{"File Name":"otm-docs\/api_framework.md","Context":"## Context\\nUsing pure PHP is kinda an uphill battle. Frameworks deliver ready-made solutions. Choosing proper framework ensures smooth workflow.\\n","Decision":"Laravel 7\\n","tokens":27,"id":3062,"text":"## Context\\nUsing pure PHP is kinda an uphill battle. Frameworks deliver ready-made solutions. Choosing proper framework ensures smooth workflow.\\n\n\n##Decision\nLaravel 7\\n","MatchedIds":[1243,1477,4271,1240,3718],"Predicted":"## Decision\nWe will use the Laravel framework because it offers an elegant syntax, comprehensive documentation, and a rich set of features that simplify common tasks such as routing, authentication, and caching. Laravel also has a strong community and a wide range of packages available, which will ensure smooth workflow and reduce development time.","GenTime":"2024-08-20 00:06:41"}
{"File Name":"clone_difitalcitizenship\/0002-use-openapi-to-defined-the-api-specs.md","Context":"## Context\\nWe need to define the API specifications of the services we're going to implement.\\n","Decision":"We use the [OpenAPI 2.0](https:\/\/swagger.io\/specification\/) specification (aka Swagger spec) as standard for our REST API definitions.\\n","tokens":20,"id":1180,"text":"## Context\\nWe need to define the API specifications of the services we're going to implement.\\n\n\n##Decision\nWe use the [OpenAPI 2.0](https:\/\/swagger.io\/specification\/) specification (aka Swagger spec) as standard for our REST API definitions.\\n","MatchedIds":[2181,4644,4428,3119,1436],"Predicted":"## Decision\nWe will adopt the OpenAPI Specification (formerly known as Swagger) for defining the API specifications of the services. \n\n### Positive Consequences\n- **Standardization**: OpenAPI is a widely accepted standard for API specification which ensures that our APIs are well-defined and consistent.\n- **Tools and Ecosystem**: There is a rich ecosystem of tools available for OpenAPI, including documentation generators, client SDK generators, and testing tools.\n- **Improved Collaboration**: OpenAPI specifications can be shared easily among team members, enhancing collaboration between developers, QA, and other stakeholders.\n- **Automated Documentation**: API documentation can be automatically generated and stay in sync with the code, reducing maintenance effort.\n- **Interoperability**: By adhering to a widely recognized standard, our APIs will be more easily consumable by external developers and other software systems.\n\n### Negative Consequences\n- **Learning Curve**: The team needs to become adept at writing and maintaining OpenAPI specifications.\n- **Initial Overhead**: Defining detailed specifications upfront may take additional time initially, which could slow down immediate progress but will pay off in the long term.\n\n### Steps to Implement\n1. **Training**: Conduct training sessions to familiarize the team with the OpenAPI specification and tools.\n2. **Tooling Setup**: Integrate OpenAPI-compatible tools into our development workflow for validation, documentation, and testing.\n3. **Define Initial Specs**: Start by defining the API specifications for the first few services as a proof-of-concept.\n4. **Iterate and Refine**: Gather feedback from the team and iterate on the specifications to ensure they meet our needs.\n\nBy adopting the OpenAPI Specification, we aim to create a clear and maintainable contract for our services, which will facilitate better development practices and smoother collaboration.","GenTime":"2024-08-20 00:06:48"}
{"File Name":"easyvista_toolkit\/0001_Using_globalVariable.md","Context":"## Context\\nWe want to be able to have autocompletion in the cmdlet to make them more user friendly.\\nBecause the easyvista rest API relies on GUID for some parameters we'll need to be able to query their friendly name within in the cmdlet parameters.\\nWe tried using environment variables but they don't seem to work with hashtable (we did not check extensively).\\n","Decision":"We will use global variables (named *$Global:EZVvariablename*) set by a dedicated cmdlet (*set-EZVcontext*). That cmdlet will define an execution context for all other cmdlet in the project.\\n","tokens":79,"id":2329,"text":"## Context\\nWe want to be able to have autocompletion in the cmdlet to make them more user friendly.\\nBecause the easyvista rest API relies on GUID for some parameters we'll need to be able to query their friendly name within in the cmdlet parameters.\\nWe tried using environment variables but they don't seem to work with hashtable (we did not check extensively).\\n\n\n##Decision\nWe will use global variables (named *$Global:EZVvariablename*) set by a dedicated cmdlet (*set-EZVcontext*). That cmdlet will define an execution context for all other cmdlet in the project.\\n","MatchedIds":[1255,2255,3571,2191,1254],"Predicted":"## Decision\nWe will implement a dynamic parameter completion system within the cmdlets. This system will query the EasyVista REST API to fetch and cache friendly names alongside their corresponding GUIDs.\n\n1. **Cached Data Store:** Create a local cached data store (e.g., an in-memory hashtable or a JSON file) to store the mappings of GUIDs to friendly names.\n2. **API Query:** When autocompletion is triggered, check if the required data is cached. If not, make a REST API call to EasyVista to retrieve the necessary information and update the cache.\n3. **Dynamic Parameters:** Use PowerShell's `ArgumentCompleter` to dynamically suggest parameters based on the cached data.\n4. **Error Handling:** Implement robust error handling to manage potential issues during API calls and caching failures gracefully.\n5. **Environment Variables:** While environment variables won't be used for hashtable storage, they can still be employed to manage configuration settings (such as EasyVista API endpoint URLs) securely and dynamically.\n\nThis approach will ensure user-friendly autocompletion while efficiently managing API data and reducing redundant API calls.","GenTime":"2024-08-20 00:06:54"}
{"File Name":"verify-service-provider\/0013-we-will-write-acceptance-tests-against-compliance-tool.md","Context":"## Context\\nVerify provide a \"compliance tool\" which allows relying parties to check that their\\nimplementations are compatible with the SAML profile of Verify.\\nCurrently relying parties must test their service against the compliance tool as part\\nof the onboarding process.\\nBy writing tests against the compliance tool we can:\\n* make sure our own implementation is compliant\\n* demonstrate to relying parties how to write tests against the compliance tool\\n* learn more about the user experience of using the compliance tool\\n","Decision":"We'll write acceptance tests against the compliance tool for the verify service provider\\nand the stub relying parties.\\nTests for the service provider will give us direct feedback on whether it's compliant.\\nTests for the stub relying parties will give us confidence they work end-to-end and should\\nprovide a template for real relying parties to write their own tests.\\n","tokens":101,"id":4442,"text":"## Context\\nVerify provide a \"compliance tool\" which allows relying parties to check that their\\nimplementations are compatible with the SAML profile of Verify.\\nCurrently relying parties must test their service against the compliance tool as part\\nof the onboarding process.\\nBy writing tests against the compliance tool we can:\\n* make sure our own implementation is compliant\\n* demonstrate to relying parties how to write tests against the compliance tool\\n* learn more about the user experience of using the compliance tool\\n\n\n##Decision\nWe'll write acceptance tests against the compliance tool for the verify service provider\\nand the stub relying parties.\\nTests for the service provider will give us direct feedback on whether it's compliant.\\nTests for the stub relying parties will give us confidence they work end-to-end and should\\nprovide a template for real relying parties to write their own tests.\\n","MatchedIds":[581,4454,1258,4179,1387],"Predicted":"## Decision\nWe will develop a suite of automated tests that target the compliance tool. These tests will be written in a manner that ensures our implementation adheres to the SAML profile of Verify. Additionally, these tests will be used as a reference for relying parties, showcasing best practices and offering guidance on how to write their own tests against the compliance tool.\n\nTo achieve this, we will:\n1. **Create an extensive test suite**: Covering all aspects of the SAML profile compliance requirements to ensure thorough validation.\n2. **Document the tests**: Provide comprehensive documentation and comments within the test code to make it easier for relying parties to understand and adapt the examples.\n3. **Use a well-known testing framework**: Opt for a testing framework commonly used in the industry to make our examples more accessible. For instance, JUnit for Java-based implementations or pytest for Python-based implementations.\n4. **Provide user experience feedback**: Regularly review and improve the compliance tool based on our testing experiences to ensure it is user-friendly and effective.\n\nBy implementing this decision, we will ensure our own compliance, assist relying parties in their onboarding process, and enhance the overall user experience with the compliance tool.","GenTime":"2024-08-20 00:06:59"}
{"File Name":"uniprot-rest-api\/0003-spring-framework.md","Context":"## Context\\nProgramming frameworks can promote the productivity of a project; producing smaller code bases, added reliability,\\nadditional features (than one would otherwise write themselves), etc.\\n","Decision":"We have used the [Spring framework](https:\/\/spring.io\/) within the team for a number of years, and its recent advances in the domain of REST applications makes it an ideal choice.\\n","tokens":35,"id":1477,"text":"## Context\\nProgramming frameworks can promote the productivity of a project; producing smaller code bases, added reliability,\\nadditional features (than one would otherwise write themselves), etc.\\n\n\n##Decision\nWe have used the [Spring framework](https:\/\/spring.io\/) within the team for a number of years, and its recent advances in the domain of REST applications makes it an ideal choice.\\n","MatchedIds":[4271,3062,4359,1416,4868],"Predicted":"## Decision\nWe will adopt the [Django](https:\/\/www.djangoproject.com\/) framework for our project. Django is a high-level Python Web framework that encourages rapid development and clean, pragmatic design. It includes a variety of built-in features, such as an ORM (Object-Relational Mapping), authentication, and an administrative interface, which will promote productivity, reduce boilerplate code, and enhance the reliability of our project. Its extensive documentation and large community support further ensure that we can efficiently resolve any issues and continuously improve our application.","GenTime":"2024-08-20 00:07:07"}
{"File Name":"beis-report-official-development-assistance\/0025-use-terraform-to-set-up-and-deploy-to-the-paas.md","Context":"## Context\\nWe like to control our infrastructure with code. On a previous project we used shell scripts to set up the spaces and services within a PaaS organisation and to also do deploys from Travis.\\nWhile shell scripts are fine there is now a useful [terraform provider](https:\/\/github.com\/cloudfoundry-community\/terraform-provider-cf) for cloudfoundry.\\nThe cloudfoundry provider also deploys the app.\\nTerraform is the default choice for provisioning infrastructure these days.\\n","Decision":"- spaces and services should be created with Terraform\\n- the application should be deployed with Terraform\\n- Terraform should be run from Travis to deploy the application\\n","tokens":107,"id":2409,"text":"## Context\\nWe like to control our infrastructure with code. On a previous project we used shell scripts to set up the spaces and services within a PaaS organisation and to also do deploys from Travis.\\nWhile shell scripts are fine there is now a useful [terraform provider](https:\/\/github.com\/cloudfoundry-community\/terraform-provider-cf) for cloudfoundry.\\nThe cloudfoundry provider also deploys the app.\\nTerraform is the default choice for provisioning infrastructure these days.\\n\n\n##Decision\n- spaces and services should be created with Terraform\\n- the application should be deployed with Terraform\\n- Terraform should be run from Travis to deploy the application\\n","MatchedIds":[2062,1372,1376,4937,4358],"Predicted":"## Decision\nWe will transition from using shell scripts to using the CloudFoundry Terraform provider to manage our PaaS spaces, services, and application deployments. This aligns with our preference for Infrastructure as Code (IaC) and ensures consistency and maintainability across our projects.","GenTime":"2024-08-20 00:07:09"}
{"File Name":"cosmos-sdk\/adr-033-protobuf-inter-module-comm.md","Context":"## Context\\nIn the current Cosmos SDK documentation on the [Object-Capability Model](https:\/\/docs.cosmos.network\/main\/learn\/advanced\/ocap#ocaps-in-practice), it is stated that:\\n> We assume that a thriving ecosystem of Cosmos SDK modules that are easy to compose into a blockchain application will contain faulty or malicious modules.\\nThere is currently not a thriving ecosystem of Cosmos SDK modules. We hypothesize that this is in part due to:\\n1. lack of a stable v1.0 Cosmos SDK to build modules off of. Module interfaces are changing, sometimes dramatically, from\\npoint release to point release, often for good reasons, but this does not create a stable foundation to build on.\\n2. lack of a properly implemented object capability or even object-oriented encapsulation system which makes refactors\\nof module keeper interfaces inevitable because the current interfaces are poorly constrained.\\n### `x\/bank` Case Study\\nCurrently the `x\/bank` keeper gives pretty much unrestricted access to any module which references it. For instance, the\\n`SetBalance` method allows the caller to set the balance of any account to anything, bypassing even proper tracking of supply.\\nThere appears to have been some later attempts to implement some semblance of OCAPs using module-level minting, staking\\nand burning permissions. These permissions allow a module to mint, burn or delegate tokens with reference to the module\u2019s\\nown account. These permissions are actually stored as a `[]string` array on the `ModuleAccount` type in state.\\nHowever, these permissions don\u2019t really do much. They control what modules can be referenced in the `MintCoins`,\\n`BurnCoins` and `DelegateCoins***` methods, but for one there is no unique object capability token that controls access \u2014\\njust a simple string. So the `x\/upgrade` module could mint tokens for the `x\/staking` module simple by calling\\n`MintCoins(\u201cstaking\u201d)`. Furthermore, all modules which have access to these keeper methods, also have access to\\n`SetBalance` negating any other attempt at OCAPs and breaking even basic object-oriented encapsulation.\\n","Decision":"Based on [ADR-021](.\/adr-021-protobuf-query-encoding.md) and [ADR-031](.\/adr-031-msg-service.md), we introduce the\\nInter-Module Communication framework for secure module authorization and OCAPs.\\nWhen implemented, this could also serve as an alternative to the existing paradigm of passing keepers between\\nmodules. The approach outlined here-in is intended to form the basis of a Cosmos SDK v1.0 that provides the necessary\\nstability and encapsulation guarantees that allow a thriving module ecosystem to emerge.\\nOf particular note \u2014 the decision is to _enable_ this functionality for modules to adopt at their own discretion.\\nProposals to migrate existing modules to this new paradigm will have to be a separate conversation, potentially\\naddressed as amendments to this ADR.\\n### New \"Keeper\" Paradigm\\nIn [ADR 021](.\/adr-021-protobuf-query-encoding.md), a mechanism for using protobuf service definitions to define queriers\\nwas introduced and in [ADR 31](.\/adr-031-msg-service.md), a mechanism for using protobuf service to define `Msg`s was added.\\nProtobuf service definitions generate two golang interfaces representing the client and server sides of a service plus\\nsome helper code. Here is a minimal example for the bank `cosmos.bank.Msg\/Send` message type:\\n```go\\npackage bank\\ntype MsgClient interface {\\nSend(context.Context, *MsgSend, opts ...grpc.CallOption) (*MsgSendResponse, error)\\n}\\ntype MsgServer interface {\\nSend(context.Context, *MsgSend) (*MsgSendResponse, error)\\n}\\n```\\n[ADR 021](.\/adr-021-protobuf-query-encoding.md) and [ADR 31](.\/adr-031-msg-service.md) specifies how modules can implement the generated `QueryServer`\\nand `MsgServer` interfaces as replacements for the legacy queriers and `Msg` handlers respectively.\\nIn this ADR we explain how modules can make queries and send `Msg`s to other modules using the generated `QueryClient`\\nand `MsgClient` interfaces and propose this mechanism as a replacement for the existing `Keeper` paradigm. To be clear,\\nthis ADR does not necessitate the creation of new protobuf definitions or services. Rather, it leverages the same proto\\nbased service interfaces already used by clients for inter-module communication.\\nUsing this `QueryClient`\/`MsgClient` approach has the following key benefits over exposing keepers to external modules:\\n1. Protobuf types are checked for breaking changes using [buf](https:\/\/buf.build\/docs\/breaking-overview) and because of\\nthe way protobuf is designed this will give us strong backwards compatibility guarantees while allowing for forward\\nevolution.\\n2. The separation between the client and server interfaces will allow us to insert permission checking code in between\\nthe two which checks if one module is authorized to send the specified `Msg` to the other module providing a proper\\nobject capability system (see below).\\n3. The router for inter-module communication gives us a convenient place to handle rollback of transactions,\\nenabling atomicy of operations ([currently a problem](https:\/\/github.com\/cosmos\/cosmos-sdk\/issues\/8030)). Any failure within a module-to-module call would result in a failure of the entire\\ntransaction\\nThis mechanism has the added benefits of:\\n* reducing boilerplate through code generation, and\\n* allowing for modules in other languages either via a VM like CosmWasm or sub-processes using gRPC\\n### Inter-module Communication\\nTo use the `Client` generated by the protobuf compiler we need a `grpc.ClientConn` [interface](https:\/\/github.com\/grpc\/grpc-go\/blob\/v1.49.x\/clientconn.go#L441-L450)\\nimplementation. For this we introduce\\na new type, `ModuleKey`, which implements the `grpc.ClientConn` interface. `ModuleKey` can be thought of as the \"private\\nkey\" corresponding to a module account, where authentication is provided through use of a special `Invoker()` function,\\ndescribed in more detail below.\\nBlockchain users (external clients) use their account's private key to sign transactions containing `Msg`s where they are listed as signers (each\\nmessage specifies required signers with `Msg.GetSigner`). The authentication checks is performed by `AnteHandler`.\\nHere, we extend this process, by allowing modules to be identified in `Msg.GetSigners`. When a module wants to trigger the execution a `Msg` in another module,\\nits `ModuleKey` acts as the sender (through the `ClientConn` interface we describe below) and is set as a sole \"signer\". It's worth to note\\nthat we don't use any cryptographic signature in this case.\\nFor example, module `A` could use its `A.ModuleKey` to create `MsgSend` object for `\/cosmos.bank.Msg\/Send` transaction. `MsgSend` validation\\nwill assure that the `from` account (`A.ModuleKey` in this case) is the signer.\\nHere's an example of a hypothetical module `foo` interacting with `x\/bank`:\\n```go\\npackage foo\\ntype FooMsgServer {\\n\/\/ ...\\nbankQuery bank.QueryClient\\nbankMsg   bank.MsgClient\\n}\\nfunc NewFooMsgServer(moduleKey RootModuleKey, ...) FooMsgServer {\\n\/\/ ...\\nreturn FooMsgServer {\\n\/\/ ...\\nmodouleKey: moduleKey,\\nbankQuery: bank.NewQueryClient(moduleKey),\\nbankMsg: bank.NewMsgClient(moduleKey),\\n}\\n}\\nfunc (foo *FooMsgServer) Bar(ctx context.Context, req *MsgBarRequest) (*MsgBarResponse, error) {\\nbalance, err := foo.bankQuery.Balance(&bank.QueryBalanceRequest{Address: fooMsgServer.moduleKey.Address(), Denom: \"foo\"})\\n...\\nres, err := foo.bankMsg.Send(ctx, &bank.MsgSendRequest{FromAddress: fooMsgServer.moduleKey.Address(), ...})\\n...\\n}\\n```\\nThis design is also intended to be extensible to cover use cases of more fine grained permissioning like minting by\\ndenom prefix being restricted to certain modules (as discussed in\\n[#7459](https:\/\/github.com\/cosmos\/cosmos-sdk\/pull\/7459#discussion_r529545528)).\\n### `ModuleKey`s and `ModuleID`s\\nA `ModuleKey` can be thought of as a \"private key\" for a module account and a `ModuleID` can be thought of as the\\ncorresponding \"public key\". From the [ADR 028](.\/adr-028-public-key-addresses.md), modules can have both a root module account and any number of sub-accounts\\nor derived accounts that can be used for different pools (ex. staking pools) or managed accounts (ex. group\\naccounts). We can also think of module sub-accounts as similar to derived keys - there is a root key and then some\\nderivation path. `ModuleID` is a simple struct which contains the module name and optional \"derivation\" path,\\nand forms its address based on the `AddressHash` method from [the ADR-028](https:\/\/github.com\/cosmos\/cosmos-sdk\/blob\/main\/docs\/architecture\/adr-028-public-key-addresses.md):\\n```go\\ntype ModuleID struct {\\nModuleName string\\nPath []byte\\n}\\nfunc (key ModuleID) Address() []byte {\\nreturn AddressHash(key.ModuleName, key.Path)\\n}\\n```\\nIn addition to being able to generate a `ModuleID` and address, a `ModuleKey` contains a special function called\\n`Invoker` which is the key to safe inter-module access. The `Invoker` creates an `InvokeFn` closure which is used as an `Invoke` method in\\nthe `grpc.ClientConn` interface and under the hood is able to route messages to the appropriate `Msg` and `Query` handlers\\nperforming appropriate security checks on `Msg`s. This allows for even safer inter-module access than keeper's whose\\nprivate member variables could be manipulated through reflection. Golang does not support reflection on a function\\nclosure's captured variables and direct manipulation of memory would be needed for a truly malicious module to bypass\\nthe `ModuleKey` security.\\nThe two `ModuleKey` types are `RootModuleKey` and `DerivedModuleKey`:\\n```go\\ntype Invoker func(callInfo CallInfo) func(ctx context.Context, request, response interface{}, opts ...interface{}) error\\ntype CallInfo {\\nMethod string\\nCaller ModuleID\\n}\\ntype RootModuleKey struct {\\nmoduleName string\\ninvoker Invoker\\n}\\nfunc (rm RootModuleKey) Derive(path []byte) DerivedModuleKey { \/* ... *\/}\\ntype DerivedModuleKey struct {\\nmoduleName string\\npath []byte\\ninvoker Invoker\\n}\\n```\\nA module can get access to a `DerivedModuleKey`, using the `Derive(path []byte)` method on `RootModuleKey` and then\\nwould use this key to authenticate `Msg`s from a sub-account. Ex:\\n```go\\npackage foo\\nfunc (fooMsgServer *MsgServer) Bar(ctx context.Context, req *MsgBar) (*MsgBarResponse, error) {\\nderivedKey := fooMsgServer.moduleKey.Derive(req.SomePath)\\nbankMsgClient := bank.NewMsgClient(derivedKey)\\nres, err := bankMsgClient.Balance(ctx, &bank.MsgSend{FromAddress: derivedKey.Address(), ...})\\n...\\n}\\n```\\nIn this way, a module can gain permissioned access to a root account and any number of sub-accounts and send\\nauthenticated `Msg`s from these accounts. The `Invoker` `callInfo.Caller` parameter is used under the hood to\\ndistinguish between different module accounts, but either way the function returned by `Invoker` only allows `Msg`s\\nfrom either the root or a derived module account to pass through.\\nNote that `Invoker` itself returns a function closure based on the `CallInfo` passed in. This will allow client implementations\\nin the future that cache the invoke function for each method type avoiding the overhead of hash table lookup.\\nThis would reduce the performance overhead of this inter-module communication method to the bare minimum required for\\nchecking permissions.\\nTo re-iterate, the closure only allows access to authorized calls. There is no access to anything else regardless of any\\nname impersonation.\\nBelow is a rough sketch of the implementation of `grpc.ClientConn.Invoke` for `RootModuleKey`:\\n```go\\nfunc (key RootModuleKey) Invoke(ctx context.Context, method string, args, reply interface{}, opts ...grpc.CallOption) error {\\nf := key.invoker(CallInfo {Method: method, Caller: ModuleID {ModuleName: key.moduleName}})\\nreturn f(ctx, args, reply)\\n}\\n```\\n### `AppModule` Wiring and Requirements\\nIn [ADR 031](.\/adr-031-msg-service.md), the `AppModule.RegisterService(Configurator)` method was introduced. To support\\ninter-module communication, we extend the `Configurator` interface to pass in the `ModuleKey` and to allow modules to\\nspecify their dependencies on other modules using `RequireServer()`:\\n```go\\ntype Configurator interface {\\nMsgServer() grpc.Server\\nQueryServer() grpc.Server\\nModuleKey() ModuleKey\\nRequireServer(msgServer interface{})\\n}\\n```\\nThe `ModuleKey` is passed to modules in the `RegisterService` method itself so that `RegisterServices` serves as a single\\nentry point for configuring module services. This is intended to also have the side-effect of greatly reducing boilerplate in\\n`app.go`. For now, `ModuleKey`s will be created based on `AppModule.Name()`, but a more flexible system may be\\nintroduced in the future. The `ModuleManager` will handle creation of module accounts behind the scenes.\\nBecause modules do not get direct access to each other anymore, modules may have unfulfilled dependencies. To make sure\\nthat module dependencies are resolved at startup, the `Configurator.RequireServer` method should be added. The `ModuleManager`\\nwill make sure that all dependencies declared with `RequireServer` can be resolved before the app starts. An example\\nmodule `foo` could declare it's dependency on `x\/bank` like this:\\n```go\\npackage foo\\nfunc (am AppModule) RegisterServices(cfg Configurator) {\\ncfg.RequireServer((*bank.QueryServer)(nil))\\ncfg.RequireServer((*bank.MsgServer)(nil))\\n}\\n```\\n### Security Considerations\\nIn addition to checking for `ModuleKey` permissions, a few additional security precautions will need to be taken by\\nthe underlying router infrastructure.\\n#### Recursion and Re-entry\\nRecursive or re-entrant method invocations pose a potential security threat. This can be a problem if Module A\\ncalls Module B and Module B calls module A again in the same call.\\nOne basic way for the router system to deal with this is to maintain a call stack which prevents a module from\\nbeing referenced more than once in the call stack so that there is no re-entry. A `map[string]interface{}` table\\nin the router could be used to perform this security check.\\n#### Queries\\nQueries in Cosmos SDK are generally un-permissioned so allowing one module to query another module should not pose\\nany major security threats assuming basic precautions are taken. The basic precaution that the router system will\\nneed to take is making sure that the `sdk.Context` passed to query methods does not allow writing to the store. This\\ncan be done for now with a `CacheMultiStore` as is currently done for `BaseApp` queries.\\n### Internal Methods\\nIn many cases, we may wish for modules to call methods on other modules which are not exposed to clients at all. For this\\npurpose, we add the `InternalServer` method to `Configurator`:\\n```go\\ntype Configurator interface {\\nMsgServer() grpc.Server\\nQueryServer() grpc.Server\\nInternalServer() grpc.Server\\n}\\n```\\nAs an example, x\/slashing's Slash must call x\/staking's Slash, but we don't want to expose x\/staking's Slash to end users\\nand clients.\\nInternal protobuf services will be defined in a corresponding `internal.proto` file in the given module's\\nproto package.\\nServices registered against `InternalServer` will be callable from other modules but not by external clients.\\nAn alternative solution to internal-only methods could involve hooks \/ plugins as discussed [here](https:\/\/github.com\/cosmos\/cosmos-sdk\/pull\/7459#issuecomment-733807753).\\nA more detailed evaluation of a hooks \/ plugin system will be addressed later in follow-ups to this ADR or as a separate\\nADR.\\n### Authorization\\nBy default, the inter-module router requires that messages are sent by the first signer returned by `GetSigners`. The\\ninter-module router should also accept authorization middleware such as that provided by [ADR 030](https:\/\/github.com\/cosmos\/cosmos-sdk\/blob\/main\/docs\/architecture\/adr-030-authz-module.md).\\nThis middleware will allow accounts to otherwise specific module accounts to perform actions on their behalf.\\nAuthorization middleware should take into account the need to grant certain modules effectively \"admin\" privileges to\\nother modules. This will be addressed in separate ADRs or updates to this ADR.\\n### Future Work\\nOther future improvements may include:\\n* custom code generation that:\\n* simplifies interfaces (ex. generates code with `sdk.Context` instead of `context.Context`)\\n* optimizes inter-module calls - for instance caching resolved methods after first invocation\\n* combining `StoreKey`s and `ModuleKey`s into a single interface so that modules have a single OCAPs handle\\n* code generation which makes inter-module communication more performant\\n* decoupling `ModuleKey` creation from `AppModule.Name()` so that app's can override root module account names\\n* inter-module hooks and plugins\\n","tokens":451,"id":812,"text":"## Context\\nIn the current Cosmos SDK documentation on the [Object-Capability Model](https:\/\/docs.cosmos.network\/main\/learn\/advanced\/ocap#ocaps-in-practice), it is stated that:\\n> We assume that a thriving ecosystem of Cosmos SDK modules that are easy to compose into a blockchain application will contain faulty or malicious modules.\\nThere is currently not a thriving ecosystem of Cosmos SDK modules. We hypothesize that this is in part due to:\\n1. lack of a stable v1.0 Cosmos SDK to build modules off of. Module interfaces are changing, sometimes dramatically, from\\npoint release to point release, often for good reasons, but this does not create a stable foundation to build on.\\n2. lack of a properly implemented object capability or even object-oriented encapsulation system which makes refactors\\nof module keeper interfaces inevitable because the current interfaces are poorly constrained.\\n### `x\/bank` Case Study\\nCurrently the `x\/bank` keeper gives pretty much unrestricted access to any module which references it. For instance, the\\n`SetBalance` method allows the caller to set the balance of any account to anything, bypassing even proper tracking of supply.\\nThere appears to have been some later attempts to implement some semblance of OCAPs using module-level minting, staking\\nand burning permissions. These permissions allow a module to mint, burn or delegate tokens with reference to the module\u2019s\\nown account. These permissions are actually stored as a `[]string` array on the `ModuleAccount` type in state.\\nHowever, these permissions don\u2019t really do much. They control what modules can be referenced in the `MintCoins`,\\n`BurnCoins` and `DelegateCoins***` methods, but for one there is no unique object capability token that controls access \u2014\\njust a simple string. So the `x\/upgrade` module could mint tokens for the `x\/staking` module simple by calling\\n`MintCoins(\u201cstaking\u201d)`. Furthermore, all modules which have access to these keeper methods, also have access to\\n`SetBalance` negating any other attempt at OCAPs and breaking even basic object-oriented encapsulation.\\n\n\n##Decision\nBased on [ADR-021](.\/adr-021-protobuf-query-encoding.md) and [ADR-031](.\/adr-031-msg-service.md), we introduce the\\nInter-Module Communication framework for secure module authorization and OCAPs.\\nWhen implemented, this could also serve as an alternative to the existing paradigm of passing keepers between\\nmodules. The approach outlined here-in is intended to form the basis of a Cosmos SDK v1.0 that provides the necessary\\nstability and encapsulation guarantees that allow a thriving module ecosystem to emerge.\\nOf particular note \u2014 the decision is to _enable_ this functionality for modules to adopt at their own discretion.\\nProposals to migrate existing modules to this new paradigm will have to be a separate conversation, potentially\\naddressed as amendments to this ADR.\\n### New \"Keeper\" Paradigm\\nIn [ADR 021](.\/adr-021-protobuf-query-encoding.md), a mechanism for using protobuf service definitions to define queriers\\nwas introduced and in [ADR 31](.\/adr-031-msg-service.md), a mechanism for using protobuf service to define `Msg`s was added.\\nProtobuf service definitions generate two golang interfaces representing the client and server sides of a service plus\\nsome helper code. Here is a minimal example for the bank `cosmos.bank.Msg\/Send` message type:\\n```go\\npackage bank\\ntype MsgClient interface {\\nSend(context.Context, *MsgSend, opts ...grpc.CallOption) (*MsgSendResponse, error)\\n}\\ntype MsgServer interface {\\nSend(context.Context, *MsgSend) (*MsgSendResponse, error)\\n}\\n```\\n[ADR 021](.\/adr-021-protobuf-query-encoding.md) and [ADR 31](.\/adr-031-msg-service.md) specifies how modules can implement the generated `QueryServer`\\nand `MsgServer` interfaces as replacements for the legacy queriers and `Msg` handlers respectively.\\nIn this ADR we explain how modules can make queries and send `Msg`s to other modules using the generated `QueryClient`\\nand `MsgClient` interfaces and propose this mechanism as a replacement for the existing `Keeper` paradigm. To be clear,\\nthis ADR does not necessitate the creation of new protobuf definitions or services. Rather, it leverages the same proto\\nbased service interfaces already used by clients for inter-module communication.\\nUsing this `QueryClient`\/`MsgClient` approach has the following key benefits over exposing keepers to external modules:\\n1. Protobuf types are checked for breaking changes using [buf](https:\/\/buf.build\/docs\/breaking-overview) and because of\\nthe way protobuf is designed this will give us strong backwards compatibility guarantees while allowing for forward\\nevolution.\\n2. The separation between the client and server interfaces will allow us to insert permission checking code in between\\nthe two which checks if one module is authorized to send the specified `Msg` to the other module providing a proper\\nobject capability system (see below).\\n3. The router for inter-module communication gives us a convenient place to handle rollback of transactions,\\nenabling atomicy of operations ([currently a problem](https:\/\/github.com\/cosmos\/cosmos-sdk\/issues\/8030)). Any failure within a module-to-module call would result in a failure of the entire\\ntransaction\\nThis mechanism has the added benefits of:\\n* reducing boilerplate through code generation, and\\n* allowing for modules in other languages either via a VM like CosmWasm or sub-processes using gRPC\\n### Inter-module Communication\\nTo use the `Client` generated by the protobuf compiler we need a `grpc.ClientConn` [interface](https:\/\/github.com\/grpc\/grpc-go\/blob\/v1.49.x\/clientconn.go#L441-L450)\\nimplementation. For this we introduce\\na new type, `ModuleKey`, which implements the `grpc.ClientConn` interface. `ModuleKey` can be thought of as the \"private\\nkey\" corresponding to a module account, where authentication is provided through use of a special `Invoker()` function,\\ndescribed in more detail below.\\nBlockchain users (external clients) use their account's private key to sign transactions containing `Msg`s where they are listed as signers (each\\nmessage specifies required signers with `Msg.GetSigner`). The authentication checks is performed by `AnteHandler`.\\nHere, we extend this process, by allowing modules to be identified in `Msg.GetSigners`. When a module wants to trigger the execution a `Msg` in another module,\\nits `ModuleKey` acts as the sender (through the `ClientConn` interface we describe below) and is set as a sole \"signer\". It's worth to note\\nthat we don't use any cryptographic signature in this case.\\nFor example, module `A` could use its `A.ModuleKey` to create `MsgSend` object for `\/cosmos.bank.Msg\/Send` transaction. `MsgSend` validation\\nwill assure that the `from` account (`A.ModuleKey` in this case) is the signer.\\nHere's an example of a hypothetical module `foo` interacting with `x\/bank`:\\n```go\\npackage foo\\ntype FooMsgServer {\\n\/\/ ...\\nbankQuery bank.QueryClient\\nbankMsg   bank.MsgClient\\n}\\nfunc NewFooMsgServer(moduleKey RootModuleKey, ...) FooMsgServer {\\n\/\/ ...\\nreturn FooMsgServer {\\n\/\/ ...\\nmodouleKey: moduleKey,\\nbankQuery: bank.NewQueryClient(moduleKey),\\nbankMsg: bank.NewMsgClient(moduleKey),\\n}\\n}\\nfunc (foo *FooMsgServer) Bar(ctx context.Context, req *MsgBarRequest) (*MsgBarResponse, error) {\\nbalance, err := foo.bankQuery.Balance(&bank.QueryBalanceRequest{Address: fooMsgServer.moduleKey.Address(), Denom: \"foo\"})\\n...\\nres, err := foo.bankMsg.Send(ctx, &bank.MsgSendRequest{FromAddress: fooMsgServer.moduleKey.Address(), ...})\\n...\\n}\\n```\\nThis design is also intended to be extensible to cover use cases of more fine grained permissioning like minting by\\ndenom prefix being restricted to certain modules (as discussed in\\n[#7459](https:\/\/github.com\/cosmos\/cosmos-sdk\/pull\/7459#discussion_r529545528)).\\n### `ModuleKey`s and `ModuleID`s\\nA `ModuleKey` can be thought of as a \"private key\" for a module account and a `ModuleID` can be thought of as the\\ncorresponding \"public key\". From the [ADR 028](.\/adr-028-public-key-addresses.md), modules can have both a root module account and any number of sub-accounts\\nor derived accounts that can be used for different pools (ex. staking pools) or managed accounts (ex. group\\naccounts). We can also think of module sub-accounts as similar to derived keys - there is a root key and then some\\nderivation path. `ModuleID` is a simple struct which contains the module name and optional \"derivation\" path,\\nand forms its address based on the `AddressHash` method from [the ADR-028](https:\/\/github.com\/cosmos\/cosmos-sdk\/blob\/main\/docs\/architecture\/adr-028-public-key-addresses.md):\\n```go\\ntype ModuleID struct {\\nModuleName string\\nPath []byte\\n}\\nfunc (key ModuleID) Address() []byte {\\nreturn AddressHash(key.ModuleName, key.Path)\\n}\\n```\\nIn addition to being able to generate a `ModuleID` and address, a `ModuleKey` contains a special function called\\n`Invoker` which is the key to safe inter-module access. The `Invoker` creates an `InvokeFn` closure which is used as an `Invoke` method in\\nthe `grpc.ClientConn` interface and under the hood is able to route messages to the appropriate `Msg` and `Query` handlers\\nperforming appropriate security checks on `Msg`s. This allows for even safer inter-module access than keeper's whose\\nprivate member variables could be manipulated through reflection. Golang does not support reflection on a function\\nclosure's captured variables and direct manipulation of memory would be needed for a truly malicious module to bypass\\nthe `ModuleKey` security.\\nThe two `ModuleKey` types are `RootModuleKey` and `DerivedModuleKey`:\\n```go\\ntype Invoker func(callInfo CallInfo) func(ctx context.Context, request, response interface{}, opts ...interface{}) error\\ntype CallInfo {\\nMethod string\\nCaller ModuleID\\n}\\ntype RootModuleKey struct {\\nmoduleName string\\ninvoker Invoker\\n}\\nfunc (rm RootModuleKey) Derive(path []byte) DerivedModuleKey { \/* ... *\/}\\ntype DerivedModuleKey struct {\\nmoduleName string\\npath []byte\\ninvoker Invoker\\n}\\n```\\nA module can get access to a `DerivedModuleKey`, using the `Derive(path []byte)` method on `RootModuleKey` and then\\nwould use this key to authenticate `Msg`s from a sub-account. Ex:\\n```go\\npackage foo\\nfunc (fooMsgServer *MsgServer) Bar(ctx context.Context, req *MsgBar) (*MsgBarResponse, error) {\\nderivedKey := fooMsgServer.moduleKey.Derive(req.SomePath)\\nbankMsgClient := bank.NewMsgClient(derivedKey)\\nres, err := bankMsgClient.Balance(ctx, &bank.MsgSend{FromAddress: derivedKey.Address(), ...})\\n...\\n}\\n```\\nIn this way, a module can gain permissioned access to a root account and any number of sub-accounts and send\\nauthenticated `Msg`s from these accounts. The `Invoker` `callInfo.Caller` parameter is used under the hood to\\ndistinguish between different module accounts, but either way the function returned by `Invoker` only allows `Msg`s\\nfrom either the root or a derived module account to pass through.\\nNote that `Invoker` itself returns a function closure based on the `CallInfo` passed in. This will allow client implementations\\nin the future that cache the invoke function for each method type avoiding the overhead of hash table lookup.\\nThis would reduce the performance overhead of this inter-module communication method to the bare minimum required for\\nchecking permissions.\\nTo re-iterate, the closure only allows access to authorized calls. There is no access to anything else regardless of any\\nname impersonation.\\nBelow is a rough sketch of the implementation of `grpc.ClientConn.Invoke` for `RootModuleKey`:\\n```go\\nfunc (key RootModuleKey) Invoke(ctx context.Context, method string, args, reply interface{}, opts ...grpc.CallOption) error {\\nf := key.invoker(CallInfo {Method: method, Caller: ModuleID {ModuleName: key.moduleName}})\\nreturn f(ctx, args, reply)\\n}\\n```\\n### `AppModule` Wiring and Requirements\\nIn [ADR 031](.\/adr-031-msg-service.md), the `AppModule.RegisterService(Configurator)` method was introduced. To support\\ninter-module communication, we extend the `Configurator` interface to pass in the `ModuleKey` and to allow modules to\\nspecify their dependencies on other modules using `RequireServer()`:\\n```go\\ntype Configurator interface {\\nMsgServer() grpc.Server\\nQueryServer() grpc.Server\\nModuleKey() ModuleKey\\nRequireServer(msgServer interface{})\\n}\\n```\\nThe `ModuleKey` is passed to modules in the `RegisterService` method itself so that `RegisterServices` serves as a single\\nentry point for configuring module services. This is intended to also have the side-effect of greatly reducing boilerplate in\\n`app.go`. For now, `ModuleKey`s will be created based on `AppModule.Name()`, but a more flexible system may be\\nintroduced in the future. The `ModuleManager` will handle creation of module accounts behind the scenes.\\nBecause modules do not get direct access to each other anymore, modules may have unfulfilled dependencies. To make sure\\nthat module dependencies are resolved at startup, the `Configurator.RequireServer` method should be added. The `ModuleManager`\\nwill make sure that all dependencies declared with `RequireServer` can be resolved before the app starts. An example\\nmodule `foo` could declare it's dependency on `x\/bank` like this:\\n```go\\npackage foo\\nfunc (am AppModule) RegisterServices(cfg Configurator) {\\ncfg.RequireServer((*bank.QueryServer)(nil))\\ncfg.RequireServer((*bank.MsgServer)(nil))\\n}\\n```\\n### Security Considerations\\nIn addition to checking for `ModuleKey` permissions, a few additional security precautions will need to be taken by\\nthe underlying router infrastructure.\\n#### Recursion and Re-entry\\nRecursive or re-entrant method invocations pose a potential security threat. This can be a problem if Module A\\ncalls Module B and Module B calls module A again in the same call.\\nOne basic way for the router system to deal with this is to maintain a call stack which prevents a module from\\nbeing referenced more than once in the call stack so that there is no re-entry. A `map[string]interface{}` table\\nin the router could be used to perform this security check.\\n#### Queries\\nQueries in Cosmos SDK are generally un-permissioned so allowing one module to query another module should not pose\\nany major security threats assuming basic precautions are taken. The basic precaution that the router system will\\nneed to take is making sure that the `sdk.Context` passed to query methods does not allow writing to the store. This\\ncan be done for now with a `CacheMultiStore` as is currently done for `BaseApp` queries.\\n### Internal Methods\\nIn many cases, we may wish for modules to call methods on other modules which are not exposed to clients at all. For this\\npurpose, we add the `InternalServer` method to `Configurator`:\\n```go\\ntype Configurator interface {\\nMsgServer() grpc.Server\\nQueryServer() grpc.Server\\nInternalServer() grpc.Server\\n}\\n```\\nAs an example, x\/slashing's Slash must call x\/staking's Slash, but we don't want to expose x\/staking's Slash to end users\\nand clients.\\nInternal protobuf services will be defined in a corresponding `internal.proto` file in the given module's\\nproto package.\\nServices registered against `InternalServer` will be callable from other modules but not by external clients.\\nAn alternative solution to internal-only methods could involve hooks \/ plugins as discussed [here](https:\/\/github.com\/cosmos\/cosmos-sdk\/pull\/7459#issuecomment-733807753).\\nA more detailed evaluation of a hooks \/ plugin system will be addressed later in follow-ups to this ADR or as a separate\\nADR.\\n### Authorization\\nBy default, the inter-module router requires that messages are sent by the first signer returned by `GetSigners`. The\\ninter-module router should also accept authorization middleware such as that provided by [ADR 030](https:\/\/github.com\/cosmos\/cosmos-sdk\/blob\/main\/docs\/architecture\/adr-030-authz-module.md).\\nThis middleware will allow accounts to otherwise specific module accounts to perform actions on their behalf.\\nAuthorization middleware should take into account the need to grant certain modules effectively \"admin\" privileges to\\nother modules. This will be addressed in separate ADRs or updates to this ADR.\\n### Future Work\\nOther future improvements may include:\\n* custom code generation that:\\n* simplifies interfaces (ex. generates code with `sdk.Context` instead of `context.Context`)\\n* optimizes inter-module calls - for instance caching resolved methods after first invocation\\n* combining `StoreKey`s and `ModuleKey`s into a single interface so that modules have a single OCAPs handle\\n* code generation which makes inter-module communication more performant\\n* decoupling `ModuleKey` creation from `AppModule.Name()` so that app's can override root module account names\\n* inter-module hooks and plugins\\n","MatchedIds":[839,842,33,1495,827],"Predicted":"## Decision\nTo address the stability and encapsulation issues in the Cosmos SDK, the following architectural decisions are made:\n\n### 1. Stabilize the v1.0 Cosmos SDK\n* **Implement API Versioning:** Introduce and maintain a stable v1.0 API that is backward compatible. This will give module developers a stable foundation to build on and minimize disruptive changes between releases.\n* **Module Maturity Levels:** Classify modules into different maturity levels such as experimental, stable, and deprecated. This can guide developers in choosing stable modules and preparing for future changes.\n\n### 2. Strengthen Object Capability (OCAP) System\n* **Unique Capability Tokens:** Replace string-based permissions with unique capability tokens. Each module that needs specific capabilities (e.g., minting, burning) will be granted a unique token that is verifiable and non-transferable.\n  ```go\n  type CapabilityToken struct {\n      ModuleName string\n      Capability string\n  }\n  \n  func (k Keeper) MintCoins(ctx sdk.Context, token CapabilityToken, amount sdk.Coins) error {\n      if !k.verifyCapabilityToken(ctx, token, \"mint\") {\n          return sdkerrors.Wrap(sdkerrors.ErrUnauthorized, \"invalid capability token\")\n      }\n      \/\/ Minting logic here\n  }\n  \n  func (k Keeper) verifyCapabilityToken(ctx sdk.Context, token CapabilityToken, capability string) bool {\n      \/\/ Verification logic here\n  }\n  ```\n\n### 3. Encapsulation of Module Interfaces\n* **Encapsulate Critical Methods:** Restrict access to critical methods like `SetBalance`, allowing only the `x\/bank` keeper to modify balances directly. Expose higher-level methods that enforce proper validation and constraints.\n  ```go\n  \/\/ x\/bank\/keeper.go\n  func (k Keeper) SetBalance(ctx sdk.Context, addr sdk.AccAddress, balance sdk.Coin) error {\n      \/\/ Validation and constraints to prevent supply inconsistencies\n      if !balance.IsValid() {\n          return sdkerrors.Wrap(sdkerrors.ErrInvalidCoins, \"invalid balance\")\n      }\n      \/\/ Set balance logic here\n  }\n  \n  func (k Keeper) AdjustBalance(ctx sdk.Context, addr sdk.AccAddress, adjustment sdk.Coin) error {\n      currentBalance := k.GetBalance(ctx, addr)\n      newBalance, err := currentBalance.Add(adjustment)\n      if err != nil {\n          return err\n      }\n      return k.SetBalance(ctx, addr, newBalance)\n  }\n  ```\n\n### 4. Audit and Refactor Existing Modules\n* **Capability Audits:** Perform an audit of existing modules to identify and rectify modules that bypass encapsulation or misuse capabilities. Refactor such modules to fit the new OCAP model.\n\n### 5. Documentation and Best Practices\n* **Guidelines for Module Development:** Create comprehensive documentation outlining the best practices for module development, emphasizing the importance of capability tokens, encapsulation, and stability.\n* **Examples and Tutorials:** Provide examples of correctly implemented modules to serve as a reference for developers.\n\n### Implementation Timeline\n1. **Immediate (within 3 months):** Introduce API versioning and documentation updates.\n2. **Short-term (within 6 months):** Implement unique capability tokens, encapsulate critical methods, and provide module development guidelines.\n3. **Mid-term (within 12 months):** Audit and refactor existing modules, stabilize the v1.0 API.\n4. **Long-term (ongoing):** Maintain API stability, continuously audit modules, and update best practices.\n\nBy implementing these measures, the Cosmos SDK can achieve a more stable foundation and secure, well-encapsulated modules, fostering a thriving and trustworthy ecosystem.","GenTime":"2024-08-20 00:07:22"}
{"File Name":"publicservice-registry\/0004-sqlstreamstore.md","Context":"## Context\\nSince we decided to use event sourcing, we need a way to store events in our database.\\nIn `Wegwijs` we stored events in `MSSQL`, which allows easy debugging of events. All sql statements to save\/read events were hand-written.\\n**However**, since we decided on async event handlers in a previous ADR, we would benefit a lot from having catch-up subscriptions for our event handlers. Catch-up subscriptions allow event handlers to be in charge of what events they are interested in, and give event handlers more autonomy over their own rebuilds.\\nWhile `GetEventStore` supports this, and is most likely a top-notch choice for storing events, this would require us to take care of hosting this. We also have doubts about the support for storing business-critical data outside of `MSSQL` in `AIV`.\\nWe currently host no VMs for business-critical concerns, and we feel that hosting `GetEventStore` ourselves, would add a significant burden.\\nAs an alternative, `SqlStreamStore` is an OSS library on GitHub which supports storing events into `MSSQL`, and has support for catch-up subscriptions. It has an active community, and has been used in several production systems successfully according to that community.\\n","Decision":"We will use the `SqlStreamStore` library as our event store. We will keep an eye on ongoing developments from `SqlStreamStore`.\\n","tokens":264,"id":3123,"text":"## Context\\nSince we decided to use event sourcing, we need a way to store events in our database.\\nIn `Wegwijs` we stored events in `MSSQL`, which allows easy debugging of events. All sql statements to save\/read events were hand-written.\\n**However**, since we decided on async event handlers in a previous ADR, we would benefit a lot from having catch-up subscriptions for our event handlers. Catch-up subscriptions allow event handlers to be in charge of what events they are interested in, and give event handlers more autonomy over their own rebuilds.\\nWhile `GetEventStore` supports this, and is most likely a top-notch choice for storing events, this would require us to take care of hosting this. We also have doubts about the support for storing business-critical data outside of `MSSQL` in `AIV`.\\nWe currently host no VMs for business-critical concerns, and we feel that hosting `GetEventStore` ourselves, would add a significant burden.\\nAs an alternative, `SqlStreamStore` is an OSS library on GitHub which supports storing events into `MSSQL`, and has support for catch-up subscriptions. It has an active community, and has been used in several production systems successfully according to that community.\\n\n\n##Decision\nWe will use the `SqlStreamStore` library as our event store. We will keep an eye on ongoing developments from `SqlStreamStore`.\\n","MatchedIds":[3129,4747,2520,1740,1661],"Predicted":"## Decision\nChosen option: \"SqlStreamStore\", because\n\n- It supports storing events into `MSSQL`, which aligns with our existing infrastructure for easy debugging and maintenance.\n- It has built-in support for catch-up subscriptions, which aligns with our decision for async event handlers.\n- It has an active community and has been successfully used in several production systems, providing reliability and community support.\n- It avoids the additional burden and complexity of hosting `GetEventStore` ourselves and integrates smoothly into our current setup without the need for hosting new VMs for business-critical concerns.","GenTime":"2024-08-20 00:07:26"}
{"File Name":"nhsuk-prototype-kit-version-one\/0003-use-npm-scripts-and-gulp-for-running-tasks.md","Context":"## Context\\nThere are lots of different tasks that need processed in order to get the prototype kit up and running. Tasks such as; installing dependencies, moving files from dependencies into the app file structure, and most importantly - running the application.\\n","Decision":"We will use a mixture on NPM scripts and [Gulp](https:\/\/gulpjs.com) to run our tasks. NPM scripts give us the core installation and start tasks as well as the ability to run Gulp tasks. Gulp, written in javascript, is very extensible and will allow us to have complete control over compilation and assembly of the applications assets.\\n","tokens":49,"id":3682,"text":"## Context\\nThere are lots of different tasks that need processed in order to get the prototype kit up and running. Tasks such as; installing dependencies, moving files from dependencies into the app file structure, and most importantly - running the application.\\n\n\n##Decision\nWe will use a mixture on NPM scripts and [Gulp](https:\/\/gulpjs.com) to run our tasks. NPM scripts give us the core installation and start tasks as well as the ability to run Gulp tasks. Gulp, written in javascript, is very extensible and will allow us to have complete control over compilation and assembly of the applications assets.\\n","MatchedIds":[3722,3681,1239,4416,4021],"Predicted":"## Decision\nWe will use a task runner like `npm scripts` for handling tasks such as installing dependencies and running the application. For more complex tasks like moving files from dependencies into the app file structure, we will use `Gulp` since it allows for easy automation of such processes. This approach provides a balance between simplicity and power, and it's well supported within the Node.js ecosystem.","GenTime":"2024-08-20 00:07:33"}
{"File Name":"fxa\/0020-application-architecture.md","Context":"## Context and Problem Statement\\nThe FxA back-end service stack contains an application architecture that is ad-hoc, not documented, and missing modern features (such as Dependency Injection) which results in the following problems:\\n- New developers struggle to get up to speed as they must learn the architecture by reading the code as we have no documentation on the application structure, why they're structured the way they are, or how new components should be added to fit in. Each back-end service may vary in its ad-hoc architecture as well.\\n- Adding new objects needed in a route handler can be time-consuming as the object must be plumbed through the entire initialization chain vs. more elegant methods like Dependency Injection (DI).\\n- Not clear where\/how to add new components and takes time to study\/understand how things are currently setup in an attempt to mimic the structure for the new component.\\n- Time consuming to setup boiler-plate for components, as we have no tooling to work with the current ad-hoc application architectures.\\n- Our ad-hoc architecture frequently mixes concerns such as having business logic mixed in with request handling logic, and has other warts from its evolution over time vs. being planned up front.\\n- New back-end services evolve differently resulting in more ad-hoc application architectures to learn.\\n- Shared components in `fxa-shared` can't rely on basic object lifecycles or setup approaches as they may be used in multiple different ad-hoc application architectures.\\nNot choosing an application framework means that we have choosen to make ad-hoc application architectures which will continue to exhibit the problems above.\\nIt is assumed that the four newest FxA back-end services (admin-server, support-panel, event-broker, gql-api) will be switched to the chosen approach for consistency.\\n## Decision Drivers\\n- Documented application architecture.\\n- Tooling that reduces boiler-plate and creates consistent code architecture.\\n- Modern paradigms to ease creation of global objects and their testability, such as DI.\\n- Training materials so that new developers can understand and work on application features easily.\\n- Ability to migrate legacy applications to similar conventions\/setup.\\n- Handles situations we need such as:\\n- Exception handling\\n- Validation\\n- Service instantiation (DI, etc)\\n- Authentication\\n- RESTful API's (Swagger\/OpenAPI a bonus)\\n- GraphQL\\n- Use of components\/services easily in a script\\n","Decision":"- Documented application architecture.\\n- Tooling that reduces boiler-plate and creates consistent code architecture.\\n- Modern paradigms to ease creation of global objects and their testability, such as DI.\\n- Training materials so that new developers can understand and work on application features easily.\\n- Ability to migrate legacy applications to similar conventions\/setup.\\n- Handles situations we need such as:\\n- Exception handling\\n- Validation\\n- Service instantiation (DI, etc)\\n- Authentication\\n- RESTful API's (Swagger\/OpenAPI a bonus)\\n- GraphQL\\n- Use of components\/services easily in a script\\nChosen Option: Use an existing framework: NestJS\\nNestJS and LoopBack are the two most compelling options, however NestJS has substantially better GraphQL support and a much larger user-base. LoopBack has its user-base divided between two versions with substantial changes between them. We will use NestJS for new projects and update the newest services (admin-server, support-panel, event-broker, gql-api) to NestJS.\\nTo reduce documentation needs and address that auth-server will not be reasonable to migrate, we will backport code organization schemes and directory\/filename conventions from NestJS and use `typedi` as a DI system to ease configuration\/testing in auth-server and possibly other legacy packages. This will allow developers familiar with NestJS projects to still locate and easily work on FxA legacy packages.\\n### Positive Consequences\\n- Less documentation to create, as we only document differences for legacy services.\\n- NestJS GraphQL support matches existing typegraphql paradigm closely for minimal effort switch.\\n### Negative Consequences\\n- There will be some effort involved to migrate the 4 packages to NestJS.\\n- We will need to document the differences between NestJS and other legacy services.\\n","tokens":499,"id":389,"text":"## Context and Problem Statement\\nThe FxA back-end service stack contains an application architecture that is ad-hoc, not documented, and missing modern features (such as Dependency Injection) which results in the following problems:\\n- New developers struggle to get up to speed as they must learn the architecture by reading the code as we have no documentation on the application structure, why they're structured the way they are, or how new components should be added to fit in. Each back-end service may vary in its ad-hoc architecture as well.\\n- Adding new objects needed in a route handler can be time-consuming as the object must be plumbed through the entire initialization chain vs. more elegant methods like Dependency Injection (DI).\\n- Not clear where\/how to add new components and takes time to study\/understand how things are currently setup in an attempt to mimic the structure for the new component.\\n- Time consuming to setup boiler-plate for components, as we have no tooling to work with the current ad-hoc application architectures.\\n- Our ad-hoc architecture frequently mixes concerns such as having business logic mixed in with request handling logic, and has other warts from its evolution over time vs. being planned up front.\\n- New back-end services evolve differently resulting in more ad-hoc application architectures to learn.\\n- Shared components in `fxa-shared` can't rely on basic object lifecycles or setup approaches as they may be used in multiple different ad-hoc application architectures.\\nNot choosing an application framework means that we have choosen to make ad-hoc application architectures which will continue to exhibit the problems above.\\nIt is assumed that the four newest FxA back-end services (admin-server, support-panel, event-broker, gql-api) will be switched to the chosen approach for consistency.\\n## Decision Drivers\\n- Documented application architecture.\\n- Tooling that reduces boiler-plate and creates consistent code architecture.\\n- Modern paradigms to ease creation of global objects and their testability, such as DI.\\n- Training materials so that new developers can understand and work on application features easily.\\n- Ability to migrate legacy applications to similar conventions\/setup.\\n- Handles situations we need such as:\\n- Exception handling\\n- Validation\\n- Service instantiation (DI, etc)\\n- Authentication\\n- RESTful API's (Swagger\/OpenAPI a bonus)\\n- GraphQL\\n- Use of components\/services easily in a script\\n\n\n##Decision\n- Documented application architecture.\\n- Tooling that reduces boiler-plate and creates consistent code architecture.\\n- Modern paradigms to ease creation of global objects and their testability, such as DI.\\n- Training materials so that new developers can understand and work on application features easily.\\n- Ability to migrate legacy applications to similar conventions\/setup.\\n- Handles situations we need such as:\\n- Exception handling\\n- Validation\\n- Service instantiation (DI, etc)\\n- Authentication\\n- RESTful API's (Swagger\/OpenAPI a bonus)\\n- GraphQL\\n- Use of components\/services easily in a script\\nChosen Option: Use an existing framework: NestJS\\nNestJS and LoopBack are the two most compelling options, however NestJS has substantially better GraphQL support and a much larger user-base. LoopBack has its user-base divided between two versions with substantial changes between them. We will use NestJS for new projects and update the newest services (admin-server, support-panel, event-broker, gql-api) to NestJS.\\nTo reduce documentation needs and address that auth-server will not be reasonable to migrate, we will backport code organization schemes and directory\/filename conventions from NestJS and use `typedi` as a DI system to ease configuration\/testing in auth-server and possibly other legacy packages. This will allow developers familiar with NestJS projects to still locate and easily work on FxA legacy packages.\\n### Positive Consequences\\n- Less documentation to create, as we only document differences for legacy services.\\n- NestJS GraphQL support matches existing typegraphql paradigm closely for minimal effort switch.\\n### Negative Consequences\\n- There will be some effort involved to migrate the 4 packages to NestJS.\\n- We will need to document the differences between NestJS and other legacy services.\\n","MatchedIds":[387,381,362,3470,1182],"Predicted":"## Decision\nUse the NestJS Framework for rebuilding the FxA back-end service stack, because:\n\n- **Documented Application Architecture**: NestJS provides comprehensive documentation that helps developers understand the framework's application architecture. This makes it easier for new developers to get up to speed without having to read through the codebase extensively.\n\n- **Tooling that Reduces Boilerplate**: NestJS comes with built-in tooling that automates boilerplate setup for components, services, modules, and other architectural elements, thus speeding up the development process and ensuring consistency across projects.\n\n- **Modern Paradigms (DI)**: NestJS is designed around modern JavaScript and TypeScript paradigms, including Dependency Injection (DI). This allows for clean, maintainable code and makes it easier to manage and test components.\n\n- **Training Materials**: The framework has a wealth of community and official training materials, including tutorials, courses, and documentation, which will be beneficial for onboarding new developers.\n\n- **Migration Capability**: While transitioning legacy applications is complex, NestJS allows for incremental migration due to its modular nature, making it easier to bring older services inline with new conventions.\n\n- **Exception Handling**: NestJS comes with a robust exception handling mechanism that can be easily extended and customized, ensuring that developers can manage error scenarios gracefully.\n\n- **Validation**: The framework supports powerful validation mechanisms out of the box, which helps maintain the integrity and correctness of data.\n\n- **Service Instantiation**: Through its DI capabilities, NestJS handles service instantiation smoothly, promoting a clean separation of concerns and single responsibility.\n\n- **Authentication**: NestJS offers various ways to implement authentication, including middleware and guards, as well as integration with popular authentication libraries.\n\n- **API Features**: NestJS provides first-class support for building RESTful APIs and GraphQL endpoints. The framework is designed to work well with Swagger\/OpenAPI specifications, and has built-in modules for seamless GraphQL integration.\n\n- **Component\/Service Usability**: NestJS services and components can be easily used in scripts, making it flexible for various use cases outside of traditional REST or GraphQL API setups.\n\nBy standardizing on NestJS for our back-end services, we aim to resolve the issues with our current ad-hoc application architectures, improve developer productivity, and create a more maintainable, scalable, and consistent service stack.","GenTime":"2024-08-20 00:07:43"}
{"File Name":"fundraising-application\/003_Validation.md","Context":"## Context\\nWhen the team for the re-write of the Fundraising formed in 2016, we discovered that team members had different approaches to do validation:\\n* Use an established library, like [Symfony Validation](https:\/\/symfony.com\/doc\/current\/validation.html).\\n* Write our own validation logic.\\nThe arguments in favor of writing our own logic were:\\n* We don't want to bind our domain layer to a concrete validation library implementation.\\n* The individual validations - checking for required fields in most cases - are so simple that using an external library would make the validation more complicated.\\n* We don't know the \"maintenance cycles\" of the library, either we need to constantly update or the library is not maintained properly.\\n* Every developer would have to learn the API of the external library.\\nAt the start of the project we did not know where we should put the validation logic:\\n* At the framework\/presentation layer, forcing us to create valid, fully formed domain objects as input for use cases.\\n* At the use case layer, making validation part of the use case.\\n","Decision":"For each use case we write a validator class that checks the `Request` value object of that use case. The validator class must ensure that the use case can create valid domain objects from the request object. The validator class uses simple `if` checks and no external framework.\\nWe return result data structures from validation classes. The result data structures that have some way of communicating to the framework layer what input caused the validation error. If necessary, one input can have more than one validation error.\\nValidation error names are language-independent unique strings in `snake_case`. When we need to translate those error codes, we put the translations in the file [`validations.js`](https:\/\/github.com\/wmde\/fundraising-frontend-content\/blob\/test\/i18n\/de_DE\/messages\/validations.json) in the [content repository](https:\/\/github.com\/wmde\/fundraising-frontend-content). We don't  map every error to a translation, we can write frontend layer code that summarizes the errors or maps them in a different way.\\n","tokens":225,"id":1532,"text":"## Context\\nWhen the team for the re-write of the Fundraising formed in 2016, we discovered that team members had different approaches to do validation:\\n* Use an established library, like [Symfony Validation](https:\/\/symfony.com\/doc\/current\/validation.html).\\n* Write our own validation logic.\\nThe arguments in favor of writing our own logic were:\\n* We don't want to bind our domain layer to a concrete validation library implementation.\\n* The individual validations - checking for required fields in most cases - are so simple that using an external library would make the validation more complicated.\\n* We don't know the \"maintenance cycles\" of the library, either we need to constantly update or the library is not maintained properly.\\n* Every developer would have to learn the API of the external library.\\nAt the start of the project we did not know where we should put the validation logic:\\n* At the framework\/presentation layer, forcing us to create valid, fully formed domain objects as input for use cases.\\n* At the use case layer, making validation part of the use case.\\n\n\n##Decision\nFor each use case we write a validator class that checks the `Request` value object of that use case. The validator class must ensure that the use case can create valid domain objects from the request object. The validator class uses simple `if` checks and no external framework.\\nWe return result data structures from validation classes. The result data structures that have some way of communicating to the framework layer what input caused the validation error. If necessary, one input can have more than one validation error.\\nValidation error names are language-independent unique strings in `snake_case`. When we need to translate those error codes, we put the translations in the file [`validations.js`](https:\/\/github.com\/wmde\/fundraising-frontend-content\/blob\/test\/i18n\/de_DE\/messages\/validations.json) in the [content repository](https:\/\/github.com\/wmde\/fundraising-frontend-content). We don't  map every error to a translation, we can write frontend layer code that summarizes the errors or maps them in a different way.\\n","MatchedIds":[2307,2502,3776,1031,3784],"Predicted":"## Decision\nWe will place validation logic at the framework\/presentation layer and write our own validation logic.\n\nBy taking this approach:\n- We avoid binding our domain layer to any specific validation library, keeping our domain layer clean and independent.\n- We handle the simplicity of our validation requirements internally, avoiding unnecessary complexity.\n- We maintain control over the maintenance and update cycles, eliminating reliance on external library updates.\n- Our developers won't need to learn the API of an external library, which simplifies development and onboarding.\n\nPlacing the validation at the framework\/presentation layer ensures that only valid and fully formed domain objects are passed as inputs to use cases, promoting consistency and reliability in data handling.","GenTime":"2024-08-20 00:07:47"}
{"File Name":"oasis-core\/0008-standard-account-key-generation.md","Context":"## Context\\nCurrently, each application interacting with the [Oasis Network] defines its own\\nmethod of generating an account's private\/public key pair.\\n[Account]'s public key is in turn used to derive the account's address of the\\nform `oasis1 ... 40 characters ...` which is used to for a variety of operations\\n(i.e. token transfers, delegations\/undelegations, ...) on the network.\\nThe blockchain ecosystem has developed many standards for generating keys which\\nimprove key storage and interoperability between different applications.\\nAdopting these standards will allow the Oasis ecosystem to:\\n- Make key derivation the same across different applications (i.e. wallets).\\n- Allow users to hold keys in hardware wallets.\\n- Allow users to hold keys in cold storage more reliably (i.e. using the\\nfamiliar 24 word mnemonics).\\n- Define how users can generate multiple keys from a single seed (i.e.\\nthe 24 or 12 word mnemonic).\\n","Decision":"### Mnemonic Codes for Master Key Derivation\\nWe use Bitcoin's [BIP-0039]: _Mnemonic code for generating deterministic keys_\\nto derivate a binary seed from a mnemonic code.\\nThe binary seed is in turn used to derive the _master key_, the root key from\\nwhich a hierarchy of deterministic keys is derived, as described in\\n[Hierarchical Key Derivation Scheme][hd-scheme].\\nWe strongly recommend using 24 word mnemonics which correspond to 256 bits of\\nentropy.\\n### Hierarchical Key Derivation Scheme\\nWe use Sathoshi Labs' [SLIP-0010]: _Universal private key derivation from master\\nprivate key_, which is a superset of\\nBitcoin's [BIP-0032]: _Hierarchical Deterministic Wallets_ derivation algorithm,\\nextended to work on other curves.\\nAccount keys use the [edwards25519 curve] from the Ed25519 signature scheme\\nspecified in [RFC 8032].\\n### Key Derivation Paths\\nWe adapt [BIP-0044]: _Multi-Account Hierarchy for Deterministic Wallets_ for\\ngenerating deterministic keys where `coin_type` equals 474, as assigned to the\\nOasis Network by [SLIP-0044].\\nThe following [BIP-0032] path should be used to generate keys:\\n```\\nm\/44'\/474'\/x'\\n```\\nwhere `x` represents the key number.\\nNote that all path levels are _hardened_, e.g. `44'` is `44 | 0x8000000` or\\n`44 + 2^31`.\\nThe key corresponding to key number 0 (i.e. `m\/44'\/474'\/0'`) is called the\\n_primary key_.\\nThe account corresponding to the _primary key_ is called the _primary account_.\\nApplications (i.e. wallets) should use this account as a user's default Oasis\\naccount.\\n","tokens":208,"id":4346,"text":"## Context\\nCurrently, each application interacting with the [Oasis Network] defines its own\\nmethod of generating an account's private\/public key pair.\\n[Account]'s public key is in turn used to derive the account's address of the\\nform `oasis1 ... 40 characters ...` which is used to for a variety of operations\\n(i.e. token transfers, delegations\/undelegations, ...) on the network.\\nThe blockchain ecosystem has developed many standards for generating keys which\\nimprove key storage and interoperability between different applications.\\nAdopting these standards will allow the Oasis ecosystem to:\\n- Make key derivation the same across different applications (i.e. wallets).\\n- Allow users to hold keys in hardware wallets.\\n- Allow users to hold keys in cold storage more reliably (i.e. using the\\nfamiliar 24 word mnemonics).\\n- Define how users can generate multiple keys from a single seed (i.e.\\nthe 24 or 12 word mnemonic).\\n\n\n##Decision\n### Mnemonic Codes for Master Key Derivation\\nWe use Bitcoin's [BIP-0039]: _Mnemonic code for generating deterministic keys_\\nto derivate a binary seed from a mnemonic code.\\nThe binary seed is in turn used to derive the _master key_, the root key from\\nwhich a hierarchy of deterministic keys is derived, as described in\\n[Hierarchical Key Derivation Scheme][hd-scheme].\\nWe strongly recommend using 24 word mnemonics which correspond to 256 bits of\\nentropy.\\n### Hierarchical Key Derivation Scheme\\nWe use Sathoshi Labs' [SLIP-0010]: _Universal private key derivation from master\\nprivate key_, which is a superset of\\nBitcoin's [BIP-0032]: _Hierarchical Deterministic Wallets_ derivation algorithm,\\nextended to work on other curves.\\nAccount keys use the [edwards25519 curve] from the Ed25519 signature scheme\\nspecified in [RFC 8032].\\n### Key Derivation Paths\\nWe adapt [BIP-0044]: _Multi-Account Hierarchy for Deterministic Wallets_ for\\ngenerating deterministic keys where `coin_type` equals 474, as assigned to the\\nOasis Network by [SLIP-0044].\\nThe following [BIP-0032] path should be used to generate keys:\\n```\\nm\/44'\/474'\/x'\\n```\\nwhere `x` represents the key number.\\nNote that all path levels are _hardened_, e.g. `44'` is `44 | 0x8000000` or\\n`44 + 2^31`.\\nThe key corresponding to key number 0 (i.e. `m\/44'\/474'\/0'`) is called the\\n_primary key_.\\nThe account corresponding to the _primary key_ is called the _primary account_.\\nApplications (i.e. wallets) should use this account as a user's default Oasis\\naccount.\\n","MatchedIds":[13,2256,1962,4623,839],"Predicted":"## Decision\nWe will adopt the BIP-32, BIP-39, and BIP-44 standards for key generation and derivation across the entire Oasis ecosystem.\n\n### Details\n1. **BIP-39**: This standard will be used for mnemonic code for generating deterministic keys. Users will generate a 12 or 24-word mnemonic code which can be easily written down, stored, and backed up. This will standardize the process of mnemonic phrase generation across different applications in the Oasis ecosystem.\n\n2. **BIP-32**: This standard will be used for Hierarchical Deterministic (HD) keys. It allows for the generation of a tree of key pairs from a single seed, improving the manageability and security of multiple keys generated from a single mnemonic.\n\n3. **BIP-44**: This standard will outline the multi-account hierarchy for HD wallets. It will define how to generate multiple accounts, allowing users to segregate different types of accounts or applications while using a single seed.\n\n### Justification\n1. **Uniform Key Derivation**: By adopting these standards, we will ensure a uniform method for deriving keys across all applications interacting with the Oasis Network, promoting interoperability and consistency.\n\n2. **Hardware Wallet Compatibility**: These standards are widely supported by hardware wallets, allowing users to securely store their keys offline.\n\n3. **Cold Storage**: Mnemonics make it easy for users to store their keys securely in cold storage. The adoption of the widely used 12 or 24-word mnemonic phrases will enhance usability and security.\n\n4. **Multi-Key Management**: With BIP-32 and BIP-44, users can generate multiple keys from a single seed, making it easier to manage and segregate keys for different purposes, enhancing both security and convenience.\n\n### Implementation Steps\n1. **Integration in the SDK**: Update the Oasis Network SDK to support BIP-32, BIP-39, and BIP-44.\n2. **Wallets and Tools**: Collaborate with wallet providers to ensure they support these standards.\n3. **Educational Materials**: Create documentation and guides to help users transition to and understand these new standards.\n4. **Testing and Validation**: Thoroughly test the new key derivation methods for security and reliability before deployment.","GenTime":"2024-08-20 00:08:00"}
{"File Name":"tech-radar\/adr-4-structure_of_principle.md","Context":"# Context\\nWe need consistency in terms of format and structure for our patterns across the customer facing, integration and other architectures.\\nWe are also keen to link back to business strategy directly so we can isolate points for consideration in design, implementsation and assessment.\\n# Decision\\nWe propose the following struture for principle artefacts:\\n* Context\\n* Where possible link to URI's for business strategy identifying bullet points the principle is designed to promote\\n* Problems\\n* Rationale\\n* Examples\\n* Discussion Points\\n* Case by case topics (e.g. If service uses legacy APIs, contact TRAP: add TRAP\/Arc Triage email hyperlink)\\n# Status\\nProposed\\n# Consequences\\nStakeholder groups will need to re-factor the format and structure of pattern material in a later editing phase. The first phase is to get the material into the repository.\\n# Example\\n* Independently deployable services\\n* Context\\n* (Link to URI for business strategy)\\n(see Martin Fowler) [http:\/\/martinfowler.com\/articles\/microservices.html#ComponentizationViaServices]\\n* Problems\\n* Respond to change quickly\\n* Availability\\n* Scalability\\n* Rationale\\nThis could otherwise be stated as \u201cloose coupling and high cohesion\u201d. When changes must be made, they should be achieved through independently deployable services. Loose coupling means that a change in one service does not require a change in any other service. High cohesion means that related changes are typically made in a single location.\\nThis allows teams to deliver at their own speed, without being constrained by any of their consumers. Failing to do this correctly would limit our ability to deliver quickly and incrementally.\\n* Examples\\n(TODO: MDTP Link explaining)\\n* Discussion Points\\nNot Applicable\\nTODO: Given scenario X, contact TRAP (email hyperlink).\\n","Decision":"We propose the following struture for principle artefacts:\\n* Context\\n* Where possible link to URI's for business strategy identifying bullet points the principle is designed to promote\\n* Problems\\n* Rationale\\n* Examples\\n* Discussion Points\\n* Case by case topics (e.g. If service uses legacy APIs, contact TRAP: add TRAP\/Arc Triage email hyperlink)\\n# Status\\nProposed\\n# Consequences\\nStakeholder groups will need to re-factor the format and structure of pattern material in a later editing phase. The first phase is to get the material into the repository.\\n# Example\\n* Independently deployable services\\n* Context\\n* (Link to URI for business strategy)\\n(see Martin Fowler) [http:\/\/martinfowler.com\/articles\/microservices.html#ComponentizationViaServices]\\n* Problems\\n* Respond to change quickly\\n* Availability\\n* Scalability\\n* Rationale\\nThis could otherwise be stated as \u201cloose coupling and high cohesion\u201d. When changes must be made, they should be achieved through independently deployable services. Loose coupling means that a change in one service does not require a change in any other service. High cohesion means that related changes are typically made in a single location.\\nThis allows teams to deliver at their own speed, without being constrained by any of their consumers. Failing to do this correctly would limit our ability to deliver quickly and incrementally.\\n* Examples\\n(TODO: MDTP Link explaining)\\n* Discussion Points\\nNot Applicable\\nTODO: Given scenario X, contact TRAP (email hyperlink).\\n","tokens":382,"id":3925,"text":"# Context\\nWe need consistency in terms of format and structure for our patterns across the customer facing, integration and other architectures.\\nWe are also keen to link back to business strategy directly so we can isolate points for consideration in design, implementsation and assessment.\\n# Decision\\nWe propose the following struture for principle artefacts:\\n* Context\\n* Where possible link to URI's for business strategy identifying bullet points the principle is designed to promote\\n* Problems\\n* Rationale\\n* Examples\\n* Discussion Points\\n* Case by case topics (e.g. If service uses legacy APIs, contact TRAP: add TRAP\/Arc Triage email hyperlink)\\n# Status\\nProposed\\n# Consequences\\nStakeholder groups will need to re-factor the format and structure of pattern material in a later editing phase. The first phase is to get the material into the repository.\\n# Example\\n* Independently deployable services\\n* Context\\n* (Link to URI for business strategy)\\n(see Martin Fowler) [http:\/\/martinfowler.com\/articles\/microservices.html#ComponentizationViaServices]\\n* Problems\\n* Respond to change quickly\\n* Availability\\n* Scalability\\n* Rationale\\nThis could otherwise be stated as \u201cloose coupling and high cohesion\u201d. When changes must be made, they should be achieved through independently deployable services. Loose coupling means that a change in one service does not require a change in any other service. High cohesion means that related changes are typically made in a single location.\\nThis allows teams to deliver at their own speed, without being constrained by any of their consumers. Failing to do this correctly would limit our ability to deliver quickly and incrementally.\\n* Examples\\n(TODO: MDTP Link explaining)\\n* Discussion Points\\nNot Applicable\\nTODO: Given scenario X, contact TRAP (email hyperlink).\\n\n\n##Decision\nWe propose the following struture for principle artefacts:\\n* Context\\n* Where possible link to URI's for business strategy identifying bullet points the principle is designed to promote\\n* Problems\\n* Rationale\\n* Examples\\n* Discussion Points\\n* Case by case topics (e.g. If service uses legacy APIs, contact TRAP: add TRAP\/Arc Triage email hyperlink)\\n# Status\\nProposed\\n# Consequences\\nStakeholder groups will need to re-factor the format and structure of pattern material in a later editing phase. The first phase is to get the material into the repository.\\n# Example\\n* Independently deployable services\\n* Context\\n* (Link to URI for business strategy)\\n(see Martin Fowler) [http:\/\/martinfowler.com\/articles\/microservices.html#ComponentizationViaServices]\\n* Problems\\n* Respond to change quickly\\n* Availability\\n* Scalability\\n* Rationale\\nThis could otherwise be stated as \u201cloose coupling and high cohesion\u201d. When changes must be made, they should be achieved through independently deployable services. Loose coupling means that a change in one service does not require a change in any other service. High cohesion means that related changes are typically made in a single location.\\nThis allows teams to deliver at their own speed, without being constrained by any of their consumers. Failing to do this correctly would limit our ability to deliver quickly and incrementally.\\n* Examples\\n(TODO: MDTP Link explaining)\\n* Discussion Points\\nNot Applicable\\nTODO: Given scenario X, contact TRAP (email hyperlink).\\n","MatchedIds":[4896,302,3483,4620,3488],"Predicted":"# Context\nWe need consistency in terms of format and structure for our patterns across customer facing, integration, and other architectures. We also aim to link back to business strategy directly, so we can isolate points for consideration during design, implementation, and assessment.\n\n# Decision\nWe propose the following structure for principal artifacts:\n- **Context**\n- **Business Strategy Links:** Where possible link to URIs for business strategy, identifying bullet points the principle is designed to promote.\n- **Problems**\n- **Rationale**\n- **Examples**\n- **Discussion Points**\n- **Case by Case Topics:** E.g., If a service uses legacy APIs, contact TRAP: add TRAP\/Arc Triage email hyperlink.\n\n# Status\nProposed\n\n# Consequences\nStakeholder groups will need to re-factor the format and structure of pattern material in a later editing phase. The first phase is to get the material into the repository.\n\n# Example\n**Independently Deployable Services**\n- **Context**\n- **Business Strategy Links:** (Link to URI for business strategy) (see Martin Fowler) [http:\/\/martinfowler.com\/articles\/microservices.html#ComponentizationViaServices]\n- **Problems:**\n  - Respond to change quickly\n  - Availability\n  - Scalability\n- **Rationale:**\n  This could otherwise be stated as \u201cloose coupling and high cohesion.\u201d When changes must be made, they should be achieved through independently deployable services. Loose coupling means that a change in one service does not require a change in any other service. High cohesion means that related changes are typically made in a single location. This allows teams to deliver at their own speed without being constrained by any of their consumers. Failing to do this correctly would limit our ability to deliver quickly and incrementally.\n- **Examples:**\n  (TODO: MDTP Link explaining)\n- **Discussion Points:**\n  Not Applicable\n  TODO: Given scenario X, contact TRAP (email hyperlink).","GenTime":"2024-08-20 00:08:07"}
{"File Name":"paas-team-manual\/ADR010-postgres-bind-behaviour.html.md","Context":"## Context\\nWe use RDS for tenant databases. Tenants can bind more than one application to a database instance created via the services console. Database migrations were broken when a binding was removed and re-added, or when another bound application other than the one that created the database tables attempted to modify them.\\nPreviously the RDS broker PostgreSQL engine copied the MySQL engine and granted all rights on the database to the newly created user. In PostgreSQL this will give the user rights to create tables, but because it has a more finely-grained permission model than MySQL this does not give the user rights on existing tables, or tables newly created by other users.\\nOnly the owner of a table can alter\/drop it, and you cannot grant this permission to other users. Users who are the owners of tables cannot be removed until the table ownership is changed.\\nWe attempted to work around the PostgreSQL permissions system in the following ways:\\n* Using [`ALTER DEFAULT PRIVILEGES`](https:\/\/www.postgresql.org\/docs\/9.5\/static\/sql-alterdefaultprivileges.html) on every newly created user to `GRANT ALL ON PUBLIC` - this means that every user can `SELECT`, `INSERT`, and `DELETE`, but because only the table owner can `ALTER` or `DROP` this will not allow other bound users to run migrations. This is also limited to the `PUBLIC` (default) schema, so would fail to work for any applications that have custom schemas.\\n* Making the group the owner of the `PUBLIC` schema. This allowed members of the group to `DROP` tables within the schema, but still did not allow them to `ALTER` these tables.\\n* Creating a \"parent\" group that is given ownership of all tables, sequences, and functions. New users are granted rights on this group and thus can carry out all owner actions on group-owned items. A [DDL event trigger](https:\/\/www.postgresql.org\/docs\/9.5\/static\/event-trigger-definition.html) using the built-in `REASSIGN OWNED BY` command handed over ownership of all resources owned by the current user to the parent `$DBNAME_owner` group. This worked perfectly on all the PostgreSQL databases we tested with, however RDS does not allow use of event triggers so it was unworkable in production.\\n","Decision":"We decided the simplest solution was to issue the same credentials to all applications bound to the database. This means that the same user will be used by each application, and they will not suffer ownership problems when issuing `DROP` or `ALTER` commands.\\nRather than deterministically deriving the credentials we decided to store them in the backing database itself meaning they can be changed at any time in case of a breach. To protect against database breaches that may leak stored credentials we also decided to encrypt the credentials using a symmetric key held by the broker.\\nWe accepted that in the future we may wish to extend this by allowing the broker to issue read-only credentials, or credentials that are suitable for `SELECT`, `INSERT`, and `DELETE` operations, but not schema modifications.\\n","tokens":470,"id":224,"text":"## Context\\nWe use RDS for tenant databases. Tenants can bind more than one application to a database instance created via the services console. Database migrations were broken when a binding was removed and re-added, or when another bound application other than the one that created the database tables attempted to modify them.\\nPreviously the RDS broker PostgreSQL engine copied the MySQL engine and granted all rights on the database to the newly created user. In PostgreSQL this will give the user rights to create tables, but because it has a more finely-grained permission model than MySQL this does not give the user rights on existing tables, or tables newly created by other users.\\nOnly the owner of a table can alter\/drop it, and you cannot grant this permission to other users. Users who are the owners of tables cannot be removed until the table ownership is changed.\\nWe attempted to work around the PostgreSQL permissions system in the following ways:\\n* Using [`ALTER DEFAULT PRIVILEGES`](https:\/\/www.postgresql.org\/docs\/9.5\/static\/sql-alterdefaultprivileges.html) on every newly created user to `GRANT ALL ON PUBLIC` - this means that every user can `SELECT`, `INSERT`, and `DELETE`, but because only the table owner can `ALTER` or `DROP` this will not allow other bound users to run migrations. This is also limited to the `PUBLIC` (default) schema, so would fail to work for any applications that have custom schemas.\\n* Making the group the owner of the `PUBLIC` schema. This allowed members of the group to `DROP` tables within the schema, but still did not allow them to `ALTER` these tables.\\n* Creating a \"parent\" group that is given ownership of all tables, sequences, and functions. New users are granted rights on this group and thus can carry out all owner actions on group-owned items. A [DDL event trigger](https:\/\/www.postgresql.org\/docs\/9.5\/static\/event-trigger-definition.html) using the built-in `REASSIGN OWNED BY` command handed over ownership of all resources owned by the current user to the parent `$DBNAME_owner` group. This worked perfectly on all the PostgreSQL databases we tested with, however RDS does not allow use of event triggers so it was unworkable in production.\\n\n\n##Decision\nWe decided the simplest solution was to issue the same credentials to all applications bound to the database. This means that the same user will be used by each application, and they will not suffer ownership problems when issuing `DROP` or `ALTER` commands.\\nRather than deterministically deriving the credentials we decided to store them in the backing database itself meaning they can be changed at any time in case of a breach. To protect against database breaches that may leak stored credentials we also decided to encrypt the credentials using a symmetric key held by the broker.\\nWe accepted that in the future we may wish to extend this by allowing the broker to issue read-only credentials, or credentials that are suitable for `SELECT`, `INSERT`, and `DELETE` operations, but not schema modifications.\\n","MatchedIds":[4061,4285,317,207,1740],"Predicted":"## Decision\nTo address the PostgreSQL permissions issue in a way that is compatible with RDS and to ensure that database migrations can be effectively handled across multiple bindings, we will implement the following approach:\n\n1. **Creation of a Master User and Role:**\n   - Create a master user and a corresponding role `$DBNAME_admin` for each tenant database. This master user will have comprehensive permissions including the ability to grant ownership rights.\n\n2. **Ownership Transfer Using Functions:**\n   - Introduce a set of PostgreSQL functions that can be run by the master user to transfer ownership of newly created tables, sequences, and functions to a predefined group `$DBNAME_team`. This will ensure that all bound users within the team have the necessary ownership rights to make schema changes.\n\n3. **Automated Ownership Assignment:**\n   - Whenever a new table, sequence, or function is created, an automated process (e.g., a scheduled job or trigger-like mechanism) will use the master user to reassign ownership of these objects to the `$DBNAME_team` group immediately. This ensures that the necessary permissions are kept up to date.\n\n4. **Granting Access to Bound Users:**\n   - All new users created as part of application bindings will be added to the `$DBNAME_team` group, granting them the necessary rights to alter or drop objects within the database. They will also have `GRANT ALL` privileges for `SELECT`, `INSERT`, `UPDATE`, and `DELETE` by default.\n\n5. **Documentation and Guidelines:**\n   - Clearly document the process for adding new bound users and managing table ownership. Provide guidelines for developers on how to ensure their database migration scripts are compliant with this model to avoid migration breaks.\n\nBy implementing this solution, we can leverage PostgreSQL's advanced permission model effectively while ensuring that all necessary operations for database migrations and schema changes can be carried out by any bound application user. This approach avoids the need for unsupported event triggers in RDS and secures a consistent, replicable process for managing tenant databases and their permissions.","GenTime":"2024-08-20 00:08:18"}
{"File Name":"james\/0015-objectstorage-blobid-list.md","Context":"## Context\\nA given mail is often written to the blob store by different components. And mail traffic is heavily duplicated (several recipients receiving similar email, same attachments). This causes a given blob to often be persisted several times.\\nCassandra was the first implementation of the blobStore. Cassandra is a heavily write optimized NoSQL database. One can assume writes to be fast on top of Cassandra. Thus we assumed we could always overwrite blobs.\\nThis usage pattern was also adopted for BlobStore on top of ObjectStorage.\\nHowever writing in Object storage:\\n- Takes time\\n- Is billed by most cloud providers\\nThus choosing a right strategy to avoid writing blob twice is desirable.\\nHowever, ObjectStorage (OpenStack Swift) `exist` method was not efficient enough to be a real cost and performance saver.\\n","Decision":"Rely on a StoredBlobIdsList API to know which blob is persisted or not in object storage. Provide a Cassandra implementation of it.\\nLocated in blob-api for convenience, this it not a top level API. It is intended to be used by some blobStore implementations\\n(here only ObjectStorage). We will provide a CassandraStoredBlobIdsList in blob-cassandra project so that guice products combining\\nobject storage and Cassandra can define a binding to it.\\n- When saving a blob with precomputed blobId, we can check the existence of the blob in storage, avoiding possibly the expensive \"save\".\\n- When saving a blob too big to precompute its blobId, once the blob had been streamed using a temporary random blobId, copy operation can be avoided and the temporary blob could be directly removed.\\nCassandra is probably faster doing \"write every time\" rather than \"read before write\" so we should not use the stored blob projection for it\\nSome performance tests will be run in order to evaluate the improvements.\\n","tokens":166,"id":2122,"text":"## Context\\nA given mail is often written to the blob store by different components. And mail traffic is heavily duplicated (several recipients receiving similar email, same attachments). This causes a given blob to often be persisted several times.\\nCassandra was the first implementation of the blobStore. Cassandra is a heavily write optimized NoSQL database. One can assume writes to be fast on top of Cassandra. Thus we assumed we could always overwrite blobs.\\nThis usage pattern was also adopted for BlobStore on top of ObjectStorage.\\nHowever writing in Object storage:\\n- Takes time\\n- Is billed by most cloud providers\\nThus choosing a right strategy to avoid writing blob twice is desirable.\\nHowever, ObjectStorage (OpenStack Swift) `exist` method was not efficient enough to be a real cost and performance saver.\\n\n\n##Decision\nRely on a StoredBlobIdsList API to know which blob is persisted or not in object storage. Provide a Cassandra implementation of it.\\nLocated in blob-api for convenience, this it not a top level API. It is intended to be used by some blobStore implementations\\n(here only ObjectStorage). We will provide a CassandraStoredBlobIdsList in blob-cassandra project so that guice products combining\\nobject storage and Cassandra can define a binding to it.\\n- When saving a blob with precomputed blobId, we can check the existence of the blob in storage, avoiding possibly the expensive \"save\".\\n- When saving a blob too big to precompute its blobId, once the blob had been streamed using a temporary random blobId, copy operation can be avoided and the temporary blob could be directly removed.\\nCassandra is probably faster doing \"write every time\" rather than \"read before write\" so we should not use the stored blob projection for it\\nSome performance tests will be run in order to evaluate the improvements.\\n","MatchedIds":[2896,2145,2897,2880,3831],"Predicted":"## Decision\nImplement a deduplication mechanism to avoid writing duplicate blobs to the object storage.\n\n1. **Blob Identification:** Use a hash function (e.g., SHA-256) to generate a unique identifier (hash) for each blob based on its content. This will help in identifying if the blob already exists.\n\n2. **Metadata Store:** Maintain a metadata store in Cassandra mapping the hash to the blob metadata (e.g., storage location, size, etc.). This allows fast lookups to check if a blob already exists before performing a write.\n\n3. **Write Process:**\n    - When a new blob is to be written, first compute its hash.\n    - Check the metadata store in Cassandra to see if a blob with this hash already exists.\n    - If the blob exists, update the reference count or simply use the existing blob's metadata.\n    - If the blob does not exist, write it to the object storage and update the metadata store with the new hash and blob metadata.\n\n4. **Read Process:** When reading a blob, use the hash to fetch the metadata from Cassandra and then retrieve the blob from the object storage based on the metadata.\n\n5. **Garbage Collection:** Implement a periodic garbage collection mechanism to remove blobs that are no longer referenced. This requires maintaining reference counts in the metadata store for each blob.\n\nThis strategy minimizes redundant writes to the object storage, reduces costs, and improves performance by leveraging the efficient lookup capabilities of Cassandra.","GenTime":"2024-08-20 00:08:25"}
{"File Name":"qc-atlas\/0002-monorepo.md","Context":"## Context and Problem Statement\\nShould the components in the PlanQK platform be splitted into individual repos?\\n","Decision":"Start with a monorepo, split up later if needed. Let FOCUS decide what is best for their semantic knowledge graph component.\\n### Positive Consequences <!-- optional -->\\n* Recommended approach by [Martin Fowler](https:\/\/martinfowler.com\/bliki\/MonolithFirst.html)\\n","tokens":22,"id":697,"text":"## Context and Problem Statement\\nShould the components in the PlanQK platform be splitted into individual repos?\\n\n\n##Decision\nStart with a monorepo, split up later if needed. Let FOCUS decide what is best for their semantic knowledge graph component.\\n### Positive Consequences <!-- optional -->\\n* Recommended approach by [Martin Fowler](https:\/\/martinfowler.com\/bliki\/MonolithFirst.html)\\n","MatchedIds":[36,1631,4609,1117,3483],"Predicted":"## Decision\nWe choose to split the components in the PlanQK platform into individual repositories.\n\n### Decision Drivers\n- Allow for independent development and deployment cycles.\n- Facilitate better maintainability and scalability.\n- Enable more granular access control and security measures.\n- Enhance clarity around ownership and responsibility of each component.\n\n### Positive Consequences\n1. **Independent Development:**\n   - Teams can work on different components without interfering with each other\u2019s progress.\n   - This would allow for faster iterations and quicker releases.\n\n2. **Better Maintainability:**\n   - Each repository can have its own set of dependencies and configurations, making it easier to manage.\n   - Bug fixes and updates can be targeted more effectively.\n\n3. **Scalability:**\n   - The platform can grow more organically, with new components added as required without affecting existing ones.\n   - Load can be distributed more evenly across different repositories and services.\n\n4. **Security and Access Control:**\n   - Fine-grained control over who has access to which components.\n   - Reduced risk as a breach in one repository doesn\u2019t affect the entire codebase.\n\n5. **Clear Ownership:**\n   - Each team can be clearly responsible for specific components, improving accountability and focus.\n\n### Negative Consequences\n1. **Increased Overhead:**\n   - More repositories mean more management and administrative overhead.\n   - Possible challenges with synchronizing dependencies and versions across components.\n\n2. **Complex CI\/CD:**\n   - Continuous Integration and Continuous Deployment pipelines may become more complex to manage.\n   - Requires more sophisticated tooling and monitoring to ensure seamless integration of all components.\n\n3. **Cross-Component Changes:**\n   - Refactoring or making changes that span across multiple components might require more coordination and effort.\n   - It could lead to slower implementation for cross-component feature updates.\n\n4. **Initial Ramp-Up:**\n   - There may be an initial learning curve and adjustment period for teams.\n   - Requires setting up new processes and workflows for effective multi-repo management.\n\nBy splitting the components into individual repositories, we strike a balance between maintaining a flexible, secure, and scalable platform while being mindful of the operational complexities introduced by this approach.","GenTime":"2024-08-20 00:08:34"}
{"File Name":"ELEN4010\/ADR Login API Choice.md","Context":"## Context\\nIt would be convenient to use the Google Login API as an alternative method for users to login. This would provide a template for our own login details stored in the DB, as well as a quick way to get the Sprint 1 User story related to login completed ASAP.\\n","Decision":"Using a well known and widely known\/supported login mechanism such as Google's OAuth2 will allow more rapid development of an appropriate security setup for the site. We will apply for an API key and start implementing the login\/registration page through the Google Login API\\n","tokens":58,"id":61,"text":"## Context\\nIt would be convenient to use the Google Login API as an alternative method for users to login. This would provide a template for our own login details stored in the DB, as well as a quick way to get the Sprint 1 User story related to login completed ASAP.\\n\n\n##Decision\nUsing a well known and widely known\/supported login mechanism such as Google's OAuth2 will allow more rapid development of an appropriate security setup for the site. We will apply for an API key and start implementing the login\/registration page through the Google Login API\\n","MatchedIds":[2041,2405,4443,577,5211],"Predicted":"## Decision\nWe will implement the Google Login API for user authentication temporarily. This will allow users to log in quickly while we focus on developing other critical features for Sprint 1. Using the Google Login API will serve as a template for storing our own login details in the database later. We acknowledge that this is a temporary solution and will migrate to our own authentication system once it is fully developed. This approach enables us to meet immediate user story requirements and ensures a faster development cycle.","GenTime":"2024-08-20 00:08:37"}
{"File Name":"titania-os\/0005-networkmanager-for-networking.md","Context":"## Context\\nSystemD provides native tools for managing the network connections. NetworkManager is an alternative service that provides more features and can be controlled and queried with a DBus interface. Wicd is another alternative, but is rarely used in embedded applications. There are several other options, but they are not feature full compared to NetworkManager.\\n","Decision":"We will disable systemd network capability and use NetworkManager for managing the networks (duh!).\\n","tokens":69,"id":2574,"text":"## Context\\nSystemD provides native tools for managing the network connections. NetworkManager is an alternative service that provides more features and can be controlled and queried with a DBus interface. Wicd is another alternative, but is rarely used in embedded applications. There are several other options, but they are not feature full compared to NetworkManager.\\n\n\n##Decision\nWe will disable systemd network capability and use NetworkManager for managing the networks (duh!).\\n","MatchedIds":[3150,4974,2236,1647,142],"Predicted":"## Decision\nWe will adopt NetworkManager as our primary tool for managing network connections. NetworkManager offers a comprehensive feature set and can be controlled and queried using a DBus interface, providing more flexibility and functionality compared to SystemD's native tools. Wicd and other alternatives are not considered due to their lack of widespread use and limited feature sets in embedded applications. \n\n### Positive Consequences\n* Enhanced network management capabilities\n* Flexibility in control and querying through the DBus interface\n* Broad support and documentation\n\n### Negative Consequences\n* Additional overhead of integrating NetworkManager into the existing system\n* Potential learning curve for team members familiar only with SystemD's native tools","GenTime":"2024-08-20 00:08:42"}
{"File Name":"super-eks\/0007-backup.md","Context":"## Context and Problem Statement\\nIn the current setup of super eks there is no backup solution installed and configured. A backup solution can be helpful for some resources that are not managed by IaC.\\n","Decision":"### Positive Consequences\\nUsers get a state of the art backup technology included with super eks, enabling them to restore a cluster after a\\ncatastropic failure.\\n### Negative Consequences\\nUsers have to think about how the backup needs to be setup and distinguish between resources that have been set up by\\nIaC and manually set up resources (e.g., secrets).\\n","tokens":41,"id":2034,"text":"## Context and Problem Statement\\nIn the current setup of super eks there is no backup solution installed and configured. A backup solution can be helpful for some resources that are not managed by IaC.\\n\n\n##Decision\n### Positive Consequences\\nUsers get a state of the art backup technology included with super eks, enabling them to restore a cluster after a\\ncatastropic failure.\\n### Negative Consequences\\nUsers have to think about how the backup needs to be setup and distinguish between resources that have been set up by\\nIaC and manually set up resources (e.g., secrets).\\n","MatchedIds":[1464,4025,1372,2539,2471],"Predicted":"## Decision\nImplement a backup solution using AWS Backup. AWS Backup will be configured to create and manage backups for resources that are not currently managed by Infrastructure as Code (IaC). This includes, but is not limited to, EKS cluster configurations, persistent storage (EBS volumes), and Amazon RDS instances.\n\nSteps to implement the solution:\n1. Create a backup plan in AWS Backup.\n2. Add backup rules to define the frequency and retention period for backups.\n3. Assign AWS resources (EKS clusters, EBS volumes, RDS instances) to the backup plan.\n4. Enable backup monitoring and alerting for backup job success or failure.\n5. Periodically review and update the backup plan to include new resources as required.\n\nThis decision ensures that critical data and configurations are protected and recoverable in case of accidental deletion or data corruption.","GenTime":"2024-08-20 00:08:46"}
{"File Name":"openchs-adr\/0006-use-row-level-security-in-postgres-to-implement-multitenancy.md","Context":"## Context\\nMultitenancy is required because we are now deploying openchs-server on the cloud. This can be achieved by\\n1. Multiple databases\\n2. Multiple schemas\\n3. Same database with a discriminator column\\n### Constraints\\n1. An organisation should not be able to view or update another organisation's data\\n2. There is common metadata for forms, concepts that organisations can override\\n### Other concerns\/requirements\\n1. The architecture should prevent mistakes in code to leak data from one organisation to another\\n2. Maintaining multiple copies of metadata (if using multiple dbs\/schemas) is hard\\n3. Data per organisation is low\\n","Decision":"- Use discriminators to achieve multitenancy.\\n- Enforce multitenancy using Postgres RLS policies\\n- Create a hierarchical organisation structure where organisations can read metadata from current or parent organisations, and read\/write data for their own organisation\\n- Use OpenCHS as the grandparent organisation. It will hold the default program metadata\\n","tokens":131,"id":2828,"text":"## Context\\nMultitenancy is required because we are now deploying openchs-server on the cloud. This can be achieved by\\n1. Multiple databases\\n2. Multiple schemas\\n3. Same database with a discriminator column\\n### Constraints\\n1. An organisation should not be able to view or update another organisation's data\\n2. There is common metadata for forms, concepts that organisations can override\\n### Other concerns\/requirements\\n1. The architecture should prevent mistakes in code to leak data from one organisation to another\\n2. Maintaining multiple copies of metadata (if using multiple dbs\/schemas) is hard\\n3. Data per organisation is low\\n\n\n##Decision\n- Use discriminators to achieve multitenancy.\\n- Enforce multitenancy using Postgres RLS policies\\n- Create a hierarchical organisation structure where organisations can read metadata from current or parent organisations, and read\/write data for their own organisation\\n- Use OpenCHS as the grandparent organisation. It will hold the default program metadata\\n","MatchedIds":[359,2824,3950,3889,1106],"Predicted":"## Decision\nWe will implement multitenancy using the **same database with a discriminator column**. This approach was chosen due to the following reasons:\n\n1. **Security and Isolation:** \n    * A discriminator column will ensure that data belonging to different organisations is easily identifiable and can be restricted via appropriate checks at the application level. \n    * By using a single database, we eliminate the need for multiple connections and reduce the risk of data leakage between organisations via unintended cross-database queries or schema access.\n\n2. **Common Metadata Management:**\n    * Since there is common metadata for forms and concepts that organisations can override, managing these entities in a single database simplifies the process. A single source for metadata ensures consistency and reduces the overhead of maintaining multiple copies of the same data in different databases or schemas.\n\n3. **Ease of Maintenance:**\n    * Maintaining multiple databases or schemas would require more complex management routines for backup, update, and scaling operations. The single database approach with a discriminator column is simpler to maintain and scale.\n    * Code changes to support multiple databases or schemas would increase the complexity of the application and increase the risk of mistakes that could lead to data leakage across organisations. Using a single database reduces this complexity.\n\n4. **Low Data Volume:**\n    * Given that data per organisation is low, a single database with a discriminator column is a more efficient use of resources.\n\n### Implementation Details:\n1. **Database Schema:**\n    * Each table holding organisation-specific data will have a discriminator column (e.g., `organization_id`) to segregate data.\n    * Appropriate indexes will be created on the discriminator column to ensure efficient query performance.\n\n2. **Application Layer:**\n    * All queries accessing organisation-specific data will include a mandatory filter based on the `organization_id`.\n    * Access control mechanisms will be implemented to ensure that the application layer checks the `organization_id` against the currently authenticated user\u2019s organisation before performing any read or write operations.\n    * Common metadata will be stored in shared tables, with an option for overrides which will reference the `organization_id` where applicable.\n\n3. **Security Measures:**\n    * Data access checks will be implemented in the data access layer to prevent accidental data leakage due to coding errors.\n    * Automated tests will be written to verify that data integrity and security checks are correctly enforced.\n\nThis approach ensures strong data isolation, ease of metadata management, and reduces maintenance overhead while efficiently supporting the requirement for multi-tenancy.","GenTime":"2024-08-20 00:08:55"}
{"File Name":"slingshot\/0003-use-one-geoserver-instance-with-multiple-workspaces.md","Context":"## Context\\nThere are a few problems that need to be solved around handling authentication with GeoServer:\\n1. We have some layers which can only be accessed by authenticated users and some that are publicly accessible.\\n2. The user never interacts directly with GeoServer. Requests to GeoServer for data come from the user's browser either through `img` tags or through AJAX requests from Leaflet. This means the user needs to be preauthenticated with GeoServer in some way.\\n3. GeoServer's authentication is incredibly difficult to work with.\\nIn the past, we solved these problems by leveraging the SSO capabilities of Shibboleth and simply running two GeoServer instances--one that was publicly accessible and one that was behind Shibboleth. Touchstone authentication happened for the user through the Rails application. Given the decision to move away from Shibboleth this is no longer an option. Running two GeoServer instances also adds additional burdens to deployment, management and the data publication process.\\n","Decision":"Run a single GeoServer instance. GeoServer supports what it calls workspaces, which is just a way of partitioning layers within the system. The authentication for each workspace can be configured separately. Configure one workspace to be readable by anyone and one workspace to require Basic Auth using a predefined username and password.\\nAccess to this GeoServer instance will need to be proxied. The proxy (our Geoblacklight instance) will handle SAML authentication and augment the proxy request to GeoServer with the Basic Auth login if the user has authenticated with Geoblacklight.\\n","tokens":200,"id":2578,"text":"## Context\\nThere are a few problems that need to be solved around handling authentication with GeoServer:\\n1. We have some layers which can only be accessed by authenticated users and some that are publicly accessible.\\n2. The user never interacts directly with GeoServer. Requests to GeoServer for data come from the user's browser either through `img` tags or through AJAX requests from Leaflet. This means the user needs to be preauthenticated with GeoServer in some way.\\n3. GeoServer's authentication is incredibly difficult to work with.\\nIn the past, we solved these problems by leveraging the SSO capabilities of Shibboleth and simply running two GeoServer instances--one that was publicly accessible and one that was behind Shibboleth. Touchstone authentication happened for the user through the Rails application. Given the decision to move away from Shibboleth this is no longer an option. Running two GeoServer instances also adds additional burdens to deployment, management and the data publication process.\\n\n\n##Decision\nRun a single GeoServer instance. GeoServer supports what it calls workspaces, which is just a way of partitioning layers within the system. The authentication for each workspace can be configured separately. Configure one workspace to be readable by anyone and one workspace to require Basic Auth using a predefined username and password.\\nAccess to this GeoServer instance will need to be proxied. The proxy (our Geoblacklight instance) will handle SAML authentication and augment the proxy request to GeoServer with the Basic Auth login if the user has authenticated with Geoblacklight.\\n","MatchedIds":[2582,527,2041,3214,5211],"Predicted":"## Decision\nWe will implement a token-based authentication mechanism using JWTs (JSON Web Tokens) for GeoServer. The Rails application will handle user authentication and issue a JWT that encodes user permissions. This JWT will be appended to requests made by the user's browser to GeoServer either through `img` tags or through AJAX requests from Leaflet.\n\n### Steps Involved:\n1. **User Authentication via Rails Application**: The user will authenticate with the Rails application using a standard user authentication mechanism (e.g., OAuth2, Devise).\n2. **JWT Issuance**: Upon successful authentication, the Rails application will generate a JWT that includes the user's identity and permissions.\n3. **Client-side Token Management**: The token will be stored securely on the client-side (considering security measures such as using `HttpOnly` and `Secure` flags on cookies, or local storage with proper precautions).\n4. **Request Modification**: The client's browser will append this JWT to each request (as a header or query parameter) made to GeoServer.\n5. **GeoServer JWT Validation**: Configure GeoServer to validate the JWT on incoming requests using a custom filter or a community module. If the JWT is valid and the user has the necessary permissions, the request will be processed. Otherwise, it will be rejected.\n\n### Advantages:\n- **Single Instance of GeoServer**: This approach eliminates the need for running multiple GeoServer instances, simplifying deployment and management.\n- **Flexible Access Control**: User permissions can be easily encoded in the JWT, providing fine-grained access control to various layers.\n- **Improved Security**: JWTs are stateless and tamper-proof when signed securely, reducing the risk associated with token-based authentication.\n- **Simplified Authentication Flow**: Users authenticate once with the Rails application, streamlining the user experience and reducing authentication complexity.\n\nThis decision leverages modern authentication practices and aligns well with our existing infrastructure, providing a scalable and secure solution for handling authentication with GeoServer.","GenTime":"2024-08-20 00:09:02"}
{"File Name":"adr-viewer\/0005-distinguish-amendments-to-records-with-colour.md","Context":"## Context\\nArchitecture Decision Records may be `amended` rather than `superseded` if e.g. only a small part of the decision changes.\\n","Decision":"Amended records, although not officially supported as a distinct flag in `adr-tools`, should be distinguished from records that are either Accepted or Superseded by.\\n","tokens":33,"id":3813,"text":"## Context\\nArchitecture Decision Records may be `amended` rather than `superseded` if e.g. only a small part of the decision changes.\\n\n\n##Decision\nAmended records, although not officially supported as a distinct flag in `adr-tools`, should be distinguished from records that are either Accepted or Superseded by.\\n","MatchedIds":[2261,1669,4361,4428,3119],"Predicted":"## Decision\nWe will allow Architecture Decision Records to be amended rather than superseded if only minor changes to the decision are required. This approach will make it easier to maintain accurate and up-to-date documentation while preserving the original decision's context and rationale.","GenTime":"2024-08-20 00:09:06"}
{"File Name":"generator-latex-template\/0005-custom-example-environment.md","Context":"## Context and Problem Statement\\nWe want to present LaTeX examples.\\nThey should be shown both as LaTeX source and as rendered output.\\n","Decision":"* Write once - have two outputs (source and rendered)\\n* Good interaction with all pacakges\\n* Should support floats\\n* Should support compilation without special configuration (such as `--shell-escape`)\\nChosen option: \"custom solution\", because resolves all forces.\\n### Positive Consequences\\n* Works with floats and umlauts\\n### Negative Consequences\\n* A little bit more than minimal code is presented to the user\\n","tokens":28,"id":2337,"text":"## Context and Problem Statement\\nWe want to present LaTeX examples.\\nThey should be shown both as LaTeX source and as rendered output.\\n\n\n##Decision\n* Write once - have two outputs (source and rendered)\\n* Good interaction with all pacakges\\n* Should support floats\\n* Should support compilation without special configuration (such as `--shell-escape`)\\nChosen option: \"custom solution\", because resolves all forces.\\n### Positive Consequences\\n* Works with floats and umlauts\\n### Negative Consequences\\n* A little bit more than minimal code is presented to the user\\n","MatchedIds":[2616,5059,160,287,5172],"Predicted":"## Decision\nWe will implement a system that displays LaTeX examples in two sections: one for the LaTeX source code and another for the rendered output. This can be achieved by leveraging a LaTeX rendering library such as MathJax or KaTeX for the web-based display, ensuring that both the source code and the rendered output are prominently displayed side by side. This approach allows users to see the exact source that generates the rendered output, facilitating a better understanding of LaTeX syntax and its visual outcomes.","GenTime":"2024-08-20 00:09:10"}
{"File Name":"verify-service-provider\/0023-we-will-report-the-version-in-a-saml-extension.md","Context":"## Context\\nThe verify-service-provider will be deployed to a number of relying parties.\\nWhich relying party is using which version could potentially become difficult\\nto keep track of.\\nTo make it as easy as possible for us to manage this we'd like the\\nverify-service-provider to report its version in some way.\\nBecause the verify-service-provider is not intended to be accessible to the\\ninternet we can't simply expose an endpoint that reports the version number.\\nAlso, since the SAML messages go via the browser we can't use a custom HTTP\\nheader.\\nThere's also a concern about the security implications of reporting a version\\nnumber in cleartext.\\nWe considered a couple of options:\\n- Requesting metadata from Verify with a custom user-agent string\\n- Sending the version in an unencrypted saml extension\\n- Sending the version in an encrypted saml extension\\n","Decision":"We decided to send the version number in the SAML AuthnRequests as an encrypted\\nSAML extension. The XML will look roughly like this:\\n```\\n<saml:AuthnRequest>\\n<saml:Issuer>...<\/saml:Issuer>\\n<saml:Signature>...<\/saml:Signature>\\n<saml:Extensions>\\n<saml:EncryptedAttribute>...<\/saml:EncryptedAttribute>\\n<\/saml:Extensions>\\n<\/saml:AuthnRequest>\\n```\\nOnce decrypted, the Attribute in the Extensions will look like:\\n```\\n<saml:Attribute Name=\"Versions\">\\n<saml:AttributeValue xsi:type=\"metrics:VersionsType\">\\n<metrics:ApplicationVersion>3.4.1<\/metrics:ApplicationVersion>\\n<\/saml:AttributeValue>\\n<\/saml:Attribute>\\n```\\n","tokens":182,"id":4436,"text":"## Context\\nThe verify-service-provider will be deployed to a number of relying parties.\\nWhich relying party is using which version could potentially become difficult\\nto keep track of.\\nTo make it as easy as possible for us to manage this we'd like the\\nverify-service-provider to report its version in some way.\\nBecause the verify-service-provider is not intended to be accessible to the\\ninternet we can't simply expose an endpoint that reports the version number.\\nAlso, since the SAML messages go via the browser we can't use a custom HTTP\\nheader.\\nThere's also a concern about the security implications of reporting a version\\nnumber in cleartext.\\nWe considered a couple of options:\\n- Requesting metadata from Verify with a custom user-agent string\\n- Sending the version in an unencrypted saml extension\\n- Sending the version in an encrypted saml extension\\n\n\n##Decision\nWe decided to send the version number in the SAML AuthnRequests as an encrypted\\nSAML extension. The XML will look roughly like this:\\n```\\n<saml:AuthnRequest>\\n<saml:Issuer>...<\/saml:Issuer>\\n<saml:Signature>...<\/saml:Signature>\\n<saml:Extensions>\\n<saml:EncryptedAttribute>...<\/saml:EncryptedAttribute>\\n<\/saml:Extensions>\\n<\/saml:AuthnRequest>\\n```\\nOnce decrypted, the Attribute in the Extensions will look like:\\n```\\n<saml:Attribute Name=\"Versions\">\\n<saml:AttributeValue xsi:type=\"metrics:VersionsType\">\\n<metrics:ApplicationVersion>3.4.1<\/metrics:ApplicationVersion>\\n<\/saml:AttributeValue>\\n<\/saml:Attribute>\\n```\\n","MatchedIds":[4448,4442,2582,578,1252],"Predicted":"## Decision\nWe will implement the reporting of the verify-service-provider version by sending the version in an encrypted SAML extension. This provides a secure way to transmit the version information without exposing it in cleartext. The encryption ensures that only authorized parties with the correct decryption keys can access the version information.\n\nThis method maintains the confidentiality of the version information while embedding it within the existing SAML messages, thus adhering to our security requirements.","GenTime":"2024-08-20 00:09:18"}
{"File Name":"lbh-adrs\/Microfrontend-Composition.md","Context":"## **Context**\\nAccording to the [Microfrontend-Architecture ADR](https:\/\/github.com\/LBHackney-IT\/lbh-adrs\/blob\/feature\/create-ADRs\/Platform\/Accepted\/Microfrontend-Architecture.md), we are going to define micro-frontends by using a vertical splitting.\\nIn this way each micro-frontends has got its repository and each team manages the development and the deployment of it.\\nAbout the client-side micro-frontends compositions, there are two options to implement that:\\n1. Writing our own boilerplate code\\nEach micro frontend is included in the html page using a \\<script> tag and each of those applications exposes a global function as its entry-point. The container application determines which frontend should be mounted and calls the relevant function to tell a micro frontend when and where to render itself.\\n2. Using a framework as Single SPA (https:\/\/single-spa.js.org\/ )\\nSingle SPA is a Javascript framework for frontend microservices. In practice, it applies a lifecycle to every application. Each app can respond to url routing events and must know how to bootstrap, mount and unmount itself from the DOM.\\n","Decision":"**Using Single SPA framework**\\nThis framework adopts the same principle as we would implement our own boilerplate code, but the advantage is that we don\u2019t have to build and document our own logic but instead we use this light framework supported by an open source community.\\nThe other advantage is that the framework allows to compose also applications written in Angualar and Vue.js\\n","tokens":242,"id":2317,"text":"## **Context**\\nAccording to the [Microfrontend-Architecture ADR](https:\/\/github.com\/LBHackney-IT\/lbh-adrs\/blob\/feature\/create-ADRs\/Platform\/Accepted\/Microfrontend-Architecture.md), we are going to define micro-frontends by using a vertical splitting.\\nIn this way each micro-frontends has got its repository and each team manages the development and the deployment of it.\\nAbout the client-side micro-frontends compositions, there are two options to implement that:\\n1. Writing our own boilerplate code\\nEach micro frontend is included in the html page using a \\<script> tag and each of those applications exposes a global function as its entry-point. The container application determines which frontend should be mounted and calls the relevant function to tell a micro frontend when and where to render itself.\\n2. Using a framework as Single SPA (https:\/\/single-spa.js.org\/ )\\nSingle SPA is a Javascript framework for frontend microservices. In practice, it applies a lifecycle to every application. Each app can respond to url routing events and must know how to bootstrap, mount and unmount itself from the DOM.\\n\n\n##Decision\n**Using Single SPA framework**\\nThis framework adopts the same principle as we would implement our own boilerplate code, but the advantage is that we don\u2019t have to build and document our own logic but instead we use this light framework supported by an open source community.\\nThe other advantage is that the framework allows to compose also applications written in Angualar and Vue.js\\n","MatchedIds":[389,1240,1067,4613,1907],"Predicted":"## Decision\n\nChosen Option: Use Single SPA framework\n\n### Rationale:\n\n1. **Standardized Lifecycle Management**: \n   - Single SPA provides a robust lifecycle management system for micro-frontends, including bootstrapping, mounting, and unmounting. This can significantly reduce the complexity and overhead associated with maintaining custom boilerplate code.\n\n2. **Scalability and Maintainability**: \n   - By using a standard framework, different teams working on various micro-frontends will have a consistent approach. This consistency helps scale the system more easily and ensures maintainability over time.\n\n3. **Routing and Shared State Management**:\n   - Single SPA handles URL routing events effectively, which helps in seamless integration and navigation between different micro-frontends. It also allows the sharing of global state and events among micro-frontends, making the development process more cohesive.\n\n4. **Community Support and Documentation**:\n   - Single SPA has a large community and extensive documentation which can be leveraged for troubleshooting, seeking advice, and finding best practices. This external support can expedite development and problem-solving processes.\n\n5. **Flexibility to Integrate with Other Frameworks**:\n   - Single SPA is framework-agnostic, meaning it can integrate micro-frontends built with different technologies (like React, Vue, Angular). This flexibility ensures that teams can choose the best tools and technologies for their specific requirements without being locked into a single stack.\n\n### Positive Consequences\n\n- **Reduced Boilerplate Code**: By leveraging Single SPA, the need to write and maintain custom boilerplate code is eliminated, allowing development teams to focus more on business logic and features.\n- **Improved Consistency across Teams**: Teams will follow a standardized method for creating and managing micro-frontends, leading to improved consistency and fewer integration issues.\n- **Enhanced Productivity**: By using a proven framework with robust documentation, teams can be more productive and face fewer roadblocks during development.\n\n### Negative Consequences\n\n- **Initial Learning Curve**: Teams will need time to familiarize themselves with the Single SPA framework, which may temporarily slow down the development process.\n- **Framework Dependency**: Relying on an external framework means any changes or updates to Single SPA can impact our micro-frontend architecture. We need to keep abreast of Single SPA updates and best practices to mitigate potential disruptions.\n\nBy using Single SPA, we ensure a more maintainable, scalable, and standardized approach to microfrontend architecture, enabling each team to focus more on delivering valuable features and less on managing infrastructure specifics.","GenTime":"2024-08-20 00:09:31"}
{"File Name":"beis-report-official-development-assistance\/0031-replace-publicactivity-with-our-own-hand-rolled-way-of-recording-changes.md","Context":"## Context\\nWe adopted the `PublicActivity` gem (as documented in [0019](https:\/\/github.com\/UKGovernmentBEIS\/beis-report-official-development-assistance\/blob\/731b91f5773c4306f919433af81976d29c5feef1\/doc\/architecture\/decisions\/0019-use-public-activity-gem.md)), but the information we record has never been used, or displayed to the user.\\nNow we're looking at surfacing information about changes to end users it's becoming increasingly clear that is `PublicActivity` is not fit for our needs, and would be easier to build our own approach to logging what changed were made by what user.\\n","Decision":"We have decided to introduce a `HistoricalEvent` model to record changes to models, starting with `Activity`s.\\nOnce we have applied `HistoricalEvents` to all the models we need to track, we can remove all the calls to `PublicActivity` and remove the gem entirely.\\n","tokens":145,"id":2403,"text":"## Context\\nWe adopted the `PublicActivity` gem (as documented in [0019](https:\/\/github.com\/UKGovernmentBEIS\/beis-report-official-development-assistance\/blob\/731b91f5773c4306f919433af81976d29c5feef1\/doc\/architecture\/decisions\/0019-use-public-activity-gem.md)), but the information we record has never been used, or displayed to the user.\\nNow we're looking at surfacing information about changes to end users it's becoming increasingly clear that is `PublicActivity` is not fit for our needs, and would be easier to build our own approach to logging what changed were made by what user.\\n\n\n##Decision\nWe have decided to introduce a `HistoricalEvent` model to record changes to models, starting with `Activity`s.\\nOnce we have applied `HistoricalEvents` to all the models we need to track, we can remove all the calls to `PublicActivity` and remove the gem entirely.\\n","MatchedIds":[5211,2261,4361,1863,1701],"Predicted":"## Decision\nWe will discontinue the use of the `PublicActivity` gem for logging changes and instead implement a custom solution tailored to our specific needs. This custom solution will allow us to accurately track and surface information about changes made by users. This approach provides us with greater flexibility and control over how this information is captured, stored, and displayed, ensuring it is both useful and relevant for end users.\n\nWe will:\n- Design and implement a custom logging mechanism to track user changes.\n- Ensure the custom solution is extensible and can evolve with future requirements.\n- Replace existing `PublicActivity` implementations with the new custom solution in our codebase.\n  \nThis decision will enhance the visibility of changes and provide a better user experience by making this critical information more accessible and relevant.","GenTime":"2024-08-20 00:09:39"}
{"File Name":"embvm-core\/0014-refactor-driver-interfaces-to-use-namespaces.md","Context":"## Context\\nDriver interfaces were previously defined in the global namespace, and associated types were defined as `struct`s in the global namespace with a generic name like `tof`:\\n```\\nstruct tof\\n{\\nusing distance_t = uint16_t;\\nusing cb_t = stdext::inplace_function<void(distance_t)>;\\nstatic const distance_t INVALID_RANGE = UINT16_MAX;\\nenum class mode\\n{\\ndefaultRange = 0,\\nshortRange,\\nmedRange,\\nlongRange,\\n};\\n}''\\n```\\nInterface classes would inherit from these structs:\\n```\\nclass TimeOfFlight : public embvm::DriverBase, public embvm::tof\\n```\\nAnd the effect was similar to namespacing (`embvm::tof::mode`):\\n```\\nvirtual embvm::tof::mode mode(embvm::tof::mode m) = 0;\\n```\\nEssentially, we are recreating a feature that `namespace` already provides and complicating our inheritance chains.\\n","Decision":"Each driver interface class and any related types are to be stored in a separate namespace per driver type. This keeps related types tied together, improves our generated documentation, and provides improved names for classes and types.\\n","tokens":225,"id":3027,"text":"## Context\\nDriver interfaces were previously defined in the global namespace, and associated types were defined as `struct`s in the global namespace with a generic name like `tof`:\\n```\\nstruct tof\\n{\\nusing distance_t = uint16_t;\\nusing cb_t = stdext::inplace_function<void(distance_t)>;\\nstatic const distance_t INVALID_RANGE = UINT16_MAX;\\nenum class mode\\n{\\ndefaultRange = 0,\\nshortRange,\\nmedRange,\\nlongRange,\\n};\\n}''\\n```\\nInterface classes would inherit from these structs:\\n```\\nclass TimeOfFlight : public embvm::DriverBase, public embvm::tof\\n```\\nAnd the effect was similar to namespacing (`embvm::tof::mode`):\\n```\\nvirtual embvm::tof::mode mode(embvm::tof::mode m) = 0;\\n```\\nEssentially, we are recreating a feature that `namespace` already provides and complicating our inheritance chains.\\n\n\n##Decision\nEach driver interface class and any related types are to be stored in a separate namespace per driver type. This keeps related types tied together, improves our generated documentation, and provides improved names for classes and types.\\n","MatchedIds":[3023,1193,2270,3637,1912],"Predicted":"## Decision\nGoing forward, we will remove the inheritance from the `struct`s and instead use `namespaces` to group related types and constants. This will simplify our inheritance chains and take advantage of the native scoping features provided by `namespace`.\n\n### Example:\n\nInstead of:\n```cpp\nstruct tof\n{\n    using distance_t = uint16_t;\n    using cb_t = stdext::inplace_function<void(distance_t)>;\n    static const distance_t INVALID_RANGE = UINT16_MAX;\n    enum class mode\n    {\n        defaultRange = 0,\n        shortRange,\n        medRange,\n        longRange,\n    };\n};\n\nclass TimeOfFlight : public embvm::DriverBase, public embvm::tof\n{\n    virtual embvm::tof::mode mode(embvm::tof::mode m) = 0;\n};\n```\n\nWe will use:\n```cpp\nnamespace embvm\n{\n    namespace tof\n    {\n        using distance_t = uint16_t;\n        using cb_t = stdext::inplace_function<void(distance_t)>;\n        static const distance_t INVALID_RANGE = UINT16_MAX;\n\n        enum class mode\n        {\n            defaultRange = 0,\n            shortRange,\n            medRange,\n            longRange,\n        };\n    }\n\n    class TimeOfFlight : public DriverBase\n    {\n        virtual tof::mode mode(tof::mode m) = 0;\n    };\n}\n```\n\nBy doing this, we maintain a clear separation and better organization of our code while reducing unnecessary complexity in our class hierarchies.","GenTime":"2024-08-20 00:09:44"}
{"File Name":"openlobby-server\/0005-python.md","Context":"## Context\\nWe need to choose main programming language for this project.\\n","Decision":"I'm now fluent in writing Python and JavaScript. Both languages are widely used\\nthese days and may attract developers to join this project.\\nPython is more mature with more stable libs for writing server side code and\\ntesting. JS may have better libraries for writing GraphQL APIs because\\nGraphQL originated in JS ecosystem.\\nI'm doing this in free time and for fun. I want to focus on project's features\\nand not to waste a time with poorly designed language lacking mature and\\nstable libraries. So for obvious reasons I'm choosing Python.\\n","tokens":15,"id":472,"text":"## Context\\nWe need to choose main programming language for this project.\\n\n\n##Decision\nI'm now fluent in writing Python and JavaScript. Both languages are widely used\\nthese days and may attract developers to join this project.\\nPython is more mature with more stable libs for writing server side code and\\ntesting. JS may have better libraries for writing GraphQL APIs because\\nGraphQL originated in JS ecosystem.\\nI'm doing this in free time and for fun. I want to focus on project's features\\nand not to waste a time with poorly designed language lacking mature and\\nstable libraries. So for obvious reasons I'm choosing Python.\\n","MatchedIds":[4761,3715,1240,1243,1239],"Predicted":"## Decision\nWe choose JavaScript as our main programming language because it allows us to handle both frontend and backend logic with the same language using frameworks like Node.js for the backend and will streamline our development process.","GenTime":"2024-08-20 00:09:46"}
{"File Name":"reactive-interaction-gateway\/0003-for-config-prefer-prefix-over-nesting-and-don-t-hide-defaults-in-code.md","Context":"## Context\\nThere no agreed-upon way of handling application configuration. There are two issues this ADR aims to address.\\n### Issue 1: Nested keys\\nIn `config.exs`, configuration keys can be nested by using a Keyword as value. Unfortunately, there is no built-in support for this nesting. For example:\\n```\\nApplication.fetch_env!(:rig, :required_key)\\n```\\nwill show a nice error, while\\n```\\nApplication.fetch_env!(:rig, :required_key)[:required_subkey]\\n```\\nwill simply return nil in case `:required_key` is present but `:required_subkey` is not.\\n### Issue 2: Default values\\nSome default values are defined where they are needed in the code, which is a problem once a key is used more than once. Also, defaults are no easily inspectable.\\n","Decision":"* We prefer not to nest configuration keys; instead, we prefix them where it makes sense. For example, instead of `config :rig, :kafka, client_id: :rig_brod_client` we write `config :rig, kafka_client_id: :rig_brod_client`. This allows us to leverage the built-in methods (e.g., `Application.fetch_env!`), which produce sensible error messages in case required values are not defined.\\n* We always set default values in config.exs (in turn this means that we prefer `Application.fetch_env!` over `Application.get_env`). This way, it is easy to reason about default values, and using a default value in more than one place is not an issue.\\n","tokens":182,"id":5227,"text":"## Context\\nThere no agreed-upon way of handling application configuration. There are two issues this ADR aims to address.\\n### Issue 1: Nested keys\\nIn `config.exs`, configuration keys can be nested by using a Keyword as value. Unfortunately, there is no built-in support for this nesting. For example:\\n```\\nApplication.fetch_env!(:rig, :required_key)\\n```\\nwill show a nice error, while\\n```\\nApplication.fetch_env!(:rig, :required_key)[:required_subkey]\\n```\\nwill simply return nil in case `:required_key` is present but `:required_subkey` is not.\\n### Issue 2: Default values\\nSome default values are defined where they are needed in the code, which is a problem once a key is used more than once. Also, defaults are no easily inspectable.\\n\n\n##Decision\n* We prefer not to nest configuration keys; instead, we prefix them where it makes sense. For example, instead of `config :rig, :kafka, client_id: :rig_brod_client` we write `config :rig, kafka_client_id: :rig_brod_client`. This allows us to leverage the built-in methods (e.g., `Application.fetch_env!`), which produce sensible error messages in case required values are not defined.\\n* We always set default values in config.exs (in turn this means that we prefer `Application.fetch_env!` over `Application.get_env`). This way, it is easy to reason about default values, and using a default value in more than one place is not an issue.\\n","MatchedIds":[1254,3389,3637,4448,1307],"Predicted":"## Decision\nTo address the issues with nested keys and default values in our configuration management, we will adopt the following solution:\n\n1. **Introduce a Configuration Module:**\n   Create a central configuration module, `ConfigManager`, which will be responsible for handling all application configurations. This module will include functions to access nested keys and manage default values.\n\n2. **Nested Key Access:**\n   Define functions within the `ConfigManager` to safely access nested keys. This ensures that if a required subkey is missing, a helpful error is raised instead of silently returning `nil`.\n\n   ```elixir\n   defmodule ConfigManager do\n     @moduledoc \"\"\"\n     A module to manage application configuration.\n     \"\"\"\n\n     @doc \"\"\"\n     Fetches a nested key from the application configuration.\n     \"\"\"\n     def fetch_nested_env!(app, key, subkey) do\n       case Application.fetch_env(app, key) do\n         {:ok, value} -> Keyword.fetch!(value, subkey)\n         :error -> raise \"Configuration key #{inspect(key)} is missing for application #{inspect(app)}.\"\n       end\n     end\n   end\n   ```\n\n3. **Centralized Default Values:**\n   Define default values centrally within the `ConfigManager`. This makes it easier to inspect and manage default values, ensuring consistency across the application.\n\n   ```elixir\n   defmodule ConfigManager do\n     @moduledoc \"\"\"\n     A module to manage application configuration.\n     \"\"\"\n\n     @default_values %{\n       :rig => [\n         required_key: [required_subkey: \"default_value\"]\n       ]\n     }\n\n     @doc \"\"\"\n     Fetches a nested key with a default value if it is missing.\n     \"\"\"\n     def fetch_nested_env_with_default!(app, key, subkey) do\n       config = Application.fetch_env(app, key, @default_values[app][key] || [])\n       Keyword.get(config, subkey, @default_values[app][key][subkey])\n     end\n   end\n   ```\n\n4. **Update Usage Across Application:**\n   Refactor the application code to use `ConfigManager` for accessing configuration values. This ensures that all configuration access is routed through a consistent interface, leveraging the new functionalities.\n\n   ```elixir\n   # Old code\n   value = Application.fetch_env!(:rig, :required_key)[:required_subkey]\n\n   # New code using ConfigManager\n   value = ConfigManager.fetch_nested_env_with_default!(:rig, :required_key, :required_subkey)\n   ```\n\nBy implementing the `ConfigManager` module as described, we ensure that nested keys are accessed safely and default values are managed centrally and consistently across the entire application. This will reduce the risk of configuration errors and improve maintainability.","GenTime":"2024-08-20 00:09:54"}
{"File Name":"opg-use-an-lpa\/0002-persistent-storage-uses-dynamodb.md","Context":"## Context\\n* The application will require persistent storage for storing LPA ownership, granted access, and possibly user credentials\\n* Current MoJ strategy is to use managed services where possible\\n","Decision":"Use DynamoDB for persistent storage\\n","tokens":37,"id":4841,"text":"## Context\\n* The application will require persistent storage for storing LPA ownership, granted access, and possibly user credentials\\n* Current MoJ strategy is to use managed services where possible\\n\n\n##Decision\nUse DynamoDB for persistent storage\\n","MatchedIds":[1480,3142,4041,2410,1182],"Predicted":"## Decision\nWe have decided to utilize Amazon RDS (Relational Database Service) for the persistent storage requirements of the application. Amazon RDS offers managed databases which align with the MoJ strategy of using managed services where possible. It provides scalability, high availability, and built-in security features suitable for storing LPA ownership details, granted access, and user credentials. Additionally, Amazon RDS supports both automated backups and manual snapshots, ensuring data durability and ease of recovery.","GenTime":"2024-08-20 00:09:56"}
{"File Name":"skunkworks\/state-management.md","Context":"## Context\\nTypes of State:\\n- Local State\\n- e.g. React `useState` Hook\\n- Shared State\\n- e.g. React Context, Redux\\n- Remote State (i.e. API)\\n- e.g. React Context, Redux\\nShared State Options:\\n- `redux` + `redux-observable` + `rxjs` + `react-redux`\\n- Drawbacks\\n- Boilerplate\\n- React Context\\n- Drawbacks\\n- \"Provider Hell\"\\n- Unintentional Re-Renders\\n- `constate` \/ `recoil` \/ `react-tracked`\\nRemote State Options:\\n- `redux` + `redux-observable` + `rxjs` + `react-redux`\\n- Drawbacks\\n- Boilerplate\\n- React Context\\n- Drawbacks\\n- \"Provider Hell\"\\n- Unintentional Re-Renders\\n- `constate` \/ `recoil` \/ `react-tracked` \/ `@tanstack\/react-query`\\n","Decision":"- Local State: React `useState` Hook\\n- Shared State: `recoil`\\n- Remote State: `@tanstack\/react-query`\\n","tokens":208,"id":2298,"text":"## Context\\nTypes of State:\\n- Local State\\n- e.g. React `useState` Hook\\n- Shared State\\n- e.g. React Context, Redux\\n- Remote State (i.e. API)\\n- e.g. React Context, Redux\\nShared State Options:\\n- `redux` + `redux-observable` + `rxjs` + `react-redux`\\n- Drawbacks\\n- Boilerplate\\n- React Context\\n- Drawbacks\\n- \"Provider Hell\"\\n- Unintentional Re-Renders\\n- `constate` \/ `recoil` \/ `react-tracked`\\nRemote State Options:\\n- `redux` + `redux-observable` + `rxjs` + `react-redux`\\n- Drawbacks\\n- Boilerplate\\n- React Context\\n- Drawbacks\\n- \"Provider Hell\"\\n- Unintentional Re-Renders\\n- `constate` \/ `recoil` \/ `react-tracked` \/ `@tanstack\/react-query`\\n\n\n##Decision\n- Local State: React `useState` Hook\\n- Shared State: `recoil`\\n- Remote State: `@tanstack\/react-query`\\n","MatchedIds":[334,1854,2667,4596,3035],"Predicted":"## Decision\nTo manage our state effectively, we will adopt the following strategy:\n\n1. **Local State:**\n   - For local component state, we will use React's `useState` hook. This is straightforward and works well for state that does not need to be shared beyond the component.\n\n2. **Shared State:**\n   - For shared state, we will use Recoil. Recoil provides a simpler API compared to Redux, without the boilerplate and complexity of setting up actions, reducers, and middleware. Additionally, it offers better performance by minimizing unnecessary re-renders and preventing \"Provider Hell\" with its atom-based state management.\n\n3. **Remote State:**\n   - For remote state management (i.e., state fetched from APIs), we will use `@tanstack\/react-query`. React Query is designed for handling fetching, caching, and updating asynchronous data, which makes it ideal for managing remote state. It also provides excellent support for error handling, caching, and optimistic updates, reducing the need for boilerplate code and improving application performance.\n\nBy leveraging Recoil for shared state and React Query for remote state, we can reduce complexity, minimize boilerplate, and improve performance, all while maintaining a clean and maintainable codebase.","GenTime":"2024-08-20 00:10:01"}
{"File Name":"buddy-auth.jwt-oidc\/adr-001.md","Context":"# Context #\\nThe library currently assumes that the ID tokens are signed by a\\npublic key (and the code unconditionally tries to build the actual\\npublic key from the data we get from the ID Token issuer).\\nAt the same time, the OpenID Connect Core 1.0 specification states\\nthat:\\n> \"Symmetric signatures MUST NOT be used by public (non-confidential)\\n> Clients because of their inability to keep secrets\"\\n(see https:\/\/openid.net\/specs\/openid-connect-core-1_0.html#Signing).\\nAs we don't expect to support confidential clients, it seems safe to\\nignore Symmetric Keys when validating tokens.\\n# Decision #\\nTokens signed with a Symmetric Key will always be marked as invalid\\n(whether they are actually valid or not).\\n# Status #\\nAccepted.\\n# Consequences #\\nEven if the ID Token issuer is configured to use Symmetric keys to\\nsign its tokens, and the library gets a perfectly valid token signed\\nusing one of such keys, the library will always return that the token\\nis invalid.\\n","Decision":"Tokens signed with a Symmetric Key will always be marked as invalid\\n(whether they are actually valid or not).\\n# Status #\\nAccepted.\\n# Consequences #\\nEven if the ID Token issuer is configured to use Symmetric keys to\\nsign its tokens, and the library gets a perfectly valid token signed\\nusing one of such keys, the library will always return that the token\\nis invalid.\\n","tokens":229,"id":3872,"text":"# Context #\\nThe library currently assumes that the ID tokens are signed by a\\npublic key (and the code unconditionally tries to build the actual\\npublic key from the data we get from the ID Token issuer).\\nAt the same time, the OpenID Connect Core 1.0 specification states\\nthat:\\n> \"Symmetric signatures MUST NOT be used by public (non-confidential)\\n> Clients because of their inability to keep secrets\"\\n(see https:\/\/openid.net\/specs\/openid-connect-core-1_0.html#Signing).\\nAs we don't expect to support confidential clients, it seems safe to\\nignore Symmetric Keys when validating tokens.\\n# Decision #\\nTokens signed with a Symmetric Key will always be marked as invalid\\n(whether they are actually valid or not).\\n# Status #\\nAccepted.\\n# Consequences #\\nEven if the ID Token issuer is configured to use Symmetric keys to\\nsign its tokens, and the library gets a perfectly valid token signed\\nusing one of such keys, the library will always return that the token\\nis invalid.\\n\n\n##Decision\nTokens signed with a Symmetric Key will always be marked as invalid\\n(whether they are actually valid or not).\\n# Status #\\nAccepted.\\n# Consequences #\\nEven if the ID Token issuer is configured to use Symmetric keys to\\nsign its tokens, and the library gets a perfectly valid token signed\\nusing one of such keys, the library will always return that the token\\nis invalid.\\n","MatchedIds":[1721,2256,1962,4623,841],"Predicted":"## Decision\nTokens signed with a Symmetric Key will always be marked as invalid (whether they are actually valid or not).\n\n## Status\nAccepted.\n\n## Consequences\n* Positive\n  - Ensures compliance with the OpenID Connect Core 1.0 specification.\n  - Enhances security by enforcing the use of asymmetric keys, which are more secure for public clients.\n  - Simplifies token validation logic by focusing solely on asymmetric key signatures.\n\n* Negative\n  - Even if the ID Token issuer uses Symmetric keys for valid tokens, these tokens will be marked as invalid by the library.\n  - May lead to confusion or integration issues if ID Token issuers are not aware of the restriction on symmetric keys.\n  - Requires ID Token issuers to update their configurations to use asymmetric keys if they are currently using symmetric keys.","GenTime":"2024-08-20 00:10:05"}
{"File Name":"GSoC-Braitenberg-Vehicles\/use_javafx_for_animation.md","Context":"Context\\n-------\\nAfter a while writing the project code, I've realized that it needs constant and probably heavyweight rendering.\\nDecision\\n--------\\nLeave with JavaFX\/TornadoFX as constant rendering\/animation framework.\\nRationalization\\n---------------\\nAlthough there are probably better frameworks\/engines (e.g. Unity, OpenGL), configuring and learning to use them would probably take time I don't have.\\nJavaFX seems to be a nice opportunity with relatively little to learn.\\n","Decision":"--------\\nLeave with JavaFX\/TornadoFX as constant rendering\/animation framework.\\nRationalization\\n---------------\\nAlthough there are probably better frameworks\/engines (e.g. Unity, OpenGL), configuring and learning to use them would probably take time I don't have.\\nJavaFX seems to be a nice opportunity with relatively little to learn.\\n","tokens":101,"id":1400,"text":"Context\\n-------\\nAfter a while writing the project code, I've realized that it needs constant and probably heavyweight rendering.\\nDecision\\n--------\\nLeave with JavaFX\/TornadoFX as constant rendering\/animation framework.\\nRationalization\\n---------------\\nAlthough there are probably better frameworks\/engines (e.g. Unity, OpenGL), configuring and learning to use them would probably take time I don't have.\\nJavaFX seems to be a nice opportunity with relatively little to learn.\\n\n\n##Decision\n--------\\nLeave with JavaFX\/TornadoFX as constant rendering\/animation framework.\\nRationalization\\n---------------\\nAlthough there are probably better frameworks\/engines (e.g. Unity, OpenGL), configuring and learning to use them would probably take time I don't have.\\nJavaFX seems to be a nice opportunity with relatively little to learn.\\n","MatchedIds":[5076,3718,472,3715,4716],"Predicted":"## Decision\nContinue using JavaFX\/TornadoFX for constant rendering and animation framework.\n\n### Status\nAccepted\n\n### Consequences\nJavaFX provides a robust platform for building rich client applications with enhanced graphics and media capabilities, which should suffice for the project's rendering needs. TornadoFX, being a lightweight Kotlin framework for JavaFX, will streamline development with concise and expressive code.\n\nWhile there may be more powerful frameworks like Unity or OpenGL, they come with a steeper learning curve and configuration overhead that may not be feasible within the current time constraints. JavaFX offers a balanced compromise with adequate performance and development speed.\n\nHowever, as the project evolves, the limitations of JavaFX in handling extremely complex or resource-intensive rendering tasks may become apparent. If the demands increase beyond what JavaFX can handle efficiently, revisiting and potentially transitioning to more specialized rendering frameworks like Unity or OpenGL might be necessary.\n\nFor now, JavaFX\/TornadoFX offers a viable, low-barrier solution that aligns well with the project's current needs and developer familiarity.","GenTime":"2024-08-20 00:10:09"}
{"File Name":"paas-team-manual\/ADR018-rds-broker-restore-last-operation.html.md","Context":"## Context\\nWe use a completely stateless implementation for the RDS broker, as described in [ADR006](..\/ADR006-rds-broker).\\nSo all the asynchronous operations on RDS instances were relying on executing a unique operation on AWS API, and querying the RDS instance status reported the AWS API.\\nBut to implement the feature of restore from snapshot, we must execute several operations sequentially.\\nThe broker must:\\n1. Start the restore from snapshot, which can take minutes.\\n2. Once finish, update several parameters of the instance (security groups, parameters, etc).\\n3. Once that is finish, reset the master password of the RDS instance.\\n4. Finally reset the passwords of the users previously bind in the original DB.\\nAs the create operation is a asynchronous operation, the Cloud Controller API will periodically request the `LastOperation` endpoint to query the state of the restored instance.\\nThe rds-broker must respond accordingly.\\nThe Cloud Controller API includes logic to ensure the resiliance of a service creation, maintaining the workers that will poll the `LastOperation` until the service is created or there is timeout.\\nTo implement this kind logic, some kind of state must be kept to track the changes on the instance.  Options are:\\n* run a background house-keeping routine. This house-keeping should be resilient to rds-broker restarts and able to work with multiple rds-broker instances..\\n* Use SNS and SQS, by subscribing to the [AWS events from RDS](http:\/\/docs.aws.amazon.com\/AmazonRDS\/latest\/UserGuide\/USER_Events.html). This requires a lot of additional work and integration effort.\\n* Store the state in some database or k\/v store.\\n","Decision":"We decided:\\n* Implement a state machine using the [AWS tags](http:\/\/docs.aws.amazon.com\/AWSEC2\/latest\/UserGuide\/Using_Tags.html) of the instance.\\nWe will add a list of tags for each pending operations to execute.\\n* We make use of `LastOperation` to check the pending operations and perform them, to finally delete the corresponding tag to mark it as done.\\nWe assume that:\\n* all the required operations are either asynchronous in the AWS API (eg. update instance) or quick to execute (e.g. reset bind user passwords)\\n* that update the tags is atomic and synchronous.\\n","tokens":360,"id":207,"text":"## Context\\nWe use a completely stateless implementation for the RDS broker, as described in [ADR006](..\/ADR006-rds-broker).\\nSo all the asynchronous operations on RDS instances were relying on executing a unique operation on AWS API, and querying the RDS instance status reported the AWS API.\\nBut to implement the feature of restore from snapshot, we must execute several operations sequentially.\\nThe broker must:\\n1. Start the restore from snapshot, which can take minutes.\\n2. Once finish, update several parameters of the instance (security groups, parameters, etc).\\n3. Once that is finish, reset the master password of the RDS instance.\\n4. Finally reset the passwords of the users previously bind in the original DB.\\nAs the create operation is a asynchronous operation, the Cloud Controller API will periodically request the `LastOperation` endpoint to query the state of the restored instance.\\nThe rds-broker must respond accordingly.\\nThe Cloud Controller API includes logic to ensure the resiliance of a service creation, maintaining the workers that will poll the `LastOperation` until the service is created or there is timeout.\\nTo implement this kind logic, some kind of state must be kept to track the changes on the instance.  Options are:\\n* run a background house-keeping routine. This house-keeping should be resilient to rds-broker restarts and able to work with multiple rds-broker instances..\\n* Use SNS and SQS, by subscribing to the [AWS events from RDS](http:\/\/docs.aws.amazon.com\/AmazonRDS\/latest\/UserGuide\/USER_Events.html). This requires a lot of additional work and integration effort.\\n* Store the state in some database or k\/v store.\\n\n\n##Decision\nWe decided:\\n* Implement a state machine using the [AWS tags](http:\/\/docs.aws.amazon.com\/AWSEC2\/latest\/UserGuide\/Using_Tags.html) of the instance.\\nWe will add a list of tags for each pending operations to execute.\\n* We make use of `LastOperation` to check the pending operations and perform them, to finally delete the corresponding tag to mark it as done.\\nWe assume that:\\n* all the required operations are either asynchronous in the AWS API (eg. update instance) or quick to execute (e.g. reset bind user passwords)\\n* that update the tags is atomic and synchronous.\\n","MatchedIds":[224,4061,1740,2791,4285],"Predicted":"## Decision\n\nWe have decided to implement the state management for the RDS broker by storing the state in a highly available and resilient k\/v store, such as Amazon DynamoDB.\n\n### Reasoning:\n\n1. **Simplicity and Minimal Overhead**: Running a background house-keeping routine would require ensuring resilience to restarts and coordination among multiple rds-broker instances. This can introduce significant complexity and potential points of failure.\n\n2. **Reduced Integration Complexity**: While using SNS and SQS to subscribe to AWS events from RDS could provide a robust event-driven mechanism, it requires extensive integration work and adds complexity to the system. This complexity is not justified for our current requirements.\n\n3. **Reliability and Ease of Use**: A k\/v store like DynamoDB provides a reliable and straightforward solution to store the state information required to track the progression of the restore process. It is highly available, scales automatically, and integrates well with other AWS services.\n\n### Implementation Details:\n\n1. **Store State in DynamoDB**: A table in DynamoDB will be created to store the state of each RDS restoration operation. The table will include information about each step's status (e.g., restoration started, parameters updated, passwords reset).\n\n2. **Track Operations Sequentially**: The rds-broker will update the state in DynamoDB after completing each operation (restoring from snapshot, updating parameters, resetting passwords). The Cloud Controller API will periodically query the `LastOperation` endpoint, which will, in turn, retrieve the current state from DynamoDB and respond accordingly.\n\n3. **Resilience and Scalability**: DynamoDB's high availability and auto-scaling features will ensure that the state storage mechanism is resilient to failures and scalable as the number of RDS instances grows.\n\n4. **Security and Compliance**: Using AWS Identity and Access Management (IAM), permissions will be configured to ensure that only the rds-broker instances have the necessary access to write\/read the state from the DynamoDB table, maintaining the security of the system.\n\nBy storing the state in DynamoDB, we can keep our implementation simple, resilient, and scalable without introducing unnecessary complexity. This approach aligns with our goal of maintaining a largely stateless rds-broker while accommodating the need for sequential operations in the RDS snapshot restoration process.","GenTime":"2024-08-20 00:10:18"}
{"File Name":"buildit-all\/0007-use-pac4j-to-validate-tokens.md","Context":"## Context\\nhttp:\/\/www.pac4j.org\\nWhile exporing alternatives to our custom filter and token validator, we came across pac4j which can deal with Azure AD out of the box.\\nThe driver for this alternative is to have a library where we can point it to the discovery url (https:\/\/login.microsoftonline.com\/organizations\/v2.0\/.well-known\/openid-configuration) and have it \"just work.\"  Microsoft is a bit off standard where they add {tenantId} to the Issuer URL because this is a multi-tenant endpoint.  While it's not hard, openid connect libraries need to be adjusted to handle this.  Pac4j does this for free.\\n","Decision":"* Use Pac4J to validate MS id_tokens instead of our current custom filter code\\n","tokens":144,"id":4422,"text":"## Context\\nhttp:\/\/www.pac4j.org\\nWhile exporing alternatives to our custom filter and token validator, we came across pac4j which can deal with Azure AD out of the box.\\nThe driver for this alternative is to have a library where we can point it to the discovery url (https:\/\/login.microsoftonline.com\/organizations\/v2.0\/.well-known\/openid-configuration) and have it \"just work.\"  Microsoft is a bit off standard where they add {tenantId} to the Issuer URL because this is a multi-tenant endpoint.  While it's not hard, openid connect libraries need to be adjusted to handle this.  Pac4j does this for free.\\n\n\n##Decision\n* Use Pac4J to validate MS id_tokens instead of our current custom filter code\\n","MatchedIds":[3217,4414,2582,5211,527],"Predicted":"## Decision\n* Adopt pac4j for authentication integration with Azure AD.\n* Replace the custom filter and token validator with pac4j's implementation.\n* Configure pac4j to use the discovery URL (https:\/\/login.microsoftonline.com\/organizations\/v2.0\/.well-known\/openid-configuration) for automatic integration with Azure AD.\n* Update internal documentation to guide developers on how to configure and use pac4j within our application stack.","GenTime":"2024-08-20 00:10:20"}
{"File Name":"Corvus.Tenancy\/0004-v2-to-v3-transition.md","Context":"## Context\\nAs described in [ADR 0004, `Corvus.Tenancy` will not create storage containers automatically](.\/0003-no-automatic-storage-container-creation.md), `Corvus.Tenancy` v3 introduces a change: applications are now responsible for creating all necessary containers when onboarding a client. This creates a challenge for applications that have already been deployed on v2, because the following things may be true:\\n* a tenant may exist in which only a subset of its storage containers exist\\n* in a no-downtime migration, a compute farm may have a mixture of v2 and v3 components in use\\nTo enable applications currently using `Corvus.Tenancy` v2 to migrate to v3 without disruption, we need a clearly defined path of how a system will be upgraded.\\n","Decision":"Upgrades from v2 to v3 use a multi-phase approach, in which any single compute node in the application goes through these steps:\\n1. using nothing but v2\\n1. using v3 libraries mostly (see below) in v2 mode\\n1. using v3 libraries, onboarding new clients in v3 style, using v3 config where available, falling back to v2 config and auto-creation of containers when v3 config not available\\n1. using v3 libraries in non-transitional mode\\nWhile in phase 3, we would run a tool to transition all v2 configuration to v3. Once this tool has completed its work, we are then free to move into phase 4. (There's no particular hurry to move into this final phase. Once all tenants that had v2 configuration have been migrated to v3, there's no behavioural difference between phases 3 and 4. The main motivation for moving to phase 4 is that it enables applications to remove transitional code once transition is complete. Phase 4 might not occur until years after the other phases. For example, libraries such as [Marain](https:\/\/github.com\/marain-dotnet) that enable developers to host their own instances of a service might choose to retain transitional code for a very long time to give customers of these libraries time to complete their migration.)\\nTo support zero-downtime upgrades, it's necessary to support a state where all compute nodes using a particular store are in a mixture of two adjacent phases. E.g., when we move from 1 to 2, there will be a period of time in which some nodes are still in phase 1, and some are in phase 2. However, we will avoid ever being in three phases simultaneously. For example, we will wait until all compute nodes have completed their move to state 2 before moving any into state 3.\\nThe following sections describe the behaviour required in each of the v3 states to support transition. (There's nothing to document here for phase 1, because that's how systems already using v2 today behave.)\\n### Phase 2: using v3 libraries, operating in v2 mode\\nA node in this phase has upgraded to v3 libraries, but is using the transition support and is essentially operating in v2 mode. It will never create new v3 configuration. New tenants continue to be onboarded in the same way as with v2 libraries\u2014the application does not pre-create containers, and expects the tenancy library to create them on demand as required. This gives applications a low-impact way in which to upgrade to v3 libraries without changing any behaviour, and also opens the path to migration towards the new style of operation.\\nThe one difference in behaviour (the reason we describe this as \"mostly\" v2 mode above) is that if v3 configuration is present for a particular configuration key, it has the following effects:\\n* the application will use the v3 configuration and will not even look to see if v2 configuration is present\\n* the application will presume that all relevant containers for this configuration have already been created, and will not attempt to create anything on demand\\nThis is necessary to support the case where all nodes have completed their transition to phase 2 (so none is in phase 1), and some have have moved to phase 3. Nodes that are still in phase 2 at this point need to be able to cope with the possibility that some clients have been onboarded by a phase 3 node, and so there will be only v3 configuration available. (We do not expect both v2 and v3 configuration to be present for any particular container at this point, because migration of tenants onboarded the v2 way into v3 configuration does not start until all nodes have reached phase 3.)\\nTo configure a node to run in this mode, use storage through a suitable transitional interface (e.g., `IBlobContainerSourceWithTenantLegacyTransition`). The application must provide two configuration keys: one for v2 configuration and one for v3 configuration. The transitional adapter will never create v3 configuration, but it will look for it, and only looks for v2 configuration when no v3 configuration is present.\\n### Phase 3: v3 libraries, operating in v3 mode, falling back to v2 as necessary\\nA node in this phase is using the v3 libraries. When onboarding new tenants, it pre-creates all necessary containers, and stores v3 config, but it still uses the transition support so that in cases where existing tenants have only v2 configuration available, it can fall back to the old behaviour.\\nThe only difference between phase 2 and phase 3 is how the application onboards new tenants. Both phases use the transitional adapter in exactly the same way.\\n### Configuration migration\\nOnce all nodes are in phase 3, a tool can be run to upgrade all v2 configuration to v3. Some aspects of this tooling are necessarily application-specific: only the application can know how to discover all of its tenants, and only the application can know what configuration it is storing, and under which keys.\\n","tokens":171,"id":714,"text":"## Context\\nAs described in [ADR 0004, `Corvus.Tenancy` will not create storage containers automatically](.\/0003-no-automatic-storage-container-creation.md), `Corvus.Tenancy` v3 introduces a change: applications are now responsible for creating all necessary containers when onboarding a client. This creates a challenge for applications that have already been deployed on v2, because the following things may be true:\\n* a tenant may exist in which only a subset of its storage containers exist\\n* in a no-downtime migration, a compute farm may have a mixture of v2 and v3 components in use\\nTo enable applications currently using `Corvus.Tenancy` v2 to migrate to v3 without disruption, we need a clearly defined path of how a system will be upgraded.\\n\n\n##Decision\nUpgrades from v2 to v3 use a multi-phase approach, in which any single compute node in the application goes through these steps:\\n1. using nothing but v2\\n1. using v3 libraries mostly (see below) in v2 mode\\n1. using v3 libraries, onboarding new clients in v3 style, using v3 config where available, falling back to v2 config and auto-creation of containers when v3 config not available\\n1. using v3 libraries in non-transitional mode\\nWhile in phase 3, we would run a tool to transition all v2 configuration to v3. Once this tool has completed its work, we are then free to move into phase 4. (There's no particular hurry to move into this final phase. Once all tenants that had v2 configuration have been migrated to v3, there's no behavioural difference between phases 3 and 4. The main motivation for moving to phase 4 is that it enables applications to remove transitional code once transition is complete. Phase 4 might not occur until years after the other phases. For example, libraries such as [Marain](https:\/\/github.com\/marain-dotnet) that enable developers to host their own instances of a service might choose to retain transitional code for a very long time to give customers of these libraries time to complete their migration.)\\nTo support zero-downtime upgrades, it's necessary to support a state where all compute nodes using a particular store are in a mixture of two adjacent phases. E.g., when we move from 1 to 2, there will be a period of time in which some nodes are still in phase 1, and some are in phase 2. However, we will avoid ever being in three phases simultaneously. For example, we will wait until all compute nodes have completed their move to state 2 before moving any into state 3.\\nThe following sections describe the behaviour required in each of the v3 states to support transition. (There's nothing to document here for phase 1, because that's how systems already using v2 today behave.)\\n### Phase 2: using v3 libraries, operating in v2 mode\\nA node in this phase has upgraded to v3 libraries, but is using the transition support and is essentially operating in v2 mode. It will never create new v3 configuration. New tenants continue to be onboarded in the same way as with v2 libraries\u2014the application does not pre-create containers, and expects the tenancy library to create them on demand as required. This gives applications a low-impact way in which to upgrade to v3 libraries without changing any behaviour, and also opens the path to migration towards the new style of operation.\\nThe one difference in behaviour (the reason we describe this as \"mostly\" v2 mode above) is that if v3 configuration is present for a particular configuration key, it has the following effects:\\n* the application will use the v3 configuration and will not even look to see if v2 configuration is present\\n* the application will presume that all relevant containers for this configuration have already been created, and will not attempt to create anything on demand\\nThis is necessary to support the case where all nodes have completed their transition to phase 2 (so none is in phase 1), and some have have moved to phase 3. Nodes that are still in phase 2 at this point need to be able to cope with the possibility that some clients have been onboarded by a phase 3 node, and so there will be only v3 configuration available. (We do not expect both v2 and v3 configuration to be present for any particular container at this point, because migration of tenants onboarded the v2 way into v3 configuration does not start until all nodes have reached phase 3.)\\nTo configure a node to run in this mode, use storage through a suitable transitional interface (e.g., `IBlobContainerSourceWithTenantLegacyTransition`). The application must provide two configuration keys: one for v2 configuration and one for v3 configuration. The transitional adapter will never create v3 configuration, but it will look for it, and only looks for v2 configuration when no v3 configuration is present.\\n### Phase 3: v3 libraries, operating in v3 mode, falling back to v2 as necessary\\nA node in this phase is using the v3 libraries. When onboarding new tenants, it pre-creates all necessary containers, and stores v3 config, but it still uses the transition support so that in cases where existing tenants have only v2 configuration available, it can fall back to the old behaviour.\\nThe only difference between phase 2 and phase 3 is how the application onboards new tenants. Both phases use the transitional adapter in exactly the same way.\\n### Configuration migration\\nOnce all nodes are in phase 3, a tool can be run to upgrade all v2 configuration to v3. Some aspects of this tooling are necessarily application-specific: only the application can know how to discover all of its tenants, and only the application can know what configuration it is storing, and under which keys.\\n","MatchedIds":[1666,359,317,621,200],"Predicted":"## Decision\nTo support a smooth migration path from `Corvus.Tenancy` v2 to v3 without disrupting existing deployments, we have made the following decisions:\n\n1. **Backward Compatibility Mode:** Introduce a backward compatibility mode in `Corvus.Tenancy` v3 that mimics the v2 behavior of automatic storage container creation. This mode will be enabled by default and can be toggled off once all tenants have been fully onboarded to v3.\n\n2. **Mixed Version Support:** Ensure that `Corvus.Tenancy` v3 components can interoperate seamlessly with v2 components. This means that during the migration period, both v2 and v3 components can coexist within the same compute farm without causing disruptions.\n\n3. **Migration Script:** Provide a dedicated migration script that can be run to audit existing tenants and create any missing storage containers before fully switching over to v3. This script will ensure that all dependencies are resolved and that no tenant is left in an inconsistent state.\n\n4. **Graceful Degradation:** Implement a mechanism for logging and alerting if a v3 component encounters a tenant with missing containers. This will help in identifying and rectifying any discrepancies during the migration process.\n\n5. **Detailed Documentation:** Offer comprehensive documentation outlining the steps for migration, potential pitfalls, and recommended best practices. This documentation will include code snippets and examples to guide developers through the transition process.\n\n6. **Phased Rollout:** Recommend a phased rollout strategy where a subset of the application\u2019s infrastructure is migrated first, monitored closely, and then gradually expanded to the rest of the deployment. This allows for identifying and mitigating issues on a smaller scale before widespread adoption.\n\n7. **Support and Monitoring:** Provide tools for real-time monitoring and support during the migration. This will include dashboards to track the migration progress and to provide insights into any issues that need immediate attention.\n\nBy following these steps, applications currently using `Corvus.Tenancy` v2 can be migrated to v3 in a controlled, consistent manner while minimizing any service disruptions or operational overhead.","GenTime":"2024-08-20 00:10:34"}
{"File Name":"registraties\/002-use-styled-components.md","Context":"## Context\\nSASS is used in most of the [Amsterdam](https:\/\/github.com\/Amsterdam) projects. The way SASS is compiled by Webpack, however, has all defined style rules affect the global scope. This can lead to unwanted and unexpected side-effect and will increase maintenance, because each developer working on the project needs to be aware of the rules that have been defined and are scattered throughout the code base. Also, using SASS requires a naming convention to prevent specifity problems.\\nAn alternative to global scope CSS is [styled components](https:\/\/www.styled-components.com\/). This approach is also used by the [Amsterdam reusable component library](https:\/\/amsterdam.github.io\/amsterdam-styled-components), it makes sense to also apply it to projects (like this one) that make use of components from that library.\\n","Decision":"SASS will not be used in the Registraties project. Instead, styled components will be used.\\n","tokens":174,"id":1125,"text":"## Context\\nSASS is used in most of the [Amsterdam](https:\/\/github.com\/Amsterdam) projects. The way SASS is compiled by Webpack, however, has all defined style rules affect the global scope. This can lead to unwanted and unexpected side-effect and will increase maintenance, because each developer working on the project needs to be aware of the rules that have been defined and are scattered throughout the code base. Also, using SASS requires a naming convention to prevent specifity problems.\\nAn alternative to global scope CSS is [styled components](https:\/\/www.styled-components.com\/). This approach is also used by the [Amsterdam reusable component library](https:\/\/amsterdam.github.io\/amsterdam-styled-components), it makes sense to also apply it to projects (like this one) that make use of components from that library.\\n\n\n##Decision\nSASS will not be used in the Registraties project. Instead, styled components will be used.\\n","MatchedIds":[157,2655,783,1992,5090],"Predicted":"## Decision\nWe will transition from using SASS to styled-components for the following reasons:\n\n1. **Avoid Global Scope Pollution**: Styled-components help in encapsulating styles, preventing any unwanted side-effects and clashes that usually occur due to global scope in SASS.\n\n2. **Consistency Across Projects**: Since the Amsterdam reusable component library already utilizes styled-components, adopting the same approach will ensure better consistency and integration within our projects.\n\n3. **Scalability and Maintainability**: Encapsulated styles help in maintaining and scaling the project more efficiently by avoiding specificity wars and making the styles easier to manage and update.\n\n### Implementation Plan\n- **Phase 1**: Gradually introduce styled-components in new components while maintaining existing SASS styles for current components.\n- **Phase 2**: Start converting existing components' styles from SASS to styled-components. \n- **Phase 3**: Phase out SASS completely from the project.\n\n### Positive Consequences\n- Improved scoped styles, reducing unexpected style override.\n- Better collaboration by adhering to a common styling methodology already in use within the Amsterdam component library.\n\n### Negative Consequences\n- Initial overhead of refactoring existing SASS styles to styled-components.\n- Learning curve for team members unfamiliar with styled-components.\n\nBy adopting styled-components, we aim to enhance the maintainability, consistency, and scalability of the project while aligning with the styling practices used in the Amsterdam reusable component library.","GenTime":"2024-08-20 00:10:40"}
{"File Name":"taxonomy-manager\/001-graph-database.md","Context":"## Context and Problem Statement\\nPersistence and retrieval of SKOS taxonomies require a storage layer that supports storing rich, free-form linked data.\\nSuch a data model could be represented in a traditional RDBMS, however, doing so would require a specialized serialization and deserialization implementation whereas graph databases can typically store RDF natively.\\n## Decision Drivers <!-- optional -->\\n* High availability\/Fault tolerance\\n* Learning curve\\n* Maintenance overhead\\n* Vendor lock-in\\n","Decision":"* High availability\/Fault tolerance\\n* Learning curve\\n* Maintenance overhead\\n* Vendor lock-in\\nChosen option: Apache Jena with underlying PostgreSQL persistence store and a custom persistence layer, because it provides a highly available database persistence that is widely available as a managed service. We opted against using Apache Jena's SDB to achieve this as it has been in a state of \"maintenance only\" since June 2013.\\n### Positive Consequences\\n* PostgreSQL can be clustered, making it highly available\\n* PostgreSQL is broadly available as a managed service\\n* We can utilise the same PostgreSQL database for storing additional data (e.g. users, roles, etc)\\n### Negative Consequences\\n* We have to design our own schema and code for interacting with the PostgreSQL database\\n","tokens":95,"id":1510,"text":"## Context and Problem Statement\\nPersistence and retrieval of SKOS taxonomies require a storage layer that supports storing rich, free-form linked data.\\nSuch a data model could be represented in a traditional RDBMS, however, doing so would require a specialized serialization and deserialization implementation whereas graph databases can typically store RDF natively.\\n## Decision Drivers <!-- optional -->\\n* High availability\/Fault tolerance\\n* Learning curve\\n* Maintenance overhead\\n* Vendor lock-in\\n\n\n##Decision\n* High availability\/Fault tolerance\\n* Learning curve\\n* Maintenance overhead\\n* Vendor lock-in\\nChosen option: Apache Jena with underlying PostgreSQL persistence store and a custom persistence layer, because it provides a highly available database persistence that is widely available as a managed service. We opted against using Apache Jena's SDB to achieve this as it has been in a state of \"maintenance only\" since June 2013.\\n### Positive Consequences\\n* PostgreSQL can be clustered, making it highly available\\n* PostgreSQL is broadly available as a managed service\\n* We can utilise the same PostgreSQL database for storing additional data (e.g. users, roles, etc)\\n### Negative Consequences\\n* We have to design our own schema and code for interacting with the PostgreSQL database\\n","MatchedIds":[4747,2520,1574,1167,4909],"Predicted":"## Decision\nAfter considering the context and decision drivers, the chosen option is \"Amazon Neptune.\" \n\n### Rationales\n* **High availability\/Fault tolerance**: Amazon Neptune offers built-in high availability with automated backups, replication across multiple Availability Zones, and seamless failover capabilities.\n* **Learning curve**: Neptune supports SPARQL queries, which are standard for querying RDF data. This reduces the learning curve for teams familiar with RDF and SPARQL.\n* **Maintenance overhead**: As a managed service, Neptune reduces the maintenance overhead associated with server provisioning, configuration, and scaling.\n* **Vendor lock-in**: While using a managed service could introduce some degree of vendor lock-in, the benefits in terms of high availability and reduced maintenance make it an acceptable trade-off for our current needs.\n\n### Positive Consequences\n* **Streamlined Development**: Using Amazon Neptune will allow the team to focus more on developing application features rather than managing the database.\n* **Scalability**: The service readily supports scaling to handle increased loads and data sizes as our SKOS taxonomy grows.\n* **Reliability**: High availability and fault tolerance features of Amazon Neptune ensure minimal downtime and data loss.\n\n### Negative Consequences\n* **Vendor Lock-in**: Relying on a managed service like Amazon Neptune ties us to AWS infrastructure, which could be costly or restrictive if organizational needs change.\n* **Cost**: Managed services generally come at a premium. Budgeting will need to account for the ongoing cost of using Amazon Neptune.","GenTime":"2024-08-20 00:10:46"}
{"File Name":"inner-source_kotlin_spark_template\/20180525-gradle-kotlin.md","Context":"## Context\\nGradle builds written in Kotlin are less esoteric comparing to Groovy ones.\\nIntellij IDEA has great support for Kotlin based builds.\\n","Decision":"We will adopt Kotlin as a Gradle build's definition language.\\n","tokens":32,"id":1463,"text":"## Context\\nGradle builds written in Kotlin are less esoteric comparing to Groovy ones.\\nIntellij IDEA has great support for Kotlin based builds.\\n\n\n##Decision\nWe will adopt Kotlin as a Gradle build's definition language.\\n","MatchedIds":[159,3860,2626,872,1477],"Predicted":"## Decision\nWe will transition our Gradle build scripts from Groovy to Kotlin (gradle.kts) for the following reasons:\n1. Kotlin-based builds are less esoteric, making them easier to read and maintain.\n2. Intellij IDEA provides excellent support for Kotlin-based builds, which can improve developer productivity through better code completion, refactoring, and debugging.\n3. Aligns with our goal to have a consistent use of Kotlin across our development stack.\n\nPositive Consequences:\n* Improved readability and maintainability of the build scripts.\n* Enhanced developer experience with better tooling support in IntelliJ IDEA.\n* Consistency in the use of Kotlin, leveraging the team's familiarity with the language.\n\nNegative Consequences:\n* Initial time investment to refactor existing build scripts from Groovy to Kotlin.\n* Potential learning curve for team members unfamiliar with Kotlin-based Gradle scripts.","GenTime":"2024-08-20 00:10:50"}
{"File Name":"fare-platform\/0002-permission-crud-operations-on-records-config.md","Context":"## Context\\nInitially a record could be made by anyone, but the problem is that that record was only some metadata without file attached on it.\\n","Decision":"To avoid the situation where a record is created without file, we put `deny_all` permission in fare\/records\/config.py that denies to all the possibilities to create, update and delete this kind of records.\\n","tokens":30,"id":2215,"text":"## Context\\nInitially a record could be made by anyone, but the problem is that that record was only some metadata without file attached on it.\\n\n\n##Decision\nTo avoid the situation where a record is created without file, we put `deny_all` permission in fare\/records\/config.py that denies to all the possibilities to create, update and delete this kind of records.\\n","MatchedIds":[4385,2488,4699,4408,4722],"Predicted":"## Decision\nTo ensure that records are complete and consistent, we will implement a feature where a file attachment becomes mandatory when creating a new record. This will ensure that each record has the necessary documentation associated with it for future reference and processing. Additionally, we will introduce a validation check in the system that prevents the creation of a record if a file is not attached. This will help maintain the integrity and usefulness of the recorded information.","GenTime":"2024-08-20 00:10:52"}
{"File Name":"iampeterbanjo.com\/0012-parallel-database-tests-mongodb-server.md","Context":"## Context\\nJest tests are fast because they can be run in parallel. If we use the same database for every test, it can cause race conditions as multiple operations are performed on models and collections. There are two ways to decouple tests:\\n- [define databases in tests][defined-test-database]\\n- [randomly create databases for each test][random-test-database]\\n","Decision":"In the context of database tests, and facing the concern of race conditions then create random databases for each test. There are different approaches for setting up the [test Mongodb server][test-mongodb-server] with Jest. I prefer using the `beforeAll` and `afterAll` hooks because this is more flexible and is less coupled to Jest's idiosyncracies.\\n","tokens":80,"id":1589,"text":"## Context\\nJest tests are fast because they can be run in parallel. If we use the same database for every test, it can cause race conditions as multiple operations are performed on models and collections. There are two ways to decouple tests:\\n- [define databases in tests][defined-test-database]\\n- [randomly create databases for each test][random-test-database]\\n\n\n##Decision\nIn the context of database tests, and facing the concern of race conditions then create random databases for each test. There are different approaches for setting up the [test Mongodb server][test-mongodb-server] with Jest. I prefer using the `beforeAll` and `afterAll` hooks because this is more flexible and is less coupled to Jest's idiosyncracies.\\n","MatchedIds":[452,4223,3323,2273,4868],"Predicted":"## Decision\nWe will randomly create databases for each test. This approach ensures that tests remain isolated, preventing race conditions and potential conflicts arising from simultaneous operations on shared models and collections. This method is more robust and scalable, providing a higher level of confidence in our test results.","GenTime":"2024-08-20 00:10:56"}
{"File Name":"compliantkubernetes\/0008-use-hostnetwork-or-loadbalancer-for-ingress.md","Context":"## Context and Problem Statement\\nMany regulations require traffic to be encrypted over public Internet. Compliant Kubernetes solves this problem via an [Ingress controller](https:\/\/kubernetes.io\/docs\/concepts\/services-networking\/ingress-controllers\/) and [cert-manager](https:\/\/github.com\/jetstack\/cert-manager). As of February 2021, Compliant Kubernetes comes by default with [nginx-ingress](https:\/\/kubernetes.github.io\/ingress-nginx\/), but [Ambassador](https:\/\/www.getambassador.io\/docs\/latest\/topics\/running\/ingress-controller\/) is planned as an alternative. The question is, how does traffic arrive at the Ingress controller?\\n## Decision Drivers\\n* We want to obey the [Principle of Least Astonishment](https:\/\/en.wikipedia.org\/wiki\/Principle_of_least_astonishment).\\n* We want to cater to hybrid cloud deployments, including bare-metal ones, which might lack support for [Kubernetes-controlled load balancer](https:\/\/kubernetes.io\/docs\/concepts\/services-networking\/service\/#loadbalancer).\\n* Some deployments, e.g., Bring-Your-Own VMs, might not allow integration with the underlying load balancer.\\n* We want to keep things simple.\\n","Decision":"* We want to obey the [Principle of Least Astonishment](https:\/\/en.wikipedia.org\/wiki\/Principle_of_least_astonishment).\\n* We want to cater to hybrid cloud deployments, including bare-metal ones, which might lack support for [Kubernetes-controlled load balancer](https:\/\/kubernetes.io\/docs\/concepts\/services-networking\/service\/#loadbalancer).\\n* Some deployments, e.g., Bring-Your-Own VMs, might not allow integration with the underlying load balancer.\\n* We want to keep things simple.\\nChosen options:\\n1. Use host network if Kubernetes-controlled load balancer is unavailable or undesired. If necessary, front the worker nodes with a manual or Terraform-controlled load-balancer. This includes:\\n* Where load-balancing does not add value, e.g., if a deployment is planned to have only a single-node or single-worker for the foreseeable future: Point the DNS entry to the worker IP instead.\\n* Exoscale currently falls in this category, due to its Kubernetes integration being rather recent.\\n* SafeSpring falls in this category, since it is missing load balancers.\\n* If the cloud provider is missing a storage controller, it might be undesirable to perform integration \"just\" for load-balancing.\\n2. Use Service Type LoadBalancer when available. This includes: AWS, Azure, GCP and CityCloud.\\nAdditional considerations: This means that, generally, it will not be possible to set up the correct DNS entries until *after* we apply Compliant Kubernetes Apps. There is a risk for \"the Internet\" -- LetsEncrypt specifically -- to perform DNS lookups too soon and cause negative DNS caches with a long lifetime. Therefore, placeholder IP addresses must be used, e.g.:\\n```\\n*.$BASE_DOMAIN     60s A 203.0.113.123\\n*.ops.$BASE_DOMAIN 60s A 203.0.113.123\\n```\\n203.0.113.123 is in TEST-NET-3 and okay to use as placeholder. This approach is inspired by [kops](https:\/\/github.com\/kubernetes\/kops\/blob\/d5d08a43034dd4c7242cf1faa020cf9a8c3965e2\/upup\/pkg\/fi\/cloudup\/dns.go#L41) and should not feel astonishing.\\n### Positive Consequences\\n* We make the best of each cloud provider.\\n* Obeys principle of least astonishment.\\n* We do not add a load balancer \"just because\".\\n### Negative Consequences\\n* Complexity is a bit increased, however, this feels like essential complexity.\\n","tokens":254,"id":3104,"text":"## Context and Problem Statement\\nMany regulations require traffic to be encrypted over public Internet. Compliant Kubernetes solves this problem via an [Ingress controller](https:\/\/kubernetes.io\/docs\/concepts\/services-networking\/ingress-controllers\/) and [cert-manager](https:\/\/github.com\/jetstack\/cert-manager). As of February 2021, Compliant Kubernetes comes by default with [nginx-ingress](https:\/\/kubernetes.github.io\/ingress-nginx\/), but [Ambassador](https:\/\/www.getambassador.io\/docs\/latest\/topics\/running\/ingress-controller\/) is planned as an alternative. The question is, how does traffic arrive at the Ingress controller?\\n## Decision Drivers\\n* We want to obey the [Principle of Least Astonishment](https:\/\/en.wikipedia.org\/wiki\/Principle_of_least_astonishment).\\n* We want to cater to hybrid cloud deployments, including bare-metal ones, which might lack support for [Kubernetes-controlled load balancer](https:\/\/kubernetes.io\/docs\/concepts\/services-networking\/service\/#loadbalancer).\\n* Some deployments, e.g., Bring-Your-Own VMs, might not allow integration with the underlying load balancer.\\n* We want to keep things simple.\\n\n\n##Decision\n* We want to obey the [Principle of Least Astonishment](https:\/\/en.wikipedia.org\/wiki\/Principle_of_least_astonishment).\\n* We want to cater to hybrid cloud deployments, including bare-metal ones, which might lack support for [Kubernetes-controlled load balancer](https:\/\/kubernetes.io\/docs\/concepts\/services-networking\/service\/#loadbalancer).\\n* Some deployments, e.g., Bring-Your-Own VMs, might not allow integration with the underlying load balancer.\\n* We want to keep things simple.\\nChosen options:\\n1. Use host network if Kubernetes-controlled load balancer is unavailable or undesired. If necessary, front the worker nodes with a manual or Terraform-controlled load-balancer. This includes:\\n* Where load-balancing does not add value, e.g., if a deployment is planned to have only a single-node or single-worker for the foreseeable future: Point the DNS entry to the worker IP instead.\\n* Exoscale currently falls in this category, due to its Kubernetes integration being rather recent.\\n* SafeSpring falls in this category, since it is missing load balancers.\\n* If the cloud provider is missing a storage controller, it might be undesirable to perform integration \"just\" for load-balancing.\\n2. Use Service Type LoadBalancer when available. This includes: AWS, Azure, GCP and CityCloud.\\nAdditional considerations: This means that, generally, it will not be possible to set up the correct DNS entries until *after* we apply Compliant Kubernetes Apps. There is a risk for \"the Internet\" -- LetsEncrypt specifically -- to perform DNS lookups too soon and cause negative DNS caches with a long lifetime. Therefore, placeholder IP addresses must be used, e.g.:\\n```\\n*.$BASE_DOMAIN     60s A 203.0.113.123\\n*.ops.$BASE_DOMAIN 60s A 203.0.113.123\\n```\\n203.0.113.123 is in TEST-NET-3 and okay to use as placeholder. This approach is inspired by [kops](https:\/\/github.com\/kubernetes\/kops\/blob\/d5d08a43034dd4c7242cf1faa020cf9a8c3965e2\/upup\/pkg\/fi\/cloudup\/dns.go#L41) and should not feel astonishing.\\n### Positive Consequences\\n* We make the best of each cloud provider.\\n* Obeys principle of least astonishment.\\n* We do not add a load balancer \"just because\".\\n### Negative Consequences\\n* Complexity is a bit increased, however, this feels like essential complexity.\\n","MatchedIds":[733,620,3909,3905,583],"Predicted":"## Decision\nWe will use NodePort and HostPort services to route traffic to the Ingress controller. \n\n* **NodePort Service** will expose the Ingress controller on a static port on each node's IP, making it accessible externally on a per-node basis. This approach allows for a consistent and straightforward configuration, aligning with the Principle of Least Astonishment.\n* **HostPort Service** can be used in hybrid and bare-metal deployments where traditional load balancers are not available or not compatible. This method binds a service to a specific port on all nodes, integrating seamlessly with Bring-Your-Own VMs and other custom environments.\n\nThis decision simplifies the routing configuration and ensures compatibility across various deployment scenarios, from cloud-based to on-premises infrastructure. It avoids the complexity and limitations associated with Kubernetes-controlled load balancers in diverse environments. \n\nCert-manager will be used to manage and obtain TLS certificates, ensuring that all traffic is encrypted in accordance with regulations.","GenTime":"2024-08-20 00:11:01"}
{"File Name":"CrossyToad\/adr-0010-support-linear-and-discrete-input.md","Context":"## Context\\nWe need to support both discrete and continous input.\\nDiscrete input is the detection of an individual keypress. We need discrete input to support\\nmovement in game menus, state transitions and other \"one keypress one action\" scenarios.\\nContinous input is the detection of the state of an input on an ongoing basis. We need continous\\nimport to support linear movement in the game, notably the movement of the toad.\\nCurrently we are using SDL to retrieve discrete input and we have no mechanism for continous input.\\nOne option for implementing continous input would be to use the [SDL_GetKeyboardState](https:\/\/wiki.libsdl.org\/SDL_GetKeyboardState) function from SDL. We could call this function every frame and\\nmake the results available to every Scene for interpretation.\\nAnother option would be to consume our existing input event stream which is fed from [SDL_PollEvent](https:\/\/www.libsdl.org\/release\/SDL-1.2.15\/docs\/html\/sdlpollevent.html) and transition our own \"InputState\" based on the \"KeyPressed\" and \"KeyReleased\" events.\\nAt the moment I prefer maintaining our own state instead of letting SDL do it as we can reduce the\\nnumber of possible key combinations we want to support through the type system.\\n","Decision":"We will support continous and discrete input using the existing poll events architecture and\\nsome new state machinery.\\n","tokens":270,"id":2482,"text":"## Context\\nWe need to support both discrete and continous input.\\nDiscrete input is the detection of an individual keypress. We need discrete input to support\\nmovement in game menus, state transitions and other \"one keypress one action\" scenarios.\\nContinous input is the detection of the state of an input on an ongoing basis. We need continous\\nimport to support linear movement in the game, notably the movement of the toad.\\nCurrently we are using SDL to retrieve discrete input and we have no mechanism for continous input.\\nOne option for implementing continous input would be to use the [SDL_GetKeyboardState](https:\/\/wiki.libsdl.org\/SDL_GetKeyboardState) function from SDL. We could call this function every frame and\\nmake the results available to every Scene for interpretation.\\nAnother option would be to consume our existing input event stream which is fed from [SDL_PollEvent](https:\/\/www.libsdl.org\/release\/SDL-1.2.15\/docs\/html\/sdlpollevent.html) and transition our own \"InputState\" based on the \"KeyPressed\" and \"KeyReleased\" events.\\nAt the moment I prefer maintaining our own state instead of letting SDL do it as we can reduce the\\nnumber of possible key combinations we want to support through the type system.\\n\n\n##Decision\nWe will support continous and discrete input using the existing poll events architecture and\\nsome new state machinery.\\n","MatchedIds":[2473,873,142,4618,3187],"Predicted":"## Decision\nWe will implement continuous input by consuming our existing input event stream fed from `SDL_PollEvent` and transitioning our own \"InputState\" based on \"KeyPressed\" and \"KeyReleased\" events.\n\n### Justification\n* This approach allows us to maintain control over supported key combinations through the type system.\n* It provides flexibility and extensibility for future input system enhancements.\n* Aligns with our current implementation of discrete input, allowing for a more unified and coherent input handling mechanism.\n\n### Positive Consequences\n* We can reduce the complexity by limiting key combinations, making the system easier to understand and maintain.\n* Ensures consistency and better control over input states.\n\n### Negative Consequences\n* Additional effort is needed to implement and manage our own input state transitions.\n* Potential performance overhead compared to directly using `SDL_GetKeyboardState`.\n\nOverall, maintaining our own input state ensures greater flexibility and control, which outweighs the negatives of increased implementation effort.","GenTime":"2024-08-20 00:11:09"}
{"File Name":"iampeterbanjo.com\/0002-blog-engine.md","Context":"## Context\\nI wanted to create a website blog using [Markdown][1], a [static site generator][2] and [HapiJS][3] to serve the files. Static site generators I tried, in order, were -\\n### [11ty][4]\\nPros\\n- Easy setup\\n- Predictable static file output\\n- Relatively small codebase\\n- Supporst different template engines\\nCons\\n- The frontmatter parser [didn't work as I expected][7]\\n- Template errors were not reported and would only fail on build\\n- Needs some attention to make production ready e.g. SEO, images, favicons etc\\n### [Gatsby][5]\\nPros\\n- Modern [JAMstack][8] project with React\\n- Batteries included by default - includes helpers for favicons, image loading, SEO etc.\\n- Opportunity to use GraphQL\\nCons\\n- Doesn't output static files - [totally depends on clientside JavaScript][9]\\n- It relies on service workers to cache responses which causes flakey e2e tests and unpredictable behaviour\\n- Developing static files based on API was difficult because of caching and unexpected error handling\\n- Relatively complex project with separate build, develop steps, config, plugins etc\\n### [Hugo][6]\\nPros\\n- Fast and mature static site generator\\n- Small footprint\\nCons\\n- Written in Go - I prefer fullstack JavaScript for this site\\n","Decision":"In the context of using several static site generators each of them had with their own downsides. And facing the concern of having blog that I can easily maintain and customise I decided to build my blog with HapiJS to achieve a smaller blog footprint, predictable behaviour and an opportunity to learn something new. I accept that I'm re-inventing the wheel.\\n","tokens":300,"id":1599,"text":"## Context\\nI wanted to create a website blog using [Markdown][1], a [static site generator][2] and [HapiJS][3] to serve the files. Static site generators I tried, in order, were -\\n### [11ty][4]\\nPros\\n- Easy setup\\n- Predictable static file output\\n- Relatively small codebase\\n- Supporst different template engines\\nCons\\n- The frontmatter parser [didn't work as I expected][7]\\n- Template errors were not reported and would only fail on build\\n- Needs some attention to make production ready e.g. SEO, images, favicons etc\\n### [Gatsby][5]\\nPros\\n- Modern [JAMstack][8] project with React\\n- Batteries included by default - includes helpers for favicons, image loading, SEO etc.\\n- Opportunity to use GraphQL\\nCons\\n- Doesn't output static files - [totally depends on clientside JavaScript][9]\\n- It relies on service workers to cache responses which causes flakey e2e tests and unpredictable behaviour\\n- Developing static files based on API was difficult because of caching and unexpected error handling\\n- Relatively complex project with separate build, develop steps, config, plugins etc\\n### [Hugo][6]\\nPros\\n- Fast and mature static site generator\\n- Small footprint\\nCons\\n- Written in Go - I prefer fullstack JavaScript for this site\\n\n\n##Decision\nIn the context of using several static site generators each of them had with their own downsides. And facing the concern of having blog that I can easily maintain and customise I decided to build my blog with HapiJS to achieve a smaller blog footprint, predictable behaviour and an opportunity to learn something new. I accept that I'm re-inventing the wheel.\\n","MatchedIds":[1931,4940,1680,3586,3470],"Predicted":"## Decision\nGiven the preference for fullstack JavaScript and the desire for a static site generator with minimal codebase complexity, I have decided to use 11ty for creating the website blog. Despite some issues with the frontmatter parser and the need for additional attention to production readiness concerns like SEO and image handling, 11ty's simplicity and easy setup make it a suitable choice. These production concerns can be addressed incrementally without significantly affecting the initial implementation. HapiJS will be used to serve the generated static files, fulfilling the requirement of using a JavaScript-based server.","GenTime":"2024-08-20 00:11:12"}
{"File Name":"cljdoc\/0008-use-circleci-as-analysis-sandbox.md","Context":"## Context\\nAnalyzing untrusted Clojure code means loading it which should only be done in some kind of\\nsandboxed environment. A Docker image has been created to help with this but this still\\nrequires us to run and monitor job execution. Bad actors could still trigger many builds\\nto run Bitcoin miners and other compute-stealing stuff.\\nAlternatives to running Docker ourselves are AWS Lambda (probably similar compute-stealing\\nissues) and \"hacking\" a continous integration service to do the job for us. More detail can\\nbe found in the [notes on Isolation](https:\/\/github.com\/martinklepsch\/cljdoc\/blob\/72da65055ab94942f33fb63b29b732e81b559508\/doc\/isolation.md)\\n","Decision":"For a first version of cljdoc we will use CircleCI to run analysis for us. The result of\\nthis analysis will be made available as a build artifact which can then be laoded in\\na trusted environment to import data into Grimoire and build HTML (or other) documentation\\nfrontends.\\n","tokens":160,"id":2601,"text":"## Context\\nAnalyzing untrusted Clojure code means loading it which should only be done in some kind of\\nsandboxed environment. A Docker image has been created to help with this but this still\\nrequires us to run and monitor job execution. Bad actors could still trigger many builds\\nto run Bitcoin miners and other compute-stealing stuff.\\nAlternatives to running Docker ourselves are AWS Lambda (probably similar compute-stealing\\nissues) and \"hacking\" a continous integration service to do the job for us. More detail can\\nbe found in the [notes on Isolation](https:\/\/github.com\/martinklepsch\/cljdoc\/blob\/72da65055ab94942f33fb63b29b732e81b559508\/doc\/isolation.md)\\n\n\n##Decision\nFor a first version of cljdoc we will use CircleCI to run analysis for us. The result of\\nthis analysis will be made available as a build artifact which can then be laoded in\\na trusted environment to import data into Grimoire and build HTML (or other) documentation\\nfrontends.\\n","MatchedIds":[108,3793,4868,2616,3917],"Predicted":"## Decision\nGiven the risk of compute-stealing and the need for robust monitoring, we will implement a dedicated, sandboxed environment using Kubernetes with resource quotas and network policies. This approach allows us to tightly control resource usage and network access, mitigating the risk of malicious activities such as Bitcoin mining. Additionally, we will integrate comprehensive logging and monitoring tools to detect and respond to suspicious activities promptly. This solution provides a balance between control, security, and scalability without relying on potentially vulnerable external platforms.","GenTime":"2024-08-20 00:11:18"}
{"File Name":"operational-data-hub\/0003-repo-directory-structure.md","Context":"## Context\\nWe feel the need to standardize the directory structure for github repos.\\n","Decision":"### 1. General rules\\n* Directory names must always:\\n* Always use underscores (\u201c\\_\u201d) in names for folders and files to split between words.\\n* use lowercase.\\n### 2. Cloud functions\\n* The root directory for cloud functions is 'functions\/'.\\n* A '.gcloudignore' file is provided to limit the uploaded files.\\n### 3. API\\n* the root directory for the API is 'app\/'.\\n* For OpenApi APIs see the separate ADR for working with OpenApi generated code.\\n### 4. Frontend\\n* the root directory for the APP is 'app\/'.\\n* The directory structure below the app directory is based on the recommendations of the framework.\\n* For the angular applications the recommendations can be found at [Angular Workspace and project file structure](https:\/\/angular.io\/guide\/file-structure).\\n### 5. Tests\\n* Files needed for testing are stored in the 'test' directory.\\n### 6. SAST\\n* Files needed for the SAST scan are stored in the 'sast-config' directory.\\n","tokens":18,"id":2703,"text":"## Context\\nWe feel the need to standardize the directory structure for github repos.\\n\n\n##Decision\n### 1. General rules\\n* Directory names must always:\\n* Always use underscores (\u201c\\_\u201d) in names for folders and files to split between words.\\n* use lowercase.\\n### 2. Cloud functions\\n* The root directory for cloud functions is 'functions\/'.\\n* A '.gcloudignore' file is provided to limit the uploaded files.\\n### 3. API\\n* the root directory for the API is 'app\/'.\\n* For OpenApi APIs see the separate ADR for working with OpenApi generated code.\\n### 4. Frontend\\n* the root directory for the APP is 'app\/'.\\n* The directory structure below the app directory is based on the recommendations of the framework.\\n* For the angular applications the recommendations can be found at [Angular Workspace and project file structure](https:\/\/angular.io\/guide\/file-structure).\\n### 5. Tests\\n* Files needed for testing are stored in the 'test' directory.\\n### 6. SAST\\n* Files needed for the SAST scan are stored in the 'sast-config' directory.\\n","MatchedIds":[2725,4766,1996,398,400],"Predicted":"## Decision\nWe will adopt a standardized directory structure for all GitHub repositories to improve the consistency, maintainability, and navigability of our codebases.\n\n### Standard Directory Structure\n\n1. **Root Directory**\n   - `README.md`: A comprehensive overview of the repository.\n   - `LICENSE`: License for the repository.\n   - `CONTRIBUTING.md`: Guidelines for contributing to the repository.\n   - `docs\/`: Documentation files.\n   - `src\/`: Source code of the project.\n   - `test\/` or `tests\/`: Test files and test-related utilities.\n   - `.gitignore`: Git ignore file.\n   - `CHANGELOG.md`: List of changes and updates.\n\n2. **Source Directory (`src\/`)**\n   - `main\/`: All main application code.\n   - `assets\/`: Static files like images, fonts, or CSS\/SCSS.\n   - `components\/`: Reusable components.\n   - `services\/`: Service layer, API calls, and business logic.\n   - `utils\/`: Utility functions, helpers.\n   - `config\/`: Configuration files and environment settings.\n   - `styles\/`: Global styles.\n   - Additional directories as required by the specific project.\n\n3. **Test Directory (`test\/` or `tests\/`)**\n   - `unit\/`: Unit tests.\n   - `integration\/`: Integration tests.\n   - `e2e\/`: End-to-end tests.\n   - Additional test-related files.\n\n### Positive Consequences\n* Enhanced consistency across all repositories making it easier for developers to switch between projects.\n* Simplified navigation within the repository, reducing the learning curve for new developers.\n* Improved automation, as tools and scripts can rely on a predictable structure.\n* Better organization, making it easier to maintain and extend the project.\n* Promotes best practices and reduces redundancy.\n\nThis structure should be followed for all new repositories, and existing repositories should be refactored to adhere to this guideline wherever feasible.","GenTime":"2024-08-20 00:11:38"}
{"File Name":"adrflow\/5-Utility_Context_Lazily_Initialized.md","Context":"# 5 Utility Context Lazily Initialized\\n## Context\\nThe [synchronous utils](..\/..\/src\/adr_util_sync.js) are intended to be used by commands to work on ADRs, with the given context of the project, e.g. the [adr marker file](1-ADR_directory_identified_by_marker_file.md).\\nSince the context is mandatory (the ADR directory, existing ADRs), it is initialized at the beginning, in the object's constructor.\\nSeveral commands (scripts) already include that script, and initialize the object on the script loading, as part of the `require` statement.\\nThis resulted in loading the context whenever a command is loaded.\\nThis might have performance implications, but the more immediate functional lacuna is that this then assumes that the context *can* be loaded every time.\\nOf course, this assumption breaks - when running the `init` command, where by definition there is no context to load - the `init` command creates that.\\nThis resulted in trying to run the `init` command, on an existing directory, in an error.\\n","Decision":"The ADR project context will be loaded lazily, only when needed.\\nThis assumes the `init` command doesn't require it, and hence will not invoke any commands that require that context (e.g. the existing files).\\nSo any command that is loaded can still create the context object and create an instance of, but the actual context will be loaded easily.\\nThis assumption seems safe enough since `init` command indeed creates the context (the `.adr` file).\\nAnd this will probably hold for similar commands.\\nCommands that will require the ADR context, will continue to use it as today, and it will be loaded lazily.\\nWe need of course to make sure that the context isn't created\/calculated every time it is called; e.g. searching for the ADR dir every time one of the utility functions is called. In other words - memoize the context-creating functions.\\n","tokens":227,"id":3635,"text":"# 5 Utility Context Lazily Initialized\\n## Context\\nThe [synchronous utils](..\/..\/src\/adr_util_sync.js) are intended to be used by commands to work on ADRs, with the given context of the project, e.g. the [adr marker file](1-ADR_directory_identified_by_marker_file.md).\\nSince the context is mandatory (the ADR directory, existing ADRs), it is initialized at the beginning, in the object's constructor.\\nSeveral commands (scripts) already include that script, and initialize the object on the script loading, as part of the `require` statement.\\nThis resulted in loading the context whenever a command is loaded.\\nThis might have performance implications, but the more immediate functional lacuna is that this then assumes that the context *can* be loaded every time.\\nOf course, this assumption breaks - when running the `init` command, where by definition there is no context to load - the `init` command creates that.\\nThis resulted in trying to run the `init` command, on an existing directory, in an error.\\n\n\n##Decision\nThe ADR project context will be loaded lazily, only when needed.\\nThis assumes the `init` command doesn't require it, and hence will not invoke any commands that require that context (e.g. the existing files).\\nSo any command that is loaded can still create the context object and create an instance of, but the actual context will be loaded easily.\\nThis assumption seems safe enough since `init` command indeed creates the context (the `.adr` file).\\nAnd this will probably hold for similar commands.\\nCommands that will require the ADR context, will continue to use it as today, and it will be loaded lazily.\\nWe need of course to make sure that the context isn't created\/calculated every time it is called; e.g. searching for the ADR dir every time one of the utility functions is called. In other words - memoize the context-creating functions.\\n","MatchedIds":[3637,2914,4592,3682,85],"Predicted":"## Decision\nRefactor the initialization of the context in the [synchronous utils](..\/..\/src\/adr_util_sync.js) to be lazily loaded instead of being initialized at the object's construction. \n\n1. **Lazy Initialization**: Modify the synchronous utils to lazily initialize the context when it is first required rather than at the time of object creation.\n    - This involves checking if the context is already initialized at every method that requires it, and if not, then initializing it.\n\n2. **Context Initialization Method**: Introduce a dedicated method that can explicitly initialize the context. This method can be called from commands that know the context should be loaded.\n\n3. **Graceful Handling in Commands**: Update all existing commands to use the lazy-initialized version of the synchronous utils.\n    - For commands where context is not guaranteed to exist (e.g., the `init` command), ensure that they either do not trigger context initialization or handle the absence of context gracefully.\n\n4. **Error Handling**: Implement appropriate error handling for scenarios where context cannot be loaded and provide meaningful error messages.\n\nBy implementing lazy initialization, the context will only be loaded when necessary, thereby avoiding the issue of attempting to load the context in commands where it is not applicable, such as during the `init` command. This will both improve performance and eliminate functional assumptions about the availability of context.","GenTime":"2024-08-20 00:11:50"}
{"File Name":"openfido-workflow-service\/0002-pipelines.md","Context":"## Context\\nA couple of client projects need infrastructure to process GridLabD jobs.\\nThe goal would be that other projects could either include this project as its\\nown service in their infrastructure, or incorporate it directly into their\\nFlask-based project.\\nThese kinds of jobs are long running, and produce artifacts that each project\\nmay store in different ways (but primarily S3).\\n","Decision":"Create a Flask Rest service, coupled with Celery for job processing.\\nOrganize the database logic into a simplified CQRS-inspired style code structure:\\n* app\/models.py contain all sql models.\\n* app\/services.py contain all db commands that modify database state.\\n* app\/queries.py contain all db queries to the database.\\nUse the structure of the [presence-account-service](https:\/\/github.com\/PresencePG\/presence-account-service) project as a reference.\\n","tokens":78,"id":4388,"text":"## Context\\nA couple of client projects need infrastructure to process GridLabD jobs.\\nThe goal would be that other projects could either include this project as its\\nown service in their infrastructure, or incorporate it directly into their\\nFlask-based project.\\nThese kinds of jobs are long running, and produce artifacts that each project\\nmay store in different ways (but primarily S3).\\n\n\n##Decision\nCreate a Flask Rest service, coupled with Celery for job processing.\\nOrganize the database logic into a simplified CQRS-inspired style code structure:\\n* app\/models.py contain all sql models.\\n* app\/services.py contain all db commands that modify database state.\\n* app\/queries.py contain all db queries to the database.\\nUse the structure of the [presence-account-service](https:\/\/github.com\/PresencePG\/presence-account-service) project as a reference.\\n","MatchedIds":[4821,771,2062,1681,745],"Predicted":"## Decision\nWe will design a modular infrastructure for processing GridLabD jobs using a microservices approach that allows for flexibility and reusability across different projects. The architecture will consist of the following components:\n\n1. **Job Processing Service**: \n   - This will be a dedicated service responsible for managing and processing GridLabD jobs.\n   - Implement the service using Docker containers managed by Kubernetes to ensure scalability and ease of deployment.\n   - Use RabbitMQ or another message broker to handle job queues, ensuring that long-running jobs are managed efficiently.\n\n2. **Storage Integration**:\n   - Design the service to be agnostic of the storage backend so it can save artifacts to various storage providers. \n   - Provide a built-in integration for AWS S3 as the primary storage solution, using boto3 for interfacing with S3.\n\n3. **Flask Integration**:\n   - Create a Flask extension to facilitate the integration of the GridLabD job processing service into existing Flask-based projects.\n   - Provide RESTful APIs to interact with the job processing service for submitting jobs, checking job status, and retrieving artifacts.\n\n4. **Modularity and Reusability**:\n   - Ensure the job processing logic is encapsulated within a reusable library that can be imported into different projects.\n   - Offer the functionality as a standalone service that can be deployed independently and accessed via APIs.\n\n5. **Configuration Management**:\n   - Use environment variables and configuration files to customize the service for different client needs.\n   - Document the configuration parameters needed for setting up the service for various environments.\n\nBy adopting this approach, we aim to create an infrastructure that can be easily integrated into various client projects while maintaining flexibility and scalability.","GenTime":"2024-08-20 00:11:59"}
{"File Name":"claim-additional-payments-for-teaching\/0009-capture-teacher-reference-number.md","Context":"## Context\\nA claimant\u2019s eligibility is, in part, determined by their qualifications. We\\nwant to be able to validate that a claimant\u2019s qualifications match those of the\\neligibility criteria.\\n","Decision":"To aid DfE in the process of validating a claimant\u2019s qualifications, we will\\ncollect the claimant\u2019s \u2018Teacher Reference Number\u2019 or TRN.\\nWith the TRN, DfE can use the Database of Qualified Teachers\\n([DQT](https:\/\/teacherservices.education.gov.uk\/SelfService\/Login)) to validate\\na claimant\u2019s qualifications.\\n","tokens":42,"id":2098,"text":"## Context\\nA claimant\u2019s eligibility is, in part, determined by their qualifications. We\\nwant to be able to validate that a claimant\u2019s qualifications match those of the\\neligibility criteria.\\n\n\n##Decision\nTo aid DfE in the process of validating a claimant\u2019s qualifications, we will\\ncollect the claimant\u2019s \u2018Teacher Reference Number\u2019 or TRN.\\nWith the TRN, DfE can use the Database of Qualified Teachers\\n([DQT](https:\/\/teacherservices.education.gov.uk\/SelfService\/Login)) to validate\\na claimant\u2019s qualifications.\\n","MatchedIds":[5009,4442,581,4454,3853],"Predicted":"## Decision\nWe will implement a Qualification Validation Service (QVS) to automate the process of verifying a claimant's qualifications against the eligibility criteria.\n\n### Validation Approach\n1. **Data Model Alignment**: \n   - Develop a standardized data model for qualifications and eligibility criteria to ensure consistency in validation.\n   - Use well-defined schemas to represent different types of qualifications and their corresponding eligibility criteria.\n\n2. **Integration with External Systems**:\n   - Integrate with authoritative qualification databases and registries to fetch claimant qualification details where applicable.\n   - Use APIs to connect with educational and professional institutions for dynamic qualification verification.\n\n3. **Validation Logic**:\n   - Implement business rules and logic to compare claimant qualifications with eligibility criteria.\n   - Use a rule engine to facilitate easy updates to the validation logic as eligibility criteria evolve.\n\n4. **Exception Handling**:\n   - Develop mechanisms for manual review and escalation in cases where automated validation cannot definitively determine eligibility.\n\n5. **Security and Compliance**:\n   - Ensure data privacy and security by encrypting sensitive information and adhering to relevant legal and regulatory requirements (e.g., GDPR).\n\n6. **User Feedback**:\n   - Provide clear and actionable feedback to claimants on the status of their qualification validation, including reasons for any rejections.\n\n7. **Logging and Monitoring**:\n   - Implement comprehensive logging and monitoring to track the performance and accuracy of the QVS.\n   - Regularly review and audit the validation process to identify and rectify any inconsistencies.\n\n### Deployment\n- The Qualification Validation Service will initially be deployed to our cloud infrastructure, leveraging scalable services to handle peak loads efficiently.\n  \n### Documentation and Training\n- Provide detailed documentation and training for internal users to ensure effective usage and administration of the QVS. \n\nBy implementing the Qualification Validation Service, we aim to streamline the eligibility determination process, improve accuracy, and enhance the user experience for claimants.","GenTime":"2024-08-20 00:12:07"}
{"File Name":"ftd-scratch3-offline\/0009-use-jackson-to-read-json-files.md","Context":"## Context\\nScratch save files are zip files that contain the actual code in a `project.json` file.\\nTo read this file we need a deserialization library.\\nJackson, GSON and org.json are common libraries.\\nJackson seems to offer the most features and the authors already have used it.\\nGSON offers many features Jackson also features.\\norg.json seems to be only a json parser and the use has to create the resulting object themselves, while the other libraries also feature object mapping.\\n","Decision":"We will use [Jackson](https:\/\/github.com\/FasterXML\/jackson-databind\/).\\n","tokens":104,"id":2625,"text":"## Context\\nScratch save files are zip files that contain the actual code in a `project.json` file.\\nTo read this file we need a deserialization library.\\nJackson, GSON and org.json are common libraries.\\nJackson seems to offer the most features and the authors already have used it.\\nGSON offers many features Jackson also features.\\norg.json seems to be only a json parser and the use has to create the resulting object themselves, while the other libraries also feature object mapping.\\n\n\n##Decision\nWe will use [Jackson](https:\/\/github.com\/FasterXML\/jackson-databind\/).\\n","MatchedIds":[2585,399,282,2274,3071],"Predicted":"## Decision\nWe will use Jackson as the deserialization library for reading the `project.json` file within Scratch save files. Jackson offers comprehensive features and support, including object mapping, which simplifies the process of converting JSON data into Java objects. Additionally, the authors of the project already have experience using Jackson, reducing the learning curve and potential for errors. While GSON offers similar features, Jackson's extensive capabilities and familiarity with the authors make it the optimal choice for this task. The org.json library, being only a JSON parser without object mapping, does not meet our needs as efficiently as Jackson.","GenTime":"2024-08-20 00:12:12"}
{"File Name":"rfcs\/0000-template.md","Context":"## Context\\n[context]: #context\\n> This section describes the forces at play, including technological, political, social, and project local. These forces are probably in tension, and should be called out as such. The language in this section is value-neutral. It is simply describing facts.\\n","Decision":"[decision]: #decision\\n> This section describes our response to these forces. It is stated in full sentences, with active voice. \"We will ...\"\\n","tokens":60,"id":1879,"text":"## Context\\n[context]: #context\\n> This section describes the forces at play, including technological, political, social, and project local. These forces are probably in tension, and should be called out as such. The language in this section is value-neutral. It is simply describing facts.\\n\n\n##Decision\n[decision]: #decision\\n> This section describes our response to these forces. It is stated in full sentences, with active voice. \"We will ...\"\\n","MatchedIds":[1665,3820,1145,4616,3546],"Predicted":"## Context\nThe company is undergoing a major digital transformation focusing on enhancing customer experience and operational efficiency. The CTO has mandated that all new developments must align with cloud-native principles and leverage containerization for deployment. The existing system is a monolithic application that poses challenges in scalability and maintainability. The development team is well-versed in Java and has some experience with microservices architecture but limited exposure to container orchestration tools like Kubernetes. The Sales and Marketing departments are advocating for faster release cycles to stay competitive in the market. There is a need to ensure that the new technological stack integrates well with existing CI\/CD pipelines and complies with the organization's security policies.\n\n## Decision\nWe will adopt a microservices architecture using a modern Java stack while leveraging containerization for deployment.\n\n### UI Technology Stack:\n- React\n- Bootstrap\n- Cypress\n- Jest\n\n### Server Side Technology Stack:\n- Java 11+\n- Spring Boot 2.4+\n- RESTful services\n- PostgreSQL\n- Spring Data JPA (Hibernate)\n- JUnit 5\n- Mockito\n\n### Development Technology Stack:\n- Local Development\n- SCM: GIT\n- IntelliJ IDEA\n- Visual Studio Code\n- Maven\n- Create React App\n- Jenkins (CI\/CD)\n- Jira\n- SonarQube\n- Docker for containerization\n- Kubernetes for container orchestration\n\n### Deployment:\n- Amazon Web Services (AWS) for cloud infrastructure\n- EKS (Elastic Kubernetes Service) for Kubernetes management\n\nBy adopting this technology stack, we ensure scalability, faster release cycles, and maintainability, aligning with the organization's strategic goals while considering the team's current skill set and the need for seamless CI\/CD integration.","GenTime":"2024-08-20 00:12:19"}
{"File Name":"operational-data-hub\/0012-repository-organisation-rules.md","Context":"## Context\\nWe feel the need to define rules to select the correct GitHub organisation for source repositories.\\n","Decision":"We identify rules for the GitHub organisation, the source repository name format and basic source code guidelines.\\n### VWT Digital\\nThis is the main organisation for all Operational Data Hub (ODH) specific source code. When creating a new repository,\\nit **has to pass** one of the following applications:\\n- An ODH-specific solution;\\n- A solution that transcends multiple solution domains (see [solution domains](https:\/\/recognize.atlassian.net\/wiki\/spaces\/DAT\/pages\/1304166628\/Cloud+naming+convention#Cloudnamingconvention-Domains)).\\nIn addition, these repositories **may not** contain:\\n- Any configuration;\\n- Any solution business logic.\\nWhen creating a name for the repository, it **has to start** with either `odh-<repo_name>` or `dat-<repo_name>`:\\n- `odh-`: When the repository contains source code specifically made for the Operational Data Hub;\\n- `dat-`: When the repository contains source code created for generic usage within the Digital Ambition Team domain.\\nSee our [repo naming convention](0002-repo-naming-conventions.md#4-normal-vwt-dat-repositories) for a more elaborate explanation.\\n### VWT Digital Solutions\\nThis is the main organisation for all Solution-specific source code. When creating a new repository, it **has to pass** the following application:\\n- A specific application for a domain solution, only used by one domain (see [solution domains](https:\/\/recognize.atlassian.net\/wiki\/spaces\/DAT\/pages\/1304166628\/Cloud+naming+convention#Cloudnamingconvention-Domains)).\\nIn addition, these repositories **may not** contain:\\n- Any configuration.\\nWhen creating a name for the repository, it has to start with the abbreviation of the solutions it is a part of.\\nSee our [repo naming convention](0002-repo-naming-conventions.md#4-normal-vwt-dat-repositories) for a more elaborate explanation.\\n### VWT Digital Configuration\\nThis is the main organisation for all configuration code. When creating a new repository, it **has to pass** the following application:\\n- Google Cloud Platform (GCP) project-specific configuration code that is for private usage only.\\nWhen creating a name for the repository, it has to contain the GCP project they are connected to minus the customer,\\nenvironment and location. Furthermore, it has to end with `-config`.\\nSee our [repo naming convention](0002-repo-naming-conventions.md#4-normal-vwt-dat-repositories) for a more elaborate explanation.\\n","tokens":21,"id":2725,"text":"## Context\\nWe feel the need to define rules to select the correct GitHub organisation for source repositories.\\n\n\n##Decision\nWe identify rules for the GitHub organisation, the source repository name format and basic source code guidelines.\\n### VWT Digital\\nThis is the main organisation for all Operational Data Hub (ODH) specific source code. When creating a new repository,\\nit **has to pass** one of the following applications:\\n- An ODH-specific solution;\\n- A solution that transcends multiple solution domains (see [solution domains](https:\/\/recognize.atlassian.net\/wiki\/spaces\/DAT\/pages\/1304166628\/Cloud+naming+convention#Cloudnamingconvention-Domains)).\\nIn addition, these repositories **may not** contain:\\n- Any configuration;\\n- Any solution business logic.\\nWhen creating a name for the repository, it **has to start** with either `odh-<repo_name>` or `dat-<repo_name>`:\\n- `odh-`: When the repository contains source code specifically made for the Operational Data Hub;\\n- `dat-`: When the repository contains source code created for generic usage within the Digital Ambition Team domain.\\nSee our [repo naming convention](0002-repo-naming-conventions.md#4-normal-vwt-dat-repositories) for a more elaborate explanation.\\n### VWT Digital Solutions\\nThis is the main organisation for all Solution-specific source code. When creating a new repository, it **has to pass** the following application:\\n- A specific application for a domain solution, only used by one domain (see [solution domains](https:\/\/recognize.atlassian.net\/wiki\/spaces\/DAT\/pages\/1304166628\/Cloud+naming+convention#Cloudnamingconvention-Domains)).\\nIn addition, these repositories **may not** contain:\\n- Any configuration.\\nWhen creating a name for the repository, it has to start with the abbreviation of the solutions it is a part of.\\nSee our [repo naming convention](0002-repo-naming-conventions.md#4-normal-vwt-dat-repositories) for a more elaborate explanation.\\n### VWT Digital Configuration\\nThis is the main organisation for all configuration code. When creating a new repository, it **has to pass** the following application:\\n- Google Cloud Platform (GCP) project-specific configuration code that is for private usage only.\\nWhen creating a name for the repository, it has to contain the GCP project they are connected to minus the customer,\\nenvironment and location. Furthermore, it has to end with `-config`.\\nSee our [repo naming convention](0002-repo-naming-conventions.md#4-normal-vwt-dat-repositories) for a more elaborate explanation.\\n","MatchedIds":[2703,3653,3554,74,3853],"Predicted":"## Decision\n\n### 1. Organization Ownership and Domain Focus\n* Repositories should be placed in an organization that aligns with the domain or focus area of the project.\n* If a project is primarily related to a specific product or service, it should be housed within the organization dedicated to that product or service.\n\n### 2. Separation of Concerns\n* Core infrastructure and platform repositories should reside in the \"infrastructure\" organization.\n* Product-specific repositories should reside in the \"products\" organization.\n* Experimental or research-based projects should reside in the \"labs\" organization.\n\n### 3. Access Control and Permissions\n* Choose the organization where the primary stakeholders and contributors have the appropriate level of access and permissions.\n* Ensure sensitive repositories are placed in an organization with stricter access controls.\n\n### 4. Reusability and Collaboration\n* If the repository is intended to be reused or collaborated on by multiple teams, place it in a \"shared\" or \"collaboration\" organization.\n* Public-facing open-source projects should reside in the \"public\" organization to encourage community contributions.\n\n### 5. Compliance and Regulations\n* For repositories that require adherence to specific compliance standards or regulatory requirements, choose an organization that enforces these standards.\n\n### 6. Naming and Documentation\n* Ensure the chosen organization follows a consistent naming convention and structure.\n* Document the rationale for the organization's selection in the repository's README or a dedicated ADR file for future reference.\n\n### Example Scenarios\n* A new microservice related to customer data should reside in the \"products\" organization under a \"customer-data-services\" repository.\n* A project focused on infrastructure as code (IaC) should be placed in the \"infrastructure\" organization.\n* An experimental AI research project should be housed in the \"labs\" organization.\n\nBy following these rules, the correct organization for each repository can be selected, ensuring clear ownership, proper access control, and alignment with domain-specific goals.","GenTime":"2024-08-20 00:12:26"}
{"File Name":"architecture\/0003-monitor-condition-and-data-selectors.md","Context":"## Context\\nA lot of Home Assistant integrations use config options like `CONF_MONITORED_CONDITIONS` to allow the user to select which data the integration should expose from the data. This means that the user needs to know what the different data points mean while setting up the integration. While configuring its Lovelace UI, the user has the option to include the entity or not. This means that we allow the user to pick twice.\\n","Decision":"Integrations should expose all available data to the backend if that data is fetched in a single API request.\\nIntegrations should only include selector logic if it make sense in the context of interface, like if it would require extra requests. User should not have to read the available documentation and API descriptions to find out which data they want have.\\n```\\nLayer model:\\nFetch            Manage           View\\n-------------      ---------      ------------\\n| Interface | ---> | Core  | ---> | Frontend |\\n-------------      ---------      ------------\\n```\\nIntegrations can set the `entity_registry_enabled_default` property on their entity objects to instruct Home Assistant to disable certain entities by default ([docs](https:\/\/developers.home-assistant.io\/docs\/entity_registry_disabled_by\/#integrations-setting-default-value-of-disabled_by-for-new-entity-registry-entries)).\\n","tokens":90,"id":1417,"text":"## Context\\nA lot of Home Assistant integrations use config options like `CONF_MONITORED_CONDITIONS` to allow the user to select which data the integration should expose from the data. This means that the user needs to know what the different data points mean while setting up the integration. While configuring its Lovelace UI, the user has the option to include the entity or not. This means that we allow the user to pick twice.\\n\n\n##Decision\nIntegrations should expose all available data to the backend if that data is fetched in a single API request.\\nIntegrations should only include selector logic if it make sense in the context of interface, like if it would require extra requests. User should not have to read the available documentation and API descriptions to find out which data they want have.\\n```\\nLayer model:\\nFetch            Manage           View\\n-------------      ---------      ------------\\n| Interface | ---> | Core  | ---> | Frontend |\\n-------------      ---------      ------------\\n```\\nIntegrations can set the `entity_registry_enabled_default` property on their entity objects to instruct Home Assistant to disable certain entities by default ([docs](https:\/\/developers.home-assistant.io\/docs\/entity_registry_disabled_by\/#integrations-setting-default-value-of-disabled_by-for-new-entity-registry-entries)).\\n","MatchedIds":[3815,5118,4261,1430,598],"Predicted":"## Decision\nTo streamline the user experience and reduce redundancy in configuration, we will eliminate the `CONF_MONITORED_CONDITIONS` option from integrations. Instead, all available data points from an integration will be exposed by default. Users will have the ability to include or exclude these entities directly within the Lovelace UI configuration.\n\n### Implementation Details\n- **Default Exposure**: All potential data points provided by an integration will be exposed by default.\n- **Lovelace UI Configuration**: Users will selectively include or exclude entities while configuring their Lovelace UI. This allows users to make informed choices based on the actual data presented.\n- **Documentation**: Comprehensive documentation will be provided to explain each data point that an integration can expose, helping users make informed decisions during Lovelace UI configuration.\n- **Backward Compatibility**: Existing configurations using `CONF_MONITORED_CONDITIONS` will continue to work, but a deprecation notice will be issued, encouraging users to migrate to the new approach.\n\nBy adopting this approach, we aim to reduce the cognitive load on users during the initial setup of integrations and provide a more intuitive and cohesive configuration experience within the Lovelace UI.","GenTime":"2024-08-20 00:12:32"}
{"File Name":"celestia-core\/adr-004-historical-validators.md","Context":"## Context\\nRight now, we can query the present validator set, but there is no history.\\nIf you were offline for a long time, there is no way to reconstruct past validators. This is needed for the light client and we agreed needs enhancement of the API.\\n","Decision":"For every block, store a new structure that contains either the latest validator set,\\nor the height of the last block for which the validator set changed. Note this is not\\nthe height of the block which returned the validator set change itself, but the next block,\\nie. the first block it comes into effect for.\\nStoring the validators will be handled by the `state` package.\\nAt some point in the future, we may consider more efficient storage in the case where the validators\\nare updated frequently - for instance by only saving the diffs, rather than the whole set.\\nAn alternative approach suggested keeping the validator set, or diffs of it, in a merkle IAVL tree.\\nWhile it might afford cheaper proofs that a validator set has not changed, it would be more complex,\\nand likely less efficient.\\n","tokens":56,"id":4101,"text":"## Context\\nRight now, we can query the present validator set, but there is no history.\\nIf you were offline for a long time, there is no way to reconstruct past validators. This is needed for the light client and we agreed needs enhancement of the API.\\n\n\n##Decision\nFor every block, store a new structure that contains either the latest validator set,\\nor the height of the last block for which the validator set changed. Note this is not\\nthe height of the block which returned the validator set change itself, but the next block,\\nie. the first block it comes into effect for.\\nStoring the validators will be handled by the `state` package.\\nAt some point in the future, we may consider more efficient storage in the case where the validators\\nare updated frequently - for instance by only saving the diffs, rather than the whole set.\\nAn alternative approach suggested keeping the validator set, or diffs of it, in a merkle IAVL tree.\\nWhile it might afford cheaper proofs that a validator set has not changed, it would be more complex,\\nand likely less efficient.\\n","MatchedIds":[1949,1944,1558,28,4119],"Predicted":"## Decision\nAfter evaluating the use cases and requirements, we have decided to introduce historical querying capabilities for the validator set. This will be accomplished by augmenting the existing API to store and provide access to past validator sets based on block height. Here are the steps and updates involved:\n\n### Design\n1. **Historical Validator Set Storage**:\n    - Store validator set snapshots at each block height within the state.\n    - Use a database schema that efficiently indexes validator sets by block height.\n   \n2. **API Enhancements**:\n    - Introduce a new endpoint to query validator sets by block height.\n    - Update the response structures to include a block height parameter.\n   \n3. **Data Management**:\n   - Implement a pruning mechanism to control the storage size, allowing nodes to configure the retention period for historical data.\n   - Provide configuration options in `config.toml` to set the retention period (`validator_history_retention_period`).\n\n### Implementation\n\n#### Schema Changes\nA new database table or collection will be created to store validator sets at each block height:\n```sql\nCREATE TABLE validator_sets (\n    block_height INT PRIMARY KEY,\n    validators JSONB\n);\n```\nThis table will contain the validator set in JSON format along with the corresponding block height.\n\n#### API Changes\nIntroduce a new API endpoint to fetch the validator set at a specific block height:\n```\nGET \/validators?height={blockHeight}\n\nResponse:\n{\n    \"block_height\": 12345,\n    \"validators\": [\n        {\n            \"address\": \"validator_address_1\",\n            \"pub_key\": \"validator_pub_key_1\",\n            \"power\": 1000\n        },\n        {\n            \"address\": \"validator_address_2\",\n            \"pub_key\": \"validator_pub_key_2\",\n            \"power\": 2000\n        },\n        ...\n    ]\n}\n```\n\n#### Configuration\nAdd the following configuration to `config.toml` to manage historical data retention:\n```toml\n# Historical validator set retention period in blocks. Set to 0 to retain indefinitely.\nvalidator_history_retention_period = 10000\n```\n\n#### Data Pruning\nImplement a pruning job to run periodically and remove old validator sets based on the configuration:\n```go\nfunc pruneOldValidatorSets(db Database, retentionPeriod int) {\n    \/\/ Calculate the oldest block height to retain\n    currentHeight := getCurrentBlockHeight(db)\n    pruneBeforeHeight := currentHeight - retentionPeriod\n\n    \/\/ Delete entries older than the calculated block height\n    db.Exec(\"DELETE FROM validator_sets WHERE block_height < ?\", pruneBeforeHeight)\n}\n```\nThis job will execute at a defined interval to ensure that only the configured number of blocks worth of historical data is retained.\n\n### Conclusion\nBy implementing these changes, we will enable both nodes and light clients to query past validator sets, thereby improving robustness in scenarios where nodes were offline and need to reconstruct historical data. This enhancement will ensure better data consistency and improve overall network reliability.","GenTime":"2024-08-20 00:12:42"}
{"File Name":"ksch-workflows\/0002-using-spring-boot.md","Context":"## Context\\nWe need [dependency injection](https:\/\/stackoverflow.com\/questions\/130794\/what-is-dependency-injection) in order to build decoupled software units.\\nAlso we need access on a relational database.\\n","Decision":"Spring offers a lot of convenience features for those purposes.\\nApache Wicket already provides a Spring integration.\\nWith the help of the [wicket-spring-boot](https:\/\/github.com\/MarcGiffing\/wicket-spring-boot) the convenience feature of Spring Boot can be applied on top of that.\\nThis makes the development faster and easier than using the alternative of dependency injection with [Google Guice](http:\/\/software.danielwatrous.com\/wicket-guice-including-unittests\/).\\nFurther usage of Spring enables potential useage of the various Spring sub-projects for advanced requirements like the integration of other software systems.\\n","tokens":46,"id":1758,"text":"## Context\\nWe need [dependency injection](https:\/\/stackoverflow.com\/questions\/130794\/what-is-dependency-injection) in order to build decoupled software units.\\nAlso we need access on a relational database.\\n\n\n##Decision\nSpring offers a lot of convenience features for those purposes.\\nApache Wicket already provides a Spring integration.\\nWith the help of the [wicket-spring-boot](https:\/\/github.com\/MarcGiffing\/wicket-spring-boot) the convenience feature of Spring Boot can be applied on top of that.\\nThis makes the development faster and easier than using the alternative of dependency injection with [Google Guice](http:\/\/software.danielwatrous.com\/wicket-guice-including-unittests\/).\\nFurther usage of Spring enables potential useage of the various Spring sub-projects for advanced requirements like the integration of other software systems.\\n","MatchedIds":[2488,4868,3657,4271,3966],"Predicted":"## Decision\nWe will use Spring Framework for our project. \n\nSpring provides a comprehensive suite for Dependency Injection through its Inversion of Control (IoC) container, making it easier to build loosely coupled applications. Additionally, it offers extensive support for relational databases through Spring Data JPA, which simplifies database interactions by handling the boilerplate code required to perform database operations.\n\nKey advantages of this decision include:\n- **Scalability and Flexibility:** Spring's modular architecture allows us to use only what is needed, ensuring efficient use of resources.\n- **Ease of Integration:** Spring integrates seamlessly with a variety of relational databases, including PostgreSQL, MySQL, and Oracle.\n- **Community and Documentation:** As one of the most widely-used frameworks in the Java ecosystem, Spring has a vibrant community and extensive documentation, which will aid in faster development and troubleshooting.","GenTime":"2024-08-20 00:12:47"}
{"File Name":"deeplearning4j\/0001-SameDiff_File_Format.md","Context":"## Context\\nSameDiff models need to be serializable - i.e., something we can save to disk or send over the network.\\nAdditionally, we need to be able to save and load model files in C++, and have those be readable in other languages (mainly Java).\\nCurrently, we have a FlatBuffers-based format for SameDiff graph serialization, but it has a number of problems, as discussed in this issue: https:\/\/github.com\/eclipse\/deeplearning4j\/issues\/8312\\n","Decision":"We will transition from a pure FlatBuffers to a Zip + FlatBuffers model format.\\nFlatBuffers will be used for the graph structure only. Parameters will be stored separately to the graph structure, also within the zip.\\nWe will introduce the ability to support multiple versions of a graph in the model files.\\nThis will enable the model file to support storing\\n* Multiple data types (for example, a FP32 version and a quantized INT8 version)\\n* Multiple different checkpoints (parameters after 1000 iterations, after 5000, and so on)\\n* Multiple versions of a given model (English vs. Chinese, or cased\/uncased, etc)\\nBy default when loading a graph (unless it is otherwise specified) we will load the most recent model tag.\\nTags must be valid file\/folder identifiers, and are not case sensitive.\\nThe structure of the zip file will be as follows:\\n```\\ntags.txt                         \/\/List of graph tags, one per line, in UTF8 format, no duplicates. Oldest first, newest last\\n<tag_name>\/graph.fb              \/\/The graph structure, in FlatBuffers format\\n<tag_name>\/params.txt            \/\/The mapping between variable names and parameter file names\\n<tag_name>\/params\/*.fb           \/\/The set of NDArrays that are the parameters, in FlatBuffers format\\n<tag_name>\/trainingConfig.fb     \/\/The training configuration - updater, learning rate, etc\\n<tag_name>\/updater.txt           \/\/The mapping between variable names and the updater state file names\\n<tag_name>\/updater\/*.fb          \/\/The set of NDArrays that are the updater state\\n```\\nNote that params.txt will allow for parameter sharing via references to other parameters:\\n```\\nmy_normal_param 0\\nshared_param <other_tag_name>\/7\\n```\\nThis means the parameters values for parameter \"my_normal_param\" are present at `<tag_name>\/params\/0.fb` within the zip file, and the parameter values for \"shared_param\" are available at `<other_tag_name>\/params\/7.fb`\\nNote also that the motivation for using the params.txt file (instead of the raw parameter name as the file name) is that some parameters will have invalid or ambiguous file names - \"my\/param\/name\", \"&MyParam*\" etc\\nIn terms of updater state, they will be stored in a similar format. For example, for the Adam updater with the M and V state arrays (each of same shape as the parameter):\\n```\\nmy_param 0 1\\nother_param 2 3\\n```\\nThat means my_param(M) is `<tag_name>\/updater\/0.fb` and my_param(V) is at `<tag_name>\/updater\/1.fb`\\nThis format also allows for updater state sharing, if we need it.\\n**Graph Structure**\\nThe graph structure will be encoded in FlatBuffers format using a schema with 2 parts:\\n1. A list of variables - each with name, datatype, and (for placeholders, constants and parameters) a shape\\n2. A list of operations - each with a name, op name\/type, input variable names, output variable names, and arguments\\nNote that both legacy and custom ops will be encoded in the same way. For legacy ops, we simply need the operation type, and the operation number.\\nOperation argument encoding will be done using named arguments: essentially, a `Map<String,T>` structure, where T is one of `{long, double, boolean, datatype}`.\\nThis allows for improved backward compatibility (no ambiguity as ops are modified after a graph file was written) and improved interpretability compared to using simple arrays of iargs, bargs, targs and dargs.\\nOne consequence\/downside of this is that we need to define mapping between our named arguments and iargs\/bargs\/targs\/dargs. In Java we have essentially done this manually, though clearly don't want to replicate this work in C++ (or any future languages).\\nTo avoid the need to do a significant amount of work (such as moving the name\/arg mapping to code generation) the following is proposed:\\nThe `Map<String,T>` is split up in the FlatBuffers schema into 4 pairs of fields.\\n* `String[] iArgNames`, `long[] iArgs`\\n* `String[] tArgNames`, `double[] dArgs`\\n* `String[] bArgNames`, `boolean[] bArgs`\\n* `String[] dArgNames`, `DataType[] dArgs`\\nClearly the name and value arrays (for each pair) would each be the same length, and name\/value correspondence is by array index.\\nThis is essentially equivalent to the `Map<String,T>` representation, but has the benefit of not needing us to define the mapping for named args to array-style args any time soon in C++, but also allowing us to add it in the future (mainly before we can write graphs from C++, or have better\/proper backward compatibility after op changes)\\n**Extensibility to Other Types**\\nSuppose in the future we want to store other data for a variable, not just an array?\\nExamples include lists and maps (for example, for NLP applications).\\nWhile we will not implement this right now, there are a number of options for adding this without breaking backward compatibility.\\nFirst: we can enhance the params.txt file format, perhaps using something like the following:\\n```\\nmap_param 0 MAP\\n```\\nSecond: We can add a similar text file for other types. For example, a params_maps.txt, same format as params.txt, with content at `<tag_name>\/params_maps\/*.fb`\\n","tokens":104,"id":2944,"text":"## Context\\nSameDiff models need to be serializable - i.e., something we can save to disk or send over the network.\\nAdditionally, we need to be able to save and load model files in C++, and have those be readable in other languages (mainly Java).\\nCurrently, we have a FlatBuffers-based format for SameDiff graph serialization, but it has a number of problems, as discussed in this issue: https:\/\/github.com\/eclipse\/deeplearning4j\/issues\/8312\\n\n\n##Decision\nWe will transition from a pure FlatBuffers to a Zip + FlatBuffers model format.\\nFlatBuffers will be used for the graph structure only. Parameters will be stored separately to the graph structure, also within the zip.\\nWe will introduce the ability to support multiple versions of a graph in the model files.\\nThis will enable the model file to support storing\\n* Multiple data types (for example, a FP32 version and a quantized INT8 version)\\n* Multiple different checkpoints (parameters after 1000 iterations, after 5000, and so on)\\n* Multiple versions of a given model (English vs. Chinese, or cased\/uncased, etc)\\nBy default when loading a graph (unless it is otherwise specified) we will load the most recent model tag.\\nTags must be valid file\/folder identifiers, and are not case sensitive.\\nThe structure of the zip file will be as follows:\\n```\\ntags.txt                         \/\/List of graph tags, one per line, in UTF8 format, no duplicates. Oldest first, newest last\\n<tag_name>\/graph.fb              \/\/The graph structure, in FlatBuffers format\\n<tag_name>\/params.txt            \/\/The mapping between variable names and parameter file names\\n<tag_name>\/params\/*.fb           \/\/The set of NDArrays that are the parameters, in FlatBuffers format\\n<tag_name>\/trainingConfig.fb     \/\/The training configuration - updater, learning rate, etc\\n<tag_name>\/updater.txt           \/\/The mapping between variable names and the updater state file names\\n<tag_name>\/updater\/*.fb          \/\/The set of NDArrays that are the updater state\\n```\\nNote that params.txt will allow for parameter sharing via references to other parameters:\\n```\\nmy_normal_param 0\\nshared_param <other_tag_name>\/7\\n```\\nThis means the parameters values for parameter \"my_normal_param\" are present at `<tag_name>\/params\/0.fb` within the zip file, and the parameter values for \"shared_param\" are available at `<other_tag_name>\/params\/7.fb`\\nNote also that the motivation for using the params.txt file (instead of the raw parameter name as the file name) is that some parameters will have invalid or ambiguous file names - \"my\/param\/name\", \"&MyParam*\" etc\\nIn terms of updater state, they will be stored in a similar format. For example, for the Adam updater with the M and V state arrays (each of same shape as the parameter):\\n```\\nmy_param 0 1\\nother_param 2 3\\n```\\nThat means my_param(M) is `<tag_name>\/updater\/0.fb` and my_param(V) is at `<tag_name>\/updater\/1.fb`\\nThis format also allows for updater state sharing, if we need it.\\n**Graph Structure**\\nThe graph structure will be encoded in FlatBuffers format using a schema with 2 parts:\\n1. A list of variables - each with name, datatype, and (for placeholders, constants and parameters) a shape\\n2. A list of operations - each with a name, op name\/type, input variable names, output variable names, and arguments\\nNote that both legacy and custom ops will be encoded in the same way. For legacy ops, we simply need the operation type, and the operation number.\\nOperation argument encoding will be done using named arguments: essentially, a `Map<String,T>` structure, where T is one of `{long, double, boolean, datatype}`.\\nThis allows for improved backward compatibility (no ambiguity as ops are modified after a graph file was written) and improved interpretability compared to using simple arrays of iargs, bargs, targs and dargs.\\nOne consequence\/downside of this is that we need to define mapping between our named arguments and iargs\/bargs\/targs\/dargs. In Java we have essentially done this manually, though clearly don't want to replicate this work in C++ (or any future languages).\\nTo avoid the need to do a significant amount of work (such as moving the name\/arg mapping to code generation) the following is proposed:\\nThe `Map<String,T>` is split up in the FlatBuffers schema into 4 pairs of fields.\\n* `String[] iArgNames`, `long[] iArgs`\\n* `String[] tArgNames`, `double[] dArgs`\\n* `String[] bArgNames`, `boolean[] bArgs`\\n* `String[] dArgNames`, `DataType[] dArgs`\\nClearly the name and value arrays (for each pair) would each be the same length, and name\/value correspondence is by array index.\\nThis is essentially equivalent to the `Map<String,T>` representation, but has the benefit of not needing us to define the mapping for named args to array-style args any time soon in C++, but also allowing us to add it in the future (mainly before we can write graphs from C++, or have better\/proper backward compatibility after op changes)\\n**Extensibility to Other Types**\\nSuppose in the future we want to store other data for a variable, not just an array?\\nExamples include lists and maps (for example, for NLP applications).\\nWhile we will not implement this right now, there are a number of options for adding this without breaking backward compatibility.\\nFirst: we can enhance the params.txt file format, perhaps using something like the following:\\n```\\nmap_param 0 MAP\\n```\\nSecond: We can add a similar text file for other types. For example, a params_maps.txt, same format as params.txt, with content at `<tag_name>\/params_maps\/*.fb`\\n","MatchedIds":[2945,3746,2932,2625,2934],"Predicted":"## Decision\nTo address the serialization requirements and compatibility issues described, we will transition from the current FlatBuffers-based format to Protocol Buffers (protobuf) for SameDiff model serialization. \n\nProtobuf offers several advantages:\n\n1. **Language Agnosticism:** It supports multiple programming languages including C++ and Java, ensuring that models can be read and written consistently across these environments.\n2. **Schema Evolution:** Protobuf handles backward and forward compatibility, which makes it easier to evolve the SameDiff models without breaking existing saved models.\n3. **Efficient Serialization:** Protobuf is known for its compact binary format, which is efficient for both storage and network transmission.\n\nAs part of this transition, we will:\n- Define new proto schemas for SameDiff models, capturing all necessary information.\n- Implement serialization and deserialization logic in both Java and C++.\n- Provide migration tools to convert existing FlatBuffers-based saved models to the new protobuf format.\n\nThis approach will address the current shortcomings and improve the robustness and maintainability of SameDiff model serialization.","GenTime":"2024-08-20 00:12:52"}
{"File Name":"google-cloud-cpp\/2018-06-13-storage-always-retries.md","Context":"**Context**: operations that change state in the server may complete with an\\nindeterminate status code. For example: an error in the 500 range may be\\nproduced by a middle-tier server after the operation was started by the backend.\\nRe-issuing the request may not work as expected for the application developer.\\n**Decision**: by default the library will retry all requests, including\\nnon-idempotent ones. The library will allow application developers to override\\nthe definition of what operations are automatically retried.\\n**Consequences**: most operations become easier to use for application\\ndevelopers. In very rare cases the operation will result in double uploads, or\\nin a new generation of the object or metadata being created. In even more rare\\ncases the operation may fail, for example, an operation to create an object with\\n`IfGenerationMatch(0)` would fail on the second attempt.\\n","Decision":"non-idempotent ones. The library will allow application developers to override\\nthe definition of what operations are automatically retried.\\n**Consequences**: most operations become easier to use for application\\ndevelopers. In very rare cases the operation will result in double uploads, or\\nin a new generation of the object or metadata being created. In even more rare\\ncases the operation may fail, for example, an operation to create an object with\\n`IfGenerationMatch(0)` would fail on the second attempt.\\n","tokens":188,"id":2466,"text":"**Context**: operations that change state in the server may complete with an\\nindeterminate status code. For example: an error in the 500 range may be\\nproduced by a middle-tier server after the operation was started by the backend.\\nRe-issuing the request may not work as expected for the application developer.\\n**Decision**: by default the library will retry all requests, including\\nnon-idempotent ones. The library will allow application developers to override\\nthe definition of what operations are automatically retried.\\n**Consequences**: most operations become easier to use for application\\ndevelopers. In very rare cases the operation will result in double uploads, or\\nin a new generation of the object or metadata being created. In even more rare\\ncases the operation may fail, for example, an operation to create an object with\\n`IfGenerationMatch(0)` would fail on the second attempt.\\n\n\n##Decision\nnon-idempotent ones. The library will allow application developers to override\\nthe definition of what operations are automatically retried.\\n**Consequences**: most operations become easier to use for application\\ndevelopers. In very rare cases the operation will result in double uploads, or\\nin a new generation of the object or metadata being created. In even more rare\\ncases the operation may fail, for example, an operation to create an object with\\n`IfGenerationMatch(0)` would fail on the second attempt.\\n","MatchedIds":[2467,75,674,4424,3987],"Predicted":"## Decision\nThe library will include a mechanism to retry requests that fail due to indeterminate status codes, by default. However, the definition of what requests should be retried, including non-idempotent operations, will be configurable by the application developers.\n\n**Consequences**: \n- **Positive**:\n  - Most operations become easier and more reliable for application developers as the retry mechanism can handle transient failures automatically.\n  - Reduces the need for developers to implement their own custom retry logic, simplifying codebases.\n\n- **Negative**:\n  - There is a risk of rare cases where operations could result in double actions (e.g., double uploads or duplicate object generations).\n  - In extremely rare scenarios, the retry mechanism might cause an operation to fail outright (e.g., creating an object with `IfGenerationMatch(0)` on a second attempt).\n\nTo mitigate the negatives, the library documentation will provide clear guidelines on configuring retries, especially for non-idempotent operations, to ensure that developers are aware of the potential risks and how to manage them effectively.","GenTime":"2024-08-20 00:12:56"}
{"File Name":"ToPS\/0001-extra-test-class-for-junit-test-with-SWI-Prolog-involved.md","Context":"## Context and Problem Statement\\nTests including Prolog queries with SWI Prolog can not be executed with CircleCI because SWI Prolog is not\\nrunning at the executing system.\\n","Decision":"Chosen option: extra test class to reduce the effort and to still enable testing SWI Prolog locally. For this the extra class is\\nannotated with @Ignore. All other tests run with CircleCI. These tests includes the core functionalities of the developed tool.\\n","tokens":38,"id":2080,"text":"## Context and Problem Statement\\nTests including Prolog queries with SWI Prolog can not be executed with CircleCI because SWI Prolog is not\\nrunning at the executing system.\\n\n\n##Decision\nChosen option: extra test class to reduce the effort and to still enable testing SWI Prolog locally. For this the extra class is\\nannotated with @Ignore. All other tests run with CircleCI. These tests includes the core functionalities of the developed tool.\\n","MatchedIds":[699,1258,4179,4868,3504],"Predicted":"## Decision\nUse Docker to run SWI Prolog as a container in CircleCI, because it ensures that SWI Prolog is available in the build environment\n\n### Positive Consequences\n* Consistent test environment across different runs\n* Tests can be executed without manual setup of SWI Prolog on CircleCI\n* The Docker container can be easily configured and maintained\n\n### Negative Consequences\n* Additional complexity in maintaining Docker configuration\n* Potential increase in build times due to container initialization","GenTime":"2024-08-20 00:12:58"}
{"File Name":"digital-paper-edit-client\/2019-05-01-security.md","Context":"## Context and Problem Statement\\nThese are notes based on the InfoSec meeting held on the 30th of May 2019.\\n## Decision Drivers\\n* Ease of use for journalists when system authenticates them\\n* Security levels (public, protect, or restrict)\\n* Data retention based on inactivity\\n* Complexity\\n","Decision":"* Ease of use for journalists when system authenticates them\\n* Security levels (public, protect, or restrict)\\n* Data retention based on inactivity\\n* Complexity\\nWe are going with Option 2 as this provides us with security, while\\nproviding a simpler way to set up access for journalists.\\nBen Thornton has agreed to this.\\nThe benefits of this approach means that we will be\\nusing existing BBC services to implement security, rather than implementing our\\nown servers. A single VPC is sufficient for **protected** levels of service, and\\nwe can apply SecurityGroups in between to have added security.\\nThe downside of this approach is that it is not using SSO but we can add that\\nlater on.\\nIn terms of data retention, we will go with the recommended 90 days.\\n![Architecture Diagram with Security](.\/dpe-transcript-EC2_security.png)\\n","tokens":67,"id":4166,"text":"## Context and Problem Statement\\nThese are notes based on the InfoSec meeting held on the 30th of May 2019.\\n## Decision Drivers\\n* Ease of use for journalists when system authenticates them\\n* Security levels (public, protect, or restrict)\\n* Data retention based on inactivity\\n* Complexity\\n\n\n##Decision\n* Ease of use for journalists when system authenticates them\\n* Security levels (public, protect, or restrict)\\n* Data retention based on inactivity\\n* Complexity\\nWe are going with Option 2 as this provides us with security, while\\nproviding a simpler way to set up access for journalists.\\nBen Thornton has agreed to this.\\nThe benefits of this approach means that we will be\\nusing existing BBC services to implement security, rather than implementing our\\nown servers. A single VPC is sufficient for **protected** levels of service, and\\nwe can apply SecurityGroups in between to have added security.\\nThe downside of this approach is that it is not using SSO but we can add that\\nlater on.\\nIn terms of data retention, we will go with the recommended 90 days.\\n![Architecture Diagram with Security](.\/dpe-transcript-EC2_security.png)\\n","MatchedIds":[4857,1252,5211,1754,2578],"Predicted":"## Decision\nUse OAuth 2.0 with Google Sign-In as the authentication mechanism for journalists.\n\n### Positive Consequences\n* **Ease of Use**: Most journalists are already familiar with Google Sign-In, reducing the learning curve.\n* **Security**: Google Sign-In leverages robust security measures and adheres to high levels of compliance.\n* **Data Management**: OAuth 2.0 allows setting specific data retention policies and handling of inactive accounts efficiently.\n* **Reduced Complexity**: Google Sign-In integration is straightforward with extensive documentation and support, simplifying development efforts.\n\n### Negative Consequences\n* **Dependency on Third-Party Provider**: Relying on Google Sign-In ties our authentication process to Google's availability and policies.\n* **Limited Customization**: Customization options for the sign-in process and interfaces might be limited compared to an in-house solution.\n* **Privacy Concerns**: Some users may have reservations about signing in using their Google accounts due to privacy concerns.\n\n### Implementation Notes\n* Ensure that our platform supports fallback authentication methods to address potential concerns with Google Sign-In.\n* Establish clear data handling policies to manage data retention based on user inactivity.\n* Regularly review and update security policies in alignment with Google\u2019s changes and recommendations.","GenTime":"2024-08-20 00:13:06"}
{"File Name":"dapr\/ENG-001-tagging.md","Context":"## Context\\nAs we embraced using Docker repositories to store our images, and keeping in mind we support multiple repositories along with versioning of images and different architectures,\\nWe needed a way to construct an accepted and constant way of naming our Docker images.\\n","Decision":"* An image will conform to the following format: \\<namespace>\/\\<repository>:\\<tag>\\n* A valid tag conforms to the following format: \\<version>-\\<architecture>, or just \\<version>, then arch is assumed Linux\\n","tokens":51,"id":74,"text":"## Context\\nAs we embraced using Docker repositories to store our images, and keeping in mind we support multiple repositories along with versioning of images and different architectures,\\nWe needed a way to construct an accepted and constant way of naming our Docker images.\\n\n\n##Decision\n* An image will conform to the following format: \\<namespace>\/\\<repository>:\\<tag>\\n* A valid tag conforms to the following format: \\<version>-\\<architecture>, or just \\<version>, then arch is assumed Linux\\n","MatchedIds":[3920,3598,4096,2703,3604],"Predicted":"## Decision\nWe will adopt the following naming convention for Docker images to ensure consistency across our repositories:\n\n### Naming Pattern\n```\n[namespace]\/[application]-[component]:[version]-[architecture]\n```\n\n### Details\n1. **Namespace**: This will refer to the repository or organizational unit, ensuring proper segregation and access control.\n    - Example: `company`, `department`, `project`\n\n2. **Application**: The main application or system the Docker image belongs to.\n    - Example: `webapp`, `database`, `api`\n\n3. **Component**: The specific component or service within the application.\n    - Example: `frontend`, `backend`, `worker`\n\n4. **Version**: The specific version of the image, following semantic versioning (`major.minor.patch`).\n    - Example: `1.0.0`, `2.1.3`, `3.4.0`\n\n5. **Architecture**: The architecture the image is built for.\n    - Example: `amd64`, `arm64`, `ppc64le`\n\n### Example Names\n- `company\/webapp-frontend:1.0.0-amd64`\n- `department\/database-backend:2.1.3-arm64`\n- `project\/api-worker:3.4.0-ppc64le`\n\n### Reasoning\n- **Consistency**: Ensures all images follow a predictable and standardized naming convention, making management and retrieval straightforward.\n- **Clarity**: Clearly identifies the namespace, application, component, version, and architecture of each image.\n- **Scalability**: Supports multiple repositories and can accommodate future organizational changes or additions in a structured manner.\n- **Versioning**: Facilitates easy tracking and updating of specific image versions.\n\nBy enforcing this naming convention, we will improve organization, maintainability, and clarity across all Docker images within our repositories.","GenTime":"2024-08-20 00:13:12"}
{"File Name":"log4brains\/20200925-multi-packages-architecture-in-a-monorepo-with-yarn-and-lerna.md","Context":"## Context and Problem Statement\\nWe have to define the initial overall architecture of the project.\\nFor now, we are sure that we want to provide these features:\\n- Local preview web UI\\n- Static Site Generation from the CI\/CD\\n- CLI to create a new ADR quickly\\nIn the future, we might want to provide these features:\\n- Create\/edit ADRs from the local web UI\\n- VSCode extension to create and maybe edit an ADR from the IDE\\n- Support ADR aggregation from multiple repositories\\n","Decision":"Chosen option: \"Multi-packages, monorepo, with Yarn and Lerna\", because\\n- We don't want a monolith because we want the core library\/API to be very well tested and probably developed with DDD and hexagonal architecture. The other packages will just call this core API, they will contain fewer business rules as possible. As we are not so sure about the features we will provide in the future, this is good for extensibility.\\n- Yarn + Lerna seems to be a very good practice used by a lot of other open-source projects to publish npm packages.\\n","tokens":110,"id":1067,"text":"## Context and Problem Statement\\nWe have to define the initial overall architecture of the project.\\nFor now, we are sure that we want to provide these features:\\n- Local preview web UI\\n- Static Site Generation from the CI\/CD\\n- CLI to create a new ADR quickly\\nIn the future, we might want to provide these features:\\n- Create\/edit ADRs from the local web UI\\n- VSCode extension to create and maybe edit an ADR from the IDE\\n- Support ADR aggregation from multiple repositories\\n\n\n##Decision\nChosen option: \"Multi-packages, monorepo, with Yarn and Lerna\", because\\n- We don't want a monolith because we want the core library\/API to be very well tested and probably developed with DDD and hexagonal architecture. The other packages will just call this core API, they will contain fewer business rules as possible. As we are not so sure about the features we will provide in the future, this is good for extensibility.\\n- Yarn + Lerna seems to be a very good practice used by a lot of other open-source projects to publish npm packages.\\n","MatchedIds":[2091,2914,1545,2317,1669],"Predicted":"## Decision\nWe will adopt a modular architecture with a mix of server-side and client-side technologies to ensure scalability and maintainability. The initial setup will include:\n\n1. **Local Preview Web UI**:\n   - We will use a lightweight web server (e.g., Express.js) to serve the local preview.\n   - For the front-end, we will use a modern JavaScript framework like React to ensure a responsive and interactive UI.\n\n2. **Static Site Generation from the CI\/CD**:\n   - We will integrate a Static Site Generator (e.g., Gatsby or Next.js) with our CI\/CD pipeline (e.g., GitHub Actions, GitLab CI) to automate the build and deployment process.\n   - This setup will ensure quick and consistent deployments and reduce the risk of discrepancies between development and production environments.\n\n3. **CLI to Create a New ADR Quickly**:\n   - We will develop a command-line interface tool (e.g., using Node.js) to facilitate the rapid creation of ADRs.\n   - The CLI will follow a template-based approach to ensure consistency and adherence to the chosen ADR format.\n\nFor future features, the architecture will be flexible enough to accommodate:\n\n- Adding APIs to support CRUD operations for ADRs via the local web UI.\n- Integration with VSCode through extensions leveraging the Language Server Protocol (LSP).\n- Implementing a microservices or modular monolith approach to support ADR aggregation from multiple repositories.\n\nBy adopting these technologies and practices, we aim to create a robust, scalable, and user-friendly system that can evolve to meet future needs.","GenTime":"2024-08-20 00:13:18"}
{"File Name":"platform\/2022-01-05-add-feature-flag-support-for-storefront-scss.md","Context":"## Context\\n* There is no possibility to check for feature flags inside the Storefront SCSS.\\n* Altering the SCSS depending on a feature flag will require workarounds like e.g. \"making up\" and additional class in the template and use the feature toggle in twig instead.\\n* The SCSS of a selector which is hidden behind a feature flag will still be in the compiled CSS.\\n* It is not easily possible to make breaking changes inside SCSS functions, mixins or variables backward-compatible with the use of feature flags.\\n","Decision":"* Add the possibility to check for feature flags inside SCSS, similar to the twig implementation.\\n* The feature configuration from `Feature::getAll()` is converted to a SCSS map inside `\\Shopware\\Storefront\\Theme\\ThemeCompiler::getFeatureConfigScssMap`.\\n* This SCSS map is always added to the SCSS string which gets processed by `\\Shopware\\Storefront\\Theme\\ThemeCompiler::compileTheme`.\\n* For webpack hot-proxy the `var\/config_js_features.json` is used instead.\\n* The SCSS map looks like this: `$sw-features: (\"FEATURE_NEXT_1234\": false, \"FEATURE_NEXT_1235\": true);`\\n* See https:\/\/sass-lang.com\/documentation\/values\/maps\\n* A globally available function `feature()` is used to read inside the SCSS map if a desired feature is active.\\nExample:\\n```scss\\nbody {\\n@if feature('FEATURE_NEXT_1') {\\nbackground-color: #ff0000;\\n} @else {\\nbackground-color: #ffcc00;\\n}\\n}\\n```\\n","tokens":112,"id":4508,"text":"## Context\\n* There is no possibility to check for feature flags inside the Storefront SCSS.\\n* Altering the SCSS depending on a feature flag will require workarounds like e.g. \"making up\" and additional class in the template and use the feature toggle in twig instead.\\n* The SCSS of a selector which is hidden behind a feature flag will still be in the compiled CSS.\\n* It is not easily possible to make breaking changes inside SCSS functions, mixins or variables backward-compatible with the use of feature flags.\\n\n\n##Decision\n* Add the possibility to check for feature flags inside SCSS, similar to the twig implementation.\\n* The feature configuration from `Feature::getAll()` is converted to a SCSS map inside `\\Shopware\\Storefront\\Theme\\ThemeCompiler::getFeatureConfigScssMap`.\\n* This SCSS map is always added to the SCSS string which gets processed by `\\Shopware\\Storefront\\Theme\\ThemeCompiler::compileTheme`.\\n* For webpack hot-proxy the `var\/config_js_features.json` is used instead.\\n* The SCSS map looks like this: `$sw-features: (\"FEATURE_NEXT_1234\": false, \"FEATURE_NEXT_1235\": true);`\\n* See https:\/\/sass-lang.com\/documentation\/values\/maps\\n* A globally available function `feature()` is used to read inside the SCSS map if a desired feature is active.\\nExample:\\n```scss\\nbody {\\n@if feature('FEATURE_NEXT_1') {\\nbackground-color: #ff0000;\\n} @else {\\nbackground-color: #ffcc00;\\n}\\n}\\n```\\n","MatchedIds":[3171,1125,4498,157,783],"Predicted":"## Decision\nTo overcome the limitations of managing feature flags within SCSS directly, we have decided to use a combination of JavaScript and Twig templates to control the application of feature flags in styles. Specifically:\n\n1. **JavaScript for Dynamic Styling:** Utilize JavaScript to append or modify CSS classes dynamically based on the active feature flags. This allows more granular control over which styles are applied without needing to modify the SCSS directly.\n\n2. **Twig Templates for Conditional Logic:** Employ Twig templates to include conditional logic based on feature flags. This will ensure that the appropriate CSS classes are applied conditionally when rendering the HTML, preventing unnecessary styles from being included in the compiled CSS.\n\n3. **Scoped SCSS for Isolation:** Wrap any feature-specific SCSS rules within a namespace or parent selector that can be toggled on or off via the class added by JavaScript or Twig. This will help ensure that feature-specific styles only apply when required, and do not affect the rest of the styles.\n\nBy adopting this approach, we can maintain backward compatibility and reduce the inclusion of unnecessary CSS in the compiled output while embracing a more modular and flexible styling strategy.","GenTime":"2024-08-20 00:13:22"}
{"File Name":"tracking-consent-frontend\/0002-use-data-attribute-for-gtm-container.md","Context":"## Context and Problem Statement\\nIn order to simplify the tracking consent build and deploy process and\\nmake integrating with tracking consent less surprising, should\\nthe configuration of the GTM container used by tracking consent be via\\ndata attributes rather than separate bundles?\\n## Decision Drivers\\n* The need to keep things simple for service developers\\n* The need to improve the operability of tracking consent\\n","Decision":"* The need to keep things simple for service developers\\n* The need to improve the operability of tracking consent\\nChosen option: \"Use a data attribute\", because based on the benefits listed below the team\\nbelieves this is the best way forward.\\n### Positive Consequences\\n* The Javascript bundle creation process is simplified.\\n* The Scala Play routing is simplified\\n* Only one endpoint needs to be managed in production\\n* Future additional containers can be supported more easily\\n* Service developers only have to configure a single URL to tracking consent and use\\na data attribute to configure the container.\\n* The central common configuration repo (owned by a separate team) only requires a single URL to\\ntracking consent defining for each environment, rather than one for each GTM container.\\n### Negative Consequences\\n* Service developers need to add id=\"tracking-consent-script-tag\" to the SCRIPT\\ntag when integrating.\\n","tokens":78,"id":3529,"text":"## Context and Problem Statement\\nIn order to simplify the tracking consent build and deploy process and\\nmake integrating with tracking consent less surprising, should\\nthe configuration of the GTM container used by tracking consent be via\\ndata attributes rather than separate bundles?\\n## Decision Drivers\\n* The need to keep things simple for service developers\\n* The need to improve the operability of tracking consent\\n\n\n##Decision\n* The need to keep things simple for service developers\\n* The need to improve the operability of tracking consent\\nChosen option: \"Use a data attribute\", because based on the benefits listed below the team\\nbelieves this is the best way forward.\\n### Positive Consequences\\n* The Javascript bundle creation process is simplified.\\n* The Scala Play routing is simplified\\n* Only one endpoint needs to be managed in production\\n* Future additional containers can be supported more easily\\n* Service developers only have to configure a single URL to tracking consent and use\\na data attribute to configure the container.\\n* The central common configuration repo (owned by a separate team) only requires a single URL to\\ntracking consent defining for each environment, rather than one for each GTM container.\\n### Negative Consequences\\n* Service developers need to add id=\"tracking-consent-script-tag\" to the SCRIPT\\ntag when integrating.\\n","MatchedIds":[3526,2067,381,1669,3784],"Predicted":"## Decision\nTo simplify the tracking consent build and deploy process and improve the integration experience for service developers, we will configure the GTM (Google Tag Manager) container used by tracking consent via data attributes rather than separate bundles.\n\n### Positive Consequences\n* **Simplicity for Developers:** Service developers can integrate tracking consent more easily by utilizing data attributes. This reduces the learning curve and minimizes integration errors.\n* **Improved Operability:** Managing configuration through data attributes rather than separate bundles makes the tracking consent more flexible and maintainable. This can lead to smoother updates and less downtime during deployments.\n* **Consistency:** Using data attributes aligns with modern web development practices, ensuring consistency in how configurations are managed across different services.\n\n### Negative Consequences\n* **Initial Refactor:** There will be a need for an initial refactor to migrate from separate bundles to data attributes for existing implementations. This requires developer time and resources.\n* **Documentation Updates:** Comprehensive documentation will need to be updated and maintained to guide developers on how to use the new data attribute configuration method.\n* **Potential Misconfigurations:** Relying on data attributes might increase the chances of misconfigurations if attributes are not properly set by service developers. Proper validation mechanisms need to be implemented to mitigate this risk.\n\n### Implementation Steps\n1. **Update Documentation:** Provide clear and detailed documentation on how to configure the GTM container using data attributes.\n2. **Refactor Existing Codebase:** Gradually migrate the existing tracking consent implementations to use data attributes for configuration.\n3. **Provide Validation Tools:** Develop and offer tools or scripts that can validate the correctness of data attribute configurations to help developers avoid misconfigurations.\n4. **Communicate the Change:** Notify all relevant teams and developers about this change well in advance, providing necessary training and support during the transition period.\n\n### Conclusion\nBy configuring the GTM container via data attributes, we can streamline the integration process, making it easier for service developers to implement tracking consent and improving the overall operability and maintenance of tracking consent functionalities.","GenTime":"2024-08-20 00:13:31"}
{"File Name":"CrossyToad\/adr-0007-SUPERSEDED-mtl-style-for-affects.md","Context":"## Context\\nIn Haskell we need a way to manage side-effects, particularly for games. There are a _bunch_ of\\noptions including:\\n- MTL Style\\n- Using `IO` for everything\\n- Freer Monads\\nI haven't used mtl style in anger yet yet so I'm keen to give it a go.\\nI'm also steaing a lot of ideas from [dino-rush](https:\/\/github.com\/jxv\/dino-rush\/) and it\\nseems to be using MTL style.\\n","Decision":"We're using MTL style.\\n","tokens":114,"id":2473,"text":"## Context\\nIn Haskell we need a way to manage side-effects, particularly for games. There are a _bunch_ of\\noptions including:\\n- MTL Style\\n- Using `IO` for everything\\n- Freer Monads\\nI haven't used mtl style in anger yet yet so I'm keen to give it a go.\\nI'm also steaing a lot of ideas from [dino-rush](https:\/\/github.com\/jxv\/dino-rush\/) and it\\nseems to be using MTL style.\\n\n\n##Decision\nWe're using MTL style.\\n","MatchedIds":[2480,2290,4592,2482,1495],"Predicted":"## Decision\nWe will use MTL style for managing side-effects. This approach will allow us to structure side-effectful computations in a way that is both modular and composable. Additionally, since the `dino-rush` project, which we are drawing inspiration from, employs MTL style, this will offer us a concrete reference for implementation and best practices.","GenTime":"2024-08-20 00:13:34"}
{"File Name":"volley-management\/0006-use-azure-table-storage-for-persistence.md","Context":"## Context and Problem Statement\\nWe need a persistent state for the system. Previously I've used Azure SQL but it is quite pricey but has it's own set of features including relational model. But given that system is not very complex I can model persistence mechanism for NoSQL model.\\n## Decision Drivers\\n* Cost\\n* Backup capabilities\\n","Decision":"* Cost\\n* Backup capabilities\\nChosen option: \"Azure Table Storage\", because cost.\\n### Positive Consequences <!-- optional -->\\n* Persistence bill should be down from ~$5\/month to less than $1\/month\\n### Negative Consequences <!-- optional -->\\n* Students in IT Academy won't be exposed to a relational model, which at this moment dominates work they will have to be doing. We will have to come up with a strategy to get them good experience.\\n","tokens":68,"id":2520,"text":"## Context and Problem Statement\\nWe need a persistent state for the system. Previously I've used Azure SQL but it is quite pricey but has it's own set of features including relational model. But given that system is not very complex I can model persistence mechanism for NoSQL model.\\n## Decision Drivers\\n* Cost\\n* Backup capabilities\\n\n\n##Decision\n* Cost\\n* Backup capabilities\\nChosen option: \"Azure Table Storage\", because cost.\\n### Positive Consequences <!-- optional -->\\n* Persistence bill should be down from ~$5\/month to less than $1\/month\\n### Negative Consequences <!-- optional -->\\n* Students in IT Academy won't be exposed to a relational model, which at this moment dominates work they will have to be doing. We will have to come up with a strategy to get them good experience.\\n","MatchedIds":[1574,4747,1740,1661,1167],"Predicted":"## Decision\nChosen option: \"Amazon DynamoDB\", because\n* Cost-effective: DynamoDB offers a flexible pricing model that allows for significant cost savings, especially for smaller, less complex systems.\n* Backup capabilities: DynamoDB provides automated backups and on-demand snapshots, ensuring that data is always protected and easily recoverable.\n* Scalability: It can automatically scale up and down to meet the demand, providing additional cost efficiency and performance improvements.\n* Integration with AWS Services: Seamless integration with other AWS services for building more complex workflows if required in the future.\n\n### Positive Consequences\n* Reduced costs compared to Azure SQL.\n* Simplified data management with automated backups and scaling.\n\n### Negative Consequences\n* Transitioning to a NoSQL model might involve a learning curve if the team is more familiar with relational databases.\n* Potential adjustments in data querying and reporting mechanisms due to the NoSQL model constraints.","GenTime":"2024-08-20 00:13:38"}
{"File Name":"devops-challenge\/0009-doc-about-monorepo.md","Context":"## Context\\nMicroservices are complex because they are small and can use different languages\\nand different stacks to run: java, spring, jakarta, python, flask, tornado, ruby\\nrails, sinatra, mysql, postgresql, mongodb, redis...\\nThere are so heterogeneous that sometimes we need to set a common way to work with\\nthem.  Each microservice has a different life-cycle, some are update more\\nfrequently, others are not.\\nUsually, when apps differs in their life-cycle speed, incompatibilities will come\\nup.  Those incompatibilities can make the system get down.  And keeping an\\nupdated and acurated matrix of compatibilities is a pain.\\nA way to minimize this risks is to put all code in the same repository, but not as\\na monolithic application, but in separate folders.  This has some advantages\\nlike:\\n- all code is tested at the same time\\n- transversal refactors can be easy\\n- compatibility matrices are simplified\\n","Decision":"To use a monorepo to keep all code together and deploy it at the same time.\\n","tokens":210,"id":3483,"text":"## Context\\nMicroservices are complex because they are small and can use different languages\\nand different stacks to run: java, spring, jakarta, python, flask, tornado, ruby\\nrails, sinatra, mysql, postgresql, mongodb, redis...\\nThere are so heterogeneous that sometimes we need to set a common way to work with\\nthem.  Each microservice has a different life-cycle, some are update more\\nfrequently, others are not.\\nUsually, when apps differs in their life-cycle speed, incompatibilities will come\\nup.  Those incompatibilities can make the system get down.  And keeping an\\nupdated and acurated matrix of compatibilities is a pain.\\nA way to minimize this risks is to put all code in the same repository, but not as\\na monolithic application, but in separate folders.  This has some advantages\\nlike:\\n- all code is tested at the same time\\n- transversal refactors can be easy\\n- compatibility matrices are simplified\\n\n\n##Decision\nTo use a monorepo to keep all code together and deploy it at the same time.\\n","MatchedIds":[302,856,2701,3488,4889],"Predicted":"## Decision\nWe will adopt a mono-repo approach for our microservices. Each microservice will reside in its own directory within a single repository. This setup will allow us to:\n\n- Test all microservices together, ensuring that integration points are validated continuously.\n- Facilitate transversal refactoring across multiple microservices, simplifying the process of making cross-cutting changes.\n- Reduce the complexity of maintaining compatibility matrices, as dependencies and compatibility issues can be managed more effectively within the mono-repo.\n\nBy centralizing our microservices in a single repository, we can leverage the benefits of cohesive testing, easier refactoring, and simplified dependency management while retaining the flexibility of independent microservices development.","GenTime":"2024-08-20 00:13:42"}
{"File Name":"occurrent\/0002-mongodb-cloudevent-serialization.md","Context":"## Context\\nCurrently, Occurrent is doing \"unnecessary\" work when converting from a `CloudEvent` to `Document` and vice versa\\nSee [issue 196](https:\/\/github.com\/cloudevents\/sdk-java\/issues\/196) in the cloud event java sdk project.\\n","Decision":"None yet\\n","tokens":59,"id":4382,"text":"## Context\\nCurrently, Occurrent is doing \"unnecessary\" work when converting from a `CloudEvent` to `Document` and vice versa\\nSee [issue 196](https:\/\/github.com\/cloudevents\/sdk-java\/issues\/196) in the cloud event java sdk project.\\n\n\n##Decision\nNone yet\\n","MatchedIds":[2181,1193,4385,816,842],"Predicted":"## Decision\nTo optimize the conversion process and reduce the unnecessary work identified in the current implementation, the following changes will be made:\n\n1. **Direct Mapping Between `CloudEvent` and `Document`**:\n   Implement a more efficient direct mapping strategy between `CloudEvent` and `Document` by using a streamlined conversion method that avoids redundant data transformations. This could involve utilizing a custom serializer\/deserializer tailored to our specific requirements.\n   \n2. **Leverage CloudEvents' Provided Utilities**:\n   Check if the CloudEvents SDK provides any utility methods or enhancements that facilitate efficient conversion without redoing work. Utilize these methods instead of creating custom conversion logic from scratch.\n\n3. **Benchmark and Optimize Critical Paths**:\n   Introduce benchmarks to identify the critical paths in the conversion process. Optimize these paths by profiling and refining any bottlenecks identified during the benchmarking phase.\n\n4. **Schema Validation Optimization**:\n   Ensure schema validation during conversion is done efficiently, possibly by caching schema validations or using lightweight validators that are sufficient for our use cases.\n\nHere is a conceptual example of a more efficient conversion function in Java that could be implemented:\n\n```java\npublic class CloudEventDocumentConverter {\n\n    \/\/ Example method to convert from CloudEvent to Document\n    public static Document convertToDocument(CloudEvent cloudEvent) {\n        Document doc = new Document();\n        doc.put(\"id\", cloudEvent.getId());\n        doc.put(\"source\", cloudEvent.getSource().toString());\n        doc.put(\"type\", cloudEvent.getType());\n        doc.put(\"specversion\", cloudEvent.getSpecVersion());\n        doc.put(\"datacontenttype\", cloudEvent.getDataContentType().orElse(null));\n        doc.put(\"data\", cloudEvent.getData().orElse(null));\n        \/\/ Add other fields as necessary\n        return doc;\n    }\n\n    \/\/ Example method to convert from Document to CloudEvent\n    public static CloudEvent convertToCloudEvent(Document doc) {\n        CloudEventBuilder builder = CloudEventBuilder.v1()\n                .withId(doc.getString(\"id\"))\n                .withSource(URI.create(doc.getString(\"source\")))\n                .withType(doc.getString(\"type\"))\n                .withDataContentType(doc.getString(\"datacontenttype\"))\n                .withData(doc.getString(\"data\").getBytes());\n\n        \/\/ Add other fields as necessary\n        return builder.build();\n    }\n}\n```\n\n### Expected Outcomes\n- **Reduced Latency**: Faster conversion between `CloudEvent` and `Document` due to more efficient processing.\n- **Lower Resource Consumption**: Reduced CPU and memory usage during conversions.\n- **Maintainability**: Simplified and more readable conversion code.\n\nBy implementing these changes, the conversion process will be optimized, addressing the unnecessary work identified, while also making it more maintainable and efficient.","GenTime":"2024-08-20 00:13:50"}
{"File Name":"ReportMI-service-manual\/0014-use-gov-uk-notify-for-sending-notifications.md","Context":"## Context\\nThe Data Submission Service will need to send notifications to suppliers at\\nvarious stages during the submission process.\\nThe notifications will, for example, include:\\n- Telling a supplier they have a task to complete\\n- Telling a supplier that a deadline is approaching\\n- Telling a supplier that their task is overdue\\n- Providing a receipt for a submission\\nInitially, the notifications will be email, but may also need to be SMS based in\\nfuture.\\nTo reduce the need for each organisation or service team to build it's own\\ninfrastructure for sending email and SMS based messages, the Government Digital\\nService (GDS) has built [GOV.UK Notify][service-notify].\\nGOV.UK Notify can send notifications either via an API call, or by uploading a\\nCSV to the website. Sending email is free.\\nThe service is already used in various parts of CCS.\\n","Decision":"We will use GOV.UK notify to send notifications for Data Submission Service.\\nFor MVP we may use the CSV upload function to send notifications, but this will\\nbe replaced by automated API calls later in development.\\n","tokens":188,"id":2050,"text":"## Context\\nThe Data Submission Service will need to send notifications to suppliers at\\nvarious stages during the submission process.\\nThe notifications will, for example, include:\\n- Telling a supplier they have a task to complete\\n- Telling a supplier that a deadline is approaching\\n- Telling a supplier that their task is overdue\\n- Providing a receipt for a submission\\nInitially, the notifications will be email, but may also need to be SMS based in\\nfuture.\\nTo reduce the need for each organisation or service team to build it's own\\ninfrastructure for sending email and SMS based messages, the Government Digital\\nService (GDS) has built [GOV.UK Notify][service-notify].\\nGOV.UK Notify can send notifications either via an API call, or by uploading a\\nCSV to the website. Sending email is free.\\nThe service is already used in various parts of CCS.\\n\n\n##Decision\nWe will use GOV.UK notify to send notifications for Data Submission Service.\\nFor MVP we may use the CSV upload function to send notifications, but this will\\nbe replaced by automated API calls later in development.\\n","MatchedIds":[996,5213,1786,1784,1767],"Predicted":"## Decision\nWe will use GOV.UK Notify to send notifications for the Data Submission Service. This will include emails initially, with the potential to accommodate SMS notifications in the future.\n\nUsing GOV.UK Notify provides several advantages:\n- It eliminates the need to build and maintain a custom infrastructure for sending emails and SMS.\n- Notifications are handled through a secure and reliable service already approved for use in other parts of CCS.\n- GOV.UK Notify ensures consistency in user experience across various CCS digital services.\n- Sending emails through GOV.UK Notify is cost-effective since it is free.\n- The service provides feedback on message delivery, enhancing reliability.\n\nThis decision aligns with our goal of maintaining a standardized and efficient notification process, and future expansion to SMS notifications can be easily integrated using the same service.","GenTime":"2024-08-20 00:13:54"}
{"File Name":"operational-data-hub\/0003-create-cloud-native-solutions.md","Context":"## Context\\nCloud-native architecture fully takes advantage of the [serverless computing](0002-use-serverless-infra-components.md) to maximise your focus on writing code, creating business value and keeping customers happy.\\nAll the developer has to worry about is orchestrating all the infrastructure they need ([sofware defined](0004-create-software-defined-everything.md)) and the application code itself.\\n","Decision":"We will build cloud-native solutions.\\n","tokens":83,"id":2752,"text":"## Context\\nCloud-native architecture fully takes advantage of the [serverless computing](0002-use-serverless-infra-components.md) to maximise your focus on writing code, creating business value and keeping customers happy.\\nAll the developer has to worry about is orchestrating all the infrastructure they need ([sofware defined](0004-create-software-defined-everything.md)) and the application code itself.\\n\n\n##Decision\nWe will build cloud-native solutions.\\n","MatchedIds":[2723,4937,631,108,1182],"Predicted":"## Decision\nWe will adopt a cloud-native architecture that leverages serverless computing and software-defined infrastructure. This will allow developers to concentrate on writing application code and delivering business value, while the infrastructure is orchestrated automatically.","GenTime":"2024-08-20 00:13:56"}
{"File Name":"ockam\/0001-record-architectural-decisions.md","Context":"## Context\\nWe need to record the architectural decisions that we make as we develop Ockam.\\n","Decision":"We will keep a collection of records for \"architecturally significant\" decisions: those that\\naffect the structure, non-functional characteristics, dependencies, interfaces, or construction\\ntechniques.\\nWe will use Architecture Decision Records, as [described by Michael Nygard](1).\\n","tokens":21,"id":4539,"text":"## Context\\nWe need to record the architectural decisions that we make as we develop Ockam.\\n\n\n##Decision\nWe will keep a collection of records for \"architecturally significant\" decisions: those that\\naffect the structure, non-functional characteristics, dependencies, interfaces, or construction\\ntechniques.\\nWe will use Architecture Decision Records, as [described by Michael Nygard](1).\\n","MatchedIds":[2261,4428,3119,1436,294],"Predicted":"## Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions.","GenTime":"2024-08-20 00:13:58"}
{"File Name":"aspan-server\/0003-replacing-ramda-with-lodash.md","Context":"## Context\\nArrow functions are a much more natural way to reduce visual noise in most contexts in JavaScript.\\n","Decision":"Decision here...\\n","tokens":22,"id":1890,"text":"## Context\\nArrow functions are a much more natural way to reduce visual noise in most contexts in JavaScript.\\n\n\n##Decision\nDecision here...\\n","MatchedIds":[1854,2687,400,3321,1855],"Predicted":"## Decision\nWe will standardize on using arrow functions for all function expressions in our JavaScript codebase. Using arrow functions will help in making our code more concise and readable. We will update our code style guides to reflect this preference and set up linters (such as ESLint) to enforce the use of arrow functions whenever applicable.","GenTime":"2024-08-20 00:14:01"}
{"File Name":"beis-report-official-development-assistance\/0011-use-govuk-design-system-form-builder.md","Context":"## Context\\nBuilding forms in Rails that are compliant with the GOVUK Design System involve\\nmanually declaring the correct HTML structure, class names and ARIA attributes,\\nwhich is time-consuming and hard to get right.\\nAdditionally, our validation errors currently use Rails' default pattern, rather\\nthan the one recommended for use as part of the design system, which is designed\\nwith accessibility in mind.\\n","Decision":"We will use DfE's `govuk_design_system_formbuilder` to simplify the creation of\\nGOV.UK Design System-compliant forms.\\nAs we are currently using Simple Form rather than Rails' default form builder\\nfor our other forms, the two form builders can co-exist for the time being,\\nwhilst we transition the forms over.\\n","tokens":83,"id":2386,"text":"## Context\\nBuilding forms in Rails that are compliant with the GOVUK Design System involve\\nmanually declaring the correct HTML structure, class names and ARIA attributes,\\nwhich is time-consuming and hard to get right.\\nAdditionally, our validation errors currently use Rails' default pattern, rather\\nthan the one recommended for use as part of the design system, which is designed\\nwith accessibility in mind.\\n\n\n##Decision\nWe will use DfE's `govuk_design_system_formbuilder` to simplify the creation of\\nGOV.UK Design System-compliant forms.\\nAs we are currently using Simple Form rather than Rails' default form builder\\nfor our other forms, the two form builders can co-exist for the time being,\\nwhilst we transition the forms over.\\n","MatchedIds":[425,3037,783,263,3586],"Predicted":"## Decision\nWe will implement a custom Rails form builder that adheres to the GOVUK Design System guidelines. This form builder will automatically include the correct HTML structure, class names, and ARIA attributes necessary for compliance.\n\nIn addition, we will modify our validation error handling to conform to the GOVUK Design System's recommended pattern, ensuring better accessibility.\n\nWe will create reusable components and helpers to streamline the development process and ensure that all forms across the application are consistent and accessible.","GenTime":"2024-08-20 00:14:04"}
{"File Name":"ios-architecture-decision-logs\/0012-use-networkManager-for-network-requests.md","Context":"## Context\\nOur team created NetworkManager for network requests. For consistency we should replace and use NetworkManager instead of Fetchable protocol\\n","Decision":"Every new interactors must use NetworkManager. Also every new endpoints should use new networker approach\\n","tokens":27,"id":4974,"text":"## Context\\nOur team created NetworkManager for network requests. For consistency we should replace and use NetworkManager instead of Fetchable protocol\\n\n\n##Decision\nEvery new interactors must use NetworkManager. Also every new endpoints should use new networker approach\\n","MatchedIds":[3050,1180,2574,1495,2359],"Predicted":"## Decision\nWe will deprecate the usage of the Fetchable protocol and standardize on using NetworkManager for all network requests in order to maintain consistency across the codebase. All existing code that relies on the Fetchable protocol will be refactored to integrate with NetworkManager.","GenTime":"2024-08-20 00:14:06"}
{"File Name":"fxa\/0016-use-graphql-and-apollo-for-settings-redesign.md","Context":"## Context and Problem Statement\\nThe [Settings Redesign project](https:\/\/jira.mozilla.com\/browse\/FXA-840) will be created [as a new React application](https:\/\/github.com\/mozilla\/fxa\/blob\/main\/docs\/adr\/0011-create-new-react-app-for-settings-redesign.md) and in turn, has opened the door to assess certain pieces of our technology stack.\\n[GraphQL](https:\/\/graphql.org\/), or GQL, is not a database query language, but is instead a query language for APIs. It _describes_ data requirements, and is a powerful alternative to REST. Some benefits can be gained by using it on top of existing REST architecture.\\n\"Apollo\" in this document refers to the \"Apollo client\" and \"Apollo server\" pieces of the Apollo platform\u00b9, which can be described as a unified data layer that enables applications to interact with data from data stores and APIs. In other words, it allows us to write and handle GraphQL on the client and server. Apollo also gives us many tools out of the box like caching.\\nThis ADR serves to lay out pros and cons of using GraphQL and Apollo in the Settings Redesign project as an alternative to hitting our conventional REST endpoints.\\n\u00b9Apollo also offers Apollo Graph Manager and Apollo Federation which are paid services, [read more from their docs](https:\/\/www.apollographql.com\/docs\/intro\/platform\/). We do not need to use these to use GQL with Apollo server or Apollo client.\\n## Decision Drivers\\n- Performance implications; consideration around the number of network requests and data transferred\\n- Ease of setup for client-side API calls\\n- Clarity around expected data, React integration, and developer tooling\\n- Development speed, both around initial setup and as new features roll out\\n","Decision":"- Performance implications; consideration around the number of network requests and data transferred\\n- Ease of setup for client-side API calls\\n- Clarity around expected data, React integration, and developer tooling\\n- Development speed, both around initial setup and as new features roll out\\nChosen option: \"B - Layer GraphQL on top of our REST architecture\", because:\\n- GQL offers performance optimizations by allowing us to consolidate our network requests by sending data requirements up in a single request, asking for only what is needed on the client, shifting the burden of figuring out how and where to gather this data from the client onto the server. The server then compensates for overfetching by only sending back what is requested.\\n- Allowing developers to query for and expect only exactly what is needed with end-to-end typing and a more \"declarative\" way of thinking towards data requirements, along with keeping data requirement schemas close to where they're consumed, makes it painfully clear what's being sent and received between client and server.\\n- Does not preclude option C, as we can replace or supplement direct calls to the FxA auth-server down the line. This is faster for initial development, and this option over option C will also help mitigate risk around using a relatively novel piece of technology for FxA.\\n- Nice side-effects include the GQL playground, managing only a single API endpoint, and the ability to store both local state and network data in the Apollo cache.\\n","tokens":366,"id":387,"text":"## Context and Problem Statement\\nThe [Settings Redesign project](https:\/\/jira.mozilla.com\/browse\/FXA-840) will be created [as a new React application](https:\/\/github.com\/mozilla\/fxa\/blob\/main\/docs\/adr\/0011-create-new-react-app-for-settings-redesign.md) and in turn, has opened the door to assess certain pieces of our technology stack.\\n[GraphQL](https:\/\/graphql.org\/), or GQL, is not a database query language, but is instead a query language for APIs. It _describes_ data requirements, and is a powerful alternative to REST. Some benefits can be gained by using it on top of existing REST architecture.\\n\"Apollo\" in this document refers to the \"Apollo client\" and \"Apollo server\" pieces of the Apollo platform\u00b9, which can be described as a unified data layer that enables applications to interact with data from data stores and APIs. In other words, it allows us to write and handle GraphQL on the client and server. Apollo also gives us many tools out of the box like caching.\\nThis ADR serves to lay out pros and cons of using GraphQL and Apollo in the Settings Redesign project as an alternative to hitting our conventional REST endpoints.\\n\u00b9Apollo also offers Apollo Graph Manager and Apollo Federation which are paid services, [read more from their docs](https:\/\/www.apollographql.com\/docs\/intro\/platform\/). We do not need to use these to use GQL with Apollo server or Apollo client.\\n## Decision Drivers\\n- Performance implications; consideration around the number of network requests and data transferred\\n- Ease of setup for client-side API calls\\n- Clarity around expected data, React integration, and developer tooling\\n- Development speed, both around initial setup and as new features roll out\\n\n\n##Decision\n- Performance implications; consideration around the number of network requests and data transferred\\n- Ease of setup for client-side API calls\\n- Clarity around expected data, React integration, and developer tooling\\n- Development speed, both around initial setup and as new features roll out\\nChosen option: \"B - Layer GraphQL on top of our REST architecture\", because:\\n- GQL offers performance optimizations by allowing us to consolidate our network requests by sending data requirements up in a single request, asking for only what is needed on the client, shifting the burden of figuring out how and where to gather this data from the client onto the server. The server then compensates for overfetching by only sending back what is requested.\\n- Allowing developers to query for and expect only exactly what is needed with end-to-end typing and a more \"declarative\" way of thinking towards data requirements, along with keeping data requirement schemas close to where they're consumed, makes it painfully clear what's being sent and received between client and server.\\n- Does not preclude option C, as we can replace or supplement direct calls to the FxA auth-server down the line. This is faster for initial development, and this option over option C will also help mitigate risk around using a relatively novel piece of technology for FxA.\\n- Nice side-effects include the GQL playground, managing only a single API endpoint, and the ability to store both local state and network data in the Apollo cache.\\n","MatchedIds":[389,381,362,334,4378],"Predicted":"## Decision\n- Performance implications; consideration around the number of network requests and data transferred\n- Ease of setup for client-side API calls\n- Clarity around expected data, React integration, and developer tooling\n- Development speed, both around initial setup and as new features roll out\nChosen option: Use GraphQL with Apollo for the Settings Redesign project.\n\n### Justification\n- **Flexibility and Performance**: GraphQL allows clients to specify the shape of the response, leading to efficient network usage by fetching only the required data in a single request.\n- **Ease of Setup and Integration**: Apollo Client provides extensive developer tools, built-in caching, and seamless integration with React, which will simplify client-side API calls.\n- **Declarative Data Fetching**: GraphQL's declarative nature clarifies expected data and helps to avoid over-fetching and under-fetching issues commonly seen with REST endpoints.\n- **Modular and Maintainable Backend**: Apollo Server supports schema stitching, allowing us to merge multiple sources into a single GraphQL schema, enabling greater modularity and maintainability.\n- **Development Speed and Tooling**: The Apollo platform has rich developer tooling that aids in query writing, debug, and performance monitoring, significantly boosting development speed.\n\n### Positive Consequences\n- Reduced number of network requests due to ability to fetch multiple resources in a single query.\n- Enhanced developer experience and productivity using Apollo's robust toolset.\n- Easy-to-read and maintain API contracts by using GraphQL schemas.\n- Better performance and efficient data transfer due to the granularity of fetched data.\n\n### Negative Consequences\n- Initial learning curve for team members who are not familiar with GraphQL and Apollo.\n- Need to maintain an additional layer (GraphQL server) alongside existing REST services.\n- Potential for increased complexity in query and schema management compared to simpler REST endpoints.\n\nOverall, the expected improvements in performance, developer productivity, and clarity around data interactions justify the decision to use GraphQL with Apollo for the Settings Redesign project.","GenTime":"2024-08-20 00:14:14"}
{"File Name":"Maud\/0018-backwards-differential-solver.md","Context":"## Context\\nTo determine the steady state conditions of the model, we require an ODE solver.\\nWe switched to the adjoint ODE solver for faster gradient evaluations,\\nhowever, recent use suggests that the solver is unstable and fails silently.\\n","Decision":"Reverting to the ode_bdf_tol() solver implemented in Stan still solves\\nstiff ODE problems but slightly slower. This is overlooked as the current\\nadjoint solver is too unstable to use and fails on larger systems. The current\\nexample files in the `tests\/data\/` folder work appropriately, however, larger\\nsystems fail with the step size approaching 0.\\nAn attempt was made to make the flux calculations in the ode more stable by\\nusing built in functions and converting products to sums of logs etc.\\nThis did not help the adjoint solver and hence this revertion was made.\\nFuture versions of Stan can easily be tested by reverting to the previous\\nadjoint solver specification and the input files will still accept the tolerances\\nfor the solver.\\n","tokens":51,"id":239,"text":"## Context\\nTo determine the steady state conditions of the model, we require an ODE solver.\\nWe switched to the adjoint ODE solver for faster gradient evaluations,\\nhowever, recent use suggests that the solver is unstable and fails silently.\\n\n\n##Decision\nReverting to the ode_bdf_tol() solver implemented in Stan still solves\\nstiff ODE problems but slightly slower. This is overlooked as the current\\nadjoint solver is too unstable to use and fails on larger systems. The current\\nexample files in the `tests\/data\/` folder work appropriately, however, larger\\nsystems fail with the step size approaching 0.\\nAn attempt was made to make the flux calculations in the ode more stable by\\nusing built in functions and converting products to sums of logs etc.\\nThis did not help the adjoint solver and hence this revertion was made.\\nFuture versions of Stan can easily be tested by reverting to the previous\\nadjoint solver specification and the input files will still accept the tolerances\\nfor the solver.\\n","MatchedIds":[4868,233,545,5171,2932],"Predicted":"## Decision\nWe will revert to using the BDF (Backward Differentiation Formula) ODE solver for determining the steady state conditions of the model. BDF solvers are known for their stability when dealing with stiff equations, which makes them more reliable in our context. Further, we will implement rigorous logging and error-checking mechanisms to catch any potential failures promptly.","GenTime":"2024-08-20 00:14:18"}
{"File Name":"raster-foundry\/adr-0013-spark-execution-environment.md","Context":"## Context\\nPart of the Raster Foundry processing workflow requires a source imagery ingest into a GeoTrellis Catalog. GeoTrellis depends on Apache Spark to parallelize work during an ingest, and that work typically occurs on an Apache Spark enabled compute cluster.\\nPrior iterations of Raster Foundry attempted ingests with an Apache YARN managed compute cluster via Amazon Elastic MapReduce (EMR) on a per-request basis. Unfortunately, that didn't perform well due to the overhead of bootstrapping an EMR cluster.\\nOur goal for the this iteration aims to keep some Spark cluster components active at all times through a combination of spot pricing and usage based cluster auto-scaling. This approach should help minimize cluster bootstrapping durations and keep cost-incurring cluster resources at a minimum.\\n","Decision":"Given that we are still early in Raster Foundry's product development, the desire to keep costs low and ship quickly carry significant weight. In order to meet those objectives, but still minimize cluster bootstrapping overhead, the first pass at having Spark enabled resources on standby will consist of a shared (across staging and production) Amazon EMR cluster.\\nThe overall Raster Foundry infrastructure will be split across three Amazon Virtual Private Clouds (VPCs):\\n- Staging (`10.0.0.0\/18`, 16382 IPs)\\n- Spark (`10.0.64.0\/18`, 16382 IPs)\\n- Production (`10.0.128.0\/18`, 16382 IPs)\\nThe `Staging` and `Production` VPCs will be peered with the `Spark` VPC to allow direct private network access, if necessary. Regardless, public facing APIs such as the Amazon EMR's Step API, or the Raster Foundry API can be used to pass information between systems.\\nDespite having thought long and hard about the consequences, I suspect that the section below does not cover all of the issues that lie ahead of us. The hope is that at least these core ideas remain in future iterations of the Spark execution environment:\\n- Shared cluster resources between environments\\n- Isolated environments at the VPC level\\n- Build on top of the Amazon EMR platform\\n- Use spot pricing and auto-scaling aggressively to keep costs low\\n","tokens":162,"id":1681,"text":"## Context\\nPart of the Raster Foundry processing workflow requires a source imagery ingest into a GeoTrellis Catalog. GeoTrellis depends on Apache Spark to parallelize work during an ingest, and that work typically occurs on an Apache Spark enabled compute cluster.\\nPrior iterations of Raster Foundry attempted ingests with an Apache YARN managed compute cluster via Amazon Elastic MapReduce (EMR) on a per-request basis. Unfortunately, that didn't perform well due to the overhead of bootstrapping an EMR cluster.\\nOur goal for the this iteration aims to keep some Spark cluster components active at all times through a combination of spot pricing and usage based cluster auto-scaling. This approach should help minimize cluster bootstrapping durations and keep cost-incurring cluster resources at a minimum.\\n\n\n##Decision\nGiven that we are still early in Raster Foundry's product development, the desire to keep costs low and ship quickly carry significant weight. In order to meet those objectives, but still minimize cluster bootstrapping overhead, the first pass at having Spark enabled resources on standby will consist of a shared (across staging and production) Amazon EMR cluster.\\nThe overall Raster Foundry infrastructure will be split across three Amazon Virtual Private Clouds (VPCs):\\n- Staging (`10.0.0.0\/18`, 16382 IPs)\\n- Spark (`10.0.64.0\/18`, 16382 IPs)\\n- Production (`10.0.128.0\/18`, 16382 IPs)\\nThe `Staging` and `Production` VPCs will be peered with the `Spark` VPC to allow direct private network access, if necessary. Regardless, public facing APIs such as the Amazon EMR's Step API, or the Raster Foundry API can be used to pass information between systems.\\nDespite having thought long and hard about the consequences, I suspect that the section below does not cover all of the issues that lie ahead of us. The hope is that at least these core ideas remain in future iterations of the Spark execution environment:\\n- Shared cluster resources between environments\\n- Isolated environments at the VPC level\\n- Build on top of the Amazon EMR platform\\n- Use spot pricing and auto-scaling aggressively to keep costs low\\n","MatchedIds":[4388,1012,1688,2471,2516],"Predicted":"## Decision\n\nTo optimize the Raster Foundry processing workflow and minimize costs while avoiding the overhead of bootstrapping an EMR cluster on a per-request basis, we will implement the following solution:\n\n1. **Persistent Spark Cluster with Auto-Scaling:**\n   - Maintain a persistent Spark cluster using Amazon EMR with cluster auto-scaling. This cluster will be configured to scale up during times of high demand and scale down during low usage periods.\n   - Use spot instances for worker nodes where feasible to take advantage of reduced pricing, thereby lowering operational costs. On-demand instances will be used for core nodes to ensure stability and availability.\n\n2. **Cluster Configuration:**\n   - The master node and a minimal set of core nodes will always remain active to minimize the time required for spinning up new worker nodes during high demand.\n   - Auto-scaling policies will be defined based on workload metrics such as CPU utilization and task backlog in Spark, allowing dynamic resource allocation.\n\n3. **Job Scheduling and Resource Management:**\n   - Utilize Amazon EMR Managed Scaling, which automatically resizes the cluster for best performance at the lowest possible cost.\n   - Implement a retry mechanism for spot instance terminations to ensure job resilience.\n\n4. **Deployment and Monitoring:**\n   - Integrate with Amazon CloudWatch for continuous monitoring and alerting based on the health and performance metrics of the Spark cluster.\n   - Use AWS Lambda for customized job triggers, auto-scaling adjustments, and to handle spot instance interruptions gracefully.\n\n5. **Cost Management:**\n   - Leverage reserved instances and savings plans for predictable workloads to further optimize costs.\n   - Regularly review and adjust instance types and scaling policies based on usage patterns and cost-performance analysis.\n\n6. **Documentation and Training:**\n   - Provide comprehensive documentation and training for the operational team on managing the persistent Spark cluster, auto-scaling configurations, and spot instance management practices.\n\nBy maintaining a persistent Spark cluster with auto-scaling capabilities and leveraging spot pricing, we aim to achieve efficient resource utilization and cost-effective processing for GeoTrellis ingests in the Raster Foundry workflow.","GenTime":"2024-08-20 00:14:26"}
{"File Name":"gatemint-sdk\/adr-014-proportional-slashing.md","Context":"## Context\\nIn Proof of Stake-based chains, centralization of consensus power amongst a small set of validators can cause harm to the network due to increased risk of censorship, liveness failure, fork attacks, etc.  However, while this centralization causes a negative externality to the network, it is not directly felt by the delegators contributing towards delegating towards already large validators.  We would like a way to pass on the negative externality cost of centralization onto those large validators and their delegators.\\n","Decision":"### Design\\nTo solve this problem, we will implement a procedure called Proportional Slashing.  The desire is that the larger a validator is, the more they should be slashed.  The first naive attempt is to make a validator's slash percent proportional to their share of consensus voting power.\\n```\\nslash_amount = k * power \/\/ power is the faulting validator's voting power and k is some on-chain constant\\n```\\nHowever, this will incentivize validators with large amounts of stake to split up their voting power amongst accounts, so that if they fault, they all get slashed at a lower percent.  The solution to this is to take into account not just a validator's own voting percentage, but also the voting percentage of all the other validators who get slashed in a specified time frame.\\n```\\nslash_amount = k * (power_1 + power_2 + ... + power_n) \/\/ where power_i is the voting power of the ith validator faulting in the specified time frame and k is some on-chain constant\\n```\\nNow, if someone splits a validator of 10% into two validators of 5% each which both fault, then they both fault in the same time frame, they both will still get slashed at the sum 10% amount.\\nHowever, an operator might still choose to split up their stake across multiple accounts with hopes that if any of them fault independently, they will not get slashed at the full amount.  In the case that the validators do fault together, they will get slashed the same amount as if they were one entity.  There is no con to splitting up.  However, if operators are going to split up their stake without actually decorrelating their setups, this also causes a negative externality to the network as it fills up validator slots that could have gone to others or increases the commit size.  In order to disincentivize this, we want it to be the case such that splitting up a validator into multiple validators and they fault together is punished more heavily that keeping it as a single validator that faults.\\nWe can achieve this by not only taking into account the sum of the percentages of the validators that faulted, but also the *number* of validators that faulted in the window.  One general form for an equation that fits this desired property looks like this:\\n```\\nslash_amount = k * ((power_1)^(1\/r) + (power_2)^(1\/r) + ... + (power_n)^(1\/r))^r \/\/ where k and r are both on-chain constants\\n```\\nSo now, for example, assuming k=1 and r=2, if one validator of 10% faults, it gets a 10% slash, while if two validators of 5% each fault together, they both get a 20% slash ((sqrt(0.05)+sqrt(0.05))^2).\\n#### Correlation across non-sybil validators\\nOne will note, that this model doesn't differentiate between multiple validators run by the same operators vs validators run by different operators.  This can be seen as an additional benefit in fact.  It incentivizes validators to differentiate their setups from other validators, to avoid having correlated faults with them or else they risk a higher slash.  So for example, operators should avoid using the same popular cloud hosting platforms or using the same Staking as a Service providers.  This will lead to a more resilient and decentralized network.\\n#### Parameterization\\nThe value of k and r can be different for different types of slashable faults.  For example, we may want to punish liveness faults 10% as severely as double signs.\\nThere can also be minimum and maximums put in place in order to bound the size of the slash percent.\\n#### Griefing\\nGriefing, the act of intentionally being slashed to make another's slash worse, could be a concern here.  However, using the protocol described here, the attacker could not substantially grief without getting slashed a substantial amount themselves.  The larger the validator is, the more heavily it can impact the slash, it needs to be non-trivial to have a significant impact on the slash percent.  Furthermore, the larger the grief, the griefer loses quadratically more.\\nIt may also be possible to, rather than the k and r factors being constants, perhaps using an inverse gini coefficient may mitigate some griefing attacks, but this an area for future research.\\n### Implementation\\nIn the slashing module, we will add two queues that will track all of the recent slash events.  For double sign faults, we will define \"recent slashes\" as ones that have occured within the last `unbonding period`.  For liveness faults, we will define \"recent slashes\" as ones that have occured withing the last `jail period`.\\n```\\ntype SlashEvent struct {\\nAddress                     sdk.ValAddress\\nSqrtValidatorVotingPercent  sdk.Dec\\nSlashedSoFar                sdk.Dec\\n}\\n```\\nThese slash events will be pruned from the queue once they are older than their respective \"recent slash period\".\\nWhenever a new slash occurs, a `SlashEvent` struct is created with the faulting validator's voting percent and a `SlashedSoFar` of 0.  Because recent slash events are pruned before the unbonding period and unjail period expires, it should not be possible for the same validator to have multiple SlashEvents in the same Queue at the same time.\\nWe then will iterate over all the SlashEvents in the queue, adding their `SqrtValidatorVotingPercent` and squaring the result to calculate the new percent to slash all the validators in the queue at, using the \"Square of Sum of Roots\" formula introduced above.\\nOnce we have the `NewSlashPercent`, we then iterate over all the `SlashEvent`s in the queue once again, and if `NewSlashPercent > SlashedSoFar` for that SlashEvent, we call the `staking.Slash(slashEvent.Address, slashEvent.Power, Math.Min(Math.Max(minSlashPercent, NewSlashPercent - SlashedSoFar), maxSlashPercent)` (we pass in the power of the validator before any slashes occured, so that we slash the right amount of tokens).  We then set `SlashEvent.SlashedSoFar` amount to `NewSlashPercent`.\\n","tokens":104,"id":28,"text":"## Context\\nIn Proof of Stake-based chains, centralization of consensus power amongst a small set of validators can cause harm to the network due to increased risk of censorship, liveness failure, fork attacks, etc.  However, while this centralization causes a negative externality to the network, it is not directly felt by the delegators contributing towards delegating towards already large validators.  We would like a way to pass on the negative externality cost of centralization onto those large validators and their delegators.\\n\n\n##Decision\n### Design\\nTo solve this problem, we will implement a procedure called Proportional Slashing.  The desire is that the larger a validator is, the more they should be slashed.  The first naive attempt is to make a validator's slash percent proportional to their share of consensus voting power.\\n```\\nslash_amount = k * power \/\/ power is the faulting validator's voting power and k is some on-chain constant\\n```\\nHowever, this will incentivize validators with large amounts of stake to split up their voting power amongst accounts, so that if they fault, they all get slashed at a lower percent.  The solution to this is to take into account not just a validator's own voting percentage, but also the voting percentage of all the other validators who get slashed in a specified time frame.\\n```\\nslash_amount = k * (power_1 + power_2 + ... + power_n) \/\/ where power_i is the voting power of the ith validator faulting in the specified time frame and k is some on-chain constant\\n```\\nNow, if someone splits a validator of 10% into two validators of 5% each which both fault, then they both fault in the same time frame, they both will still get slashed at the sum 10% amount.\\nHowever, an operator might still choose to split up their stake across multiple accounts with hopes that if any of them fault independently, they will not get slashed at the full amount.  In the case that the validators do fault together, they will get slashed the same amount as if they were one entity.  There is no con to splitting up.  However, if operators are going to split up their stake without actually decorrelating their setups, this also causes a negative externality to the network as it fills up validator slots that could have gone to others or increases the commit size.  In order to disincentivize this, we want it to be the case such that splitting up a validator into multiple validators and they fault together is punished more heavily that keeping it as a single validator that faults.\\nWe can achieve this by not only taking into account the sum of the percentages of the validators that faulted, but also the *number* of validators that faulted in the window.  One general form for an equation that fits this desired property looks like this:\\n```\\nslash_amount = k * ((power_1)^(1\/r) + (power_2)^(1\/r) + ... + (power_n)^(1\/r))^r \/\/ where k and r are both on-chain constants\\n```\\nSo now, for example, assuming k=1 and r=2, if one validator of 10% faults, it gets a 10% slash, while if two validators of 5% each fault together, they both get a 20% slash ((sqrt(0.05)+sqrt(0.05))^2).\\n#### Correlation across non-sybil validators\\nOne will note, that this model doesn't differentiate between multiple validators run by the same operators vs validators run by different operators.  This can be seen as an additional benefit in fact.  It incentivizes validators to differentiate their setups from other validators, to avoid having correlated faults with them or else they risk a higher slash.  So for example, operators should avoid using the same popular cloud hosting platforms or using the same Staking as a Service providers.  This will lead to a more resilient and decentralized network.\\n#### Parameterization\\nThe value of k and r can be different for different types of slashable faults.  For example, we may want to punish liveness faults 10% as severely as double signs.\\nThere can also be minimum and maximums put in place in order to bound the size of the slash percent.\\n#### Griefing\\nGriefing, the act of intentionally being slashed to make another's slash worse, could be a concern here.  However, using the protocol described here, the attacker could not substantially grief without getting slashed a substantial amount themselves.  The larger the validator is, the more heavily it can impact the slash, it needs to be non-trivial to have a significant impact on the slash percent.  Furthermore, the larger the grief, the griefer loses quadratically more.\\nIt may also be possible to, rather than the k and r factors being constants, perhaps using an inverse gini coefficient may mitigate some griefing attacks, but this an area for future research.\\n### Implementation\\nIn the slashing module, we will add two queues that will track all of the recent slash events.  For double sign faults, we will define \"recent slashes\" as ones that have occured within the last `unbonding period`.  For liveness faults, we will define \"recent slashes\" as ones that have occured withing the last `jail period`.\\n```\\ntype SlashEvent struct {\\nAddress                     sdk.ValAddress\\nSqrtValidatorVotingPercent  sdk.Dec\\nSlashedSoFar                sdk.Dec\\n}\\n```\\nThese slash events will be pruned from the queue once they are older than their respective \"recent slash period\".\\nWhenever a new slash occurs, a `SlashEvent` struct is created with the faulting validator's voting percent and a `SlashedSoFar` of 0.  Because recent slash events are pruned before the unbonding period and unjail period expires, it should not be possible for the same validator to have multiple SlashEvents in the same Queue at the same time.\\nWe then will iterate over all the SlashEvents in the queue, adding their `SqrtValidatorVotingPercent` and squaring the result to calculate the new percent to slash all the validators in the queue at, using the \"Square of Sum of Roots\" formula introduced above.\\nOnce we have the `NewSlashPercent`, we then iterate over all the `SlashEvent`s in the queue once again, and if `NewSlashPercent > SlashedSoFar` for that SlashEvent, we call the `staking.Slash(slashEvent.Address, slashEvent.Power, Math.Min(Math.Max(minSlashPercent, NewSlashPercent - SlashedSoFar), maxSlashPercent)` (we pass in the power of the validator before any slashes occured, so that we slash the right amount of tokens).  We then set `SlashEvent.SlashedSoFar` amount to `NewSlashPercent`.\\n","MatchedIds":[1944,4101,839,3768,841],"Predicted":"## Decision\nTo address the centralization issue in Proof of Stake (PoS)-based chains and pass on the negative externality cost of centralization to large validators and their delegators, we will implement a dynamic commission rate mechanism. This mechanism will increase the commission rate proportionally to the validator's share of the total stake, thus discouraging disproportionate centralization and encouraging delegation towards smaller validators.\n\n### Approach\n1. **Dynamic Commission Rate Calculation**:\n    - Implement a formula to determine the commission rate based on a validator's share of the total stake.\n    - The commission rate \\( C \\) for a validator will increase as the validator's share \\( S \\) of the total stake increases. \n    - Formula: \\( C = C_{\\text{base}} + k \\times (S - S_{\\text{threshold}}) \\)\n        - \\( C_{\\text{base}} \\): Base commission rate.\n        - \\( k \\): A constant that determines the rate of increase in the commission rate concerning the validator's share.\n        - \\( S \\): Validator's share of the total stake.\n        - \\( S_{\\text{threshold}} \\): A predefined threshold share of the total stake, above which the dynamic commission rate starts applying.\n\n2. **Commission Rate Caps and Floors**:\n    - Define a minimum (floor) and maximum (cap) commission rates to prevent extremely low or high commission rates.\n    - For example, the floor could be set at 5% and the cap at 20%.\n\n3. **Adjustments and Governance**:\n    - The parameters \\( C_{\\text{base}} \\), \\( k \\), and \\( S_{\\text{threshold}} \\) will be adjustable through governance proposals, allowing the community to adjust to changing network conditions.\n\n4. **Metrics and Reporting**:\n    - Introduce metrics and reporting tools to monitor validator shares and commission rates.\n    - Provide transparency so delegators can make informed decisions when choosing validators.\n\n### Example\nAssume the base commission rate \\( C_{\\text{base}} \\) is 5%, the constant \\( k \\) is 1, and the threshold \\( S_{\\text{threshold}} \\) is 5%:\n- If a validator\u2019s share \\( S \\) is 4%, the commission rate \\( C \\) remains at the base rate of 5%.\n- If a validator\u2019s share \\( S \\) increases to 7%, the commission rate \\( C \\) will be:\n  \\[\n  C = 5\\% + 1 \\times (7\\% - 5\\%) = 7\\%\n  \\]\n- If a validator\u2019s share \\( S \\) climbs to 15%, the commission rate \\( C \\) will be capped at the defined maximum (e.g., 20%).\n\n### Implementation\n1. **Modify Staking Module**:\n    - Update the staking module to store and calculate the dynamic commission rate during the delegation process.\n\n2. **Validators\u2019 Web Dashboard**:\n    - Update validators' dashboards to show the current dynamic commission rates and how close they are to crossing certain thresholds.\n\n3. **Documentation and Communication**:\n    - Provide documentation to explain the rationale, benefits, and details of the dynamic commission rates.\n    - Engage in community communication to discuss the changes and gather feedback.\n\nBy implementing dynamic commission rates, we align the incentives of delegators and validators with the network's health, helping to mitigate the risks associated with centralization in PoS-based blockchains.","GenTime":"2024-08-20 00:14:41"}
{"File Name":"atlasdb\/0007-use-cql-for-column-paging-for-sweep.md","Context":"## Context\\nAs of version 0.12.0, our implementation of sweep for Cassandra relied upon the getPageWithRangeCreator method of\\nCassandraKeyValueService, which fetches values for all columns and timestamps from Cassandra. In cases where a cell with\\na large value was overwritten (with other large values) many times, this caused Cassandra to run out of memory, even\\nif only a single row was fetched. (internal issue 44272).\\nWe needed a way to run sweep without Cassandra running out of memory in such cases.\\n","Decision":"We are unaware of any way to fetch columns and timestamps without also temporarily loading values into memory (within\\nCassandra). Therefore, to avoid running out of memory, we needed to make it possible to fetch only a certain number of\\ncells (rather than rows) from Cassandra at once.\\nWe decided to introduce a more granular batching solution. In particular, we decided to page through the\\ncolumns for each row. Briefly, the algorithm implemented is as follows:\\n1. Fetch a number of rows equal to the row batch size (but only one column per row).\\n2. Use CQL queries with limits to collect the <column, timestamp> pairs.\\nFurther, since performing a number of additional CQL queries for every row will be less efficient, and unnecessary for\\nmost users, we decided to add an optional parameter, timestampsGetterBatchSize, in CassandraKVSConfig, and use the method\\ndescribed above only if this parameter is set.\\nWe chose CQL queries over thrift because CQL queries are simpler to write, and have been empirically shown to have\\nhigher throughput.\\n","tokens":112,"id":3090,"text":"## Context\\nAs of version 0.12.0, our implementation of sweep for Cassandra relied upon the getPageWithRangeCreator method of\\nCassandraKeyValueService, which fetches values for all columns and timestamps from Cassandra. In cases where a cell with\\na large value was overwritten (with other large values) many times, this caused Cassandra to run out of memory, even\\nif only a single row was fetched. (internal issue 44272).\\nWe needed a way to run sweep without Cassandra running out of memory in such cases.\\n\n\n##Decision\nWe are unaware of any way to fetch columns and timestamps without also temporarily loading values into memory (within\\nCassandra). Therefore, to avoid running out of memory, we needed to make it possible to fetch only a certain number of\\ncells (rather than rows) from Cassandra at once.\\nWe decided to introduce a more granular batching solution. In particular, we decided to page through the\\ncolumns for each row. Briefly, the algorithm implemented is as follows:\\n1. Fetch a number of rows equal to the row batch size (but only one column per row).\\n2. Use CQL queries with limits to collect the <column, timestamp> pairs.\\nFurther, since performing a number of additional CQL queries for every row will be less efficient, and unnecessary for\\nmost users, we decided to add an optional parameter, timestampsGetterBatchSize, in CassandraKVSConfig, and use the method\\ndescribed above only if this parameter is set.\\nWe chose CQL queries over thrift because CQL queries are simpler to write, and have been empirically shown to have\\nhigher throughput.\\n","MatchedIds":[3094,2122,1558,2897,1606],"Predicted":"## Decision\nWe have decided to implement a paginated approach to the sweep operation, where large values are fetched in smaller chunks rather than all at once. Specifically, we will modify the `getPageWithRangeCreator` method to support pagination and to allow specification of a maximum page size. This way, even if a cell has been overwritten with large values many times, only a manageable amount of data will be fetched into memory at any given time, preventing Cassandra from running out of memory.\n\nAdditionally, we will investigate and implement options for limiting the amount of memory each sweep operation can use, such as:\n- Implementing a maximum memory usage per sweep process and pausing or throttling the sweep if this limit is reached.\n- Using streaming APIs to process data as it's read from Cassandra rather than loading it all into memory.\n\nWe will also add metrics and monitoring to track the memory usage of sweep operations to ensure that our changes have the desired effect and to enable tuning if required. These changes will improve the robustness and reliability of the sweep process in our system.","GenTime":"2024-08-20 00:14:46"}
{"File Name":"opg-data\/0009-api-domain-structure.md","Context":"## Context\\nWe need to establish a domain structure which\\n* Is product agnostic\\n* Is consistent across the opg-data service\\n","Decision":"We will adopt the pattern:\\n[pull-request-id].[account-stage].[microservice-domain].api.opg.service.justive.gov.uk\\nWhere [pull-request-id] is for ephemeral dev environments.\\nWhere [account-stage] is the stage in our path to live\/accounts i.e. dev,pre and empty for prod services.\\nWhere [microservice-domain] is the scoped domain of the microservice\/integration in in question.\\n* hey\\n### examples\\n#### root:\\n* https:\/\/api.opg.service.justice.gov.uk\\n#### integration:\\n* https:\/\/deputy-reporting.api.opg.service.justice.gov.uk\\n#### environments per integration:\\n* https:\/\/pre.deputy-reporting.api.opg.service.justice.gov.uk\\n* https:\/\/dev.deputy-reporting.api.opg.service.justice.gov.uk\\n#### pr raised on an environment per integration:\\n* https:\/\/pr-1234.dev.deputy-reporting.api.opg.service.justice.gov.uk\\n","tokens":28,"id":2183,"text":"## Context\\nWe need to establish a domain structure which\\n* Is product agnostic\\n* Is consistent across the opg-data service\\n\n\n##Decision\nWe will adopt the pattern:\\n[pull-request-id].[account-stage].[microservice-domain].api.opg.service.justive.gov.uk\\nWhere [pull-request-id] is for ephemeral dev environments.\\nWhere [account-stage] is the stage in our path to live\/accounts i.e. dev,pre and empty for prod services.\\nWhere [microservice-domain] is the scoped domain of the microservice\/integration in in question.\\n* hey\\n### examples\\n#### root:\\n* https:\/\/api.opg.service.justice.gov.uk\\n#### integration:\\n* https:\/\/deputy-reporting.api.opg.service.justice.gov.uk\\n#### environments per integration:\\n* https:\/\/pre.deputy-reporting.api.opg.service.justice.gov.uk\\n* https:\/\/dev.deputy-reporting.api.opg.service.justice.gov.uk\\n#### pr raised on an environment per integration:\\n* https:\/\/pr-1234.dev.deputy-reporting.api.opg.service.justice.gov.uk\\n","MatchedIds":[2502,2181,4021,1180,875],"Predicted":"## Decision\nWe will adopt a domain-driven design (DDD) approach, structuring our domains based on the core business functionalities rather than product specifics. The domain structure will be encapsulated within bounded contexts to ensure clear separation and maintain consistency across the opg-data service.\n\n### Domain Organization\n1. **Core Domains:** Represent the fundamental business logic and operations.\n   - **UserManagement:** Handles user authentication, authorization, and profile management.\n   - **ApplicationProcessing:** Manages the lifecycle of applications, including submission, validation, and processing workflows.\n   - **CourseManagement:** Controls operations related to courses, providers, and training locations.\n   \n2. **Supporting Domains:** Offer functionalities that support the core business processes.\n   - **Communication:** Manages notifications, alerts, and messaging services.\n   - **Reporting:** Provides data analytics, reporting, and business intelligence functionalities.\n\n3. **Generic Domains:** Provide common services and utilities that can be reused across multiple core and supporting domains.\n   - **Logging:** Standardized logging services for consistent capturing and querying of log data.\n   - **ConfigurationManagement:** Centralized handling of application settings and configurations.\n   - **AuditTrail:** Ensures auditability and traceability of actions performed within the system.\n\n### Implementation Guidelines\n- Each domain will have clearly defined boundaries and interfaces to interact with other domains, ensuring loose coupling and high cohesion.\n- Domain models will be designed to be reusable and agnostic to specific products, allowing for easy expansion and adaptation across various use cases.\n- Consistent naming conventions and data structures will be maintained to ensure clarity and uniformity throughout the opg-data service.\n- Each domain will include comprehensive documentation, detailing its purpose, interactions, and any dependencies it may have on other domains.\n\nBy organizing our service architecture in this manner, we maintain a robust, scalable, and product-agnostic domain structure that ensures consistency and adaptability across the opg-data service.","GenTime":"2024-08-20 00:14:53"}
{"File Name":"cfo\/0002-use-yarn-install-instead-of-npm-install-for-web.md","Context":"## Context\\nI see found both `yarn.lock` and `package-lock.json` in the repository.\\n","Decision":"Sticking to `yarn` for `web` for now.\\n","tokens":23,"id":1320,"text":"## Context\\nI see found both `yarn.lock` and `package-lock.json` in the repository.\\n\n\n##Decision\nSticking to `yarn` for `web` for now.\\n","MatchedIds":[2924,3827,3841,3721,3328],"Predicted":"## Decision\nRemove `package-lock.json` from the repository and standardize on using `yarn.lock` for dependency management in all environments. Update documentation to reflect the use of Yarn as the package manager and make sure all team members are aware of this change.","GenTime":"2024-08-20 00:14:55"}
{"File Name":"Data-Platform-Playbook\/010-using-pytest-for-verifying-pyspark-transformations.md","Context":"## Context\\nThe Data Platform team has been writing Apache Spark jobs using PySpark to transform data within the platform.\\nExamples include:\\n- Address matching\\n- Address cleaning\\n- Repairs sheets data cleaning\\nThese jobs lack automated tests, which has meant that debugging these scripts has involved slow feedback loops, running against actual data within the platform.\\nBy introducing testing practices, frameworks and tools we hope to:\\n- Improve the speed at which PySpark scripts can be developed\\n- Provide documentation for each script with example data they expect, and what results they output\\n- Increase the proportion of defects found before they reach staging environment\\n","Decision":"We will:\\n- Use a Python testing framework, [pytest][pytest]\\n- Use the same [Docker container][awsglue-dockerhub] we use for the Jypiter Notebook for running the tests, as it replicates the AWS Glue Spark environment locally.\\n- Integrate that framework into Apache Spark, and provide example test code\\n- Create documentation and guidance around how to productively test PySpark scripts\\n- Run the suite of Python tests as part of the deployment pipeline, and prevent failing tests from being deployed to staging\\n[pytest]: https:\/\/docs.pytest.org\\n[awsglue-dockerhub]: https:\/\/hub.docker.com\/r\/amazon\/aws-glue-libs\\n","tokens":128,"id":1012,"text":"## Context\\nThe Data Platform team has been writing Apache Spark jobs using PySpark to transform data within the platform.\\nExamples include:\\n- Address matching\\n- Address cleaning\\n- Repairs sheets data cleaning\\nThese jobs lack automated tests, which has meant that debugging these scripts has involved slow feedback loops, running against actual data within the platform.\\nBy introducing testing practices, frameworks and tools we hope to:\\n- Improve the speed at which PySpark scripts can be developed\\n- Provide documentation for each script with example data they expect, and what results they output\\n- Increase the proportion of defects found before they reach staging environment\\n\n\n##Decision\nWe will:\\n- Use a Python testing framework, [pytest][pytest]\\n- Use the same [Docker container][awsglue-dockerhub] we use for the Jypiter Notebook for running the tests, as it replicates the AWS Glue Spark environment locally.\\n- Integrate that framework into Apache Spark, and provide example test code\\n- Create documentation and guidance around how to productively test PySpark scripts\\n- Run the suite of Python tests as part of the deployment pipeline, and prevent failing tests from being deployed to staging\\n[pytest]: https:\/\/docs.pytest.org\\n[awsglue-dockerhub]: https:\/\/hub.docker.com\/r\/amazon\/aws-glue-libs\\n","MatchedIds":[4359,1681,1258,4179,1016],"Predicted":"## Decision\nWe will adopt the use of [pytest] along with the [pyspark-testing] library to write unit and integration tests for our PySpark jobs. In addition, we will utilize [tox] to automate the testing process and ensure consistency across different environments.\n\n- **pytest**: Provides a mature testing framework with rich features like fixtures, parametrization, and plugins.\n- **pyspark-testing**: Provides a suite of utilities for testing PySpark code, including the creation of Spark sessions and DataFrame comparisons.\n- **tox**: Automates the testing process and ensures that the tests run the same way across different environments. It\u2019s particularly useful in Continuous Integration (CI) pipelines.\n\nBy leveraging these tools, we aim to:\n- Reduce the feedback loop when debugging and developing PySpark scripts.\n- Provide clear documentation for each script through well-documented test cases that include example input data and expected output.\n- Identify and fix defects early in the development process, significantly reducing the likelihood of defects making it to the staging environment.\n\nAdditionally, we will establish a comprehensive testing strategy that includes:\n- Unit tests to validate individual transformations and logic.\n- Integration tests to ensure that the entire data pipeline works as expected.\n- Continuous Integration (CI) setup using tools like Jenkins or GitHub Actions to run tests automatically on every code commit.\n\nThis approach will help maintain the quality and reliability of our data transformation jobs, facilitating faster and more confident iterations.","GenTime":"2024-08-20 00:15:05"}
{"File Name":"pottery\/0003-pottery-shards-are-markdown-text.md","Context":"## Context\\nEach event recorded by Pottery (aka \"shard\") is a short piece of text.  Maybe with links, or basic formatting.\\nPottery is used on the command line, so shards need to be generated by Unix command line tools, readable on the terminal and edited in a text editor.\\n","Decision":"The content of each shard will be in Markdown format.\\n","tokens":65,"id":2510,"text":"## Context\\nEach event recorded by Pottery (aka \"shard\") is a short piece of text.  Maybe with links, or basic formatting.\\nPottery is used on the command line, so shards need to be generated by Unix command line tools, readable on the terminal and edited in a text editor.\\n\n\n##Decision\nThe content of each shard will be in Markdown format.\\n","MatchedIds":[2508,4385,1472,2215,2914],"Predicted":"## Decision\nStore each shard as a plain text file with a `.txt` extension to ensure compatibility with Unix command line tools. Utilize Markdown formatting within the text files to allow for basic formatting and links. This will ensure shards are easily readable on the terminal and editable in any standard text editor. Store these text files in a structured directory by date to maintain organization and facilitate easy navigation.","GenTime":"2024-08-20 00:15:08"}
{"File Name":"axiomatic\/0001-enabling-configuration-repo-access.md","Context":"## Context and Problem Statement\\nHow should Axiomatic ensure that dir2consul, when run as a Nomad batch job, has access to configuration repos hosted on GitHub?\\n## Decision Drivers\\n* GitHub prevents deployment key reuse. The SSH key can only be attached to one repo.\\n* There is no appetite for spending money on Service User accounts.\\n* Option 2 needs a process for creating the ssh key pair, adding the public key to the repo, and storing the private key in Vault (potentially automated via Terraform)\\n* Option 2 setup is an infrequent activity as would be key rotation\\n* Option 3 fundamentally cannot be done due to bugs\/non-support in Nomad\\n* Option 4 was eliminated due to possible violation of our acceptable use policies\\n","Decision":"* GitHub prevents deployment key reuse. The SSH key can only be attached to one repo.\\n* There is no appetite for spending money on Service User accounts.\\n* Option 2 needs a process for creating the ssh key pair, adding the public key to the repo, and storing the private key in Vault (potentially automated via Terraform)\\n* Option 2 setup is an infrequent activity as would be key rotation\\n* Option 3 fundamentally cannot be done due to bugs\/non-support in Nomad\\n* Option 4 was eliminated due to possible violation of our acceptable use policies\\nChosen option: Option 2, Axiomatic Retrieves Deployment Keys\\n### Positive Consequences\\n* Allows us to provide a repeatable solution for configurationn repos in the short term\\n* We will default to using read-only ssh access\\n### Negative Consequences\\n* Requires codifying a key management process and a potential automation effort\\n","tokens":161,"id":3595,"text":"## Context and Problem Statement\\nHow should Axiomatic ensure that dir2consul, when run as a Nomad batch job, has access to configuration repos hosted on GitHub?\\n## Decision Drivers\\n* GitHub prevents deployment key reuse. The SSH key can only be attached to one repo.\\n* There is no appetite for spending money on Service User accounts.\\n* Option 2 needs a process for creating the ssh key pair, adding the public key to the repo, and storing the private key in Vault (potentially automated via Terraform)\\n* Option 2 setup is an infrequent activity as would be key rotation\\n* Option 3 fundamentally cannot be done due to bugs\/non-support in Nomad\\n* Option 4 was eliminated due to possible violation of our acceptable use policies\\n\n\n##Decision\n* GitHub prevents deployment key reuse. The SSH key can only be attached to one repo.\\n* There is no appetite for spending money on Service User accounts.\\n* Option 2 needs a process for creating the ssh key pair, adding the public key to the repo, and storing the private key in Vault (potentially automated via Terraform)\\n* Option 2 setup is an infrequent activity as would be key rotation\\n* Option 3 fundamentally cannot be done due to bugs\/non-support in Nomad\\n* Option 4 was eliminated due to possible violation of our acceptable use policies\\nChosen option: Option 2, Axiomatic Retrieves Deployment Keys\\n### Positive Consequences\\n* Allows us to provide a repeatable solution for configurationn repos in the short term\\n* We will default to using read-only ssh access\\n### Negative Consequences\\n* Requires codifying a key management process and a potential automation effort\\n","MatchedIds":[3915,3789,3142,210,731],"Predicted":"## Decision\n\nWe will proceed with Option 2: creating an SSH key pair for each repository. Here's the implementation process:\n\n1. **SSH Key Creation**: Generate a unique SSH key pair for each GitHub repository that dir2consul needs to access.\n2. **Public Key Configuration**: Add the generated public SSH key to the corresponding GitHub repository's deployment keys.\n3. **Secret Management**: Store the private SSH key securely in Vault. This allows the Nomad job to access it securely at runtime.\n4. **Automation**: Implement this process using Terraform to automate the SSH key creation, public key addition, and Vault storage.\n\nThis approach ensures secure access to the GitHub repositories while adhering to GitHub's policy on deployment key reuse and avoiding the cost of Service User accounts. The key rotation and setup activities, though infrequent, can be efficiently managed through the automated process.","GenTime":"2024-08-20 00:15:12"}
{"File Name":"arch\/0036-ha-for-mysql.md","Context":"## Context\\n1. \u6570\u636e\u5e93\u7248\u672c 5.1\uff0c\u592a\u65e7\uff0c\u6027\u80fd\uff0c\u5b89\u5168\uff0c\u4e3b\u4ece\u590d\u5236\u90fd\u5b58\u5728\u95ee\u9898\uff1b\\n2. \u6570\u636e\u5e93\u90e8\u7f72\u5728 ECS \u4e0a\uff0c\u4f46\u78c1\u76d8\u4f7f\u7528\u7684\u662f\u666e\u901a\u4e91\u76d8\uff0cIOPS \u5df2\u5230\u9608\u503c\uff08\u4f18\u5148\u7ea7\u6700\u9ad8\uff09\uff1b\\n3. \u6570\u636e\u5e93\u4e00\u4e3b\u4e24\u4ece\uff0c\u4f46\u65e0\u9ad8\u53ef\u7528\uff1b\\n4. \u4e1a\u52a1\u7aef\u4f7f\u7528 IP \u8fde\u63a5\u4e3b\u6570\u636e\u5e93\u3002\\n","Decision":"1. \u63d0\u4ea4 Aliyun \u5de5\u5355\uff0c\u5c1d\u8bd5\u662f\u5426\u80fd\u7533\u8bf7\u4e0b 5.1 \u7248\u672c\u7684 MySQL\uff0c\u8fc1\u79fb\u6570\u636e\u81f3 RDS\uff0c\u89e3\u51b3 2\uff0c3\uff0c4 \u95ee\u9898\uff08\u6c9f\u901a\u540e\uff0c5.1 \u7248\u672c\u5df2\u4e0d\u518d\u63d0\u4f9b\uff0cPASS\uff09\uff1b\\n2. \u5c06\u90e8\u5206\u6570\u636e\u5e93\u8fc1\u79fb\u51fa\uff0c\u7f13\u89e3\u5f53\u524d MySQL \u670d\u52a1\u5668\u538b\u529b\uff0c\u7ef4\u62a4\u591a\u4e2a\u6570\u636e\u5e93\u5b9e\u4f8b\uff08\u5e76\u672a\u89e3\u51b3\u5b9e\u9645\u95ee\u9898\uff0cPASS\uff0c\u5f53\u524d\u538b\u529b\u6700\u7ec8\u786e\u8ba4\u662f\u6162\u67e5\u8be2\u539f\u56e0\uff09\uff1b\\n3. ECS \u4e0a\u81ea\u5efa HA\uff0c\u5e76\u542f\u7528\u65b0\u7684\u5b9e\u4f8b\u78c1\u76d8\u4e3a SSD\uff0c\u5207\u6362\u65b0\u5b9e\u4f8b\u4e3a Master\uff0c\u505c\u6389\u65e7\u5b9e\u4f8b\uff08\u6839\u672c\u95ee\u9898\u672a\u89e3\u51b3\uff0c\u6280\u672f\u503a\u4e00\u76f4\u5b58\u5728\uff0c\u81ea\u884c\u7ef4\u62a4\u4ecd\u7136\u5b58\u5728\u98ce\u9669\u70b9\uff09\uff1b\\n4. \u8c03\u7814 5.5 \u548c 5.1 \u7684\u5dee\u5f02\uff0c\u76f4\u63a5\u8fc1\u79fb\u81ea\u5efa\u6570\u636e\u5e93\u81f3 Aliyun RDS MySQL 5.5\u3002\\n\u9274\u4e8e\u67e5\u770b\u6587\u6863\u540e\uff0c 5.1 \u5230 5.5 \u7684\u5dee\u5f02\u6027\u5f71\u54cd\u4e0d\u5927\uff0cAliyun \u5b98\u65b9\u4e5f\u652f\u6301\u76f4\u63a5 5.1 \u5230 5.5 \u7684\u8fc1\u79fb\uff0c\u6240\u4ee5\u8ba1\u5212\u76f4\u63a5\u8fc1\u79fb\u81f3 RDS \u7684 5.5 \u7248\u672c\u3002\\n\u4e3a\u4e86\u675c\u7edd\u98ce\u9669\uff1a\\n1. \u6309\u4e1a\u52a1\u5206\u6570\u636e\u5e93\u5206\u522b\u8fc1\u79fb\uff1b\\n2. \u6240\u6709\u8fc1\u79fb\u5148\u8d70\u6d4b\u8bd5\u6570\u636e\u5e93\uff0c\u7531 QA \u505a\u5b8c\u6574\u7684\u6d4b\u8bd5\u3002\\nECS self built MySQL 5.1 to RDS 5.5 with DTS \u8fc1\u79fb\u6d41\u7a0b\uff1a\\n1. \u5728 RDS \u4e2d\u521b\u5efa\u539f MySQL \u6570\u636e\u5e93\u5bf9\u5e94\u7684\u8d26\u53f7(\u5404\u4e2a\u9879\u76ee\u8d26\u53f7\u72ec\u7acb)\uff1b\\n2. \u66f4\u65b0\u767d\u540d\u5355\uff1a\u6dfb\u52a0\u9879\u76ee\u6240\u90e8\u7f72\u7684\u670d\u52a1\u5668\uff1b\\n3. \u660e\u786e\u6570\u636e\u89c4\u6a21\uff0c\u5bf9\u540c\u6b65\u65f6\u95f4\u505a\u4e2a\u9884\u671f\uff1b\\n4. \u540c\u6b65\uff08\u5168\u91cf or \u589e\u91cf\uff09\uff0c\u660e\u786e\u65e0\u5ef6\u8fdf\u72b6\u6001\uff1b\\n5. \u66f4\u65b0\u6570\u636e\u5e93\u8fde\u63a5\u914d\u7f6e\u6587\u4ef6\uff1b\\n6. \u660e\u786e\u65e0\u5ef6\u8fdf\u72b6\u6001\uff0c\u505c\u670d\uff1b\\n7. \u786e\u5b9a\u6570\u636e\u91cf\u4e00\u81f4\uff08\u7531\u9884\u5148\u5199\u597d\u7684\u811a\u672c\u5224\u65ad\uff09(1min)\uff1b\\n8. \u5173\u95ed\u8fc1\u79fb\u670d\u52a1(10s)\uff1b\\n9. \u91cd\u542f\u670d\u52a1\u5668\uff0810s\uff09\u3002\\n6 \u81f3 9 \u6b65\u51b3\u5b9a\u6211\u4eec\u7684\u505c\u670d\u65f6\u95f4\u3002\\n\u9274\u4e8e\u6211\u4eec\u4f7f\u7528\u4ece\u5e93\u4f5c\u4e3a\u8fc1\u79fb\u7684\u6570\u636e\u6e90\uff0c\u9700\u66f4\u65b0\u5982\u4e0b\u914d\u7f6e\uff1a\\n* log-slave-updates=1\\n* binlog-format=row\\n","tokens":112,"id":2459,"text":"## Context\\n1. \u6570\u636e\u5e93\u7248\u672c 5.1\uff0c\u592a\u65e7\uff0c\u6027\u80fd\uff0c\u5b89\u5168\uff0c\u4e3b\u4ece\u590d\u5236\u90fd\u5b58\u5728\u95ee\u9898\uff1b\\n2. \u6570\u636e\u5e93\u90e8\u7f72\u5728 ECS \u4e0a\uff0c\u4f46\u78c1\u76d8\u4f7f\u7528\u7684\u662f\u666e\u901a\u4e91\u76d8\uff0cIOPS \u5df2\u5230\u9608\u503c\uff08\u4f18\u5148\u7ea7\u6700\u9ad8\uff09\uff1b\\n3. \u6570\u636e\u5e93\u4e00\u4e3b\u4e24\u4ece\uff0c\u4f46\u65e0\u9ad8\u53ef\u7528\uff1b\\n4. \u4e1a\u52a1\u7aef\u4f7f\u7528 IP \u8fde\u63a5\u4e3b\u6570\u636e\u5e93\u3002\\n\n\n##Decision\n1. \u63d0\u4ea4 Aliyun \u5de5\u5355\uff0c\u5c1d\u8bd5\u662f\u5426\u80fd\u7533\u8bf7\u4e0b 5.1 \u7248\u672c\u7684 MySQL\uff0c\u8fc1\u79fb\u6570\u636e\u81f3 RDS\uff0c\u89e3\u51b3 2\uff0c3\uff0c4 \u95ee\u9898\uff08\u6c9f\u901a\u540e\uff0c5.1 \u7248\u672c\u5df2\u4e0d\u518d\u63d0\u4f9b\uff0cPASS\uff09\uff1b\\n2. \u5c06\u90e8\u5206\u6570\u636e\u5e93\u8fc1\u79fb\u51fa\uff0c\u7f13\u89e3\u5f53\u524d MySQL \u670d\u52a1\u5668\u538b\u529b\uff0c\u7ef4\u62a4\u591a\u4e2a\u6570\u636e\u5e93\u5b9e\u4f8b\uff08\u5e76\u672a\u89e3\u51b3\u5b9e\u9645\u95ee\u9898\uff0cPASS\uff0c\u5f53\u524d\u538b\u529b\u6700\u7ec8\u786e\u8ba4\u662f\u6162\u67e5\u8be2\u539f\u56e0\uff09\uff1b\\n3. ECS \u4e0a\u81ea\u5efa HA\uff0c\u5e76\u542f\u7528\u65b0\u7684\u5b9e\u4f8b\u78c1\u76d8\u4e3a SSD\uff0c\u5207\u6362\u65b0\u5b9e\u4f8b\u4e3a Master\uff0c\u505c\u6389\u65e7\u5b9e\u4f8b\uff08\u6839\u672c\u95ee\u9898\u672a\u89e3\u51b3\uff0c\u6280\u672f\u503a\u4e00\u76f4\u5b58\u5728\uff0c\u81ea\u884c\u7ef4\u62a4\u4ecd\u7136\u5b58\u5728\u98ce\u9669\u70b9\uff09\uff1b\\n4. \u8c03\u7814 5.5 \u548c 5.1 \u7684\u5dee\u5f02\uff0c\u76f4\u63a5\u8fc1\u79fb\u81ea\u5efa\u6570\u636e\u5e93\u81f3 Aliyun RDS MySQL 5.5\u3002\\n\u9274\u4e8e\u67e5\u770b\u6587\u6863\u540e\uff0c 5.1 \u5230 5.5 \u7684\u5dee\u5f02\u6027\u5f71\u54cd\u4e0d\u5927\uff0cAliyun \u5b98\u65b9\u4e5f\u652f\u6301\u76f4\u63a5 5.1 \u5230 5.5 \u7684\u8fc1\u79fb\uff0c\u6240\u4ee5\u8ba1\u5212\u76f4\u63a5\u8fc1\u79fb\u81f3 RDS \u7684 5.5 \u7248\u672c\u3002\\n\u4e3a\u4e86\u675c\u7edd\u98ce\u9669\uff1a\\n1. \u6309\u4e1a\u52a1\u5206\u6570\u636e\u5e93\u5206\u522b\u8fc1\u79fb\uff1b\\n2. \u6240\u6709\u8fc1\u79fb\u5148\u8d70\u6d4b\u8bd5\u6570\u636e\u5e93\uff0c\u7531 QA \u505a\u5b8c\u6574\u7684\u6d4b\u8bd5\u3002\\nECS self built MySQL 5.1 to RDS 5.5 with DTS \u8fc1\u79fb\u6d41\u7a0b\uff1a\\n1. \u5728 RDS \u4e2d\u521b\u5efa\u539f MySQL \u6570\u636e\u5e93\u5bf9\u5e94\u7684\u8d26\u53f7(\u5404\u4e2a\u9879\u76ee\u8d26\u53f7\u72ec\u7acb)\uff1b\\n2. \u66f4\u65b0\u767d\u540d\u5355\uff1a\u6dfb\u52a0\u9879\u76ee\u6240\u90e8\u7f72\u7684\u670d\u52a1\u5668\uff1b\\n3. \u660e\u786e\u6570\u636e\u89c4\u6a21\uff0c\u5bf9\u540c\u6b65\u65f6\u95f4\u505a\u4e2a\u9884\u671f\uff1b\\n4. \u540c\u6b65\uff08\u5168\u91cf or \u589e\u91cf\uff09\uff0c\u660e\u786e\u65e0\u5ef6\u8fdf\u72b6\u6001\uff1b\\n5. \u66f4\u65b0\u6570\u636e\u5e93\u8fde\u63a5\u914d\u7f6e\u6587\u4ef6\uff1b\\n6. \u660e\u786e\u65e0\u5ef6\u8fdf\u72b6\u6001\uff0c\u505c\u670d\uff1b\\n7. \u786e\u5b9a\u6570\u636e\u91cf\u4e00\u81f4\uff08\u7531\u9884\u5148\u5199\u597d\u7684\u811a\u672c\u5224\u65ad\uff09(1min)\uff1b\\n8. \u5173\u95ed\u8fc1\u79fb\u670d\u52a1(10s)\uff1b\\n9. \u91cd\u542f\u670d\u52a1\u5668\uff0810s\uff09\u3002\\n6 \u81f3 9 \u6b65\u51b3\u5b9a\u6211\u4eec\u7684\u505c\u670d\u65f6\u95f4\u3002\\n\u9274\u4e8e\u6211\u4eec\u4f7f\u7528\u4ece\u5e93\u4f5c\u4e3a\u8fc1\u79fb\u7684\u6570\u636e\u6e90\uff0c\u9700\u66f4\u65b0\u5982\u4e0b\u914d\u7f6e\uff1a\\n* log-slave-updates=1\\n* binlog-format=row\\n","MatchedIds":[2440,2471,3657,207,4061],"Predicted":"## Decision\n1. \u5347\u7ea7\u6570\u636e\u5e93\u5230\u6700\u65b0\u7684\u7a33\u5b9a\u7248\u672c\u4ee5\u63d0\u5347\u6027\u80fd\u548c\u5b89\u5168\u6027\uff0c\u5e76\u89e3\u51b3\u590d\u5236\u95ee\u9898\uff1b\n2. \u5c06\u6570\u636e\u5e93\u78c1\u76d8\u5347\u7ea7\u4e3a\u9ad8\u6027\u80fd\u7684 SSD \u4e91\u76d8\uff0c\u63d0\u9ad8 IOPS \u5e76\u907f\u514d\u8d44\u6e90\u74f6\u9888\uff1b\n3. \u90e8\u7f72\u4e00\u4e2a\u9ad8\u53ef\u7528\u67b6\u6784\uff0c\u4f7f\u7528\u81ea\u52a8\u6545\u969c\u8f6c\u79fb\uff08\u5982\u4f7f\u7528 RDS \u7684 Multi-AZ \u6216\u8005 MySQL \u7684 MHA\/MGR\uff09\uff1b\n4. \u4e1a\u52a1\u7aef\u6539\u4e3a\u4f7f\u7528\u6570\u636e\u5e93\u8fde\u63a5\u5b57\u7b26\u4e32\u6216\u8005\u57df\u540d\u4ee3\u66ff IP \u5730\u5740\uff0c\u4ee5\u4fbf\u66f4\u7075\u6d3b\u5730\u7ba1\u7406\u6570\u636e\u5e93\u5b9e\u4f8b\u7684\u4e3b\u4ece\u5207\u6362\u3002","GenTime":"2024-08-20 00:15:15"}
{"File Name":"dotcom-rendering\/004-emotion.md","Context":"## Context\\nUsing a CSS-in-JS library will allow us to scope styles to a component. Popular CSS-in-JS libraries include Styled Components, Emotion and Styletron.\\nStyletron generates atomic CSS, guaranteeing the smallest possible CSS output. However, it does not support language features that cannot be atomised, such as nesting and sibling selectors.\\nStyled Components and Emotion support more complex features and have a similar API (composing styled components using the `styled()` method). Emotion additionally supports adding styles to the `css` prop on React components. It is also smaller (5.66KB Gzipped vs 16.27KB for styled-components).\\n","Decision":"We will use Emotion as our CSS-in-JS library.\\n","tokens":138,"id":2655,"text":"## Context\\nUsing a CSS-in-JS library will allow us to scope styles to a component. Popular CSS-in-JS libraries include Styled Components, Emotion and Styletron.\\nStyletron generates atomic CSS, guaranteeing the smallest possible CSS output. However, it does not support language features that cannot be atomised, such as nesting and sibling selectors.\\nStyled Components and Emotion support more complex features and have a similar API (composing styled components using the `styled()` method). Emotion additionally supports adding styles to the `css` prop on React components. It is also smaller (5.66KB Gzipped vs 16.27KB for styled-components).\\n\n\n##Decision\nWe will use Emotion as our CSS-in-JS library.\\n","MatchedIds":[1125,1992,157,5090,3829],"Predicted":"## Decision\nWe will use Emotion as our primary CSS-in-JS library moving forward.\n\n### Rationale:\n- **Scoping Styles to Components:** Using a CSS-in-JS approach will allow us to scope styles to individual components effectively, reducing the risk of style conflicts and improving maintainability.\n- **Comprehensive Feature Set:** Emotion supports all the necessary CSS features, including complex ones like nesting and sibling selectors, providing us with greater flexibility in our styling approach.\n- **Size Efficiency:** Emotion is smaller in size (5.66KB Gzipped) as compared to Styled Components (16.27KB Gzipped), which helps in reducing the overall bundle size and improving performance.\n- **Flexibility with `css` Prop:** Emotion offers the ability to add styles directly to the `css` prop on React components, giving us more flexibility in applying styles dynamically.\n- **Familiar API:** Emotion and Styled Components have a similar API (`styled()` method), making the transition smoother for developers who are already familiar with this pattern.\n\n### Positive Consequences:\n- **Efficient Performance:** Smaller bundle size improves the performance of our applications.\n- **Feature-Rich:** Full support for CSS features allows for more complex and fine-tuned styling.\n- **Developer Experience:** Familiar API makes it easier to onboard new developers and leverage existing experience within the team.\n- **Scope Safety:** Styles scoped to components reduce global conflicts and make the codebase more maintainable.\n\n### Negative Consequences:\n- **Learning Curve:** Some team members may need to get accustomed to Emotion if they are only familiar with other CSS-in-JS libraries.\n- **Library Evolution:** As with any third-party library, there is a dependency on the ongoing development and support of Emotion.\n\nBy using Emotion, we aim to achieve a balance between performance, flexibility, and maintainability in our styling approach.","GenTime":"2024-08-20 00:15:21"}
{"File Name":"monocle\/0007-production-ready-in-all-in-one-setup.md","Context":"## Context and Problem Statement\\nHow do we serve the API using a production ready server?\\n## Decision Drivers\\n* use production ready server.\\n* good integration with reverse proxy servers like Nginx and Apache.\\n* support Flask in a simple and performant way\\n","Decision":"* use production ready server.\\n* good integration with reverse proxy servers like Nginx and Apache.\\n* support Flask in a simple and performant way\\nChosen option: \"[option 1]\", because it is well known in the\\nindustry. It is not the most performant but it has good performances\\nto serve WSGI app like Flask thanks to the uwsgi protocol used between\\nNginx\/Apache.\\n### Negative Consequences\\n* complexify the architecture.\\n","tokens":56,"id":4642,"text":"## Context and Problem Statement\\nHow do we serve the API using a production ready server?\\n## Decision Drivers\\n* use production ready server.\\n* good integration with reverse proxy servers like Nginx and Apache.\\n* support Flask in a simple and performant way\\n\n\n##Decision\n* use production ready server.\\n* good integration with reverse proxy servers like Nginx and Apache.\\n* support Flask in a simple and performant way\\nChosen option: \"[option 1]\", because it is well known in the\\nindustry. It is not the most performant but it has good performances\\nto serve WSGI app like Flask thanks to the uwsgi protocol used between\\nNginx\/Apache.\\n### Negative Consequences\\n* complexify the architecture.\\n","MatchedIds":[583,1540,469,4833,2516],"Predicted":"## Decision\nWe will use Gunicorn to serve the API.\n\n### Rationale\n* **Production Ready**: Gunicorn is a widely-used WSGI HTTP server known for its stability and performance in production environments.\n* **Integration with Reverse Proxies**: Gunicorn integrates well with popular reverse proxy servers like Nginx and Apache, providing a robust solution for handling client requests.\n* **Flask Support**: Gunicorn has straightforward support for Flask applications, allowing us to serve our Flask API efficiently with minimal configuration.\n\n### Consequences\n* **Configuration**: We may need to spend some time configuring Gunicorn and the reverse proxy server to ensure optimal performance and security.\n* **Monitoring and Maintenance**: While Gunicorn is reliable, it will require attention to ensure that the server and application are running smoothly, especially as traffic scales.","GenTime":"2024-08-20 00:15:27"}
{"File Name":"terraform-aws-dynamodb\/0004-terraform-requirements-in-module.md","Context":"## Context\\nTerraform enables you to constrain the version of Terraform able to run a\\nmodules, as well as the minimium allowed versions of providers. Many of LIC\\nexisting modules do not leverage these configurations.\\n","Decision":"The Terraform version able to run this module will be set in `main.tf`.\\nThe miniumum allowed versions of providers will be set in `main.tf`\\n","tokens":46,"id":2536,"text":"## Context\\nTerraform enables you to constrain the version of Terraform able to run a\\nmodules, as well as the minimium allowed versions of providers. Many of LIC\\nexisting modules do not leverage these configurations.\\n\n\n##Decision\nThe Terraform version able to run this module will be set in `main.tf`.\\nThe miniumum allowed versions of providers will be set in `main.tf`\\n","MatchedIds":[621,2539,3144,2793,2409],"Predicted":"## Decision\nWe decided to enforce version constraints on both Terraform and its providers across all modules. Each module will specify the minimum and maximum supported versions of Terraform and the required providers. This ensures compatibility, predictability, and reduces the risk of unexpected issues during deployments and upgrades. A guideline document will be created to specify the proper way to define these constraints for consistency. Additionally, we will update all existing modules to include these version constraints.","GenTime":"2024-08-20 00:15:29"}
{"File Name":"DunkMe\/0003-relational-database.md","Context":"#### Context and Problem Statement\\nA relational database is required to persist the applications data.\\n#### Considered Options\\n- Microsoft SQL\\n- MySQL\\n- PostgreSQL\\n- SQLite\\n#### Decision Outcome\\nChosen option: \"Microsoft SQL\", because\\n- This is in-line with Trade Me's technical stack.\\n- Microsoft SQL is already widely and almost exclusively used at Trade Me.\\n","Decision":"Chosen option: \"Microsoft SQL\", because\\n- This is in-line with Trade Me's technical stack.\\n- Microsoft SQL is already widely and almost exclusively used at Trade Me.\\n","tokens":78,"id":1661,"text":"#### Context and Problem Statement\\nA relational database is required to persist the applications data.\\n#### Considered Options\\n- Microsoft SQL\\n- MySQL\\n- PostgreSQL\\n- SQLite\\n#### Decision Outcome\\nChosen option: \"Microsoft SQL\", because\\n- This is in-line with Trade Me's technical stack.\\n- Microsoft SQL is already widely and almost exclusively used at Trade Me.\\n\n\n##Decision\nChosen option: \"Microsoft SQL\", because\\n- This is in-line with Trade Me's technical stack.\\n- Microsoft SQL is already widely and almost exclusively used at Trade Me.\\n","MatchedIds":[1167,1740,1660,4832,4266],"Predicted":"## Decision\nChosen option: \"Microsoft SQL\", because\n- This is in-line with Trade Me's technical stack.\n- Microsoft SQL is already widely and almost exclusively used at Trade Me.\n- The familiarity with Microsoft SQL allows for better support and maintenance by the current team.\n- Integrations with other existing enterprise solutions and tools at Trade Me are seamless.\n- Microsoft SQL provides robust performance, scalability, and security features required by Trade Me applications.","GenTime":"2024-08-20 00:15:32"}
{"File Name":"Sylius\/2020_05_13_handling_non_crud_operations_in_api.md","Context":"## Context and Problem Statement\\nHandling non-CRUD operation over REST API is not trivial. Once operations are beyond Create(`POST`\/`PUT` HTTP methods),\\nRead(`GET` HTTP method), Update(`PUT`\/`PATCH` HTTP methods), Delete(`DELETE` HTTP method) there is no clear recommendation\\nhow to map others actions. These actions include, but are not limited to, changes of states (described in the form of\\nworkflow in a state machine) or command execution.\\n## Decision Drivers\\n* Solution should not be limited by its convention. We may need to support two different actions of the same name,\\nbut with different business logic. E.g., one object may have two transitions with the same name but described by\\ntwo different state machines.\\n* Solution should allow for an easy understanding of what is expected behavior of its execution.\\n* Solution should provide easy to implement a way of exposing the next possible actions, according to HATEOAS paradigm.\\n* Solution should provide a clean way of adding additional fields during the execution of requested operations.\\n","Decision":"* Solution should not be limited by its convention. We may need to support two different actions of the same name,\\nbut with different business logic. E.g., one object may have two transitions with the same name but described by\\ntwo different state machines.\\n* Solution should allow for an easy understanding of what is expected behavior of its execution.\\n* Solution should provide easy to implement a way of exposing the next possible actions, according to HATEOAS paradigm.\\n* Solution should provide a clean way of adding additional fields during the execution of requested operations.\\nThe \"Taking advantage of the `Controller` REST archetype\" should be considered as a recommended solution. All Sylius\\nusers are already familiar with it, and it is easy to understand expected behavior. Linked data references should provide\\nthe discoverability of the new endpoints. The possible operation may be sent in the `Link` header\\nor new schema should be introduced for the JSON-LD structure.\\nOption 2: \"Defining custom operations in the style of command pattern\" may be useful once async data processing is\\ndelivered with vanilla Sylius installation.\\n","tokens":225,"id":674,"text":"## Context and Problem Statement\\nHandling non-CRUD operation over REST API is not trivial. Once operations are beyond Create(`POST`\/`PUT` HTTP methods),\\nRead(`GET` HTTP method), Update(`PUT`\/`PATCH` HTTP methods), Delete(`DELETE` HTTP method) there is no clear recommendation\\nhow to map others actions. These actions include, but are not limited to, changes of states (described in the form of\\nworkflow in a state machine) or command execution.\\n## Decision Drivers\\n* Solution should not be limited by its convention. We may need to support two different actions of the same name,\\nbut with different business logic. E.g., one object may have two transitions with the same name but described by\\ntwo different state machines.\\n* Solution should allow for an easy understanding of what is expected behavior of its execution.\\n* Solution should provide easy to implement a way of exposing the next possible actions, according to HATEOAS paradigm.\\n* Solution should provide a clean way of adding additional fields during the execution of requested operations.\\n\n\n##Decision\n* Solution should not be limited by its convention. We may need to support two different actions of the same name,\\nbut with different business logic. E.g., one object may have two transitions with the same name but described by\\ntwo different state machines.\\n* Solution should allow for an easy understanding of what is expected behavior of its execution.\\n* Solution should provide easy to implement a way of exposing the next possible actions, according to HATEOAS paradigm.\\n* Solution should provide a clean way of adding additional fields during the execution of requested operations.\\nThe \"Taking advantage of the `Controller` REST archetype\" should be considered as a recommended solution. All Sylius\\nusers are already familiar with it, and it is easy to understand expected behavior. Linked data references should provide\\nthe discoverability of the new endpoints. The possible operation may be sent in the `Link` header\\nor new schema should be introduced for the JSON-LD structure.\\nOption 2: \"Defining custom operations in the style of command pattern\" may be useful once async data processing is\\ndelivered with vanilla Sylius installation.\\n","MatchedIds":[2184,4644,389,1050,4713],"Predicted":"## Decision\nChosen Option: Use custom `\/actions` endpoints for non-CRUD operations.\n\n### Positive Consequences\n- Allows flexibility to support multiple actions with the same name but different logic, as actions are specified explicitly.\n- Enhances clarity of behavior, since each action has its dedicated endpoint and can be documented separately.\n- Enables easy implementation of HATEOAS by returning possible next actions in the response of each operation.\n- Facilitates adding additional fields as required during the execution of requested operations, since each action can define its own schema.\n\n### Negative Consequences\n- May lead to an increased number of endpoints, making the API surface larger.\n- Requires careful documentation and standardization to ensure that developers understand the purpose and usage of each action endpoint.\n  \n### Implementation Details\n- For each object requiring non-CRUD operations, define an `\/actions` endpoint that encapsulates the specific behavior.\n- The endpoint should be structured as `\/objects\/{objectId}\/actions\/{actionName}`, where `{objectId}` is the identifier of the object and `{actionName}` is the name of the action to be executed.\n- Ensure responses include possible next actions as hypermedia links to adhere to HATEOAS paradigms.\n- Document each action clearly, including input fields, state changes, and expected outputs.\n\nExample:\n```\nPOST \/objects\/{objectId}\/actions\/{actionName}\nBODY { \"additionalField1\": \"value1\", \"additionalField2\": \"value2\" }\n```\n\nResponse:\n```\n{\n  \"status\": \"success\",\n  \"nextActions\": [\n    {\n      \"name\": \"nextAction1\",\n      \"link\": \"\/objects\/{objectId}\/actions\/nextAction1\"\n    },\n    {\n      \"name\": \"nextAction2\",\n      \"link\": \"\/objects\/{objectId}\/actions\/nextAction2\"\n    }\n  ]\n}\n```","GenTime":"2024-08-20 00:15:41"}
{"File Name":"island.is\/0001-use-nx.md","Context":"## Context and Problem Statement\\nWe want a monorepo tool to help us to scale development up for multiple projects and teams. It should not be too much in the way, but help us manage code, dependencies and CI\/CD.\\n## Decision Drivers\\n- Low complexity and overhead in development.\\n- Fit for our stack.\\n- Optimize CI\/CD with dependency graphs and\/or caching.\\n- Flexible.\\n","Decision":"- Low complexity and overhead in development.\\n- Fit for our stack.\\n- Optimize CI\/CD with dependency graphs and\/or caching.\\n- Flexible.\\nChosen option: \"Nx\", because:\\n- It's specially designed around our stack (TypeScript, React, Node.JS, NPM, ESLint, Prettier, Cypress, Jest, NextJS).\\n- It's relatively easy to learn with focused documentation.\\n- It has schematics to generate apps, libraries and components that includes all of our tools.\\n- It is opinionated, which gives us a good base to start developing faster. Many things can still be configured or extended.\\n","tokens":87,"id":1117,"text":"## Context and Problem Statement\\nWe want a monorepo tool to help us to scale development up for multiple projects and teams. It should not be too much in the way, but help us manage code, dependencies and CI\/CD.\\n## Decision Drivers\\n- Low complexity and overhead in development.\\n- Fit for our stack.\\n- Optimize CI\/CD with dependency graphs and\/or caching.\\n- Flexible.\\n\n\n##Decision\n- Low complexity and overhead in development.\\n- Fit for our stack.\\n- Optimize CI\/CD with dependency graphs and\/or caching.\\n- Flexible.\\nChosen option: \"Nx\", because:\\n- It's specially designed around our stack (TypeScript, React, Node.JS, NPM, ESLint, Prettier, Cypress, Jest, NextJS).\\n- It's relatively easy to learn with focused documentation.\\n- It has schematics to generate apps, libraries and components that includes all of our tools.\\n- It is opinionated, which gives us a good base to start developing faster. Many things can still be configured or extended.\\n","MatchedIds":[3521,4766,1996,4609,1680],"Predicted":"## Decision\nChosen option: Nx\n\nWe decided to use Nx as our monorepo tool because it offers a good balance between flexibility and simplicity, making it suitable for our needs across multiple projects and teams. Nx optimizes CI\/CD processes through its efficient dependency graph management and caching mechanisms. Additionally, Nx is designed to integrate well with modern JavaScript and TypeScript stacks, ensuring compatibility with our existing technologies.\n\n### Positive Consequences\n* Efficient builds and CI\/CD processes due to Nx\u2019s intelligent caching and dependency graph management.\n* Simplified management of multiple projects and teams within a single monorepo environment.\n* Consistent developer experience with low overhead and complexity.\n* Strong integration with our existing stack, ensuring minimal disruptions.\n\n### Negative Consequences\n* Initial learning curve for the team to get up to speed with Nx\u2019s features and capabilities.\n* Potential for over-reliance on Nx-specific configurations, which may complicate future migrations or changes in tooling.","GenTime":"2024-08-20 00:15:44"}
{"File Name":"pharmacy-data-etl\/0004-make-etl-re-entrant.md","Context":"## Context\\nIf the ETL if is interrupted it will need to start over again, i.e. a 6 hour ETL is stopped in the 5th hour, restarting will take another 6 hours to finish.\\n","Decision":"The ETL will be re-entrant, storing state on a regular basis.\\nRestarting the ETL will pick up from the last point.\\n","tokens":47,"id":965,"text":"## Context\\nIf the ETL if is interrupted it will need to start over again, i.e. a 6 hour ETL is stopped in the 5th hour, restarting will take another 6 hours to finish.\\n\n\n##Decision\nThe ETL will be re-entrant, storing state on a regular basis.\\nRestarting the ETL will pick up from the last point.\\n","MatchedIds":[672,1434,8,1799,3361],"Predicted":"## Decision\nTo handle interruptions more effectively and reduce total processing time in case of failures, we will implement checkpointing within the ETL process. This will involve dividing the ETL workflow into smaller, independent stages and saving the state at the end of each stage. If an interruption occurs, the ETL process can resume from the last successful checkpoint instead of starting over from the beginning.\n\nPros:\n----\n* Reduces total recovery time after interruptions.\n* Minimizes data reprocessing, which can save computational resources.\n* Improves reliability and robustness of the ETL process.\n\nCons:\n----\n* Adds complexity to the ETL workflow due to the need to manage saved states and checkpoints.\n* May require additional storage to save intermediate states.\n* Requires development effort to refactor existing ETL processes to support checkpointing.","GenTime":"2024-08-20 00:15:48"}
{"File Name":"ftd-scratch3-offline\/0017-don-t-support-scratch-images-sprites-and-image-sprite-related-blocks.md","Context":"## Context\\nScratch supports images\/sprites.\\nThe ftduino has no way to show images\/sprites.\\n","Decision":"Scratch images\/sprites and image\/sprite related blocks are not supported.\\n","tokens":23,"id":2637,"text":"## Context\\nScratch supports images\/sprites.\\nThe ftduino has no way to show images\/sprites.\\n\n\n##Decision\nScratch images\/sprites and image\/sprite related blocks are not supported.\\n","MatchedIds":[2631,2634,5198,2629,3982],"Predicted":"We will exclude image\/sprite-related operations from the scope of the ftduino implementation and focus on other supported Scratch operations. Any attempts to use image\/sprite features will result in a no-op (no operation) or a user notification about the unsupported feature.","GenTime":"2024-08-20 00:15:50"}
{"File Name":"pace-developers\/0012-matlab-python-wrapper.md","Context":"## Context\\nBoth Euphonic and Brille are PACE projects with primarily a Python user interface (UI).\\nPACE, however, aims to provide both a Python and a Matlab UI to users,\\nand also to foster inter-operability between projects which are written both in Matlab and Python.\\nIn particular, `pyHorace` ([prototype](https:\/\/github.com\/mducle\/hugo)) cannot use the\\n[standard method](https:\/\/uk.mathworks.com\/help\/matlab\/call-python-libraries.html) for Matlab to run Python code,\\nwhere calls to Python from Matlab are prefixed with `py.` followed by the full module specification.\\nFor example, `r = py.numpy.random.rand()` uses `numpy` to generate a random number.\\nThis is because such a call causes Matlab to\\n[automatically spawn](https:\/\/uk.mathworks.com\/help\/matlab\/ref\/pyenv.html) a dependent Python interpreter,\\nwhich can be either created within the same process as the Matlab interpreter (`InProcess`)\\nor in an external process (`OutOfProcess`).\\n`pyHorace` already runs within a Python interpreter and the compiled Matlab library *must* be loaded in-process.\\nThus, if Matlab spawns a second Python intepreter with the default `InProcess` execution mode,\\nthe two Python interpreters will conflict causing memory errors and a crash.\\nWe can force Matlab to launch the dependent Python interpreter `OutOfProcess`\\nbut this imposes a significant performance penalty\\n(extensive testing was not done but Brille+SpinW runs about 10x slower than with `InProcess`).\\n","Decision":"At a meeting on Jan 7 2021, the developers of `pyHorace`, `brillem` and `horace-euphonic-interface` agreed to accept this proposal.\\n`brillem` and `horace-euphonic-interface` will be refactored to use the `light_python_wrapper` proposed here.\\nThe meeting also agreed implementation details which will be described in [ADR #13](0013-light-python-wrapper-implementation-detail.md).\\n","tokens":333,"id":5171,"text":"## Context\\nBoth Euphonic and Brille are PACE projects with primarily a Python user interface (UI).\\nPACE, however, aims to provide both a Python and a Matlab UI to users,\\nand also to foster inter-operability between projects which are written both in Matlab and Python.\\nIn particular, `pyHorace` ([prototype](https:\/\/github.com\/mducle\/hugo)) cannot use the\\n[standard method](https:\/\/uk.mathworks.com\/help\/matlab\/call-python-libraries.html) for Matlab to run Python code,\\nwhere calls to Python from Matlab are prefixed with `py.` followed by the full module specification.\\nFor example, `r = py.numpy.random.rand()` uses `numpy` to generate a random number.\\nThis is because such a call causes Matlab to\\n[automatically spawn](https:\/\/uk.mathworks.com\/help\/matlab\/ref\/pyenv.html) a dependent Python interpreter,\\nwhich can be either created within the same process as the Matlab interpreter (`InProcess`)\\nor in an external process (`OutOfProcess`).\\n`pyHorace` already runs within a Python interpreter and the compiled Matlab library *must* be loaded in-process.\\nThus, if Matlab spawns a second Python intepreter with the default `InProcess` execution mode,\\nthe two Python interpreters will conflict causing memory errors and a crash.\\nWe can force Matlab to launch the dependent Python interpreter `OutOfProcess`\\nbut this imposes a significant performance penalty\\n(extensive testing was not done but Brille+SpinW runs about 10x slower than with `InProcess`).\\n\n\n##Decision\nAt a meeting on Jan 7 2021, the developers of `pyHorace`, `brillem` and `horace-euphonic-interface` agreed to accept this proposal.\\n`brillem` and `horace-euphonic-interface` will be refactored to use the `light_python_wrapper` proposed here.\\nThe meeting also agreed implementation details which will be described in [ADR #13](0013-light-python-wrapper-implementation-detail.md).\\n","MatchedIds":[5178,5172,5177,4245,864],"Predicted":"## Decision\nTo ensure seamless interoperability between Matlab and Python interfaces in PACE projects, and to avoid performance and stability issues due to multiple Python interpreters, the following approach will be adopted:\n\n1. **Use a Single Python Interpreter**: The system will ensure that only one Python interpreter instance is active. This implies that `pyHorace` and other similar setups will preload the necessary Python environment upfront, rather than relying on Matlab to spawn additional interpreters.\n   \n2. **Custom Wrapper Functions**: Develop custom Matlab wrapper functions for calling Python code. These wrappers will handle the inter-process communication and maintain a single Python interpreter session, thereby preventing conflicts. These functions would act as intermediaries, forwarding Matlab calls to the already loaded in-process Python interpreter, bypassing Matlab\u2019s automatic Python environment processing.\n\n3. **Optimize Performance**: Extensive performance testing and optimizations will be done on the custom wrapper implementation to ensure minimal overhead and efficient memory usage, mitigating the performance penalties observed with `OutOfProcess`.\n\n4. **Extensive Documentation and Examples**: Comprehensive documentation and user-friendly examples will be provided to guide the users on how to use the custom Matlab wrappers to call Python code, ensuring ease of adoption and correct usage.\n\nBy adopting this strategy, PACE can provide a robust and high-performance integration between Matlab and Python UIs without the drawbacks of multiple Python interpreter conflicts.","GenTime":"2024-08-20 00:16:02"}
{"File Name":"ockam\/0007-rust-error-handling.md","Context":"## Context\\nError handling is a very important process that is needed by every crate of the original Ockam rust library, as well as any third-party crate that was designed to supplement Ockam rust library (such as transport and vault implementations).\\nThere are multiple requirements to error handling:\\n- agile enough to be used in different cases\\n- portable to work in different environments with different constraints\\n- convenient for both library developer and library user\\n","Decision":"In search of balance between above-mentioned requirements it has been decided that errors are handled in native to Rust way of using Result type, Error type for such Result should be either of ockam_core::Error of implement Into<ockam_core::Error>\\n","tokens":91,"id":4536,"text":"## Context\\nError handling is a very important process that is needed by every crate of the original Ockam rust library, as well as any third-party crate that was designed to supplement Ockam rust library (such as transport and vault implementations).\\nThere are multiple requirements to error handling:\\n- agile enough to be used in different cases\\n- portable to work in different environments with different constraints\\n- convenient for both library developer and library user\\n\n\n##Decision\nIn search of balance between above-mentioned requirements it has been decided that errors are handled in native to Rust way of using Result type, Error type for such Result should be either of ockam_core::Error of implement Into<ockam_core::Error>\\n","MatchedIds":[4656,3159,4650,4539,1924],"Predicted":"## Decision\nWe will use a unified error handling pattern across the Ockam Rust library and any supplementary third-party crates. The approach will include:\n\n### Error Enumeration\nEvery crate will define its own enumeration of possible errors. Each variant will encompass a distinct error type relevant to the functionalities of the crate. This enumeration should implement the `std::error::Error` trait, allowing for compatibility with Rust's standard error handling utilities.\n\nExample:\n```rust\n#[derive(Debug, Error)]\npub enum VaultError {\n    #[error(\"Invalid key length\")]\n    InvalidKeyLength,\n    #[error(\"Encryption failed\")]\n    EncryptionFailed,\n    #[error(\"Decryption failed\")]\n    DecryptionFailed,\n}\n```\n\n### Contextual Error Handling\nWe will utilize the `thiserror` crate to simplify the creation and management of error enums and the `anyhow` crate for contexts where errors propagate through multiple layers and need contextual information.\n\nExample:\n```rust\nuse thiserror::Error;\nuse anyhow::{Context, Result};\n\n#[derive(Debug, Error)]\npub enum TransportError {\n    #[error(\"Connection timeout\")]\n    ConnectionTimeout,\n    #[error(\"Invalid address\")]\n    InvalidAddress,\n}\n\nfn do_something() -> Result<()> {\n    Err(TransportError::ConnectionTimeout).context(\"Failed to connect to the transport\")?;\n    Ok(())\n}\n```\n\n### Compatible With `std::error::Error` Trait\nExtending the `std::error::Error` trait for our custom error types ensures they integrate smoothly with existing Rust error propagation mechanisms.\n\nExample:\n```rust\nimpl std::error::Error for VaultError {}\n```\n\n### Integration With `Result` Type\nOur custom errors will be used with the `Result<T, E>` type, where `E` is our custom error enumeration. This adherence to idiomatic Rust error handling makes the library more ergonomic for developers.\n\nExample:\n```rust\ntype VaultResult<T> = Result<T, VaultError>;\n\nfn perform_encryption() -> VaultResult<String> {\n    \/\/ encryption logic\n    Ok(\"encrypted data\".into())\n}\n```\n\n### Portability\nThe error handling mechanism will be designed to work seamlessly across different environments. This includes ensuring that the error types are compatible with various environments\u2019 constraints, such as `no_std` environments.\n\nExample:\n```rust\n#[cfg(not(feature = \"std\"))]\nuse core::fmt;\n\n#[cfg(feature = \"std\")]\nuse std::fmt;\n\nimpl fmt::Display for VaultError {\n    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {\n        write!(f, \"{:?}\", self)\n    }\n}\n```\n\n### Developer and User Convenience\nBy leveraging `thiserror`, enums are easier to define, which simplifies the developer's task. The errors will be descriptive and provide clear guidance for handling them, making the library more user-friendly.\n\nThis approach ensures a consistent, portable, and convenient error handling strategy that can cater to different case requirements, enhancing both development and user experience.","GenTime":"2024-08-20 00:16:12"}
{"File Name":"educational-platform\/0010-use-axon-framework.md","Context":"## Context\\nCurrently, a lot of custom classes are defined for DDD building blocks. It will be better to use DDD library for these goals.\\n","Decision":"Axon Framework will be used as DDD library.\\n","tokens":32,"id":2498,"text":"## Context\\nCurrently, a lot of custom classes are defined for DDD building blocks. It will be better to use DDD library for these goals.\\n\n\n##Decision\nAxon Framework will be used as DDD library.\\n","MatchedIds":[2696,2502,1792,1758,4896],"Predicted":"## Decision\nWe will adopt a well-established DDD library such as [Axon Framework](https:\/\/axoniq.io\/) or [DDD Toolkit](https:\/\/github.com\/ddd-crew\/ddd-toolkit). These libraries provide strong support for domain-driven design building blocks such as aggregates, entities, value objects, repositories, and domain events. Using such a library will improve maintainability, consistency, and robustness across our codebase by leveraging proven implementations and avoiding the pitfalls of custom solutions.","GenTime":"2024-08-20 00:16:15"}
{"File Name":"sdbmss\/0008-rdf-sparql-support.md","Context":"## Context\\nAs part of participation in the [Mapping Manuscript Migrations (MMM)][mmm] Linked Data project, the SDBM needed to export its data in RDF format for aggregation in a unified set of data from the project's three contributing organizations.\\n[mmm]: http:\/\/mappingmanuscriptmigrations.org \"Mapping Manuscript Migrations project site\"\\nAt the time the export was built a single unified data model had not been agreed upon.\\n","Decision":"The project decided to build a SPARQL endpoint built on Apache Jena. Since there was no target data model, the SDBM was exported to a custom namespace using a simple, direct mapping from SDBM model attributes to RDF. RabbitMQ messenger queue and a listener interface (`interface.rb`) to push updates from the SDBM to Jena. Also a simple SPARQL query interface was added for SDBM users.\\n","tokens":94,"id":1839,"text":"## Context\\nAs part of participation in the [Mapping Manuscript Migrations (MMM)][mmm] Linked Data project, the SDBM needed to export its data in RDF format for aggregation in a unified set of data from the project's three contributing organizations.\\n[mmm]: http:\/\/mappingmanuscriptmigrations.org \"Mapping Manuscript Migrations project site\"\\nAt the time the export was built a single unified data model had not been agreed upon.\\n\n\n##Decision\nThe project decided to build a SPARQL endpoint built on Apache Jena. Since there was no target data model, the SDBM was exported to a custom namespace using a simple, direct mapping from SDBM model attributes to RDF. RabbitMQ messenger queue and a listener interface (`interface.rb`) to push updates from the SDBM to Jena. Also a simple SPARQL query interface was added for SDBM users.\\n","MatchedIds":[1837,1510,1326,4167,5244],"Predicted":"## Decision\nThe following decisions were made to ensure flexibility and compatibility with the upcoming unified data model, while allowing SDBM to export its data in RDF format:\n\n- **RDF Framework**: We selected Apache Jena as the framework to generate and manipulate RDF data due to its comprehensive support for RDF standards and its wide adoption in the Linked Data community.\n\n- **Export Format**: All data exports from SDBM will be initially structured using a provisional RDF schema that is flexible and easily tweakable. This provisional schema takes into account the most common properties and relationships expected to be part of the final unified data model.\n\n- **Modular Design**: The export functionality will be implemented in a modular way to easily accommodate changes once the unified data model is finalized. This involves separating the data extraction, transformation, and RDF serialization into distinct components.\n\n- **Mapping Tool**: We will use a mapping tool such as Karma or RMLMapper to define the mapping rules from the SDBM data model to the provisional RDF schema. These tools provide the capability to update mappings with minimal effort.\n\n- **Regular Updates**: Establish a process for regular updates and reviews of the provisional schema with input from the other contributing organizations to ensure that the provisional schema evolves in line with the emerging unified data model.\n\n### Positive Consequences\n- **Flexibility**: By using a provisional RDF schema and modular design, the system can quickly adapt to the finalized data model without significant redevelopment.\n- **Community Alignment**: Utilizing widely accepted tools like Apache Jena and Karma ensures compatibility and support within the broader Linked Data community.\n- **Consistency**: Regularly updating the provisional schema with feedback from all participating organizations will keep all data exports in sync, minimizing discrepancies.\n\n### Negative Consequences\n- **Maintenance Overhead**: Regular updates to the provisional schema and consistent review processes might introduce additional maintenance overhead.\n- **Initial Learning Curve**: There may be an initial learning curve for the team to familiarize themselves with the chosen RDF tools and frameworks, which could require some training time.\n\n","GenTime":"2024-08-20 00:16:22"}
{"File Name":"modernisation-platform\/0006-use-a-multi-account-strategy-for-applications.md","Context":"## Context\\nIn the Modernisation Platform, we want to reduce the blast radius and increase the scalability of how we create, maintain, and support applications in the context of what AWS account(s) they sit within.\\n","Decision":"We've decided to use a multi-account strategy, split by application. We have a complete write-up as part of our [environments concept](https:\/\/user-guide.modernisation-platform.service.justice.gov.uk\/concepts\/environments\/).\\n","tokens":44,"id":3147,"text":"## Context\\nIn the Modernisation Platform, we want to reduce the blast radius and increase the scalability of how we create, maintain, and support applications in the context of what AWS account(s) they sit within.\\n\n\n##Decision\nWe've decided to use a multi-account strategy, split by application. We have a complete write-up as part of our [environments concept](https:\/\/user-guide.modernisation-platform.service.justice.gov.uk\/concepts\/environments\/).\\n","MatchedIds":[3144,2471,1464,3145,4041],"Predicted":"## Decision\nWe will adopt an approach where each application or set of related applications will have its own dedicated AWS account. This approach will leverage AWS Organizations to manage multiple accounts efficiently and apply consistent policies across all of them. By isolating applications into separate accounts, we can achieve:\n\n- **Reduced Blast Radius:** Any issue or security vulnerability in one application will be confined to its own AWS account, minimizing the potential impact on other applications.\n- **Enhanced Security:** Fine-grained IAM policies specific to each account can be implemented, reducing the risk of privilege escalation.\n- **Cost Management:** Separate billing for each account will provide clear visibility into the costs incurred by each application, enabling better cost allocation and optimization.\n- **Scalability:** Resources can be scaled independently per account, facilitating horizontal scaling and accommodating the growth of individual applications without affecting others.\n- **Compliance and Governance:** Consistent application of security policies, compliance controls, and resource tagging across all accounts through AWS Organizations Service Control Policies (SCPs).\n\nTo implement this, we will:\n1. Create a new AWS account for each application or related set of applications.\n2. Use AWS Control Tower to automate the setup and governance of new accounts within AWS Organizations.\n3. Implement AWS CloudFormation StackSets and AWS Service Catalog to facilitate resource provisioning across multiple accounts.\n4. Establish cross-account IAM roles for secure access and administrative purposes.\n5. Configure centralized logging and monitoring using AWS CloudWatch, AWS CloudTrail, and AWS Security Hub for consolidated oversight.\n\nThis architecture ensures that our goal of reducing the blast radius and increasing the scalability of our platform is adequately met.","GenTime":"2024-08-20 00:16:28"}
{"File Name":"cljdoc\/0014-add-support-for-examples.md","Context":"## Context\\nExamples are an often suggested feature for cljdoc that could enable library authors and the community to further guide library users.\\n","Decision":"Add support for examples to cljdoc. Allow libary users to provide examples through their Git repository but also run a community-maintained examples repository where examples can be maintained outside of a project's own Git repository.\\n","tokens":28,"id":2616,"text":"## Context\\nExamples are an often suggested feature for cljdoc that could enable library authors and the community to further guide library users.\\n\n\n##Decision\nAdd support for examples to cljdoc. Allow libary users to provide examples through their Git repository but also run a community-maintained examples repository where examples can be maintained outside of a project's own Git repository.\\n","MatchedIds":[108,1477,3069,1416,2181],"Predicted":"## Decision\nWe will implement an \"Examples\" section within cljdoc, allowing library authors to submit usage examples directly alongside the documentation of their libraries. To accommodate contributions from the community, we will also provide a mechanism for users to suggest, discuss, and upvote examples. This feature will enhance the usability and accessibility of libraries by providing concrete usage scenarios, making it easier for users to understand and effectively implement the libraries in their projects.","GenTime":"2024-08-20 00:16:38"}
{"File Name":"alfresco-anaxes-shipyard\/0002-docker-registry-for-internal-and-protected-images.md","Context":"## Context\\nAs part of our efforts around containerized deployment of the Alfresco Digital Business Platform we need to standardize on a Docker Image Registry that Alfresco engineering teams, other internal groups in the organization, and customers and partners can use to publish and consume Docker images.  We can describe those as 'internal' and 'protected' tiers.\\nThe Nexus3 implementation in use at the time of writing does not meet our requirements around access control, security scanning, scalability and global performance, usability, or maintainability.\\nOur IT resources are currently stretched very thin and we should avoid adding another system for them to deploy and maintain if possible.\\n","Decision":"We will use [Quay.io](https:\/\/quay.io) for the internal and protected tiers of access and use Docker Hub for public repositories (images of community versions and\/or enterprise artifacts with trail licenses).\\nWe\u2019d like to limit the introduction of additional deployments (particularly customer-facing) that our IT staff has to maintain, so we'd prefer a SaaS solution.\\nThe REST API of Quay.io allows our organization to potentially automate user provisioning\/invitation and user\/group management which is not available for Docker Cloud at this time.\\nAdditionally, Quay \/ CoreOS seems strongly committed to their SaaS offering while Docker seems entirely focused on their Enterprise \u2018on-prem\u2019 product.\\nThe summary [![report of the comparison](https:\/\/img.shields.io\/badge\/report%20of%20the%20comparison-PRIVATE-red.svg)](https:\/\/ts.alfresco.com\/share\/s\/mVAV1sGIReC_iqgMN0GGnQ) also contains reference links to the full investigation.\\n","tokens":130,"id":3598,"text":"## Context\\nAs part of our efforts around containerized deployment of the Alfresco Digital Business Platform we need to standardize on a Docker Image Registry that Alfresco engineering teams, other internal groups in the organization, and customers and partners can use to publish and consume Docker images.  We can describe those as 'internal' and 'protected' tiers.\\nThe Nexus3 implementation in use at the time of writing does not meet our requirements around access control, security scanning, scalability and global performance, usability, or maintainability.\\nOur IT resources are currently stretched very thin and we should avoid adding another system for them to deploy and maintain if possible.\\n\n\n##Decision\nWe will use [Quay.io](https:\/\/quay.io) for the internal and protected tiers of access and use Docker Hub for public repositories (images of community versions and\/or enterprise artifacts with trail licenses).\\nWe\u2019d like to limit the introduction of additional deployments (particularly customer-facing) that our IT staff has to maintain, so we'd prefer a SaaS solution.\\nThe REST API of Quay.io allows our organization to potentially automate user provisioning\/invitation and user\/group management which is not available for Docker Cloud at this time.\\nAdditionally, Quay \/ CoreOS seems strongly committed to their SaaS offering while Docker seems entirely focused on their Enterprise \u2018on-prem\u2019 product.\\nThe summary [![report of the comparison](https:\/\/img.shields.io\/badge\/report%20of%20the%20comparison-PRIVATE-red.svg)](https:\/\/ts.alfresco.com\/share\/s\/mVAV1sGIReC_iqgMN0GGnQ) also contains reference links to the full investigation.\\n","MatchedIds":[3917,3602,3604,3920,74],"Predicted":"## Decision\nWe will use a managed Docker Registry service such as AWS Elastic Container Registry (ECR) or Google Container Registry (GCR).\n\n### Rationale:\n1. **Access Control**: Both AWS ECR and GCR provide robust access control mechanisms tightly integrated with their respective cloud platforms.\n2. **Security Scanning**: These services offer built-in security scanning capabilities to identify vulnerabilities in stored images.\n3. **Scalability and Global Performance**: Managed services from AWS and Google offer virtually unlimited scalability and optimized performance across their global infrastructure.\n4. **Usability**: These registries provide user-friendly interfaces and integrate seamlessly with CI\/CD pipelines.\n5. **Maintainability**: Utilizing a managed service allows us to avoid the overhead associated with deploying and maintaining our own registry infrastructure.\n6. **Resource Constraints**: Given the current constraints on IT resources, a managed solution reduces the burden on IT by offloading operational aspects to the service provider.","GenTime":"2024-08-20 00:16:46"}
{"File Name":"corona-hackathon\/0010-branching-strategy.md","Context":"## Context\\nWe have to make a decision on the branching strategy for development.\\n","Decision":"[Git Flow](https:\/\/danielkummer.github.io\/git-flow-cheatsheet\/) it will be.\\n","tokens":17,"id":3653,"text":"## Context\\nWe have to make a decision on the branching strategy for development.\\n\n\n##Decision\n[Git Flow](https:\/\/danielkummer.github.io\/git-flow-cheatsheet\/) it will be.\\n","MatchedIds":[2261,1239,4428,3119,1436],"Predicted":"## Decision\nWe will adopt Gitflow as our branching strategy because it provides a robust framework for managing feature development, releases, and hotfixes. This strategy will facilitate parallel development, help us maintain a stable main branch, and make it easier to handle production fixes. For further details, we will follow the guidelines outlined in this article: https:\/\/nvie.com\/posts\/a-successful-git-branching-model\/","GenTime":"2024-08-20 00:16:48"}
{"File Name":"ReportMI-service-manual\/0011-use-auth0-for-temporary-user-authentication.md","Context":"## Context\\nOur user facing applications will need to authenticate users.\\nIn the long-term, we want to use the standard user sign-on services in CCS for\\neach appropriate user group.\\nFor suppliers, this is the [Supplier Registration Service (SRS)][service-srs].\\nFor CCS staff it is their G-Suite account.\\nUsing a single authentication service will reduce administration overheads\\nbecause we won't have to manage a JML (Joiners Movers Leavers) process. For\\nusers it will also mean they don't have to remember yet another username and\\npassword combination.\\nHowever, we want to get moving quickly, and integration with SRS\/CCS G-Suite\\nwill take time. For now, we want a simple sign-in service that we can integrate\\nwith quickly, to enable us to test and iterate the application.\\n","Decision":"We will use [Auth0][service-auth0] for authentication on a temporary basis.\\nAuth0 is an authentication service that uses the OAuth protocol. It provides\\nand simple integration path, and is free for several thousand active users.\\nWe acknowledge that we are incurring technical debt by not integrating\\nimmediately with SRS and G-Suite, however we believe that this will let us move\\nmore quickly during development.\\nThis decision will also allow us to gain a better understanding of the user\\nneeds around authentication before a decision around the final authentication\\nsolution is made.\\nWe will replace Auth0 with the proper authentication service later in the\\ndevelopment process.\\n","tokens":179,"id":2041,"text":"## Context\\nOur user facing applications will need to authenticate users.\\nIn the long-term, we want to use the standard user sign-on services in CCS for\\neach appropriate user group.\\nFor suppliers, this is the [Supplier Registration Service (SRS)][service-srs].\\nFor CCS staff it is their G-Suite account.\\nUsing a single authentication service will reduce administration overheads\\nbecause we won't have to manage a JML (Joiners Movers Leavers) process. For\\nusers it will also mean they don't have to remember yet another username and\\npassword combination.\\nHowever, we want to get moving quickly, and integration with SRS\/CCS G-Suite\\nwill take time. For now, we want a simple sign-in service that we can integrate\\nwith quickly, to enable us to test and iterate the application.\\n\n\n##Decision\nWe will use [Auth0][service-auth0] for authentication on a temporary basis.\\nAuth0 is an authentication service that uses the OAuth protocol. It provides\\nand simple integration path, and is free for several thousand active users.\\nWe acknowledge that we are incurring technical debt by not integrating\\nimmediately with SRS and G-Suite, however we believe that this will let us move\\nmore quickly during development.\\nThis decision will also allow us to gain a better understanding of the user\\nneeds around authentication before a decision around the final authentication\\nsolution is made.\\nWe will replace Auth0 with the proper authentication service later in the\\ndevelopment process.\\n","MatchedIds":[2405,1252,5211,2582,61],"Predicted":"## Decision\nWe will use Auth0's free tier as a simple sign-in service for the initial phase of our application. This will enable quick integration and allow us to test and iterate our application. As we move towards the long-term solution, we will plan for a seamless migration to the Supplier Registration Service (SRS) for suppliers and G-Suite authentication for CCS staff.","GenTime":"2024-08-20 00:16:51"}
{"File Name":"libelektra\/lookup_every_key.md","Context":"## Problem\\nOn structures like maps or [arrays](array.md) there are different\\npossibilities which keys are looked up in the KeySet and which\\nare simply iterated.\\nWithout any guidelines, applications would provide arbitrary inconsistent\\nbehavior.\\n","Decision":"Every key that an application wants to use, must be looked up\\nwith `ksLookup` using a cascading lookup key.\\n- [Arrays](array.md)\\n","tokens":51,"id":1310,"text":"## Problem\\nOn structures like maps or [arrays](array.md) there are different\\npossibilities which keys are looked up in the KeySet and which\\nare simply iterated.\\nWithout any guidelines, applications would provide arbitrary inconsistent\\nbehavior.\\n\n\n##Decision\nEvery key that an application wants to use, must be looked up\\nwith `ksLookup` using a cascading lookup key.\\n- [Arrays](array.md)\\n","MatchedIds":[1309,1271,1289,1318,1292],"Predicted":"## Decision\n- Define clear guidelines on how keys within maps and arrays should be handled.\n- Establish a uniform approach where:\n  - Keys within maps are always explicitly looked up in the KeySet.\n  - Elements within arrays are always iterated sequentially.\n- Document these guidelines extensively in the developer documentation to ensure consistent application behavior.\n- Implement validation checks to ensure that all entries within maps and arrays conform to these guidelines.\n- Update relevant APIs to enforce and reflect these guidelines.\n","GenTime":"2024-08-20 00:16:53"}
{"File Name":"where-away\/0009-use-xml-for-bookmarks-format.md","Context":"## Context\\nOptions:\\n- custom format, custom parser\\n- that's a lot of work\\n- json\\n- awkward format for depicting a tree\\n- yaml\\n- significant whitespace makes this more error-prone, and the bookmarks file\\nis likely to be edited frequently\\n- xml\\n- classic\\n- shows trees well\\n- less error prone to edit\\n- not supported natively in node\\n- neither parsing nor schema validation\\n- schema is simple enough that I could hand-roll a validator\\n- see discussion on parsing -- it needs a library: https:\/\/stackoverflow.com\/questions\/11398419\/trying-to-use-the-domparser-with-node-js\\n- top two promising libraries:\\n- jsdom, 32 dependencies https:\/\/www.npmjs.com\/package\/jsdom\\n- xmldom, 0 dependencies https:\/\/www.npmjs.com\/package\/xmldom\\n","Decision":"Use XML for the bookmarks format. Use\\n[xmldom](https:\/\/www.npmjs.com\/package\/xmldom) to parse, and hand-roll the\\nvalidation.\\n","tokens":180,"id":2274,"text":"## Context\\nOptions:\\n- custom format, custom parser\\n- that's a lot of work\\n- json\\n- awkward format for depicting a tree\\n- yaml\\n- significant whitespace makes this more error-prone, and the bookmarks file\\nis likely to be edited frequently\\n- xml\\n- classic\\n- shows trees well\\n- less error prone to edit\\n- not supported natively in node\\n- neither parsing nor schema validation\\n- schema is simple enough that I could hand-roll a validator\\n- see discussion on parsing -- it needs a library: https:\/\/stackoverflow.com\/questions\/11398419\/trying-to-use-the-domparser-with-node-js\\n- top two promising libraries:\\n- jsdom, 32 dependencies https:\/\/www.npmjs.com\/package\/jsdom\\n- xmldom, 0 dependencies https:\/\/www.npmjs.com\/package\/xmldom\\n\n\n##Decision\nUse XML for the bookmarks format. Use\\n[xmldom](https:\/\/www.npmjs.com\/package\/xmldom) to parse, and hand-roll the\\nvalidation.\\n","MatchedIds":[4323,3568,3521,1599,5140],"Predicted":"## Decision\nChosen option: XML\n\n### Rationale\n- **Representation**: XML is a classic format that depicts trees well, which aligns with the need to represent bookmark structures.\n- **Editing**: XML is generally less error-prone to edit manually, mitigating some concerns around frequent edits impacting data integrity.\n- **Dependencies**: Although XML is not supported natively in Node.js, the `xmldom` library offers a lightweight solution with 0 dependencies, making it a clean addition to the project.\n- **Schema Validation**: The schema for the bookmarks is simple enough to consider hand-rolling a validator, which eliminates the need for complex parsing or validation tools.\n- **Community Support**: There is ample community support and documentation available for XML, ensuring that any potential issues can be addressed efficiently.\n\n### Positive Consequences\n- **Maintainability**: Structured and clean representation of the data makes it easier to maintain.\n- **Validation**: Hand-rolled validator ensures that the XML structure adheres to defined schema, providing data consistency.\n- **Dependencies**: Using a lightweight library without many dependencies avoids potential bloat and simplifies project maintenance.\n\n### Negative Consequences\n- **Library Requirement**: Introducing a new library (`xmldom`) adds a dependency to the project, albeit a minimal one.\n- **Learning Curve**: Developers might need to familiarize themselves with XML parsing and validation in Node.js, although this is mitigated by the simplicity of the XML structure.","GenTime":"2024-08-20 00:16:59"}
{"File Name":"eslint-config-nhsuk\/0002-replace-travis-ci-with-github-actions.md","Context":"## Context\\nGitHub Actions are the new shiny and have been in Beta for sometime. They are\\ngoing to be generally available on\\n[2019-11-13](https:\/\/github.blog\/2019-08-08-github-actions-now-supports-ci-cd\/).\\nGitHub Actions will have a long term future. It is likely GitHub Actions\\nwill become the default CI mechanism (and possibly more) for projects hosted on\\nGitHub. Using them in this repo, which has a basic use case will provide some\\nexposure to the service.\\n","Decision":"The decision is to replace Travis CI with GitHub Actions.\\n","tokens":114,"id":152,"text":"## Context\\nGitHub Actions are the new shiny and have been in Beta for sometime. They are\\ngoing to be generally available on\\n[2019-11-13](https:\/\/github.blog\/2019-08-08-github-actions-now-supports-ci-cd\/).\\nGitHub Actions will have a long term future. It is likely GitHub Actions\\nwill become the default CI mechanism (and possibly more) for projects hosted on\\nGitHub. Using them in this repo, which has a basic use case will provide some\\nexposure to the service.\\n\n\n##Decision\nThe decision is to replace Travis CI with GitHub Actions.\\n","MatchedIds":[118,3456,2393,1741,3328],"Predicted":"## Decision\nWe will implement GitHub Actions for CI\/CD in this repository to gain experience with the service and evaluate its features, preparing for a potential wider adoption in the future.","GenTime":"2024-08-20 00:17:01"}
{"File Name":"ODT_OnlineBank\/ARD-0001.md","Context":"## Context\\n*This section describes the forces at play, including technological, political, social, and project local.\\nThese forces are probably in tension, and should be called out as such. The language in this section is value-neutral.\\nIt is simply describing facts.*\\nThe Office of Development Transformation (ODT) is focused on helping CAD learn and adopt lean, agile, and devOps techniques and\\nprocesses. ODT will be initialy focus on Green Field Developent. The Online Bank is a sample app used in the ODT University classes.  The Purpose of theapplication is to provide a\\nsample app as a context for the training courses.  We believe most people are familiar with online\\nbanking, and won't need to spend a lot of time learning the problem domain.\\nThe purpose of this document is to determine the technology stack for the sample application.\\n**NOTE:** While we are initially focused on a Java Web stack, we may elect to develop out an example in the Microsoft Stack.\\nIf you would like to help, please contact us.\\n","Decision":"*This section describes our response to these forces. It is stated in full sentences, with active voice.\\n\"We will ...\"*\\nWe will be focusing on the modern web java stack.\\n### UI Technology Stack:\\n- Angular 2\\n- Bootstrap\\n- protractor\\n- jasmine\\n### Server Side Technology Stack:\\n- Java 1.8+\\n- SpringBoot 1.4+\\n- Restful\\n- MySQL\\n- Spring Data JPA (Hibernate)\\n- JUnit 5 \/ Spock\\n### Development Technology Stack:\\n- Local Development\\n- SCM: GIT\\n- Intellij\\n- Favorite Editor\\n- Gradle\\n- Angular Quickstart\\n- Concourse CI (CI\/CD)\\n- Jira\\n- Sonar\\n#### Deployment:\\n- Pivotal Cloud Foundry (Java and Static Build Packs)\\n","tokens":220,"id":1665,"text":"## Context\\n*This section describes the forces at play, including technological, political, social, and project local.\\nThese forces are probably in tension, and should be called out as such. The language in this section is value-neutral.\\nIt is simply describing facts.*\\nThe Office of Development Transformation (ODT) is focused on helping CAD learn and adopt lean, agile, and devOps techniques and\\nprocesses. ODT will be initialy focus on Green Field Developent. The Online Bank is a sample app used in the ODT University classes.  The Purpose of theapplication is to provide a\\nsample app as a context for the training courses.  We believe most people are familiar with online\\nbanking, and won't need to spend a lot of time learning the problem domain.\\nThe purpose of this document is to determine the technology stack for the sample application.\\n**NOTE:** While we are initially focused on a Java Web stack, we may elect to develop out an example in the Microsoft Stack.\\nIf you would like to help, please contact us.\\n\n\n##Decision\n*This section describes our response to these forces. It is stated in full sentences, with active voice.\\n\"We will ...\"*\\nWe will be focusing on the modern web java stack.\\n### UI Technology Stack:\\n- Angular 2\\n- Bootstrap\\n- protractor\\n- jasmine\\n### Server Side Technology Stack:\\n- Java 1.8+\\n- SpringBoot 1.4+\\n- Restful\\n- MySQL\\n- Spring Data JPA (Hibernate)\\n- JUnit 5 \/ Spock\\n### Development Technology Stack:\\n- Local Development\\n- SCM: GIT\\n- Intellij\\n- Favorite Editor\\n- Gradle\\n- Angular Quickstart\\n- Concourse CI (CI\/CD)\\n- Jira\\n- Sonar\\n#### Deployment:\\n- Pivotal Cloud Foundry (Java and Static Build Packs)\\n","MatchedIds":[1879,3784,1182,3374,4620],"Predicted":"## Decision\nWe will develop the Online Bank sample application using a Java Web stack. This decision aligns with our initial focus on Java technologies and leverages our existing expertise in the field. The chosen stack will include technologies such as Spring Boot for the backend, Thymeleaf for the front-end templating, and Hibernate for ORM.\n\nTo facilitate an agile and DevOps-friendly environment, we will incorporate continuous integration and continuous deployment pipelines using Jenkins and Docker. We will also include automated testing frameworks like JUnit and Mockito to ensure code quality.\n\nDespite our initial decision to focus on Java, we remain open to developing a parallel example using the Microsoft Stack in the future. This will allow us to cater to a broader audience and provide more versatile training materials.\n\nIf you would like to contribute to the development of the sample application or have expertise in the Microsoft Stack, please contact us.","GenTime":"2024-08-20 00:17:05"}
{"File Name":"texas-holdem-code-challenge\/0009-use-typescript-3-4-5.md","Context":"## Context\\nThe TypeScript plugin for ESLint requires TypeScript below 3.5.0. The next earlier\\nversion is 3.4.5 (from ~ 1 month ago)\\nWarning from ESLint:\\n```\\n=============\\nWARNING: You are currently running a version of TypeScript which is not officially supported by typescript-estree.\\nYou may find that it works just fine, or you may not.\\nSUPPORTED TYPESCRIPT VERSIONS: >=3.2.1 <3.5.0\\nYOUR TYPESCRIPT VERSION: 3.5.1\\nPlease only submit bug reports when using the officially supported version.\\n=============\\n```\\nFurther, there are no cutting edge TypeScript features I need for this.\\n","Decision":"Use TypeScript 3.4.5\\n","tokens":158,"id":2296,"text":"## Context\\nThe TypeScript plugin for ESLint requires TypeScript below 3.5.0. The next earlier\\nversion is 3.4.5 (from ~ 1 month ago)\\nWarning from ESLint:\\n```\\n=============\\nWARNING: You are currently running a version of TypeScript which is not officially supported by typescript-estree.\\nYou may find that it works just fine, or you may not.\\nSUPPORTED TYPESCRIPT VERSIONS: >=3.2.1 <3.5.0\\nYOUR TYPESCRIPT VERSION: 3.5.1\\nPlease only submit bug reports when using the officially supported version.\\n=============\\n```\\nFurther, there are no cutting edge TypeScript features I need for this.\\n\n\n##Decision\nUse TypeScript 3.4.5\\n","MatchedIds":[1404,4224,1594,2652,1337],"Predicted":"## Decision\nWe will downgrade TypeScript to version 3.4.5 to ensure compatibility with the TypeScript plugin for ESLint. This version is within the officially supported range for typescript-estree, ensuring that we avoid potential issues and unsupported configurations. Given that we do not require any features present in TypeScript 3.5.0 or higher, this downgrade should not impact our development.","GenTime":"2024-08-20 00:17:08"}
{"File Name":"govuk-aws\/0009-environment-bootstrapping-process.md","Context":"## Context\\nWe need a process to bootstrap a new Amazon environment. This should be both heavily automated\\nand include set places where the operator can evaluate the status of the provisioning.\\nThis bootstrapping process assumes the following:\\n* The required repositories are available\\n* An Amazon Web Services admin account is available\\n* Backups of our data can be retrieved (for the restore process)\\n","Decision":"The chosen process has a few, early bootstrap steps, that differ from normal operations. These\\naim to quickly provide the usual level of self-service to allow teams to independently\\nrestore services without a dependency on a central operations team. An early draft of\\nthis process, which will be updated as changes are required, will be:\\n1. Clone all the relevant repositories\\n1. Build the S3 bucket for Terraform state\\n1. Provision the new DNS zones for this environment\\n1. Build the Puppet master\\n1. Deploy the puppet code and secrets\\n1. Build the deploy Jenkins\\n* Rebuild everything else in the usual deployment ways\\nThis document will be written so that one of the non-infrastructure members of the team\\nwill be able to provision a complete environment within our allowed recovery time.\\n","tokens":80,"id":4025,"text":"## Context\\nWe need a process to bootstrap a new Amazon environment. This should be both heavily automated\\nand include set places where the operator can evaluate the status of the provisioning.\\nThis bootstrapping process assumes the following:\\n* The required repositories are available\\n* An Amazon Web Services admin account is available\\n* Backups of our data can be retrieved (for the restore process)\\n\n\n##Decision\nThe chosen process has a few, early bootstrap steps, that differ from normal operations. These\\naim to quickly provide the usual level of self-service to allow teams to independently\\nrestore services without a dependency on a central operations team. An early draft of\\nthis process, which will be updated as changes are required, will be:\\n1. Clone all the relevant repositories\\n1. Build the S3 bucket for Terraform state\\n1. Provision the new DNS zones for this environment\\n1. Build the Puppet master\\n1. Deploy the puppet code and secrets\\n1. Build the deploy Jenkins\\n* Rebuild everything else in the usual deployment ways\\nThis document will be written so that one of the non-infrastructure members of the team\\nwill be able to provision a complete environment within our allowed recovery time.\\n","MatchedIds":[4358,2471,771,1464,3794],"Predicted":"## Decision\nWe will use [Terraform](https:\/\/www.terraform.io\/) in conjunction with AWS CloudFormation StackSets to bootstrap the new Amazon environment. Terraform will manage the infrastructure as code, providing automation at scale, while CloudFormation StackSets will help create, update, or delete stacks across multiple AWS accounts and Regions with a single operation.\n\n### Steps:\n1. **Repository Configuration**:\n   - Create a repository containing the Terraform and CloudFormation templates. \n   - Ensure the repository is accessible to the AWS Admin account.\n\n2. **Terraform Automation**:\n   - Using Terraform, define the initial infrastructure required (e.g., VPCs, subnets, security groups).\n   - Use CloudFormation resource types supported within Terraform to define specific CloudFormation Stack instances.\n\n3. **Operator Checkpoints**:\n   - Incorporate `terraform plan` stages where the operator can review the execution plan before applying.\n   - Implement AWS CLI commands or SDK calls to verify the status of CloudFormation stacks at each major step, providing checkpoints.\n\n4. **Scripts for Restoration and Backup**:\n   - Design and include scripts within the Terraform repository to retrieve and restore backups as the final setup step.\n   - Ensure these scripts have appropriate logging and error-handling mechanisms, providing visibility to the operator.\n\n### Notes:\n- All credentials and sensitive configurations should be managed via AWS Secrets Manager or Parameter Store.\n- Ensure that the Terraform state is managed securely, potentially using an S3 bucket with versioning and encryption.\n\n### Summary:\nThis combination leverages Terraform for robust, automated provisioning of infrastructure alongside CloudFormation StackSets for multi-account and region orchestration. Clear operator checkpoints ensure scrutinization at critical stages, while predefined scripts facilitate data restoration from backups.\n","GenTime":"2024-08-20 00:17:17"}
{"File Name":"winery\/0030-multiple-repositories.md","Context":"## Context and Problem Statement\\nTo help developers obtain TOSCA elements in a more practical way, an approach to supporting multiple repositories distributed across different servers is presented here.\\nThe result provides the ability to define a set of URLs and load them into the current winery-repository.\\n","Decision":"Option 2 was chosen to manage each repository individually. This makes it possible to use existing version control systems in the individual repositories.\\nThe repositories are structured as follows.\\n![Repository Structure](graphics\/repo-structure.png)\\n### Positive Consequences <!-- optional -->\\n* User can search for elements online and load a repository using the url\\n* It's enough to send a URL instead of a CSAR\\n* Manage each repository independently\\n* The origin and the version history of a repository can be checked\\n* The possibility to contribute directly to the development of a repository\\n### Negative consequences <!-- optional -->\\n* User is forced to define namespaces in Namespaces.json.\\n* Additional configuration file (repositories.json)\\n","tokens":57,"id":4305,"text":"## Context and Problem Statement\\nTo help developers obtain TOSCA elements in a more practical way, an approach to supporting multiple repositories distributed across different servers is presented here.\\nThe result provides the ability to define a set of URLs and load them into the current winery-repository.\\n\n\n##Decision\nOption 2 was chosen to manage each repository individually. This makes it possible to use existing version control systems in the individual repositories.\\nThe repositories are structured as follows.\\n![Repository Structure](graphics\/repo-structure.png)\\n### Positive Consequences <!-- optional -->\\n* User can search for elements online and load a repository using the url\\n* It's enough to send a URL instead of a CSAR\\n* Manage each repository independently\\n* The origin and the version history of a repository can be checked\\n* The possibility to contribute directly to the development of a repository\\n### Negative consequences <!-- optional -->\\n* User is forced to define namespaces in Namespaces.json.\\n* Additional configuration file (repositories.json)\\n","MatchedIds":[2342,3598,3483,697,1372],"Predicted":"## Decision\nEnhance the current winery-repository system by adding a feature that allows developers to define and configure multiple repository URLs. The system should support the following functionalities:\n\n1. **Repository Configuration Management:**\n   - Introduce a configuration file where users can list multiple repository URLs.\n   - Provide options to prioritize repositories, allowing the system to check higher-priority repositories first for requested elements.\n\n2. **Repository Synchronization:**\n   - Implement a synchronization mechanism that periodically updates the local winery-repository with elements from the configured URLs.\n   - Allow manual synchronization triggers for immediate updates.\n\n3. **Caching Mechanism:**\n   - Develop a caching layer to store frequently accessed elements locally, minimizing repetitive fetch operations and improving performance.\n   - Ensure cache invalidation strategies are robust to avoid stale data issues.\n\n4. **Security Considerations:**\n   - Support authenticated access to repositories that require credentials.\n   - Implement HTTPS communication for secure data transfer and to protect repository access credentials.\n\n5. **Logging and Monitoring:**\n   - Introduce detailed logging to track repository synchronization activities and errors.\n   - Provide a dashboard or monitoring tool to visualize the status and health of repository connectivity and synchronization operations.\n\nBy implementing these features, developers will find it easier and more efficient to manage and access TOSCA elements across multiple distributed repositories. This approach also enhances scalability and flexibility by allowing repositories to be added or removed dynamically without significant system disruption.","GenTime":"2024-08-20 00:17:21"}
{"File Name":"simple-android\/001-screen-controllers.md","Context":"## Context\\nWe don\u2019t want to put business logic inside Android framework classes (like an `Activity` or `Fragment`) because those cannot be unit tested. To enable\\na fast feedback loop (i.e. tests that run on the JVM and not Android VM), we separate screens and controllers using\\nthe [MVI architecture](https:\/\/medium.com\/@ragunathjawahar\/mvi-series-a-pragmatic-reactive-architecture-for-android-87ca176387d1) [pattern](https:\/\/medium.com\/@ragunathjawahar\/mvi-series-a-pragmatic-reactive-architecture-for-android-87ca176387d1)\\n.\\n","Decision":"Every screen has one controller that consumes user events, performs business logic with the help of data repositories and communicates UI changes back\\nto the screen.\\nUser interactions happening on the screen are abstracted inside data classes of type `UiEvent`. These events flow to the controller in the form of\\nRxJava streams.\\n```kotlin\\n\/\/ Create the UsernameTextChanged event by listening to the EditText\\nRxTextView\\n.textChanges(usernameEditText)\\n.map { text -> UsernameTextChanged(text) }\\n\/\/ Event\\ndata class UsernameTextChanged(text: String) : UiEvent\\n```\\nThe screen sends a single stream of `UiEvent`s to the controller and gets back a transformed stream of UI changes. The flow of data is\\nuni-directional. To merge multiple streams into one, RxJava\u2019s `merge()`  operator is used.\\n```kotlin\\n\/\/ Login screen\\nObservable.merge(usernameChanges(), passwordChanges(), submitClicks())\\n.compose(controller)\\n.takeUntil(screenDestroy)\\n.subscribe { uiChange -> uiChange(this) }\\n```\\nIn the controller, `UiEvent`s are transformed as per the business logic and `UiChange`s are sent back to the screen. The `UiChange` is a simple lambda\\nfunction that takes the screen itself as an argument, which can call a method implemented by the screen interface.\\n```kotlin\\ntypealias Ui = LoginScreen\\ntypealias UiChange = (LoginScreen) -> Unit\\nclass LoginScreenController : ObservableTransformer<UiEvent, UiChange>() {\\nfun apply(events: Observable<UiEvent>): Observable<UiChange> {\\nevents.ofType<UsernameTextChanged>\\n.map { isValidUsername(it.text) }\\n.map { isValid ->\\n{ ui: Ui -> ui.setSubmitButtonEnabled(isValid) } \/\/ UiChange lambda!\\n}\\n}\\n}\\n```\\nWhen the events have to observed across multiple functions in the controller, the stream is shared using `replay()` + `refCount()` so that the UI\\nevents aren't recreated once for every subscription. `replay()` shares a single subscription to the screen by replaying the events to every observer\\nand `refCount()` keeps the subscription alive as long as there is at least one observer.\\n```kotlin\\nclass LoginScreenController : ObservableTransformer<UiEvent, UiChange>() {\\nfun apply(events: Observable<UiEvent>): Observable<UiChange> {\\nval replayedEvents = events.replay().refCount()\\nreturn Observable.merge(\\nenableSubmitButton(replayedEvents),\\nloginUser(replayedEvents))\\n}\\nfun enableSubmitButton(events: Observable<UiEvent>): Observable<UiChange>()\\nfun loginOnSubmit(events: Observable<UiEvent>): Observable<UiChange>()\\n}\\n```\\n![data flow from the Ui to the controller](images\/diagram_screen_controller.png)\\n([diagram source](https:\/\/docs.google.com\/drawings\/d\/1I_VdUM8Pf9O3nOYViqVF6kiyqFaYFD2fHmKRyvwmEl4\/edit?usp=sharing))\\n","tokens":137,"id":1133,"text":"## Context\\nWe don\u2019t want to put business logic inside Android framework classes (like an `Activity` or `Fragment`) because those cannot be unit tested. To enable\\na fast feedback loop (i.e. tests that run on the JVM and not Android VM), we separate screens and controllers using\\nthe [MVI architecture](https:\/\/medium.com\/@ragunathjawahar\/mvi-series-a-pragmatic-reactive-architecture-for-android-87ca176387d1) [pattern](https:\/\/medium.com\/@ragunathjawahar\/mvi-series-a-pragmatic-reactive-architecture-for-android-87ca176387d1)\\n.\\n\n\n##Decision\nEvery screen has one controller that consumes user events, performs business logic with the help of data repositories and communicates UI changes back\\nto the screen.\\nUser interactions happening on the screen are abstracted inside data classes of type `UiEvent`. These events flow to the controller in the form of\\nRxJava streams.\\n```kotlin\\n\/\/ Create the UsernameTextChanged event by listening to the EditText\\nRxTextView\\n.textChanges(usernameEditText)\\n.map { text -> UsernameTextChanged(text) }\\n\/\/ Event\\ndata class UsernameTextChanged(text: String) : UiEvent\\n```\\nThe screen sends a single stream of `UiEvent`s to the controller and gets back a transformed stream of UI changes. The flow of data is\\nuni-directional. To merge multiple streams into one, RxJava\u2019s `merge()`  operator is used.\\n```kotlin\\n\/\/ Login screen\\nObservable.merge(usernameChanges(), passwordChanges(), submitClicks())\\n.compose(controller)\\n.takeUntil(screenDestroy)\\n.subscribe { uiChange -> uiChange(this) }\\n```\\nIn the controller, `UiEvent`s are transformed as per the business logic and `UiChange`s are sent back to the screen. The `UiChange` is a simple lambda\\nfunction that takes the screen itself as an argument, which can call a method implemented by the screen interface.\\n```kotlin\\ntypealias Ui = LoginScreen\\ntypealias UiChange = (LoginScreen) -> Unit\\nclass LoginScreenController : ObservableTransformer<UiEvent, UiChange>() {\\nfun apply(events: Observable<UiEvent>): Observable<UiChange> {\\nevents.ofType<UsernameTextChanged>\\n.map { isValidUsername(it.text) }\\n.map { isValid ->\\n{ ui: Ui -> ui.setSubmitButtonEnabled(isValid) } \/\/ UiChange lambda!\\n}\\n}\\n}\\n```\\nWhen the events have to observed across multiple functions in the controller, the stream is shared using `replay()` + `refCount()` so that the UI\\nevents aren't recreated once for every subscription. `replay()` shares a single subscription to the screen by replaying the events to every observer\\nand `refCount()` keeps the subscription alive as long as there is at least one observer.\\n```kotlin\\nclass LoginScreenController : ObservableTransformer<UiEvent, UiChange>() {\\nfun apply(events: Observable<UiEvent>): Observable<UiChange> {\\nval replayedEvents = events.replay().refCount()\\nreturn Observable.merge(\\nenableSubmitButton(replayedEvents),\\nloginUser(replayedEvents))\\n}\\nfun enableSubmitButton(events: Observable<UiEvent>): Observable<UiChange>()\\nfun loginOnSubmit(events: Observable<UiEvent>): Observable<UiChange>()\\n}\\n```\\n![data flow from the Ui to the controller](images\/diagram_screen_controller.png)\\n([diagram source](https:\/\/docs.google.com\/drawings\/d\/1I_VdUM8Pf9O3nOYViqVF6kiyqFaYFD2fHmKRyvwmEl4\/edit?usp=sharing))\\n","MatchedIds":[2816,4896,2502,4695,1792],"Predicted":"## Decision\nWe will adopt the MVI (Model-View-Intent) architecture pattern to separate business logic from Android framework classes. \n\n### Reasons\n1. **Testability**: By isolating business logic from `Activity` or `Fragment`, we enable unit testing on the JVM, leading to a faster feedback loop.\n2. **Separation of Concerns**: MVI promotes clear separation between UI, business logic, and state management, leading to a more maintainable and scalable codebase.\n3. **Reactive Programming**: MVI leverages reactive programming paradigms which improve handling of asynchronous data streams, leading to more responsive and robust applications.\n\n### Positive Consequences\n- **Improved Test Coverage**: Business logic can be thoroughly tested using unit tests.\n- **Improved Maintainability**: Clear separation of concerns leads to better maintainable code.\n- **Better Code Quality**: Following a proactive architectural pattern promotes best practices and clean code.\n\n### Negative Consequences\n- **Initial Learning Curve**: Team members may require time to get up to speed with the MVI architecture pattern.\n- **Increased Complexity**: Introduces additional layers (Model, View, Intent) which might add complexity for simple applications.\n\nWe will include training and resources to ensure the team is comfortable with the MVI architecture.","GenTime":"2024-08-20 00:17:26"}
{"File Name":"adr\/ADR-9-DecentralandInterface-evolution-plan.md","Context":"## Context and Problem Statement\\nHow should we proceed to maintain compatibility between the current SDK and the next-gen SDK currently being developed?\\nTaking into consideration:\\n* All the scenes currently deployed must work as they work today.\\n* `DecentralandInterface` should work indefinitely to maintain backwards compatibility.\\n* Maintainance costs should be taken into account\\n","Decision":"We decided to develop a new and clean interface for the next-gen SDK, then create an adapter for the legacy `DecentralandInterface` (option 2) because:\\n* We should not be limited by the decisions of the past SDK\\n* The new SDK must eventually prove itself by reaching a feature parity with the current interface\\n* Avoid sunk cost fallacy with the old SDK\\n","tokens":72,"id":4618,"text":"## Context and Problem Statement\\nHow should we proceed to maintain compatibility between the current SDK and the next-gen SDK currently being developed?\\nTaking into consideration:\\n* All the scenes currently deployed must work as they work today.\\n* `DecentralandInterface` should work indefinitely to maintain backwards compatibility.\\n* Maintainance costs should be taken into account\\n\n\n##Decision\nWe decided to develop a new and clean interface for the next-gen SDK, then create an adapter for the legacy `DecentralandInterface` (option 2) because:\\n* We should not be limited by the decisions of the past SDK\\n* The new SDK must eventually prove itself by reaching a feature parity with the current interface\\n* Avoid sunk cost fallacy with the old SDK\\n","MatchedIds":[4609,4613,138,4614,816],"Predicted":"## Decision\nWe choose to create a compatibility layer within the next-gen SDK that serves as a bridge to the current SDK. This approach ensures that all existing scenes remain functional and `DecentralandInterface` continues to work indefinitely.\n\n### Actionables\n1. **Compatibility Layer Development**:\n    - Develop a compatibility layer within the next-gen SDK that mimics the behavior of the current SDK.\n    - Ensure that this layer seamlessly interfaces with the current `DecentralandInterface`.\n\n2. **Test Suite**:\n    - Create an extensive test suite that tests the compatibility layer against all existing functionality in the current SDK.\n    - Ensure that it covers a wide range of scenarios and edge cases to guarantee that existing scenes function without issues.\n\n3. **Documentation**:\n    - Document the compatibility layer thoroughly, including migration guides for developers who may want to upgrade their scenes in the future.\n    - Provide guidelines on best practices for utilizing the compatibility layer to avoid pitfalls and ensure optimal performance.\n\n4. **Deprecation Warnings**:\n    - Introduce deprecation warnings for features in the current SDK that are not recommended for use going forward.\n    - Provide clear alternatives within the next-gen SDK to encourage gradual adoption of the new API.\n\n5. **Maintenance Costs Assessment**:\n    - Regularly assess the maintenance costs associated with the compatibility layer.\n    - Plan for regular updates and iterations of the compatibility layer to keep it in sync with new features and improvements in the next-gen SDK.\n\n6. **Community Support**:\n    - Engage with the developer community to gather feedback and address any issues that arise with the compatibility layer.\n    - Offer support channels to assist developers in transitioning to the next-gen SDK.\n\nBy implementing this compatibility layer, we can ensure that all deployed scenes continue to function as intended while gradually transitioning to the next-gen SDK. This approach minimizes disruption and balances maintenance costs effectively.","GenTime":"2024-08-20 00:17:34"}
{"File Name":"adr\/ADR-28-smart-contract-wallets-and-meta-transactions.md","Context":"## Context and Problem Statement\\nCurrently we support meta-transactions on Polygon from several of our dApps (Marketplace, Builder, Account) and the Explorer.\\nMeta-transactions require the user to sign a function signature that they want us to call on their behalf (so we can subsidize the gas).\\nThe smart contracts that support meta-transactions (like the Marketplace contract) recover the account from the signature on chain, and uses that as the message sender.\\nSmart contract wallets are smart contracts that can hold assets, and have a set of signers that can command the wallet to perform actions on those assets by providing signed messages authorizing given actions.\\nWhen a user of a contract wallet signs a meta transaction, and the public key is recovered on chain by the smart contract that supports meta-transactions, the public key is the one from the signer, and not the contract address of the smart contract wallet (which is the actual owner of the assets). This causes the meta-transactions to fail on chain.\\n","Decision":"We choose option 2 since we can never be sure that the smart contract wallet has a matching deployment on Polygon, and is not the case for the most popular smart contract wallets (like Argent or Authereum)\\n","tokens":205,"id":4623,"text":"## Context and Problem Statement\\nCurrently we support meta-transactions on Polygon from several of our dApps (Marketplace, Builder, Account) and the Explorer.\\nMeta-transactions require the user to sign a function signature that they want us to call on their behalf (so we can subsidize the gas).\\nThe smart contracts that support meta-transactions (like the Marketplace contract) recover the account from the signature on chain, and uses that as the message sender.\\nSmart contract wallets are smart contracts that can hold assets, and have a set of signers that can command the wallet to perform actions on those assets by providing signed messages authorizing given actions.\\nWhen a user of a contract wallet signs a meta transaction, and the public key is recovered on chain by the smart contract that supports meta-transactions, the public key is the one from the signer, and not the contract address of the smart contract wallet (which is the actual owner of the assets). This causes the meta-transactions to fail on chain.\\n\n\n##Decision\nWe choose option 2 since we can never be sure that the smart contract wallet has a matching deployment on Polygon, and is not the case for the most popular smart contract wallets (like Argent or Authereum)\\n","MatchedIds":[4346,1962,1957,13,21],"Predicted":"## Decision\n\n### Introduce a Meta-Transaction Proxy Contract\n\nTo resolve the issue of recovering the public key of the signer instead of the smart contract wallet's address, we will implement a Meta-Transaction Proxy Contract. This proxy contract will act as an intermediary to correctly handle the smart contract wallets and ensure that the meta-transactions are performed by the contract address of the smart contract wallet.\n\n### Solution Details\n\n1. **Meta-Transaction Proxy Contract**: \n   - The proxy contract will accept meta-transactions signed by the signers of the smart contract wallet.\n   - The proxy contract will validate the signatures against the set of authorized signers of the smart contract wallet.\n   - Once validated, the proxy contract will execute the original meta-transaction on behalf of the smart contract wallet by calling the target dApp with the smart contract wallet's address as the message sender.\n\n2. **Signature Validation**:\n   - The proxy contract will recover the public key from the signature.\n   - It will check if this public key corresponds to one of the authorized signers stored in the smart contract wallet.\n\n3. **Execution Flow**:\n   - The user signs a meta-transaction.\n   - The encoded function call and the user's signature are sent to the Meta-Transaction Proxy Contract.\n   - The proxy contract verifies the signer's signature against the smart contract wallet's authorized signers.\n   - If valid, it forwards the transaction to the target smart contract, using the smart contract wallet's address as `msg.sender`.\n\n### Benefits\n\n- **Compatibility**: This approach ensures that existing smart contract wallets can continue to be used without requiring changes to their implementation.\n- **Security**: Only actions signed by authorized signers of the smart contract wallet will be executed, maintaining high security standards.\n- **Transparency**: The dApps and users will be aware of the intermediary role of the proxy contract, ensuring clear understanding and traceability of the transaction flow.\n\n### Example Implementation\n\n```solidity\npragma solidity ^0.8.0;\n\ncontract MetaTransactionProxy {\n    struct MetaTransaction {\n        address from;\n        bytes functionSignature;\n    }\n    \n    mapping(address => address[]) public authorizedSigners;\n    \n    function executeMetaTransaction(\n        MetaTransaction memory metaTx,\n        bytes memory signature\n    ) public {\n        address signer = recoverSigner(metaTx, signature);\n        require(isAuthorizedSigner(metaTx.from, signer), \"Unauthorized signer\");\n\n        (bool success, ) = metaTx.from.call(metaTx.functionSignature);\n        require(success, \"Meta-Transaction failed\");\n    }\n    \n    function recoverSigner(MetaTransaction memory metaTx, bytes memory signature) internal pure returns (address) {\n        bytes32 hash = keccak256(abi.encode(metaTx.from, metaTx.functionSignature));\n        return ECDSA.recover(hash, signature);\n    }\n    \n    function isAuthorizedSigner(address wallet, address signer) internal view returns (bool) {\n        address[] memory signers = authorizedSigners[wallet];\n        for (uint256 i = 0; i < signers.length; i++) {\n            if (signers[i] == signer) {\n                return true;\n            }\n        }\n        return false;\n    }\n    \n    function addAuthorizedSigner(address wallet, address signer) public {\n        authorizedSigners[wallet].push(signer);\n    }\n    \n    function removeAuthorizedSigner(address wallet, address signer) public {\n        address[] storage signers = authorizedSigners[wallet];\n        for (uint256 i = 0; i < signers.length; i++) {\n            if (signers[i] == signer) {\n                signers[i] = signers[signers.length - 1];\n                signers.pop();\n                break;\n            }\n        }\n    }\n}\n```\n\n### Implementation Strategy\n\n1. Implement and deploy the Meta-Transaction Proxy Contract.\n2. Update the dApps to submit meta-transactions through the proxy contract.\n3. Inform the users and developers about the change and provide necessary documentation.\n\nBy adopting this solution, we ensure that smart contract wallets can seamlessly participate in meta-transactions, thereby enhancing the user experience and expanding the usability of our dApps on the Polygon network.","GenTime":"2024-08-20 00:17:48"}
{"File Name":"libelektra\/cryptograhic_key_handling.md","Context":"## Problem\\nThe crypto plugin applies cryptographic operations to Keys and KeySets.\\nIn order to do that it needs keys and initialization vectors (IV).\\nThe problem is how to retrieve or derivate those keys in a safe way and how\\nto pass them on to the underlying crypto libraries (OpenSSL and libgcrypt\\nat the time of writing).\\n","Decision":"### General Approach\\nThe introduction of a GPG interface enables the user to utilize her existing key-pairs for cryptographic operations in Elektra.\\nThe private key is used for encrypting a random sequence, which serves as seed for a key derivation function (KDF).\\nThis way we can safely derivate cryptographic keys for symmetric value encryption.\\nBoth OpenSSL and libgcrypt have built-in support for the PBKDF2 (see RFC 2898).\\nThe PBKDF2 needs an iteration number and a salt in order to work.\\nThose values will be stored per Key as MetaKey.\\n### Implementation Details\\nDuring the **mount phase** a random master password _r_ is being generated. _r_ is sent to the gpg binary for encryption. The resulting encrypted master password _m_ is stored in the plugin configuration at `config\/masterChallenge`.\\nDuring the **set phase** the master password _m_ is sent to the gpg binary for decryption in order to retrieve _r_. The following steps will be repeated for every Key _k_, that is supposed to be encrypted. A random salt _s(k)_ is generated. By applying the PBKDF2 (mentioned earlier) with _r_ and _s(k)_, the cryptographic key _e(k)_ and the initialization vector _i(k)_ is being derived. The value of _k_ will be encrypted using _e(k)_ and _i(k)_. The seed _s(k)_ will be encoded as prefix into the encrypted value.\\nDuring the **get phase** the master password _m_ is sent to the gpg binary for decryption in order to retrieve _r_. The following steps will be repeated for every Key _k_, that is supposed to be decrypted. The salt _s(k)_ is read from the encrypted message. By applying the PBKDF2 with _r_ and _s(k)_ the values of _e(k)_ and _i(k)_ are restored. Then the encrypted message can be decrypted.\\n","tokens":73,"id":1270,"text":"## Problem\\nThe crypto plugin applies cryptographic operations to Keys and KeySets.\\nIn order to do that it needs keys and initialization vectors (IV).\\nThe problem is how to retrieve or derivate those keys in a safe way and how\\nto pass them on to the underlying crypto libraries (OpenSSL and libgcrypt\\nat the time of writing).\\n\n\n##Decision\n### General Approach\\nThe introduction of a GPG interface enables the user to utilize her existing key-pairs for cryptographic operations in Elektra.\\nThe private key is used for encrypting a random sequence, which serves as seed for a key derivation function (KDF).\\nThis way we can safely derivate cryptographic keys for symmetric value encryption.\\nBoth OpenSSL and libgcrypt have built-in support for the PBKDF2 (see RFC 2898).\\nThe PBKDF2 needs an iteration number and a salt in order to work.\\nThose values will be stored per Key as MetaKey.\\n### Implementation Details\\nDuring the **mount phase** a random master password _r_ is being generated. _r_ is sent to the gpg binary for encryption. The resulting encrypted master password _m_ is stored in the plugin configuration at `config\/masterChallenge`.\\nDuring the **set phase** the master password _m_ is sent to the gpg binary for decryption in order to retrieve _r_. The following steps will be repeated for every Key _k_, that is supposed to be encrypted. A random salt _s(k)_ is generated. By applying the PBKDF2 (mentioned earlier) with _r_ and _s(k)_, the cryptographic key _e(k)_ and the initialization vector _i(k)_ is being derived. The value of _k_ will be encrypted using _e(k)_ and _i(k)_. The seed _s(k)_ will be encoded as prefix into the encrypted value.\\nDuring the **get phase** the master password _m_ is sent to the gpg binary for decryption in order to retrieve _r_. The following steps will be repeated for every Key _k_, that is supposed to be decrypted. The salt _s(k)_ is read from the encrypted message. By applying the PBKDF2 with _r_ and _s(k)_ the values of _e(k)_ and _i(k)_ are restored. Then the encrypted message can be decrypted.\\n","MatchedIds":[13,2256,4346,4848,1318],"Predicted":"## Decision\n1. **Key and IV Derivation:**\n   Utilize a Key Derivation Function (KDF) such as PBKDF2, scrypt, or Argon2 to derive cryptographic keys and IVs from a master key. This ensures the keys and IVs are securely generated and difficult to guess.\n\n2. **Master Key Management:**\n   Use a secure key management solution such as HashiCorp Vault or AWS KMS to store and manage the master key. This master key should only be accessible to the service performing cryptographic operations and should never be hardcoded in the source code.\n\n3. **Key Retrieval:**\n   Implement an interface for retrieving the master key securely from the key management system. Ensure the master key is only retrieved in the environments where cryptographic operations are necessary (e.g., application runtime).\n\n4. **Pass Key and IV to Libraries:**\n   Pass the derived keys and IVs in memory directly to the OpenSSL and libgcrypt libraries using secure memory management practices. Avoid storing these derived keys and IVs on disk or in logs.\n\n5. **Environment Separation:**\n   Ensure the development, staging, and production environments have separate master keys and access controls to minimize the risk of cross-environment attacks.\n\n6. **Periodic Key Rotation:**\n   Implement periodic key rotation policies for the master key to mitigate the risk of key compromise. Ensure backward compatibility and a smooth transition for any in-progress cryptographic operations while rotating the keys. \n\n7. **Audit and Monitoring:**\n   Enable auditing and monitoring on key access and cryptographic operations to detect any unauthorized attempts or anomalies. Ensure logs are secured and monitored consistently.\n\nThese measures will ensure that keys and IVs used in the crypto plugin are securely derived and managed, maintaining the integrity and confidentiality of cryptographic operations.","GenTime":"2024-08-20 00:17:59"}
{"File Name":"jabref\/0010-use-h2-as-internal-database.md","Context":"## Context and Problem Statement\\nWe need to store data internally in a structured way to gain performance.\\n## Decision Drivers\\n* Easy to integrate\\n* Easy to use\\n* Common technology\\n","Decision":"* Easy to integrate\\n* Easy to use\\n* Common technology\\nChosen option: \"H2 Database Engine\", because it was straight-forward to use.\\n","tokens":39,"id":4747,"text":"## Context and Problem Statement\\nWe need to store data internally in a structured way to gain performance.\\n## Decision Drivers\\n* Easy to integrate\\n* Easy to use\\n* Common technology\\n\n\n##Decision\n* Easy to integrate\\n* Easy to use\\n* Common technology\\nChosen option: \"H2 Database Engine\", because it was straight-forward to use.\\n","MatchedIds":[1574,2520,4859,1167,1510],"Predicted":"## Decision\nGiven the need for a performant, structured data store that is easy to integrate, use, and based on common technology, we have decided to use PostgreSQL.\n\n### Rationale:\n* **Easy to integrate**: PostgreSQL has a wide range of libraries and tools for integration with various programming languages and frameworks.\n* **Easy to use**: PostgreSQL provides a robust set of features including a powerful SQL engine, indexing capabilities, and support for complex queries.\n* **Common technology**: PostgreSQL is a well-established and widely used RDBMS, meaning there's abundant documentation, community support, and skilled professionals available.\n\n### Positive Consequences\n* **High Performance**: Leveraging advanced indexing and optimization features of PostgreSQL will enhance performance.\n* **Flexibility**: PostgreSQL\u2019s support for advanced data types and transactions allows us to evolve our data models easily.\n* **Reliability**: PostgreSQL is known for its stability and reliability, making it suitable for mission-critical applications.\n\n### Negative Consequences\n* **Initial Setup**: Some initial effort will be required to set up PostgreSQL and optimize it for our specific use cases.\n* **Maintenance**: Depending on the scale, ongoing maintenance (upgrades, backups, monitoring) may be needed to ensure optimal performance.\n\nThis decision aims to balance ease of use, integration capabilities, and performance using a trusted and proven technology.","GenTime":"2024-08-20 00:18:04"}
{"File Name":"digitalrig-metal-aws\/0005-use-aws-rds-aurora-mysql-for-database-persistence.md","Context":"## Context\\nBookit needs a persistence mechanism.  There are many to choose from that fit an application's needs.  Currently, we believe a SQL\/RDBMS approach fits better than NoSQL.  There's not a lot of context to add to that, just a quick poll of the engineers when we kicked off the project.  With that in mind, we wanted something hosted\/PaaS.\\nGiven we're in AWS, RDS is an obvious choice.  We don't currently have a preference for DB vendor\/implementation, but are drawn to open source and free.  MySql and PostgreSql fit that criteria.\\nFurther, AWS RDS has their own MySql implementation which provides much better performance and up to the minute backups with no degredation for fractions of a penny\/hr more than the standard MySql over RDS.  And while Bookit's usage might not warrant the need for higher performance, there is always a need for high availability and Aurora provides that in a very hands off way.  There is also an Aurora implentation for PostgreSql but at the time of this decision, that is in Preview so we decided to skip it.\\n","Decision":"Use AWS RDS Aurora MySql implemntation for our database persistence mechansim.  Create via Riglet CloudFormation stacks.  Production will run with 1 primary and 1 replica for HA.  Staging & Integration will run with only 1 Primary to keep costs down.\\n","tokens":238,"id":1740,"text":"## Context\\nBookit needs a persistence mechanism.  There are many to choose from that fit an application's needs.  Currently, we believe a SQL\/RDBMS approach fits better than NoSQL.  There's not a lot of context to add to that, just a quick poll of the engineers when we kicked off the project.  With that in mind, we wanted something hosted\/PaaS.\\nGiven we're in AWS, RDS is an obvious choice.  We don't currently have a preference for DB vendor\/implementation, but are drawn to open source and free.  MySql and PostgreSql fit that criteria.\\nFurther, AWS RDS has their own MySql implementation which provides much better performance and up to the minute backups with no degredation for fractions of a penny\/hr more than the standard MySql over RDS.  And while Bookit's usage might not warrant the need for higher performance, there is always a need for high availability and Aurora provides that in a very hands off way.  There is also an Aurora implentation for PostgreSql but at the time of this decision, that is in Preview so we decided to skip it.\\n\n\n##Decision\nUse AWS RDS Aurora MySql implemntation for our database persistence mechansim.  Create via Riglet CloudFormation stacks.  Production will run with 1 primary and 1 replica for HA.  Staging & Integration will run with only 1 Primary to keep costs down.\\n","MatchedIds":[1167,1661,3657,2520,4061],"Predicted":"## Decision\nChosen option: \"Amazon Aurora (MySQL-compatible)\", because\n- Bookit requires a SQL\/RDBMS approach and Amazon Aurora (MySQL-compatible) fits this requirement well.\n- The AWS RDS implementation of MySQL provides superior performance and up-to-the-minute backups with minimal cost increase compared to standard MySQL over RDS.\n- High availability is a critical requirement for Bookit, and Aurora provides this in a highly automated fashion.\n- The PostgreSQL-compatible Aurora is still in Preview, making it less reliable and mature for production use at this time.\n\n### Positive Consequences\n- Better performance and reliable backup mechanisms leading to improved data safety and recovery.\n- Ensured high availability with minimal management overhead, allowing the team to focus on application development rather than database maintenance.\n\n### Negative Consequences\n- Potential future migration effort if the team decides to switch to PostgreSQL-compatible Aurora once it becomes generally available and stable.","GenTime":"2024-08-20 00:18:09"}
{"File Name":"TANF-app\/011-buildpacks.md","Context":"## Context\\nCurrently, our frontend and backend apps are running in Docker containers on Cloud.gov. The deployment process involves building the docker containers at [DockerHub](https:\/\/dockerhub.com). Because there is very little security documentation on DockerHub, the path to getting an ATO with this process would be very difficult. There are other options that may be easier to document, but none of them offer the benefits of buildpacks, which have already been Fed Ramped and documented.\\n","Decision":"Our recommendation is to move to Cloud.gov buildpacks at this time. They are already Fed Ramped, [shift responsibility to Cloud.gov](https:\/\/cloud.gov\/docs\/technology\/responsibilities\/) and ensure tightened security.\\n","tokens":99,"id":3793,"text":"## Context\\nCurrently, our frontend and backend apps are running in Docker containers on Cloud.gov. The deployment process involves building the docker containers at [DockerHub](https:\/\/dockerhub.com). Because there is very little security documentation on DockerHub, the path to getting an ATO with this process would be very difficult. There are other options that may be easier to document, but none of them offer the benefits of buildpacks, which have already been Fed Ramped and documented.\\n\n\n##Decision\nOur recommendation is to move to Cloud.gov buildpacks at this time. They are already Fed Ramped, [shift responsibility to Cloud.gov](https:\/\/cloud.gov\/docs\/technology\/responsibilities\/) and ensure tightened security.\\n","MatchedIds":[3917,1376,3794,3789,4096],"Predicted":"## Decision\nWe will migrate to using Cloud.gov buildpacks for building and deploying our frontend and backend applications. This decision is based on the following reasons:\n* Buildpacks are already FedRAMP authorized, making it significantly easier to document and obtain an Authority to Operate (ATO).\n* Cloud.gov integrates seamlessly with buildpacks, offering a well-documented and secure deployment process.\n* Migrating to buildpacks will simplify our deployment pipeline and reduce dependencies on external services like DockerHub.\n  \nBy leveraging the existing FedRAMP documentation for buildpacks, we streamline our path to compliance while maintaining a reliable and efficient deployment process.","GenTime":"2024-08-20 00:18:12"}
{"File Name":"decode-demo\/0003-remove-spectre-css-and-replace-with-bootstrap.md","Context":"## Context\\nThe deployed demo site must be designed to render nicely on mobile devices.\\nThis is so that participants in the pilot can pretend the demo site provides\\nthe same basic user experience as the wallet once this exists.\\nSpectre CSS at least in the configuration implemented in the original version\\nof this app doesn't render a UI that is usable from a mobile device,\\nparticularly when it comes to form inputs (buttons and fields were tiny and\\nhard to read).\\n","Decision":"We will replace [Spectre.CSS](https:\/\/picturepan2.github.io\/spectre\/) with\\n[Bootstrap](https:\/\/getbootstrap.com\/).\\n","tokens":98,"id":3999,"text":"## Context\\nThe deployed demo site must be designed to render nicely on mobile devices.\\nThis is so that participants in the pilot can pretend the demo site provides\\nthe same basic user experience as the wallet once this exists.\\nSpectre CSS at least in the configuration implemented in the original version\\nof this app doesn't render a UI that is usable from a mobile device,\\nparticularly when it comes to form inputs (buttons and fields were tiny and\\nhard to read).\\n\n\n##Decision\nWe will replace [Spectre.CSS](https:\/\/picturepan2.github.io\/spectre\/) with\\n[Bootstrap](https:\/\/getbootstrap.com\/).\\n","MatchedIds":[847,783,3037,4613,1992],"Predicted":"## Decision\nWe will redesign the deployed demo site using [Bootstrap](https:\/\/getbootstrap.com\/) instead of Spectre CSS. Bootstrap provides extensive support for responsive design out of the box and ensures that form inputs, buttons, and other UI elements are appropriately scaled and readable across various screen sizes, including mobile devices.\n\n### Key Implementations:\n- **Responsive Grid System**: Utilize Bootstrap's responsive grid system to ensure the layout adjusts smoothly for different device sizes.\n- **Mobile-friendly Components**: Replace form inputs, buttons, and other critical UI components with Bootstrap\u2019s mobile-optimized counterparts.\n- **Media Queries**: Implement media queries to handle any custom styling requirements for specific device sizes to further enhance usability.\n- **Enhanced Readability**: Ensure that fonts and touch targets are sized appropriately for mobile users.\n\n### Alternative Solutions Considered:\n- **Spectre CSS Customization**: Customizing Spectre CSS to make elements more usable on mobile devices was considered but deemed too time-consuming and less reliable compared to Bootstrap's well-established mobile support.\n- **Tailwind CSS**: TailwindCSS was also considered for its flexibility and utility-first approach; however, the need for a quicker migration and Bootstrap's comprehensive set of pre-styled components made Bootstrap the better choice.\n\nBy implementing Bootstrap, we aim to provide a seamless and engaging user experience for the demo site that closely mirrors the final wallet application.","GenTime":"2024-08-20 00:18:17"}
{"File Name":"fxa\/0018-use-tailwind-with-custom-scss.md","Context":"## Context and Problem Statement\\nThe [Settings Redesign project](https:\/\/github.com\/mozilla\/fxa\/issues\/3740) provides us with an opportunity to review how FxA approaches and employs CSS, both while building out new components for this project and for FxA going forward.\\nHistorically, the Firefox Accounts codebase has not adhered to a formal CSS structure. This ADR serves to determine how we'll approach our CSS architecture in the Settings Redesign project, evaluating libraries and frameworks to determine which if any will be the best option for the FxA ecosystem. It is part 2 of two [Settings Redesign CSS ADRs](https:\/\/github.com\/mozilla\/fxa\/issues\/5087); part 1, detailing how we'll approach build conventions and variables, [can be found here](https:\/\/github.com\/mozilla\/fxa\/blob\/main\/docs\/adr\/0015-use-css-variables-and-scss.md).\\nConsiderations around class naming conventions, color and measurement standards, interoperability across shared components, and custom configuration options offered by each library to meet Settings Redesign design standards are taken into account. Notably, the new design uses space measurements in increments of 8px and [colors](https:\/\/protocol.mozilla.org\/fundamentals\/color.html) are based in Mozilla Protocol's design system, where a hue's brightness scales in increments of 10.\\n## Decision Drivers\\n- **Reusability** - does the approach yield DRY, lean code that can be reused and repurposed?\\n- **Longevity** - will the approach be supported in upcoming years and will it provide a stable platform for years of revolving HTML through added features and bug fixes?\\n- **Developer experience** - are some team members already familiar with the approach, making the transition easier than an unfamiliar one?\\n- **Ease of use** - will the choice result in a large learning curve or be easy for engineers to pick up?\\n","Decision":"- **Reusability** - does the approach yield DRY, lean code that can be reused and repurposed?\\n- **Longevity** - will the approach be supported in upcoming years and will it provide a stable platform for years of revolving HTML through added features and bug fixes?\\n- **Developer experience** - are some team members already familiar with the approach, making the transition easier than an unfamiliar one?\\n- **Ease of use** - will the choice result in a large learning curve or be easy for engineers to pick up?\\nChosen options: \"Option B\" with Tailwind CSS for majority styling, and implementation details from \"Option D\" when utility classes don't meet the entire need, because:\\n- Of the options set forth, a utility library provides us with the most flexible yet durable set of tools.\\n- Single-purpose classes are performant and reduce the possibility of overly-complex or convoluted stylesheets.\\n- A utility library is leaner and less opinionated compared to a set of UI components and other options, allowing greater flexibility and reusability across various projects.\\n- Our team has prior experience with Tailwind in particular and newcomers should ramp up quickly with a utility pattern.\\n- Tailwind is highly configurable without being cumbersome, allowing us to modify type and spacing scales, define color ranges, and set up media queries to meet our exact needs.\\n- For cases when we do need to write custom SCSS we will structure our React components to initially rely on utility classes, but allow additional custom styles to be written in an adjacent SCSS file when needed. This is also applicable to components in `fxa-components` where the component can accept a `classes` prop with a list of needed utility classes, and any additional styling can be done in an external SCSS file located where the component was composed as needed (e.g., outside of `fxa-components`). CSS variables can be shared across the Tailwind configuration and in custom SCSS.\\n- Note: class name conventions for the custom SCSS will be declared when the library configuration is setup, as we'd like them to make sense together. Examples will be provided in the `fxa-settings` README at this time as well.\\n","tokens":397,"id":381,"text":"## Context and Problem Statement\\nThe [Settings Redesign project](https:\/\/github.com\/mozilla\/fxa\/issues\/3740) provides us with an opportunity to review how FxA approaches and employs CSS, both while building out new components for this project and for FxA going forward.\\nHistorically, the Firefox Accounts codebase has not adhered to a formal CSS structure. This ADR serves to determine how we'll approach our CSS architecture in the Settings Redesign project, evaluating libraries and frameworks to determine which if any will be the best option for the FxA ecosystem. It is part 2 of two [Settings Redesign CSS ADRs](https:\/\/github.com\/mozilla\/fxa\/issues\/5087); part 1, detailing how we'll approach build conventions and variables, [can be found here](https:\/\/github.com\/mozilla\/fxa\/blob\/main\/docs\/adr\/0015-use-css-variables-and-scss.md).\\nConsiderations around class naming conventions, color and measurement standards, interoperability across shared components, and custom configuration options offered by each library to meet Settings Redesign design standards are taken into account. Notably, the new design uses space measurements in increments of 8px and [colors](https:\/\/protocol.mozilla.org\/fundamentals\/color.html) are based in Mozilla Protocol's design system, where a hue's brightness scales in increments of 10.\\n## Decision Drivers\\n- **Reusability** - does the approach yield DRY, lean code that can be reused and repurposed?\\n- **Longevity** - will the approach be supported in upcoming years and will it provide a stable platform for years of revolving HTML through added features and bug fixes?\\n- **Developer experience** - are some team members already familiar with the approach, making the transition easier than an unfamiliar one?\\n- **Ease of use** - will the choice result in a large learning curve or be easy for engineers to pick up?\\n\n\n##Decision\n- **Reusability** - does the approach yield DRY, lean code that can be reused and repurposed?\\n- **Longevity** - will the approach be supported in upcoming years and will it provide a stable platform for years of revolving HTML through added features and bug fixes?\\n- **Developer experience** - are some team members already familiar with the approach, making the transition easier than an unfamiliar one?\\n- **Ease of use** - will the choice result in a large learning curve or be easy for engineers to pick up?\\nChosen options: \"Option B\" with Tailwind CSS for majority styling, and implementation details from \"Option D\" when utility classes don't meet the entire need, because:\\n- Of the options set forth, a utility library provides us with the most flexible yet durable set of tools.\\n- Single-purpose classes are performant and reduce the possibility of overly-complex or convoluted stylesheets.\\n- A utility library is leaner and less opinionated compared to a set of UI components and other options, allowing greater flexibility and reusability across various projects.\\n- Our team has prior experience with Tailwind in particular and newcomers should ramp up quickly with a utility pattern.\\n- Tailwind is highly configurable without being cumbersome, allowing us to modify type and spacing scales, define color ranges, and set up media queries to meet our exact needs.\\n- For cases when we do need to write custom SCSS we will structure our React components to initially rely on utility classes, but allow additional custom styles to be written in an adjacent SCSS file when needed. This is also applicable to components in `fxa-components` where the component can accept a `classes` prop with a list of needed utility classes, and any additional styling can be done in an external SCSS file located where the component was composed as needed (e.g., outside of `fxa-components`). CSS variables can be shared across the Tailwind configuration and in custom SCSS.\\n- Note: class name conventions for the custom SCSS will be declared when the library configuration is setup, as we'd like them to make sense together. Examples will be provided in the `fxa-settings` README at this time as well.\\n","MatchedIds":[1992,387,389,157,802],"Predicted":"## Decision\nChosen option: Use a combination of BEM (Block Element Modifier) and CSS Modules, because:\n\n1. **Reusability**: BEM promotes reusability and scalability by using a structured naming convention that avoids style conflicts. CSS Modules further enhance this by scope-limiting CSS to individual components, making it easier to repurpose styles without unexpected clashes.\n\n2. **Longevity**: Both BEM and CSS Modules are established approaches with widespread adoption and proven staying power. This means our codebase will remain maintainable and accessible to developers for years to come.\n\n3. **Developer Experience**: Many developers are already familiar with BEM, and CSS Modules integrate nicely with modern JavaScript frameworks like React. This familiarity shortens the ramp-up time for new developers and allows them to be productive faster.\n\n4. **Ease of Use**: BEM\u2019s straightforward naming convention paired with CSS Modules' encapsulation leads to a low learning curve. The simplicity of the BEM convention ensures that class names remain readable and the project structure clear, while CSS Modules require minimal configuration to get started.\n\n### Positive Consequences\n- **Readability and Maintainability**: BEM\u2019s naming convention ensures that class names convey their role and relationship to other elements, which makes the CSS more readable and easier to maintain.\n- **Scope Limitation**: CSS Modules prevent style leakage across components, allowing for a more modular architecture.\n- **Interoperability with Tools**: Both BEM and CSS Modules are compatible with a variety of pre-processors and build tools, making the transition smooth.\n- **Design Consistency**: Adhering to a formal structure ensures that color and spacing standards are uniformly applied across the project, aligning with our design tokens and new design principles.\n\n### Negative Consequences\n- **Initial Setup Effort**: There will be some initial overhead in setting up and enforcing a new CSS structure across the project. This includes refactoring existing styles to match the new conventions.\n- **Developer Adjustment**: While many may be familiar, some developers will need to adjust to the BEM and CSS Modules syntaxes if they haven't used them before.\n- **Naming Overhead**: The verbosity of BEM can lead to longer class names, which might be seen as a drawback by some developers.\n\nBy choosing a combination of BEM and CSS Modules, we align our CSS architecture with best practices, making our codebase more maintainable and scalable while promoting a consistent and reusable styling system.","GenTime":"2024-08-20 00:18:24"}
{"File Name":"meadow\/0018-preservation-storage-object-naming-scheme.md","Context":"## Context\\nThe application needs a way to store objects in preservation buckets that facilitate upload and retrieval and allows for duplicate file names among FileSets associated with a Work object.\\n","Decision":"Use a combination of the pairtree of a FileSet ULID plus its sha256 checksum as the key for S3 objects in the preservation bucket. So an FileSet with a ULID `01dpxt2xajvkdsbckqrs8ry677`\\nand sha256 checksum `87d2c7faf7774a8b07c43be06055c67c4bd602b8ec0e9d6b15241967d500d356`will be stored in s3 as\\n`s3:\/\/[preservation-bucket]\/01\/dp\/xt\/2x\/01dpxt2xajvkdsbckqrs8ry677\/87d2c7faf7774a8b07c43be06055c67c4bd602b8ec0e9d6b15241967d500d356`\\n","tokens":35,"id":3831,"text":"## Context\\nThe application needs a way to store objects in preservation buckets that facilitate upload and retrieval and allows for duplicate file names among FileSets associated with a Work object.\\n\n\n##Decision\nUse a combination of the pairtree of a FileSet ULID plus its sha256 checksum as the key for S3 objects in the preservation bucket. So an FileSet with a ULID `01dpxt2xajvkdsbckqrs8ry677`\\nand sha256 checksum `87d2c7faf7774a8b07c43be06055c67c4bd602b8ec0e9d6b15241967d500d356`will be stored in s3 as\\n`s3:\/\/[preservation-bucket]\/01\/dp\/xt\/2x\/01dpxt2xajvkdsbckqrs8ry677\/87d2c7faf7774a8b07c43be06055c67c4bd602b8ec0e9d6b15241967d500d356`\\n","MatchedIds":[2515,2215,2122,2896,2145],"Predicted":"## Decision\nAfter evaluating various storage solutions, we have decided to use Amazon S3 for storing objects in preservation buckets. The rationale for this decision includes:\n\n- **Facilitates Easy Upload and Retrieval**: Amazon S3 provides robust APIs and SDKs that make it simple to upload and retrieve objects.\n- **Support for Versioning**: Amazon S3 supports versioning, which allows for keeping multiple versions of an object in the same bucket. This will help us manage duplicate file names among FileSets associated with a Work object without data loss.\n- **Scalable**: Amazon S3 is highly scalable and can automatically handle increasing amounts of data without manual intervention.\n- **Durability and Availability**: Amazon S3 offers high durability (99.999999999%) and availability (99.99% uptime) for data, ensuring that our preservation buckets are reliably accessible.\n- **Storage Classes and Lifecycle Policies**: We can leverage different storage classes and lifecycle policies to optimize costs by transitioning older data to more cost-effective storage options.\n- **Security**: Amazon S3 supports granular permissions and encryption for secure data storage and access control.\n\n### Implementation Plan\n1. **Bucket Creation**: Create preservation buckets on Amazon S3 that will store the objects.\n2. **Enable Versioning**: Enable versioning on these buckets to handle duplicate file names by managing different versions of the same object.\n3. **Access Control**: Define and implement bucket policies and access control lists (ACLs) to manage permissions and ensure only authorized users can upload and retrieve objects.\n4. **Object Storage Key Structure**: Utilize a structured key naming convention to organize objects logically while allowing duplicate file names within different FileSets. For example, prefix the file names with a unique identifier associated with the FileSet.\n5. **Lifecycle Policies**: Set up lifecycle policies to transition older versions of objects to more cost-effective storage classes or delete them after a specified period.\n\nHere is a quick example of a key naming convention:\n\n```\nworkObjectID\/FileSetID\/filename.ext\n```\n\nThis structure allows duplicate `filename.ext` within different `FileSetID` for the same `workObjectID`.\n\nBy leveraging Amazon S3, we can ensure that our preservation buckets meet the application's requirements for efficient object storage, handling of duplicate file names, scalability, durability, and security.","GenTime":"2024-08-20 00:18:44"}
{"File Name":"cygnus-infra\/0002-use-debian-as-a-host-os.md","Context":"## Context\\nI used the following points to make my decision:\\n- Ubuntu 20.04 uses a unique auto-installation method, which is generally nice, but completely non-transferable\\n- `snapd` remains controversial, but using Ubuntu systems forces that path more nowadays\\n- Debian should be relatively \"install and forget\"\\n- Centos is more opinionated about some things like how network configuration is done\\n- Centos doesn't do automatic security upgrades, at least not in the same way RHEL advertises\\n- I've successfully used Debian before for this role, before trying out Ubuntu again\\n- Remastering a Debian iso with the preseed configuration can result in a one-button install process\\n","Decision":"Use Debian buster as the hypervisor OS, with an automated installation via Debian preseeding.\\n","tokens":145,"id":3868,"text":"## Context\\nI used the following points to make my decision:\\n- Ubuntu 20.04 uses a unique auto-installation method, which is generally nice, but completely non-transferable\\n- `snapd` remains controversial, but using Ubuntu systems forces that path more nowadays\\n- Debian should be relatively \"install and forget\"\\n- Centos is more opinionated about some things like how network configuration is done\\n- Centos doesn't do automatic security upgrades, at least not in the same way RHEL advertises\\n- I've successfully used Debian before for this role, before trying out Ubuntu again\\n- Remastering a Debian iso with the preseed configuration can result in a one-button install process\\n\n\n##Decision\nUse Debian buster as the hypervisor OS, with an automated installation via Debian preseeding.\\n","MatchedIds":[2619,923,1376,583,4937],"Predicted":"## Decision\nWe will switch to using Debian for our systems. Debian's stability and \"install and forget\" nature make it an ideal choice for our use case. Additionally, remastering a Debian ISO with the preseed configuration will allow us to streamline the installation process, making it efficient and less error-prone. Given past successful experiences with Debian, we can proceed with confidence, avoiding issues related to `snapd` and the non-transferable auto-installation method of Ubuntu, as well as the lack of automatic security upgrades in Centos.","GenTime":"2024-08-20 00:18:47"}
{"File Name":"cloud-on-k8s\/0005-configurable-operator.md","Context":"## Context and Problem Statement\\nMost operators out there operate in one of these two modes:\\n1. Cluster-wide operator. Can manage resources in all namespaces, with cluster-wide admin privilege. A single operator running on the cluster.\\n2. Namespaced operator. Can manage resources in the namespace it's deployed in, with admin permissions in that namespace. Several operators can be running in different namespaces.\\nThe first option (cluster-wide single operator) has some major drawbacks:\\n* it does not scale well with the number of clusters\\n* it requires elevated permissions on the cluster\\nThe second option (namespace operator) also has some major drawbacks:\\n* it does not play well with cross-namespace features (a single enterprise license pool for multiple clusters in multiple namespaces, cross-cluster search and replication on clusters across namespaces)\\n* to deploy 5 clusters in 5 different namespaces, it requires 5 operators running. A single one could have been technically enough.\\n## Decision Drivers\\n* Scalability (down): must be able to scale down to single-cluster deployments without being overly complex to manage.\\n* Scalability (up): Must be able to scale up with large k8s installations and manage tens of thousands of clusters simultaneously.\\n* In any sufficiently large installation with clusters under load there is going to be high variance between response times for different ES API calls, and one clusters responsiveness should not be able to negatively affect the operations of any other cluster.\\n* Security: The solution should have an easy to understand story around credentials, RBAC permissions and service accounts.\\n* As far as possible, adhere to the principle of least amount of access: we should not require more permissions than strictly necessary for the operators to accomplish what they need to.\\n","Decision":"* Scalability (down): must be able to scale down to single-cluster deployments without being overly complex to manage.\\n* Scalability (up): Must be able to scale up with large k8s installations and manage tens of thousands of clusters simultaneously.\\n* In any sufficiently large installation with clusters under load there is going to be high variance between response times for different ES API calls, and one clusters responsiveness should not be able to negatively affect the operations of any other cluster.\\n* Security: The solution should have an easy to understand story around credentials, RBAC permissions and service accounts.\\n* As far as possible, adhere to the principle of least amount of access: we should not require more permissions than strictly necessary for the operators to accomplish what they need to.\\nChosen option: option 2 (configurable operator), because it gives us more flexibility on the deployment strategy, and allows restricting RBAC permissions to a finer-grained level.\\n### Positive Consequences\\n* Much more flexibility to cover various deployment scenarios\\n* a single cluster-wide operator\\n* one operator per namespace\\n* one operator for all production namespaces, another one for all staging namespaces\\n* and so on\\n* We don't have to require cluster-level permissions to handle enterprise licensing\\n* A single operator concept, no namespace\/global\/ecosystem vocabulary madness\\n### Negative Consequences\\n* Too many options can lead to confusion, we need proper documentation\\n* Increased yaml complexity: need to develop a tool to generate yaml specifications\\n* The controller-runtime is not ready yet for multi-namespace watches\\n","tokens":354,"id":4707,"text":"## Context and Problem Statement\\nMost operators out there operate in one of these two modes:\\n1. Cluster-wide operator. Can manage resources in all namespaces, with cluster-wide admin privilege. A single operator running on the cluster.\\n2. Namespaced operator. Can manage resources in the namespace it's deployed in, with admin permissions in that namespace. Several operators can be running in different namespaces.\\nThe first option (cluster-wide single operator) has some major drawbacks:\\n* it does not scale well with the number of clusters\\n* it requires elevated permissions on the cluster\\nThe second option (namespace operator) also has some major drawbacks:\\n* it does not play well with cross-namespace features (a single enterprise license pool for multiple clusters in multiple namespaces, cross-cluster search and replication on clusters across namespaces)\\n* to deploy 5 clusters in 5 different namespaces, it requires 5 operators running. A single one could have been technically enough.\\n## Decision Drivers\\n* Scalability (down): must be able to scale down to single-cluster deployments without being overly complex to manage.\\n* Scalability (up): Must be able to scale up with large k8s installations and manage tens of thousands of clusters simultaneously.\\n* In any sufficiently large installation with clusters under load there is going to be high variance between response times for different ES API calls, and one clusters responsiveness should not be able to negatively affect the operations of any other cluster.\\n* Security: The solution should have an easy to understand story around credentials, RBAC permissions and service accounts.\\n* As far as possible, adhere to the principle of least amount of access: we should not require more permissions than strictly necessary for the operators to accomplish what they need to.\\n\n\n##Decision\n* Scalability (down): must be able to scale down to single-cluster deployments without being overly complex to manage.\\n* Scalability (up): Must be able to scale up with large k8s installations and manage tens of thousands of clusters simultaneously.\\n* In any sufficiently large installation with clusters under load there is going to be high variance between response times for different ES API calls, and one clusters responsiveness should not be able to negatively affect the operations of any other cluster.\\n* Security: The solution should have an easy to understand story around credentials, RBAC permissions and service accounts.\\n* As far as possible, adhere to the principle of least amount of access: we should not require more permissions than strictly necessary for the operators to accomplish what they need to.\\nChosen option: option 2 (configurable operator), because it gives us more flexibility on the deployment strategy, and allows restricting RBAC permissions to a finer-grained level.\\n### Positive Consequences\\n* Much more flexibility to cover various deployment scenarios\\n* a single cluster-wide operator\\n* one operator per namespace\\n* one operator for all production namespaces, another one for all staging namespaces\\n* and so on\\n* We don't have to require cluster-level permissions to handle enterprise licensing\\n* A single operator concept, no namespace\/global\/ecosystem vocabulary madness\\n### Negative Consequences\\n* Too many options can lead to confusion, we need proper documentation\\n* Increased yaml complexity: need to develop a tool to generate yaml specifications\\n* The controller-runtime is not ready yet for multi-namespace watches\\n","MatchedIds":[3889,3110,3109,2834,631],"Predicted":"## Decision\n\nWe will implement a hybrid operator deployment strategy, combining the benefits of both cluster-wide and namespaced operators.\n\n### Implementation\n\n1. **Scoped Cluster-Wide Operator**:\n   - Deploy a single cluster-wide operator with limited, scoped permissions.\n   - The operator will be granted permissions only for specific resources it needs to manage, thereby adhering to the principle of least privilege.\n   - This operator will manage cross-namespace functionalities such as enterprise license pooling, cross-cluster search, and replication.\n\n2. **Namespaced Operators**:\n   - Deploy separate namespaced operators for managing individual clusters within their respective namespaces.\n   - These operators will have admin permissions but only within their assigned namespaces.\n   - This ensures that large installations with multiple clusters maintain independence, reducing the likelihood that issues in one cluster will affect others.\n\n### Positive Consequences\n\n- **Scalability Down**: Small single-cluster deployments can use only the namespaced operator, keeping the setup straightforward and manageable.\n- **Scalability Up**: Large installations gain the ability to manage many clusters efficiently while maintaining cross-namespace functionalities through the scoped cluster-wide operator.\n- **Security**: By adhering to the principle of least privilege, both operator types will have the minimum required permissions, thus reducing potential security risks.\n- **Operational Isolation**: Ensures that issues in one cluster do not negatively impact others, as namespaced operators manage clusters independently.\n\n### Negative Consequences\n\n- **Operational Complexity**: Managing a hybrid strategy can introduce more complexity in the deployment pipeline and operational management.\n- **Resource Consumption**: Running multiple operators (namespaced and cluster-wide) will increase the resource footprint on the Kubernetes cluster.\n- **Consistency Management**: Ensuring consistency between the scoped permissions of the cluster-wide operator and the full permissions of the namespaced operators requires vigilant configuration management.\n\n### Conclusion\n\nBy implementing this hybrid strategy, we balance the needs for scalability, security, and operational isolation. This approach offers the flexibility to manage both small and large installations efficiently while maintaining clear and strict control over permissions and resources.","GenTime":"2024-08-20 00:18:57"}
{"File Name":"experimenter\/0009-nimbus-web-sdk-architecture.md","Context":"## Context and Problem Statement\\nThis is part of a body of work necessary to support the use of Nimbus within web applications.\\nThe current Nimbus SDK is written in such a way that it supports client-oriented experimentation \u2014 experiments are downloaded, evaluated, and stored on the client, and a feature store is exposed with the experiment branches applied.\\nIn previous decisions (not in this repository), we've already decided that in order to support web experimentation the Nimbus SDK will need to be updated to be stateless, to support a more statically defined set of helper methods, and to have additional support for Python.\\nUltimately, the problem we're trying to solve can be boiled down to one question \u2014 how can we update the Nimbus SDK to support web applications while continuing to support the existing clients?\\nAs an example of what the Cirrus API might look like, we can likely expect endpoints to perform the following:\\n* Enroll a user into available experiment(s)\\n* This would return the enrolled experiments as well as the feature values given the enrollments\\n* Fetch the default feature values\\n* Fetch the feature manifest\\n* Fetch a specific feature value given enrolled experiments\\n## Decision Drivers\\n* Architecture decisions for Nimbus on the web[<sup>[1.i]<\/sup>](#links)\\n* The core logic of the existing SDK (Rust) will be re-used for the Nimbus web service (Cirrus)\\n* The SDK needs to support Python via UniFFI\\n* The SDK needs to be stateless\\n","Decision":"* Architecture decisions for Nimbus on the web[<sup>[1.i]<\/sup>](#links)\\n* The core logic of the existing SDK (Rust) will be re-used for the Nimbus web service (Cirrus)\\n* The SDK needs to support Python via UniFFI\\n* The SDK needs to be stateless\\nWe have decided to move forward with option number 2, \"Cargo features, one library\".\\nThis option, like the others mentioned, meets the key decision drivers.\\nWe believe using this option will be the most maintainable long-term, despite the added complexity of using cargo features.\\nIn addition, implementing this option has a similarly short timeline and amount of work necessary as compared to the \"Cargo features, separate library\" option, without the additional overhead of more complex typing.\\n### Positive Consequences\\n* Meets all decision drivers.\\n* Small amount of work necessary with a very limited amount of code churn.\\n### Negative Consequences\\n* It will be difficult to draw boundaries between Cirrus and Nimbus code.\\n* We could run into unexpected issues with UniFFI, as multiple UDL files in a single crate has not yet been tested.\\n2. [Cargo Features](https:\/\/doc.rust-lang.org\/cargo\/reference\/features.html)\\n2. Examples\\n1. [Multi-module typing example](https:\/\/github.com\/jeddai\/application-services\/pull\/1\/files#diff-02305e8e02a7900352e67be1fd2eef0b5a7c7cf91f4cc3e4559668c124d34e88R11-R19)\\n","tokens":307,"id":1819,"text":"## Context and Problem Statement\\nThis is part of a body of work necessary to support the use of Nimbus within web applications.\\nThe current Nimbus SDK is written in such a way that it supports client-oriented experimentation \u2014 experiments are downloaded, evaluated, and stored on the client, and a feature store is exposed with the experiment branches applied.\\nIn previous decisions (not in this repository), we've already decided that in order to support web experimentation the Nimbus SDK will need to be updated to be stateless, to support a more statically defined set of helper methods, and to have additional support for Python.\\nUltimately, the problem we're trying to solve can be boiled down to one question \u2014 how can we update the Nimbus SDK to support web applications while continuing to support the existing clients?\\nAs an example of what the Cirrus API might look like, we can likely expect endpoints to perform the following:\\n* Enroll a user into available experiment(s)\\n* This would return the enrolled experiments as well as the feature values given the enrollments\\n* Fetch the default feature values\\n* Fetch the feature manifest\\n* Fetch a specific feature value given enrolled experiments\\n## Decision Drivers\\n* Architecture decisions for Nimbus on the web[<sup>[1.i]<\/sup>](#links)\\n* The core logic of the existing SDK (Rust) will be re-used for the Nimbus web service (Cirrus)\\n* The SDK needs to support Python via UniFFI\\n* The SDK needs to be stateless\\n\n\n##Decision\n* Architecture decisions for Nimbus on the web[<sup>[1.i]<\/sup>](#links)\\n* The core logic of the existing SDK (Rust) will be re-used for the Nimbus web service (Cirrus)\\n* The SDK needs to support Python via UniFFI\\n* The SDK needs to be stateless\\nWe have decided to move forward with option number 2, \"Cargo features, one library\".\\nThis option, like the others mentioned, meets the key decision drivers.\\nWe believe using this option will be the most maintainable long-term, despite the added complexity of using cargo features.\\nIn addition, implementing this option has a similarly short timeline and amount of work necessary as compared to the \"Cargo features, separate library\" option, without the additional overhead of more complex typing.\\n### Positive Consequences\\n* Meets all decision drivers.\\n* Small amount of work necessary with a very limited amount of code churn.\\n### Negative Consequences\\n* It will be difficult to draw boundaries between Cirrus and Nimbus code.\\n* We could run into unexpected issues with UniFFI, as multiple UDL files in a single crate has not yet been tested.\\n2. [Cargo Features](https:\/\/doc.rust-lang.org\/cargo\/reference\/features.html)\\n2. Examples\\n1. [Multi-module typing example](https:\/\/github.com\/jeddai\/application-services\/pull\/1\/files#diff-02305e8e02a7900352e67be1fd2eef0b5a7c7cf91f4cc3e4559668c124d34e88R11-R19)\\n","MatchedIds":[2067,389,1182,1540,4952],"Predicted":"## Decision\n* Architecture decisions for Nimbus on the web[<sup>[1.i]<\/sup>](#links)\\n* The core logic of the existing SDK (Rust) will be re-used for the Nimbus web service (Cirrus)\\n* The SDK needs to support Python via UniFFI\\n* The SDK needs to be stateless\\n\n\nChosen Option: **Leverage the existing Rust core logic and expose a stateless API via a web service (Cirrus) while using UniFFI for Python support.**\n\n- **Core Logic in Rust:** The core experimentation logic that is currently in the Nimbus SDK will be reused and adapted to be stateless. This will allow the same logic to be applied consistently across both client and web scenarios.\n  \n- **Stateless Cirrus API:** Develop a stateless web service, named Cirrus, that provides endpoints for the necessary operations including enrolling users into experiments, fetching default feature values, fetching the feature manifest, and retrieving specific feature values given enrollments. This will ensure that experiments and their evaluations are done on the server rather than the client, making it easier to support web applications.\n  \n- **UniFFI for Python Support:** Use UniFFI to automatically generate Python bindings for the Nimbus SDK. This will ensure that the core Rust logic can be leveraged within Python applications, providing seamless support for web-based experimentation.\n\n### Positive Consequences\n- **Consistency:** Having a single Rust core logic ensures consistent behavior across both web and client-side experimentation.\n- **Efficiency:** By reusing existing code, implementation time can be reduced, and the existing, well-tested logic can be trusted.\n- **Flexibility:** The stateless nature of the Cirrus API allows it to scale easily and handle a large number of requests without maintaining client states.\n\n### Negative Consequences\n- **Complexity in Migration:** The current client-oriented SDK will need significant changes to adapt to the stateless architecture for web services.\n- **Additional Development:** New API endpoints and the Cirrus web service infrastructure must be developed and maintained.\n\nTherefore, by leveraging existing Rust logic, introducing a stateless Cirrus API, and supporting Python via UniFFI, we can update the Nimbus SDK to effectively support web applications while continuing to serve the needs of existing clients.","GenTime":"2024-08-20 00:19:09"}
{"File Name":"cf-k8s-networking\/0008-implement-workarounds-for-capi-and-log-cache-to-unblock-global-strict-mtls.md","Context":"## Context\\nWe need to turn on STRICT mTLS for all components on the mesh. However, some\\ncomponents are currently incompatible with this mode.\\nCAPI is incompatible because it uses an init container to run migrations. This\\ninit container comes up before the sidecar, so it is unable to establish an mTLS\\nconnection with the capi database. This causes the init container to fail and\\nprevents capi from coming up. See [this\\nissue](https:\/\/github.com\/cloudfoundry\/capi-k8s-release\/issues\/12) in capi.\\nLog-cache is incompatible because it is configured to establish its own tls\\nconnection, which is incompatible with the mTLS the sidecars are attempting to\\nestablish.\\n","Decision":"We have provided configuration workarounds in the form of Policies, that were\\nplaced in the cf-for-k8s repo to be owned by the respective teams that manage\\nthe troublesome components.\\n[Pull Request](https:\/\/github.com\/cloudfoundry\/cf-for-k8s\/pull\/35)\\n","tokens":153,"id":2249,"text":"## Context\\nWe need to turn on STRICT mTLS for all components on the mesh. However, some\\ncomponents are currently incompatible with this mode.\\nCAPI is incompatible because it uses an init container to run migrations. This\\ninit container comes up before the sidecar, so it is unable to establish an mTLS\\nconnection with the capi database. This causes the init container to fail and\\nprevents capi from coming up. See [this\\nissue](https:\/\/github.com\/cloudfoundry\/capi-k8s-release\/issues\/12) in capi.\\nLog-cache is incompatible because it is configured to establish its own tls\\nconnection, which is incompatible with the mTLS the sidecars are attempting to\\nestablish.\\n\n\n##Decision\nWe have provided configuration workarounds in the form of Policies, that were\\nplaced in the cf-for-k8s repo to be owned by the respective teams that manage\\nthe troublesome components.\\n[Pull Request](https:\/\/github.com\/cloudfoundry\/cf-for-k8s\/pull\/35)\\n","MatchedIds":[2240,2243,2236,3909,50],"Predicted":"## Decision\nTo enable STRICT mTLS for all compatible components while allowing for exceptions, we will implement the following two-fold approach:\n\n### 1. Selective Namespace Exclusion\nWe will create a dedicated namespace for CAPI and log-cache and label it with `istio-injection=disabled`. This will exclude these components from automatic sidecar injection and, consequently, from the STRICT mTLS policy. This allows CAPI and log-cache to function normally while other components can operate under STRICT mTLS.\n\n```yaml\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: capi-logcache-namespace\n  labels:\n    istio-injection: disabled\n```\n\n### 2. Apply Custom Destination Rules\nWe will configure custom destination rules for CAPI and log-cache to explicitly disable mTLS for these components. This ensures that any communication to and from these components does not attempt to use mTLS.\n\n```yaml\napiVersion: networking.istio.io\/v1alpha3\nkind: DestinationRule\nmetadata:\n  name: disable-mtls-for-capi\nspec:\n  host: capi-service.capi-logcache-namespace.svc.cluster.local\n  trafficPolicy:\n    tls:\n      mode: DISABLE\n---\napiVersion: networking.istio.io\/v1alpha3\nkind: DestinationRule\nmetadata:\n  name: disable-mtls-for-logcache\nspec:\n  host: logcache-service.capi-logcache-namespace.svc.cluster.local\n  trafficPolicy:\n    tls:\n      mode: DISABLE\n```\n\n### Implementation Steps:\n\n1. **Create Namespace:**\n   - Deploy a new namespace named `capi-logcache-namespace` with the `istio-injection=disabled` label.\n\n2. **Update Deployment Configs:**\n   - Update the deployment configuration for CAPI and log-cache to deploy these components into the new namespace `capi-logcache-namespace`.\n\n3. **Apply Custom Destination Rules:**\n   - Deploy the destination rules to disable mTLS specifically for the `capi-service` and `logcache-service`.\n\n4. **Enable mTLS Globally:**\n   - Update the Istio configuration to enforce STRICT mTLS for all other namespaces and components.\n\n### Future Considerations:\n- **CAPI Init Container:** Work with the CAPI team to modify the init container's behavior to be compatible with mTLS, allowing it to bring up the sidecar before running migrations.\n- **Log-cache TLS Configuration:** Review and align the TLS configuration of log-cache with the service mesh requirements to eliminate the need for these exclusions.\n\nBy following these steps, we can achieve the goal of enforcing STRICT mTLS on all compatible components while ensuring that CAPI and log-cache can continue to operate until they are refactored for full compatibility.","GenTime":"2024-08-20 00:19:17"}
{"File Name":"dapr\/ENG-004-signing.md","Context":"## Context\\nAuthenticode signing of binaries.\\n","Decision":"* Binaries will not be signed with Microsoft keys. In future we can revisit to sign the binaries with dapr.io keys.\\n","tokens":11,"id":72,"text":"## Context\\nAuthenticode signing of binaries.\\n\n\n##Decision\n* Binaries will not be signed with Microsoft keys. In future we can revisit to sign the binaries with dapr.io keys.\\n","MatchedIds":[241,5070,3853,3797,2181],"Predicted":"## Decision\nWe will use [SignTool](https:\/\/learn.microsoft.com\/en-us\/windows\/win32\/seccrypto\/signtool) to Authenticode sign our binaries. SignTool is a command-line tool that is part of the Windows SDK and can be integrated into our build pipeline. This will ensure that our binaries are signed with a trusted certificate, enhancing security and trustworthiness. Additionally, we will automate the integration of SignTool in our CI\/CD pipeline to ensure that all produced binaries are consistently and properly signed before they are released.","GenTime":"2024-08-20 00:19:20"}
{"File Name":"firefox-tv\/adr-0004-fftv-release-versioning.md","Context":"## Context\\nWe build several different versions of Firefox for purposes such as release, beta testing, and staged rollout. It's not obvious how the version numbers are related to these types of builds, and features have sometimes disappeared in rollback versions, so we document our strategy here.\\nFirefox TV releases are listed [https:\/\/github.com\/mozilla-mobile\/firefox-tv\/releases](here). (These include LATs, which are not included in the changelog, and the changelog may include additional information, like reasons for re-releasing a version.)\\nAs of the time of writing, the current release version is `3.9`.\\n","Decision":"Firefox TV versioning is based off of [https:\/\/semver.org\/](semantic versioning) of MAJOR.MINOR.PATCH, but reflects features rather than API compatibility.\\nAdditionally, we also use alphanumeric suffixes to clearly differentiate between early test builds, releases, and re-releases.\\nEach release has a *tag* prefixed by `v`, such as `v3.8` and are listed in the [https:\/\/github.com\/mozilla-mobile\/firefox-tv\/tags](Tags) page of the repo.\\n### Semantic Versioning\\n* MAJOR version changes signal significant changes to UI or functionality\\n* MINOR version changes are released every Sprint, unless they are skipped for release blockers\\n* PATCH version changes are for critical bug-fixes that cannot wait for the next Sprint.\\n* (LETTER-SUFFIX) reflects builds for our additional purposes that are detailed in following sections.\\n### Release\\nAs of 3.8, public releases have no suffix, and are released using the staged rollout capability of the Amazon Developer portal.\\n### Live App Testing (-LAT1)\\nAs part of our early testing, we create Live App Test (LAT) builds to send out candidate builds to our early testing groups before a release.\\nThese have a `-LAT1` suffix, where the number is incremented per test build sent out per version. For example, the second test build for 3.5 would be `3.5-LAT2`.\\nThis is first used in `3.3.0-LAT1`. These are used for testing, not general release.\\n#### Deprecated LAT versioning\\nPreviously, the versioning was much more confusing. We wanted to preserve monotonic order versioning, so a LAT would have an additional number appended at the end of the *previous* version; for example, the second LAT testing the 3.2 release would be versioned `3.1.3.2`, because the last released version before `3.2` was `3.1.3`.\\nThis deprecated LAT versioning was used between `2.1.0.1` and `3.1.3.2`.\\n### GeckoView (-GV)\\nCurrently, there are two distinct web engines that Firefox for Fire TV can be build with: the system WebView or GeckoView. Since a build currently can only use one of these, when we make a build that uses the GeckoView engine, we need a separate suffix to differentiate it.\\nThese GeckoView builds are suffixed with `-GV`.\\nThis is first used in `3.4-GV`, but is used for testing and not released to the general population.\\n### Re-Release (-A)\\nThere are two cases for re-release:\\n1) Rollback to a previous version due to critical bugs (e.g. rollback of 3.4 should be 3.3-A, although this is untested, and the platform may not allow decremented versioning, in which case, we would release the rollback as 3.4-A)\\n1) (deprecated) Release of a tested \"staged rollout\" build to the rest of the devices. (This is no longer used because staged rollout capability has been added to the app store.) This was monotonic because the \"general population\" devices had not been upgraded past this version.\\nThis is a build that has already been released before, either to a portion of the population, or because in a subsequent release we needed to do a version bump in order to push out a release, but did not change the code.\\nThese re-release builds are suffixed with a letter starting with `-A`, which is incremented with each re-release.\\nThis is first used in `3.4-A`.\\n#### Deprecated re-release versioning\\nBefore we started using letters to signify re-releases, we simply bumped the version number, so there are several versions that are simply re-releases of previous versions, but with different version numbers. These are listed below:\\n**3.1.3** is the same version as:\\n* 3.2.5\\n* 3.3\\n* 3.4-A\\n* 3.4-B\\n(and also listed in the changelog)\\n### Deprecated Split-Device Staged Rollout (-RO)\\nThese split-device staged rollout releases were suffixed with a `-RO`, e.g. `3.7-RO`.\\nBefore the staged rollout capability was added to the Amazon Developer portal, we handled staged rollout by releasing the newest version only to a single device, in this case the Firefox TV 4K Pendant (AFTN).\\nThis versioning scheme was used between `3.5-RO` and `3.7-RO`.\\n### Status: Accepted\\n","tokens":131,"id":4364,"text":"## Context\\nWe build several different versions of Firefox for purposes such as release, beta testing, and staged rollout. It's not obvious how the version numbers are related to these types of builds, and features have sometimes disappeared in rollback versions, so we document our strategy here.\\nFirefox TV releases are listed [https:\/\/github.com\/mozilla-mobile\/firefox-tv\/releases](here). (These include LATs, which are not included in the changelog, and the changelog may include additional information, like reasons for re-releasing a version.)\\nAs of the time of writing, the current release version is `3.9`.\\n\n\n##Decision\nFirefox TV versioning is based off of [https:\/\/semver.org\/](semantic versioning) of MAJOR.MINOR.PATCH, but reflects features rather than API compatibility.\\nAdditionally, we also use alphanumeric suffixes to clearly differentiate between early test builds, releases, and re-releases.\\nEach release has a *tag* prefixed by `v`, such as `v3.8` and are listed in the [https:\/\/github.com\/mozilla-mobile\/firefox-tv\/tags](Tags) page of the repo.\\n### Semantic Versioning\\n* MAJOR version changes signal significant changes to UI or functionality\\n* MINOR version changes are released every Sprint, unless they are skipped for release blockers\\n* PATCH version changes are for critical bug-fixes that cannot wait for the next Sprint.\\n* (LETTER-SUFFIX) reflects builds for our additional purposes that are detailed in following sections.\\n### Release\\nAs of 3.8, public releases have no suffix, and are released using the staged rollout capability of the Amazon Developer portal.\\n### Live App Testing (-LAT1)\\nAs part of our early testing, we create Live App Test (LAT) builds to send out candidate builds to our early testing groups before a release.\\nThese have a `-LAT1` suffix, where the number is incremented per test build sent out per version. For example, the second test build for 3.5 would be `3.5-LAT2`.\\nThis is first used in `3.3.0-LAT1`. These are used for testing, not general release.\\n#### Deprecated LAT versioning\\nPreviously, the versioning was much more confusing. We wanted to preserve monotonic order versioning, so a LAT would have an additional number appended at the end of the *previous* version; for example, the second LAT testing the 3.2 release would be versioned `3.1.3.2`, because the last released version before `3.2` was `3.1.3`.\\nThis deprecated LAT versioning was used between `2.1.0.1` and `3.1.3.2`.\\n### GeckoView (-GV)\\nCurrently, there are two distinct web engines that Firefox for Fire TV can be build with: the system WebView or GeckoView. Since a build currently can only use one of these, when we make a build that uses the GeckoView engine, we need a separate suffix to differentiate it.\\nThese GeckoView builds are suffixed with `-GV`.\\nThis is first used in `3.4-GV`, but is used for testing and not released to the general population.\\n### Re-Release (-A)\\nThere are two cases for re-release:\\n1) Rollback to a previous version due to critical bugs (e.g. rollback of 3.4 should be 3.3-A, although this is untested, and the platform may not allow decremented versioning, in which case, we would release the rollback as 3.4-A)\\n1) (deprecated) Release of a tested \"staged rollout\" build to the rest of the devices. (This is no longer used because staged rollout capability has been added to the app store.) This was monotonic because the \"general population\" devices had not been upgraded past this version.\\nThis is a build that has already been released before, either to a portion of the population, or because in a subsequent release we needed to do a version bump in order to push out a release, but did not change the code.\\nThese re-release builds are suffixed with a letter starting with `-A`, which is incremented with each re-release.\\nThis is first used in `3.4-A`.\\n#### Deprecated re-release versioning\\nBefore we started using letters to signify re-releases, we simply bumped the version number, so there are several versions that are simply re-releases of previous versions, but with different version numbers. These are listed below:\\n**3.1.3** is the same version as:\\n* 3.2.5\\n* 3.3\\n* 3.4-A\\n* 3.4-B\\n(and also listed in the changelog)\\n### Deprecated Split-Device Staged Rollout (-RO)\\nThese split-device staged rollout releases were suffixed with a `-RO`, e.g. `3.7-RO`.\\nBefore the staged rollout capability was added to the Amazon Developer portal, we handled staged rollout by releasing the newest version only to a single device, in this case the Firefox TV 4K Pendant (AFTN).\\nThis versioning scheme was used between `3.5-RO` and `3.7-RO`.\\n### Status: Accepted\\n","MatchedIds":[2951,5177,1048,3171,4424],"Predicted":"## Decision\n1. We will use Semantic Versioning 2.0 for all Firefox builds to ensure clarity around version increments corresponding to new features, bug fixes, and breaking changes. The version format will be `major.minor.patch`.\n2. The Release versions will be incremented as follows:\n   - `Major` version will change when there are significant updates or breaking changes.\n   - `Minor` version will change when new features are added in a backward-compatible manner.\n   - `Patch` version will change for backward-compatible bug fixes only.\n3. Beta versions will append `-beta` followed by an incremental number to the main version. For example, a first beta release for version `3.9` would be `3.9.0-beta.1`.\n4. Staged rollout versions meant for testing specific environments will append `-alpha` followed by an incremental number. For example, an alpha release for version `3.9` would be `3.9.0-alpha.1`.\n5. All versions, including rollbacks, will be documented within the changelog. Rollback versions will be appended with `-rollback` followed by a unique identifier to prevent confusion, for example, `3.9.1-rollback.1`.\n6. For each release - whether alpha, beta, or final - we will ensure that the changes are comprehensively documented within the changelog, including the rationale for rollbacks.\n7. For visibility and tracking, all released assets, including LATs, will be tagged and listed on the [https:\/\/github.com\/mozilla-mobile\/firefox-tv\/releases](release page).\n\nThis strategy will provide clearer guidelines for determining build lineage, ensuring stakeholders and users can track the changes effectively.","GenTime":"2024-08-20 00:19:31"}
{"File Name":"rtk-companion\/0003-mvp-functionality-in-add-card-window.md","Context":"## Context and Problem Statement\\nAfter experimenting with the previous MVP goal defined by [ADR-0002](0002-define-goal-for-an-mvp.md), I've decided it's not a good approach.\\nMy main issue with is that in a way, it is re-creating Anki's \"Add Card\" screen, albeit a custom one which adds RTK cards.\\nHere's a screenshot of the UI created and filled with placeholder text.\\n![](..\/assets\/rtk-browser-screenshot.png)\\nWhile I still think it'd be cool to have an RTK browser in Anki and working with QT is ....interesting, I thought\\na simpler approach should be sought.\\nSo the problem stands, what should be done as an MVP and what should it contain?\\n","Decision":"After some experimenting and research, I've decided an MVP should augment the `Add Card`.\\nHere's the vision of how a user interacts with the MVP.\\n1. User opens the `Add Card` window which has a \"RTK\" button, and a \"Stories\" button.\\n1. User types in their desired Heisig Kanji number is the \"Number\" field.\\n1. User clicks the RTK button and the character, and stroke order get auto filled.\\n1. User clicks \"Stories\" button, which opens an embedded browser window to user stories.\\n### Positive Consequences <!-- optional -->\\n- The approach is simpler.\\n- We keep Building UI elements with `QT` to a minimum under this approach.\\n### Negative Consequences <!-- optional -->\\n- We leave out Additional keyword meanings and the radicals for the Kanji\\n","tokens":161,"id":4402,"text":"## Context and Problem Statement\\nAfter experimenting with the previous MVP goal defined by [ADR-0002](0002-define-goal-for-an-mvp.md), I've decided it's not a good approach.\\nMy main issue with is that in a way, it is re-creating Anki's \"Add Card\" screen, albeit a custom one which adds RTK cards.\\nHere's a screenshot of the UI created and filled with placeholder text.\\n![](..\/assets\/rtk-browser-screenshot.png)\\nWhile I still think it'd be cool to have an RTK browser in Anki and working with QT is ....interesting, I thought\\na simpler approach should be sought.\\nSo the problem stands, what should be done as an MVP and what should it contain?\\n\n\n##Decision\nAfter some experimenting and research, I've decided an MVP should augment the `Add Card`.\\nHere's the vision of how a user interacts with the MVP.\\n1. User opens the `Add Card` window which has a \"RTK\" button, and a \"Stories\" button.\\n1. User types in their desired Heisig Kanji number is the \"Number\" field.\\n1. User clicks the RTK button and the character, and stroke order get auto filled.\\n1. User clicks \"Stories\" button, which opens an embedded browser window to user stories.\\n### Positive Consequences <!-- optional -->\\n- The approach is simpler.\\n- We keep Building UI elements with `QT` to a minimum under this approach.\\n### Negative Consequences <!-- optional -->\\n- We leave out Additional keyword meanings and the radicals for the Kanji\\n","MatchedIds":[1067,4167,5244,160,3187],"Predicted":"## Decision\nChosen option: Simplify the MVP to focus on core functional improvements rather than recreating existing features from Anki.\n\n### Explanation\nRecreating the \"Add Card\" screen in Anki leads to redundant functionality. Instead, the MVP should target providing unique value propositions while keeping the implementation straightforward and achievable. This could include:\n\n1. **RTK Integration Plugin**: Develop a plugin that seamlessly integrates RTK cards into Anki without creating a new UI.\n2. **Card Generation Automation**: Automate the generation of RTK cards based on user input or predefined criteria.\n3. **Enhanced Tagging and Search**: Improve the tagging and search capabilities specifically tailored for RTK cards.\n4. **User Feedback Mechanism**: Include a simple mechanism for users to provide feedback and suggestions on the RTK integration within Anki.\n\n### Positive Consequences\n* Streamlined Development: Focused effort on core improvements will lead to faster and more efficient development.\n* Unique Value Addition: Instead of duplicating existing features, the MVP adds unique value to users, enhancing their Anki experience.\n* User-Centric Approach: By seeking user feedback early on, the product can evolve based on actual user needs and preferences.\n\n### Negative Consequences\n* Missing Custom UI: Some users might miss a specialized custom UI for RTK cards.\n* Limited Scope Initially: The initial version will have a narrower scope of features compared to the re-created \"Add Card\" screen approach.\n\n### Implementation Steps\n1. Outline the key functionalities that the simplified MVP will include.\n2. Develop the RTK integration plugin with a focus on automation and enhanced tagging\/search.\n3. Test the integration thoroughly with a small user group.\n4. Collect feedback and iterate the features based on user input.\n5. Plan additional features and improvements for subsequent releases based on user needs and feedback.","GenTime":"2024-08-20 00:19:39"}
{"File Name":"macondo\/0007-automatic-aligning-of-user-host-user-for-better-interoperability.md","Context":"## Context\\nA well constructed macondo command should be easy to run directly (without\\nmacondo) by just executing it. This is hard to achieve unless the user in the\\ncontainer resembles as much as possible the host running it.\\nOne case where this is important is modifying files in the host. This is\\nachieved by mounting a volume into the running container. However, the files\\nwritten from the docker container are owned by the docker user, i.e. the user id\\nand group id of the file is that of whatever docker user happened to write it.\\nSo if the container runs with the root user, which is unfortunately common, then\\nthe files written to the host also are owned by root, making them innacessible\\nto the host user.\\nPotential ideas:\\n- A flag that enables user alignment and mounts HOME into docker's HOME.\\n- Customize home when running in OSX to be \/Users\/bla instead of \/home\/ble\\n- Even if mounting the whole home, it should be easy to mount current PWD into\\nsomething else \/mnt\/blablabla and use tha as working directory\\n","Decision":"Add a mechanism to align the user\/group of the host system with that of the docker container.\\n","tokens":237,"id":2193,"text":"## Context\\nA well constructed macondo command should be easy to run directly (without\\nmacondo) by just executing it. This is hard to achieve unless the user in the\\ncontainer resembles as much as possible the host running it.\\nOne case where this is important is modifying files in the host. This is\\nachieved by mounting a volume into the running container. However, the files\\nwritten from the docker container are owned by the docker user, i.e. the user id\\nand group id of the file is that of whatever docker user happened to write it.\\nSo if the container runs with the root user, which is unfortunately common, then\\nthe files written to the host also are owned by root, making them innacessible\\nto the host user.\\nPotential ideas:\\n- A flag that enables user alignment and mounts HOME into docker's HOME.\\n- Customize home when running in OSX to be \/Users\/bla instead of \/home\/ble\\n- Even if mounting the whole home, it should be easy to mount current PWD into\\nsomething else \/mnt\/blablabla and use tha as working directory\\n\n\n##Decision\nAdd a mechanism to align the user\/group of the host system with that of the docker container.\\n","MatchedIds":[3604,4096,1666,2968,3915],"Predicted":"## Decision\nChosen option: Implement a flag that enables user alignment and mounts HOME into docker's HOME, because:\n\n* This option provides the most flexibility and aligns user and groups IDs between the host and the container, minimizing permission issues.\n* It allows the user's home directory to be mounted inside the container which ensures a more seamless experience when running commands directly.\n* This solution is more platform-agnostic and can be implemented consistently across different systems like Linux and OSX.\n* Additional customizations, such as adjusting the home directory path on OSX or mounting the current working directory to a specific path, can be layered on top of this approach, providing further flexibility without complicating the initial implementation.","GenTime":"2024-08-20 00:19:42"}
{"File Name":"scholarsphere\/0008-asset-pipeline.md","Context":"## Context\\nWe were unable to address a security vulnerability in jQuery because we couldn't update Bootstrap. This was due to the\\nfact that it was present as both a gem and an npm package.\\n","Decision":"We removed asset pipeline completely and moved all css and image assets to webpacker. This allowed us to update jQuery\\nvia yarn.\\n","tokens":41,"id":4557,"text":"## Context\\nWe were unable to address a security vulnerability in jQuery because we couldn't update Bootstrap. This was due to the\\nfact that it was present as both a gem and an npm package.\\n\n\n##Decision\nWe removed asset pipeline completely and moved all css and image assets to webpacker. This allowed us to update jQuery\\nvia yarn.\\n","MatchedIds":[3841,3806,3328,4225,3827],"Predicted":"## Decision\nMigrate Bootstrap to be managed solely through npm instead of both a gem and an npm package. This will streamline our dependency management, making it easier to update Bootstrap and address any associated security vulnerabilities. Remove the Bootstrap gem from the project and ensure all references are updated to use the npm package.","GenTime":"2024-08-20 00:19:44"}
{"File Name":"operational-data-hub\/0057-naming-convention-defines-uri-tags.md","Context":"## Context\\nA Uniform Resource Identifier (URI) is a string of characters that unambiguously identifies a particular resource. To guarantee uniformity, all URIs follow a predefined set of syntax rules, but also maintain extensibility through a separately defined hierarchical naming scheme\\n","Decision":"We will use [the 'tag' URI Scheme](https:\/\/tools.ietf.org\/html\/rfc4151).\\n","tokens":55,"id":2763,"text":"## Context\\nA Uniform Resource Identifier (URI) is a string of characters that unambiguously identifies a particular resource. To guarantee uniformity, all URIs follow a predefined set of syntax rules, but also maintain extensibility through a separately defined hierarchical naming scheme\\n\n\n##Decision\nWe will use [the 'tag' URI Scheme](https:\/\/tools.ietf.org\/html\/rfc4151).\\n","MatchedIds":[2740,2963,2181,2270,2757],"Predicted":"## Decision\nOur URI structure will be based on the [RFC 3986](https:\/\/tools.ietf.org\/html\/rfc3986) URL specification. The URIs will follow a hierarchical structure to ensure unambiguous identification and extended accessibility.\n\n### URI Structure:\n```\nscheme:[\/\/authority]path[?query][#fragment]\n```\n- **scheme:** A string that defines the protocol (e.g., `http`, `https`, `ftp`).\n- **authority:** An optional string that identifies the authority governing the namespace (e.g., `user@host:port`).\n- **path:** A string of characters that specifies the hierarchical structure leading to the resource.\n- **query:** An optional string used to provide additional parameters.\n- **fragment:** An optional string for identifying a sub-resource or a part of the resource.\n\n### Example:\n`https:\/\/example.com\/mycompany\/catalog\/articles\/313?version=2#section`\n\nThis decision ensures that our URIs are uniform, human-readable, and hierarchically structured to easily identify and access resources in a consistent manner. All URIs within our system will adhere to this format to maintain extensibility and uniqueness.","GenTime":"2024-08-20 00:19:49"}
{"File Name":"architecture\/0013-home-assistant-container.md","Context":"## Context\\nDefine a supported installation method as per [ADR-0012](https:\/\/github.com\/home-assistant\/architecture\/blob\/master\/adr\/0012-define-supported-installation-method.md).\\n","Decision":"This is for running just the Home Assistant Core application on native OCI compatible containerization system. It does not provide the Supervisor experience, and thus does not provide the Supervisor panel and add-ons.\\nThis is a general installation method that is recommended as an alternative to the Home Assistant OS installation method. Due to the shared image with the Home Assistant OS installation method, almost all documentation applies to the Home Assistant Container as well.\\nThe only supported way to run the container is on the host network as root with full privileges.\\n### Supported Containerization system and version\\n- Any Open Container Initiative (OCI) compatible containerization system.\\n### Supported boards\/hardware\/machines\\n- Machines of the following architectures: amd64, i386, armhf, aarch64, armv7\\n### Supported Operating Systems and versions\\n- Running Home Assistant Container is only supported on Linux.\\n- Windows and BSD installations (e.g., macOS and FreeBSD) are not supported.\\n### Additional notes\\nThere is a wide variety of containerization software available. From that perspective, Home Assistant will only actively document the use of Docker.\\n### Required Expertise\\n- **Installation**\\nThis requires the user to have an existing system that can run Docker containers. Installation is either done by running a command from the Docker-cli or via a user interface (Synology, Portainer)\\n* **Start when the system is started:** The user is responsible for configuring the system to start the container when the system is started.\\n* **Run with full network access:** Default installation instructions prescribe net=host to be configured.\\n* **Access USB devices:** It is up to the user to ensure that all devices are correctly passed through to the container.\\n* **Maintaining the Home Assistant installation**\\nIf using the Docker-cli the user needs to manually update the run command. If using a UI the user might be notified of an upgrade or automatically update \u2013 automatically applying updates may result in the system not coming back online. There is no rollback in case the instance does not come online after an update.\\n- **Python upgrades:** Included in the Home Assistant container\\n- **Installing Python dependencies:** Included in the Home Assistant container\\n- **Updating Home Assistant:** Included in the Home Assistant container\\n- **Maintaining the Operating System**\\nSince this is just the core container, all OS responsibilities are with the user.\\n- **Security updates for OS:** Responsibility of the user.\\n- **Maintaining the components required for the Supervisor:** No supervisor, so N\/A\\n**Conclusion:** medium expertise required. Some Docker UIs make it easy to run and update containers. Mapping devices and manually updating Home Assistant will be challenging as they depend per platform.\\n","tokens":41,"id":1430,"text":"## Context\\nDefine a supported installation method as per [ADR-0012](https:\/\/github.com\/home-assistant\/architecture\/blob\/master\/adr\/0012-define-supported-installation-method.md).\\n\n\n##Decision\nThis is for running just the Home Assistant Core application on native OCI compatible containerization system. It does not provide the Supervisor experience, and thus does not provide the Supervisor panel and add-ons.\\nThis is a general installation method that is recommended as an alternative to the Home Assistant OS installation method. Due to the shared image with the Home Assistant OS installation method, almost all documentation applies to the Home Assistant Container as well.\\nThe only supported way to run the container is on the host network as root with full privileges.\\n### Supported Containerization system and version\\n- Any Open Container Initiative (OCI) compatible containerization system.\\n### Supported boards\/hardware\/machines\\n- Machines of the following architectures: amd64, i386, armhf, aarch64, armv7\\n### Supported Operating Systems and versions\\n- Running Home Assistant Container is only supported on Linux.\\n- Windows and BSD installations (e.g., macOS and FreeBSD) are not supported.\\n### Additional notes\\nThere is a wide variety of containerization software available. From that perspective, Home Assistant will only actively document the use of Docker.\\n### Required Expertise\\n- **Installation**\\nThis requires the user to have an existing system that can run Docker containers. Installation is either done by running a command from the Docker-cli or via a user interface (Synology, Portainer)\\n* **Start when the system is started:** The user is responsible for configuring the system to start the container when the system is started.\\n* **Run with full network access:** Default installation instructions prescribe net=host to be configured.\\n* **Access USB devices:** It is up to the user to ensure that all devices are correctly passed through to the container.\\n* **Maintaining the Home Assistant installation**\\nIf using the Docker-cli the user needs to manually update the run command. If using a UI the user might be notified of an upgrade or automatically update \u2013 automatically applying updates may result in the system not coming back online. There is no rollback in case the instance does not come online after an update.\\n- **Python upgrades:** Included in the Home Assistant container\\n- **Installing Python dependencies:** Included in the Home Assistant container\\n- **Updating Home Assistant:** Included in the Home Assistant container\\n- **Maintaining the Operating System**\\nSince this is just the core container, all OS responsibilities are with the user.\\n- **Security updates for OS:** Responsibility of the user.\\n- **Maintaining the components required for the Supervisor:** No supervisor, so N\/A\\n**Conclusion:** medium expertise required. Some Docker UIs make it easy to run and update containers. Mapping devices and manually updating Home Assistant will be challenging as they depend per platform.\\n","MatchedIds":[1545,1048,1239,2502,1196],"Predicted":"## Decision\nSupported installation methods should align with the guidelines specified in ADR-0012. Specifically, we will support:\n\n1. **Home Assistant Operating System (HAOS)**: The recommended installation method for a fully managed experience.\n2. **Home Assistant Supervised**: For advanced users who want the full Home Assistant experience while retaining their own operating system.\n3. **Home Assistant Container**: For users who want to manage their own Docker environment.\n4. **Home Assistant Core**: For advanced users who prefer a minimal installation with just the core application, without the additional managed supervisor features.\n\nUnsupported methods will include any installation variations outside these officially documented methods to ensure consistency, reliability, and maintainability of our platform.","GenTime":"2024-08-20 00:19:52"}
{"File Name":"digitalrig-metal-aws\/0003-use-aws-codepipeline-and-codebuild-instead-of-travis.md","Context":"## Context\\nTravis has some limitations about what stages you can use to create a pipleine.  We still desire to have a hosted\/PaaS CI\/CD solution\\n","Decision":"* Use AWS CodePipeline and CodeBuild instead of Travis\\n* We will aim to create a new Pipeline\/Build and potentially execution environment per branch.\\n* This will be manual at first and later could be automated via webhooks and lambda functions\\n","tokens":36,"id":1741,"text":"## Context\\nTravis has some limitations about what stages you can use to create a pipleine.  We still desire to have a hosted\/PaaS CI\/CD solution\\n\n\n##Decision\n* Use AWS CodePipeline and CodeBuild instead of Travis\\n* We will aim to create a new Pipeline\/Build and potentially execution environment per branch.\\n* This will be manual at first and later could be automated via webhooks and lambda functions\\n","MatchedIds":[2393,351,1739,2471,187],"Predicted":"## Decision\nSwitch to using CircleCI as our hosted\/PaaS CI\/CD solution. CircleCI provides more flexibility in creating complex pipelines with multiple stages. Configure CircleCI with the necessary build and deploy stages to meet our requirements. Additionally, ensure all necessary environment variables and secrets are securely stored and accessible within CircleCI.","GenTime":"2024-08-20 00:19:56"}
{"File Name":"biosamples-v4\/0004-inverse-relationships-sample-update.md","Context":"## Context\\nWhen a relation is created between two samples, the sample target of the relationship doesn't change the update date.\\nStated in another way, the relationship inversion process doesn't change the update date of the sample.\\nThis is associated with BSD-1088 - https:\/\/www.ebi.ac.uk\/panda\/jira\/browse\/BSD-1088\\n","Decision":"We decided that is good for the relationship inversion process to not change the update date of the sample as we don't\\nhave any actual usecase for this to happen and it would also create issues for NCBI as they don't care about relationships\\nand no real information is added to the sample\\n","tokens":74,"id":4726,"text":"## Context\\nWhen a relation is created between two samples, the sample target of the relationship doesn't change the update date.\\nStated in another way, the relationship inversion process doesn't change the update date of the sample.\\nThis is associated with BSD-1088 - https:\/\/www.ebi.ac.uk\/panda\/jira\/browse\/BSD-1088\\n\n\n##Decision\nWe decided that is good for the relationship inversion process to not change the update date of the sample as we don't\\nhave any actual usecase for this to happen and it would also create issues for NCBI as they don't care about relationships\\nand no real information is added to the sample\\n","MatchedIds":[2829,317,2215,1606,3127],"Predicted":"## Decision\nTo maintain the integrity of the update timestamps and avoid unnecessary changes to the update dates, the creation of a relationship between two samples will not trigger an update to the `update_date` field of the target sample. This decision ensures that the `update_date` field accurately reflects substantial changes to the sample itself rather than metadata operations such as relationship creations or inversions. This behavior aligns with the requirements stated in BSD-1088.","GenTime":"2024-08-20 00:19:59"}
{"File Name":"octagon\/0002-components-shall-be-stateless-by-default.md","Context":"## Context\\nOctagon strives to be a view layer for consumers.  Applications manage their state via their own independent mechanisms.  State embedded within Octagon components may reduce the declaritive ability of said components & violate the top-down, _\"config via props\"_ React mantra.  Consequently, stateful components generally deviate from the pure view layer goals we wish to provide.  Embedded state reduces the ablitity for apps to \"declare\" Octagon components into view state.\\nExample, consider if a `<OctoNavItem \/>` maintains state on whether or not that navigation page is active or not.  If a user reloaded the page, an active item may suddenly show as not active, because that component's state is lost on reload.\\nInstead, the API should be stateless, and be used like: `<OctoNavItem active \/>`.\\nNow, the user of the view layer is in control.\\n","Decision":"Make things stateless.  In rare cases, some emphemeral state is OK to maintain internally to components.  These should be highlighted, drawn out, and discussed before merge.  Ideally, these issues should be raised during design, pre-merge if a feature branch is longer running.\\n","tokens":189,"id":4596,"text":"## Context\\nOctagon strives to be a view layer for consumers.  Applications manage their state via their own independent mechanisms.  State embedded within Octagon components may reduce the declaritive ability of said components & violate the top-down, _\"config via props\"_ React mantra.  Consequently, stateful components generally deviate from the pure view layer goals we wish to provide.  Embedded state reduces the ablitity for apps to \"declare\" Octagon components into view state.\\nExample, consider if a `<OctoNavItem \/>` maintains state on whether or not that navigation page is active or not.  If a user reloaded the page, an active item may suddenly show as not active, because that component's state is lost on reload.\\nInstead, the API should be stateless, and be used like: `<OctoNavItem active \/>`.\\nNow, the user of the view layer is in control.\\n\n\n##Decision\nMake things stateless.  In rare cases, some emphemeral state is OK to maintain internally to components.  These should be highlighted, drawn out, and discussed before merge.  Ideally, these issues should be raised during design, pre-merge if a feature branch is longer running.\\n","MatchedIds":[1854,2298,2667,2816,1151],"Predicted":"## Decision\n- We will ensure that all Octagon components are stateless, focusing purely on being a declarative view layer.\n- Any required state should be managed externally by the consuming application, and passed down to Octagon components via props.\n- Components will expose necessary configuration options through props, enabling the consuming application to control their behavior and state.\n- This approach will improve predictability and reusability, ensuring that the state is consistent and maintainable, even across page reloads or other lifecycle events.\n- We will document this convention clearly to guide developers on using Octagon components in a stateless manner.","GenTime":"2024-08-20 00:20:03"}
{"File Name":"infection\/0003-PHPUnit-this-over-self.md","Context":"### Context\\nPHPUnit assertions are static methods, yet in our code base we call them with `$this` instead of\\n`self`.\\nWhilst \"incorrect\", this usage does not break anything. Besides:\\n- [PHUnit documentation][phpunit-doc] itself uses this by default\\n- `$this` is much more widely used than `self` in this context in the community\\n- all Infection code uses `$this`\\nThere is not much shortcomings from using this other than the \"incorrectness\" of using a static\\nmethod as a non-static one.\\n### Decision\\nSince there is no clear benefits of adopting `self` over `$this` and given the context of its usage,\\nthe decision is to keep the usage of `$this` over `self` in the codebase.\\n### Status\\nAccepted ([#1061][1061])\\n[phpunit-doc]: https:\/\/phpunit.de\/manual\/6.5\/en\/appendixes.assertions.html\\n[1061]: https:\/\/github.com\/infection\/infection\/pull\/1061\\n","Decision":"Since there is no clear benefits of adopting `self` over `$this` and given the context of its usage,\\nthe decision is to keep the usage of `$this` over `self` in the codebase.\\n### Status\\nAccepted ([#1061][1061])\\n[phpunit-doc]: https:\/\/phpunit.de\/manual\/6.5\/en\/appendixes.assertions.html\\n[1061]: https:\/\/github.com\/infection\/infection\/pull\/1061\\n","tokens":221,"id":66,"text":"### Context\\nPHPUnit assertions are static methods, yet in our code base we call them with `$this` instead of\\n`self`.\\nWhilst \"incorrect\", this usage does not break anything. Besides:\\n- [PHUnit documentation][phpunit-doc] itself uses this by default\\n- `$this` is much more widely used than `self` in this context in the community\\n- all Infection code uses `$this`\\nThere is not much shortcomings from using this other than the \"incorrectness\" of using a static\\nmethod as a non-static one.\\n### Decision\\nSince there is no clear benefits of adopting `self` over `$this` and given the context of its usage,\\nthe decision is to keep the usage of `$this` over `self` in the codebase.\\n### Status\\nAccepted ([#1061][1061])\\n[phpunit-doc]: https:\/\/phpunit.de\/manual\/6.5\/en\/appendixes.assertions.html\\n[1061]: https:\/\/github.com\/infection\/infection\/pull\/1061\\n\n\n##Decision\nSince there is no clear benefits of adopting `self` over `$this` and given the context of its usage,\\nthe decision is to keep the usage of `$this` over `self` in the codebase.\\n### Status\\nAccepted ([#1061][1061])\\n[phpunit-doc]: https:\/\/phpunit.de\/manual\/6.5\/en\/appendixes.assertions.html\\n[1061]: https:\/\/github.com\/infection\/infection\/pull\/1061\\n","MatchedIds":[68,3853,1339,4179,1258],"Predicted":"### Decision\nSince there is no clear benefit of adopting `self` over `$this` and given that the broader community and PHPUnit documentation itself use `$this` extensively, the decision is to continue using `$this` for PHPUnit assertions in the codebase. This maintains consistency with existing code and prevalent practices.\n\n### Status\nAccepted ([#1061][1061])\n\n[phpunit-doc]: https:\/\/phpunit.de\/manual\/6.5\/en\/appendixes.assertions.html\n[1061]: https:\/\/github.com\/infection\/infection\/pull\/1061","GenTime":"2024-08-20 00:20:06"}
{"File Name":"arch\/0019-server-request-and-upgrade-capacity-evaluation.md","Context":"## Context\\n1. \u90e8\u5206\u673a\u5668\u7684 CPU\uff0c\u5185\u5b58\uff0c\u786c\u76d8\uff0c\u4f7f\u7528\u7387\u5747\u5728 90% \u5de6\u53f3\uff0c\u53e6\u4e00\u4e9b\u673a\u5668\u5404\u9879\u6307\u6807\u4f7f\u7528\u7387\u5728 1% \u5de6\u53f3\uff1b\\n2. \u90e8\u5206\u673a\u5668\u7684 CPU\uff0c\u5185\u5b58\uff0c\u786c\u76d8\u642d\u914d\u4e0d\u5408\u7406\uff0cCPU \u4f7f\u7528\u7387 1%\uff0c\u4f46\u5185\u5b58\u4f7f\u7528\u7387\u5728 90% \u5de6\u53f3\uff1b\\n3. \u4e00\u4e9b\u5bf9\u78c1\u76d8\u8bfb\u5199\u8981\u6c42\u9ad8\u7684\u670d\u52a1\uff0c\u4f7f\u7528\u7684\u662f\u666e\u901a\u4e91\u76d8\uff0c\u6bd4\u5982\uff0c\u6570\u636e\u5e93\uff0cSVN\u7b49\uff1b\\n4. \u7533\u8bf7\u673a\u5668\u65f6\uff0c\u65e0\u6cd5\u63d0\u51fa\u914d\u7f6e\u8981\u6c42\uff0c\u57fa\u672c\u9760\u62cd\u8111\u95e8\u51b3\u5b9a\uff1b\\n5. \u5bf9\u670d\u52a1\u7684\u53d1\u5c55\u6ca1\u6709\u601d\u8003\uff0c\u914d\u7f6e\u8981\u4e86 12 \u4e2a\u6708\u540e\u624d\u80fd\u4f7f\u7528\u5230\u7684\u914d\u7f6e\u3002\\n","Decision":"1. \u538b\u529b\u6d4b\u8bd5\uff1b\\n2. \u5206\u6790\u4e1a\u52a1\u5404\u4e2a\u6307\u6807\u7684\u4f7f\u7528\u60c5\u51b5\uff0cCPU \u5bc6\u96c6\u578b\uff0c\u5185\u5b58\u5bc6\u96c6\u578b\u8fd8\u662f\u6709\u5176\u4ed6\u7684\u7279\u70b9\uff1b\\n3. \u9274\u4e8e Aliyun ECS \u968f\u65f6\u53ef\u4ee5\u6269\u5c55\uff0c\u53ef\u4ee5\u5148\u7528\u4f4e\u914d\u673a\u5668\uff0c\u6839\u636e\u4f7f\u7528\u60c5\u51b5\uff0c\u8fdb\u884c\u5355\u6307\u6807\u5782\u76f4\u5347\u7ea7\uff1b\\n4. \u6c34\u5e73\u6269\u5c55\uff0c\u5373\u63d0\u5347\u4e86\u670d\u52a1\u7684\u5904\u7406\u80fd\u529b\u53c8\u505a\u4e86\u9ad8\u53ef\u7528\uff1b\\n5. \u5bf9\u4e8e\u4e0d\u5408\u7406\u7684\u5185\u5b58\u4f7f\u7528\uff0c\u8981\u5206\u6790\u81ea\u5df1\u7a0b\u5e8f\u4e2d\u662f\u5426\u6709\u5185\u5b58\u6cc4\u6f0f\u6216\u5927\u6570\u636e\u52a0\u8f7d\u3002\\n","tokens":208,"id":2440,"text":"## Context\\n1. \u90e8\u5206\u673a\u5668\u7684 CPU\uff0c\u5185\u5b58\uff0c\u786c\u76d8\uff0c\u4f7f\u7528\u7387\u5747\u5728 90% \u5de6\u53f3\uff0c\u53e6\u4e00\u4e9b\u673a\u5668\u5404\u9879\u6307\u6807\u4f7f\u7528\u7387\u5728 1% \u5de6\u53f3\uff1b\\n2. \u90e8\u5206\u673a\u5668\u7684 CPU\uff0c\u5185\u5b58\uff0c\u786c\u76d8\u642d\u914d\u4e0d\u5408\u7406\uff0cCPU \u4f7f\u7528\u7387 1%\uff0c\u4f46\u5185\u5b58\u4f7f\u7528\u7387\u5728 90% \u5de6\u53f3\uff1b\\n3. \u4e00\u4e9b\u5bf9\u78c1\u76d8\u8bfb\u5199\u8981\u6c42\u9ad8\u7684\u670d\u52a1\uff0c\u4f7f\u7528\u7684\u662f\u666e\u901a\u4e91\u76d8\uff0c\u6bd4\u5982\uff0c\u6570\u636e\u5e93\uff0cSVN\u7b49\uff1b\\n4. \u7533\u8bf7\u673a\u5668\u65f6\uff0c\u65e0\u6cd5\u63d0\u51fa\u914d\u7f6e\u8981\u6c42\uff0c\u57fa\u672c\u9760\u62cd\u8111\u95e8\u51b3\u5b9a\uff1b\\n5. \u5bf9\u670d\u52a1\u7684\u53d1\u5c55\u6ca1\u6709\u601d\u8003\uff0c\u914d\u7f6e\u8981\u4e86 12 \u4e2a\u6708\u540e\u624d\u80fd\u4f7f\u7528\u5230\u7684\u914d\u7f6e\u3002\\n\n\n##Decision\n1. \u538b\u529b\u6d4b\u8bd5\uff1b\\n2. \u5206\u6790\u4e1a\u52a1\u5404\u4e2a\u6307\u6807\u7684\u4f7f\u7528\u60c5\u51b5\uff0cCPU \u5bc6\u96c6\u578b\uff0c\u5185\u5b58\u5bc6\u96c6\u578b\u8fd8\u662f\u6709\u5176\u4ed6\u7684\u7279\u70b9\uff1b\\n3. \u9274\u4e8e Aliyun ECS \u968f\u65f6\u53ef\u4ee5\u6269\u5c55\uff0c\u53ef\u4ee5\u5148\u7528\u4f4e\u914d\u673a\u5668\uff0c\u6839\u636e\u4f7f\u7528\u60c5\u51b5\uff0c\u8fdb\u884c\u5355\u6307\u6807\u5782\u76f4\u5347\u7ea7\uff1b\\n4. \u6c34\u5e73\u6269\u5c55\uff0c\u5373\u63d0\u5347\u4e86\u670d\u52a1\u7684\u5904\u7406\u80fd\u529b\u53c8\u505a\u4e86\u9ad8\u53ef\u7528\uff1b\\n5. \u5bf9\u4e8e\u4e0d\u5408\u7406\u7684\u5185\u5b58\u4f7f\u7528\uff0c\u8981\u5206\u6790\u81ea\u5df1\u7a0b\u5e8f\u4e2d\u662f\u5426\u6709\u5185\u5b58\u6cc4\u6f0f\u6216\u5927\u6570\u636e\u52a0\u8f7d\u3002\\n","MatchedIds":[2459,2446,215,200,923],"Predicted":"## Decision\n\n### Immediate Actions:\n1. **Resource Reallocation**:\n   - Redistribute workloads to balance CPU, memory, and disk usage across machines.\n   - Implement resource monitoring tools to dynamically allocate resources based on real-time usage.\n\n2. **Optimize Resource Allocation**:\n   - Identify and migrate resource-intensive services, such as databases and SVN, to SSD-backed cloud storage for better performance.\n   - Evaluate and adjust each machine's CPU and memory allocation to ensure optimal usage, relieving any mismatched usage scenarios.\n\n### Mid-term Strategies:\n3. **Infrastructure Upgrade**:\n   - Move towards a scalable and flexible cloud infrastructure where resource configurations can be modified as needed. Consider auto-scaling groups and container orchestration systems such as Kubernetes to manage resource allocation effectively.\n   - Use Infrastructure as Code (IaC) tools like Terraform or CloudFormation to standardize and automate infrastructure provisioning.\n\n4. **Service Development Planning**:\n   - Draft detailed capacity planning for services. Develop a projection model based on historical usage data and expected growth patterns to ensure future resource requirements are adequately met.\n   - Establish a review mechanism for resource requests to be based on data-driven insights rather than arbitrary decisions.\n\n### Long-term Considerations:\n5. **Refine Configuration Request Process**:\n   - Develop and implement a standardized process for requesting machine configurations. This should include predefined configurations catering to different service needs, ensuring a more effective and responsive allocation strategy.\n   - Train relevant teams on best practices for resource allocation and capacity forecasting to reduce inefficiencies.\n\n6. **Continuous Monitoring and Optimization**:\n   - Implement continuous integration and continuous deployment (CI\/CD) pipelines integrated with monitoring tools to automatically adjust resources as per the evolving service demands.\n   - Regularly review and audit resource usage and allocation strategies to adapt to changing service and operational requirements.\n\nBy redistributing current workloads, optimizing resource allocation, and moving to a flexible and scalable infrastructure, the current inefficiencies can be addressed effectively. Also, instituting a structured configuration request process and better planning for service development will ensure the long-term sustainability and efficiency of the infrastructure.","GenTime":"2024-08-20 00:20:15"}
{"File Name":"celestia-core\/adr-024-sign-bytes.md","Context":"## Context\\nCurrently, the messages exchanged between tendermint and a (potentially remote) signer\/validator,\\nnamely votes, proposals, and heartbeats, are encoded as a JSON string\\n(e.g., via `Vote.SignBytes(...)`) and then\\nsigned . JSON encoding is sub-optimal for both, hardware wallets\\nand for usage in ethereum smart contracts. Both is laid down in detail in [issue#1622].\\nAlso, there are currently no differences between sign-request and -replies. Also, there is no possibility\\nfor a remote signer to include an error code or message in case something went wrong.\\nThe messages exchanged between tendermint and a remote signer currently live in\\n[privval\/socket.go] and encapsulate the corresponding types in [types].\\n[privval\/socket.go]: https:\/\/github.com\/tendermint\/tendermint\/blob\/d419fffe18531317c28c29a292ad7d253f6cafdf\/privval\/socket.go#L496-L502\\n[issue#1622]: https:\/\/github.com\/tendermint\/tendermint\/issues\/1622\\n[types]: https:\/\/github.com\/tendermint\/tendermint\/tree\/master\/types\\n","Decision":"- restructure vote, proposal, and heartbeat such that their encoding is easily parseable by\\nhardware devices and smart contracts using a  binary encoding format ([amino] in this case)\\n- split up the messages exchanged between tendermint and remote signers into requests and\\nresponses (see details below)\\n- include an error type in responses\\n### Overview\\n```\\n+--------------+                      +----------------+\\n|              |     SignXRequest     |                |\\n|Remote signer |<---------------------+  tendermint    |\\n| (e.g. KMS)   |                      |                |\\n|              +--------------------->|                |\\n+--------------+    SignedXReply      +----------------+\\nSignXRequest {\\nx: X\\n}\\nSignedXReply {\\nx: X\\nsig: Signature \/\/ []byte\\nerr: Error{\\ncode: int\\ndesc: string\\n}\\n}\\n```\\nTODO: Alternatively, the type `X` might directly include the signature. A lot of places expect a vote with a\\nsignature and do not necessarily deal with \"Replies\".\\nStill exploring what would work best here.\\nThis would look like (exemplified using X = Vote):\\n```\\nVote {\\n\/\/ all fields besides signature\\n}\\nSignedVote {\\nVote Vote\\nSignature []byte\\n}\\nSignVoteRequest {\\nVote Vote\\n}\\nSignedVoteReply {\\nVote SignedVote\\nErr  Error\\n}\\n```\\n**Note:** There was a related discussion around including a fingerprint of, or, the whole public-key\\ninto each sign-request to tell the signer which corresponding private-key to\\nuse to sign the message. This is particularly relevant in the context of the KMS\\nbut is currently not considered in this ADR.\\n[amino]: https:\/\/github.com\/tendermint\/go-amino\/\\n### Vote\\nAs explained in [issue#1622] `Vote` will be changed to contain the following fields\\n(notation in protobuf-like syntax for easy readability):\\n```proto\\n\/\/ vanilla protobuf \/ amino encoded\\nmessage Vote {\\nVersion       fixed32\\nHeight        sfixed64\\nRound         sfixed32\\nVoteType      fixed32\\nTimestamp     Timestamp         \/\/ << using protobuf definition\\nBlockID       BlockID           \/\/ << as already defined\\nChainID       string            \/\/ at the end because length could vary a lot\\n}\\n\/\/ this is an amino registered type; like currently privval.SignVoteMsg:\\n\/\/ registered with \"tendermint\/socketpv\/SignVoteRequest\"\\nmessage SignVoteRequest {\\nVote vote\\n}\\n\/\/  amino registered type\\n\/\/ registered with \"tendermint\/socketpv\/SignedVoteReply\"\\nmessage SignedVoteReply {\\nVote      Vote\\nSignature Signature\\nErr       Error\\n}\\n\/\/ we will use this type everywhere below\\nmessage Error {\\nType        uint  \/\/ error code\\nDescription string  \/\/ optional description\\n}\\n```\\nThe `ChainID` gets moved into the vote message directly. Previously, it was injected\\nusing the [Signable] interface method `SignBytes(chainID string) []byte`. Also, the\\nsignature won't be included directly, only in the corresponding `SignedVoteReply` message.\\n[Signable]: https:\/\/github.com\/tendermint\/tendermint\/blob\/d419fffe18531317c28c29a292ad7d253f6cafdf\/types\/signable.go#L9-L11\\n### Proposal\\n```proto\\n\/\/ vanilla protobuf \/ amino encoded\\nmessage Proposal {\\nHeight            sfixed64\\nRound             sfixed32\\nTimestamp         Timestamp         \/\/ << using protobuf definition\\nBlockPartsHeader  PartSetHeader     \/\/ as already defined\\nPOLRound          sfixed32\\nPOLBlockID        BlockID           \/\/ << as already defined\\n}\\n\/\/ amino registered with \"tendermint\/socketpv\/SignProposalRequest\"\\nmessage SignProposalRequest {\\nProposal proposal\\n}\\n\/\/ amino registered with \"tendermint\/socketpv\/SignProposalReply\"\\nmessage SignProposalReply {\\nProp   Proposal\\nSig    Signature\\nErr    Error     \/\/ as defined above\\n}\\n```\\n### Heartbeat\\n**TODO**: clarify if heartbeat also needs a fixed offset and update the fields accordingly:\\n```proto\\nmessage Heartbeat {\\nValidatorAddress Address\\nValidatorIndex   int\\nHeight           int64\\nRound            int\\nSequence         int\\n}\\n\/\/ amino registered with \"tendermint\/socketpv\/SignHeartbeatRequest\"\\nmessage SignHeartbeatRequest {\\nHb Heartbeat\\n}\\n\/\/ amino registered with \"tendermint\/socketpv\/SignHeartbeatReply\"\\nmessage SignHeartbeatReply {\\nHb     Heartbeat\\nSig    Signature\\nErr    Error     \/\/ as defined above\\n}\\n```\\n","tokens":249,"id":4097,"text":"## Context\\nCurrently, the messages exchanged between tendermint and a (potentially remote) signer\/validator,\\nnamely votes, proposals, and heartbeats, are encoded as a JSON string\\n(e.g., via `Vote.SignBytes(...)`) and then\\nsigned . JSON encoding is sub-optimal for both, hardware wallets\\nand for usage in ethereum smart contracts. Both is laid down in detail in [issue#1622].\\nAlso, there are currently no differences between sign-request and -replies. Also, there is no possibility\\nfor a remote signer to include an error code or message in case something went wrong.\\nThe messages exchanged between tendermint and a remote signer currently live in\\n[privval\/socket.go] and encapsulate the corresponding types in [types].\\n[privval\/socket.go]: https:\/\/github.com\/tendermint\/tendermint\/blob\/d419fffe18531317c28c29a292ad7d253f6cafdf\/privval\/socket.go#L496-L502\\n[issue#1622]: https:\/\/github.com\/tendermint\/tendermint\/issues\/1622\\n[types]: https:\/\/github.com\/tendermint\/tendermint\/tree\/master\/types\\n\n\n##Decision\n- restructure vote, proposal, and heartbeat such that their encoding is easily parseable by\\nhardware devices and smart contracts using a  binary encoding format ([amino] in this case)\\n- split up the messages exchanged between tendermint and remote signers into requests and\\nresponses (see details below)\\n- include an error type in responses\\n### Overview\\n```\\n+--------------+                      +----------------+\\n|              |     SignXRequest     |                |\\n|Remote signer |<---------------------+  tendermint    |\\n| (e.g. KMS)   |                      |                |\\n|              +--------------------->|                |\\n+--------------+    SignedXReply      +----------------+\\nSignXRequest {\\nx: X\\n}\\nSignedXReply {\\nx: X\\nsig: Signature \/\/ []byte\\nerr: Error{\\ncode: int\\ndesc: string\\n}\\n}\\n```\\nTODO: Alternatively, the type `X` might directly include the signature. A lot of places expect a vote with a\\nsignature and do not necessarily deal with \"Replies\".\\nStill exploring what would work best here.\\nThis would look like (exemplified using X = Vote):\\n```\\nVote {\\n\/\/ all fields besides signature\\n}\\nSignedVote {\\nVote Vote\\nSignature []byte\\n}\\nSignVoteRequest {\\nVote Vote\\n}\\nSignedVoteReply {\\nVote SignedVote\\nErr  Error\\n}\\n```\\n**Note:** There was a related discussion around including a fingerprint of, or, the whole public-key\\ninto each sign-request to tell the signer which corresponding private-key to\\nuse to sign the message. This is particularly relevant in the context of the KMS\\nbut is currently not considered in this ADR.\\n[amino]: https:\/\/github.com\/tendermint\/go-amino\/\\n### Vote\\nAs explained in [issue#1622] `Vote` will be changed to contain the following fields\\n(notation in protobuf-like syntax for easy readability):\\n```proto\\n\/\/ vanilla protobuf \/ amino encoded\\nmessage Vote {\\nVersion       fixed32\\nHeight        sfixed64\\nRound         sfixed32\\nVoteType      fixed32\\nTimestamp     Timestamp         \/\/ << using protobuf definition\\nBlockID       BlockID           \/\/ << as already defined\\nChainID       string            \/\/ at the end because length could vary a lot\\n}\\n\/\/ this is an amino registered type; like currently privval.SignVoteMsg:\\n\/\/ registered with \"tendermint\/socketpv\/SignVoteRequest\"\\nmessage SignVoteRequest {\\nVote vote\\n}\\n\/\/  amino registered type\\n\/\/ registered with \"tendermint\/socketpv\/SignedVoteReply\"\\nmessage SignedVoteReply {\\nVote      Vote\\nSignature Signature\\nErr       Error\\n}\\n\/\/ we will use this type everywhere below\\nmessage Error {\\nType        uint  \/\/ error code\\nDescription string  \/\/ optional description\\n}\\n```\\nThe `ChainID` gets moved into the vote message directly. Previously, it was injected\\nusing the [Signable] interface method `SignBytes(chainID string) []byte`. Also, the\\nsignature won't be included directly, only in the corresponding `SignedVoteReply` message.\\n[Signable]: https:\/\/github.com\/tendermint\/tendermint\/blob\/d419fffe18531317c28c29a292ad7d253f6cafdf\/types\/signable.go#L9-L11\\n### Proposal\\n```proto\\n\/\/ vanilla protobuf \/ amino encoded\\nmessage Proposal {\\nHeight            sfixed64\\nRound             sfixed32\\nTimestamp         Timestamp         \/\/ << using protobuf definition\\nBlockPartsHeader  PartSetHeader     \/\/ as already defined\\nPOLRound          sfixed32\\nPOLBlockID        BlockID           \/\/ << as already defined\\n}\\n\/\/ amino registered with \"tendermint\/socketpv\/SignProposalRequest\"\\nmessage SignProposalRequest {\\nProposal proposal\\n}\\n\/\/ amino registered with \"tendermint\/socketpv\/SignProposalReply\"\\nmessage SignProposalReply {\\nProp   Proposal\\nSig    Signature\\nErr    Error     \/\/ as defined above\\n}\\n```\\n### Heartbeat\\n**TODO**: clarify if heartbeat also needs a fixed offset and update the fields accordingly:\\n```proto\\nmessage Heartbeat {\\nValidatorAddress Address\\nValidatorIndex   int\\nHeight           int64\\nRound            int\\nSequence         int\\n}\\n\/\/ amino registered with \"tendermint\/socketpv\/SignHeartbeatRequest\"\\nmessage SignHeartbeatRequest {\\nHb Heartbeat\\n}\\n\/\/ amino registered with \"tendermint\/socketpv\/SignHeartbeatReply\"\\nmessage SignHeartbeatReply {\\nHb     Heartbeat\\nSig    Signature\\nErr    Error     \/\/ as defined above\\n}\\n```\\n","MatchedIds":[1949,21,1962,4538,1985],"Predicted":"## Decision\n### Migration to Protobuf Encoding\n\nAfter thorough consideration and discussions with multiple stakeholders, we have decided to migrate the encoding of messages exchanged between Tendermint and remote signers\/validators to use Protobuf instead of JSON.\n\n### Rationale\n1. **Hardware Wallet Compatibility**: Protobuf is much more suitable for hardware wallets than JSON due to its binary and efficient nature. This change will significantly improve the performance and compatibility with hardware wallets.\n  \n2. **Ethereum Smart Contracts**: Protobuf is more efficient for usage within Ethereum smart contracts, which often have restrictions related to storage and processing power.\n\n3. **Structured and Versioned Messaging**: Protobuf provides a more structured and versioned approach to message definition, ensuring better backward and forward compatibility than JSON.\n\n4. **Error Handling**: We will introduce distinct message types for sign requests and replies. This will allow the inclusion of error codes and messages within the protocol, enabling sophisticated error handling and debugging.\n\n### Plan\n1. **Message Definition**:\n   Define Protobuf message types for the interactions that include votes, proposals, and heartbeats. Example:\n   ```protobuf\n   syntax = \"proto3\";\n   \n   package tendermint;\n\n   message Vote {\n     \/\/ original Vote fields here\n   }\n\n   message Proposal {\n     \/\/ original Proposal fields here\n   }\n\n   message Heartbeat {\n     \/\/ original Heartbeat fields here\n   }\n\n   message SignRequest {\n     oneof payload {\n       Vote vote = 1;\n       Proposal proposal = 2;\n       Heartbeat heartbeat = 3;\n     }\n   }\n\n   message SignReply {\n     bool success = 1;\n     string error_message = 2;\n     SignRequest payload = 3;\n   }\n   ```\n\n2. **Implementation**:\n   Update the Go code in `privval\/socket.go` and related modules to handle Protobuf encoding and decoding. Example:\n   ```go\n   import (\n    \"google.golang.org\/protobuf\/proto\"\n    ... \/\/ other imports\n   )\n   \n   func sendSignRequest(conn net.Conn, request *types.SignRequest) error {\n    data, err := proto.Marshal(request)\n    if err != nil {\n      return fmt.Errorf(\"failed to marshal request: %w\", err)\n    }\n    _, err = conn.Write(data)\n    return err\n   }\n   \n   func handleSignReply(conn net.Conn) (*types.SignReply, error) {\n    var reply types.SignReply\n    buffer, err := readFromConn(conn)\n    if err != nil {\n      return nil, err\n    }\n    if err := proto.Unmarshal(buffer, &reply); err != nil {\n      return nil, fmt.Errorf(\"failed to unmarshal reply: %w\", err)\n    }\n    return &reply, nil\n   }\n   ```\n\n3. **Testing**:\n   Comprehensive testing is essential for the new Protobuf-based protocol to ensure backward compatibility and correctness. This will involve unit tests, integration tests, and also testing with existing hardware wallets and Ethereum smart contract environments.\n\n4. **Documentation**:\n   Update the documentation to guide developers through the new protocol, including example code snippets and explanations on how to handle different scenarios and error messages.\n\nBy migrating to Protobuf, we aim to enhance efficiency, compatibility, and maintainability of the message exchange between Tendermint and remote signers\/validators. This move sets a robust foundation for future developments and integrations.","GenTime":"2024-08-20 00:20:32"}
{"File Name":"tdr-dev-documentation\/0017-govuk-notify-keycloak-message.md","Context":"## Context\\nGovUK Notify is used as the email provider for Keycloak: [0012 Keycloak Email Provider](0012-keycloak-email-provider.md)\\n","Decision":"It was decided to pass the entire Keycloak message into the GovUK Notify template via the `keycloakMessage` personalisation parameter, rather than having a mix of text and parameters in the Keycloak template.\\nThis is because the Keycloak message is passed in as a complete constructed string. It would be very difficult to extract particular portions of this string, for example a link for use within the GovUK Notify template.\\nThis approach does mean it is harder for non-developers to make changes to the email messages. The decision may be revisited in the future if this proves to be causing issues.\\n","tokens":33,"id":1786,"text":"## Context\\nGovUK Notify is used as the email provider for Keycloak: [0012 Keycloak Email Provider](0012-keycloak-email-provider.md)\\n\n\n##Decision\nIt was decided to pass the entire Keycloak message into the GovUK Notify template via the `keycloakMessage` personalisation parameter, rather than having a mix of text and parameters in the Keycloak template.\\nThis is because the Keycloak message is passed in as a complete constructed string. It would be very difficult to extract particular portions of this string, for example a link for use within the GovUK Notify template.\\nThis approach does mean it is harder for non-developers to make changes to the email messages. The decision may be revisited in the future if this proves to be causing issues.\\n","MatchedIds":[1784,996,2050,5213,1767],"Predicted":"## Decision\nWe will continue to use GOV.UK Notify as the email provider for Keycloak to ensure consistency in email delivery and take advantage of the service's reliability and cost-effectiveness. This decision aligns with the existing integration outlined in [0012 Keycloak Email Provider](0012-keycloak-email-provider.md). Future enhancements and notifications required by Keycloak will also follow the current setup with GOV.UK Notify. This approach ensures a unified and streamlined email infrastructure across our services.","GenTime":"2024-08-20 00:20:38"}
{"File Name":"apply-for-teacher-training\/0015-carrying-over-applications.md","Context":"## Context\\nThe current recruitment cycle ends on 18th September 2020. At that point there\\nwill be some candidates who could benefit from their application being carried\\nover to the next cycle. Carrying over an application means the candidate can\\napply to courses in the new recruitment cycle without having to fill in the\\nwhole application form again.\\n### Carrying over an application makes sense in the following states\\n#### Before the application reaches the provider\\nThese applications could be carried over because the provider has not seen them yet.\\n- Withdrawn\\n- Unsubmitted\\n- Ready to send to provider\\n#### After the application can\u2019t progress any further\\nThese applications could be carried over because they have reached an\\nunsuccessful end state. Enabling candidates to turn these into fresh applications\\nin the next cycle makes it as easy as possible for them to try again.\\n- Conditions not met\\n- Offer withdrawn\\n- Offer declined\\n- Application cancelled\\n- Rejected\\n### Carrying over an application does not make sense in the following states\\n#### While the application is already under consideration by the provider\\n- Awaiting provider decision\\n#### When the application already has an offer in flight\\n- Offer\\n- Meeting conditions (i.e. offer accepted)\\n- Recruited\\n### Copying the Apply again approach\\nThe current approach for moving applications into Apply again is to copy the\\nentire application (including references) and invite the user to add a new\\ncourse choice. This approach seems like it will work here too, with a couple of\\nextra things to take into account:\\n- applications that are carried over might be in Apply 1 or Apply again as the\\ncycle ends. All carried-over applications should start over as Apply 1\\napplications applications moving into Apply again all have complete\\nreferences because they\u2019ve already completed Apply 1, for which references\\nare required.\\n- Carried over applications might have no references, references in flight, or\\ncompleted references.\\nMoving the new application into the next cycle is a question of making sure its\\ncourse choices come from that cycle. As long as carrying over is only possible\\nonce the current cycle is closed, this should present no problems because the\\navailable courses will all come from the new cycle.\\n","Decision":"- We will only allow applications to be carried over once the current cycle is\\nover, and we\u2019ll only allow applications in the above states\\n- To carry over an application, we will adopt the Apply again pattern of\\ncloning the ApplicationForm and removing the courses\\n- We will copy references onto the carried-over application, but only if\\nthey\u2019re complete. Referees who had been contacted but had not responded\\nbefore the application was carried over will need to be cancelled.\\n- The applications that were carried over will remain in the database without\\nany further state change\\n- Applications which were not yet sent to the provider at end of cycle and also\\nnot carried over will still be in the database \u2014 we would like to mark these\\nwith a new state equivalent to \u201cincomplete at end of cycle\u201d. This state would\\nnever be visible to providers.\\n- It\u2019s up to the candidate whether to carry over their application, and we\u2019ll\\ngive them a button to do this\\n","tokens":467,"id":4010,"text":"## Context\\nThe current recruitment cycle ends on 18th September 2020. At that point there\\nwill be some candidates who could benefit from their application being carried\\nover to the next cycle. Carrying over an application means the candidate can\\napply to courses in the new recruitment cycle without having to fill in the\\nwhole application form again.\\n### Carrying over an application makes sense in the following states\\n#### Before the application reaches the provider\\nThese applications could be carried over because the provider has not seen them yet.\\n- Withdrawn\\n- Unsubmitted\\n- Ready to send to provider\\n#### After the application can\u2019t progress any further\\nThese applications could be carried over because they have reached an\\nunsuccessful end state. Enabling candidates to turn these into fresh applications\\nin the next cycle makes it as easy as possible for them to try again.\\n- Conditions not met\\n- Offer withdrawn\\n- Offer declined\\n- Application cancelled\\n- Rejected\\n### Carrying over an application does not make sense in the following states\\n#### While the application is already under consideration by the provider\\n- Awaiting provider decision\\n#### When the application already has an offer in flight\\n- Offer\\n- Meeting conditions (i.e. offer accepted)\\n- Recruited\\n### Copying the Apply again approach\\nThe current approach for moving applications into Apply again is to copy the\\nentire application (including references) and invite the user to add a new\\ncourse choice. This approach seems like it will work here too, with a couple of\\nextra things to take into account:\\n- applications that are carried over might be in Apply 1 or Apply again as the\\ncycle ends. All carried-over applications should start over as Apply 1\\napplications applications moving into Apply again all have complete\\nreferences because they\u2019ve already completed Apply 1, for which references\\nare required.\\n- Carried over applications might have no references, references in flight, or\\ncompleted references.\\nMoving the new application into the next cycle is a question of making sure its\\ncourse choices come from that cycle. As long as carrying over is only possible\\nonce the current cycle is closed, this should present no problems because the\\navailable courses will all come from the new cycle.\\n\n\n##Decision\n- We will only allow applications to be carried over once the current cycle is\\nover, and we\u2019ll only allow applications in the above states\\n- To carry over an application, we will adopt the Apply again pattern of\\ncloning the ApplicationForm and removing the courses\\n- We will copy references onto the carried-over application, but only if\\nthey\u2019re complete. Referees who had been contacted but had not responded\\nbefore the application was carried over will need to be cancelled.\\n- The applications that were carried over will remain in the database without\\nany further state change\\n- Applications which were not yet sent to the provider at end of cycle and also\\nnot carried over will still be in the database \u2014 we would like to mark these\\nwith a new state equivalent to \u201cincomplete at end of cycle\u201d. This state would\\nnever be visible to providers.\\n- It\u2019s up to the candidate whether to carry over their application, and we\u2019ll\\ngive them a button to do this\\n","MatchedIds":[1799,5009,4277,1754,915],"Predicted":"## Decision\nImplement the carry-over mechanism for applications based on their current states, ensuring a smooth transition for candidates into the new recruitment cycle.\n\n### Details\n1. **Carry Over Eligible Applications**:\n   - States: Withdrawn, Unsubmitted, Ready to send to provider, Conditions not met, Offer withdrawn, Offer declined, Application cancelled, Rejected.\n   - Approach: Copy the entire application, including references. Invite the user to add new course choices from the upcoming cycle and ensure all carried-over applications start as Apply 1.\n\n2. **Applications Not Eligible for Carry Over**:\n   - States: Awaiting provider decision, Offer, Meeting conditions, Recruited.\n   - Rationale: These applications are either under active consideration or have future commitments, which makes them unsuitable for carry-over.\n\n### Benefits\n- **Efficiency**: Candidates do not need to fill out the entire application form again, minimizing their effort.\n- **Consistency**: Following the existing \"Apply again\" approach ensures predictability and reduces implementation complexity.\n- **User Experience**: Enhances the candidate's experience by making it easy for them to reapply in the next cycle.\n\n### Implementation Considerations\n- **References Handling**: \n  - Applications may have no references, references in-flight, or completed references.\n  - Ensure the system can handle all these states and carry over the references appropriately.\n- **Course Choices Update**:\n  - Ensure that the course choices are updated to adapt to the new cycle's available courses.\n  \nBy adhering to this approach, we aim to streamline the carry-over process, making it beneficial and straightforward for candidates to apply in the upcoming recruitment cycle.","GenTime":"2024-08-20 00:20:48"}
{"File Name":"holochain-rust\/0003-redux-architecture-pattern.md","Context":"## Context\\nWe are doing a rewrite.\\nHolochain Go code shows many implicit dependencies between different modules and stateful objects. In conjunction with the complexity of a p2p network of agents, this leads to a level of overall complexity that feels too much to manage. A clean and fitting architecture for this Rust rebuild is needed.\\nHaving a single global state within the agent feels appropriate and even balancing the distributed nature of the network of agents.\\n","Decision":"The new holochain architecture will follow a redux architecture in order for an agent to have one global state.\\nWe will apply nested state objects which represent a state tree, with sub states for each module.\\nWe use reference counting smart pointers for the sub\\nstates such that it is possible for each module's\\nreducer to decide if the sub state is to be mutated or reused.\\n","tokens":92,"id":1495,"text":"## Context\\nWe are doing a rewrite.\\nHolochain Go code shows many implicit dependencies between different modules and stateful objects. In conjunction with the complexity of a p2p network of agents, this leads to a level of overall complexity that feels too much to manage. A clean and fitting architecture for this Rust rebuild is needed.\\nHaving a single global state within the agent feels appropriate and even balancing the distributed nature of the network of agents.\\n\n\n##Decision\nThe new holochain architecture will follow a redux architecture in order for an agent to have one global state.\\nWe will apply nested state objects which represent a state tree, with sub states for each module.\\nWe use reference counting smart pointers for the sub\\nstates such that it is possible for each module's\\nreducer to decide if the sub state is to be mutated or reused.\\n","MatchedIds":[1494,1500,1956,4539,311],"Predicted":"## Decision\nWe will implement a single global state management system within each agent for the Rust rewrite. This central state will provide a consistent and unified interface for interacting with the agent's various modules and stateful objects. By consolidating state management, we aim to reduce the implicit dependencies and overall complexity of the system, thereby making it easier to manage and reason about. This approach will also help in balancing the distributed nature of the network while keeping the agent's internal operations coherent and streamlined.","GenTime":"2024-08-20 00:20:51"}
{"File Name":"libelektra\/store_name.md","Context":"## Problem\\nMemory Consumption in Elektra is quite high as the key names\\nare long and stored twice in memory.\\n","Decision":"Only store the unescaped key name, suitable for comparing\/searching\/iterating over name, i.e.:\\n- Remove the escaped name from `struct _Key` and use it only when necessary.\\n- Clarify and reduce [terminology](\/doc\/help\/elektra-glossary.md).\\n- API Changes:\\n- `keyNew (const char*, size_t)`\\n- `keyName` returns the unescaped name\\n- remove `keyUnescapedName`, `keyGetUnescapedNameSize`.\\n- reverse terminology: with \"key name\" we will refer to the unescaped (base) name,\\nthe escaped name will be explicitly called \"escaped key name\".\\n- escaped name will be outside the core for tooling\\n- `keyDup(.., int)` with options to filter which parts are copied\\n(to allow copy of keys where only the key name is copied)\\n","tokens":26,"id":1291,"text":"## Problem\\nMemory Consumption in Elektra is quite high as the key names\\nare long and stored twice in memory.\\n\n\n##Decision\nOnly store the unescaped key name, suitable for comparing\/searching\/iterating over name, i.e.:\\n- Remove the escaped name from `struct _Key` and use it only when necessary.\\n- Clarify and reduce [terminology](\/doc\/help\/elektra-glossary.md).\\n- API Changes:\\n- `keyNew (const char*, size_t)`\\n- `keyName` returns the unescaped name\\n- remove `keyUnescapedName`, `keyGetUnescapedNameSize`.\\n- reverse terminology: with \"key name\" we will refer to the unescaped (base) name,\\nthe escaped name will be explicitly called \"escaped key name\".\\n- escaped name will be outside the core for tooling\\n- `keyDup(.., int)` with options to filter which parts are copied\\n(to allow copy of keys where only the key name is copied)\\n","MatchedIds":[1293,1303,1318,1319,1292],"Predicted":"## Decision\nTo reduce memory consumption due to long key names being stored twice, the following measures will be implemented:\n\n1. **Key Name Deduplication:** \n   - Introduce a mechanism to store key names in a shared string pool. This pool will maintain unique instances of key names, and keys will reference these pooled strings. This ensures key names are not redundantly stored multiple times in memory.\n  \n2. **String Compression:**\n   - Apply lightweight string compression techniques to key names in memory where feasible. This will further reduce the memory footprint for long key names.\n\n3. **Lazy Key Name Allocation:** \n   - Delay the actual allocation and storage of key names until it is absolutely necessary. For example, initial parsing and internal representations might operate on indices or temporary tokens.\n\n4. **Refactor Key Structures:**\n   - Optimize the internal structure of key objects to avoid redundant storage and improve memory efficiency. This could involve restructuring the internal data format to store key names more compactly.\n\n5. **Cache Frequently Used Key Names:**\n   - Implement a caching layer for frequently accessed key names, reducing the need to repeatedly store these names in the main memory.\n\n6. **Optimize Memory Management:**\n   - Use specialized memory allocators and pools to manage key name storage more efficiently, reducing overall memory fragmentation and overhead.\n\nBy implementing these measures, we expect a significant reduction in memory consumption caused by the storage of key names, without compromising the performance or functionality of Elektra.","GenTime":"2024-08-20 00:20:56"}
{"File Name":"TruBudget\/0003-project-data-model.md","Context":"## Context\\nWe need to define how to save and query project, subproject and workflow data efficiently.\\n### Definition of terms\\n- Project: Is the root element of the hierarchy. It contains metadata and Subprojects\\n- Subproject: Are a subset of Projects and collect a number of Workflowitems\\n- Workflowitems: Are a number of steps which need to be executed in a certain order\\n- Resources: Projects, Subprojects, Workflowitems are resources. This means they contain permissions, a log and metadata.\\n### Relations\\n```mermaid\\ngraph TD;\\nProject-->Subproject_A;\\nProject-->Subproject_B;\\nSubproject_A-->Workflowitem_A;\\nSubproject_A-->Workflowitem_B;\\nSubproject_B-->Workflowitem_C;\\nSubproject_B-->Workflowitem_D;\\n```\\n### Options\\n1. Each resource has its own stream. Each resource has an array which links to the underlying resources (e.g. A Projects has an array of the ids of its Subrojects). That means if we want to query Subprojects of a specific Project, we need to lookup the Subproject ID's inside the projects to afterswards fetch all the underlying Subproject streams. **Pro:** We are only fetching the data which is necessary, changes inside the resources are storage efficent. **Con:** Resources with a high number of relations result in multiple (maybe hundreds) of queries.\\n2. Each Project has its own stream. All relational resources of the projects are stored inside a JSON and resolved on the API Layer. **Pro:** Only one query necessary to resolve the whole resource tree for one project. **Con:** Large amount of storage needed, since every change inside a single resource would trigger a whole copy of the project tree. Large datastructures have to be kept in memory\\n3. Each project has its own stream. All relational resources are organized with multiple key-items inside the project stream. This allows to specifically query resources with a single query. Updates to resources will only result in the copy of the affected resource. **Pro:** Easy to query, low storage \/ memory footprint **Con:** None\\n","Decision":"Since we need to be able to scale out to larger projects, it is important to be able to execute fast queries and having a low storage \/ memory footprint. The goal is to exploit the Multichain API as much as possible, since we can expect it to be more performant than our API. Option 3 is the option, which covers the required capabilities in the best way.\\n","tokens":438,"id":1225,"text":"## Context\\nWe need to define how to save and query project, subproject and workflow data efficiently.\\n### Definition of terms\\n- Project: Is the root element of the hierarchy. It contains metadata and Subprojects\\n- Subproject: Are a subset of Projects and collect a number of Workflowitems\\n- Workflowitems: Are a number of steps which need to be executed in a certain order\\n- Resources: Projects, Subprojects, Workflowitems are resources. This means they contain permissions, a log and metadata.\\n### Relations\\n```mermaid\\ngraph TD;\\nProject-->Subproject_A;\\nProject-->Subproject_B;\\nSubproject_A-->Workflowitem_A;\\nSubproject_A-->Workflowitem_B;\\nSubproject_B-->Workflowitem_C;\\nSubproject_B-->Workflowitem_D;\\n```\\n### Options\\n1. Each resource has its own stream. Each resource has an array which links to the underlying resources (e.g. A Projects has an array of the ids of its Subrojects). That means if we want to query Subprojects of a specific Project, we need to lookup the Subproject ID's inside the projects to afterswards fetch all the underlying Subproject streams. **Pro:** We are only fetching the data which is necessary, changes inside the resources are storage efficent. **Con:** Resources with a high number of relations result in multiple (maybe hundreds) of queries.\\n2. Each Project has its own stream. All relational resources of the projects are stored inside a JSON and resolved on the API Layer. **Pro:** Only one query necessary to resolve the whole resource tree for one project. **Con:** Large amount of storage needed, since every change inside a single resource would trigger a whole copy of the project tree. Large datastructures have to be kept in memory\\n3. Each project has its own stream. All relational resources are organized with multiple key-items inside the project stream. This allows to specifically query resources with a single query. Updates to resources will only result in the copy of the affected resource. **Pro:** Easy to query, low storage \/ memory footprint **Con:** None\\n\n\n##Decision\nSince we need to be able to scale out to larger projects, it is important to be able to execute fast queries and having a low storage \/ memory footprint. The goal is to exploit the Multichain API as much as possible, since we can expect it to be more performant than our API. Option 3 is the option, which covers the required capabilities in the best way.\\n","MatchedIds":[4285,3776,3071,1031,4699],"Predicted":"## Decision\nBased on the context and the provided options, we will adopt **Option 3** where each project has its own stream, and all relational resources are organized with multiple key-items inside the project stream. This decision is made for the following reasons:\n\n- **Efficiency in Querying:** This approach allows querying specific resources with a single query, reducing the complexity and time required to fetch related data.\n- **Storage and Memory Optimization:** It maintains a low storage and memory footprint as updates to individual resources only result in copying the affected resource rather than the entire project tree.\n- **Balance of Pros and Cons:** Compared to the other options, Option 3 offers a good balance with no significant drawbacks.\n\nThis decision enables efficient data management and querying while ensuring scalability and maintainability of the project hierarchy.","GenTime":"2024-08-20 00:20:59"}
{"File Name":"tdr-dev-documentation\/0015-govuk-notify-staging-configuration.md","Context":"## Context\\nGovUK Notify is used as the email provider for Keycloak: [0012 Keycloak Email Provider](0012-keycloak-email-provider.md)\\nIt was decided to use a separate GovUK Notify service for each TDR environment as GovUK Notify does not have the concept of environments: [0014 GovUK Notify Multi-environment Configuration](0014-govuk-notify-multi-environment-configuration.md)\\nGovUK services have a \"trial mode\", and a \"live mode\".\\nThe trial mode has limits placed on who can receive emails, and the number of emails that can be sent.\\n","Decision":"The \"staging\" GovUK Notify service is to be live to allow for user testing on this environment.\\nAlso, it will mirror the TDR production environment as closely as possible.\\n","tokens":126,"id":1784,"text":"## Context\\nGovUK Notify is used as the email provider for Keycloak: [0012 Keycloak Email Provider](0012-keycloak-email-provider.md)\\nIt was decided to use a separate GovUK Notify service for each TDR environment as GovUK Notify does not have the concept of environments: [0014 GovUK Notify Multi-environment Configuration](0014-govuk-notify-multi-environment-configuration.md)\\nGovUK services have a \"trial mode\", and a \"live mode\".\\nThe trial mode has limits placed on who can receive emails, and the number of emails that can be sent.\\n\n\n##Decision\nThe \"staging\" GovUK Notify service is to be live to allow for user testing on this environment.\\nAlso, it will mirror the TDR production environment as closely as possible.\\n","MatchedIds":[1786,1767,996,2050,5213],"Predicted":"## Decision\nIt was decided to set the GovUK Notify services for the non-production TDR environments to \"trial mode\" to ensure that emails are only sent to whitelisted addresses and to limit the volume of emails, thereby preventing accidental spamming. For the production environment, the GovUK Notify service will be set to \"live mode\" to ensure that all users can receive emails without restrictions.\n\nThe separate configurations for each TDR environment will allow us to effectively manage email notifications while adhering to the usage policies of GovUK Notify. Should there be a need for higher testing volumes or broader reach in non-production environments, this decision can be revisited and adjusted accordingly.","GenTime":"2024-08-20 00:21:04"}
{"File Name":"sre-challenge\/0007-use-a-dashboard-for-etcd.md","Context":"## Context\\nTo visualize the metrics of the etcd cluster, I would like to implement a\\ndashboard using the USE and RED methods.\\n- USE to measure the performance of the system hosting the etcd cluster\\n- RED to measure the performance of the gRPC side of the etcd cluster\\nBut I haven't found anything like this and I have no time to waste.  So\\nI have found a dashboard on the grafana site that has some metrics.\\n","Decision":"As I'm out of time, I'm just going to implement it at the configuration\\nmanagement system, to automatically provision the dashboard and the\\ndatasource.\\n","tokens":97,"id":3004,"text":"## Context\\nTo visualize the metrics of the etcd cluster, I would like to implement a\\ndashboard using the USE and RED methods.\\n- USE to measure the performance of the system hosting the etcd cluster\\n- RED to measure the performance of the gRPC side of the etcd cluster\\nBut I haven't found anything like this and I have no time to waste.  So\\nI have found a dashboard on the grafana site that has some metrics.\\n\n\n##Decision\nAs I'm out of time, I'm just going to implement it at the configuration\\nmanagement system, to automatically provision the dashboard and the\\ndatasource.\\n","MatchedIds":[1701,3469,2998,4909,4705],"Predicted":"## Decision\nWe will use the existing Grafana dashboard template available on the Grafana website to visualize the metrics of the etcd cluster. This expedites the process as it saves time on creating and validating our own dashboard from scratch, which aligns with our current time constraints.\n\n### Positive Consequences\n1. **Time Efficiency**: Allows for rapid deployment and visualization with minimal setup time.\n2. **Proven Solution**: Provides a reliable and validated set of metrics which are widely used and tested by the community.\n3. **Improved Monitoring**: Helps in quickly gaining visibility into key metrics using well-established practices like USE and RED methodologies.\n\n### Negative Consequences\n1. **Limited Customization**: Might not cover all the custom metrics and specific monitoring needs we might encounter.\n2. **Learning Curve**: Team may need to spend some time understanding the existing dashboard layout and metrics if it does not align with their previous experiences.\n\nDespite these negatives, adopting the available Grafana dashboard ensures we meet our immediate need for visualizing etcd cluster metrics without significant delay.","GenTime":"2024-08-20 00:21:08"}
{"File Name":"AYCHAIN.github.io\/adr-005-substeps-list.md","Context":"## Context\\nSubstepsList needs to be animated between showing and hiding the substeps of a step.\\nAnimating proves convenient on `max-height`\\nWe were initially hard coding the `max-height` property of the substeps list to have a maximum to animate too (from 0).\\nHowever, hard-coding that value proved not flexible, especially when the substep was also holding an `ErrorMessage`, which could span an undeterminate height (multiple lines, responsiveness, etc).\\nThe height of the list all determines the height of the progress bar which at this point is settled and working with little work.\\n","Decision":"We chose to follow the second approach which requires less code to work. It is however creating issues with the `html` templating function of `lit-element`, since we change the state of the attribute without informing the library.\\nDoing the latter would mean re-rendering via a state prop which would have to be updated at the `_didRender` moment, and would create complains from Polymer.\\nWe thus decided to trick the component:\\n- we only calculate the `totalHeight` once, since content will not change once rendered.\\n- we only force the `max-height` with the calculated height once, if the component was forced rendered and if we haven't applied the trick already\\nWe discovered that it would then require 3 clicks on the `hide` button to finally close the list. Some research led us to discover that the `html` templating function handles the values of the attributes, and does not trigger a change (cf: https:\/\/github.com\/Polymer\/lit-html\/blob\/master\/src\/core.ts#L538). As such, in the state of the code, the active value is `0` and not `[totalHeight]px` as we forced it to be. Since the closing value is 0 too, the code didn't see the reason for the change.\\nTo circumvent this issue, we are now setting the `hidden` state value to be 1px, which allows for the initial closing.\\n","tokens":124,"id":158,"text":"## Context\\nSubstepsList needs to be animated between showing and hiding the substeps of a step.\\nAnimating proves convenient on `max-height`\\nWe were initially hard coding the `max-height` property of the substeps list to have a maximum to animate too (from 0).\\nHowever, hard-coding that value proved not flexible, especially when the substep was also holding an `ErrorMessage`, which could span an undeterminate height (multiple lines, responsiveness, etc).\\nThe height of the list all determines the height of the progress bar which at this point is settled and working with little work.\\n\n\n##Decision\nWe chose to follow the second approach which requires less code to work. It is however creating issues with the `html` templating function of `lit-element`, since we change the state of the attribute without informing the library.\\nDoing the latter would mean re-rendering via a state prop which would have to be updated at the `_didRender` moment, and would create complains from Polymer.\\nWe thus decided to trick the component:\\n- we only calculate the `totalHeight` once, since content will not change once rendered.\\n- we only force the `max-height` with the calculated height once, if the component was forced rendered and if we haven't applied the trick already\\nWe discovered that it would then require 3 clicks on the `hide` button to finally close the list. Some research led us to discover that the `html` templating function handles the values of the attributes, and does not trigger a change (cf: https:\/\/github.com\/Polymer\/lit-html\/blob\/master\/src\/core.ts#L538). As such, in the state of the code, the active value is `0` and not `[totalHeight]px` as we forced it to be. Since the closing value is 0 too, the code didn't see the reason for the change.\\nTo circumvent this issue, we are now setting the `hidden` state value to be 1px, which allows for the initial closing.\\n","MatchedIds":[4416,1855,5185,3738,5090],"Predicted":"## Decision\nUse JavaScript to dynamically calculate and set the `max-height` property for the SubstepsList animation.\n* This approach provides the needed flexibility to accommodate undetermined heights of elements within the substeps, including `ErrorMessage`.\n* Use an event listener to detect when the substeps are expanding or collapsing, and then calculate the actual height of the substeps content.\n* Set the `max-height` property to the calculated height value for the animation, reverting back to `0` when hiding.\n* Ensure the progress bar height is recalculated based on the dynamically set `max-height` value for consistency.\n\n### Example\n```js\nconst substepsList = document.querySelector('.substeps-list');\n\nfunction toggleSubsteps() {\n  const expanded = substepsList.classList.toggle('expanded');\n  \n  if (expanded) {\n    const contentHeight = substepsList.scrollHeight; \/\/ Get full height of content\n    substepsList.style.maxHeight = `${contentHeight}px`;\n  } else {\n    substepsList.style.maxHeight = '0';\n  }\n}\n\n\/\/ Example with animate CSS class\ndocument.querySelector('.toggle-button').addEventListener('click', () => {\n  toggleSubsteps();\n});\n```\n\n### CSS\n```css\n.substeps-list {\n  overflow: hidden;\n  transition: max-height 0.3s ease;\n}\n\n.substeps-list.expanded {\n  max-height: \/* dynamically set by JavaScript *\/;\n}\n```\n\nThis solution ensures flexibility and responsiveness, allowing the SubstepsList to properly accommodate varying content sizes while maintaining smooth animations.","GenTime":"2024-08-20 00:21:14"}
{"File Name":"python-library-project-generator\/0000-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on Opinionated Digital Center.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":18,"id":875,"text":"## Context\\nWe need to record the architectural decisions made on Opinionated Digital Center.\\n\n\n##Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","MatchedIds":[2261,4428,3119,1436,294],"Predicted":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).","GenTime":"2024-08-20 00:21:16"}
{"File Name":"LogLady\/0003-implement-redux.md","Context":"## Context\\nState handling in React becomes very complicated when you have too many states\\n","Decision":"We implement Redux to handle state\\n","tokens":17,"id":1854,"text":"## Context\\nState handling in React becomes very complicated when you have too many states\\n\n\n##Decision\nWe implement Redux to handle state\\n","MatchedIds":[4596,2298,2667,1855,3721],"Predicted":"## Decision\n- Prioritize using the `useState` hook for simple and localized state management within components.\n- Use `useReducer` for more complex state transitions and to manage state logic separately from component logic.\n- For shared state across multiple components, consider using `recoil` or `context + useReducer` to avoid prop drilling and reduce boilerplate.\n- When dealing with remote state, employ a tool like `@tanstack\/react-query` to handle data fetching, caching, and synchronization.\n- Where possible, keep state management isolated and scoped minimal to avoid the complexities that come from numerous states being managed simultaneously.\n- Document and review state management strategy periodically to ensure simplicity and maintainability as the application evolves.","GenTime":"2024-08-20 00:21:19"}
{"File Name":"opg-use-an-lpa\/0016-logging-and-tracing.md","Context":"## Context\\nWe want to have an application wide logging infrastructure in place so that we can trace user requests through the\\napplication. It should be possible to trace each request through the application layers so that we can see what outgoing\\ncalls were made for incoming requests.\\n","Decision":"The use of `monolog\/monolog` to provide configurable logging levels throughout the application. Unique tracing\\ninformation is already provided by the amazon loadbalancers so this should be made available to the logging library and\\nattached to the logged information.\\n* Logging of service code to be main source of log information. If needed handlers can also be logged.\\n* Most logging to be done at an _info_ level.\\n* User actions that need monitoring (e.g. authentication failures) at _notice_ level\\n* Personally Identifiable Information (PII) to **not** be logged anywhere but _debug_\\n","tokens":55,"id":4852,"text":"## Context\\nWe want to have an application wide logging infrastructure in place so that we can trace user requests through the\\napplication. It should be possible to trace each request through the application layers so that we can see what outgoing\\ncalls were made for incoming requests.\\n\n\n##Decision\nThe use of `monolog\/monolog` to provide configurable logging levels throughout the application. Unique tracing\\ninformation is already provided by the amazon loadbalancers so this should be made available to the logging library and\\nattached to the logged information.\\n* Logging of service code to be main source of log information. If needed handlers can also be logged.\\n* Most logging to be done at an _info_ level.\\n* User actions that need monitoring (e.g. authentication failures) at _notice_ level\\n* Personally Identifiable Information (PII) to **not** be logged anywhere but _debug_\\n","MatchedIds":[1924,3469,4271,4021,2502],"Predicted":"## Decision\nWe will implement a distributed tracing system to provide end-to-end request tracking and detailed performance monitoring. This will include:\n\n* **Trace ID and Span ID Propagation**: Each incoming request will be assigned a unique Trace ID. Spans representing each individual operation within the request lifecycle will have unique Span IDs. These IDs will be propagated through all layers of the application, including outgoing calls to external services.\n* **Logging Enhancements**: The logging framework will be updated to include Trace ID and Span ID in all log entries. This will enable correlation of logs across services.\n* **Integration with a Distributed Tracing Tool**: We will leverage a distributed tracing tool like Jaeger or Zipkin. These tools will collect and visualize trace data, providing insights into the request flow and identifying bottlenecks.\n* **Middleware\/Interceptors**: Implement middleware or interceptors in each service to automatically capture and propagate tracing information.\n\n```mermaid\nsequenceDiagram\nparticipant User\nparticipant Frontend\nparticipant Backend\nparticipant ExternalService\nUser ->> Frontend: HTTP Request\nFrontend ->> Backend: Forward Request (Trace ID)\nBackend ->> ExternalService: API Call (Trace ID)\nExternalService -->> Backend: Response (Trace ID)\nBackend -->> Frontend: Response (Trace ID)\nFrontend -->> User: HTTP Response (Trace ID)\n```\n\nBy implementing these measures, we will gain comprehensive traceability of user requests throughout our application, enabling us to diagnose and resolve performance issues more effectively.","GenTime":"2024-08-20 00:21:28"}
{"File Name":"figgy\/0002-queues.md","Context":"## Context\\nWe have background jobs that are processed by Sidekiq workers on several dedicated background processing\\nmachines.  The background jobs include a variety of different tasks from different sources:\\n* processing user requests (e.g., ingesting files, (re)generating derivatives)\\n* cleaning up after user actions (e.g., propagating state and visibility, removing files before deleting\\ntheir parent resource)\\n* refreshing metadata from Voyager\/PULFA\\n* bulk-ingesting content\\n* routine cleanup (cleaning out guest users)\\n* fixity checking\\n","Decision":"We will have three queues for processing background jobs:\\n1. `default` for processing user-initiated requests needed for ingesting and displaying objects\\n2. `low` for bulk processing, validation, cleanup, etc.\\n3. `super_low` for fixity checking and other long-term preservation actions\\n","tokens":116,"id":4821,"text":"## Context\\nWe have background jobs that are processed by Sidekiq workers on several dedicated background processing\\nmachines.  The background jobs include a variety of different tasks from different sources:\\n* processing user requests (e.g., ingesting files, (re)generating derivatives)\\n* cleaning up after user actions (e.g., propagating state and visibility, removing files before deleting\\ntheir parent resource)\\n* refreshing metadata from Voyager\/PULFA\\n* bulk-ingesting content\\n* routine cleanup (cleaning out guest users)\\n* fixity checking\\n\n\n##Decision\nWe will have three queues for processing background jobs:\\n1. `default` for processing user-initiated requests needed for ingesting and displaying objects\\n2. `low` for bulk processing, validation, cleanup, etc.\\n3. `super_low` for fixity checking and other long-term preservation actions\\n","MatchedIds":[3361,4388,672,3497,1681],"Predicted":"## Decision\nWe will categorize and prioritize the different background job types and implement a job queue partitioning strategy in Sidekiq. By doing so, we can use separate queues to ensure critical jobs get processed promptly without being delayed by less critical tasks. Here are the steps and configurations that will be implemented:\n\n1. **Categorize Jobs**:\n   - **High Priority**: processing user requests, cleaning up after user actions, and refreshing metadata from Voyager\/PULFA.\n   - **Medium Priority**: bulk-ingesting content.\n   - **Low Priority**: routine cleanup, fixity checking.\n\n2. **Configure Sidekiq Queues**:\n   Define multiple queues in the `sidekiq.yml` file with different weights:\n   ```yaml\n   :queues:\n     - [critical, 10]\n     - [default, 6]\n     - [low, 2]\n   ```\n\n3. **Classify Jobs and Assign Queues**:\n   Assign individual jobs to their respective queues based on the categorization:\n   ```ruby\n   class IngestFileWorker\n     include Sidekiq::Worker\n     sidekiq_options queue: 'critical'\n     ...\n   end\n\n   class BulkIngestWorker\n     include Sidekiq::Worker\n     sidekiq_options queue: 'default'\n     ...\n   end\n\n   class CleanUpWorker\n     include Sidekiq::Worker\n     sidekiq_options queue: 'low'\n     ...\n   end\n   ```\n\n4. **Allocate Dedicated Worker Instances**:\n   Allocate dedicated worker instances or machines to process different queues based on the workload:\n   ```yaml\n   # Procfile\n   worker_high: bundle exec sidekiq -q critical\n   worker_medium: bundle exec sidekiq -q default\n   worker_low: bundle exec sidekiq -q low\n   ```\n\n5. **Monitor and Scale Dynamically**:\n   Implement monitoring, alerting, and autoscaling based on the queue metrics to ensure that the system scales appropriately with the workload.\n\nPros\n----\n* Ensures high-priority jobs are processed without delay.\n* Reduces the likelihood of critical job starvation.\n* Improves overall system reliability and performance.\n\nCons\n----\n* Requires additional configuration and maintenance.\n* Potentially higher resource usage due to multiple dedicated queues and workers.","GenTime":"2024-08-20 00:21:36"}
{"File Name":"datalab\/0012-public-and-private-subnets.md","Context":"## Context\\nWe would like to minimise our externally exposed footprint. To achieve this we have\\nalready decided that access to cluster servers will be via a Bastion server. We do need\\nto have some externally accessible services and need to decide how to achieve this.\\n","Decision":"We have decided to have external proxy servers for each environment that expose ports 80\\nand 443 to the public Internet by assigning a NAT in the vCloud environment. These proxy\\nservers will route traffic to the Kubernetes cluster services based on the Host Headers.\\n","tokens":55,"id":729,"text":"## Context\\nWe would like to minimise our externally exposed footprint. To achieve this we have\\nalready decided that access to cluster servers will be via a Bastion server. We do need\\nto have some externally accessible services and need to decide how to achieve this.\\n\n\n##Decision\nWe have decided to have external proxy servers for each environment that expose ports 80\\nand 443 to the public Internet by assigning a NAT in the vCloud environment. These proxy\\nservers will route traffic to the Kubernetes cluster services based on the Host Headers.\\n","MatchedIds":[745,731,4056,1182,3917],"Predicted":"## Decision\nWe have decided to use a reverse proxy server to manage externally accessible services. This proxy server will serve as an intermediary between external users and internal services, allowing us to expose only the necessary endpoints to the public while keeping direct access to our internal network restricted.\n\n### Implementation Details:\n1. **Reverse Proxy Setup**:\n   - We will deploy a reverse proxy (such as NGINX or HAProxy) in a DMZ (Demilitarized Zone) network segment, separate from the internal network hosting the cluster servers.\n   - The reverse proxy will be configured to forward traffic from external requests to the appropriate internal services.\n\n2. **Firewall Configuration**:\n   - The firewall will be configured to allow traffic only to the reverse proxy server on required ports (e.g., port 80 for HTTP and port 443 for HTTPS).\n   - Direct traffic to internal services will be blocked, ensuring that the reverse proxy is the only point of entry.\n\n3. **SSL\/TLS Termination**:\n   - The reverse proxy will handle SSL\/TLS termination, decrypting incoming traffic and forwarding it as HTTP to the internal services. This ensures secure communication with external clients while simplifying internal network traffic.\n\n4. **Service Authentication and Authorization**:\n   - Robust authentication and authorization mechanisms will be applied at the reverse proxy level to ensure only authorized users can access the exposed services.\n   - Options such as OAuth, JWT, or other token-based mechanisms will be considered based on the specific requirements of each service.\n\n5. **Monitoring and Logging**:\n   - All incoming and outgoing traffic through the reverse proxy will be subjected to thorough monitoring and logging to detect potential security threats and enable quick response to any incidents.\n\n6. **Scalability and Resilience**:\n   - The reverse proxy server will be deployed in a scalable and resilient architecture, using load balancers and auto-scaling groups where necessary, to handle increases in external traffic and maintain high availability.\n\nBy using a reverse proxy server, we can expose only the necessary services to the internet while preserving the overall security and integrity of our internal network.","GenTime":"2024-08-20 00:21:42"}
{"File Name":"Horace\/0007-use-herbert-as-library-dependency.md","Context":"## Context\\nThe Horace and Herbert projects are tightly coupled. Herbert build artifacts are required for integration and MATLAB testing of Horace source.\\nChanges made to Herbert may change or break dependent MATLAB or C++ code in Horace.\\n","Decision":"To make the depdencency explicit Herbert will be regarded as a library.\\nAs a consequence:\\n- Herbert builds will NOT trigger Horace builds\\n- Horace builds (both `PR` and `master`) will always use the latest `master` build of Herbert\\n- Build artifacts will will copied from the latest successful `master-<target-os>-<target-matlab>` build on the Herbert CI server.\\n","tokens":48,"id":4245,"text":"## Context\\nThe Horace and Herbert projects are tightly coupled. Herbert build artifacts are required for integration and MATLAB testing of Horace source.\\nChanges made to Herbert may change or break dependent MATLAB or C++ code in Horace.\\n\n\n##Decision\nTo make the depdencency explicit Herbert will be regarded as a library.\\nAs a consequence:\\n- Herbert builds will NOT trigger Horace builds\\n- Horace builds (both `PR` and `master`) will always use the latest `master` build of Herbert\\n- Build artifacts will will copied from the latest successful `master-<target-os>-<target-matlab>` build on the Herbert CI server.\\n","MatchedIds":[4234,3691,5171,128,5178],"Predicted":"## Decision\nTo ensure smooth integration and mitigate the risk of breaking changes, Horace and Herbert will be migrated to a monorepo structure. Continuous Integration (CI) processes will be set up to validate changes across both projects in a unified pipeline. This will include running comprehensive MATLAB and C++ tests for both Horace and Herbert, ensuring that any changes in Herbert are immediately tested for potential issues in Horace. Additionally, version-locking will be implemented so that any significant changes in Herbert can trigger version updates in Horace to maintain compatibility.","GenTime":"2024-08-20 00:21:45"}
{"File Name":"manuela\/0003-gitops-approach.md","Context":"##  Context\\nSee [2. Overall context and container platform](0002-overall-context-and-container-platform.md)\\n","Decision":"We use git to address the requirement of a data store:\\n* it is a replicated data store\\n* the hierarchy of deployment targets can be represented as hierarchy of directores in the git repository\\n* can host kubernetes manifests describing application deployments\\n* approval workflows, auditability, etc... is built in\\n* can create replicas with limited content, e.g. a regional DC only replicates what applies to the region with all its factories and line data servers, a factory DC then only replicates what applies to the factory dc and the line data servers, etc.\\n","tokens":25,"id":2968,"text":"##  Context\\nSee [2. Overall context and container platform](0002-overall-context-and-container-platform.md)\\n\n\n##Decision\nWe use git to address the requirement of a data store:\\n* it is a replicated data store\\n* the hierarchy of deployment targets can be represented as hierarchy of directores in the git repository\\n* can host kubernetes manifests describing application deployments\\n* approval workflows, auditability, etc... is built in\\n* can create replicas with limited content, e.g. a regional DC only replicates what applies to the region with all its factories and line data servers, a factory DC then only replicates what applies to the factory dc and the line data servers, etc.\\n","MatchedIds":[4096,631,3920,2752,74],"Predicted":"## Decision\nChosen option: \"Kubernetes as the container orchestration platform,\" because:\n\n* Kubernetes is an industry-standard for container orchestration.\n* Our teams have previous experience with Kubernetes which minimizes the learning curve.\n* Kubernetes provides extensive support for scaling, self-healing, and service discovery.\n* The platform enables better resource utilization and management through Kubernetes features such as pod scheduling, horizontal scaling, and namespaces.\n* Kubernetes' extensive ecosystem supports a broad range of tools and plugins, facilitating seamless integration with our existing CI\/CD pipeline.\n* Kubernetes supports our cloud-native architecture vision with versatile deployment options and high portability across different cloud service providers.\n\nThis decision aligns with our overall strategy of adopting industry-standard, well-supported, and highly flexible technologies.","GenTime":"2024-08-20 00:21:48"}
{"File Name":"pace-developers\/0005-pace-projects-must-be-semantically-versioned.md","Context":"## Context\\nThe PACE projects will evolve over time and breaking changes will be introduced. Users will need to be able to easily finding the correct documentation for their build.\\n[Semantic versioning](https:\/\/semver.org\/) defines a schema in which releases are given `major.minor.patch` version numbers where increments are made to the:\\n- `major` version with incompatible API changes,\\n- `minor` version when functionality is added in a backwards compatible manner, and\\n- `patch` version for backwards compatible bug fixes.\\nSphinx plugins are available that support multiple documentation versions ([sphinx-multiversion](https:\/\/pypi.org\/project\/sphinx-multiversion\/)) - this will not work correctly for Brille where documentation is extracted from the build artifacts.\\n","Decision":"Projects will be semantically versioned and documentation will be retained and be available for users to access for each major or minor version released.\\nThere is no requirement to have patch-release specific documentation.\\n","tokens":159,"id":5177,"text":"## Context\\nThe PACE projects will evolve over time and breaking changes will be introduced. Users will need to be able to easily finding the correct documentation for their build.\\n[Semantic versioning](https:\/\/semver.org\/) defines a schema in which releases are given `major.minor.patch` version numbers where increments are made to the:\\n- `major` version with incompatible API changes,\\n- `minor` version when functionality is added in a backwards compatible manner, and\\n- `patch` version for backwards compatible bug fixes.\\nSphinx plugins are available that support multiple documentation versions ([sphinx-multiversion](https:\/\/pypi.org\/project\/sphinx-multiversion\/)) - this will not work correctly for Brille where documentation is extracted from the build artifacts.\\n\n\n##Decision\nProjects will be semantically versioned and documentation will be retained and be available for users to access for each major or minor version released.\\nThere is no requirement to have patch-release specific documentation.\\n","MatchedIds":[5172,2951,1048,4424,5171],"Predicted":"## Decision\nWe will use Semantic Versioning to manage versioning for all PACE project documentation. For projects like Brille, where documentation is extracted from build artifacts, we will implement a custom solution to handle multiple documentation versions. This custom solution will generate and host documentation separately for each tagged release, ensuring that users can access the correct documentation corresponding to their build version. The documentation site will include a version selector to allow users to switch between different documentation versions easily.","GenTime":"2024-08-20 00:21:51"}
{"File Name":"report-a-defect\/0008-usegovuknotifyforemailing.md","Context":"## Context\\nThe issue motivating this decision, and any context that influences or constrains the decision.\\nThis service has a need to send emails and SMS to users. As Hackney is approved as an organisation that can use GOV.UK Notify we are able to use this service and take advantage of the reliability and low costs.\\nHackney already use Notify for other services, although it is not yet referenced in their playbook https:\/\/github.com\/LBHackney-IT\/API-Playbook\\n","Decision":"Use Notify over a third party service such as SendGrid for the sending of emails and SMS\\n","tokens":101,"id":5213,"text":"## Context\\nThe issue motivating this decision, and any context that influences or constrains the decision.\\nThis service has a need to send emails and SMS to users. As Hackney is approved as an organisation that can use GOV.UK Notify we are able to use this service and take advantage of the reliability and low costs.\\nHackney already use Notify for other services, although it is not yet referenced in their playbook https:\/\/github.com\/LBHackney-IT\/API-Playbook\\n\n\n##Decision\nUse Notify over a third party service such as SendGrid for the sending of emails and SMS\\n","MatchedIds":[996,2050,1786,1767,1784],"Predicted":"## Decision\nWe will use GOV.UK Notify to send both emails and SMS notifications for this service. Given that Hackney is approved to use this service and has already successfully implemented it in other contexts, we are confident in its reliability and cost-effectiveness.\n\nDespite GOV.UK Notify not being explicitly mentioned in Hackney's API Playbook, its proven track record within Hackney's services provides a strong basis for its adoption here. Steps will be taken to update the API Playbook to include references to GOV.UK Notify and provide guidelines on its integration and best practices.\n\nThis decision ensures consistency with other Hackney services and leverages a known, efficient solution for bulk messaging needs.","GenTime":"2024-08-20 00:21:55"}
{"File Name":"dapr\/ENG-003-test-infrastructure.md","Context":"## Context\\nE2E tests ensure the functional correctness in an e2e environment in order to make sure Dapr works with the user code deployments. The tests will be run before \/ after PR is merged or by a scheduler.\\nDapr E2E tests require the test infrastructure in order to not only test Dapr functionalities, but also show these test results in a consistent way. This document will decide how to bring up the test cluster, run the test, and report the test results.\\n","Decision":"### Test environments\\nAlthough Dapr is designed for multi cloud environments, e2e tests will be run under Kubernetes environments for now. We will support two different options to run e2e tests with local machine and CI on the pre-built Kubernetes cluster.\\n* **Local machine**. contributors or developers will use [Minikube](https:\/\/github.com\/kubernetes\/minikube) to validate their changes and run new tests before creating Pull Request.\\n* **Continuous Integration**. E2E tests will be run in the pre-built [Azure Kubernetes Service](https:\/\/azure.microsoft.com\/en-us\/services\/kubernetes-service\/) before\/after PR is merged or by a scheduler. Even if we will use [Azure Kubernetes Service](https:\/\/azure.microsoft.com\/en-us\/services\/kubernetes-service\/) in our test infrastructure, contributors should run e2e tests in any  RBAC-enabled Kubernetes clusters.\\n### Bring up test cluster\\nWe will provide the manual instruction or simple script to bring up test infrastructure unlike the other Kubernetes projects using [kubetest](https:\/\/github.com\/kubernetes\/test-infra\/tree\/master\/kubetest). Dapr E2E tests will clean up and revert all configurations in the cluster once the test is done. Without kubetest, we can create e2e tests simpler without the dependency of the 3rd party test frameworks, such as ginkgo, gomega.\\n### CI\/CD and test result report for tests\\nMany Kubernetes-related projects use [Prow](https:\/\/github.com\/kubernetes\/test-infra\/tree\/master\/prow), and [Testgrid](https:\/\/github.com\/kubernetes\/test-infra\/tree\/master\/testgrid) for Test CI, PR, and test result management. However, we will not use them to run Dapr E2E tests and share the test result since we need to self-host them on Google cloud platform.\\nInstead, Dapr will use [Azure Pipeline](https:\/\/azure.microsoft.com\/en-us\/services\/devops\/pipelines\/) to run e2e tests and its [test report feature](https:\/\/docs.microsoft.com\/en-us\/azure\/devops\/pipelines\/test\/review-continuous-test-results-after-build?view=azure-devops) without self-hosted CI and test report services. Even contributors can get their own azure pipelines accounts **for free** without self-hosting them.\\n","tokens":102,"id":70,"text":"## Context\\nE2E tests ensure the functional correctness in an e2e environment in order to make sure Dapr works with the user code deployments. The tests will be run before \/ after PR is merged or by a scheduler.\\nDapr E2E tests require the test infrastructure in order to not only test Dapr functionalities, but also show these test results in a consistent way. This document will decide how to bring up the test cluster, run the test, and report the test results.\\n\n\n##Decision\n### Test environments\\nAlthough Dapr is designed for multi cloud environments, e2e tests will be run under Kubernetes environments for now. We will support two different options to run e2e tests with local machine and CI on the pre-built Kubernetes cluster.\\n* **Local machine**. contributors or developers will use [Minikube](https:\/\/github.com\/kubernetes\/minikube) to validate their changes and run new tests before creating Pull Request.\\n* **Continuous Integration**. E2E tests will be run in the pre-built [Azure Kubernetes Service](https:\/\/azure.microsoft.com\/en-us\/services\/kubernetes-service\/) before\/after PR is merged or by a scheduler. Even if we will use [Azure Kubernetes Service](https:\/\/azure.microsoft.com\/en-us\/services\/kubernetes-service\/) in our test infrastructure, contributors should run e2e tests in any  RBAC-enabled Kubernetes clusters.\\n### Bring up test cluster\\nWe will provide the manual instruction or simple script to bring up test infrastructure unlike the other Kubernetes projects using [kubetest](https:\/\/github.com\/kubernetes\/test-infra\/tree\/master\/kubetest). Dapr E2E tests will clean up and revert all configurations in the cluster once the test is done. Without kubetest, we can create e2e tests simpler without the dependency of the 3rd party test frameworks, such as ginkgo, gomega.\\n### CI\/CD and test result report for tests\\nMany Kubernetes-related projects use [Prow](https:\/\/github.com\/kubernetes\/test-infra\/tree\/master\/prow), and [Testgrid](https:\/\/github.com\/kubernetes\/test-infra\/tree\/master\/testgrid) for Test CI, PR, and test result management. However, we will not use them to run Dapr E2E tests and share the test result since we need to self-host them on Google cloud platform.\\nInstead, Dapr will use [Azure Pipeline](https:\/\/azure.microsoft.com\/en-us\/services\/devops\/pipelines\/) to run e2e tests and its [test report feature](https:\/\/docs.microsoft.com\/en-us\/azure\/devops\/pipelines\/test\/review-continuous-test-results-after-build?view=azure-devops) without self-hosted CI and test report services. Even contributors can get their own azure pipelines accounts **for free** without self-hosting them.\\n","MatchedIds":[4847,1258,4179,1387,650],"Predicted":"## Decision\n\n### Test Infrastructure Setup\nWe will use Kubernetes to bring up the test cluster because it integrates well with Dapr. A dedicated namespace will be created for each E2E test run to isolate resources and facilitate clean-up.\n\n### Test Execution\n1. **Cluster Initialization:**\n    - Deploy a Kubernetes cluster using a managed service like Amazon EKS, Google GKE, or Azure AKS.\n    - Use Helm to install the Dapr runtime on the cluster.\n\n2. **Namespace Creation:**\n    - For each test run, a new Kubernetes namespace will be created. The namespace name will be derived from the PR number and branch name (e.g., `test-123-branch-name`).\n\n3. **Test Deployment:**\n    - Deploy the user code and Dapr sidecars in the newly created namespace using Kubernetes manifests or Helm charts.\n    - Trigger the E2E tests using a Kubernetes Job or a CI\/CD pipeline step.\n\n### Reporting Test Results\n1. **Test Result Collection:**\n    - Use a testing framework compatible with Dapr and Kubernetes, such as Ginkgo for Golang or Jest for Node.js. \n    - Collect test results in a standard format (e.g., JUnit XML).\n\n2. **Persistent Storage for Test Results:**\n    - Use a central S3 bucket (or equivalent) to store test artifacts and results.\n    - Each test run uploads its results to a dedicated directory named after the namespace.\n\n3. **CI\/CD Integration:**\n    - Integrate test execution and result reporting with the CI\/CD pipeline (e.g., using Jenkins, GitHub Actions, GitLab CI) to run tests on PR open, merge, or scheduled runs.\n    - Visualize test results in a consistent way using plugins or services like Allure or TestRail.\n\n### Environment Cleanup\n1. **Automatic Cleanup:**\n    - After the test execution, the Kubernetes namespace will be deleted automatically to clean up all resources.\n    - A scheduled job can also run periodically to ensure no stale namespaces or resources are lingering.\n\nBy combining Kubernetes for isolated and scalable environments, Helm for package management, and established CI\/CD tools for test scheduling and reporting, we ensure a robust and consistent E2E testing infrastructure for Dapr functionalities.","GenTime":"2024-08-20 00:22:07"}
{"File Name":"play-frontend-hmrc\/0012-create-an-endpoint-in-play-frontend-hmrc-for-surfacing-session-metadata.md","Context":"## Context and Problem Statement\\nTax users are sometimes, without warning, unnecessarily signed out when accessing\\nMDTP services using multiple browser tabs or windows. This is a violation of [WCAG 2.1\\nsuccess criterion 2.1.1 (Timing adjustable)](https:\/\/www.w3.org\/WAI\/WCAG21\/Understanding\/timing-adjustable.html).\\nThis problem is a consequence of the fact that a user's session can be refreshed via user activity in any\\ntab or window belonging to the same browser profile. However, the Javascript-powered [hmrcTimeoutDialog](https:\/\/github.com\/hmrc\/play-frontend-hmrc#warning-users-before-timing-them-out)\\ncurrently has no way of knowing about this activity following initial page load.\\nSolving this issue requires providing the timeout dialog component with knowledge of the actual time\\nremaining on the user's active session via an endpoint that is itself excluded from\\nsession management.\\nHow can we achieve this cost-effectively while minimising impact for service teams, limiting duplication of\\nknowledge and avoiding introducing additional coupling between frontend microservices?\\n## Decision Drivers\\n* The need to minimise code changes for service teams other than a library upgrade.\\n* The avoidance of requiring service teams to add any additional routing rules.\\n* The avoidance of requiring service teams to add any additional configuration.\\n* The need to limit duplication of knowledge between SessionTimeoutFilter in bootstrap-play and\\nany other library or service.\\n* The preference for maintaining loose coupling of frontend services and avoiding adding single points\\nof failure between frontend microservices.\\n* The need for the endpoint used to interrogate the session to not itself affect the session.\\n","Decision":"* The need to minimise code changes for service teams other than a library upgrade.\\n* The avoidance of requiring service teams to add any additional routing rules.\\n* The avoidance of requiring service teams to add any additional configuration.\\n* The need to limit duplication of knowledge between SessionTimeoutFilter in bootstrap-play and\\nany other library or service.\\n* The preference for maintaining loose coupling of frontend services and avoiding adding single points\\nof failure between frontend microservices.\\n* The need for the endpoint used to interrogate the session to not itself affect the session.\\nChosen option: option 9, because it is an option PlatUI and PlatOps agree on, is technically feasible,\\nsatisfies most of the decision drivers and is the smallest possible change with the least impact to service teams. The intention would be\\nto revisit option 7 (play-session) at a later date in order to address any outstanding concerns\\naround knowledge duplication.\\n","tokens":348,"id":560,"text":"## Context and Problem Statement\\nTax users are sometimes, without warning, unnecessarily signed out when accessing\\nMDTP services using multiple browser tabs or windows. This is a violation of [WCAG 2.1\\nsuccess criterion 2.1.1 (Timing adjustable)](https:\/\/www.w3.org\/WAI\/WCAG21\/Understanding\/timing-adjustable.html).\\nThis problem is a consequence of the fact that a user's session can be refreshed via user activity in any\\ntab or window belonging to the same browser profile. However, the Javascript-powered [hmrcTimeoutDialog](https:\/\/github.com\/hmrc\/play-frontend-hmrc#warning-users-before-timing-them-out)\\ncurrently has no way of knowing about this activity following initial page load.\\nSolving this issue requires providing the timeout dialog component with knowledge of the actual time\\nremaining on the user's active session via an endpoint that is itself excluded from\\nsession management.\\nHow can we achieve this cost-effectively while minimising impact for service teams, limiting duplication of\\nknowledge and avoiding introducing additional coupling between frontend microservices?\\n## Decision Drivers\\n* The need to minimise code changes for service teams other than a library upgrade.\\n* The avoidance of requiring service teams to add any additional routing rules.\\n* The avoidance of requiring service teams to add any additional configuration.\\n* The need to limit duplication of knowledge between SessionTimeoutFilter in bootstrap-play and\\nany other library or service.\\n* The preference for maintaining loose coupling of frontend services and avoiding adding single points\\nof failure between frontend microservices.\\n* The need for the endpoint used to interrogate the session to not itself affect the session.\\n\n\n##Decision\n* The need to minimise code changes for service teams other than a library upgrade.\\n* The avoidance of requiring service teams to add any additional routing rules.\\n* The avoidance of requiring service teams to add any additional configuration.\\n* The need to limit duplication of knowledge between SessionTimeoutFilter in bootstrap-play and\\nany other library or service.\\n* The preference for maintaining loose coupling of frontend services and avoiding adding single points\\nof failure between frontend microservices.\\n* The need for the endpoint used to interrogate the session to not itself affect the session.\\nChosen option: option 9, because it is an option PlatUI and PlatOps agree on, is technically feasible,\\nsatisfies most of the decision drivers and is the smallest possible change with the least impact to service teams. The intention would be\\nto revisit option 7 (play-session) at a later date in order to address any outstanding concerns\\naround knowledge duplication.\\n","MatchedIds":[1754,1654,3526,5211,4011],"Predicted":"## Decision\nImplement a shared library function in the `hmrcTimeoutDialog` that requests the actual time remaining on the user's active session from an endpoint that is excluded from session management. This will be achieved by creating a new endpoint in the `bootstrap-play` library that other services can use without additional routing rules or configurations.\n\n### Reasons for the Decision\n* **Minimising Code Changes:** Service teams will only need to perform a library upgrade to gain access to the new functionality, reducing the impact on individual services.\n* **Avoiding Additional Routing Rules:** By adding the endpoint to the `bootstrap-play` library, service teams do not need to add routing rules, simplifying integration.\n* **Avoiding Additional Configuration:** The default configuration provided by the library will mean service teams do not have to manage additional settings.\n* **Limiting Duplication of Knowledge:** The `hmrcTimeoutDialog` library will rely on the existing session management logic encapsulated in `bootstrap-play`, avoiding duplication.\n* **Maintaining Loose Coupling:** Using a shared library ensures that frontend services remain loosely coupled and do not introduce single points of failure.\n* **Non-Intrusive Endpoint:** The endpoint will be designed such that querying it does not refresh the session timer, thus providing an accurate measure of session time remaining without affecting user activity.\n\n### Positive Consequences\n* **Compliance with WCAG 2.1:** This approach directly addresses the compliance issue with WCAG 2.1 success criterion 2.1.1 by ensuring that users are not unexpectedly signed out.\n* **User Experience Improvement:** The session timeout dialog will now accurately reflect the user's active session status across multiple tabs or windows, leading to a better user experience.\n* **Ease of Integration:** Service teams will have a straightforward upgrade path with minimal changes required.\n* **Consistent Session Handling:** Centralising session time interrogation in the `bootstrap-play` library ensures a consistent approach across all services using the `hmrcTimeoutDialog`.\n\n### Negative Consequences\n* **Library Dependency:** Services will become dependent on the `bootstrap-play` library for this functionality, though this can be mitigated by ensuring the library is well-maintained and updated.\n* **Initial Upgrade Effort:** There will be an initial effort required by service teams to upgrade the library and test the new functionality. However, this is a one-time effort that brings long-term benefits.\n\n### Implementation Steps\n1. **Define Endpoint in `bootstrap-play`:** Implement a new non-session affecting endpoint in the `bootstrap-play` library that returns the remaining session time.\n2. **Update `hmrcTimeoutDialog`:** Modify the `hmrcTimeoutDialog` to query this new endpoint periodically to refresh its knowledge of the remaining session time.\n3. **Library Upgrade:** Ensure the upgraded versions of `bootstrap-play` and `hmrcTimeoutDialog` are released and well-documented.\n4. **Communication:** Communicate the changes and upgrade instructions to the service teams to facilitate a smooth transition.\n\nBy following this approach, we can efficiently solve the issue of unexpected sign-outs while maintaining minimal impact and avoiding additional complexities for service teams.","GenTime":"2024-08-20 00:22:26"}
{"File Name":"celestia-core\/adr-005-consensus-params.md","Context":"## Context\\nConsensus critical parameters controlling blockchain capacity have until now been hard coded, loaded from a local config, or neglected.\\nSince they may be need to be different in different networks, and potentially to evolve over time within\\nnetworks, we seek to initialize them in a genesis file, and expose them through the ABCI.\\nWhile we have some specific parameters now, like maximum block and transaction size, we expect to have more in the future,\\nsuch as a period over which evidence is valid, or the frequency of checkpoints.\\n","Decision":"### ConsensusParams\\nNo consensus critical parameters should ever be found in the `config.toml`.\\nA new `ConsensusParams` is optionally included in the `genesis.json` file,\\nand loaded into the `State`. Any items not included are set to their default value.\\nA value of 0 is undefined (see ABCI, below). A value of -1 is used to indicate the parameter does not apply.\\nThe parameters are used to determine the validity of a block (and tx) via the union of all relevant parameters.\\n```\\ntype ConsensusParams struct {\\nBlockSize\\nTxSize\\nBlockGossip\\n}\\ntype BlockSize struct {\\nMaxBytes int\\nMaxTxs int\\nMaxGas int\\n}\\ntype TxSize struct {\\nMaxBytes int\\nMaxGas int\\n}\\ntype BlockGossip struct {\\nBlockPartSizeBytes int\\n}\\n```\\nThe `ConsensusParams` can evolve over time by adding new structs that cover different aspects of the consensus rules.\\nThe `BlockPartSizeBytes` and the `BlockSize.MaxBytes` are enforced to be greater than 0.\\nThe former because we need a part size, the latter so that we always have at least some sanity check over the size of blocks.\\n### ABCI\\n#### InitChain\\nInitChain currently takes the initial validator set. It should be extended to also take parts of the ConsensusParams.\\nThere is some case to be made for it to take the entire Genesis, except there may be things in the genesis,\\nlike the BlockPartSize, that the app shouldn't really know about.\\n#### EndBlock\\nThe EndBlock response includes a `ConsensusParams`, which includes BlockSize and TxSize, but not BlockGossip.\\nOther param struct can be added to `ConsensusParams` in the future.\\nThe `0` value is used to denote no change.\\nAny other value will update that parameter in the `State.ConsensusParams`, to be applied for the next block.\\nTendermint should have hard-coded upper limits as sanity checks.\\n","tokens":111,"id":4099,"text":"## Context\\nConsensus critical parameters controlling blockchain capacity have until now been hard coded, loaded from a local config, or neglected.\\nSince they may be need to be different in different networks, and potentially to evolve over time within\\nnetworks, we seek to initialize them in a genesis file, and expose them through the ABCI.\\nWhile we have some specific parameters now, like maximum block and transaction size, we expect to have more in the future,\\nsuch as a period over which evidence is valid, or the frequency of checkpoints.\\n\n\n##Decision\n### ConsensusParams\\nNo consensus critical parameters should ever be found in the `config.toml`.\\nA new `ConsensusParams` is optionally included in the `genesis.json` file,\\nand loaded into the `State`. Any items not included are set to their default value.\\nA value of 0 is undefined (see ABCI, below). A value of -1 is used to indicate the parameter does not apply.\\nThe parameters are used to determine the validity of a block (and tx) via the union of all relevant parameters.\\n```\\ntype ConsensusParams struct {\\nBlockSize\\nTxSize\\nBlockGossip\\n}\\ntype BlockSize struct {\\nMaxBytes int\\nMaxTxs int\\nMaxGas int\\n}\\ntype TxSize struct {\\nMaxBytes int\\nMaxGas int\\n}\\ntype BlockGossip struct {\\nBlockPartSizeBytes int\\n}\\n```\\nThe `ConsensusParams` can evolve over time by adding new structs that cover different aspects of the consensus rules.\\nThe `BlockPartSizeBytes` and the `BlockSize.MaxBytes` are enforced to be greater than 0.\\nThe former because we need a part size, the latter so that we always have at least some sanity check over the size of blocks.\\n### ABCI\\n#### InitChain\\nInitChain currently takes the initial validator set. It should be extended to also take parts of the ConsensusParams.\\nThere is some case to be made for it to take the entire Genesis, except there may be things in the genesis,\\nlike the BlockPartSize, that the app shouldn't really know about.\\n#### EndBlock\\nThe EndBlock response includes a `ConsensusParams`, which includes BlockSize and TxSize, but not BlockGossip.\\nOther param struct can be added to `ConsensusParams` in the future.\\nThe `0` value is used to denote no change.\\nAny other value will update that parameter in the `State.ConsensusParams`, to be applied for the next block.\\nTendermint should have hard-coded upper limits as sanity checks.\\n","MatchedIds":[3764,4354,1558,28,1985],"Predicted":"## Decision\nWe will initialize consensus critical parameters in the genesis file and expose them via the ABCI interface. The initialization process will involve specifying these parameters within the genesis file, ensuring they can be modified to meet the needs of different networks and dynamically evolve over time.\n\n### Implementation Steps\n1. **Modify Genesis File:**\n    - Add fields for the current and future consensus parameters, including maximum block size, maximum transaction size, evidence validity period, and checkpoint frequency.\n    - Provide default values for each parameter in the genesis file.\n\n2. **Expose Parameters via ABCI:**\n    - Implement ABCI methods to query the current values of these parameters.\n    - Ensure any modifications to these parameters are validated and securely propagated throughout the network.\n\n3. **Parameter Update Mechanism:**\n    - Create a mechanism to update these parameters through a governance proposal and voting process, ensuring changes are agreed upon by the majority of the network participants.\n    - Introduce a versioning scheme to manage different versions of parameters and maintain compatibility.\n\n4. **Documentation and Validation:**\n    - Update documentation to provide clear guidelines on how to define and update these parameters in the genesis file.\n    - Implement validation checks to ensure parameters adhere to consensus rules and constraints before committing them to the blockchain.\n\nBy initiating consensus critical parameters in the genesis file and exposing them through the ABCI, we allow for greater flexibility, network-specific customization, and the ability to adapt to future requirements seamlessly. This approach ensures a resilient and scalable blockchain network.","GenTime":"2024-08-20 00:22:32"}
{"File Name":"up-fiscal-data\/001-source.md","Context":"## Context and Problem Statement\\nSource of data and information to be decided based on the requirements for **OBI: Phase 2** and **HAQ: GEWST** projects at CivicDataLab.\\n**_Platform 1_**\\n**Name** : Budget - Government of Uttar Pradesh\\n**URL** : http:\/\/budget.up.nic.in\/\\n**Usage** : Analyse budget documents and understand accounting information available.\\n**_Platform 2_**\\n**Name** : Koshvani web -- A Gateway to Finance Activities in the State of Uttar Pradesh\\n**URL** : http:\/\/koshvani.up.nic.in\/\\n**Usage** : Scrape, analyse and publish open budget data for public consumption.\\n## Decision Drivers\\n- Create better underdstanding Uttar Pradesh fiscal data\\n- Scrape data to be consumed in machine readable format\\n","Decision":"- Create better underdstanding Uttar Pradesh fiscal data\\n- Scrape data to be consumed in machine readable format\\nExplore both platforms to understand the open fiscal data available and techinical requirements.\\n","tokens":183,"id":446,"text":"## Context and Problem Statement\\nSource of data and information to be decided based on the requirements for **OBI: Phase 2** and **HAQ: GEWST** projects at CivicDataLab.\\n**_Platform 1_**\\n**Name** : Budget - Government of Uttar Pradesh\\n**URL** : http:\/\/budget.up.nic.in\/\\n**Usage** : Analyse budget documents and understand accounting information available.\\n**_Platform 2_**\\n**Name** : Koshvani web -- A Gateway to Finance Activities in the State of Uttar Pradesh\\n**URL** : http:\/\/koshvani.up.nic.in\/\\n**Usage** : Scrape, analyse and publish open budget data for public consumption.\\n## Decision Drivers\\n- Create better underdstanding Uttar Pradesh fiscal data\\n- Scrape data to be consumed in machine readable format\\n\n\n##Decision\n- Create better underdstanding Uttar Pradesh fiscal data\\n- Scrape data to be consumed in machine readable format\\nExplore both platforms to understand the open fiscal data available and techinical requirements.\\n","MatchedIds":[445,443,442,447,1863],"Predicted":"## Decision\n\nUtilize both platforms to meet the requirements for **OBI: Phase 2** and **HAQ: GEWST** projects at CivicDataLab:\n\n1. **Platform 1: Budget - Government of Uttar Pradesh**\n   - **Usage**: Analyze budget documents and understand accounting information.\n   - **Actions**:\n     - Extract budget documents and manually process key financial data points to establish a baseline understanding.\n     - Use the extracted information to cross-verify and validate data obtained from other sources.\n     - Create contextual reports and accounting information to support the analyses.\n\n2. **Platform 2: Koshvani web -- A Gateway to Finance Activities in the State of Uttar Pradesh**\n   - **Usage**: Scrape, analyze, and publish open budget data for public consumption.\n   - **Actions**:\n     - Implement web scraping using tools like Selenium to automate the extraction of fiscal data in a machine-readable format.\n     - Focus on extracting clean, structured data suitable for machine consumption.\n     - Publish the raw and processed datasets as open data to facilitate public access and transparency.\n\nBy leveraging both platforms, we ensure comprehensive data extraction, thereby achieving a better understanding of Uttar Pradesh's fiscal data and meeting the project's analytical and public reporting requirements.","GenTime":"2024-08-20 00:22:39"}
{"File Name":"dotcom-rendering\/018-react-context-api.md","Context":"# React Context API\\n## Context\\nWe don't use any state management in dotcom at the moment and this means props have to be\\nexplicitly passed around; this can lead to 'prop drilling'.\\n[This PR](https:\/\/github.com\/guardian\/dotcom-rendering\/pull\/801) was a spike to demonstrate using\\nthe react context api to extract the `edition` property to prevent this.\\n","Decision":"-   Our component tree is shallow so we shouldn't implement any 'magic' that\\nsteps away from having explicit props showing where and what data a component is using.\\n-   This decision should be revisited as the codebase grows.\\n","tokens":88,"id":2667,"text":"# React Context API\\n## Context\\nWe don't use any state management in dotcom at the moment and this means props have to be\\nexplicitly passed around; this can lead to 'prop drilling'.\\n[This PR](https:\/\/github.com\/guardian\/dotcom-rendering\/pull\/801) was a spike to demonstrate using\\nthe react context api to extract the `edition` property to prevent this.\\n\n\n##Decision\n-   Our component tree is shallow so we shouldn't implement any 'magic' that\\nsteps away from having explicit props showing where and what data a component is using.\\n-   This decision should be revisited as the codebase grows.\\n","MatchedIds":[1854,334,2298,4596,4613],"Predicted":"## Decision\nWe will adopt the React Context API to manage and share the `edition` property across components. This will prevent the need for prop drilling and promote a cleaner, more maintainable codebase. Components that require the `edition` data can now access it directly from the context, reducing the complexity of passing props through multiple levels of the component tree.","GenTime":"2024-08-20 00:22:42"}
{"File Name":"princeton_ansible\/0004-key-scope","Context":"## Context\\nIn many cases we can generate keys or tokens as-needed. We may also be able to create keys with varying permissions.\\nWe'd like to reduce our vulnerability in the case of one of these keys becoming compromised, either via a compromised box or via a software exploit to application code.\\n","Decision":"Keys should be minted on a per-feature, per-application basis.\\n","tokens":61,"id":2256,"text":"## Context\\nIn many cases we can generate keys or tokens as-needed. We may also be able to create keys with varying permissions.\\nWe'd like to reduce our vulnerability in the case of one of these keys becoming compromised, either via a compromised box or via a software exploit to application code.\\n\n\n##Decision\nKeys should be minted on a per-feature, per-application basis.\\n","MatchedIds":[4848,211,13,4346,1721],"Predicted":"## Decision\nTo mitigate the risk of key or token compromise, we will implement the following measures:\n\n1. **Least Privilege Principle**:\n   - **Configure Fine-Grained Permissions**: Generate keys or tokens with the minimal required permissions for specific actions or applications to reduce the potential impact of a compromise.\n   - **Review and Adjust Permissions Regularly**: Continuously review and adjust permissions based on actual usage patterns and evolving security requirements.\n\n2. **Key\/Token Rotations**:\n   - **Automatic Rotation**: Implement automatic key\/token rotation mechanisms to ensure keys\/tokens do not remain static and become less vulnerable to long-term attacks.\n   - **Rotation Alerts**: Configure alerts to notify administrators of pending rotation deadlines to ensure timely action.\n\n3. **Short-Lived Tokens**:\n   - **Utilize Short-Lived Tokens**: Where applicable, use short-lived tokens to limit the window of opportunity for misuse if compromised.\n   - **Renewal Process**: Implement seamless renewal processes to minimize disruption due to short token lifespans.\n\n4. **Logging and Monitoring**:\n   - **Implement Comprehensive Logging**: Log all key\/token usage activities and access attempts to maintain an audit trail.\n   - **Real-Time Monitoring**: Integrate real-time monitoring and anomaly detection to identify and respond to suspicious activity rapidly.\n\n5. **Environment Segregation**:\n   - **Dev\/Staging\/Prod Environment Isolation**: Isolate keys\/tokens by environment (development, staging, production) to mitigate the impact of a compromise in a non-production environment.\n\n6. **Encryption at Rest and Transit**:\n   - **Ensure Encryption**: Encrypt keys\/tokens both at rest and in transit using industry-standard encryption mechanisms.\n\n7. **Expiration and Revocation Policies**:\n   - **Set Expiration Dates**: Assign expiration dates to keys\/tokens to limit their validity period.\n   - **Revocation Mechanisms**: Implement easy-to-use revocation mechanisms to quickly invalidate compromised keys\/tokens.\n\nBy incorporating the above strategies, we will enhance our security posture by minimizing potential vulnerabilities associated with key\/token compromise.","GenTime":"2024-08-20 00:22:50"}
{"File Name":"island.is\/0012-chart-library.md","Context":"## Context and Problem Statement\\nMultiple projects need to show data visually using charts and graphs. In order to provide unified look and feel across island.is we should commit to a single approach to implementing charts, i.e. choose one library for the whole repository.\\n## Decision Drivers\\n- Meet all requirements listed above\\n- API quality\\n- Pricing\\n- Bundle size\\n- Typescript support\\n","Decision":"- Meet all requirements listed above\\n- API quality\\n- Pricing\\n- Bundle size\\n- Typescript support\\nChosen option: \"Recharts\", because it meets all requirements, and overall has a very nice, dev-friendly API. It is the most popular (downloads per week) react charting library on github, and recommended across the community. We can customize how it looks, and start using it quickly without much groundwork.\\n### Positive Consequences\\n- We can start implementing charts and graphs as needed in island.is\\n### Negative Consequences\\n- It is a big dependency, but almost all chart libraries are big due to their nature. We will minimize the impact of this by enforcing charts to be lazy loaded in our codebase.\\n","tokens":79,"id":1119,"text":"## Context and Problem Statement\\nMultiple projects need to show data visually using charts and graphs. In order to provide unified look and feel across island.is we should commit to a single approach to implementing charts, i.e. choose one library for the whole repository.\\n## Decision Drivers\\n- Meet all requirements listed above\\n- API quality\\n- Pricing\\n- Bundle size\\n- Typescript support\\n\n\n##Decision\n- Meet all requirements listed above\\n- API quality\\n- Pricing\\n- Bundle size\\n- Typescript support\\nChosen option: \"Recharts\", because it meets all requirements, and overall has a very nice, dev-friendly API. It is the most popular (downloads per week) react charting library on github, and recommended across the community. We can customize how it looks, and start using it quickly without much groundwork.\\n### Positive Consequences\\n- We can start implementing charts and graphs as needed in island.is\\n### Negative Consequences\\n- It is a big dependency, but almost all chart libraries are big due to their nature. We will minimize the impact of this by enforcing charts to be lazy loaded in our codebase.\\n","MatchedIds":[4689,1117,701,3718,381],"Predicted":"## Decision\nChosen option: Chart.js, because:\n\n- **Meet all requirements listed above**: Chart.js offers a wide range of charts and customization options to meet the diverse needs of multiple projects.\n- **API quality**: It has a well-documented and intuitive API, making it easy for developers to use and integrate.\n- **Pricing**: Chart.js is open-source and free to use, eliminating any licensing costs.\n- **Bundle size**: It is relatively lightweight compared to other charting libraries, which helps to keep the bundle size minimal.\n- **TypeScript support**: Excellent TypeScript support with types available out-of-the-box, making it easier to develop and maintain.\n\n### Positive Consequences\n- **Consistency**: Helps maintain a consistent look and feel across different projects.\n- **Ease of Use**: Well-documented API and TypeScript support make it easier for developers to implement and maintain.\n- **Cost Effective**: Being open-source, it avoids additional costs.\n\n### Negative consequences\n- **Customization Limitations**: While Chart.js offers a lot of flexibility, there may be some highly specific use cases where more specialized charting libraries might be more suitable.\n- **Learning Curve**: Although Chart.js is well-documented, developers who are unfamiliar with it might need some time to get up to speed.","GenTime":"2024-08-20 00:22:58"}
{"File Name":"nso.aurora\/MicroserviceDesgin.md","Context":"## Context\\nTo ensure scalability, reliability and performance, microservice architecture should be used\\n","Decision":"All agreed with this decision\\n","tokens":18,"id":302,"text":"## Context\\nTo ensure scalability, reliability and performance, microservice architecture should be used\\n\n\n##Decision\nAll agreed with this decision\\n","MatchedIds":[3488,3483,2502,1094,4896],"Predicted":"## Decision\nWe will adopt a microservice architecture to ensure scalability, reliability, and performance. Each microservice will be designed as an independently deployable unit, focusing on a specific business capability. This approach will allow for horizontal scaling, fault isolation, and the ability to deploy updates to individual services without impacting the entire system. Additionally, we will leverage containerization technologies like Docker and orchestration platforms like Kubernetes to manage service deployment, scaling, and resilience.","GenTime":"2024-08-20 00:23:23"}
{"File Name":"saas-platform-frontend\/0003-use-javascript-over-typescript.md","Context":"## Context\\nI need to decide in which language I implement the frontend.\\n","Decision":"I use Javascript.\\n","tokens":16,"id":3715,"text":"## Context\\nI need to decide in which language I implement the frontend.\\n\n\n##Decision\nI use Javascript.\\n","MatchedIds":[1240,472,3721,1243,1239],"Predicted":"## Decision\nWe will implement the frontend in JavaScript using React. JavaScript is the most widely used language for web frontends and React is a popular library with a large community and extensive documentation. This will enable faster development and easier onboarding of new developers. Additionally, React's component-based architecture will help us build a maintainable and scalable application.","GenTime":"2024-08-20 00:23:27"}
{"File Name":"architecture-decision-log\/0002-github-template-for-issues-and-pull-requests.md","Context":"* [Context](#context)\\n* [Decision](#decision)\\n* [Status](#status)\\n* [Consequences](#consequences)\\n","Decision":"* [Status](#status)\\n* [Consequences](#consequences)\\nAn easy way to improve our communication is by creating templates for both `issues` and `pull requests`. Those templates must be simple, easy to use, and express all required sections for our developers and users.\\nYou can check the suggested templates [in the following folder](..\/assets\/0002-github-template-for-issues-and-pull-requests).\\n","tokens":34,"id":4919,"text":"* [Context](#context)\\n* [Decision](#decision)\\n* [Status](#status)\\n* [Consequences](#consequences)\\n\n\n##Decision\n* [Status](#status)\\n* [Consequences](#consequences)\\nAn easy way to improve our communication is by creating templates for both `issues` and `pull requests`. Those templates must be simple, easy to use, and express all required sections for our developers and users.\\nYou can check the suggested templates [in the following folder](..\/assets\/0002-github-template-for-issues-and-pull-requests).\\n","MatchedIds":[4547,4910,4918,4544,4550],"Predicted":"* [Context](#context)\\nThe team is currently considering ways to improve the overall performance and scalability of our web application. Given the increase in traffic and user demand, our current monolithic architecture is showing signs of strain. We need a solution that allows for better resource allocation, easier maintenance, and the ability to scale specific parts of the application independently.\n\n* [Decision](#decision)\\nWe've decided to move towards a microservices architecture. This involves breaking down the monolithic application into smaller, independent services that communicate over standardized interfaces. Each service will be responsible for a specific piece of functionality and will be deployed independently, allowing for greater flexibility and scalability.\n\n* [Status](#status)\\nApproved\n\n* [Consequences](#consequences)\\nMoving to a microservices architecture will have several impacts:\n  * Positive:\n    * Improved scalability: Each microservice can be scaled independently based on its specific demands.\n    * Better fault isolation: A failure in one microservice is less likely to impact the entire system.\n    * Flexibility in technology choices: Teams can choose the best technology stack for each microservice.\n  * Negative:\n    * Increased complexity: Managing a distributed system can introduce challenges such as network latency, security, and data consistency.\n    * Deployment overhead: Deploying and managing multiple microservices requires robust CI\/CD pipelines and comprehensive monitoring.\n\n* [More reading](#more-reading)\\nFor a deeper understanding of microservices and best practices in implementing them, refer to [Building Microservices by Sam Newman](https:\/\/www.oreilly.com\/library\/view\/building-microservices\/9781491950340\/).\\n\n\n* [Updates](#updates)\\nRegular updates will be provided as the migration to the microservices architecture progresses.","GenTime":"2024-08-20 00:23:32"}
{"File Name":"mlmc\/0002-use-mongodb-for-operation.md","Context":"## Context and Problem Statement\\nOperation API need some data storage to be able to list\/commission\/decommission\/launch missiles.\\n## Decision Drivers\\n* Easy to use storage\\n* Fast storage\\n* Dynamic and\/or schemaless storage\\n","Decision":"* Easy to use storage\\n* Fast storage\\n* Dynamic and\/or schemaless storage\\nDecided to use MongoDB since\\n* The product growths without strict requirements, so entitites are changed very often. In this circumstances schemaless approach is better than predefined schema approach\\n* No need to use RDBMS just to store some random missiless on a stock\\n* I wanted to get some experience in MongoDB\\n### Positive Consequences\\n* Implementation will be faster and easier since schema is going to be changed lot of times\\n### Negative Consequences\\n* n\/a\\n","tokens":48,"id":1574,"text":"## Context and Problem Statement\\nOperation API need some data storage to be able to list\/commission\/decommission\/launch missiles.\\n## Decision Drivers\\n* Easy to use storage\\n* Fast storage\\n* Dynamic and\/or schemaless storage\\n\n\n##Decision\n* Easy to use storage\\n* Fast storage\\n* Dynamic and\/or schemaless storage\\nDecided to use MongoDB since\\n* The product growths without strict requirements, so entitites are changed very often. In this circumstances schemaless approach is better than predefined schema approach\\n* No need to use RDBMS just to store some random missiless on a stock\\n* I wanted to get some experience in MongoDB\\n### Positive Consequences\\n* Implementation will be faster and easier since schema is going to be changed lot of times\\n### Negative Consequences\\n* n\/a\\n","MatchedIds":[4747,2520,4859,1167,4285],"Predicted":"## Decision\nBased on the decision drivers:\n\n* **Easy to use storage**\n* **Fast storage**\n* **Dynamic and\/or schemaless storage**\n\nChosen option: **MongoDB**, because it is a widely-used NoSQL database known for its ease of use, high performance, and flexible schema design.\n\n### Positive Consequences\n* Development speed should increase due to MongoDB's flexible schema and JSON-like documents.\n* High read and write performance will ensure fast operations for mission-critical tasks.\n* The dynamic schema will allow for easy modifications and iterations on the data model as operational requirements evolve.\n\n### Negative Consequences\n* Lack of strong transactional support could be a concern for operations requiring atomicity.\n* Depending on the amount of data and number of queries, scaling horizontally might require additional architectural considerations.\n\nBy using MongoDB, the operation API can efficiently handle the storage needs related to missile management with a flexible and robust solution.","GenTime":"2024-08-20 00:23:48"}
{"File Name":"james-project\/0031-distributed-mail-queue.md","Context":"## Context\\nMailQueue is a central component of SMTP infrastructure allowing asynchronous mail processing. This enables a short\\nSMTP reply time despite a potentially longer mail processing time. It also works as a buffer during SMTP peak workload\\nto not overload a server.\\nFurthermore, when used as a Mail Exchange server (MX), the ability to add delays to be observed before dequeing elements\\nallows, among others:\\n- Delaying retries upon MX delivery failure to a remote site.\\n- Throttling, which could be helpful for not being considered a spammer.\\nA mailqueue also enables advanced administration operations like traffic review, discarding emails, resetting wait\\ndelays, purging the queue, etc.\\nSpring implementation and non distributed implementations rely on an embedded ActiveMQ to implement the MailQueue.\\nEmails are being stored in a local file system. An administrator wishing to administrate the mailQueue will thus need\\nto interact with all its James servers, which is not friendly in a distributed setup.\\nDistributed James relies on the following third party softwares (among other):\\n- **RabbitMQ** for messaging. Good at holding a queue, however some advanced administrative operations can't be\\nimplemented with this component alone. This is the case for `browse`, `getSize` and `arbitrary mail removal`.\\n- **Cassandra** is the metadata database. Due to **tombstone** being used for delete, queue is a well known anti-pattern.\\n- **ObjectStorage** (Swift or S3) holds byte content.\\n","Decision":"Distributed James should ship a distributed MailQueue composing the following softwares with the following\\nresponsibilities:\\n- **RabbitMQ** for messaging. A rabbitMQ consumer will trigger dequeue operations.\\n- A time series projection of the queue content (order by time list of mail metadata) will be maintained in **Cassandra** (see later). Time series avoid the\\naforementioned tombstone anti-pattern, and no polling is performed on this projection.\\n- **ObjectStorage** (Swift or S3) holds large byte content. This avoids overwhelming other softwares which do not scale\\nas well in term of Input\/Output operation per seconds.\\nHere are details of the tables composing Cassandra MailQueue View data-model:\\n- **enqueuedMailsV3** holds the time series. The primary key holds the queue name, the (rounded) time of enqueue\\ndesigned as a slice, and a bucketCount. Slicing enables listing a large amount of items from a given point in time, in an\\nfashion that is not achievable with a classic partition approach. The bucketCount enables sharding and avoids all writes\\nat a given point in time to go to the same Cassandra partition. The clustering key is composed of an enqueueId - a\\nunique identifier. The content holds the metadata of the email. This table enables, from a starting date, to load all of\\nthe emails that have ever been in the mailQueue. Its content is never deleted.\\n- **deletedMailsV2** tells wether a mail stored in *enqueuedMailsV3* had been deleted or not. The queueName and\\nenqueueId are used as primary key. This table is updated upon dequeue and deletes. This table is queried upon dequeue\\nto filter out deleted\/purged items.\\n- **browseStart** store the latest known point in time from which all previous emails had been deleted\/dequeued. It\\nenables to skip most deleted items upon browsing\/deleting queue content. Its update is probability based and\\nasynchronously piggy backed on dequeue.\\nHere are the main mail operation sequences:\\n- Upon **enqueue** mail content is stored in the *object storage*, an entry is added in *enqueuedMailsV3* and a message\\nis fired on *rabbitMQ*.\\n- **dequeue** is triggered by a rabbitMQ message to be received. *deletedMailsV2* is queried to know if the message had\\nalready been deleted. If not, the mail content is retrieved from the *object storage*, then an entry is added in\\n*deletedMailsV2* to notice the email had been dequeued. A dequeue has a random probability to trigger a browse start\\nupdate. If so, from current browse start, *enqueuedMailsV3* content is iterated, and checked against *deletedMailsV2*\\nuntil the first non deleted \/ dequeued email is found. This point becomes the new browse start. BrowseStart can never\\npoint after the start of the current slice. A grace period upon browse start update is left to tolerate clock skew.\\nUpdate of the browse start is done randomly as it is a simple way to avoid synchronisation in a distributed system: we\\nensure liveness while uneeded browseStart updates being triggered would simply waste a few resources.\\n- Upon **browse**, *enqueuedMailsV3* content is iterated, and checked against *deletedMailsV2*, starting from the\\ncurrent browse start.\\n- Upon **delete\/purge**, *enqueuedMailsV3* content is iterated, and checked against *deletedMailsV2*. Mails matching\\nthe condition are marked as deleted in *enqueuedMailsV3*.\\n- Upon **getSize**, we perform a browse and count the returned elements.\\nThe distributed mail queue requires a fine tuned configuration, which mostly depends of the count of Cassandra servers,\\nand of the mailQueue throughput:\\n- **sliceWindow** is the time period of a slice. All the elements of **enqueuedMailsV3** sharing the same slice are\\nretrieved at once. The bigger, the more elements are going to be read at once, the less frequent browse start update\\nwill be. Lower values might result in many almost empty slices to be read, generating higher read load. We recommend\\n**sliceWindow** to be chosen from users maximum throughput so that approximately 10.000 emails be contained in a slice.\\nOnly values dividing the current *sliceWindow* are allowed as new values (otherwize previous slices might not be found).\\n- **bucketCount** enables spreading the writes in your Cassandra cluster using a bucketting strategy. Low values will\\nlead to workload not to be spread evenly, higher values might result in uneeded reads upon browse. The count of Cassandra\\nservers should be a good starting value. Only increasing the count of buckets is supported as a configuration update as\\ndecreasing the bucket count might result in some buckets to be lost.\\n- **updateBrowseStartPace** governs the probability of updating browseStart upon dequeue\/deletes. We recommend choosing\\na value guarantying a reasonable probability of updating the browse start every few slices. Too big values will lead to\\nuneeded update of not yet finished slices. Too low values will end up in a more expensive browseStart update and browse\\niterating through slices with all their content deleted. This value can be changed freely.\\nWe rely on eventSourcing to validate the mailQueue configuration changes upon James start following the aforementioned rules.\\n","tokens":321,"id":2892,"text":"## Context\\nMailQueue is a central component of SMTP infrastructure allowing asynchronous mail processing. This enables a short\\nSMTP reply time despite a potentially longer mail processing time. It also works as a buffer during SMTP peak workload\\nto not overload a server.\\nFurthermore, when used as a Mail Exchange server (MX), the ability to add delays to be observed before dequeing elements\\nallows, among others:\\n- Delaying retries upon MX delivery failure to a remote site.\\n- Throttling, which could be helpful for not being considered a spammer.\\nA mailqueue also enables advanced administration operations like traffic review, discarding emails, resetting wait\\ndelays, purging the queue, etc.\\nSpring implementation and non distributed implementations rely on an embedded ActiveMQ to implement the MailQueue.\\nEmails are being stored in a local file system. An administrator wishing to administrate the mailQueue will thus need\\nto interact with all its James servers, which is not friendly in a distributed setup.\\nDistributed James relies on the following third party softwares (among other):\\n- **RabbitMQ** for messaging. Good at holding a queue, however some advanced administrative operations can't be\\nimplemented with this component alone. This is the case for `browse`, `getSize` and `arbitrary mail removal`.\\n- **Cassandra** is the metadata database. Due to **tombstone** being used for delete, queue is a well known anti-pattern.\\n- **ObjectStorage** (Swift or S3) holds byte content.\\n\n\n##Decision\nDistributed James should ship a distributed MailQueue composing the following softwares with the following\\nresponsibilities:\\n- **RabbitMQ** for messaging. A rabbitMQ consumer will trigger dequeue operations.\\n- A time series projection of the queue content (order by time list of mail metadata) will be maintained in **Cassandra** (see later). Time series avoid the\\naforementioned tombstone anti-pattern, and no polling is performed on this projection.\\n- **ObjectStorage** (Swift or S3) holds large byte content. This avoids overwhelming other softwares which do not scale\\nas well in term of Input\/Output operation per seconds.\\nHere are details of the tables composing Cassandra MailQueue View data-model:\\n- **enqueuedMailsV3** holds the time series. The primary key holds the queue name, the (rounded) time of enqueue\\ndesigned as a slice, and a bucketCount. Slicing enables listing a large amount of items from a given point in time, in an\\nfashion that is not achievable with a classic partition approach. The bucketCount enables sharding and avoids all writes\\nat a given point in time to go to the same Cassandra partition. The clustering key is composed of an enqueueId - a\\nunique identifier. The content holds the metadata of the email. This table enables, from a starting date, to load all of\\nthe emails that have ever been in the mailQueue. Its content is never deleted.\\n- **deletedMailsV2** tells wether a mail stored in *enqueuedMailsV3* had been deleted or not. The queueName and\\nenqueueId are used as primary key. This table is updated upon dequeue and deletes. This table is queried upon dequeue\\nto filter out deleted\/purged items.\\n- **browseStart** store the latest known point in time from which all previous emails had been deleted\/dequeued. It\\nenables to skip most deleted items upon browsing\/deleting queue content. Its update is probability based and\\nasynchronously piggy backed on dequeue.\\nHere are the main mail operation sequences:\\n- Upon **enqueue** mail content is stored in the *object storage*, an entry is added in *enqueuedMailsV3* and a message\\nis fired on *rabbitMQ*.\\n- **dequeue** is triggered by a rabbitMQ message to be received. *deletedMailsV2* is queried to know if the message had\\nalready been deleted. If not, the mail content is retrieved from the *object storage*, then an entry is added in\\n*deletedMailsV2* to notice the email had been dequeued. A dequeue has a random probability to trigger a browse start\\nupdate. If so, from current browse start, *enqueuedMailsV3* content is iterated, and checked against *deletedMailsV2*\\nuntil the first non deleted \/ dequeued email is found. This point becomes the new browse start. BrowseStart can never\\npoint after the start of the current slice. A grace period upon browse start update is left to tolerate clock skew.\\nUpdate of the browse start is done randomly as it is a simple way to avoid synchronisation in a distributed system: we\\nensure liveness while uneeded browseStart updates being triggered would simply waste a few resources.\\n- Upon **browse**, *enqueuedMailsV3* content is iterated, and checked against *deletedMailsV2*, starting from the\\ncurrent browse start.\\n- Upon **delete\/purge**, *enqueuedMailsV3* content is iterated, and checked against *deletedMailsV2*. Mails matching\\nthe condition are marked as deleted in *enqueuedMailsV3*.\\n- Upon **getSize**, we perform a browse and count the returned elements.\\nThe distributed mail queue requires a fine tuned configuration, which mostly depends of the count of Cassandra servers,\\nand of the mailQueue throughput:\\n- **sliceWindow** is the time period of a slice. All the elements of **enqueuedMailsV3** sharing the same slice are\\nretrieved at once. The bigger, the more elements are going to be read at once, the less frequent browse start update\\nwill be. Lower values might result in many almost empty slices to be read, generating higher read load. We recommend\\n**sliceWindow** to be chosen from users maximum throughput so that approximately 10.000 emails be contained in a slice.\\nOnly values dividing the current *sliceWindow* are allowed as new values (otherwize previous slices might not be found).\\n- **bucketCount** enables spreading the writes in your Cassandra cluster using a bucketting strategy. Low values will\\nlead to workload not to be spread evenly, higher values might result in uneeded reads upon browse. The count of Cassandra\\nservers should be a good starting value. Only increasing the count of buckets is supported as a configuration update as\\ndecreasing the bucket count might result in some buckets to be lost.\\n- **updateBrowseStartPace** governs the probability of updating browseStart upon dequeue\/deletes. We recommend choosing\\na value guarantying a reasonable probability of updating the browse start every few slices. Too big values will lead to\\nuneeded update of not yet finished slices. Too low values will end up in a more expensive browseStart update and browse\\niterating through slices with all their content deleted. This value can be changed freely.\\nWe rely on eventSourcing to validate the mailQueue configuration changes upon James start following the aforementioned rules.\\n","MatchedIds":[2888,2896,2145,2880,2897],"Predicted":"## Decision\nImplement a hybrid solution combining RabbitMQ for real-time message queuing and a distributed database (Cassandra) for storing metadata about the messages. To accommodate for the anti-pattern issue with Cassandra's tombstone, we will leverage time-based compaction strategies and TTLs.\n\nKey points:\n- **RabbitMQ** will continue to be used for holding the queue, handling the real-time aspects like dequeuing and delaying retry mechanisms.\n- **Cassandra** will be used to store metadata about emails like their status, retry counts, and timestamps. This data will support administrative operations such as `browse`, `getSize`, and `arbitrary mail removal`. We will implement time-based compaction strategies and TTLs to handle the anti-pattern issue related to tombstones.\n- **ObjectStorage** (S3 or Swift) will persist the actual email content.\n\nBy separating the responsibilities:\n- **RabbitMQ** deals with real-time processing and management of the queue.\n- **Cassandra** will manage email metadata, enabling administrative operations.\n- **ObjectStorage** will handle the storage of email content.\n\nThis setup enhances manageability in a distributed environment, allowing administrators to manage and monitor the MailQueue seamlessly across multiple James servers.","GenTime":"2024-08-20 00:23:53"}
{"File Name":"adr\/ADR-6-git-style-guide.md","Context":"## Context or problem to solve\\n* How should git histories look like?\\n* What format should we use to write commit messages?\\n","Decision":"We agreed on normalizing commit messages to master branches to avoid things like `Merge pull request #15 from client\/menduz-patch-10` in persuit of more semantic messages like `fix: commit style guide, closes #15`. That is particularly helpful in repositories with several contributors and fosters professionalism in open source repositories.\\n### Branches\\nWhen you work on a branch on a specific issue, we keep the spirit of [semantic branch naming](https:\/\/medium.com\/@hanuman_95739\/how-to-integrate-branch-naming-commit-message-and-push-rules-in-gitlab-fe9cd642cc1a). Think of this as writing what is and what you are doing in a three word sentence The first one must be oune of the list. For instance:\\n```\\nfix\/wrong_host\\n^  ^^------------^\\n|  ||\\n|  |+----> Summary in present tense.\\n|  +-----> Slash\\n+--------> Type: chore, docs, feat, fix, refactor, style, or test.\\n```\\nOther examples are:\\n```\\ndocs\/update_readme\\nrefactor\/new_welcome_message\\n```\\nLook for the *Examples* in section *Commit messages* for a description of the allowed branch types.\\nIt's OK to use hyphens (`-`) or underscores (`_`) to replace spaces. Avoid any other special characters, like `#` or `$`, as they might lead to problems, for example, when deploying the content using the branch name as part of the URL. The branch name should match this regexp: `(chore|docs|feat|fix|refactor|style|test)\/[0-9a-zA-Z_-]+`\\n#### Exceptions:\\nThis rules are ignored when you work on *environment* branches like `master`, `development`, `staging` and so on.\\n","tokens":28,"id":4600,"text":"## Context or problem to solve\\n* How should git histories look like?\\n* What format should we use to write commit messages?\\n\n\n##Decision\nWe agreed on normalizing commit messages to master branches to avoid things like `Merge pull request #15 from client\/menduz-patch-10` in persuit of more semantic messages like `fix: commit style guide, closes #15`. That is particularly helpful in repositories with several contributors and fosters professionalism in open source repositories.\\n### Branches\\nWhen you work on a branch on a specific issue, we keep the spirit of [semantic branch naming](https:\/\/medium.com\/@hanuman_95739\/how-to-integrate-branch-naming-commit-message-and-push-rules-in-gitlab-fe9cd642cc1a). Think of this as writing what is and what you are doing in a three word sentence The first one must be oune of the list. For instance:\\n```\\nfix\/wrong_host\\n^  ^^------------^\\n|  ||\\n|  |+----> Summary in present tense.\\n|  +-----> Slash\\n+--------> Type: chore, docs, feat, fix, refactor, style, or test.\\n```\\nOther examples are:\\n```\\ndocs\/update_readme\\nrefactor\/new_welcome_message\\n```\\nLook for the *Examples* in section *Commit messages* for a description of the allowed branch types.\\nIt's OK to use hyphens (`-`) or underscores (`_`) to replace spaces. Avoid any other special characters, like `#` or `$`, as they might lead to problems, for example, when deploying the content using the branch name as part of the URL. The branch name should match this regexp: `(chore|docs|feat|fix|refactor|style|test)\/[0-9a-zA-Z_-]+`\\n#### Exceptions:\\nThis rules are ignored when you work on *environment* branches like `master`, `development`, `staging` and so on.\\n","MatchedIds":[4899,4890,3521,2703,160],"Predicted":"## Decision\n**We will adopt a semi-linear commit history using a stable mainline branching model and enforce a structured commit message format using customized conventional commits.**\n\n### Git Histories\nTo ensure our Git histories are both coherent and readable, we will use a stable mainline branching model. This model will help maintain a semi-linear history which highlights the relationship between commits clearly and prevents a chaotic commit history.\n\n### Semi-Linear Commit History\n1. **Feature Branches**:\n    - Naming: Feature branches should be named using the pattern `feature\/<short-description>`.\n    - Merging: Once feature development is complete, rebase the feature branch to the latest mainline and merge it using the `--no-ff` option to ensure a merge commit. Collaborators must be informed before rebasing and force-pushing.\n\n2. **Commit Organization**:\n    - Feature commits should follow a \"recipe\" style where the sequence of commits clearly demonstrates the development process in a logical order.\n\n3. **Communication**:\n    - Regular communication within the team is essential, especially before rebasing feature branches to ensure everyone is aware and synchronized.\n\n### Commit Message Format\nWe will use a customized version of conventional commits to ensure commit messages are consistent, descriptive, and facilitate automated tooling like changelog generation.\n\n#### Structure of Commit Message\n```\n<type>(optional <scope>): <description> {optional <metadata>}\n```\n1. **Types**:\n    - **feat**: A new feature\n    - **fix**: A bug fix\n    - **docs**: Documentation only changes\n    - **style**: Code style improvements (e.g., formatting, white-space)\n    - **refactor**: Code refactoring without functional changes\n    - **test**: Adding or correcting tests\n    - **build**: Changes to the build system or dependencies\n    - **ci**: Changes to CI configuration\n    - **perf**: Performance improvements\n    - **chore**: Routine tasks\n    - **enhance**: Improvements to existing features\n    - **deps**: Dependency updates\n    - **task**: Same as chore, use preferably\n    - **article**: Changes related to articles\n    - **misc**: Miscellaneous changes\n    - **notype**: Special case for merge commits where no other type applies\n\n2. **Rules**:\n    - Commit titles should be imperative (e.g., add not adding).\n    - Titles should start with an uppercase letter and not end with punctuation.\n    - Titles should not exceed 120 characters.\n    - Include relevant issue\/pull request references in the metadata if necessary.\n    - Separate the message body from the title with a blank line.\n    - Lines in the message body should not exceed 150 characters.\n\n#### Enforcing Rules\n1. **Commit Hooks**: \n   - Git hooks will enforce these rules, ensuring compliance before commits are made. Hooks can be skipped if necessary using `--no-verify`.\n\n2. **IDE Integration**:\n   - Recommend using IDE plugins that support these commit message rules for seamless integration.\n\n### Positive Consequences\n1. **Consistent History**: Histories will be clean and semi-linear, improving comprehension and traceability.\n2. **Automated Tools**: Structured commit messages enable automated tools like changelog generators.\n3. **Team Alignment**: Clear guidelines and consistent commit messages foster better collaboration and understanding within the team.\n\n### Negative Consequences\n1. **Initial Learning Curve**: Team members may need time to get accustomed to the new commit message format and branching model.\n2. **Pre-Commit Time**: Slight increase in time during the commit process due to message format validation via commit hooks.","GenTime":"2024-08-20 00:24:09"}
{"File Name":"FlowKit\/0006-JWTs.md","Context":"## Context\\nAuthentication and access control for FlowKit has previously been very limited, making use of only of postgres usernames and passwords. WIth the introduction of the API, we can be much more granular in allowing access through the use of [JSON Web Tokens](https:\/\/jwt.io).\\nThese are an encoded and cryptographically signed string, which permit access to some set of API functionality. The string specifies the identity of the user, exactly what they can access, and the time period for which the token is valid.\\nTampering with the token will result in the signature not matching, an event which can be logged an subsequently investigated.\\nA key advantage of JWTs is that they can be centrally managed, even where the service they will be used with is not accessible to the internet. Another significant advantage is that the tokens are inherently ephemeral - should a token be stolen, the time window for it to lead to a data breach will (given appropriate usage of the system) be small.\\nJWTs are also advantageous in that the add relatively minimal overhead to the day to day experience of analysts, and have broad support across many languages.\\nBut perhaps the most important advantage is the granularity of access they afford - for example it is possible to allow access to only one specific result with this method. This is useful where developing dashboards and the like, or where access to outputs but not data must be provided to other parties.\\nOther alternatives would be to use a more traditional login system, backed by a database, integrate with the Docker host's authentication system, or to use public key based authentication. All of these require more and local administration, or necessitate the use of either a JWT equivalent to actually provide API access, or the use of cookies to avoid a need to log in for every communication with the API.\\n","Decision":"API authentication and access control will use JWTs.\\n","tokens":369,"id":5057,"text":"## Context\\nAuthentication and access control for FlowKit has previously been very limited, making use of only of postgres usernames and passwords. WIth the introduction of the API, we can be much more granular in allowing access through the use of [JSON Web Tokens](https:\/\/jwt.io).\\nThese are an encoded and cryptographically signed string, which permit access to some set of API functionality. The string specifies the identity of the user, exactly what they can access, and the time period for which the token is valid.\\nTampering with the token will result in the signature not matching, an event which can be logged an subsequently investigated.\\nA key advantage of JWTs is that they can be centrally managed, even where the service they will be used with is not accessible to the internet. Another significant advantage is that the tokens are inherently ephemeral - should a token be stolen, the time window for it to lead to a data breach will (given appropriate usage of the system) be small.\\nJWTs are also advantageous in that the add relatively minimal overhead to the day to day experience of analysts, and have broad support across many languages.\\nBut perhaps the most important advantage is the granularity of access they afford - for example it is possible to allow access to only one specific result with this method. This is useful where developing dashboards and the like, or where access to outputs but not data must be provided to other parties.\\nOther alternatives would be to use a more traditional login system, backed by a database, integrate with the Docker host's authentication system, or to use public key based authentication. All of these require more and local administration, or necessitate the use of either a JWT equivalent to actually provide API access, or the use of cookies to avoid a need to log in for every communication with the API.\\n\n\n##Decision\nAPI authentication and access control will use JWTs.\\n","MatchedIds":[2397,2256,527,1721,2578],"Predicted":"## Decision\nWe will implement JSON Web Tokens (JWTs) for authentication and access control in FlowKit. This decision is driven by the following factors:\n\n1. **Granularity of Access**: JWTs allow us to specify precisely what parts of the API a user can access, making them ideal for our needs, such as granting access to only one specific result.\n2. **Security**: The cryptographic nature of JWTs ensures that any tampering with the token can be detected and logged for investigation. Additionally, the ephemeral nature of JWTs reduces the risk window if a token is compromised.\n3. **Central Management**: JWTs can be centrally managed, providing a streamlined approach even if the service is not accessible to the internet.\n4. **Minimal Overhead**: JWTs introduce minimal overhead to the user experience, making them less intrusive for analysts.\n5. **Broad Support**: JWTs are widely supported across various programming languages, ensuring compatibility and ease of integration.\n\n### Implementation Strategy:\n- **Token Generation and Management**: Implement a token generation and management system that creates signed JWTs containing user identity, access permissions, and validity period.\n- **Access Control**: Use the JWTs to control access to different parts of the API, checking the token's validity and permissions on each request.\n- **Logging and Monitoring**: Establish logging mechanisms to detect and investigate any tampering with the tokens.\n- **Token Rotation and Expiry**: Implement token rotation and expiry policies to ensure that tokens are short-lived and minimize the risk of compromised tokens.\n\nBy adopting JWTs, we improve security, provide fine-grained access control, and enhance the overall user experience for FlowKit users.","GenTime":"2024-08-20 00:24:16"}
{"File Name":"nada-kafkarator\/0003-use-poetry-for-dependency-management.md","Context":"## Context\\nPython projects should use virtualenvs to isolate them from the system Python. In addition, it is useful\\nto use a tool to manage dependencies installed in that virtualenv. There are a number of options in this\\nspace, and the Python community has not landed on a single standard. Each project needs to select which\\ntools to use, usually a selection of tools that integrate well.\\nThe Python Package Authority usually recommends Pipenv, but it has several problems that are not being\\ndealt with for various reasons. A newcomer in the space is Poetry, which has a bit of a following, and\\nclaims to support the use case well. It is also following the new pyproject.toml standard.\\n","Decision":"We will use Poetry for this project. (https:\/\/python-poetry.org\/)\\n","tokens":148,"id":864,"text":"## Context\\nPython projects should use virtualenvs to isolate them from the system Python. In addition, it is useful\\nto use a tool to manage dependencies installed in that virtualenv. There are a number of options in this\\nspace, and the Python community has not landed on a single standard. Each project needs to select which\\ntools to use, usually a selection of tools that integrate well.\\nThe Python Package Authority usually recommends Pipenv, but it has several problems that are not being\\ndealt with for various reasons. A newcomer in the space is Poetry, which has a bit of a following, and\\nclaims to support the use case well. It is also following the new pyproject.toml standard.\\n\n\n##Decision\nWe will use Poetry for this project. (https:\/\/python-poetry.org\/)\\n","MatchedIds":[400,2446,2393,5176,1117],"Predicted":"## Decision\nWe will standardize on using Poetry for managing dependencies and virtual environments in our Python projects. The reasons for this decision are:\n\n1. **Modern Standards**: Poetry supports the pyproject.toml standard, which is the direction the Python community is heading.\n2. **Dependency Management**: Poetry offers robust dependency management and makes it straightforward to manage dependencies and dev-dependencies separately.\n3. **Virtual Environment Handling**: Poetry seamlessly integrates virtualenv management, reducing the manual steps required to set up isolated environments.\n4. **Community and Support**: Although newer, Poetry has a growing community and active development, providing confidence that it will be maintained and improved.\n5. **User Experience**: Poetry provides a streamlined and cohesive user experience, integrating multiple functionalities under one tool, reducing the learning curve for onboarding new developers.\n\nBy adopting Poetry, we streamline our dependency and virtual environment management while aligning with modern practices in the Python ecosystem.","GenTime":"2024-08-20 00:24:20"}
{"File Name":"deeplearning4j\/0008 - Nd4j eager  shape computation .md","Context":"## Context\\nNd4j's model import framework often has the need to\\ncompute  shapes as variables are created.\\nThis is  in order to resolve how to properly\\ncreate a graph based on a graph descriptor from another framework\\nsuch as tensorflow or pytorch.\\nThis is often called eager mode. This proposal focuses on just eager shape computation\\nintended for use in model import. The assumption is that we could\\nbuild on this later for fully eager computation.\\n","Decision":"In order to aid building model import easier,\\nthis proposal is focused on implementing just dynamic shape computation\\nfor use in the model import context.\\nThis will be composed of a few parts:\\n1. Each outputVariables() call in SDVariable triggers\\nan Nd4j.getExecutioner().exec(..) call on the relevant operation\\nto extract out op shapes. It then sets the appropriate shapes\\nbased on the result for each SDVariable field.\\n2. This will intentionally include dummy calls for control flow ops\\nsuch as if, enter, and while. Shapes from these don't matter\\nbeyond knowing the number of outputs.\\n3. Each SameDiff instance will have an eager mode boolean\\nthat will determine whether this functionality is invoked.\\nThis eager mode variable will be required for some model import use cases.\\nUsually the model import framework will turn eager on as needed\\nwithout the user needing to be involved.\\n4. Each SameDiff instance will have a separate ArrayHolder\\nthat will be used for looking up ndarrays relevant\\nto the eager computation. This will not use proper sessions\\nbut instead store that will be used once for computing shapes.\\n","tokens":97,"id":2933,"text":"## Context\\nNd4j's model import framework often has the need to\\ncompute  shapes as variables are created.\\nThis is  in order to resolve how to properly\\ncreate a graph based on a graph descriptor from another framework\\nsuch as tensorflow or pytorch.\\nThis is often called eager mode. This proposal focuses on just eager shape computation\\nintended for use in model import. The assumption is that we could\\nbuild on this later for fully eager computation.\\n\n\n##Decision\nIn order to aid building model import easier,\\nthis proposal is focused on implementing just dynamic shape computation\\nfor use in the model import context.\\nThis will be composed of a few parts:\\n1. Each outputVariables() call in SDVariable triggers\\nan Nd4j.getExecutioner().exec(..) call on the relevant operation\\nto extract out op shapes. It then sets the appropriate shapes\\nbased on the result for each SDVariable field.\\n2. This will intentionally include dummy calls for control flow ops\\nsuch as if, enter, and while. Shapes from these don't matter\\nbeyond knowing the number of outputs.\\n3. Each SameDiff instance will have an eager mode boolean\\nthat will determine whether this functionality is invoked.\\nThis eager mode variable will be required for some model import use cases.\\nUsually the model import framework will turn eager on as needed\\nwithout the user needing to be involved.\\n4. Each SameDiff instance will have a separate ArrayHolder\\nthat will be used for looking up ndarrays relevant\\nto the eager computation. This will not use proper sessions\\nbut instead store that will be used once for computing shapes.\\n","MatchedIds":[2945,2932,2944,2934,5147],"Predicted":"## Decision\nTo implement eager shape computation for Nd4j's model import framework, we will introduce a specialized Shape Computation Engine (SCE). This engine will operate in eager mode, performing shape inference as variables are created and operations are added to the graph. \n\n### Key Components\n1. **Shape Computation Engine (SCE)**:\n    - The SCE will be responsible for keeping track of the current state of the graph and performing shape computations immediately as new nodes and edges are added.\n    - It will leverage a registry of known operations and their corresponding shape functions to compute the output shapes based on input shapes.\n\n2. **Shape Function Registry**:\n    - We will maintain a registry that maps operation types to their respective shape computation functions.\n    - Each shape function will take input shapes, attributes, and other context information to compute the output shapes.\n\n3. **Integration with Model Import Framework**:\n    - The model import framework will utilize the SCE during the import process.\n    - As each node is imported, the SCE will compute and assign the shape for that node, ensuring that the graph's structure is correctly defined.\n\n### Implementation Steps\n1. **Define Shape Function Interface**:\n    ```kotlin\n    interface ShapeFunction {\n        fun compute(inputShapes: List<longArray>, attributes: Map<String, Any>): longArray\n    }\n    ```\n\n2. **Create the Shape Computation Engine**:\n    ```kotlin\n    class ShapeComputationEngine {\n        private val shapeFunctionRegistry: MutableMap<String, ShapeFunction> = mutableMapOf()\n\n        fun registerShapeFunction(opName: String, shapeFunction: ShapeFunction) {\n            shapeFunctionRegistry[opName] = shapeFunction\n        }\n\n        fun computeShape(opName: String, inputShapes: List<longArray>, attributes: Map<String, Any>): longArray {\n            val shapeFunction = shapeFunctionRegistry[opName]\n                ?: throw IllegalArgumentException(\"No shape function registered for operation: $opName\")\n            return shapeFunction.compute(inputShapes, attributes)\n        }\n    }\n    ```\n\n3. **Integrate with Model Import Process**:\n    ```kotlin\n    fun importNode(graph: IRGraph, node: IRNode) {\n        val inputShapes = node.inputNodes.map { graph.getNodeShape(it) }\n        val attributes = node.attributes\n\n        val outputShape = shapeComputationEngine.computeShape(node.opType, inputShapes, attributes)\n        graph.setNodeShape(node, outputShape)\n    }\n    ```\n\n4. **Example Shape Function Registration**:\n    ```kotlin\n    shapeComputationEngine.registerShapeFunction(\"MatMul\", MatMulShapeFunction())\n    ```\n\n### Benefits\n- **Immediate Feedback**: Eager shape computation will provide immediate feedback on the shape consistency of the graph as it is being built.\n- **Improved Import Robustness**: It will ensure that the imported model's graph structure is valid and can potentially catch shape mismatches early in the import process.\n- **Foundation for Full Eager Execution**: This setup will lay the groundwork for moving towards fully eager computation in the future.\n\nBy implementing this eager shape computation approach, we aim to improve the robustness and user experience of Nd4j's model import framework, ensuring a solid foundation for future extensions.","GenTime":"2024-08-20 00:24:37"}
{"File Name":"modernisation-platform\/0009-use-secrets-manager-for-secrets.md","Context":"## Context\\nThe Modernisation Platform team and its users need a way to store secrets securely. There are several different methods currently used across the MoJ, including [Secrets Manager](https:\/\/aws.amazon.com\/secrets-manager\/), [Parameter Store](https:\/\/docs.aws.amazon.com\/systems-manager\/latest\/userguide\/systems-manager-parameter-store.html), [LastPass](https:\/\/www.lastpass.com\/) and [Git-Crypt](https:\/\/github.com\/AGWA\/git-crypt).\\nThere are also other well known industry solutions such as [HashiCorp Vault](https:\/\/www.vaultproject.io\/). We want to have a consistent solution across the Modernisation Platform.\\n","Decision":"We've decided to use [Secrets Manager](https:\/\/aws.amazon.com\/secrets-manager\/) for our secrets storage.\\nParameter store can be used to store non secret parameters if needed for environment specific configuration, but the first choice should be using an app_variables.json like [here](https:\/\/github.com\/ministryofjustice\/modernisation-platform-environments\/blob\/main\/terraform\/environments\/sprinkler\/app_variables.json)\\n","tokens":138,"id":3142,"text":"## Context\\nThe Modernisation Platform team and its users need a way to store secrets securely. There are several different methods currently used across the MoJ, including [Secrets Manager](https:\/\/aws.amazon.com\/secrets-manager\/), [Parameter Store](https:\/\/docs.aws.amazon.com\/systems-manager\/latest\/userguide\/systems-manager-parameter-store.html), [LastPass](https:\/\/www.lastpass.com\/) and [Git-Crypt](https:\/\/github.com\/AGWA\/git-crypt).\\nThere are also other well known industry solutions such as [HashiCorp Vault](https:\/\/www.vaultproject.io\/). We want to have a consistent solution across the Modernisation Platform.\\n\n\n##Decision\nWe've decided to use [Secrets Manager](https:\/\/aws.amazon.com\/secrets-manager\/) for our secrets storage.\\nParameter store can be used to store non secret parameters if needed for environment specific configuration, but the first choice should be using an app_variables.json like [here](https:\/\/github.com\/ministryofjustice\/modernisation-platform-environments\/blob\/main\/terraform\/environments\/sprinkler\/app_variables.json)\\n","MatchedIds":[13,4841,1252,4041,947],"Predicted":"## Decision\nWe recommend using AWS Secrets Manager as the consistent solution for securely storing secrets across the Modernisation Platform.\n\n### Justification\n1. **Integration and Compatibility:** AWS Secrets Manager integrates seamlessly with other AWS services, which are extensively used within the Modernisation Platform. This ensures minimal friction in implementation and ongoing operations.\n\n2. **Security:** AWS Secrets Manager supports strong encryption and access control mechanisms, ensuring that secrets are protected using industry best practices.\n\n3. **Scalability:** AWS Secrets Manager is designed to scale effortlessly with the platform's growth, accommodating increasing loads without compromising performance or security.\n\n4. **Management and Rotations:** It offers built-in secret management with automated rotation policies and versioning, significantly reducing the administrative overhead and risks associated with manual secret rotation.\n\n5. **Auditing and Compliance:** AWS Secrets Manager provides detailed logging and auditing capabilities via AWS CloudTrail, supporting compliance requirements and enabling thorough monitoring of secret access and usage.\n\nBy standardizing on AWS Secrets Manager, we can ensure a consistent, secure, and scalable approach to secrets management across the Modernisation Platform, aligning with the broader MoJ strategy of leveraging managed services where possible.","GenTime":"2024-08-20 00:24:42"}
{"File Name":"docnow-infra\/0002-use-terraform-workspaces-for-environments.md","Context":"## Context\\nThere are two primary patterns of use when managing multiple environments (staging, prod, etc) in Terraform. The first is to use multiple directories--one for each environment. This has the advantage of being explicit, with an associated cost of repeated TF configuration. The second alternative uses TF workspaces to switch between environments. This option appears to be [recommended](https:\/\/www.terraform.io\/docs\/enterprise\/guides\/recommended-practices\/part1.html#one-workspace-per-environment-per-terraform-configuration) by Terraform. The latter which we will use here allows a DRY approach across the environment.\\n","Decision":"Use workspaces to manage multiple environments.\\n","tokens":129,"id":328,"text":"## Context\\nThere are two primary patterns of use when managing multiple environments (staging, prod, etc) in Terraform. The first is to use multiple directories--one for each environment. This has the advantage of being explicit, with an associated cost of repeated TF configuration. The second alternative uses TF workspaces to switch between environments. This option appears to be [recommended](https:\/\/www.terraform.io\/docs\/enterprise\/guides\/recommended-practices\/part1.html#one-workspace-per-environment-per-terraform-configuration) by Terraform. The latter which we will use here allows a DRY approach across the environment.\\n\n\n##Decision\nUse workspaces to manage multiple environments.\\n","MatchedIds":[2409,2793,3789,621,856],"Predicted":"## Decision\n- We will use Terraform workspaces to manage multiple environments (staging, prod, etc). \n- This approach aligns with Terraform's recommendation and allows for a DRY (Don't Repeat Yourself) methodology.\n- Utilizing workspaces will help us avoid duplicating Terraform configuration across different directories, reducing redundancy and simplifying maintenance.\n- Each environment will be represented by a separate workspace within the same Terraform configuration, ensuring that environment-specific variables and settings can be applied appropriately while maintaining a shared infrastructure codebase.\n- Documentation and team training will be provided to ensure smooth adoption and correct usage of Terraform workspaces for managing multiple environments.","GenTime":"2024-08-20 00:24:47"}
{"File Name":"heptaconnect-docs\/2021-10-30-route-capabilities.md","Context":"## Context\\nRoutes define directions for data to flow.\\nThe interpretation or use-case for a flow can be different for various reasons.\\nIn general, we support read-multiple-times write-multiple-times scenarios, and they are very generous in options to work with but often needs to be limited in integrations.\\nLimitations like transferring data only once or transferring data for finding the equivalent on the target are missing but requested.\\nWe need a way to configure route behaviour in core without adding more work to the integrators.\\n","Decision":"All limitations (e.g. transferring once) will be implemented as skipping existing steps.\\nThese changes in behaviour can be represented by simple boolean flags.\\nEvery step that is not a limitation will result in further flow components that will get a boolean flag.\\n","tokens":106,"id":3210,"text":"## Context\\nRoutes define directions for data to flow.\\nThe interpretation or use-case for a flow can be different for various reasons.\\nIn general, we support read-multiple-times write-multiple-times scenarios, and they are very generous in options to work with but often needs to be limited in integrations.\\nLimitations like transferring data only once or transferring data for finding the equivalent on the target are missing but requested.\\nWe need a way to configure route behaviour in core without adding more work to the integrators.\\n\n\n##Decision\nAll limitations (e.g. transferring once) will be implemented as skipping existing steps.\\nThese changes in behaviour can be represented by simple boolean flags.\\nEvery step that is not a limitation will result in further flow components that will get a boolean flag.\\n","MatchedIds":[2111,1050,3205,1210,5142],"Predicted":"## Decision\nImplement a configuration layer within the core system to manage route behaviors, tailored to specific use-cases. This configuration would dictate how data flows are handled, allowing for scenarios such as \"transfer data only once\" and \"transfer data for finding the equivalent on the target.\" The configuration should be easily adjustable via a declarative approach, ensuring that integrators do not need to alter their existing implementations.\n\n### Configuration Options\n1. **Transfer-Once**: Ensures data is only transferred a single time.\n2. **Find-Equivalent**: Transfers data to find and match the equivalent data on the target.\n\n### Implementation Steps\n1. **Configuration Schema**: Define a clear schema for route behaviors.\n    - Example: \n    ```json\n    {\n        \"routeId\": \"route1\",\n        \"behaviors\": [\"transfer-once\", \"find-equivalent\"]\n    }\n    ```\n\n2. **Core Engine Enhancement**: Update the core engine to interpret and enforce these configurations.\n    - Hook into the existing routing logic to apply the behavior constraints as data is processed.\n\n3. **Default Behaviors**: Ensure that default route behavior remains \"read-multiple-times write-multiple-times\" unless overridden by configuration.\n    \n4. **Integrations Transparency**: Maintain a transparent API layer for integrators, abstracting away the complexities of configurable route behaviors.\n\n### Example Configuration\n```json\n{\n    \"routes\": [\n        {\n            \"id\": \"route1\",\n            \"source\": \"source-system\",\n            \"destination\": \"destination-system\",\n            \"behaviors\": [\"transfer-once\"]\n        },\n        {\n            \"id\": \"route2\",\n            \"source\": \"source-system2\",\n            \"destination\": \"destination-system2\",\n            \"behaviors\": [\"find-equivalent\"]\n        }\n    ]\n}\n```\n\nBy incorporating these configurable options within the core, we can provide greater flexibility and control over data flows without imposing additional burdens on integrators.","GenTime":"2024-08-20 00:24:56"}
{"File Name":"tove\/adr-03.md","Context":"## Context\\nWhen a transcription is approved, a set of flat files containing the transcription data will be saved to Azure. Users will have the option to download a zip file containing their requested subject, group, workflow, or project. Depending on the speed at which we are able to zip the necessary files, we will either trigger a direct download, or provide a link to the location of the zip file to the user.\\nThe goal is to investigate Azure\u2019s storage options (specifically Blob Storage and File Services) and decide which tool is best suited for our needs.\\n### Factors to consider:\\n* How easy is it to share a file to the end user? What is the process for this?\\n* Ease of use, how complicated is it to set up, maintain, edit\\n* access permission features\\n* Speed of accessing and iterating through files (e.g. getting all files in a given directory)\\n### Terminology:\\n**Blob:** acronym for \u201cBinary Large Object\u201d\\n**Container:** synonym for \u201dS3 Bucket\u201d\\n**Shared Access Signature:** similar functionality as \u201cS3 Presigned URLs\u201d\\n","Decision":"We don't appear to have any need for most of the additional functionality that comes with File Service, which makes me reluctant to want to use it. In addition, the number of articles and resources available on communicating with Blob Storage to set up file zipping is much greater than what's available for File Service. My initial understanding of Blob Storage led me to believe that permissions could only be set at the container level, but this turned out to be wrong. With the ability to set blob-specific permissions, we will be able to use a single container to store the transcription-specific files, and the user-requested zip files.\\nUltimately, my choice is to go with Blob Storage: the more basic, simple storage tool that gives us what we need and nothing more. That being said, I'd still like to keep the option of using Azure File Service on the table, in case it turns out that we *would* benefit from the additional functionality that it offers.\\nAs for what type of blob we will use, my choice would be to store each data file in its own block blob. If we were to choose to store multiple files within a single blob (and have each file be associated with a block ID on that blob), we would lose the ability to name each individual file. Hypothetically, it would be possible to create a database table with columns \u201cblock ID\u201d and \u201cname\u201d, to emulate a naming functionality, but this seems far more complicated than its worth. In addition, the [azure-storage-blob](https:\/\/github.com\/azure\/azure-storage-ruby\/tree\/master\/blob) gem gives us a simple interface for working with block blobs and saves us the trouble of having to write HTTP requests ourselves.\\nFinal questions:\\n1. Q: Blob Storage doesn't have any concrete hierarchy beyond Storage Account\/Blob Container - within a container, directories are virtual, demarcated by prefixes in the file name. Will this end up being problematic for us? Will it complicate file retrieval?\\nA: Retrieving files from a file system with virtual directories shouldn't be any different than retrieving files from a normal file system. As long as blob prefixes are constructed in a way that reflects the organizational system used within the application\/database, there should be no trouble. File retrieval may be helped by append blobs - final decision on blob type is still TBD.\\n2. Q: Would there be any benefit to caching files on on-premises file servers? If this sounds like something we'd like to employ, it would be worth reconsidering Azure File Service.\\nA: This doesn't appear to be something we will need.\\n### Links and Articles:\\n1. [Microsoft: Deciding when to use Azure Blobs, Azure Files, or Azure Disks](https:\/\/docs.microsoft.com\/en-us\/azure\/storage\/common\/storage-decide-blobs-files-disks)\\n2. [Azure Files FAQ](https:\/\/docs.microsoft.com\/en-us\/azure\/storage\/files\/storage-files-faq) (see \u2018Why would I use an Azure file share versus Azure Blob Storage for my data?\u2019)\\n3. [Stack Overflow: Blob Storage vs File Service](https:\/\/stackoverflow.com\/questions\/24880430\/azure-blob-storage-vs-file-service)\\n4. [Microsoft: Introducing Azure File Service](https:\/\/blogs.msdn.microsoft.com\/windowsazurestorage\/2014\/05\/12\/introducing-microsoft-azure-file-service\/) (scroll to When to use Azure Files vs Azure Blobs vs Azure Disks)\\n5. [Microsoft: Azure Storage scalability and performance targets for storage accounts](https:\/\/docs.microsoft.com\/en-us\/azure\/storage\/common\/storage-scalability-targets)\\n6. [Azure Blob Overview](https:\/\/docs.microsoft.com\/en-us\/azure\/storage\/blobs\/storage-blobs-overview)\\n7. [Azure Blob Introduction](https:\/\/docs.microsoft.com\/en-us\/azure\/storage\/blobs\/storage-blobs-introduction)\\n8. [How to mount Blob storage as a file system with blobfuse](https:\/\/docs.microsoft.com\/en-us\/azure\/storage\/blobs\/storage-how-to-mount-container-linux)\\n9. [Block blobs, append blobs, and page blobs](https:\/\/docs.microsoft.com\/en-us\/rest\/api\/storageservices\/understanding-block-blobs--append-blobs--and-page-blobs)\\n10. [Azure Blob Storage gem](https:\/\/github.com\/azure\/azure-storage-ruby\/tree\/master\/blob)\\n","tokens":231,"id":4533,"text":"## Context\\nWhen a transcription is approved, a set of flat files containing the transcription data will be saved to Azure. Users will have the option to download a zip file containing their requested subject, group, workflow, or project. Depending on the speed at which we are able to zip the necessary files, we will either trigger a direct download, or provide a link to the location of the zip file to the user.\\nThe goal is to investigate Azure\u2019s storage options (specifically Blob Storage and File Services) and decide which tool is best suited for our needs.\\n### Factors to consider:\\n* How easy is it to share a file to the end user? What is the process for this?\\n* Ease of use, how complicated is it to set up, maintain, edit\\n* access permission features\\n* Speed of accessing and iterating through files (e.g. getting all files in a given directory)\\n### Terminology:\\n**Blob:** acronym for \u201cBinary Large Object\u201d\\n**Container:** synonym for \u201dS3 Bucket\u201d\\n**Shared Access Signature:** similar functionality as \u201cS3 Presigned URLs\u201d\\n\n\n##Decision\nWe don't appear to have any need for most of the additional functionality that comes with File Service, which makes me reluctant to want to use it. In addition, the number of articles and resources available on communicating with Blob Storage to set up file zipping is much greater than what's available for File Service. My initial understanding of Blob Storage led me to believe that permissions could only be set at the container level, but this turned out to be wrong. With the ability to set blob-specific permissions, we will be able to use a single container to store the transcription-specific files, and the user-requested zip files.\\nUltimately, my choice is to go with Blob Storage: the more basic, simple storage tool that gives us what we need and nothing more. That being said, I'd still like to keep the option of using Azure File Service on the table, in case it turns out that we *would* benefit from the additional functionality that it offers.\\nAs for what type of blob we will use, my choice would be to store each data file in its own block blob. If we were to choose to store multiple files within a single blob (and have each file be associated with a block ID on that blob), we would lose the ability to name each individual file. Hypothetically, it would be possible to create a database table with columns \u201cblock ID\u201d and \u201cname\u201d, to emulate a naming functionality, but this seems far more complicated than its worth. In addition, the [azure-storage-blob](https:\/\/github.com\/azure\/azure-storage-ruby\/tree\/master\/blob) gem gives us a simple interface for working with block blobs and saves us the trouble of having to write HTTP requests ourselves.\\nFinal questions:\\n1. Q: Blob Storage doesn't have any concrete hierarchy beyond Storage Account\/Blob Container - within a container, directories are virtual, demarcated by prefixes in the file name. Will this end up being problematic for us? Will it complicate file retrieval?\\nA: Retrieving files from a file system with virtual directories shouldn't be any different than retrieving files from a normal file system. As long as blob prefixes are constructed in a way that reflects the organizational system used within the application\/database, there should be no trouble. File retrieval may be helped by append blobs - final decision on blob type is still TBD.\\n2. Q: Would there be any benefit to caching files on on-premises file servers? If this sounds like something we'd like to employ, it would be worth reconsidering Azure File Service.\\nA: This doesn't appear to be something we will need.\\n### Links and Articles:\\n1. [Microsoft: Deciding when to use Azure Blobs, Azure Files, or Azure Disks](https:\/\/docs.microsoft.com\/en-us\/azure\/storage\/common\/storage-decide-blobs-files-disks)\\n2. [Azure Files FAQ](https:\/\/docs.microsoft.com\/en-us\/azure\/storage\/files\/storage-files-faq) (see \u2018Why would I use an Azure file share versus Azure Blob Storage for my data?\u2019)\\n3. [Stack Overflow: Blob Storage vs File Service](https:\/\/stackoverflow.com\/questions\/24880430\/azure-blob-storage-vs-file-service)\\n4. [Microsoft: Introducing Azure File Service](https:\/\/blogs.msdn.microsoft.com\/windowsazurestorage\/2014\/05\/12\/introducing-microsoft-azure-file-service\/) (scroll to When to use Azure Files vs Azure Blobs vs Azure Disks)\\n5. [Microsoft: Azure Storage scalability and performance targets for storage accounts](https:\/\/docs.microsoft.com\/en-us\/azure\/storage\/common\/storage-scalability-targets)\\n6. [Azure Blob Overview](https:\/\/docs.microsoft.com\/en-us\/azure\/storage\/blobs\/storage-blobs-overview)\\n7. [Azure Blob Introduction](https:\/\/docs.microsoft.com\/en-us\/azure\/storage\/blobs\/storage-blobs-introduction)\\n8. [How to mount Blob storage as a file system with blobfuse](https:\/\/docs.microsoft.com\/en-us\/azure\/storage\/blobs\/storage-how-to-mount-container-linux)\\n9. [Block blobs, append blobs, and page blobs](https:\/\/docs.microsoft.com\/en-us\/rest\/api\/storageservices\/understanding-block-blobs--append-blobs--and-page-blobs)\\n10. [Azure Blob Storage gem](https:\/\/github.com\/azure\/azure-storage-ruby\/tree\/master\/blob)\\n","MatchedIds":[5163,3351,3797,3416,3914],"Predicted":"## Decision\nWe will utilize Azure Blob Storage for saving the transcription data and generating downloadable zip files for the following reasons:\n\n### Positive Consequences\n* **Ease of Sharing:** Azure Blob Storage supports Shared Access Signatures (SAS), which makes it straightforward to generate URLs for users to download files securely. This aligns well with the requirement to provide a download link if the zipping process takes longer.\n* **Access Permissions:** Blob Storage offers granular access permissions, allowing fine-tuned control over who can access the stored files. This is crucial for maintaining data security and privacy.\n* **Ease of Use:** Setting up and maintaining Azure Blob Storage is relatively simple compared to Azure File Services. Administration tasks, such as configuring storage accounts and monitoring usage, can be handled easily through Azure's management tools.\n* **Speed and Performance:** Azure Blob Storage is optimized for high-speed access and large volumes of data, ensuring rapid retrieval and iteration through files. This is especially important for generating zip files containing large datasets efficiently.\n* **File Organization:** Blobs can be organized within containers, providing a flexible way to handle directories and large datasets, which can be beneficial for managing transcription data.\n\n### Negative Consequences\n* **Initial Learning Curve:** There may be a slight learning curve for team members not familiar with Blob Storage; however, this can be mitigated with proper training and documentation.\n* **Cost Considerations:** Depending on the data volume and access patterns, costs could vary. Azure Blob Storage pricing should be monitored to ensure it remains within budget.\n\nGiven these considerations, Azure Blob Storage is the optimal choice for managing transcription data and providing user downloads in a secure, efficient, and scalable manner.","GenTime":"2024-08-20 00:25:03"}
{"File Name":"cloud_controller_ng\/0003-switching-to-request-specs-for-controllers.md","Context":"Context\\n-------\\nThere are two approaches to unit testing controllers, neither of which is very useful: If we mock most of the\\ndependencies of the controller, our tests become tightly coupled to the implementation and do not give us a\\nlot of confidence. If we make them more integration-style tests, we end up duplicating lots of tests between the\\ncontroller and the request specs.\\nAdditionally, writing controller specs is a discouraged pattern:\\n> Controller specs can be used to describe the behaviour of Rails controllers. As of version 3.5, however, controller specs are discouraged in favour of request specs (which also focus largely on controllers, but capture other critical aspects of application behaviour as well). Controller specs will continue to be supported until at least version 4.0 (see the release notes for details).\\nFrom [RSpec\u200a\u2014\u200aController or Request Specs?][]\\n**Warning**: Request specs in the `cloud_controller_ng` codebase are not actually Rspec\/Rails request specs (which would be [declared with `type: :request`](https:\/\/relishapp.com\/rspec\/rspec-rails\/docs\/request-specs\/request-spec)), but use [Rack Test](https:\/\/github.com\/rack-test\/rack-test).  Rack Test has similar looking helper methods for making requests (e.g. `get`, `post`, etc), but these take different arguments than the equivalent Rspec methods.\\nDecision\\n--------\\nMoving forward, we will only write request specs for controllers.\\nConsequences\\n------------\\n- Reduced duplication in testing controllers\\n- Controller spec helpers need to be adjusted to no longer mock out CC app directly - ie they need to be transformed into Request spec helpers.\\n- Delete controller specs once they've been transitioned to request specs.\\n[RSpec\u200a\u2014\u200aController or Request Specs?]: https:\/\/medium.com\/just-tech\/rspec-controller-or-request-specs-d93ef563ef11\\n","Decision":"--------\\nMoving forward, we will only write request specs for controllers.\\nConsequences\\n------------\\n- Reduced duplication in testing controllers\\n- Controller spec helpers need to be adjusted to no longer mock out CC app directly - ie they need to be transformed into Request spec helpers.\\n- Delete controller specs once they've been transitioned to request specs.\\n[RSpec\u200a\u2014\u200aController or Request Specs?]: https:\/\/medium.com\/just-tech\/rspec-controller-or-request-specs-d93ef563ef11\\n","tokens":403,"id":1829,"text":"Context\\n-------\\nThere are two approaches to unit testing controllers, neither of which is very useful: If we mock most of the\\ndependencies of the controller, our tests become tightly coupled to the implementation and do not give us a\\nlot of confidence. If we make them more integration-style tests, we end up duplicating lots of tests between the\\ncontroller and the request specs.\\nAdditionally, writing controller specs is a discouraged pattern:\\n> Controller specs can be used to describe the behaviour of Rails controllers. As of version 3.5, however, controller specs are discouraged in favour of request specs (which also focus largely on controllers, but capture other critical aspects of application behaviour as well). Controller specs will continue to be supported until at least version 4.0 (see the release notes for details).\\nFrom [RSpec\u200a\u2014\u200aController or Request Specs?][]\\n**Warning**: Request specs in the `cloud_controller_ng` codebase are not actually Rspec\/Rails request specs (which would be [declared with `type: :request`](https:\/\/relishapp.com\/rspec\/rspec-rails\/docs\/request-specs\/request-spec)), but use [Rack Test](https:\/\/github.com\/rack-test\/rack-test).  Rack Test has similar looking helper methods for making requests (e.g. `get`, `post`, etc), but these take different arguments than the equivalent Rspec methods.\\nDecision\\n--------\\nMoving forward, we will only write request specs for controllers.\\nConsequences\\n------------\\n- Reduced duplication in testing controllers\\n- Controller spec helpers need to be adjusted to no longer mock out CC app directly - ie they need to be transformed into Request spec helpers.\\n- Delete controller specs once they've been transitioned to request specs.\\n[RSpec\u200a\u2014\u200aController or Request Specs?]: https:\/\/medium.com\/just-tech\/rspec-controller-or-request-specs-d93ef563ef11\\n\n\n##Decision\n--------\\nMoving forward, we will only write request specs for controllers.\\nConsequences\\n------------\\n- Reduced duplication in testing controllers\\n- Controller spec helpers need to be adjusted to no longer mock out CC app directly - ie they need to be transformed into Request spec helpers.\\n- Delete controller specs once they've been transitioned to request specs.\\n[RSpec\u200a\u2014\u200aController or Request Specs?]: https:\/\/medium.com\/just-tech\/rspec-controller-or-request-specs-d93ef563ef11\\n","MatchedIds":[1387,4785,1608,2816,4359],"Predicted":"## Decision\nWe will only write request specs for controllers moving forward.\n\n## Consequences\n- Reduced duplication in testing controllers.\n- Controller spec helpers need to be adjusted to no longer mock out the CC app directly; they need to be transformed into request spec helpers.\n- Controller specs will be deleted once they've been transitioned to request specs.","GenTime":"2024-08-20 00:25:05"}
{"File Name":"klokwrk-project\/0014-commit-message-format.md","Context":"## Context\\nUsing commit messages without any structure looks convenient for developers as they do not have to think about messages too much. Unfortunately, that freedom and lack of thinking can impose some\\nadditional burden on long-term project maintenance.\\nQuite often, we can find incomprehensible commit messages that do not communicate anything useful. Hopefully, imposing some lightweight rules and guidance will help developers create commit messages\\nthat are helpful for their colleagues.\\nIn addition, with unstructured commit messages, there is much less opportunity to introduce any tools on top of commit history. For example, we would like to employ an automated changelog generator\\nbased on extracting some semantical meaning from commits, but this will not work if commit messages lack any structure. Without the commit message structure, we can just dump the commit log in the\\nchangelog, which does not make the changelog more helpful than looking at the history of commits in the first place.\\n### Architectural Context\\n* System (`klokwrk-project`)\\n","Decision":"**We will use a customized [conventional commits](https:\/\/www.conventionalcommits.org\/en\/v1.0.0\/) format for writing commit messages.**\\nConventional commits format is nice and short and defines the simple structure that is easy to learn and follow. Here is basic structure of our customized conventional commits format:\\n<type>(optional <scope>): <description> {optional <metadata>}\\nOur customization:\\n- defines additional message types as an extension of [types defined by the Angular team](https:\/\/github.com\/angular\/angular\/blob\/22b96b9\/CONTRIBUTING.md#-commit-message-guidelines)\\n- allows adding additional metadata in the message title if useful and appropriate (see details section for more info)\\n- requires format compliance only for messages of \"significant\" commits (see details section for more info)\\n### Decision details\\nDetails about the decision are given mainly as a set of strong recommendations (strongly recommended) and rules enforced by tooling (rule). In our case, the tooling is implemented as git commit hooks.\\nEvery contributor should install git hooks provided in this repository. That can be done with following command (executed from the project root):\\ngit config core.hooksPath support\/git\/hooks\\nThere might be cases when implemented rules are not appropriate and should be updated or removed or just temporarily ignored. In such scenarios, hooks can be skipped with git's `--no-verify` option.\\nWhile describing details, following terms are used as described:\\n- *commit message title*: refers to the first line of a commit message\\n- *commit message description*: refers to the part of the title describing a commit with human-readable message. In conventional commits specification that part is called `description`.\\n#### General guidance and rules for all commit messages\\n- (strongly recommended) - avoid trivial commit messages titles or descriptions\\n- (strongly recommended) - use imperative mood in title or description (add instead of adding or added, update instead of updating or updated etc.) as you are spelling out a command\\n- (rule) - message title or description must start with the uppercase letter <br\/>\\n<br\/>\\nThe main reason is a desire for better readability as we want easily spot the beginning message description or title. There are some arguments for using the lowercase like \"message titles are not\\nsentences\". While this is true, we prefer to have better readability than comply with some vague constraints.<br\/>\\n<br\/>\\n- (rule) - message title or description should not end with common punctuation characters: `.!?`\\n- (strongly recommended) - message title or description should not be comprised of multiple sentences\\n- (rule) - message title should not be longer than 120 characters. Use the message body if more space for description is needed<br\/>\\n<br\/>\\nActually, there is a common convention that we should not use more than 69 characters in the message title. It looks like the main reason for it is that GitHub truncates anything above 69 chars\\nfrom message titles. Having such a tight constraint seems unreasonable today, and the apparent shortcomings of any tool shouldn't restrict us, even if the tool is GitHub.<br\/>\\n<br\/>\\n- (strongly recommended) - commit message title or description should describe \"what\" (and sometimes \"why\"), instead of \"how\"<br\/>\\n<br\/>\\nFor describing \"why\", the message body is more appropriate as we have more space there. If needed, the message body may contain \"how\" too, but it should be clearly separated (at least with a blank\\nline) from \"what\" and \"why\".<br\/>\\n<br\/>\\n- (recommended) - commit message title should provide optional scope (from conventional commit specification) if applicable\\n- if commit refers to multiple scopes, scopes should be separated with `\/` character\\n- if commit refers to the work which influences the whole project, the scope should be `project` or it can be left out\\n- the scope should be a single word in lowercase<br\/>\\n<br\/>\\n- (strongly recommended) - message body must be separated from message title with a single blank line\\n- (option) - message body can contain additional blank lines\\n- (recommended) - message body should not use lines longer than 150 characters\\n- (strongly recommended) - include relevant references to issues or pull request to the metadata section of message title<br\/>\\n<br\/>\\nExample: `feat(some-module): Adding a new feature {m, fixes i#123, pr#13, related to i#333, resolves i#444}`<br\/>\\n<br\/>\\n- (option) - include relevant feature\/bug ticket links in message footer according to conventional commits guidelines<br\/>\\n<br\/>\\nFooter is separated from body with a single blank line.\\n#### Guidance and rules for \"normal\" commits to the main development branch\\n- (rule) - all commits to the main development branch must have a message title in customized conventional commit format\\n#### Guidance and rules for merge commits to the main development branch\\nWhen used with [semi-linear commit history](.\/0007-git-workflow-with-linear-history.md), merge commits are the primary carriers of completed work units. As such, they are the most interesting for\\ncreating a changelog.\\nBefore merging, merge commits must be rebased against main development branch, and merging must be executed with no-fast-forward option (`--no-ff`).\\n- (rule) - merge commits must have 'merge' metadata (`{m}`) present at the end of the title <br\/>\\n<br\/>\\nThat way, merge commits can be easily distinguished on GitHub and in the changelog.\\n- (option) - merge commit metadata can carry additional information related to the issues and PRs references like in the following example\\nfeat(klokwrk-tool-gradle-source-repack): Adding a new feature {m, fixes i#123, pr#13, related to i#333, resolves i#444}\\nHere, `i#123` is a reference to the issue, while `pr#12` is a reference to the pull request. Additional metadata are not controlled or enforced by git hooks.\\n#### Guidance and rules for normal commits to the feature branches\\n- (option) - normal commits don't have to follow custom conventional commits format for message title\\n- (strongly recommended) - normal commits should use conventional commits format when contained change is significant enough on its own to be placed in the changelog\\nWhen all useful changelog entries are contained in normal commits of a feature branch, we can do two different things depending on the situation:\\n- use merge commit with type of `notype`. Such merge commit will be ignored when creating a changelog.\\n- merge a branch with fast-forward option (no merge commit will be present)\\nPreferably, use `notype` merge commits, as they are still useful for clear separation of related work.\\n#### Types for conventional commits format\\n- common (angular)\\n- `feat` or `feature` - a new feature\\n- `fix` - a bug fix\\n- `docs` - documentation only changes\\n- `style` - changes that do not affect the meaning of the code (white-space, formatting, missing semi-colons, etc)\\n- `test` - adding missing tests or correcting existing tests\\n- `build` - changes that affect the build system or external dependencies\\n- `ci` - changes to our CI configuration files and scripts\\n- `refactor` - a code change that neither fixes a bug nor adds a feature\\n- `perf` - a code change that improves performance\\n- `chore` - routine task\\n- custom\\n- `enhance` or `enhancement` - improvements to the existing features\\n- `deps` - dependencies updates (use instead of `build` when commit only updates dependencies)<br\/>\\n<br\/>\\nThere are two main scenarios when upgrading dependencies, a simple version bump and the more involved upgrade requiring resolving various issues like compilation errors, API upgrades, etc.<br\/>\\n<br\/>\\nSimple version bumps should be contained in a single individual commit with a description message starting with the word \"Bump\". For example: `deps: Bump Micronaut to 2.5.2 version`.<br\/>\\n<br\/>\\nMore complicated upgrades should be organized as feature branches where each non-conventional commit resolves a single step in the process. When finished, the feature branch should be merged\\ninto the main development branch with a description starting with the word \"Upgrade\". For example: `deps: Upgrade Spring Boot to 2.5.0 version {m}`.<br\/>\\n<br\/>\\n- `task` - same meaning as `chore`. Prefer using `task`.\\n- `article` - use instead of `docs` when changes are related only to articles\\n- `misc` - anything that does not fit into previous categories\\n- `notype` - only for merge commits in situations where contained plain commits carries all relevant types and merge commit doesn't add anything useful. It is ignored in changelog.<br\/>\\n<br\/>\\nTypical example is when we are updating number of dependencies through commits in a feature branch. In this situation each feature branch commit should contain updates to a single dependency\\nand communicate the update through its own commit message in conventional commit format. When all updates in the branch are tested, we can merge them in the main branch. However, that merge\\nshould be ignored by changelog as it does not communicate anything new.\\n","tokens":205,"id":4890,"text":"## Context\\nUsing commit messages without any structure looks convenient for developers as they do not have to think about messages too much. Unfortunately, that freedom and lack of thinking can impose some\\nadditional burden on long-term project maintenance.\\nQuite often, we can find incomprehensible commit messages that do not communicate anything useful. Hopefully, imposing some lightweight rules and guidance will help developers create commit messages\\nthat are helpful for their colleagues.\\nIn addition, with unstructured commit messages, there is much less opportunity to introduce any tools on top of commit history. For example, we would like to employ an automated changelog generator\\nbased on extracting some semantical meaning from commits, but this will not work if commit messages lack any structure. Without the commit message structure, we can just dump the commit log in the\\nchangelog, which does not make the changelog more helpful than looking at the history of commits in the first place.\\n### Architectural Context\\n* System (`klokwrk-project`)\\n\n\n##Decision\n**We will use a customized [conventional commits](https:\/\/www.conventionalcommits.org\/en\/v1.0.0\/) format for writing commit messages.**\\nConventional commits format is nice and short and defines the simple structure that is easy to learn and follow. Here is basic structure of our customized conventional commits format:\\n<type>(optional <scope>): <description> {optional <metadata>}\\nOur customization:\\n- defines additional message types as an extension of [types defined by the Angular team](https:\/\/github.com\/angular\/angular\/blob\/22b96b9\/CONTRIBUTING.md#-commit-message-guidelines)\\n- allows adding additional metadata in the message title if useful and appropriate (see details section for more info)\\n- requires format compliance only for messages of \"significant\" commits (see details section for more info)\\n### Decision details\\nDetails about the decision are given mainly as a set of strong recommendations (strongly recommended) and rules enforced by tooling (rule). In our case, the tooling is implemented as git commit hooks.\\nEvery contributor should install git hooks provided in this repository. That can be done with following command (executed from the project root):\\ngit config core.hooksPath support\/git\/hooks\\nThere might be cases when implemented rules are not appropriate and should be updated or removed or just temporarily ignored. In such scenarios, hooks can be skipped with git's `--no-verify` option.\\nWhile describing details, following terms are used as described:\\n- *commit message title*: refers to the first line of a commit message\\n- *commit message description*: refers to the part of the title describing a commit with human-readable message. In conventional commits specification that part is called `description`.\\n#### General guidance and rules for all commit messages\\n- (strongly recommended) - avoid trivial commit messages titles or descriptions\\n- (strongly recommended) - use imperative mood in title or description (add instead of adding or added, update instead of updating or updated etc.) as you are spelling out a command\\n- (rule) - message title or description must start with the uppercase letter <br\/>\\n<br\/>\\nThe main reason is a desire for better readability as we want easily spot the beginning message description or title. There are some arguments for using the lowercase like \"message titles are not\\nsentences\". While this is true, we prefer to have better readability than comply with some vague constraints.<br\/>\\n<br\/>\\n- (rule) - message title or description should not end with common punctuation characters: `.!?`\\n- (strongly recommended) - message title or description should not be comprised of multiple sentences\\n- (rule) - message title should not be longer than 120 characters. Use the message body if more space for description is needed<br\/>\\n<br\/>\\nActually, there is a common convention that we should not use more than 69 characters in the message title. It looks like the main reason for it is that GitHub truncates anything above 69 chars\\nfrom message titles. Having such a tight constraint seems unreasonable today, and the apparent shortcomings of any tool shouldn't restrict us, even if the tool is GitHub.<br\/>\\n<br\/>\\n- (strongly recommended) - commit message title or description should describe \"what\" (and sometimes \"why\"), instead of \"how\"<br\/>\\n<br\/>\\nFor describing \"why\", the message body is more appropriate as we have more space there. If needed, the message body may contain \"how\" too, but it should be clearly separated (at least with a blank\\nline) from \"what\" and \"why\".<br\/>\\n<br\/>\\n- (recommended) - commit message title should provide optional scope (from conventional commit specification) if applicable\\n- if commit refers to multiple scopes, scopes should be separated with `\/` character\\n- if commit refers to the work which influences the whole project, the scope should be `project` or it can be left out\\n- the scope should be a single word in lowercase<br\/>\\n<br\/>\\n- (strongly recommended) - message body must be separated from message title with a single blank line\\n- (option) - message body can contain additional blank lines\\n- (recommended) - message body should not use lines longer than 150 characters\\n- (strongly recommended) - include relevant references to issues or pull request to the metadata section of message title<br\/>\\n<br\/>\\nExample: `feat(some-module): Adding a new feature {m, fixes i#123, pr#13, related to i#333, resolves i#444}`<br\/>\\n<br\/>\\n- (option) - include relevant feature\/bug ticket links in message footer according to conventional commits guidelines<br\/>\\n<br\/>\\nFooter is separated from body with a single blank line.\\n#### Guidance and rules for \"normal\" commits to the main development branch\\n- (rule) - all commits to the main development branch must have a message title in customized conventional commit format\\n#### Guidance and rules for merge commits to the main development branch\\nWhen used with [semi-linear commit history](.\/0007-git-workflow-with-linear-history.md), merge commits are the primary carriers of completed work units. As such, they are the most interesting for\\ncreating a changelog.\\nBefore merging, merge commits must be rebased against main development branch, and merging must be executed with no-fast-forward option (`--no-ff`).\\n- (rule) - merge commits must have 'merge' metadata (`{m}`) present at the end of the title <br\/>\\n<br\/>\\nThat way, merge commits can be easily distinguished on GitHub and in the changelog.\\n- (option) - merge commit metadata can carry additional information related to the issues and PRs references like in the following example\\nfeat(klokwrk-tool-gradle-source-repack): Adding a new feature {m, fixes i#123, pr#13, related to i#333, resolves i#444}\\nHere, `i#123` is a reference to the issue, while `pr#12` is a reference to the pull request. Additional metadata are not controlled or enforced by git hooks.\\n#### Guidance and rules for normal commits to the feature branches\\n- (option) - normal commits don't have to follow custom conventional commits format for message title\\n- (strongly recommended) - normal commits should use conventional commits format when contained change is significant enough on its own to be placed in the changelog\\nWhen all useful changelog entries are contained in normal commits of a feature branch, we can do two different things depending on the situation:\\n- use merge commit with type of `notype`. Such merge commit will be ignored when creating a changelog.\\n- merge a branch with fast-forward option (no merge commit will be present)\\nPreferably, use `notype` merge commits, as they are still useful for clear separation of related work.\\n#### Types for conventional commits format\\n- common (angular)\\n- `feat` or `feature` - a new feature\\n- `fix` - a bug fix\\n- `docs` - documentation only changes\\n- `style` - changes that do not affect the meaning of the code (white-space, formatting, missing semi-colons, etc)\\n- `test` - adding missing tests or correcting existing tests\\n- `build` - changes that affect the build system or external dependencies\\n- `ci` - changes to our CI configuration files and scripts\\n- `refactor` - a code change that neither fixes a bug nor adds a feature\\n- `perf` - a code change that improves performance\\n- `chore` - routine task\\n- custom\\n- `enhance` or `enhancement` - improvements to the existing features\\n- `deps` - dependencies updates (use instead of `build` when commit only updates dependencies)<br\/>\\n<br\/>\\nThere are two main scenarios when upgrading dependencies, a simple version bump and the more involved upgrade requiring resolving various issues like compilation errors, API upgrades, etc.<br\/>\\n<br\/>\\nSimple version bumps should be contained in a single individual commit with a description message starting with the word \"Bump\". For example: `deps: Bump Micronaut to 2.5.2 version`.<br\/>\\n<br\/>\\nMore complicated upgrades should be organized as feature branches where each non-conventional commit resolves a single step in the process. When finished, the feature branch should be merged\\ninto the main development branch with a description starting with the word \"Upgrade\". For example: `deps: Upgrade Spring Boot to 2.5.0 version {m}`.<br\/>\\n<br\/>\\n- `task` - same meaning as `chore`. Prefer using `task`.\\n- `article` - use instead of `docs` when changes are related only to articles\\n- `misc` - anything that does not fit into previous categories\\n- `notype` - only for merge commits in situations where contained plain commits carries all relevant types and merge commit doesn't add anything useful. It is ignored in changelog.<br\/>\\n<br\/>\\nTypical example is when we are updating number of dependencies through commits in a feature branch. In this situation each feature branch commit should contain updates to a single dependency\\nand communicate the update through its own commit message in conventional commit format. When all updates in the branch are tested, we can merge them in the main branch. However, that merge\\nshould be ignored by changelog as it does not communicate anything new.\\n","MatchedIds":[4899,4600,2517,4889,1416],"Predicted":"## Decision\nWe will adopt a structured approach for commit messages in the `klokwrk-project` by enforcing the use of [Conventional Commits](https:\/\/www.conventionalcommits.org\/en\/v1.0.0\/). This decision aims to ensure commit messages are clear, meaningful, and useful for long-term project maintenance, as well as enable automated tools to generate changelogs and perform other semantic versioning tasks.\n\n### Decision Details\n1. **Commit Message Format**:\n   * Each commit message must have a type, an optional scope, and a subject. The format is as follows:\n     ```\n     <type>(<scope>): <subject>\n     ```\n   * The available types are:\n     - `feat`: A new feature.\n     - `fix`: A bug fix.\n     - `docs`: Documentation changes.\n     - `style`: Formatting changes that do not affect code functionality (e.g., whitespace, formatting, missing semicolons).\n     - `refactor`: Code changes that neither fix a bug nor add a feature.\n     - `perf`: Code changes that improve performance.\n     - `test`: Adding or modifying tests.\n     - `chore`: Changes to the build process or auxiliary tools and libraries such as documentation generation.\n     - `revert`: Reverts a previous commit.\n   * Example commit messages:\n     ```\n     feat(authentication): add support for OAuth2 login\n     fix(auth): resolve token expiration issue\n     docs(README): update setup instructions\n     ```\n\n2. **Scope**:\n   * The scope is optional but should be used to indicate the section of the codebase the commit affects (e.g., `auth`, `api`, `ui`).\n\n3. **Subject**:\n   * The subject should be a concise description of the change in the imperative mood (i.e., as if giving a command).\n\n4. **Body and Footer**:\n   * For more detailed information, the commit message can include a body and\/or footer. The body should provide a detailed description of the change when necessary. The footer can include issue references or breaking changes.\n   * Example:\n     ```\n     feat(authentication): add support for OAuth2 login\n\n     This change adds OAuth2 login support to the authentication module, \n     providing the ability to authenticate users via third-party OAuth2 providers.\n\n     BREAKING CHANGE: This alters the initialization flow, requiring reconfiguration of OAuth2 settings.\n     ```\n\n### Benefits\n1. **Improved Readability**: Commit messages will be more consistent and informative, making it easier for team members to understand changes.\n2. **Automated Tools**: Structured commit messages enable the use of tools for generating changelogs, release notes, and semantic versioning.\n3. **Maintenance**: Structured commit messages provide a clear history of changes, facilitating easier project maintenance and onboarding of new team members.\n\n### Implementation\n1. **Training**: Provide team members with initial training on how to write Conventional Commits.\n2. **Tools Integration**: Configure tools like commitlint to enforce commit message rules.\n3. **Documentation**: Update project documentation to include guidelines and examples for writing commit messages according to the Conventional Commits specification.\n\nBy establishing these guidelines, we ensure that commit messages remain a helpful and essential part of our project's development workflow.","GenTime":"2024-08-20 00:25:18"}
{"File Name":"gsp\/ADR041-service-operated-policies.md","Context":"## Context\\nOur service-operator allows service teams to provision various AWS services by\\ndeclaratively defining resources and submitting them via the kubernetes api.\\nSome of these resources require IAM to authorise how the provisioned service\\ncan be used. The types of actions that can be performed on.\\n#### Example\\nThe service operator allows provisioning of S3 buckets and bucket configuration such as:\\n```\\n---\\napiVersion: storage.govsvc.uk\/v1beta1\\nkind: S3Bucket\\nmetadata:\\nname: s3-bucket-sample\\nspec:\\naws:\\nLifecycleRules:\\n- Expiration: 90days\\nVersioning:\\nEnabled: true\\n```\\nIn order to access a provisioned bucket via the the AWS SDK users will require\\nan IAM role\/policy that allows access.\\nWe want things like bucket ACL, versioning configuration and lifecycle policy\\nto be defined declaratively via the resource manifest (see example above), and continuously managed\\nby the service operator.\\nWe want users of the provisioned bucket to be able to read back all\\nconfiguration, and be able to fully utilise the specific bucket for reading,\\nwriting and managing their objects within the provisioned bucket, but we want\\nto avoid giving permissions to users that could cause conflicts with the\\nproperties that are managed by the service operator's reconcile loop.\\nFor example, given the example manifest above, we would like to avoid giving\\npermissions that would allow a user to alter the Expiration LifeCycleRules,\\nsince any changes the user made would be periodically overwritten by the\\nservice operator's reconciliation.\\n","Decision":"* We will provision policy that gives full access for users to _use_ the\\nprovisioned service.\\n* We will avoid provisioning policy that allows users to create, destroy or\\nconfigure the provisioned service, so that this can remain the declarative\\ndomain of the service-operator.\\n","tokens":335,"id":3907,"text":"## Context\\nOur service-operator allows service teams to provision various AWS services by\\ndeclaratively defining resources and submitting them via the kubernetes api.\\nSome of these resources require IAM to authorise how the provisioned service\\ncan be used. The types of actions that can be performed on.\\n#### Example\\nThe service operator allows provisioning of S3 buckets and bucket configuration such as:\\n```\\n---\\napiVersion: storage.govsvc.uk\/v1beta1\\nkind: S3Bucket\\nmetadata:\\nname: s3-bucket-sample\\nspec:\\naws:\\nLifecycleRules:\\n- Expiration: 90days\\nVersioning:\\nEnabled: true\\n```\\nIn order to access a provisioned bucket via the the AWS SDK users will require\\nan IAM role\/policy that allows access.\\nWe want things like bucket ACL, versioning configuration and lifecycle policy\\nto be defined declaratively via the resource manifest (see example above), and continuously managed\\nby the service operator.\\nWe want users of the provisioned bucket to be able to read back all\\nconfiguration, and be able to fully utilise the specific bucket for reading,\\nwriting and managing their objects within the provisioned bucket, but we want\\nto avoid giving permissions to users that could cause conflicts with the\\nproperties that are managed by the service operator's reconcile loop.\\nFor example, given the example manifest above, we would like to avoid giving\\npermissions that would allow a user to alter the Expiration LifeCycleRules,\\nsince any changes the user made would be periodically overwritten by the\\nservice operator's reconciliation.\\n\n\n##Decision\n* We will provision policy that gives full access for users to _use_ the\\nprovisioned service.\\n* We will avoid provisioning policy that allows users to create, destroy or\\nconfigure the provisioned service, so that this can remain the declarative\\ndomain of the service-operator.\\n","MatchedIds":[211,4707,2397,4025,3890],"Predicted":"## Decision\nTo ensure that the service operator retains control over configuration aspects like bucket ACL, versioning, and lifecycle policies, while still allowing users to effectively utilize the provisioned S3 buckets, the following decisions are made:\n\n1. **IAM Policy Management:**\n    - The service operator will create and manage IAM roles and policies that grant users the necessary permissions to access and interact with the provisioned S3 buckets. \n    - Users will be granted permissions for actions like `s3:GetObject`, `s3:PutObject`, and `s3:ListBucket` to enable reading, writing, and listing objects within the S3 buckets.\n    - Permissions that could lead to conflicts with the service operator's managed configurations, such as `s3:PutBucketLifecycleConfiguration` and `s3:PutBucketVersioning`, will not be included in these roles\/policies.\n\n2. **Declarative Configuration Management:**\n    - The configuration aspects like bucket ACLs, versioning, and lifecycle policies will be strictly controlled by the service operator through resource manifests.\n    - Any changes to these configurations will be managed by the service operator\u2019s reconciliation loop to ensure consistency and prevent conflicts.\n\n3. **Bucket Policy Implementation:**\n    - The service operator will enforce bucket policies that further restrict the permissions to manage configurations. This will prevent direct modifications by end-users in a way that could disrupt the desired configurations.\n\n4. **Role Segregation:**\n    - Distinguish between roles for configuration management and data operations. Configuration management roles will be highly restricted and used only by the service operator, while data operation roles will be more permissive but limited to data-related actions.\n\n5. **Auditing and Logging:**\n    - Implement logging and auditing for actions taken on the buckets. This will help in monitoring unauthorized access attempts and ensuring compliance with the defined policies.\n\n### Positive Consequences\n- Ensures consistency and integrity of bucket configurations by having them exclusively managed by the service operator.\n- Allows users to perform necessary operations on the bucket without conflicting with the service operator's managed configurations.\n- Simplifies the configuration management lifecycle by keeping it declarative and automated.\n\n### Negative Consequences\n- Users may have limited flexibility in managing certain configurations directly, potentially requiring more interaction with the service operator for specific needs.\n- Increased complexity in managing and maintaining IAM roles and policies to ensure least privilege and avoid configuration conflicts.","GenTime":"2024-08-20 00:25:25"}
{"File Name":"adr-tools\/0009-help-scripts.md","Context":"## Context\\nCurrently help text is generated by extracting specially formatted comments from the top of the command script.\\nThis makes it easy for developers of the tool:  documentation and code is all in one place.\\nBut, it means that help text cannot include calculated values, such as the location of files.\\n","Decision":"Where necessary, help text can be generated by a script.\\nThe script will be called _adr_help_<command>_<subcommand>\\n","tokens":63,"id":3571,"text":"## Context\\nCurrently help text is generated by extracting specially formatted comments from the top of the command script.\\nThis makes it easy for developers of the tool:  documentation and code is all in one place.\\nBut, it means that help text cannot include calculated values, such as the location of files.\\n\n\n##Decision\nWhere necessary, help text can be generated by a script.\\nThe script will be called _adr_help_<command>_<subcommand>\\n","MatchedIds":[3566,2191,3069,3815,1410],"Predicted":"## Decision\nWe will refactor the help text generation to support both static and dynamic content. We will:\n- Introduce a mechanism to include placeholders in the help text that can be replaced with calculated values at runtime.\n- Create a new function in the script that generates the help text by combining static comments and dynamic values.\n- Update the script to call this function when the help text is needed, ensuring that any required calculations are performed and included in the output.\n- Continue to maintain documentation in comments as much as possible, but extend it to allow for runtime augmentation.\n\nFor example, placeholders in comments could be denoted by a special syntax like `{{placeholder}}` and the function would replace these with the appropriate dynamic values.","GenTime":"2024-08-20 00:25:29"}
{"File Name":"winery\/0022-tosca-model-is-more-relaxed-than-the-xsd.md","Context":"## Context and Problem Statement\\nThere is a data model for a) serializing\/deserializing the XML contents, b) internal backend handling, c) working with algorithms, d) communicating with the REST service.\\nCurrently, this is the same model.\\nThe UI might generate non-valid XML files (in the sence of not passing the XSD validation).\\nFor instance, if a user creates a service template, that service template does not contain a topology template.\\nFurthermore, a topolgoy template needs to have at least one node template.\\n","Decision":"Chosen option: \"Keep one model and allow non-XSD validating models in `org.eclipse.winery.model.tosca`\", because\\n- XSD is meant for \"executable\" TOSCA definitions, not for intermediate modeling results\\n- currently too much effort to develop two models\\n","tokens":114,"id":4307,"text":"## Context and Problem Statement\\nThere is a data model for a) serializing\/deserializing the XML contents, b) internal backend handling, c) working with algorithms, d) communicating with the REST service.\\nCurrently, this is the same model.\\nThe UI might generate non-valid XML files (in the sence of not passing the XSD validation).\\nFor instance, if a user creates a service template, that service template does not contain a topology template.\\nFurthermore, a topolgoy template needs to have at least one node template.\\n\n\n##Decision\nChosen option: \"Keep one model and allow non-XSD validating models in `org.eclipse.winery.model.tosca`\", because\\n- XSD is meant for \"executable\" TOSCA definitions, not for intermediate modeling results\\n- currently too much effort to develop two models\\n","MatchedIds":[2342,4644,1172,1225,1031],"Predicted":"## Decision\nWe will decouple the data model into separate models for each context to improve validation, maintainability, and flexibility.\n\n### Why Decouple the Data Model?\n\n**Serialization\/Deserialization (XML Handling):**\nWe will create a specific data model that adheres strictly to the XSD schema for XML serialization and deserialization. This model will ensure that any XML generated will pass XSD validation and comply with all schema requirements, including mandatory elements like topology templates and node templates.\n\n**Internal Backend Handling:**\nThe internal data model will be designed to facilitate efficient processing and manipulation of data within the backend. This model will include structures optimized for backend logic and algorithms, free from the constraints of the XML schema.\n\n**Working with Algorithms:**\nA separate model optimized for algorithmic operations will be introduced. This will allow algorithms to operate efficiently without needing to consider XML-specific constraints and requirements.\n\n**Communicating with the REST Service:**\nThe REST service will use a data model designed for Web API interactions. This model will focus on simplicity and ease of use for clients, ensuring fast serialization and deserialization while maintaining a clear contract with clients.\n\n### Implementation Steps:\n\n1. **Define Separate Models:**\n   - Create an XML model strictly adhering to the XSD schema, ensuring all mandatory fields are always present.\n   - Develop an internal backend model optimized for data manipulation and processing.\n   - Implement an algorithmic model focusing on the efficient execution of business rules and logic.\n   - Define a REST service model curated for external communications, ensuring a clear and consistent API schema.\n\n2. **Implement Mapping Layers:**\n   - Develop mapping layers that transform data between these different models to keep them in sync.\n   - Ensure all data transformations include validation steps, such as checking the presence of mandatory fields like topology templates and node templates, during transformation from the internal model to the XML model.\n\n3. **UI Enhancements:**\n   - Update the UI to provide immediate feedback to users if they attempt to create non-valid structures (e.g., creating a service template without a topology template or a topology template without a node template).\n   - Integrate client-side validation techniques based on the defined REST service model to ensure that input from users aligns with backend expectations.\n\n### Positive Consequences:\n- Improved validation and adherence to schema requirements for XML files.\n- Enhanced maintainability by separating concerns for different aspects of the data model.\n- Increased flexibility to optimize each model for its specific context, improving performance.\n- Clearer contract and communication between the backend and REST clients.\n\n### Negative Consequences:\n- Increased complexity due to the need to maintain multiple data models.\n- Additional development effort required to implement mapping layers and ensure their correctness.\n- Potential for data synchronization issues if mapping layers are not properly managed.","GenTime":"2024-08-20 00:25:39"}
{"File Name":"git-en-boite\/0016-handle-concurrent-git-operations","Context":"## Context\\nConcurrent git operations can fail because git is locking the repo. The current implementation is waiting for jobs to finish, but multiple workers in different containers can still take jobs in the queue and proccess them in parallel.\\n","Decision":"Reduce worker concurrency to 1 by using a single container (for now).\\n","tokens":47,"id":3497,"text":"## Context\\nConcurrent git operations can fail because git is locking the repo. The current implementation is waiting for jobs to finish, but multiple workers in different containers can still take jobs in the queue and proccess them in parallel.\\n\n\n##Decision\nReduce worker concurrency to 1 by using a single container (for now).\\n","MatchedIds":[210,1434,4821,3915,3504],"Predicted":"## Decision\nWe will implement a centralized locking mechanism to ensure that only one worker can perform git operations on the repository at a time. This lock will be managed using a distributed lock service like Redis or etcd. Workers will acquire the lock before starting a git operation and release it once the operation is completed. This will prevent concurrent git operations and ensure the consistency and integrity of the repository.","GenTime":"2024-08-20 00:25:42"}
{"File Name":"opg-lpa\/0006-modernise-the-code-base.md","Context":"## Context\\nWe have inherited a relatively large and complex legacy code base, mostly written in PHP.\\nPHP [appears to be on a downwards trend as a language](https:\/\/pypl.github.io\/PYPL.html?country=GB),\\nespecially in contrast with Python. It's likely it will become increasingly difficult\\nto find good PHP developers in future.\\nAnecdotally, PHP is not seen as a desirable language for developers to work with. It doesn't\\nhave the cool factor of newer languages like golang; nor the clean syntax and API of\\nlanguages of similar pedigree, such as Python.\\nOur code base is also showing its age somewhat. Some of the libraries are starting to rot.\\nA mix of contractors and developers working on the code base over several years has\\nresulted in a mix of styles and approaches. While we have already cleared out a lot\\nof unused and\/or broken code, there is likely to be more we haven't found yet.\\nWe are also lagging behind the latest Design System guidelines, as our application was one\\nof the first to go live, before the current iteration of the Design System existed.\\nThis means that any changes to design have to be done piecemeal and manually: we can't\\nsimply import the newest version of the design system and have everything magically update.\\nThis combination of factors means that the code base can be difficult to work with:\\nresistant to change and easy to break.\\n","Decision":"We have decided to modernise the code base to make it easier to work with and better\\naligned with modern web architecture and standards. This is not a small job, but\\nthe guiding principles we've decided on, shown below, should help us achieve our aims.\\n(\"Modernising the code base\" is not to be confused with \"modernising LPAs\". Here\\nwe're just talking about modernising the code base for the Make an LPA tool.)\\n* **Don't rewrite everything at once**\\nWhere possible, migrate part of an application to a new\\ncomponent and split traffic coming into the domain so that some paths are diverted to that\\ncomponent. This will typically use nginx in dev, but may be done at the AWS level if\\nappropriate (e.g in a load balancer or application gateway).\\nThis is challenging, but means that we don't have to do a \"big bang\" release of the new\\nversion of the tool. Our aim is to gradually replace existing components with new\\nones, which are (hopefully) simpler, future-proofed, more efficient, and don't rely on PHP.\\n* **Use Python for new work**\\nWe considered golang, but don't have the experience in the team to build applications with it.\\nWe felt that learning a new language + frameworks would only reduce our ability to deliver, with\\nminimal benefits: our application is not under heavy load and responds in an\\nacceptable amount of time, so golang's super efficiency isn't essential.\\nWe feel that we could scale horizontally if necessary and have not had any major issues\\nwith capacity in the past.\\n* **Choose containers or lambdas as appropriate**\\nUse a container for components which stay up most of the time, and lambdas for\\n\"bursty\" applications (e.g. background processes like PDF generation, daily statistics aggregation).\\n* **Choose the right lambda for the job**\\nUse \"pure\" lambdas where possible. This is only the case where an application has simple dependencies\\nwhich don't require unusual native libraries outside the\\n[stock AWS Docker images for lambdas](https:\/\/gallery.ecr.aws\/lambda\/python)).\\nIf a component is problematic to run as a pure lambda, use a lambda running a Docker image based\\non one of the stock AWS Docker images for lambdas.\\n* **Choose the right Docker image**\\nWhen using Docker images, prefer the following:\\n* Images based on AWS Lambda images (if writing a component which will run as a Docker lambda).\\n* Images based on Alpine (for other cases).\\n* Images based on a non-Alpine foundation like Ubuntu, but only if an Alpine image is not available.\\n* **Use Flask and gunicorn**\\nUse [Flask](https:\/\/flask.palletsprojects.com\/) for new Python web apps, fronted by\\n[gunicorn](https:\/\/gunicorn.org\/) for the WSGI implementation.\\n* **Use the latest Design System**\\nUse the [Government Design System](https:\/\/design-system.service.gov.uk\/) guidelines for new UI. In\\nparticular, use the\\n[Land Registry's Python implementation of the design system](https:\/\/github.com\/LandRegistry\/govuk-frontend-jinja),\\nwritten as [Jinja2 templates](https:\/\/jinja.palletsprojects.com\/).\\nOur aim should be to utilise it without modification as far as possible, so that we can easily upgrade\\nif it is changed by developers at the Land Registry.\\n* **Migrate legacy code to PHP 8**\\nWhere possible, upgrade PHP applications to PHP 8, when supported by [Laminas](https:\/\/getlaminas.org\/).\\nAt the time of writing, Laminas support for PHP 8 is only partial, so we are stuck with PHP 7 for now,\\nas large parts of our stack are implemented on top of Laminas.\\n* **Specify new APIs with OpenAPI**\\nSpecify new APIs using [OpenAPI](https:\/\/swagger.io\/specification\/). Ideally, use tooling\\nwhich enables an API to be automatically built from an OpenAPI specification, binding to\\ncode only when necessary, to avoid repetitive boilerplate.\\n* **Controlled, incremental releases**\\nProvision new infrastructure behind a feature flag wherever possible. This allows us to\\nwork on new components, moving them into the live environment as they are ready, but hidden\\nfrom the outside world. When ready for delivery, we switch the flag over to make that piece\\nof infrastructure live.\\n* **Follow good practices for web security**\\nBe aware of the [OWASP Top Ten](https:\/\/owasp.org\/www-project-top-ten\/) and code to avoid those\\nissues. Use tools like [Talisman](https:\/\/github.com\/GoogleCloudPlatform\/flask-talisman) to\\nimprove security.\\n* **Be mindful of accessibility**\\nConsider accessibility requirements at every step of the design and coding phases. Aim to\\ncomply with [WCAG 2.1 Level AA](https:\/\/www.w3.org\/WAI\/WCAG22\/quickref\/) as a minimum. While the\\nDesign System helps a lot with this, always bear accessibility in mind when building workflows\\nand custom components it doesn't cover.\\n* **Be properly open source**\\nMake the code base properly open source. While our code is open, there are still barriers to entry\\nfor developers outside the Ministry of Justice, such as the requirement to have access to AWS secrets,\\nS3, postcode API, the Government payment gateway and SendGrid for the system to work correctly. We\\nwill work towards removing these barriers so that onboarding of new developers (internally and\\nexternally) is seamless, and to enable potentially anyone to fully contribute to the project.\\n* **Improve test coverage everywhere**\\nAs we work on the code, be aware of gaps in testing and plug them as they arise. Don't wait for\\nan opportunity to fix everything at once: make refactoring and adding unit tests part of the\\nwork on an issue (unless it's going to take longer than working on the issue!).\\nWhere a whole category of testing is missing, add it (for example, we\\nhave recently implemented the foundations for load testing; see\\n[0004-implement-load-testing](.\/0004-implement-load-testing.md)).\\n* **Automate code quality metrics**\\nIntroduce tools to lint and scan code as we go, to ensure consistent, easy-to-follow code. See\\n[0003-linting-and-scanning](.\/0003-linting-and-scanning.md)) for a starting point.\\n* **Peer review everything**\\nAll commits to the code base must go through peer review before merging. No lone wolf developers.\\n* **Be pragmatic**\\nSee the [pragmatic quick reference](https:\/\/www.ccs.neu.edu\/home\/lieber\/courses\/csg110\/sp08\/Pragmatic%20Quick%20Reference.htm)\\nfor a summary. These are generally good principles for software engineering.\\n","tokens":306,"id":1652,"text":"## Context\\nWe have inherited a relatively large and complex legacy code base, mostly written in PHP.\\nPHP [appears to be on a downwards trend as a language](https:\/\/pypl.github.io\/PYPL.html?country=GB),\\nespecially in contrast with Python. It's likely it will become increasingly difficult\\nto find good PHP developers in future.\\nAnecdotally, PHP is not seen as a desirable language for developers to work with. It doesn't\\nhave the cool factor of newer languages like golang; nor the clean syntax and API of\\nlanguages of similar pedigree, such as Python.\\nOur code base is also showing its age somewhat. Some of the libraries are starting to rot.\\nA mix of contractors and developers working on the code base over several years has\\nresulted in a mix of styles and approaches. While we have already cleared out a lot\\nof unused and\/or broken code, there is likely to be more we haven't found yet.\\nWe are also lagging behind the latest Design System guidelines, as our application was one\\nof the first to go live, before the current iteration of the Design System existed.\\nThis means that any changes to design have to be done piecemeal and manually: we can't\\nsimply import the newest version of the design system and have everything magically update.\\nThis combination of factors means that the code base can be difficult to work with:\\nresistant to change and easy to break.\\n\n\n##Decision\nWe have decided to modernise the code base to make it easier to work with and better\\naligned with modern web architecture and standards. This is not a small job, but\\nthe guiding principles we've decided on, shown below, should help us achieve our aims.\\n(\"Modernising the code base\" is not to be confused with \"modernising LPAs\". Here\\nwe're just talking about modernising the code base for the Make an LPA tool.)\\n* **Don't rewrite everything at once**\\nWhere possible, migrate part of an application to a new\\ncomponent and split traffic coming into the domain so that some paths are diverted to that\\ncomponent. This will typically use nginx in dev, but may be done at the AWS level if\\nappropriate (e.g in a load balancer or application gateway).\\nThis is challenging, but means that we don't have to do a \"big bang\" release of the new\\nversion of the tool. Our aim is to gradually replace existing components with new\\nones, which are (hopefully) simpler, future-proofed, more efficient, and don't rely on PHP.\\n* **Use Python for new work**\\nWe considered golang, but don't have the experience in the team to build applications with it.\\nWe felt that learning a new language + frameworks would only reduce our ability to deliver, with\\nminimal benefits: our application is not under heavy load and responds in an\\nacceptable amount of time, so golang's super efficiency isn't essential.\\nWe feel that we could scale horizontally if necessary and have not had any major issues\\nwith capacity in the past.\\n* **Choose containers or lambdas as appropriate**\\nUse a container for components which stay up most of the time, and lambdas for\\n\"bursty\" applications (e.g. background processes like PDF generation, daily statistics aggregation).\\n* **Choose the right lambda for the job**\\nUse \"pure\" lambdas where possible. This is only the case where an application has simple dependencies\\nwhich don't require unusual native libraries outside the\\n[stock AWS Docker images for lambdas](https:\/\/gallery.ecr.aws\/lambda\/python)).\\nIf a component is problematic to run as a pure lambda, use a lambda running a Docker image based\\non one of the stock AWS Docker images for lambdas.\\n* **Choose the right Docker image**\\nWhen using Docker images, prefer the following:\\n* Images based on AWS Lambda images (if writing a component which will run as a Docker lambda).\\n* Images based on Alpine (for other cases).\\n* Images based on a non-Alpine foundation like Ubuntu, but only if an Alpine image is not available.\\n* **Use Flask and gunicorn**\\nUse [Flask](https:\/\/flask.palletsprojects.com\/) for new Python web apps, fronted by\\n[gunicorn](https:\/\/gunicorn.org\/) for the WSGI implementation.\\n* **Use the latest Design System**\\nUse the [Government Design System](https:\/\/design-system.service.gov.uk\/) guidelines for new UI. In\\nparticular, use the\\n[Land Registry's Python implementation of the design system](https:\/\/github.com\/LandRegistry\/govuk-frontend-jinja),\\nwritten as [Jinja2 templates](https:\/\/jinja.palletsprojects.com\/).\\nOur aim should be to utilise it without modification as far as possible, so that we can easily upgrade\\nif it is changed by developers at the Land Registry.\\n* **Migrate legacy code to PHP 8**\\nWhere possible, upgrade PHP applications to PHP 8, when supported by [Laminas](https:\/\/getlaminas.org\/).\\nAt the time of writing, Laminas support for PHP 8 is only partial, so we are stuck with PHP 7 for now,\\nas large parts of our stack are implemented on top of Laminas.\\n* **Specify new APIs with OpenAPI**\\nSpecify new APIs using [OpenAPI](https:\/\/swagger.io\/specification\/). Ideally, use tooling\\nwhich enables an API to be automatically built from an OpenAPI specification, binding to\\ncode only when necessary, to avoid repetitive boilerplate.\\n* **Controlled, incremental releases**\\nProvision new infrastructure behind a feature flag wherever possible. This allows us to\\nwork on new components, moving them into the live environment as they are ready, but hidden\\nfrom the outside world. When ready for delivery, we switch the flag over to make that piece\\nof infrastructure live.\\n* **Follow good practices for web security**\\nBe aware of the [OWASP Top Ten](https:\/\/owasp.org\/www-project-top-ten\/) and code to avoid those\\nissues. Use tools like [Talisman](https:\/\/github.com\/GoogleCloudPlatform\/flask-talisman) to\\nimprove security.\\n* **Be mindful of accessibility**\\nConsider accessibility requirements at every step of the design and coding phases. Aim to\\ncomply with [WCAG 2.1 Level AA](https:\/\/www.w3.org\/WAI\/WCAG22\/quickref\/) as a minimum. While the\\nDesign System helps a lot with this, always bear accessibility in mind when building workflows\\nand custom components it doesn't cover.\\n* **Be properly open source**\\nMake the code base properly open source. While our code is open, there are still barriers to entry\\nfor developers outside the Ministry of Justice, such as the requirement to have access to AWS secrets,\\nS3, postcode API, the Government payment gateway and SendGrid for the system to work correctly. We\\nwill work towards removing these barriers so that onboarding of new developers (internally and\\nexternally) is seamless, and to enable potentially anyone to fully contribute to the project.\\n* **Improve test coverage everywhere**\\nAs we work on the code, be aware of gaps in testing and plug them as they arise. Don't wait for\\nan opportunity to fix everything at once: make refactoring and adding unit tests part of the\\nwork on an issue (unless it's going to take longer than working on the issue!).\\nWhere a whole category of testing is missing, add it (for example, we\\nhave recently implemented the foundations for load testing; see\\n[0004-implement-load-testing](.\/0004-implement-load-testing.md)).\\n* **Automate code quality metrics**\\nIntroduce tools to lint and scan code as we go, to ensure consistent, easy-to-follow code. See\\n[0003-linting-and-scanning](.\/0003-linting-and-scanning.md)) for a starting point.\\n* **Peer review everything**\\nAll commits to the code base must go through peer review before merging. No lone wolf developers.\\n* **Be pragmatic**\\nSee the [pragmatic quick reference](https:\/\/www.ccs.neu.edu\/home\/lieber\/courses\/csg110\/sp08\/Pragmatic%20Quick%20Reference.htm)\\nfor a summary. These are generally good principles for software engineering.\\n","MatchedIds":[1000,1416,3853,783,929],"Predicted":"## Decision\nGiven the challenges presented by the legacy PHP code base and the diminishing popularity of PHP, we have decided to undertake a phased migration of the code base to Python. This decision is based on several factors:\n\n1. **Future-proofing**: Python is trending upwards and has a larger pool of talented developers, which will make future hiring and maintenance easier.\n2. **Developer Experience**: Python has a cleaner syntax and is generally more enjoyable to work with, which can increase developer productivity and satisfaction.\n3. **Libraries and Tools**: Python has a rich ecosystem of libraries and tools that can replace the aging, rotting libraries currently in use, improving both functionality and security.\n4. **Design System Alignment**: Migrating to Python will allow us to leverage modern development frameworks that are easily integrated with the latest Design System guidelines, ensuring a more consistent and up-to-date user interface.\n\n### Implementation Plan\n\n1. **Assessment**: Conduct a detailed assessment of the current PHP code base to identify the core components and dependencies.\n2. **Incremental Migration**: Migrate the code base incrementally to avoid large-scale disruptions. Start with non-critical components and services before moving to core functionalities.\n3. **Code Standards**: Establish stringent coding standards and guidelines for the new Python code to minimize the mix of styles and approaches that plagued the PHP code.\n4. **Automated Testing**: Implement automated testing to ensure that the migration does not introduce new bugs or regressions.\n5. **Training**: Provide training sessions and resources for the current team to get up to speed with Python, including best practices for the new code base.\n\nThis decision will be re-evaluated in 12 months to assess progress and make adjustments as necessary. Current critical and high-performing PHP components will continue to be maintained during the transition period to ensure service continuity.","GenTime":"2024-08-20 00:25:51"}
{"File Name":"reaction-component-library\/0007-publish-components.md","Context":"## Context\\nWe need to be able to pull in these components to other apps as one or more NPM packages. There are pros and cons to a single-package approach versus one package per component.\\n### One Package Per Component\\nPros:\\n- Minimum dependencies pulled in with each component used. You don't download or package anything you aren't using.\\n- When you need to pull in a fix to one component, you aren't unintentionally changing the behavior of a hundred other components.\\nCons:\\n- How do we track dependencies per component but also install them so that the whole Style Guide app can run as one?\\n- Each component package has to be installed by developers as it is needed\\n- Publishing them will be tricky, especially if there are any interdependencies. Lerna + semantic-release may help, but the typical Lerna repo structure may not be ideal with the context of the Style Guide app we have here.\\n### A Single Package Exporting All Components\\nPros:\\n- Install a single package and you get access to them all in your app\\n- Much simpler to publish vs multiple packages\\n- Easier for people working on the style guide repo to understand.\\nCons:\\n- Every time you bump the dependency version of the component package, you'll pull in potentially changed versions of every component in your app.\\n- Extra work will be required to ensure that bundled JavaScript does not include any components that an app does not use.\\n### What about the style guide\\nThere is a further complication here, which is \"how does the style guide release cycle relate to the release cycle of the components within it, and how does the style guide represent multiple versions of components as they change?\"\\n","Decision":"The complexity of setting up, maintaining, and understanding a one-package-per-component approach, even with the help of tools like Lerna, is a very strong con. So if we assume that a single package is our preference, let's look at how we might mitigate the cons of that apprach.\\nThe first con, pulling in unwanted component changes, is MAJOR. This causes a lot of headaches for a lot of people. There are a few things we can do to avoid it:\\n- Every component change that changes the appearance of a component in any way should be exported as a new component. The whole component folder can be copied into a `v2` folder. That way, any app that pulls in an updated package will not see any changes until they change `import { Button } from \"@reactioncommerce\/components\/v1` to `import { Button } from \"@reactioncommerce\/components\/v2`\\n- No React component in this library will import any other React component in this library. All components that a component needs to render will be passed in as props or children.\\n- Every component will have Jest snapshot testing, which will give a clue to developers and reviewers that a component's appearance may have changed. Part of the review process can be deciding whether the changes are visual and whether they require splitting off a new version of the component (i.e. are considered \"breaking\" changes). There is the possibility of automating this even more through automated screen shot comparison.\\nThis approach also answers the question of how the style guide will show all versions of the component. Since all versions will remain in the repo in versioned folder names, they'll just naturally appear.\\nThe second con of a single package can be avoided by the following suggestion:\\n- Apps using the component package should set up a \"tree shaking\" solution to avoid bundling any components they do not import.\\n","tokens":350,"id":3031,"text":"## Context\\nWe need to be able to pull in these components to other apps as one or more NPM packages. There are pros and cons to a single-package approach versus one package per component.\\n### One Package Per Component\\nPros:\\n- Minimum dependencies pulled in with each component used. You don't download or package anything you aren't using.\\n- When you need to pull in a fix to one component, you aren't unintentionally changing the behavior of a hundred other components.\\nCons:\\n- How do we track dependencies per component but also install them so that the whole Style Guide app can run as one?\\n- Each component package has to be installed by developers as it is needed\\n- Publishing them will be tricky, especially if there are any interdependencies. Lerna + semantic-release may help, but the typical Lerna repo structure may not be ideal with the context of the Style Guide app we have here.\\n### A Single Package Exporting All Components\\nPros:\\n- Install a single package and you get access to them all in your app\\n- Much simpler to publish vs multiple packages\\n- Easier for people working on the style guide repo to understand.\\nCons:\\n- Every time you bump the dependency version of the component package, you'll pull in potentially changed versions of every component in your app.\\n- Extra work will be required to ensure that bundled JavaScript does not include any components that an app does not use.\\n### What about the style guide\\nThere is a further complication here, which is \"how does the style guide release cycle relate to the release cycle of the components within it, and how does the style guide represent multiple versions of components as they change?\"\\n\n\n##Decision\nThe complexity of setting up, maintaining, and understanding a one-package-per-component approach, even with the help of tools like Lerna, is a very strong con. So if we assume that a single package is our preference, let's look at how we might mitigate the cons of that apprach.\\nThe first con, pulling in unwanted component changes, is MAJOR. This causes a lot of headaches for a lot of people. There are a few things we can do to avoid it:\\n- Every component change that changes the appearance of a component in any way should be exported as a new component. The whole component folder can be copied into a `v2` folder. That way, any app that pulls in an updated package will not see any changes until they change `import { Button } from \"@reactioncommerce\/components\/v1` to `import { Button } from \"@reactioncommerce\/components\/v2`\\n- No React component in this library will import any other React component in this library. All components that a component needs to render will be passed in as props or children.\\n- Every component will have Jest snapshot testing, which will give a clue to developers and reviewers that a component's appearance may have changed. Part of the review process can be deciding whether the changes are visual and whether they require splitting off a new version of the component (i.e. are considered \"breaking\" changes). There is the possibility of automating this even more through automated screen shot comparison.\\nThis approach also answers the question of how the style guide will show all versions of the component. Since all versions will remain in the repo in versioned folder names, they'll just naturally appear.\\nThe second con of a single package can be avoided by the following suggestion:\\n- Apps using the component package should set up a \"tree shaking\" solution to avoid bundling any components they do not import.\\n","MatchedIds":[1631,3035,3827,1992,36],"Predicted":"## Decision\nWe'll use a monorepo setup with Lerna to manage multiple packages: one for each component.\n\n### Reasoning\n1. **Better Dependency Management**: By creating a package for each component, we ensure that applications using these components only pull in the dependencies they need, increasing the efficiency of their builds and reducing the chance of unintentional behavior changes in components they are not using.\n\n2. **Flexibility in Component Updates**: Developers can pull in updates for a single component without impacting others, reducing the risk of widespread issues caused by updates. This approach allows for more controlled and incremental adoption of updates.\n\n3. **Organizational Benefits**: Although managing multiple packages does add complexity, tools like Lerna and semantic-release can streamline the process. Lerna will help manage dependencies across the packages, and semantic-release will ensure accurate and consistent versioning.\n\n4. **Style Guide Integration**: By using Lerna in a monorepo, we can ensure that all component packages are easily accessible and can be used within the style guide without needing separate installs. The style guide can be set up to reflect the most current versions of all components, making it easier for designers and developers to work with up-to-date information.\n\n### Positive Consequences\n- **Granular Control**: Developers can precisely control which versions of components they pull into their projects, reducing the risk of unexpected changes.\n- **Isolated Updates**: Publishing fixes or features for individual components won't require a new version of the entire package.\n- **Reduced Dependency Bloat**: Only the necessary dependencies are included, potentially improving build times and application performance.\n\n### Negative Consequences\n- **Increased Management Complexity**: Managing multiple packages adds overhead in terms of setting up and maintaining the build and release processes.\n- **Learning Curve for Developers**: Developers need to be familiar with managing a monorepo and using tools like Lerna and semantic-release.\n- **Potential Interdependencies**: Handling interdependencies between components will require careful management to avoid circular dependencies and ensure smooth integration.\n\n### Implementation Steps\n1. **Setup Monorepo with Lerna**: Initialize a monorepo and configure Lerna to manage multiple packages.\n2. **Create Individual Component Packages**: Create a package for each reusable component with its own dependencies and versioning.\n3. **Integrate with Style Guide**: Ensure that the style guide can reference and demonstrate each component from the monorepo.\n4. **Setup Continuous Integration (CI)**: Implement CI pipelines to automatically build, test, and publish packages using Lerna and semantic-release.\n\nOverall, this approach strikes a balance between flexibility, performance, and maintainability, ensuring that our applications and style guide can evolve efficiently with minimal friction.","GenTime":"2024-08-20 00:26:03"}
{"File Name":"sdbmss\/0002-build-crud-and-search.md","Context":"## Context\\nThe Schoenberg Institute won three-year NEH grant to rebuild the existing Schoenberg Database of Manuscripts to\\nreplace the then current SDBM which was written in ColdFusion, lacked critical data model features and user-based editing.\\nThe new application needed a more flexible, user contribution, complex search and an up-to-date technology stack.\\n","Decision":"The following technologies were select for the following reasons.\\n- Blacklight, which uses Solr, was chosen to provide complex, configurable search, and uses technology common in the library, and for which long term product support could be expected. Blacklight was also used for bookmarking.\\n- Rails is required by Blacklight\\n- MySQL was selected as it was the database of choice within library technology services, and supported by sysadmin staff\\n- Solr is required by Blacklight\\n- Delayed Job (URL) was chosen as for orchestrating background jobs, namely sending updates from the database to the Solr index\\n- Sunspot was chosen for the Rails-Solr interaction\\n- AngularJS was chosen to support complex, dynamic page interactions\\n- cancancan was selected to provide tiered, role-based access for users with different permission levels\\n","tokens":72,"id":1837,"text":"## Context\\nThe Schoenberg Institute won three-year NEH grant to rebuild the existing Schoenberg Database of Manuscripts to\\nreplace the then current SDBM which was written in ColdFusion, lacked critical data model features and user-based editing.\\nThe new application needed a more flexible, user contribution, complex search and an up-to-date technology stack.\\n\n\n##Decision\nThe following technologies were select for the following reasons.\\n- Blacklight, which uses Solr, was chosen to provide complex, configurable search, and uses technology common in the library, and for which long term product support could be expected. Blacklight was also used for bookmarking.\\n- Rails is required by Blacklight\\n- MySQL was selected as it was the database of choice within library technology services, and supported by sysadmin staff\\n- Solr is required by Blacklight\\n- Delayed Job (URL) was chosen as for orchestrating background jobs, namely sending updates from the database to the Solr index\\n- Sunspot was chosen for the Rails-Solr interaction\\n- AngularJS was chosen to support complex, dynamic page interactions\\n- cancancan was selected to provide tiered, role-based access for users with different permission levels\\n","MatchedIds":[1839,2582,5009,4021,3470],"Predicted":"## Decision\nWe will rebuild the Schoenberg Database of Manuscripts (SDBM) using a modern, robust technology stack centered around Python and Django. This framework will allow us to achieve the following objectives:\n\n1. **Flexible Data Model**: Utilize Django's ORM to design a scalable and flexible data model capable of accommodating the complex requirements of manuscript data.\n2. **User Contributions**: Implement robust user authentication and authorization using Django's built-in system complemented by third-party packages such as JWT (JSON Web Tokens) for enhanced security.\n3. **Complex Search**: Enhance search capabilities using Elasticsearch integrated with Django via the Django-Haystack library, providing full-text search, faceting, and other advanced search features.\n4. **Modern Technology Stack**: Move away from ColdFusion to a more maintainable and widely-supported stack that includes PostgreSQL for the database, React.js for a dynamic and responsive front-end, and Docker for containerization to ensure consistency across development, staging, and production environments.\n\nThis approach will allow us to leverage the latest advancements in web technologies, ensure long-term maintainability, and better serve the needs of our diverse user base.","GenTime":"2024-08-20 00:26:09"}
{"File Name":"james-project\/0009-disable-elasticsearch-dynamic-mapping.md","Context":"## Context\\nWe rely on dynamic mappings to expose our mail headers as a JSON map. Dynamic mapping is enabled for adding not yet encountered headers in the mapping.\\nThis causes a serie of functional issues:\\n- Maximum field count can easily be exceeded\\n- Field type 'guess' can be wrong, leading to subsequent headers omissions [1]\\n- Document indexation needs to be paused at the index level during mapping changes to avoid concurrent changes, impacting negatively performance.\\n","Decision":"Rely on nested objects to represent mail headers within a mapping\\n","tokens":96,"id":2884,"text":"## Context\\nWe rely on dynamic mappings to expose our mail headers as a JSON map. Dynamic mapping is enabled for adding not yet encountered headers in the mapping.\\nThis causes a serie of functional issues:\\n- Maximum field count can easily be exceeded\\n- Field type 'guess' can be wrong, leading to subsequent headers omissions [1]\\n- Document indexation needs to be paused at the index level during mapping changes to avoid concurrent changes, impacting negatively performance.\\n\n\n##Decision\nRely on nested objects to represent mail headers within a mapping\\n","MatchedIds":[475,485,2896,2145,2880],"Predicted":"## Decision\nWe will disable dynamic mappings for mail headers in Elasticsearch. Instead, we will establish a predefined static mapping for commonly encountered headers. This predefined mapping includes specific field names and types to ensure consistency and avoid type guessing issues.\n\nTo handle new or uncommon headers that are not part of the static mapping, we will use a defined schema update process. This process will periodically review incoming data, identify new headers, and update the schema accordingly during scheduled maintenance windows to avoid impacting performance.\n\nAdditionally, we will implement a logging mechanism to track any headers that were not indexed due to being outside the predefined static mapping. This log will be periodically reviewed, and necessary schema updates will be incorporated in the next scheduled maintenance.\n\nBy implementing static mappings and a controlled schema update process, we can prevent the issues of maximum field count exceedance, incorrect field type guesses, and performance hits due to dynamic mapping changes.","GenTime":"2024-08-20 00:26:12"}
{"File Name":"webwritertechandhumanity.com\/0002-how-to-implement-special-pages.md","Context":"## Context\\nI need to implement two special pages, the privacy policy page and the about me page.\\nThey can't be in the map, I don't want them there because the user would\\nbe forced to read them when scanning the spiral.\\n","Decision":"I'll implement solution 1 because solution 2 is complicated and I want to prioritise\\nthe release of a first working website.\\n","tokens":52,"id":5001,"text":"## Context\\nI need to implement two special pages, the privacy policy page and the about me page.\\nThey can't be in the map, I don't want them there because the user would\\nbe forced to read them when scanning the spiral.\\n\n\n##Decision\nI'll implement solution 1 because solution 2 is complicated and I want to prioritise\\nthe release of a first working website.\\n","MatchedIds":[1883,1894,2207,3982,160],"Predicted":"## Decision\nImplement separate, standalone routes for the privacy policy and about me pages, ensuring they are accessible but not included in the main site map or navigation spiral.\n\n### Rationale\n- This allows users to access these pages when needed without disrupting the primary user journey.\n- These pages are typically less frequently accessed and contain important information that should not interrupt regular content consumption.\n\n### Implementation\n- Create isolated routes (e.g., `\/privacy-policy` and `\/about-me`) for these pages.\n- Add links to these routes in the footer or other secondary navigation sections rather than the main navigation.\n\n### Positive Consequences\n- Maintains focus on the primary content and user journey within the main site map or navigation spiral.\n- Provides easy access to legal and informational content without disrupting the user experience.\n\n### Negative Consequences\n- Users may need to scroll to the footer or search for these links if they don't notice them initially.\n\nReferences:\n- [Best Practices for Standalone Pages](https:\/\/webdev.bestpractices.com\/standalone-pages)\n- [SEO Considerations for Privacy Policy Pages](https:\/\/seo.considerations.com\/privacy-policy)","GenTime":"2024-08-20 00:26:17"}
{"File Name":"nr-arch\/2020-04-24-ARCH-OpenJDK-Versions.md","Context":"## Context\\nThis is a response from IITD Architecture to the IITD Infrastructure Team, and others, for an Official Statement on OpenJDK positioning.\\n* Status: proposed\\n* Deciders: Licence Change\\n* Date: 2020-08-27\\nTechnical Story: [description | <https:\/\/apps.nrs.gov.bc.ca\/int\/jira\/browse\/ARCH-62]>\\n## Context and Problem Statement\\nThis is a response from IITD Architecture to the IITD Infrastructure Team, and others, for an Official Statement on OpenJDK positioning.\\n## Decision Drivers\\n* An official Information Innovation & Technology Division (IITD) statement is needed on the direction forward for target versions of Java\/JDKs for all IITD hosted java applications.  Oracle has changed its licensing model, and will now be using a subscription model for charging for the use of its Oracle Java SE product releases.\\n* Java SE 8 is the end of the legacy versioning and release cadence model; Java 9 was the new beginning.\\n* Oracle extended the public updates of its Oracle JDK to January 2019 for commercial production use, and at least the end of 2020 for individual desktop use.\\n* Oracle JDK should be, for the most part, interchangeable with Oracle\u2019s OpenJDK builds for those who want to move up to the next releases, as they did for Java 6->7 and Java 7->8.\\n* Java 10 is the suggested release by Oracle\\n* Oracle has posted more information on the roadmap for the Java Client (including Applets and Web Start).\\n* Oracle plans to discontinue contributions to the JDK 8 Updates project until January 2019.\\n* For security reasons, IITD Architecture encourages upgrade\/migration of all java applications to at least JDK 11.\\n* Scope - IITD all server and client side applications owned by IITD that run Java.\\n","Decision":"* An official Information Innovation & Technology Division (IITD) statement is needed on the direction forward for target versions of Java\/JDKs for all IITD hosted java applications.  Oracle has changed its licensing model, and will now be using a subscription model for charging for the use of its Oracle Java SE product releases.\\n* Java SE 8 is the end of the legacy versioning and release cadence model; Java 9 was the new beginning.\\n* Oracle extended the public updates of its Oracle JDK to January 2019 for commercial production use, and at least the end of 2020 for individual desktop use.\\n* Oracle JDK should be, for the most part, interchangeable with Oracle\u2019s OpenJDK builds for those who want to move up to the next releases, as they did for Java 6->7 and Java 7->8.\\n* Java 10 is the suggested release by Oracle\\n* Oracle has posted more information on the roadmap for the Java Client (including Applets and Web Start).\\n* Oracle plans to discontinue contributions to the JDK 8 Updates project until January 2019.\\n* For security reasons, IITD Architecture encourages upgrade\/migration of all java applications to at least JDK 11.\\n* Scope - IITD all server and client side applications owned by IITD that run Java.\\nJDK 8 & Later\\nOracles OpenJDK JDK binaries for Windows, macOS, and Linux are available on release-specific pages of jdk.java.net as .tar.gz or .zip archives.\\nAs an example, the archives for JDK 13 may be found on jdk.java.net\/13 and may be extracted on the command line using\\n$ tar xvf openjdk-13*_bin.tar.gz\\nor\\n$ unzip openjdk-13*_bin.zip\\ndepending on the archive type.\\n### Positive Consequences\\n* removes the dependencies on Oracle JDK Licensing\\n* reduces security vulnerabilities of older JDK versions\\nJava 7 is still in predominant use. It goes without saying that any version of Java below 7 should be updated immediately even version 7 needs significant remediation for its fleet of vulnerabilities.\\nFurther vulnerabilities -\\n* <https:\/\/www.cvedetails.com\/product\/19117\/Oracle-JRE.html?vendor_id=93>\\n*\t<https:\/\/www.cvedetails.com\/product\/23642\/Oracle-Openjdk.html?vendor_id=93>\\n### Negative Consequences\\n* slow performance may occur\\n* migration issues will need to be addressed\\n* Migrate all Java JDK dependencies from Oracle JDK to OpenJDK.\\n* Upgrade all older versions to at least JDK 8, preference is to encourage teams to target move to JDK 11.\\n","tokens":403,"id":4802,"text":"## Context\\nThis is a response from IITD Architecture to the IITD Infrastructure Team, and others, for an Official Statement on OpenJDK positioning.\\n* Status: proposed\\n* Deciders: Licence Change\\n* Date: 2020-08-27\\nTechnical Story: [description | <https:\/\/apps.nrs.gov.bc.ca\/int\/jira\/browse\/ARCH-62]>\\n## Context and Problem Statement\\nThis is a response from IITD Architecture to the IITD Infrastructure Team, and others, for an Official Statement on OpenJDK positioning.\\n## Decision Drivers\\n* An official Information Innovation & Technology Division (IITD) statement is needed on the direction forward for target versions of Java\/JDKs for all IITD hosted java applications.  Oracle has changed its licensing model, and will now be using a subscription model for charging for the use of its Oracle Java SE product releases.\\n* Java SE 8 is the end of the legacy versioning and release cadence model; Java 9 was the new beginning.\\n* Oracle extended the public updates of its Oracle JDK to January 2019 for commercial production use, and at least the end of 2020 for individual desktop use.\\n* Oracle JDK should be, for the most part, interchangeable with Oracle\u2019s OpenJDK builds for those who want to move up to the next releases, as they did for Java 6->7 and Java 7->8.\\n* Java 10 is the suggested release by Oracle\\n* Oracle has posted more information on the roadmap for the Java Client (including Applets and Web Start).\\n* Oracle plans to discontinue contributions to the JDK 8 Updates project until January 2019.\\n* For security reasons, IITD Architecture encourages upgrade\/migration of all java applications to at least JDK 11.\\n* Scope - IITD all server and client side applications owned by IITD that run Java.\\n\n\n##Decision\n* An official Information Innovation & Technology Division (IITD) statement is needed on the direction forward for target versions of Java\/JDKs for all IITD hosted java applications.  Oracle has changed its licensing model, and will now be using a subscription model for charging for the use of its Oracle Java SE product releases.\\n* Java SE 8 is the end of the legacy versioning and release cadence model; Java 9 was the new beginning.\\n* Oracle extended the public updates of its Oracle JDK to January 2019 for commercial production use, and at least the end of 2020 for individual desktop use.\\n* Oracle JDK should be, for the most part, interchangeable with Oracle\u2019s OpenJDK builds for those who want to move up to the next releases, as they did for Java 6->7 and Java 7->8.\\n* Java 10 is the suggested release by Oracle\\n* Oracle has posted more information on the roadmap for the Java Client (including Applets and Web Start).\\n* Oracle plans to discontinue contributions to the JDK 8 Updates project until January 2019.\\n* For security reasons, IITD Architecture encourages upgrade\/migration of all java applications to at least JDK 11.\\n* Scope - IITD all server and client side applications owned by IITD that run Java.\\nJDK 8 & Later\\nOracles OpenJDK JDK binaries for Windows, macOS, and Linux are available on release-specific pages of jdk.java.net as .tar.gz or .zip archives.\\nAs an example, the archives for JDK 13 may be found on jdk.java.net\/13 and may be extracted on the command line using\\n$ tar xvf openjdk-13*_bin.tar.gz\\nor\\n$ unzip openjdk-13*_bin.zip\\ndepending on the archive type.\\n### Positive Consequences\\n* removes the dependencies on Oracle JDK Licensing\\n* reduces security vulnerabilities of older JDK versions\\nJava 7 is still in predominant use. It goes without saying that any version of Java below 7 should be updated immediately even version 7 needs significant remediation for its fleet of vulnerabilities.\\nFurther vulnerabilities -\\n* <https:\/\/www.cvedetails.com\/product\/19117\/Oracle-JRE.html?vendor_id=93>\\n*\t<https:\/\/www.cvedetails.com\/product\/23642\/Oracle-Openjdk.html?vendor_id=93>\\n### Negative Consequences\\n* slow performance may occur\\n* migration issues will need to be addressed\\n* Migrate all Java JDK dependencies from Oracle JDK to OpenJDK.\\n* Upgrade all older versions to at least JDK 8, preference is to encourage teams to target move to JDK 11.\\n","MatchedIds":[1665,73,1563,3551,2261],"Predicted":"## Decision\nWe will adopt OpenJDK as the standard JDK for all IITD hosted java applications.\n\n*### Short-term Actions:*\n1. **Immediate Migration:** We will begin immediate migration of all Java applications to OpenJDK 11 or higher, given Oracle's licensing model change and its robust community support.\n2. **Upgrade Planning:** All existing applications running on Java SE 8 should be planned for an upgrade to JDK 11 or higher by the end of the current fiscal year to align with security and support guidelines.\n3. **Compatibility Testing:** Conduct compatibility testing to ensure that applications run properly on OpenJDK versions and identify any issues early in the migration process.\n\n*### Medium-term Actions:*\n1. **Training & Documentation:** Provide training materials and sessions on the differences and migration steps from Oracle JDK to OpenJDK for all development teams.\n2. **Monitor & Assess:** Continuously monitor performance and gather feedback post-migration to ensure stability and address any issues that arise.\n3. **Deprecation Notices:** Issue deprecation notices for any internal libraries or components that are incompatible with OpenJDK 11+.\n\n*### Long-term Actions:*\n1. **Regular Updates:** Establish a regular update cycle to adopt early access and stable releases of newer OpenJDK versions to leverage performance improvements and security updates.\n2. **Retirement of Legacy Systems:** Plan the phased retirement of all applications and systems that rely on older, unsupported versions of Java within the next two years.\n\n*### Positive Consequences:*\n- **Cost Savings:** Reduction in licensing costs due to the adoption of the open-source OpenJDK.\n- **Enhanced Security:** Improved security posture by upgrading to JDK 11+, which includes numerous security improvements and new features.\n- **Community Support:** Leveraging community-driven support and innovation within the OpenJDK ecosystem.\n\n*### Negative Consequences:*\n- **Migration Efforts:** The initial migration effort may require additional resources and time for testing and validation.\n- **Learning Curve:** Development teams might encounter a learning curve with new JDK features and differences from Oracle JDK. \n\nThe decision to adopt OpenJDK and migrate to JDK 11 or higher ensures IITD's compliance with modern standards, improves security, and eliminates dependency on Oracle's subscription model.","GenTime":"2024-08-20 00:26:26"}
{"File Name":"exercise3-group-6\/0006-create-observation-repository-search.md","Context":"## Context\\nResearchers may require methods to search for a particular Whale Observation or set of Whale Observations during their\\nstudies. For the program to accomplish this task two search methods may require implementation.\\n","Decision":"Create `getByDate` and `getById` methods in ObservationRepository.\\n`getByDate` uses the Observation iterator to compare Observation objects `sightingTime` (Date) to the date passed to the\\nmethod and creates an array list of matching objects. This function uses an iterator rather than `Collections.binarySearch()`\\nto reduce the runtime given that there may exist multiple objects of a similar date.\\n`getById` uses `Collections.binarySearch()` to search and possibly return the object with the `ObservationId` matching\\nthe long passed to the method.\\nCollection.sort()` used in `getById` implements the abstract strategy of the `Comparator` interface using the concrete\\nstrategy `compareByDate` and returns an integer referring to the order of the objects.\\n","tokens":41,"id":2772,"text":"## Context\\nResearchers may require methods to search for a particular Whale Observation or set of Whale Observations during their\\nstudies. For the program to accomplish this task two search methods may require implementation.\\n\n\n##Decision\nCreate `getByDate` and `getById` methods in ObservationRepository.\\n`getByDate` uses the Observation iterator to compare Observation objects `sightingTime` (Date) to the date passed to the\\nmethod and creates an array list of matching objects. This function uses an iterator rather than `Collections.binarySearch()`\\nto reduce the runtime given that there may exist multiple objects of a similar date.\\n`getById` uses `Collections.binarySearch()` to search and possibly return the object with the `ObservationId` matching\\nthe long passed to the method.\\nCollection.sort()` used in `getById` implements the abstract strategy of the `Comparator` interface using the concrete\\nstrategy `compareByDate` and returns an integer referring to the order of the objects.\\n","MatchedIds":[2775,2513,4637,4741,532],"Predicted":"## Decision\nImplement two distinct search methods for Whale Observations. \n\n1. **Search by ObservationId**: \n   - This method will allow researchers to quickly locate a specific Whale Observation using its unique `ObservationId` (long).\n   - Implement an index on the `ObservationId` field to speed up lookups.\n   \n2. **Search by Criteria**:\n   - This method enables complex searches based on various attributes such as `observationDate`, `location`, `species`, and `researcherName`.\n   - Use a multi-criteria search engine like ElasticSearch to handle the more complex queries. \n   - Allow advanced search features including \"and\", \"or\", \"startsWith\", and \"contains\".\n\n### Positive Consequences\n- Enables precise and flexible searching capabilities, enhancing the researchers' efficiency.\n- The use of indexing and a specialized search engine will result in faster query processing times.\n- The dual-method approach provides a balance of simple and complex search functionalities.\n\n### Negative Consequences\n- Maintaining the search engine and indexes will require additional resources and administration.\n- The complexity of implementing and maintaining the multi-criteria search may result in higher initial development time.\n\nIn summary, the dual-method search approach will effectively cover both simple and advanced search needs, balancing efficiency with comprehensive search capabilities.","GenTime":"2024-08-20 00:26:34"}
{"File Name":"cosmos-sdk\/adr-002-docs-structure.md","Context":"## Context\\nThere is a need for a scalable structure of the Cosmos SDK documentation. Current documentation includes a lot of non-related Cosmos SDK material, is difficult to maintain and hard to follow as a user.\\nIdeally, we would have:\\n* All docs related to dev frameworks or tools live in their respective github repos (sdk repo would contain sdk docs, hub repo would contain hub docs, lotion repo would contain lotion docs, etc.)\\n* All other docs (faqs, whitepaper, high-level material about Cosmos) would live on the website.\\n","Decision":"Re-structure the `\/docs` folder of the Cosmos SDK github repo as follows:\\n```text\\ndocs\/\\n\u251c\u2500\u2500 README\\n\u251c\u2500\u2500 intro\/\\n\u251c\u2500\u2500 concepts\/\\n\u2502   \u251c\u2500\u2500 baseapp\\n\u2502   \u251c\u2500\u2500 types\\n\u2502   \u251c\u2500\u2500 store\\n\u2502   \u251c\u2500\u2500 server\\n\u2502   \u251c\u2500\u2500 modules\/\\n\u2502   \u2502   \u251c\u2500\u2500 keeper\\n\u2502   \u2502   \u251c\u2500\u2500 handler\\n\u2502   \u2502   \u251c\u2500\u2500 cli\\n\u2502   \u251c\u2500\u2500 gas\\n\u2502   \u2514\u2500\u2500 commands\\n\u251c\u2500\u2500 clients\/\\n\u2502   \u251c\u2500\u2500 lite\/\\n\u2502   \u251c\u2500\u2500 service-providers\\n\u251c\u2500\u2500 modules\/\\n\u251c\u2500\u2500 spec\/\\n\u251c\u2500\u2500 translations\/\\n\u2514\u2500\u2500 architecture\/\\n```\\nThe files in each sub-folders do not matter and will likely change. What matters is the sectioning:\\n* `README`: Landing page of the docs.\\n* `intro`: Introductory material. Goal is to have a short explainer of the Cosmos SDK and then channel people to the resource they need. The [Cosmos SDK tutorial](https:\/\/github.com\/cosmos\/sdk-application-tutorial\/) will be highlighted, as well as the `godocs`.\\n* `concepts`: Contains high-level explanations of the abstractions of the Cosmos SDK. It does not contain specific code implementation and does not need to be updated often. **It is not an API specification of the interfaces**. API spec is the `godoc`.\\n* `clients`: Contains specs and info about the various Cosmos SDK clients.\\n* `spec`: Contains specs of modules, and others.\\n* `modules`: Contains links to `godocs` and the spec of the modules.\\n* `architecture`: Contains architecture-related docs like the present one.\\n* `translations`: Contains different translations of the documentation.\\nWebsite docs sidebar will only include the following sections:\\n* `README`\\n* `intro`\\n* `concepts`\\n* `clients`\\n`architecture` need not be displayed on the website.\\n","tokens":115,"id":816,"text":"## Context\\nThere is a need for a scalable structure of the Cosmos SDK documentation. Current documentation includes a lot of non-related Cosmos SDK material, is difficult to maintain and hard to follow as a user.\\nIdeally, we would have:\\n* All docs related to dev frameworks or tools live in their respective github repos (sdk repo would contain sdk docs, hub repo would contain hub docs, lotion repo would contain lotion docs, etc.)\\n* All other docs (faqs, whitepaper, high-level material about Cosmos) would live on the website.\\n\n\n##Decision\nRe-structure the `\/docs` folder of the Cosmos SDK github repo as follows:\\n```text\\ndocs\/\\n\u251c\u2500\u2500 README\\n\u251c\u2500\u2500 intro\/\\n\u251c\u2500\u2500 concepts\/\\n\u2502   \u251c\u2500\u2500 baseapp\\n\u2502   \u251c\u2500\u2500 types\\n\u2502   \u251c\u2500\u2500 store\\n\u2502   \u251c\u2500\u2500 server\\n\u2502   \u251c\u2500\u2500 modules\/\\n\u2502   \u2502   \u251c\u2500\u2500 keeper\\n\u2502   \u2502   \u251c\u2500\u2500 handler\\n\u2502   \u2502   \u251c\u2500\u2500 cli\\n\u2502   \u251c\u2500\u2500 gas\\n\u2502   \u2514\u2500\u2500 commands\\n\u251c\u2500\u2500 clients\/\\n\u2502   \u251c\u2500\u2500 lite\/\\n\u2502   \u251c\u2500\u2500 service-providers\\n\u251c\u2500\u2500 modules\/\\n\u251c\u2500\u2500 spec\/\\n\u251c\u2500\u2500 translations\/\\n\u2514\u2500\u2500 architecture\/\\n```\\nThe files in each sub-folders do not matter and will likely change. What matters is the sectioning:\\n* `README`: Landing page of the docs.\\n* `intro`: Introductory material. Goal is to have a short explainer of the Cosmos SDK and then channel people to the resource they need. The [Cosmos SDK tutorial](https:\/\/github.com\/cosmos\/sdk-application-tutorial\/) will be highlighted, as well as the `godocs`.\\n* `concepts`: Contains high-level explanations of the abstractions of the Cosmos SDK. It does not contain specific code implementation and does not need to be updated often. **It is not an API specification of the interfaces**. API spec is the `godoc`.\\n* `clients`: Contains specs and info about the various Cosmos SDK clients.\\n* `spec`: Contains specs of modules, and others.\\n* `modules`: Contains links to `godocs` and the spec of the modules.\\n* `architecture`: Contains architecture-related docs like the present one.\\n* `translations`: Contains different translations of the documentation.\\nWebsite docs sidebar will only include the following sections:\\n* `README`\\n* `intro`\\n* `concepts`\\n* `clients`\\n`architecture` need not be displayed on the website.\\n","MatchedIds":[3035,827,2181,5172,4609],"Predicted":"## Decision\nWe will adopt a structured documentation strategy that separates developer-focused documentation from high-level and general information about Cosmos. \n\n### Implementation Details:\n1. **Repository-Specific Documentation:**\n   - All documentation related to specific development frameworks or tools will be hosted and maintained in their respective GitHub repositories.\n     - **SDK Documentation:** Hosted in the Cosmos SDK GitHub repository.\n     - **Hub Documentation:** Hosted in the Cosmos Hub GitHub repository.\n     - **Lotion Documentation:** Hosted in the Lotion GitHub repository.\n   - Each repository will include:\n     * Setup instructions, API references, and developer guides specific to the project.\n     * Versioning and changelogs to keep track of updates.\n\n2. **Centralized General Documentation:**\n   - All FAQs, whitepapers, and high-level materials about Cosmos will be hosted on the main Cosmos website.\n     - **FAQs:** Comprehensive FAQ section to address common queries.\n     - **Whitepapers:** Easily accessible research papers, including foundational and exploratory documents.\n     - **High-level Overviews:** General information about the Cosmos ecosystem and its components.\n\n### Benefits:\n* **Enhanced Usability:** Clear distinction between developer-oriented documentation and general information, making it easier for users to find relevant materials.\n* **Ease of Maintenance:** Separate repositories will streamline the documentation update process, ensuring that changes to a specific framework or tool do not impact unrelated documents.\n* **Focused Contribution:** Clear scopes and responsibilities for each documentation repository will foster better contributions from the community and maintainers.\n\nThis structure will not only improve the scalability and maintainability of the Cosmos SDK documentation but also enhance the overall user experience by making the documentation more organized and navigable.","GenTime":"2024-08-20 00:26:41"}
{"File Name":"tendermint\/adr-012-peer-transport.md","Context":"## Context\\nOne of the more apparent problems with the current architecture in the p2p\\npackage is that there is no clear separation of concerns between different\\ncomponents. Most notably the `Switch` is currently doing physical connection\\nhandling. An artifact is the dependency of the Switch on\\n`[config.P2PConfig`](https:\/\/github.com\/tendermint\/tendermint\/blob\/05a76fb517f50da27b4bfcdc7b4cf185fc61eff6\/config\/config.go#L272-L339).\\nAddresses:\\n- [#2046](https:\/\/github.com\/tendermint\/tendermint\/issues\/2046)\\n- [#2047](https:\/\/github.com\/tendermint\/tendermint\/issues\/2047)\\nFirst iteraton in [#2067](https:\/\/github.com\/tendermint\/tendermint\/issues\/2067)\\n","Decision":"Transport concerns will be handled by a new component (`PeerTransport`) which\\nwill provide Peers at its boundary to the caller. In turn `Switch` will use\\nthis new component accept new `Peer`s and dial them based on `NetAddress`.\\n### PeerTransport\\nResponsible for emitting and connecting to Peers. The implementation of `Peer`\\nis left to the transport, which implies that the chosen transport dictates the\\ncharacteristics of the implementation handed back to the `Switch`. Each\\ntransport implementation is responsible to filter establishing peers specific\\nto its domain, for the default multiplexed implementation the following will\\napply:\\n- connections from our own node\\n- handshake fails\\n- upgrade to secret connection fails\\n- prevent duplicate ip\\n- prevent duplicate id\\n- nodeinfo incompatibility\\n```go\\n\/\/ PeerTransport proxies incoming and outgoing peer connections.\\ntype PeerTransport interface {\\n\/\/ Accept returns a newly connected Peer.\\nAccept() (Peer, error)\\n\/\/ Dial connects to a Peer.\\nDial(NetAddress) (Peer, error)\\n}\\n\/\/ EXAMPLE OF DEFAULT IMPLEMENTATION\\n\/\/ multiplexTransport accepts tcp connections and upgrades to multiplexted\\n\/\/ peers.\\ntype multiplexTransport struct {\\nlistener net.Listener\\nacceptc chan accept\\nclosec  <-chan struct{}\\nlistenc <-chan struct{}\\ndialTimeout      time.Duration\\nhandshakeTimeout time.Duration\\nnodeAddr         NetAddress\\nnodeInfo         NodeInfo\\nnodeKey          NodeKey\\n\/\/ TODO(xla): Remove when MConnection is refactored into mPeer.\\nmConfig conn.MConnConfig\\n}\\nvar _ PeerTransport = (*multiplexTransport)(nil)\\n\/\/ NewMTransport returns network connected multiplexed peers.\\nfunc NewMTransport(\\nnodeAddr NetAddress,\\nnodeInfo NodeInfo,\\nnodeKey NodeKey,\\n) *multiplexTransport\\n```\\n### Switch\\nFrom now the Switch will depend on a fully setup `PeerTransport` to\\nretrieve\/reach out to its peers. As the more low-level concerns are pushed to\\nthe transport, we can omit passing the `config.P2PConfig` to the Switch.\\n```go\\nfunc NewSwitch(transport PeerTransport, opts ...SwitchOption) *Switch\\n```\\n","tokens":179,"id":1956,"text":"## Context\\nOne of the more apparent problems with the current architecture in the p2p\\npackage is that there is no clear separation of concerns between different\\ncomponents. Most notably the `Switch` is currently doing physical connection\\nhandling. An artifact is the dependency of the Switch on\\n`[config.P2PConfig`](https:\/\/github.com\/tendermint\/tendermint\/blob\/05a76fb517f50da27b4bfcdc7b4cf185fc61eff6\/config\/config.go#L272-L339).\\nAddresses:\\n- [#2046](https:\/\/github.com\/tendermint\/tendermint\/issues\/2046)\\n- [#2047](https:\/\/github.com\/tendermint\/tendermint\/issues\/2047)\\nFirst iteraton in [#2067](https:\/\/github.com\/tendermint\/tendermint\/issues\/2067)\\n\n\n##Decision\nTransport concerns will be handled by a new component (`PeerTransport`) which\\nwill provide Peers at its boundary to the caller. In turn `Switch` will use\\nthis new component accept new `Peer`s and dial them based on `NetAddress`.\\n### PeerTransport\\nResponsible for emitting and connecting to Peers. The implementation of `Peer`\\nis left to the transport, which implies that the chosen transport dictates the\\ncharacteristics of the implementation handed back to the `Switch`. Each\\ntransport implementation is responsible to filter establishing peers specific\\nto its domain, for the default multiplexed implementation the following will\\napply:\\n- connections from our own node\\n- handshake fails\\n- upgrade to secret connection fails\\n- prevent duplicate ip\\n- prevent duplicate id\\n- nodeinfo incompatibility\\n```go\\n\/\/ PeerTransport proxies incoming and outgoing peer connections.\\ntype PeerTransport interface {\\n\/\/ Accept returns a newly connected Peer.\\nAccept() (Peer, error)\\n\/\/ Dial connects to a Peer.\\nDial(NetAddress) (Peer, error)\\n}\\n\/\/ EXAMPLE OF DEFAULT IMPLEMENTATION\\n\/\/ multiplexTransport accepts tcp connections and upgrades to multiplexted\\n\/\/ peers.\\ntype multiplexTransport struct {\\nlistener net.Listener\\nacceptc chan accept\\nclosec  <-chan struct{}\\nlistenc <-chan struct{}\\ndialTimeout      time.Duration\\nhandshakeTimeout time.Duration\\nnodeAddr         NetAddress\\nnodeInfo         NodeInfo\\nnodeKey          NodeKey\\n\/\/ TODO(xla): Remove when MConnection is refactored into mPeer.\\nmConfig conn.MConnConfig\\n}\\nvar _ PeerTransport = (*multiplexTransport)(nil)\\n\/\/ NewMTransport returns network connected multiplexed peers.\\nfunc NewMTransport(\\nnodeAddr NetAddress,\\nnodeInfo NodeInfo,\\nnodeKey NodeKey,\\n) *multiplexTransport\\n```\\n### Switch\\nFrom now the Switch will depend on a fully setup `PeerTransport` to\\nretrieve\/reach out to its peers. As the more low-level concerns are pushed to\\nthe transport, we can omit passing the `config.P2PConfig` to the Switch.\\n```go\\nfunc NewSwitch(transport PeerTransport, opts ...SwitchOption) *Switch\\n```\\n","MatchedIds":[1979,1982,1495,1494,4650],"Predicted":"## Decision\n\nWe will refactor the `p2p` package to introduce a clear separation of concerns among its components, focusing initially on decoupling the `Switch` from the physical connection handling. This will be achieved through the following steps:\n\n1. **Introduce a ConnectionManager**: Create a dedicated `ConnectionManager` component responsible solely for handling physical connections. This component will encapsulate all connection-related logic currently embedded in the `Switch`.\n\n2. **Decouple Configuration Dependencies**: Refactor the `Switch` to remove its direct dependency on `P2PConfig`. Instead, configuration settings will be injected into the relevant components (e.g., `ConnectionManager`) that require them.\n\n3. **Modify Switch Responsibilities**: The `Switch` will focus on higher-level responsibilities such as message routing and peer management, deferring any connection-related operations to the `ConnectionManager`.\n\n4. **Incremental Refactoring Approach**: We will follow an incremental approach to implement these changes, ensuring at each step that the system remains functional and performs as expected. Each iteration will be documented and reviewed to manage complexity and maintain stability.\n\nThe refactoring will address the immediate issues described in issues [#2046](https:\/\/github.com\/tendermint\/tendermint\/issues\/2046) and [#2047](https:\/\/github.com\/tendermint\/tendermint\/issues\/2047), building on the preliminary work mentioned in [#2067](https:\/\/github.com\/tendermint\/tendermint\/issues\/2067). This decision aims to enable easier maintenance, improve modularity, and enhance the overall readability of the `p2p` package.","GenTime":"2024-08-20 00:26:48"}
{"File Name":"connecting-to-services\/0008-add-basic-authentication-to-test-environments.md","Context":"## Context\\nEvery environment the application is available in, be that development, review,\\nstaging or public are openly available to anybody. There is no access control\\nat all.  This isn't suitable for a site that could be misinterpreted as the\\n'real' version either by people or by search engines and other bots. There\\nneeds to be some form of hurdle to overcome in order to prompt people to\\nconsider whether this is the correct site. There should also be a barrier to\\nprevent access by bots.\\n","Decision":"We have decided to use basic authentication on the service in all environments\\napart from the public facing one. This will only be applicable to the\\nenvironments hosted within the Rancher environment. The username and password\\nwill not be secret and will be included within the `README` of the application.\\n","tokens":107,"id":2207,"text":"## Context\\nEvery environment the application is available in, be that development, review,\\nstaging or public are openly available to anybody. There is no access control\\nat all.  This isn't suitable for a site that could be misinterpreted as the\\n'real' version either by people or by search engines and other bots. There\\nneeds to be some form of hurdle to overcome in order to prompt people to\\nconsider whether this is the correct site. There should also be a barrier to\\nprevent access by bots.\\n\n\n##Decision\nWe have decided to use basic authentication on the service in all environments\\napart from the public facing one. This will only be applicable to the\\nenvironments hosted within the Rancher environment. The username and password\\nwill not be secret and will be included within the `README` of the application.\\n","MatchedIds":[3070,3950,4271,783,1754],"Predicted":"## Decision\nWe decided to implement basic HTTP authentication and IP whitelisting for non-production environments (development, review, and staging). This decision was made to create a simple barrier that will prompt users to ensure they are accessing the correct site and to prevent unauthorized access by bots.\n\n### Implementation Details:\n1. **HTTP Authentication:**\n   - Each non-production environment will be protected with basic HTTP authentication. Authorized users will be provided with a username and password to access these environments.\n   - HTTP authentication prompts users with a login dialogue, making it clear that these environments are not the public-facing version.\n\n2. **IP Whitelisting:**\n   - Only specific IP addresses (e.g., those belonging to the organization or specific team members) will be allowed access to these non-production environments. This adds an additional layer of security ensuring only authorized personnel can access these sites.\n   - This will be configured through firewall rules or at the web server level depending on the infrastructure being used.\n\n### Positive Consequences:\n- **Mitigates Unauthorized Access:**\n  - Basic HTTP authentication and IP whitelisting prevent unauthorized users and bots from accessing non-production environments, thereby reducing misunderstandings.\n- **Clear Differentiation:**\n  - The added login prompt serves as a clear indication to users that they are accessing a non-production site, reducing the risk of misinterpretation.\n\n### Negative Consequences:\n- **Maintenance Overhead:**\n  - Maintaining a list of authorized IP addresses and managing credentials for HTTP authentication will require additional administrative work.\n- **Potential Access Issues:**\n  - Team members working remotely or from dynamic IP addresses might face access issues and require a process to temporarily whitelist their IPs.\n\nBy implementing these security measures, we ensure that non-production environments are secure and that users are clearly prompted to consider the authenticity of the site they are accessing.","GenTime":"2024-08-20 00:27:07"}
{"File Name":"opg-use-an-lpa\/0009-use-aws-kms-to-manage-session-encryption-keys.md","Context":"## Context\\nFollowing on from [8. Session storage using an encrypted cookie](0008-session-storage-using-an-encrypted-cookie.md)\\n* The encrypted cookie will need to be encrypted using a key.\\n* Keys should be able to be rotated easily and often.\\n* Key rotations should have no effect on active users.\\n","Decision":"* We will use AWS' KMS to manage our encryption keys.\\n* Keys will be cached at the contained level in volatile memory.\\n","tokens":67,"id":4848,"text":"## Context\\nFollowing on from [8. Session storage using an encrypted cookie](0008-session-storage-using-an-encrypted-cookie.md)\\n* The encrypted cookie will need to be encrypted using a key.\\n* Keys should be able to be rotated easily and often.\\n* Key rotations should have no effect on active users.\\n\n\n##Decision\n* We will use AWS' KMS to manage our encryption keys.\\n* Keys will be cached at the contained level in volatile memory.\\n","MatchedIds":[2256,4843,4011,841,1654],"Predicted":"## Decision\nImplement a key rotation scheme by maintaining a set of active encryption keys and using a key identifier embedded in the cookie metadata.\n\n### Details:\n1. **Multiple Active Keys**: Maintain a set of active encryption keys, where one is designated as the primary key for new cookies, and others are retained for decrypting existing cookies.\n2. **Key Identifier**: Embed a key identifier (key ID) in the cookie metadata which indicates the key used for encryption.\n3. **Rotate Keys**:\n   - Periodically update the primary key by generating a new key.\n   - Shift the previous primary key to a secondary position in the active keys set.\n   - Continue using secondary keys for decrypting cookies that were created with those keys.\n4. **Key Storage**: Securely store all active keys, ideally in a key management service (KMS) which provides facilities for key generation, storage, and access control.\n5. **Fallback Mechanism**: Ensure the system gracefully handles decryption attempts with inactive or missing keys by falling back to a secure default reset mechanism.\n6. **Performance Considerations**: Optimize lookups for the correct decryption key using the key ID embedded in the cookie, ensuring low-latency operations.\n\n### Workflow:\n1. **Cookie Creation**:\n   - Encrypt cookie data using the primary key.\n   - Embed the key ID of the primary key in the cookie metadata.\n2. **Cookie Decryption**:\n   - Retrieve the key ID from the cookie metadata.\n   - Use the corresponding key to decrypt the cookie.\n   - If decryption fails, reset the cookie (apply secure default behaviors).\n\nThis approach will allow seamless key rotations without impacting active sessions, ensuring improved security with minimal disruption to user experience.","GenTime":"2024-08-20 00:27:19"}
{"File Name":"dogma\/0015-routing-unrecognized-messages.md","Context":"## Context\\n[ADR-14](0014-apply-historical-events-to-aggregates.md) relaxed the\\nspecification such that `AggregateRoot.ApplyEvent()` implementations were no\\nlonger required to panic with an `UnrecognizedMessage` value when passed an\\nunexpected message type.\\nPrompted by this requirement, we relaxed the requirement for ALL handler\\nmethods, which was likely too broad of a change.\\nSpecifically, unlike when handling a message, the routing methods\\n`AggregateMessageHandler.RouteCommandToInstance()` and\\n`ProcessMessageHandler.RouteEventToInstance()` do not have the option of \"doing\\nnothing\" when passed an unexpected message type.\\n","Decision":"Reinstate the hard requirement that the handlers MUST panic with\\n`UnexpectedMessage` when asked to route a message type that was not configured\\nas being consumed by that handler.\\n","tokens":138,"id":1612,"text":"## Context\\n[ADR-14](0014-apply-historical-events-to-aggregates.md) relaxed the\\nspecification such that `AggregateRoot.ApplyEvent()` implementations were no\\nlonger required to panic with an `UnrecognizedMessage` value when passed an\\nunexpected message type.\\nPrompted by this requirement, we relaxed the requirement for ALL handler\\nmethods, which was likely too broad of a change.\\nSpecifically, unlike when handling a message, the routing methods\\n`AggregateMessageHandler.RouteCommandToInstance()` and\\n`ProcessMessageHandler.RouteEventToInstance()` do not have the option of \"doing\\nnothing\" when passed an unexpected message type.\\n\n\n##Decision\nReinstate the hard requirement that the handlers MUST panic with\\n`UnexpectedMessage` when asked to route a message type that was not configured\\nas being consumed by that handler.\\n","MatchedIds":[1442,3210,4656,21,1050],"Predicted":"## Decision\n\nRouting methods `AggregateMessageHandler.RouteCommandToInstance()` and `ProcessMessageHandler.RouteEventToInstance()` will be required to panic with an `UnrecognizedMessage` value when passed an unexpected message type. This ensures that there is a clear and immediate indication when an unrecognized message type is encountered, preventing potential silent failures or undefined behavior in the system. This change aligns the behavior of routing methods with the need for explicit handling of unexpected message types. \n\nTo implement this, the routing methods will include a check for recognized message types and will panic with an informative error message if an unrecognized type is encountered. This ensures robust error handling and maintains the integrity of the message routing process.","GenTime":"2024-08-20 00:27:25"}
{"File Name":"runner\/0280-command-input-echoing.md","Context":"## Context\\nCommand echoing as a default behavior tends to clutter the user logs, so we want to swap to a system where users have to opt in to see this information.\\nCommand outputs will still be echoed in the case there are any errors processing such commands. This is so the end user can have more context on why the command failed and help with troubleshooting.\\nEcho output in the user logs can be explicitly controlled by the new commands `::echo::on` and `::echo::off`. By default, echoing is enabled if `ACTIONS_STEP_DEBUG` secret is enabled, otherwise echoing is disabled.\\n","Decision":"- The only commands that currently echo output are\\n- `remove-matcher`\\n- `add-matcher`\\n- `add-path`\\n- These will no longer echo the command, if processed successfully\\n- All commands echo the input when any of these conditions is fulfilled:\\n1. When such commands fail with an error\\n2. When `::echo::on` is set\\n3. When the `ACTIONS_STEP_DEBUG` is set, and echoing hasn't been explicitly disabled with `::echo::off`\\n- There are a few commands that won't be echoed, even when echo is enabled. These are (as of 2019\/11\/04):\\n- `add-mask`\\n- `debug`\\n- `warning`\\n- `error`\\n- The three commands above will not echo, either because echoing the command would leak secrets (e.g. `add-mask`), or it would not add any additional troubleshooting information to the logs (e.g. `debug`). It's expected that future commands would follow these \"echo-suppressing\" guidelines as well. Echo-suppressed commands are still free to output other information to the logs, as deemed fit.\\n","tokens":124,"id":3741,"text":"## Context\\nCommand echoing as a default behavior tends to clutter the user logs, so we want to swap to a system where users have to opt in to see this information.\\nCommand outputs will still be echoed in the case there are any errors processing such commands. This is so the end user can have more context on why the command failed and help with troubleshooting.\\nEcho output in the user logs can be explicitly controlled by the new commands `::echo::on` and `::echo::off`. By default, echoing is enabled if `ACTIONS_STEP_DEBUG` secret is enabled, otherwise echoing is disabled.\\n\n\n##Decision\n- The only commands that currently echo output are\\n- `remove-matcher`\\n- `add-matcher`\\n- `add-path`\\n- These will no longer echo the command, if processed successfully\\n- All commands echo the input when any of these conditions is fulfilled:\\n1. When such commands fail with an error\\n2. When `::echo::on` is set\\n3. When the `ACTIONS_STEP_DEBUG` is set, and echoing hasn't been explicitly disabled with `::echo::off`\\n- There are a few commands that won't be echoed, even when echo is enabled. These are (as of 2019\/11\/04):\\n- `add-mask`\\n- `debug`\\n- `warning`\\n- `error`\\n- The three commands above will not echo, either because echoing the command would leak secrets (e.g. `add-mask`), or it would not add any additional troubleshooting information to the logs (e.g. `debug`). It's expected that future commands would follow these \"echo-suppressing\" guidelines as well. Echo-suppressed commands are still free to output other information to the logs, as deemed fit.\\n","MatchedIds":[1254,3389,118,152,2191],"Predicted":"## Decision\nCommand echoing will be disabled by default to reduce log clutter. Users can opt-in to see command outputs by using the new commands `::echo::on` and `::echo::off`. Command outputs will still be echoed in case of any errors to aid in troubleshooting. Additionally, echoing will be enabled by default if the `ACTIONS_STEP_DEBUG` secret is set, enabling more verbose logging for debugging purposes.","GenTime":"2024-08-20 00:27:28"}
{"File Name":"alfresco-anaxes-shipyard\/0008-applying-extensions-to-docker-images.md","Context":"## Context\\nOne of the main advantages of Docker images is their immutability. This means once an image has been tested and verified there is a high confidence it will work as intended in other environments as it doesn't change.\\nAlfresco allows the core product to the enhanced via external modules in the form of [AMPs](https:\/\/docs.alfresco.com\/5.2\/concepts\/dev-extensions-packaging-techniques-amps.html) or [simple JARs](https:\/\/docs.alfresco.com\/5.2\/concepts\/dev-extensions-packaging-techniques-jar-files.html).\\nThis results in two big problems, how do we release containers with every combination of AMP available and how do customers apply their own extensions?\\nWe have three options; apply extensions at build time (thus retaining the immutability advantage), apply extensions at runtime using a mechanism that doesn't change the contents of the container or apply extensions as the container initializes (breaking immutability).\\nApplying extensions at build time means we will be forcing customers to build their own images depending on which official and custom extensions they require.\\nHowever, there are a number of disadvantages of applying extensions at runtime; The immutability advantage is lost (the WAR file is changed), an extension could fail to apply and prevent the container from starting, it has the potential to introduce performance issues as the AMP or JAR has to be fetched and applied and finally it opens a potential security hole as code can be applied to a container via a simple environment variable.\\nWe investigated potential approaches to keep the main container immutable by using volumes and initContainers but this increases the complexity of the solution and doesn't resolve all the issues outlined above.\\n","Decision":"We will proceed with the build time option as there are a number of disadvantages to applying extensions at runtime, customers are already used to applying their own extensions and they're having to learn a new deployment mechanism anyway.\\nWe will release a small number of images with and without common AMPs applied.\\n","tokens":351,"id":3604,"text":"## Context\\nOne of the main advantages of Docker images is their immutability. This means once an image has been tested and verified there is a high confidence it will work as intended in other environments as it doesn't change.\\nAlfresco allows the core product to the enhanced via external modules in the form of [AMPs](https:\/\/docs.alfresco.com\/5.2\/concepts\/dev-extensions-packaging-techniques-amps.html) or [simple JARs](https:\/\/docs.alfresco.com\/5.2\/concepts\/dev-extensions-packaging-techniques-jar-files.html).\\nThis results in two big problems, how do we release containers with every combination of AMP available and how do customers apply their own extensions?\\nWe have three options; apply extensions at build time (thus retaining the immutability advantage), apply extensions at runtime using a mechanism that doesn't change the contents of the container or apply extensions as the container initializes (breaking immutability).\\nApplying extensions at build time means we will be forcing customers to build their own images depending on which official and custom extensions they require.\\nHowever, there are a number of disadvantages of applying extensions at runtime; The immutability advantage is lost (the WAR file is changed), an extension could fail to apply and prevent the container from starting, it has the potential to introduce performance issues as the AMP or JAR has to be fetched and applied and finally it opens a potential security hole as code can be applied to a container via a simple environment variable.\\nWe investigated potential approaches to keep the main container immutable by using volumes and initContainers but this increases the complexity of the solution and doesn't resolve all the issues outlined above.\\n\n\n##Decision\nWe will proceed with the build time option as there are a number of disadvantages to applying extensions at runtime, customers are already used to applying their own extensions and they're having to learn a new deployment mechanism anyway.\\nWe will release a small number of images with and without common AMPs applied.\\n","MatchedIds":[3598,3917,4096,3145,3920],"Predicted":"## Decision\nWe will apply extensions at build time.\n\n* This retains the immutability advantage of Docker images, ensuring that tested and verified images remain consistent across different environments.\n* Although it requires customers to build their own images based on the official and custom extensions they need, it provides a higher level of control and security.\n* This approach prevents issues related to runtime extension failures, performance impacts due to fetching and applying extensions, and potential security vulnerabilities introduced by applying code via environment variables.\n\nTo facilitate the customer experience, we will provide detailed documentation and support tools that simplify the process of building custom images with the required extensions. This balances the need for immutability with the flexibility for customization.","GenTime":"2024-08-20 00:27:31"}
{"File Name":"docspell\/0001_components.md","Context":"# Context and Problem Statement\\nHow should the application be structured into its main components? The\\ngoal is to be able to have multiple rest servers\/webapps and multiple\\ndocument processor components working togehter.\\n# Decision Outcome\\nThe following are the \"main\" modules. There may be more helper modules\\nand libraries that support implementing a feature.\\n","Decision":"The following are the \"main\" modules. There may be more helper modules\\nand libraries that support implementing a feature.\\n","tokens":75,"id":276,"text":"# Context and Problem Statement\\nHow should the application be structured into its main components? The\\ngoal is to be able to have multiple rest servers\/webapps and multiple\\ndocument processor components working togehter.\\n# Decision Outcome\\nThe following are the \"main\" modules. There may be more helper modules\\nand libraries that support implementing a feature.\\n\n\n##Decision\nThe following are the \"main\" modules. There may be more helper modules\\nand libraries that support implementing a feature.\\n","MatchedIds":[3470,697,4021,4644,389],"Predicted":"## Decision\n\nWe will structure the application into the following main components:\n\n1. **API Gateway**\n    - Acts as the entry point for all client interactions.\n    - Routes requests to the appropriate REST servers or service components.\n    - Handles cross-cutting concerns such as authentication, authorization, and request logging.\n\n2. **REST Server(s) \/ Web Applications**\n    - Consists of multiple independent REST servers.\n    - Each REST server handles a specific domain or subset of the application's functionality.\n    - Implements HTTP endpoints for client interaction.\n\n3. **Document Processor Cluster**\n    - A pool of document processing components that can work concurrently.\n    - Each processor is responsible for handling specific types of document transformations or analyses.\n    - Communicates with the REST servers via well-defined APIs or messaging systems.\n\n4. **Service Coordination Layer**\n    - Manages the lifecycle and coordination of the document processors and REST servers.\n    - Ensures consistency and reliability of services through monitoring and service discovery mechanisms.\n\n5. **Data Storage Layer**\n    - Centralized or distributed databases and file storage systems.\n    - Includes relational databases, NoSQL databases, and object storage systems required by both REST servers and document processors.\n\n6. **Messaging and Event Handling System**\n    - Allows asynchronous communication between components.\n    - Used for passing tasks to document processors and raising events related to business processes.\n\n### Positive Consequences\n- **Scalability:** Components can be scaled independently based on demand, allowing the system to handle a larger volume of requests and documents.\n- **Flexibility:** Different teams can develop and deploy REST servers and document processors autonomously.\n- **Reliability:** The service coordination layer ensures redundancy and failover mechanisms, improving the overall reliability of the system.\n\n### Negative Consequences\n- **Complexity:** Coordinating multiple independent components introduces additional complexity in deployment and management.\n- **Maintenance:** The need to maintain and synchronize multiple modules requires stricter governance and monitoring.\n\n### Follow-up Actions\n- Develop clear APIs and contracts between all components.\n- Implement centralized logging and monitoring to oversee all components' health and performance.\n- Create comprehensive documentation and training materials for developers working on different parts of the system.\n\nThis structured approach will ensure that the application is modular, maintainable, and scalable, allowing it to efficiently handle multiple REST servers and document processor components working in tandem.","GenTime":"2024-08-20 00:27:40"}
{"File Name":"gatemint-sdk\/adr-020-protobuf-transaction-encoding.md","Context":"## Context\\nThis ADR is a continuation of the motivation, design, and context established in\\n[ADR 019](.\/adr-019-protobuf-state-encoding.md), namely, we aim to design the\\nProtocol Buffer migration path for the client-side of the Cosmos SDK.\\nSpecifically, the client-side migration path primarily includes tx generation and\\nsigning, message construction and routing, in addition to CLI & REST handlers and\\nbusiness logic (i.e. queriers).\\nWith this in mind, we will tackle the migration path via two main areas, txs and\\nquerying. However, this ADR solely focuses on transactions. Querying should be\\naddressed in a future ADR, but it should build off of these proposals.\\nBased on detailed discussions ([\\#6030](https:\/\/github.com\/cosmos\/cosmos-sdk\/issues\/6030)\\nand [\\#6078](https:\/\/github.com\/cosmos\/cosmos-sdk\/issues\/6078)), the original\\ndesign for transactions was changed substantially from an `oneof` \/JSON-signing\\napproach to the approach described below.\\n","Decision":"### Transactions\\nSince interface values are encoded with `google.protobuf.Any` in state (see [ADR 019](adr-019-protobuf-state-encoding.md)),\\n`sdk.Msg`s are encoding with `Any` in transactions.\\nOne of the main goals of using `Any` to encode interface values is to have a\\ncore set of types which is reused by apps so that\\nclients can safely be compatible with as many chains as possible.\\nIt is one of the goals of this specification to provide a flexible cross-chain transaction\\nformat that can serve a wide variety of use cases without breaking client\\ncompatibility.\\nIn order to facilitate signing, transactions are separated into `TxBody`,\\nwhich will be re-used by `SignDoc` below, and `signatures`:\\n```proto\\n\/\/ types\/types.proto\\npackage cosmos_sdk.v1;\\nmessage Tx {\\nTxBody body = 1;\\nAuthInfo auth_info = 2;\\n\/\/ A list of signatures that matches the length and order of AuthInfo's signer_infos to\\n\/\/ allow connecting signature meta information like public key and signing mode by position.\\nrepeated bytes signatures = 3;\\n}\\n\/\/ A variant of Tx that pins the signer's exact binary represenation of body and\\n\/\/ auth_info. This is used for signing, broadcasting and verification. The binary\\n\/\/ `serialize(tx: TxRaw)` is stored in Tendermint and the hash `sha256(serialize(tx: TxRaw))`\\n\/\/ becomes the \"txhash\", commonly used as the transaction ID.\\nmessage TxRaw {\\n\/\/ A protobuf serialization of a TxBody that matches the representation in SignDoc.\\nbytes body = 1;\\n\/\/ A protobuf serialization of an AuthInfo that matches the representation in SignDoc.\\nbytes auth_info = 2;\\n\/\/ A list of signatures that matches the length and order of AuthInfo's signer_infos to\\n\/\/ allow connecting signature meta information like public key and signing mode by position.\\nrepeated bytes signatures = 3;\\n}\\nmessage TxBody {\\n\/\/ A list of messages to be executed. The required signers of those messages define\\n\/\/ the number and order of elements in AuthInfo's signer_infos and Tx's signatures.\\n\/\/ Each required signer address is added to the list only the first time it occurs.\\n\/\/\\n\/\/ By convention, the first required signer (usually from the first message) is referred\\n\/\/ to as the primary signer and pays the fee for the whole transaction.\\nrepeated google.protobuf.Any messages = 1;\\nstring memo = 2;\\nint64 timeout_height = 3;\\nrepeated google.protobuf.Any extension_options = 1023;\\n}\\nmessage AuthInfo {\\n\/\/ This list defines the signing modes for the required signers. The number\\n\/\/ and order of elements must match the required signers from TxBody's messages.\\n\/\/ The first element is the primary signer and the one which pays the fee.\\nrepeated SignerInfo signer_infos = 1;\\n\/\/ The fee can be calculated based on the cost of evaluating the body and doing signature verification of the signers. This can be estimated via simulation.\\nFee fee = 2;\\n}\\nmessage SignerInfo {\\n\/\/ The public key is optional for accounts that already exist in state. If unset, the\\n\/\/ verifier can use the required signer address for this position and lookup the public key.\\nPublicKey public_key = 1;\\n\/\/ ModeInfo describes the signing mode of the signer and is a nested\\n\/\/ structure to support nested multisig pubkey's\\nModeInfo mode_info = 2;\\n\/\/ sequence is the sequence of the account, which describes the\\n\/\/ number of committed transactions signed by a given address. It is used to prevent\\n\/\/ replay attacks.\\nuint64 sequence = 3;\\n}\\nmessage ModeInfo {\\noneof sum {\\nSingle single = 1;\\nMulti multi = 2;\\n}\\n\/\/ Single is the mode info for a single signer. It is structured as a message\\n\/\/ to allow for additional fields such as locale for SIGN_MODE_TEXTUAL in the future\\nmessage Single {\\nSignMode mode = 1;\\n}\\n\/\/ Multi is the mode info for a multisig public key\\nmessage Multi {\\n\/\/ bitarray specifies which keys within the multisig are signing\\nCompactBitArray bitarray = 1;\\n\/\/ mode_infos is the corresponding modes of the signers of the multisig\\n\/\/ which could include nested multisig public keys\\nrepeated ModeInfo mode_infos = 2;\\n}\\n}\\nenum SignMode {\\nSIGN_MODE_UNSPECIFIED = 0;\\nSIGN_MODE_DIRECT = 1;\\nSIGN_MODE_TEXTUAL = 2;\\nSIGN_MODE_LEGACY_AMINO_JSON = 127;\\n}\\n```\\nAs will be discussed below, in order to include as much of the `Tx` as possible\\nin the `SignDoc`, `SignerInfo` is separated from signatures so that only the\\nraw signatures themselves live outside of what is signed over.\\nBecause we are aiming for a flexible, extensible cross-chain transaction\\nformat, new transaction processing options should be added to `TxBody` as soon\\nthose use cases are discovered, even if they can't be implemented yet.\\nBecause there is coordination overhead in this, `TxBody` includes an\\n`extension_options` field which can be used for any transaction processing\\noptions that are not already covered. App developers should, nevertheless,\\nattempt to upstream important improvements to `Tx`.\\n### Signing\\nAll of the signing modes below aim to provide the following guarantees:\\n- **No Malleability**: `TxBody` and `AuthInfo` cannot change once the transaction\\nis signed\\n- **Predictable Gas**: if I am signing a transaction where I am paying a fee,\\nthe final gas is fully dependent on what I am signing\\nThese guarantees give the maximum amount confidence to message signers that\\nmanipulation of `Tx`s by intermediaries can't result in any meaningful changes.\\n#### `SIGN_MODE_DIRECT`\\nThe \"direct\" signing behavior is to sign the raw `TxBody` bytes as broadcast over\\nthe wire. This has the advantages of:\\n- requiring the minimum additional client capabilities beyond a standard protocol\\nbuffers implementation\\n- leaving effectively zero holes for transaction malleability (i.e. there are no\\nsubtle differences between the signing and encoding formats which could\\npotentially be exploited by an attacker)\\nSignatures are structured using the `SignDoc` below which reuses the serialization of\\n`TxBody` and `AuthInfo` and only adds the fields which are needed for signatures:\\n```proto\\n\/\/ types\/types.proto\\nmessage SignDoc {\\n\/\/ A protobuf serialization of a TxBody that matches the representation in TxRaw.\\nbytes body = 1;\\n\/\/ A protobuf serialization of an AuthInfo that matches the representation in TxRaw.\\nbytes auth_info = 2;\\nstring chain_id = 3;\\nuint64 account_number = 4;\\n}\\n```\\nIn order to sign in the default mode, clients take the following steps:\\n1. Serialize `TxBody` and `AuthInfo` using any valid protobuf implementation.\\n2. Create a `SignDoc` and serialize it using [ADR 027](.\/adr-027-deterministic-protobuf-serialization.md).\\n3. Sign the encoded `SignDoc` bytes.\\n4. Build a `TxRaw` and serialize it for broadcasting.\\nSignature verification is based on comparing the raw `TxBody` and `AuthInfo`\\nbytes encoded in `TxRaw` not based on any [\"canonicalization\"](https:\/\/github.com\/regen-network\/canonical-proto3)\\nalgorithm which creates added complexity for clients in addition to preventing\\nsome forms of upgradeability (to be addressed later in this document).\\nSignature verifiers do:\\n1. Deserialize a `TxRaw` and pull out `body` and `auth_info`.\\n2. Create a list of required signer addresses from the messages.\\n3. For each required signer:\\n- Pull account number and sequence from the state.\\n- Obtain the public key either from state or `AuthInfo`'s `signer_infos`.\\n- Create a `SignDoc` and serialize it using [ADR 027](.\/adr-027-deterministic-protobuf-serialization.md).\\n- Verify the signature at the the same list position against the serialized `SignDoc`.\\n#### `SIGN_MODE_LEGACY_AMINO`\\nIn order to support legacy wallets and exchanges, Amino JSON will be temporarily\\nsupported transaction signing. Once wallets and exchanges have had a\\nchance to upgrade to protobuf based signing, this option will be disabled. In\\nthe meantime, it is foreseen that disabling the current Amino signing would cause\\ntoo much breakage to be feasible. Note that this is mainly a requirement of the\\nCosmos Hub and other chains may choose to disable Amino signing immediately.\\nLegacy clients will be able to sign a transaction using the current Amino\\nJSON format and have it encoded to protobuf using the REST `\/tx\/encode`\\nendpoint before broadcasting.\\n#### `SIGN_MODE_TEXTUAL`\\nAs was discussed extensively in [\\#6078](https:\/\/github.com\/cosmos\/cosmos-sdk\/issues\/6078),\\nthere is a desire for a human-readable signing encoding, especially for hardware\\nwallets like the [Ledger](https:\/\/www.ledger.com) which display\\ntransaction contents to users before signing. JSON was an attempt at this but\\nfalls short of the ideal.\\n`SIGN_MODE_TEXTUAL` is intended as a placeholder for a human-readable\\nencoding which will replace Amino JSON. This new encoding should be even more\\nfocused on readability than JSON, possibly based on formatting strings like\\n[MessageFormat](http:\/\/userguide.icu-project.org\/formatparse\/messages).\\nIn order to ensure that the new human-readable format does not suffer from\\ntransaction malleability issues, `SIGN_MODE_TEXTUAL`\\nrequires that the _human-readable bytes are concatenated with the raw `SignDoc`_\\nto generate sign bytes.\\nMultiple human-readable formats (maybe even localized messages) may be supported\\nby `SIGN_MODE_TEXTUAL` when it is implemented.\\n### Unknown Field Filtering\\nUnknown fields in protobuf messages should generally be rejected by transaction\\nprocessors because:\\n- important data may be present in the unknown fields, that if ignored, will\\ncause unexpected behavior for clients\\n- they present a malleability vulnerability where attackers can bloat tx size\\nby adding random uninterpreted data to unsigned content (i.e. the master `Tx`,\\nnot `TxBody`)\\nThere are also scenarios where we may choose to safely ignore unknown fields\\n(https:\/\/github.com\/cosmos\/cosmos-sdk\/issues\/6078#issuecomment-624400188) to\\nprovide graceful forwards compatibility with newer clients.\\nWe propose that field numbers with bit 11 set (for most use cases this is\\nthe range of 1024-2047) be considered non-critical fields that can safely be\\nignored if unknown.\\nTo handle this we will need a unknown field filter that:\\n- always rejects unknown fields in unsigned content (i.e. top-level `Tx` and\\nunsigned parts of `AuthInfo` if present based on the signing mode)\\n- rejects unknown fields in all messages (including nested `Any`s) other than\\nfields with bit 11 set\\nThis will likely need to be a custom protobuf parser pass that takes message bytes\\nand `FileDescriptor`s and returns a boolean result.\\n### Public Key Encoding\\nPublic keys in the Cosmos SDK implement Tendermint's `crypto.PubKey` interface,\\nso a natural solution might be to use `Any` as we are doing for other interfaces.\\nThere are, however, a limited number of public keys in existence and new ones\\naren't created overnight. The proposed solution is to use a `oneof` that:\\n- attempts to catalog all known key types even if a given app can't use them all\\n- has an `Any` member that can be used when a key type isn't present in the `oneof`\\nEx:\\n```proto\\nmessage PublicKey {\\noneof sum {\\nbytes secp256k1 = 1;\\nbytes ed25519 = 2;\\n...\\ngoogle.protobuf.Any any_pubkey = 15;\\n}\\n}\\n```\\nApps should only attempt to handle a registered set of public keys that they\\nhave tested. The provided signature verification ante handler decorators will\\nenforce this.\\n### CLI & REST\\nCurrently, the REST and CLI handlers encode and decode types and txs via Amino\\nJSON encoding using a concrete Amino codec. Being that some of the types dealt with\\nin the client can be interfaces, similar to how we described in [ADR 019](.\/adr-019-protobuf-state-encoding.md),\\nthe client logic will now need to take a codec interface that knows not only how\\nto handle all the types, but also knows how to generate transactions, signatures,\\nand messages.\\n```go\\ntype AccountRetriever interface {\\nEnsureExists(clientCtx client.Context, addr sdk.AccAddress) error\\nGetAccountNumberSequence(clientCtx client.Context, addr sdk.AccAddress) (uint64, uint64, error)\\n}\\ntype Generator interface {\\nNewTx() TxBuilder\\nNewFee() ClientFee\\nNewSignature() ClientSignature\\nMarshalTx(tx types.Tx) ([]byte, error)\\n}\\ntype TxBuilder interface {\\nGetTx() sdk.Tx\\nSetMsgs(...sdk.Msg) error\\nGetSignatures() []sdk.Signature\\nSetSignatures(...sdk.Signature)\\nGetFee() sdk.Fee\\nSetFee(sdk.Fee)\\nGetMemo() string\\nSetMemo(string)\\n}\\n```\\nWe then update `Context` to have new fields: `JSONMarshaler`, `TxGenerator`,\\nand `AccountRetriever`, and we update `AppModuleBasic.GetTxCmd` to take\\na `Context` which should have all of these fields pre-populated.\\nEach client method should then use one of the `Init` methods to re-initialize\\nthe pre-populated `Context`. `tx.GenerateOrBroadcastTx` can be used to\\ngenerate or broadcast a transaction. For example:\\n```go\\nimport \"github.com\/spf13\/cobra\"\\nimport \"github.com\/cosmos\/cosmos-sdk\/client\"\\nimport \"github.com\/cosmos\/cosmos-sdk\/client\/tx\"\\nfunc NewCmdDoSomething(clientCtx client.Context) *cobra.Command {\\nreturn &cobra.Command{\\nRunE: func(cmd *cobra.Command, args []string) error {\\nclientCtx := ctx.InitWithInput(cmd.InOrStdin())\\nmsg := NewSomeMsg{...}\\ntx.GenerateOrBroadcastTx(clientCtx, msg)\\n},\\n}\\n}\\n```\\n","tokens":234,"id":21,"text":"## Context\\nThis ADR is a continuation of the motivation, design, and context established in\\n[ADR 019](.\/adr-019-protobuf-state-encoding.md), namely, we aim to design the\\nProtocol Buffer migration path for the client-side of the Cosmos SDK.\\nSpecifically, the client-side migration path primarily includes tx generation and\\nsigning, message construction and routing, in addition to CLI & REST handlers and\\nbusiness logic (i.e. queriers).\\nWith this in mind, we will tackle the migration path via two main areas, txs and\\nquerying. However, this ADR solely focuses on transactions. Querying should be\\naddressed in a future ADR, but it should build off of these proposals.\\nBased on detailed discussions ([\\#6030](https:\/\/github.com\/cosmos\/cosmos-sdk\/issues\/6030)\\nand [\\#6078](https:\/\/github.com\/cosmos\/cosmos-sdk\/issues\/6078)), the original\\ndesign for transactions was changed substantially from an `oneof` \/JSON-signing\\napproach to the approach described below.\\n\n\n##Decision\n### Transactions\\nSince interface values are encoded with `google.protobuf.Any` in state (see [ADR 019](adr-019-protobuf-state-encoding.md)),\\n`sdk.Msg`s are encoding with `Any` in transactions.\\nOne of the main goals of using `Any` to encode interface values is to have a\\ncore set of types which is reused by apps so that\\nclients can safely be compatible with as many chains as possible.\\nIt is one of the goals of this specification to provide a flexible cross-chain transaction\\nformat that can serve a wide variety of use cases without breaking client\\ncompatibility.\\nIn order to facilitate signing, transactions are separated into `TxBody`,\\nwhich will be re-used by `SignDoc` below, and `signatures`:\\n```proto\\n\/\/ types\/types.proto\\npackage cosmos_sdk.v1;\\nmessage Tx {\\nTxBody body = 1;\\nAuthInfo auth_info = 2;\\n\/\/ A list of signatures that matches the length and order of AuthInfo's signer_infos to\\n\/\/ allow connecting signature meta information like public key and signing mode by position.\\nrepeated bytes signatures = 3;\\n}\\n\/\/ A variant of Tx that pins the signer's exact binary represenation of body and\\n\/\/ auth_info. This is used for signing, broadcasting and verification. The binary\\n\/\/ `serialize(tx: TxRaw)` is stored in Tendermint and the hash `sha256(serialize(tx: TxRaw))`\\n\/\/ becomes the \"txhash\", commonly used as the transaction ID.\\nmessage TxRaw {\\n\/\/ A protobuf serialization of a TxBody that matches the representation in SignDoc.\\nbytes body = 1;\\n\/\/ A protobuf serialization of an AuthInfo that matches the representation in SignDoc.\\nbytes auth_info = 2;\\n\/\/ A list of signatures that matches the length and order of AuthInfo's signer_infos to\\n\/\/ allow connecting signature meta information like public key and signing mode by position.\\nrepeated bytes signatures = 3;\\n}\\nmessage TxBody {\\n\/\/ A list of messages to be executed. The required signers of those messages define\\n\/\/ the number and order of elements in AuthInfo's signer_infos and Tx's signatures.\\n\/\/ Each required signer address is added to the list only the first time it occurs.\\n\/\/\\n\/\/ By convention, the first required signer (usually from the first message) is referred\\n\/\/ to as the primary signer and pays the fee for the whole transaction.\\nrepeated google.protobuf.Any messages = 1;\\nstring memo = 2;\\nint64 timeout_height = 3;\\nrepeated google.protobuf.Any extension_options = 1023;\\n}\\nmessage AuthInfo {\\n\/\/ This list defines the signing modes for the required signers. The number\\n\/\/ and order of elements must match the required signers from TxBody's messages.\\n\/\/ The first element is the primary signer and the one which pays the fee.\\nrepeated SignerInfo signer_infos = 1;\\n\/\/ The fee can be calculated based on the cost of evaluating the body and doing signature verification of the signers. This can be estimated via simulation.\\nFee fee = 2;\\n}\\nmessage SignerInfo {\\n\/\/ The public key is optional for accounts that already exist in state. If unset, the\\n\/\/ verifier can use the required signer address for this position and lookup the public key.\\nPublicKey public_key = 1;\\n\/\/ ModeInfo describes the signing mode of the signer and is a nested\\n\/\/ structure to support nested multisig pubkey's\\nModeInfo mode_info = 2;\\n\/\/ sequence is the sequence of the account, which describes the\\n\/\/ number of committed transactions signed by a given address. It is used to prevent\\n\/\/ replay attacks.\\nuint64 sequence = 3;\\n}\\nmessage ModeInfo {\\noneof sum {\\nSingle single = 1;\\nMulti multi = 2;\\n}\\n\/\/ Single is the mode info for a single signer. It is structured as a message\\n\/\/ to allow for additional fields such as locale for SIGN_MODE_TEXTUAL in the future\\nmessage Single {\\nSignMode mode = 1;\\n}\\n\/\/ Multi is the mode info for a multisig public key\\nmessage Multi {\\n\/\/ bitarray specifies which keys within the multisig are signing\\nCompactBitArray bitarray = 1;\\n\/\/ mode_infos is the corresponding modes of the signers of the multisig\\n\/\/ which could include nested multisig public keys\\nrepeated ModeInfo mode_infos = 2;\\n}\\n}\\nenum SignMode {\\nSIGN_MODE_UNSPECIFIED = 0;\\nSIGN_MODE_DIRECT = 1;\\nSIGN_MODE_TEXTUAL = 2;\\nSIGN_MODE_LEGACY_AMINO_JSON = 127;\\n}\\n```\\nAs will be discussed below, in order to include as much of the `Tx` as possible\\nin the `SignDoc`, `SignerInfo` is separated from signatures so that only the\\nraw signatures themselves live outside of what is signed over.\\nBecause we are aiming for a flexible, extensible cross-chain transaction\\nformat, new transaction processing options should be added to `TxBody` as soon\\nthose use cases are discovered, even if they can't be implemented yet.\\nBecause there is coordination overhead in this, `TxBody` includes an\\n`extension_options` field which can be used for any transaction processing\\noptions that are not already covered. App developers should, nevertheless,\\nattempt to upstream important improvements to `Tx`.\\n### Signing\\nAll of the signing modes below aim to provide the following guarantees:\\n- **No Malleability**: `TxBody` and `AuthInfo` cannot change once the transaction\\nis signed\\n- **Predictable Gas**: if I am signing a transaction where I am paying a fee,\\nthe final gas is fully dependent on what I am signing\\nThese guarantees give the maximum amount confidence to message signers that\\nmanipulation of `Tx`s by intermediaries can't result in any meaningful changes.\\n#### `SIGN_MODE_DIRECT`\\nThe \"direct\" signing behavior is to sign the raw `TxBody` bytes as broadcast over\\nthe wire. This has the advantages of:\\n- requiring the minimum additional client capabilities beyond a standard protocol\\nbuffers implementation\\n- leaving effectively zero holes for transaction malleability (i.e. there are no\\nsubtle differences between the signing and encoding formats which could\\npotentially be exploited by an attacker)\\nSignatures are structured using the `SignDoc` below which reuses the serialization of\\n`TxBody` and `AuthInfo` and only adds the fields which are needed for signatures:\\n```proto\\n\/\/ types\/types.proto\\nmessage SignDoc {\\n\/\/ A protobuf serialization of a TxBody that matches the representation in TxRaw.\\nbytes body = 1;\\n\/\/ A protobuf serialization of an AuthInfo that matches the representation in TxRaw.\\nbytes auth_info = 2;\\nstring chain_id = 3;\\nuint64 account_number = 4;\\n}\\n```\\nIn order to sign in the default mode, clients take the following steps:\\n1. Serialize `TxBody` and `AuthInfo` using any valid protobuf implementation.\\n2. Create a `SignDoc` and serialize it using [ADR 027](.\/adr-027-deterministic-protobuf-serialization.md).\\n3. Sign the encoded `SignDoc` bytes.\\n4. Build a `TxRaw` and serialize it for broadcasting.\\nSignature verification is based on comparing the raw `TxBody` and `AuthInfo`\\nbytes encoded in `TxRaw` not based on any [\"canonicalization\"](https:\/\/github.com\/regen-network\/canonical-proto3)\\nalgorithm which creates added complexity for clients in addition to preventing\\nsome forms of upgradeability (to be addressed later in this document).\\nSignature verifiers do:\\n1. Deserialize a `TxRaw` and pull out `body` and `auth_info`.\\n2. Create a list of required signer addresses from the messages.\\n3. For each required signer:\\n- Pull account number and sequence from the state.\\n- Obtain the public key either from state or `AuthInfo`'s `signer_infos`.\\n- Create a `SignDoc` and serialize it using [ADR 027](.\/adr-027-deterministic-protobuf-serialization.md).\\n- Verify the signature at the the same list position against the serialized `SignDoc`.\\n#### `SIGN_MODE_LEGACY_AMINO`\\nIn order to support legacy wallets and exchanges, Amino JSON will be temporarily\\nsupported transaction signing. Once wallets and exchanges have had a\\nchance to upgrade to protobuf based signing, this option will be disabled. In\\nthe meantime, it is foreseen that disabling the current Amino signing would cause\\ntoo much breakage to be feasible. Note that this is mainly a requirement of the\\nCosmos Hub and other chains may choose to disable Amino signing immediately.\\nLegacy clients will be able to sign a transaction using the current Amino\\nJSON format and have it encoded to protobuf using the REST `\/tx\/encode`\\nendpoint before broadcasting.\\n#### `SIGN_MODE_TEXTUAL`\\nAs was discussed extensively in [\\#6078](https:\/\/github.com\/cosmos\/cosmos-sdk\/issues\/6078),\\nthere is a desire for a human-readable signing encoding, especially for hardware\\nwallets like the [Ledger](https:\/\/www.ledger.com) which display\\ntransaction contents to users before signing. JSON was an attempt at this but\\nfalls short of the ideal.\\n`SIGN_MODE_TEXTUAL` is intended as a placeholder for a human-readable\\nencoding which will replace Amino JSON. This new encoding should be even more\\nfocused on readability than JSON, possibly based on formatting strings like\\n[MessageFormat](http:\/\/userguide.icu-project.org\/formatparse\/messages).\\nIn order to ensure that the new human-readable format does not suffer from\\ntransaction malleability issues, `SIGN_MODE_TEXTUAL`\\nrequires that the _human-readable bytes are concatenated with the raw `SignDoc`_\\nto generate sign bytes.\\nMultiple human-readable formats (maybe even localized messages) may be supported\\nby `SIGN_MODE_TEXTUAL` when it is implemented.\\n### Unknown Field Filtering\\nUnknown fields in protobuf messages should generally be rejected by transaction\\nprocessors because:\\n- important data may be present in the unknown fields, that if ignored, will\\ncause unexpected behavior for clients\\n- they present a malleability vulnerability where attackers can bloat tx size\\nby adding random uninterpreted data to unsigned content (i.e. the master `Tx`,\\nnot `TxBody`)\\nThere are also scenarios where we may choose to safely ignore unknown fields\\n(https:\/\/github.com\/cosmos\/cosmos-sdk\/issues\/6078#issuecomment-624400188) to\\nprovide graceful forwards compatibility with newer clients.\\nWe propose that field numbers with bit 11 set (for most use cases this is\\nthe range of 1024-2047) be considered non-critical fields that can safely be\\nignored if unknown.\\nTo handle this we will need a unknown field filter that:\\n- always rejects unknown fields in unsigned content (i.e. top-level `Tx` and\\nunsigned parts of `AuthInfo` if present based on the signing mode)\\n- rejects unknown fields in all messages (including nested `Any`s) other than\\nfields with bit 11 set\\nThis will likely need to be a custom protobuf parser pass that takes message bytes\\nand `FileDescriptor`s and returns a boolean result.\\n### Public Key Encoding\\nPublic keys in the Cosmos SDK implement Tendermint's `crypto.PubKey` interface,\\nso a natural solution might be to use `Any` as we are doing for other interfaces.\\nThere are, however, a limited number of public keys in existence and new ones\\naren't created overnight. The proposed solution is to use a `oneof` that:\\n- attempts to catalog all known key types even if a given app can't use them all\\n- has an `Any` member that can be used when a key type isn't present in the `oneof`\\nEx:\\n```proto\\nmessage PublicKey {\\noneof sum {\\nbytes secp256k1 = 1;\\nbytes ed25519 = 2;\\n...\\ngoogle.protobuf.Any any_pubkey = 15;\\n}\\n}\\n```\\nApps should only attempt to handle a registered set of public keys that they\\nhave tested. The provided signature verification ante handler decorators will\\nenforce this.\\n### CLI & REST\\nCurrently, the REST and CLI handlers encode and decode types and txs via Amino\\nJSON encoding using a concrete Amino codec. Being that some of the types dealt with\\nin the client can be interfaces, similar to how we described in [ADR 019](.\/adr-019-protobuf-state-encoding.md),\\nthe client logic will now need to take a codec interface that knows not only how\\nto handle all the types, but also knows how to generate transactions, signatures,\\nand messages.\\n```go\\ntype AccountRetriever interface {\\nEnsureExists(clientCtx client.Context, addr sdk.AccAddress) error\\nGetAccountNumberSequence(clientCtx client.Context, addr sdk.AccAddress) (uint64, uint64, error)\\n}\\ntype Generator interface {\\nNewTx() TxBuilder\\nNewFee() ClientFee\\nNewSignature() ClientSignature\\nMarshalTx(tx types.Tx) ([]byte, error)\\n}\\ntype TxBuilder interface {\\nGetTx() sdk.Tx\\nSetMsgs(...sdk.Msg) error\\nGetSignatures() []sdk.Signature\\nSetSignatures(...sdk.Signature)\\nGetFee() sdk.Fee\\nSetFee(sdk.Fee)\\nGetMemo() string\\nSetMemo(string)\\n}\\n```\\nWe then update `Context` to have new fields: `JSONMarshaler`, `TxGenerator`,\\nand `AccountRetriever`, and we update `AppModuleBasic.GetTxCmd` to take\\na `Context` which should have all of these fields pre-populated.\\nEach client method should then use one of the `Init` methods to re-initialize\\nthe pre-populated `Context`. `tx.GenerateOrBroadcastTx` can be used to\\ngenerate or broadcast a transaction. For example:\\n```go\\nimport \"github.com\/spf13\/cobra\"\\nimport \"github.com\/cosmos\/cosmos-sdk\/client\"\\nimport \"github.com\/cosmos\/cosmos-sdk\/client\/tx\"\\nfunc NewCmdDoSomething(clientCtx client.Context) *cobra.Command {\\nreturn &cobra.Command{\\nRunE: func(cmd *cobra.Command, args []string) error {\\nclientCtx := ctx.InitWithInput(cmd.InOrStdin())\\nmsg := NewSomeMsg{...}\\ntx.GenerateOrBroadcastTx(clientCtx, msg)\\n},\\n}\\n}\\n```\\n","MatchedIds":[810,1442,4097,4650,1979],"Predicted":"## Decision\n\n### Introduction and Goals\nThe primary goal of this ADR is to define the protocol buffer migration path for transaction generation, signing, and broadcasting in the Cosmos SDK. By fully migrating to protocol buffers, we aim to standardize transactions, improve efficiency, and ensure compatibility with broader ecosystem tools.\n\n### Protobuf Transaction Definition\nWe will define transactions (`Tx`) using protocol buffer syntax. Each `Tx` will consist of the following components:\n- **`body`**: The `TxBody` message that contains details such as messages and memo.\n- **`auth_info`**: The `AuthInfo` message which includes details on signer information, fees, and gas limits.\n- **`signatures`**: A list of cryptographic signatures.\n\n```protobuf\n\/\/ transactions.proto\nsyntax = \"proto3\";\npackage cosmos.tx;\n\nmessage Tx {\n  TxBody body = 1;\n  AuthInfo auth_info = 2;\n  repeated bytes signatures = 3;\n}\n\nmessage TxBody {\n  repeated cosmos.Msg messages = 1;\n  string memo = 2;\n}\n\nmessage AuthInfo {\n  repeated SignerInfo signer_infos = 1;\n  Fee fee = 2;\n}\n\nmessage SignerInfo {\n  PublicKey public_key = 1;\n  google.protobuf.Int64Value sequence = 2; \/\/ Sequence number of the signer account\n}\n\nmessage Fee {\n  cosmos.base.v1beta1.Coin amount = 1;\n  int64 gas_limit = 2;\n}\n\nmessage PublicKey {\n  oneof sum {\n    bytes secp256k1 = 1;\n    bytes ed25519 = 2;\n  }\n}\n```\n\n### Client-side Transaction Construction\nTo construct a transaction, the client will follow these steps:\n1. **Message Creation**: Create a list of `cosmos.Msg` messages.\n2. **Transaction Body**: Formulate the `TxBody` with the necessary messages and memo.\n3. **Authorization Info**: Construct `AuthInfo` containing `SignerInfo` and `Fee`.\n4. **Signatures**: Attach signatures after signing the `signDoc`.\n\n```go\nfunc CreateTx(body tx.TxBody, authInfo tx.AuthInfo, signBytes [][]byte) tx.Tx {\n  return tx.Tx{\n    Body:      body,\n    AuthInfo:  authInfo,\n    Signatures: signBytes,\n  }\n}\n```\n\n### Signer Interface\nThe signer interface will generate the `signDoc` from `TxBody` and `AuthInfo`, sign it, and then attach the signature to the `Tx`.\n\n```go\ntype Signer interface {\n  Sign(signBytes []byte, publicKey crypto.PublicKey) ([]byte, error)\n}\n\nfunc SignTransaction(signer Signer, tx tx.Tx, publicKey crypto.PublicKey) tx.Tx {\n  signBytes := \/\/ Create signDoc using tx.Body and tx.AuthInfo\n  signature, _ := signer.Sign(signBytes, publicKey)\n  tx.Signatures = append(tx.Signatures, signature)\n  return tx\n}\n```\n\n### Achieving Compatibility\nThe Cosmos SDK will ensure backward compatibility with existing `JSON` signing infrastructures by providing translation layers to convert between `JSON` and `protobuf`.\n\n### Validation and Broadcast\nDespite leveraging protocol buffer serialization, the transaction broadcast mechanism remains unchanged and communicates with Tendermint's RPC endpoints.\n\n### Additional Considerations\nFuture ADRs will address the following:\n1. **Query Migration**: Extend similar protobuf patterns to querying with compatible GRPC support.\n2. **Codec Registration**: Ensure all protocol buffer definitions are registered within the appropriate codec to facilitate encoding and decoding.\n\nThis approach aims to streamline transaction handling on the client side, increase interoperability, and enhance the overall efficiency within the Cosmos SDK framework.","GenTime":"2024-08-20 00:27:52"}
{"File Name":"architecture\/0007-use-static-code-analysis.md","Context":"## Context\\nYou never think of everything. Sticking to standards is a very good\\nthing to prevent you from doing things that can go bad. Those also\\nhelps making the code be more readable and structured.\\n","Decision":"Use Static Code Analysis to find violations of standards.\\n","tokens":45,"id":1418,"text":"## Context\\nYou never think of everything. Sticking to standards is a very good\\nthing to prevent you from doing things that can go bad. Those also\\nhelps making the code be more readable and structured.\\n\n\n##Decision\nUse Static Code Analysis to find violations of standards.\\n","MatchedIds":[5070,3853,1416,400,2270],"Predicted":"## Decision\nWe will adhere to established coding standards and best practices, such as the [Google Style Guides](https:\/\/google.github.io\/styleguide\/) or [PEP 8](https:\/\/www.python.org\/dev\/peps\/pep-0008\/) for Python. Additionally, we will enforce these standards through automated linters and formatters integrated into our CI\/CD pipeline to ensure consistent code quality and structure.","GenTime":"2024-08-20 00:27:54"}
{"File Name":"csc-swr-architecture\/003-Platform-and-Hosting.html.md.erb","Context":"## Context\\nThe Childrens Social Care Placement service is in Alpha and needs to explore solutions to the domain problem. Given this is still very much an exploratory phase in the Software Delivery LifeCycle (SDLC) it seems appropriate to optimise around developer productivity so that the service can flex to the most appropriate fit.\\n","Decision":"The decision is to use the [Gov.UK PaaS].\\n**What does Gov.UK PaaS offer?**\\nGov.UK PaaS is a fully managed platform service which has central government accredition for Offical data and provides teams with:-\\n* Fast application delivery, with short cycles\\n* Flexible development tools\\n* Simplified DevOps\\n* Seamless scalability\\n* Integrated data services\\n* A secure, trusted platform\\n* 24x7 support\\nIt is also important to note that Ruby on Rails is a first class citizen on Gov.UK as one of its core buildpack languages and frameworks.\\n**Why not DfE Azure?**\\nThere are two primary reasons why; simplified DevOps and increased developer productivity.\\n**Is it secure?**\\nOne of the main drivers for Departmental Platforms is security assurance. Gov.UK PaaS is accredited by for Official information.\\nSome useful URLs for security in general on Gov.UK PaaS are [here](https:\/\/www.cloud.service.gov.uk\/security\/) and the [security principles](https:\/\/www.cloud.service.gov.uk\/cloud-security-principles\/).\\nAnd for the DfE there is a specific [paper](https:\/\/docs.google.com\/document\/d\/1_81bqO2snLDn95p9W5OyMXwqzUqX5qcHSYJvV5CUeeQ\/edit#heading=h.du8tl2830srf) from MBS.\\n","tokens":65,"id":920,"text":"## Context\\nThe Childrens Social Care Placement service is in Alpha and needs to explore solutions to the domain problem. Given this is still very much an exploratory phase in the Software Delivery LifeCycle (SDLC) it seems appropriate to optimise around developer productivity so that the service can flex to the most appropriate fit.\\n\n\n##Decision\nThe decision is to use the [Gov.UK PaaS].\\n**What does Gov.UK PaaS offer?**\\nGov.UK PaaS is a fully managed platform service which has central government accredition for Offical data and provides teams with:-\\n* Fast application delivery, with short cycles\\n* Flexible development tools\\n* Simplified DevOps\\n* Seamless scalability\\n* Integrated data services\\n* A secure, trusted platform\\n* 24x7 support\\nIt is also important to note that Ruby on Rails is a first class citizen on Gov.UK as one of its core buildpack languages and frameworks.\\n**Why not DfE Azure?**\\nThere are two primary reasons why; simplified DevOps and increased developer productivity.\\n**Is it secure?**\\nOne of the main drivers for Departmental Platforms is security assurance. Gov.UK PaaS is accredited by for Official information.\\nSome useful URLs for security in general on Gov.UK PaaS are [here](https:\/\/www.cloud.service.gov.uk\/security\/) and the [security principles](https:\/\/www.cloud.service.gov.uk\/cloud-security-principles\/).\\nAnd for the DfE there is a specific [paper](https:\/\/docs.google.com\/document\/d\/1_81bqO2snLDn95p9W5OyMXwqzUqX5qcHSYJvV5CUeeQ\/edit#heading=h.du8tl2830srf) from MBS.\\n","MatchedIds":[915,2093,914,1480,3982],"Predicted":"## Decision\nWe will adopt a flexible and agile development environment optimized for developer productivity. Specifically:\n\n1. **Use Heroku for Deployment**: Leveraging Heroku for its ease of deployment, built-in CI\/CD pipelines, and general ease of use, allowing developers to quickly iterate and deploy new features.\n2. **Utilize Fake Data**: All data processed during the Alpha phase will be fake data to circumvent the overhead associated with security and data governance for real data.\n3. **Adopt Lightweight Frameworks and Libraries**: Prefer lightweight and flexible frameworks like Flask or Express.js for rapid prototyping.\n4. **Implement Feature Flags**: Use feature flags to enable or disable features in the application dynamically, facilitating quick experimentation.\n5. **Focus on Mockups and Prototypes**: Prioritize developing user interface mockups and interactive prototypes to gather user feedback early and often.\n6. **Automated Testing and Continuous Integration**: Implement automated testing and continuous integration from the start to ensure high code quality without slowing down the development process.\n7. **Use of Collaborative Tools**: Utilize collaborative tools such as Slack, Jira, or Trello to streamline communication and project management within the development team.\n\nThis approach is intended to allow for rapid, iterative development, allowing the team to quickly adapt and respond to user needs and feedback while minimizing overhead.","GenTime":"2024-08-20 00:27:59"}
{"File Name":"abracadabra\/0008-don-t-propose-quick-fix-for-react-convert-to-pure-component.md","Context":"## Context\\nAfter analysing performances of the extension on a big JavaScript file, [we noticed][comment] the \"Convert to Pure Component\" Quick Fix took most of the time:\\n![][flame-chart]\\nThis refactoring can only be applied under specific circumstances. It doesn't worth the toll for everyone, on every trigger.\\nAlso, there's nothing we can do to improve the performance of this refactoring. The code is implemented by [react-codemod](https:\/\/github.com\/reactjs\/react-codemod). It was originally meant to be run through a CLI command.\\n","Decision":"We will stop proposing a Quick Fix for this refactoring.\\n","tokens":122,"id":586,"text":"## Context\\nAfter analysing performances of the extension on a big JavaScript file, [we noticed][comment] the \"Convert to Pure Component\" Quick Fix took most of the time:\\n![][flame-chart]\\nThis refactoring can only be applied under specific circumstances. It doesn't worth the toll for everyone, on every trigger.\\nAlso, there's nothing we can do to improve the performance of this refactoring. The code is implemented by [react-codemod](https:\/\/github.com\/reactjs\/react-codemod). It was originally meant to be run through a CLI command.\\n\n\n##Decision\nWe will stop proposing a Quick Fix for this refactoring.\\n","MatchedIds":[5140,3428,2687,1652,3321],"Predicted":"## Decision\n- **Disable Automatic Triggering for \"Convert to Pure Component\" Quick Fix**\n  Given the high performance toll and the specific circumstances under which this refactoring is applicable, we have decided to remove the automatic triggering of the \"Convert to Pure Component\" Quick Fix.\n\n- **Manually Trigger the Quick Fix**\n  Users will have the option to manually invoke the \"Convert to Pure Component\" Quick Fix when they specifically need it. This allows developers to leverage the functionality without negatively impacting performance during general usage.\n\n- **Update Documentation**\n  Clearly document the usage and performance considerations of the \"Convert to Pure Component\" refactoring in the extension\u2019s help files and guides. This ensures developers are aware that this feature is available but comes with performance costs and is best used manually.\n\n- **Evaluate Performance Regularly**\n  Continue to monitor the performance of the extension and evaluate other Quick Fix operations to ensure that they do not impose similar performance tolls. If other similar issues are identified, consider similar measures or investigate performance optimization strategies.\n\nBy implementing this decision, we aim to significantly improve the extension's responsiveness while still providing valuable refactoring tools to developers as needed.","GenTime":"2024-08-20 00:28:05"}
{"File Name":"latis3\/0005-Operations-defined-in-FDML-schema-as-elements.md","Context":"## Context and Problem Statement\\nOperations can be defined in the FDML as elements and attributes or simply as elements.  For example the operation take can be described as:\\n```\\n<xs:element name=\"take\" type=\"xs:integer\">\\n```\\nor with attributes as:\\n```\\n<xs:element name=\"take\">\\n<xs:complexType>\\n<xs:attribute name=\"value\"\/>\\n<\/xs:complexType>\\n<\/xs:element>\\n```\\n## Decision Drivers\\n* consistency\\n* expressiveness\\n","Decision":"* consistency\\n* expressiveness\\nChosen option: Try to describe operations as elements only\\n### Positive Consequences\\n* consistency\\n### Negative Consequences\\n* future operations may not be definable\\n","tokens":115,"id":4713,"text":"## Context and Problem Statement\\nOperations can be defined in the FDML as elements and attributes or simply as elements.  For example the operation take can be described as:\\n```\\n<xs:element name=\"take\" type=\"xs:integer\">\\n```\\nor with attributes as:\\n```\\n<xs:element name=\"take\">\\n<xs:complexType>\\n<xs:attribute name=\"value\"\/>\\n<\/xs:complexType>\\n<\/xs:element>\\n```\\n## Decision Drivers\\n* consistency\\n* expressiveness\\n\n\n##Decision\n* consistency\\n* expressiveness\\nChosen option: Try to describe operations as elements only\\n### Positive Consequences\\n* consistency\\n### Negative Consequences\\n* future operations may not be definable\\n","MatchedIds":[674,3529,4307,21,994],"Predicted":"## Decision\n\n### Chosen Option: Use Elements with Attributes\n\nHaving evaluated the decision drivers of consistency and expressiveness, the decision is to prefer defining operations as elements with attributes, as shown below:\n\n```xml\n<xs:element name=\"take\">\n  <xs:complexType>\n    <xs:attribute name=\"value\" type=\"xs:integer\"\/>\n  <\/xs:complexType>\n<\/xs:element>\n```\n\n### Positive Consequences\n1. **Consistency**: All operations will have a uniform structure, ensuring that attributes are always defined in a consistent manner. This helps in maintaining clear documentation and predictability in the FDML schema.\n2. **Expressiveness**:\n   - Attributes provide a more detailed context for element definitions, making it easier to describe complex configurations and extend functionality in the future.\n   - It allows for additional metadata to be attached to elements, improving the clarity and usability of the schema.\n3. **Validation and Parsing**:\n   - XSD validation can more effectively ensure that required attributes are present and correctly typed.\n   - XML parsers can easily handle elements with attributes, enabling better tooling support and error handling.\n\n### Negative Consequences\n1. **Verbosity**: Using attributes can make the XML schema slightly more verbose, which might add to the complexity of writing and reading the schema.\n2. **Complexity in Simple Cases**: For very simple operations, using attributes might introduce unnecessary complexity. Simple elements could be more succinct without attributes.\n\n### Overall Decision\nThe benefits of consistency and expressiveness outweigh the downsides of verbosity and complexity in simple cases. Hence, for FDML definitions, using elements with attributes is the preferred solution.","GenTime":"2024-08-20 00:28:12"}
{"File Name":"operational-visualizations\/0001-about-cursors.md","Context":"## Context\\nIn the grid in cell function we recieve `FragmentFrame` which is vertical slice of original `DataFrame` and then when we render visualaisation we iterate through raw data with `mapRows` method of `IterableFrame` and then we need to access some exact value in raw row. One of the ideas on how to do this universally was to use some kind of **cursor**.\\nCurrent implementation of **cursor** looks like this\\n```tsx\\nexport interface ColumnCursor<Name extends string, ValueInRawRow = any> {\\n(row: RowCursor): ValueInRawRow;\\ncolumn: Name;\\nindex: number;\\n}\\n```\\nso we can access value like this `cursor(row)` or more hacky and optimised way `row[cursor.index]`. Second option is fragile, in case we would want to change implementation of `row` (for example, from row-oriented storage to column-orinted) it will brake in many places (at least we have types, which will make this refactoring easier).\\nCursor at the moment can be recieved from \"root\" `DataFrame`, which we think of as source of all derivative `PivotFrame`s and `FragmentFrame`s. Cursors are cached for referential transparency, so if they got passed down to React components, they will not trigger rerender.\\n**Question** raised in [one of PR](https:\/\/github.com\/contiamo\/operational-visualizations\/pull\/70\/files#diff-2044c7c7ba6c3fbf04dd49cf3cfa68b9R38): Shall we provide `getCursor` functionality in `FragmentFrame`? From one side it may make code simpler, on the other side we need to make sure that `FragmentFrame`s are referentially transperent (which is not the case, for example, for `cell` method) and it will create a lot of copies of the same cursors. We can as well pass reference to \"root\" `DataFrame` along all derivative structures and \"proxy\" `getCursor` method call to it.\\n","Decision":"At the moment we decided to use proposed version of cursors and adjust it in the future depending if we like DX or not. We will provide `getCursor` method only for `DataFrame` for now, but we can chnage this in the future.\\n","tokens":439,"id":977,"text":"## Context\\nIn the grid in cell function we recieve `FragmentFrame` which is vertical slice of original `DataFrame` and then when we render visualaisation we iterate through raw data with `mapRows` method of `IterableFrame` and then we need to access some exact value in raw row. One of the ideas on how to do this universally was to use some kind of **cursor**.\\nCurrent implementation of **cursor** looks like this\\n```tsx\\nexport interface ColumnCursor<Name extends string, ValueInRawRow = any> {\\n(row: RowCursor): ValueInRawRow;\\ncolumn: Name;\\nindex: number;\\n}\\n```\\nso we can access value like this `cursor(row)` or more hacky and optimised way `row[cursor.index]`. Second option is fragile, in case we would want to change implementation of `row` (for example, from row-oriented storage to column-orinted) it will brake in many places (at least we have types, which will make this refactoring easier).\\nCursor at the moment can be recieved from \"root\" `DataFrame`, which we think of as source of all derivative `PivotFrame`s and `FragmentFrame`s. Cursors are cached for referential transparency, so if they got passed down to React components, they will not trigger rerender.\\n**Question** raised in [one of PR](https:\/\/github.com\/contiamo\/operational-visualizations\/pull\/70\/files#diff-2044c7c7ba6c3fbf04dd49cf3cfa68b9R38): Shall we provide `getCursor` functionality in `FragmentFrame`? From one side it may make code simpler, on the other side we need to make sure that `FragmentFrame`s are referentially transperent (which is not the case, for example, for `cell` method) and it will create a lot of copies of the same cursors. We can as well pass reference to \"root\" `DataFrame` along all derivative structures and \"proxy\" `getCursor` method call to it.\\n\n\n##Decision\nAt the moment we decided to use proposed version of cursors and adjust it in the future depending if we like DX or not. We will provide `getCursor` method only for `DataFrame` for now, but we can chnage this in the future.\\n","MatchedIds":[528,498,873,4323,4613],"Predicted":"## Decision\n\nTo maintain the simplicity and flexibility of code while ensuring referential transparency and minimizing redundancy, we will implement the following approach:\n\n1. **Proxy `getCursor` Method in `FragmentFrame`**: We will provide the `getCursor` functionality in `FragmentFrame`, but instead of creating multiple copies of the same cursors, we will proxy the `getCursor` method call to the root `DataFrame`. This approach allows `FragmentFrame` to leverage the existing cursor cache in the root `DataFrame`, preventing unnecessary duplication and ensuring referential transparency.\n\n2. **Maintain Referential Transparency**: By proxying the `getCursor` calls, `FragmentFrame` can remain referentially transparent. Any changes to the cursor implementation or caching strategy will be centralized in the root `DataFrame`, keeping derivative frames lightweight and consistent.\n\n3. **Example Implementation**:\n   ```tsx\n   class FragmentFrame extends IterableFrame {\n     constructor(public rootDataFrame: DataFrame) {\n       super();\n     }\n\n     getCursor<Name extends string, ValueInRawRow = any>(columnName: Name): ColumnCursor<Name, ValueInRawRow> {\n       return this.rootDataFrame.getCursor(columnName);\n     }\n   }\n   ```\n\n### Advantages:\n- **Single Source of Truth**: This approach centralizes cursor management in the root `DataFrame`, making it easier to maintain and update.\n- **Performance**: Referential transparency and caching are preserved, ensuring that no unnecessary re-renders are triggered in React components.\n- **Flexibility**: Future changes in cursor implementation will be easier to manage as they are confined to the root `DataFrame`.\n\nBy following this strategy, we can ensure that `FragmentFrame` remains efficient and maintainable while providing the needed cursor functionality.","GenTime":"2024-08-20 00:28:18"}
{"File Name":"molgenis-r-armadillo\/0002-manage-data-in-armadillo-suite.md","Context":"## Context\\nIn this project we manage the data in the Armadillo suite.\\n","Decision":"We will use the MolgenisArmadillo client to manage data in the Armadillo suite. This means managing folders and files in the data backend based on R-data.\\n","tokens":18,"id":545,"text":"## Context\\nIn this project we manage the data in the Armadillo suite.\\n\n\n##Decision\nWe will use the MolgenisArmadillo client to manage data in the Armadillo suite. This means managing folders and files in the data backend based on R-data.\\n","MatchedIds":[1326,1792,2488,4361,3982],"Predicted":"## Decision\nWe will implement a modular data management system within the Armadillo suite to effectively handle the variety of data operations required. This system will be designed with the following considerations:\n\n1. **Data Ingestion**: Create an API endpoint to support the ingestion of various data formats such as CSV, JSON, and RData.\n2. **Data Storage**: Use a robust, scalable database, such as PostgreSQL, for storing ingested data. This choice provides both relational capabilities and excellent support for analytics.\n3. **Data Processing**: Integrate a processing layer using Apache Spark. Spark's distributed computing capabilities are well-suited for executing large-scale data transformations and computations.\n4. **Data Access**: Implement a RESTful API to allow users to query and retrieve data. Access will be controlled through a role-based access control (RBAC) system to ensure data security.\n5. **Data Export**: Provide an endpoint to export data in multiple formats such as CSV, JSON, and RData, to accommodate different user needs.\n6. **Metadata Management**: Incorporate a metadata management system to track data provenance, schema, and other relevant annotations.\n\nBy adopting this modular approach, we ensure that the Armadillo suite remains flexible, scalable, and secure, while also providing a robust foundation for future enhancements.","GenTime":"2024-08-20 00:28:25"}
{"File Name":"google-cloud-cpp\/2019-03-26-binary-distributions.md","Context":"**Context**: Compiling google-cloud-cpp from source is not always possible nor\\ndesired by all users. Some users may want to install a binary package that\\ncontains our public headers and pre-compiled libraries so that they can link\\nagainst our library. There is no single standard binary package manager in C++,\\ninstead there are a variety of others that users might want to use (e.g., dpkg,\\nrpm, vcpkg).\\n**Decision**: We will not directly support nor provide any binary packages. We\\nwill not test binary distributions of our code. And we will not host configs for\\nbinary distributions, since that would involve hosting files which we do not\\ntest. However, we will do our best to make our code easily packageable by others\\nin a wide variety of formats. For more context, see\\n[#333](https:\/\/github.com\/googleapis\/google-cloud-cpp\/issues\/333).\\n**Consequences**: This decision will shield from endorsing any particular binary\\npackage management system. Other individuals who want to build and maintain a\\nbinary distribution of our code may do so without our permission or knowledge.\\nIf they need small and reasonable tweaks from us to support their binary\\ndistributions, they may file an issue and we'll do our best to accommodate them\\nwithin reason.\\n","Decision":"will not test binary distributions of our code. And we will not host configs for\\nbinary distributions, since that would involve hosting files which we do not\\ntest. However, we will do our best to make our code easily packageable by others\\nin a wide variety of formats. For more context, see\\n[#333](https:\/\/github.com\/googleapis\/google-cloud-cpp\/issues\/333).\\n**Consequences**: This decision will shield from endorsing any particular binary\\npackage management system. Other individuals who want to build and maintain a\\nbinary distribution of our code may do so without our permission or knowledge.\\nIf they need small and reasonable tweaks from us to support their binary\\ndistributions, they may file an issue and we'll do our best to accommodate them\\nwithin reason.\\n","tokens":272,"id":2462,"text":"**Context**: Compiling google-cloud-cpp from source is not always possible nor\\ndesired by all users. Some users may want to install a binary package that\\ncontains our public headers and pre-compiled libraries so that they can link\\nagainst our library. There is no single standard binary package manager in C++,\\ninstead there are a variety of others that users might want to use (e.g., dpkg,\\nrpm, vcpkg).\\n**Decision**: We will not directly support nor provide any binary packages. We\\nwill not test binary distributions of our code. And we will not host configs for\\nbinary distributions, since that would involve hosting files which we do not\\ntest. However, we will do our best to make our code easily packageable by others\\nin a wide variety of formats. For more context, see\\n[#333](https:\/\/github.com\/googleapis\/google-cloud-cpp\/issues\/333).\\n**Consequences**: This decision will shield from endorsing any particular binary\\npackage management system. Other individuals who want to build and maintain a\\nbinary distribution of our code may do so without our permission or knowledge.\\nIf they need small and reasonable tweaks from us to support their binary\\ndistributions, they may file an issue and we'll do our best to accommodate them\\nwithin reason.\\n\n\n##Decision\nwill not test binary distributions of our code. And we will not host configs for\\nbinary distributions, since that would involve hosting files which we do not\\ntest. However, we will do our best to make our code easily packageable by others\\nin a wide variety of formats. For more context, see\\n[#333](https:\/\/github.com\/googleapis\/google-cloud-cpp\/issues\/333).\\n**Consequences**: This decision will shield from endorsing any particular binary\\npackage management system. Other individuals who want to build and maintain a\\nbinary distribution of our code may do so without our permission or knowledge.\\nIf they need small and reasonable tweaks from us to support their binary\\ndistributions, they may file an issue and we'll do our best to accommodate them\\nwithin reason.\\n","MatchedIds":[5061,2467,3873,1631,1117],"Predicted":"## Decision\nWe will not directly support nor provide any binary packages. We will not test binary distributions of our code. And we will not host configs for binary distributions, since that would involve hosting files which we do not test. However, we will do our best to make our code easily packageable by others in a wide variety of formats. For more context, see [#333](https:\/\/github.com\/googleapis\/google-cloud-cpp\/issues\/333).\n\n## Consequences\nThis decision will shield us from endorsing any particular binary package management system. Other individuals who want to build and maintain a binary distribution of our code may do so without our permission or knowledge. If they need small and reasonable tweaks from us to support their binary distributions, they may file an issue and we'll do our best to accommodate them within reason.","GenTime":"2024-08-20 00:28:30"}
{"File Name":"casa\/0004-use-bootstrap.md","Context":"## Context\\nWe would like to have an easy-to-use system for consistent styles that doesn't\\ntake much tinkering. We propose using the `bootstrap` gem.\\n","Decision":"Pending\\n","tokens":36,"id":425,"text":"## Context\\nWe would like to have an easy-to-use system for consistent styles that doesn't\\ntake much tinkering. We propose using the `bootstrap` gem.\\n\n\n##Decision\nPending\\n","MatchedIds":[783,400,3829,2386,1992],"Predicted":"## Decision\nWe will use the `bootstrap` gem to achieve a consistent and easy-to-use styling system. This decision is based on the following factors:\n\n- **Ease of Use**: Bootstrap provides a comprehensive set of pre-designed components and styles that can be easily customized with minimal effort.\n- **Consistency**: By using Bootstrap, we can ensure that our styles are consistent across different pages and components.\n- **Community and Documentation**: Bootstrap is a widely used framework with extensive documentation and a large community, making it easier to find solutions and best practices.\n- **Responsiveness**: Bootstrap's built-in grid system and responsive classes make it easy to create mobile-first, responsive designs.\n\n### Implementation\n- We will include the `bootstrap` gem in our Gemfile and run `bundle install`.\n- We will import Bootstrap styles and JavaScript components into our asset pipeline.\n- Existing styles and components will be gradually transitioned to use Bootstrap classes and components to ensure a smooth migration.\n\n### Positive Consequences\n- Rapid development: Developers can focus more on functionality due to pre-existing styles and components.\n- Enhanced maintainability: Consistent use of Bootstrap reduces the complexity of custom styles.\n- Scalability: As our application grows, the use of a popular framework like Bootstrap ensures that we can easily extend and maintain our styles.\n\n### Negative Consequences\n- Limited Customization: Depending on Bootstrap's predefined styles might limit the uniqueness of our design.\n- Potential overhead: Including an entire framework might introduce unnecessary CSS and JavaScript if not used judiciously.\n\nOverall, the benefits of using the `bootstrap` gem far outweigh the drawbacks, allowing us to build a more consistent and maintainable design system efficiently.","GenTime":"2024-08-20 00:28:35"}
{"File Name":"verify-onboarding-prototypes\/0002-how-do-we-secure-the-api.md","Context":"## Context\\nWe need to secure the interaction between the \"client\" code (e.g. node JS)\\nand the server side code (which will be a dropwizard app).\\nDepending on how the users want to run the service provider we may need\\ndifferent security solutions.\\n","Decision":"If possible users can talk to the service provider on the loopback (127.0.0.1)\\nIf that doesn't work for some reason then they can use the dropwizard config\\nto set up basic auth or tls or something.\\nSee http:\/\/www.dropwizard.io\/1.1.0\/docs\/manual\/configuration.html#connectors\\n","tokens":59,"id":578,"text":"## Context\\nWe need to secure the interaction between the \"client\" code (e.g. node JS)\\nand the server side code (which will be a dropwizard app).\\nDepending on how the users want to run the service provider we may need\\ndifferent security solutions.\\n\n\n##Decision\nIf possible users can talk to the service provider on the loopback (127.0.0.1)\\nIf that doesn't work for some reason then they can use the dropwizard config\\nto set up basic auth or tls or something.\\nSee http:\/\/www.dropwizard.io\/1.1.0\/docs\/manual\/configuration.html#connectors\\n","MatchedIds":[577,4443,2397,3470,527],"Predicted":"## Decision\nWe will implement HTTPS using SSL\/TLS to secure the communication channel between the client (e.g., Node.js) and the server-side Dropwizard application. This ensures encrypted data transmission to protect against man-in-the-middle attacks.\n\nFor a more flexible security solution, we will support both OAuth 2.0 and API key-based authentication. OAuth 2.0 will be recommended for users requiring robust, token-based security, and API keys will cater to those needing a simpler, more straightforward authentication method.\n\nAdditionally, both client and server will validate all incoming requests to guard against common web vulnerabilities such as SQL injection and cross-site scripting (XSS). Security best practices, such as regular security audits and dependency updates, will be followed to maintain a secure environment.","GenTime":"2024-08-20 00:28:39"}
{"File Name":"ansible-docker\/choose_database.md","Context":"### Context: A method of data storage is necessary to fufill the requirements for the assignment.\\n* Having used mySQL in previous courses and personal projects, I found mySQL a suitable choice for supporting database capabilities.\\n* Many of the examples provided by my instructor are using mySQL, thus for the benefit of staying consistent with course material mySQL is a good choice.\\n### Decision: The change proposed to the current implementation is to add mySQL as a method for data storage.\\n### Consequences: MySQL is popular, however there may be a more modern tool with additional features that could be used instead. Thus, there is a risk of limitied functionality.\\n","Decision":"### Consequences: MySQL is popular, however there may be a more modern tool with additional features that could be used instead. Thus, there is a risk of limitied functionality.\\n### Consequences: Acid Transactions can be considered \"pessimistic\" and not as flexible as base transactions.\\n","tokens":138,"id":4832,"text":"### Context: A method of data storage is necessary to fufill the requirements for the assignment.\\n* Having used mySQL in previous courses and personal projects, I found mySQL a suitable choice for supporting database capabilities.\\n* Many of the examples provided by my instructor are using mySQL, thus for the benefit of staying consistent with course material mySQL is a good choice.\\n### Decision: The change proposed to the current implementation is to add mySQL as a method for data storage.\\n### Consequences: MySQL is popular, however there may be a more modern tool with additional features that could be used instead. Thus, there is a risk of limitied functionality.\\n\n\n##Decision\n### Consequences: MySQL is popular, however there may be a more modern tool with additional features that could be used instead. Thus, there is a risk of limitied functionality.\\n### Consequences: Acid Transactions can be considered \"pessimistic\" and not as flexible as base transactions.\\n","MatchedIds":[1661,1167,1740,3071,4747],"Predicted":"## Decision\nChosen Option: \"MySQL\", because\n- It aligns with previous experience and familiarity, making implementation quicker and easier.\n- It maintains consistency with course material and instructor examples, providing a unified learning experience.\n\n### Consequences\n- There is a potential risk that more modern databases with enhanced features might offer better performance or functionality.\n- However, the decision to use MySQL reduces the learning curve and potential integration issues, allowing for a more seamless development process given the educational context.","GenTime":"2024-08-20 00:28:42"}
{"File Name":"PIMS\/geocoder.md","Context":"## Context\\nPresently when submitting new properties or editing existing properties within inventory the only way to set the latitude and longitude values is manually.\\nIdeally the inventory would use GIS location values that are pulled from Data BC (better source of truth).\\nProviding a way through the property address to pull valid GIS coordinates from Data BC Geocoder would improve the data and the user experience.\\nAdditionally Geocoder can be used to verify addresses that are manually entered.\\n- [Geocoder](https:\/\/www2.gov.bc.ca\/gov\/content\/data\/geographic-data-services\/location-services\/geocoder)\\n- [Data BC](https:\/\/catalogue.data.gov.bc.ca\/dataset\/bc-address-geocoder-web-service)\\n- [API Swagger](https:\/\/catalogue.data.gov.bc.ca\/dataset\/bc-address-geocoder-web-service\/resource\/40d6411e-ab98-4df9-a24e-67f81c45f6fa\/view\/1d3c42fc-53dc-4aab-ae3b-f4d056cb00e0)\\n- [Developer API Keys](https:\/\/github.com\/bcgov\/gwa\/wiki\/Developer-Guide#developer-api-keys)\\n- API Host = `https:\/\/geocoder.api.gov.bc.ca`\\n","Decision":"Integrate with Data BC Geocoder API.\\nWhen a user types an address a list of viable matches will be displayed.\\nIf the user selects one of the matches it will be used to set the address and GIS coordinates,.\\n","tokens":262,"id":3968,"text":"## Context\\nPresently when submitting new properties or editing existing properties within inventory the only way to set the latitude and longitude values is manually.\\nIdeally the inventory would use GIS location values that are pulled from Data BC (better source of truth).\\nProviding a way through the property address to pull valid GIS coordinates from Data BC Geocoder would improve the data and the user experience.\\nAdditionally Geocoder can be used to verify addresses that are manually entered.\\n- [Geocoder](https:\/\/www2.gov.bc.ca\/gov\/content\/data\/geographic-data-services\/location-services\/geocoder)\\n- [Data BC](https:\/\/catalogue.data.gov.bc.ca\/dataset\/bc-address-geocoder-web-service)\\n- [API Swagger](https:\/\/catalogue.data.gov.bc.ca\/dataset\/bc-address-geocoder-web-service\/resource\/40d6411e-ab98-4df9-a24e-67f81c45f6fa\/view\/1d3c42fc-53dc-4aab-ae3b-f4d056cb00e0)\\n- [Developer API Keys](https:\/\/github.com\/bcgov\/gwa\/wiki\/Developer-Guide#developer-api-keys)\\n- API Host = `https:\/\/geocoder.api.gov.bc.ca`\\n\n\n##Decision\nIntegrate with Data BC Geocoder API.\\nWhen a user types an address a list of viable matches will be displayed.\\nIf the user selects one of the matches it will be used to set the address and GIS coordinates,.\\n","MatchedIds":[4276,2204,1122,4281,2578],"Predicted":"## Decision\nThe decision is to integrate the Data BC Geocoder API within the inventory system to automatically pull valid GIS coordinates based on property address input. \n\nThis integration will provide the following benefits:\n1. **Accurate GIS Data**: By using the Data BC Geocoder, the inventory system ensures that the latitude and longitude values are accurate and authoritative.\n2. **Improved User Experience**: Users no longer need to manually enter coordinates, reducing manual errors and streamlining the data entry process.\n3. **Address Verification**: Utilizing the Geocoder API to verify addresses during data entry ensures that entered addresses are valid and standardized.\n\nWe'll implement this by:\n- Using the Data BC Geocoder API's `\/addresses` endpoint to retrieve GIS coordinates from property addresses.\n- Updating the property submission and editing workflows to include a step that queries the Geocoder API.\n- Displaying suggested addresses and coordinates to users for selection and confirmation.\n- Storing the fetched GIS coordinates in the inventory database along with the property records.\n\nWe will also handle cases where the API fails to return results by allowing users to manually enter coordinates if necessary. This ensures continuity of service even if the external system experiences downtimes.","GenTime":"2024-08-20 00:28:49"}
{"File Name":"talktofrank-beta\/0003-site-wide-cms-configuration.md","Context":"## Context\\nCertain elements on the Talk To Frank site need to be loaded regardless of the user entry point,\\ne.g. the current user story involves adding a warning message bar that appears on all pages.\\nThe CMS needs to be updated to support the addition of site wide 'settings'.\\n","Decision":"A new 'site settings' content model will be created.  A 'Sitewide' item will then be created to contain all sitewide settings.\\nThis would then allow for the following (future) setup,\\nContent items of type 'site settings'\\n- Sitewide\\n- Homepage - any settings that should override the sitewide values on the homepage.\\n- Drugs A-Z - any settings that should override the sitewide values on the Drugs A-Z page.\\nReferences:\\n- https:\/\/www.contentful.com\/r\/knowledgebase\/dynamic-microcopy\/\\n- https:\/\/www.contentfulcommunity.com\/t\/is-it-common-to-have-a-settings-content-model\/924\/2\\n","tokens":59,"id":1894,"text":"## Context\\nCertain elements on the Talk To Frank site need to be loaded regardless of the user entry point,\\ne.g. the current user story involves adding a warning message bar that appears on all pages.\\nThe CMS needs to be updated to support the addition of site wide 'settings'.\\n\n\n##Decision\nA new 'site settings' content model will be created.  A 'Sitewide' item will then be created to contain all sitewide settings.\\nThis would then allow for the following (future) setup,\\nContent items of type 'site settings'\\n- Sitewide\\n- Homepage - any settings that should override the sitewide values on the homepage.\\n- Drugs A-Z - any settings that should override the sitewide values on the Drugs A-Z page.\\nReferences:\\n- https:\/\/www.contentful.com\/r\/knowledgebase\/dynamic-microcopy\/\\n- https:\/\/www.contentfulcommunity.com\/t\/is-it-common-to-have-a-settings-content-model\/924\/2\\n","MatchedIds":[3037,783,2207,1205,3470],"Predicted":"## Decision\nWe will introduce a global settings module within the CMS to manage site-wide elements. This module will allow for the configuration of elements that need to be loaded regardless of user entry points, such as a warning message bar.\n\n### Implementations Steps:\n1. **Global Settings Module**:\n   - Create a new section in the CMS dedicated to global settings.\n   - Allow for the addition and management of site-wide components such as banners, notification bars, and other persistent elements.\n\n2. **Warning Message Bar**:\n   - Implement a feature within the global settings to configure the warning message bar, including options for text, links, and display conditions.\n   - Ensure this bar can be easily activated or deactivated from the CMS.\n\n3. **Template Integration**:\n   - Update the site\u2019s templates to include the logic for loading the global settings.\n   - Ensure that the warning message bar and other global elements are rendered on all pages.\n\n4. **Testing and Validation**:\n   - Conduct thorough testing to ensure that the global settings module works as expected across different user entry points.\n   - Validate that the warning message bar and other global elements are displayed correctly on all pages.\n\nThis approach ensures that site-wide elements are managed efficiently and can be easily updated by non-technical staff through the CMS.","GenTime":"2024-08-20 00:28:56"}
{"File Name":"james-project\/0017-file-mail-queue-deprecation.md","Context":"## Context\\nJames offers several implementation for MailQueue, a component allowing asynchronous mail processing upon smtp mail\\nreception. These includes:\\n- Default embedded ActiveMQ mail queue implementation, leveraging the JMS APIs and using the filesystem.\\n- RabbitMQMailQueue allowing several James instances to share their MailQueue content.\\n- And FileMailQueue directly leveraging the file system.\\nWe introduced a junit5 test contract regarding management features, concurrency issues, and FileMailQueue do not meet this\\ncontract. This results in some tests being disabled and in an unstable test suite.\\nFileMailQueue tries to implement a message queue within James code, which does not really makes sense as some other projects\\nalready provides one.\\n","Decision":"Deprecate FileMailQueue components.\\nDisable FileMailQueue tests.\\nTarget a removal as part of 3.6.0.\\n","tokens":146,"id":2888,"text":"## Context\\nJames offers several implementation for MailQueue, a component allowing asynchronous mail processing upon smtp mail\\nreception. These includes:\\n- Default embedded ActiveMQ mail queue implementation, leveraging the JMS APIs and using the filesystem.\\n- RabbitMQMailQueue allowing several James instances to share their MailQueue content.\\n- And FileMailQueue directly leveraging the file system.\\nWe introduced a junit5 test contract regarding management features, concurrency issues, and FileMailQueue do not meet this\\ncontract. This results in some tests being disabled and in an unstable test suite.\\nFileMailQueue tries to implement a message queue within James code, which does not really makes sense as some other projects\\nalready provides one.\\n\n\n##Decision\nDeprecate FileMailQueue components.\\nDisable FileMailQueue tests.\\nTarget a removal as part of 3.6.0.\\n","MatchedIds":[2892,2343,2880,2896,2145],"Predicted":"## Decision\nDeprecate and eventually remove the FileMailQueue implementation from the James project. Instead, encourage the use of the existing and well-supported MailQueue implementations such as the embedded ActiveMQ or RabbitMQMailQueue. These solutions are more robust, well-tested, and better aligned with modern message queuing needs. Additionally, updating the test suite to fully support and verify these implementations will ensure stability and consistency across the project.","GenTime":"2024-08-20 00:28:58"}
{"File Name":"docspell\/0007_convert_html_files.md","Context":"# Context and Problem Statement\\nHow can HTML documents be converted into a PDF file that looks as much\\nas possible like the original?\\nIt would be nice to have a java-only solution. But if an external tool\\nhas a better outcome, then an external tool is fine, too.\\nSince Docspell is free software, the tools must also be free.\\n# Considered Options\\n* [pandoc](https:\/\/pandoc.org\/) external command\\n* [wkhtmltopdf](https:\/\/wkhtmltopdf.org\/) external command\\n* [Unoconv](https:\/\/github.com\/unoconv\/unoconv) external command\\nNative (firefox) view:\\n{{ figure(file=\"example-html-native.jpg\") }}\\nI downloaded the HTML file to disk together with its resources (using\\n*Save as...* in the browser).\\n","Decision":"wkhtmltopdf.\\nIt shows the best results.\\n","tokens":176,"id":287,"text":"# Context and Problem Statement\\nHow can HTML documents be converted into a PDF file that looks as much\\nas possible like the original?\\nIt would be nice to have a java-only solution. But if an external tool\\nhas a better outcome, then an external tool is fine, too.\\nSince Docspell is free software, the tools must also be free.\\n# Considered Options\\n* [pandoc](https:\/\/pandoc.org\/) external command\\n* [wkhtmltopdf](https:\/\/wkhtmltopdf.org\/) external command\\n* [Unoconv](https:\/\/github.com\/unoconv\/unoconv) external command\\nNative (firefox) view:\\n{{ figure(file=\"example-html-native.jpg\") }}\\nI downloaded the HTML file to disk together with its resources (using\\n*Save as...* in the browser).\\n\n\n##Decision\nwkhtmltopdf.\\nIt shows the best results.\\n","MatchedIds":[284,273,282,5059,5172],"Predicted":"## Decision\nwkhtmltopdf.\n\nAfter evaluating the considered options, `wkhtmltopdf` provides the best results for converting HTML documents into PDF files that closely resemble their original appearance in the browser. Here's a detailed comparison of the options:\n\n1. **Pandoc**: Though Pandoc is a powerful tool for document conversion and has good support for converting HTML to PDF, the visual fidelity of the PDF does not always match the original HTML document. Tables, images, and some CSS styling may not be rendered accurately, leading to discrepancies.\n\n2. **Unoconv**: While Unoconv utilizes LibreOffice for conversions and supports HTML, its primary strength lies in converting office documents (e.g., DOCX, ODT). The conversion of complex HTML with extensive CSS, JavaScript, or modern web components might not be as reliable.\n\n3. **wkhtmltopdf**: This tool uses the Webkit rendering engine to process HTML and CSS, which closely mimics how modern browsers render web pages. As a result, the converted PDFs maintain high visual fidelity to the original HTML documents, making it the best option among the three. It supports various advanced HTML and CSS features and is highly configurable.\n\nAdditionally, `wkhtmltopdf` is a widely used open-source command-line utility, which guarantees that it aligns with the free software requirement of Docspell.\n\nFor these reasons, `wkhtmltopdf` is chosen as the tool for converting HTML documents to PDF.","GenTime":"2024-08-20 00:29:03"}
{"File Name":"gsp\/ADR022-logging.md","Context":"## Context\\nWe have various log sources:\\n* The EKS control plane (audit logs, api service, scheduler, control-manager)\\n* VPC Flow logs\\n* Load Balancer\\n* Pod\/Application logs\\n* CloudHSM\\nAll of these with the exception of the Pod\/Application logs are stored by AWS in [CloudWatch](https:\/\/aws.amazon.com\/cloudwatch\/).\\nWe would like a single storage location for indexing and search our logs for auditing and debugging purposes.\\nGDS currently have several common storage locations for logs:\\n* Logit.io (a SaaS ELK stack provider)\\n* Self hosted ELK stacks\\n* CloudWatch\\n* Splunk\\nOptions:\\n### Option 1:\\nWe could ship the Cloudwatch logs to logit.io using AWS lambda and ship the Pod\/Application logs to Logit.io using something like [fluentd](https:\/\/www.fluentd.org\/). This would assume that all users of the platform have a Logit.io instance and would end up duplicating a large number of the logs in both CloudWatch and Logit.io\\n### Option 2:\\nWe could host a dedicate ELK stack (either in cluster or from AWS's managed offering) and ingest logs from both Pods and CloudWatch into the ELK stack. Managing ELK stacks has been a maintenance burden at GDS previously and this would require duplicating logs already stored in CloudWatch.\\n### Option 3:\\nWe could ship the Pod\/Application logs to CloudWatch using [fluentd](https:\/\/www.fluentd.org\/) and expose CloudWatch insights interface to users of the platform\\n### Option 4:\\nWe could ship the CloudWatch logs to Splunk using AWS lambda and ship the Pod\/Application logs to Splunk using something like [fluentd](https:\/\/www.fluentd.org\/). This would assume that all users of the platform have a Splunk instance and would end up duplicating a large number of the logs in both CloudWatch and Splunk.\\n","Decision":"We will use [fluentd](https:\/\/www.fluentd.org\/) to ship pod\/application logs to [AWS CloudWatch](https:\/\/aws.amazon.com\/cloudwatch\/) to aggregate all platform\/application logs to avoid double spending on log storage.\\n","tokens":416,"id":3914,"text":"## Context\\nWe have various log sources:\\n* The EKS control plane (audit logs, api service, scheduler, control-manager)\\n* VPC Flow logs\\n* Load Balancer\\n* Pod\/Application logs\\n* CloudHSM\\nAll of these with the exception of the Pod\/Application logs are stored by AWS in [CloudWatch](https:\/\/aws.amazon.com\/cloudwatch\/).\\nWe would like a single storage location for indexing and search our logs for auditing and debugging purposes.\\nGDS currently have several common storage locations for logs:\\n* Logit.io (a SaaS ELK stack provider)\\n* Self hosted ELK stacks\\n* CloudWatch\\n* Splunk\\nOptions:\\n### Option 1:\\nWe could ship the Cloudwatch logs to logit.io using AWS lambda and ship the Pod\/Application logs to Logit.io using something like [fluentd](https:\/\/www.fluentd.org\/). This would assume that all users of the platform have a Logit.io instance and would end up duplicating a large number of the logs in both CloudWatch and Logit.io\\n### Option 2:\\nWe could host a dedicate ELK stack (either in cluster or from AWS's managed offering) and ingest logs from both Pods and CloudWatch into the ELK stack. Managing ELK stacks has been a maintenance burden at GDS previously and this would require duplicating logs already stored in CloudWatch.\\n### Option 3:\\nWe could ship the Pod\/Application logs to CloudWatch using [fluentd](https:\/\/www.fluentd.org\/) and expose CloudWatch insights interface to users of the platform\\n### Option 4:\\nWe could ship the CloudWatch logs to Splunk using AWS lambda and ship the Pod\/Application logs to Splunk using something like [fluentd](https:\/\/www.fluentd.org\/). This would assume that all users of the platform have a Splunk instance and would end up duplicating a large number of the logs in both CloudWatch and Splunk.\\n\n\n##Decision\nWe will use [fluentd](https:\/\/www.fluentd.org\/) to ship pod\/application logs to [AWS CloudWatch](https:\/\/aws.amazon.com\/cloudwatch\/) to aggregate all platform\/application logs to avoid double spending on log storage.\\n","MatchedIds":[3290,3917,4041,1376,3909],"Predicted":"## Decision\nWe will ship the Pod\/Application logs to CloudWatch using [fluentd](https:\/\/www.fluentd.org\/) and expose the CloudWatch Insights interface to users of the platform.\n\n### Rationale\n- **Unified Logging Platform**: By consolidating all logs in CloudWatch, we avoid the complexity and overhead of managing multiple logging systems.\n- **Minimal Duplication**: This approach minimizes log duplication, as CloudWatch will serve as the single source of truth for all logs.\n- **Ease of Use**: CloudWatch Insights provides a powerful interface for indexing, searching, and analyzing logs, making it accessible to all users without requiring additional tools or instances.\n- **Cost Efficiency**: Avoiding the setup and maintenance of additional ELK stacks (self-hosted or SaaS) reduces operational costs and administrative overhead.\n- **Integration**: Fluentd is a well-supported, flexible tool for shipping logs, making it a reliable choice for integrating Pod\/Application logs with CloudWatch.\n\nBy leveraging CloudWatch for a unified logging solution, we streamline our infrastructure and deliver a robust, centralized platform for auditing and debugging purposes.","GenTime":"2024-08-20 00:29:08"}
{"File Name":"connecting-to-services\/0015-add-info-page.md","Context":"## Context\\nWhen debugging it is useful to be sure of what the deployed version of the application is. In addition, when debugging date and time issues (e.g openness) then it is useful to know the time in UTC from the point of view of the application.\\n","Decision":"The application will provide an info page.\\n","tokens":55,"id":2198,"text":"## Context\\nWhen debugging it is useful to be sure of what the deployed version of the application is. In addition, when debugging date and time issues (e.g openness) then it is useful to know the time in UTC from the point of view of the application.\\n\n\n##Decision\nThe application will provide an info page.\\n","MatchedIds":[1173,4852,1416,1048,1462],"Predicted":"## Decision\nWe will inject the deployed version and current timestamp in UTC into the application's logging context. The deployed version will be derived from environment variables set during the deployment process, while the current UTC time will be obtained from the system clock.\n\n* The deployed version will be included in all logs to easily identify which version is running.\n* The current UTC timestamp will be logged with each log entry to aid debugging of date and time issues.\n* These additions will be implemented in a centralized logging middleware or logger configuration to ensure consistency across the application.","GenTime":"2024-08-20 00:29:13"}
{"File Name":"automate\/adr-2019-01-23.md","Context":"## Context\\nPreviously, we did not have any rules for governing Automate UI route\\nlifecycle. In the past, we likely broke backwards compatibility when\\nre-naming or moving routes.\\nAdditionally, the top nav highlighting in Automate UI was driven off the\\nroot route (e.g. everything under the `Settings` tab must live under the `\/settings`\\nroute or it will not be highlighted properly for pages that live under it).\\nThis means that if you want to move a page to a new tab, it will require a new route.\\n","Decision":"Every route that originally linked to a page that is still in use in the webapp\\nmust still work and backwards compatibility must be maintained. Only when the webpage\\nthat the original route linked to is being removed from the app can a route be removed.\\nThis is so that deep linking from our own sources (blog posts, etc.) as well as links\\ncustomers use internally will continue to work until they no longer make sense at all\\nand the original resource no longer exists.\\nTo maintain backwards compatibility, the original route must redirect to the new route\\n(achievable in `app-routing.module.ts`) and an e2e test must be added to verify\\nthat redirection in `deprecated-routes.e2e-spec.ts`. See\\n[this commit](https:\/\/github.com\/chef\/automate\/commit\/38779c5aabb1af8d6c22e4b6e1b07eaf9c8fa06c)\\nfor an example of routes being renamed with proper backwards compatibility, what the e2e\\ntests should look like, and how a page can move between topnav tabs.\\n","tokens":115,"id":1210,"text":"## Context\\nPreviously, we did not have any rules for governing Automate UI route\\nlifecycle. In the past, we likely broke backwards compatibility when\\nre-naming or moving routes.\\nAdditionally, the top nav highlighting in Automate UI was driven off the\\nroot route (e.g. everything under the `Settings` tab must live under the `\/settings`\\nroute or it will not be highlighted properly for pages that live under it).\\nThis means that if you want to move a page to a new tab, it will require a new route.\\n\n\n##Decision\nEvery route that originally linked to a page that is still in use in the webapp\\nmust still work and backwards compatibility must be maintained. Only when the webpage\\nthat the original route linked to is being removed from the app can a route be removed.\\nThis is so that deep linking from our own sources (blog posts, etc.) as well as links\\ncustomers use internally will continue to work until they no longer make sense at all\\nand the original resource no longer exists.\\nTo maintain backwards compatibility, the original route must redirect to the new route\\n(achievable in `app-routing.module.ts`) and an e2e test must be added to verify\\nthat redirection in `deprecated-routes.e2e-spec.ts`. See\\n[this commit](https:\/\/github.com\/chef\/automate\/commit\/38779c5aabb1af8d6c22e4b6e1b07eaf9c8fa06c)\\nfor an example of routes being renamed with proper backwards compatibility, what the e2e\\ntests should look like, and how a page can move between topnav tabs.\\n","MatchedIds":[3210,1522,2280,2111,1050],"Predicted":"## Decision\nEstablish a set of rules for governing the lifecycle of routes in the Automate UI to ensure backward compatibility and proper navigation highlighting.\n\n### Rules:\n1. **Route Naming Convention**:\n    - Use meaningful and descriptive names for routes. Follow a hierarchical structure to reflect the navigation hierarchy.\n    - Example: Routes under the `Settings` tab should follow `\/settings\/feature` format.\n\n2. **Backward Compatibility**:\n    - Avoid renaming or moving routes unless absolutely necessary.\n    - If a route rename or move is required, implement a redirect from the old route to the new route to preserve backward compatibility.\n\n3. **Top Navigation Highlighting**:\n    - Ensure all pages under a specific tab inherit the root route of that tab for proper navigation highlighting.\n    - Example: All routes under the `Settings` tab should start with `\/settings`.\n\n4. **Route Deprecation Policy**:\n    - Establish a deprecation policy where deprecated routes will continue to work for a specified period (e.g., 6 months) before being removed.\n    - Provide clear communication to users about upcoming deprecations and alternative routes.\n\n5. **Versioning Routes**:\n    - For significant changes in routes, introduce versioned routes (e.g., `\/v1\/settings`, `\/v2\/settings`) to maintain compatibility with older versions while allowing for improvements.\n\n6. **Documentation**:\n    - Maintain clear and up-to-date documentation for all routes, including their purpose, hierarchy, and deprecation status if applicable.\n\n### Implementation Steps:\n1. **Define Root Routes**:\n    - Establish clear root routes for each top-level navigation tab.\n\n2. **Implement Redirects**:\n    - Set up redirects for any renamed or moved routes to ensure backward compatibility.\n\n3. **Update Navigation Logic**:\n    - Refactor the top navigation logic to ensure highlighting is based on the root route structure defined.\n\n4. **Communicate Changes**:\n    - Inform users about the route lifecycle rules, deprecation policies, and any upcoming changes to the routes.\n\nBy implementing these rules, we will ensure a consistent and reliable route lifecycle in the Automate UI, preventing backward compatibility issues and maintaining proper navigation highlighting.","GenTime":"2024-08-20 00:29:20"}
{"File Name":"opg-data-lpa-codes\/0001-flask-in-aws-lambda-function.md","Context":"## Context\\nFor this project, we would like to package up the API endpoints and logic into a small Flask app in a single lambda function.\\nWe found whilst working on the Documents integration that managing multiple lambda functions quickly became quite hard work,\\nespecially as in that project there was a lot of shared code about the place. In hindsight, we should have refactored the\\nshared code into separate lambda functions, but we never got to it due to time constraints. Also this would just give us\\nmore lambda functions to maintain.\\n#### Why Flask (or any other WSGI app) in a Lambda is a good idea\\n* We can re-use most of the infra setup from the last project so we can get started fast\\n* Easy to run and develop locally, just run like a normal Flask app\\n* Easy to test as it's a normal Flask app\\n* I have written a lot of Flask apps(!) and so have a LOT of other people, so documentation is plentiful and well established\\n* Is it an anti-pattern? AWS have a library called Chalice which does a very similar thing to how we propose to use Flask\\n(but with more deployment stuff in that we don't need), so if AWS have an official package that does it, can it be an\\nanti-pattern?\\n* Maintaining a single lambda function is much easier than maintaining many\\n* This project is very small and has very well defined boundaries. None of its data or other artifacts will be accessed\\nby anything other than its own API, meaning it's completely independent of any other service (excluding AWS security) so\\nit's a good candidate for experimenting with this, as nothing will need to be reused outside of this app\\n#### Why Flask (or any other WSGI app) in a Lambda is a bad idea\\n* A whole app in a FaaS seems like an anti-pattern (can't find any evidence to support\/contradict this)\\n* Potentially longer cold-start as it has to initialise the whole app not just a single function\\n* [Flask-Lambda](https:\/\/github.com\/sivel\/flask-lambda) is a small package with only 1 contributor, though it is a pretty\\nsimple script that just maps some request variables depending on the source of the request\\n","Decision":"* Single lambda function containing small Flask app that provides all endpoints\\n* Flask-Lambda to help easily switch between local dev and AWS\\n","tokens":469,"id":1540,"text":"## Context\\nFor this project, we would like to package up the API endpoints and logic into a small Flask app in a single lambda function.\\nWe found whilst working on the Documents integration that managing multiple lambda functions quickly became quite hard work,\\nespecially as in that project there was a lot of shared code about the place. In hindsight, we should have refactored the\\nshared code into separate lambda functions, but we never got to it due to time constraints. Also this would just give us\\nmore lambda functions to maintain.\\n#### Why Flask (or any other WSGI app) in a Lambda is a good idea\\n* We can re-use most of the infra setup from the last project so we can get started fast\\n* Easy to run and develop locally, just run like a normal Flask app\\n* Easy to test as it's a normal Flask app\\n* I have written a lot of Flask apps(!) and so have a LOT of other people, so documentation is plentiful and well established\\n* Is it an anti-pattern? AWS have a library called Chalice which does a very similar thing to how we propose to use Flask\\n(but with more deployment stuff in that we don't need), so if AWS have an official package that does it, can it be an\\nanti-pattern?\\n* Maintaining a single lambda function is much easier than maintaining many\\n* This project is very small and has very well defined boundaries. None of its data or other artifacts will be accessed\\nby anything other than its own API, meaning it's completely independent of any other service (excluding AWS security) so\\nit's a good candidate for experimenting with this, as nothing will need to be reused outside of this app\\n#### Why Flask (or any other WSGI app) in a Lambda is a bad idea\\n* A whole app in a FaaS seems like an anti-pattern (can't find any evidence to support\/contradict this)\\n* Potentially longer cold-start as it has to initialise the whole app not just a single function\\n* [Flask-Lambda](https:\/\/github.com\/sivel\/flask-lambda) is a small package with only 1 contributor, though it is a pretty\\nsimple script that just maps some request variables depending on the source of the request\\n\n\n##Decision\n* Single lambda function containing small Flask app that provides all endpoints\\n* Flask-Lambda to help easily switch between local dev and AWS\\n","MatchedIds":[4642,3470,4937,389,4388],"Predicted":"## Decision Drivers\n* Ease of refactoring and managing shared code\n* Simplification of the infrastructure (e.g., reducing the number of Lambda functions)\n* Ability to develop, run, and test locally\n* Well-documented and familiar technology (Flask)\n* Potential performance trade-offs (cold starts)\n* Size and scope of the project (small with well-defined boundaries)\n\n## Decision\nWe will package up the API endpoints and logic into a small Flask app deployed as a single lambda function. \n\n### Rationale\n* **Re-using Existing Infrastructure**: We can leverage the infrastructure setup from the previous project, which will allow us to start quickly and reduce initial setup time.\n* **Ease of Development**: Developing the application locally as a standard Flask app simplifies the development workflow, making it easier to debug and test changes.\n* **Simplified Maintenance**: Maintaining a single Lambda function will reduce the operational overhead compared to managing multiple Lambda functions, especially in the context of this project's well-defined boundaries.\n* **Expertise and Documentation**: Flask is a well-known and well-documented framework that many developers are already familiar with, which will help streamline development and onboarding processes.\n\n### Potential Drawbacks\n* **Cold Start Times**: Packaging the entire app into a single Lambda function may result in longer cold start times. However, given the small size and scope of the project, this is considered an acceptable trade-off.\n* **Single Point of Failure**: With a single Lambda function, there is a risk that a bug could affect the entire application. This risk is mitigated by extensive testing and the well-defined scope of the project.\n* **Framework Maintenance**: The `Flask-Lambda` package has only one contributor and might not be as actively maintained. However, it is a simple script that should not introduce significant risk or complexity.\n\n### Positive Consequences\n* Quick setup and start due to reusing existing infrastructure.\n* Reduced complexity in maintaining multiple Lambda functions.\n* Easier local development, debugging, and testing.\n* Streamlined development with familiar, well-documented technology.\n\n### Negative Consequences\n* Possible increase in cold start latency.\n* Heavy reliance on a lesser-known package (`Flask-Lambda`) which might require in-house maintenance or customization in the future.\n\nIn summary, packaging the Flask app into a single Lambda function is a pragmatic decision for this project's specific context, balancing ease of development and maintenance with manageable trade-offs in performance and potential risks.","GenTime":"2024-08-20 00:29:29"}
{"File Name":"Marain.Instance\/0005-multitenancy-approach-for-marain.md","Context":"## Context\\nTenancy has always been a first class citizen of all Marain services, however this by itself is not enough to make the system truly multitenanted. In order to do this, we need to determine how tenants should be created, managed and used within the Marain \"world\".\\nWe would like the option of deploying Marain as either a managed service, hosted by us and licenced to users as a PaaS offering, or for clients to deploy private instances into their own cloud subscriptions. We also want to give clients of the managed services the option for data to be stored in their own storage accounts or databases, but still have us run the compute aspects of the platform on their behalf. This also extends to those clients who are using Marain to implement their own multi-tenanted services: these clients should also be able to isolate their own clients' storage.\\nIn addition, we need to be able to differentiate between a Marain service being available for a client to use directly and one being used as a dependency of a service they are using. For example, the Workflow service makes use of the Operations service. As a result, clients that are licenced to use the Workflow service will be using the Operations service indirectly, despite the fact that they may not be licenced to use it directly.\\nWe need to define a tenancy model that will support these scenarios and can be implemented using the `Marain.Tenancy` service.\\n","Decision":"To support this, we have made the following decisions\\n1. Every client using a Marain instance will have a Marain tenant created for them. For the remainder of this document, these will be referred to as \"Client Tenants\".\\n1. Every Marain service will also have a Marain tenant created for it. For the remainder of this document, these will be referred to as \"Service Tenants\".\\n1. We will make use of the tenant hierarchy to group Client Tenants and Service Tenants under their own top-level parent. This means that we will have a top-level tenant called \"Client Tenants\" which parents all of the Client Tenants, and an equivalent one called \"Service Tenants\" that parents the Service Tenants (this is shown in the diagram below).\\n1. Clients will access the Marain services they are licenced for using their own tenant Id. Whilst the Marain services themselves expect this to be supplied as part of endpoint paths, there is nothing to prevent an API Gateway (e.g. Azure API Management) being put in front of this so that custom URLs can be mapped to tenants, or so that tenant IDs can be passed in headers.\\n1. When a Marain service depends on another one as part of an operation, it will pass the Id of a tenant that is a subtenant of it's own Service Tenant. This subtenant will be specific to the client that is making the original call. For example, the Workflow service has a dependency on the Operations Control service. If there are two Client Tenants for the Workflow Service, each will have a corresponding sub-tenant of the Workflow Service Tenant and these will be used to make the call to the Operation service. This approach allows the depended-upon service to be used on behalf of the client without making it available for direct usage.\\nEach of these tenants - Client, Service, and the client-specific sub-tenants of the Service Tenants - will need to hold configuration appropriate for their expected use cases. This will normally be any required storage configuration for the services they use, plus the Ids of any subtenants that have been created for them in those services, but could also include other things.\\nAs an example, suppose we have two customers; Contoso and Litware. For these customers to be able to use Marain, we must create Contoso and Litware tenants. We also have two Marain services available, Workflow and Operations. These also have tenants created for them (in the following diagrams, Service Tenants are shown in ALL CAPS and Client Tenants in normal sentence case. Service-specific client subtenants use a mix to indicate what they relate to):\\n```\\nRoot tenant\\n|\\n+-> Client Tenants\\n|     |\\n|     +-> Contoso\\n|     |\\n|     +-> Litware\\n|\\n+-> Service Tenants\\n|\\n+-> WORKFLOW\\n|\\n+-> OPERATIONS\\n```\\nContoso is licenced to use Workflow, and Litware is licenced to use both Workflow and Operations. This means that:\\n- The Contoso tenant will contain storage configuration for the Workflow service (as with all this configuration, the onboarding process will default this to standard Marain storage, where data is siloed by tenant in shared storage accounts - e.g. a single Cosmos database containing a collection per tenant. However, clients can supply their own storage configuration where required).\\n- The Litware tenant will contain storage configuration for both Workflow and Operations services, because it uses both directly.\\nIn addition, because both clients are licenced for workflow, they will each have a sub-tenant of the Workflow Service Tenant, containing the storage configuration that should be used with the Operations service. The Operations service does not have any sub-tenants because it does not have dependencies on any other Marain services:\\n```\\nRoot tenant\\n|\\n+-> Client Tenants\\n|     |\\n|     +-> Contoso\\n|     |     +-> (Workflow storage configuration)\\n|     |     +-> (The Id of the WORKFLOW+Contoso sub-tenant for the Workflow service to use)\\n|     |\\n|     +-> Litware\\n|           +-> (Workflow storage configuration)\\n|           +-> (The Id of the WORKFLOW+Litware sub-tenant for the Workflow service to use)\\n|           +-> (Operations storage configuration)\\n|\\n+-> Service Tenants\\n|\\n+-> WORKFLOW\\n|     |\\n|     +-> WORKFLOW+Contoso\\n|     |     +-> (Operations storage configuration)\\n|     |\\n|     +-> WORKFLOW+Litware\\n|           +-> (Operations storage configuration)\\n|\\n+-> OPERATIONS\\n```\\nAs can be seen from the above, each tenant holds appropriate configuration for the services they use directly. In the case of the Client Tenants, they also hold the Id of the sub-tenant that the Workflow service will use when calling out to the Operations service on their behalf; this is necessary to avoid a costly search for the correct sub-tenant to use.\\nYou will notice from the above that Litware ends up with two sets of configuration for Operations storage; that which is employed when using the Operations service directly, and that used when calling the Workflow service and thus using the Operations service indirectly. This gives clients the maximum flexibility in controlling where their data is stored.\\nNow let's look at a slightly more complex example. Imagine in the scenario above, there is a third service, which we'll just call the FooBar service, and that both the Workflow and Operations service are dependent on it. In addition, Contoso are licenced to use it directly. This is what the dependency graph now looks like:\\n```\\n+------------+\\n|            |\\n+-------> WORKFLOW   +------+-----------------+\\n+---------+       |       |            |      |                 |\\n|         +-------+       +-^----------+      |                 |\\n| Contoso |                 |                 |                 |\\n|         |                 |                 |                 |\\n+----+----+                 |           +-----v------+          |\\n|                      |           |            |          |\\n|                      |     +-----> OPERATIONS +----+     |\\n|      +---------+     |     |     |            |    |     |\\n|      |         +-----+     |     +------------+    |     |\\n|      | Litware |           |                       |     |\\n|      |         +-----------+                       |     |\\n|      +---------+                               +---v-----v--+\\n|                                                |            |\\n+------------------------------------------------> FOOBAR     |\\n|            |\\n+------------+\\n```\\nIn order to support this, we start with an additional Service Tenant for the FooBar tenant.\\n```\\nRoot tenant\\n|\\n+-> Client Tenants\\n|     |\\n|     +-> Contoso\\n|     |\\n|     +-> Litware\\n|\\n+-> Service Tenants\\n|\\n+-> WORKFLOW\\n|\\n+-> OPERATIONS\\n|\\n+-> FOOBAR\\n```\\nWe then enroll Contoso to use the Workflow service. This causes a chain of enrollments whereby a sub-tenant is created for WORKFLOW+Contoso, which is then enrolled to use the Operations service, creating a sub-tenant of OPERATIONS, OPERATIONS+WORKFLOW+Contoso, which is then enrolled to use the FooBar service (since FooBar does not have dependencies, this does not create any further sub tenants). The Workflow service is also directly dependent on FooBar, so WORKFLOW+Contoso is also enrolled to use FooBar resulting in storage configuration for FooBar being added to it.\\nThis leaves the tenant hierarchy looking like this:\\n```\\nRoot tenant\\n|\\n+-> Client Tenants\\n|     |\\n|     +-> Contoso\\n|     |     +-> (Workflow storage configuration)\\n|     |     +-> (The Id of the WORKFLOW+Contoso sub-tenant for the Workflow service to use)\\n|     |\\n|     +-> Litware\\n|\\n+-> Service Tenants\\n|\\n+-> WORKFLOW\\n|     |\\n|     +-> WORKFLOW+Contoso\\n|           +-> (Operations storage configuration)\\n|           +-> (The Id of the OPERATIONS+WORKFLOW+Contoso sub-tenant for the Operations service to use)\\n|           +-> (FooBar storage configuration)\\n|\\n+-> OPERATIONS\\n|     |\\n|     +-> OPERATIONS+WORKFLOW+Contoso\\n|           +-> (FooBar storage configuration)\\n|\\n+-> FOOBAR\\n```\\nWe then enroll Contoso for the FooBar service. Since there are no additional dependencies, this does not result in any further sub-tenants being created, but does add storage configuration for FooBar to the Contoso tenant. As in the first example, Contoso now has two sets of storage configuration for the FooBar service, one for direct use and one for indirect use.\\n```\\nRoot tenant\\n|\\n+-> Client Tenants\\n|     |\\n|     +-> Contoso\\n|     |     +-> (Workflow storage configuration)\\n|     |     +-> (The Id of the WORKFLOW+Contoso sub-tenant for the Workflow service to use)\\n|     |     +-> (FooBar storage configuration)\\n|     |\\n|     +-> Litware\\n|\\n+-> Service Tenants\\n|\\n+-> WORKFLOW\\n|     |\\n|     +-> WORKFLOW+Contoso\\n|           +-> (Operations storage configuration)\\n|           +-> (The Id of the OPERATIONS+WORKFLOW+Contoso sub-tenant for the Operations service to use)\\n|           +-> (FooBar storage configuration)\\n|\\n+-> OPERATIONS\\n|     |\\n|     +-> OPERATIONS+WORKFLOW+Contoso\\n|           +-> (FooBar storage configuration)\\n|\\n+-> FOOBAR\\n```\\nWe now repeat the process of enrolling Litware for the Workflow service:\\n```\\nRoot tenant\\n|\\n+-> Client Tenants\\n|     |\\n|     +-> Contoso\\n|     |     +-> (Workflow storage configuration)\\n|     |     +-> (The Id of the WORKFLOW+Contoso sub-tenant for the Workflow service to use)\\n|     |     +-> (FooBar storage configuration)\\n|     |\\n|     +-> Litware\\n|           +-> (Workflow storage configuration)\\n|           +-> (The Id of the WORKFLOW+Litware sub-tenant for the Workflow service to use)\\n|\\n+-> Service Tenants\\n|\\n+-> WORKFLOW\\n|     |\\n|     +-> WORKFLOW+Contoso\\n|     |     +-> (Operations storage configuration)\\n|     |     +-> (The Id of the OPERATIONS+WORKFLOW+Contoso sub-tenant for the Operations service to use)\\n|     |     +-> (FooBar storage configuration)\\n|     |\\n|     +-> WORKFLOW+Litware\\n|           +-> (Operations storage configuration)\\n|           +-> (The Id of the OPERATIONS+WORKFLOW+Litware sub-tenant for the Operations service to use)\\n|           +-> (FooBar storage configuration)\\n|\\n+-> OPERATIONS\\n|     |\\n|     +-> OPERATIONS+WORKFLOW+Contoso\\n|     |     +-> (FooBar storage configuration)\\n|     |\\n|     +-> OPERATIONS+WORKFLOW+Litware\\n|           +-> (FooBar storage configuration)\\n|\\n+-> FOOBAR\\n```\\nSince Litware is not licenced to use FooBar, the Litware Client Tenant does not hold any configuration for that service itself.\\nFinally, we enroll Litware to use the Operations service. In this example, because Operations depends on FooBar, we need to create another sub-tenant of Operations to call FooBar with when Litware uses Operations directly, and enroll this new subtenant with FooBar. This leaves us with the following:\\n```\\nRoot tenant\\n|\\n+-> Client Tenants\\n|     |\\n|     +-> Contoso\\n|     |     +-> (Workflow storage configuration)\\n|     |     +-> (The Id of the WORKFLOW+Contoso sub-tenant for the Workflow service to use)\\n|     |     +-> (FooBar storage configuration)\\n|     |\\n|     +-> Litware\\n|           +-> (Workflow storage configuration)\\n|           +-> (The Id of the WORKFLOW+Litware sub-tenant for the Workflow service to use)\\n|           +-> (Operations storage configuration)\\n|           +-> (The Id of the OPERATIONS+Litware sub-tenant for the Operations service to use)\\n|\\n+-> Service Tenants\\n|\\n+-> WORKFLOW\\n|     |\\n|     +-> WORKFLOW+Contoso\\n|     |     +-> (Operations storage configuration)\\n|     |     +-> (The Id of the OPERATIONS+WORKFLOW+Contoso sub-tenant for the Operations service to use)\\n|     |     +-> (FooBar storage configuration)\\n|     |\\n|     +-> WORKFLOW+Litware\\n|           +-> (Operations storage configuration)\\n|           +-> (The Id of the OPERATIONS+WORKFLOW+Litware sub-tenant for the Operations service to use)\\n|           +-> (FooBar storage configuration)\\n|\\n+-> OPERATIONS\\n|     |\\n|     +-> OPERATIONS+WORKFLOW+Contoso\\n|     |     +-> (FooBar storage configuration)\\n|     |\\n|     +-> OPERATIONS+WORKFLOW+Litware\\n|     |     +-> (FooBar storage configuration)\\n|     |\\n|     +-> OPERATIONS+Litware\\n|           +-> (FooBar storage configuration)\\n|\\n+-> FOOBAR\\n```\\n","tokens":297,"id":359,"text":"## Context\\nTenancy has always been a first class citizen of all Marain services, however this by itself is not enough to make the system truly multitenanted. In order to do this, we need to determine how tenants should be created, managed and used within the Marain \"world\".\\nWe would like the option of deploying Marain as either a managed service, hosted by us and licenced to users as a PaaS offering, or for clients to deploy private instances into their own cloud subscriptions. We also want to give clients of the managed services the option for data to be stored in their own storage accounts or databases, but still have us run the compute aspects of the platform on their behalf. This also extends to those clients who are using Marain to implement their own multi-tenanted services: these clients should also be able to isolate their own clients' storage.\\nIn addition, we need to be able to differentiate between a Marain service being available for a client to use directly and one being used as a dependency of a service they are using. For example, the Workflow service makes use of the Operations service. As a result, clients that are licenced to use the Workflow service will be using the Operations service indirectly, despite the fact that they may not be licenced to use it directly.\\nWe need to define a tenancy model that will support these scenarios and can be implemented using the `Marain.Tenancy` service.\\n\n\n##Decision\nTo support this, we have made the following decisions\\n1. Every client using a Marain instance will have a Marain tenant created for them. For the remainder of this document, these will be referred to as \"Client Tenants\".\\n1. Every Marain service will also have a Marain tenant created for it. For the remainder of this document, these will be referred to as \"Service Tenants\".\\n1. We will make use of the tenant hierarchy to group Client Tenants and Service Tenants under their own top-level parent. This means that we will have a top-level tenant called \"Client Tenants\" which parents all of the Client Tenants, and an equivalent one called \"Service Tenants\" that parents the Service Tenants (this is shown in the diagram below).\\n1. Clients will access the Marain services they are licenced for using their own tenant Id. Whilst the Marain services themselves expect this to be supplied as part of endpoint paths, there is nothing to prevent an API Gateway (e.g. Azure API Management) being put in front of this so that custom URLs can be mapped to tenants, or so that tenant IDs can be passed in headers.\\n1. When a Marain service depends on another one as part of an operation, it will pass the Id of a tenant that is a subtenant of it's own Service Tenant. This subtenant will be specific to the client that is making the original call. For example, the Workflow service has a dependency on the Operations Control service. If there are two Client Tenants for the Workflow Service, each will have a corresponding sub-tenant of the Workflow Service Tenant and these will be used to make the call to the Operation service. This approach allows the depended-upon service to be used on behalf of the client without making it available for direct usage.\\nEach of these tenants - Client, Service, and the client-specific sub-tenants of the Service Tenants - will need to hold configuration appropriate for their expected use cases. This will normally be any required storage configuration for the services they use, plus the Ids of any subtenants that have been created for them in those services, but could also include other things.\\nAs an example, suppose we have two customers; Contoso and Litware. For these customers to be able to use Marain, we must create Contoso and Litware tenants. We also have two Marain services available, Workflow and Operations. These also have tenants created for them (in the following diagrams, Service Tenants are shown in ALL CAPS and Client Tenants in normal sentence case. Service-specific client subtenants use a mix to indicate what they relate to):\\n```\\nRoot tenant\\n|\\n+-> Client Tenants\\n|     |\\n|     +-> Contoso\\n|     |\\n|     +-> Litware\\n|\\n+-> Service Tenants\\n|\\n+-> WORKFLOW\\n|\\n+-> OPERATIONS\\n```\\nContoso is licenced to use Workflow, and Litware is licenced to use both Workflow and Operations. This means that:\\n- The Contoso tenant will contain storage configuration for the Workflow service (as with all this configuration, the onboarding process will default this to standard Marain storage, where data is siloed by tenant in shared storage accounts - e.g. a single Cosmos database containing a collection per tenant. However, clients can supply their own storage configuration where required).\\n- The Litware tenant will contain storage configuration for both Workflow and Operations services, because it uses both directly.\\nIn addition, because both clients are licenced for workflow, they will each have a sub-tenant of the Workflow Service Tenant, containing the storage configuration that should be used with the Operations service. The Operations service does not have any sub-tenants because it does not have dependencies on any other Marain services:\\n```\\nRoot tenant\\n|\\n+-> Client Tenants\\n|     |\\n|     +-> Contoso\\n|     |     +-> (Workflow storage configuration)\\n|     |     +-> (The Id of the WORKFLOW+Contoso sub-tenant for the Workflow service to use)\\n|     |\\n|     +-> Litware\\n|           +-> (Workflow storage configuration)\\n|           +-> (The Id of the WORKFLOW+Litware sub-tenant for the Workflow service to use)\\n|           +-> (Operations storage configuration)\\n|\\n+-> Service Tenants\\n|\\n+-> WORKFLOW\\n|     |\\n|     +-> WORKFLOW+Contoso\\n|     |     +-> (Operations storage configuration)\\n|     |\\n|     +-> WORKFLOW+Litware\\n|           +-> (Operations storage configuration)\\n|\\n+-> OPERATIONS\\n```\\nAs can be seen from the above, each tenant holds appropriate configuration for the services they use directly. In the case of the Client Tenants, they also hold the Id of the sub-tenant that the Workflow service will use when calling out to the Operations service on their behalf; this is necessary to avoid a costly search for the correct sub-tenant to use.\\nYou will notice from the above that Litware ends up with two sets of configuration for Operations storage; that which is employed when using the Operations service directly, and that used when calling the Workflow service and thus using the Operations service indirectly. This gives clients the maximum flexibility in controlling where their data is stored.\\nNow let's look at a slightly more complex example. Imagine in the scenario above, there is a third service, which we'll just call the FooBar service, and that both the Workflow and Operations service are dependent on it. In addition, Contoso are licenced to use it directly. This is what the dependency graph now looks like:\\n```\\n+------------+\\n|            |\\n+-------> WORKFLOW   +------+-----------------+\\n+---------+       |       |            |      |                 |\\n|         +-------+       +-^----------+      |                 |\\n| Contoso |                 |                 |                 |\\n|         |                 |                 |                 |\\n+----+----+                 |           +-----v------+          |\\n|                      |           |            |          |\\n|                      |     +-----> OPERATIONS +----+     |\\n|      +---------+     |     |     |            |    |     |\\n|      |         +-----+     |     +------------+    |     |\\n|      | Litware |           |                       |     |\\n|      |         +-----------+                       |     |\\n|      +---------+                               +---v-----v--+\\n|                                                |            |\\n+------------------------------------------------> FOOBAR     |\\n|            |\\n+------------+\\n```\\nIn order to support this, we start with an additional Service Tenant for the FooBar tenant.\\n```\\nRoot tenant\\n|\\n+-> Client Tenants\\n|     |\\n|     +-> Contoso\\n|     |\\n|     +-> Litware\\n|\\n+-> Service Tenants\\n|\\n+-> WORKFLOW\\n|\\n+-> OPERATIONS\\n|\\n+-> FOOBAR\\n```\\nWe then enroll Contoso to use the Workflow service. This causes a chain of enrollments whereby a sub-tenant is created for WORKFLOW+Contoso, which is then enrolled to use the Operations service, creating a sub-tenant of OPERATIONS, OPERATIONS+WORKFLOW+Contoso, which is then enrolled to use the FooBar service (since FooBar does not have dependencies, this does not create any further sub tenants). The Workflow service is also directly dependent on FooBar, so WORKFLOW+Contoso is also enrolled to use FooBar resulting in storage configuration for FooBar being added to it.\\nThis leaves the tenant hierarchy looking like this:\\n```\\nRoot tenant\\n|\\n+-> Client Tenants\\n|     |\\n|     +-> Contoso\\n|     |     +-> (Workflow storage configuration)\\n|     |     +-> (The Id of the WORKFLOW+Contoso sub-tenant for the Workflow service to use)\\n|     |\\n|     +-> Litware\\n|\\n+-> Service Tenants\\n|\\n+-> WORKFLOW\\n|     |\\n|     +-> WORKFLOW+Contoso\\n|           +-> (Operations storage configuration)\\n|           +-> (The Id of the OPERATIONS+WORKFLOW+Contoso sub-tenant for the Operations service to use)\\n|           +-> (FooBar storage configuration)\\n|\\n+-> OPERATIONS\\n|     |\\n|     +-> OPERATIONS+WORKFLOW+Contoso\\n|           +-> (FooBar storage configuration)\\n|\\n+-> FOOBAR\\n```\\nWe then enroll Contoso for the FooBar service. Since there are no additional dependencies, this does not result in any further sub-tenants being created, but does add storage configuration for FooBar to the Contoso tenant. As in the first example, Contoso now has two sets of storage configuration for the FooBar service, one for direct use and one for indirect use.\\n```\\nRoot tenant\\n|\\n+-> Client Tenants\\n|     |\\n|     +-> Contoso\\n|     |     +-> (Workflow storage configuration)\\n|     |     +-> (The Id of the WORKFLOW+Contoso sub-tenant for the Workflow service to use)\\n|     |     +-> (FooBar storage configuration)\\n|     |\\n|     +-> Litware\\n|\\n+-> Service Tenants\\n|\\n+-> WORKFLOW\\n|     |\\n|     +-> WORKFLOW+Contoso\\n|           +-> (Operations storage configuration)\\n|           +-> (The Id of the OPERATIONS+WORKFLOW+Contoso sub-tenant for the Operations service to use)\\n|           +-> (FooBar storage configuration)\\n|\\n+-> OPERATIONS\\n|     |\\n|     +-> OPERATIONS+WORKFLOW+Contoso\\n|           +-> (FooBar storage configuration)\\n|\\n+-> FOOBAR\\n```\\nWe now repeat the process of enrolling Litware for the Workflow service:\\n```\\nRoot tenant\\n|\\n+-> Client Tenants\\n|     |\\n|     +-> Contoso\\n|     |     +-> (Workflow storage configuration)\\n|     |     +-> (The Id of the WORKFLOW+Contoso sub-tenant for the Workflow service to use)\\n|     |     +-> (FooBar storage configuration)\\n|     |\\n|     +-> Litware\\n|           +-> (Workflow storage configuration)\\n|           +-> (The Id of the WORKFLOW+Litware sub-tenant for the Workflow service to use)\\n|\\n+-> Service Tenants\\n|\\n+-> WORKFLOW\\n|     |\\n|     +-> WORKFLOW+Contoso\\n|     |     +-> (Operations storage configuration)\\n|     |     +-> (The Id of the OPERATIONS+WORKFLOW+Contoso sub-tenant for the Operations service to use)\\n|     |     +-> (FooBar storage configuration)\\n|     |\\n|     +-> WORKFLOW+Litware\\n|           +-> (Operations storage configuration)\\n|           +-> (The Id of the OPERATIONS+WORKFLOW+Litware sub-tenant for the Operations service to use)\\n|           +-> (FooBar storage configuration)\\n|\\n+-> OPERATIONS\\n|     |\\n|     +-> OPERATIONS+WORKFLOW+Contoso\\n|     |     +-> (FooBar storage configuration)\\n|     |\\n|     +-> OPERATIONS+WORKFLOW+Litware\\n|           +-> (FooBar storage configuration)\\n|\\n+-> FOOBAR\\n```\\nSince Litware is not licenced to use FooBar, the Litware Client Tenant does not hold any configuration for that service itself.\\nFinally, we enroll Litware to use the Operations service. In this example, because Operations depends on FooBar, we need to create another sub-tenant of Operations to call FooBar with when Litware uses Operations directly, and enroll this new subtenant with FooBar. This leaves us with the following:\\n```\\nRoot tenant\\n|\\n+-> Client Tenants\\n|     |\\n|     +-> Contoso\\n|     |     +-> (Workflow storage configuration)\\n|     |     +-> (The Id of the WORKFLOW+Contoso sub-tenant for the Workflow service to use)\\n|     |     +-> (FooBar storage configuration)\\n|     |\\n|     +-> Litware\\n|           +-> (Workflow storage configuration)\\n|           +-> (The Id of the WORKFLOW+Litware sub-tenant for the Workflow service to use)\\n|           +-> (Operations storage configuration)\\n|           +-> (The Id of the OPERATIONS+Litware sub-tenant for the Operations service to use)\\n|\\n+-> Service Tenants\\n|\\n+-> WORKFLOW\\n|     |\\n|     +-> WORKFLOW+Contoso\\n|     |     +-> (Operations storage configuration)\\n|     |     +-> (The Id of the OPERATIONS+WORKFLOW+Contoso sub-tenant for the Operations service to use)\\n|     |     +-> (FooBar storage configuration)\\n|     |\\n|     +-> WORKFLOW+Litware\\n|           +-> (Operations storage configuration)\\n|           +-> (The Id of the OPERATIONS+WORKFLOW+Litware sub-tenant for the Operations service to use)\\n|           +-> (FooBar storage configuration)\\n|\\n+-> OPERATIONS\\n|     |\\n|     +-> OPERATIONS+WORKFLOW+Contoso\\n|     |     +-> (FooBar storage configuration)\\n|     |\\n|     +-> OPERATIONS+WORKFLOW+Litware\\n|     |     +-> (FooBar storage configuration)\\n|     |\\n|     +-> OPERATIONS+Litware\\n|           +-> (FooBar storage configuration)\\n|\\n+-> FOOBAR\\n```\\n","MatchedIds":[357,2828,714,200,631],"Predicted":"## Decision\n\nTo accommodate the various deployment and usage scenarios for Marain services and ensure robust multitenant support, we will adopt the following tenancy model:\n\n### Tenancy Model\n\n1. **Tenant Management**:\n   - Each tenant will have a unique identifier and a set of associated metadata for tracking their configuration and usage.\n   - Tenants can be created, updated, and deleted through a central tenant management service (`Marain.Tenancy`).\n\n2. **Deployment Options**:\n   - **Managed Service (PaaS)**: Marain will be deployed as a managed service in our cloud infrastructure. Tenants will be provisioned in our environment, with options for data residency in either shared or dedicated storage accounts\/databases per tenant.\n   - **Private Instances**: Clients can deploy private instances of Marain in their own cloud subscriptions. Tenants will be managed locally within those instances.\n\n3. **Storage Isolation**:\n   - For managed service clients requiring data to be stored in their own storage accounts or databases, each tenant will have configuration options specifying their storage endpoints.\n   - For clients implementing their own multi-tenanted services, they must provide a mechanism for isolating their tenants' data within their storage infrastructure.\n\n4. **Service Dependencies and Licensing**:\n   - Services like Workflow, which depend on other services (e.g., Operations), will use a hierarchical licensing model.\n   - Tenants licensed for a service will inherit access to dependent services as part of their usage, but direct access will be regulated based on their specific licensing agreements.\n  \n5. **Service Usage Tracking**:\n   - Each service will log both direct and indirect usage by tenants to ensure correct billing and support.\n\n### Implementation Details\n\n1. **Marain.Tenancy Service**:\n   - A centralized service to manage tenant lifecycle operations (create, read, update, delete).\n   - It will allow configuration for data residency requirements and linkage to specific storage accounts or databases.\n\n2. **Configuration Management**:\n   - A configuration service will manage tenant-specific configurations, particularly for data endpoints, ensuring proper isolation and compliance with data residency requirements.\n\n3. **Access Control and Multi-tenancy Enforcement**:\n   - **For managed services**: Use Role-Based Access Control (RBAC) combined with scopes to ensure tenants can only access their own data.\n   - **For private instances**: Encourage clients to implement robust RBAC and potentially integrate with our `Marain.Tenancy` service to maintain consistency.\n\n4. **Service Onboarding**:\n   - A standardized onboarding process that supports both managed tenants and private instance tenants, ensuring consistent configuration and enforcement of tenancy rules.\n\n5. **Dependency Management**:\n   - Services will be aware of their dependencies and ensure proper access control and logging for dependent service usage.\n   - This will be facilitated through enhanced service descriptors that list dependencies and their respective usage constraints.\n\n6. **Monitoring and Logging**:\n   - Centralized logging and monitoring will be implemented to track tenant activity for billing, support, and compliance purposes, available through a managed dashboard.\n\nBy adopting this model, we provide a robust and flexible multitenancy framework that can support varied deployment models and ensure data isolation and security for all tenants.","GenTime":"2024-08-20 00:29:41"}
{"File Name":"moneycount-api\/004-Use_Heroku_As_Deployment.md","Context":"## Context\\nIt's desirable to keep the project online for testing purposes, and it has to be an easy to use environment with no cost.\\n","Decision":"I decided to use Heroku, as it has an easy to use environment, CLI tool that makes it easy to deploy with just few commands and has a free plan. It also has a great integration with lots of platforms and recognizes Spring Boot out of the box.\\n","tokens":30,"id":3070,"text":"## Context\\nIt's desirable to keep the project online for testing purposes, and it has to be an easy to use environment with no cost.\\n\n\n##Decision\nI decided to use Heroku, as it has an easy to use environment, CLI tool that makes it easy to deploy with just few commands and has a free plan. It also has a great integration with lots of platforms and recognizes Spring Boot out of the box.\\n","MatchedIds":[4868,3982,1792,398,847],"Predicted":"## Decision\nWe will host the project using [GitHub Pages](https:\/\/pages.github.com\/). GitHub Pages provides a free, user-friendly platform for hosting static websites directly from a GitHub repository. This allows us to keep the project online for testing purposes without incurring any costs. Additionally, it integrates seamlessly with our existing version control system (GitHub), making deployments easy and efficient.\n\nWe also evaluated alternatives:\n* [Netlify](https:\/\/www.netlify.com\/) \u2013 It offers a free tier, but has more limitations on usage compared to GitHub Pages.\n* [Vercel](https:\/\/vercel.com\/) \u2013 Also has a free tier but with a usage cap that could be an issue for our requirements.\n\nUltimately, GitHub Pages was chosen due to its cost-effectiveness, ease of use, and smooth integration with our current workflow.","GenTime":"2024-08-20 00:29:45"}
{"File Name":"gp-finder\/0006-use-prometheus-for-exposing-metrics.md","Context":"## Context\\nWe need to know what the application is doing in a more light weight way than\\nscraping logs. We need to be able to monitor KPIs of the application in order\\nto understand the health of the application. This will allow us to react and\\npotentially pro-actively initiate measures as to ensure the application's\\nhealth if sound. Ultimately providing a better service for our users.\\n","Decision":"We will use Prometheus to monitor and alert on the state of the application.\\n","tokens":86,"id":3469,"text":"## Context\\nWe need to know what the application is doing in a more light weight way than\\nscraping logs. We need to be able to monitor KPIs of the application in order\\nto understand the health of the application. This will allow us to react and\\npotentially pro-actively initiate measures as to ensure the application's\\nhealth if sound. Ultimately providing a better service for our users.\\n\n\n##Decision\nWe will use Prometheus to monitor and alert on the state of the application.\\n","MatchedIds":[4852,669,1924,1701,1863],"Predicted":"## Decision\nImplement a metrics instrumentation system using Prometheus and Grafana.\n\n### Components\n1. **Prometheus**: \n    * **Prometheus Server**: Collects metrics from the application at defined intervals.\n    * **Node Exporter**: Exposes hardware and OS metrics.\n    * **Application Exporter**: Custom exporters or instrumentation code will expose application-specific metrics.\n\n2. **Grafana**:\n    * **Visualization**: Provides a dashboard for visualizing and monitoring the collected metrics.\n    * **Alerting**: Alerts based on the metrics collected by Prometheus to inform teams of potential issues proactively.\n\n### Details\n\n* **Instrumentation**: Integrate Prometheus client libraries into our application code to expose metrics such as request counts, error rates, and response times.\n* **Metrics Collection**: \n    * Use predefined exporters for common metrics (CPU, memory, disk usage).\n    * Develop custom metrics (business KPIs, application-specific metrics).\n* **Visualization**: \n    * Set up Grafana dashboards with key metrics related to application health.\n    * Create visualizations for critical KPIs to monitor in real-time.\n* **Alerting**: \n    * Configure Grafana to send alerts to the support team via email, Slack, or other communication tools based on thresholds for KPIs.\n    * Define thresholds and rules for alerts to ensure timely response to issues.\n\n### Key Metrics\n* **System Metrics**:\n    * CPU usage\n    * Memory usage\n    * Disk I\/O\n* **Application Metrics**:\n    * Request rates\n    * Error rates\n    * Response times\n    * Database query performance\n* **Business KPIs**:\n    * User engagement metrics\n    * Transaction success\/failure rates\n\nThis approach provides a lightweight and flexible way to monitor the health of the application and its KPIs. It allows proactive measures to be taken to ensure the application's health and ultimately provide a better service for our users.","GenTime":"2024-08-20 00:29:55"}
{"File Name":"cdk\/002-component-constuctors.md","Context":"## Context\\n<!--- What is the issue that we're seeing that is motivating this decision or change? -->\\nThis project contains a large number of classes, making up the various constructs and patterns. The project is intended to be used as a component library and, therefore, used by a number of people who don't have extensive knowledge of either the CDK or this library. It is therefore important to make the experience of using these classes as intuitive as possible.\\n","Decision":"<!-- What is the change that we're proposing and\/or doing? -->\\nConstructors should follow the following rules for consistency.\\n1. The first parameter should be a `scope` of type `GuStack`:\\n:white_check_mark: Valid\\n```ts\\nclass MyConstruct {\\nconstructor(scope: GuStack) {\\n...\\n}\\n}\\n```\\n:x: Invalid\\n```ts\\nclass MyConstruct {\\nconstructor(scope: Stack) {\\n...\\n}\\n}\\n```\\nThe construct\/pattern will then have a static `id` as it will never change, for example the `Stage` parameter.\\n2. They can also take a `props` object which should be correctly typed:\\n:white_check_mark: Valid\\n```ts\\nclass MyConstruct {\\nconstructor(scope: GuStack, props: MyConstructProps) {\\n...\\n}\\n}\\n```\\n:x: Invalid\\n```ts\\nclass MyConstruct {\\nconstructor(scope: Stack, props: object) {\\n...\\n}\\n}\\n```\\nThe construct\/pattern will then derive `id` from `props` as it will never change, for example `InstanceTypeFor${props.app}`.\\n3. They can also take an `id` of type string and a `props` object which should be correctly typed\\n:white_check_mark: Valid\\n```ts\\ninterface MyConstructProps {...}\\nclass MyConstruct {\\nconstructor(scope: GuStack, id: string, props: MyConstructProps) {\\n...\\n}\\n}\\n```\\n:x: Invalid\\n```ts\\nclass MyConstruct {\\nconstructor(scope: GuStack, id: any, props: object) {\\n...\\n}\\n}\\n```\\n4. Where all `props` are optional, the `props` object should be optional as a whole\\n:white_check_mark: Valid\\n```ts\\ninterface MyConstructProps {\\nprop1?: string;\\nprop2?: string\\n}\\nclass MyConstruct {\\nconstructor(scope: GuStack, id: string, props?: MyConstructProps) {\\n...\\n}\\n}\\n```\\n:x: Invalid\\n```ts\\ninterface MyConstructProps {\\nprop1?: string;\\nprop2?: string\\n}\\nclass MyConstruct {\\nconstructor(scope: GuStack, id: string, props: MyConstructProps) {\\n...\\n}\\n}\\n```\\n","tokens":95,"id":1193,"text":"## Context\\n<!--- What is the issue that we're seeing that is motivating this decision or change? -->\\nThis project contains a large number of classes, making up the various constructs and patterns. The project is intended to be used as a component library and, therefore, used by a number of people who don't have extensive knowledge of either the CDK or this library. It is therefore important to make the experience of using these classes as intuitive as possible.\\n\n\n##Decision\n<!-- What is the change that we're proposing and\/or doing? -->\\nConstructors should follow the following rules for consistency.\\n1. The first parameter should be a `scope` of type `GuStack`:\\n:white_check_mark: Valid\\n```ts\\nclass MyConstruct {\\nconstructor(scope: GuStack) {\\n...\\n}\\n}\\n```\\n:x: Invalid\\n```ts\\nclass MyConstruct {\\nconstructor(scope: Stack) {\\n...\\n}\\n}\\n```\\nThe construct\/pattern will then have a static `id` as it will never change, for example the `Stage` parameter.\\n2. They can also take a `props` object which should be correctly typed:\\n:white_check_mark: Valid\\n```ts\\nclass MyConstruct {\\nconstructor(scope: GuStack, props: MyConstructProps) {\\n...\\n}\\n}\\n```\\n:x: Invalid\\n```ts\\nclass MyConstruct {\\nconstructor(scope: Stack, props: object) {\\n...\\n}\\n}\\n```\\nThe construct\/pattern will then derive `id` from `props` as it will never change, for example `InstanceTypeFor${props.app}`.\\n3. They can also take an `id` of type string and a `props` object which should be correctly typed\\n:white_check_mark: Valid\\n```ts\\ninterface MyConstructProps {...}\\nclass MyConstruct {\\nconstructor(scope: GuStack, id: string, props: MyConstructProps) {\\n...\\n}\\n}\\n```\\n:x: Invalid\\n```ts\\nclass MyConstruct {\\nconstructor(scope: GuStack, id: any, props: object) {\\n...\\n}\\n}\\n```\\n4. Where all `props` are optional, the `props` object should be optional as a whole\\n:white_check_mark: Valid\\n```ts\\ninterface MyConstructProps {\\nprop1?: string;\\nprop2?: string\\n}\\nclass MyConstruct {\\nconstructor(scope: GuStack, id: string, props?: MyConstructProps) {\\n...\\n}\\n}\\n```\\n:x: Invalid\\n```ts\\ninterface MyConstructProps {\\nprop1?: string;\\nprop2?: string\\n}\\nclass MyConstruct {\\nconstructor(scope: GuStack, id: string, props: MyConstructProps) {\\n...\\n}\\n}\\n```\\n","MatchedIds":[3203,4766,1996,3784,4361],"Predicted":"## Decision\n\nTo ensure the project is intuitive and accessible for users with varying levels of expertise, we will adopt the following strategies:\n\n1. **Clear and Consistent Naming Conventions**:\n    - Use descriptive and consistent naming conventions for classes, methods, and properties. This helps users quickly understand the purpose and usage of different components.\n    - Avoid abbreviations that may not be universally understood.\n\n2. **Comprehensive Documentation**:\n    - Provide detailed documentation for each class, including examples of common use cases.\n    - Use inline comments to explain complex logic and assumptions within the code.\n\n3. **Intuitive Class Structure**:\n    - Organize classes in a hierarchical structure that mirrors common design patterns and logical groupings, making it easier to locate and understand dependencies.\n    - Ensure that public APIs are straightforward and encapsulate internal complexity.\n\n4. **User-Friendly APIs**:\n    - Design APIs that prioritize ease of use, even at the expense of additional implementation complexity.\n    - Use fluent interfaces and method chaining where appropriate to allow for more readable and concise code.\n\n5. **Extensive Testing and Examples**:\n    - Include extensive test cases and example projects demonstrating how to use various components within the library.\n    - Showcase best practices and offer solutions for common scenarios.\n\n6. **Regular Feedback Loops**:\n    - Establish channels for user feedback and use this input to improve the library continuously.\n    - Update documentation and examples in response to common questions and issues from the user community.\n\n7. **Versioning and Changelog**:\n    - Implement semantic versioning to communicate changes clearly.\n    - Maintain a detailed changelog to help users easily track updates, new features, and bug fixes.\n\nBy implementing these strategies, we aim to create a component library that is both powerful and easy to use, encouraging adoption and minimizing the learning curve for new users.","GenTime":"2024-08-20 00:30:01"}
{"File Name":"register-trainee-teachers\/0004-non-integer-trainee-ids.md","Context":"## Context\\nWe have been using trainee IDs in the URL as per standard Rails convention\/operation.\\nIt was felt that we should use non-integer IDs for a number of reasons:\\n* remove predictability\\n* interoperability with other systems without depending on DB IDs\\n","Decision":"We chose to use option 3 as it met the needs we had with the minimum of effort and avoided the really long URLs that\\noption 1 would have caused.\\n","tokens":57,"id":3802,"text":"## Context\\nWe have been using trainee IDs in the URL as per standard Rails convention\/operation.\\nIt was felt that we should use non-integer IDs for a number of reasons:\\n* remove predictability\\n* interoperability with other systems without depending on DB IDs\\n\n\n##Decision\nWe chose to use option 3 as it met the needs we had with the minimum of effort and avoided the really long URLs that\\noption 1 would have caused.\\n","MatchedIds":[2403,4011,1522,2386,1866],"Predicted":"## Decision\nWe have decided to adopt UUIDs (Universally Unique Identifiers) for trainee IDs in the URLs. \n\nOur reasoning behind the decision:\n* **Remove Predictability**: UUIDs are significantly harder to guess, thus improving the security by making it difficult for malicious users to enumerate valid IDs.\n* **Interoperability**: UUIDs are widely used and recognized across different systems and platforms, facilitating easier integration and data exchange without relying on internal DB IDs.\n\nImplementation Steps:\n1. Migrate existing entries to include a UUID column for trainee IDs.\n2. Update routes and controllers to use UUIDs instead of integer IDs.\n3. Ensure backward compatibility and create necessary redirects from old integer-based URLs to new UUID-based URLs if needed.\n4. Update documentation and notify all stakeholders about this change.\n5. Thoroughly test the changes to ensure no disruption in service.\n\nThis decision aligns with our goals of improving security and ensuring seamless interoperability with other systems.","GenTime":"2024-08-20 00:30:06"}
{"File Name":"linshare-mobile-android-app\/0011-upload-document.md","Context":"## Context\\nAt this time, we used `android.net.Uri` to extract information and open an `InputStream` to upload.\\nBut, an `Uri` has `UriPermission` protect the data which is represented.\\nThis permission will be revoked by shared application when the received Activity no-longer run.\\nIt is not critical to Android 7, but with Android 9 we always get `Permission Denied` when deliver Uri to Worker to execute.\\n","Decision":"- We extract all requirement information of the Document at the time we receive the Intent.\\n- Instead of using Uri directly, we create a temporary file to store the file which Uri represent\\n- We deliver temporary file path to Worker\\n","tokens":94,"id":1643,"text":"## Context\\nAt this time, we used `android.net.Uri` to extract information and open an `InputStream` to upload.\\nBut, an `Uri` has `UriPermission` protect the data which is represented.\\nThis permission will be revoked by shared application when the received Activity no-longer run.\\nIt is not critical to Android 7, but with Android 9 we always get `Permission Denied` when deliver Uri to Worker to execute.\\n\n\n##Decision\n- We extract all requirement information of the Document at the time we receive the Intent.\\n- Instead of using Uri directly, we create a temporary file to store the file which Uri represent\\n- We deliver temporary file path to Worker\\n","MatchedIds":[1647,1133,3797,3831,4748],"Predicted":"## Decision\nInstead of directly passing the `android.net.Uri` to the `Worker`, we will take the following approach:\n\n1. **Read the data from the `Uri` in the Activity\/Fragment:** Before starting the `Worker`, we will read the data from the `Uri` and save it temporarily within the application sandbox or cache directory.\n2. **Pass the file path to the `Worker`:** Instead of passing the `Uri`, we will pass the file path of the saved data to the `Worker`.\n3. **Ensure cleanup after upload:** Once the `Worker` completes the upload operation, it should delete the temporary file to avoid unnecessary storage consumption.\n\nThis approach ensures that `UriPermission` issues are avoided as we handle the file directly within our application instead of relying on permissions that can be revoked.","GenTime":"2024-08-20 00:30:09"}
{"File Name":"cloud-sdk-js\/0021-odata-url-builder.md","Context":"## Context\\nThe current request builder APIs are not able to handle some odata requests like:\\n- query navigation properties `GET \/People('scottketchum')\/Friends`\\n- getting \"raw value\" of a property `\/People('scottketchum')\/$value`\\n","Decision":"- Implement A for now as a powerful workaround.\\n- Proposal B\/C\/variant will be a `2.0` task, where it seems C might be the winner and we might review the decision later as they close to each other.\\nAt least, implement it as a separate task so we have a workaround for custom URL.\\n### Proposal B\\n```ts\\n\/\/ Problem 1\\n\/\/ \/People('russellwhyte')\/Friends\\nTripPinService.entity(People, 'russellwhyte') \/\/ single item can continue linking\\n.navigationProp(People.Friends)\\n.buildGetRequest() \/\/xxxRequestBuilder, which can be called by single item\/multi items and others\\n.customHeaders(headers)\\n.execute(destination);\\n```\\n```ts\\n\/\/ Problem 2,3,4\\n\/\/ \/People('russellwhyte')\/Friends('scottketchum')\/BestFriend\/BestFriend\\nTripPinService.entity(People, 'russellwhyte') \/\/ single item can continue linking\\n.navigationProp(People.Friends, 'scottketchum') \/\/ single item can continue linking\\n.navigationProp(People.BestFriend) \/\/ single item can continue linking\\n.navigationProp(People.BestFriend); \/\/ single item can continue linking\\n```\\n#### Pros and cons:\\n##### Pros:\\n- Better fluent API (compared to `asChildOf`) with builder pattern.\\n- Can be extended for supporting problem 5-7.\\n- Typed.\\n##### Cons:\\n- Lots of effort to build the new structure, which seems to be a `2.0` task.\\n### Proposal C\\nBasically, the same idea but with different API in terms of reaching single items.(e.g., \"getByKey\" and 1-to-1 navigation properties)\\n```ts\\n\/\/ Problem 1\\n\/\/ \/People('russellwhyte')\/Friends\\nTripPinService.entity(People) \/\/ multi item can call \"key\" to become a single item\\n.key('russellwhyte') \/\/ single item can continue linking\\n.navigationProp(People.Friends);\\n```\\n```ts\\n\/\/ Problem 2,3,4\\n\/\/ \/People('russellwhyte')\/Friends('scottketchum')\/BestFriend\/BestFriend\\nTripPinService.entity(People) \/\/ multi item can call \"key\" to become a single item\\n.key('russellwhyte') \/\/ single item can continue linking\\n.navigationProp(People.Friends) \/\/ multi item can call \"key\" to become a single item\\n.key('scottketchum') \/\/ single item can continue linking\\n.navigationProp(People.BestFriend)\\n.navigationProp(People.BestFriend)\\n.buildGetRequest() \/\/xxxRequestBuilder, which can be called by single item\/multi items and others\\n.customHeaders(headers)\\n.execute(destination);\\n```\\n### Proposal C variants\\n```\\n\/\/frank\\nPeople.requestBuilder()\\n.getByKey('key') \/\/ xxxRequestBuilder\\n.toFriend('abc')\\n.toBestFriend()\\n.toFriends()\\n.getBuilder()\/\/create\\n\/\/marika\\nPeople.requestBuilder()\\n.key('scottketchum')\\n.navigationProp(People.BestFriend)\\n.navigationProp(People.BestFriend)\\n.buildGetRequest() \/\/xxxRequestBuilder, which can be called by single item\/multi items and others\\n.customHeaders(headers)\\n.execute(destination)\\n```\\n#### Pros and cons:\\nSame as `Proposal B`, but with more methods instead of overloading functions with more parameters.\\n### Proposal D\\nUse the similar API like `asChildOf`\\n```ts\\n\/\/ \/People(personKey)\/Friends\\nFriends.requestBuilder().getAll().asChildOf(person, People.Friends);\\n```\\n```ts\\n\/\/ \/People(personKey)\/Friends(friendKey)\\nFriends.requestBuilder().getByKey(friendKey).asChildOf(person, People.Friends);\\n```\\n#### Pros and cons:\\n##### Pros:\\n- Consistent with `asChildOf` for generating same URL.\\n- Medium complexity\\n- Typed.\\n##### Cons:\\n- Cannot be extended for supporting problem 5-7, so we need to find solution for them.\\n- The ugly API `asChildOf` is used with additional use cases.\\n- Different order: `Entity -> NavigationProp` (url) V.S. `NavigationProp -> Entity` (API usage)\\n#### Decision:\\nNot chosen due to the lack of extension and confusing API.\\n### previous docs\\nFind related discussion [here](..\/implementation-documentation\/api-improvements.md)\\n","tokens":61,"id":3630,"text":"## Context\\nThe current request builder APIs are not able to handle some odata requests like:\\n- query navigation properties `GET \/People('scottketchum')\/Friends`\\n- getting \"raw value\" of a property `\/People('scottketchum')\/$value`\\n\n\n##Decision\n- Implement A for now as a powerful workaround.\\n- Proposal B\/C\/variant will be a `2.0` task, where it seems C might be the winner and we might review the decision later as they close to each other.\\nAt least, implement it as a separate task so we have a workaround for custom URL.\\n### Proposal B\\n```ts\\n\/\/ Problem 1\\n\/\/ \/People('russellwhyte')\/Friends\\nTripPinService.entity(People, 'russellwhyte') \/\/ single item can continue linking\\n.navigationProp(People.Friends)\\n.buildGetRequest() \/\/xxxRequestBuilder, which can be called by single item\/multi items and others\\n.customHeaders(headers)\\n.execute(destination);\\n```\\n```ts\\n\/\/ Problem 2,3,4\\n\/\/ \/People('russellwhyte')\/Friends('scottketchum')\/BestFriend\/BestFriend\\nTripPinService.entity(People, 'russellwhyte') \/\/ single item can continue linking\\n.navigationProp(People.Friends, 'scottketchum') \/\/ single item can continue linking\\n.navigationProp(People.BestFriend) \/\/ single item can continue linking\\n.navigationProp(People.BestFriend); \/\/ single item can continue linking\\n```\\n#### Pros and cons:\\n##### Pros:\\n- Better fluent API (compared to `asChildOf`) with builder pattern.\\n- Can be extended for supporting problem 5-7.\\n- Typed.\\n##### Cons:\\n- Lots of effort to build the new structure, which seems to be a `2.0` task.\\n### Proposal C\\nBasically, the same idea but with different API in terms of reaching single items.(e.g., \"getByKey\" and 1-to-1 navigation properties)\\n```ts\\n\/\/ Problem 1\\n\/\/ \/People('russellwhyte')\/Friends\\nTripPinService.entity(People) \/\/ multi item can call \"key\" to become a single item\\n.key('russellwhyte') \/\/ single item can continue linking\\n.navigationProp(People.Friends);\\n```\\n```ts\\n\/\/ Problem 2,3,4\\n\/\/ \/People('russellwhyte')\/Friends('scottketchum')\/BestFriend\/BestFriend\\nTripPinService.entity(People) \/\/ multi item can call \"key\" to become a single item\\n.key('russellwhyte') \/\/ single item can continue linking\\n.navigationProp(People.Friends) \/\/ multi item can call \"key\" to become a single item\\n.key('scottketchum') \/\/ single item can continue linking\\n.navigationProp(People.BestFriend)\\n.navigationProp(People.BestFriend)\\n.buildGetRequest() \/\/xxxRequestBuilder, which can be called by single item\/multi items and others\\n.customHeaders(headers)\\n.execute(destination);\\n```\\n### Proposal C variants\\n```\\n\/\/frank\\nPeople.requestBuilder()\\n.getByKey('key') \/\/ xxxRequestBuilder\\n.toFriend('abc')\\n.toBestFriend()\\n.toFriends()\\n.getBuilder()\/\/create\\n\/\/marika\\nPeople.requestBuilder()\\n.key('scottketchum')\\n.navigationProp(People.BestFriend)\\n.navigationProp(People.BestFriend)\\n.buildGetRequest() \/\/xxxRequestBuilder, which can be called by single item\/multi items and others\\n.customHeaders(headers)\\n.execute(destination)\\n```\\n#### Pros and cons:\\nSame as `Proposal B`, but with more methods instead of overloading functions with more parameters.\\n### Proposal D\\nUse the similar API like `asChildOf`\\n```ts\\n\/\/ \/People(personKey)\/Friends\\nFriends.requestBuilder().getAll().asChildOf(person, People.Friends);\\n```\\n```ts\\n\/\/ \/People(personKey)\/Friends(friendKey)\\nFriends.requestBuilder().getByKey(friendKey).asChildOf(person, People.Friends);\\n```\\n#### Pros and cons:\\n##### Pros:\\n- Consistent with `asChildOf` for generating same URL.\\n- Medium complexity\\n- Typed.\\n##### Cons:\\n- Cannot be extended for supporting problem 5-7, so we need to find solution for them.\\n- The ugly API `asChildOf` is used with additional use cases.\\n- Different order: `Entity -> NavigationProp` (url) V.S. `NavigationProp -> Entity` (API usage)\\n#### Decision:\\nNot chosen due to the lack of extension and confusing API.\\n### previous docs\\nFind related discussion [here](..\/implementation-documentation\/api-improvements.md)\\n","MatchedIds":[779,75,2184,674,2802],"Predicted":"## Decision\nTo support the specified OData requests effectively, the request builder APIs will be enhanced as follows:\n\n### Query Navigation Properties\n1. **Navigation Queries Enhancement**:\n   - The request builder API will be updated to support querying navigation properties.\n   - The API will allow chaining navigation properties in the request building process.\n   - Example usage: `requestBuilder.getEntity('People').withKey('scottketchum').navigateTo('Friends').get();`\n\n### Getting \"Raw Value\" of a Property\n2. **Raw Value Access**:\n   - The request builder API will be extended to facilitate access to the raw value of a property using OData's `$value`.\n   - Example usage: `requestBuilder.getEntity('People').withKey('scottketchum').getValue();`\n\n### API Structure and Methods\n3. **Method Additions**:\n   - Introduce methods such as `withKey(key)`, `navigateTo(propertyName)`, and `getValue()` to the request builder API.\n   - Ensure that the internal URL construction logic adheres to OData standards to correctly format the requests.\n\n### Documentation and Examples\n4. **Documentation**:\n   - Update the API documentation to include examples of how to use the new methods to perform the specified OData requests.\n   - Provide detailed explanations and code snippets to enhance developer understanding.\n\n### Testing and Validation\n5. **Testing**:\n   - Implement unit and integration tests to verify that the new capabilities work as expected.\n   - Include edge cases to ensure robustness.\n\nBy implementing these enhancements, the request builder API will be able to handle complex OData queries involving navigation properties and retrieval of raw property values, providing a more versatile and powerful tool for developers.","GenTime":"2024-08-20 00:30:16"}
{"File Name":"cena\/0005-use-spring-framework.md","Context":"## Context\\n[Hexagonal architecture](0003-adopt-hexagonal-architecture.md) requires inversion of control to inject infrastructure\\nservices implementations dependencies into the services defined in the core hexagon.\\nREST API implementation requires a dedicated library to define incoming adapters handling the HTTP resources.\\nSpring is a well established framework for Java. It is non-invasive and provides multiple features such as IoC, AOP,\\nREST services implementation, security that will help speed up implementation in a cohesive way. The author has also used\\nSpring for many years and masters many of the provided features.\\n","Decision":"Spring framework will be the backbone for `menu-generation` application.\\n","tokens":120,"id":649,"text":"## Context\\n[Hexagonal architecture](0003-adopt-hexagonal-architecture.md) requires inversion of control to inject infrastructure\\nservices implementations dependencies into the services defined in the core hexagon.\\nREST API implementation requires a dedicated library to define incoming adapters handling the HTTP resources.\\nSpring is a well established framework for Java. It is non-invasive and provides multiple features such as IoC, AOP,\\nREST services implementation, security that will help speed up implementation in a cohesive way. The author has also used\\nSpring for many years and masters many of the provided features.\\n\n\n##Decision\nSpring framework will be the backbone for `menu-generation` application.\\n","MatchedIds":[1035,3374,2500,389,660],"Predicted":"## Decision\nWe will adopt the Spring framework for implementing the REST API in accordance with the Hexagonal architecture. Spring's robust support for Inversion of Control (IoC), Aspect-Oriented Programming (AOP), REST services, and security aligns well with the Hexagonal architecture principles, facilitating a streamlined and maintainable implementation process. Additionally, the team\u2019s familiarity with Spring will help in leveraging its features effectively, ensuring swift development and expertise-driven decision making.","GenTime":"2024-08-20 00:30:19"}
{"File Name":"scholarsphere\/0003-no-derivatives.md","Context":"## Context\\nWhen viewing different file types over the web, large video files or other non-standard files, may not display well in\\nthe browser, so the common solution is to create derivatives for a better user experience. These also include thumbnails\\nfor a sample picture of what the resource looks like.\\nManaging and creating derivatives can be hard and requires careful architectural considerations.\\nHowever, the modern web has adapted to larger files, and generally handles a wider array of file types than it did in\\nthe past. Also, thumbnails only work for image and video formats. Generic icon-based images for any file type can be\\nmore informative.\\n","Decision":"Scholarsphere will not create any derivative files. Icons will be used to represent file types, and the original\\nuploaded file can be rendered when applicable.\\n","tokens":129,"id":4553,"text":"## Context\\nWhen viewing different file types over the web, large video files or other non-standard files, may not display well in\\nthe browser, so the common solution is to create derivatives for a better user experience. These also include thumbnails\\nfor a sample picture of what the resource looks like.\\nManaging and creating derivatives can be hard and requires careful architectural considerations.\\nHowever, the modern web has adapted to larger files, and generally handles a wider array of file types than it did in\\nthe past. Also, thumbnails only work for image and video formats. Generic icon-based images for any file type can be\\nmore informative.\\n\n\n##Decision\nScholarsphere will not create any derivative files. Icons will be used to represent file types, and the original\\nuploaded file can be rendered when applicable.\\n","MatchedIds":[2515,2859,4308,3987,3249],"Predicted":"## Decision\nWe decided to use a hybrid approach combining both thumbnails and generic icon-based images for better user experience and easier management. The specific decisions are as follows:\n\n1. **For Image and Video Files**:\n   - Generate thumbnails for image and video files to provide a quick preview.\n   - Utilize existing tools like FFmpeg for video thumbnails and libraries like ImageMagick for image thumbnails.\n   \n2. **For Other File Types**:\n   - Use generic icon-based images representing the file types (e.g., PDF, DOCX). Libraries like FontAwesome provide a vast collection of icons that can be used for this purpose.\n   \n3. **Adaptive Streaming for Large Video Files**:\n   - Implement adaptive streaming (e.g., using HLS or DASH) for large video files to ensure smooth playback in various network conditions.\n   \n4. **Fallback Mechanism**:\n   - For unsupported or unrecognized file types, display a generic file icon along with the file name and any metadata available.\n\n### Reasons:\n- **Improved User Experience**: Thumbnails provide a quick visual representation for images and videos, while generic icons offer instant recognition for other file types.\n- **Ease of Management**: Generating icons for non-image\/video files is easier to manage and implement compared to generating thumbnails for every possible file type.\n- **Modern Browser Capability**: Leverage the capabilities of modern browsers to handle larger files and various file types more efficiently.\n- **Scalability**: The hybrid approach ensures that the system remains scalable and manageable, even as the number of file types and volume of files increase.","GenTime":"2024-08-20 00:30:26"}
{"File Name":"Maud\/0004-python-package-versioning.md","Context":"## Context\\nWe need a simple way to manage our package version.\\n","Decision":"We use versioneer to do this for us.\\n","tokens":15,"id":241,"text":"## Context\\nWe need a simple way to manage our package version.\\n\n\n##Decision\nWe use versioneer to do this for us.\\n","MatchedIds":[1462,398,3328,4225,4358],"Predicted":"## Decision\nWe will use [Semantic Versioning](https:\/\/semver.org\/) to manage our package versions. Semantic Versioning involves using three numbers to specify versions: MAJOR.MINOR.PATCH. This will ensure our versioning is clear and predictable, helping both our team and users understand the impact of changes.","GenTime":"2024-08-20 00:30:31"}
{"File Name":"functionaut\/0003-test-distributed-files-only.md","Context":"## Context\\nIt is not uncommon to have successful tests against development sources failing against the production bundle. We must make sure that code distributed to the public works as intended and avoid false positive in testing. The advanced compilation mode of the Google Closure Compiler makes this class of errors more likely to happen as it transforms the development sources radically.\\n","Decision":"Testing will be made against the production bundle to catch compilation errors before they reach our users.\\n","tokens":67,"id":2689,"text":"## Context\\nIt is not uncommon to have successful tests against development sources failing against the production bundle. We must make sure that code distributed to the public works as intended and avoid false positive in testing. The advanced compilation mode of the Google Closure Compiler makes this class of errors more likely to happen as it transforms the development sources radically.\\n\n\n##Decision\nTesting will be made against the production bundle to catch compilation errors before they reach our users.\\n","MatchedIds":[2687,3428,5070,1258,4179],"Predicted":"## Decision\nTo mitigate the risk of false positives and ensure that the code distributed to the public is reliable, we have decided to run our test suite against the production bundle generated by the Google Closure Compiler in advanced compilation mode. This ensures that our tests validate the actual production code, accounting for any transformations applied during the compilation process. We will automate this step in our Continuous Integration (CI) pipeline to maintain consistency and reduce the likelihood of errors slipping through to production.","GenTime":"2024-08-20 00:30:33"}
{"File Name":"qc-atlas\/0005-use-OpenAPI.md","Context":"## Context and Problem Statement\\nThe API has to be documented in human and machine readable form. If the API is documented in machine readable form, the automatic generation of client services is possible.\\n## Decision Drivers\\n* readable API documentation\\n* effort of manually creating client services\\n","Decision":"* readable API documentation\\n* effort of manually creating client services\\nChosen option: Use OpenAPI, because the API is described in a standardized format which is human and machine readable.\\n### Positive Consequences\\n* Standardized documentation of the API\\n* Automatic service generation for clients is possible\\n### Negative Consequences <!-- optional -->\\n* OpenAPI annotations have to be maintained\\n","tokens":56,"id":693,"text":"## Context and Problem Statement\\nThe API has to be documented in human and machine readable form. If the API is documented in machine readable form, the automatic generation of client services is possible.\\n## Decision Drivers\\n* readable API documentation\\n* effort of manually creating client services\\n\n\n##Decision\n* readable API documentation\\n* effort of manually creating client services\\nChosen option: Use OpenAPI, because the API is described in a standardized format which is human and machine readable.\\n### Positive Consequences\\n* Standardized documentation of the API\\n* Automatic service generation for clients is possible\\n### Negative Consequences <!-- optional -->\\n* OpenAPI annotations have to be maintained\\n","MatchedIds":[4644,3069,2181,1180,160],"Predicted":"## Decision\nWe decide to use the OpenAPI 3.0 specification for documenting our API. This choice allows us to meet the requirement for both human-readable and machine-readable API documentation, facilitating automatic generation of client services.\n\n### Positive Consequences\n- **Comprehensive Documentation**: OpenAPI 3.0 provides a standardized and widely adopted format for API documentation, ensuring clarity and readability both for humans and machines.\n- **Automatic Client Generation**: Tools like Swagger Codegen and OpenAPI Generator can be employed to automatically generate client libraries, reducing the effort and potential errors of manual creation.\n- **Developer Adoption**: The OpenAPI 3.0 specification is well-supported with a broad ecosystem of tools, making it easier for developers to adopt and integrate.\n\n### Negative Consequences\n- **Learning Curve**: The team will need to invest time in learning the OpenAPI 3.0 specification and associated tools.\n- **Initial Setup**: There may be an initial overhead in setting up the OpenAPI documentation and integrating it into the development workflow.\n\nThis decision supports the goals of providing readable API documentation and reducing the manual effort required to create client services.","GenTime":"2024-08-20 00:30:37"}
{"File Name":"terraform-aws-dynamodb\/0010-terraform-backend-in-module.md","Context":"## Context\\nWhen LIC teams begun using this module it became apparent that the current\\nimplementation pattern does not meet their needs. Without a backend in the\\nmodule teams would need to add a Terraform `backend` configuration into there\\nlocal implementation for it to be picked up.\\n","Decision":"Restored the `backend` into the module for the time being.\\nWe still feel this should be removed at some time and teams become familiar with\\nhow to use Terragrunt\/Terraform configuring there own `backend`.\\n","tokens":57,"id":2539,"text":"## Context\\nWhen LIC teams begun using this module it became apparent that the current\\nimplementation pattern does not meet their needs. Without a backend in the\\nmodule teams would need to add a Terraform `backend` configuration into there\\nlocal implementation for it to be picked up.\\n\n\n##Decision\nRestored the `backend` into the module for the time being.\\nWe still feel this should be removed at some time and teams become familiar with\\nhow to use Terragrunt\/Terraform configuring there own `backend`.\\n","MatchedIds":[2536,621,3144,1372,2793],"Predicted":"## Decision\nWe have decided to include a default Terraform `backend` configuration in the module itself. This backend configuration will use a remote state storage appropriate for global use by all teams. This will eliminate the need for teams to add their own `backend` configuration locally, ensuring a consistent and easier setup process across all implementations. Documentation will be provided to inform teams about the default backend and how to customize it if their use case requires different configurations.","GenTime":"2024-08-20 00:30:40"}
{"File Name":"MoviesAndroid\/0001 ADR.md","Context":"## Context and Problem Statement\\nI need a way to document design decisions.\\n","Decision":"Chosen option: Smaller markdown files in Git of format MADR, no cmd tool, IDEA is sufficient.\\nPositive Consequences:\\n* Choices and reasoning are persisted and versioned\\nNegative consequences:\\n* Having to document\\n","tokens":16,"id":160,"text":"## Context and Problem Statement\\nI need a way to document design decisions.\\n\n\n##Decision\nChosen option: Smaller markdown files in Git of format MADR, no cmd tool, IDEA is sufficient.\\nPositive Consequences:\\n* Choices and reasoning are persisted and versioned\\nNegative consequences:\\n* Having to document\\n","MatchedIds":[2261,4361,4428,3119,1436],"Predicted":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).","GenTime":"2024-08-20 00:30:42"}
{"File Name":"once-ui\/0002-build-a-high-quality-ui-components-library.md","Context":"## Context\\nMoving towards a suite of multiple applications, we need a library of shared Angular components that can be reused across the different apps, all with the same design language and a consistent api.\\n","Decision":"- We will build a library of atomic ui components that will be used throughout the applications.\\n- Developers should be able to easily import only the components they need and use them in their applications.\\n- Components should be well tested and isolated in a way that doesn't break the host application's code.\\n","tokens":40,"id":1151,"text":"## Context\\nMoving towards a suite of multiple applications, we need a library of shared Angular components that can be reused across the different apps, all with the same design language and a consistent api.\\n\n\n##Decision\n- We will build a library of atomic ui components that will be used throughout the applications.\\n- Developers should be able to easily import only the components they need and use them in their applications.\\n- Components should be well tested and isolated in a way that doesn't break the host application's code.\\n","MatchedIds":[847,157,1240,3031,3718],"Predicted":"## Decision\nWe will create a shared Angular component library using [Angular CLI](https:\/\/angular.io\/cli) to generate the initial structure. This library will follow best practices and guidelines for reusable Angular libraries, ensuring consistency in both design language and API. \n\n### Implementation Steps:\n1. **Library Setup:** \n   - Use Angular CLI to generate a new workspace and library:\n     ```sh\n     ng new my-workspace --create-application=false\n     cd my-workspace\n     ng generate library shared-components\n     ```\n\n2. **Component Development:**\n   - Define and create components within the `shared-components` library and enforce consistent design and API structure:\n     ```sh\n     ng generate component shared-components\/<component-name>\n     ```\n\n3. **Styling:**\n   - Leverage [Angular Material](https:\/\/material.angular.io\/) or [Tailwind CSS](https:\/\/tailwindcss.com\/) for consistent design language. \n   - Incorporate a global style guide and maintain it under `src\/shared-components\/styles`.\n\n4. **Documentation:**\n   - Use [Storybook](https:\/\/storybook.js.org\/) for documenting and showcasing each component:\n     ```sh\n     npx -p @angular\/cli ng add @storybook\/angular\n     ```\n   - Create comprehensive documentation for usage, examples, and APIs for each component.\n\n5. **Versioning and Release:**\n   - Utilize [Semantic Release](https:\/\/semantic-release.gitbook.io\/) for automated versioning and publishing of the library to an npm registry:\n     ```sh\n     npm install --save-dev semantic-release @semantic-release\/npm @semantic-release\/git\n     ```\n\n### Considerations:\nWe also considered using a monorepo setup with Nx for better dependency management and integration between different applications and the shared component library. However, given the simplicity and focus on component reuse, Angular CLI's library capabilities suffice for our current needs. \n\nDocumentation, consistent API design, and a robust testing strategy (including unit tests with Jasmine and Karma) will be key aspects of our approach to ensure the long-term success and maintainability of this shared component library.","GenTime":"2024-08-20 00:30:49"}
{"File Name":"paas-team-manual\/ADR037-automated-certificate-rotation.html.md","Context":"## Context\\nOur certificate rotation was a largely manual process, involving an operator triggering a series of Concourse pipeline jobs in a particular sequence. We did not have a routine for doing rotations, and would typically only do them as part of a CF upgrade.\\nThe only means we had for knowing if a cert rotation was necessary was the `check-certificates` job, in the `create-cloudfoundry` Concourse pipeline, which would fail if any certificate had less than 30 days until it expired.\\nIn Q2 2019 (August\/September) we moved all of our platform secrets from AWS S3 to [Credhub](https:\/\/docs.cloudfoundry.org\/credhub\/). This covered third-party service credentials, platform passwords, and certificates. Since Credhub supports [certificate rotation](https:\/\/github.com\/pivotal-cf\/credhub-release\/blob\/master\/docs\/ca-rotation.md), we chose to implement automatic certificate rotation. This ADR contains details of how we did it.\\n","Decision":"Credhub has the notion of a transitional certificate. As written in [their documentation](https:\/\/github.com\/pivotal-cf\/credhub-release\/blob\/master\/docs\/ca-rotation.md), a transitional certificate is\\n> a new version that will not be used for signing yet, but can be added to your servers trusted certificate lists.\\nOur certificate rotation process is built around the setting and migration of the `transitional` flag, such that over a number of deployments an active certificate is retired and a new certificate is deployed, without downtime.\\nIn order to make certificate rotation automatic, and require no operator interaction, it is implemented as a job at the tail end of the `create-cloudfoundry` pipeline; after acceptance tests and before releases tagging.\\nThe new `rotate-certs` job has three tasks:\\n- `remove-transitional-flag-for-ca`\\n- `move-transitional-flag-for-ca`\\n- `set-transitional-flag-for-ca`\\nThese three tasks are in reverse order of the process for rotating a certificate. If the tasks were ordered normally, the first task would set up the state for the second, and the second would set up the state for the third, and Bosh would be unable to deploy the certificates without downtime. However, here the tasks are explained in the proper order to make it easier to understand how a certificate is rotated. To understand how it happens in the pipeline, assume a Bosh deploy happens between each step.\\n`set-transitional-flag-for-ca` is the first step in the process. It iterates through all CA certificates in Credhub, looking for any expiring under 30 days. Any that are, are regenerated as transitional certificates. This results in Credhub holding two certificates for the same credential name: the expiring certificate, and the new certificate with the `transitional` flag.\\n`move-transitional-flag-for-ca` is the second step in the process, and has two jobs:\\n1. It finds all CA certificates in Credhub which have 2 values, where the oldest certificate does not have the `transitional` flag and the newer one does. For each of those, it swaps the `transitional` flag to the older certificate. Finally, it looks for any leaf certificates signed by the CA certificate and regenerates them using the new CA certificate.\\n2. It looks for any leaf certificates that are expiring in less than 30 days and regenerates them. This is a one step process and they are deployed on the next Bosh deploy.\\n`remove-transitional-flag-for-ca` is the third and final step in the process. It iterates through all of the CA certificates in Credhub, looking for any with 2 values, where the older certificate is marked as `transitional` and the newer certificate is not. It then removes the `transitional` flag from the older certificate, which has the effect of dropping the certificate.\\nThe existing `check-certificates` job has also been modified to check for certificates that are expiring in less than 15 days. If a certificate fails this check, that should suggest to us that something has gone wrong in our certificate rotation process.\\n","tokens":209,"id":180,"text":"## Context\\nOur certificate rotation was a largely manual process, involving an operator triggering a series of Concourse pipeline jobs in a particular sequence. We did not have a routine for doing rotations, and would typically only do them as part of a CF upgrade.\\nThe only means we had for knowing if a cert rotation was necessary was the `check-certificates` job, in the `create-cloudfoundry` Concourse pipeline, which would fail if any certificate had less than 30 days until it expired.\\nIn Q2 2019 (August\/September) we moved all of our platform secrets from AWS S3 to [Credhub](https:\/\/docs.cloudfoundry.org\/credhub\/). This covered third-party service credentials, platform passwords, and certificates. Since Credhub supports [certificate rotation](https:\/\/github.com\/pivotal-cf\/credhub-release\/blob\/master\/docs\/ca-rotation.md), we chose to implement automatic certificate rotation. This ADR contains details of how we did it.\\n\n\n##Decision\nCredhub has the notion of a transitional certificate. As written in [their documentation](https:\/\/github.com\/pivotal-cf\/credhub-release\/blob\/master\/docs\/ca-rotation.md), a transitional certificate is\\n> a new version that will not be used for signing yet, but can be added to your servers trusted certificate lists.\\nOur certificate rotation process is built around the setting and migration of the `transitional` flag, such that over a number of deployments an active certificate is retired and a new certificate is deployed, without downtime.\\nIn order to make certificate rotation automatic, and require no operator interaction, it is implemented as a job at the tail end of the `create-cloudfoundry` pipeline; after acceptance tests and before releases tagging.\\nThe new `rotate-certs` job has three tasks:\\n- `remove-transitional-flag-for-ca`\\n- `move-transitional-flag-for-ca`\\n- `set-transitional-flag-for-ca`\\nThese three tasks are in reverse order of the process for rotating a certificate. If the tasks were ordered normally, the first task would set up the state for the second, and the second would set up the state for the third, and Bosh would be unable to deploy the certificates without downtime. However, here the tasks are explained in the proper order to make it easier to understand how a certificate is rotated. To understand how it happens in the pipeline, assume a Bosh deploy happens between each step.\\n`set-transitional-flag-for-ca` is the first step in the process. It iterates through all CA certificates in Credhub, looking for any expiring under 30 days. Any that are, are regenerated as transitional certificates. This results in Credhub holding two certificates for the same credential name: the expiring certificate, and the new certificate with the `transitional` flag.\\n`move-transitional-flag-for-ca` is the second step in the process, and has two jobs:\\n1. It finds all CA certificates in Credhub which have 2 values, where the oldest certificate does not have the `transitional` flag and the newer one does. For each of those, it swaps the `transitional` flag to the older certificate. Finally, it looks for any leaf certificates signed by the CA certificate and regenerates them using the new CA certificate.\\n2. It looks for any leaf certificates that are expiring in less than 30 days and regenerates them. This is a one step process and they are deployed on the next Bosh deploy.\\n`remove-transitional-flag-for-ca` is the third and final step in the process. It iterates through all of the CA certificates in Credhub, looking for any with 2 values, where the older certificate is marked as `transitional` and the newer certificate is not. It then removes the `transitional` flag from the older certificate, which has the effect of dropping the certificate.\\nThe existing `check-certificates` job has also been modified to check for certificates that are expiring in less than 15 days. If a certificate fails this check, that should suggest to us that something has gone wrong in our certificate rotation process.\\n","MatchedIds":[841,3915,211,3144,3794],"Predicted":"## Decision\nTo streamline and secure our certificate management process, we have implemented automatic certificate rotation using Credhub's built-in support for certificate management. The steps and considerations taken to achieve this are outlined below:\n\n### Implementation Steps\n1. **Migration to Credhub:**\n   - All platform secrets, including third-party service credentials, platform passwords, and certificates, were migrated from AWS S3 to Credhub.\n\n2. **Credhub Integration:**\n   - Integrated Credhub with our Concourse pipelines to facilitate automatic certificate generation, storage, and rotation.\n\n3. **Pipeline Automation:**\n   - Developed a new set of Concourse jobs to manage certificate rotation automatically. These jobs are designed to:\n     1. Check for certificates approaching expiration.\n     2. Generate new certificates as needed.\n     3. Rotate existing certificates with minimal downtime.\n\n4. **Monitoring and Alerts:**\n   - Enhanced the `check-certificates` job in our `create-cloudfoundry` Concourse pipeline to monitor certificate expiration proactively.\n   - Configured alerts to notify operators of any issues in the certificate rotation process.\n\n5. **Testing and Validation:**\n   - Conducted thorough testing to ensure that the automatic rotations are seamless and do not disrupt platform services.\n   - Ensured that all certificates are correctly validated and applied across environments.\n\n### Considerations\n- **Security:** \n  - By centralizing our secrets management with Credhub, we improved the security of our sensitive information.\n  - Ensured that all communication with Credhub is encrypted and access-controlled.\n\n- **Reliability:**\n  - Established redundant checks and validations to ensure the reliability of automated rotations.\n  \n- **Documentation:**\n  - Updated internal documentation to reflect the automated processes and provide guidance on managing Credhub and Concourse integrations.\n\n### Benefits\n- **Reduced Manual Intervention:**\n  - The automation significantly reduces the need for manual interventions, lowering the risk of human error.\n  \n- **Proactive Management:**\n  - Automated and proactive monitoring ensures certificates are rotated well before expiration, maintaining platform integrity.\n\n- **Consistent Routine:**\n  - Regular and consistent certificate rotations as part of our automated workflows ensure that certificates are always up to date.\n\n### Future Improvements\n- Continuously monitor and adapt the automation to handle new certificate types and additional secrets that may be introduced.\n- Gather feedback and experiences to further refine and enhance the automated rotation process. \n\nBy implementing automatic certificate rotation using Credhub, we have significantly enhanced our platform's security posture and operational efficiency.","GenTime":"2024-08-20 00:31:02"}
{"File Name":"dddsample-pelargir\/0003-install-citerus-as-a-git-submodule.md","Context":"## Context\\nI can't be bothered to be installing the citerus library into my\\nrepositories all the time.\\nI want to keep my copy of that closely synchronized to the common\\nproject, so that any progress that is made can be incorporated\\ninto my work.\\nI want a convenient way to introduce fixes in the existing implementation\\nas I discover the need.\\nI want to keep the touch on the existing code _light_; I expect\\nto be making some fairly drastic changes in approach, and I don't\\nwant to be debating my choices with the maintainers.\\nI'm not currently comfortable working with maven projects where\\nthe elements are not rooted under a common location.\\n","Decision":"Add the upstream as a git submodule, so that it's clear precisely\\nwhich version of the upstream is in play at any given time.\\nChanges that are of general interest can be in that space.\\nMost radical changes (for a _gentle_ definition of radical) should\\nbe maintained elsewhere.\\n","tokens":144,"id":128,"text":"## Context\\nI can't be bothered to be installing the citerus library into my\\nrepositories all the time.\\nI want to keep my copy of that closely synchronized to the common\\nproject, so that any progress that is made can be incorporated\\ninto my work.\\nI want a convenient way to introduce fixes in the existing implementation\\nas I discover the need.\\nI want to keep the touch on the existing code _light_; I expect\\nto be making some fairly drastic changes in approach, and I don't\\nwant to be debating my choices with the maintainers.\\nI'm not currently comfortable working with maven projects where\\nthe elements are not rooted under a common location.\\n\n\n##Decision\nAdd the upstream as a git submodule, so that it's clear precisely\\nwhich version of the upstream is in play at any given time.\\nChanges that are of general interest can be in that space.\\nMost radical changes (for a _gentle_ definition of radical) should\\nbe maintained elsewhere.\\n","MatchedIds":[2626,1193,2701,1931,3483],"Predicted":"## Decision\nWe will fork the citerus library into our own repository. This allows us to make necessary changes and fixes as needed without waiting for approvals from the original maintainers. We will regularly sync our fork with the upstream repository to ensure that we are incorporating the latest changes and improvements from the common project. This approach will also let us have all elements rooted under a common location within our Maven project, which aligns with our current comfort levels and development practices.","GenTime":"2024-08-20 00:31:05"}
{"File Name":"blueprint\/0019-org-management.md","Context":"## Context and Problem Statement\\nManaging GitHub organizations via UI is restricted to few individuals only and does not offer any review mechanisms. The same apply to repository creation, organization membership management and label management. This ADR focuses on bringing transparency into this aspect of housekeeping within a community and aims to provide a way to individual contributors, so they can influence and propose changes themselves.\\nADR seeks a way to manage GitHub organization as a code, declaratively via GitOps as a natural extension of the core paradigm of Operate First movement.\\n## Decision Drivers\\nImprove transparency, provide auditing and reviews and allow individual contributors to propose changes.\\n","Decision":"Improve transparency, provide auditing and reviews and allow individual contributors to propose changes.\\nChosen option: _\"1. Prow's Peribolos\"_, because it's a declarative solution that is widely embraced in upstream communities like Kubernetes.\\n### Positive Consequences\\n* Repositories can be created declaratively.\\n* Even external users can request themselves to be added to the organization via a pull request.\\n* Teams can be created and maintained declaratively.\\n* Labels can be centrally managed in a unified way as well as updated per repository bases.\\n* Solution can be easily automated via post-submit Prow jobs.\\n### Negative Consequences\\n* Using a declarative configuration may not be as straightforward as clicking buttons in the UI.\\n* Some users can still do manual changes to repositories they own, these changes are undone by the automation.\\n","tokens":128,"id":3554,"text":"## Context and Problem Statement\\nManaging GitHub organizations via UI is restricted to few individuals only and does not offer any review mechanisms. The same apply to repository creation, organization membership management and label management. This ADR focuses on bringing transparency into this aspect of housekeeping within a community and aims to provide a way to individual contributors, so they can influence and propose changes themselves.\\nADR seeks a way to manage GitHub organization as a code, declaratively via GitOps as a natural extension of the core paradigm of Operate First movement.\\n## Decision Drivers\\nImprove transparency, provide auditing and reviews and allow individual contributors to propose changes.\\n\n\n##Decision\nImprove transparency, provide auditing and reviews and allow individual contributors to propose changes.\\nChosen option: _\"1. Prow's Peribolos\"_, because it's a declarative solution that is widely embraced in upstream communities like Kubernetes.\\n### Positive Consequences\\n* Repositories can be created declaratively.\\n* Even external users can request themselves to be added to the organization via a pull request.\\n* Teams can be created and maintained declaratively.\\n* Labels can be centrally managed in a unified way as well as updated per repository bases.\\n* Solution can be easily automated via post-submit Prow jobs.\\n### Negative Consequences\\n* Using a declarative configuration may not be as straightforward as clicking buttons in the UI.\\n* Some users can still do manual changes to repositories they own, these changes are undone by the automation.\\n","MatchedIds":[1614,1431,1563,3915,2725],"Predicted":"## Decision\nWe will adopt a GitOps approach to managing GitHub organizations, repositories, membership, and labels. The implementation will use declarative configurations stored in a dedicated repository within the GitHub organization. \n\n### Key Steps and Guidelines:\n1. **Declarative Configuration Repository**:\n    - Create a dedicated repository named `org-management-config` within the GitHub organization.\n    - This repository will store YAML or JSON configuration files that define the state of the organization's repositories, membership, and labels.\n\n2. **Configuration Structure**:\n    - Define a clear and consistent structure for configuration files.\n    - For example, separate directories for each aspect:\n        - `repositories\/`: Config for repository creation and management.\n        - `memberships\/`: Config for organization membership management.\n        - `labels\/`: Config for label management across repositories.\n\n3. **Automated Sync Process**:\n    - Set up a Continuous Integration\/Continuous Deployment (CI\/CD) pipeline using GitHub Actions or similar tooling to apply the configurations to the GitHub organization.\n    - The pipeline should run automatically on changes to the `org-management-config` repository.\n\n4. **Review and Approval Workflow**:\n    - Implement a pull request (PR) based workflow for changes to the configuration repository.\n    - Require reviews and approvals from designated maintainers before merging changes.\n\n5. **Auditing and Transparency**:\n    - Enable audit logging for all changes to the configurations and applied states.\n    - Provide an accessible audit trail within the `org-management-config` repository or an associated logging system.\n\n6. **Access Control**:\n    - Limit direct access to manage organization settings via the GitHub UI.\n    - Ensure that organization owners and a selected group of maintainers have permissions to approve PRs in the configuration repository.\n\n### Implementation Tools:\n- **GitHub Actions**: To automate the application of configurations.\n- **Terraform or other Infrastructure as Code (IaC) tools**: To manage GitHub resources declaratively.\n\nBy implementing this decision, we will achieve improved transparency, robust auditing, and an inclusive process for managing GitHub organizational settings, aligned with the principles of the Operate First movement.","GenTime":"2024-08-20 00:31:15"}
{"File Name":"arch\/0045-reuse-python-custom-libs.md","Context":"## Context\\n\u6211\u4eec\u6709\u591a\u4e2a\u9879\u76ee\u4f7f\u7528 Python \u5f00\u53d1\uff0c\u968f\u7740\u9879\u76ee\u7684\u53d1\u5c55\uff0c\u5927\u5bb6\u4e5f\u5199\u4e86\u8bb8\u591a\u7684\u5e93\uff0c\u6bd4\u5982\uff0cprice\u3001sms\u3001mail \u7b49\u3002\u800c\u5176\u4ed6\u9879\u76ee\u4e5f\u6709\u8fd9\u6837\u7684\u9700\u6c42\uff0c\u5f53\u524d\u9879\u76ee\u4e4b\u95f4\u662f\u901a\u8fc7\u62f7\u8d1d\u7684\u65b9\u5f0f\u8fdb\u884c\u590d\u7528\uff0c\u4e0d\u662f\u5e93\u8fd8\u5b58\u5728\u9879\u76ee\u5185\u72ec\u81ea\u81ea\u884c\u66f4\u65b0\u3002\u8fd9\u5c31\u5bfc\u81f4\u9879\u76ee\u4e4b\u95f4\u6240\u4f7f\u7528\u7684\u5e93\u4ea7\u751f\u4e0d\u4e00\u81f4\uff0c\u5e76\u91cd\u590d\u9020\u4e86\u5f88\u591a\u7684\u8f6e\u5b50\u3002\\n","Decision":"1. \u6784\u5efa\u81ea\u5df1\u7684 pypi \u670d\u52a1\u5668\uff1b\\n* \u4e0d\u53ea\u53ef\u4ee5\u89e3\u51b3\u81ea\u5efa\u5e93\u7684\u590d\u7528\u95ee\u9898\uff1b\\n* \u4e5f\u53ef\u4ee5\u5c06\u6211\u4eec\u7684\u5e38\u7528\u5e93\u7f13\u5b58\uff0c\u52a0\u901f pip \u7684\u5b89\u88c5\uff1b\\n* \u6709\u7ef4\u62a4\u6210\u672c\u3002\\n2. \u4f7f\u7528 git \u4f5c\u4e3a pip \u5b89\u88c5\u5305\\n* \u5355 repo \u5355 package\uff1a\u6240\u6709 libs \u805a\u5408\u5728\u4e00\u4e2a package \u91cc\uff0c\u5bf9\u4e8e\u5927\u91cf\u7b80\u5355 lib \u53ef\u884c\uff1b\\n* \u5355 repo \u591a package\uff1a\u5206\u76ee\u5f55\u6784\u5efa package\uff0c\u9002\u5408\u7a0d\u5fae\u5927\u4e9b\u7684 libs \u4e4b\u95f4\u505a\u9694\u79bb\uff1b\\n* \u591a repo \u591a package\uff1a\u6bcf\u4e2a repo \u662f\u4e00\u4e2a package\uff0c\u9002\u5408\u5927\u7684 lib \u5e93\uff0clib \u7c92\u5ea6\u8fc7\u7ec6\u4f1a\u6709\u5e93\u7ba1\u7406\u95ee\u9898\u3002\\n3. \u4f7f\u7528 Artifactory\uff0cPro+ \u7248\u672c\u624d\u63d0\u4f9b pypi \u670d\u52a1\u3002\\n\u9274\u4e8e\u5f53\u524d lib \u5e76\u4e0d\u591a\uff0c\u800c\u4e14\u6bcf\u4e2a lib \u5f88\u7b80\u5355\uff0c\u6545\u9009\u62e9\u5355 repo \u5355 package \u65b9\u6848\u3002\\n","tokens":119,"id":2446,"text":"## Context\\n\u6211\u4eec\u6709\u591a\u4e2a\u9879\u76ee\u4f7f\u7528 Python \u5f00\u53d1\uff0c\u968f\u7740\u9879\u76ee\u7684\u53d1\u5c55\uff0c\u5927\u5bb6\u4e5f\u5199\u4e86\u8bb8\u591a\u7684\u5e93\uff0c\u6bd4\u5982\uff0cprice\u3001sms\u3001mail \u7b49\u3002\u800c\u5176\u4ed6\u9879\u76ee\u4e5f\u6709\u8fd9\u6837\u7684\u9700\u6c42\uff0c\u5f53\u524d\u9879\u76ee\u4e4b\u95f4\u662f\u901a\u8fc7\u62f7\u8d1d\u7684\u65b9\u5f0f\u8fdb\u884c\u590d\u7528\uff0c\u4e0d\u662f\u5e93\u8fd8\u5b58\u5728\u9879\u76ee\u5185\u72ec\u81ea\u81ea\u884c\u66f4\u65b0\u3002\u8fd9\u5c31\u5bfc\u81f4\u9879\u76ee\u4e4b\u95f4\u6240\u4f7f\u7528\u7684\u5e93\u4ea7\u751f\u4e0d\u4e00\u81f4\uff0c\u5e76\u91cd\u590d\u9020\u4e86\u5f88\u591a\u7684\u8f6e\u5b50\u3002\\n\n\n##Decision\n1. \u6784\u5efa\u81ea\u5df1\u7684 pypi \u670d\u52a1\u5668\uff1b\\n* \u4e0d\u53ea\u53ef\u4ee5\u89e3\u51b3\u81ea\u5efa\u5e93\u7684\u590d\u7528\u95ee\u9898\uff1b\\n* \u4e5f\u53ef\u4ee5\u5c06\u6211\u4eec\u7684\u5e38\u7528\u5e93\u7f13\u5b58\uff0c\u52a0\u901f pip \u7684\u5b89\u88c5\uff1b\\n* \u6709\u7ef4\u62a4\u6210\u672c\u3002\\n2. \u4f7f\u7528 git \u4f5c\u4e3a pip \u5b89\u88c5\u5305\\n* \u5355 repo \u5355 package\uff1a\u6240\u6709 libs \u805a\u5408\u5728\u4e00\u4e2a package \u91cc\uff0c\u5bf9\u4e8e\u5927\u91cf\u7b80\u5355 lib \u53ef\u884c\uff1b\\n* \u5355 repo \u591a package\uff1a\u5206\u76ee\u5f55\u6784\u5efa package\uff0c\u9002\u5408\u7a0d\u5fae\u5927\u4e9b\u7684 libs \u4e4b\u95f4\u505a\u9694\u79bb\uff1b\\n* \u591a repo \u591a package\uff1a\u6bcf\u4e2a repo \u662f\u4e00\u4e2a package\uff0c\u9002\u5408\u5927\u7684 lib \u5e93\uff0clib \u7c92\u5ea6\u8fc7\u7ec6\u4f1a\u6709\u5e93\u7ba1\u7406\u95ee\u9898\u3002\\n3. \u4f7f\u7528 Artifactory\uff0cPro+ \u7248\u672c\u624d\u63d0\u4f9b pypi \u670d\u52a1\u3002\\n\u9274\u4e8e\u5f53\u524d lib \u5e76\u4e0d\u591a\uff0c\u800c\u4e14\u6bcf\u4e2a lib \u5f88\u7b80\u5355\uff0c\u6545\u9009\u62e9\u5355 repo \u5355 package \u65b9\u6848\u3002\\n","MatchedIds":[1477,3483,864,400,4868],"Predicted":"## Decision\n\u4e3a\u4e86\u907f\u514d\u5e93\u7684\u4e0d\u4e00\u81f4\u6027\u548c\u91cd\u590d\u5f00\u53d1\u7684\u60c5\u5f62\uff0c\u6211\u4eec\u51b3\u5b9a\u5c06\u8fd9\u4e9b\u901a\u7528\u5e93\u63d0\u53d6\u51fa\u6765\uff0c\u5e76\u53d1\u5e03\u5230\u5185\u90e8\u7684\u79c1\u6709 Python \u5305\u7ba1\u7406\u4ed3\u5e93\uff08\u5982 PyPI \u6216 Artifactory\uff09\u3002\u901a\u8fc7\u8fd9\u79cd\u65b9\u5f0f\uff0c\u6240\u6709\u9879\u76ee\u53ef\u4ee5\u901a\u8fc7\u4f9d\u8d56\u5305\u7684\u7248\u672c\u6765\u786e\u4fdd\u4e00\u81f4\u6027\uff0c\u800c\u4e0d\u662f\u901a\u8fc7\u62f7\u8d1d\u7684\u65b9\u5f0f\u8fdb\u884c\u590d\u7528\u3002\u8fd9\u5c06\u63d0\u5347\u4ee3\u7801\u7684\u53ef\u7ef4\u62a4\u6027\u548c\u4e00\u81f4\u6027\uff0c\u540c\u65f6\u51cf\u5c11\u91cd\u590d\u5f00\u53d1\u7684\u5de5\u4f5c\u91cf\u3002","GenTime":"2024-08-20 00:31:17"}
{"File Name":"archcolider\/010 Feedback System separation.md","Context":"## Context\\nA user would like to read or write a review for a meal that he has ordered. Businesses would like to obtain feedback on its service in order to improve it.\\nTwo different feedback methods:\\n- surveys (occasional questionnaires about general aspects of the app\/service)\\n- feedback (is an opinion about an order or app\/service)\\n","Decision":"We'll create a simple in app feedback system that will allow users to provide feedback about orders and service.\\nWe'll incorporate surveys into feedback in order to take advantage of 3rd parties services which specialize in feedback acquisitions such survey monkey, google surveys, etc..\\n","tokens":74,"id":2681,"text":"## Context\\nA user would like to read or write a review for a meal that he has ordered. Businesses would like to obtain feedback on its service in order to improve it.\\nTwo different feedback methods:\\n- surveys (occasional questionnaires about general aspects of the app\/service)\\n- feedback (is an opinion about an order or app\/service)\\n\n\n##Decision\nWe'll create a simple in app feedback system that will allow users to provide feedback about orders and service.\\nWe'll incorporate surveys into feedback in order to take advantage of 3rd parties services which specialize in feedback acquisitions such survey monkey, google surveys, etc..\\n","MatchedIds":[306,307,2678,2672,647],"Predicted":"## Decision\nImplement two separate mechanisms for collecting user feedback:\n1. **Surveys**: Utilize a survey tool to periodically distribute questionnaires to users. This can be achieved through email, in-app notifications, or push notifications to gather general feedback on various aspects of the app\/service.\n2. **Feedback**: Add a feature within the app that allows users to leave reviews specifically tied to their orders. After a meal delivery, prompt the user to provide a rating and write a review about the specific meal they have received. This feature would include the option to provide feedback on the overall app\/service experience.\n\nThis dual approach ensures both structured periodic feedback through surveys and immediate, specific feedback tied directly to user orders and experiences.","GenTime":"2024-08-20 00:31:20"}
{"File Name":"deeplearning4j\/0007 - Nd4j classifiers.md","Context":"## Context\\nNd4j relies upon the c++ library [libnd4j](..\/libnd4j) for native math execution.\\nIt uses [javacpp](https:\/\/github.com\/bytedeco\/javacpp) to link against\\nlibnd4j. Libnd4j is capable of being compiled a myriad of different ways allowing different trade offs to be made\\nin terms of performance and dependencies. This presents complexity in exposing this flexibility to the end user.\\n","Decision":"In order to allow users to pick which configuration they would like to use, while avoiding adding a lot of different artifact\\nids to the project, the following javacpp platform extensions are used:\\ncompiled type (avx etc or blank if normal) - software linked against (cudnn, onednn, armcompute) - version\\nAn example for the one dnn platform extension could be:\\ndnnl-2.2\\navx256-dnnl-2.2\\nThis presents 2 examples where a special compilation is enabled and one where it's not\\nboth linking against dnnl\/onednn 2.2.\\n","tokens":99,"id":2932,"text":"## Context\\nNd4j relies upon the c++ library [libnd4j](..\/libnd4j) for native math execution.\\nIt uses [javacpp](https:\/\/github.com\/bytedeco\/javacpp) to link against\\nlibnd4j. Libnd4j is capable of being compiled a myriad of different ways allowing different trade offs to be made\\nin terms of performance and dependencies. This presents complexity in exposing this flexibility to the end user.\\n\n\n##Decision\nIn order to allow users to pick which configuration they would like to use, while avoiding adding a lot of different artifact\\nids to the project, the following javacpp platform extensions are used:\\ncompiled type (avx etc or blank if normal) - software linked against (cudnn, onednn, armcompute) - version\\nAn example for the one dnn platform extension could be:\\ndnnl-2.2\\navx256-dnnl-2.2\\nThis presents 2 examples where a special compilation is enabled and one where it's not\\nboth linking against dnnl\/onednn 2.2.\\n","MatchedIds":[2945,2934,2933,2944,1035],"Predicted":"## Decision\nTo balance performance and dependency management while maintaining flexibility for the end-user, we will introduce a configuration system to manage the compilation options of `libnd4j`. The key aspects of this decision are:\n\n1. **Predefined Configuration Profiles**: Provide a set of predefined profiles (e.g., `performance`, `minimal-dependencies`, `balanced`) that compile `libnd4j` with different trade-offs. This simplifies choice for most users.\n   \n2. **Custom Configuration**: Allow advanced users to specify custom compilation flags via a configuration file or through environment variables. This grants users the ability to fine-tune the library according to their specific needs.\n\n3. **Build-Time Integration**: Integrate these configuration options with the build system, ensuring that the correct version of `libnd4j` is compiled and linked. This can be managed via CMake options or similar build tools.\n\n4. **Documentation and Tooling**: Provide comprehensive documentation and tooling to guide users through the process of selecting and customizing their configurations. This should include examples and recommended settings for common use cases.\n\n5. **Fallback Mechanism**: Implement a fallback mechanism to default to a safe, general-purpose configuration if the user does not specify any preferences. This ensures a smooth out-of-the-box experience.\n\nBy introducing a flexible yet user-friendly configuration system, we can cater to both novice and advanced users, ensuring that they can make informed decisions based on their performance and dependency requirements.","GenTime":"2024-08-20 00:31:26"}
{"File Name":"paas-team-manual\/ADR023-idle-cpu-alerting-change.html.md","Context":"## Context\\nWith the implementation of ADR021 we have reduced the number of cells in\\nproduction in order to make more efficent use of our budget. This in turn means\\nthat we have increased the load on the individual cells. Originally the idle CPU\\nmonitor was set in line with the free memory on cells monitor (for alerting on\\na need to scale the cell pool), however CPU usage does not appear to grow\\nlinearly with allocated memory for tenant applications.\\n","Decision":"In order to avoid false positives from triggering due to CPU load spiking rather\\nthan being a constant level we will increase the monitoring window to 24 hours.\\nBased upon examining our CPU idle load in ADR021, we will reduce the CPU idle\\nthresholds to warn at 37% and become critical at 33%.\\n","tokens":98,"id":215,"text":"## Context\\nWith the implementation of ADR021 we have reduced the number of cells in\\nproduction in order to make more efficent use of our budget. This in turn means\\nthat we have increased the load on the individual cells. Originally the idle CPU\\nmonitor was set in line with the free memory on cells monitor (for alerting on\\na need to scale the cell pool), however CPU usage does not appear to grow\\nlinearly with allocated memory for tenant applications.\\n\n\n##Decision\nIn order to avoid false positives from triggering due to CPU load spiking rather\\nthan being a constant level we will increase the monitoring window to 24 hours.\\nBased upon examining our CPU idle load in ADR021, we will reduce the CPU idle\\nthresholds to warn at 37% and become critical at 33%.\\n","MatchedIds":[231,3469,200,2440,620],"Predicted":"## Decision\nWe will decouple the scaling criteria for CPU and memory usage. Specifically, we will implement separate monitoring and alerting thresholds for CPU and memory usage on cells. \n\n- For CPU, we will set an alert threshold when CPU usage exceeds 70% sustained over a 15-minute period.\n- For memory, we will set an alert threshold when free memory on cells drops below 20%.\n\nThis will ensure that we accurately reflect the resource consumption patterns of tenant applications and provide more targeted scaling actions.\n\n## Status\nAccepted\n\n## Consequences\n* We will develop and deploy new monitoring configurations for CPU and memory usage separately.\n* We will create a new set of scaling policies and automation scripts to handle CPU and memory scaling independently.\n* This may lead to scaling actions being triggered more frequently, but it will more accurately match our infrastructure resources to application demands.\n* We will need to monitor the new alerts for any false positives and adjust thresholds if necessary.\n* The infrastructure team will continuously assess the resource usage patterns and adjust scaling policies as needed to optimize both performance and budget.\n","GenTime":"2024-08-20 00:31:31"}
{"File Name":"docspell\/0011_extract_text.md","Context":"# Context and Problem Statement\\nWith support for more file types there must be a way to extract text\\nfrom all of them. It is better to extract text from the source files,\\nin contrast to extracting the text from the converted pdf file.\\nThere are multiple options and multiple file types. Again, most\\npriority is to use a java\/scala library to reduce external\\ndependencies.\\n# Considered Options\\n","Decision":"- MS Office files: POI library\\n- Open Document files: Tika, but integrating the few source files that\\nmake up the open document parser. Due to its huge dependency tree,\\nthe library is not added.\\n- PDF: Apache PDFBox. I know this library better than itext.\\n","tokens":86,"id":282,"text":"# Context and Problem Statement\\nWith support for more file types there must be a way to extract text\\nfrom all of them. It is better to extract text from the source files,\\nin contrast to extracting the text from the converted pdf file.\\nThere are multiple options and multiple file types. Again, most\\npriority is to use a java\/scala library to reduce external\\ndependencies.\\n# Considered Options\\n\n\n##Decision\n- MS Office files: POI library\\n- Open Document files: Tika, but integrating the few source files that\\nmake up the open document parser. Due to its huge dependency tree,\\nthe library is not added.\\n- PDF: Apache PDFBox. I know this library better than itext.\\n","MatchedIds":[284,273,287,445,2625],"Predicted":"# Considered Options\n* [Apache Tika](https:\/\/tika.apache.org\/) library\n* [PDFBox](https:\/\/pdfbox.apache.org\/) library\n* [POI](https:\/\/poi.apache.org\/) library\n* [iText](https:\/\/itextpdf.com) library\n* Custom solutions using regular expressions and plaintext parsing.\n\n## Decision\nApache Tika.\n\nApache Tika is chosen because it is a comprehensive toolkit for extracting text, metadata, and other information from a wide variety of file types. It supports many file formats natively and integrates well with other Java applications. By using Tika, we can streamline the text extraction process across different file types, reducing the need for multiple specialized libraries and thereby minimizing external dependencies. \n\nWhile PDFBox, POI, and iText are excellent libraries, they are more specialized towards specific file types. For an all-encompassing solution that can handle multiple file formats, Apache Tika is the best fit. It also avoids the need for custom text extraction logic, ensuring robustness and maintainability.","GenTime":"2024-08-20 00:31:35"}
{"File Name":"monocle\/0008-unify-idents-across-code-review-systems.md","Context":"## Context and Problem Statement\\nHow do we merge contributor identities across code review systems ?\\n## Decision Drivers\\n* Do not complexify the EL query system\\n* Identities can be configured via the Monocle configuration file\\n","Decision":"* Do not complexify the EL query system\\n* Identities can be configured via the Monocle configuration file\\nChosen option: \"[option 2]\", Simpler implementation, better query performance (no complexification of queries) and no potential metrics errorneous results returned compared\\nto \"[option 1]\".\\nHere is a configuration example.\\n```\\nidents:\\n- ident: John Doe\\naliases:\\n- github.com\/john-doe\\n- review.opendev.org\/John Doe\/12345\\n```\\n### Negative Consequences\\n* A tool need to crawl the whole index to discover objects to update. However this\\noperation should not happen often.\\n","tokens":47,"id":4645,"text":"## Context and Problem Statement\\nHow do we merge contributor identities across code review systems ?\\n## Decision Drivers\\n* Do not complexify the EL query system\\n* Identities can be configured via the Monocle configuration file\\n\n\n##Decision\n* Do not complexify the EL query system\\n* Identities can be configured via the Monocle configuration file\\nChosen option: \"[option 2]\", Simpler implementation, better query performance (no complexification of queries) and no potential metrics errorneous results returned compared\\nto \"[option 1]\".\\nHere is a configuration example.\\n```\\nidents:\\n- ident: John Doe\\naliases:\\n- github.com\/john-doe\\n- review.opendev.org\/John Doe\/12345\\n```\\n### Negative Consequences\\n* A tool need to crawl the whole index to discover objects to update. However this\\noperation should not happen often.\\n","MatchedIds":[4857,3214,2582,3421,4166],"Predicted":"## Decision\nIdentities across different code review systems will be merged by configuring them via the Monocle configuration file. This approach ensures that the EL query system remains uncomplexified as it does not introduce additional data processing steps within the query layer. Configuring identities through the Monocle configuration file provides a centralized and manageable way to handle identity mapping.\n\n### Positive Consequences\n* The EL query system remains straightforward and performant since no additional complexity is introduced.\n* Centralized management of identities via the configuration file simplifies administrative tasks and minimizes error potential.\n* Flexibility to easily update and modify identity mappings as needed through configuration changes.\n\n### Negative Consequences\n* Initial setup might require detailed mapping and thorough verification of identities in the configuration file.\n* Changes to identity mappings necessitate edits to the configuration file, requiring careful version control and testing to ensure consistency.\n\nBy utilizing the Monocle configuration file for identity management, we achieve a balance between simplicity and flexibility, catering to the decision drivers identified.","GenTime":"2024-08-20 00:31:44"}
{"File Name":"sre-challenge\/0008-use-taurus-for-benchmark-tests.md","Context":"## Context\\nTo perform benchmarking tests, I want to set some resources capable to generate\\na high volume of requests against the cluster.\\nTo do this, I could use several tools, like JMeter, Garret, or Taurus.\\nBut I like Blazemeter's Taurus over all because it is very simple to setup and\\nexecute tests.\\nWith Taurus you can create JMeter tests, or use other tools like Selenium, in\\norder to execute and collect results.\\nThen It generates a report that can be easily imported in a JMeter tool or\\nopen it in a data analysis tool like jupyter notebook to draw new diagrams.\\n","Decision":"Create an instance with an easy test (Just request the keys and the metrics).\\n","tokens":137,"id":2998,"text":"## Context\\nTo perform benchmarking tests, I want to set some resources capable to generate\\na high volume of requests against the cluster.\\nTo do this, I could use several tools, like JMeter, Garret, or Taurus.\\nBut I like Blazemeter's Taurus over all because it is very simple to setup and\\nexecute tests.\\nWith Taurus you can create JMeter tests, or use other tools like Selenium, in\\norder to execute and collect results.\\nThen It generates a report that can be easily imported in a JMeter tool or\\nopen it in a data analysis tool like jupyter notebook to draw new diagrams.\\n\n\n##Decision\nCreate an instance with an easy test (Just request the keys and the metrics).\\n","MatchedIds":[4868,4785,3004,1681,1012],"Predicted":"## Decision\nWe will utilize Blazemeter's Taurus for our benchmarking tests due to its simplicity in setup and execution. Taurus supports creating tests with JMeter and integrating with other tools like Selenium, providing versatility in how we execute and collect our benchmarking results.\n\nAdditionally, Taurus produces detailed reports that can be easily imported into JMeter or analyzed further using data analysis tools such as Jupyter Notebook. This allows us to generate comprehensive insights and visualizations of our benchmarking data.\n\nBy choosing Taurus, we ensure a streamlined and efficient process for setting up high-volume request generation, executing tests, and analyzing the results, ultimately leading to more effective performance benchmarking of our cluster.","GenTime":"2024-08-20 00:31:50"}
{"File Name":"moneycount-api\/003-Document_API_With_Swagger.md","Context":"## Context\\nOffer an API without a good documentation is not a choice. Document this API with text documents is not efficient, and as the code changes more and more effort are necessary to keep it up-to-date. It's necessary to document APIs in a simply and efficient way, preferably in a way that developers can test it.\\n","Decision":"I decided to use Swagger as a documentation tool for the APIs, as it is a de facto standard. I choosed springfox-swagger2 and springfox-swagger-ui because of its smooth integration with Spring Boot.\\n","tokens":67,"id":3069,"text":"## Context\\nOffer an API without a good documentation is not a choice. Document this API with text documents is not efficient, and as the code changes more and more effort are necessary to keep it up-to-date. It's necessary to document APIs in a simply and efficient way, preferably in a way that developers can test it.\\n\n\n##Decision\nI decided to use Swagger as a documentation tool for the APIs, as it is a de facto standard. I choosed springfox-swagger2 and springfox-swagger-ui because of its smooth integration with Spring Boot.\\n","MatchedIds":[2181,693,1180,1410,1416],"Predicted":"## Decision\nWe will use the [OpenAPI 3.0](https:\/\/swagger.io\/specification\/) specification to document our APIs. This allows for a standardized, human-readable, and machine-readable format that enables easy testing and automatic client generation.\n\n### Implementation Details\n1. **Tooling**: \n   - Adopt Swagger UI for interactive documentation.\n   - Utilize tools like Swagger Editor or ReDoc for managing and visualizing API documentation.\n\n2. **Integration with Code**: \n   - Use libraries or frameworks specific to our programming language to auto-generate OpenAPI annotations from code (e.g., springdoc-openapi for Java Spring).\n   - Ensure that documentation is auto-generated and kept in sync with the codebase through CI\/CD pipelines.\n\n3. **Testing APIs**: \n   - Leverage tools like Postman or Swagger UI to allow developers to test APIs interactively, directly from the documentation.\n\n### Positive Consequences\n* Easy to maintain up-to-date documentation linked to code changes.\n* Improved developer experience through interactive API documentation and testing capabilities.\n* Facilitates automatic generation of client libraries and server stubs, reducing manual coding effort.\n\n### Negative Consequences\n* Initial setup and integration effort with existing codebase.\n* Training team members on how to use and maintain OpenAPI annotations and documentation tools.\n\nBy adopting the OpenAPI 3.0 specification, we facilitate more efficient and reliable documentation and testing processes for our APIs.","GenTime":"2024-08-20 00:31:57"}
{"File Name":"ibc-go\/adr-027-ibc-wasm.md","Context":"## Context\\nCurrently in ibc-go light clients are defined as part of the codebase and are implemented as modules under\\n`modules\/light-clients`. Adding support for new light clients or updating an existing light client in the event\\nof a security issue or consensus update is a multi-step process which is both time-consuming and error-prone.\\nIn order to enable new IBC light client implementations it is necessary to modify the codebase of ibc-go (if the light\\nclient is part of its codebase), re-build chains' binaries, pass a governance proposal and validators upgrade their nodes.\\nAnother problem stemming from the above process is that if a chain wants to upgrade its own consensus, it will\\nneed to convince every chain or hub connected to it to upgrade its light client in order to stay connected. Due\\nto the time consuming process required to upgrade a light client, a chain with lots of connections needs to be\\ndisconnected for quite some time after upgrading its consensus, which can be very expensive in terms of time and effort.\\nWe are proposing simplifying this workflow by integrating a Wasm light client module that makes adding support for\\nnew light clients a simple governance-gated transaction. The light client bytecode, written in Wasm-compilable Rust,\\nruns inside a Wasm VM. The Wasm light client submodule exposes a proxy light client interface that routes incoming\\nmessages to the appropriate handler function, inside the Wasm VM for execution.\\nWith the Wasm light client module, anybody can add new IBC light client in the form of Wasm bytecode (provided they are\\nable to submit the governance proposal transaction and that it passes) as well as instantiate clients using any created\\nclient type. This allows any chain to update its own light client in other chains without going through the steps outlined above.\\n","Decision":"We decided to implement the Wasm light client module as a light client proxy that will interface with the actual light client\\nuploaded as Wasm bytecode. To enable usage of the Wasm light client module, users need to add it to the list of allowed clients\\nby updating the `AllowedClients` parameter in the 02-client submodule of core IBC.\\n```go\\nparams := clientKeeper.GetParams(ctx)\\nparams.AllowedClients = append(params.AllowedClients, exported.Wasm)\\nclientKeeper.SetParams(ctx, params)\\n```\\nAdding a new light client contract is governance-gated. To upload a new light client users need to submit\\na [governance v1 proposal](https:\/\/docs.cosmos.network\/main\/modules\/gov#proposals) that contains the `sdk.Msg` for storing\\nthe Wasm contract's bytecode. The required message is `MsgStoreCode` and the bytecode is provided in the field `wasm_byte_code`:\\n```proto\\n\/\/ MsgStoreCode defines the request type for the StoreCode rpc.\\nmessage MsgStoreCode {\\n\/\/ signer address\\nstring signer = 1;\\n\/\/ wasm byte code of light client contract. It can be raw or gzip compressed\\nbytes wasm_byte_code = 2;\\n}\\n```\\nThe RPC handler processing `MsgStoreCode` will make sure that the signer of the message matches the address of authority allowed to\\nsubmit this message (which is normally the address of the governance module).\\n```go\\n\/\/ StoreCode defines a rpc handler method for MsgStoreCode\\nfunc (k Keeper) StoreCode(goCtx context.Context, msg *types.MsgStoreCode) (*types.MsgStoreCodeResponse, error) {\\nif k.GetAuthority() != msg.Signer {\\nreturn nil, errorsmod.Wrapf(ibcerrors.ErrUnauthorized, \"expected %s, got %s\", k.GetAuthority(), msg.Signer)\\n}\\nctx := sdk.UnwrapSDKContext(goCtx)\\nchecksum, err := k.storeWasmCode(ctx, msg.WasmByteCode, ibcwasm.GetVM().StoreCode)\\nif err != nil {\\nreturn nil, errorsmod.Wrap(err, \"failed to store wasm bytecode\")\\n}\\nemitStoreWasmCodeEvent(ctx, checksum)\\nreturn &types.MsgStoreCodeResponse{\\nChecksum: checksum,\\n}, nil\\n}\\n```\\nThe contract's bytecode is not stored in state (it is actually unnecessary and wasteful to store it, since\\nthe Wasm VM already stores it and can be queried back, if needed). The checksum is simply the hash of the bytecode\\nof the contract and it is stored in state in an entry with key `checksums` that contains the checksums for the bytecodes that have been stored.\\n### How light client proxy works?\\nThe light client proxy behind the scenes will call a CosmWasm smart contract instance with incoming arguments serialized\\nin JSON format with appropriate environment information. Data returned by the smart contract is deserialized and\\nreturned to the caller.\\nConsider the example of the `VerifyClientMessage` function of `ClientState` interface. Incoming arguments are\\npackaged inside a payload object that is then JSON serialized and passed to `queryContract`, which executes `WasmVm.Query`\\nand returns the slice of bytes returned by the smart contract. This data is deserialized and passed as return argument.\\n```go\\ntype QueryMsg struct {\\nStatus               *StatusMsg               `json:\"status,omitempty\"`\\nExportMetadata       *ExportMetadataMsg       `json:\"export_metadata,omitempty\"`\\nTimestampAtHeight    *TimestampAtHeightMsg    `json:\"timestamp_at_height,omitempty\"`\\nVerifyClientMessage  *VerifyClientMessageMsg  `json:\"verify_client_message,omitempty\"`\\nCheckForMisbehaviour *CheckForMisbehaviourMsg `json:\"check_for_misbehaviour,omitempty\"`\\n}\\ntype verifyClientMessageMsg struct {\\nClientMessage *ClientMessage `json:\"client_message\"`\\n}\\n\/\/ VerifyClientMessage must verify a ClientMessage.\\n\/\/ A ClientMessage could be a Header, Misbehaviour, or batch update.\\n\/\/ It must handle each type of ClientMessage appropriately.\\n\/\/ Calls to CheckForMisbehaviour, UpdateSta\u00e5te, and UpdateStateOnMisbehaviour\\n\/\/ will assume that the content of the ClientMessage has been verified\\n\/\/ and can be trusted. An error should be returned\\n\/\/ if the ClientMessage fails to verify.\\nfunc (cs ClientState) VerifyClientMessage(\\nctx sdk.Context,\\n_ codec.BinaryCodec,\\nclientStore storetypes.KVStore,\\nclientMsg exported.ClientMessage\\n) error {\\nclientMessage, ok := clientMsg.(*ClientMessage)\\nif !ok {\\nreturn errorsmod.Wrapf(ibcerrors.ErrInvalidType, \"expected type: %T, got: %T\", &ClientMessage{}, clientMsg)\\n}\\npayload := QueryMsg{\\nVerifyClientMessage: &VerifyClientMessageMsg{ClientMessage: clientMessage.Data},\\n}\\n_, err := wasmQuery[EmptyResult](ctx, clientStore, &cs, payload)\\nreturn err\\n}\\n```\\n### Global Wasm VM variable\\nThe 08-wasm keeper structure keeps a reference to the Wasm VM instantiated in the keeper constructor function. The keeper uses\\nthe Wasm VM to store the bytecode of light client contracts. However, the Wasm VM is also needed in the 08-wasm implementations of\\nsome of the `ClientState` interface functions to initialise a contract, execute calls on the contract and query the contract. Since\\nthe `ClientState` functions do not have access to the 08-wasm keeper, then it has been decided to keep a global pointer variable that\\npoints to the same instance as the one in the 08-wasm keeper. This global pointer variable is then used in the implementations of\\nthe `ClientState` functions.\\n","tokens":374,"id":1441,"text":"## Context\\nCurrently in ibc-go light clients are defined as part of the codebase and are implemented as modules under\\n`modules\/light-clients`. Adding support for new light clients or updating an existing light client in the event\\nof a security issue or consensus update is a multi-step process which is both time-consuming and error-prone.\\nIn order to enable new IBC light client implementations it is necessary to modify the codebase of ibc-go (if the light\\nclient is part of its codebase), re-build chains' binaries, pass a governance proposal and validators upgrade their nodes.\\nAnother problem stemming from the above process is that if a chain wants to upgrade its own consensus, it will\\nneed to convince every chain or hub connected to it to upgrade its light client in order to stay connected. Due\\nto the time consuming process required to upgrade a light client, a chain with lots of connections needs to be\\ndisconnected for quite some time after upgrading its consensus, which can be very expensive in terms of time and effort.\\nWe are proposing simplifying this workflow by integrating a Wasm light client module that makes adding support for\\nnew light clients a simple governance-gated transaction. The light client bytecode, written in Wasm-compilable Rust,\\nruns inside a Wasm VM. The Wasm light client submodule exposes a proxy light client interface that routes incoming\\nmessages to the appropriate handler function, inside the Wasm VM for execution.\\nWith the Wasm light client module, anybody can add new IBC light client in the form of Wasm bytecode (provided they are\\nable to submit the governance proposal transaction and that it passes) as well as instantiate clients using any created\\nclient type. This allows any chain to update its own light client in other chains without going through the steps outlined above.\\n\n\n##Decision\nWe decided to implement the Wasm light client module as a light client proxy that will interface with the actual light client\\nuploaded as Wasm bytecode. To enable usage of the Wasm light client module, users need to add it to the list of allowed clients\\nby updating the `AllowedClients` parameter in the 02-client submodule of core IBC.\\n```go\\nparams := clientKeeper.GetParams(ctx)\\nparams.AllowedClients = append(params.AllowedClients, exported.Wasm)\\nclientKeeper.SetParams(ctx, params)\\n```\\nAdding a new light client contract is governance-gated. To upload a new light client users need to submit\\na [governance v1 proposal](https:\/\/docs.cosmos.network\/main\/modules\/gov#proposals) that contains the `sdk.Msg` for storing\\nthe Wasm contract's bytecode. The required message is `MsgStoreCode` and the bytecode is provided in the field `wasm_byte_code`:\\n```proto\\n\/\/ MsgStoreCode defines the request type for the StoreCode rpc.\\nmessage MsgStoreCode {\\n\/\/ signer address\\nstring signer = 1;\\n\/\/ wasm byte code of light client contract. It can be raw or gzip compressed\\nbytes wasm_byte_code = 2;\\n}\\n```\\nThe RPC handler processing `MsgStoreCode` will make sure that the signer of the message matches the address of authority allowed to\\nsubmit this message (which is normally the address of the governance module).\\n```go\\n\/\/ StoreCode defines a rpc handler method for MsgStoreCode\\nfunc (k Keeper) StoreCode(goCtx context.Context, msg *types.MsgStoreCode) (*types.MsgStoreCodeResponse, error) {\\nif k.GetAuthority() != msg.Signer {\\nreturn nil, errorsmod.Wrapf(ibcerrors.ErrUnauthorized, \"expected %s, got %s\", k.GetAuthority(), msg.Signer)\\n}\\nctx := sdk.UnwrapSDKContext(goCtx)\\nchecksum, err := k.storeWasmCode(ctx, msg.WasmByteCode, ibcwasm.GetVM().StoreCode)\\nif err != nil {\\nreturn nil, errorsmod.Wrap(err, \"failed to store wasm bytecode\")\\n}\\nemitStoreWasmCodeEvent(ctx, checksum)\\nreturn &types.MsgStoreCodeResponse{\\nChecksum: checksum,\\n}, nil\\n}\\n```\\nThe contract's bytecode is not stored in state (it is actually unnecessary and wasteful to store it, since\\nthe Wasm VM already stores it and can be queried back, if needed). The checksum is simply the hash of the bytecode\\nof the contract and it is stored in state in an entry with key `checksums` that contains the checksums for the bytecodes that have been stored.\\n### How light client proxy works?\\nThe light client proxy behind the scenes will call a CosmWasm smart contract instance with incoming arguments serialized\\nin JSON format with appropriate environment information. Data returned by the smart contract is deserialized and\\nreturned to the caller.\\nConsider the example of the `VerifyClientMessage` function of `ClientState` interface. Incoming arguments are\\npackaged inside a payload object that is then JSON serialized and passed to `queryContract`, which executes `WasmVm.Query`\\nand returns the slice of bytes returned by the smart contract. This data is deserialized and passed as return argument.\\n```go\\ntype QueryMsg struct {\\nStatus               *StatusMsg               `json:\"status,omitempty\"`\\nExportMetadata       *ExportMetadataMsg       `json:\"export_metadata,omitempty\"`\\nTimestampAtHeight    *TimestampAtHeightMsg    `json:\"timestamp_at_height,omitempty\"`\\nVerifyClientMessage  *VerifyClientMessageMsg  `json:\"verify_client_message,omitempty\"`\\nCheckForMisbehaviour *CheckForMisbehaviourMsg `json:\"check_for_misbehaviour,omitempty\"`\\n}\\ntype verifyClientMessageMsg struct {\\nClientMessage *ClientMessage `json:\"client_message\"`\\n}\\n\/\/ VerifyClientMessage must verify a ClientMessage.\\n\/\/ A ClientMessage could be a Header, Misbehaviour, or batch update.\\n\/\/ It must handle each type of ClientMessage appropriately.\\n\/\/ Calls to CheckForMisbehaviour, UpdateSta\u00e5te, and UpdateStateOnMisbehaviour\\n\/\/ will assume that the content of the ClientMessage has been verified\\n\/\/ and can be trusted. An error should be returned\\n\/\/ if the ClientMessage fails to verify.\\nfunc (cs ClientState) VerifyClientMessage(\\nctx sdk.Context,\\n_ codec.BinaryCodec,\\nclientStore storetypes.KVStore,\\nclientMsg exported.ClientMessage\\n) error {\\nclientMessage, ok := clientMsg.(*ClientMessage)\\nif !ok {\\nreturn errorsmod.Wrapf(ibcerrors.ErrInvalidType, \"expected type: %T, got: %T\", &ClientMessage{}, clientMsg)\\n}\\npayload := QueryMsg{\\nVerifyClientMessage: &VerifyClientMessageMsg{ClientMessage: clientMessage.Data},\\n}\\n_, err := wasmQuery[EmptyResult](ctx, clientStore, &cs, payload)\\nreturn err\\n}\\n```\\n### Global Wasm VM variable\\nThe 08-wasm keeper structure keeps a reference to the Wasm VM instantiated in the keeper constructor function. The keeper uses\\nthe Wasm VM to store the bytecode of light client contracts. However, the Wasm VM is also needed in the 08-wasm implementations of\\nsome of the `ClientState` interface functions to initialise a contract, execute calls on the contract and query the contract. Since\\nthe `ClientState` functions do not have access to the 08-wasm keeper, then it has been decided to keep a global pointer variable that\\npoints to the same instance as the one in the 08-wasm keeper. This global pointer variable is then used in the implementations of\\nthe `ClientState` functions.\\n","MatchedIds":[842,21,1982,4650,4113],"Predicted":"## Decision\n### Architecture and Implementation\nTo address the challenges of upgrading and adding light clients in IBC-go, we will integrate a Wasm light client module. Here are the steps we will take:\n\n1. **Introduce Wasm VM Support**:\n   - Integrate a Wasm Virtual Machine (VM) into the ibc-go framework to enable the execution of Wasm bytecode. We can utilize existing Wasm execution libraries such as Wasmer or Wasmtime for this purpose.\n\n2. **Create Wasm Light Client Submodule**:\n   - Develop a Wasm light client submodule which will act as a proxy interface. This proxy will handle incoming IBC messages, routing them to the appropriate Wasm bytecode handler function within the Wasm VM.\n\n3. **Governance-Gated Deployment**:\n   - Implement a governance mechanism to facilitate the submission and approval of proposals to add or update Wasm light clients. The governance proposal should include the Wasm bytecode for the new or updated light client.\n   - Ensure that once a proposal passes, the new light client is automatically deployed and becomes callable within the ibc-go framework.\n\n4. **Wasm Module Interface**:\n   - Define a standardized interface for Wasm light clients in Rust. This interface will include functions for IBC packet verification, state transition, and other light client functionalities.\n\n### Example:\n```rust\n#[ic_cdk_macros::update(name = \"verifyClientState\")]\nfn verify_client_state(client_state: Vec<u8>, header: Vec<u8>) -> bool {\n    \/\/ Implement the logic to verify the client state against the header\n    \/\/ Return true if verification succeeds, otherwise false\n}\n\n#[ic_cdk_macros::update(name = \"relayPacket\")]\nfn relay_packet(packet: Vec<u8>) -> Vec<u8> {\n    \/\/ Implement the logic to process and relay IBC packets\n    \/\/ Return the processed packet or an acknowledgment\n}\n```\n\n### Deployment Process:\n1. **Proposal Submission**:\n   - A proposer submits a governance proposal including the Wasm bytecode and relevant metadata.\n```json\n{\n  \"title\": \"Add new XYZ Light Client\",\n  \"description\": \"This proposal aims to add support for the XYZ consensus algorithm.\",\n  \"wasm_bytecode\": \"<base64-encoded-wasm-bytecode>\"\n}\n```\n\n2. **Governance Vote**:\n   - The proposal undergoes a voting period. If it passes, the Wasm bytecode is stored on-chain and registered with the Wasm VM.\n\n3. **Instantiation**:\n   - Once the light client is registered, it can be instantiated and used for IBC transactions without needing further modifications to ibc-go.\n\n### Benefits:\n- **Flexibility**: Enables the addition of diverse light clients without modifying the core IBC-go codebase.\n- **Decoupled Updates**: Allows chains to update their light clients independently, reducing downtime and coordination effort.\n- **Governance Control**: Ensures that only community-approved light clients are added, enhancing security and trust.\n\n### Future Work:\n- **Security Audits**: Conduct thorough security audits of the Wasm bytecode and the Wasm VM integration.\n- **Testing**: Implement extensive test cases to cover various scenarios and edge cases involving Wasm light clients.\n- **Tooling**: Develop user-friendly tools and documentation to help developers create and deploy Wasm light clients.\n\nBy implementing these steps, we aim to significantly streamline the process of adding and updating IBC light clients, improving the overall robustness and flexibility of the IBC-go ecosystem.","GenTime":"2024-08-20 00:32:08"}
{"File Name":"ibc-rs\/adr-005-relayer-v0-implementation.md","Context":"## Context\\nThis ADR documents the implementation of the `v0.1` [relayer lib crate]\\n[ibc-relayer].\\nThis library is instantiated in the [Hermes][hermes] binary of the\\n[ibc-relayer-cli crate][ibc-relayer-cli] (which is not the focus of this discussion).\\nAs a main design goal, `v0.1` is meant to lay a foundation upon which we can\\nadd more features and enhancements incrementally with later relayer versions.\\nThis is to say that `v0.1` may be deficient in terms of features or\\nrobustness, and rather aims to be simple, adaptable, and extensible.\\nFor this reason, we primarily discuss aspects of concurrency and architecture.\\n### Relayer versioning scheme\\nOn the mid-term, the relayer architecture is set out to evolve across three\\nversions.\\nThe first of these, `v0.1`, makes several simplifying assumptions\\nabout the environment of the relayer and its features. These assumptions\\nare important towards limiting the scope that `v0.1` aims to\\ncover, and allowing a focus on the architecture and concurrency model to\\nprovide for growth in the future.\\nThese assumptions are documented below in the [decision](#decision) section.\\n","Decision":"### Configuration\\nFor the most part, the relayer configuration will be\\nstatic: the configuration for chains and their respective objects (clients,\\nconnections, or channels) will be fully specified in the relayer\\nconfiguration file and will not change throughout execution.\\nLight clients are also statically defined in the config file, and cannot be\\nswitched dynamically at runtime.\\nRecent changes to the ICS protocol specifies identifier\\nselection for clients, connections, and channels to be [deterministic][ids].\\nFor this reason, we will not need to specify any identifiers in the\\nconfiguration file.\\nWe only specify which pairs of chains should communicate with one\\nanother, and the port identifier to use for that purpose.\\nThis pair of chains plus their corresponding port identifiers is called a\\n__relaying path__.\\nAny relaying path is unidirectional.\\nAn example with the relevant section of the configuration file follows.\\n```toml\\n[[connections]]\\na_chain = 'ibc-0'\\nb_chain = 'ibc-1'\\n[[connections.paths]]\\na_port = 'transfer'\\nb_port = 'transfer'\\n```\\nHere there are two chains, ith one connection between them, and a path for\\nrelaying on the port called `transfer` on both chains, from chain `ibc-0`\\nto `ibc-1`.\\n### Links\\nA [link][link] is a relayer-level protocol that implements packet relay across\\none relaying path.\\nThe relayer at `v0.1` will focus on a single link.\\nThis limitation will be lifted in subsequent versions.\\n### Chain State\\nEach chain is assumed to start with an empty IBC state.\\nThis means that the relayer will take care of creating the client,\\nconnection, and channel objects respectively on each side of a link.\\n### Proof Verification\\nThe `v0.1` relayer will _not_ do proof verification.\\n### Feature set\\nThe [complete list of features is documented elsewhere][features] in detail.\\n","tokens":275,"id":4650,"text":"## Context\\nThis ADR documents the implementation of the `v0.1` [relayer lib crate]\\n[ibc-relayer].\\nThis library is instantiated in the [Hermes][hermes] binary of the\\n[ibc-relayer-cli crate][ibc-relayer-cli] (which is not the focus of this discussion).\\nAs a main design goal, `v0.1` is meant to lay a foundation upon which we can\\nadd more features and enhancements incrementally with later relayer versions.\\nThis is to say that `v0.1` may be deficient in terms of features or\\nrobustness, and rather aims to be simple, adaptable, and extensible.\\nFor this reason, we primarily discuss aspects of concurrency and architecture.\\n### Relayer versioning scheme\\nOn the mid-term, the relayer architecture is set out to evolve across three\\nversions.\\nThe first of these, `v0.1`, makes several simplifying assumptions\\nabout the environment of the relayer and its features. These assumptions\\nare important towards limiting the scope that `v0.1` aims to\\ncover, and allowing a focus on the architecture and concurrency model to\\nprovide for growth in the future.\\nThese assumptions are documented below in the [decision](#decision) section.\\n\n\n##Decision\n### Configuration\\nFor the most part, the relayer configuration will be\\nstatic: the configuration for chains and their respective objects (clients,\\nconnections, or channels) will be fully specified in the relayer\\nconfiguration file and will not change throughout execution.\\nLight clients are also statically defined in the config file, and cannot be\\nswitched dynamically at runtime.\\nRecent changes to the ICS protocol specifies identifier\\nselection for clients, connections, and channels to be [deterministic][ids].\\nFor this reason, we will not need to specify any identifiers in the\\nconfiguration file.\\nWe only specify which pairs of chains should communicate with one\\nanother, and the port identifier to use for that purpose.\\nThis pair of chains plus their corresponding port identifiers is called a\\n__relaying path__.\\nAny relaying path is unidirectional.\\nAn example with the relevant section of the configuration file follows.\\n```toml\\n[[connections]]\\na_chain = 'ibc-0'\\nb_chain = 'ibc-1'\\n[[connections.paths]]\\na_port = 'transfer'\\nb_port = 'transfer'\\n```\\nHere there are two chains, ith one connection between them, and a path for\\nrelaying on the port called `transfer` on both chains, from chain `ibc-0`\\nto `ibc-1`.\\n### Links\\nA [link][link] is a relayer-level protocol that implements packet relay across\\none relaying path.\\nThe relayer at `v0.1` will focus on a single link.\\nThis limitation will be lifted in subsequent versions.\\n### Chain State\\nEach chain is assumed to start with an empty IBC state.\\nThis means that the relayer will take care of creating the client,\\nconnection, and channel objects respectively on each side of a link.\\n### Proof Verification\\nThe `v0.1` relayer will _not_ do proof verification.\\n### Feature set\\nThe [complete list of features is documented elsewhere][features] in detail.\\n","MatchedIds":[1982,4656,21,1979,4539],"Predicted":"## Decision\nThe `v0.1` version of the `ibc-relayer` library will be implemented with the following assumptions to simplify its initial scope and establish a foundation for future enhancements:\n\n1. **Single-Threaded Execution**: Concurrency will not be a focus in `v0.1`. The relayer will operate in a single-threaded manner, simplifying the design and debugging process. Future versions will introduce concurrency models as needed.\n   \n2. **Basic Connection Support**: Only basic functionalities for establishing and maintaining connections between IBC-enabled chains will be implemented. Advanced features such as relay optimizations and dynamic discovery of new connections will be deferred to future versions.\n\n3. **Simplified Error Handling**: Error handling will be rudimentary, focusing on establishing a stable foundation. Comprehensive error handling, retries, and recovery mechanisms will be addressed in later versions.\n\n4. **Limited Protocol Support**: `v0.1` will support a minimal subset of IBC protocols necessary to demonstrate basic relaying functionality. This provides an opportunity to build a scalable architecture without the need to support the entire IBC protocol suite initially.\n\n5. **Manual Configuration**: All configurations, such as chain details and connection parameters, will be manually specified and read from static configuration files. Automated configuration management and dynamic updates will be considered in future iterations.\n\n6. **No Advanced Features**: Features such as advanced security mechanisms, performance optimizations, and high availability will not be included in `v0.1`. The primary goal is to ensure basic relaying functionality within a simplified environment.\n\nBy adhering to these constraints, the `v0.1` relayer will provide a basic, stable implementation that serves as a foundation for building more complex and robust versions (`v0.2`, `v0.3`, etc.) in the future.","GenTime":"2024-08-20 00:32:15"}
{"File Name":"adr-demo\/0004-use-elastic-search-for-image-search.md","Context":"## Context\\nAs our users base is growing and we have more and more search queries, we would like to improve current search form to:\\n* Be able to search on newly added fields (e.g. created date, modified date, author, licence, etc)\\n* Allow more complex searches on existing and new fields such as:\\n* \"and\" queries to search on multiple tags\\n* \"or\" queries to search on an exclusive list of tags\\n* \"startsWith\" and \"contains\" search instead of current exact match searches\\n* synonyms\\n","Decision":"Given our needs it seems pretty clear that we need a search engine, as databases search capabilities are limited and would cost more and more as data is growing.\\nWe will use Elastic Search as our search engine as it is the de facto standard and is based on battle-tested search engine\\nLucene. Our current microservices will now evolve to this schema:\\n![](microservices-architecture-with-es.png)\\nNote: ES indexes do not need to be updated in real-time when a new image\/gif is uploaded, in fact we can easily\\ntolerate a few minutes of delay between an upload and its actual availability for search. All we need is to be\\neventually consistent.\\n","tokens":113,"id":2513,"text":"## Context\\nAs our users base is growing and we have more and more search queries, we would like to improve current search form to:\\n* Be able to search on newly added fields (e.g. created date, modified date, author, licence, etc)\\n* Allow more complex searches on existing and new fields such as:\\n* \"and\" queries to search on multiple tags\\n* \"or\" queries to search on an exclusive list of tags\\n* \"startsWith\" and \"contains\" search instead of current exact match searches\\n* synonyms\\n\n\n##Decision\nGiven our needs it seems pretty clear that we need a search engine, as databases search capabilities are limited and would cost more and more as data is growing.\\nWe will use Elastic Search as our search engine as it is the de facto standard and is based on battle-tested search engine\\nLucene. Our current microservices will now evolve to this schema:\\n![](microservices-architecture-with-es.png)\\nNote: ES indexes do not need to be updated in real-time when a new image\/gif is uploaded, in fact we can easily\\ntolerate a few minutes of delay between an upload and its actual availability for search. All we need is to be\\neventually consistent.\\n","MatchedIds":[4637,4738,1255,4378,102],"Predicted":"## Decision\nChosen option: \"Elasticsearch Query DSL\", because it offers robust and flexible search capabilities that meet our current requirements and is designed to handle complex search queries effectively.\n\n### Positive Consequences\n- **Support for Newly Added Fields**: Elasticsearch allows for dynamic mapping, making it easy to add and search on new fields such as created date, modified date, author, and licence.\n- **Complex Search Queries**: Elasticsearch Query DSL supports a wide range of query types including \"and\", \"or\", \"startsWith\", and \"contains\" searches.\n- **Synonym Support**: Elasticsearch has built-in support for synonyms, allowing us to create comprehensive search experiences for users.\n- **Scalability**: Elasticsearch is designed to handle large volumes of search queries, making it suitable as our user base grows.\n- **Performance**: Elasticsearch is optimized for both speed and relevance, ensuring quick response times for user queries.\n- **Flexibility**: The query DSL is highly flexible and allows for fine-tuning of search behavior as per the specific business requirements.\n\n### Negative Consequences\n- **Implementation Effort**: Setting up Elasticsearch and migrating existing search functionality will require a significant amount of development time and effort.\n- **Learning Curve**: The Elasticsearch Query DSL can be complex, necessitating training for the development team.\n- **Maintenance**: Maintaining Elasticsearch infrastructure and ensuring it scales with increasing data and query volume adds ongoing operational overhead.\n\nTo address the growing needs of our search functionality, Elasticsearch Query DSL will provide the robust and flexible solution required to handle complex search scenarios while accommodating new fields efficiently.","GenTime":"2024-08-20 00:32:22"}
{"File Name":"DunkMe\/0002-data-access-api.md","Context":"#### Context and Problem Statement\\nA technology stack is required for the projects data access application programming interface (API).\\n#### Considered Options\\n- ASP.NET Web API (C#)\\n#### Decision Outcome\\nChosen option: \"ASP.NET Web API (C#)\", because\\n- This can target .Net Core 2\\n- Dapper Object Relational Mapper (ORM) can be used, leveraging reuse of skills already within Trade Me.\\n- A repository pattern can be applied facilitating Unit Testing.\\n([back](README.md))\\n","Decision":"Chosen option: \"ASP.NET Web API (C#)\", because\\n- This can target .Net Core 2\\n- Dapper Object Relational Mapper (ORM) can be used, leveraging reuse of skills already within Trade Me.\\n- A repository pattern can be applied facilitating Unit Testing.\\n([back](README.md))\\n","tokens":111,"id":1660,"text":"#### Context and Problem Statement\\nA technology stack is required for the projects data access application programming interface (API).\\n#### Considered Options\\n- ASP.NET Web API (C#)\\n#### Decision Outcome\\nChosen option: \"ASP.NET Web API (C#)\", because\\n- This can target .Net Core 2\\n- Dapper Object Relational Mapper (ORM) can be used, leveraging reuse of skills already within Trade Me.\\n- A repository pattern can be applied facilitating Unit Testing.\\n([back](README.md))\\n\n\n##Decision\nChosen option: \"ASP.NET Web API (C#)\", because\\n- This can target .Net Core 2\\n- Dapper Object Relational Mapper (ORM) can be used, leveraging reuse of skills already within Trade Me.\\n- A repository pattern can be applied facilitating Unit Testing.\\n([back](README.md))\\n","MatchedIds":[1659,1661,3470,389,3776],"Predicted":"## Decision\nChosen option: \"ASP.NET Web API (C#)\", because\n- This can target .Net Core 2.\n- Dapper Object Relational Mapper (ORM) can be used, leveraging reuse of skills already within Trade Me.\n- A repository pattern can be applied facilitating Unit Testing.","GenTime":"2024-08-20 00:32:25"}
{"File Name":"winery\/0031-reuse-refinement-code-for-pattern-detection.md","Context":"## Context and Problem Statement\\nTo create an executable deployment model, the pattern refinement process replaces a matching subgraph with the Refinement Structure of a PRM.\\nTo create a PbDCM, the pattern detection process replaces a matching subgraph with the Detector of a PRM.\\nThe replacement procedure is identical for both processes, only the structures used for the replacement differ.\\nTherefore, the implementation of the pattern refinement process should be reused to implement the pattern detection process.\\n## Decision Drivers\\n* Avoid duplicate code\\n* Avoid introducing errors and inconsistencies during reimplementation\\n","Decision":"* Avoid duplicate code\\n* Avoid introducing errors and inconsistencies during reimplementation\\nChosen option: \"Swap the Detector of all PRMs with their Refinement Structures\", because reimplementation introduces too much duplicate code and a common interface requires a lot of boilerplate code while also decreasing readability.\\n### Positive Consequences <!-- optional -->\\n* Complete pattern refinement implementation can be reused\\n### Negative consequences <!-- optional -->\\n* Readability and understandability decreases\\n","tokens":115,"id":4299,"text":"## Context and Problem Statement\\nTo create an executable deployment model, the pattern refinement process replaces a matching subgraph with the Refinement Structure of a PRM.\\nTo create a PbDCM, the pattern detection process replaces a matching subgraph with the Detector of a PRM.\\nThe replacement procedure is identical for both processes, only the structures used for the replacement differ.\\nTherefore, the implementation of the pattern refinement process should be reused to implement the pattern detection process.\\n## Decision Drivers\\n* Avoid duplicate code\\n* Avoid introducing errors and inconsistencies during reimplementation\\n\n\n##Decision\n* Avoid duplicate code\\n* Avoid introducing errors and inconsistencies during reimplementation\\nChosen option: \"Swap the Detector of all PRMs with their Refinement Structures\", because reimplementation introduces too much duplicate code and a common interface requires a lot of boilerplate code while also decreasing readability.\\n### Positive Consequences <!-- optional -->\\n* Complete pattern refinement implementation can be reused\\n### Negative consequences <!-- optional -->\\n* Readability and understandability decreases\\n","MatchedIds":[3925,2859,856,1193,4307],"Predicted":"## Decision\nWe will refactor the common replacement procedure into a shared utility function.\n\n### Implementation Details\n1. **Create a Utility Module:**\n   - Develop a module that encapsulates the replacement procedure logic.\n   - The module will accept parameters for the subgraph to be replaced and the structure (either Refinement Structure or Detector) to use for replacement.\n\n2. **Parameterize Replacement Logic:**\n   - The utility function will take the type of structure to use (Refinement Structure or Detector) as one of its parameters.\n   - Ensure that the function is flexible and can handle the data types and structures for both pattern refinement and pattern detection.\n\n3. **Reuse in Pattern Refinement and Detection:**\n   - Update the pattern refinement process to call the new utility function.\n   - Update the pattern detection process to also call this utility function, ensuring reuse of the replacement logic.\n   \n4. **Testing and Validation:**\n   - Develop comprehensive tests to ensure that the utility function performs correctly for both types of structures.\n   - Validate the integration within both pattern refinement and pattern detection processes to ensure there are no regressions or new issues introduced.\n\n## Status\nProposed\n\n## Consequences\n- **Positive:**\n  - **Code Reusability:** The replacement logic is now centralized, making it easier to maintain and update.\n  - **Consistency:** Reduces the risk of inconsistencies and errors that may arise from having duplicate implementations.\n  - **Simplified Maintenance:** Future changes to the replacement logic only need to be made in a single location.\n- **Negative:**\n  - **Initial Refactor Effort:** There will be an initial effort required to refactor and implement the shared utility function.\n  - **Testing Overhead:** Comprehensive testing will be needed to ensure the new utility function integrates seamlessly with existing processes.\n\n## Example\n### Utility Function (Pseudocode):\n```python\ndef replace_subgraph(subgraph, structure):\n    # Logic to replace subgraph with the given structure\n    ...\n```\n\n### Pattern Refinement Usage:\n```python\nreplace_subgraph(matching_subgraph, refinement_structure)\n```\n\n### Pattern Detection Usage:\n```python\nreplace_subgraph(matching_subgraph, detector_structure)\n```\n\nBy implementing this shared utility function, we ensure that the replacement logic is consistent and maintainable across both pattern refinement and pattern detection processes.","GenTime":"2024-08-20 00:32:33"}
{"File Name":"lbh-adrs\/Validation.md","Context":"## **Context**\\nValidation is an important function within APIs, to ensure that data that is submitted via API calls is properly checked to ensure it meets the requirements set by the business.\\nWe will look at two options for validation:\\n- **Data Annotations**\\nThis involves \"annotating\" each class model with specific validation, such as\\n- `[Required(ErrorMessage = \"This field is required\")]`\\n- `[MaxLength(20)]`\\nThere are a number of issues with this approach:\\n- Validation is scattered throughout the codebase as attributes on data model classes\\n- Testing is not straightforward\\n- Error messages are part of the compiled code and it is not possible to decouple this, e.g. to allow for customisable error messages\\n- Does not allow for conditional validation\\n- **Fluent Validation**\\nFluent Validation solves a number of the issues that DataAnnotation cannot be solved by Data Annotations. It:\\n- Is easy to configure with minimal, unintrusive setup in `Startup.cs`\\n- Lives outside of the data model classes\\n- Very easy to test validation in isolation\\n- Error messaging can be externalised using dependency injection\\n- Allows for chaining of validators and conditional validation\\n- Has a lot of built-in validation already (*if **x** exists, then **y** must also exist*)\\n","Decision":"**Fluent Validation**\\nFluent Valdation is widely used, offers a lot of flexibility and allows for a clean, customisable and testable approach to validation.\\n","tokens":276,"id":2307,"text":"## **Context**\\nValidation is an important function within APIs, to ensure that data that is submitted via API calls is properly checked to ensure it meets the requirements set by the business.\\nWe will look at two options for validation:\\n- **Data Annotations**\\nThis involves \"annotating\" each class model with specific validation, such as\\n- `[Required(ErrorMessage = \"This field is required\")]`\\n- `[MaxLength(20)]`\\nThere are a number of issues with this approach:\\n- Validation is scattered throughout the codebase as attributes on data model classes\\n- Testing is not straightforward\\n- Error messages are part of the compiled code and it is not possible to decouple this, e.g. to allow for customisable error messages\\n- Does not allow for conditional validation\\n- **Fluent Validation**\\nFluent Validation solves a number of the issues that DataAnnotation cannot be solved by Data Annotations. It:\\n- Is easy to configure with minimal, unintrusive setup in `Startup.cs`\\n- Lives outside of the data model classes\\n- Very easy to test validation in isolation\\n- Error messaging can be externalised using dependency injection\\n- Allows for chaining of validators and conditional validation\\n- Has a lot of built-in validation already (*if **x** exists, then **y** must also exist*)\\n\n\n##Decision\n**Fluent Validation**\\nFluent Valdation is widely used, offers a lot of flexibility and allows for a clean, customisable and testable approach to validation.\\n","MatchedIds":[1532,3776,2386,1660,2316],"Predicted":"## Decision\nWe will use **Fluent Validation** for implementing validation logic within our APIs. \n\nThe reasons include:\n- **Separation of Concerns**: Validation logic is kept outside of data model classes, leading to a cleaner codebase and better separation of concerns.\n- **Easy Configuration**: Fluent Validation can be easily configured in `Startup.cs` with minimal and unobtrusive setup.\n- **Isolation Testing**: It is straightforward to test validation rules in isolation, which simplifies unit testing.\n- **Externalised Error Messaging**: Error messages can be decoupled from the code using dependency injection, allowing for customizable and localized messages.\n- **Conditional Validation**: Fluent Validation supports complex validation scenarios, such as conditional validation where rules depend on the presence of other values.\n- **Rich Built-in Validators**: The library comes pre-loaded with a variety of built-in validators, which can save development time and effort.\n\nBy adopting Fluent Validation, we ensure that our validation logic remains maintainable, testable, and extensible, meeting the business requirements effectively.","GenTime":"2024-08-20 00:32:37"}
{"File Name":"sre-adrs\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made for our team.\\n","Decision":"We will use Architecture Decision Records, as discussed in our original proposal: https:\/\/docs.google.com\/document\/d\/1pZlYCyXcZbmQq1O-g4BNJD1uZTjluYKkk7BSu2BwOGU\/edit#\\n","tokens":16,"id":2261,"text":"## Context\\nWe need to record the architectural decisions made for our team.\\n\n\n##Decision\nWe will use Architecture Decision Records, as discussed in our original proposal: https:\/\/docs.google.com\/document\/d\/1pZlYCyXcZbmQq1O-g4BNJD1uZTjluYKkk7BSu2BwOGU\/edit#\\n","MatchedIds":[4428,3119,1436,294,2583],"Predicted":"## Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions","GenTime":"2024-08-20 00:32:39"}
{"File Name":"scholarsphere\/0002-define-use-of-decorators-and-presenters.md","Context":"## Context\\nThe issue motivating this decision, and any context that influences or constrains the decision.\\nWe need to distinguish between decorator and presenter objects in order to clarify which would be used in a given\\nsituation.\\n","Decision":"Decorators extend SimpleDelegator and will always delegate undefined methods to the delegated object.\\nPresenters take the form of \"plain ol' Ruby objects\" (POROs) and would generally not delegate methods to an object.\\nTheir usage is designed to be more flexible when the rendering of content isn't tied specifically to one object.\\n","tokens":46,"id":4556,"text":"## Context\\nThe issue motivating this decision, and any context that influences or constrains the decision.\\nWe need to distinguish between decorator and presenter objects in order to clarify which would be used in a given\\nsituation.\\n\n\n##Decision\nDecorators extend SimpleDelegator and will always delegate undefined methods to the delegated object.\\nPresenters take the form of \"plain ol' Ruby objects\" (POROs) and would generally not delegate methods to an object.\\nTheir usage is designed to be more flexible when the rendering of content isn't tied specifically to one object.\\n","MatchedIds":[4804,3820,1145,4616,3546],"Predicted":"## Decision\nWe will establish naming conventions and clear documentation to distinguish between decorator and presenter objects. Decorator objects will be prefixed with \"Dec\" and focus on dynamically adding behavior to individual objects, whereas presenter objects will be prefixed with \"Pres\" and will manage the presentation logic and formatting for views. This convention will be enforced through code reviews and automated linting rules.","GenTime":"2024-08-20 00:32:44"}
{"File Name":"pace-developers\/0002-use-github-pages-for-user-documentation.md","Context":"## Context\\nA consistent platform is required for user documentation from the PACE projects: Brille, Euphonic and Horace\/Herbert.\\nTwo platforms support, are well used for this service:\\n- [Read the Docs](https:\/\/readthedocs.org\/)\\n- [GitHub pages](https:\/\/pages.github.com\/)\\nBoth platforms will display documentation built by [Sphinx](https:\/\/www.sphinx-doc.org\/) from reStructuredText source files.\\nBrille includes compiled C libraries that contribute APIs to the project. The Read the Docs build\/deploy pipeline does not support inclusion of documentation generated from C source.\\n","Decision":"We will use GitHub pages for all PACE project documentation.\\n","tokens":132,"id":5172,"text":"## Context\\nA consistent platform is required for user documentation from the PACE projects: Brille, Euphonic and Horace\/Herbert.\\nTwo platforms support, are well used for this service:\\n- [Read the Docs](https:\/\/readthedocs.org\/)\\n- [GitHub pages](https:\/\/pages.github.com\/)\\nBoth platforms will display documentation built by [Sphinx](https:\/\/www.sphinx-doc.org\/) from reStructuredText source files.\\nBrille includes compiled C libraries that contribute APIs to the project. The Read the Docs build\/deploy pipeline does not support inclusion of documentation generated from C source.\\n\n\n##Decision\nWe will use GitHub pages for all PACE project documentation.\\n","MatchedIds":[5177,3586,3035,1196,4087],"Predicted":"## Decision\nUse GitHub Pages as the platform for user documentation for all PACE projects: Brille, Euphonic, and Horace\/Herbert.\n\n### Reasons:\n1. **Consistent Platform**: GitHub Pages can consistently serve documentation for all PACE projects, ensuring uniformity in how user documentation is accessed.\n2. **Sphinx Compatibility**: Both platforms support Sphinx, but GitHub Pages does not have the limitation of excluding documentation generated from C source, crucial for Brille.\n3. **Build\/Deploy Support**: GitHub Pages integrates well with GitHub Actions, allowing for more flexible and customizable build and deploy pipelines that can handle the specific requirements of each project, including the compiled C libraries.\n\n### Outcome:\nStandardize the user documentation on GitHub Pages to achieve a consistent and functional documentation experience across all PACE projects.","GenTime":"2024-08-20 00:32:48"}
{"File Name":"DunkMe\/0001-front-end.md","Context":"#### Context and Problem Statement\\nA front end, client facing technology stack \/ language is needed.\\n#### Considered Options\\n- ASP.NET MVC (C#)\\n- ASP.NET Web Forms (C#)\\n- Angular (with TypeScript)\\n#### Decision Outcome\\nChosen option: \"Angular\", because\\n- This is in-line with Trade Me's technical stack trajectory.\\n- In line with what is considered industry standard for green field projects.\\n","Decision":"Chosen option: \"Angular\", because\\n- This is in-line with Trade Me's technical stack trajectory.\\n- In line with what is considered industry standard for green field projects.\\n","tokens":93,"id":1659,"text":"#### Context and Problem Statement\\nA front end, client facing technology stack \/ language is needed.\\n#### Considered Options\\n- ASP.NET MVC (C#)\\n- ASP.NET Web Forms (C#)\\n- Angular (with TypeScript)\\n#### Decision Outcome\\nChosen option: \"Angular\", because\\n- This is in-line with Trade Me's technical stack trajectory.\\n- In line with what is considered industry standard for green field projects.\\n\n\n##Decision\nChosen option: \"Angular\", because\\n- This is in-line with Trade Me's technical stack trajectory.\\n- In line with what is considered industry standard for green field projects.\\n","MatchedIds":[1660,1661,3715,1240,1000],"Predicted":"## Decision\nChosen option: \"Angular\", because\n- This is in-line with Trade Me's technical stack trajectory.\n- In line with what is considered industry standard for green field projects.\n- Angular allows for a highly modular and component-based architecture, which enhances maintainability and scalability.\n- Leveraging TypeScript improves code quality and developer productivity.\n- Angular has extensive community support and a wide range of third-party libraries, which can reduce development time and effort.\n\n([back](README.md))","GenTime":"2024-08-20 00:33:00"}
{"File Name":"AYCHAIN.github.io\/adr-002-css-structure.md","Context":"## Context\\nWeb Components encapsulate their shadow dom away from the rest of the DOM, meaning that a general stylesheet cannot style the internals of a Web Component.\\nAs such, we need to find a strategy to maintain component level styles but enable shared styles so that we limit code duplication.\\nWe had previously decided to follow ITCSS for its way to enable scalability as well as limit specificity war, and would like to keep following this approach.\\n","Decision":"Considering the outcome of the exploration, and seeing that we want to maintain a sense of scalability and separation of concerns as advocated by the ITCSS methodology, we have decided to maintain component specific stylesheets at component directory level, and use Sass `import` within these sheets to add specific classes that many component might use (basically any other layer from ITCSS beyond components).\\nShared styles will be hosted under `src\/shared-styles` folder and organized following the ITCSS nomenclature.\\nAt this moment the `main.scss` file is maintained as it contains information around the available files, and may prove worthy in the future. The value of this needs to be reassessed.\\n","tokens":91,"id":157,"text":"## Context\\nWeb Components encapsulate their shadow dom away from the rest of the DOM, meaning that a general stylesheet cannot style the internals of a Web Component.\\nAs such, we need to find a strategy to maintain component level styles but enable shared styles so that we limit code duplication.\\nWe had previously decided to follow ITCSS for its way to enable scalability as well as limit specificity war, and would like to keep following this approach.\\n\n\n##Decision\nConsidering the outcome of the exploration, and seeing that we want to maintain a sense of scalability and separation of concerns as advocated by the ITCSS methodology, we have decided to maintain component specific stylesheets at component directory level, and use Sass `import` within these sheets to add specific classes that many component might use (basically any other layer from ITCSS beyond components).\\nShared styles will be hosted under `src\/shared-styles` folder and organized following the ITCSS nomenclature.\\nAt this moment the `main.scss` file is maintained as it contains information around the available files, and may prove worthy in the future. The value of this needs to be reassessed.\\n","MatchedIds":[1992,1125,783,5090,2655],"Predicted":"## Decision\nWe will adopt a hybrid approach by utilizing CSS custom properties (variables) combined with ITCSS principles for styling our Web Components. \n\n### Decision Drivers\n- Maintainability: Adherence to ITCSS principles for organization and scalability.\n- Encapsulation: Styled internals of Web Components using Shadow DOM.\n- Reusability: Enable shared styles using CSS custom properties to reduce code duplication.\n\n### Positive Consequences\n- **Scalability**: ITCSS allows us to scale our stylesheets effectively, managing specificity and ensuring consistency.\n- **Encapsulation**: Using Shadow DOM ensures that styles within Web Components do not leak out and are not affected by external styles.\n- **Reusability**: CSS custom properties enable us to define shared styles that can be reused across different Web Components, reducing redundancy.\n- **Flexibility**: Custom properties can be dynamically altered, allowing for theming and easier maintenance.\n\n### Negative Consequences\n- **Browser Support**: While CSS custom properties are well-supported, they may not work in all older browsers without polyfills.\n- **Learning Curve**: Developers need to understand both ITCSS and the usage of CSS custom properties, which might involve additional training.\n- **Complexity**: Managing ITCSS in conjunction with Web Component encapsulation might introduce complexity in the setup.\n\n### Implementation Plan\n1. Define a base layer of CSS custom properties adhering to ITCSS principles.\n2. Use these custom properties within the scoped styles of Web Components:\n```css\n:root {\n  --primary-color: #3498db;\n  --font-base: 'Arial, sans-serif';\n  \/* Other variables following ITCSS architecture *\/\n}\n\n:host {\n  color: var(--primary-color);\n  font-family: var(--font-base);\n  \/* Component specific styling *\/\n}\n\n\/* Apply ITCSS for structuring the stylesheet *\/\n```\n3. Ensure developers are familiar with both ITCSS and Shadow DOM encapsulation patterns.\n4. Regularly review and refactor the custom properties to keep the stylesheets DRY and maintainable.\n\nBy using this approach, we maintain the encapsulation benefits of Web Components while leveraging the shared styling advantages provided by CSS custom properties, in line with ITCSS methodologies.","GenTime":"2024-08-20 00:33:09"}
{"File Name":"ditto\/DADR-0004-signal-enrichment.md","Context":"## Context\\nSupporting a new feature, the so called [signal enrichment](https:\/\/github.com\/eclipse-ditto\/ditto\/issues\/561), raises a few\\nquestions towards throughput and scalability impact of that new feature.\\nIn the current architecture, Ditto internally publishes events (as part of the applied \"event sourcing\" pattern) for\\neach change which was done to a `Thing`. This event is the same as the persisted one only containing the actually\\nchanged fields.\\nThe \"signal enrichment\" feature shall support defining `extraFields` to be sent out to external event subscribers, e.g.\\nbeing notified about changes via WebSocket, Server Sent Events (SSEs) or connections (AMQP, MQTT, Kafka, ...).\\nThe following alternatives were considered on how to implement that feature:\\n1. Sending along the complete `Thing` state in each event in the cluster\\n* upside: \"tell, don't ask\" principle -> would lead to a minimum of required cluster remoting \/ roundtrips\\n* downside: bigger payload sent around\\n* downside: a lot of deserialization effort for all event consuming services\\n* downside: policy filtering would have to be additionally done somewhere only included data which the `authSubject` is allowed to READ\\n* downside: overall a lot of overhead for probably only few consumers\\n2. Enriching the data for sessions\/connections which selected `extraFields` for each incoming event\\n* upside: no additional payload for existing events\\n* upside: data is only enriched for sessions\/connections really using that feature\\n* upside: policy enforcement\/filtering is done by default concierge mechanism for each single request, so is always up-to-date with policy\\n* downside: additional 4 remoting (e.g.: gateway-concierge-things-concierge-gateway) calls for each to be enriched event\\n* delayed event publishing\\n* additional deserialization efforts\\n* potentially asking for the same static values each time\\n3. Cache based enriching of the data for sessions\/connections which selected `extraFields` for each incoming event\\n* upsides: all upsides of approach 2 except that policy is always up-to-date\\n* upside: mitigating downsides of approach 2 (because of cache the additional roundtrips are reduced or even completely skipped)\\n* downside: cached data as well as policy information might be outdated a configurable amount of time (e.g. 2 minutes)\\n","Decision":"We provide 2 different facade implementations providing approach 2 and 3:\\n* `org.eclipse.ditto.internal.models.signalenrichment.ByRoundTripSignalEnrichmentFacade`:\\nRound-trip for each to-be-enriched event resulting in a guaranteed up-to-dateness of data and applied policy.\\n* `org.eclipse.ditto.internal.models.signalenrichment.DittoCachingSignalEnrichmentFacade`:\\nUsing cache for each to-be-enriched event resulting in reduced remoting effort and a time interval where the cache might be out of sync with the current data or policy information.\\n* the implementation uses a cluster-instance wide cache using a cache key consisting of: `thingId, authSubjects, jsonFieldSelector`\\n* the overall size of this cache is configured, by default to `20,000` entries\\n* there is an additional \"smart-update\" mechanism for cache entries related to enrichment of twin events:\\nin the absence of skipped events, the cache entry can be completely deduced from the twin events triggering\\nenrichment and will stay up-to-date with thing changes (but not with policy changes).\\nThe configured default in Ditto is the `CachingSignalEnrichmentFacade` but may be configured via\\n* connectivity service: environment variable `CONNECTIVITY_SIGNAL_ENRICHMENT_PROVIDER`\\n* gateway service: environment variable `GATEWAY_SIGNAL_ENRICHMENT_PROVIDER`\\n","tokens":500,"id":775,"text":"## Context\\nSupporting a new feature, the so called [signal enrichment](https:\/\/github.com\/eclipse-ditto\/ditto\/issues\/561), raises a few\\nquestions towards throughput and scalability impact of that new feature.\\nIn the current architecture, Ditto internally publishes events (as part of the applied \"event sourcing\" pattern) for\\neach change which was done to a `Thing`. This event is the same as the persisted one only containing the actually\\nchanged fields.\\nThe \"signal enrichment\" feature shall support defining `extraFields` to be sent out to external event subscribers, e.g.\\nbeing notified about changes via WebSocket, Server Sent Events (SSEs) or connections (AMQP, MQTT, Kafka, ...).\\nThe following alternatives were considered on how to implement that feature:\\n1. Sending along the complete `Thing` state in each event in the cluster\\n* upside: \"tell, don't ask\" principle -> would lead to a minimum of required cluster remoting \/ roundtrips\\n* downside: bigger payload sent around\\n* downside: a lot of deserialization effort for all event consuming services\\n* downside: policy filtering would have to be additionally done somewhere only included data which the `authSubject` is allowed to READ\\n* downside: overall a lot of overhead for probably only few consumers\\n2. Enriching the data for sessions\/connections which selected `extraFields` for each incoming event\\n* upside: no additional payload for existing events\\n* upside: data is only enriched for sessions\/connections really using that feature\\n* upside: policy enforcement\/filtering is done by default concierge mechanism for each single request, so is always up-to-date with policy\\n* downside: additional 4 remoting (e.g.: gateway-concierge-things-concierge-gateway) calls for each to be enriched event\\n* delayed event publishing\\n* additional deserialization efforts\\n* potentially asking for the same static values each time\\n3. Cache based enriching of the data for sessions\/connections which selected `extraFields` for each incoming event\\n* upsides: all upsides of approach 2 except that policy is always up-to-date\\n* upside: mitigating downsides of approach 2 (because of cache the additional roundtrips are reduced or even completely skipped)\\n* downside: cached data as well as policy information might be outdated a configurable amount of time (e.g. 2 minutes)\\n\n\n##Decision\nWe provide 2 different facade implementations providing approach 2 and 3:\\n* `org.eclipse.ditto.internal.models.signalenrichment.ByRoundTripSignalEnrichmentFacade`:\\nRound-trip for each to-be-enriched event resulting in a guaranteed up-to-dateness of data and applied policy.\\n* `org.eclipse.ditto.internal.models.signalenrichment.DittoCachingSignalEnrichmentFacade`:\\nUsing cache for each to-be-enriched event resulting in reduced remoting effort and a time interval where the cache might be out of sync with the current data or policy information.\\n* the implementation uses a cluster-instance wide cache using a cache key consisting of: `thingId, authSubjects, jsonFieldSelector`\\n* the overall size of this cache is configured, by default to `20,000` entries\\n* there is an additional \"smart-update\" mechanism for cache entries related to enrichment of twin events:\\nin the absence of skipped events, the cache entry can be completely deduced from the twin events triggering\\nenrichment and will stay up-to-date with thing changes (but not with policy changes).\\nThe configured default in Ditto is the `CachingSignalEnrichmentFacade` but may be configured via\\n* connectivity service: environment variable `CONNECTIVITY_SIGNAL_ENRICHMENT_PROVIDER`\\n* gateway service: environment variable `GATEWAY_SIGNAL_ENRICHMENT_PROVIDER`\\n","MatchedIds":[2316,3123,786,2791,2309],"Predicted":"## Decision\n**Cache based enriching of the data for sessions\/connections which selected `extraFields` for each incoming event**\n\n### Rationale\nAfter considering the alternatives, the third option of using a cache-based approach seems to strike the best balance between performance and data accuracy. Here are the main reasons for selecting this approach:\n\n1. **Performance and Throughput**:\n    * By using caching, we mitigate the overhead of remoting calls, reducing the number of roundtrips required for each event. This ensures that throughput and latency impacts are minimized in comparison to the other options.\n    * By avoiding the need to repeatedly deserializing the same static values multiple times, we further optimize performance.\n\n2. **Minimal Payload Overhead**:\n    * Unlike the first approach, there is no additional payload for events that don't require enrichment. This maintains the event size close to its original, avoiding unnecessary data transfers.\n    \n3. **Selective Enrichment**:\n    * Enrichment is only applied to sessions or connections that have opted for `extraFields`. This avoids unnecessary computations and payload increases for other consumers, making it an efficient use of resources.\n\n4. **Policy Enforcement**:\n    * While the policy enforcement might not be 100% up to date due to caching, the cache duration can be configured to a minimal time period (e.g., 2 minutes). This makes stale data scenarios rare and somewhat acceptable given the performance benefits achieved.\n    * Cached policy information can be regularly refreshed and invalidated to keep it as current as possible.\n\n### Positive Consequences\n* **Improved Throughput**: Reduced roundtrips and optimized event handling lead to better throughput and lower latency.\n* **Scalability**: Efficient data handling allows the system to scale better under increased load.\n* **Resource Optimization**: Avoids unnecessary enrichment for sessions not requiring extra fields, reducing computation and storage overhead.\n\n### Negative Consequences\n* **Potential Staleness**: Cached data could be slightly out-of-date. However, by setting appropriate cache expiration times, the impact of this can be minimized.\n* **Complexity**: Implementing and maintaining a caching strategy adds some complexity to the system, particularly ensuring that cache invalidation and refresh mechanisms are robust.\n\nIn summary, using a cache-based approach provides a balanced solution that optimizes for performance and scalability while still catering to the functionality required for signal enrichment. This approach ensures that the system remains efficient and performant, meeting the needs of both the new feature and existing services.","GenTime":"2024-08-20 00:33:19"}
{"File Name":"smarthub\/0002-use-lerna-for-package-versioning.md","Context":"## Context\\nSmarthub project consist of multiple packages which are the part of Smarthub SDK. Complex dependency graph forces us to update dependent packages manually every time dependency has changed.\\n","Decision":"Migrate code base to monorepo structure and use `lerna` for versioning management.\\n","tokens":39,"id":3691,"text":"## Context\\nSmarthub project consist of multiple packages which are the part of Smarthub SDK. Complex dependency graph forces us to update dependent packages manually every time dependency has changed.\\n\n\n##Decision\nMigrate code base to monorepo structure and use `lerna` for versioning management.\\n","MatchedIds":[241,3328,4225,1852,1631],"Predicted":"## Decision\nWe will use Lerna to manage dependencies and automate updates within the Smarthub SDK. Lerna provides tooling for managing JavaScript projects with multiple packages and can automate the process of propagating dependency changes throughout the dependency graph. This will simplify maintenance and reduce the manual effort required to keep dependent packages up to date.","GenTime":"2024-08-20 00:33:22"}
{"File Name":"raster-foundry\/adr-0004-static-asset-pipeline.md","Context":"## Context\\nIn selecting a static asset pipeline, there are several factors that are important to us.\\nWe don\u2019t want to lose features that speed up developer feedback, such as a dev server with linting, auto-gen, and browser reload.\\nThe solution needs to support tests run with a single command similar to what we\u2019ve had in previous projects.\\nThe solution needs to be fast.\\nSome team members have expressed dissatisfaction with the stability of the NPM ecosystem, but pretty much everything relies on it to some extent, so the issue is probably best addressed elsewhere (possibly using a lazy NPM cache).\\nIn the past, our team has used Grunt to manage and build static assets. Various team members \/ designers have expressed discontent with the idea of going forward using Grunt, primarily due to inherent factors such as long compile times and configuration complexity.\\nOther teams in Azavea have had good experiences with Webpack. Webpack is not a complete replacement for Grunt, but of the difference in functionality, it is highly likely that NPM tasks will cover it if used in conjunction. Despite scattered complaints of complexity and poor documentation found while researching, I was able to set up pipeline using npm and webpack overnight without too many issues. Significant features which are built in include a dev server capable of supporting hot reloading, very fast feedback on changes, and support for all the tools we commonly use in our applications. In addition to quality of life features, Webpack also gives us a dependency graph which includes static assets and bundle splitting. Combined with NPM, it appears to be a good solution.\\nA final option that we have to consider is Gulp, which has been used on a couple of projects by the Urban Apps team. Where grunt uses configuration files, Gulp uses code. Gulp\u2019s main advantage is speed: streaming operations mean that files never touch disk between chained tasks. Other than the method of configuration and speed of operating, the basic functionality of Gulp and Grunt seem to be fairly aligned.\\n","Decision":"Given the good experiences that others in the company have had with Webpack, we\u2019ve decided to run the static asset pipeline using a combination of npm and webpack. Npm will be used to manage dependencies and initiating tasks, while webpack will be used to run the dev server and build static assets.\\n","tokens":408,"id":1680,"text":"## Context\\nIn selecting a static asset pipeline, there are several factors that are important to us.\\nWe don\u2019t want to lose features that speed up developer feedback, such as a dev server with linting, auto-gen, and browser reload.\\nThe solution needs to support tests run with a single command similar to what we\u2019ve had in previous projects.\\nThe solution needs to be fast.\\nSome team members have expressed dissatisfaction with the stability of the NPM ecosystem, but pretty much everything relies on it to some extent, so the issue is probably best addressed elsewhere (possibly using a lazy NPM cache).\\nIn the past, our team has used Grunt to manage and build static assets. Various team members \/ designers have expressed discontent with the idea of going forward using Grunt, primarily due to inherent factors such as long compile times and configuration complexity.\\nOther teams in Azavea have had good experiences with Webpack. Webpack is not a complete replacement for Grunt, but of the difference in functionality, it is highly likely that NPM tasks will cover it if used in conjunction. Despite scattered complaints of complexity and poor documentation found while researching, I was able to set up pipeline using npm and webpack overnight without too many issues. Significant features which are built in include a dev server capable of supporting hot reloading, very fast feedback on changes, and support for all the tools we commonly use in our applications. In addition to quality of life features, Webpack also gives us a dependency graph which includes static assets and bundle splitting. Combined with NPM, it appears to be a good solution.\\nA final option that we have to consider is Gulp, which has been used on a couple of projects by the Urban Apps team. Where grunt uses configuration files, Gulp uses code. Gulp\u2019s main advantage is speed: streaming operations mean that files never touch disk between chained tasks. Other than the method of configuration and speed of operating, the basic functionality of Gulp and Grunt seem to be fairly aligned.\\n\n\n##Decision\nGiven the good experiences that others in the company have had with Webpack, we\u2019ve decided to run the static asset pipeline using a combination of npm and webpack. Npm will be used to manage dependencies and initiating tasks, while webpack will be used to run the dev server and build static assets.\\n","MatchedIds":[1576,3428,3827,3031,1599],"Predicted":"## Decision\nWe will move forward with using Webpack as our static asset pipeline.\n\n### Reasons:\n1. **Developer Experience**: Webpack includes a built-in dev server capable of supporting hot reloading, linting, and very fast feedback on changes, which aligns well with our goal of not losing features that speed up developer feedback.\n2. **Comprehensive Tooling Support**: Webpack supports all the tools we commonly use in our applications, ensuring a seamless integration with our current workflow.\n3. **Dependency Management**: It provides a dependency graph and bundle splitting, which are advanced features that can aid in both development and optimization.\n4. **Proven Success**: Other teams at Azavea have had positive experiences with Webpack, indicating that it is a stable and reliable choice.\n5. **Ease of Setup**: Initial setup and integration with npm have been straightforward, and any complexity is manageable with proper research and documentation.\n6. **Future-Proofing**: Given its popularity and widespread use in the developer community, Webpack is likely to continue evolving and improving, thereby future-proofing our static asset pipeline.\n\n### Mitigation for Complexity:\n- We will invest time in training and creating internal documentation to address the complexity and poor documentation concerns.\n- Establish best practices and reusable configurations to ensure a smooth onboarding process for new team members.\n\nBy choosing Webpack, we aim to enhance the developer experience, maintain our current feature set, and build a more efficient and maintainable static asset pipeline.","GenTime":"2024-08-20 00:33:27"}
{"File Name":"fundraising-application\/020_PayPal_IPN_Queue.md","Context":"## Context and Problem Statement\\nThe PayPal IPNs on Fundraising Frontend started to fail after a deployment\\nand was only noticed some days after when the Project Manager needed to\\nexport the data. Upon investigation, it was discovered that:\\n* Error logging was inactive on the application. (Now fixed)\\n* We can\u2019t debug using the responses our system returned to PayPal as\\nwe don\u2019t have access to the IPN log.\\nThis led to a situation where we couldn't get the information required\\nto debug the error. It was suggested we queue all incoming requests from\\nPayPal on our own system for processing by our system.\\n## Decision Drivers\\n* **Transparency**: If our system fails we would have a stored queue to\\nuse for debugging.\\n* **Automation**: The IPNs wouldn't need to be fired again once an error\\nbecomes fixed as our system would resume processing the queue.\\n","Decision":"* **Transparency**: If our system fails we would have a stored queue to\\nuse for debugging.\\n* **Automation**: The IPNs wouldn't need to be fired again once an error\\nbecomes fixed as our system would resume processing the queue.\\nSince this was the first occurrence of the problem, and the Fundraising\\nApplication system is now running well again, we decided against introducing\\nthe IPN queue feature.\\n","tokens":190,"id":1523,"text":"## Context and Problem Statement\\nThe PayPal IPNs on Fundraising Frontend started to fail after a deployment\\nand was only noticed some days after when the Project Manager needed to\\nexport the data. Upon investigation, it was discovered that:\\n* Error logging was inactive on the application. (Now fixed)\\n* We can\u2019t debug using the responses our system returned to PayPal as\\nwe don\u2019t have access to the IPN log.\\nThis led to a situation where we couldn't get the information required\\nto debug the error. It was suggested we queue all incoming requests from\\nPayPal on our own system for processing by our system.\\n## Decision Drivers\\n* **Transparency**: If our system fails we would have a stored queue to\\nuse for debugging.\\n* **Automation**: The IPNs wouldn't need to be fired again once an error\\nbecomes fixed as our system would resume processing the queue.\\n\n\n##Decision\n* **Transparency**: If our system fails we would have a stored queue to\\nuse for debugging.\\n* **Automation**: The IPNs wouldn't need to be fired again once an error\\nbecomes fixed as our system would resume processing the queue.\\nSince this was the first occurrence of the problem, and the Fundraising\\nApplication system is now running well again, we decided against introducing\\nthe IPN queue feature.\\n","MatchedIds":[1799,3388,2387,2403,786],"Predicted":"## Decision\nWe will implement a queue to store all incoming PayPal IPNs on our own system for processing. \n\n### Positive Consequences\n* **Increased Transparency**: By queuing the IPNs, we can store the failed requests and use them for debugging purposes.\n* **Automation**: This approach automates the retry mechanism. If the system fails, it will resume processing from the queue without needing to refire the IPNs from PayPal.\n* **Error Handling**: We can enhance error logging and notifications to immediately flag and address issues with incoming IPNs.\n\n### Negative Consequences\n* **Operational Overhead**: Implementing and maintaining the queue system requires additional development and operational resources.\n* **Potential Latency**: There might be some delay in processing the IPNs as they are queued and then processed in order.\n* **System Complexity**: Introducing a queuing mechanism adds complexity to the system and requires careful handling to ensure data integrity and consistency.","GenTime":"2024-08-20 00:33:31"}
{"File Name":"volley-management\/0002-use-sendgrid-as-mailing-system.md","Context":"## Context and Problem Statement\\nWe need a way to send emails to users for several workflows.\\n","Decision":"Chosen option: SendGrid, because it is SaaS and has generous free tier. Integration is easy.\\n### Positive Consequences <!-- optional -->\\n* Fast time to develop\\n### Negative Consequences\\n* We need to think about setting up development environment\\n","tokens":20,"id":2523,"text":"## Context and Problem Statement\\nWe need a way to send emails to users for several workflows.\\n\n\n##Decision\nChosen option: SendGrid, because it is SaaS and has generous free tier. Integration is easy.\\n### Positive Consequences <!-- optional -->\\n* Fast time to develop\\n### Negative Consequences\\n* We need to think about setting up development environment\\n","MatchedIds":[3722,1786,996,5213,2050],"Predicted":"## Decision\nWe will use GOV.UK Notify to send emails to users for our various workflows. Given its integration capabilities via API, ease of template management, and cost-effectiveness (no-cost for emails), it ensures a consistent user experience and minimizes the development overhead of maintaining a separate email infrastructure. This decision will streamline our notification process and leverage an established, reliable government service.","GenTime":"2024-08-20 00:33:36"}
{"File Name":"saas-plaform-tenant-identity-provider\/0002-use-a-hosted-identity-solution-over-a-self-hosted-one.md","Context":"## Context\\nWe need an identity provider for our platform. This could be a host (SaaS) solution vs a self hosted solution. While the Cost & Lockin is higher at scale with a SaaS the self hosted has a higher Management Effort & Less Security & Expensive Bootstraping. From an evolutionary architecture perspective it is not core to value generation and does not change rapidly[source](https:\/\/www.youtube.com\/watch?v=8bEsNT7jdC4&t=112s&index=57&list=WL) - identity will mostlikly the same (OpenIDConnect, Login page ...) but it will be hard to change in the future if you decided for one provider.\\n","Decision":"We will use a SaaS solution as we don't have the skill to host our own solution in the efficient quality. And from a evolutionary architecture perspective the system is commondity or support.\\n","tokens":142,"id":3421,"text":"## Context\\nWe need an identity provider for our platform. This could be a host (SaaS) solution vs a self hosted solution. While the Cost & Lockin is higher at scale with a SaaS the self hosted has a higher Management Effort & Less Security & Expensive Bootstraping. From an evolutionary architecture perspective it is not core to value generation and does not change rapidly[source](https:\/\/www.youtube.com\/watch?v=8bEsNT7jdC4&t=112s&index=57&list=WL) - identity will mostlikly the same (OpenIDConnect, Login page ...) but it will be hard to change in the future if you decided for one provider.\\n\n\n##Decision\nWe will use a SaaS solution as we don't have the skill to host our own solution in the efficient quality. And from a evolutionary architecture perspective the system is commondity or support.\\n","MatchedIds":[4857,5211,3217,4414,1182],"Predicted":"## Decision\n\n* After evaluating the trade-offs between a hosted (SaaS) solution and a self-hosted solution for our platform's identity provider, we should opt for a hosted (SaaS) solution, given our current context and priorities.\n* The hosted solution offers significant benefits in terms of security, lower management overhead, and quicker bootstrapping.\n\n### Positive Consequences\n\n* **Higher Security:** SaaS providers generally offer high levels of security and compliance, which would be costly and complex to replicate in a self-hosted environment.\n* **Lower Management Effort:** Leveraging a hosted solution will significantly reduce the operational overhead for our team, allowing us to focus on core activities and value generation.\n* **Quick Bootstrap:** The time and effort required to get a SaaS identity provider up and running are substantially lower than setting up and securing a self-hosted solution.\n\n### Negative Consequences\n\n* **Higher Cost and Lock-in at Scale:** While SaaS solutions may become more expensive as we scale, the operational savings and reduced management complexity currently outweigh these concerns.\n* **Vendor Lock-in:** Once we commit to a provider, changing providers will be challenging and involve considerable effort and risk. However, this aligns with our evolutionary architecture perspective, where identity management does not change rapidly.\n\n### Selected Solution\n\nWe decide to use Auth0 as our identity provider. Auth0 provides a scalable, secure, and easy-to-manage identity solution, allowing us to meet our current needs without extensive initial investment and management overhead.\n\n### Implementation Steps\n\n1. **Initial Setup:** Set up an Auth0 tenant and configure basic authentication and authorization flows.\n2. **Integration:** Integrate Auth0 with our platform, leveraging OpenID Connect for a standardized identity management approach.\n3. **Testing:** Thoroughly test the integration to ensure seamless user experience and robust security.\n4. **Monitoring:** Set up monitoring and alerting to keep track of authentication metrics and security events.\n\nBy opting for Auth0, we aim to leverage a robust and managed identity solution that minimizes operational overhead and maximizes security and scalability.","GenTime":"2024-08-20 00:33:43"}
{"File Name":"docspell\/0016_custom_fields.md","Context":"# Context and Problem Statement\\nUsers want to add custom metadata to items. For example, for invoices\\nfields like `invoice-number` or `total`\/`amount` make sense. When\\nusing a pagination stamp, every item gets a pagination number.\\nThis is currently not possible to realize in docspell. But it is an\\nessential part when organizing and archiving documents. It should be\\nsupported.\\n# Considered Options\\n","Decision":"- values are strings at the database\\n- values are strings when transported from\/to server\\n- client must provide the correct formatted strings per type\\n- numeric: some decimal number\\n- money: decimal number\\n- text: no restrictions\\n- date: a local date as iso string, e.g. `2011-10-09`\\n- bool: either `\"true\"` or `\"false\"`, case insensitive\\n","tokens":90,"id":285,"text":"# Context and Problem Statement\\nUsers want to add custom metadata to items. For example, for invoices\\nfields like `invoice-number` or `total`\/`amount` make sense. When\\nusing a pagination stamp, every item gets a pagination number.\\nThis is currently not possible to realize in docspell. But it is an\\nessential part when organizing and archiving documents. It should be\\nsupported.\\n# Considered Options\\n\n\n##Decision\n- values are strings at the database\\n- values are strings when transported from\/to server\\n- client must provide the correct formatted strings per type\\n- numeric: some decimal number\\n- money: decimal number\\n- text: no restrictions\\n- date: a local date as iso string, e.g. `2011-10-09`\\n- bool: either `\"true\"` or `\"false\"`, case insensitive\\n","MatchedIds":[3372,1278,4323,2215,2513],"Predicted":"## Decision\nSupport the addition of custom metadata fields to items.\n\nTo implement this, we will introduce a flexible metadata model allowing users to define custom fields associated with each item. This approach will allow users to specify fields such as `invoice-number`, `total`, `amount`, and `pagination-number` based on their needs.\n\nThese custom fields will be part of a new metadata schema that is dynamic and extensible. Users can create, update, and manage custom metadata fields through a dedicated interface. For each item, the metadata will be stored in a way that it can be indexed and easily retrieved for querying and reporting purposes.\n\nKey considerations for this implementation will include:\n- User interface enhancements to allow easy addition and management of custom metadata fields.\n- Backend modifications to support the flexible storage and retrieval of custom metadata fields.\n- Ensuring that custom metadata fields are searchable and filterable to facilitate their use in organization and archiving tasks.\n\nThis decision will improve the usability and flexibility of the docspell system, making it more adaptable to various user requirements for document organization and archival.","GenTime":"2024-08-20 00:33:47"}
{"File Name":"opg-lpa\/0002-custom-save-handler-in-service-front.md","Context":"## Context\\nThe service-front component, written in PHP, uses the default Redis save\\nhandler for persisting session data. In certain situations, the\\napplication may request a resource *A* which takes significant time to deliver,\\nsuch as LPA statuses via the Sirius data API. If resource *A*\\nis requested via an Ajax request, it's possible that the client\\nwill request a new resource *B* before *A* is fully processed. If processing for\\n*B* then completes before processing for *A*, the process for *A* can erroneously\\noverwrite session data added by *B*, resulting in loss of session data required\\nby *A*.\\nThis causes particular problems for CSRF tokens, as shown by this typical sequence\\non service-front:\\n1.  dashboard page loads in browser, triggering client-side Ajax request to statuses controller\\n2.  statuses controller reads session data **S** and initiates (slow) request to Sirius\\nAPI to get LPA statuses\\n3.  meanwhile, user goes to replacement-attorney page; the Ajax request is now redundant, as the\\nuser isn't on the dashboard page any more, but the statuses controller doesn't know this\\n4.  replacement-attorney controller reads session data **S**\\n5.  statuses controller continues processing Sirius response, unaware of new data about to be added to\\nsession by replacement-attorney...\\n6.  replacement-attorney adds CSRF data to session, creating **S'**\\n7.  replacement-attorney page renders form with CSRF token, associated with data in **S'**\\n8.  replacement-attorney writes **S'** to session, including CSRF data\\n9.  statuses page finishes processing, unaware of **S'**; it assumes it has\\nthe correct data **S** and writes it to the session, losing the delta between\\n**S** and **S'** (including the CSRF token!)\\n10. user submits form to replacement-attorney controller with CSRF token in the form\\n11. replacement-attorney controller loads again, but retrieves **S** from session (just written by\\nstatuses controller in 9); this doesn't have the CSRF token (which was in **S'**)\\nfor comparison with the value in the form submitted by the user; CSRF validation fails!\\n","Decision":"Use a custom save handler to prevent certain Ajax requests from writing data to the session.\\nThis will still use Redis as the storage back-end.\\nThe approach is to send some Ajax requests with a custom `X-SessionReadOnly: true` header,\\nimplying that the controller they invoke should only read from the session and never write to it.\\nThe save handler inspects the header on the incoming request and ignores any requests to write\\nthe session if accompanied by this header.\\nPHP 7+ provides a mechanism to only read from the session, via:\\n```\\nsession_start(array('read_and_close' => true))\\n```\\nHowever, the complexity of the processing in the Laminas stack, which does its own session\\nmanagement, overrides any attempts to call this function. Consequently, the pragmatic\\nsolution is to move down the stack to the lower-level save handler, and implement the read-only\\nbehaviour there for requests we know to be problematic.\\n","tokens":490,"id":1654,"text":"## Context\\nThe service-front component, written in PHP, uses the default Redis save\\nhandler for persisting session data. In certain situations, the\\napplication may request a resource *A* which takes significant time to deliver,\\nsuch as LPA statuses via the Sirius data API. If resource *A*\\nis requested via an Ajax request, it's possible that the client\\nwill request a new resource *B* before *A* is fully processed. If processing for\\n*B* then completes before processing for *A*, the process for *A* can erroneously\\noverwrite session data added by *B*, resulting in loss of session data required\\nby *A*.\\nThis causes particular problems for CSRF tokens, as shown by this typical sequence\\non service-front:\\n1.  dashboard page loads in browser, triggering client-side Ajax request to statuses controller\\n2.  statuses controller reads session data **S** and initiates (slow) request to Sirius\\nAPI to get LPA statuses\\n3.  meanwhile, user goes to replacement-attorney page; the Ajax request is now redundant, as the\\nuser isn't on the dashboard page any more, but the statuses controller doesn't know this\\n4.  replacement-attorney controller reads session data **S**\\n5.  statuses controller continues processing Sirius response, unaware of new data about to be added to\\nsession by replacement-attorney...\\n6.  replacement-attorney adds CSRF data to session, creating **S'**\\n7.  replacement-attorney page renders form with CSRF token, associated with data in **S'**\\n8.  replacement-attorney writes **S'** to session, including CSRF data\\n9.  statuses page finishes processing, unaware of **S'**; it assumes it has\\nthe correct data **S** and writes it to the session, losing the delta between\\n**S** and **S'** (including the CSRF token!)\\n10. user submits form to replacement-attorney controller with CSRF token in the form\\n11. replacement-attorney controller loads again, but retrieves **S** from session (just written by\\nstatuses controller in 9); this doesn't have the CSRF token (which was in **S'**)\\nfor comparison with the value in the form submitted by the user; CSRF validation fails!\\n\n\n##Decision\nUse a custom save handler to prevent certain Ajax requests from writing data to the session.\\nThis will still use Redis as the storage back-end.\\nThe approach is to send some Ajax requests with a custom `X-SessionReadOnly: true` header,\\nimplying that the controller they invoke should only read from the session and never write to it.\\nThe save handler inspects the header on the incoming request and ignores any requests to write\\nthe session if accompanied by this header.\\nPHP 7+ provides a mechanism to only read from the session, via:\\n```\\nsession_start(array('read_and_close' => true))\\n```\\nHowever, the complexity of the processing in the Laminas stack, which does its own session\\nmanagement, overrides any attempts to call this function. Consequently, the pragmatic\\nsolution is to move down the stack to the lower-level save handler, and implement the read-only\\nbehaviour there for requests we know to be problematic.\\n","MatchedIds":[560,4843,4011,4848,2256],"Predicted":"## Decision Drivers\n* The need to ensure data consistency across multiple session changes.\n* The impact of session data overwrites, especially regarding CSRF tokens.\n* The existing infrastructure of using Redis for session handling.\n* The potential for performance overhead or impact on service availability.\n\n## Decision\nImplement optimistic locking mechanisms to prevent sessions from being overwritten erroneously.\n\n### Details:\n1. **Use Session Versioning:**\n   - Introduce a version identifier in the session data. Each time session data is modified, the version is incremented.\n   - On write operations, the session handler should check the version of the session data in Redis. If the version has changed since it was read, the write should be aborted.\n\n2. **Optimistic Locking Implementation:**\n   - The session data structure should include a version number.\n   - When a session is read, the session data along with its version is fetched.\n   - Before writing session data back to Redis, check the version number. If the version matches the current session data in Redis, proceed with the write and increment the version number. If not, retry the operation or throw an exception.\n\n3. **Handle Write Conflicts:**\n   - Implement a retry mechanism with a maximum number of attempts if a write conflict is detected.\n   - Log conflicts for monitoring and to gather data for potential further optimization.\n\n4. **Graceful Degradation for Long-Running Ajax Requests:**\n   - For long-running operations like the statuses controller, consider refreshing session data before completing the write operation to ensure it is working with the latest session data.\n\n### Pros:\n- Ensures that session data, especially sensitive data like CSRF tokens, remains consistent.\n- Prevents data loss due to concurrent operations modifying the session.\n- Can be implemented within the existing Redis infrastructure.\n\n### Cons:\n- Introduces additional complexity in session handling code.\n- Potential performance overhead due to version checks and potential retries.\n- Requires careful handling to avoid excessive retry loops or throwing too many exceptions.\n\n### Conclusion:\nImplementing optimistic locking with session versioning addresses the problem of concurrent session modifications, ensuring that sensitive data such as CSRF tokens are not lost and maintaining overall data consistency in the service-front component. This solution leverages the existing Redis infrastructure while providing a mechanism to handle conflicting session writes gracefully.","GenTime":"2024-08-20 00:33:57"}
{"File Name":"pcmt\/adr-007.md","Context":"## Context\\nPCMT wants to accomplish a number of goals through the use of versions:\\n1. Convey to stakeholders the initial suitability of the project:\\n* Pre-v1: In development for global stakeholders.\\n* Version 1: Production ready for global stakeholders.\\n* Version 2: Production ready for national stakeholders.\\n1. Leverage semantic versioning to convey to dependents the nature of the\\nrelease.  e.g. is it a patch-release, major new functionality, etc.\\n1. Mark a specific state of the code-base, tied to a released asset.\\n1. Indicate which version of Akeneo that PCMT is derived from.\\n","Decision":"1. We will version PCMT by [Semantic Versioning 2.0][semver].  However since\\nDocker tags do not support the use of the `+` symbol, we'll use `-` in\\nits place.\\n1. We will have a build number (from the commit SHA), which is from a CI\\npipeline, that includes a distinct set of commits.\\n1. We will expire and remove old Build numbers, so they need to be pruned\\nregularly.\\n1. We will not publish assets if a pipeline fails, and conversely if a pipeline\\nis re-run, it consists of the same commits (state of the code), and therefore\\nit should have the same build number.\\n1. We will promote build numbers to a showcase or UAT server, manually.\\n1. We will promote build numbers to a release, manually.\\n1. We will keep the semantic version of the system separate from the build\\nnumber, in a file in the SCM.  When an asset is labeled from\\nthe CI pipeline, it'll use the full form by placing the build number in\\nthe build meta-data of the Semantic Version.  e.g. `1.0.0+48af4a30`\\n1. When promoting to a release we will co-label the published assets such that\\none asset will have the build number, and the released asset will not include\\nthat number. e.g. version 1.0.0 might be `image:v1.0.0+SHA` and\\n`image:v1.0.0`, which are equivalent assets.\\n","tokens":141,"id":2951,"text":"## Context\\nPCMT wants to accomplish a number of goals through the use of versions:\\n1. Convey to stakeholders the initial suitability of the project:\\n* Pre-v1: In development for global stakeholders.\\n* Version 1: Production ready for global stakeholders.\\n* Version 2: Production ready for national stakeholders.\\n1. Leverage semantic versioning to convey to dependents the nature of the\\nrelease.  e.g. is it a patch-release, major new functionality, etc.\\n1. Mark a specific state of the code-base, tied to a released asset.\\n1. Indicate which version of Akeneo that PCMT is derived from.\\n\n\n##Decision\n1. We will version PCMT by [Semantic Versioning 2.0][semver].  However since\\nDocker tags do not support the use of the `+` symbol, we'll use `-` in\\nits place.\\n1. We will have a build number (from the commit SHA), which is from a CI\\npipeline, that includes a distinct set of commits.\\n1. We will expire and remove old Build numbers, so they need to be pruned\\nregularly.\\n1. We will not publish assets if a pipeline fails, and conversely if a pipeline\\nis re-run, it consists of the same commits (state of the code), and therefore\\nit should have the same build number.\\n1. We will promote build numbers to a showcase or UAT server, manually.\\n1. We will promote build numbers to a release, manually.\\n1. We will keep the semantic version of the system separate from the build\\nnumber, in a file in the SCM.  When an asset is labeled from\\nthe CI pipeline, it'll use the full form by placing the build number in\\nthe build meta-data of the Semantic Version.  e.g. `1.0.0+48af4a30`\\n1. When promoting to a release we will co-label the published assets such that\\none asset will have the build number, and the released asset will not include\\nthat number. e.g. version 1.0.0 might be `image:v1.0.0+SHA` and\\n`image:v1.0.0`, which are equivalent assets.\\n","MatchedIds":[241,1048,5177,4424,939],"Predicted":"## Decision\nPCMT will adopt the following versioning strategy:\n\n1. **Version Suitability for Stakeholders:**\n   - **Pre-v1:** Indicate that the project is in development and not yet stable for global stakeholders.\n   - **Version 1.x.x:** Indicate that the project is production-ready for global stakeholders.\n   - **Version 2.x.x:** Indicate that the project is production-ready for national stakeholders.\n\n2. **Semantic Versioning:**\n   - PCMT will use semantic versioning to communicate the nature of each release:\n     - **Major version (`X.0.0`)**: Indicates incompatible API changes or major new functionality.\n     - **Minor version (`0.X.0`)**: Indicates backwards-compatible new features.\n     - **Patch version (`0.0.X`)**: Indicates backward-compatible bug fixes.\n\n3. **Version Tagging and Released Assets:**\n   - Each version will mark a specific state of the codebase and will be tied to a released asset (e.g., a packaged distribution or release artifact).\n\n4. **Indicating Akeneo Derivation:**\n   - The versioning scheme will include metadata or documentation indicating the specific version of Akeneo from which PCMT is derived. For instance, the project documentation or release notes will state the Akeneo version, ensuring transparency about the underlying base of PCMT.\n\nBy implementing this versioning strategy, PCMT will adequately convey the project's status, nature of updates, and its relationship to Akeneo to all stakeholders and dependents.","GenTime":"2024-08-20 00:34:03"}
{"File Name":"register-a-food-business-front-end\/0002-implement-open-source-govuk-react-components-library.md","Context":"## Context\\nThe website needs to comply with the GDS design language. In the future, the standard GDS design language might need to be themed to match food.gov colours, fonts, etc.\\n","Decision":"We will implement the open source govuk-react npm package hosted at [https:\/\/github.com\/penx\/govuk-react](https:\/\/github.com\/penx\/govuk-react) wherever it has a component that fits our needs.\\nWe will create custom components and layouts where necessary to follow the GDS design language.\\nWe will contribute back to the project to ensure it fulfils all of our needs.\\n","tokens":41,"id":3037,"text":"## Context\\nThe website needs to comply with the GDS design language. In the future, the standard GDS design language might need to be themed to match food.gov colours, fonts, etc.\\n\n\n##Decision\nWe will implement the open source govuk-react npm package hosted at [https:\/\/github.com\/penx\/govuk-react](https:\/\/github.com\/penx\/govuk-react) wherever it has a component that fits our needs.\\nWe will create custom components and layouts where necessary to follow the GDS design language.\\nWe will contribute back to the project to ensure it fulfils all of our needs.\\n","MatchedIds":[783,2386,1894,2410,4041],"Predicted":"## Decision\nWe will implement the GDS Design System to ensure compliance with the GDS design language. To enable future theming to match Food Standards Agency (FSA) colors, fonts, and other branding elements, we will:\n\n1. Use the `GOV.UK Frontend` library which allows for customization through SASS variables.\n2. Establish a custom SASS theme that overrides the GDS defaults with FSA-specific styles.\n3. Document the process for adding and modifying the theming to ensure consistency and ease of future updates.\n\nThis approach ensures immediate compliance with GDS guidelines while maintaining flexibility for future theming requirements.","GenTime":"2024-08-20 00:34:06"}
{"File Name":"documents-api\/0002-use-base64-content-for-file-uploading.md","Context":"## Context\\nThere are a number of ways an API could allow clients to upload files to S3, the popular ones:\\n- Allow the API to accept Base 64 encoded files in a JSON POST request and subsequently send this blob to S3\\n- Allow the API to accept multipart form uploads, compile the parts on the server then send the file to S3\\n- Use the S3 Presigned URL functionality, which allows the client to act as the IAM which created the URL for a single operation, and upload the file directly to S3 themselves\\n","Decision":"We decided to use the first option (base 64 encoded uploads), for the following reason:\\n- We do not want to expose any AWS links to outside parties. This ensures that all access to AWS resources is controlled by Hackney authentication mechanisms.\\n","tokens":113,"id":3987,"text":"## Context\\nThere are a number of ways an API could allow clients to upload files to S3, the popular ones:\\n- Allow the API to accept Base 64 encoded files in a JSON POST request and subsequently send this blob to S3\\n- Allow the API to accept multipart form uploads, compile the parts on the server then send the file to S3\\n- Use the S3 Presigned URL functionality, which allows the client to act as the IAM which created the URL for a single operation, and upload the file directly to S3 themselves\\n\n\n##Decision\nWe decided to use the first option (base 64 encoded uploads), for the following reason:\\n- We do not want to expose any AWS links to outside parties. This ensures that all access to AWS resources is controlled by Hackney authentication mechanisms.\\n","MatchedIds":[3797,3416,5099,2515,4533],"Predicted":"## Decision\nWe will use the S3 Presigned URL functionality for file uploads. \n\n### Reasons:\n1. **Security:** Presigned URLs grant temporary, limited access to S3, reducing the risk and attack surface area compared to directly handling files on the backend. This ensures that clients can upload files to S3 without exposing our AWS credentials.\n   \n2. **Scalability:** Uploading files directly to S3 offloads the I\/O burden from our API servers, allowing them to efficiently handle other tasks. This is particularly beneficial when dealing with large files or high volumes of uploads.\n\n3. **Performance:** By allowing clients to upload files directly to S3, we reduce latency, as the file doesn't need to make an additional hop via our servers. This provides a quicker user experience for file uploads.\n\n4. **Costs:** Direct uploads to S3 can potentially reduce server costs, as we eliminate data transfer and processing overhead associated with handling large file uploads on our API servers.\n\n5. **Implementation Simplicity:** Using presigned URLs requires minimal changes to our backend, as the majority of the file-handling logic is delegated to the S3 service.\n\n### Drawbacks and Mitigations:\n1. **Handling Large Files:** While presigned URLs work well for most use cases, multipart uploads might be necessary for very large files. AWS S3 supports multipart uploads with presigned URLs, which can be utilized if needed.\n \n2. **Client Complexity:** Clients need to handle the upload process. However, generating presigned URLs is straightforward, and libraries in various programming languages support this workflow.\n\n### Alternative Options Considered:\n- **Base64 Encoded Files:** This would increase the payload size, causing higher network usage and slower upload times. Moreover, it requires additional processing on the server to decode and validate the file.\n- **Multipart Form Uploads:** This method centralizes file handling, which can impose significant load on the server, leading to potential bottlenecks and scalability issues. \n\nBy adopting the S3 Presigned URL approach, we ensure a secure, efficient, and scalable file upload mechanism.","GenTime":"2024-08-20 00:34:15"}
{"File Name":"front-end-monorepo\/adr-15.md","Context":"## Context\\nThe way the drawing tools currently function on Panoptes-Front-End (PFE) have numerous issues including:\\n- Updating the classification annotation on each touch or pointer event which causes unnecessary re-rendering of the DOM\\n- The separation concerns are not clear between components and stores. Multiple components can update the annotation making it hard to debug or add new features to.\\n- Example: The `MarkingsRenderer` and the `FrameAnnotator` both call change handlers that update the classification annotation? Can the drawing annotation be updated by both or is one solely responsible? It is unclear by reading the code. Why does something named `MarkingsRenderer` update the annotation?\\n- Drawing tools have a complex API that involves exposing static methods to be called by their parent component\\n- Annotation \/ classification payloads have no consistent standards for describing data: some tools mark rotation in differing directions, for example.\\n","Decision":"What we do not want to do:\\n- Re-render on every pointer or touch event.\\n- update annotation state while drawing is in progress.\\n- support more than one drawing task in a step.\\n- Use D3.js since it has its own internal data store and it would be complicated to integrate that with a observable stream.\\nWhat we do want to do:\\n- Have a component, the interactive layer, that manages the interaction with the marks and pointer and touch events.\\n- The interactive layer should not allow events to bubble so the events are encapsulated to just the interaction with the subject. This is to help prevent browser scroll during drawing. An attempted fix on PFE for reference: zooniverse\/Panoptes-Front-End#5411\\n- Events will be observed and be streamed via an observable. We will use rx.js to create an observer\/observable event stream.\\n- The last state of the event stream will be used to add an annotation to the classification when next or done is clicked.\\n- The interactive layer will have local component state to keep track of which mark is selected for editing as well as handle the edit and delete events.\\n- Have a component, the markings renderer, that manages the rendering of previous annotation marks as read only. It will hide them if hide previous marks is toggled.\\n- These two components will initially use separate SVG layers that connect to the same stores. Later any duplicated code will be refactored into a higher order component.\\n- Have a multi-image subject viewer. We will have to support projects that want each frame to have independent drawing annotations as well as projects that want each frame to have the same drawing annotations. Each frame should have the same pan\/zoom function. We have two proposed options in implementation:\\n-  Create an interactive layer and markings renderer for each frame. Each interactive layer will initialize its own event stream.\\n- Create a single interactive layer and markings renderer and filter what is rendered by the frame index.\\n- Projects have requested each frame to have the same pan\/zoom function, but we were unable to implement in PFE: zooniverse\/Panoptes-Front-End#3465\\n- Are there any cases where projects want separate pan\/zoom function for each frame?\\n- Have a schema, or set of schemas, describing annotations.\\n","tokens":187,"id":528,"text":"## Context\\nThe way the drawing tools currently function on Panoptes-Front-End (PFE) have numerous issues including:\\n- Updating the classification annotation on each touch or pointer event which causes unnecessary re-rendering of the DOM\\n- The separation concerns are not clear between components and stores. Multiple components can update the annotation making it hard to debug or add new features to.\\n- Example: The `MarkingsRenderer` and the `FrameAnnotator` both call change handlers that update the classification annotation? Can the drawing annotation be updated by both or is one solely responsible? It is unclear by reading the code. Why does something named `MarkingsRenderer` update the annotation?\\n- Drawing tools have a complex API that involves exposing static methods to be called by their parent component\\n- Annotation \/ classification payloads have no consistent standards for describing data: some tools mark rotation in differing directions, for example.\\n\n\n##Decision\nWhat we do not want to do:\\n- Re-render on every pointer or touch event.\\n- update annotation state while drawing is in progress.\\n- support more than one drawing task in a step.\\n- Use D3.js since it has its own internal data store and it would be complicated to integrate that with a observable stream.\\nWhat we do want to do:\\n- Have a component, the interactive layer, that manages the interaction with the marks and pointer and touch events.\\n- The interactive layer should not allow events to bubble so the events are encapsulated to just the interaction with the subject. This is to help prevent browser scroll during drawing. An attempted fix on PFE for reference: zooniverse\/Panoptes-Front-End#5411\\n- Events will be observed and be streamed via an observable. We will use rx.js to create an observer\/observable event stream.\\n- The last state of the event stream will be used to add an annotation to the classification when next or done is clicked.\\n- The interactive layer will have local component state to keep track of which mark is selected for editing as well as handle the edit and delete events.\\n- Have a component, the markings renderer, that manages the rendering of previous annotation marks as read only. It will hide them if hide previous marks is toggled.\\n- These two components will initially use separate SVG layers that connect to the same stores. Later any duplicated code will be refactored into a higher order component.\\n- Have a multi-image subject viewer. We will have to support projects that want each frame to have independent drawing annotations as well as projects that want each frame to have the same drawing annotations. Each frame should have the same pan\/zoom function. We have two proposed options in implementation:\\n-  Create an interactive layer and markings renderer for each frame. Each interactive layer will initialize its own event stream.\\n- Create a single interactive layer and markings renderer and filter what is rendered by the frame index.\\n- Projects have requested each frame to have the same pan\/zoom function, but we were unable to implement in PFE: zooniverse\/Panoptes-Front-End#3465\\n- Are there any cases where projects want separate pan\/zoom function for each frame?\\n- Have a schema, or set of schemas, describing annotations.\\n","MatchedIds":[498,1193,3815,4613,5140],"Predicted":"## Decision\n\n### 1. Annotation Management:\n**Single Source of Truth for Annotations:**\nWe will create a centralized Annotation Store that will exclusively manage the state of annotations. This will eliminate the ambiguity about which component is responsible for updating the annotation.\n\n**Decoupling Rendering from Annotation Management:**\n- `MarkingsRenderer` will be refactored to focus solely on rendering and will not update annotations.\n- `FrameAnnotator` will manage the interaction logic and update the annotations via the centralized Annotation Store.\n\n### 2. Event Handling:\n**Throttle Annotation Updates:**\nAnnotations will be updated only after the user completes drawing (e.g., on `pointerup` or after a debounce period). This will minimize unnecessary re-renders of the DOM.\n\n**Optimizing Event Handling:**\nImplement a debounced or throttled approach for handling pointer or touch events to ensure that the UI does not unnecessarily re-render with every small movement.\n\n### 3. Standardize API:\n**Consistent Annotation Payload:**\nDefine a standard schema for annotation payloads. For instance, rotation will be consistently represented in a fixed direction and unit (e.g., degrees clockwise).\n\n**Simplifying Tool API:**\nRefactor the drawing tool APIs to be instance methods rather than static methods. This will help in maintaining state within the component instance and make the API more intuitive.\n\n### 4. Documentation and Debugging:\n**Clear Documentation:**\nDocument the responsibilities of each component (`MarkingsRenderer`, `FrameAnnotator`, etc.) and how they interact with the Annotation Store. This will ensure that future developers understand the separation of concerns.\n\n**Enhanced Debugging:**\nImplement logging within the centralized Annotation Store to track changes to annotations. This will make it easier to debug issues related to annotation updates.\n\n### 5. Testing:\n**Comprehensive Testing:**\nIntroduce unit tests and integration tests to verify that components are correctly rendering and updating annotations as per the new architecture. Ensure that edge cases, like rapid drawing movements, are appropriately handled.\n\nBy implementing these changes, we aim to improve performance, enhance maintainability, and ensure a clearer separation of concerns within the Panoptes-Front-End drawing tools.","GenTime":"2024-08-20 00:34:22"}
{"File Name":"dos-capacity-status-api\/002-shared_database.md","Context":"## Context\\nThis is not an external API to DoS, it is part of the DoS ecosystem. The operation of the API is updating the capacity of services in DoS, for visible in the DoS UI and applications that use Core DoS data.\\n","Decision":"The relevant content of the database is shared between the Core DoS application and the Capacity Status API. The decision was made to have the API use and update the Core DoS database. This approach is the most logical solution at this time, as any other alternative would likely need to incorporate some kind of interim internal API between the Capacity Status API and the Core DoS database. Having an interim API would be replicating functionality of the Capacity Status API and would therefore be redundant.\\n","tokens":54,"id":3937,"text":"## Context\\nThis is not an external API to DoS, it is part of the DoS ecosystem. The operation of the API is updating the capacity of services in DoS, for visible in the DoS UI and applications that use Core DoS data.\\n\n\n##Decision\nThe relevant content of the database is shared between the Core DoS application and the Capacity Status API. The decision was made to have the API use and update the Core DoS database. This approach is the most logical solution at this time, as any other alternative would likely need to incorporate some kind of interim internal API between the Capacity Status API and the Core DoS database. Having an interim API would be replicating functionality of the Capacity Status API and would therefore be redundant.\\n","MatchedIds":[779,1180,200,4424,1050],"Predicted":"## Decision\n\nWe will implement the capacity update API endpoint within the existing DoS ecosystem using the following specifications:\n\n* **Endpoint:** `\/services\/{serviceId}\/capacity`\n* **HTTP Method:** `PUT`\n* **Payload:** The payload will be in JSON format containing the updated capacity information.\n* **Request Headers:** \n  * `Content-Type`: `application\/json`\n  * `Authorization`: A token-based authentication mechanism to ensure secure access within the DoS ecosystem.\n* **Response:** The API will return a `200 OK` status with the updated capacity details in JSON format upon a successful update. For errors, a suitable HTTP status code and error message will be returned.\n\n### Example Request\n```\nPUT \/services\/12345\/capacity HTTP\/1.1\nHost: dos.example.com\nContent-Type: application\/json\nAuthorization: Bearer <token>\n\n{\n    \"capacity\": 50,\n    \"timestamp\": \"2023-10-01T12:00:00Z\"\n}\n```\n\n### Example Response (Success)\n```\nHTTP\/1.1 200 OK\nContent-Type: application\/json\n\n{\n    \"serviceId\": 12345,\n    \"capacity\": 50,\n    \"timestamp\": \"2023-10-01T12:00:00Z\"\n}\n```\n\nThis approach ensures the API is aligned with existing DoS ecosystem practices while providing the necessary functionality to update service capacities.","GenTime":"2024-08-20 00:34:28"}
{"File Name":"dapr\/CLI-001-cli-and-runtime-versioning.md","Context":"## Context\\nAs we formally establish Dapr component version, we need to decide if we want to couple CLI versions with runtime versions.\\n","Decision":"* We'll keep CLI versioning and runtime versioning separate.\\n* CLI will pull down latest runtime binary during the *init()* command.\\n* Version scheme is: *major.minor.revision.build* for both CLI and runtime.\\n","tokens":28,"id":78,"text":"## Context\\nAs we formally establish Dapr component version, we need to decide if we want to couple CLI versions with runtime versions.\\n\n\n##Decision\n* We'll keep CLI versioning and runtime versioning separate.\\n* CLI will pull down latest runtime binary during the *init()* command.\\n* Version scheme is: *major.minor.revision.build* for both CLI and runtime.\\n","MatchedIds":[241,85,939,3653,1048],"Predicted":"## Decision\nWe will decouple CLI versions from runtime versions.\n\n* **Rationale:**\n  - This allows independent release cycles for CLI and runtime, enabling faster iterations and bug fixes for the CLI without waiting for runtime updates.\n  - Users can upgrade their CLI tool without being forced to upgrade the entire runtime, which can be crucial in production environments where runtime stability is a priority.\n  - This will also enable easier backward compatibility support, where users can choose CLI versions that are compatible with older runtime versions if needed.\n\n* **Implementation:**\n  - Clearly document the compatibility matrix between CLI and runtime versions.\n  - Ensure that our release process includes comprehensive testing across different combinations of CLI and runtime versions to guarantee compatibility.\n  - Develop a version check feature within the CLI that can notify users if they are using an incompatible combination of CLI and runtime versions.","GenTime":"2024-08-20 00:34:33"}
{"File Name":"aws_infrastructure\/0005-use-terraform-as-infrastructure-as-code.md","Context":"## Context\\nNeed of using Infrastructure as code to, track what resources are running in the AWS account\\n","Decision":"Use Terraform\\n","tokens":21,"id":1464,"text":"## Context\\nNeed of using Infrastructure as code to, track what resources are running in the AWS account\\n\n\n##Decision\nUse Terraform\\n","MatchedIds":[2062,2471,4358,2997,4937],"Predicted":"## Decision\nWe will use Terraform to implement Infrastructure as Code (IaC) to track and manage all resources in our AWS account. Terraform's state management will enable us to maintain an accurate and version-controlled log of all deployed resources, ensuring consistency and auditability across our environments. Additionally, Terraform's extensive library of AWS providers and modules will facilitate the declarative management of our infrastructure, promoting best practices and reducing the risk of configuration drift.","GenTime":"2024-08-20 00:34:36"}
{"File Name":"docs\/0018-requirements-application-composition.md","Context":"## Context and Problem Statement\\nWe want to have clear and simple requirements when it comes to the way applications can be created in the user interface.\\n## Decision Drivers\\n* MUST be compatible with Lombok\\n","Decision":"* MUST be compatible with Lombok\\nChosen option: the first option, since we want a simple solution, in order to have the system running as soon as possible.\\n### Positive Consequences\\n* Lombok can be used.\\n* Better code quality.\\n### Negative consequences\\n* Applications cannot be created using other existing applications.\\n","tokens":42,"id":4695,"text":"## Context and Problem Statement\\nWe want to have clear and simple requirements when it comes to the way applications can be created in the user interface.\\n## Decision Drivers\\n* MUST be compatible with Lombok\\n\n\n##Decision\n* MUST be compatible with Lombok\\nChosen option: the first option, since we want a simple solution, in order to have the system running as soon as possible.\\n### Positive Consequences\\n* Lombok can be used.\\n* Better code quality.\\n### Negative consequences\\n* Applications cannot be created using other existing applications.\\n","MatchedIds":[159,2502,4361,389,1545],"Predicted":"## Decision\nChosen option: Use a standardized approach with Spring Boot\n\n### Rationale\nSpring Boot offers a clear and straightforward mechanism to build applications and is highly compatible with Lombok, a Java library that automatically plugs into your editor and build tools, spicing up your Java.\n\n### Positive Consequences\n- **Compatibility with Lombok:** Ensures that the application development process is streamlined and reduces boilerplate code, making it easier and faster to write and maintain code.\n- **Clear and Simple Requirements:** Spring Boot's convention-over-configuration approach will help in maintaining clear guidelines and requirements for application creation.\n- **Ease of Use:** Spring Boot\u2019s comprehensive documentation and widespread community support will aid developers in adhering to the defined requirements easily.\n- **Scalability:** Will support the application as it grows in complexity and scale, adjusting resources and configurations dynamically.\n\n### Negative Consequences\n- **Learning Curve:** Some developers might need time to familiarize themselves with Spring Boot if they haven't used it before.\n- **Initial Setup:** There might be an initial time investment to set up the standardized environment and ensure all developers are on board with using Spring Boot effectively.","GenTime":"2024-08-20 00:34:40"}
{"File Name":"james\/0005-distributed-task-termination-ackowledgement.md","Context":"## Context\\nBy switching the task manager to a distributed implementation, we need to be able to execute a `Task` on any node of the cluster.\\nWe need a way for nodes to be signaled of any termination event so that we can notify blocking clients.\\n","Decision":"* Creating a `RabbitMQEventHandler` which publish `Event`s pushed to the task manager's event system to RabbitMQ\\n* All the events which end a `Task` (`Completed`, `Failed`, and `Canceled`) have to be transmitted to other nodes\\n","tokens":54,"id":2121,"text":"## Context\\nBy switching the task manager to a distributed implementation, we need to be able to execute a `Task` on any node of the cluster.\\nWe need a way for nodes to be signaled of any termination event so that we can notify blocking clients.\\n\n\n##Decision\n* Creating a `RabbitMQEventHandler` which publish `Event`s pushed to the task manager's event system to RabbitMQ\\n* All the events which end a `Task` (`Completed`, `Failed`, and `Canceled`) have to be transmitted to other nodes\\n","MatchedIds":[2127,2140,2499,885,279],"Predicted":"## Decision\nUtilize a distributed messaging system, like Apache Kafka or RabbitMQ, to handle signaling termination events across nodes. \n\n- **Event Generation**: Each node, upon task completion, will publish a termination event to a specific topic\/queue on the messaging system.\n- **Event Consumption**: All nodes in the cluster will have consumers subscribed to the termination event topic\/queue.\n- **Client Notification**: Once a node receives a termination event, it will notify any blocking clients waiting for the task's completion.\n\nImplementing this solution ensures that task completion events are propagated efficiently across the entire cluster, thereby allowing any node to notify clients about task statuses.","GenTime":"2024-08-20 00:34:44"}
{"File Name":"klokwrk-project\/0003-cqrs-and-event-sourcing-for-applications.md","Context":"## Context\\nWhen building monoliths or microservices that contain non-trivial business logic, we want to have a high-level component architecture pattern that natively leverages a strategic Domain Driven\\nDesign (DDD) concepts like ubiquitous language and bounded context. It should also support or allow, at least, the usage of application design architecture patterns like hexagonal architecture.\\nAt the lower technical level, component architecture pattern should provide native support for tactical DDD concepts like aggregates, entities, value objects, and events.\\nWhen building microservices, the chosen component architecture pattern should natively support asynchronous communication through events.\\nIt would be ideal to have all of these in a single coherent open-source framework and platform.\\nFor anemic CRUD style applications, more traditional approaches, like layered architecture, would be just fine.\\n### Architectural Context\\n* System (`klokwrk-project`)\\n","Decision":"**We will use the Axon framework and Axon Server as a mature open-source implementation of the CQRS\/ES component architecture pattern.**\\nEspecially as one of the major `klokwrk-project` goals is to demonstrate the creation of complex CQRS\/ES microservice applications. We can use more traditional approaches like classic layered\\narchitecture and anemic model for anemic CRUD style applications.\\n","tokens":182,"id":4896,"text":"## Context\\nWhen building monoliths or microservices that contain non-trivial business logic, we want to have a high-level component architecture pattern that natively leverages a strategic Domain Driven\\nDesign (DDD) concepts like ubiquitous language and bounded context. It should also support or allow, at least, the usage of application design architecture patterns like hexagonal architecture.\\nAt the lower technical level, component architecture pattern should provide native support for tactical DDD concepts like aggregates, entities, value objects, and events.\\nWhen building microservices, the chosen component architecture pattern should natively support asynchronous communication through events.\\nIt would be ideal to have all of these in a single coherent open-source framework and platform.\\nFor anemic CRUD style applications, more traditional approaches, like layered architecture, would be just fine.\\n### Architectural Context\\n* System (`klokwrk-project`)\\n\n\n##Decision\n**We will use the Axon framework and Axon Server as a mature open-source implementation of the CQRS\/ES component architecture pattern.**\\nEspecially as one of the major `klokwrk-project` goals is to demonstrate the creation of complex CQRS\/ES microservice applications. We can use more traditional approaches like classic layered\\narchitecture and anemic model for anemic CRUD style applications.\\n","MatchedIds":[2502,4889,2696,302,3925],"Predicted":"## Decision\nWe will adopt a component architecture pattern that emphasizes Domain Driven Design (DDD) principles, such as ubiquitous language and bounded context, for complex business logic. This pattern will inherently support hexagonal architecture. At the technical level, it will accommodate tactical DDD elements such as aggregates, entities, value objects, and events. For microservices, we will ensure that the architecture supports asynchronous communication through events. We will select an open-source framework and platform that cohesively integrates these features.\n\nFor simpler CRUD-based applications, where business logic is not intricate, more traditional approaches such as layered architecture will be utilized.","GenTime":"2024-08-20 00:34:47"}
{"File Name":"hmpps-interventions-docs\/0005-for-release-prioritise-consistency-over-availability.md","Context":"## Context\\n1. We have a fixed deadline\\n1. We will have our first users week beginning 7 June\u00a02021\\n1. For our Delius integration, we rely on an intricate set of reference data\\n1. We are uncertain if there are any issues with the reference data for all possible combinations of offenders\\n1. We rely on these integrations to set up \"container\" data in nDelius. If they fail, we need to recover before providers book appointments\\nWe want to ensure when updates fail, we have a way to resume that retries the interaction.\\nHowever, **we do not have retry mechanisms in place yet**.\\nWe could add an admin task\/script to retry a certain event (we use Spring Application Events that don't\\ncreate side effects, so we could retrigger those).\\nWe feel this would create an overhead that would be too much administrative burden together with\\nthe anticipated noise of most users starting the service at the same time.\\n","Decision":"Due to the uncertainties and lack of convenient retry mechanism,\\nwe favour **consistency over availabilty** in the short term.\\n","tokens":201,"id":1799,"text":"## Context\\n1. We have a fixed deadline\\n1. We will have our first users week beginning 7 June\u00a02021\\n1. For our Delius integration, we rely on an intricate set of reference data\\n1. We are uncertain if there are any issues with the reference data for all possible combinations of offenders\\n1. We rely on these integrations to set up \"container\" data in nDelius. If they fail, we need to recover before providers book appointments\\nWe want to ensure when updates fail, we have a way to resume that retries the interaction.\\nHowever, **we do not have retry mechanisms in place yet**.\\nWe could add an admin task\/script to retry a certain event (we use Spring Application Events that don't\\ncreate side effects, so we could retrigger those).\\nWe feel this would create an overhead that would be too much administrative burden together with\\nthe anticipated noise of most users starting the service at the same time.\\n\n\n##Decision\nDue to the uncertainties and lack of convenient retry mechanism,\\nwe favour **consistency over availabilty** in the short term.\\n","MatchedIds":[1523,3915,3361,929,560],"Predicted":"## Decision Drivers\n* **Fixed Deadline**: We must ensure that the system is ready for the first users by the week beginning 7 June 2021.\n* **Reliability**: It is critical that the Delius integration works reliably due to the intricate set of reference data involved.\n* **Error Recovery**: We need a robust way to recover from failures to avoid disruption in setting up \"container\" data in nDelius.\n* **Minimize Administrative Overhead**: The solution should not create significant administrative burdens, especially with the anticipated increase in user activity.\n\n## Decision\nTo ensure reliability and reduce administrative burden, we will implement an automated retry mechanism for Delius integration failures.\n\n**Implementation Details:**\n1. **Automated Retry Mechanism**: We will develop a basic, automatic retry mechanism for Spring Application Events. This will involve configuring a retry template that retries failed interactions with exponential backoff and a maximum attempt limit.\n2. **Monitoring and Alerts**: We will set up monitoring and alerting for retry failures. This will help us quickly identify and intervene if retries are not successful after the maximum attempts.\n3. **Fallback Script**: As a fallback, we will develop a minimal admin task\/script that can manually retrigger the specific failed events. This will serve as a backup plan if the automated retry mechanism encounters issues.\n\n**Pros:**\n* Ensures that critical integrations can automatically recover from transient failures.\n* Minimizes the administrative overhead by reducing the need for manual intervention.\n* Provides a fallback mechanism that can be used in worst-case scenarios.\n\n**Cons:**\n* Initial development and testing of the retry mechanism might take additional time.\n* Monitoring and alerting setup require configuration and might introduce new complexities.\n\nBy implementing the automated retry mechanism and monitoring, we balance reliability with minimal administrative burden, meeting the fixed deadline and ensuring a smoother onboarding experience for the first users.","GenTime":"2024-08-20 00:34:57"}
{"File Name":"uniffi-rs\/0004-only-threadsafe-interfaces.md","Context":"## Context and Problem Statement\\n[ADR-0003](0003-threadsafe-interfaces.md) introduced support for \"thread-safe\\ninterfaces\" - possibly leading to the impression that there is such a thing as\\nnon-threadsafe interfaces and confusion about exactly what the attribute means.\\nHowever, the entire concept of non-threadsafe interfaces is a misconception -\\nthe Rust compiler insists that everything wrapped by uniffi is thread-safe -\\nthe only question is who manages this thread-safety. Interfaces which are not\\nmarked as thread-safe cause uniffi to wrap the interface in a mutex which is\\nhidden in the generated code and therefore not obvious to the casual reader.\\nThe `[Threadsafe]` marker acts as a way for the component author to opt out of\\nthe overhead and blocking behaviour of this mutex, at the cost of opting in to\\nmanaging their own locking internally. This ADR proposes that uniffi forces\\ncomponent authors to explicitly manage that locking in all cases - or to put\\nthis in Rust terms, that all structs supported by uniffi must already be\\n`Send+Sync`\\nNote that this ADR will hence-forth use the term `Send+Sync` instead of\\n\"Threadsafe\" because it more accurately describes the actual intent and avoids\\nany misunderstandings that might be caused by using the somewhat broad and\\ngeneric \"Threadsafe\".\\n## Decision Drivers\\n* Supporting non-`Send+Sync` structs means uniffi must add hidden locking to make\\nthem `Send+Sync`. We consider this a \"foot-gun\" as it may lead to accidentally\\nhaving method calls unexpectedly block for long periods, such as\\n[this Fenix bug](https:\/\/github.com\/mozilla-mobile\/fenix\/issues\/17086)\\n(with more details available in [this JIRA ticket](https:\/\/jira.mozilla.com\/browse\/SDK-157)).\\n* Supporting such structs will hinder uniffi growing in directions that we've\\nfound are desired in practice, such as allowing structs to use [alternative\\nmethod receivers](https:\/\/github.com\/mozilla\/uniffi-rs\/issues\/417) or to\\n[pass interface references over the FFI](https:\/\/github.com\/mozilla\/uniffi-rs\/issues\/419).\\n","Decision":"* Supporting non-`Send+Sync` structs means uniffi must add hidden locking to make\\nthem `Send+Sync`. We consider this a \"foot-gun\" as it may lead to accidentally\\nhaving method calls unexpectedly block for long periods, such as\\n[this Fenix bug](https:\/\/github.com\/mozilla-mobile\/fenix\/issues\/17086)\\n(with more details available in [this JIRA ticket](https:\/\/jira.mozilla.com\/browse\/SDK-157)).\\n* Supporting such structs will hinder uniffi growing in directions that we've\\nfound are desired in practice, such as allowing structs to use [alternative\\nmethod receivers](https:\/\/github.com\/mozilla\/uniffi-rs\/issues\/417) or to\\n[pass interface references over the FFI](https:\/\/github.com\/mozilla\/uniffi-rs\/issues\/419).\\nChosen option:\\n* **[Option 2] Immediately deprecate, then remove entirely, support for\\nnon-`Send+Sync` interfaces.**\\nThis decision was taken because our real world experience tells us that\\nnon-`Send+Sync` interfaces are only useful in toy or example applications (eg,\\nthe nimbus and autofill projects didn't get very far before needing these\\ncapabilities), so the extra ongoing work in supporting these interfaces cannot\\nbe justified.\\n### Positive Consequences\\n* The locking in all uniffi supported components will be more easily\\ndiscoverable - it will be in hand-written rust code and not hidden inside\\ngenerated code. This is a benefit to the developers of the uniffi supported\\ncomponent rather than to the consumers of it; while we are considering other\\nfeatures to help communicate the lock semantics to such consumers, that is\\nbeyond the scope of this ADR.\\n* Opens the door to enhancements that would be impossible for non-`Send+Sync`\\ninterfaces, and simpler to implement for `Send+Sync` interfaces if support\\nfor non-`Send+Sync` interfaces did not exist.\\n* Simpler implementation and documentation.\\n### Negative Consequences\\n* All consumers (both inside Mozilla and external) will need to change their\\ninterfaces to be `Send+Sync`. As an example of what this entails,\\nsee [this commit](https:\/\/github.com\/mozilla\/uniffi-rs\/commit\/454dfff6aa560dffad980a9258853108a44d5985)\\nwhich converts the `todolist` example.\\n* Simple, toy applications may be more difficult to wrap - consumers will not\\nbe able to defer decisions about `Send+Sync` support and will instead need to\\nimplement simple locking as demonstrated in [this commit](\\nhttps:\/\/github.com\/mozilla\/uniffi-rs\/commit\/454dfff6aa560dffad980a9258853108a44d5985).\\n* Existing applications that are yet to consider how to make their\\nimplementations `Send+Sync` cannot be wrapped until they have.\\n* The examples which aren't currently marked with the `[Threadsafe]` attribute\\nwill become more complex as they will all need to implement and explain how\\nthey achieve being `Send+Sync`.\\n* The perception that its more difficult to wrap interfaces will lead to less\\nadoption of the tool.\\n","tokens":468,"id":4952,"text":"## Context and Problem Statement\\n[ADR-0003](0003-threadsafe-interfaces.md) introduced support for \"thread-safe\\ninterfaces\" - possibly leading to the impression that there is such a thing as\\nnon-threadsafe interfaces and confusion about exactly what the attribute means.\\nHowever, the entire concept of non-threadsafe interfaces is a misconception -\\nthe Rust compiler insists that everything wrapped by uniffi is thread-safe -\\nthe only question is who manages this thread-safety. Interfaces which are not\\nmarked as thread-safe cause uniffi to wrap the interface in a mutex which is\\nhidden in the generated code and therefore not obvious to the casual reader.\\nThe `[Threadsafe]` marker acts as a way for the component author to opt out of\\nthe overhead and blocking behaviour of this mutex, at the cost of opting in to\\nmanaging their own locking internally. This ADR proposes that uniffi forces\\ncomponent authors to explicitly manage that locking in all cases - or to put\\nthis in Rust terms, that all structs supported by uniffi must already be\\n`Send+Sync`\\nNote that this ADR will hence-forth use the term `Send+Sync` instead of\\n\"Threadsafe\" because it more accurately describes the actual intent and avoids\\nany misunderstandings that might be caused by using the somewhat broad and\\ngeneric \"Threadsafe\".\\n## Decision Drivers\\n* Supporting non-`Send+Sync` structs means uniffi must add hidden locking to make\\nthem `Send+Sync`. We consider this a \"foot-gun\" as it may lead to accidentally\\nhaving method calls unexpectedly block for long periods, such as\\n[this Fenix bug](https:\/\/github.com\/mozilla-mobile\/fenix\/issues\/17086)\\n(with more details available in [this JIRA ticket](https:\/\/jira.mozilla.com\/browse\/SDK-157)).\\n* Supporting such structs will hinder uniffi growing in directions that we've\\nfound are desired in practice, such as allowing structs to use [alternative\\nmethod receivers](https:\/\/github.com\/mozilla\/uniffi-rs\/issues\/417) or to\\n[pass interface references over the FFI](https:\/\/github.com\/mozilla\/uniffi-rs\/issues\/419).\\n\n\n##Decision\n* Supporting non-`Send+Sync` structs means uniffi must add hidden locking to make\\nthem `Send+Sync`. We consider this a \"foot-gun\" as it may lead to accidentally\\nhaving method calls unexpectedly block for long periods, such as\\n[this Fenix bug](https:\/\/github.com\/mozilla-mobile\/fenix\/issues\/17086)\\n(with more details available in [this JIRA ticket](https:\/\/jira.mozilla.com\/browse\/SDK-157)).\\n* Supporting such structs will hinder uniffi growing in directions that we've\\nfound are desired in practice, such as allowing structs to use [alternative\\nmethod receivers](https:\/\/github.com\/mozilla\/uniffi-rs\/issues\/417) or to\\n[pass interface references over the FFI](https:\/\/github.com\/mozilla\/uniffi-rs\/issues\/419).\\nChosen option:\\n* **[Option 2] Immediately deprecate, then remove entirely, support for\\nnon-`Send+Sync` interfaces.**\\nThis decision was taken because our real world experience tells us that\\nnon-`Send+Sync` interfaces are only useful in toy or example applications (eg,\\nthe nimbus and autofill projects didn't get very far before needing these\\ncapabilities), so the extra ongoing work in supporting these interfaces cannot\\nbe justified.\\n### Positive Consequences\\n* The locking in all uniffi supported components will be more easily\\ndiscoverable - it will be in hand-written rust code and not hidden inside\\ngenerated code. This is a benefit to the developers of the uniffi supported\\ncomponent rather than to the consumers of it; while we are considering other\\nfeatures to help communicate the lock semantics to such consumers, that is\\nbeyond the scope of this ADR.\\n* Opens the door to enhancements that would be impossible for non-`Send+Sync`\\ninterfaces, and simpler to implement for `Send+Sync` interfaces if support\\nfor non-`Send+Sync` interfaces did not exist.\\n* Simpler implementation and documentation.\\n### Negative Consequences\\n* All consumers (both inside Mozilla and external) will need to change their\\ninterfaces to be `Send+Sync`. As an example of what this entails,\\nsee [this commit](https:\/\/github.com\/mozilla\/uniffi-rs\/commit\/454dfff6aa560dffad980a9258853108a44d5985)\\nwhich converts the `todolist` example.\\n* Simple, toy applications may be more difficult to wrap - consumers will not\\nbe able to defer decisions about `Send+Sync` support and will instead need to\\nimplement simple locking as demonstrated in [this commit](\\nhttps:\/\/github.com\/mozilla\/uniffi-rs\/commit\/454dfff6aa560dffad980a9258853108a44d5985).\\n* Existing applications that are yet to consider how to make their\\nimplementations `Send+Sync` cannot be wrapped until they have.\\n* The examples which aren't currently marked with the `[Threadsafe]` attribute\\nwill become more complex as they will all need to implement and explain how\\nthey achieve being `Send+Sync`.\\n* The perception that its more difficult to wrap interfaces will lead to less\\nadoption of the tool.\\n","MatchedIds":[1979,4656,1819,1563,21],"Predicted":"## Decision\nAll structs supported by uniffi must be `Send+Sync`. This decision intends to eliminate the implicit and hidden locking behavior currently added by uniffi to non-threadsafe interfaces. \n\n### Implementation Details\n- Uniffi will no longer wrap interfaces in an implicit mutex. Instead, it will enforce that all structs are explicitly marked as `Send+Sync`.\n- The attribute previously known as `[Threadsafe]` will be retired, and all relevant components must transition to explicitly manage their own thread safety and locking mechanisms internally, providing explicit guarantees of `Send+Sync` compliance.\n- Clear documentation and migration guides will be provided to support component authors in transitioning their codebases to adhere to the `Send+Sync` requirement.\n\n### Positive Consequences\n- **Transparent Thread Safety:** Developers will have to explicitly manage thread safety, leading to more transparent and predictable behavior in their code.\n- **Reduction of Hidden Issues:** By removing hidden mutex locks, we mitigate risks of unexpected blocking or performance issues that might arise from implicit locking mechanisms.\n- **Flexibility for Future Enhancements:** Supporting only `Send+Sync` structs aligns with the desired future directions of uniffi, such as adopting alternative method receivers and passing interface references over the FFI.\n\n### Negative Consequences\n- **Migration Overhead:** Existing codebases that rely on implicit locking provided by uniffi will need to be refactored to ensure that all structs are `Send+Sync`. This will introduce some initial overhead for developers.\n- **Increased Complexity:** Developers will have the additional responsibility of ensuring that their components correctly handle synchronization and thread safety, which may increase the complexity of the code.\n\n### References\n1. [Fenix bug related to hidden mutex locks](https:\/\/github.com\/mozilla-mobile\/fenix\/issues\/17086)\n2. [SDK-157 JIRA ticket with more details](https:\/\/jira.mozilla.com\/browse\/SDK-157)\n3. [Alternative method receivers discussion](https:\/\/github.com\/mozilla\/uniffi-rs\/issues\/417)\n4. [Pass interface references over the FFI discussion](https:\/\/github.com\/mozilla\/uniffi-rs\/issues\/419)","GenTime":"2024-08-20 00:35:07"}
{"File Name":"saas-platform-frontend\/0005-use-storybook-to-build-self-contained-components-but-not-for-uis.md","Context":"## Context\\nWe need a workflow to build our appliation and components.\\n","Decision":"We use Storybook only for building new self contained components.\\n","tokens":16,"id":3722,"text":"## Context\\nWe need a workflow to build our appliation and components.\\n\n\n##Decision\nWe use Storybook only for building new self contained components.\\n","MatchedIds":[3682,4358,1239,4021,1240],"Predicted":"## Decision\nWe will use GitHub Actions for our CI\/CD workflow. GitHub Actions provides a simple and integrated way to define workflows for building, testing, and deploying our application and components. \n\n1. **Build**: We will define actions to automatically build our application and components upon each push to the repository. This will ensure that the latest code changes are always compiled and ready.\n\n2. **Test**: We will set up actions to run automated tests after each build. This will help in identifying issues early in the development cycle.\n\n3. **Deploy**: For deployment, we will configure actions to deploy the application to our target environment (e.g., AWS, Azure, or any other cloud service) whenever changes are merged into the main branch.\n\n4. **Artifacts**: We will store build artifacts in GitHub Packages or another artifact repository to ensure traceability and reproducibility of builds.\n\nUsing GitHub Actions will also allow us to set up branch protection rules, ensuring that code changes are tested and reviewed before being merged into the main branch. The integration with GitHub brings ease of use and efficiency, making it suitable for our workflow needs.","GenTime":"2024-08-20 00:35:14"}
{"File Name":"branchout\/0002-language.md","Context":"## Context\\nA language should be universal, simple and easily testable\\nOptions\\n* Shell\\n* Go\\n* Java\\n* JavaScript\\nThere should be very few dependencies\\n","Decision":"Shell\\n* No dependencies\\n* Installed pretty much everywhere developers are\\n","tokens":36,"id":1472,"text":"## Context\\nA language should be universal, simple and easily testable\\nOptions\\n* Shell\\n* Go\\n* Java\\n* JavaScript\\nThere should be very few dependencies\\n\n\n##Decision\nShell\\n* No dependencies\\n* Installed pretty much everywhere developers are\\n","MatchedIds":[472,4761,4868,3715,400],"Predicted":"## Decision\nWe choose Go as our primary language for this project. Go is designed to be simple and efficient, with a minimal set of language features, making it easy to write, read, and maintain code. It offers strong support for concurrency, which is crucial for modern software development. Furthermore, Go has excellent built-in testing capabilities and produces statically linked binaries with very few dependencies, ensuring ease of deployment and simplicity in the overall development process.","GenTime":"2024-08-20 00:35:16"}
{"File Name":"evidence-api\/0002-create-two-platform-apis-instead-of-a-single-api.md","Context":"## Context\\nThis application was created to allow services to manage gathered evidence documents in a consistent, safe and centralised way. An early decision point was reached with regards to designing the architecture around this capability.\\nThe basic functionalities known to be required are:\\n- secure storage and management of documents provided by residents and other third parties\\n- tracking of rights and retention over those documents by the council\\n- requests and approval of evidence from third parties\\n- management of evidence requests by officers\\nInfluencing factors:\\n- HackIT's [API Playbook](https:\/\/github.com\/LBHackney-IT\/API-Playbook-v2-beta)\\n- [Clean Architecture](https:\/\/github.com\/madetech\/clean-architecture) principles\\n","Decision":"We decided to create two Platform APIs (as defined in the API Playbook):\\n- A [Documents API](https:\/\/github.com\/LBHackney-IT\/documents-api) for the storage and management of documents and claims over them\\n- An [Evidence API](https:\/\/github.com\/LBHackney-IT\/evidence-api) for the management of evidence requests and reviews\\n","tokens":151,"id":1480,"text":"## Context\\nThis application was created to allow services to manage gathered evidence documents in a consistent, safe and centralised way. An early decision point was reached with regards to designing the architecture around this capability.\\nThe basic functionalities known to be required are:\\n- secure storage and management of documents provided by residents and other third parties\\n- tracking of rights and retention over those documents by the council\\n- requests and approval of evidence from third parties\\n- management of evidence requests by officers\\nInfluencing factors:\\n- HackIT's [API Playbook](https:\/\/github.com\/LBHackney-IT\/API-Playbook-v2-beta)\\n- [Clean Architecture](https:\/\/github.com\/madetech\/clean-architecture) principles\\n\n\n##Decision\nWe decided to create two Platform APIs (as defined in the API Playbook):\\n- A [Documents API](https:\/\/github.com\/LBHackney-IT\/documents-api) for the storage and management of documents and claims over them\\n- An [Evidence API](https:\/\/github.com\/LBHackney-IT\/evidence-api) for the management of evidence requests and reviews\\n","MatchedIds":[4841,1792,4361,4428,3119],"Predicted":"## Decision\nWe will design the application architecture following the principles of Clean Architecture to ensure that it is maintainable, scalable, and testable. Moreover, we will adhere to HackIT's API Playbook to standardize our API interactions and ensure consistency across services.\n\nKey components of our decision:\n- **Secure Storage and Management**: Utilize a secure, managed document storage service such as AWS S3 with encryption and access control to handle the secure storage and management of documents.\n- **Rights and Retention Tracking**: Implement a metadata management system to track ownership, rights, and retention policies associated with each document, ensuring legal compliance and proper lifecycle management.\n- **Requests and Approval Flow**: Design a robust workflow engine to handle the submission, review, and approval processes related to evidence documents.\n- **Management by Officers**: Develop an administrative interface that allows council officers to efficiently manage evidence requests and document policies.\n\nBy aligning the architecture with Clean Architecture principles, we will separate concerns and allow for easy adaptation and extension of the system. Following the API Playbook will ensure our APIs are well-documented, consistent, and easy for third parties to integrate with.","GenTime":"2024-08-20 00:35:22"}
{"File Name":"gsp\/ADR025-ingress.md","Context":"## Context\\nWe currently have two [ingress][Ingress] systems:\\n* Istio (see [ADR019])\\n* nginx-ingress (see the old Ingress [ADR005])\\nIstio's [Virtual Service] records are essentially advanced `Ingress` records.\\nDo we need both?\\n","Decision":"No. We will use an [Istio Ingress Gateway](https:\/\/istio.io\/docs\/tasks\/traffic-management\/ingress\/ingress-control\/)\\n","tokens":69,"id":3905,"text":"## Context\\nWe currently have two [ingress][Ingress] systems:\\n* Istio (see [ADR019])\\n* nginx-ingress (see the old Ingress [ADR005])\\nIstio's [Virtual Service] records are essentially advanced `Ingress` records.\\nDo we need both?\\n\n\n##Decision\nNo. We will use an [Istio Ingress Gateway](https:\/\/istio.io\/docs\/tasks\/traffic-management\/ingress\/ingress-control\/)\\n","MatchedIds":[620,733,3104,3909,2236],"Predicted":"## Decision\nWe have decided to consolidate our ingress systems by deprecating the use of nginx-ingress in favor of exclusively using Istio for ingress management. This decision is driven by the following factors:\n\n1. **Unified Configuration**: Istio's Virtual Service records provide advanced and more flexible ingress configurations that encompass the functionality provided by nginx-ingress, eliminating redundancy.\n  \n2. **Simplified Maintenance**: Maintaining a single ingress system simplifies our operational workload, reducing complexity in deployment, monitoring, and troubleshooting.\n\n3. **Consistent Policies**: Using Istio alone allows for consistent application of security, traffic management, and observability policies across all ingress routes.\n\n4. **Community and Support**: Istio is an emerging standard with a large and active community, ensuring ongoing support and enhancements that will keep the ingress system robust and up-to-date.\n\nThe transition plan includes:\n- Gradually phasing out nginx-ingress by migrating existing ingress configurations to Istio.\n- Updating internal documentation and training materials to reflect the use of Istio for all ingress needs.\n- Ensuring thorough testing and monitoring during the migration to minimize disruption.\n\nThis approach will streamline our ingress management and leverage Istio's advanced features for a more efficient and scalable system.","GenTime":"2024-08-20 00:35:27"}
{"File Name":"akvo-product-design\/ADR-004.md","Context":"## Context\\nAs we introduce new and more complex response types, we need to come up with a consistent, homogeneous solution for serializing all the values.\\nThe historical solution for this issue has been to separate multiple values with pipes, and in some scenarios, creating a JSON-encoded response (i.e. geoshapes).\\nWe currently need to add a new `code` property to some responses, namely to cascade and option types, but we also want to avoid introducing new properties as the response content grows.\\n","Decision":"As response values are getting more and more complex, there's an increasing need for finding a common solution for all values, and adding new properties will not scale well.\\nThe decision is to extend the JSON-based schema, and encode complex answers in this packed string.\\nWe will also review the meaning of each *response type*, which will determine the encoding of each value (JSON? string? array? etc).\\nMultiple responses (barcodes, option questions, cascades, etc) will be encoded using a **JSON array**.\\n","tokens":103,"id":2802,"text":"## Context\\nAs we introduce new and more complex response types, we need to come up with a consistent, homogeneous solution for serializing all the values.\\nThe historical solution for this issue has been to separate multiple values with pipes, and in some scenarios, creating a JSON-encoded response (i.e. geoshapes).\\nWe currently need to add a new `code` property to some responses, namely to cascade and option types, but we also want to avoid introducing new properties as the response content grows.\\n\n\n##Decision\nAs response values are getting more and more complex, there's an increasing need for finding a common solution for all values, and adding new properties will not scale well.\\nThe decision is to extend the JSON-based schema, and encode complex answers in this packed string.\\nWe will also review the meaning of each *response type*, which will determine the encoding of each value (JSON? string? array? etc).\\nMultiple responses (barcodes, option questions, cascades, etc) will be encoded using a **JSON array**.\\n","MatchedIds":[674,2585,523,4097,5118],"Predicted":"## Decision\n\nWe have decided to adopt JSON-encoded responses as the standard format for serializing all response values. This decision addresses the following concerns:\n\n1. **Consistency**: JSON offers a well-established, human-readable, and versatile format that can handle complex data structures uniformly across different response types including the new `code` property.\n2. **Scalability**: JSON can comfortably accommodate additional properties in future without altering the existing serialization approach. Introducing new properties will not disrupt the existing data structure or introduce parsing complexities, unlike the pipe-separated format.\n3. **Homogeneity**: Using a single serialization method for all responses simplifies parsing logic and reduces the risk of errors arising from inconsistent handling of different response types. \n4. **Extensibility**: JSON's hierarchical structure supports nested data effectively and will easily allow the addition of more complex response attributes that might be required in future without the need for major changes.\n\n### Implementation Details\n\n- All responses, including cascade and option types, will be serialized in JSON format. For example:\n    ```json\n    {\n        \"responseType\": \"cascade\",\n        \"values\": [ ... ],\n        \"code\": \"XYZ123\"\n    }\n    ```\n- The library or utility responsible for serialization and deserialization will be updated to support the new format consistently across all response types.\n- Extensive documentation and examples will be provided to ensure developers understand the new standard and can implement it without issues.\n- A migration plan will be put in place to convert existing responses in pipe-separated format to the new JSON format, ensuring backward compatibility during the transition period.\n\nBy standardizing on JSON-encoded responses, we ensure that our system remains robust, flexible, and easily maintainable as it evolves to meet new requirements.","GenTime":"2024-08-20 00:35:34"}
{"File Name":"toc-poc\/1577574698834_architecture_choice_for_template_generation.md","Context":"## Context\\nThe POC assumes generation of the table of contents based on the content presented on the page. Data need to be fetched from somewhere, and then presented on the view in some way. Data model won't be simple data structure, as it can be multi-leveled list, which might also contain some behavior, to make it easier to prepare for rendering. It's not going to be simple CRUD application, even if model will not contain much behavior.\\n","Decision":"Use ports and adapters architecture style.\\n","tokens":94,"id":1883,"text":"## Context\\nThe POC assumes generation of the table of contents based on the content presented on the page. Data need to be fetched from somewhere, and then presented on the view in some way. Data model won't be simple data structure, as it can be multi-leveled list, which might also contain some behavior, to make it easier to prepare for rendering. It's not going to be simple CRUD application, even if model will not contain much behavior.\\n\n\n##Decision\nUse ports and adapters architecture style.\\n","MatchedIds":[4021,1792,1031,3982,1243],"Predicted":"## Decision\nTo handle the complexity of generating a table of contents (TOC) from page content effectively, we will adopt the following approach:\n\n1. **Domain-Driven Design (DDD) Principles**: \n   - We will model the TOC as a hierarchical entity, where each node can have multiple child nodes. Each node will be an object that encapsulates both data (e.g., title, URL, and depth) and behavior (e.g., methods to add, remove or reorder child nodes).\n\n2. **Repository Pattern**:\n   - A repository will act as a mediator between the domain (TOC objects) and the data source, abstracting away data fetching and persistence logic.\n   - This allows us to easily switch data sources if needed, and promotes a more maintainable codebase by isolating domain logic from data access logic.\n\n3. **Service Layer**:\n   - A dedicated service will be responsible for fetching content data, creating TOC entities, and handling any complexities related to preparation for rendering.\n   - This service will also handle integration with external data sources or APIs if needed.\n\n4. **Template Rendering**:\n   - Use a server-side or client-side template rendering engine to convert the TOC object model into the desired HTML structure efficiently.\n   - This will leverage recursive templates to handle the nested structure of the multi-leveled list.\n\n5. **Incremental Enhancements**:\n   - Implement initial support for primary TOC use cases and gradually add more complex behaviors or enhancements based on feedback and requirements.\n\nBy applying DDD, Repository, and Service Layer patterns, we ensure the TOC generation logic is cleanly separated, maintainable, and flexible for future updates or changes.","GenTime":"2024-08-20 00:35:43"}
{"File Name":"platform\/2021-12-07-admin-extension-api-standards.md","Context":"## Context\\nWe need to provide ways for extension developers to add custom components and views to different places in the administration. Multiple solutions where discussed and tested, this ADR contains a summary of the final solution.\\n","Decision":"### Word definitions\\nFor a better understanding of the following text it is good to have a definition for specific words:\\n#### Location\\nExtensions can render custom views with the Admin-Extension-API via iFrames. To support multiple views in different places every \"location\" of the iFrame gets a unique ID. These can be defined by the app\/plugin developer itself.\\n*Example:*\\nAn app wants to render a custom iFrame in a card on the dashboard. The \"location\" of the iFrame has then a specific \"locationID\" like `sw-dashboard-example-app-dashboard-card`. The app can also render another iFrames which also get \"locationIDs\". In our example it is a iFrame in a custom modal: `example-app-example-modal-content`.\\nThe app want to render different views depending on the \"location\" of the iFrame. So the app developer can render the correct view depending on the \"locationID\":\\n```js\\nif (sw.location.is('sw-dashboard-example-app-dashboard-card')) {\\nrenderDashboardCard();\\n}\\nif (sw.location.is('example-app-example-modal-content')) {\\nrenderModalContent();\\n}\\n```\\n#### PositionID (PositionIdentifier)\\nDevelopers can extend existing areas or create new areas in the administration with the Admin-Extension-API. To identify the positions which the developer want to extend we need a unique ID for every position. We call these IDs \"positionID\".\\n*Example:*\\nAn app wants to add a new tab item to a tab-bar. In the administration are many tab-bars available. So the developer needs to choose the correct \"positionID\" to determine which tab-bar should be extended. In this example the developer adds a new tab item to the tab-bar in the product detail page.\\n```js\\nsw.ui.tabs('sw-product-detail').addTabItem({ ... })\\n```\\n### Solution:\\nWe use the concept of component sections for providing injection points for extension components.\\n#### Component Sections\\nIn most cases developers will directly use the extension capabilities of the UI components (e.g. adding tab items, adding button to grid, ...). This will cover most needs of many extensions.\\nTo give them more flexibility we introduce a feature named \"Component Sections\". These are sections where any extension developer can inject components. These components are prebuilt and they can also contain custom render views with iFrames. The developer needs to use the feature and choose the matching positionID for the component position.\\n```js\\n\/\/ Adding a card before the manufacturer card with custom fields entries.\\nsw.ui.componentSection('sw-manufacturer-card-custom-fields__before').add({\\n\/\/ The Extension-API provides different components out of the box\\ncomponent: 'card',\\n\/\/ Props are depending on the type of component\\nprops: {\\ntitle: 'This is the title',\\nsubtitle: 'I am the subtitle',\\n\/\/ Some components can render a custom view. In this case the extension can render custom content in the card.\\nlocationId: 'example-app-card-before-manufactuer-custom-fields-card'\\n}\\n})\\n```\\n#### Vue Devtools Plugin for finding the PositionIDs\\nIt is impossible to create a list of all potential position IDs. And they would be hard to manage. To solve this problem we are writing a custom plugin for the Vue Devtools. This plugin will be available for Vue Devtools 6+. It makes identifying the position IDs very easy.\\nJust open the plugin in the Devtools (It is available directly when you open the Administration). Then you can see all positions at the current administration view which are available for extending. If you click at one position ID you get more information about it. Like the property in the Meteor-Extension-SDK so that you directly know what functionality this position has.\\nIn summary: the Devtool plugin provides a visual way to see which parts can be extended and what are the positionIDs for the extension position.\\n","tokens":43,"id":4507,"text":"## Context\\nWe need to provide ways for extension developers to add custom components and views to different places in the administration. Multiple solutions where discussed and tested, this ADR contains a summary of the final solution.\\n\n\n##Decision\n### Word definitions\\nFor a better understanding of the following text it is good to have a definition for specific words:\\n#### Location\\nExtensions can render custom views with the Admin-Extension-API via iFrames. To support multiple views in different places every \"location\" of the iFrame gets a unique ID. These can be defined by the app\/plugin developer itself.\\n*Example:*\\nAn app wants to render a custom iFrame in a card on the dashboard. The \"location\" of the iFrame has then a specific \"locationID\" like `sw-dashboard-example-app-dashboard-card`. The app can also render another iFrames which also get \"locationIDs\". In our example it is a iFrame in a custom modal: `example-app-example-modal-content`.\\nThe app want to render different views depending on the \"location\" of the iFrame. So the app developer can render the correct view depending on the \"locationID\":\\n```js\\nif (sw.location.is('sw-dashboard-example-app-dashboard-card')) {\\nrenderDashboardCard();\\n}\\nif (sw.location.is('example-app-example-modal-content')) {\\nrenderModalContent();\\n}\\n```\\n#### PositionID (PositionIdentifier)\\nDevelopers can extend existing areas or create new areas in the administration with the Admin-Extension-API. To identify the positions which the developer want to extend we need a unique ID for every position. We call these IDs \"positionID\".\\n*Example:*\\nAn app wants to add a new tab item to a tab-bar. In the administration are many tab-bars available. So the developer needs to choose the correct \"positionID\" to determine which tab-bar should be extended. In this example the developer adds a new tab item to the tab-bar in the product detail page.\\n```js\\nsw.ui.tabs('sw-product-detail').addTabItem({ ... })\\n```\\n### Solution:\\nWe use the concept of component sections for providing injection points for extension components.\\n#### Component Sections\\nIn most cases developers will directly use the extension capabilities of the UI components (e.g. adding tab items, adding button to grid, ...). This will cover most needs of many extensions.\\nTo give them more flexibility we introduce a feature named \"Component Sections\". These are sections where any extension developer can inject components. These components are prebuilt and they can also contain custom render views with iFrames. The developer needs to use the feature and choose the matching positionID for the component position.\\n```js\\n\/\/ Adding a card before the manufacturer card with custom fields entries.\\nsw.ui.componentSection('sw-manufacturer-card-custom-fields__before').add({\\n\/\/ The Extension-API provides different components out of the box\\ncomponent: 'card',\\n\/\/ Props are depending on the type of component\\nprops: {\\ntitle: 'This is the title',\\nsubtitle: 'I am the subtitle',\\n\/\/ Some components can render a custom view. In this case the extension can render custom content in the card.\\nlocationId: 'example-app-card-before-manufactuer-custom-fields-card'\\n}\\n})\\n```\\n#### Vue Devtools Plugin for finding the PositionIDs\\nIt is impossible to create a list of all potential position IDs. And they would be hard to manage. To solve this problem we are writing a custom plugin for the Vue Devtools. This plugin will be available for Vue Devtools 6+. It makes identifying the position IDs very easy.\\nJust open the plugin in the Devtools (It is available directly when you open the Administration). Then you can see all positions at the current administration view which are available for extending. If you click at one position ID you get more information about it. Like the property in the Meteor-Extension-SDK so that you directly know what functionality this position has.\\nIn summary: the Devtool plugin provides a visual way to see which parts can be extended and what are the positionIDs for the extension position.\\n","MatchedIds":[1545,3203,2502,5149,1067],"Predicted":"## Decision\n\n### Context\nWe need to provide ways for extension developers to add custom components and views to different places in the administration. Multiple solutions were discussed and tested, and this ADR contains a summary of the final solution.\n\n### Decision Drivers\n1. **Flexibility**: The solution must allow developers to easily add and customize components.\n2. **Consistency**: The solution must ensure that all custom components adhere to a unified structure and interact seamlessly with the core system.\n3. **Simplicity**: The solution should be straightforward to implement and maintain, both for the core developers and extension developers.\n4. **Scalability**: The solution must be scalable to accommodate potential future expansions without major revisions.\n\n### Decisions\n\n#### Custom Components Integration\n1. **Utilize a Plugin Architecture:**\n    - **Description**: Implement a plugin-based architecture where developers can create and register custom plugins.\n    - **Rationale**: This approach allows for a high degree of flexibility in adding new components and views. Each plugin can encapsulate its logic and UI elements, ensuring separation of concerns.\n    - **Outcome**: Offers an organized way to manage customizations and ensures they do not interfere with core functionalities.\n\n2. **Provide Extension Points:**\n    - **Description**: Define several extension points within the administration UI where custom views or components can be plugged in.\n    - **Rationale**: Clear extension points reduce the complexity for developers by providing predefined hooks to insert their custom components.\n    - **Outcome**: Ensures a consistent method for extending the UI, reducing the risk of conflicts or discrepancies.\n\n#### API and Tools for Developers\n3. **Expose a Component API:**\n    - **Description**: Develop and expose a robust API that developers can use to create and integrate their custom components.\n    - **Rationale**: A well-documented API simplifies the integration process by providing clear guidance and reusable methods for developers.\n    - **Outcome**: Enhances the developer experience and reduces the learning curve, leading to faster and more efficient custom component development.\n\n4. **Create a CLI Tool:**\n    - **Description**: Provide a Command Line Interface (CLI) tool to scaffold new plugins and components.\n    - **Rationale**: A CLI tool can streamline the creation process, ensuring that all new components adhere to the required structure and conventions from the start.\n    - **Outcome**: Increases productivity and consistency across custom components developed by different teams.\n\n#### Documentation and Support\n5. **Comprehensive Documentation:**\n    - **Description**: Maintain detailed documentation covering the plugin architecture, extension points, API usage, and examples.\n    - **Rationale**: Clear and accessible documentation is crucial for empowering developers to confidently extend the administration UI.\n    - **Outcome**: Reduces support overhead and fosters a strong community of developers contributing high-quality extensions.\n\n6. **Sample Projects and Templates:**\n    - **Description**: Provide sample projects and templates that demonstrate typical use cases and best practices for extending the administration UI.\n    - **Rationale**: Practical examples help developers understand how to effectively use the provided tools and APIs.\n    - **Outcome**: Facilitates rapid adoption and encourages best practices, ensuring extensions are well-structured and maintainable.\n\n### Conclusion\nAdopting a plugin architecture with well-defined extension points, complemented by a robust API and developer tools, provides a powerful and flexible way for extension developers to add custom components and views. Comprehensive documentation and support resources will ensure that developers have the guidance they need to create high-quality extensions, enhancing the overall administration UI.","GenTime":"2024-08-20 00:36:03"}
{"File Name":"nso.aurora\/AsynchronousMessages.md","Context":"## Context\\nThe messages that are sent from Order Management, Rating Manager and Recommendation Manager can be asynchronous?\\n","Decision":"Yes it can be asynchronous because we don't need to wait for an ack before sending the next one.\\n","tokens":22,"id":307,"text":"## Context\\nThe messages that are sent from Order Management, Rating Manager and Recommendation Manager can be asynchronous?\\n\n\n##Decision\nYes it can be asynchronous because we don't need to wait for an ack before sending the next one.\\n","MatchedIds":[306,2672,1610,2309,786],"Predicted":"## Decision\n**Implement Asynchronous Messaging for Order Management, Rating Manager, and Recommendation Manager**\n\nGiven that the messages from Order Management, Rating Manager, and Recommendation Manager can be asynchronous, it is beneficial to adopt an asynchronous messaging strategy to decouple these services and improve system resilience and scalability.\n\n### Rationale:\n1. **Decoupling Services:**\n   Asynchronous messaging allows these services to operate independently, reducing the tight coupling between them and enabling each to be developed, deployed, and scaled independently.\n\n2. **Improved Resilience and Fault Tolerance:**\n   With asynchronous messaging, if any of these services are temporarily unavailable, messages can be queued and processed once the service becomes available again, enhancing system resilience.\n\n3. **Scalability:**\n   Asynchronous messaging systems, such as message queues, can handle varying loads by buffering messages during peak times and processing them as resources become available, improving overall system scalability.\n\n### Implementation:\n1. **Message Broker Selection:**\n   Utilize a robust message broker (e.g., RabbitMQ, Apache Kafka, AWS SNS + SQS) to manage asynchronous communication between Order Management, Rating Manager, and Recommendation Manager.\n\n2. **Message Queue Setup:**\n   Create separate queues for each service interaction. For example, a queue for messages from Order Management, another for messages from Rating Manager, and a third for Recommendation Manager.\n\n3. **Message Handling:**\n   Ensure each service has a dedicated consumer that processes messages from its respective queue. Implement message acknowledgment mechanisms to confirm successful processing and handle retries for failed messages.\n\nBy implementing asynchronous messaging for Order Management, Rating Manager, and Recommendation Manager, we can achieve a more resilient, scalable, and decoupled architecture.","GenTime":"2024-08-20 00:36:08"}
{"File Name":"openlobby-server\/0011-add-relational-database.md","Context":"## Context\\nNumber of document types which does not use Elasticsearch's fulltext\\ncapabilities is growing. Recently released Elasticsearch 6 is bringing one type\\nper index which means management of many indices.\\n","Decision":"We will add relational database as primary database. Elasticsearch will be used\\nfor denormalized reports and related data intended for fulltext search.\\n","tokens":42,"id":475,"text":"## Context\\nNumber of document types which does not use Elasticsearch's fulltext\\ncapabilities is growing. Recently released Elasticsearch 6 is bringing one type\\nper index which means management of many indices.\\n\n\n##Decision\nWe will add relational database as primary database. Elasticsearch will be used\\nfor denormalized reports and related data intended for fulltext search.\\n","MatchedIds":[102,2884,2513,485,2181],"Predicted":"## Decision\nGiven the recent release of Elasticsearch 6, which enforces the one type per index rule, it becomes cumbersome to manage numerous indices, particularly for document types that do not utilize Elasticsearch's fulltext capabilities. To optimize index management and reduce complexity, we will perform the following:\n\n1. **Consolidate Indices**: Combine document types that do not need fulltext search into a single index per category or function. This will reduce the overhead associated with managing numerous indices while adhering to the one type per index rule.\n\n2. **Use Custom Routing**: Implement custom routing to ensure that documents of different types are efficiently retrievable without being scattered across multiple indices.\n\n3. **Leverage Index Templates**: Define index templates that pre-configure settings and mappings for the consolidated indices, ensuring consistency and reducing the need for manual reconfiguration.\n\n4. **Minimize Index Overhead**: For document types that do not require fulltext search capabilities, we will minimize the use of analyzed fields and instead employ keyword fields or fixed mappings that suit their search requirements.\n\n5. **Archive Old Data**: Where relevant, periodically archive older data that belongs to document types not utilizing fulltext search, further reducing the active index size and improving performance.\n\nBy consolidating document types into fewer indices and minimizing unnecessary search capabilities, we can streamline index management while maximizing Elasticsearch's performance efficiencies.","GenTime":"2024-08-20 00:36:13"}
{"File Name":"james\/0013-precompute-jmap-preview.md","Context":"## Context\\nJMAP messages have a handy preview property displaying the firsts 256 characters of meaningful test of a message.\\nThis property is often displayed for message listing in JMAP clients, thus it is queried a lot.\\nCurrently, to get the preview, James retrieves the full message body, parse it using MIME parsers, removes HTML and keep meaningful text.\\n","Decision":"We should pre-compute message preview.\\nA MailboxListener will compute the preview and store it in a MessagePreviewStore.\\nWe should have a Cassandra and memory implementation.\\nWhen the preview is precomputed then for these messages we can consider the \"preview\" property as a metadata.\\nWhen the preview is not precomputed then we should compute the preview for these messages, and save the result for later.\\nWe should provide a webAdmin task allowing to rebuild the projection. The computing and storing in MessagePreviewStore\\nis idempotent and the task can be run in live without any concurrency problem.\\nSome performance tests will be run in order to evaluate the improvements.\\n","tokens":75,"id":2134,"text":"## Context\\nJMAP messages have a handy preview property displaying the firsts 256 characters of meaningful test of a message.\\nThis property is often displayed for message listing in JMAP clients, thus it is queried a lot.\\nCurrently, to get the preview, James retrieves the full message body, parse it using MIME parsers, removes HTML and keep meaningful text.\\n\n\n##Decision\nWe should pre-compute message preview.\\nA MailboxListener will compute the preview and store it in a MessagePreviewStore.\\nWe should have a Cassandra and memory implementation.\\nWhen the preview is precomputed then for these messages we can consider the \"preview\" property as a metadata.\\nWhen the preview is not precomputed then we should compute the preview for these messages, and save the result for later.\\nWe should provide a webAdmin task allowing to rebuild the projection. The computing and storing in MessagePreviewStore\\nis idempotent and the task can be run in live without any concurrency problem.\\nSome performance tests will be run in order to evaluate the improvements.\\n","MatchedIds":[2896,2145,2884,2886,2888],"Predicted":"## Decision\nIntroduce a message preview storage mechanism in James to precompute and store the preview property. When a message is initially processed and stored, calculate the preview property and save it as a separate field in the metadata of the message.\n\n- **Storage**: Extend the message metadata storage to include a 'preview' field.\n- **Precomputation**: During message ingestion, compute the first 256 characters of meaningful text after stripping HTML and store it in the 'preview' field.\n- **Retrieval**: When a JMAP client requests the preview property, retrieve it directly from this metadata field instead of parsing the full message body.\n- **Updates**: Ensure that any update to the message body triggers a re-computation and update of the preview field.\n- **Fallback**: For messages already stored without previews, on first access, compute the preview and store it back to avoid repeated full-body parsing in subsequent accesses.\n\nThis approach reduces the load on MIME parsers and improves the response time for JMAP clients querying message previews, as the need to parse the full message body repeatedly is eliminated.","GenTime":"2024-08-20 00:36:18"}
{"File Name":"gsp\/ADR004-tenant-isolation.md","Context":"## Context\\nThe two main isolation models for Kubernetes are:\\n* Namespaces within a single cluster\\n* Running multiple clusters\\nAll Service Teams currently have separate AWS accounts.\\nSome Service Teams have separate AWS accounts for separate environment (ie. Staging, Production etc)\\nMany Service Teams have micro-service architectures\\nSome Service Teams have unique network isolation requirements that may be hard to implement in a shared environment.\\nTo ensure \"smooth transition\" during a migration it would be preferable to have clusters deployed to Service Team's VPCs.\\nTo ensure separation of billing it would be preferable to deploy clusters to Service Team's AWS accounts.\\nTo ensure strong network\/compute isolation between Service Teams it would be preferable to deploy separate clusters for separate environments.\\n","Decision":"We will manage multiple Kubernetes clusters, one per environment per tenant.\\nThis should provide:\\n* Strong network\/compute isolation between service teams\\n* Clear separation of billing\\n* Ability to deploy into target VPCs to make piecemeal microservice migration practical\\n","tokens":153,"id":3889,"text":"## Context\\nThe two main isolation models for Kubernetes are:\\n* Namespaces within a single cluster\\n* Running multiple clusters\\nAll Service Teams currently have separate AWS accounts.\\nSome Service Teams have separate AWS accounts for separate environment (ie. Staging, Production etc)\\nMany Service Teams have micro-service architectures\\nSome Service Teams have unique network isolation requirements that may be hard to implement in a shared environment.\\nTo ensure \"smooth transition\" during a migration it would be preferable to have clusters deployed to Service Team's VPCs.\\nTo ensure separation of billing it would be preferable to deploy clusters to Service Team's AWS accounts.\\nTo ensure strong network\/compute isolation between Service Teams it would be preferable to deploy separate clusters for separate environments.\\n\n\n##Decision\nWe will manage multiple Kubernetes clusters, one per environment per tenant.\\nThis should provide:\\n* Strong network\/compute isolation between service teams\\n* Clear separation of billing\\n* Ability to deploy into target VPCs to make piecemeal microservice migration practical\\n","MatchedIds":[631,4707,3147,856,3909],"Predicted":"## Decision\n\n### Chosen Option: Multiple Clusters in Separate AWS Accounts\n\n1. **Clusters in Service Teams' AWS Accounts:**\n   Each Service Team will have their Kubernetes clusters deployed in their specific AWS accounts. This ensures that billing is isolated per team, simplifying cost management and financial accountability.\n\n2. **Separate Clusters for Different Environments:**\n   For Service Teams that require separate environments (e.g., Staging, Production), each environment will have its independent Kubernetes cluster. This approach ensures strong isolation between different environments, mitigating the risk of issues in one environment affecting another.\n\n3. **VPC-based Cluster Deployment:**\n   Clusters will be deployed in distinct Virtual Private Clouds (VPCs) for each Service Team. This practice facilitates a smooth transition during migration, aligning with existing infrastructure and minimizing disruption.\n\n4. **Handling Unique Network Isolation Requirements:**\n   For Service Teams with unique network isolation requirements, specific VPC configurations and Network Policies will be tuned to meet their needs. This ensures that even within separate clusters, network isolation can be achieved to the desired extent.\n\n### Positive Consequences\n- **Improved Security and Isolation:** Each team's clusters and environments are strictly isolated, significantly improving security and reducing the blast radius in case of a breach.\n- **Cost Management:** Separate AWS accounts and clusters per environment\/channel ensure clear and accountable billing, simplifying budget tracking and cost allocations.\n- **Flexibility and Customization:** Service Teams have full control over their cluster configurations, allowing them to set up their environments according to their specific needs without impacting others.\n\n### Negative Consequences\n- **Increased Management Overhead:** Managing multiple clusters across various AWS accounts would require robust automation and monitoring solutions to handle the complexity and ensure smooth operations.\n- **Resource Utilization:** Launching multiple clusters can lead to an increase in resource consumption and associated costs. Ensuring efficient use of resources and potentially leveraging Kubernetes autoscaling features will be essential.\n- **Potential Overhead in Compliance:** Each AWS account and cluster setup may introduce slight deviations in configuration standards. Ensuring ongoing compliance checks and standardization might require additional effort. \n\nBy following this strategy, we can ensure strong network and compute isolation, provide flexibility to meet unique team needs, and manage costs effectively while maintaining robust security and operational efficiency.","GenTime":"2024-08-20 00:36:29"}
{"File Name":"mediawiki-extensions-Kartographer\/webgl_renderer.md","Context":"## Context\\nIn order to decrease maintenance overhead in the maps infrastructure, the tile server will receive improvements in the vector-tile generation using Tegola. Although Kartotherian already have a vector-tile endpoint, now there is some effort to evolve the platform as a whole, and it includes client-side rendering, that will take advantage of future capabilities supported by Tegola.\\nThe client-side render process will provide decoupling of some server-side components and will make it possible to replace mapnik, which is a maintenance overhead.\\nAfter analyzing all the open-source options in the market, we have reached the decision of using the mapbox-gl-style specification as the next-level in terms of maps rendering. The reason is because it can be easily integrated with the top client-side maps frameworks: maplibre-gl, Leaflet, and OpenLayers.\\nBecause of its flexibility and support across frameworks, it gives us freedom to experiment different engines in the early stages of this work.\\n","Decision":"Accepted (02-16-2021)\\n","tokens":198,"id":3286,"text":"## Context\\nIn order to decrease maintenance overhead in the maps infrastructure, the tile server will receive improvements in the vector-tile generation using Tegola. Although Kartotherian already have a vector-tile endpoint, now there is some effort to evolve the platform as a whole, and it includes client-side rendering, that will take advantage of future capabilities supported by Tegola.\\nThe client-side render process will provide decoupling of some server-side components and will make it possible to replace mapnik, which is a maintenance overhead.\\nAfter analyzing all the open-source options in the market, we have reached the decision of using the mapbox-gl-style specification as the next-level in terms of maps rendering. The reason is because it can be easily integrated with the top client-side maps frameworks: maplibre-gl, Leaflet, and OpenLayers.\\nBecause of its flexibility and support across frameworks, it gives us freedom to experiment different engines in the early stages of this work.\\n\n\n##Decision\nAccepted (02-16-2021)\\n","MatchedIds":[4281,4609,1119,5140,528],"Predicted":"## Decision\n\nWe have decided to adopt the mapbox-gl-style specification for the next-generation maps rendering on the client-side. This decision allows us to take advantage of its flexibility and support across multiple leading client-side maps frameworks such as maplibre-gl, Leaflet, and OpenLayers.\n\n### Reasons for Decision:\n- **Flexibility**: The mapbox-gl-style specification offers a versatile and flexible approach to style vector tiles, which can be seamlessly integrated with various mapping libraries. This supports our goal of reducing maintenance overhead by standardizing on a single, well-supported style specification.\n- **Support Across Frameworks**: By choosing mapbox-gl-style, we gain the added benefit of compatibility with multiple established client-side maps engines, such as maplibre-gl, Leaflet, and OpenLayers. This extensive support provides us with the freedom to experiment and potentially swap engines as needed without significant rework.\n- **Reduced Maintenance Overhead**: Shifting rendering responsibilities to the client-side helps in decoupling server-side components. This reduces the overall complexity and maintenance load associated with our current server-side rendering system that utilizes Mapnik.\n- **Future Capabilities with Tegola**: Leveraging Tegola's capabilities to improve vector-tile generation aligns with this decision, as it simplifies our infrastructure and harnesses the strengths of client-side rendering. This ensures we stay future-proof and can easily adapt to emerging technologies.\n\n### Positive Consequences:\n- Simplifies the server architecture by offloading the render processing to clients, reducing server-side complexity and maintenance.\n- Provides flexibility to switch or experiment with different client-side rendering frameworks without significant technical debt.\n- Aligns with modern best practices by utilizing widely adopted open standards, thereby improving interoperation and community support.\n\n### Negative Consequences:\n- The shift to client-side rendering may initially demand a revamp of current client-side applications to integrate mapbox-gl-style specification, requiring development effort and resources.\n- Potential performance concerns on lower-end client devices which might now bear the rendering load traditionally handled server-side.\n- Dependent on the continued support and updates from the open-source mapbox-gl-style community and related libraries for long-term sustainability.\n\nWe believe these trade-offs are manageable and essential for evolving our platform while maintaining robustness and staying aligned with modern development practices. This decision positions our maps infrastructure for future scalability and ease of maintenance.","GenTime":"2024-08-20 00:36:38"}
{"File Name":"govuk-kubernetes-discovery\/0005-consistent-structure-for-terraform-files.md","Context":"## Context\\nWe should have a consistent structure for working with our Terraform files. We\\nshould be able to look into a file and understand it's function is and what it\\ncreates, what variables it needs to run and what it outputs after it's finished.\\nWe should not have to spend time picking apart the code to understand what is\\ncreated by Terraform.\\nWe should not have to work with multiple files to understand a single component.\\n","Decision":"Create style guidelines based upon the following:\\n- Header with title and description\\n- Variables listed\\n- Outputs listed\\n- Full descriptions of what variables and outputs do in the code\\n- `terraform fmt` across each file\\nCreate a style guideline document explaining this structure.\\n","tokens":93,"id":2793,"text":"## Context\\nWe should have a consistent structure for working with our Terraform files. We\\nshould be able to look into a file and understand it's function is and what it\\ncreates, what variables it needs to run and what it outputs after it's finished.\\nWe should not have to spend time picking apart the code to understand what is\\ncreated by Terraform.\\nWe should not have to work with multiple files to understand a single component.\\n\n\n##Decision\nCreate style guidelines based upon the following:\\n- Header with title and description\\n- Variables listed\\n- Outputs listed\\n- Full descriptions of what variables and outputs do in the code\\n- `terraform fmt` across each file\\nCreate a style guideline document explaining this structure.\\n","MatchedIds":[621,3144,2409,1372,2062],"Predicted":"## Decision\n1. We decided to adopt a standard file naming convention for our Terraform files to indicate their purpose clearly. Each Terraform component will be self-contained within a single directory.\n  \n2. Each component directory will include:\n    - `main.tf`: Contains the primary resource definitions and logic.\n    - `variables.tf`: Defines the input variables required by the component.\n    - `outputs.tf`: Specifies the output variables that will be produced.\n    - `README.md`: Provides a summary of what the component does, its inputs, outputs, and any other necessary documentation.\n\n3. We will use consistent naming conventions for resources, variables, and outputs to enhance readability and maintainability.\n  \n4. We will incorporate Terraform modules where necessary to abstract and reuse common configurations.\n  \n5. Every component will be accompanied by a terratest or a similar testing framework to ensure proper functioning before deployment.\n\n6. Periodic code reviews will be conducted to ensure adherence to these standards and to facilitate knowledge sharing and consistency across teams.","GenTime":"2024-08-20 00:36:42"}
{"File Name":"clone_difitalcitizenship\/0007-choice-of-azure-region.md","Context":"## Context\\nWe must guarantee that the data stored, transferred to and from the cloud services will never leave the borders of the European Union.\\n","Decision":"We decide to deploy our services on the [West Europe](https:\/\/azure.microsoft.com\/en-us\/regions\/) region, located in the Netherlands.\\n","tokens":29,"id":1186,"text":"## Context\\nWe must guarantee that the data stored, transferred to and from the cloud services will never leave the borders of the European Union.\\n\n\n##Decision\nWe decide to deploy our services on the [West Europe](https:\/\/azure.microsoft.com\/en-us\/regions\/) region, located in the Netherlands.\\n","MatchedIds":[1180,2768,745,4841,2471],"Predicted":"## Decision\nWe will use cloud services provided by companies with data centers located exclusively within the European Union. Specifically, we will utilize [EU regions of AWS](https:\/\/aws.amazon.com\/about-aws\/global-infrastructure\/regions_az\/#EU) (such as Frankfurt or Ireland), [Azure EU regions](https:\/\/azure.microsoft.com\/en-us\/global-infrastructure\/geographies\/#geographies) (such as West Europe or North Europe), or [Google Cloud's EU regions](https:\/\/cloud.google.com\/about\/locations#region) (such as Belgium or Netherlands). This ensures that all data storage, processing, and transfer activities comply with EU data residency regulations and GDPR requirements.","GenTime":"2024-08-20 00:36:46"}
{"File Name":"teaching-vacancies\/0010_migrate_from_algolia_to_postgres.md","Context":"## Context and Problem Statement\\nGiven the user needs we want to meet with our search functionality, and the cost and complexity involved in our integration with Algolia, would we be better off with a simpler solution of leveraging our existing database for search?\\n## Decision Drivers\\n* Financial cost of Algolia (and risk of eventually losing our cheap grandfathered legacy plan)\\n* Technical complexity and brittleness of Algolia integration\\n* Lack of customisability and lack of configuration-as-code on Algolia\\n* Simplicity of underlying search requirements means enterprise-grade search engine is overkill\\n* Lack of ability to integration test search results when using SaaS search engine\\n* Availability of constantly improving full-text search functionality in our database (PostgreSQL)\\n* Potential for richer geographical querying using PostGIS\\n","Decision":"* Financial cost of Algolia (and risk of eventually losing our cheap grandfathered legacy plan)\\n* Technical complexity and brittleness of Algolia integration\\n* Lack of customisability and lack of configuration-as-code on Algolia\\n* Simplicity of underlying search requirements means enterprise-grade search engine is overkill\\n* Lack of ability to integration test search results when using SaaS search engine\\n* Availability of constantly improving full-text search functionality in our database (PostgreSQL)\\n* Potential for richer geographical querying using PostGIS\\nDecided to migrate away from Algolia and move to using our database for search functionality.\\n### Positive Consequences\\n* Simplified infrastructure and one fewer third-party service integration\\n* Significant complexity savings in search code\\n* Ability to integration test search results and keep search configuration in code (versioned and auditable)\\n* End of reliance on Algolia's goodwill in keeping us grandfathered on their legacy plan\\n* Minor cost savings in the short term (and we no longer need to avoid potential increases in queries in the long term)\\n* Improved performance and reliability due to not having to interact with a third-party service\\n* Ability to completely control and debug all aspects of how search works\\n* Richer geographical querying through use of PostGIS\\n### Negative Consequences\\n* Some implementation effort (estimated at 2x developers for ~3-4 sprints)\\n* Synonym logic will need custom implementation (but at the same time, this allows us to make it significantly \"smarter\" than on Algolia)\\n* Typo tolerance\/fuzzy search will need custom implementation (but this is an edge-case for us because the majority of queries are straightforward)\\n","tokens":162,"id":4378,"text":"## Context and Problem Statement\\nGiven the user needs we want to meet with our search functionality, and the cost and complexity involved in our integration with Algolia, would we be better off with a simpler solution of leveraging our existing database for search?\\n## Decision Drivers\\n* Financial cost of Algolia (and risk of eventually losing our cheap grandfathered legacy plan)\\n* Technical complexity and brittleness of Algolia integration\\n* Lack of customisability and lack of configuration-as-code on Algolia\\n* Simplicity of underlying search requirements means enterprise-grade search engine is overkill\\n* Lack of ability to integration test search results when using SaaS search engine\\n* Availability of constantly improving full-text search functionality in our database (PostgreSQL)\\n* Potential for richer geographical querying using PostGIS\\n\n\n##Decision\n* Financial cost of Algolia (and risk of eventually losing our cheap grandfathered legacy plan)\\n* Technical complexity and brittleness of Algolia integration\\n* Lack of customisability and lack of configuration-as-code on Algolia\\n* Simplicity of underlying search requirements means enterprise-grade search engine is overkill\\n* Lack of ability to integration test search results when using SaaS search engine\\n* Availability of constantly improving full-text search functionality in our database (PostgreSQL)\\n* Potential for richer geographical querying using PostGIS\\nDecided to migrate away from Algolia and move to using our database for search functionality.\\n### Positive Consequences\\n* Simplified infrastructure and one fewer third-party service integration\\n* Significant complexity savings in search code\\n* Ability to integration test search results and keep search configuration in code (versioned and auditable)\\n* End of reliance on Algolia's goodwill in keeping us grandfathered on their legacy plan\\n* Minor cost savings in the short term (and we no longer need to avoid potential increases in queries in the long term)\\n* Improved performance and reliability due to not having to interact with a third-party service\\n* Ability to completely control and debug all aspects of how search works\\n* Richer geographical querying through use of PostGIS\\n### Negative Consequences\\n* Some implementation effort (estimated at 2x developers for ~3-4 sprints)\\n* Synonym logic will need custom implementation (but at the same time, this allows us to make it significantly \"smarter\" than on Algolia)\\n* Typo tolerance\/fuzzy search will need custom implementation (but this is an edge-case for us because the majority of queries are straightforward)\\n","MatchedIds":[4909,1740,4747,1510,4266],"Predicted":"## Decision\nWe will leverage our existing PostgreSQL database for implementing search functionality. Given the simplicity of our search requirements, the cost and technical complexities associated with Algolia are not justified.\n\n### Positive Consequences\n* **Cost Savings:** By utilizing PostgreSQL\u2019s built-in capabilities, we avoid the financial costs associated with Algolia, including the risk of losing our legacy pricing plan.\n* **Reduced Complexity:** PostgreSQL integration is less technically complex than maintaining a separate Algolia service, reducing brittleness and potential points of failure.\n* **Customization:** PostgreSQL allows for greater customization and configuration-as-code, which Algolia lacks.\n* **Improved Testing:** Full control over the search logic enables easier integration testing of search results.\n* **Enhanced Querying:** Leveraging PostGIS within PostgreSQL opens up the potential for richer geographical queries that Algolia cannot support.\n\n### Negative Consequences\n* **Initial Setup:** Implementing full-text search and geographical querying in PostgreSQL might require an initial setup effort and potentially adjusting our schema or data indexing strategies.\n* **Performance Considerations:** Depending on the volume of data and complexity of queries, performance tuning or additional resources might be necessary to ensure fast and efficient searches.\n* **Maintenance:** Moving away from SaaS means our team will be responsible for maintenance and potential updates to the search functionality within PostgreSQL.\n\nBy leveraging PostgreSQL\u2019s full-text search and PostGIS functionalities, we simplify our architecture and maintain greater control over our search capabilities, aligning with both our technical and financial goals.","GenTime":"2024-08-20 00:36:51"}
{"File Name":"openfido-app-service\/0003-deployment.md","Context":"## Context\\nWe plan to use AWS as a development\/staging environment and as a production environment, on ECS.\\n","Decision":"Use CircleCI for CI, and deploy docker images to AWS ECR for use in deployments.\\nCircleCI will need to be configured with the following environmental variables\\nin order to deploy docker images (using [CircleCI's aws-ecr\\norb](https:\/\/circleci.com\/developer\/orbs\/orb\/circleci\/aws-ecr)):\\n- DEV_AWS_ACCESS_KEY_ID\\n- DEV_AWS_SECRET_ACCESS_KEY\\n- PROD_AWS_ACCESS_KEY_ID\\n- PROD_AWS_SECRET_ACCESS_KEY\\n- AWS_REGION\\n- AWS_ECR_ACCOUNT_URL\\nThis project's Dockerfile requires access to a privately hosted github project\\n(openfido-utils). Use [Buildkit](https:\/\/docs.docker.com\/develop\/develop-images\/build_enhancements\/) on CircleCI to grant SSH access to the docker build processes.\\n","tokens":24,"id":2471,"text":"## Context\\nWe plan to use AWS as a development\/staging environment and as a production environment, on ECS.\\n\n\n##Decision\nUse CircleCI for CI, and deploy docker images to AWS ECR for use in deployments.\\nCircleCI will need to be configured with the following environmental variables\\nin order to deploy docker images (using [CircleCI's aws-ecr\\norb](https:\/\/circleci.com\/developer\/orbs\/orb\/circleci\/aws-ecr)):\\n- DEV_AWS_ACCESS_KEY_ID\\n- DEV_AWS_SECRET_ACCESS_KEY\\n- PROD_AWS_ACCESS_KEY_ID\\n- PROD_AWS_SECRET_ACCESS_KEY\\n- AWS_REGION\\n- AWS_ECR_ACCOUNT_URL\\nThis project's Dockerfile requires access to a privately hosted github project\\n(openfido-utils). Use [Buildkit](https:\/\/docs.docker.com\/develop\/develop-images\/build_enhancements\/) on CircleCI to grant SSH access to the docker build processes.\\n","MatchedIds":[4358,2997,4847,923,1464],"Predicted":"## Decision\n### Environment Separation\nWe will create separate ECS clusters for development, staging, and production environments. This will ensure isolation of resources and reduce the risk of inadvertent issues affecting production environments.\n\n### Infrastructure as Code\nWe will use Terraform to define and manage the infrastructure for all environments. This approach allows for version-controlled, repeatable, and consistent deployments.\n\n### CI\/CD Pipeline\nWe will implement a CI\/CD pipeline using AWS CodePipeline and AWS CodeBuild. The pipeline will include steps to build, test, and deploy the application to the respective ECS clusters for each environment.\n\n### Networking\nFor networking, we will use separate VPCs for development, staging, and production environments. Each VPC will have its own subnets, security groups, and other networking resources to ensure isolation and security.\n\n### Resource Management\nWe will tag all AWS resources with environment-specific tags (e.g., `Environment=Dev`, `Environment=Staging`, `Environment=Prod`) to facilitate cost monitoring, resource management, and troubleshooting.\n\n### Secrets Management\nWe will use AWS Secrets Manager to securely manage and rotate secrets such as database passwords, API keys, and other sensitive configuration data for each environment.\n\n### Monitoring and Logging\nWe will use AWS CloudWatch for monitoring and logging across all environments. Separate log groups and metrics will be configured for development, staging, and production to facilitate easier debugging and performance analysis.\n\n### Scaling and Load Balancing\nElastic Load Balancing (ELB) and Auto Scaling will be configured for all ECS clusters to handle varying loads and ensure high availability and fault tolerance for each environment.\n\n### Database\nRDS instances will be set up for each environment with appropriate replication and backups as per the needs of development, staging, and production workloads.\n\nBy adopting these practices, we can ensure robust, secure, and manageable environments for development, staging, and production on AWS ECS.","GenTime":"2024-08-20 00:36:56"}
{"File Name":"content-data-api\/adr-009-track-metrics-by-basepath.md","Context":"## Context\\nWe started tracking performance and quality metrics by `content_id`, but after\\nthe first 3 months it was clear that the user needs to track metrics at the\\nbase_path level in Guides and Travel Advice.\\n","Decision":"Track metrics at the base_path level\\n","tokens":46,"id":1866,"text":"## Context\\nWe started tracking performance and quality metrics by `content_id`, but after\\nthe first 3 months it was clear that the user needs to track metrics at the\\nbase_path level in Guides and Travel Advice.\\n\n\n##Decision\nTrack metrics at the base_path level\\n","MatchedIds":[2403,1857,3802,3469,1701],"Predicted":"## Decision\nWe have decided to pivot our tracking approach to record performance and quality metrics at the `base_path` level for Guides and Travel Advice. To implement this change, we will:\n\n1. Update our data collection mechanisms to retrieve and store metrics with `base_path` as the primary identifier.\n2. Modify existing reports and dashboards to reflect metrics aggregated by `base_path` instead of `content_id`.\n3. Migrate historical data currently stored by `content_id` to also include `base_path` mappings, ensuring continuity in trend analysis and reporting.\n4. Communicate the change to our stakeholders and provide guidelines on how this affects their reports and usage.\n\nThis approach aligns with user needs and provides a more granular and meaningful way to track performance and quality metrics for specific content areas.","GenTime":"2024-08-20 00:37:00"}
{"File Name":"green_log\/0006-use-lock-free-io.md","Context":"## Context\\nWe want to be able to write log entries (to file, or STDOUT), without them being interleaved.\\nBut also, we want logging to perform well.\\n","Decision":"_Unlike_ the Ruby standard `Logger`, GreenLog will use a [lock-free logging](https:\/\/www.jstorimer.com\/blogs\/workingwithcode\/7982047-is-lock-free-logging-safe) approach. That is, we will:\\n- avoid using of mutexes to serialise output\\n- perform atomic writes to `IO` streams (using `<<`)\\n","tokens":37,"id":1927,"text":"## Context\\nWe want to be able to write log entries (to file, or STDOUT), without them being interleaved.\\nBut also, we want logging to perform well.\\n\n\n##Decision\n_Unlike_ the Ruby standard `Logger`, GreenLog will use a [lock-free logging](https:\/\/www.jstorimer.com\/blogs\/workingwithcode\/7982047-is-lock-free-logging-safe) approach. That is, we will:\\n- avoid using of mutexes to serialise output\\n- perform atomic writes to `IO` streams (using `<<`)\\n","MatchedIds":[1924,4852,3290,4868,1387],"Predicted":"## Decision\nWe will implement a thread-safe logging mechanism, using a queue to store log entries before they are written to file or STDOUT by a dedicated logging thread. This ensures log entries are not interleaved and maintains good performance.\n\n* A `LogQueue` object will be used to queue log entries.\n* A `LoggerThread` will be dedicated to dequeueing and writing the log entries to the desired output.\n* Log entries will be flushed to the output in a batch mode to reduce I\/O overhead.\n\n```mermaid\nsequenceDiagram\nApp ->> LogQueue:           enqueue(log_entry)\nLoggerThread ->> LogQueue:  dequeue(log_entry)\nLoggerThread ->> Output:    write(log_entry)\n```\n   \nThis architecture ensures serialized writing without sacrificing performance.","GenTime":"2024-08-20 00:37:03"}
{"File Name":"editions\/06-\u2705-to-use-step-functions.md","Context":"## Context\\nThe archiving process is time consuming and consists of many stages.\\n","Decision":"For the archiving process to be run as an amazon step function so we can break the process down into component stages.\\nThis will allow anyone supporting the system to immediately find what broke in case something goes wrong.\\nThis will also allow the publication process to fire the notification at the desired time.\\n","tokens":17,"id":672,"text":"## Context\\nThe archiving process is time consuming and consists of many stages.\\n\n\n##Decision\nFor the archiving process to be run as an amazon step function so we can break the process down into component stages.\\nThis will allow anyone supporting the system to immediately find what broke in case something goes wrong.\\nThis will also allow the publication process to fire the notification at the desired time.\\n","MatchedIds":[2508,1434,3722,4416,4699],"Predicted":"## Decision\nAutomate as many stages of the archiving process as possible with the use of scripts and tools. Additionally, parallelize tasks where feasible to reduce the overall time taken for the archiving process.","GenTime":"2024-08-20 00:37:05"}
{"File Name":"Head-Start-TTADP\/0007-use-plantuml-for-diagrams.md","Context":"## Context\\nCertain things we need to document, such as our data model and boundary diagrams are\\nbetter represented as diagrams than text. Choosing a single service will make it easier to\\nlearn the system and keep diagrams up to date.\\n","Decision":"We will use https:\/\/www.plantuml.com to generate diagrams with the following checks:\\n1. We will not rely on the plantuml.com service to be the sole storage medium for the actual UML text\\n1. We will store each type of diagram in its own markdown file in the \/docs folder\\nPlantUML can also be run locally, in case the plantuml.com service ever goes away. This risk is also why\\nwe must keep the UML source in our source control.\\n","tokens":48,"id":1172,"text":"## Context\\nCertain things we need to document, such as our data model and boundary diagrams are\\nbetter represented as diagrams than text. Choosing a single service will make it easier to\\nlearn the system and keep diagrams up to date.\\n\n\n##Decision\nWe will use https:\/\/www.plantuml.com to generate diagrams with the following checks:\\n1. We will not rely on the plantuml.com service to be the sole storage medium for the actual UML text\\n1. We will store each type of diagram in its own markdown file in the \/docs folder\\nPlantUML can also be run locally, in case the plantuml.com service ever goes away. This risk is also why\\nwe must keep the UML source in our source control.\\n","MatchedIds":[1180,4361,2261,4428,3119],"Predicted":"## Decision\nWe will use [Lucidchart](https:\/\/www.lucidchart.com) as our primary tool for creating and maintaining system diagrams, including data models and boundary diagrams, to ensure consistency and ease of use across the team.","GenTime":"2024-08-20 00:37:08"}
{"File Name":"katas\/002-no-eslint.md","Context":"## Context\\nSee [ADR1](.\/001-formatting-katas.md), which describes the formatting of a kata.\\nThis would normally lead to using a linter.\\n","Decision":"But I am not a big fan of a huge set of rules\\nand currently configuring a linter just for those couple of rules is overhead.\\nSomething like prettier which automatically applies a rule set is fine, but a linter\\nis kinda like someone who just says that a cleanup is needed, an auto-fixer (like prettier)\\ndoes clean up instead of just talking about it, my 2 cents.\\n","tokens":36,"id":4336,"text":"## Context\\nSee [ADR1](.\/001-formatting-katas.md), which describes the formatting of a kata.\\nThis would normally lead to using a linter.\\n\n\n##Decision\nBut I am not a big fan of a huge set of rules\\nand currently configuring a linter just for those couple of rules is overhead.\\nSomething like prettier which automatically applies a rule set is fine, but a linter\\nis kinda like someone who just says that a cleanup is needed, an auto-fixer (like prettier)\\ndoes clean up instead of just talking about it, my 2 cents.\\n","MatchedIds":[4338,400,2914,3853,5070],"Predicted":"## Decision\nWe will integrate [ESLint](https:\/\/eslint.org\/) as our linter to enforce code formatting and style rules as described in ADR1. This ensures that all katas meet the required formatting standards.","GenTime":"2024-08-20 00:37:14"}
{"File Name":"ichatsdk-evolution\/0001-use-c-language.md","Context":"## Context and Problem Statement\\n[Describe the context and problem statement, e.g., in free form using two to three sentences. You may want to articulate the problem in form of a question.]\\n## Decision Drivers <!-- optional -->\\n* [driver 1, e.g., a force, facing concern, \u2026]\\n* [driver 2, e.g., a force, facing concern, \u2026]\\n* \u2026 <!-- numbers of drivers can vary -->\\n","Decision":"* [driver 1, e.g., a force, facing concern, \u2026]\\n* [driver 2, e.g., a force, facing concern, \u2026]\\n* \u2026 <!-- numbers of drivers can vary -->\\nChosen option: \"[option 1]\", because [justification. e.g., only option, which meets k.o. criterion decision driver | which resolves force force | \u2026 | comes out best (see below)].\\n### Positive Consequences <!-- optional -->\\n* [e.g., improvement of quality attribute satisfaction, follow-up decisions required, \u2026]\\n* \u2026\\n### Negative Consequences <!-- optional -->\\n* [e.g., compromising quality attribute, follow-up decisions required, \u2026]\\n* \u2026\\n","tokens":96,"id":4062,"text":"## Context and Problem Statement\\n[Describe the context and problem statement, e.g., in free form using two to three sentences. You may want to articulate the problem in form of a question.]\\n## Decision Drivers <!-- optional -->\\n* [driver 1, e.g., a force, facing concern, \u2026]\\n* [driver 2, e.g., a force, facing concern, \u2026]\\n* \u2026 <!-- numbers of drivers can vary -->\\n\n\n##Decision\n* [driver 1, e.g., a force, facing concern, \u2026]\\n* [driver 2, e.g., a force, facing concern, \u2026]\\n* \u2026 <!-- numbers of drivers can vary -->\\nChosen option: \"[option 1]\", because [justification. e.g., only option, which meets k.o. criterion decision driver | which resolves force force | \u2026 | comes out best (see below)].\\n### Positive Consequences <!-- optional -->\\n* [e.g., improvement of quality attribute satisfaction, follow-up decisions required, \u2026]\\n* \u2026\\n### Negative Consequences <!-- optional -->\\n* [e.g., compromising quality attribute, follow-up decisions required, \u2026]\\n* \u2026\\n","MatchedIds":[4701,794,3820,1145,4616],"Predicted":"## Context and Problem Statement\nOur team needs to decide on the database technology for a new web application that expects high read and write throughput. The problem at hand involves determining the most suitable database system to use. Should we go with a traditional relational database or a NoSQL database?\n\n## Decision Drivers\n* Scalability: The ability to handle high read and write throughput.\n* Flexibility: Ability to handle various data types and queries.\n* Performance: Low latency and high speed for read and write operations.\n* Maintenance: Ease of management and maintenance.\n* Cost: Overall cost of ownership including licensing, infrastructure, and operational costs.\n\n## Decision\nChosen option: \"NoSQL Database,\" because it meets key criteria such as scalability and low latency. Additionally, NoSQL databases are designed to handle high throughput and large volumes of unstructured data, which aligns with our needs.\n\n### Positive Consequences\n* Improved scalability to handle high read and write operations.\n* Better performance with lower latency for data operations.\n* Increased flexibility in handling various data formats and queries.\n\n### Negative Consequences\n* Potential complexity in implementing and maintaining NoSQL databases.\n* Possibility of higher costs associated with specific NoSQL solutions.\n* Requirement for developer expertise in NoSQL technologies, which might involve a learning curve.","GenTime":"2024-08-20 00:37:19"}
{"File Name":"ibc-rs\/adr-003-handler-implementation.md","Context":"## Context\\nIn this ADR, we provide recommendations for implementing the IBC\\nhandlers within the `ibc` (modules) crate.\\n","Decision":"Concepts are introduced in the order given by a topological sort of their dependencies on each other.\\n### Events\\nIBC handlers must be able to emit events which will then be broadcasted via the node's pub\/sub mechanism,\\nand eventually picked up by the IBC relayer.\\nAn event has an arbitrary structure, depending on the handler that produces it.\\nHere is the [list of all IBC-related events][events], as seen by the relayer.\\nNote that the consumer of these events in production would not be the relayer directly\\n(instead the consumer is the node\/SDK where the IBC module executes),\\nbut nevertheless handlers will reuse these event definitions.\\n[events]: https:\/\/github.com\/informalsystems\/hermes\/blob\/bf84a73ef7b3d5e9a434c9af96165997382dcc9d\/modules\/src\/events.rs#L15-L43\\n```rust\\npub enum IBCEvent {\\nNewBlock(NewBlock),\\nCreateClient(ClientEvents::CreateClient),\\nUpdateClient(ClientEvents::UpdateClient),\\nClientMisbehavior(ClientEvents::ClientMisbehavior),\\nOpenInitConnection(ConnectionEvents::OpenInit),\\nOpenTryConnection(ConnectionEvents::OpenTry),\\n\/\/     ...\\n}\\n```\\n### Logging\\nIBC handlers must be able to log information for introspectability and ease of debugging.\\nA handler can output multiple log records, which are expressed as a pair of a status and a\\nlog line. The interface for emitting log records is described in the next section.\\n```rust\\npub enum LogStatus {\\nSuccess,\\nInfo,\\nWarning,\\nError,\\n}\\npub struct Log {\\nstatus: LogStatus,\\nbody: String,\\n}\\nimpl Log {\\nfn success(msg: impl Display) -> Self;\\nfn info(msg: impl Display) -> Self;\\nfn warning(msg: impl Display) -> Self;\\nfn error(msg: impl Display) -> Self;\\n}\\n```\\n### Handler output\\nIBC handlers must be able to return arbitrary data, together with events and log records, as described above.\\nAs a handler may fail, it is necessary to keep track of errors.\\nTo this end, we introduce a type for the return value of a handler:\\n```rust\\npub type HandlerResult<T, E> = Result<HandlerOutput<T>, E>;\\npub struct HandlerOutput<T> {\\npub result: T,\\npub log: Vec<Log>,\\npub events: Vec<Event>,\\n}\\n```\\nWe introduce a builder interface to be used within the handler implementation to incrementally build a `HandlerOutput` value.\\n```rust\\nimpl<T> HandlerOutput<T> {\\npub fn builder() -> HandlerOutputBuilder<T> {\\nHandlerOutputBuilder::new()\\n}\\n}\\npub struct HandlerOutputBuilder<T> {\\nlog: Vec<String>,\\nevents: Vec<Event>,\\nmarker: PhantomData<T>,\\n}\\nimpl<T> HandlerOutputBuilder<T> {\\npub fn log(&mut self, log: impl Into<Log>);\\npub fn emit(&mut self, event: impl Into<Event>);\\npub fn with_result(self, result: T) -> HandlerOutput<T>;\\n}\\n```\\nWe provide below an example usage of the builder API:\\n```rust\\nfn some_ibc_handler() -> HandlerResult<u64, Error> {\\nlet mut output = HandlerOutput::builder();\\n\/\/ ...\\noutput.log(Log::info(\"did something\"))\\n\/\/ ...\\noutput.log(Log::success(\"all good\"));\\noutput.emit(SomeEvent::AllGood);\\nOk(output.with_result(42));\\n}\\n```\\n### IBC Submodule\\nThe various IBC messages and their processing logic, as described in the IBC specification,\\nare split into a collection of submodules, each pertaining to a specific aspect of\\nthe IBC protocol, eg. client lifecycle management, connection lifecycle management,\\npacket relay, etc.\\nIn this section we propose a general approach to implement the handlers for a submodule.\\nAs a running example we will use a dummy submodule that deals with connections, which should not\\nbe mistaken for the actual ICS 003 Connection submodule.\\n#### Reader\\nA typical handler will need to read data from the chain state at the current height,\\nvia the private and provable stores.\\nTo avoid coupling between the handler interface and the store API, we introduce an interface\\nfor accessing this data. This interface, called a `Reader`, is shared between all handlers\\nin a submodule, as those typically access the same data.\\nHaving a high-level interface for this purpose helps avoiding coupling which makes\\nwriting unit tests for the handlers easier, as one does not need to provide a concrete\\nstore, or to mock one.\\n```rust\\npub trait ConnectionReader\\n{\\nfn connection_end(&self, connection_id: &ConnectionId) -> Option<ConnectionEnd>;\\n}\\n```\\nA production implementation of this `Reader` would hold references to both the private and provable\\nstore at the current height where the handler executes, but we omit the actual implementation as\\nthe store interfaces are yet to be defined, as is the general IBC top-level module machinery.\\nA mock implementation of the `ConnectionReader` trait could looks as follows:\\n```rust\\nstruct MockConnectionReader {\\nconnection_id: ConnectionId,\\nconnection_end: Option<ConnectionEnd>,\\nclient_reader: MockClientReader,\\n}\\nimpl ConnectionReader for MockConnectionReader {\\nfn connection_end(&self, connection_id: &ConnectionId) -> Option<ConnectionEnd> {\\nif connection_id == &self.connection_id {\\nself.connection_end.clone()\\n} else {\\nNone\\n}\\n}\\n}\\n```\\n#### Keeper\\nOnce a handler executes successfully, some data will typically need to be persisted in the chain state\\nvia the private\/provable store interfaces. In the same vein as for the reader defined in the previous section,\\na submodule should define a trait which provides operations to persist such data.\\nThe same considerations w.r.t. to coupling and unit-testing apply here as well.\\n```rust\\npub trait ConnectionKeeper {\\nfn store_connection(\\n&mut self,\\nclient_id: ConnectionId,\\nclient_type: ConnectionType,\\n) -> Result<(), Error>;\\nfn add_connection_to_client(\\n&mut self,\\nclient_id: ClientId,\\nconnection_id: ConnectionId,\\n) -> Result<(), Error>;\\n}\\n```\\n#### Submodule implementation\\nWe now come to the actual definition of a handler for a submodule.\\nWe recommend each handler to be defined within its own Rust module, named\\nafter the handler itself. For example, the \"Create Client\" handler of ICS 002 would\\nbe defined in `modules::ics02_client::handler::create_client`.\\n##### Message type\\nEach handler must define a datatype which represent the message it can process.\\n```rust\\npub struct MsgConnectionOpenInit {\\nconnection_id: ConnectionId,\\nclient_id: ClientId,\\ncounterparty: Counterparty,\\n}\\n```\\n##### Handler implementation\\nIn this section we provide guidelines for implementing an actual handler.\\nWe divide the handler in two parts: processing and persistence.\\n###### Processing\\nThe actual logic of the handler is expressed as a pure function, typically named\\n`process`, which takes as arguments a `Reader` and the corresponding message, and returns\\na `HandlerOutput<T, E>`, where `T` is a concrete datatype and `E` is an error type which defines\\nall potential errors yielded by the handlers of the current submodule.\\n```rust\\npub struct ConnectionMsgProcessingResult {\\nconnection_id: ConnectionId,\\nconnection_end: ConnectionEnd,\\n}\\n```\\nThe `process` function will typically read data via the `Reader`, perform checks and validation, construct new\\ndatatypes, emit log records and events, and eventually return some data together with objects to be persisted.\\nTo this end, this `process` function will create and manipulate a `HandlerOutput` value like described in\\nthe corresponding section.\\n```rust\\npub fn process(\\nreader: &dyn ConnectionReader,\\nmsg: MsgConnectionOpenInit,\\n) -> HandlerResult<ConnectionMsgProcessingResult, Error>\\n{\\nlet mut output = HandlerOutput::builder();\\nlet MsgConnectionOpenInit { connection_id, client_id, counterparty, } = msg;\\nif reader.connection_end(&connection_id).is_some() {\\nreturn Err(Kind::ConnectionAlreadyExists(connection_id).into());\\n}\\noutput.log(\"success: no connection state found\");\\nif reader.client_reader.client_state(&client_id).is_none() {\\nreturn Err(Kind::ClientForConnectionMissing(client_id).into());\\n}\\noutput.log(\"success: client found\");\\noutput.emit(IBCEvent::ConnectionOpenInit(connection_id.clone()));\\nOk(output.with_result(ConnectionMsgProcessingResult {\\nconnection_id,\\nclient_id,\\ncounterparty,\\n}))\\n}\\n```\\n###### Persistence\\nIf the `process` function specified above succeeds, the result value it yielded is then\\npassed to a function named `keep`, which is responsible for persisting the objects constructed\\nby the processing function. This `keep` function takes the submodule's `Keeper` and the result\\ntype defined above, and performs side-effecting calls to the keeper's methods to persist the result.\\nBelow is given an implementation of the `keep` function for the \"Create Connection\" handlers:\\n```rust\\npub fn keep(\\nkeeper: &mut dyn ConnectionKeeper,\\nresult: ConnectionMsgProcessingResult,\\n) -> Result<(), Error>\\n{\\nkeeper.store_connection(result.connection_id.clone(), result.connection_end)?;\\nkeeper.add_connection_to_client(result.client_id, result.connection_id)?;\\nOk(())\\n}\\n```\\n##### Submodule dispatcher\\n> This section is very much a work in progress, as further investigation into what\\n> a production-ready implementation of the `ctx` parameter of the top-level dispatcher\\n> is required. As such, implementers should feel free to disregard the recommendations\\n> below, and are encouraged to come up with amendments to this ADR to better capture\\n> the actual requirements.\\nEach submodule is responsible for dispatching the messages it is given to the appropriate\\nmessage processing function and, if successful, pass the resulting data to the persistence\\nfunction defined in the previous section.\\nTo this end, the submodule should define an enumeration of all messages, in order\\nfor the top-level submodule dispatcher to forward them to the appropriate processor.\\nSuch a definition for the ICS 003 Connection submodule is given below.\\n```rust\\npub enum ConnectionMsg {\\nConnectionOpenInit(MsgConnectionOpenInit),\\nConnectionOpenTry(MsgConnectionOpenTry),\\n...\\n}\\n```\\nThe actual implementation of a submodule dispatcher is quite straightforward and unlikely to vary\\nmuch in substance between submodules. We give an implementation for the ICS 003 Connection module below.\\n```rust\\npub fn dispatch<Ctx>(ctx: &mut Ctx, msg: Msg) -> Result<HandlerOutput<()>, Error>\\nwhere\\nCtx: ConnectionReader + ConnectionKeeper,\\n{\\nmatch msg {\\nMsg::ConnectionOpenInit(msg) => {\\nlet HandlerOutput {\\nresult,\\nlog,\\nevents,\\n} = connection_open_init::process(ctx, msg)?;\\nconnection::keep(ctx, result)?;\\nOk(HandlerOutput::builder()\\n.with_log(log)\\n.with_events(events)\\n.with_result(()))\\n}\\nMsg::ConnectionOpenTry(msg) => \/\/ omitted\\n}\\n}\\n```\\nIn essence, a top-level dispatcher is a function of a message wrapped in the enumeration introduced above,\\nand a \"context\" which implements both the `Reader` and `Keeper` interfaces.\\n### Dealing with chain-specific datatypes\\nThe ICS 002 Client submodule stands out from the other submodules as it needs\\nto deal with chain-specific datatypes, such as `Header`, `ClientState`, and\\n`ConsensusState`.\\nTo abstract over chain-specific datatypes, we introduce a trait which specifies\\nboth which types we need to abstract over, and their interface.\\nFor the ICS 002 Client submodule, this trait looks as follow:\\n```rust\\npub trait ClientDef {\\ntype Header: Header;\\ntype ClientState: ClientState;\\ntype ConsensusState: ConsensusState;\\n}\\n```\\nThe `ClientDef` trait specifies three datatypes, and their corresponding interface, which is provided\\nvia a trait defined in the same submodule.\\nA production implementation of this interface would instantiate these types with the concrete\\ntypes used by the chain, eg. Tendermint datatypes. Each concrete datatype must be provided\\nwith a `From` instance to lift it into its corresponding `Any...` enumeration.\\nFor the purpose of unit-testing, a mock implementation of the `ClientDef` trait could look as follows:\\n```rust\\nstruct MockHeader(u32);\\nimpl Header for MockHeader {\\n\/\/ omitted\\n}\\nimpl From<MockHeader> for AnyHeader {\\nfn from(mh: MockHeader) -> Self {\\nSelf::Mock(mh)\\n}\\n}\\nstruct MockClientState(u32);\\nimpl ClientState for MockClientState {\\n\/\/ omitted\\n}\\nimpl From<MockClientState> for AnyClientState {\\nfn from(mcs: MockClientState) -> Self {\\nSelf::Mock(mcs)\\n}\\n}\\nstruct MockConsensusState(u32);\\nimpl ConsensusState for MockConsensusState {\\n\/\/ omitted\\n}\\nimpl From<MockConsensusState> for AnyConsensusState {\\nfn from(mcs: MockConsensusState) -> Self {\\nSelf::Mock(mcs)\\n}\\n}\\nstruct MockClient;\\nimpl ClientDef for MockClient {\\ntype Header = MockHeader;\\ntype ClientState = MockClientState;\\ntype ConsensusState = MockConsensusState;\\n}\\n```\\nSince the actual type of client can only be determined at runtime, we cannot encode\\nthe type of client within the message itself.\\nBecause of some limitations of the Rust type system, namely the lack of proper support\\nfor existential types, it is currently impossible to define `Reader` and `Keeper` traits\\nwhich are agnostic to the actual type of client being used.\\nWe could alternatively model all chain-specific datatypes as boxed trait objects (`Box<dyn Trait>`),\\nbut this approach runs into a lot of limitations of trait objects, such as the inability to easily\\nrequire such trait objects to be Clonable, or Serializable, or to define an equality relation on them.\\nSome support for such functionality can be found in third-party libraries, but the overall experience\\nfor the developer is too subpar.\\nWe thus settle on a different strategy: lifting chain-specific data into an `enum` over all\\npossible chain types.\\nFor example, to model a chain-specific `Header` type, we would define an enumeration in the following\\nway:\\n```rust\\n#[derive(Clone, Debug, PartialEq, Serialize, Deserialize)] \/\/ TODO: Add Eq\\npub enum AnyHeader {\\nMock(mocks::MockHeader),\\nTendermint(tendermint::header::Header),\\n}\\nimpl Header for AnyHeader {\\nfn height(&self) -> Height {\\nmatch self {\\nSelf::Mock(header) => header.height(),\\nSelf::Tendermint(header) => header.height(),\\n}\\n}\\nfn client_type(&self) -> ClientType {\\nmatch self {\\nSelf::Mock(header) => header.client_type(),\\nSelf::Tendermint(header) => header.client_type(),\\n}\\n}\\n}\\n```\\nThis enumeration dispatches method calls to the underlying datatype at runtime, while\\nhiding the latter, and is thus akin to a proper existential type without running\\ninto any limitations of the Rust type system (`impl Header` bounds not being allowed\\neverywhere, `Header` not being able to be treated as a trait objects because of `Clone`,\\n`PartialEq` and `Serialize`, `Deserialize` bounds, etc.)\\nOther chain-specific datatypes, such as `ClientState` and `ConsensusState` require their own\\nenumeration over all possible implementations.\\nOn top of that, we also need to lift the specific client definitions (`ClientDef` instances),\\ninto their own enumeration, as follows:\\n```rust\\n#[derive(Clone, Debug, PartialEq, Eq)]\\npub enum AnyClient {\\nMock(mocks::MockClient),\\nTendermint(tendermint::TendermintClient),\\n}\\nimpl ClientDef for AnyClient {\\ntype Header = AnyHeader;\\ntype ClientState = AnyClientState;\\ntype ConsensusState = AnyConsensusState;\\n}\\n```\\nMessages can now be defined generically over the `ClientDef` instance:\\n```rust\\n#[derive(Clone, Debug, PartialEq, Serialize, Deserialize)]\\npub struct MsgCreateClient<CD: ClientDef> {\\npub client_id: ClientId,\\npub client_type: ClientType,\\npub consensus_state: CD::ConsensusState,\\n}\\npub struct MsgUpdateClient<CD: ClientDef> {\\npub client_id: ClientId,\\npub header: CD::Header,\\n}\\n```\\nThe `Keeper` and `Reader` traits are defined for any client:\\n```rust\\npub trait ClientReader {\\nfn client_type(&self, client_id: &ClientId) -> Option<ClientType>;\\nfn client_state(&self, client_id: &ClientId) -> Option<AnyClientState>;\\nfn consensus_state(&self, client_id: &ClientId, height: Height) -> Option<AnyConsensusState>;\\n}\\npub trait ClientKeeper {\\nfn store_client_type(\\n&mut self,\\nclient_id: ClientId,\\nclient_type: ClientType,\\n) -> Result<(), Error>;\\nfn store_client_state(\\n&mut self,\\nclient_id: ClientId,\\nclient_state: AnyClientState,\\n) -> Result<(), Error>;\\nfn store_consensus_state(\\n&mut self,\\nclient_id: ClientId,\\nconsensus_state: AnyConsensusState,\\n) -> Result<(), Error>;\\n}\\n```\\nThis way, only one implementation of the `ClientReader` and `ClientKeeper` trait is required,\\nas it can delegate eg. the serialization of the underlying datatypes to the `Serialize` bound\\nof the `Any...` wrapper.\\nBoth the `process` and `keep` function are defined to take a message generic over\\nthe actual client type:\\n```rust\\npub fn process(\\nctx: &dyn ClientReader,\\nmsg: MsgCreateClient<AnyClient>,\\n) -> HandlerResult<CreateClientResult<AnyClient>, Error>;\\npub fn keep(\\nkeeper: &mut dyn ClientKeeper,\\nresult: CreateClientResult<AnyClient>,\\n) -> Result<(), Error>;\\n```\\nSame for the top-level dispatcher:\\n```rust\\npub fn dispatch<Ctx>(ctx: &mut Ctx, msg: ClientMsg<AnyClient>) -> Result<HandlerOutput<()>, Error>\\nwhere\\nCtx: ClientReader + ClientKeeper;\\n```\\nWith this boilerplate out of way, one can write tests using a mock client, and associated mock datatypes\\nin a fairly straightforward way, taking advantage of the `From` instance to lift concerete mock datatypes\\ninto the `Any...` enumeration:\\n```rust\\n#[test]\\nfn test_create_client_ok() {\\nlet client_id: ClientId = \"mockclient\".parse().unwrap();\\nlet reader = MockClientReader {\\nclient_id: client_id.clone(),\\nclient_type: None,\\nclient_state: None,\\nconsensus_state: None,\\n};\\nlet msg = MsgCreateClient {\\nclient_id,\\nclient_type: ClientType::Tendermint,\\nconsensus_state: MockConsensusState(42).into(), \/\/ lift into `AnyConsensusState`\\n};\\nlet output = process(&reader, msg.clone());\\nmatch output {\\nOk(HandlerOutput {\\nresult,\\nevents,\\nlog,\\n}) => {\\n\/\/ snip\\n}\\nErr(err) => {\\npanic!(\"unexpected error: {}\", err);\\n}\\n}\\n}\\n```\\n","tokens":30,"id":4656,"text":"## Context\\nIn this ADR, we provide recommendations for implementing the IBC\\nhandlers within the `ibc` (modules) crate.\\n\n\n##Decision\nConcepts are introduced in the order given by a topological sort of their dependencies on each other.\\n### Events\\nIBC handlers must be able to emit events which will then be broadcasted via the node's pub\/sub mechanism,\\nand eventually picked up by the IBC relayer.\\nAn event has an arbitrary structure, depending on the handler that produces it.\\nHere is the [list of all IBC-related events][events], as seen by the relayer.\\nNote that the consumer of these events in production would not be the relayer directly\\n(instead the consumer is the node\/SDK where the IBC module executes),\\nbut nevertheless handlers will reuse these event definitions.\\n[events]: https:\/\/github.com\/informalsystems\/hermes\/blob\/bf84a73ef7b3d5e9a434c9af96165997382dcc9d\/modules\/src\/events.rs#L15-L43\\n```rust\\npub enum IBCEvent {\\nNewBlock(NewBlock),\\nCreateClient(ClientEvents::CreateClient),\\nUpdateClient(ClientEvents::UpdateClient),\\nClientMisbehavior(ClientEvents::ClientMisbehavior),\\nOpenInitConnection(ConnectionEvents::OpenInit),\\nOpenTryConnection(ConnectionEvents::OpenTry),\\n\/\/     ...\\n}\\n```\\n### Logging\\nIBC handlers must be able to log information for introspectability and ease of debugging.\\nA handler can output multiple log records, which are expressed as a pair of a status and a\\nlog line. The interface for emitting log records is described in the next section.\\n```rust\\npub enum LogStatus {\\nSuccess,\\nInfo,\\nWarning,\\nError,\\n}\\npub struct Log {\\nstatus: LogStatus,\\nbody: String,\\n}\\nimpl Log {\\nfn success(msg: impl Display) -> Self;\\nfn info(msg: impl Display) -> Self;\\nfn warning(msg: impl Display) -> Self;\\nfn error(msg: impl Display) -> Self;\\n}\\n```\\n### Handler output\\nIBC handlers must be able to return arbitrary data, together with events and log records, as described above.\\nAs a handler may fail, it is necessary to keep track of errors.\\nTo this end, we introduce a type for the return value of a handler:\\n```rust\\npub type HandlerResult<T, E> = Result<HandlerOutput<T>, E>;\\npub struct HandlerOutput<T> {\\npub result: T,\\npub log: Vec<Log>,\\npub events: Vec<Event>,\\n}\\n```\\nWe introduce a builder interface to be used within the handler implementation to incrementally build a `HandlerOutput` value.\\n```rust\\nimpl<T> HandlerOutput<T> {\\npub fn builder() -> HandlerOutputBuilder<T> {\\nHandlerOutputBuilder::new()\\n}\\n}\\npub struct HandlerOutputBuilder<T> {\\nlog: Vec<String>,\\nevents: Vec<Event>,\\nmarker: PhantomData<T>,\\n}\\nimpl<T> HandlerOutputBuilder<T> {\\npub fn log(&mut self, log: impl Into<Log>);\\npub fn emit(&mut self, event: impl Into<Event>);\\npub fn with_result(self, result: T) -> HandlerOutput<T>;\\n}\\n```\\nWe provide below an example usage of the builder API:\\n```rust\\nfn some_ibc_handler() -> HandlerResult<u64, Error> {\\nlet mut output = HandlerOutput::builder();\\n\/\/ ...\\noutput.log(Log::info(\"did something\"))\\n\/\/ ...\\noutput.log(Log::success(\"all good\"));\\noutput.emit(SomeEvent::AllGood);\\nOk(output.with_result(42));\\n}\\n```\\n### IBC Submodule\\nThe various IBC messages and their processing logic, as described in the IBC specification,\\nare split into a collection of submodules, each pertaining to a specific aspect of\\nthe IBC protocol, eg. client lifecycle management, connection lifecycle management,\\npacket relay, etc.\\nIn this section we propose a general approach to implement the handlers for a submodule.\\nAs a running example we will use a dummy submodule that deals with connections, which should not\\nbe mistaken for the actual ICS 003 Connection submodule.\\n#### Reader\\nA typical handler will need to read data from the chain state at the current height,\\nvia the private and provable stores.\\nTo avoid coupling between the handler interface and the store API, we introduce an interface\\nfor accessing this data. This interface, called a `Reader`, is shared between all handlers\\nin a submodule, as those typically access the same data.\\nHaving a high-level interface for this purpose helps avoiding coupling which makes\\nwriting unit tests for the handlers easier, as one does not need to provide a concrete\\nstore, or to mock one.\\n```rust\\npub trait ConnectionReader\\n{\\nfn connection_end(&self, connection_id: &ConnectionId) -> Option<ConnectionEnd>;\\n}\\n```\\nA production implementation of this `Reader` would hold references to both the private and provable\\nstore at the current height where the handler executes, but we omit the actual implementation as\\nthe store interfaces are yet to be defined, as is the general IBC top-level module machinery.\\nA mock implementation of the `ConnectionReader` trait could looks as follows:\\n```rust\\nstruct MockConnectionReader {\\nconnection_id: ConnectionId,\\nconnection_end: Option<ConnectionEnd>,\\nclient_reader: MockClientReader,\\n}\\nimpl ConnectionReader for MockConnectionReader {\\nfn connection_end(&self, connection_id: &ConnectionId) -> Option<ConnectionEnd> {\\nif connection_id == &self.connection_id {\\nself.connection_end.clone()\\n} else {\\nNone\\n}\\n}\\n}\\n```\\n#### Keeper\\nOnce a handler executes successfully, some data will typically need to be persisted in the chain state\\nvia the private\/provable store interfaces. In the same vein as for the reader defined in the previous section,\\na submodule should define a trait which provides operations to persist such data.\\nThe same considerations w.r.t. to coupling and unit-testing apply here as well.\\n```rust\\npub trait ConnectionKeeper {\\nfn store_connection(\\n&mut self,\\nclient_id: ConnectionId,\\nclient_type: ConnectionType,\\n) -> Result<(), Error>;\\nfn add_connection_to_client(\\n&mut self,\\nclient_id: ClientId,\\nconnection_id: ConnectionId,\\n) -> Result<(), Error>;\\n}\\n```\\n#### Submodule implementation\\nWe now come to the actual definition of a handler for a submodule.\\nWe recommend each handler to be defined within its own Rust module, named\\nafter the handler itself. For example, the \"Create Client\" handler of ICS 002 would\\nbe defined in `modules::ics02_client::handler::create_client`.\\n##### Message type\\nEach handler must define a datatype which represent the message it can process.\\n```rust\\npub struct MsgConnectionOpenInit {\\nconnection_id: ConnectionId,\\nclient_id: ClientId,\\ncounterparty: Counterparty,\\n}\\n```\\n##### Handler implementation\\nIn this section we provide guidelines for implementing an actual handler.\\nWe divide the handler in two parts: processing and persistence.\\n###### Processing\\nThe actual logic of the handler is expressed as a pure function, typically named\\n`process`, which takes as arguments a `Reader` and the corresponding message, and returns\\na `HandlerOutput<T, E>`, where `T` is a concrete datatype and `E` is an error type which defines\\nall potential errors yielded by the handlers of the current submodule.\\n```rust\\npub struct ConnectionMsgProcessingResult {\\nconnection_id: ConnectionId,\\nconnection_end: ConnectionEnd,\\n}\\n```\\nThe `process` function will typically read data via the `Reader`, perform checks and validation, construct new\\ndatatypes, emit log records and events, and eventually return some data together with objects to be persisted.\\nTo this end, this `process` function will create and manipulate a `HandlerOutput` value like described in\\nthe corresponding section.\\n```rust\\npub fn process(\\nreader: &dyn ConnectionReader,\\nmsg: MsgConnectionOpenInit,\\n) -> HandlerResult<ConnectionMsgProcessingResult, Error>\\n{\\nlet mut output = HandlerOutput::builder();\\nlet MsgConnectionOpenInit { connection_id, client_id, counterparty, } = msg;\\nif reader.connection_end(&connection_id).is_some() {\\nreturn Err(Kind::ConnectionAlreadyExists(connection_id).into());\\n}\\noutput.log(\"success: no connection state found\");\\nif reader.client_reader.client_state(&client_id).is_none() {\\nreturn Err(Kind::ClientForConnectionMissing(client_id).into());\\n}\\noutput.log(\"success: client found\");\\noutput.emit(IBCEvent::ConnectionOpenInit(connection_id.clone()));\\nOk(output.with_result(ConnectionMsgProcessingResult {\\nconnection_id,\\nclient_id,\\ncounterparty,\\n}))\\n}\\n```\\n###### Persistence\\nIf the `process` function specified above succeeds, the result value it yielded is then\\npassed to a function named `keep`, which is responsible for persisting the objects constructed\\nby the processing function. This `keep` function takes the submodule's `Keeper` and the result\\ntype defined above, and performs side-effecting calls to the keeper's methods to persist the result.\\nBelow is given an implementation of the `keep` function for the \"Create Connection\" handlers:\\n```rust\\npub fn keep(\\nkeeper: &mut dyn ConnectionKeeper,\\nresult: ConnectionMsgProcessingResult,\\n) -> Result<(), Error>\\n{\\nkeeper.store_connection(result.connection_id.clone(), result.connection_end)?;\\nkeeper.add_connection_to_client(result.client_id, result.connection_id)?;\\nOk(())\\n}\\n```\\n##### Submodule dispatcher\\n> This section is very much a work in progress, as further investigation into what\\n> a production-ready implementation of the `ctx` parameter of the top-level dispatcher\\n> is required. As such, implementers should feel free to disregard the recommendations\\n> below, and are encouraged to come up with amendments to this ADR to better capture\\n> the actual requirements.\\nEach submodule is responsible for dispatching the messages it is given to the appropriate\\nmessage processing function and, if successful, pass the resulting data to the persistence\\nfunction defined in the previous section.\\nTo this end, the submodule should define an enumeration of all messages, in order\\nfor the top-level submodule dispatcher to forward them to the appropriate processor.\\nSuch a definition for the ICS 003 Connection submodule is given below.\\n```rust\\npub enum ConnectionMsg {\\nConnectionOpenInit(MsgConnectionOpenInit),\\nConnectionOpenTry(MsgConnectionOpenTry),\\n...\\n}\\n```\\nThe actual implementation of a submodule dispatcher is quite straightforward and unlikely to vary\\nmuch in substance between submodules. We give an implementation for the ICS 003 Connection module below.\\n```rust\\npub fn dispatch<Ctx>(ctx: &mut Ctx, msg: Msg) -> Result<HandlerOutput<()>, Error>\\nwhere\\nCtx: ConnectionReader + ConnectionKeeper,\\n{\\nmatch msg {\\nMsg::ConnectionOpenInit(msg) => {\\nlet HandlerOutput {\\nresult,\\nlog,\\nevents,\\n} = connection_open_init::process(ctx, msg)?;\\nconnection::keep(ctx, result)?;\\nOk(HandlerOutput::builder()\\n.with_log(log)\\n.with_events(events)\\n.with_result(()))\\n}\\nMsg::ConnectionOpenTry(msg) => \/\/ omitted\\n}\\n}\\n```\\nIn essence, a top-level dispatcher is a function of a message wrapped in the enumeration introduced above,\\nand a \"context\" which implements both the `Reader` and `Keeper` interfaces.\\n### Dealing with chain-specific datatypes\\nThe ICS 002 Client submodule stands out from the other submodules as it needs\\nto deal with chain-specific datatypes, such as `Header`, `ClientState`, and\\n`ConsensusState`.\\nTo abstract over chain-specific datatypes, we introduce a trait which specifies\\nboth which types we need to abstract over, and their interface.\\nFor the ICS 002 Client submodule, this trait looks as follow:\\n```rust\\npub trait ClientDef {\\ntype Header: Header;\\ntype ClientState: ClientState;\\ntype ConsensusState: ConsensusState;\\n}\\n```\\nThe `ClientDef` trait specifies three datatypes, and their corresponding interface, which is provided\\nvia a trait defined in the same submodule.\\nA production implementation of this interface would instantiate these types with the concrete\\ntypes used by the chain, eg. Tendermint datatypes. Each concrete datatype must be provided\\nwith a `From` instance to lift it into its corresponding `Any...` enumeration.\\nFor the purpose of unit-testing, a mock implementation of the `ClientDef` trait could look as follows:\\n```rust\\nstruct MockHeader(u32);\\nimpl Header for MockHeader {\\n\/\/ omitted\\n}\\nimpl From<MockHeader> for AnyHeader {\\nfn from(mh: MockHeader) -> Self {\\nSelf::Mock(mh)\\n}\\n}\\nstruct MockClientState(u32);\\nimpl ClientState for MockClientState {\\n\/\/ omitted\\n}\\nimpl From<MockClientState> for AnyClientState {\\nfn from(mcs: MockClientState) -> Self {\\nSelf::Mock(mcs)\\n}\\n}\\nstruct MockConsensusState(u32);\\nimpl ConsensusState for MockConsensusState {\\n\/\/ omitted\\n}\\nimpl From<MockConsensusState> for AnyConsensusState {\\nfn from(mcs: MockConsensusState) -> Self {\\nSelf::Mock(mcs)\\n}\\n}\\nstruct MockClient;\\nimpl ClientDef for MockClient {\\ntype Header = MockHeader;\\ntype ClientState = MockClientState;\\ntype ConsensusState = MockConsensusState;\\n}\\n```\\nSince the actual type of client can only be determined at runtime, we cannot encode\\nthe type of client within the message itself.\\nBecause of some limitations of the Rust type system, namely the lack of proper support\\nfor existential types, it is currently impossible to define `Reader` and `Keeper` traits\\nwhich are agnostic to the actual type of client being used.\\nWe could alternatively model all chain-specific datatypes as boxed trait objects (`Box<dyn Trait>`),\\nbut this approach runs into a lot of limitations of trait objects, such as the inability to easily\\nrequire such trait objects to be Clonable, or Serializable, or to define an equality relation on them.\\nSome support for such functionality can be found in third-party libraries, but the overall experience\\nfor the developer is too subpar.\\nWe thus settle on a different strategy: lifting chain-specific data into an `enum` over all\\npossible chain types.\\nFor example, to model a chain-specific `Header` type, we would define an enumeration in the following\\nway:\\n```rust\\n#[derive(Clone, Debug, PartialEq, Serialize, Deserialize)] \/\/ TODO: Add Eq\\npub enum AnyHeader {\\nMock(mocks::MockHeader),\\nTendermint(tendermint::header::Header),\\n}\\nimpl Header for AnyHeader {\\nfn height(&self) -> Height {\\nmatch self {\\nSelf::Mock(header) => header.height(),\\nSelf::Tendermint(header) => header.height(),\\n}\\n}\\nfn client_type(&self) -> ClientType {\\nmatch self {\\nSelf::Mock(header) => header.client_type(),\\nSelf::Tendermint(header) => header.client_type(),\\n}\\n}\\n}\\n```\\nThis enumeration dispatches method calls to the underlying datatype at runtime, while\\nhiding the latter, and is thus akin to a proper existential type without running\\ninto any limitations of the Rust type system (`impl Header` bounds not being allowed\\neverywhere, `Header` not being able to be treated as a trait objects because of `Clone`,\\n`PartialEq` and `Serialize`, `Deserialize` bounds, etc.)\\nOther chain-specific datatypes, such as `ClientState` and `ConsensusState` require their own\\nenumeration over all possible implementations.\\nOn top of that, we also need to lift the specific client definitions (`ClientDef` instances),\\ninto their own enumeration, as follows:\\n```rust\\n#[derive(Clone, Debug, PartialEq, Eq)]\\npub enum AnyClient {\\nMock(mocks::MockClient),\\nTendermint(tendermint::TendermintClient),\\n}\\nimpl ClientDef for AnyClient {\\ntype Header = AnyHeader;\\ntype ClientState = AnyClientState;\\ntype ConsensusState = AnyConsensusState;\\n}\\n```\\nMessages can now be defined generically over the `ClientDef` instance:\\n```rust\\n#[derive(Clone, Debug, PartialEq, Serialize, Deserialize)]\\npub struct MsgCreateClient<CD: ClientDef> {\\npub client_id: ClientId,\\npub client_type: ClientType,\\npub consensus_state: CD::ConsensusState,\\n}\\npub struct MsgUpdateClient<CD: ClientDef> {\\npub client_id: ClientId,\\npub header: CD::Header,\\n}\\n```\\nThe `Keeper` and `Reader` traits are defined for any client:\\n```rust\\npub trait ClientReader {\\nfn client_type(&self, client_id: &ClientId) -> Option<ClientType>;\\nfn client_state(&self, client_id: &ClientId) -> Option<AnyClientState>;\\nfn consensus_state(&self, client_id: &ClientId, height: Height) -> Option<AnyConsensusState>;\\n}\\npub trait ClientKeeper {\\nfn store_client_type(\\n&mut self,\\nclient_id: ClientId,\\nclient_type: ClientType,\\n) -> Result<(), Error>;\\nfn store_client_state(\\n&mut self,\\nclient_id: ClientId,\\nclient_state: AnyClientState,\\n) -> Result<(), Error>;\\nfn store_consensus_state(\\n&mut self,\\nclient_id: ClientId,\\nconsensus_state: AnyConsensusState,\\n) -> Result<(), Error>;\\n}\\n```\\nThis way, only one implementation of the `ClientReader` and `ClientKeeper` trait is required,\\nas it can delegate eg. the serialization of the underlying datatypes to the `Serialize` bound\\nof the `Any...` wrapper.\\nBoth the `process` and `keep` function are defined to take a message generic over\\nthe actual client type:\\n```rust\\npub fn process(\\nctx: &dyn ClientReader,\\nmsg: MsgCreateClient<AnyClient>,\\n) -> HandlerResult<CreateClientResult<AnyClient>, Error>;\\npub fn keep(\\nkeeper: &mut dyn ClientKeeper,\\nresult: CreateClientResult<AnyClient>,\\n) -> Result<(), Error>;\\n```\\nSame for the top-level dispatcher:\\n```rust\\npub fn dispatch<Ctx>(ctx: &mut Ctx, msg: ClientMsg<AnyClient>) -> Result<HandlerOutput<()>, Error>\\nwhere\\nCtx: ClientReader + ClientKeeper;\\n```\\nWith this boilerplate out of way, one can write tests using a mock client, and associated mock datatypes\\nin a fairly straightforward way, taking advantage of the `From` instance to lift concerete mock datatypes\\ninto the `Any...` enumeration:\\n```rust\\n#[test]\\nfn test_create_client_ok() {\\nlet client_id: ClientId = \"mockclient\".parse().unwrap();\\nlet reader = MockClientReader {\\nclient_id: client_id.clone(),\\nclient_type: None,\\nclient_state: None,\\nconsensus_state: None,\\n};\\nlet msg = MsgCreateClient {\\nclient_id,\\nclient_type: ClientType::Tendermint,\\nconsensus_state: MockConsensusState(42).into(), \/\/ lift into `AnyConsensusState`\\n};\\nlet output = process(&reader, msg.clone());\\nmatch output {\\nOk(HandlerOutput {\\nresult,\\nevents,\\nlog,\\n}) => {\\n\/\/ snip\\n}\\nErr(err) => {\\npanic!(\"unexpected error: {}\", err);\\n}\\n}\\n}\\n```\\n","MatchedIds":[1442,4650,4536,21,4952],"Predicted":"## Decision\n\n### IBC Handler Design Principles\n\n1. **Modularity**:\n   - IBC handlers should be modular to enable independent development, testing, and reuse.\n   - Each IBC message (e.g., ChannelOpenInit, ChannelOpenAck) should correspond to a distinct handler module.\n\n2. **Separation of Concerns**:\n   - IBC handlers should separate concerns such as message validation, state transition logic, and proof verification.\n   - Use distinct functions for logic separation (e.g., `validate_msg`, `execute_state_transition`, `verify_proof`).\n\n3. **Interoperability**:\n   - Handlers must be compatible with the broader Cosmos SDK and other IBC-compliant applications.\n   - Ensure usage of standard data structures and patterns defined by the Cosmos and IBC specifications.\n\n4. **State Management**:\n   - Leverage existing state management provided by the `ibc` crate using the `Context` and `Store` traits.\n   - Handlers should operate on immutable snapshots of state for read operations and use transactions for write operations.\n\n### Handler Structure\n\nHere is a recommended structure for an IBC handler module:\n\n```rust\n\/\/ Example IBC handler module\n\n\/\/ Imports and module declarations\nuse ibc::prelude::*;\nuse ibc::HandlerResult;\nuse ibc::messages::{MsgChannelOpenInit, MsgChannelOpenAck};\nuse ibc::context::Context;\nuse ibc::store::Store;\nuse ibc::errors::Error;\n\n\/\/ Validate the incoming message\nfn validate_msg(ctx: &Context, msg: &MsgChannelOpenInit) -> Result<(), Error> {\n    \/\/ Perform validation logic (e.g., check signatures, state constraints)\n    \/\/ ...\n    \n    Ok(())\n}\n\n\/\/ Perform state transition based on the validated message\nfn execute_state_transition(ctx: &mut Context, msg: &MsgChannelOpenInit) -> Result<(), Error> {\n    \/\/ Modify the state according to message semantics\n    \/\/ ...\n    \n    Ok(())\n}\n\n\/\/ Verify any necessary proofs associated with the message\nfn verify_proof(ctx: &Context, msg: &MsgChannelOpenInit) -> Result<(), Error> {\n    \/\/ Proof verification logic (e.g., Merkle proof validation)\n    \/\/ ...\n    \n    Ok(())\n}\n\n\/\/ IBC handler function\npub fn ibc_channel_open_init(ctx: &mut Context, msg: MsgChannelOpenInit) -> HandlerResult {\n    \/\/ Validate the message\n    validate_msg(ctx, &msg)?;\n    \n    \/\/ Verify proofs if necessary\n    verify_proof(ctx, &msg)?;\n    \n    \/\/ Perform state transition\n    execute_state_transition(ctx, &msg)?;\n    \n    Ok(())\n}\n\n```\n\n### Integration with IBC Modules Crate\n\n1. **Registry of Handlers**:\n   - Maintain a registry of all available IBC handlers within the crate, allowing dynamic invocation based on message types.\n   - Implement a dispatcher function that routes incoming messages to the appropriate handler module.\n\n2. **Testing and Validation**:\n   - Ensure comprehensive unit tests for each handler module.\n   - Integrate handlers with existing IBC test suites and validation tools for consistency checking.\n\n3. **Documentation**:\n   - Provide clear documentation for each handler module, including examples and expected behavior.\n   - Ensure the crate-level README and documentation are updated to reflect new handler structures and integrations.\n\n### Example Dispatcher Function\n\n```rust\n\/\/ IBC message dispatcher function\npub fn dispatch_ibc_message(ctx: &mut Context, msg: IbcMessage) -> HandlerResult {\n    match msg {\n        IbcMessage::ChannelOpenInit(msg) => ibc_channel_open_init(ctx, msg),\n        IbcMessage::ChannelOpenAck(msg) => ibc_channel_open_ack(ctx, msg),\n        \/\/ Add handlers for other IBC messages\n        _ => Err(Error::UnknownMessageType),\n    }\n}\n```\n\nBy adhering to these principles and structures, we will ensure that the IBC handlers within the `ibc` crate are robust, maintainable, and conformant to IBC standards.","GenTime":"2024-08-20 00:37:37"}
{"File Name":"adr-demo\/0001-record-architecture-decisions.md","Context":"## Context\\nMost projects suffer from a common problem : they usually don't keep a clear history of all the architectural decisions if the project.\\nIt might not appear as an issue at first but as the project evolves it becomes less and less clear why each change was made,\\nleading to somewhat wrong decisions : should we change existing code and take the risk of breaking the application because\\nwe might have missed an important decision, or should we keep it (fearing we might break something) and take the risk of\\nparalyzing the project with an accumulation of potentially wrongly-kept decisions and changes ?\\nTo avoid this dilemna it appears we have to do something to keep a record of all architectural decisions.\\n","Decision":"We will start using Lightweight Architecture Decision Records (further refered as ADR) as explained\\n[here](https:\/\/blog.stack-labs.com\/code\/adr-to-remember-past-archiectural-decisions\/#format-d-un-adr)\\nor [here](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\nHere are a few hints of how we will use ADRs :\\n* We will keep all ADRs in a Git respository so they can be versioned\\n* We will materialize Each ADR as a separate file within the repository\\n* We will prefix each ADR by an ordered number (0001 to 9999), and keep ADRs numbers sequential\\n* We will keep each ADR as short as possible, trying to limit ourselves to 1-2 pages max\\n* We will use Markdown as the template engine of our ADRs\\n* We will always keep ALL written ADRs but we will mark old ADRs as superseded if they are\\nThe markdown format we will use when writing an ADR is the following :\\n```markdown\\n# Title: These documents have names that are short noun phrases. For example, \"ADR-0001 - Deployment on Ruby on Rails 3.0.10\" or \"ADR 9: LDAP for Multitenant Integration\"\\nThis section describes our response to these forces. It is stated in full sentences, with active voice. \"We will ...\"\\n","tokens":146,"id":2517,"text":"## Context\\nMost projects suffer from a common problem : they usually don't keep a clear history of all the architectural decisions if the project.\\nIt might not appear as an issue at first but as the project evolves it becomes less and less clear why each change was made,\\nleading to somewhat wrong decisions : should we change existing code and take the risk of breaking the application because\\nwe might have missed an important decision, or should we keep it (fearing we might break something) and take the risk of\\nparalyzing the project with an accumulation of potentially wrongly-kept decisions and changes ?\\nTo avoid this dilemna it appears we have to do something to keep a record of all architectural decisions.\\n\n\n##Decision\nWe will start using Lightweight Architecture Decision Records (further refered as ADR) as explained\\n[here](https:\/\/blog.stack-labs.com\/code\/adr-to-remember-past-archiectural-decisions\/#format-d-un-adr)\\nor [here](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\nHere are a few hints of how we will use ADRs :\\n* We will keep all ADRs in a Git respository so they can be versioned\\n* We will materialize Each ADR as a separate file within the repository\\n* We will prefix each ADR by an ordered number (0001 to 9999), and keep ADRs numbers sequential\\n* We will keep each ADR as short as possible, trying to limit ourselves to 1-2 pages max\\n* We will use Markdown as the template engine of our ADRs\\n* We will always keep ALL written ADRs but we will mark old ADRs as superseded if they are\\nThe markdown format we will use when writing an ADR is the following :\\n```markdown\\n# Title: These documents have names that are short noun phrases. For example, \"ADR-0001 - Deployment on Ruby on Rails 3.0.10\" or \"ADR 9: LDAP for Multitenant Integration\"\\nThis section describes our response to these forces. It is stated in full sentences, with active voice. \"We will ...\"\\n","MatchedIds":[4620,3784,2091,4361,4699],"Predicted":"## Decision\nWe will implement a system of Architecture Decision Records (ADRs) to maintain a clear history of all architectural decisions made throughout the project. This will help us avoid the dilemma of choosing between risky changes and project paralysis by providing documented context and rationale behind each decision.\n\nThe ADRs will:\n- Be stored in a dedicated `docs\/ADR-####-title.md` directory within the project repository.\n- Use a lightweight text formatting language such as Markdown.\n- Follow a structured format with sections for Title, Context, Decision, Status, and Consequences.\n- Be numbered sequentially to maintain a clear and chronological record.\n- Include both active and deprecated decisions to provide a comprehensive historical record, marking superseded decisions accordingly.\n\nBy maintaining meticulous ADRs, we ensure future developers and stakeholders can understand and re-evaluate past decisions, thus enabling informed and confident decision-making as the project evolves.","GenTime":"2024-08-20 00:37:53"}
{"File Name":"james-project\/0047-jmap-push-over-websockets.md","Context":"## Context\\nJMAP Push notifications allow clients to efficiently update (almost) instantly to stay in sync with data changes on the server.\\nIn order to support this, we need to have the server emit newest state changes to the client over a PUSH channel.\\n","Decision":"We will implement [RFC-8887 - A JSON Meta Application Protocol (JMAP) Subprotocol for WebSocket](https:\/\/tools.ietf.org\/html\/rfc8887)\\nas a transport mechanism for PUSH notifications.\\nWe will [generalize EventBus](0046-generalize-event-bus.md) in order to achieve an out-of-the box Publish-Subscribe\\nsystem for JMAP related events, that does not fit in the mailbox API.\\nWe will implement listeners registered on the JMAP event bus for WebSocket clients in order to carry over state changes\\nto the client.\\n","tokens":53,"id":2886,"text":"## Context\\nJMAP Push notifications allow clients to efficiently update (almost) instantly to stay in sync with data changes on the server.\\nIn order to support this, we need to have the server emit newest state changes to the client over a PUSH channel.\\n\n\n##Decision\nWe will implement [RFC-8887 - A JSON Meta Application Protocol (JMAP) Subprotocol for WebSocket](https:\/\/tools.ietf.org\/html\/rfc8887)\\nas a transport mechanism for PUSH notifications.\\nWe will [generalize EventBus](0046-generalize-event-bus.md) in order to achieve an out-of-the box Publish-Subscribe\\nsystem for JMAP related events, that does not fit in the mailbox API.\\nWe will implement listeners registered on the JMAP event bus for WebSocket clients in order to carry over state changes\\nto the client.\\n","MatchedIds":[4601,3127,3845,3337,2309],"Predicted":"## Decision\nWe will implement JMAP Push notifications using WebSockets. This approach ensures a persistent connection between the client and the server, allowing real-time updates and efficient use of resources. The server will emit state changes over this WebSocket connection, ensuring that clients remain up-to-date with the latest data changes almost instantly.\n\n### Action Plan\n1. **Establish WebSocket Connections:** On client initialization, establish a WebSocket connection to the server for receiving push notifications.\n2. **Authenticate and Authorize:** Ensure WebSocket connections are authenticated and authorized to prevent unauthorized access to state changes.\n3. **Emit State Changes:** Modify the server's state change handlers to emit relevant updates through the WebSocket connection.\n4. **Handle Connectivity Issues:** Implement retry mechanisms and back-off strategies to handle temporary disconnections and ensure seamless reconnections of WebSocket connections.\n5. **Client Update Handling:** Ensure the client handles incoming push notifications appropriately, updating the UI or internal state as required.\n6. **Monitor and Log:** Implement monitoring and logging to track the performance and reliability of the WebSocket push notifications.\n\nBy using WebSockets, we achieve low-latency, real-time updates that keep clients efficiently in sync with server data changes.","GenTime":"2024-08-20 00:38:00"}
{"File Name":"rails-template\/0009-use-scripts-to-rule-them-all.md","Context":"## Context\\ndxw have approved an RFC for following the pattern of Scripts To Rule Them\\nAll[1].\\nThis repository should include reference and document this decision.\\n[1]\\nhttps:\/\/github.com\/dxw\/tech-team-rfcs\/blob\/main\/rfc-023-use-scripts-to-rule-them-all.md\\n","Decision":"By default we will follow the Scripts To Rule Them All pattern for common tasks\\nin this template.\\n","tokens":67,"id":4178,"text":"## Context\\ndxw have approved an RFC for following the pattern of Scripts To Rule Them\\nAll[1].\\nThis repository should include reference and document this decision.\\n[1]\\nhttps:\/\/github.com\/dxw\/tech-team-rfcs\/blob\/main\/rfc-023-use-scripts-to-rule-them-all.md\\n\n\n##Decision\nBy default we will follow the Scripts To Rule Them All pattern for common tasks\\nin this template.\\n","MatchedIds":[400,4361,2703,4428,3119],"Predicted":"## Decision\nWe will adopt the \"Scripts To Rule Them All\" pattern as described in the [dxw RFC-023](https:\/\/github.com\/dxw\/tech-team-rfcs\/blob\/main\/rfc-023-use-scripts-to-rule-them-all.md). This repository will include the necessary scripts and documentation to follow this pattern.","GenTime":"2024-08-20 00:38:09"}
{"File Name":"status-react\/0007-masking-sensitive-data.md","Context":"## Context\\nWe have some data that we don't want to appear in the logs (user passwords are\\na good example). Currently, they are passed around as strings, that could be\\nprinted out by mistake in a log entry (see https:\/\/github.com\/status-im\/status-react\/issues\/4053)\\n","Decision":"To minimize the risk of leaking passwords through logs, we should not pass\\npasswords as strings in our codebase. We introduced a new type `MaskedData` in\\n`status-im.utils.security`.\\nWe use `(security\/mask-data <data to hide>` to wrap sensitive data into this\\ntype and then use `(security\/unmask <masked-data>)` to get the plaintext back.\\nIt is important to keep that sensitive data masked as much as possible, until\\nyou need the plaintext to pass to the extenral APIs.\\nExample:\\n```clojure\\n(println (security\/mask-data \"my-plaintext-password\")) ;; Outputs \"******\"\\n(println (security\/unmask (security\/mask-data \"my-plaintext-password\"))) ;; Outputs \"my-plaintext-password\"\\n```\\n","tokens":63,"id":2958,"text":"## Context\\nWe have some data that we don't want to appear in the logs (user passwords are\\na good example). Currently, they are passed around as strings, that could be\\nprinted out by mistake in a log entry (see https:\/\/github.com\/status-im\/status-react\/issues\/4053)\\n\n\n##Decision\nTo minimize the risk of leaking passwords through logs, we should not pass\\npasswords as strings in our codebase. We introduced a new type `MaskedData` in\\n`status-im.utils.security`.\\nWe use `(security\/mask-data <data to hide>` to wrap sensitive data into this\\ntype and then use `(security\/unmask <masked-data>)` to get the plaintext back.\\nIt is important to keep that sensitive data masked as much as possible, until\\nyou need the plaintext to pass to the extenral APIs.\\nExample:\\n```clojure\\n(println (security\/mask-data \"my-plaintext-password\")) ;; Outputs \"******\"\\n(println (security\/unmask (security\/mask-data \"my-plaintext-password\"))) ;; Outputs \"my-plaintext-password\"\\n```\\n","MatchedIds":[4843,577,4443,13,2256],"Predicted":"## Decision\nWe will introduce a `SensitiveData` wrapper class to handle sensitive information such as user passwords. This class will override the `toString` method to return a non-sensitive placeholder (e.g., \"*****\") and will not allow the actual sensitive data to be printed.\n\nAdditionally, we will enforce stricter logging policies:\n1. **Review and Update Logging Statements:** We will audit current logging statements and verify that no method prints any instance of the `SensitiveData` class directly.\n2. **Training and Guidelines:** We will provide training and update developer guidelines to emphasize the importance of not logging sensitive information.\n3. **Automated Scans:** Implement automated scans and code reviews to identify instances where sensitive information might inadvertently leak into logs.\n\nThis approach ensures that even if sensitive information is inadvertently passed to a logging function, it will not be printed in the logs.","GenTime":"2024-08-20 00:38:14"}
{"File Name":"bob\/0004-introduce-temporal-db.md","Context":"## Context\\nCurrently we are using a traditional RDBMS, PostgreSQL as the storage of all the CI\/CD state.\\nThe DB is the one and only state of the whole CI\/CD cluster, storing all of the pipeline, resource, artifact definitions, runs, logs etc. For all of these, tracking historical changes is of utmost importance.\\nUsing a traditional CRUD workflow with PostgeSQL raises the following issues:\\n- Analytics are really hard as the Storage and Querying is coupled and a heavy query slows the whole cluster down\\n- Central locking is a severe impediment to scale\\n- When a change occurs in the pipeline, resource provider or artifact store definition its quite difficult to track these changes across time for audit and rollback needs\\n- To debug failures we need to have extended historical logging and is difficult to trace back in an update-in place system\\n- The tabular structure comes in our way with rigidity and is not simple to introduce schema changes\\n- Need of elaborate migration strategies which is further complicated with distributed state\\n","Decision":"Based on the above facts the following is decided:\\n- Use [XTDB](https:\/\/xtdb.com) as the temporal, document store for the following reasons:\\n- Though being newer compared to [Datomic](https:\/\/www.datomic.com\/) its free and open source and ready for production use\\n- Has a quite unbundled design and uses a variety of storage backends and is transactor free\\n- Is [bi-temporal](https:\/\/xtdb.com\/articles\/bitemporality.html) and thereby offering more systematic analytical queries across time\\n- Has an HTTP interface for non JVM clients\\n- Use it with JDBC\/PostgreSQL backend which is quite readily available and managed in all popular cloud providers\\n- Remove the CRUD way of doing things, expose the DB too via API for more powerful, direct analytical querying\\n","tokens":209,"id":4285,"text":"## Context\\nCurrently we are using a traditional RDBMS, PostgreSQL as the storage of all the CI\/CD state.\\nThe DB is the one and only state of the whole CI\/CD cluster, storing all of the pipeline, resource, artifact definitions, runs, logs etc. For all of these, tracking historical changes is of utmost importance.\\nUsing a traditional CRUD workflow with PostgeSQL raises the following issues:\\n- Analytics are really hard as the Storage and Querying is coupled and a heavy query slows the whole cluster down\\n- Central locking is a severe impediment to scale\\n- When a change occurs in the pipeline, resource provider or artifact store definition its quite difficult to track these changes across time for audit and rollback needs\\n- To debug failures we need to have extended historical logging and is difficult to trace back in an update-in place system\\n- The tabular structure comes in our way with rigidity and is not simple to introduce schema changes\\n- Need of elaborate migration strategies which is further complicated with distributed state\\n\n\n##Decision\nBased on the above facts the following is decided:\\n- Use [XTDB](https:\/\/xtdb.com) as the temporal, document store for the following reasons:\\n- Though being newer compared to [Datomic](https:\/\/www.datomic.com\/) its free and open source and ready for production use\\n- Has a quite unbundled design and uses a variety of storage backends and is transactor free\\n- Is [bi-temporal](https:\/\/xtdb.com\/articles\/bitemporality.html) and thereby offering more systematic analytical queries across time\\n- Has an HTTP interface for non JVM clients\\n- Use it with JDBC\/PostgreSQL backend which is quite readily available and managed in all popular cloud providers\\n- Remove the CRUD way of doing things, expose the DB too via API for more powerful, direct analytical querying\\n","MatchedIds":[1167,1574,2520,1701,1740],"Predicted":"## Decision Drivers\n* Scalability\n* Flexibility for schema changes\n* Efficient analytics\n* Effective historical tracking for audit and rollback\n* Minimal locking to improve performance\n* Simplified debugging and historical logging capabilities\n\n## Decision\nWe will transition to a NoSQL database for the CI\/CD state storage. Specifically, we will use MongoDB for the following reasons:\n- **Scalability**: MongoDB provides horizontal scaling capabilities, allowing us to distribute data across multiple servers to handle more queries and larger datasets efficiently.\n- **Flexible Schema**: MongoDB's schemaless nature permits more dynamic data handling, making it easier to introduce and manage schema changes without the need for extensive migration strategies.\n- **Efficient Analytics**: By decoupling storage and querying, MongoDB can optimize performance for analytics workloads and reduce the risk of heavy queries slowing down the entire cluster.\n- **Historical Tracking**: MongoDB supports embedding documents within documents, which is beneficial for tracking changes across time and improves audit and rollback capabilities.\n- **Reduced Locking**: MongoDB uses more granular locking mechanisms which can reduce the impact of locks on overall system performance, compared to PostgreSQL's central locking system.\n- **Simplified Debugging**: MongoDB's capability to store extended historical logging natively within its document structure simplifies tracing and debugging issues over time.\n\n### Positive Consequences\n* Improved performance and scalability to handle more concurrent queries without impacting cluster performance.\n* Easier to manage and adapt schema changes in a dynamic CI\/CD environment.\n* Enhanced tracking and auditing abilities, making it easier to roll back and understand state changes.\n* Simplified and more effective logging and debugging capabilities.\n\n### Negative Consequences\n* Initial migration effort required to move data from PostgreSQL to MongoDB.\n* Potential learning curve for the engineering team to adapt to MongoDB's data modeling and querying paradigms.\n* Adjustments needed in existing tools and interfaces that were tailored for PostgreSQL.\n\nBy leveraging MongoDB, we aim to overcome the limitations of our current system and build a more robust and flexible CI\/CD state storage mechanism.","GenTime":"2024-08-20 00:38:20"}
{"File Name":"agentframework\/0008-minify-build.md","Context":"## Context\\nAgentFramework is also use for Browser\\n","Decision":"We need minify build because package.json don't have a property called `minMain`\\n","tokens":11,"id":2359,"text":"## Context\\nAgentFramework is also use for Browser\\n\n\n##Decision\nWe need minify build because package.json don't have a property called `minMain`\\n","MatchedIds":[4271,1243,847,1240,3982],"Predicted":"## Decision\nWe will ensure that AgentFramework supports browser environments by leveraging tools and frameworks that facilitate JavaScript interoperability. Specifically, we will use [WebAssembly](https:\/\/webassembly.org\/) to compile critical components of AgentFramework to run efficiently in the browser. Additionally, we will utilize [TypeScript](https:\/\/www.typescriptlang.org\/) for better type management and to foster easier integration with front-end frameworks like [React](https:\/\/reactjs.org\/) or [Vue.js](https:\/\/vuejs.org\/).\n\n### Implementation Plan\n1. **WebAssembly Compilation**: Compile the core components of AgentFramework to WebAssembly to ensure high performance and maintainability in a browser environment.\n2. **TypeScript Integration**: Use TypeScript for client-side scripting to ensure type safety and to streamline the development process.\n3. **Frontend Integration**: Facilitate integration with popular frontend frameworks such as React or Vue.js for smooth agent interactions and UI updates.\n\nBy adopting these technologies, we can ensure that AgentFramework remains robust and efficient for both server and browser environments, providing a seamless experience across different platforms.","GenTime":"2024-08-20 00:38:26"}
{"File Name":"banking-cqrs-es-go\/0002-use-adr-tools-to-manage-the-adrs.md","Context":"## Context\\nA tool to make creating these ADRs helps reduce friction around creating documentation.\\n`adr-tools` is a simple cli tool that has been used in the past on previous projects to great effect (https:\/\/github.com\/npryce\/adr-tools)\\n","Decision":"Install `adr-tools` locally and use it to help quickly generate consistent ADRs\\n","tokens":56,"id":2914,"text":"## Context\\nA tool to make creating these ADRs helps reduce friction around creating documentation.\\n`adr-tools` is a simple cli tool that has been used in the past on previous projects to great effect (https:\/\/github.com\/npryce\/adr-tools)\\n\n\n##Decision\nInstall `adr-tools` locally and use it to help quickly generate consistent ADRs\\n","MatchedIds":[5033,1067,1614,1545,400],"Predicted":"## Decision\nWe will adopt `adr-tools` for managing and creating Architectural Decision Records (ADRs) due to its simplicity, proven effectiveness in past projects, and minimal friction it introduces in the documentation process.","GenTime":"2024-08-20 00:38:50"}
{"File Name":"tech-team\/0006-supported-node-and-npm-versions.md","Context":"## Context\\neLife has projects built in Node.\\neLife has projects that use the Node Package Manager (npm) in at least part of their own build process, even if the main technology of the project is not Node.\\nIn order to provide language version consistency across projects we need to get a consensus on which Node versions we are going to support.\\nIn order to provide dependency management consistency, we need a consensus on which npm versions to support.\\nStaying up to date with the major and minor versions of Node is important:\\n- to be able to use new features (nice to have)\\n- to keep working on a version that receives bug fixes and security updates (must have)\\n","Decision":"To use 8.x, the current Long Term Support (LTS) line.\\nIn or around January 2019 to review upgrading to the next LTS line before 8.x drops out of active LTS in April 2019 (see [node release schedule](https:\/\/github.com\/nodejs\/Release#release-schedule)).\\nTo use whichever npm version is bundled with the Node version we use.\\nWhen upgrading, we will make a concerted effort to upgrade all projects as part of the same piece of work.\\n","tokens":143,"id":939,"text":"## Context\\neLife has projects built in Node.\\neLife has projects that use the Node Package Manager (npm) in at least part of their own build process, even if the main technology of the project is not Node.\\nIn order to provide language version consistency across projects we need to get a consensus on which Node versions we are going to support.\\nIn order to provide dependency management consistency, we need a consensus on which npm versions to support.\\nStaying up to date with the major and minor versions of Node is important:\\n- to be able to use new features (nice to have)\\n- to keep working on a version that receives bug fixes and security updates (must have)\\n\n\n##Decision\nTo use 8.x, the current Long Term Support (LTS) line.\\nIn or around January 2019 to review upgrading to the next LTS line before 8.x drops out of active LTS in April 2019 (see [node release schedule](https:\/\/github.com\/nodejs\/Release#release-schedule)).\\nTo use whichever npm version is bundled with the Node version we use.\\nWhen upgrading, we will make a concerted effort to upgrade all projects as part of the same piece of work.\\n","MatchedIds":[3031,1202,1852,2924,3827],"Predicted":"## Decision\nWe will standardize on using the Long-Term Support (LTS) versions of Node.js for all eLife projects. Specifically, the \"Active LTS\" version will be the baseline supported version, as it ensures we are using a version that receives both bug fixes and security updates. As of this decision, the current Active LTS version is Node.js 18.x.\n\nAdditionally, for npm, we will align with the npm version that is bundled with the Active LTS version of Node.js. This ensures compatibility and makes it easier to manage our dependency management tools across various projects.\n\n### Implementation Steps:\n1. **Node.js Version Management:** All projects should use the Active LTS version of Node.js (currently 18.x). This version should be specified in the project's `.nvmrc` file to ensure consistency when using Node Version Manager (nvm).\n2. **npm Version Management:** The npm version bundled with the Active LTS version of Node.js should be used. This ensures compatibility and reduces dependency management overhead.\n3. **CI\/CD Pipeline Updates:** Update CI\/CD pipeline configurations to use the specified Node.js and npm versions. This should be done by explicitly specifying the Node.js version in the pipeline configuration files (e.g., `.travis.yml`, `circle.yml`, etc.).\n4. **Documentation:** Update project documentation to reflect the required Node.js and npm versions. This should be clearly stated in the README files and any contributing guidelines.\n\nBy adopting this decision, we ensure that all projects across eLife have consistent, secure, and up-to-date Node.js and npm versions, facilitating better maintainability and collaboration.","GenTime":"2024-08-20 00:38:59"}
{"File Name":"mlmc\/0003-use-rabbit-mq-for-eda.md","Context":"## Context and Problem Statement\\nDifferent services are going to push different kind of messages so there is a need in EDA architecture and some messaging server.\\n## Decision Drivers\\n* Easy to use\\n* Easy to learn\\n* Possibility to run from Docker\\n","Decision":"* Easy to use\\n* Easy to learn\\n* Possibility to run from Docker\\nDecided to use RabbitMQ since\\n* It provides exact functionality that is neded for the product (publish-subscribe event messaging)\\n* It is easy to learn and use and I already had some knowledge of how to use it\\n* It has official container on Docker Hub\\n* It seems to be popular and trending these days\\n### Positive Consequences\\n* I will be able to setup EDA messaging in the product.\\n### Negative Consequences\\n* n\/a\\n","tokens":53,"id":1575,"text":"## Context and Problem Statement\\nDifferent services are going to push different kind of messages so there is a need in EDA architecture and some messaging server.\\n## Decision Drivers\\n* Easy to use\\n* Easy to learn\\n* Possibility to run from Docker\\n\n\n##Decision\n* Easy to use\\n* Easy to learn\\n* Possibility to run from Docker\\nDecided to use RabbitMQ since\\n* It provides exact functionality that is neded for the product (publish-subscribe event messaging)\\n* It is easy to learn and use and I already had some knowledge of how to use it\\n* It has official container on Docker Hub\\n* It seems to be popular and trending these days\\n### Positive Consequences\\n* I will be able to setup EDA messaging in the product.\\n### Negative Consequences\\n* n\/a\\n","MatchedIds":[979,786,4694,2309,3488],"Predicted":"## Decision\nChosen option: **RabbitMQ**, because:\n- It is known for its ease of use and intuitive interface, making it easy to learn for new team members.\n- It has a wealth of documentation and community support, which facilitates the learning curve.\n- RabbitMQ can be easily run via Docker as there are official RabbitMQ Docker images available, simplifying deployment and management in containerized environments.\n- It supports a variety of messaging protocols such as AMQP, MQTT, and STOMP, providing flexibility to handle different kinds of messages.\n- RabbitMQ has robust tooling and monitoring capabilities which enhance operational management.","GenTime":"2024-08-20 00:39:02"}
{"File Name":"digital-paper-edit-firebase\/2020-03-12-integrating-with-new-pstt-module.md","Context":"## Context and Problem Statement\\nWe want to connect to Newslabs' shared STT service (named Newslabs PSTT).\\n","Decision":"Ashley is building a service that will have an API Gateway and an S3 bucket. Uploading to the S3 bucket will trigger the STT event. We will be uploading to a bucket, as the previous architecture, with limitations that will define how we will communicate with Newslabs' PSTT service.\\nThere will only be one shared bucket per environment for services using this service. The name of the bucket is \"newslabs-stt-media-to-transcribe\" and \"newslabs-stt-media-to-transcribe-test\".\\n### Limitations\\n- Newslabs pstt will only handle audio files (`mp3`, `mp4`, `wav`, `flac`)\\n- the client (dpe) need to ensure they upload audio not video\\n- do not use the eTag as the reference, use object key instead: `<service_name>\/<object_key>.<ext>`\\n- assume that the client is sending a unique object key\\n### Example\\n#### Upload\\nWhen uploading a file with Object Key: `280612.mp3`, the Object Key should be prepended with the service name: `dpe\/280612.mp3`\\n#### Status\\nThe endpoint for requesting the status of a transcription uses [this lambda](https:\/\/github.com\/bbc\/newslabs-stt\/tree\/master\/newslabs-stt-check-transcription), which returns the transcription status.\\nThere is an example response in the README.\\nMake a request to an API Gateway endpoint (please ask) with something like this in the request body:\\n```json\\n{\\n\"objectKey\": \"dpe\/uuid.ext\"\\n}\\n```\\n","tokens":28,"id":5243,"text":"## Context and Problem Statement\\nWe want to connect to Newslabs' shared STT service (named Newslabs PSTT).\\n\n\n##Decision\nAshley is building a service that will have an API Gateway and an S3 bucket. Uploading to the S3 bucket will trigger the STT event. We will be uploading to a bucket, as the previous architecture, with limitations that will define how we will communicate with Newslabs' PSTT service.\\nThere will only be one shared bucket per environment for services using this service. The name of the bucket is \"newslabs-stt-media-to-transcribe\" and \"newslabs-stt-media-to-transcribe-test\".\\n### Limitations\\n- Newslabs pstt will only handle audio files (`mp3`, `mp4`, `wav`, `flac`)\\n- the client (dpe) need to ensure they upload audio not video\\n- do not use the eTag as the reference, use object key instead: `<service_name>\/<object_key>.<ext>`\\n- assume that the client is sending a unique object key\\n### Example\\n#### Upload\\nWhen uploading a file with Object Key: `280612.mp3`, the Object Key should be prepended with the service name: `dpe\/280612.mp3`\\n#### Status\\nThe endpoint for requesting the status of a transcription uses [this lambda](https:\/\/github.com\/bbc\/newslabs-stt\/tree\/master\/newslabs-stt-check-transcription), which returns the transcription status.\\nThere is an example response in the README.\\nMake a request to an API Gateway endpoint (please ask) with something like this in the request body:\\n```json\\n{\\n\"objectKey\": \"dpe\/uuid.ext\"\\n}\\n```\\n","MatchedIds":[745,786,1180,771,4096],"Predicted":"## Decision\nWe have decided to connect to the Newslabs PSTT (shared STT service) using REST API endpoints over HTTPS to ensure secure communication. The integration will be implemented using JSON payloads for request and response formats. OAuth 2.0 will be used for authentication and authorization to manage access securely.\n\n### Positive Consequences\n* Secure communication via HTTPS.\n* Seamless integration using REST APIs with industry-standard JSON formats, ensuring interoperability.\n* Enhanced security through OAuth 2.0 for managing access controls.\n* Leverage the capabilities of Newslabs PSTT service without needing to develop an STT service in-house.\n\n### Negative Consequences\n* Dependency on the availability and performance of the Newslabs PSTT service.\n* Need for ongoing management of OAuth 2.0 tokens and access credentials.\n* Potential initial development effort to integrate and test connectivity with Newslabs PSTT APIs.","GenTime":"2024-08-20 00:39:17"}
{"File Name":"interlok\/0003-deprecate-dynamicservicelocator.md","Context":"## Context and Problem Statement\\nWith the standard document trading network, there is a move towards centralised processing such that companies need only define their \"mapping specification\" and key information, and everything is handled centrally.\\nTraditionally, this was done via DynamicServiceLocator with files located on the fileystem named in the form : `SRC-DST-MSG_TYPE.xml`; recently, this has been done in a dedicated application which is now deprecated.\\nThe way that DynamicServiceLocator works contains a lot of extraneous configuration that only have a single implementation; it was designed for extensibility, but it's over complicated in terms of XML and coupling. It needs to be simplified so that it's more understandable in the UI.\\n","Decision":"Deprecate DynamicServiceLocator, leave it available but marked as deprecated. Improve DynamicServiceExcutor so that it can be used instead.\\n### Enhancing DynamicServiceExecutor.\\nWhat this means is to implement additional ServiceExtractor implementations that allow the user to extract the _`_service-to-execute_ from an external location; currently the two supplied implementations simply use the _AdaptrisMessage_ object to extract the services.\\nThe additions required are :\\n* Extract from a URL such that you can configure `http:\/\/my.server.com\/%message{source}\/%message{destination}\/%message{messageType}.xml`. This would replace the existing DynamicServiceLocator functionality from _RemoteServiceStore_ and _LocalServiceStore_.\\n* Extract from Database -> such that you can configure something like `SELECT dynamicService FROM services WHERE src='%message{source}' AND dest='%message{destination}' AND msgType='%message{messageType}'`\\n* The above would be quite \"open\" to SQL injection style attacks; so people should probably use _JdbcDataQueryService_ and then handle it from metadata.\\n* Extract from a Cache -> similar to doing a RetrieveFromCache\\n* Extract it from metadata.\\n","tokens":146,"id":2342,"text":"## Context and Problem Statement\\nWith the standard document trading network, there is a move towards centralised processing such that companies need only define their \"mapping specification\" and key information, and everything is handled centrally.\\nTraditionally, this was done via DynamicServiceLocator with files located on the fileystem named in the form : `SRC-DST-MSG_TYPE.xml`; recently, this has been done in a dedicated application which is now deprecated.\\nThe way that DynamicServiceLocator works contains a lot of extraneous configuration that only have a single implementation; it was designed for extensibility, but it's over complicated in terms of XML and coupling. It needs to be simplified so that it's more understandable in the UI.\\n\n\n##Decision\nDeprecate DynamicServiceLocator, leave it available but marked as deprecated. Improve DynamicServiceExcutor so that it can be used instead.\\n### Enhancing DynamicServiceExecutor.\\nWhat this means is to implement additional ServiceExtractor implementations that allow the user to extract the _`_service-to-execute_ from an external location; currently the two supplied implementations simply use the _AdaptrisMessage_ object to extract the services.\\nThe additions required are :\\n* Extract from a URL such that you can configure `http:\/\/my.server.com\/%message{source}\/%message{destination}\/%message{messageType}.xml`. This would replace the existing DynamicServiceLocator functionality from _RemoteServiceStore_ and _LocalServiceStore_.\\n* Extract from Database -> such that you can configure something like `SELECT dynamicService FROM services WHERE src='%message{source}' AND dest='%message{destination}' AND msgType='%message{messageType}'`\\n* The above would be quite \"open\" to SQL injection style attacks; so people should probably use _JdbcDataQueryService_ and then handle it from metadata.\\n* Extract from a Cache -> similar to doing a RetrieveFromCache\\n* Extract it from metadata.\\n","MatchedIds":[786,4307,4694,4644,1545],"Predicted":"## Decision\n\nChosen option: \"Implement a simplified, centralized service for mapping specifications\", because:\n\n- Simplifying the configuration will make the system more maintainable and easier to understand.\n- Centralized processing reduces the need for individual companies to manage their own configurations, lowering operational overhead.\n- A centralized service can provide a unified user interface for managing mapping specifications, making it more user-friendly.\n- It eliminates the deprecated dedicated application and the complexity of the DynamicServiceLocator.\n\n### Positive Consequences\n* Increased maintainability and understandability of configuration files.\n* Reduction in operational overhead for client companies.\n* A more user-friendly interface for managing mapping specifications.\n* Elimination of the deprecated application, reducing technical debt.\n\n### Negative Consequences\n* Initial development effort required to create the new centralized service.\n* Possible migration challenges from the current system to the new centralized service.\n* Dependency on the centralized system for processing, which could become a single point of failure if not designed robustly.\n\n","GenTime":"2024-08-20 00:39:21"}
{"File Name":"platform\/2021-09-22-refactor-theme-inheritance.md","Context":"## Context\\nCurrently, the themes can only inherit config fields from the default Storefront theme.\\nAlso, this inheritance is only a snapshot by activation time of the theme - The configs are copied to the new theme and changes to the default theme config will not appear in the new theme without a re-activation.\\nThe different possibilities to inherit different parts of a theme, like scripts, templates and config, can also cause problems on later updates.\\n","Decision":"To take this points into account, we have decided to add a new inheritance key for the `configFields` in the `theme.json` which allow a theme to inherit its config from other themes in a given order:\\n```json\\n\"configInheritance\": [\\n\"@Storefront\",\\n\"@PreviousTheme\",\\n\"@MyDevelopmentTheme\"\\n],\\n```\\n<details>\\n<summary>Complete theme.json with part inheritances<\/summary>\\n```json\\n{\\n\"name\": \"MyDevelopmentTheme\",\\n\"author\": \"Shopware AG\",\\n\"views\": [\\n\"@Storefront\",\\n\"@Plugins\",\\n\"@MyDevelopmentTheme\"\\n],\\n\"style\": [\\n\"app\/storefront\/src\/scss\/overrides.scss\",\\n\"@Storefront\",\\n\"app\/storefront\/src\/scss\/base.scss\"\\n],\\n\"script\": [\\n\"@Storefront\",\\n\"app\/storefront\/dist\/storefront\/js\/my-development-theme.js\"\\n],\\n\"asset\": [\\n\"@Storefront\",\\n\"app\/storefront\/src\/assets\"\\n],\\n\"configInheritance\": [\\n\"@Storefront\",\\n\"@PreviousTheme\",\\n\"@MyDevelopmentTheme\"\\n],\\n\"config\": {\\n\"blocks\": {\\n\"exampleBlock\": {\\n\"label\": {\\n\"en-GB\": \"Example block\",\\n\"de-DE\": \"Beispiel Block\"\\n}\\n}\\n},\\n\"sections\": {\\n\"exampleSection\": {\\n\"label\": {\\n\"en-GB\": \"Example section\",\\n\"de-DE\": \"Beispiel Sektion\"\\n}\\n}\\n},\\n\"fields\": {\\n\"my-single-test-select-field\": {\\n\"editable\": false\\n},\\n\"my-single-select-field\": {\\n\"label\": {\\n\"en-GB\": \"Select a font size\",\\n\"de-DE\": \"W\u00e4hle ein Schriftgr\u00f6\u00dfe\"\\n},\\n\"type\": \"text\",\\n\"value\": \"24\",\\n\"custom\": {\\n\"componentName\": \"sw-single-select\",\\n\"options\": [\\n{\\n\"value\": \"16\",\\n\"label\": {\\n\"en-GB\": \"16px\",\\n\"de-DE\": \"16px\"\\n}\\n},\\n{\\n\"value\": \"20\",\\n\"label\": {\\n\"en-GB\": \"20px\",\\n\"de-DE\": \"20px\"\\n}\\n},\\n{\\n\"value\": \"24\",\\n\"label\": {\\n\"en-GB\": \"24px\",\\n\"de-DE\": \"24px\"\\n}\\n}\\n]\\n},\\n\"editable\": true,\\n\"block\": \"exampleBlock\",\\n\"section\": \"exampleSection\"\\n},\\n\"usps-positions\": {\\n\"label\":\\n{\\n\"en-GB\": \"Position\",\\n\"de-DE\": \"Position\"\\n},\\n\"scss\": true,\\n\"type\": \"text\",\\n\"value\": [\\n\"top\",\\n\"bottom\"\\n],\\n\"custom\": {\\n\"componentName\": \"sw-multi-select\",\\n\"options\": [\\n{\\n\"value\": \"bottom\",\\n\"label\":\\n{\\n\"en-GB\": \"bottom\",\\n\"de-DE\": \"unten\"\\n}\\n},\\n{\\n\"value\": \"top\",\\n\"label\":\\n{\\n\"en-GB\": \"top\",\\n\"de-DE\": \"oben\"\\n}\\n},\\n{\\n\"value\": \"middle\",\\n\"label\":\\n{\\n\"en-GB\": \"middle\",\\n\"de-DE\": \"mittel\"\\n}\\n}\\n]\\n},\\n\"editable\": true,\\n\"tab\": \"usps\",\\n\"block\": \"exampleBlock\",\\n\"section\": \"exampleSection\"\\n}\\n}\\n}\\n}\\n```\\n<\/details>\\n","tokens":90,"id":4498,"text":"## Context\\nCurrently, the themes can only inherit config fields from the default Storefront theme.\\nAlso, this inheritance is only a snapshot by activation time of the theme - The configs are copied to the new theme and changes to the default theme config will not appear in the new theme without a re-activation.\\nThe different possibilities to inherit different parts of a theme, like scripts, templates and config, can also cause problems on later updates.\\n\n\n##Decision\nTo take this points into account, we have decided to add a new inheritance key for the `configFields` in the `theme.json` which allow a theme to inherit its config from other themes in a given order:\\n```json\\n\"configInheritance\": [\\n\"@Storefront\",\\n\"@PreviousTheme\",\\n\"@MyDevelopmentTheme\"\\n],\\n```\\n<details>\\n<summary>Complete theme.json with part inheritances<\/summary>\\n```json\\n{\\n\"name\": \"MyDevelopmentTheme\",\\n\"author\": \"Shopware AG\",\\n\"views\": [\\n\"@Storefront\",\\n\"@Plugins\",\\n\"@MyDevelopmentTheme\"\\n],\\n\"style\": [\\n\"app\/storefront\/src\/scss\/overrides.scss\",\\n\"@Storefront\",\\n\"app\/storefront\/src\/scss\/base.scss\"\\n],\\n\"script\": [\\n\"@Storefront\",\\n\"app\/storefront\/dist\/storefront\/js\/my-development-theme.js\"\\n],\\n\"asset\": [\\n\"@Storefront\",\\n\"app\/storefront\/src\/assets\"\\n],\\n\"configInheritance\": [\\n\"@Storefront\",\\n\"@PreviousTheme\",\\n\"@MyDevelopmentTheme\"\\n],\\n\"config\": {\\n\"blocks\": {\\n\"exampleBlock\": {\\n\"label\": {\\n\"en-GB\": \"Example block\",\\n\"de-DE\": \"Beispiel Block\"\\n}\\n}\\n},\\n\"sections\": {\\n\"exampleSection\": {\\n\"label\": {\\n\"en-GB\": \"Example section\",\\n\"de-DE\": \"Beispiel Sektion\"\\n}\\n}\\n},\\n\"fields\": {\\n\"my-single-test-select-field\": {\\n\"editable\": false\\n},\\n\"my-single-select-field\": {\\n\"label\": {\\n\"en-GB\": \"Select a font size\",\\n\"de-DE\": \"W\u00e4hle ein Schriftgr\u00f6\u00dfe\"\\n},\\n\"type\": \"text\",\\n\"value\": \"24\",\\n\"custom\": {\\n\"componentName\": \"sw-single-select\",\\n\"options\": [\\n{\\n\"value\": \"16\",\\n\"label\": {\\n\"en-GB\": \"16px\",\\n\"de-DE\": \"16px\"\\n}\\n},\\n{\\n\"value\": \"20\",\\n\"label\": {\\n\"en-GB\": \"20px\",\\n\"de-DE\": \"20px\"\\n}\\n},\\n{\\n\"value\": \"24\",\\n\"label\": {\\n\"en-GB\": \"24px\",\\n\"de-DE\": \"24px\"\\n}\\n}\\n]\\n},\\n\"editable\": true,\\n\"block\": \"exampleBlock\",\\n\"section\": \"exampleSection\"\\n},\\n\"usps-positions\": {\\n\"label\":\\n{\\n\"en-GB\": \"Position\",\\n\"de-DE\": \"Position\"\\n},\\n\"scss\": true,\\n\"type\": \"text\",\\n\"value\": [\\n\"top\",\\n\"bottom\"\\n],\\n\"custom\": {\\n\"componentName\": \"sw-multi-select\",\\n\"options\": [\\n{\\n\"value\": \"bottom\",\\n\"label\":\\n{\\n\"en-GB\": \"bottom\",\\n\"de-DE\": \"unten\"\\n}\\n},\\n{\\n\"value\": \"top\",\\n\"label\":\\n{\\n\"en-GB\": \"top\",\\n\"de-DE\": \"oben\"\\n}\\n},\\n{\\n\"value\": \"middle\",\\n\"label\":\\n{\\n\"en-GB\": \"middle\",\\n\"de-DE\": \"mittel\"\\n}\\n}\\n]\\n},\\n\"editable\": true,\\n\"tab\": \"usps\",\\n\"block\": \"exampleBlock\",\\n\"section\": \"exampleSection\"\\n}\\n}\\n}\\n}\\n```\\n<\/details>\\n","MatchedIds":[2262,4508,5185,783,4500],"Predicted":"## Decision\nWe will implement a dynamic inheritance mechanism for themes to ensure that changes to the default Storefront theme are immediately reflected in child themes. This mechanism will allow themes to inherit config fields, scripts, templates, and other pertinent assets directly at runtime rather than at activation time.\n\n### Actions:\n1. **Dynamic Inheritance Implementation:**\n    - Refactor the theme inheritance model to allow real-time updates, ensuring that modifications to the default Storefront theme propagate automatically to child themes.\n    - Use a dependency injection mechanism to inject the parent theme configurations into child themes at runtime.\n\n2. **Unified Inheritance Paths:**\n    - Standardize the inheritance paths for scripts, templates, and configurations to avoid inconsistencies and ensure that all themes inherit the necessary parts from their parent themes.\n    \n3. **Backward Compatibility:**\n    - Maintain backward compatibility with existing themes by preserving the initial activation-time copy mechanism but marking it as deprecated. This will give developers time to transition to the new model.\n    - Provide detailed documentation and migration guides to help developers adapt their themes to the new dynamic inheritance system.\n\n4. **Extended Configuration Options:**\n    - Develop a user-friendly interface to define inheritance rules within the admin panel. This will allow users to specify which parts of the theme to inherit or override easily.\n    \n5. **Update Mechanism:**\n    - Implement a robust versioning and update mechanism to handle potential conflicts and issues during updates, ensuring seamless transitions and backward compatibility.\n\n6. **Testing and Validation:**\n    - Introduce unit and integration tests to validate the dynamic inheritance functionality and ensure stability during updates.\n    - Carry out thorough performance testing to confirm that the new dynamic inheritance approach does not negatively impact the system's overall performance.\n\nBy implementing this dynamic inheritance model, we ensure a more flexible and maintainable theme system that aligns better with real-time updates and reduces the potential for conflicts during updates.","GenTime":"2024-08-20 00:39:27"}
{"File Name":"caia\/0011-pin-mbtest-library-to-v2.5.1.md","Context":"## Context\\nOn March 22, 2021, it was discovered that the \"caia\" Jenkins builds were\\nfailing. All the failing tests were failing with the following error, related\\nto a \"get_actual_requests\" method call:\\n```\\nTypeError: 'generator' object is not subscriptable\\n```\\nThe \"caia\" build was last successful in Jenkins on October 7, 2020. No builds\\nwere performed again until March 22, 2021, as there was no development work\\nbeing done on the project.\\nBuilds were made on March 22, 2021 because of a move to the\\n\"GitHub organization\" pipeline in LIBITD-1880, which triggered rebuilds in all\\nexisting projects.\\nWhen the last successful build was made in October, the \"mbtest\" library\\n([https:\/\/github.com\/brunns\/mbtest](mbtest)) was at v2.5.1. In v2.5.2, the\\n\"src\/mbtest\/server.py\" file was modified, changing the \"get_actual_requests\"\\nmethod signature (see [this commit e398f2f1f32420](mbtest_commit)). from:\\n```\\ndef get_actual_requests(self) -> Mapping[int, JsonStructure]:\\n```\\nto\\n```\\ndef get_actual_requests(self) -> Iterable[Request]:\\n```\\nThe change from a Mapping to an Iterable is the cause of the error in the tests.\\n","Decision":"The simplest solution for the moment is to \"pin\" the version of the \"mbtest\"\\nlibrary to v2.5.1 in the \"setup.py\" file. This will preserve the current\\nbehavior, until further \"caia\" development warrants additional testing.\\n","tokens":315,"id":3507,"text":"## Context\\nOn March 22, 2021, it was discovered that the \"caia\" Jenkins builds were\\nfailing. All the failing tests were failing with the following error, related\\nto a \"get_actual_requests\" method call:\\n```\\nTypeError: 'generator' object is not subscriptable\\n```\\nThe \"caia\" build was last successful in Jenkins on October 7, 2020. No builds\\nwere performed again until March 22, 2021, as there was no development work\\nbeing done on the project.\\nBuilds were made on March 22, 2021 because of a move to the\\n\"GitHub organization\" pipeline in LIBITD-1880, which triggered rebuilds in all\\nexisting projects.\\nWhen the last successful build was made in October, the \"mbtest\" library\\n([https:\/\/github.com\/brunns\/mbtest](mbtest)) was at v2.5.1. In v2.5.2, the\\n\"src\/mbtest\/server.py\" file was modified, changing the \"get_actual_requests\"\\nmethod signature (see [this commit e398f2f1f32420](mbtest_commit)). from:\\n```\\ndef get_actual_requests(self) -> Mapping[int, JsonStructure]:\\n```\\nto\\n```\\ndef get_actual_requests(self) -> Iterable[Request]:\\n```\\nThe change from a Mapping to an Iterable is the cause of the error in the tests.\\n\n\n##Decision\nThe simplest solution for the moment is to \"pin\" the version of the \"mbtest\"\\nlibrary to v2.5.1 in the \"setup.py\" file. This will preserve the current\\nbehavior, until further \"caia\" development warrants additional testing.\\n","MatchedIds":[3504,351,1739,3361,1012],"Predicted":"## Decision\nWe will pin the \"mbtest\" library to version 2.5.1 to ensure compatibility with the existing codebase until we can refactor the affected code to handle the updated method signature in version 2.5.2 and above. This will allow the Jenkins builds to pass and provide stability in the short term.\n\nSteps:\n1. Modify the project's `requirements.txt` or equivalent dependency management file to pin \"mbtest\" to version 2.5.1:\n```\nmbtest==2.5.1\n```\n2. Investigate and refactor the areas of the code where the \"get_actual_requests\" method is used to be compatible with the new `Iterable[Request]` return type introduced in \"mbtest\" version 2.5.2.\n3. After refactoring, update the \"mbtest\" dependency to the latest version, ensuring all tests pass with the new implementation:\n```\nmbtest>=2.5.2\n```\n4. Remove the strict pinning once compatibility is ensured, allowing for future updates of the \"mbtest\" library.\n\nPros:\n* Immediate resolution for the current build failures.\n* Provides time to methodically refactor the codebase for the new library changes.\n\nCons:\n* Temporarily locks the project to an outdated version of the \"mbtest\" library.","GenTime":"2024-08-20 00:39:49"}
{"File Name":"godspeed-you-blocked-developer\/003. Serverless.md","Context":"## Context\\nServices become unique and fragile when you configure them by hand. I want to have a provisioning strategy before I actually start adding components, so I don't end up with behavior that's hard to reproduce.\\nI like the principles in the book Infrastructure as Code. It suggests avoiding making changes to services except in an automated, configured manner.\\nI've had some exposure to the infrastructure provisioning tool [Serverless](https:\/\/serverless.com), so I'd like to try using it on this project. Serverless abstracts AWS CloudFormation and similar tools from other cloud vendors.\\n","Decision":"I will use Serverless to describe and provision my AWS components.\\n","tokens":119,"id":4937,"text":"## Context\\nServices become unique and fragile when you configure them by hand. I want to have a provisioning strategy before I actually start adding components, so I don't end up with behavior that's hard to reproduce.\\nI like the principles in the book Infrastructure as Code. It suggests avoiding making changes to services except in an automated, configured manner.\\nI've had some exposure to the infrastructure provisioning tool [Serverless](https:\/\/serverless.com), so I'd like to try using it on this project. Serverless abstracts AWS CloudFormation and similar tools from other cloud vendors.\\n\n\n##Decision\nI will use Serverless to describe and provision my AWS components.\\n","MatchedIds":[2062,2997,1464,2409,2471],"Predicted":"## Decision\nWe will use the Serverless framework to provision and manage our infrastructure for this project. This will ensure that changes to services are automated and reproducible, adhering to the principles of Infrastructure as Code. The Serverless framework will abstract the underlying complexities of AWS CloudFormation and other cloud vendor tools, allowing for a streamlined, consistent approach to infrastructure management.","GenTime":"2024-08-20 00:39:52"}
{"File Name":"aws-sync-routes\/0005-uri.md","Context":"## Context\\nThe requested functionality was to synchronize a specified route (either add or replace) from the main route table to the custom route tables, triggered from a log event, which would have limited information available to construct the request.\\nThere isn't an obvious fit for this in the official REST API URI specifications.\\n","Decision":"The `\/vpcs\/{vpcId}\/route-tables\/{routeTableId}` URI will be used.\\n* `{vpcId}` is the VPC ID.\\n* `{routeTableId}` is the main route table ID\\n","tokens":64,"id":1050,"text":"## Context\\nThe requested functionality was to synchronize a specified route (either add or replace) from the main route table to the custom route tables, triggered from a log event, which would have limited information available to construct the request.\\nThere isn't an obvious fit for this in the official REST API URI specifications.\\n\n\n##Decision\nThe `\/vpcs\/{vpcId}\/route-tables\/{routeTableId}` URI will be used.\\n* `{vpcId}` is the VPC ID.\\n* `{routeTableId}` is the main route table ID\\n","MatchedIds":[3210,674,1210,779,4538],"Predicted":"## Decision\nTo accommodate the synchronization of specified routes from the main route table to custom route tables, we will define a specialized endpoint in the REST API which will handle these operations. The design is as follows:\n\n1. **Endpoint Definition**: Introduce a new endpoint `\/sync-route` which will handle both adding and replacing the routes.\n2. **HTTP Method**: Use the `POST` method since the operation involves a change in the state of the server.\n3. **Request Payload**: The request payload should include the necessary information to perform the synchronization:\n    ```json\n    {\n        \"action\": \"add\" | \"replace\",\n        \"main_route\": {\n            \"destination\": \"string\",\n            \"next_hop\": \"string\"\n        },\n        \"custom_route_table_ids\": [\"string\"]\n    }\n    ```\n4. **Content-Type**: The `Content-Type` header for the request should be set to `application\/json`.\n5. **Response**: The endpoint should return a JSON response indicating the success or failure of the operation for each custom route table.\n    ```json\n    {\n        \"success\": true,\n        \"details\": [\n            {\n                \"route_table_id\": \"string\",\n                \"result\": \"added\" | \"replaced\" | \"failed\",\n                \"message\": \"string\"\n            }\n        ]\n    }\n    ```\n6. **Error Handling**: Comprehensive error handling should be implemented to account for cases such as invalid route information, non-existent custom route table IDs, and other common issues.\n7. **Security**: Ensure that any necessary authentication and authorization mechanisms are in place to restrict access to this endpoint.\n\nBy adhering to this approach, we provide a straightforward and extensible method for synchronizing routes based on log events, while conforming to REST principles and ensuring the implementation remains clear and understandable.","GenTime":"2024-08-20 00:39:57"}
{"File Name":"php-docker-template\/0004-default-php-settings-memory-limit.md","Context":"## Context\\nThis set of Docker images are opinionated and meant to run within a Docker orchestrator, kubernetes for instance.\\nSince most (if not all) the orchestrators have resource management built-in there are certain PHP settings which can be tweaked to make use of them, if PHP has memory limits itself, it'll die as a fatal error, in which case the orchestrator would be unaware that it's actually a out of memory situation.\\n","Decision":"Set php ini configuration to have `memory_limit = -1`, this will affect both fpm and cli processes since it's added in the `default.ini` file of this repository.\\n","tokens":92,"id":5235,"text":"## Context\\nThis set of Docker images are opinionated and meant to run within a Docker orchestrator, kubernetes for instance.\\nSince most (if not all) the orchestrators have resource management built-in there are certain PHP settings which can be tweaked to make use of them, if PHP has memory limits itself, it'll die as a fatal error, in which case the orchestrator would be unaware that it's actually a out of memory situation.\\n\n\n##Decision\nSet php ini configuration to have `memory_limit = -1`, this will affect both fpm and cli processes since it's added in the `default.ini` file of this repository.\\n","MatchedIds":[74,1173,4096,3604,2968],"Predicted":"## Decision\nWe will configure PHP to rely on the resource limits set by the Docker orchestrator instead of setting PHP-specific memory limits. This will ensure that the orchestrator has full visibility into out of memory situations and can manage resources effectively. The following changes will be implemented:\n\n* Set `memory_limit` in PHP to `-1` (unlimited), allowing the orchestrator to enforce memory constraints.\n* Ensure CPU and memory limits are set at the container level within the orchestrator (Kubernetes) to prevent uncontrolled resource utilization.\n* Configure appropriate liveness and readiness probes in Kubernetes to detect and manage application health and availability.\n* Document these settings clearly for developers and system administrators to ensure they understand the resource management strategy and how to monitor and troubleshoot issues related to resource constraints.\n\nThis approach ensures seamless resource management and error handling by leveraging the orchestrator's capabilities.","GenTime":"2024-08-20 00:40:01"}
{"File Name":"paas-csls-splunk-broker\/ADR004-deploy-broker-as-lambda.md","Context":"## Context\\nWe need to deploy the Broker somewhere.\\nThe Broker implements the service broker API to generate per-application syslog\\ndrain URLs (Adapter URLs).\\nThe Adapter is written in Go.\\nThe Broker is written in Go.\\nThe Adapter runs as a lambda in AWS alongside the CSLS infrastructure.\\nWe have a pipeline to continuously build, test, deploy the Adapter to lambda.\\n","Decision":"We will deploy the Broker as an AWS Lambda\\n","tokens":77,"id":3295,"text":"## Context\\nWe need to deploy the Broker somewhere.\\nThe Broker implements the service broker API to generate per-application syslog\\ndrain URLs (Adapter URLs).\\nThe Adapter is written in Go.\\nThe Broker is written in Go.\\nThe Adapter runs as a lambda in AWS alongside the CSLS infrastructure.\\nWe have a pipeline to continuously build, test, deploy the Adapter to lambda.\\n\n\n##Decision\nWe will deploy the Broker as an AWS Lambda\\n","MatchedIds":[3291,4358,3290,979,786],"Predicted":"## Decision\nWe will deploy the Broker to AWS, leveraging AWS Lambda similar to the Adapter. This decision is based on the following considerations:\n\n1. **Consistency**: Deploying the Broker in the same environment (AWS Lambda) as the Adapter maintains consistency in deployment and management.\n   \n2. **Scalability**: AWS Lambda provides an auto-scaling mechanism, which will help handle varying loads without manual intervention.\n   \n3. **Cost Efficiency**: AWS Lambda's pay-as-you-go pricing model ensures cost efficiency by only charging for the actual compute time used, which can be beneficial for intermittent workloads.\n   \n4. **Operational Simplicity**: Given that the Adapter is already successfully deployed as a Lambda function, we can reuse the existing CI\/CD pipeline with minimal changes, reducing the operational complexity.\n   \n5. **Maintainability**: With both the Broker and Adapter written in Go and running in the same environment, it simplifies the maintenance, monitoring, and troubleshooting processes.\n\n### Implementation Steps\n- Update the existing CI\/CD pipeline to include the Broker.\n- Package the Broker as a Lambda function.\n- Deploy the Broker to AWS Lambda alongside the existing Adapter.\n- Update necessary configurations and permissions to ensure smooth communication between the Broker and Adapter within the CSLS infrastructure.\n\nBy following this strategy, we maintain a coherent infrastructure, leverage existing capabilities, and ensure our deployment is scalable, cost-effective, and manageable.","GenTime":"2024-08-20 00:41:00"}
{"File Name":"cena\/0014-adopt-commmand-query-separation.md","Context":"## Context\\n[Command-query separation](https:\/\/martinfowler.com\/bliki\/CommandQuerySeparation.html) states that every method should\\neither be a command that performs an action, or a query that returns data to the caller, but not both.\\nAdopting command-query separation makes a clear separation of methods that change state from those that don't. So we can\\nuse queries with much more confidence, and only be careful with commands orchestration.\\nCommands and queries terminology is already used in the `menu-generation` application.\\n","Decision":"Command-query separation will be enforced in the [core hexagon](.\/0003-adopt-hexagonal-architecture.md), especially in\\napplication services.\\n","tokens":113,"id":647,"text":"## Context\\n[Command-query separation](https:\/\/martinfowler.com\/bliki\/CommandQuerySeparation.html) states that every method should\\neither be a command that performs an action, or a query that returns data to the caller, but not both.\\nAdopting command-query separation makes a clear separation of methods that change state from those that don't. So we can\\nuse queries with much more confidence, and only be careful with commands orchestration.\\nCommands and queries terminology is already used in the `menu-generation` application.\\n\n\n##Decision\nCommand-query separation will be enforced in the [core hexagon](.\/0003-adopt-hexagonal-architecture.md), especially in\\napplication services.\\n","MatchedIds":[899,1792,3776,674,1031],"Predicted":"## Decision\nWe will fully adopt the Command-Query Separation (CQS) principle in the `menu-generation` application. By adhering to this principle, we will clearly distinguish between methods that change state (commands) and methods that return data (queries). This will not only enhance our confidence and safety in using queries\u2014knowing they won't alter any application state\u2014but will also ensure that command methods are orchestrated with the necessary caution and precision.\n\nBenefits:\n1. **Clear Separation of Concerns**: Enhances code readability and maintainability by ensuring methods have a single responsibility.\n2. **Predictability**: Queries are safe to call multiple times without side effects, which can make debugging and reasoning about code easier.\n3. **Enhanced Testing**: Makes unit testing simpler as we can test commands and queries independently.\n\nImplementation Strategy:\n1. **Command Handlers**: We'll create dedicated classes or methods for handling state-changing operations.\n2. **Query Handlers**: Queries will reside in their own classes or methods focused purely on retrieving data.\n3. **Consistency with Existing Terminology**: By continuing to use the existing \"commands\" and \"queries\" terminology within the `menu-generation` application, we ensure conceptual continuity and ease of adoption for current team members.\n\nThis approach aligns with the principles already in use and will bolster our design's robustness and maintainability.","GenTime":"2024-08-20 00:41:05"}
{"File Name":"gatemint-sdk\/adr-004-split-denomination-keys.md","Context":"## Context\\nWith permissionless IBC, anyone will be able to send arbitrary denominations to any other account. Currently, all non-zero balances are stored along with the account in an `sdk.Coins` struct, which creates a potential denial-of-service concern, as too many denominations will become expensive to load & store each time the account is modified. See issues [5467](https:\/\/github.com\/cosmos\/cosmos-sdk\/issues\/5467) and [4982](https:\/\/github.com\/cosmos\/cosmos-sdk\/issues\/4982) for additional context.\\nSimply rejecting incoming deposits after a denomination count limit doesn't work, since it opens up a griefing vector: someone could send a user lots of nonsensical coins over IBC, and then prevent the user from receiving real denominations (such as staking rewards).\\n","Decision":"Balances shall be stored per-account & per-denomination under a denomination- and account-unique key, thus enabling O(1) read & write access to the balance of a particular account in a particular denomination.\\n### Account interface (x\/auth)\\n`GetCoins()` and `SetCoins()` will be removed from the account interface, since coin balances will\\nnow be stored in & managed by the bank module.\\nThe vesting account interface will replace `SpendableCoins` in favor of `LockedCoins` which does\\nnot require the account balance anymore. In addition, `TrackDelegation()`  will now accept the\\naccount balance of all tokens denominated in the vesting balance instead of loading the entire\\naccount balance.\\nVesting accounts will continue to store original vesting, delegated free, and delegated\\nvesting coins (which is safe since these cannot contain arbitrary denominations).\\n### Bank keeper (x\/bank)\\nThe following APIs will be added to the `x\/bank` keeper:\\n- `GetAllBalances(ctx Context, addr AccAddress) Coins`\\n- `GetBalance(ctx Context, addr AccAddress, denom string) Coin`\\n- `SetBalance(ctx Context, addr AccAddress, coin Coin)`\\n- `LockedCoins(ctx Context, addr AccAddress) Coins`\\n- `SpendableCoins(ctx Context, addr AccAddress) Coins`\\nAdditional APIs may be added to facilitate iteration and auxiliary functionality not essential to\\ncore functionality or persistence.\\nBalances will be stored first by the address, then by the denomination (the reverse is also possible,\\nbut retrieval of all balances for a single account is presumed to be more frequent):\\n```golang\\nvar BalancesPrefix = []byte(\"balances\")\\nfunc (k Keeper) SetBalance(ctx Context, addr AccAddress, balance Coin) error {\\nif !balance.IsValid() {\\nreturn err\\n}\\nstore := ctx.KVStore(k.storeKey)\\nbalancesStore := prefix.NewStore(store, BalancesPrefix)\\naccountStore := prefix.NewStore(balancesStore, addr.Bytes())\\nbz := Marshal(balance)\\naccountStore.Set([]byte(balance.Denom), bz)\\nreturn nil\\n}\\n```\\nThis will result in the balances being indexed by the byte representation of\\n`balances\/{address}\/{denom}`.\\n`DelegateCoins()` and `UndelegateCoins()` will be altered to only load each individual\\naccount balance by denomination found in the (un)delegation amount. As a result,\\nany mutations to the account balance by will made by denomination.\\n`SubtractCoins()` and `AddCoins()` will be altered to read & write the balances\\ndirectly instead of calling `GetCoins()` \/ `SetCoins()` (which no longer exist).\\n`trackDelegation()` and `trackUndelegation()` will be altered to no longer update\\naccount balances.\\nExternal APIs will need to scan all balances under an account to retain backwards-compatibility. It\\nis advised that these APIs use `GetBalance` and `SetBalance` instead of `GetAllBalances` when\\npossible as to not load the entire account balance.\\n### Supply module\\nThe supply module, in order to implement the total supply invariant, will now need\\nto scan all accounts & call `GetAllBalances` using the `x\/bank` Keeper, then sum\\nthe balances and check that they match the expected total supply.\\n","tokens":175,"id":33,"text":"## Context\\nWith permissionless IBC, anyone will be able to send arbitrary denominations to any other account. Currently, all non-zero balances are stored along with the account in an `sdk.Coins` struct, which creates a potential denial-of-service concern, as too many denominations will become expensive to load & store each time the account is modified. See issues [5467](https:\/\/github.com\/cosmos\/cosmos-sdk\/issues\/5467) and [4982](https:\/\/github.com\/cosmos\/cosmos-sdk\/issues\/4982) for additional context.\\nSimply rejecting incoming deposits after a denomination count limit doesn't work, since it opens up a griefing vector: someone could send a user lots of nonsensical coins over IBC, and then prevent the user from receiving real denominations (such as staking rewards).\\n\n\n##Decision\nBalances shall be stored per-account & per-denomination under a denomination- and account-unique key, thus enabling O(1) read & write access to the balance of a particular account in a particular denomination.\\n### Account interface (x\/auth)\\n`GetCoins()` and `SetCoins()` will be removed from the account interface, since coin balances will\\nnow be stored in & managed by the bank module.\\nThe vesting account interface will replace `SpendableCoins` in favor of `LockedCoins` which does\\nnot require the account balance anymore. In addition, `TrackDelegation()`  will now accept the\\naccount balance of all tokens denominated in the vesting balance instead of loading the entire\\naccount balance.\\nVesting accounts will continue to store original vesting, delegated free, and delegated\\nvesting coins (which is safe since these cannot contain arbitrary denominations).\\n### Bank keeper (x\/bank)\\nThe following APIs will be added to the `x\/bank` keeper:\\n- `GetAllBalances(ctx Context, addr AccAddress) Coins`\\n- `GetBalance(ctx Context, addr AccAddress, denom string) Coin`\\n- `SetBalance(ctx Context, addr AccAddress, coin Coin)`\\n- `LockedCoins(ctx Context, addr AccAddress) Coins`\\n- `SpendableCoins(ctx Context, addr AccAddress) Coins`\\nAdditional APIs may be added to facilitate iteration and auxiliary functionality not essential to\\ncore functionality or persistence.\\nBalances will be stored first by the address, then by the denomination (the reverse is also possible,\\nbut retrieval of all balances for a single account is presumed to be more frequent):\\n```golang\\nvar BalancesPrefix = []byte(\"balances\")\\nfunc (k Keeper) SetBalance(ctx Context, addr AccAddress, balance Coin) error {\\nif !balance.IsValid() {\\nreturn err\\n}\\nstore := ctx.KVStore(k.storeKey)\\nbalancesStore := prefix.NewStore(store, BalancesPrefix)\\naccountStore := prefix.NewStore(balancesStore, addr.Bytes())\\nbz := Marshal(balance)\\naccountStore.Set([]byte(balance.Denom), bz)\\nreturn nil\\n}\\n```\\nThis will result in the balances being indexed by the byte representation of\\n`balances\/{address}\/{denom}`.\\n`DelegateCoins()` and `UndelegateCoins()` will be altered to only load each individual\\naccount balance by denomination found in the (un)delegation amount. As a result,\\nany mutations to the account balance by will made by denomination.\\n`SubtractCoins()` and `AddCoins()` will be altered to read & write the balances\\ndirectly instead of calling `GetCoins()` \/ `SetCoins()` (which no longer exist).\\n`trackDelegation()` and `trackUndelegation()` will be altered to no longer update\\naccount balances.\\nExternal APIs will need to scan all balances under an account to retain backwards-compatibility. It\\nis advised that these APIs use `GetBalance` and `SetBalance` instead of `GetAllBalances` when\\npossible as to not load the entire account balance.\\n### Supply module\\nThe supply module, in order to implement the total supply invariant, will now need\\nto scan all accounts & call `GetAllBalances` using the `x\/bank` Keeper, then sum\\nthe balances and check that they match the expected total supply.\\n","MatchedIds":[812,3764,839,827,28],"Predicted":"## Decision\n\nWe will introduce an enhanced `Denom Metadata` system and a new module `denom-limits` to tackle the issue of potential denial-of-service due to excessive denomination entries within user accounts, while preventing the griefing vector that simple deposit rejections might introduce.\n\n### Denom Metadata System\n\n1. **Create Denom Metadata Records**:\n    - Introduce a `DenomMetadata` structure that will contain detailed metadata for each denomination, uniquely identified. This structure includes:\n      ```protobuf\n      message DenomMetadata {\n        string denom_id = 1; \/\/ Unique Denomination Identifier\n        string description = 2; \/\/ Description of the Denomination\n        string display_name = 3; \/\/ Name to be displayed in UIs\n        repeated DenomUnit unit_info = 4; \/\/ List of units and conversions\n      }\n      message DenomUnit {\n        string denom_unit = 1; \/\/ E.g., micro, milli, base\n        uint32 exponent = 2; \/\/ Conversion factor from base unit\n      }\n      ```\n\n2. **Denom Metadata Registration**:\n    - Add a registry for authorized denominations and their metadata. Only denominations registered within this system can be stored and used by accounts (`storeValidDenoms`).\n\n3. **Default Denoms**:\n    - Define a set of \u2018default\u2019 denominations (e.g., ATOM, UATOM) considered as core to the ecosystem and not subject to the limit.\n\n### denom-limits Module\n\n1. **Storage Structure**:\n    - Implement a new module `denom-limits` that maintains an indexed record of unique denominated balances associated with user accounts for fast lookup and efficient denomination counting.\n\n2. **Configurable Denomination Storage Limits**:\n    - Allow chain administrators to set configurable limits on the number of active denominations an account can hold (e.g., `maxDenomCountPerAccount`).\n\n3. **Graceful Handling of Overflows**:\n    - When a denomination limit is exceeded, all excess denominations are redirected to a holding or escrow account, accessible only by the user once they have managed their active denominations below the limit.\n\n### Processes and Example Scenarios\n\n#### Incoming IBC Transfer Handling\n\n1. **Check Denoms**:\n    - On incoming IBC transfer, the system checks if the denomination is registered in `DenomMetadata`.\n    - If the user's active denom count exceeds their preset limit:\n      - The transfer does not fail but excess denominations are allocated to an escrow\/holding account linked to the user.\n\n2. **User Interface Notifications**:\n    - Users receive notifications (transaction receipts, in-wallet alerts) about excessive denominations being held in escrow.\n    - Interfaces guiding them through the process of managing their active denoms.\n\n#### Edge Case Scenarios\n\n1. **Griefing Attempts**:\n    - If a bad actor attempts to spam an account with arbitrary denominations, after reaching the configured limit, new denominations are recorded in escrow, preventing direct DOS but maintaining a clear and reclaimable ledger without rejecting legitimate transactions.\n\n2. **Denom Cleanup Mechanism**:\n    - Implement periodic cleanup and denomination count reconciliation processes, ensuring the valid state of user accounts and associated meta-records.\n\n3. **Expanding Denom Limits**:\n    - Allow certain trust levels\/accounts (e.g., validators, staking pools) higher limits to accommodate more complexity.\n  \nBy implementing a finer-grained and smarter approach combining metadata for denominations with a dedicated module to manage limits and excess denominations, we create a robust system resistant to DOS and griefing attacks while maintaining user convenience and operational efficiency. Further optimization and performance adjustments will be tested and iterated on in integration stages.","GenTime":"2024-08-20 00:41:18"}
{"File Name":"jabref\/0002-use-slf4j-for-logging.md","Context":"## Context and Problem Statement\\nUp to version 4.1 JabRef uses apache-commons-logging 1.2 for logging errors and messages. However, this is not compatible with java 9 and is superseded by log4j.\\n## Decision Drivers\\n* SLF4J provides a fa\u00e7ade for several logging frameworks, including log4j and supports already java 9\\n* Log4j is already defined as dependency and slf4j has already been required by a third party dependency\\n","Decision":"* SLF4J provides a fa\u00e7ade for several logging frameworks, including log4j and supports already java 9\\n* Log4j is already defined as dependency and slf4j has already been required by a third party dependency\\nChosen option: \"SLF4J with Log4j2 binding\", because comes out best \\(see below\\).\\n","tokens":104,"id":4742,"text":"## Context and Problem Statement\\nUp to version 4.1 JabRef uses apache-commons-logging 1.2 for logging errors and messages. However, this is not compatible with java 9 and is superseded by log4j.\\n## Decision Drivers\\n* SLF4J provides a fa\u00e7ade for several logging frameworks, including log4j and supports already java 9\\n* Log4j is already defined as dependency and slf4j has already been required by a third party dependency\\n\n\n##Decision\n* SLF4J provides a fa\u00e7ade for several logging frameworks, including log4j and supports already java 9\\n* Log4j is already defined as dependency and slf4j has already been required by a third party dependency\\nChosen option: \"SLF4J with Log4j2 binding\", because comes out best \\(see below\\).\\n","MatchedIds":[4695,1065,1035,159,4748],"Predicted":"## Decision\nWe will replace the usage of apache-commons-logging with SLF4J, and configure SLF4J to use Log4j as the underlying logging framework.\n\n### Reasons\n* SLF4J acts as a fa\u00e7ade for various logging frameworks, offering flexibility for future changes.\n* SLF4J and Log4j are both compatible with Java 9.\n* Log4j is already included as a dependency, minimizing the effort required to switch.\n* SLF4J is already required by a third-party dependency, so unifying the logging framework will reduce redundancy and potential conflicts.\n\n### Positive Consequences\n* Compatibility with Java 9 and future versions.\n* Increased flexibility in case we need to switch logging frameworks again in the future.\n* Reduced redundancy and potential conflicts in dependencies.\n\n### Negative Consequences\n* Initial effort needed to replace apache-commons-logging with SLF4J in the codebase.\n\n### Migration Steps\n1. Add SLF4J dependencies to the project.\n2. Replace `commons-logging` dependencies with SLF4J.\n3. Update the logging configuration to ensure SLF4J uses Log4j.\n4. Test the log output to verify that the transition was successful.","GenTime":"2024-08-20 00:42:04"}
{"File Name":"adr-tools\/0004-markdown-format.md","Context":"## Context\\nThe decision records must be stored in a plain text format:\\n* This works well with version control systems.\\n* It allows the tool to modify the status of records and insert\\nhyperlinks when one decision supercedes another.\\n* Decisions can be read in the terminal, IDE, version control\\nbrowser, etc.\\nPeople will want to use some formatting: lists, code examples,\\nand so on.\\nPeople will want to view the decision records in a more readable\\nformat than plain text, and maybe print them out.\\n","Decision":"browser, etc.\\nPeople will want to use some formatting: lists, code examples,\\nand so on.\\nPeople will want to view the decision records in a more readable\\nformat than plain text, and maybe print them out.\\nRecord architecture decisions in [Markdown format](https:\/\/daringfireball.net\/projects\/markdown\/).\\nDecisions will be formatted nicely and hyperlinked by the\\nbrowsers of project hosting sites like GitHub and Bitbucket.\\nTools like [Pandoc](http:\/\/pandoc.org\/) can be used to convert\\nthe decision records into HTML or PDF.\\n","tokens":114,"id":3568,"text":"## Context\\nThe decision records must be stored in a plain text format:\\n* This works well with version control systems.\\n* It allows the tool to modify the status of records and insert\\nhyperlinks when one decision supercedes another.\\n* Decisions can be read in the terminal, IDE, version control\\nbrowser, etc.\\nPeople will want to use some formatting: lists, code examples,\\nand so on.\\nPeople will want to view the decision records in a more readable\\nformat than plain text, and maybe print them out.\\n\n\n##Decision\nbrowser, etc.\\nPeople will want to use some formatting: lists, code examples,\\nand so on.\\nPeople will want to view the decision records in a more readable\\nformat than plain text, and maybe print them out.\\nRecord architecture decisions in [Markdown format](https:\/\/daringfireball.net\/projects\/markdown\/).\\nDecisions will be formatted nicely and hyperlinked by the\\nbrowsers of project hosting sites like GitHub and Bitbucket.\\nTools like [Pandoc](http:\/\/pandoc.org\/) can be used to convert\\nthe decision records into HTML or PDF.\\n","MatchedIds":[160,2261,4361,3521,4428],"Predicted":"## Decision\nWe will use Markdown for storing the decision records.\n\n### Rationale\nMarkdown meets the requirement of plain text format, which works well with version control systems and enables easy modification, diffing, and hyperlink insertion. It also supports the necessary formatting for lists, code examples, and other text enhancements. Markdown can be rendered to more readable formats (e.g., HTML, PDF) for better viewing and printing options.\n\n### Positive Consequences\n* **Version Control Friendly**: Markdown files are plain text, ensuring smooth integration with version control systems.\n* **Readable**: Markdown is easy to read in its raw form and can be rendered to more readable formats.\n* **Flexible Formatting**: Supports lists, code examples, and other formatting needed for comprehensive documentation.\n* **Tool Compatibility**: Can be viewed and edited in terminals, IDEs, and version control browsers without issues.\n\n### Negative Consequences\n* **Learning Curve**: Team members need to be familiar with Markdown syntax, but this can be mitigated through training.\n* **Rendering**: For viewing in more readable formats, additional tools or plugins might be needed to render Markdown to HTML, PDF, etc.","GenTime":"2024-08-20 00:42:43"}
{"File Name":"buy-for-your-school\/0020-use-accessible-autocomplete-for-autocomplete-fields.md","Context":"## Context\\nIt is necessary to provide autocomplete functionality to make certain fields quicker to enter by suggesting potential results to the user.\\n","Decision":"We will use [accessible-autocomplete](https:\/\/github.com\/alphagov\/accessible-autocomplete) to provide the autocomplete capability in our pages.\\nThis package has been chosen because accessibility has been carefully considered when developing the package.\\nAlso it is designed to be used with `govuk` form styles so it will be in keeping with other form fields\\nand not be jarring to the user.\\n","tokens":26,"id":1255,"text":"## Context\\nIt is necessary to provide autocomplete functionality to make certain fields quicker to enter by suggesting potential results to the user.\\n\n\n##Decision\nWe will use [accessible-autocomplete](https:\/\/github.com\/alphagov\/accessible-autocomplete) to provide the autocomplete capability in our pages.\\nThis package has been chosen because accessibility has been carefully considered when developing the package.\\nAlso it is designed to be used with `govuk` form styles so it will be in keeping with other form fields\\nand not be jarring to the user.\\n","MatchedIds":[2513,847,3718,2488,1240],"Predicted":"## Decision\nTo implement autocomplete functionality, we will integrate a well-supported autocomplete library into our frontend framework. After evaluating several options, we have decided to use [Algolia Autocomplete](https:\/\/www.algolia.com\/products\/instantsearch\/autocomplete\/) for the following reasons:\n\n1. **Enhanced User Experience:** Algolia Autocomplete provides a fast and responsive experience, delivering suggestions as the user types.\n2. **Ease of Integration:** It offers easy integration with popular frontend frameworks like React, which aligns with our existing tech stack.\n3. **Rich Feature Set:** It not only supports basic autocomplete but also features like highlighting, custom templates, and multi-source autocompletion.\n4. **Scalability:** Algolia is designed to handle large datasets efficiently, ensuring that our autocomplete functionality will scale as our user base grows.\n\nConsidering these benefits, integrating Algolia Autocomplete will help us to significantly improve the user experience for form entry in our application.","GenTime":"2024-08-20 00:43:06"}
{"File Name":"galaxy-zoo-touch-table\/adr-2.md","Context":"### Context\\nThe touch table app should support multi touch while also being robust enough to handle various UI demands. Although we initially considered a 3D model of the universe in the center of the table, that approach was dropped as it didn't help accomplish our learning goals. Given that we use a Windows device for the touch table, we must choose a technology that can run on such a machine.\\n### Decision\\nAlthough considering Unity early in the planning phases, we dropped that consideration due to the learning curve of C++. Unity has also dropped support of Javascript, a language most of our team is familiar with. The transition from Javascript to C# seemed less jarring, and we have more resources and shared knowledge in C#. Initial work on the table was also done with the help of Florian Block, who designed the [DeepTree](https:\/\/lifeonearth.seas.harvard.edu\/learning-activities\/deeptree\/), another app built in WPF.\\n### Status\\nAccepted\\n### Consequences\\nFortunately, documentation for C# is quite strong, and the language allows enough flexibility to fit the needs of the table. The .NET community is also very active, and it is easy to find answers to obstacles we would encounter during development. However, WPF seems to be losing popularity, and many of the articles concerning WPF were written a decade ago, although some Stack Overflow posts are only a year or two old.\\n_In Retrospect_: Although I'm concerned about the longevity of WPF, the subsystem fits our needs for this project. It's also helpful to have the [Panoptes .NET client](https:\/\/github.com\/zooniverse\/panoptes-net-client) available for future C# projects. However, it is discerning to see many frameworks and packages used by WPF becoming deprecated (Silverlight, Microsoft Surface SDK).\\nIn the future, I think it would be worthwhile to explore new technologies; however, WPF is a suitable choice for most of our needs.\\n","Decision":"Although considering Unity early in the planning phases, we dropped that consideration due to the learning curve of C++. Unity has also dropped support of Javascript, a language most of our team is familiar with. The transition from Javascript to C# seemed less jarring, and we have more resources and shared knowledge in C#. Initial work on the table was also done with the help of Florian Block, who designed the [DeepTree](https:\/\/lifeonearth.seas.harvard.edu\/learning-activities\/deeptree\/), another app built in WPF.\\n### Status\\nAccepted\\n### Consequences\\nFortunately, documentation for C# is quite strong, and the language allows enough flexibility to fit the needs of the table. The .NET community is also very active, and it is easy to find answers to obstacles we would encounter during development. However, WPF seems to be losing popularity, and many of the articles concerning WPF were written a decade ago, although some Stack Overflow posts are only a year or two old.\\n_In Retrospect_: Although I'm concerned about the longevity of WPF, the subsystem fits our needs for this project. It's also helpful to have the [Panoptes .NET client](https:\/\/github.com\/zooniverse\/panoptes-net-client) available for future C# projects. However, it is discerning to see many frameworks and packages used by WPF becoming deprecated (Silverlight, Microsoft Surface SDK).\\nIn the future, I think it would be worthwhile to explore new technologies; however, WPF is a suitable choice for most of our needs.\\n","tokens":409,"id":5076,"text":"### Context\\nThe touch table app should support multi touch while also being robust enough to handle various UI demands. Although we initially considered a 3D model of the universe in the center of the table, that approach was dropped as it didn't help accomplish our learning goals. Given that we use a Windows device for the touch table, we must choose a technology that can run on such a machine.\\n### Decision\\nAlthough considering Unity early in the planning phases, we dropped that consideration due to the learning curve of C++. Unity has also dropped support of Javascript, a language most of our team is familiar with. The transition from Javascript to C# seemed less jarring, and we have more resources and shared knowledge in C#. Initial work on the table was also done with the help of Florian Block, who designed the [DeepTree](https:\/\/lifeonearth.seas.harvard.edu\/learning-activities\/deeptree\/), another app built in WPF.\\n### Status\\nAccepted\\n### Consequences\\nFortunately, documentation for C# is quite strong, and the language allows enough flexibility to fit the needs of the table. The .NET community is also very active, and it is easy to find answers to obstacles we would encounter during development. However, WPF seems to be losing popularity, and many of the articles concerning WPF were written a decade ago, although some Stack Overflow posts are only a year or two old.\\n_In Retrospect_: Although I'm concerned about the longevity of WPF, the subsystem fits our needs for this project. It's also helpful to have the [Panoptes .NET client](https:\/\/github.com\/zooniverse\/panoptes-net-client) available for future C# projects. However, it is discerning to see many frameworks and packages used by WPF becoming deprecated (Silverlight, Microsoft Surface SDK).\\nIn the future, I think it would be worthwhile to explore new technologies; however, WPF is a suitable choice for most of our needs.\\n\n\n##Decision\nAlthough considering Unity early in the planning phases, we dropped that consideration due to the learning curve of C++. Unity has also dropped support of Javascript, a language most of our team is familiar with. The transition from Javascript to C# seemed less jarring, and we have more resources and shared knowledge in C#. Initial work on the table was also done with the help of Florian Block, who designed the [DeepTree](https:\/\/lifeonearth.seas.harvard.edu\/learning-activities\/deeptree\/), another app built in WPF.\\n### Status\\nAccepted\\n### Consequences\\nFortunately, documentation for C# is quite strong, and the language allows enough flexibility to fit the needs of the table. The .NET community is also very active, and it is easy to find answers to obstacles we would encounter during development. However, WPF seems to be losing popularity, and many of the articles concerning WPF were written a decade ago, although some Stack Overflow posts are only a year or two old.\\n_In Retrospect_: Although I'm concerned about the longevity of WPF, the subsystem fits our needs for this project. It's also helpful to have the [Panoptes .NET client](https:\/\/github.com\/zooniverse\/panoptes-net-client) available for future C# projects. However, it is discerning to see many frameworks and packages used by WPF becoming deprecated (Silverlight, Microsoft Surface SDK).\\nIn the future, I think it would be worthwhile to explore new technologies; however, WPF is a suitable choice for most of our needs.\\n","MatchedIds":[3718,1659,1400,1660,1000],"Predicted":"### Context\nThe touch table app should support multi-touch while also being robust enough to handle various UI demands. Although we initially considered a 3D model of the universe in the center of the table, that approach was dropped as it didn't help accomplish our learning goals. Given that we use a Windows device for the touch table, we must choose a technology that can run on such a machine.\n\n### Decision\nChosen option: WPF (Windows Presentation Foundation) with C#, because:\n- The transition from JavaScript to C# is less jarring for our team, given our existing expertise and resources.\n- C# has strong documentation and an active .NET community, which makes problem-solving easier.\n- Initial work on the table was done with the help of Florian Block, who designed another successful app in WPF, DeepTree.\n\n### Status\nAccepted\n\n### Consequences\nFortunately, the documentation for C# is quite strong, and the language allows enough flexibility to fit the needs of the table. The .NET community is also very active, making it easy to find answers to potential obstacles during development. However, WPF is losing popularity, and much of the available documentation is dated, although some relevant posts are more recent.\n\n*In Retrospect:* While there are concerns about the longevity of WPF, it fits our project needs well. The availability of the Panoptes .NET client can be beneficial for future C# projects. However, the deprecation of several frameworks and packages used by WPF (e.g., Silverlight, Microsoft Surface SDK) is concerning. \n\nIn the future, exploring new technologies could be worthwhile, but WPF remains a suitable choice for the current project.","GenTime":"2024-08-20 00:43:12"}
{"File Name":"simple-server\/011-rollup-summary-tables-for-dashboards.md","Context":"## Context\\nEarlier we went with a [materialized view approach](https:\/\/github.com\/simpledotorg\/simple-server\/blob\/master\/doc\/arch\/008-materialized-views-for-dashboards.md) for dashboard reporting data. This approach has worked out well for the current reports but has limitations in terms of scaling and complexity.\\nWe currently have a need for a new set of dashboard reports that rely on similar sets of data, and anticipate more needs for summary data for monthly and quarterly reports. We'd like to try to find a simpler, more stable, and more scalable approach to making that data easily accessible.\\n","Decision":"We will create rollup summary tables for to capture the \"most recent blood pressure per patient per period\". These rollup tables will be updated in real-time as blood pressures are captured, removing the need for recurring batch jobs. We will denormalize as much data as we need to to make reporting queries simple and cheap to execute, to prevent the need for any complicated SQL queries or additional data layer caching.\\nOnce the current period has completed, we will treat the data in these rollup tables for that period as immutable. This will let us and our users have more confidence in reports from past periods, as they will not change due to patients moving to different facilities or changes in how we calculate things.\\nWe will focus on the District controlled blood pressure trend report as a first test case for this sort of approach, as it is a key report and one we'd like to gather feedback on as quickly as possible.\\n","tokens":123,"id":1701,"text":"## Context\\nEarlier we went with a [materialized view approach](https:\/\/github.com\/simpledotorg\/simple-server\/blob\/master\/doc\/arch\/008-materialized-views-for-dashboards.md) for dashboard reporting data. This approach has worked out well for the current reports but has limitations in terms of scaling and complexity.\\nWe currently have a need for a new set of dashboard reports that rely on similar sets of data, and anticipate more needs for summary data for monthly and quarterly reports. We'd like to try to find a simpler, more stable, and more scalable approach to making that data easily accessible.\\n\n\n##Decision\nWe will create rollup summary tables for to capture the \"most recent blood pressure per patient per period\". These rollup tables will be updated in real-time as blood pressures are captured, removing the need for recurring batch jobs. We will denormalize as much data as we need to to make reporting queries simple and cheap to execute, to prevent the need for any complicated SQL queries or additional data layer caching.\\nOnce the current period has completed, we will treat the data in these rollup tables for that period as immutable. This will let us and our users have more confidence in reports from past periods, as they will not change due to patients moving to different facilities or changes in how we calculate things.\\nWe will focus on the District controlled blood pressure trend report as a first test case for this sort of approach, as it is a key report and one we'd like to gather feedback on as quickly as possible.\\n","MatchedIds":[4909,1863,4285,4637,2403],"Predicted":"## Decision\nWe have decided to implement a data warehousing strategy using Amazon Redshift for our dashboard reporting needs.\n\n### Positive Consequences\n1. **Scalability**: Amazon Redshift can effectively scale both storage and compute resources, allowing us to handle increasing data volumes and complexity without significant performance degradation.\n2. **Stability**: Being a managed service, Redshift provides built-in optimizations for data loading, querying, and maintenance.\n3. **Simpler ETL Processes**: Redshift supports seamless integration with AWS DMS, AWS Glue, and other ETL tools, which can simplify the process of extracting, transforming, and loading data from various sources.\n4. **Complex Queries**: Redshift is optimized for complex analytical queries, making it easier to derive summary data for monthly and quarterly reports.\n5. **Cost Efficiency**: With its columnar storage and data compression capabilities, Redshift can offer cost efficiencies as compared to maintaining multiple materialized views.\n\n### Negative Consequences\n1. **Learning Curve**: There might be a steep learning curve for the team to become proficient with Redshift and its ecosystem.\n2. **Migration Overhead**: Transitioning from the current materialized view approach to a Redshift-based data warehousing solution will require significant migration efforts, including schema design, data migration, and query rewriting.\n3. **Potential Latency**: Depending on the network setup and data transfer volumes, there might be some latency involved in ETL processes and query execution.\n4. **Vendor Lock-In**: Choosing Amazon Redshift ties us to the AWS ecosystem, which may pose risks if strategic directions change regarding cloud provider preferences.\n\nThe team's priority will be to manage these negatives through proper planning, training, and phased implementation to ensure a smooth transition and realization of anticipated benefits.","GenTime":"2024-08-20 00:43:24"}
{"File Name":"PerformanceTestDocs\/0006-vellum-node-fails.md","Context":"## Context\\nVellum node fails\\n","Decision":"Vellum is always addressed by its cluster name and all of its data is stored in distributed databases with replicas of data on multiple nodes.\\n","tokens":9,"id":3952,"text":"## Context\\nVellum node fails\\n\n\n##Decision\nVellum is always addressed by its cluster name and all of its data is stored in distributed databases with replicas of data on multiple nodes.\\n","MatchedIds":[1855,5070,2181,3841,3853],"Predicted":"## Decision\nImplement robust monitoring and alerting for Vellum nodes using [Prometheus](https:\/\/prometheus.io\/) for monitoring and [Alertmanager](https:\/\/prometheus.io\/docs\/alerting\/alertmanager\/) for alert notifications. Additionally, establish regular maintenance and health check routines to identify and resolve issues proactively. If a node failure occurs, an automated fallback mechanism should switch to a redundant node with minimal downtime.","GenTime":"2024-08-20 00:43:29"}
{"File Name":"digital-paper-edit-storybook\/adr-28-08.md","Context":"## Context and Problem Statement\\nWe needed to clarify the relationship between the DPE Client repository, where the components we are using to populate the Storybook repo have already been written, and the Storybook repository.\\n- From which repo would components be published?\\n- Which repos would consume components from NPM?\\n- Should the Storybook live inside the Client repo?\\n## Decision Drivers\\nN\/A\\n","Decision":"N\/A\\nChosen option: Option 2, because this allows us to refactor components' code and preview changes within the Storybook locally \u2014 before publishing the component to the hosted Storybook and NPM.\\nThis means that our workflow for populating the Storybook and refactoring the Client code is as follows:\\n1. Duplicate component code to Storybook repo\\n2. Publish completed components to NPM\\n3. Remove the original component code from the Client and import via NPM\\n### Positive Consequences\\n### Negative consequences\\nCaveat: If more than one person is working on the Storybook and DPE Client, they'll need to sync up to ensure that details in code refactors are not lost due to overlapping work.\\nIf possible, also avoid having people working simultaneously on a component that consumes \/ is consumed by another component (i.e., one person working on a card component and another person working on a list component that consumes card components).\\n","tokens":83,"id":36,"text":"## Context and Problem Statement\\nWe needed to clarify the relationship between the DPE Client repository, where the components we are using to populate the Storybook repo have already been written, and the Storybook repository.\\n- From which repo would components be published?\\n- Which repos would consume components from NPM?\\n- Should the Storybook live inside the Client repo?\\n## Decision Drivers\\nN\/A\\n\n\n##Decision\nN\/A\\nChosen option: Option 2, because this allows us to refactor components' code and preview changes within the Storybook locally \u2014 before publishing the component to the hosted Storybook and NPM.\\nThis means that our workflow for populating the Storybook and refactoring the Client code is as follows:\\n1. Duplicate component code to Storybook repo\\n2. Publish completed components to NPM\\n3. Remove the original component code from the Client and import via NPM\\n### Positive Consequences\\n### Negative consequences\\nCaveat: If more than one person is working on the Storybook and DPE Client, they'll need to sync up to ensure that details in code refactors are not lost due to overlapping work.\\nIf possible, also avoid having people working simultaneously on a component that consumes \/ is consumed by another component (i.e., one person working on a card component and another person working on a list component that consumes card components).\\n","MatchedIds":[3035,3031,697,4766,1996],"Predicted":"## Decision\n### Decision\nWe will use the following approach:\n\n1. **From which repo would components be published?**\n   - Components will be published from the DPE Client repository. This allows for a single source of truth for the components' source code.\n\n2. **Which repos would consume components from NPM?**\n   - The Storybook repository and any other client or service that requires these components will consume them from NPM. This ensures consistency in component usage across different environments and applications.\n\n3. **Should the Storybook live inside the Client repo?**\n   - Storybook will not live inside the Client repo. Instead, the Storybook repo will be a separate repository. This decoupling allows for independent versioning and updates of the Storybook documentation without affecting the development and stability of the actual components within the DPE Client repository.\n\n### Positive Consequences\n- **Separation of concerns:** Decoupling the Storybook from the Client repo helps maintain clear boundaries between component development and documentation, making both easier to manage.\n- **Single Source of Truth:** Publishing components from a single repo (DPE Client repository) ensures consistency and maintains a centralized location for component updates and bug fixes.\n- **Reuse and Consumption:** By hosting the components on NPM, it becomes straightforward for multiple repositories to consume the components, ensuring uniformity across applications.\n- **Independent Versioning:** The Storybook can be updated and versioned independently of the Client repo, allowing more flexibility in documentation versions and releases.\n\n### Negative Consequences\n- **Management Overhead:** Separating Storybook into a different repo introduces an additional repository to manage, which might increase the operational complexity.\n- **Builds and CI:** Changes in components in the DPE Client repo may require updates and synchronizations in the Storybook repo, necessitating a well-defined CI\/CD process to handle dependencies and updates efficiently.\n\n### Mitigation Strategies\n- **CI\/CD Integration:** Set up CI\/CD pipelines to automatically publish updated components to NPM and to synchronize updates between the DPE Client repo and the Storybook repo.\n- **Documentation:** Maintain comprehensive documentation around the processes of component publishing, Storybook updates, and NPM consumption to reduce the learning curve and avoid confusion.\n\nBy implementing these strategies, we can maximize the benefits of having a separated but interconnected system for component development and documentation.","GenTime":"2024-08-20 00:43:41"}
{"File Name":"sexual-health-service-finder\/0002-use-express-web-framework.md","Context":"## Context\\nA web framework is needed in order to provide a level of abstraction over low\\nlevel language features. Effort spent solving non-domain specific problems\\nshould be kept to an absolute minimum.\\n","Decision":"We will use the [Express](http:\/\/expressjs.com\/) web framework. This will\\nprovide a lot of well\\n[documented](http:\/\/expressjs.com\/en\/resources\/books-blogs.html) features and\\nfunctionality. Whilst providing a fast, unopinionated, minimalist web\\nframework.\\nExpress is one of the most popular web frameworks available for NodeJS and as\\nsuch has been well tested and is deployed across many\\n[organisations](http:\/\/expressjs.com\/en\/resources\/companies-using-express.html).\\n","tokens":41,"id":4271,"text":"## Context\\nA web framework is needed in order to provide a level of abstraction over low\\nlevel language features. Effort spent solving non-domain specific problems\\nshould be kept to an absolute minimum.\\n\n\n##Decision\nWe will use the [Express](http:\/\/expressjs.com\/) web framework. This will\\nprovide a lot of well\\n[documented](http:\/\/expressjs.com\/en\/resources\/books-blogs.html) features and\\nfunctionality. Whilst providing a fast, unopinionated, minimalist web\\nframework.\\nExpress is one of the most popular web frameworks available for NodeJS and as\\nsuch has been well tested and is deployed across many\\n[organisations](http:\/\/expressjs.com\/en\/resources\/companies-using-express.html).\\n","MatchedIds":[1243,3470,1477,3062,1240],"Predicted":"## Decision\nWe will use the [Django](https:\/\/www.djangoproject.com\/) web framework. Django is a high-level Python web framework that encourages rapid development and clean, pragmatic design. It comes with a range of built-in functionalities that minimize the need to write repetitive code, helping us to concentrate on solving business-specific problems. Additionally, Django's strong community support and extensive documentation make it a reliable choice for our project.","GenTime":"2024-08-20 00:43:44"}
{"File Name":"hello\/008-deployment.md","Context":"## Context\\nWe need a way to deploy our application to AWS.\\n","Decision":"We will use [Terraform] to deploy our application. More specifically,\\nwe will use [Chalice] to create the build artifact for the source\\ncode (.zip) and then use [Terraform] to manage the infrastructure\\nresources.\\n\ud83d\udc49 **Note:** Ideally, we would publish the build artifacts to S3 before\\ndeployment. That way the deployment process could happen independently\\nof the build process.\\n\ud83d\udc49 **Note:** To begin with, we'll just convert the JSON data created\\nby [Chalice] to [HCL] and add any other resources needed. Going forward,\\nwe will use modules to create reusable components.\\n","tokens":15,"id":4358,"text":"## Context\\nWe need a way to deploy our application to AWS.\\n\n\n##Decision\nWe will use [Terraform] to deploy our application. More specifically,\\nwe will use [Chalice] to create the build artifact for the source\\ncode (.zip) and then use [Terraform] to manage the infrastructure\\nresources.\\n\ud83d\udc49 **Note:** Ideally, we would publish the build artifacts to S3 before\\ndeployment. That way the deployment process could happen independently\\nof the build process.\\n\ud83d\udc49 **Note:** To begin with, we'll just convert the JSON data created\\nby [Chalice] to [HCL] and add any other resources needed. Going forward,\\nwe will use modules to create reusable components.\\n","MatchedIds":[1239,2471,3722,4096,4025],"Predicted":"## Decision\nUse AWS Elastic Beanstalk because it simplifies the deployment, management, and scaling of our application. Elastic Beanstalk handles the provisioning of underlying infrastructure such as EC2 instances, load balancing, and auto-scaling, allowing the development team to focus more on writing code rather than managing infrastructure.\n\nTo integrate this with our CI\/CD process, we will:\n1. Use GitHub Actions for continuous integration and delivery.\n2. Configure GitHub Actions to build our application and push the artifacts to an S3 bucket.\n3. Set up a deployment pipeline in GitHub Actions that triggers Elastic Beanstalk deployments using the AWS CLI.\n\nRequired configurations for GitHub Actions:\n- AWS_ACCESS_KEY_ID\n- AWS_SECRET_ACCESS_KEY\n- AWS_REGION\n- ELASTIC_BEANSTALK_ENVIRONMENT_NAME\n\nThis approach will allow us to have a streamlined and automated deployment process, leveraging the robustness and ease of use provided by AWS Elastic Beanstalk.","GenTime":"2024-08-20 00:44:21"}
{"File Name":"infra\/0002-use-helm-for-service-deployment-definitions.md","Context":"## Context\\nOne of the primary problems encountered in deployments is drift between different environments.  An important element that creates drift is deltas between the deployment of different environments.  We are already mitigating that by storing our configurations as code, and doing automated deployments of those configurations.  An extension of these practices is to practice code reuse, or DRY (don't repeat yourself).\\nGiven that we have existing kubernetes deployments, and that our primary supported services (bedrock\/www, snippets, basket) are already deployed there, it seems reasonable to invest further in the kubernetes eco-system.  Our current pattern is to define a separate set of yml files which are deployed via `kubectl apply -f` per region\/cloud. For example, if we have gcp iowa-a and an aws frankfurt cluster, if we wish to deploy to both we'd have two copies of nearly identical files to define the deployments to those two clusters.\\n","Decision":"Use helm3 in order to define the 'template' of our services.  Helm calls these templates 'charts', templates can have injected 'values'. The deployment, and associated kubernetes objects (such as services, and scaling policies) should be defined once, with sensible defaults chosen for the primary chart (these should be the 'prod' values).  Secrets should be referenced, but not included in the charts (paths to secrets, not the secrets themselves). Then environments that need different values should have an override file in their repo, which can be combined with defaults at deploy time.\\nThere should be a single mozmeao helm repo, that contains all of our custom written charts. We would expect there to be a single chart per service, where bedrock\/www is a service.  There should be a pipeline for that helm repo (that includes testing).  The pipeline for each service would then reference and deploy those charts for the dev\/staging\/prod versions of the service.\\nOne advantage of undertaking this work is making it easier to read and understand our deployments.  Answering 'what's different between dev and prod' is difficult when the full configuration is repeated.  It's much easier to answer when dev is defaults + a small override file, and prod is the same.  We should also end up with fewer differences, since each difference is clearly visible in the charts, and we can seek to reduce that count.\\nThe other primary advantage is reducing the class of errors where some new feature worked in dev, but doesn't in prod because you forgot to do X. Where X is likely adding an environment variable, or creating a secret.  Having a template means we should be able to fail the deployment earlier in each environment if that configuration is not present.\\n","tokens":190,"id":856,"text":"## Context\\nOne of the primary problems encountered in deployments is drift between different environments.  An important element that creates drift is deltas between the deployment of different environments.  We are already mitigating that by storing our configurations as code, and doing automated deployments of those configurations.  An extension of these practices is to practice code reuse, or DRY (don't repeat yourself).\\nGiven that we have existing kubernetes deployments, and that our primary supported services (bedrock\/www, snippets, basket) are already deployed there, it seems reasonable to invest further in the kubernetes eco-system.  Our current pattern is to define a separate set of yml files which are deployed via `kubectl apply -f` per region\/cloud. For example, if we have gcp iowa-a and an aws frankfurt cluster, if we wish to deploy to both we'd have two copies of nearly identical files to define the deployments to those two clusters.\\n\n\n##Decision\nUse helm3 in order to define the 'template' of our services.  Helm calls these templates 'charts', templates can have injected 'values'. The deployment, and associated kubernetes objects (such as services, and scaling policies) should be defined once, with sensible defaults chosen for the primary chart (these should be the 'prod' values).  Secrets should be referenced, but not included in the charts (paths to secrets, not the secrets themselves). Then environments that need different values should have an override file in their repo, which can be combined with defaults at deploy time.\\nThere should be a single mozmeao helm repo, that contains all of our custom written charts. We would expect there to be a single chart per service, where bedrock\/www is a service.  There should be a pipeline for that helm repo (that includes testing).  The pipeline for each service would then reference and deploy those charts for the dev\/staging\/prod versions of the service.\\nOne advantage of undertaking this work is making it easier to read and understand our deployments.  Answering 'what's different between dev and prod' is difficult when the full configuration is repeated.  It's much easier to answer when dev is defaults + a small override file, and prod is the same.  We should also end up with fewer differences, since each difference is clearly visible in the charts, and we can seek to reduce that count.\\nThe other primary advantage is reducing the class of errors where some new feature worked in dev, but doesn't in prod because you forgot to do X. Where X is likely adding an environment variable, or creating a secret.  Having a template means we should be able to fail the deployment earlier in each environment if that configuration is not present.\\n","MatchedIds":[1376,1372,631,187,2409],"Predicted":"## Decision\nWe will adopt a templating engine such as Helm to manage our Kubernetes deployments. By using Helm, we can define reusable templates for our Kubernetes resources and parameterize them for different regions and clouds. This approach will allow us to maintain a single set of deployment charts, thereby reducing duplication and minimizing the risk of drift between environments. The following steps will be taken:\n\n1. **Template Creation**: Develop Helm charts for our primary supported services (bedrock\/www, snippets, basket) that encapsulate our YAML configurations.\n2. **Parameterization**: Introduce parameters within these Helm charts to handle differences between regions and clouds, such as image tags, replicas, and environment-specific configurations.\n3. **Environment Values Files**: Create separate values files for each of our target environments (e.g., gcp-iowa-a, aws-frankfurt) that will pass specific parameters to the Helm charts.\n4. **Deployment Process**: Update our deployment pipeline to use `helm upgrade --install` commands instead of `kubectl apply -f`. The deployment pipeline will use the appropriate values file for the target environment.\n5. **Documentation and Training**: Document the new process and provide training to the development and operations teams to ensure a smooth transition to using Helm.\n\nThis approach will ensure consistency across deployments, facilitate code reuse, and help us adhere to the DRY principle.","GenTime":"2024-08-20 00:44:28"}
{"File Name":"monocle\/0009-changes-extended-with-task-data.md","Context":"## Context and Problem Statement\\nAs a user, I want to get Changes metrics related to tasks defined\\nin a task tracker. A simple usecase example is to get insight of the\\nratio of changes related to Feature Requests vs Bug fixing.\\n## Decision Drivers\\n* Simple implementation\\n* No assumption about the tasks tracker\\n* Support of a set of generic fields related to a task\\n","Decision":"* Simple implementation\\n* No assumption about the tasks tracker\\n* Support of a set of generic fields related to a task\\nChosen option: \"Monocle API provides an interface for external task data crawlers\", because it will ease integration between Monocle and the\\nvarious task trackers available in the market.\\nA Monocle operator will need to write the crawler for its\\nown task tracker. However, over the time, operators might have written crawlers for most popular systems and released them under a open source license.\\nFrom the Monocle side, we provide a clear API for a task tracker\\ncrawler to push task related data to Monocle. Each task data sent to Monocle must at least set a predifined set of generic attributes like\\n\"severity\", \"change_url\", \"title\", ... The monocle API is then able to\\nfind corresponding Changes in the database that match the \"change_url\" field.\\nEach changes get a new attribute called \"task_data\" that is a list of\\nrelated task data records. Indeed multiple tasks might be related to a\\nsingle change.\\nAlso, Monocle keeps track of task data that do not match any Changes in the\\ndatabase. The Monocle change crawlers engine triggers a search for adoption of orphaned task data.\\n","tokens":79,"id":4636,"text":"## Context and Problem Statement\\nAs a user, I want to get Changes metrics related to tasks defined\\nin a task tracker. A simple usecase example is to get insight of the\\nratio of changes related to Feature Requests vs Bug fixing.\\n## Decision Drivers\\n* Simple implementation\\n* No assumption about the tasks tracker\\n* Support of a set of generic fields related to a task\\n\n\n##Decision\n* Simple implementation\\n* No assumption about the tasks tracker\\n* Support of a set of generic fields related to a task\\nChosen option: \"Monocle API provides an interface for external task data crawlers\", because it will ease integration between Monocle and the\\nvarious task trackers available in the market.\\nA Monocle operator will need to write the crawler for its\\nown task tracker. However, over the time, operators might have written crawlers for most popular systems and released them under a open source license.\\nFrom the Monocle side, we provide a clear API for a task tracker\\ncrawler to push task related data to Monocle. Each task data sent to Monocle must at least set a predifined set of generic attributes like\\n\"severity\", \"change_url\", \"title\", ... The monocle API is then able to\\nfind corresponding Changes in the database that match the \"change_url\" field.\\nEach changes get a new attribute called \"task_data\" that is a list of\\nrelated task data records. Indeed multiple tasks might be related to a\\nsingle change.\\nAlso, Monocle keeps track of task data that do not match any Changes in the\\ndatabase. The Monocle change crawlers engine triggers a search for adoption of orphaned task data.\\n","MatchedIds":[1701,2403,3469,1866,4285],"Predicted":"## Decision\nWe will implement a data abstraction layer that collects and normalizes task data from various task trackers into a unified format. This will be done by:\n\n- Creating a standard schema that includes generic fields such as `task_id`, `task_type`, `status`, `priority`, `created_at`, `updated_at`, and `changed_by`.\n- Developing connectors for popular task trackers (e.g., Jira, Trello, Asana) to pull data into our unified schema.\n- Storing the normalized task data in a centralized database to facilitate query and analytics operations.\n- Building a set of predefined queries to generate metrics, such as the ratio of Feature Requests to Bug Fixes, over different time periods and project scopes.\n- Providing an API to allow users to fetch the metrics without needing to know the specific underlying task tracker.\n  \nThis approach ensures simplicity in implementation, does not make assumptions about the task tracker, and supports a generic set of fields related to tasks.","GenTime":"2024-08-28 14:17:18"}
{"File Name":"form-builder\/0006-reduce-number-of-emails-per-submission.md","Context":"## Context\\nForm Builder sends one email per file uploaded for each application. For those forms that allow multiple files to be uploaded this generates a lot of email noise for both form owners and the applicants. This can result in additional work to sift these emails or even create unforced errors in workflows that are manual in nature.\\nThe use of AWS SES as the email service means that there is a 10MB upper limit on the size of each email that is sent.\\nOptions discussed:\\n1. Attach as many files as possible to a single email up to the 10MB limit.\\n2. Hard cap limit on 10MB in total accross _all_ the files uploaded per form.\\n3. Admin interface for form owners to log in and retrieve file uploads per submission.\\n4. Email a link to the form owner allowing the ability to download all attachments in one go.\\n","Decision":"We feel that it is reasonable to reduce the number of emails that recipients of each submission get to the absolute minimum.\\nOption 1 is the one which is the most reasonable solution at this time. It requires the least amount of code change and does not change the architecture or increase the threat surface of Form Builder.\\n","tokens":181,"id":5099,"text":"## Context\\nForm Builder sends one email per file uploaded for each application. For those forms that allow multiple files to be uploaded this generates a lot of email noise for both form owners and the applicants. This can result in additional work to sift these emails or even create unforced errors in workflows that are manual in nature.\\nThe use of AWS SES as the email service means that there is a 10MB upper limit on the size of each email that is sent.\\nOptions discussed:\\n1. Attach as many files as possible to a single email up to the 10MB limit.\\n2. Hard cap limit on 10MB in total accross _all_ the files uploaded per form.\\n3. Admin interface for form owners to log in and retrieve file uploads per submission.\\n4. Email a link to the form owner allowing the ability to download all attachments in one go.\\n\n\n##Decision\nWe feel that it is reasonable to reduce the number of emails that recipients of each submission get to the absolute minimum.\\nOption 1 is the one which is the most reasonable solution at this time. It requires the least amount of code change and does not change the architecture or increase the threat surface of Form Builder.\\n","MatchedIds":[3987,2523,4533,5163,3797],"Predicted":"## Decision\n\nChosen option: Email a link to the form owner allowing the ability to download all attachments in one go.\n\n### Positive Consequences\n* Reduces email noise significantly by consolidating multiple file uploads into a single notification.\n* Simplifies the workflow for form owners, who can download all attachments with one click.\n* Eliminates the 10MB email size issue as files are stored elsewhere and accessed via a link.\n* Improved user experience for both form owners and applicants.\n\n### Negative Consequences\n* Requires additional implementation effort to generate and secure the download links.\n* Form owners need to rely on the link being accessible when they wish to retrieve the files, which necessitates proper link management and expiration policies.\n\n### Additional Considerations\n* Implement security measures such as token-based access or temporary access links to ensure that file downloads are protected.\n* Provide notifications in the admin interface when new file uploads are received, allowing form owners to manage their submissions more effectively.\n* Ensure the storage solution for the uploaded files is robust and can handle concurrent access smoothly.","GenTime":"2024-08-28 14:17:22"}
{"File Name":"spin-archetype-base\/0002-unified-terminology.md","Context":"## Context\\nIt is easy to use different terms meaning the same thing in code. A common aphorism is that the most difficult thing in coding is naming.\\n","Decision":"A `Terminology.md` document will be created, that will be used to collect the official terms for the project. If naming diverts from this terminology, it should be considerd a bug.\\n","tokens":34,"id":2270,"text":"## Context\\nIt is easy to use different terms meaning the same thing in code. A common aphorism is that the most difficult thing in coding is naming.\\n\n\n##Decision\nA `Terminology.md` document will be created, that will be used to collect the official terms for the project. If naming diverts from this terminology, it should be considerd a bug.\\n","MatchedIds":[1416,5070,1418,400,1477],"Predicted":"## Decision\nWe will establish and adhere to a consistent naming convention across the entire codebase and document this convention in the project's guidelines to ensure clarity and uniformity.","GenTime":"2024-08-28 14:17:24"}
{"File Name":"sepa-customer-platform\/0006-relating-documents-to-cases.md","Context":"## Context and Problem Statement\\nDocument uploads will be stored in Azure Blob storage because of the requirement to store very large files & the fact that Azure Blob storage offers high availability and low costs.\\nThe Azure Blob storage system is a separate cloud service which is not directly linked to Dynamics CRM, therefore we need to consider how the permits, stored using CRM case\/incident entities, will be linked to the the files within Azure.\\n## Decision Drivers\\n* Files will be displayed to users via a web frontend, therefore the entity used to store the file reference attribute should be accessible and filterable via the WebAPI.\\n* The existing Dynamics UI should be able to display links to the Azure files for administrators.\\n","Decision":"* Files will be displayed to users via a web frontend, therefore the entity used to store the file reference attribute should be accessible and filterable via the WebAPI.\\n* The existing Dynamics UI should be able to display links to the Azure files for administrators.\\n[Option 1] Use the existing 'annotation' entity in the CRM & set the entity attributes to match Azure blob data.\\n### Positive Consequences\\n* Minimises customisation of the CRM data, reuse of existing\\n### Negative Consequences\\n* Labelling within the CRM does not match the context of the upload.\\n","tokens":142,"id":5163,"text":"## Context and Problem Statement\\nDocument uploads will be stored in Azure Blob storage because of the requirement to store very large files & the fact that Azure Blob storage offers high availability and low costs.\\nThe Azure Blob storage system is a separate cloud service which is not directly linked to Dynamics CRM, therefore we need to consider how the permits, stored using CRM case\/incident entities, will be linked to the the files within Azure.\\n## Decision Drivers\\n* Files will be displayed to users via a web frontend, therefore the entity used to store the file reference attribute should be accessible and filterable via the WebAPI.\\n* The existing Dynamics UI should be able to display links to the Azure files for administrators.\\n\n\n##Decision\n* Files will be displayed to users via a web frontend, therefore the entity used to store the file reference attribute should be accessible and filterable via the WebAPI.\\n* The existing Dynamics UI should be able to display links to the Azure files for administrators.\\n[Option 1] Use the existing 'annotation' entity in the CRM & set the entity attributes to match Azure blob data.\\n### Positive Consequences\\n* Minimises customisation of the CRM data, reuse of existing\\n### Negative Consequences\\n* Labelling within the CRM does not match the context of the upload.\\n","MatchedIds":[4533,5157,2520,1574,1480],"Predicted":"## Decision\nTo link permits stored in Dynamics CRM with the files in Azure Blob Storage, we will use a new custom entity within Dynamics CRM to store file metadata and references.\n\n### Implementation Details:\n1. **Custom Entity Creation:**\n   - Create a new custom entity in Dynamics CRM called \"Document Reference\".\n   - This entity will store metadata such as the file name, URL to the Azure Blob, upload date, and any relevant tags or categories.\n\n2. **Entity Relationships:**\n   - Define a relationship between the \"Document Reference\" entity and the existing case\/incident entities. This will allow us to associate multiple documents with a single case or incident.\n   - Ensure that the relationship is established such that the file references can be easily queried through the WebAPI.\n\n3. **WebAPI Accessibility:**\n   - Make the new \"Document Reference\" entity accessible and filterable via the Dynamics CRM WebAPI. This will allow the frontend application to query and display the necessary file references based on the case or incident being viewed.\n   - Implement necessary WebAPI end-points to support query operations for the \"Document Reference\" entity.\n\n4. **UI Integration:**\n   - Update the existing Dynamics CRM UI to include a new section or tab within the case\/incident forms to display the list of associated documents.\n   - Each document entry should be a clickable link that redirects administrators to the respective file in Azure Blob Storage.\n\n5. **Blob Storage Integration:**\n   - Ensure that when a new file is uploaded to Azure Blob Storage, an entry is also created in the \"Document Reference\" entity with the corresponding metadata and URL.\n   - Implement a mechanism to handle updates and deletions so that changes are reflected in both Azure Blob Storage and the CRM system.\n\n### Positive Consequences\n* Reusing existing Dynamics CRM features (custom entities and relationships) to link documents with cases\/incidents ensures a smooth integration.\n* The new custom entity ensures that file references are accessible and filterable via WebAPI, facilitating the frontend display of files.\n* Administrators can easily access files directly from the Dynamics CRM UI, enhancing usability.\n\n### Negative Consequences\n* Requires additional development to create and maintain the custom \"Document Reference\" entity and its integration with Azure Blob Storage.\n* Potentially higher complexity in managing the lifecycle (create, update, delete) synchronization between Dynamics CRM and Azure Blob Storage.\n\n### Final Considerations\nBy leveraging a custom entity, this approach provides a flexible, scalable, and maintainable solution to integrate Azure Blob Storage with Dynamics CRM while meeting the defined decision drivers.","GenTime":"2024-08-28 14:17:32"}
{"File Name":"up-fiscal-data\/006-time.md","Context":"## Context and Problem Statement\\nThe data extraction w\/ transaction timestamp from Koshvani platform is a challenging task.\\n- The temporal data directly available on the platform is not easy to extract\\n- The validation of temporal data to ensure accuracy depends on factors\\n- Districts\/Treasuries which have updated their data\\n- Data\/Time of data upload vs extraction from departments\\n## Decision Drivers\\nThe platform shares the data updation status for each treasury at a daily level. But due to the factors stated above, an in-depth check and analysis is required to extract the correct data.\\n","Decision":"The platform shares the data updation status for each treasury at a daily level. But due to the factors stated above, an in-depth check and analysis is required to extract the correct data.\\n- Do a daily level data extraction from the platform\\n- Conduct analysis to identify daily, weekly and monthly expenditure numbers\\n- Select the accurate time period and mention methodology for the same\\n- Ignore previous time period missed in this analysis.\\nConduct the analysis on accurate data collection at in November, 2020 to take decision.\\n","tokens":120,"id":442,"text":"## Context and Problem Statement\\nThe data extraction w\/ transaction timestamp from Koshvani platform is a challenging task.\\n- The temporal data directly available on the platform is not easy to extract\\n- The validation of temporal data to ensure accuracy depends on factors\\n- Districts\/Treasuries which have updated their data\\n- Data\/Time of data upload vs extraction from departments\\n## Decision Drivers\\nThe platform shares the data updation status for each treasury at a daily level. But due to the factors stated above, an in-depth check and analysis is required to extract the correct data.\\n\n\n##Decision\nThe platform shares the data updation status for each treasury at a daily level. But due to the factors stated above, an in-depth check and analysis is required to extract the correct data.\\n- Do a daily level data extraction from the platform\\n- Conduct analysis to identify daily, weekly and monthly expenditure numbers\\n- Select the accurate time period and mention methodology for the same\\n- Ignore previous time period missed in this analysis.\\nConduct the analysis on accurate data collection at in November, 2020 to take decision.\\n","MatchedIds":[445,447,446,443,1863],"Predicted":"## Decision\n- Implement a multi-step validation process to verify the accuracy of the temporal data.\n- Establish a consistent schedule for data extraction that aligns with the daily update cycle of the platform.\n- Create a logging mechanism to record the data extraction timestamps and compare them with the upload timestamps from the treasuries.\n- Develop a monitoring tool that flags any discrepancies in data updates and provides alerts for further investigation.\n- Conduct regular audits of the extracted data to ensure its integrity and accuracy.\n- Use an ETL (Extract, Transform, Load) framework to automate the data extraction and transformation process, ensuring consistency and reliability.","GenTime":"2024-08-28 14:17:38"}
{"File Name":"tendermint\/adr-073-libp2p.md","Context":"## Context\\nAs part of the 0.35 development cycle, the Tendermint team completed\\nthe first phase of the work described in ADRs 61 and 62, which included a\\nlarge scale refactoring of the reactors and the p2p message\\nrouting. This replaced the switch and many of the other legacy\\ncomponents without breaking protocol or network-level\\ninteroperability and left the legacy connection\/socket handling code.\\nFollowing the release, the team has reexamined the state of the code\\nand the design, as well as Tendermint's requirements. The notes\\nfrom that process are available in the [P2P Roadmap\\nRFC][rfc].\\nThis ADR supersedes the decisions made in ADRs 60 and 61, but\\nbuilds on the completed portions of this work. Previously, the\\nboundaries of peer management, message handling, and the higher level\\nbusiness logic (e.g., \"the reactors\") were intermingled, and core\\nelements of the p2p system were responsible for the orchestration of\\nhigher-level business logic. Refactoring the legacy components\\nmade it more obvious that this entanglement of responsibilities\\nhad outsized influence on the entire implementation, making\\nit difficult to iterate within the current abstractions.\\nIt would not be viable to maintain interoperability with legacy\\nsystems while also achieving many of our broader objectives.\\nLibP2P is a thoroughly-specified implementation of a peer-to-peer\\nnetworking stack, designed specifically for systems such as\\nours. Adopting LibP2P as the basis of Tendermint will allow the\\nTendermint team to focus more of their time on other differentiating\\naspects of the system, and make it possible for the ecosystem as a\\nwhole to take advantage of tooling and efforts of the LibP2P\\nplatform.\\n","Decision":"Tendermint will adopt libp2p during the 0.37 development cycle,\\nreplacing the bespoke Tendermint P2P stack. This will remove the\\n`Endpoint`, `Transport`, `Connection`, and `PeerManager` abstractions\\nand leave the reactors, `p2p.Router` and `p2p.Channel`\\nabstractions.\\nLibP2P may obviate the need for a dedicated peer exchange (PEX)\\nreactor, which would also in turn obviate the need for a dedicated\\nseed mode. If this is the case, then all of this functionality would\\nbe removed.\\nIf it turns out (based on the advice of Protocol Labs) that it makes\\nsense to maintain separate pubsub or gossipsub topics\\nper-message-type, then the `Router` abstraction could also\\nbe entirely subsumed.\\n","tokens":387,"id":1982,"text":"## Context\\nAs part of the 0.35 development cycle, the Tendermint team completed\\nthe first phase of the work described in ADRs 61 and 62, which included a\\nlarge scale refactoring of the reactors and the p2p message\\nrouting. This replaced the switch and many of the other legacy\\ncomponents without breaking protocol or network-level\\ninteroperability and left the legacy connection\/socket handling code.\\nFollowing the release, the team has reexamined the state of the code\\nand the design, as well as Tendermint's requirements. The notes\\nfrom that process are available in the [P2P Roadmap\\nRFC][rfc].\\nThis ADR supersedes the decisions made in ADRs 60 and 61, but\\nbuilds on the completed portions of this work. Previously, the\\nboundaries of peer management, message handling, and the higher level\\nbusiness logic (e.g., \"the reactors\") were intermingled, and core\\nelements of the p2p system were responsible for the orchestration of\\nhigher-level business logic. Refactoring the legacy components\\nmade it more obvious that this entanglement of responsibilities\\nhad outsized influence on the entire implementation, making\\nit difficult to iterate within the current abstractions.\\nIt would not be viable to maintain interoperability with legacy\\nsystems while also achieving many of our broader objectives.\\nLibP2P is a thoroughly-specified implementation of a peer-to-peer\\nnetworking stack, designed specifically for systems such as\\nours. Adopting LibP2P as the basis of Tendermint will allow the\\nTendermint team to focus more of their time on other differentiating\\naspects of the system, and make it possible for the ecosystem as a\\nwhole to take advantage of tooling and efforts of the LibP2P\\nplatform.\\n\n\n##Decision\nTendermint will adopt libp2p during the 0.37 development cycle,\\nreplacing the bespoke Tendermint P2P stack. This will remove the\\n`Endpoint`, `Transport`, `Connection`, and `PeerManager` abstractions\\nand leave the reactors, `p2p.Router` and `p2p.Channel`\\nabstractions.\\nLibP2P may obviate the need for a dedicated peer exchange (PEX)\\nreactor, which would also in turn obviate the need for a dedicated\\nseed mode. If this is the case, then all of this functionality would\\nbe removed.\\nIf it turns out (based on the advice of Protocol Labs) that it makes\\nsense to maintain separate pubsub or gossipsub topics\\nper-message-type, then the `Router` abstraction could also\\nbe entirely subsumed.\\n","MatchedIds":[1979,1956,4650,21,1442],"Predicted":"## Decision\nAdopt LibP2P as the foundational framework for the Tendermint peer-to-peer networking stack. The transition will involve the following phases:\n\n1. **Phase 1: Integration and Baseline Establishment**\n    - Replace the legacy connection and socket handling code with LibP2P components.\n    - Ensure interoperability by maintaining protocol compatibility during this transition.\n    - Establish a baseline for performance and reliability to compare against the legacy system.\n\n2. **Phase 2: Optimization and Feature Parity**\n    - Optimize the integration to leverage LibP2P's built-in features and performance enhancements.\n    - Achieve feature parity with the current Tendermint P2P stack, ensuring no loss of functionality during the transition.\n    - Gradually refactor and separate peer management, message handling, and business logic to align with LibP2P's modular architecture.\n\n3. **Phase 3: Enhancement and Extension**\n    - Utilize advanced features of LibP2P to introduce new capabilities, such as improved NAT traversal, flexible peer discovery mechanisms, and enhanced transport protocols.\n    - Streamline the codebase by removing obsolete components and abstractions, making the system more maintainable and extensible.\n    - Engage with the LibP2P community for potential collaborations and to stay aligned with the latest advancements in the LibP2P ecosystem.\n\n4. **Phase 4: Testing and Validation**\n    - Implement rigorous testing to ensure robustness, security, and performance stability.\n    - Conduct interoperability testing with existing Tendermint nodes to ensure a smooth transition and backward compatibility.\n    - Gather feedback from the community and iterate on any identified issues or improvements.\n\n5. **Phase 5: Documentation and Community Support**\n    - Provide comprehensive documentation on the new P2P stack, including migration guides, API references, and best practices.\n    - Engage with the Tendermint and broader blockchain community to address concerns, provide support, and facilitate the adoption of the new P2P stack.\n\nBy adopting LibP2P, Tendermint will benefit from a mature, well-supported networking stack, allowing the development team to focus more on higher-level protocol improvements and innovations. This transition will also position Tendermint to leverage future advancements in peer-to-peer networking technology efficiently.","GenTime":"2024-08-28 14:17:46"}
{"File Name":"optimint\/adr-005-serialization.md","Context":"## Context\\nAll the basic data types needs to be efficiently serialized into binary format before saving in KV store or sending to network.\\n","Decision":"`protobuf` is used for data serialization both for storing and network communication.\\n`protobuf` is used widely in entire Cosmos ecosystem, and we would need to use it anyways.\\n","tokens":27,"id":3746,"text":"## Context\\nAll the basic data types needs to be efficiently serialized into binary format before saving in KV store or sending to network.\\n\n\n##Decision\n`protobuf` is used for data serialization both for storing and network communication.\\n`protobuf` is used widely in entire Cosmos ecosystem, and we would need to use it anyways.\\n","MatchedIds":[1558,2944,2585,4747,2896],"Predicted":"## Decision\nWe will use Protocol Buffers (protobuf) for serializing all basic data types into a compact binary format. \n\nProtocol Buffers are a language-neutral, platform-neutral extensible mechanism for serializing structured data. It provides:\n\n- **Efficiency**: Compact encoding which results in a smaller size and faster serialization\/deserialization.\n- **Schema Evolution**: Easy to update the data structure without breaking backward compatibility.\n- **Interoperability**: Supported in multiple programming languages, making it easier to work with distributed systems.\n\nEach data type will be defined in a .proto file, which will be used to auto-generate the necessary serialization code.\n\nHere's an example of what this process will look like:\n\n1. **Define Data Structures**: Create a .proto file describing the data structures.\n    ```proto\n    syntax = \"proto3\";\n    \n    message MyData {\n        int32 id = 1;\n        string name = 2;\n        bool is_active = 3;\n    }\n    ```\n\n2. **Generate Code**: Use the Protocol Buffers compiler to generate classes that correspond to the message types.\n    ```bash\n    protoc --java_out=src\/main\/java mydata.proto\n    ```\n\n3. **Use Generated Classes**: Utilize the generated classes in your application to serialize and deserialize the data.\n    ```java\n    MyData data = MyData.newBuilder().setId(123).setName(\"example\").setIsActive(true).build();\n    byte[] binaryData = data.toByteArray(); \/\/ To serialize\n    MyData deserializedData = MyData.parseFrom(binaryData); \/\/ To deserialize\n    ```\n\nBy using Protocol Buffers, we aim to enhance the performance, maintainability, and interoperability of our data serialization process.","GenTime":"2024-08-28 14:17:52"}
{"File Name":"educational-platform\/0011-use-axon-event-publishing-mechanism.md","Context":"## Context\\nIn [0002-integration-events-implementation.md](0002-integration-events-implementation.md) was defined the solution for using Spring-event related classes for publishing and listening integration events. In current implementation of application we have Axon Framework which have rich tools for implementing such functionality. After migrating to Axon implementation of integration events, in future, we can enable event sourcing.\\n","Decision":"Axon Framework will be used for integration events implementation.\\n","tokens":79,"id":2500,"text":"## Context\\nIn [0002-integration-events-implementation.md](0002-integration-events-implementation.md) was defined the solution for using Spring-event related classes for publishing and listening integration events. In current implementation of application we have Axon Framework which have rich tools for implementing such functionality. After migrating to Axon implementation of integration events, in future, we can enable event sourcing.\\n\n\n##Decision\nAxon Framework will be used for integration events implementation.\\n","MatchedIds":[2499,649,885,2309,3123],"Predicted":"## Decision\nWe will migrate the implementation of integration events from Spring-event related classes to the Axon Framework. Axon offers a comprehensive suite of tools that can enhance our event-driven architecture, providing us with more flexibility and features, such as built-in support for complex sagas, advanced event handling, and event replay capabilities. By leveraging Axon Framework, we position ourselves to easily incorporate event sourcing in the future if required. \n\nWe will continue to monitor the integration to ensure minimal disruption and maintain the capability to revert to Spring Events if necessary. This incremental approach will help us harness the strengths of Axon while mitigating risks associated with the transition. \n\n- https:\/\/axoniq.io\/documentation\n- https:\/\/docs.spring.io\/spring\/docs\/current\/javadoc-api\/org\/springframework\/context\/event\/EventListener.html","GenTime":"2024-08-28 14:17:55"}
{"File Name":"monitoring-doc\/0002-configuration-management.md","Context":"## Context\\nWe have the requirement of adding some resources to the base cloud instances. We currently do\\nthis via the [cloud.conf](https:\/\/github.com\/alphagov\/prometheus-aws-configuration\/blob\/375f34600e373aa0e4c66fcae032ceee361d8c21\/terraform\/modules\/prometheus\/cloud.conf) system. This presents us with some limitations, such as configuration\\nbeing limited to 16kb, duplication in each instance terraform and a lack of fast feedback testing.\\n","Decision":"We have decided to move away from cloud.conf as much as possible and instead use it to instantiate\\na masterless puppet agent which will manage the resources.\\n","tokens":109,"id":1372,"text":"## Context\\nWe have the requirement of adding some resources to the base cloud instances. We currently do\\nthis via the [cloud.conf](https:\/\/github.com\/alphagov\/prometheus-aws-configuration\/blob\/375f34600e373aa0e4c66fcae032ceee361d8c21\/terraform\/modules\/prometheus\/cloud.conf) system. This presents us with some limitations, such as configuration\\nbeing limited to 16kb, duplication in each instance terraform and a lack of fast feedback testing.\\n\n\n##Decision\nWe have decided to move away from cloud.conf as much as possible and instead use it to instantiate\\na masterless puppet agent which will manage the resources.\\n","MatchedIds":[2409,3144,2062,856,1464],"Predicted":"## Decision\nWe will transition from the current cloud.conf system to a more scalable and maintainable approach using Terraform modules. The new approach will have the following characteristics:\n\n1. **Modularization**:\n   - Create reusable Terraform modules for commonly used resources. This will reduce duplication and improve maintainability across different cloud instances.\n\n2. **State Management**:\n   - Use a centralized Terraform state backend, such as Terraform Cloud or an S3 bucket with DynamoDB for state locking. This will facilitate easier tracking of resources and prevent conflicts.\n\n3. **Configuration Storage**:\n   - Store configuration in a version-controlled repository such as GitHub, enabling better change tracking and collaboration.\n\n4. **Testing and Validation**:\n   - Integrate Terratest for infrastructure testing. This will allow for fast feedback and ensure that deployments are reliable and error-free.\n\n5. **Scalability**:\n   - Remove the 16kb limitation by eliminating reliance on cloud.conf and instead use Terraform variables and configurations that do not have such limits.\n\n6. **Automation**:\n   - Utilize CI\/CD pipelines to automate testing, validation, and deployment of infrastructure changes. This will minimize manual intervention and increase deployment efficiency.\n\nBy adopting these practices, we will overcome the existing limitations and ensure a more robust, maintainable, and scalable infrastructure management process.","GenTime":"2024-08-28 14:17:59"}
{"File Name":"contact-frontend\/0003-remove-login-redirection.md","Context":"## Context and Problem Statement\\nWithin contact-frontend, for the standalone pages, two routes into the page exist. The first requires a tax service\\nuser to be logged in, and redirects to login if the service user is not logged in. The second, on a different URL\\nsuffixed with \"-unauthenticated\", does not require login, but serves the same page. After discussion, the PlatUI team\\ndecided that the requirement for a user to be logged in was making a worse experience for the service end user, adding\\nthe requirement of login,when all the same functionality is also available without logging in via a different URL.\\n## Decision Drivers\\n* Current functionality with login leads to a possibly bad tax service user journey, in particular in the use case where\\na user is signed out in the background whilst trying to report an issue, given that all this functionality is\\navailable without login\\n* If a tax service user is logged in, but has clicked on a link to the unauthenticated version of the form,\\ncontact-frontend currently doesn't even attempt to look up their enrolments, meaning potentially less information is\\npersisted to Deskpro agents\\n* Requiring login for any of these technical problem report forms makes them less accessible and therefore makes the\\nsite less likely to receive valuable user feedback in particular from users with additional accessibility needs\\n* From a development perspective, maintaining logged-in and non-logged in versions of the pages adds to complexity in\\nthe codebase, making our development process slower and our testing time longer\\n","Decision":"* Current functionality with login leads to a possibly bad tax service user journey, in particular in the use case where\\na user is signed out in the background whilst trying to report an issue, given that all this functionality is\\navailable without login\\n* If a tax service user is logged in, but has clicked on a link to the unauthenticated version of the form,\\ncontact-frontend currently doesn't even attempt to look up their enrolments, meaning potentially less information is\\npersisted to Deskpro agents\\n* Requiring login for any of these technical problem report forms makes them less accessible and therefore makes the\\nsite less likely to receive valuable user feedback in particular from users with additional accessibility needs\\n* From a development perspective, maintaining logged-in and non-logged in versions of the pages adds to complexity in\\nthe codebase, making our development process slower and our testing time longer\\nChosen option: \"Remove login redirect from all pages and keep existing endpoints for now\", because:\\n* The benefits to end users in terms of accessibility feel significant enough to warrant the change\\n* Information about logged in users will still be persisted to Deskpro but there will not be a redirect for users\\nwho have been logged out in the background\\n* Codebase is significantly simplified for developers working on contact-frontend\\n* However, deleting the \"-unauthenticated\" routes will be a major breaking change across the Platform, and should be\\ndecoupled from this\\n### Positive Consequences\\n* Capturing information about all logged in users, not just the ones who have clicked on a contact-frontend route not\\nending in \"-unauthenticated\"\\n* More accessible experience for tax platform end users who are not forced to log in to report technical problems\\n* Significant simplification of codebase for developers\\n### Negative Consequences\\n* There is a risk that without forcing users to log in to particular implementations of the contact-frontend forms,\\nthere will be less information sent to DCST in certain cases. On the other hand, for certain users, the login\\nrequirement might lock them out of the contact-frontend forms completely, e.g. if they are trying to report a problem\\nwith login\\n","tokens":314,"id":1754,"text":"## Context and Problem Statement\\nWithin contact-frontend, for the standalone pages, two routes into the page exist. The first requires a tax service\\nuser to be logged in, and redirects to login if the service user is not logged in. The second, on a different URL\\nsuffixed with \"-unauthenticated\", does not require login, but serves the same page. After discussion, the PlatUI team\\ndecided that the requirement for a user to be logged in was making a worse experience for the service end user, adding\\nthe requirement of login,when all the same functionality is also available without logging in via a different URL.\\n## Decision Drivers\\n* Current functionality with login leads to a possibly bad tax service user journey, in particular in the use case where\\na user is signed out in the background whilst trying to report an issue, given that all this functionality is\\navailable without login\\n* If a tax service user is logged in, but has clicked on a link to the unauthenticated version of the form,\\ncontact-frontend currently doesn't even attempt to look up their enrolments, meaning potentially less information is\\npersisted to Deskpro agents\\n* Requiring login for any of these technical problem report forms makes them less accessible and therefore makes the\\nsite less likely to receive valuable user feedback in particular from users with additional accessibility needs\\n* From a development perspective, maintaining logged-in and non-logged in versions of the pages adds to complexity in\\nthe codebase, making our development process slower and our testing time longer\\n\n\n##Decision\n* Current functionality with login leads to a possibly bad tax service user journey, in particular in the use case where\\na user is signed out in the background whilst trying to report an issue, given that all this functionality is\\navailable without login\\n* If a tax service user is logged in, but has clicked on a link to the unauthenticated version of the form,\\ncontact-frontend currently doesn't even attempt to look up their enrolments, meaning potentially less information is\\npersisted to Deskpro agents\\n* Requiring login for any of these technical problem report forms makes them less accessible and therefore makes the\\nsite less likely to receive valuable user feedback in particular from users with additional accessibility needs\\n* From a development perspective, maintaining logged-in and non-logged in versions of the pages adds to complexity in\\nthe codebase, making our development process slower and our testing time longer\\nChosen option: \"Remove login redirect from all pages and keep existing endpoints for now\", because:\\n* The benefits to end users in terms of accessibility feel significant enough to warrant the change\\n* Information about logged in users will still be persisted to Deskpro but there will not be a redirect for users\\nwho have been logged out in the background\\n* Codebase is significantly simplified for developers working on contact-frontend\\n* However, deleting the \"-unauthenticated\" routes will be a major breaking change across the Platform, and should be\\ndecoupled from this\\n### Positive Consequences\\n* Capturing information about all logged in users, not just the ones who have clicked on a contact-frontend route not\\nending in \"-unauthenticated\"\\n* More accessible experience for tax platform end users who are not forced to log in to report technical problems\\n* Significant simplification of codebase for developers\\n### Negative Consequences\\n* There is a risk that without forcing users to log in to particular implementations of the contact-frontend forms,\\nthere will be less information sent to DCST in certain cases. On the other hand, for certain users, the login\\nrequirement might lock them out of the contact-frontend forms completely, e.g. if they are trying to report a problem\\nwith login\\n","MatchedIds":[560,2405,377,1252,5211],"Predicted":"## Decision\n- Remove the requirement for tax service users to be logged in when accessing any standalone pages in contact-frontend.\n- Consolidate the two routes (authenticated and unauthenticated) into a single, unauthenticated route, ensuring all functionality is available without requiring a login.\n- Enhance the new single route to optionally look up user enrolments if the user is already logged in, ensuring that valuable information is still passed to Deskpro agents if available.\n\n### Positive Consequences\n- Improved user journey: Users will not be interrupted by a login requirement, especially if they are logged out in the background while trying to report an issue.\n- Increased accessibility: More users, including those with additional accessibility needs, will be able to provide feedback without encountering barriers.\n- Simplified codebase: Removing the distinction between authenticated and unauthenticated routes will reduce complexity in the codebase, leading to faster development and testing cycles.\n\n### Negative Consequences\n- Potential security considerations: Some information that could be useful from authenticated sessions will not automatically be available. This impact can be mitigated by allowing optional information lookup if the user is logged in.\n- Possible requirement updates: If future requirements necessitate separating authenticated from unauthenticated interactions, additional refactoring may be needed. However, the benefits support proceeding with this simplification at this point.","GenTime":"2024-08-28 14:18:04"}
{"File Name":"CrossyToad\/adr-0008-use-lenses.md","Context":"## Context\\nBecause we're writing a game we have lots of nested state that we need to update, traverse and\\notherwise manipulate. Querying data is relatively straightforward in vanilla haskell but\\nupdating nested records is a pain.\\nThis is the problem [lens](https:\/\/github.com\/ekmett\/lens) is supposed to help us with.\\nAdditionally by using lenses we have the option to use the \"classy\" lenses style to let\\nindividual subsystems of our game target a subset of our State monad instead of requiring the\\nfull state.\\n","Decision":"It's Lens time!\\n","tokens":118,"id":2480,"text":"## Context\\nBecause we're writing a game we have lots of nested state that we need to update, traverse and\\notherwise manipulate. Querying data is relatively straightforward in vanilla haskell but\\nupdating nested records is a pain.\\nThis is the problem [lens](https:\/\/github.com\/ekmett\/lens) is supposed to help us with.\\nAdditionally by using lenses we have the option to use the \"classy\" lenses style to let\\nindividual subsystems of our game target a subset of our State monad instead of requiring the\\nfull state.\\n\n\n##Decision\nIt's Lens time!\\n","MatchedIds":[2473,1495,1854,4592,4596],"Predicted":"## Decision\nWe will use the `lens` library to manage and manipulate nested state in our game. By leveraging lenses, we can simplify the process of updating deeply nested records. Additionally, we will utilize the \"classy\" lenses style to allow individual subsystems of the game to focus on specific subsets of our State monad, rather than requiring access to the entire state. This approach will improve code readability and maintainability while making our state management more modular and flexible.","GenTime":"2024-08-28 14:18:07"}
{"File Name":"lbh-frontend-react\/0010-use-typedoc-to-generate-api-documentation.md","Context":"## Context\\nWe want to have API documentation for the code we publish. Rather than writing\\nseparate API docs that quickly get out of date, we would like to be able to\\ngenerate it from our code and comments in our code that live next to the thing\\nthey refer to. For JavaScript, the standard is [JSDoc](https:\/\/jsdoc.app\/), but\\nwith TypeScript, we're already defining the types in our code, and duplicating\\nthat in the associated comments is repeated effort and requires manual action to\\nkeep up-to-date.\\n[TypeDoc](https:\/\/typedoc.org\/) is a documentation generator based on JSDoc. It\\nuses a combination of comments and TypeScripts own types to generate API\\ndocumentation automatically.\\n","Decision":"We will use TypeDoc to generate documentation.\\nWe will document all exported code for the benefit of end users.\\nWe will commit the documentation we generate to the repository alongside changes\\nto behaviour.\\n","tokens":159,"id":1410,"text":"## Context\\nWe want to have API documentation for the code we publish. Rather than writing\\nseparate API docs that quickly get out of date, we would like to be able to\\ngenerate it from our code and comments in our code that live next to the thing\\nthey refer to. For JavaScript, the standard is [JSDoc](https:\/\/jsdoc.app\/), but\\nwith TypeScript, we're already defining the types in our code, and duplicating\\nthat in the associated comments is repeated effort and requires manual action to\\nkeep up-to-date.\\n[TypeDoc](https:\/\/typedoc.org\/) is a documentation generator based on JSDoc. It\\nuses a combination of comments and TypeScripts own types to generate API\\ndocumentation automatically.\\n\n\n##Decision\nWe will use TypeDoc to generate documentation.\\nWe will document all exported code for the benefit of end users.\\nWe will commit the documentation we generate to the repository alongside changes\\nto behaviour.\\n","MatchedIds":[3069,2652,1404,4224,1337],"Predicted":"## Decision\nWe decided to use TypeDoc for generating API documentation for our TypeScript codebase. TypeDoc leverages the existing type definitions in the code and combines them with comments, reducing duplication of effort and ensuring that the documentation remains up-to-date with the codebase. This approach will streamline our documentation process and maintain accuracy without requiring additional manual documentation updates.","GenTime":"2024-08-28 14:18:09"}
{"File Name":"alfresco-anaxes-shipyard\/0006-helm-chart-repository.md","Context":"## Context\\nAs part of our efforts around containerized deployment of the Alfresco Digital Business Platform we need to decide on a Helm Chart Repository that Alfresco engineering teams, other internal groups in the organization, and customers and partners can use to publish and consume Helm Charts. When we looked for a solution, we considered that this is going to be a public-facing customer repository. We also took into consideration the need for a custom domain for it.\\nThe criterias we looked for when we investigated each option are: if it uses AWS and Quay, if it is a PaaS\/Cloud Storage solution, if the project is mature enough for our use case, if it is a personal project or if it is maintained by a company and if it requires IT support (if it does can it be avoided by altering the pipeline).  We considered that authentication and SSL is not needed.\\nWe had several options that we considered. The full list of pros and cons for each option can be found [here](https:\/\/issues.alfresco.com\/jira\/secure\/attachment\/97743\/DEPLOY-150%20Helm%20Chart%20Repos.xlsx). More comments on the investigation we have done can be found [here](https:\/\/issues.alfresco.com\/jira\/browse\/DEPLOY-150).\\n","Decision":"We will use Github Pages to store the Helm Charts. The reasons why we elected this solution are: it is a cloud storage solution, it is mature project that has been actively mantained, doesn't require IT support and it offers an easy, well-known pipeline.\\n","tokens":266,"id":3602,"text":"## Context\\nAs part of our efforts around containerized deployment of the Alfresco Digital Business Platform we need to decide on a Helm Chart Repository that Alfresco engineering teams, other internal groups in the organization, and customers and partners can use to publish and consume Helm Charts. When we looked for a solution, we considered that this is going to be a public-facing customer repository. We also took into consideration the need for a custom domain for it.\\nThe criterias we looked for when we investigated each option are: if it uses AWS and Quay, if it is a PaaS\/Cloud Storage solution, if the project is mature enough for our use case, if it is a personal project or if it is maintained by a company and if it requires IT support (if it does can it be avoided by altering the pipeline).  We considered that authentication and SSL is not needed.\\nWe had several options that we considered. The full list of pros and cons for each option can be found [here](https:\/\/issues.alfresco.com\/jira\/secure\/attachment\/97743\/DEPLOY-150%20Helm%20Chart%20Repos.xlsx). More comments on the investigation we have done can be found [here](https:\/\/issues.alfresco.com\/jira\/browse\/DEPLOY-150).\\n\n\n##Decision\nWe will use Github Pages to store the Helm Charts. The reasons why we elected this solution are: it is a cloud storage solution, it is mature project that has been actively mantained, doesn't require IT support and it offers an easy, well-known pipeline.\\n","MatchedIds":[3598,3917,3914,3604,1205],"Predicted":"## Decision\nWe will use [Artifact Hub](https:\/\/artifacthub.io\/) as the Helm Chart Repository for Alfresco engineering teams, internal groups, customers, and partners. Artifact Hub is a mature project maintained by the Cloud Native Computing Foundation (CNCF), minimizing the risk associated with personal or unsupported projects. It allows for public-facing repositories and supports using a custom domain, which aligns with our requirements.\n\nThis decision avoids the need for IT support by leveraging a PaaS\/Cloud Storage solution, enabling automated publishing and consuming via the existing pipeline with minimal alterations. Although authentication and SSL are not needed for our immediate use case, Artifact Hub provides options for enhanced security features if required in the future.\n\nThe detailed investigation and analysis supporting this decision can be accessed in the referenced [DEPLOY-150 Helm Chart Repos.xlsx](https:\/\/issues.alfresco.com\/jira\/secure\/attachment\/97743\/DEPLOY-150%20Helm%20Chart%20Repos.xlsx) document.","GenTime":"2024-08-28 14:18:13"}
{"File Name":"hee-web-blueprint\/0008-use-npm-to-host-hee-frontend-framework.md","Context":"## Context\\nWe need to identify how and where we will store packages that describe our front end framework. There are a number of NHS UK and NHS Digital projects that already use NPM as their package repository for front end code.\\n","Decision":"We have chosen to store our packages on NPM.\\n","tokens":47,"id":1202,"text":"## Context\\nWe need to identify how and where we will store packages that describe our front end framework. There are a number of NHS UK and NHS Digital projects that already use NPM as their package repository for front end code.\\n\n\n##Decision\nWe have chosen to store our packages on NPM.\\n","MatchedIds":[783,3470,3827,3031,36],"Predicted":"## Decision\nWe will store our front end framework packages in the [NPM](https:\/\/www.npmjs.com\/) repository. This decision is influenced by the following reasons:\n\n1. **Consistency**: Using NPM aligns with existing NHS UK and NHS Digital projects, ensuring consistency across various projects.\n2. **Ecosystem Compatibility**: NPM is a well-established ecosystem for JavaScript packages, ensuring compatibility and ease of integration with other tools and libraries commonly used in front-end development.\n3. **Widespread Use**: NPM is widely used and recognized in the development community, which means better support, extensive documentation, and a larger pool of resources and plugins.\n4. **Dependency Management**: NPM offers robust dependency management capabilities, which will help ensure our packages and their dependencies are well managed and up-to-date.\n5. **Versioning and Distribution**: NPM provides efficient versioning and distribution mechanisms which make it easier to publish updates and manage different versions of packages.\n\nBy using NPM, we can leverage existing infrastructure and best practices within our organization while ensuring ease of use and accessibility for developers.","GenTime":"2024-08-28 14:18:17"}
{"File Name":"hospitalrun\/redux_to_react_query.md","Context":"# Migrate to React Query\/Context API\/Hooks from Redux\\n## Context\\nCurrently, HospitalRun Frontend uses [redux](https:\/\/react-redux.js.org\/) to manage async data\\nfetching through [redux thunk](https:\/\/github.com\/reduxjs\/redux-thunk).\\nIt also uses redux for handling the business logic. Redux's main use case is for handling and\\nmanaging shared application state. HospitalRun Frontend has almost no shared state across components\\nsince it always goes back to PouchDB\/CouchDB for the most recent data. Redux code is often verbose and\\ncontains a lot of boilerplate. Although, HospitalRun Frontend uses [redux toolkit](https:\/\/redux-toolkit.js.org\/),\\nthe application still has a lot of boilerplate. Due to the limited amount of global application\\nstate, the desire to reduce boilerplate redux code, and new\/other libraries available,\\nRedux is no longer the correct tool.\\nRedux also makes testing more complicated. Since components that display data (i.e. patient data) are\\nconnected to the redux store, a mock redux store must be provided during tests. This makes it\\ndifficult to isolate testing just to the specific component. For components that save data, it\\nis difficult to mock the actions that are dispatched.\\n[react-query](https:\/\/github.com\/tannerlinsley\/react-query) is a library for \"handling fetching,\\ncaching, and updating asynchronous data in React\". This library has become a popular replacement\\nfor the redux pattern of dispatching an action and storing the result of the data fetch in the redux\\nstore.\\nFor the few uses cases that require global application state (i.e. session information),\\nthe [React Context API](https:\/\/reactjs.org\/docs\/context.html) is an alternative from the\\nReact library. The context API is designed for the use case of sharing state across\\ncomponents.\\n","Decision":"HospitalRun has chosen to use React Query to manage asynchronous requests for fetching data, the\\ncontext api to manage shared application state such as user information, and hooks for sharing\\ncode and business logic.\\n","tokens":396,"id":334,"text":"# Migrate to React Query\/Context API\/Hooks from Redux\\n## Context\\nCurrently, HospitalRun Frontend uses [redux](https:\/\/react-redux.js.org\/) to manage async data\\nfetching through [redux thunk](https:\/\/github.com\/reduxjs\/redux-thunk).\\nIt also uses redux for handling the business logic. Redux's main use case is for handling and\\nmanaging shared application state. HospitalRun Frontend has almost no shared state across components\\nsince it always goes back to PouchDB\/CouchDB for the most recent data. Redux code is often verbose and\\ncontains a lot of boilerplate. Although, HospitalRun Frontend uses [redux toolkit](https:\/\/redux-toolkit.js.org\/),\\nthe application still has a lot of boilerplate. Due to the limited amount of global application\\nstate, the desire to reduce boilerplate redux code, and new\/other libraries available,\\nRedux is no longer the correct tool.\\nRedux also makes testing more complicated. Since components that display data (i.e. patient data) are\\nconnected to the redux store, a mock redux store must be provided during tests. This makes it\\ndifficult to isolate testing just to the specific component. For components that save data, it\\nis difficult to mock the actions that are dispatched.\\n[react-query](https:\/\/github.com\/tannerlinsley\/react-query) is a library for \"handling fetching,\\ncaching, and updating asynchronous data in React\". This library has become a popular replacement\\nfor the redux pattern of dispatching an action and storing the result of the data fetch in the redux\\nstore.\\nFor the few uses cases that require global application state (i.e. session information),\\nthe [React Context API](https:\/\/reactjs.org\/docs\/context.html) is an alternative from the\\nReact library. The context API is designed for the use case of sharing state across\\ncomponents.\\n\n\n##Decision\nHospitalRun has chosen to use React Query to manage asynchronous requests for fetching data, the\\ncontext api to manage shared application state such as user information, and hooks for sharing\\ncode and business logic.\\n","MatchedIds":[2298,2667,4613,387,3035],"Predicted":"## Decision\n\nWe will migrate from Redux to a combination of React Query, React Context API, and React Hooks.\n\n### Reasons for this Decision:\n1. **Reduce Boilerplate Code**:\n   - **Redux**: Although Redux Toolkit minimizes some boilerplate, it still requires considerable setup and repetitive code patterns, especially for asynchronous data fetching.\n   - **React Query**: Provides built-in functionalities for fetching, caching, and synchronizing asynchronous data with significantly less boilerplate code.\n\n2. **Minimal Shared State**:\n   - The application relies on frequent data retrieval from PouchDB\/CouchDB, meaning there's minimal need for a global shared state.\n   - **React Context API**: Ideal for managing the limited global state such as session information.\n\n3. **Improved Testing**:\n   - **Redux**: Makes testing challenging by requiring a mock Redux store and complicating the isolation of component tests.\n   - **React Query & Context API**: Simplifies the testing process by easily isolating components and mocking responses.\n\n4. **Optimized Data Management**:\n   - **React Query**: Efficiently handles data fetching, caching, and synchronization, reducing the need for custom solutions and manual state management.\n\n5. **Simplified Codebase**:\n   - By using **React Hooks** and **React Query**, our codebase becomes more maintainable and easier to understand, leading to faster development and fewer errors.\n\n6. **Seamless Integration**:\n   - **React Query**: Integrates well with React components and hooks, enhancing the developer experience and productivity.\n\n### Implementation Steps:\n1. **Initial Assessment**:\n   - Identify and list all current Redux actions, reducers, and thunks.\n   - Assess which parts of the state can be moved to React Query or Context API.\n\n2. **Migration Plan**:\n   - For asynchronous data fetching:\n     - Replace Redux Thunk actions with React Query hooks.\n   - For global state:\n     - Implement React Context for managing limited global state (e.g., session data).\n\n3. **Component Refactoring**:\n   - Update components to use React Query for data fetching.\n   - Use Context API for accessing global state where necessary.\n   - Replace Redux selectors with direct state access or hooks.\n\n4. **Testing**:\n   - Refactor existing tests to accommodate the changes.\n   - Ensure all new components and hooks are isolated and easily testable.\n   - Mock data flows for components using React Query and Context API.\n\n5. **Documentation & Training**:\n   - Update documentation to reflect new patterns and best practices.\n   - Conduct training sessions for the development team to get familiar with React Query, Context API, and Hooks.\n\n### Monitoring and Follow-Up:\n- After the migration, closely monitor the application for any issues or performance degradation.\n- Collect feedback from the development team and make necessary adjustments.\n- Periodically review the implementation to ensure it aligns with evolving best practices and project needs.\n\nBy adopting React Query, Context API, and Hooks, we aim to create a more maintainable, testable, and efficient codebase that meets the current needs of HospitalRun Frontend.","GenTime":"2024-08-28 14:18:26"}
{"File Name":"paas-team-manual\/ADR017-cell-capacity-assignment.md","Context":"Context\\n=======\\nWe want to ensure our platform remains available when a single AZ fails. This means that we need to have enough spare memory capacity left on cells to cover deploying apps from the failed zone. In case of 3 zones, that means each zone should be able to host 50% more apps (memory capacity wise). We can calculate maximum memory usable by all orgs by doing sum of their quotas. However, in practice much less memory is consumed. This is because\\n1. Org quotas come in T-shirt sizes and have considerable size jumps (e.g. 2, 10, 60 100G). You need to reserve next quota if previous one is too small for your needs, yet it doesn't mean you will be using all the capacity of the larger quota.\\n1. App instance memory limits are set as upper memory consumption limit. Because of that, they tend to be set larger for safety. Actual app memory consumption is always lower, many times considerably.\\nPractical example - this is a snapsot of our prod deployment in Feb 2017:\\n```\\nMemory reserved by orgs: 368640 MB (360 GB)\\nMemory reserved by apps: 107108 MB (104 GB)\\nMemory actually used by apps: 32868 (32 GB)\\n```\\nThis is not unusual and CF v1 had default overprovisioning factor of 2 (that is, it advertised 2 times more capacity than actual).\\nDecision\\n========\\nWe will maintain at least 50% of total org reserved capacity available when a zone fails. That is, remaining zones will have to be able to cover 50% of total reserved capacity.\\nStatus\\n======\\nAccepted\\nConsequences\\n============\\n* We will check if we have enough capacity available whenever we add a new organisation or increase quota of existing one. We will deploy more cells if we need more capacity.\\n* We have implemented `show-cf-memory-usage` makefile target to help us get current org and app reservation and real usage totals.\\n","Decision":"========\\nWe will maintain at least 50% of total org reserved capacity available when a zone fails. That is, remaining zones will have to be able to cover 50% of total reserved capacity.\\nStatus\\n======\\nAccepted\\nConsequences\\n============\\n* We will check if we have enough capacity available whenever we add a new organisation or increase quota of existing one. We will deploy more cells if we need more capacity.\\n* We have implemented `show-cf-memory-usage` makefile target to help us get current org and app reservation and real usage totals.\\n","tokens":428,"id":231,"text":"Context\\n=======\\nWe want to ensure our platform remains available when a single AZ fails. This means that we need to have enough spare memory capacity left on cells to cover deploying apps from the failed zone. In case of 3 zones, that means each zone should be able to host 50% more apps (memory capacity wise). We can calculate maximum memory usable by all orgs by doing sum of their quotas. However, in practice much less memory is consumed. This is because\\n1. Org quotas come in T-shirt sizes and have considerable size jumps (e.g. 2, 10, 60 100G). You need to reserve next quota if previous one is too small for your needs, yet it doesn't mean you will be using all the capacity of the larger quota.\\n1. App instance memory limits are set as upper memory consumption limit. Because of that, they tend to be set larger for safety. Actual app memory consumption is always lower, many times considerably.\\nPractical example - this is a snapsot of our prod deployment in Feb 2017:\\n```\\nMemory reserved by orgs: 368640 MB (360 GB)\\nMemory reserved by apps: 107108 MB (104 GB)\\nMemory actually used by apps: 32868 (32 GB)\\n```\\nThis is not unusual and CF v1 had default overprovisioning factor of 2 (that is, it advertised 2 times more capacity than actual).\\nDecision\\n========\\nWe will maintain at least 50% of total org reserved capacity available when a zone fails. That is, remaining zones will have to be able to cover 50% of total reserved capacity.\\nStatus\\n======\\nAccepted\\nConsequences\\n============\\n* We will check if we have enough capacity available whenever we add a new organisation or increase quota of existing one. We will deploy more cells if we need more capacity.\\n* We have implemented `show-cf-memory-usage` makefile target to help us get current org and app reservation and real usage totals.\\n\n\n##Decision\n========\\nWe will maintain at least 50% of total org reserved capacity available when a zone fails. That is, remaining zones will have to be able to cover 50% of total reserved capacity.\\nStatus\\n======\\nAccepted\\nConsequences\\n============\\n* We will check if we have enough capacity available whenever we add a new organisation or increase quota of existing one. We will deploy more cells if we need more capacity.\\n* We have implemented `show-cf-memory-usage` makefile target to help us get current org and app reservation and real usage totals.\\n","MatchedIds":[215,3915,216,3488,3789],"Predicted":"## Decision\nWe will maintain at least 50% of total org reserved capacity available when a zone fails. That is, remaining zones will have to be able to cover 50% of total reserved capacity.\n\nStatus\n======\nAccepted\n\nConsequences\n============\n* We will check if we have enough capacity available whenever we add a new organisation or increase quota of existing one. We will deploy more cells if we need more capacity.\n* We have implemented `show-cf-memory-usage` makefile target to help us get current org and app reservation and real usage totals.\n* This approach will ensure that our platform remains resilient and available even in the event of a single AZ failure.\n* We may incur additional costs in maintaining spare capacity, but this is necessary for meeting our availability requirements.\n* Continuous monitoring and adjusting of capacity will be required to align with changes in organizational quotas and actual memory usage trends.","GenTime":"2024-08-28 14:18:32"}
{"File Name":"paas-team-manual\/ADR020-deletion_of_ci_environment.html.md","Context":"## Context\\nWe have three environments in our deployment pipeline. Two non-production ones - CI and Staging and one Production. We think that it takes to much time for a change to reach production state in the current setup. We don't think having two environments before production is providing us much value, compared to the cost of running, maintaining, and waiting for deployments to be promoted.\\n","Decision":"We will delete CI environment and migrate it's customizations, like tests , apps etc. to staging. We have decided to delete CI instead of staging as we want to separate build CI in it's own AWS account. Also, staging environment has valid certificates.\\n","tokens":78,"id":187,"text":"## Context\\nWe have three environments in our deployment pipeline. Two non-production ones - CI and Staging and one Production. We think that it takes to much time for a change to reach production state in the current setup. We don't think having two environments before production is providing us much value, compared to the cost of running, maintaining, and waiting for deployments to be promoted.\\n\n\n##Decision\nWe will delete CI environment and migrate it's customizations, like tests , apps etc. to staging. We have decided to delete CI instead of staging as we want to separate build CI in it's own AWS account. Also, staging environment has valid certificates.\\n","MatchedIds":[351,1739,3789,2471,856],"Predicted":"## Decision\nConsolidate the CI and Staging environments into a single pre-production environment, named \"Test.\" The Test environment will serve as the sole non-production environment for validating changes before they are promoted to Production. This consolidation aims to reduce deployment times, maintenance efforts, and operational costs.\n\n### Details:\n1. **Test Environment Setup:**\n   - Migrate all existing CI and Staging environment configurations, test scripts, and data to the new Test environment.\n   - Ensure that the Test environment mirrors the Production environment as closely as possible to maintain high confidence in tests.\n\n2. **Deployment Pipeline Changes:**\n   - Update the deployment pipeline to remove the separate CI and Staging stages.\n   - Implement a streamlined pipeline with only Test and Production stages.\n\n3. **Automated Testing:**\n   - Integrate both unit and integration tests in the Test environment to validate changes comprehensively.\n   - Perform automated regression tests to ensure new changes do not break existing functionality.\n\n4. **Promotion to Production:**\n   - Only changes that pass all tests in the Test environment will be promoted to Production.\n   - Implement a manual approval step before final promotion to Production for added security.\n\n5. **Monitoring and Rollback:**\n   - Set up monitoring and alerting mechanisms in the Test environment to detect issues early.\n   - Ensure that rollback procedures are in place and tested for both Test and Production environments to handle any unforeseen issues.\n\nThis decision is expected to reduce the overall time and resources required for a change to reach Production while maintaining the integrity and reliability of the deployment process.","GenTime":"2024-08-28 14:18:37"}
{"File Name":"katas\/003-katafication.md","Context":"## Context\\nAll katas in here are supposed to be used to fix failing tests.\\nThe problem with storing failing tests (the katas) in the repo is that there is no\\nway but fixing them each time to find out if they actually work or the engine\\nthe kata is run on supports a certain feature.\\n","Decision":"All tests have to pass and the special comment `\/\/\/\/` marks how to katafy\\nthis test.\\nFor example:\\n```\\n\/\/\/\/ const expectedType = 'not a function!';\\nconst expectedType = 'object';\\nassert.equal(typeof Reflect, expectedType);\\n```\\nThe above test passes. Katafy means uncomment the first line and remove the following line, so\\nthat the code looks like this afterwards:\\n```\\nconst expectedType = 'not a function!';\\nassert.equal(typeof Reflect, expectedType);\\n```\\nNow this test fails, and it can be used as a kata.\\n","tokens":66,"id":4338,"text":"## Context\\nAll katas in here are supposed to be used to fix failing tests.\\nThe problem with storing failing tests (the katas) in the repo is that there is no\\nway but fixing them each time to find out if they actually work or the engine\\nthe kata is run on supports a certain feature.\\n\n\n##Decision\nAll tests have to pass and the special comment `\/\/\/\/` marks how to katafy\\nthis test.\\nFor example:\\n```\\n\/\/\/\/ const expectedType = 'not a function!';\\nconst expectedType = 'object';\\nassert.equal(typeof Reflect, expectedType);\\n```\\nThe above test passes. Katafy means uncomment the first line and remove the following line, so\\nthat the code looks like this afterwards:\\n```\\nconst expectedType = 'not a function!';\\nassert.equal(typeof Reflect, expectedType);\\n```\\nNow this test fails, and it can be used as a kata.\\n","MatchedIds":[4336,1387,1258,4179,4868],"Predicted":"## Decision\nImplement a continuous integration (CI) system that automatically runs the katas on different supported engines. This way, we can verify if the katas work and if the engines support the required features without manually fixing the tests each time. Using a CI system will ensure any incompatibility or failure is promptly identified and can be addressed systematically.","GenTime":"2024-08-28 14:18:40"}
{"File Name":"mediawiki-extensions-WikibaseLexeme\/0003-adr-with-all-team-members.md","Context":"## Context\\nWe use architectural decision records (ADR) on this project.\\nThe project's code base is owned by the team, the team organizes itself (see Journey model) into smaller, short lived \"journey\" sub-units to - amongst others - increase focus. Architecture decisions affect the project for a long time, and will likely soon be faced by developers that were not part of the journey at the time. Consequently their feedback about the architectural decision is inevitable. Additionally, given the intentionally small size of an individual journey's group, the amount of opinions concerning any given ADR could be as small as one or two, should the ADR be voted upon by members of the same journey exclusively. To avoid a flood of ADRs trying to unwrite each other and to increase the standing of ADRs in general and the quality of the individual ADR they should be vetted (RFC) by the entire team.\\nTo avoid long-running feedback loops that block the individual journey team from fulfilling their commitments RFCs should be time-limited.\\n","Decision":"We put proposed Architecture Decision Records up for feedback by the entire team. ADRs will be proposed as dedicated changes and iterated upon through the tools provided by our code review system. ADR RFCs will have a deadline; it should be no sooner than one working day after initial request.\\n","tokens":214,"id":1563,"text":"## Context\\nWe use architectural decision records (ADR) on this project.\\nThe project's code base is owned by the team, the team organizes itself (see Journey model) into smaller, short lived \"journey\" sub-units to - amongst others - increase focus. Architecture decisions affect the project for a long time, and will likely soon be faced by developers that were not part of the journey at the time. Consequently their feedback about the architectural decision is inevitable. Additionally, given the intentionally small size of an individual journey's group, the amount of opinions concerning any given ADR could be as small as one or two, should the ADR be voted upon by members of the same journey exclusively. To avoid a flood of ADRs trying to unwrite each other and to increase the standing of ADRs in general and the quality of the individual ADR they should be vetted (RFC) by the entire team.\\nTo avoid long-running feedback loops that block the individual journey team from fulfilling their commitments RFCs should be time-limited.\\n\n\n##Decision\nWe put proposed Architecture Decision Records up for feedback by the entire team. ADRs will be proposed as dedicated changes and iterated upon through the tools provided by our code review system. ADR RFCs will have a deadline; it should be no sooner than one working day after initial request.\\n","MatchedIds":[1669,2091,2261,4361,4428],"Predicted":"## Decision\nWe will use a Request For Comments (RFC) process for vetting Architecture Decision Records (ADRs). \n\n- **Time-limited RFCs:** Each RFC will have a strict time limit for feedback to ensure that the process does not block the journey team from meeting their commitments. \n- **Team-wide Review:** ADRs will be reviewed by the entire team, not just the journey sub-unit, to ensure the quality and stability of the decisions and to avoid a flood of contradictory ADRs.\n- **Documentation:** The feedback and final decisions after the RFC period will be documented within the ADR to maintain a clear historical record.\n\nThis approach ensures comprehensive feedback and maintains the quality and durability of architectural decisions across the project.","GenTime":"2024-08-28 14:18:43"}
{"File Name":"claim-additional-payments-for-teaching\/0004-deployment-on-heroku.md","Context":"## Context\\nDepartment for Education have a Cloud Infrastructure Program based on Azure that\\nthey would like digital services to use. Access to Azure is heavily restricted\\nfor production, and slightly restricted for lower environments.\\nWe need to be able to work quickly, particularly in the early stages of this\\nproject.\\nWe need to be able to deploy prototypes and experimental features and versions\\nof the service for user research.\\n","Decision":"We will use Heroku to deploy the application.\\nWe will use Heroku's pipeline feature to run CI and deploy the application.\\n","tokens":84,"id":2093,"text":"## Context\\nDepartment for Education have a Cloud Infrastructure Program based on Azure that\\nthey would like digital services to use. Access to Azure is heavily restricted\\nfor production, and slightly restricted for lower environments.\\nWe need to be able to work quickly, particularly in the early stages of this\\nproject.\\nWe need to be able to deploy prototypes and experimental features and versions\\nof the service for user research.\\n\n\n##Decision\nWe will use Heroku to deploy the application.\\nWe will use Heroku's pipeline feature to run CI and deploy the application.\\n","MatchedIds":[2410,920,4041,1182,4056],"Predicted":"## Decision\nTo facilitate rapid development and experimentation, we will use a hybrid approach for our cloud infrastructure. For production environments, we adhere to the Department for Education's Cloud Infrastructure Program on Azure. However, to enable rapid prototyping, deployment of experimental features, and user research:\n\n1. **Development and Staging Environments:**\n   - We will use [Gov.UK PaaS](https:\/\/www.cloud.service.gov.uk\/) for lower environments such as development and staging. This allows us to take advantage of its simplified DevOps, flexible development tools, and fast application delivery cycles.\n   - Gov.UK PaaS is a fully managed platform service that provides fast application delivery, integrated data services, and seamless scalability, which aligns well with our need for rapid iterations and testing.\n\n2. **Production Environment:**\n   - We will deploy production workloads on Azure as per the Department for Education's Cloud Infrastructure Program.\n   - Using Azure for production ensures compliance with security and governance requirements.\n\n3. **Integration and Testing:**\n   - Utilize a CI\/CD pipeline to ensure continuous integration and consistent deployment across both Gov.UK PaaS (for rapid deployment stages) and Azure (for production readiness).\n   - This approach allows us to maintain productivity and flexibility during the early stages while ensuring compliance and security for production deployments.\n\nBy adopting this hybrid model, we can balance the need for rapid development and experimentation with the stringent compliance and security requirements of the production environment. This strategy will enable us to move quickly and effectively during the project's early phases while ensuring a seamless transition to Azure for production.","GenTime":"2024-08-28 14:18:47"}
{"File Name":"open-apparel-registry\/adr-002-decide-how-to-display-more-facilities.md","Context":"## Context\\nThe Open Apparel Registry currently includes more than 18,000 facilities. For\\nperformance reasons, we have paginated the facilities data API endpoint data so\\nthat it will [return a maximum of 500 results][pagination-pr] for any single\\nrequest. In turn this means that the frontend client will only ever display a\\nmaximum of 500 facilities at a time, rendered as clustered Leaflet markers via\\nReact-Leaflet. Facilities API requests are currently filtered using Django\\nquerysets whose inputs are querystring parameters included in the API requests.\\nTo enable users to view all of the OAR's facilities on the map simultaneously,\\nwe'll need to update how the API returns facilities for display and how the\\nclient renders them on the map. At present this means updating the application\\nso that it can display 18,000+ facilities simultaneously. Following upcoming MSI\\nintegration work, we anticipate that the number of OAR facilities will increase\\nto around 100,000 -- which the application should be able to map. In addition,\\nwe also want users to be able to filter these vector tiles by query parameters\\nlike contributor, facility name, and country, along with the map bounding box.\\nTo accomplish this we have decided to use vector tiles generated, ultimately,\\nby PostGIS's [`ST_AsMVT`][st-asmvt] function, rendering them in the frontend\\nwith [Leaflet Vector Grid][leaflet-vector-grid] (possibly via\\n[react-leaflet-vector-grid][react-leaflet-vector-grid]). We've decided to have\\nthe vector tiles cluster facilities by zoom level, which would limit the number\\nof actual points the frontend needs to display at any given time.\\nThis ADR documents a subsequent decision between setting up a dedicated\\n`ST_AsMVT`-based vector tile server, like [Martin][martin] or adding a new\\nvector tile endpoint to the existing Django web application which would make\\nthe `ST_AsMVT` query.\\n","Decision":"We have decided to add a vector tile endpoint to the existing Django app.\\nWhile Martin, in particular, seemed like a compelling solution, we had enough\\nopen questions about it to discourage us from taking on the complexity of\\nusing it here.\\nOur main apprehension about adding a tile endpoint to the existing web app is\\nthat it'll mingle tile requests with other requests in a way that could cause\\nperformance problems. However, given the size of the OAR's traffic and the\\npossibility of addressing traffic increases by scaling the number of app\\ninstances, this seemed like an acceptable tradeoff.\\n","tokens":412,"id":4281,"text":"## Context\\nThe Open Apparel Registry currently includes more than 18,000 facilities. For\\nperformance reasons, we have paginated the facilities data API endpoint data so\\nthat it will [return a maximum of 500 results][pagination-pr] for any single\\nrequest. In turn this means that the frontend client will only ever display a\\nmaximum of 500 facilities at a time, rendered as clustered Leaflet markers via\\nReact-Leaflet. Facilities API requests are currently filtered using Django\\nquerysets whose inputs are querystring parameters included in the API requests.\\nTo enable users to view all of the OAR's facilities on the map simultaneously,\\nwe'll need to update how the API returns facilities for display and how the\\nclient renders them on the map. At present this means updating the application\\nso that it can display 18,000+ facilities simultaneously. Following upcoming MSI\\nintegration work, we anticipate that the number of OAR facilities will increase\\nto around 100,000 -- which the application should be able to map. In addition,\\nwe also want users to be able to filter these vector tiles by query parameters\\nlike contributor, facility name, and country, along with the map bounding box.\\nTo accomplish this we have decided to use vector tiles generated, ultimately,\\nby PostGIS's [`ST_AsMVT`][st-asmvt] function, rendering them in the frontend\\nwith [Leaflet Vector Grid][leaflet-vector-grid] (possibly via\\n[react-leaflet-vector-grid][react-leaflet-vector-grid]). We've decided to have\\nthe vector tiles cluster facilities by zoom level, which would limit the number\\nof actual points the frontend needs to display at any given time.\\nThis ADR documents a subsequent decision between setting up a dedicated\\n`ST_AsMVT`-based vector tile server, like [Martin][martin] or adding a new\\nvector tile endpoint to the existing Django web application which would make\\nthe `ST_AsMVT` query.\\n\n\n##Decision\nWe have decided to add a vector tile endpoint to the existing Django app.\\nWhile Martin, in particular, seemed like a compelling solution, we had enough\\nopen questions about it to discourage us from taking on the complexity of\\nusing it here.\\nOur main apprehension about adding a tile endpoint to the existing web app is\\nthat it'll mingle tile requests with other requests in a way that could cause\\nperformance problems. However, given the size of the OAR's traffic and the\\npossibility of addressing traffic increases by scaling the number of app\\ninstances, this seemed like an acceptable tradeoff.\\n","MatchedIds":[3286,2578,1688,387,1701],"Predicted":"## Decision\nWe will opt for adding a new vector tile endpoint to the existing Django web application which would make the `ST_AsMVT` query. This decision is motivated by several key factors:\n\n1. **Simplicity and Maintainability**: By integrating the vector tile generation directly into our Django application, we reduce the complexity of our overall architecture. We leverage our existing infrastructure and avoid the overhead of managing an additional service. This integration will simplify deployment and maintenance processes, as all infrastructure changes remain centralized within the Django framework.\n\n2. **Development Agility**: Development and testing will be more straightforward since our team is already familiar with Django. This can enable quick iterations and refinements of the vector tile endpoint as new requirements or optimizations emerge. It also allows for easier integration with the existing authentication and filtering mechanisms already present in our Django application.\n\n3. **Resource Utilization**: By utilizing the existing Django web servers to handle the vector tile generation, we make efficient use of our current resources. This avoids introducing new overhead that would come with running a dedicated tile server, potentially saving on infrastructure costs and resource allocation.\n\n4. **Scalability**: The Django application can scale horizontally if required, allowing us to handle increased load as the number of facilities grows. With proper optimizations on the database and application level, this approach will be scalable enough to meet the demand of rendering up to 100,000 facilities.\n\n5. **Flexibility**: Implementing the `ST_AsMVT`-based endpoint within Django offers flexibility to easily incorporate complex filtering criteria directly within our existing querysets. By centralizing our filtering and tile generation logic within Django, we ensure that the behavior is consistent and easily manageable.\n\nBy taking this approach, we position ourselves to deliver an efficient, maintainable, and scalable solution for rendering large numbers of facilities on the map, while leveraging our existing stack and expertise.","GenTime":"2024-08-28 14:18:53"}
{"File Name":"kitsune\/0002-es-l10n-content.md","Context":"## Context\\nKitsune supports many locales,\\nand has content which we want to be searchable in those locales.\\nElasticsearch has support for many language-specific analyzers:\\nhttps:\/\/www.elastic.co\/guide\/en\/elasticsearch\/reference\/7.9\/analysis-lang-analyzer.html\\nSearch v1 used per-document analyzers,\\nthat is to say, within the same index:\\n```\\ndoc_1: { \"content\": \"Hello world\" }\\ndoc_2: { \"content\": \"Hallo Welt\" }\\n```\\n`doc_1.content` could be analyzed using an english analyzer,\\nand `doc_2.content` could be analyzed using a german analyzer.\\nWell before version 7 ES removed this feature,\\nand now all fields of the same name across an index must be analyzed the same,\\nso we must take a different approach with the current Search implementation.\\nWe can either place separate locales in their own index,\\nand set up locale-specific analyzers for the same field name across indices.\\nOr we can keep separate locales within the same index,\\nand define unique field names for each field which needs to be analyzed under a specific locale.\\n","Decision":"Heavily influenced by: https:\/\/www.elastic.co\/blog\/multilingual-search-using-language-identification-in-elasticsearch\\nWe will store all documents within the same index and use an Object field for fields which need to use locale-specific analyzers.\\nWe will call this field `SumoLocaleAwareTextField` and will have a key for each locale,\\nwith the appropriate analyzer defined on that key,\\nsuch that:\\n```\\ndoc_1: { \"content\": { \"en-US\": \"Hello world\" }}\\ndoc_2: { \"content\": { \"de\": \"Hallo Welt\" }}\\n```\\n`doc_1.content.en-US` is analyzed using an english analyzer,\\nand `doc_2.content.de` is analyzed using a german analyzer.\\n","tokens":249,"id":102,"text":"## Context\\nKitsune supports many locales,\\nand has content which we want to be searchable in those locales.\\nElasticsearch has support for many language-specific analyzers:\\nhttps:\/\/www.elastic.co\/guide\/en\/elasticsearch\/reference\/7.9\/analysis-lang-analyzer.html\\nSearch v1 used per-document analyzers,\\nthat is to say, within the same index:\\n```\\ndoc_1: { \"content\": \"Hello world\" }\\ndoc_2: { \"content\": \"Hallo Welt\" }\\n```\\n`doc_1.content` could be analyzed using an english analyzer,\\nand `doc_2.content` could be analyzed using a german analyzer.\\nWell before version 7 ES removed this feature,\\nand now all fields of the same name across an index must be analyzed the same,\\nso we must take a different approach with the current Search implementation.\\nWe can either place separate locales in their own index,\\nand set up locale-specific analyzers for the same field name across indices.\\nOr we can keep separate locales within the same index,\\nand define unique field names for each field which needs to be analyzed under a specific locale.\\n\n\n##Decision\nHeavily influenced by: https:\/\/www.elastic.co\/blog\/multilingual-search-using-language-identification-in-elasticsearch\\nWe will store all documents within the same index and use an Object field for fields which need to use locale-specific analyzers.\\nWe will call this field `SumoLocaleAwareTextField` and will have a key for each locale,\\nwith the appropriate analyzer defined on that key,\\nsuch that:\\n```\\ndoc_1: { \"content\": { \"en-US\": \"Hello world\" }}\\ndoc_2: { \"content\": { \"de\": \"Hallo Welt\" }}\\n```\\n`doc_1.content.en-US` is analyzed using an english analyzer,\\nand `doc_2.content.de` is analyzed using a german analyzer.\\n","MatchedIds":[475,2513,5123,3126,4378],"Predicted":"## Decision\nGiven the requirement to support many locales and the limitations introduced in Elasticsearch version 7, we will separate locales into their own indices and set up locale-specific analyzers for the same field name across those indices.\n\n### Rationale\n1. **Maintainability and Simplicity**:\n   - Separating locales into their own indices will allow us to use the same field names across different indices, each with its own language-specific analyzer.\n   - This approach avoids the complexity of managing unique field names for each locale within a single index, which can become cumbersome as the number of supported locales grows.\n\n2. **Performance**:\n   - Having smaller, localized indices can improve query performance because each index would be optimized for a specific language's analysis and indexing peculiarities.\n   - It can also help with more efficient resource usage, as different language indices can be allocated to the appropriate nodes based on resource availability and usage patterns.\n\n3. **Elastic Search Limitations**:\n   - Since Elasticsearch no longer supports per-document analyzers within the same index, creating separate indices for each locale respects this constraint and leverages Elasticsearch's capabilities most effectively.\n\n4. **Flexibility**:\n   - This approach provides flexibility in scaling and optimizing each locale-specific index independently. For example, we can introduce specific custom analyzers or stop words that make sense for a particular locale without affecting other locales.\n\n### Positive Consequences\n- **Cleaner and Easier to Reason About Index Structure**: Each document is associated with an index specifically designed for its locale.\n- **Improved Search Relevance**: Locale-specific analyzers will improve the relevance of search results for each corresponding locale.\n- **Scalability**: The number of documents or content specific to a locale can be managed more predictably within separate indices.\n\n### Negative Consequences\n- **Increased Infrastructure Complexity**: Managing multiple indices necessitates careful monitoring and resource allocation.\n- **Potential for Increased Storage**: Separate indices might lead to some redundancy in terms of storage needs.\n\nOverall, separating locales into their own indices is the most effective and maintainable approach given our multilingual search requirements and the constraints of Elasticsearch version 7.","GenTime":"2024-08-28 14:18:59"}
{"File Name":"architecture\/0004-waiting-is-waste.md","Context":"## Context\\nHaving a developer waiting for any process to finish is a stupid thing. It's not only that it's to long for the developer to wait, it also hinders you from fixing issues fast.\\n","Decision":"Any processes and workflows need to be very fast and fully automated.\\n","tokens":43,"id":1434,"text":"## Context\\nHaving a developer waiting for any process to finish is a stupid thing. It's not only that it's to long for the developer to wait, it also hinders you from fixing issues fast.\\n\n\n##Decision\nAny processes and workflows need to be very fast and fully automated.\\n","MatchedIds":[4868,672,3722,187,4416],"Predicted":"## Decision\nWe will implement asynchronous job processing for long-running tasks using a queuing system like RabbitMQ or AWS SQS. By offloading these tasks to background processes, developers can continue working without waiting for these jobs to complete. Additionally, we will provide real-time notifications upon job completion, which will facilitate immediate issue resolution when necessary.","GenTime":"2024-08-28 14:19:01"}
{"File Name":"terraform-aws-dynamodb\/0007-map-list-variables.md","Context":"## Context\\nDynamoDB provides the ability to supply additional attributes, a local\\nsecondary index and a global secondary index. These additional attributes\\nconsumed by the DynamoDB AWS Provider as maps\/lists.\\nIn addtion to the consumption as maps\/lists there are additional requirements\\nthat the range\/hash keys be added to the additional attributes if declared. They\\nare not added if undeclared.\\n","Decision":"The module will use a `null_resource` to manage the secondary indexes. Creating\\nthem from the existences of appropriate maps\/lists.\\nProperties related to these resources will consume a list of maps as input.\\nThese will them be mapped to the appropriate resource within in the module.\\nThe range\/hash key will be added automatically to the additional attributes by\\nthe module, reducing the load on the consumer with implementation detail.\\n","tokens":80,"id":2543,"text":"## Context\\nDynamoDB provides the ability to supply additional attributes, a local\\nsecondary index and a global secondary index. These additional attributes\\nconsumed by the DynamoDB AWS Provider as maps\/lists.\\nIn addtion to the consumption as maps\/lists there are additional requirements\\nthat the range\/hash keys be added to the additional attributes if declared. They\\nare not added if undeclared.\\n\n\n##Decision\nThe module will use a `null_resource` to manage the secondary indexes. Creating\\nthem from the existences of appropriate maps\/lists.\\nProperties related to these resources will consume a list of maps as input.\\nThese will them be mapped to the appropriate resource within in the module.\\nThe range\/hash key will be added automatically to the additional attributes by\\nthe module, reducing the load on the consumer with implementation detail.\\n","MatchedIds":[1990,3192,2538,1372,485],"Predicted":"## Decision\nWe will ensure that if range\/hash keys are declared in the additional attributes section, they will automatically be included in the map\/list consumed by the DynamoDB AWS Provider. If the range\/hash keys are undeclared, they will not be added to the additional attributes. This will maintain consistency with DynamoDB's handling of these keys and reduce potential errors in key management.","GenTime":"2024-08-28 14:19:03"}
{"File Name":"k8s-vagrant-centos-cluster\/0002-why-not-use-kubeadm.md","Context":"## Context\\nkubeadm can be used to setup Kubernetes cluster with apiserver, etcd, controller, scheduler...\\n","Decision":"Setup Kubernetes cluster with apiserver, etcd, controller, scheduler without using kubeadm and docker\\n","tokens":26,"id":3440,"text":"## Context\\nkubeadm can be used to setup Kubernetes cluster with apiserver, etcd, controller, scheduler...\\n\n\n##Decision\nSetup Kubernetes cluster with apiserver, etcd, controller, scheduler without using kubeadm and docker\\n","MatchedIds":[3890,2236,3600,2997,771],"Predicted":"## Decision\nWe will use kubeadm as the tool to set up our Kubernetes cluster. Kubeadm provides a streamlined and modular approach to deploying Kubernetes clusters by automating the installation and configuration of core cluster components such as the API server, etcd, controller manager, and scheduler. It is well-documented and supported by the Kubernetes community, making it a suitable choice for both development and production environments.","GenTime":"2024-08-28 14:19:05"}
{"File Name":"csc-swr-architecture\/004-Use-Fake-Data.html.md.erb","Context":"## Context\\nThe Childrens Social Care Placement Service is in the [Alpha] phase of delivery. As such it is not expected to process real data for users, more to the point, it is an exploratory phase for building prototypes and testing different ideas.\\n","Decision":"The placement alpha prototypes and development work will only process fake data.\\nThe reasoning for this is to allow the Service Team to focus on developing features in fast iteration feedback loops, rather than undertaking the additional non-functional needs for processing live data. The Security needs, including passing the \"Authority to Operate\" governance gateway alone, would jeopardise the desired delivery timeframe for Alpha of January 2021.\\n","tokens":53,"id":915,"text":"## Context\\nThe Childrens Social Care Placement Service is in the [Alpha] phase of delivery. As such it is not expected to process real data for users, more to the point, it is an exploratory phase for building prototypes and testing different ideas.\\n\n\n##Decision\nThe placement alpha prototypes and development work will only process fake data.\\nThe reasoning for this is to allow the Service Team to focus on developing features in fast iteration feedback loops, rather than undertaking the additional non-functional needs for processing live data. The Security needs, including passing the \"Authority to Operate\" governance gateway alone, would jeopardise the desired delivery timeframe for Alpha of January 2021.\\n","MatchedIds":[920,914,2093,581,4454],"Predicted":"## Decision\nWe will use the Gov.UK Notify service for sending emails and text messages.\n\n### Justification:\n1. **Ease of Use**: Gov.UK Notify is designed to be easy to integrate with minimal setup, allowing developers to focus on building and testing core functionalities quickly.\n2. **Government Accreditation**: Being a government-approved service, it provides a level of security and compliance that reduces the overhead of considering approval from security teams.\n3. **Cost-Effective**: During the Alpha phase, where budget constraints are often a concern, Gov.UK Notify offers a cost-effective solution for sending notifications without a significant financial outlay.\n4. **Scalability**: Though we are in an Alpha phase, using a service that can scale as we move through Beta and live phases means we can smoothly transition without overhauling our notification system.\n5. **Support for Multiple Channels**: Gov.UK Notify supports both email and SMS, allowing for flexibility in how we communicate with potential users and stakeholders during this exploratory phase.\n\n### Implementation:\n1. **Integrate with Prototypes**: Set up integration with our prototype systems to send notifications as part of user interaction flows.\n2. **Sandbox Environment**: Use the sandbox environment provided by Gov.UK Notify for initial testing and development.\n3. **Example Messages**: Create example notifications (both email and SMS) to demonstrate and test how different message types and content styles work.\n\nBy leveraging Gov.UK Notify, we ensure quick and compliant communication capabilities during our exploratory and prototype phases in the Alpha stage.","GenTime":"2024-08-28 14:19:11"}
{"File Name":"js-sdk\/0003-allow-only-tft-token.md","Context":"## Context\\nDrop FreeTFT, TFTA tokens from SDK\\n","Decision":"Completely drop FreeTFT, TFTA from currency options and don't ask for it at all. should always be TFT in the UI\\n","tokens":14,"id":5198,"text":"## Context\\nDrop FreeTFT, TFTA tokens from SDK\\n\n\n##Decision\nCompletely drop FreeTFT, TFTA from currency options and don't ask for it at all. should always be TFT in the UI\\n","MatchedIds":[2637,2631,1171,3982,2629],"Predicted":"## Decision\nFreeTFT and TFTA tokens will be removed from the SDK. This includes the removal of all related code, functionalities, and documentation from the SDK to ensure a clean and streamlined codebase moving forward. This decision will help to maintain focus on the currently supported tokens and reduce any confusion or maintenance overhead related to obsolete components.","GenTime":"2024-08-28 14:19:13"}
{"File Name":"dos-server\/adr-14-persistent-file-links.md","Context":"## Context\\nDOS returns internally generated unique identifiers for files it ingests.\\nThe URLs are roughly of the form (at the time of this ADR, plain integer identifiers are returned):\\nhttps:\/\/dos-stage.mitlib.net\/file?id=123e4567-e89b-12d3-a456-556642440000\\nOnce files are ingested, the identifiers are stored in ArchivesSpace.\\nThis may result in a situation where if URLs to files need to change (e.g., if\\nthe AWS bucket name for DOS changes), all links in ArchivesSpace need to be updated for\\nall ingested files.\\nAlthough DOS can be made to generate persistent identifies, it is desirable to externalize\\nthis functionality for reuse and portability considerations.\\nThe Handle System can be used for this purpose. Handles are already being used in\\nDome and DSpace, and there is organizational familiarity with the system.\\n","Decision":"Handle System will be used to generate persistent links. DOS will use the\\nHandle server API to generate handles. Handles will be returned when the\\nrelevant DOS end point is invoked.\\n","tokens":187,"id":3369,"text":"## Context\\nDOS returns internally generated unique identifiers for files it ingests.\\nThe URLs are roughly of the form (at the time of this ADR, plain integer identifiers are returned):\\nhttps:\/\/dos-stage.mitlib.net\/file?id=123e4567-e89b-12d3-a456-556642440000\\nOnce files are ingested, the identifiers are stored in ArchivesSpace.\\nThis may result in a situation where if URLs to files need to change (e.g., if\\nthe AWS bucket name for DOS changes), all links in ArchivesSpace need to be updated for\\nall ingested files.\\nAlthough DOS can be made to generate persistent identifies, it is desirable to externalize\\nthis functionality for reuse and portability considerations.\\nThe Handle System can be used for this purpose. Handles are already being used in\\nDome and DSpace, and there is organizational familiarity with the system.\\n\n\n##Decision\nHandle System will be used to generate persistent links. DOS will use the\\nHandle server API to generate handles. Handles will be returned when the\\nrelevant DOS end point is invoked.\\n","MatchedIds":[3372,3831,1823,2740,3852],"Predicted":"## Decision\nWe will transition to using the Handle System for generating persistent identifiers for all files ingested by DOS. This will ensure that:\n\n1. URLs for files are decoupled from the underlying storage architecture, reducing the need for updates in ArchivesSpace if changes occur (e.g., AWS bucket name changes).\n2. Organizational consistency and familiarity are maintained, as the Handle System is already in use within Dome and DSpace.\n\nThe Handle System will manage the resolution of these persistent identifiers to their corresponding URLs, providing a stable and reusable solution.","GenTime":"2024-08-28 14:19:15"}
{"File Name":"tendermint\/adr-063-privval-grpc.md","Context":"## Context\\nValidators use remote signers to help secure their keys. This system is Tendermint's recommended way to secure validators, but the path to integration with Tendermint's private validator client is plagued with custom protocols.\\nTendermint uses its own custom secure connection protocol (`SecretConnection`) and a raw tcp\/unix socket connection protocol. The secure connection protocol until recently was exposed to man in the middle attacks and can take longer to integrate if not using Golang. The raw tcp connection protocol is less custom, but has been causing minute issues with users.\\nMigrating Tendermint's private validator client to a widely adopted protocol, gRPC, will ease the current maintenance and integration burden experienced with the current protocol.\\n","Decision":"After discussing with multiple stake holders, [gRPC](https:\/\/grpc.io\/) was decided on to replace the current private validator protocol. gRPC is a widely adopted protocol in the micro-service and cloud infrastructure world. gRPC uses [protocol-buffers](https:\/\/developers.google.com\/protocol-buffers) to describe its services, providing a language agnostic implementation. Tendermint uses protobuf for on disk and over the wire encoding already making the integration with gRPC simpler.\\n","tokens":147,"id":1949,"text":"## Context\\nValidators use remote signers to help secure their keys. This system is Tendermint's recommended way to secure validators, but the path to integration with Tendermint's private validator client is plagued with custom protocols.\\nTendermint uses its own custom secure connection protocol (`SecretConnection`) and a raw tcp\/unix socket connection protocol. The secure connection protocol until recently was exposed to man in the middle attacks and can take longer to integrate if not using Golang. The raw tcp connection protocol is less custom, but has been causing minute issues with users.\\nMigrating Tendermint's private validator client to a widely adopted protocol, gRPC, will ease the current maintenance and integration burden experienced with the current protocol.\\n\n\n##Decision\nAfter discussing with multiple stake holders, [gRPC](https:\/\/grpc.io\/) was decided on to replace the current private validator protocol. gRPC is a widely adopted protocol in the micro-service and cloud infrastructure world. gRPC uses [protocol-buffers](https:\/\/developers.google.com\/protocol-buffers) to describe its services, providing a language agnostic implementation. Tendermint uses protobuf for on disk and over the wire encoding already making the integration with gRPC simpler.\\n","MatchedIds":[4097,1985,4101,841,1979],"Predicted":"## Decision\n- Migrate Tendermint's private validator client to use gRPC as the communication protocol instead of the custom `SecretConnection` and raw tcp\/unix socket connection protocols.\n- Ensure the gRPC interface is well-documented and includes support for authentication, encryption, and error handling.\n- Design a backward compatibility layer or provide migration tools to help current users transition smoothly to the new gRPC-based system.\n- Maintain an initial phase where both the old and new protocols are supported to enable a seamless transition period.\n- Conduct thorough testing to ensure stability and performance improvements with gRPC, particularly under high-load scenarios.\n- Collect feedback from early adopters and iterate on the implementation to address any concerns or issues that arise during real-world usage.\n- Plan for deprecating the old protocols in future releases, with clear communication to users about timelines and support phases.","GenTime":"2024-08-28 14:19:19"}
{"File Name":"archcolider\/011 Every meal delivery has pick up pin code.md","Context":"## Context\\nNetwork connection might be lost, when a meal is delivered to a fridge and a user comes to grab it. Then the fridge can't check data of the user online by card swapping, or in-app distance opening.\\nBut the fridge has a pin pad keyboard and still has quite sophisticated software and internal memory to process orders.\\nUpdate 2020-11-24:\\nWe expect that every meal has its own unique id provided by the kitchen because some meals might be customized from the general catalog. Let's say lactose-free lasagna should be addressed to a specific user.\\nThen, at the purchase or production process, we can update the user's device with the meal's unique id and generate an access code based on meal ID.\\n","Decision":"Meals dispatched from a Ghost Kitchen, will have a special 6-8 digit code.\\n","tokens":154,"id":2678,"text":"## Context\\nNetwork connection might be lost, when a meal is delivered to a fridge and a user comes to grab it. Then the fridge can't check data of the user online by card swapping, or in-app distance opening.\\nBut the fridge has a pin pad keyboard and still has quite sophisticated software and internal memory to process orders.\\nUpdate 2020-11-24:\\nWe expect that every meal has its own unique id provided by the kitchen because some meals might be customized from the general catalog. Let's say lactose-free lasagna should be addressed to a specific user.\\nThen, at the purchase or production process, we can update the user's device with the meal's unique id and generate an access code based on meal ID.\\n\n\n##Decision\nMeals dispatched from a Ghost Kitchen, will have a special 6-8 digit code.\\n","MatchedIds":[2672,1712,2681,4843,2256],"Predicted":"## Decision\nUtilize a locally stored access code system on the fridge that leverages the meal's unique ID to generate an access code during the purchase or production process.\n\n### Mechanism\n1. **Access Code Generation:**\n   - At the time of meal production, generate a unique access code for each meal based on its unique ID.\n   - Share this access code with the user's device.\n\n2. **Local Validation:**\n   - Store the meal's unique ID and the corresponding access code in the fridge's internal memory.\n   - When a user comes to retrieve their meal, they can enter the access code on the fridge's pin pad keyboard.\n   - The fridge validates the entered code against its stored data.\n\n3. **Offline Capability:**\n   - The system does not rely on network connectivity for validation.\n   - Fridge software handles the authentication process locally using the pre-stored data.\n\n4. **Security Measures:**\n   - Ensure the access code generation algorithm is secure and difficult to guess.\n   - Implement measures to handle incorrect access code attempts, such as timeouts or alerting mechanisms.\n  \n### Advantages:\n- **Reliability:** Ensures meal retrieval can function even when the network connection is unavailable.\n- **Customization Support:** Each meal customized for a specific user can be securely accessed using the unique access code.\n- **Security:** Local storage of codes and the requirement for input via pin pad mitigate the risks associated with network-based authentication failures.","GenTime":"2024-08-28 14:19:23"}
{"File Name":"operational-data-hub\/0029-components-are-named-according-to-naming-conventions.md","Context":"## Context\\nOrganize your cloud assets to support operational management and accounting requirements. Well-defined naming and metadata tagging conventions help to quickly locate and manage resources. These conventions also help associate cloud usage costs with business teams via chargeback and showback accounting mechanisms.\\n","Decision":"The naming cloud object naming convention is based on the Ironstone Naming conventions and can be found on our interal Wiki\\n","tokens":52,"id":2757,"text":"## Context\\nOrganize your cloud assets to support operational management and accounting requirements. Well-defined naming and metadata tagging conventions help to quickly locate and manage resources. These conventions also help associate cloud usage costs with business teams via chargeback and showback accounting mechanisms.\\n\n\n##Decision\nThe naming cloud object naming convention is based on the Ironstone Naming conventions and can be found on our interal Wiki\\n","MatchedIds":[1464,2752,2793,3147,200],"Predicted":"## Decision\nAdopt the following naming and tagging conventions for cloud assets:\n\n### Naming Conventions\n- **Environment**: Define the environment and use the standard prefixes such as `dev`, `test`, `stage`, `prod`.\n- **Service**: Use a short identifier for the service or application the resource belongs to.\n- **Resource Type**: Use abbreviations to represent the resource type, such as `vm` for virtual machines, `db` for databases.\n\n**Example**: `prod-web-vm1`, `test-app-db2`\n\n### Tagging Conventions\n- **Environment**: `Key: Environment, Value: <Environment>`\n- **Owner**: `Key: Owner, Value: <Team or Person>`\n- **Cost Center**: `Key: CostCenter, Value: <CostAllocationId>`\n- **Purpose**: `Key: Purpose, Value: <BriefDescription>`\n- **Project**: `Key: Project, Value: <ProjectName>`\n- **Compliance**: `Key: Compliance, Value: <ComplianceRequirement>`\n\n**Example Tags**:\n```\nEnvironment: prod\nOwner: finance-team\nCostCenter: CC1234\nPurpose: customer-facing-web-app\nProject: project-alpha\nCompliance: GDPR\n```\n\nCreate a detailed documentation outlining these conventions and ensure all relevant teams are trained to follow these guidelines moving forward.","GenTime":"2024-08-28 14:19:28"}
{"File Name":"adr\/ADR-24-decouple-kernel-and-unity-apis.md","Context":"## Context and Problem Statement\\nAs of today, kernel and unity have huge dependencies in business logic: sagas in TS, requests are made in TS, and everything is used from Unity.\\nThat generates coupling between technologies and teams, generating blockers and planning problems.\\n","Decision":"* Go for this proposal \u2705\\n### Actionables\\n- When starting development on new features, aim for this approach from the get-go\\n- Features that are already advanced in development or finished will be refactored gradually\\n- We will start by exposing signed requests and migrating the quests feature saga\\nNew features for the ECS\/SDK will have the common product process (RFC, roadmap, backlog, refinement). Alvaro will gather together the team when necessary to discuss implementation of new changes to the SDK.\\nCLI should be handled in the same way as the kernel and SDK developments.\\n","tokens":54,"id":4614,"text":"## Context and Problem Statement\\nAs of today, kernel and unity have huge dependencies in business logic: sagas in TS, requests are made in TS, and everything is used from Unity.\\nThat generates coupling between technologies and teams, generating blockers and planning problems.\\n\n\n##Decision\n* Go for this proposal \u2705\\n### Actionables\\n- When starting development on new features, aim for this approach from the get-go\\n- Features that are already advanced in development or finished will be refactored gradually\\n- We will start by exposing signed requests and migrating the quests feature saga\\nNew features for the ECS\/SDK will have the common product process (RFC, roadmap, backlog, refinement). Alvaro will gather together the team when necessary to discuss implementation of new changes to the SDK.\\nCLI should be handled in the same way as the kernel and SDK developments.\\n","MatchedIds":[4609,3483,311,1495,4896],"Predicted":"## Decision\nWe choose to refactor the system to decouple business logic from the Unity and kernel components. This will involve extracting the business logic into a set of dedicated microservices that can be independently developed and managed. By using a microservice architecture:\n\n1. Each service will be responsible for a specific piece of business logic, making it easier to manage and scale.\n2. Services will communicate via well-defined APIs, reducing the coupling between Unity, the kernel, and the business logic.\n3. Teams can work independently on different services without causing blockers for other teams, improving the concurrency of development.\n  \nAdditionally, we will adopt an asynchronous messaging system for inter-service communication to foster loose coupling and improve the system's scalability and fault tolerance. By following these steps, we aim to streamline the development process and enhance the flexibility and robustness of the system.","GenTime":"2024-08-28 14:19:31"}
{"File Name":"openchs-adr\/0005-deploy-openchs-server-on-the-cloud.md","Context":"## Context\\n1. Cost - Most users of OpenCHS are NGOs having 5-10 health workers. A running postgres instance and a tomcat server per installation is hard to support\/maintain.\\n2. Availability of internet - While users might not have internet connectivity in the field, they are usually attached to a head office where there is internet connectivity. They usually come to this common location regularly. Since sync is the only reason for internet connectivity, it can be limited during these visits.\\n3. Future direction - 3g\/4g is getting more common across the country, and cloud-based solutions are also getting more pervasive.\\n","Decision":"Openchs server will have capabilities to be deployed on the cloud\\n","tokens":132,"id":2824,"text":"## Context\\n1. Cost - Most users of OpenCHS are NGOs having 5-10 health workers. A running postgres instance and a tomcat server per installation is hard to support\/maintain.\\n2. Availability of internet - While users might not have internet connectivity in the field, they are usually attached to a head office where there is internet connectivity. They usually come to this common location regularly. Since sync is the only reason for internet connectivity, it can be limited during these visits.\\n3. Future direction - 3g\/4g is getting more common across the country, and cloud-based solutions are also getting more pervasive.\\n\n\n##Decision\nOpenchs server will have capabilities to be deployed on the cloud\\n","MatchedIds":[1712,1708,2828,3070,4859],"Predicted":"## Decision\n- Use a lightweight, embedded database like SQLite for local data storage on individual devices. SQLite does not require a separate server setup and is easy to maintain within the app.\n- Implement a periodic sync mechanism that only requires internet connectivity during specified times when users are at the head office. This sync can push local data to a cloud-based PostgreSQL instance and fetch updates, ensuring data consistency across devices.\n- Leverage cloud services (such as AWS, Google Cloud, or Azure) for maintaining the central PostgreSQL database and any other backend services. This approach reduces the need for separate server maintenance at each NGO site.\n- Prepare the architecture for future transition to more continuous sync models as 3g\/4g connectivity becomes more widespread. This can be achieved by introducing configurable sync intervals and background sync capabilities that can be enabled as internet availability improves.\n\n### Levers\n1. Synchronization frequency: Adjust sync intervals based on the internet availability and user schedules.\n2. Sync batch size: Control the amount of data synced during each session to manage bandwidth and improve efficiency.\n\nThis solution provides a balance of cost efficiency, ease of maintenance, and future readiness for improved internet connectivity.","GenTime":"2024-08-28 14:19:37"}
{"File Name":"tendermint\/adr-014-secp-malleability.md","Context":"## Context\\nSecp256k1 has two layers of malleability.\\nThe signer has a random nonce, and thus can produce many different valid signatures.\\nThis ADR is not concerned with that.\\nThe second layer of malleability basically allows one who is given a signature\\nto produce exactly one more valid signature for the same message from the same public key.\\n(They don't even have to know the message!)\\nThe math behind this will be explained in the subsequent section.\\nNote that in many downstream applications, signatures will appear in a transaction, and therefore in the tx hash.\\nThis means that if someone broadcasts a transaction with secp256k1 signature, the signature can be altered into the other form by anyone in the p2p network.\\nThus the tx hash will change, and this altered tx hash may be committed instead.\\nThis breaks the assumption that you can broadcast a valid transaction and just wait for its hash to be included on chain.\\nOne example is if you are broadcasting a tx in cosmos,\\nand you wait for it to appear on chain before incrementing your sequence number.\\nYou may never increment your sequence number if a different tx hash got committed.\\nRemoving this second layer of signature malleability concerns could ease downstream development.\\n### ECDSA context\\nSecp256k1 is ECDSA over a particular curve.\\nThe signature is of the form `(r, s)`, where `s` is a field element.\\n(The particular field is the `Z_n`, where the elliptic curve has order `n`)\\nHowever `(r, -s)` is also another valid solution.\\nNote that anyone can negate a group element, and therefore can get this second signature.\\n","Decision":"We can just distinguish a canonical form for the ECDSA signatures.\\nThen we require that all ECDSA signatures be in the form which we defined as canonical.\\nWe reject signatures in non-canonical form.\\nA canonical form is rather easy to define and check.\\nIt would just be the smaller of the two values for `s`, defined lexicographically.\\nThis is a simple check, instead of checking if `s < n`, instead check `s <= (n - 1)\/2`.\\nAn example of another cryptosystem using this\\nis the parity definition here https:\/\/github.com\/zkcrypto\/pairing\/pull\/30#issuecomment-372910663.\\nThis is the same solution Ethereum has chosen for solving secp malleability.\\n","tokens":356,"id":1957,"text":"## Context\\nSecp256k1 has two layers of malleability.\\nThe signer has a random nonce, and thus can produce many different valid signatures.\\nThis ADR is not concerned with that.\\nThe second layer of malleability basically allows one who is given a signature\\nto produce exactly one more valid signature for the same message from the same public key.\\n(They don't even have to know the message!)\\nThe math behind this will be explained in the subsequent section.\\nNote that in many downstream applications, signatures will appear in a transaction, and therefore in the tx hash.\\nThis means that if someone broadcasts a transaction with secp256k1 signature, the signature can be altered into the other form by anyone in the p2p network.\\nThus the tx hash will change, and this altered tx hash may be committed instead.\\nThis breaks the assumption that you can broadcast a valid transaction and just wait for its hash to be included on chain.\\nOne example is if you are broadcasting a tx in cosmos,\\nand you wait for it to appear on chain before incrementing your sequence number.\\nYou may never increment your sequence number if a different tx hash got committed.\\nRemoving this second layer of signature malleability concerns could ease downstream development.\\n### ECDSA context\\nSecp256k1 is ECDSA over a particular curve.\\nThe signature is of the form `(r, s)`, where `s` is a field element.\\n(The particular field is the `Z_n`, where the elliptic curve has order `n`)\\nHowever `(r, -s)` is also another valid solution.\\nNote that anyone can negate a group element, and therefore can get this second signature.\\n\n\n##Decision\nWe can just distinguish a canonical form for the ECDSA signatures.\\nThen we require that all ECDSA signatures be in the form which we defined as canonical.\\nWe reject signatures in non-canonical form.\\nA canonical form is rather easy to define and check.\\nIt would just be the smaller of the two values for `s`, defined lexicographically.\\nThis is a simple check, instead of checking if `s < n`, instead check `s <= (n - 1)\/2`.\\nAn example of another cryptosystem using this\\nis the parity definition here https:\/\/github.com\/zkcrypto\/pairing\/pull\/30#issuecomment-372910663.\\nThis is the same solution Ethereum has chosen for solving secp malleability.\\n","MatchedIds":[1962,21,4623,4097,1944],"Predicted":"## Decision\n\nTo mitigate the second layer of malleability in secp256k1 signatures, we will adopt the canonicalization approach ensuring that only one valid form of the signature can be used. This will greatly reduce the potential for transaction hash manipulation and provide consistency in downstream applications.\n\n### Signature Canonicalization\n\n- For every secp256k1 signature `(r, s)`, we will enforce `s` to be the lower value choice. This means if `s > n\/2`, we will use `(r, n - s)` instead of `(r, s)`.\n- This transformation ensures that each signature has a unique, canonical representation, thereby removing the ability to alter the transaction hash by changing the signature to its alternative valid form.\n\n### Implementation Steps\n\n1. **Signature Creation**:\n   - During the signing process, after generating the signature `(r, s)`, check the value of `s`.\n   - If `s > n\/2`, replace `s` with `n - s` to ensure `s` is always the smaller value.\n\n2. **Signature Verification**:\n   - When verifying a signature `(r, s)`, first check if `s > n\/2`.\n   - If true, replace `s` with `n - s` before proceeding with the standard ECDSA verification.\n\n### Benefits\n\n- **Consistency**: Ensuring a unique representation for each signature removes ambiguity and guarantees that the same transaction will always produce the same transaction hash.\n- **Security**: Mitigates potential vulnerabilities associated with transaction malleability, which can disrupt transaction ordering and sequencing.\n- **Compatibility**: The canonicalization approach is computationally inexpensive and can be integrated seamlessly into existing signing and verification workflows.\n\nThis approach ensures robustness against tampering with transaction hashes due to signature malleability and provides a more predictable environment for developers and users interacting with the blockchain.","GenTime":"2024-08-28 14:19:43"}
{"File Name":"dogma\/0008-location-of-testing-features.md","Context":"## Context\\nWe need to decide whether Dogma's testing features should reside in the `dogma`\\nmodule itself, or a separate `dogmatest` module.\\n### Benefits to separate module\\n1. They can be versioned separately. A `dogma` release can be made without\\nhaving to update the testing tools. This is a _pro_ for the releaser, but\\npossibly a _con_ for the users.\\n1. Users that opt not to test, or not to test using our tools do not need to\\ndownload `dogmatest` at all. This is not a particularly strong argument.\\n1. We can make BC breaking changes to `dogmatest`, without having to make\\na new major release of `dogma`. We would need to document clearly which\\nmajor `dogma` versions are supported by which major `dogmatest` versions.\\n### Benefits to a single module\\n1. The user does not need to determine which `dogmatest` version to use with\\ntheir application; compatible tools are always available right from `dogma`. If\\nwe want to encourage users to test their domain logic using these utilities;\\nthey should be easy to access.\\n1. As mentioned in [#16](github.com\/dogmatiq\/dogma), splitting the testing\\ntools would mean that the example code would also need to be moved elsewhere.\\nHowever, we have since already decided to move the example code to a separate\\nmodule in [ADR-7](0007-location-of-examples.md).\\n","Decision":"For the same reasons [ADR-7](0007-location-of-examples.md) we've decided to\\nimplement testing tools in their own module.\\n","tokens":324,"id":1608,"text":"## Context\\nWe need to decide whether Dogma's testing features should reside in the `dogma`\\nmodule itself, or a separate `dogmatest` module.\\n### Benefits to separate module\\n1. They can be versioned separately. A `dogma` release can be made without\\nhaving to update the testing tools. This is a _pro_ for the releaser, but\\npossibly a _con_ for the users.\\n1. Users that opt not to test, or not to test using our tools do not need to\\ndownload `dogmatest` at all. This is not a particularly strong argument.\\n1. We can make BC breaking changes to `dogmatest`, without having to make\\na new major release of `dogma`. We would need to document clearly which\\nmajor `dogma` versions are supported by which major `dogmatest` versions.\\n### Benefits to a single module\\n1. The user does not need to determine which `dogmatest` version to use with\\ntheir application; compatible tools are always available right from `dogma`. If\\nwe want to encourage users to test their domain logic using these utilities;\\nthey should be easy to access.\\n1. As mentioned in [#16](github.com\/dogmatiq\/dogma), splitting the testing\\ntools would mean that the example code would also need to be moved elsewhere.\\nHowever, we have since already decided to move the example code to a separate\\nmodule in [ADR-7](0007-location-of-examples.md).\\n\n\n##Decision\nFor the same reasons [ADR-7](0007-location-of-examples.md) we've decided to\\nimplement testing tools in their own module.\\n","MatchedIds":[4359,3031,1387,5176,2290],"Predicted":"## Decision\nDogma's testing features will reside in a separate `dogmatest` module.\n\n### Rationale\n1. **Versioning Flexibility**: By separating the testing tools into their own module, we can version them independently. This allows us to release updates or bug fixes for `dogma` without needing to make corresponding releases for `dogmatest`, unless absolutely necessary. This is particularly advantageous for maintaining backward compatibility (BC) as we can make breaking changes to `dogmatest` without affecting `dogma`.\n  \n2. **Modularity and Optionality**: Users who choose not to use our testing tools will not need to download and install `dogmatest`. This slight reduction in dependencies can simplify installations and reduce potential conflicts.\n\n3. **Clear Documentation**: The separation makes it easier to document and manage which versions of `dogmatest` are compatible with which versions of `dogma`. This can be clearly stated in the documentation, ensuring that users can easily find and use the correct versions.\n\n4. **Encapsulation of Concerns**: By keeping testing concerns separate from the core logic within `dogma`, we maintain a cleaner, more modular codebase. This separation of concerns will likely simplify the development and maintenance of both the core and the testing aspects of Dogma.\n\nAlthough there are benefits to having a single module, the flexibility, and modularity provided by separate modules are more aligned with best practices for long-term maintenance and scalability.","GenTime":"2024-08-28 14:19:49"}
{"File Name":"atlasdb\/0006-create-schema-lock-table-using-configuration.md","Context":"## Context\\nDue to [table creation issue](0002-prevent-tables-from-being-creating-simultaneously-in-cassandra-via-a-locks-table.md) we need to be able to safely create _lock table as a one off operation for each keyspace that atlas uses. The discussed options include:\\n- Have Atlas clients refuse to start if the _locks table is missing and provide a CLI to create it.\\n- This does require manual interview for whoever is handling operations\\n- Is very, very hard to get wrong\\n- Should be easily automatable in most setups, removing the manual step\\n- Use the lock service for locking rather than the _locks table, then we don't have to create it.\\n- Completely automatic and removes code\\n- Leaves open the possibility of locks failing and operations never realising that the issue has been triggered\\n- Have each node create a unique table, then run paxos to decide which one is the winner\\n- This requires a bunch of extra, error prone code\\n- Create an additional entry in the configuration, lockLeader, to denote which host is responsible for creating the locks table.\\n- The host whose name is the same as lockLeader will create the lock table, others will wait until the lockLeader is up.\\n- Requires all hosts to have the same configuration for lock leader\\n","Decision":"We decided to use an extra item of configuration, because:\\n1. Internal tools enable us to be confident that different Atlas servers on the same cluster are configured consistently.\\n2. Running paxos to decide which table is the winner was more complex than anticipated.\\n","tokens":276,"id":3092,"text":"## Context\\nDue to [table creation issue](0002-prevent-tables-from-being-creating-simultaneously-in-cassandra-via-a-locks-table.md) we need to be able to safely create _lock table as a one off operation for each keyspace that atlas uses. The discussed options include:\\n- Have Atlas clients refuse to start if the _locks table is missing and provide a CLI to create it.\\n- This does require manual interview for whoever is handling operations\\n- Is very, very hard to get wrong\\n- Should be easily automatable in most setups, removing the manual step\\n- Use the lock service for locking rather than the _locks table, then we don't have to create it.\\n- Completely automatic and removes code\\n- Leaves open the possibility of locks failing and operations never realising that the issue has been triggered\\n- Have each node create a unique table, then run paxos to decide which one is the winner\\n- This requires a bunch of extra, error prone code\\n- Create an additional entry in the configuration, lockLeader, to denote which host is responsible for creating the locks table.\\n- The host whose name is the same as lockLeader will create the lock table, others will wait until the lockLeader is up.\\n- Requires all hosts to have the same configuration for lock leader\\n\n\n##Decision\nWe decided to use an extra item of configuration, because:\\n1. Internal tools enable us to be confident that different Atlas servers on the same cluster are configured consistently.\\n2. Running paxos to decide which table is the winner was more complex than anticipated.\\n","MatchedIds":[3094,3096,224,207,3595],"Predicted":"## Decision\n\nWe decided to implement the additional entry in the configuration, `lockLeader`, to denote which host is responsible for creating the `_lock` table. The host whose name matches the `lockLeader` will create the lock table, while others will wait for the `lockLeader` to complete this task.\n\n### Rationale\n\n1. **Manual CLI Creation**: Although a CLI for creating the lock table is very hard to get wrong and easily automatable, it requires initial manual intervention and an extra step in managing operations. This introduces a potential for human error or missed steps, especially in larger deployments.\n2. **Using Lock Service**: This option completely automates the process and reduces the codebase complexity but leaves a critical gap where lock failures could go unnoticed, potentially leading to severe operational issues.\n3. **Unique Table per Node with Paxos**: Although leveraging Paxos could be an interesting approach, it significantly complicates the code and introduces potential error-prone scenarios. The coordination overhead would be too high for the benefit gained.\n4. **Configuration-based `lockLeader`**: This approach strikes a balance between automation and control. It ensures that only a designated host will create the lock table, thus minimizing the risks of concurrent creation while avoiding the need for a manual step. The requirement for a uniform configuration across hosts is manageable and ensures a smooth, predictable startup process.\n\n### Implementation Details\n\n- **Configuration Update**: Add an entry in the configuration file specifying the `lockLeader`.\n- **Lock Table Creation**: Ensure the host configured as `lockLeader` attempts to create the lock table. If the table already exists, the creation operation will be idempotent, causing no side effects.\n- **Startup Synchronization**: All other hosts will check the presence of the `_lock` table before proceeding with their startup protocol. They will wait for a pre-configured period to allow the `lockLeader` to complete the table creation.\n\n### Positive Consequences\n\n- **Reduced Complexity**: Compared to using Paxos or manual CLI steps, this method simplifies the overall process while providing the necessary safety.\n- **Automated Yet Controlled**: This provides an automated approach for lock table creation but with a clear control point (the `lockLeader`), ensuring predictability.\n- **Operational Consistency**: Ensures that all nodes operate with the same configuration, promoting consistency.\n\n### Negative Consequences\n\n- **Single Point of Failure**: If the `lockLeader` is unavailable or misconfigured, it can delay the startup process. However, this can be mitigated with proper redundancy and monitoring.\n- **Configuration Management**: Requires careful attention to ensure all hosts have the correct configuration, though this is typically manageable with appropriate deployment tooling.","GenTime":"2024-08-28 14:19:57"}
{"File Name":"wikiindex\/adr-003-testing_library.md","Context":"## Context\\n* We want to write programmer tests to support a TDD workflow.\\n* We want to be able to mock out functions.\\n","Decision":"* We will use Midje to test our code.\\n* Despite it's heavy macro design, it allows you to write expressive code and easily makes mocks\\n","tokens":30,"id":1387,"text":"## Context\\n* We want to write programmer tests to support a TDD workflow.\\n* We want to be able to mock out functions.\\n\n\n##Decision\n* We will use Midje to test our code.\\n* Despite it's heavy macro design, it allows you to write expressive code and easily makes mocks\\n","MatchedIds":[4868,4359,452,1258,4179],"Predicted":"## Decision\nWe will use Mocha and Chai as our testing framework and assertion library respectively to support a TDD workflow. For mocking functions, we will incorporate Sinon.js, which works seamlessly with Mocha and Chai to provide strong support for spies, stubs, and mocks. This combination will help us effectively write programmer tests and mock out functions as needed.","GenTime":"2024-08-28 14:19:59"}
{"File Name":"cena\/0009-test-features-with-cucumber.md","Context":"## Context\\nAcceptance tests aim to test the application behaviors as a whole from a consumer point of view.\\nThose tests must be understandable by any user, either technical or not and help documenting the provided features.\\nDefining new acceptance tests must be easy, through reusable step definitions.\\nApplication sources must be self-contained, including the acceptance tests definition and implementation, so that\\nacceptance tests can be run during the development lifecycle in a [Behavior-driven development](https:\/\/en.wikipedia.org\/wiki\/Behavior-driven_development)\\napproach.\\n","Decision":"[Cucumber](https:\/\/cucumber.io\/) will be used to describe and execute acceptance tests in `menu-generation` application.\\n","tokens":111,"id":650,"text":"## Context\\nAcceptance tests aim to test the application behaviors as a whole from a consumer point of view.\\nThose tests must be understandable by any user, either technical or not and help documenting the provided features.\\nDefining new acceptance tests must be easy, through reusable step definitions.\\nApplication sources must be self-contained, including the acceptance tests definition and implementation, so that\\nacceptance tests can be run during the development lifecycle in a [Behavior-driven development](https:\/\/en.wikipedia.org\/wiki\/Behavior-driven_development)\\napproach.\\n\n\n##Decision\n[Cucumber](https:\/\/cucumber.io\/) will be used to describe and execute acceptance tests in `menu-generation` application.\\n","MatchedIds":[3166,1258,4179,4359,1416],"Predicted":"## Decision\nWe will adopt the [Cucumber](https:\/\/cucumber.io\/) framework for writing our acceptance tests. \n\n* Cucumber allows for specifications written in plain language, making tests understandable to both technical and non-technical users.\n* It supports [Behavior-driven development (BDD)](https:\/\/en.wikipedia.org\/wiki\/Behavior-driven_development) by allowing us to define the expected behavior of the application as specifications.\n* Cucumber uses reusable step definitions, making it easier to define new acceptance tests.\n* Keeping the acceptance tests within the application sources ensures that they are self-contained and can be run during the development lifecycle.\n* This approach improves our documentation by embedding it into our tests, which is aligned with BDD practices.\n\nImplementing this decision promotes collaboration across the team and ensures that our application behaviors are well-documented and comprehensively tested.","GenTime":"2024-08-28 14:20:02"}
{"File Name":"libelektra\/spec_metadata.md","Context":"## Problem\\nTo make storage-plugins suitable for `spec` they need to be able to store\\nall the metadata as specified in [METADATA.ini](\/doc\/METADATA.ini).\\nMost file formats do not have support for that.\\nIf metadata is merged from different namespaces, e.g., `spec:` and `user:`,\\nmetadata from one namespace might end up in keys of other namespaces, e.g.,\\nmetadata from `spec:` might end up in `user:`.\\n","Decision":"Do not store metadata unrelated to the configuration file structure in any namespace except in `spec:\/`.\\n- Trying to store any other metadata in any other namespace leads to an error.\\nE.g. `kdb set-meta user:\/data metadata_not_suitable_for_storage_plugins something` would fail\\n(validated by `spec` plugin).\\n- Metadata that is designed to be stored by storage plugins to preserve configuration file structure.\\nE.g. `comment` or `order`, might be stored in any namespace.\\nSometimes, the same metadata can be used in several namespaces but with different meanings\\nand ways of serialization, e.g. `type` and `array`:\\n- In `spec:\/` the metadata `array=` (empty value) means \"this is an array\".\\nIf you give it a value e.g. `array=#4` it means \"this is an array with default size X\" (e.g. `#4` = size 5).\\n- In any other namespace `array=` means \"this is an empty array\" and e.g. `array=#4` means \"this is an array with max index #4\".\\n`array=#4` is not stored literally but inferred.\\n- Either the storage plugin does not support arrays, then the metadata will be discarded\\non `kdbSet` but `spec` will keep on adding it for every `kdbGet`.\\n- Or, if the storage plugin supports arrays, the data will be serialized as array\\n(even if the metadata comes from `spec`) and as such available in the next `kdbGet`\\nfrom the storage plugin to be validated by `spec`.\\nUse different storage plugins, or plugins with different configurations,\\nfor the `spec:\/` namespace:\\n- `ni`\\n- TOML with `meta` configuration\\nThe `kdb mount` tool will add the `meta` plugin configuration when mounting a storage plugin to `spec:\/`.\\n- [Spec Expressiveness](spec_expressiveness.md)\\n- [Arrays](array.md)\\n","tokens":103,"id":1278,"text":"## Problem\\nTo make storage-plugins suitable for `spec` they need to be able to store\\nall the metadata as specified in [METADATA.ini](\/doc\/METADATA.ini).\\nMost file formats do not have support for that.\\nIf metadata is merged from different namespaces, e.g., `spec:` and `user:`,\\nmetadata from one namespace might end up in keys of other namespaces, e.g.,\\nmetadata from `spec:` might end up in `user:`.\\n\n\n##Decision\nDo not store metadata unrelated to the configuration file structure in any namespace except in `spec:\/`.\\n- Trying to store any other metadata in any other namespace leads to an error.\\nE.g. `kdb set-meta user:\/data metadata_not_suitable_for_storage_plugins something` would fail\\n(validated by `spec` plugin).\\n- Metadata that is designed to be stored by storage plugins to preserve configuration file structure.\\nE.g. `comment` or `order`, might be stored in any namespace.\\nSometimes, the same metadata can be used in several namespaces but with different meanings\\nand ways of serialization, e.g. `type` and `array`:\\n- In `spec:\/` the metadata `array=` (empty value) means \"this is an array\".\\nIf you give it a value e.g. `array=#4` it means \"this is an array with default size X\" (e.g. `#4` = size 5).\\n- In any other namespace `array=` means \"this is an empty array\" and e.g. `array=#4` means \"this is an array with max index #4\".\\n`array=#4` is not stored literally but inferred.\\n- Either the storage plugin does not support arrays, then the metadata will be discarded\\non `kdbSet` but `spec` will keep on adding it for every `kdbGet`.\\n- Or, if the storage plugin supports arrays, the data will be serialized as array\\n(even if the metadata comes from `spec`) and as such available in the next `kdbGet`\\nfrom the storage plugin to be validated by `spec`.\\nUse different storage plugins, or plugins with different configurations,\\nfor the `spec:\/` namespace:\\n- `ni`\\n- TOML with `meta` configuration\\nThe `kdb mount` tool will add the `meta` plugin configuration when mounting a storage plugin to `spec:\/`.\\n- [Spec Expressiveness](spec_expressiveness.md)\\n- [Arrays](array.md)\\n","MatchedIds":[1318,1279,285,3372,1291],"Predicted":"## Decision\nImplement a two-tiered metadata storage system:\n- **Primary Metadata Tier**: Store essential metadata that is universally supported by most file formats within the main key-value structure.\n- **Secondary Metadata Tier**: Use a supplementary file or database (e.g., an auxiliary JSON or SQLite file) to store complex or additional metadata that cannot be handled by the primary format.\n\nTo handle namespace merging:\n- Introduce a namespace prefixing scheme where metadata keys are prefixed with their respective namespaces (e.g., `spec:`, `user:`).\n- Ensure that during read and write operations, the metadata is appropriately serialized and deserialized with its namespace prefixes to avoid intermingling of data.\n\nKey Changes:\n- Develop helper functions to manage the two-tiered metadata system transparently to avoid impacting current user workflows.\n- Adapt storage plugins to adhere to this new metadata storage strategy.\n- Update documentation and provide guidelines to developers on how to implement and use the two-tiered system. \n\nBy implementing this two-tiered approach, we can ensure full compatibility with the `spec` plugin's metadata requirements while mitigating issues arising from the limitations of various file formats and namespace mixing.","GenTime":"2024-08-28 14:20:08"}
{"File Name":"operational-data-hub\/0013-dataflow-diagrams.md","Context":"## Context\\nWe feel the need to define that every config folder should contain at least one dataflow diagram.\\n","Decision":"We decided that every config folder should contain at least one dataflow diagram.\\n### Dataflow Diagram\\nEvery config with a cloudbuild should contain a dataflow diagram. This diagram contains the dataflow of the GCP project the config is for. It is also possible to have multiple diagrams if there are multiple dataflows. [Here](https:\/\/github.com\/vwt-digital-config\/snb-link2-int-config\/tree\/develop\/dataflow_information\/tmob_emails_link2) and [here](https:\/\/github.com\/vwt-digital-config\/snb-tmob-problm-int-config\/tree\/develop\/dataflow_information\/tmob_emails_link2) examples of dataflow diagrams can be found.\\nThe diagrams are made via the python package [diagrams](https:\/\/github.com\/mingrammer\/diagrams).\\n","tokens":23,"id":2706,"text":"## Context\\nWe feel the need to define that every config folder should contain at least one dataflow diagram.\\n\n\n##Decision\nWe decided that every config folder should contain at least one dataflow diagram.\\n### Dataflow Diagram\\nEvery config with a cloudbuild should contain a dataflow diagram. This diagram contains the dataflow of the GCP project the config is for. It is also possible to have multiple diagrams if there are multiple dataflows. [Here](https:\/\/github.com\/vwt-digital-config\/snb-link2-int-config\/tree\/develop\/dataflow_information\/tmob_emails_link2) and [here](https:\/\/github.com\/vwt-digital-config\/snb-tmob-problm-int-config\/tree\/develop\/dataflow_information\/tmob_emails_link2) examples of dataflow diagrams can be found.\\nThe diagrams are made via the python package [diagrams](https:\/\/github.com\/mingrammer\/diagrams).\\n","MatchedIds":[2703,1172,2502,1792,3722],"Predicted":"## Decision\n### 1. Dataflow Diagram Requirement\n* Every config folder must contain at least one dataflow diagram that accurately represents the flow of data within the application.\n  \n### 2. Diagram Creation Guidelines\n* Dataflow diagrams should be created using standardized tooling, such as PlantUML.\n* The diagrams must be stored as markdown files (.md) in the config folder for easy access and version control.\n* Naming conventions should include the context and purpose of the diagram, e.g., `dataflow_authentication.md`.\n\n### 3. Content Requirements\n* Each dataflow diagram should clearly indicate the data sources, destinations, storage points, and processing steps within the system.\n* Essential components such as data stores, processes, data flows, and external entities must be depicted accurately.\n\n### 4. Version Control and Updates\n* All dataflow diagrams must be kept up-to-date with any architectural or code changes that alter the flow of data.\n* Changes to these diagrams should be reviewed as part of the standard code review process to ensure accuracy and completeness.\n\n### 5. Documentation\n* An accompanying README file must be provided in the config folder, explaining the purpose and details of the dataflow diagram.\n* The README should include a brief description of each element in the diagram to ensure clarity and comprehension.\n\n### 6. Compliance Checks\n* Automated checks should be integrated into the CI\/CD pipeline to confirm the presence of dataflow diagrams in the config folders.\n* Manual audits should also be conducted quarterly to ensure compliance with this requirement.","GenTime":"2024-08-28 14:20:14"}
{"File Name":"dogma\/0004-adr-process.md","Context":"## Context\\nWe need a documented process for proposing, discussing and ultimate accepting or\\nrejecting ADRs via pull requests.\\n","Decision":"We have decided to adopt a process that favours rapid changes, at least while\\nthe project is in its infancy. To this end, we will allow ADRs in the `proposed`\\nstatus to be merged to `main` via PRs. The PRs may contain related code changes.\\nWhenever a `proposed` ADR is merged to `main`, a GitHub issue is created to\\ncapture discussion about the ADR. Such issues are given the `adr` label.\\nAny `proposed` ADRs remaining on `main` must be resolved either by approving\\nthe ADR, or by rejecting it and reverting any associated code changes.\\n","tokens":27,"id":1614,"text":"## Context\\nWe need a documented process for proposing, discussing and ultimate accepting or\\nrejecting ADRs via pull requests.\\n\n\n##Decision\nWe have decided to adopt a process that favours rapid changes, at least while\\nthe project is in its infancy. To this end, we will allow ADRs in the `proposed`\\nstatus to be merged to `main` via PRs. The PRs may contain related code changes.\\nWhenever a `proposed` ADR is merged to `main`, a GitHub issue is created to\\ncapture discussion about the ADR. Such issues are given the `adr` label.\\nAny `proposed` ADRs remaining on `main` must be resolved either by approving\\nthe ADR, or by rejecting it and reverting any associated code changes.\\n","MatchedIds":[3554,2914,2964,1563,1545],"Predicted":"## Decision\nEstablish a documented process for proposing, discussing, and ultimately accepting or rejecting ADRs via pull requests.\n\n### Proposed Process Steps:\n1. **Proposal Submission**:\n   - A team member initiates an ADR proposal by opening a new pull request (PR) in the designated repository.\n   - The PR should include the initial draft of the ADR, providing a comprehensive explanation of the context, problem statement, proposed decision, and its consequences (both positive and negative).\n\n2. **Initial Review**:\n   - The proposed ADR is assigned to a minimum of two reviewers by the PR creator.\n   - Reviewers have a period of 2 working days to provide their initial feedback on the proposal. During this time, they should review the ADR draft for clarity, feasibility, and alignment with project goals.\n\n3. **Discussion Period**:\n   - Team members are encouraged to participate in the discussion by commenting directly in the PR.\n   - Discussion should be constructive and focus on improving the proposal or identifying potential issues.\n   - The discussion period lasts for a minimum of 3 working days, providing ample time for thorough review and consideration.\n\n4. **Iteration and Refinement**:\n   - Based on the feedback received, the original author may update the ADR.\n   - The reviewers should re-evaluate the updated ADR, providing further feedback if necessary.\n   - This iteration continues until no major concerns remain.\n\n5. **Final Vote**:\n   - Once the reviewers are satisfied with the proposal, a final vote is called.\n   - Each member of the team can cast a vote (approve or reject) in the PR.\n   - The voting period lasts for 2 working days.\n\n6. **Decision**:\n   - The outcome is determined by a majority vote. \n   - If the proposal receives the majority of \"approve\" votes, it is accepted and merged into the repository.\n   - If the proposal does not receive the majority of \"approve\" votes, it is rejected, and the author is informed.\n\n7. **Documentation and Communication**:\n   - Accepted ADRs are documented in the ADR directory in the repository.\n   - The decision, along with any pertinent discussion points, is communicated to the wider team to ensure transparency and awareness.\n\n### Positive Consequences\n- **Transparency**: The documented process ensures that all team members are aware of proposed changes and have the opportunity to participate in the decision-making process.\n- **Consistency**: A standardized approach to ADRs helps maintain consistency in documenting and implementing architectural decisions.\n- **Collaboration**: Encourages collaboration and collective ownership of architectural decisions.\n\n### Negative Consequences\n- **Time-Consuming**: The multi-step process may be time-consuming, especially if there are lengthy discussions and iterations.\n- **Potential Delays**: Waiting for feedback and votes may introduce delays in making architectural decisions.\n\nBy following this documented process, we ensure that ADRs are thoroughly vetted, collaboratively discussed, and transparently accepted or rejected, thus enhancing the overall quality and consistency of our architectural decisions.","GenTime":"2024-08-28 14:20:24"}
{"File Name":"smarthub\/0003-create-backend-client-lib.md","Context":"## Context\\nOff-chain data is accessible via REST API. Currently all system components uses direct REST calls in various places making unit test hard.\\n","Decision":"Create client library and use it as dependency in components that want to read the off-chain data. Include the mocked version of the service so unit-tests does not have to rely on the implementation.\\n","tokens":29,"id":3692,"text":"## Context\\nOff-chain data is accessible via REST API. Currently all system components uses direct REST calls in various places making unit test hard.\\n\n\n##Decision\nCreate client library and use it as dependency in components that want to read the off-chain data. Include the mocked version of the service so unit-tests does not have to rely on the implementation.\\n","MatchedIds":[1558,4868,1180,1387,3069],"Predicted":"## Decision\nWe will introduce an abstraction layer in the form of a service interface to encapsulate the REST API calls. This interface will provide method signatures for all interactions with the off-chain data. Concrete implementations of this interface will handle the actual REST API calls. For unit testing purposes, we will implement mock versions of these service interfaces. This approach will decouple the business logic from the REST calls, making it easier to write unit tests for our components.","GenTime":"2024-08-28 14:20:28"}
{"File Name":"CICD-pipeline\/001-open-source-shared-jenkins-library.md","Context":"## Context\\n1) We are [open by default](https:\/\/github.com\/baloise\/baloise.github.io\/blob\/master\/design\/OpenByDefault.adoc)\\n1) The myCloud API spec [is not confidential](https:\/\/github.com\/baloise\/CICD-pipeline\/issues\/15)\\n1) Openness is a driver for clean, secure design and good documentation\\n1) There are currently no obvious drivers for confidentiality\\n","Decision":"We release the Shared Jenkins Library under Apache 2.0.\\n","tokens":90,"id":2768,"text":"## Context\\n1) We are [open by default](https:\/\/github.com\/baloise\/baloise.github.io\/blob\/master\/design\/OpenByDefault.adoc)\\n1) The myCloud API spec [is not confidential](https:\/\/github.com\/baloise\/CICD-pipeline\/issues\/15)\\n1) Openness is a driver for clean, secure design and good documentation\\n1) There are currently no obvious drivers for confidentiality\\n\n\n##Decision\nWe release the Shared Jenkins Library under Apache 2.0.\\n","MatchedIds":[2181,2759,1180,1182,1186],"Predicted":"## Decision\nWe decide to make the myCloud API specification publicly accessible. \n\nIn alignment with our principle of being open by default, and given that the API spec is not confidential, this transparency will encourage clean, secure design and improve documentation quality. There are no clear requirements that necessitate confidentiality, thus making the information open will foster collaboration and potentially drive innovation and improvements from the wider community.","GenTime":"2024-08-28 14:20:30"}
{"File Name":"community\/dr-012-Prettier_as_JavaScript_code_formatter.md","Context":"## Context\\nThe Code Style Guide is a set of conventions on how to write the source code. It covers many areas, including the use of `camelCase` vs `PascalCase` for variable identifiers, whether or not to use a semicolon as a statement terminator, or the use of tabs or spaces for indentation.\\nIt is obvious that an enforced, unified code style in a project is beneficial. Not only does it improve the readability, but it also saves you from a lot of noise while looking at diffs, caused by unadjusted whitespaces, different break line settings, and other issues. It also ends discussions around the style itself. Once applied, everyone can write code the way they want, and let the formatter do the work. In the end, it saves time and energy.\\n[Prettier](https:\/\/prettier.io) is one of the solutions dedicated to code formatting. It does not enforce such code quality rules as the use of globally scoped variables or naming. It only enforces the formatting rules. It can be used as a plugin for selected IDEs, as a pre-commit `git` hook, or as a standalone CLI tool. No matter which option you choose, it produces the same output given its configuration.\\nIt was chosen because of its simplicity, ease of configuration, small amount of available options, and support for JavaScript, TypeScript, GraphQL, CSS, SCSS, and JSON, all of which are used in Kyma projects.\\n","Decision":"The decision is to use Prettier as the only code formatter for JavaScript-based projects.\\n","tokens":301,"id":3451,"text":"## Context\\nThe Code Style Guide is a set of conventions on how to write the source code. It covers many areas, including the use of `camelCase` vs `PascalCase` for variable identifiers, whether or not to use a semicolon as a statement terminator, or the use of tabs or spaces for indentation.\\nIt is obvious that an enforced, unified code style in a project is beneficial. Not only does it improve the readability, but it also saves you from a lot of noise while looking at diffs, caused by unadjusted whitespaces, different break line settings, and other issues. It also ends discussions around the style itself. Once applied, everyone can write code the way they want, and let the formatter do the work. In the end, it saves time and energy.\\n[Prettier](https:\/\/prettier.io) is one of the solutions dedicated to code formatting. It does not enforce such code quality rules as the use of globally scoped variables or naming. It only enforces the formatting rules. It can be used as a plugin for selected IDEs, as a pre-commit `git` hook, or as a standalone CLI tool. No matter which option you choose, it produces the same output given its configuration.\\nIt was chosen because of its simplicity, ease of configuration, small amount of available options, and support for JavaScript, TypeScript, GraphQL, CSS, SCSS, and JSON, all of which are used in Kyma projects.\\n\n\n##Decision\nThe decision is to use Prettier as the only code formatter for JavaScript-based projects.\\n","MatchedIds":[1408,5255,4226,3321,400],"Predicted":"## Decision\nWe will standardize the code style across all Kyma projects using Prettier.\n\n- **Configuration:** We will set up a common Prettier configuration file that dictates the formatting rules across all projects.\n- **IDE Integration:** Developers are encouraged to install Prettier plugins available for their IDEs to ensure real-time formatting.\n- **Pre-commit Hook:** We will implement a pre-commit Git hook that automatically runs Prettier to format code before it is committed.\n- **Continuous Integration:** We will configure our CI pipeline to check that the committed code adheres to the Prettier formatting rules and reject any commits that do not comply.\n  \nThese steps will ensure consistency in code formatting, improve code readability, and reduce unnecessary changes due to formatting differences.","GenTime":"2024-08-28 14:20:33"}
{"File Name":"hello\/001-record-decisions.md","Context":"## Context\\nWe want to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael\\nNygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":4361,"text":"## Context\\nWe want to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as described by Michael\\nNygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","MatchedIds":[4428,3119,1436,294,2583],"Predicted":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).","GenTime":"2024-08-28 14:20:34"}
{"File Name":"insight\/0002-tweepy.md","Context":"## Context\\nTo programmatically access twitter we need to use a Python library able to support APIs we need. The chosen library should be well reviewed and have wide community support.\\n","Decision":"We choose [Tweepy](https:\/\/github.com\/tweepy\/tweepy) as our Twitter API Pyhon library.\\n","tokens":36,"id":4760,"text":"## Context\\nTo programmatically access twitter we need to use a Python library able to support APIs we need. The chosen library should be well reviewed and have wide community support.\\n\n\n##Decision\nWe choose [Tweepy](https:\/\/github.com\/tweepy\/tweepy) as our Twitter API Pyhon library.\\n","MatchedIds":[2446,1243,864,4761,400],"Predicted":"## Decision\nWe will use Tweepy for programmatically accessing Twitter. Tweepy is well-documented, widely supported by the community, and supports the most recent Twitter APIs. (https:\/\/www.tweepy.org\/)","GenTime":"2024-08-28 14:20:36"}
{"File Name":"delayer-aws\/0003-dymamodb-update-strategy.md","Context":"## Context and Problem Statement\\nIn order to maintain the serverless nature of the project, a DynamoDB table was\\nchosen as main persistence mechanism, which means that a schedule is primarily\\nstored in DynamoDB and then distributed to other components, which in turn\\ngoes back to DynamoDB to update state. So, DynamoDB holds both state and\\nhistorical data.\\nThe problem here is that both ~~warmer~~ `task-1minute-enqueuer` and ~~poller~~ `task-1minute-sqs2sns` will concur by Dynamo resources and probably will be throttled (it's easy to reproduce this behavior only by setting Dynamo's read and write capacity to 1 and trying to send some hundreds of schedules while some other are ~~moving from *WARM* state~~ being enqueued in delayer queue).\\n## Decision Drivers\\n*   Solution must kept as simple as possible\\n*   Even it could delay the problem, increase read and write capacity of\\nDynamoDB is not an architectural solution\\n","Decision":"*   Solution must kept as simple as possible\\n*   Even it could delay the problem, increase read and write capacity of\\nDynamoDB is not an architectural solution\\nTake the decision for the use of the DynamoDB introduced a new concept for the entire architecture: the layered concern.\\nThe `delayer-aws` solution aims to provide a way to schedule future operations reliably. It's not part of this system store or ingest or even present information about these schedules. In this sense, the use of DynamoDB is needed only because there's a need of store schedules that could not be posted in delayer queue, and there's only 2 options for those records: or they are in the delayer queue, or they're not. That's why the \"state\" field is needed, but it will not hold the *entire* lifecycle of a schedule.\\nWith this in mind, we realize that all 3 options will be considered, but in different contexts:\\n-   Present data of scheduler is not `delayer-aws`'s concern, but it will be needed. So all the data events should be published by `delayer-aws` to be consumed by another \"view\" platform - this is a kind of *event driven approach*.\\n-   In this sense, if another system will ingest all of this published data, state and historical data will be stored in different persistent stores;\\n### How it should work\\nWhen a schedule is inserted, if their timeframe was greater then the timeframe of the delayer queue, the schedule is stored in DynamoDB with a *currentStatus* marked as `NEW` and a TTL of 1 hour after schedule time.\\nWhen `task-1minute-enqueuer` runs and finds this scheduler, it will be updated to `PROCESSED`. After that, no more updates are made in DynamoDB.\\nFor each event that occurs with this schedule, a message should be posted in the correspondent topic.\\n### Don't use state as a field in database\\nThe state of a schedule is noted by currentStatus\/currentState attribute in\\nDynamoDB. The proposal of this optional is to treat state as part of the\\ngeneral flow of application:\\n1.  When scheduled, it's COLD\\n1.  When *warmer* puts it on SQS, it's WARM\\n1.  When *poller* publishes it on SNS is DOING\\n1.  When *actions* execute\\n*   Good, simplified update policy\\n*   Good, reduced DynamoDB costs\\n*   Bad, no information about past events\\n","tokens":211,"id":2791,"text":"## Context and Problem Statement\\nIn order to maintain the serverless nature of the project, a DynamoDB table was\\nchosen as main persistence mechanism, which means that a schedule is primarily\\nstored in DynamoDB and then distributed to other components, which in turn\\ngoes back to DynamoDB to update state. So, DynamoDB holds both state and\\nhistorical data.\\nThe problem here is that both ~~warmer~~ `task-1minute-enqueuer` and ~~poller~~ `task-1minute-sqs2sns` will concur by Dynamo resources and probably will be throttled (it's easy to reproduce this behavior only by setting Dynamo's read and write capacity to 1 and trying to send some hundreds of schedules while some other are ~~moving from *WARM* state~~ being enqueued in delayer queue).\\n## Decision Drivers\\n*   Solution must kept as simple as possible\\n*   Even it could delay the problem, increase read and write capacity of\\nDynamoDB is not an architectural solution\\n\n\n##Decision\n*   Solution must kept as simple as possible\\n*   Even it could delay the problem, increase read and write capacity of\\nDynamoDB is not an architectural solution\\nTake the decision for the use of the DynamoDB introduced a new concept for the entire architecture: the layered concern.\\nThe `delayer-aws` solution aims to provide a way to schedule future operations reliably. It's not part of this system store or ingest or even present information about these schedules. In this sense, the use of DynamoDB is needed only because there's a need of store schedules that could not be posted in delayer queue, and there's only 2 options for those records: or they are in the delayer queue, or they're not. That's why the \"state\" field is needed, but it will not hold the *entire* lifecycle of a schedule.\\nWith this in mind, we realize that all 3 options will be considered, but in different contexts:\\n-   Present data of scheduler is not `delayer-aws`'s concern, but it will be needed. So all the data events should be published by `delayer-aws` to be consumed by another \"view\" platform - this is a kind of *event driven approach*.\\n-   In this sense, if another system will ingest all of this published data, state and historical data will be stored in different persistent stores;\\n### How it should work\\nWhen a schedule is inserted, if their timeframe was greater then the timeframe of the delayer queue, the schedule is stored in DynamoDB with a *currentStatus* marked as `NEW` and a TTL of 1 hour after schedule time.\\nWhen `task-1minute-enqueuer` runs and finds this scheduler, it will be updated to `PROCESSED`. After that, no more updates are made in DynamoDB.\\nFor each event that occurs with this schedule, a message should be posted in the correspondent topic.\\n### Don't use state as a field in database\\nThe state of a schedule is noted by currentStatus\/currentState attribute in\\nDynamoDB. The proposal of this optional is to treat state as part of the\\ngeneral flow of application:\\n1.  When scheduled, it's COLD\\n1.  When *warmer* puts it on SQS, it's WARM\\n1.  When *poller* publishes it on SNS is DOING\\n1.  When *actions* execute\\n*   Good, simplified update policy\\n*   Good, reduced DynamoDB costs\\n*   Bad, no information about past events\\n","MatchedIds":[279,3361,207,2526,2309],"Predicted":"## Decision\nIntroduce a caching layer between the `task-1minute-enqueuer` and `task-1minute-sqs2sns` processes to reduce the load on DynamoDB. \n\n**Implementation:**\n1. **Use AWS ElastiCache (Redis)**: Implement an in-memory cache using AWS ElastiCache (Redis) which will serve as a buffer between the components and DynamoDB. Whenever `task-1minute-enqueuer` needs to enqueue data, it will first write to Redis. Similarly, `task-1minute-sqs2sns` will read from Redis before updating the state in DynamoDB.\n2. **TTL (Time to Live) Settings**: Set appropriate TTL for cached data to ensure eventual consistency and to automatically remove stale data.\n3. **Batch Writes to DynamoDB**: Implement batch write operations for processes updating DynamoDB to minimize the number of write operations and hence reduce the possibility of throttling.\n\n**Pros:**\n- **Reduced Load on DynamoDB**: By offloading some read and write operations to Redis, the load on DynamoDB will be significantly reduced.\n- **Improved Performance**: In-memory caching will provide faster access to the necessary data.\n- **Maintains Serverless Architecture**: AWS ElastiCache is a managed service, thus aligning with the serverless nature of the project.\n\n**Cons:**\n- **Additional Component**: Introducing Redis adds complexity to the architecture.\n- **Cost Implications**: There will be additional costs associated with using AWS ElastiCache.\n\n**Summary:**\nBy introducing a caching layer with AWS ElastiCache (Redis), we can alleviate the pressure on DynamoDB without increasing its read\/write capacity. This solution ensures that the system remains performant and reliable without deviating from a serverless architecture.","GenTime":"2024-08-28 14:20:42"}
{"File Name":"zendesk_app_scaffold_react_redux\/replace-migration-scaffold-with-clean-scaffold-in-master-branch.md","Context":"## Context\\nAt the moment, the master branch has the scaffold specifically built to help developers migrate v1 apps to v2. From monitor, the majority usage comes from [App Migrator](https:\/\/github.com\/zendesk\/zendesk_app_migrator) and [ZAT](https:\/\/github.com\/zendesk\/zendesk_apps_tools) which use the master branch as the download source.\\nIt is more helpful to show developers coming directly to this repo a clean (from scratch) scaffold, one usable as a starting point to build new apps respecting our current conventions out of the box.\\n","Decision":"### New app scaffold\\nA new *clean* app scaffold has been built on branch [offapps-migration](https:\/\/github.com\/zendesk\/app_scaffold\/tree\/offapps-migration) based on branch [from-scratch](https:\/\/github.com\/zendesk\/app_scaffold\/tree\/from-scratch)\\n### Move migration scaffold into App Migrator repository\\nMigration scaffold in master branch will be moved into [App Migrator](https:\/\/github.com\/zendesk\/zendesk_app_migrator), used and maintained as a *built-in* template resource\\n### Move new app scaffold to master branch\\n### Add support in [ZAT](https:\/\/github.com\/zendesk\/zendesk_apps_tools) to create new app with the new scaffold\\n* New command option `zat new --scaffold` will create a v2 app using the new app scaffold.\\n","tokens":124,"id":2378,"text":"## Context\\nAt the moment, the master branch has the scaffold specifically built to help developers migrate v1 apps to v2. From monitor, the majority usage comes from [App Migrator](https:\/\/github.com\/zendesk\/zendesk_app_migrator) and [ZAT](https:\/\/github.com\/zendesk\/zendesk_apps_tools) which use the master branch as the download source.\\nIt is more helpful to show developers coming directly to this repo a clean (from scratch) scaffold, one usable as a starting point to build new apps respecting our current conventions out of the box.\\n\n\n##Decision\n### New app scaffold\\nA new *clean* app scaffold has been built on branch [offapps-migration](https:\/\/github.com\/zendesk\/app_scaffold\/tree\/offapps-migration) based on branch [from-scratch](https:\/\/github.com\/zendesk\/app_scaffold\/tree\/from-scratch)\\n### Move migration scaffold into App Migrator repository\\nMigration scaffold in master branch will be moved into [App Migrator](https:\/\/github.com\/zendesk\/zendesk_app_migrator), used and maintained as a *built-in* template resource\\n### Move new app scaffold to master branch\\n### Add support in [ZAT](https:\/\/github.com\/zendesk\/zendesk_apps_tools) to create new app with the new scaffold\\n* New command option `zat new --scaffold` will create a v2 app using the new app scaffold.\\n","MatchedIds":[3682,3982,1243,4021,2251],"Predicted":"## Decision\nWe will create a new branch named `migration` to contain the existing scaffold that helps developers migrate v1 apps to v2. The `master` branch will be updated to contain a clean scaffold that follows our latest conventions for building new apps from scratch. This provides a clear separation between migration tools and fresh starts, making it more intuitive for developers to find the resources they need. Additionally, the documentation will be updated to indicate the purpose and intended usage of each branch to guide developers appropriately.","GenTime":"2024-08-28 14:20:45"}
{"File Name":"cf-k8s-networking\/0006-rewrite-http-liveness-readiness-probes-for-healthchecks.md","Context":"## Context\\nWith Istio auto mTLS enabled in `STRICT` mode, [http liveness and readiness\\nprobes](https:\/\/kubernetes.io\/docs\/tasks\/configure-pod-container\/configure-liveness-readiness-startup-probes\/) no longer work because the `kubelet`, which makes the http requests, does not have Istio issued certificates.\\nIstio [supports rewriting `http` probes](https:\/\/istio.io\/docs\/ops\/configuration\/mesh\/app-health-check\/#enable-globally-via-install-option) during the sidecar injection process.\\n#### Figure 1\\n_Liveness probe flow when Istio mTLS is disabled or `PERMISSIVE`. Probe `GET` request regularly travels through the Envoy sidecar to the app._\\n![No mTLS\/PERMISSIVE mTLS mode liveness probe diagram](..\/assets\/liveness-probe-adr-1.png)\\n#### Figure 2\\n_Liveness probe flow when Istio mTLS is `STRICT` and the probe is not rewritten. Probe `GET` request fails at the Envoy sidecar because it does not include the correct certificates._\\n![STRICT mTLS liveness probe diagram with no probe rewrite](..\/assets\/liveness-probe-adr-2.png)\\n#### Figure 3\\n_Liveness probe flow when Istio mTLS is `STRICT` and the probe **is rewritten by Istio**. Probe `GET` request bypasses the sidecar and goes through the Istio `pilot-agent` instead. The `pilot-agent` is configured to direct the request to the app._\\n![STRICT mTLS liveness probe diagram with probe rewrite](..\/assets\/liveness-probe-adr-3.png)\\n","Decision":"We have decided to install Istio with\\n`--set values.sidecarInjectorWebhook.rewriteAppHTTPProbe=true`\\nThis will rewrite the liveness and readiness probes on any app pods when injecting a sidecar into the app pod. Any namespaces that have the label `istio-injection=enabled` will have their liveness and readiness probes rewritten, as shown in Figure 3.\\n","tokens":360,"id":2243,"text":"## Context\\nWith Istio auto mTLS enabled in `STRICT` mode, [http liveness and readiness\\nprobes](https:\/\/kubernetes.io\/docs\/tasks\/configure-pod-container\/configure-liveness-readiness-startup-probes\/) no longer work because the `kubelet`, which makes the http requests, does not have Istio issued certificates.\\nIstio [supports rewriting `http` probes](https:\/\/istio.io\/docs\/ops\/configuration\/mesh\/app-health-check\/#enable-globally-via-install-option) during the sidecar injection process.\\n#### Figure 1\\n_Liveness probe flow when Istio mTLS is disabled or `PERMISSIVE`. Probe `GET` request regularly travels through the Envoy sidecar to the app._\\n![No mTLS\/PERMISSIVE mTLS mode liveness probe diagram](..\/assets\/liveness-probe-adr-1.png)\\n#### Figure 2\\n_Liveness probe flow when Istio mTLS is `STRICT` and the probe is not rewritten. Probe `GET` request fails at the Envoy sidecar because it does not include the correct certificates._\\n![STRICT mTLS liveness probe diagram with no probe rewrite](..\/assets\/liveness-probe-adr-2.png)\\n#### Figure 3\\n_Liveness probe flow when Istio mTLS is `STRICT` and the probe **is rewritten by Istio**. Probe `GET` request bypasses the sidecar and goes through the Istio `pilot-agent` instead. The `pilot-agent` is configured to direct the request to the app._\\n![STRICT mTLS liveness probe diagram with probe rewrite](..\/assets\/liveness-probe-adr-3.png)\\n\n\n##Decision\nWe have decided to install Istio with\\n`--set values.sidecarInjectorWebhook.rewriteAppHTTPProbe=true`\\nThis will rewrite the liveness and readiness probes on any app pods when injecting a sidecar into the app pod. Any namespaces that have the label `istio-injection=enabled` will have their liveness and readiness probes rewritten, as shown in Figure 3.\\n","MatchedIds":[2249,2240,3909,4705,3104],"Predicted":"## Decision\nWe will enable Istio's feature for rewriting `http` probes globally during the sidecar injection process. This rewriting allows the `kubelet` to bypass the Envoy sidecar and direct the liveness and readiness probe requests through the Istio `pilot-agent`, which will forward them to the application.\n\n### Positive Consequences\n* Ensures that liveness and readiness probes continue to function correctly even when Istio mTLS is enabled in `STRICT` mode.\n* Simplifies the configuration for health checks by avoiding the need for manual adjustments to probes.\n* Maintains the security benefits of mTLS without compromising on the reliability of the Kubernetes health checks.\n\n### Negative Consequences\n* Adds some complexity to the sidecar injection process, which may require additional testing and validation.\n* There could be minor performance overhead introduced by the Istio `pilot-agent` handling the probes. However, this is expected to be negligible compared to the benefits gained.","GenTime":"2024-08-28 14:20:49"}
{"File Name":"kotlin-guice\/0002-use-packaging-for-major-version-interoperability.md","Context":"## Context\\nMajor version updates to libraries often include breaking API changes. Upgrading an application to a new major version of a library can be difficult since other libraries may also have dependencies on the updated library.\\nFor this library, `kotlin-guice`, the API tracks heavily to the Guice API. When Guice\\nmakes a major version upgrade, this library will also likely need to make a corresponding major version upgrade.\\n","Decision":"We will use the Java package and Maven group ID to allow interoperability of major versions of this library. As described by Jake Wharton in the blog post below, we will accomplish this in three ways.\\n1. The Java package name will include the version number.\\n1. The library name will be part of the group ID.\\n1. The group ID will include the version number.\\nSince this library is meant mostly as an extension to the corresponding Guice libraries, we will use the Guice major version. If we need to make a major version change to `kotlin-guice` within a single version of Guice, then we may have to include both version numbers. Until such a situation arises, we do not have to make that decision, but one possible option is `com.authzee.kotlinguice4_2`.\\nhttp:\/\/jakewharton.com\/java-interoperability-policy-for-major-version-updates\/\\n","tokens":88,"id":872,"text":"## Context\\nMajor version updates to libraries often include breaking API changes. Upgrading an application to a new major version of a library can be difficult since other libraries may also have dependencies on the updated library.\\nFor this library, `kotlin-guice`, the API tracks heavily to the Guice API. When Guice\\nmakes a major version upgrade, this library will also likely need to make a corresponding major version upgrade.\\n\n\n##Decision\nWe will use the Java package and Maven group ID to allow interoperability of major versions of this library. As described by Jake Wharton in the blog post below, we will accomplish this in three ways.\\n1. The Java package name will include the version number.\\n1. The library name will be part of the group ID.\\n1. The group ID will include the version number.\\nSince this library is meant mostly as an extension to the corresponding Guice libraries, we will use the Guice major version. If we need to make a major version change to `kotlin-guice` within a single version of Guice, then we may have to include both version numbers. Until such a situation arises, we do not have to make that decision, but one possible option is `com.authzee.kotlinguice4_2`.\\nhttp:\/\/jakewharton.com\/java-interoperability-policy-for-major-version-updates\/\\n","MatchedIds":[4424,1035,1048,1463,2240],"Predicted":"## Decision\nWhenever Guice makes a major version upgrade, we will also increase the major version of `kotlin-guice`. This keeps our versioning strategy aligned with Guice and helps signal to our users that there may be breaking changes. We will ensure that release notes clearly document any breaking changes and offer migration guides to assist developers in upgrading their applications.","GenTime":"2024-08-28 14:20:51"}
{"File Name":"react-template\/0003-use-prettier-to-format-code.md","Context":"## Context\\nWe want to ensure we're all using one code style, that is familiar across\\nprojects. [Prettier](https:\/\/prettier.io\/) is an opinionated code formatter with\\nsupport for most, if not all, of the languages in the JavaScript ecosystem. As\\nof writing, it is used by over\\n[1 million repositories](https:\/\/github.com\/prettier\/prettier\/network\/dependents?package_id=UGFja2FnZS00OTAwMTEyNTI%3D)\\non GitHub, including React itself, and has become a standard.\\n","Decision":"We will enforce that everything supported by Prettier has its style enforced by\\nit.\\nWe will set up Git hooks to automatically run the formatter before committing.\\nWe will set continuous integration up to reject commits that are not correctly\\nformatted.\\n","tokens":125,"id":3321,"text":"## Context\\nWe want to ensure we're all using one code style, that is familiar across\\nprojects. [Prettier](https:\/\/prettier.io\/) is an opinionated code formatter with\\nsupport for most, if not all, of the languages in the JavaScript ecosystem. As\\nof writing, it is used by over\\n[1 million repositories](https:\/\/github.com\/prettier\/prettier\/network\/dependents?package_id=UGFja2FnZS00OTAwMTEyNTI%3D)\\non GitHub, including React itself, and has become a standard.\\n\n\n##Decision\nWe will enforce that everything supported by Prettier has its style enforced by\\nit.\\nWe will set up Git hooks to automatically run the formatter before committing.\\nWe will set continuous integration up to reject commits that are not correctly\\nformatted.\\n","MatchedIds":[5255,4226,1408,3451,400],"Predicted":"## Decision\nWe will adopt Prettier as the standardized code formatter for all JavaScript-based projects. \n\n- Prettier will be configured to enforce a consistent code style.\n- We will integrate Prettier with our version control system using Git hooks to automatically format code before commits.\n- Continuous Integration (CI) pipelines will include steps to verify that code is formatted according to Prettier's rules.\n- Developers will be encouraged to configure their IDEs to use Prettier for real-time formatting.\n\nBy standardizing on Prettier, we aim to improve code readability, reduce formatting-related discussions, and streamline the development workflow.","GenTime":"2024-08-28 14:20:54"}
{"File Name":"alfa\/adr-003.md","Context":"## Context\\nHaving decided on [ADR 2](adr-002.md), we foresee that the Alfa code base will be both significantly larger and more complex than the code base of our proprietary engine. This is due to the fact that we will have to implement a great deal of APIs that we have previously relied on the browser implementations of. Coupled with the fact that the most common type of bug we have encountered in the past has been stray `undefined` or `null` values and APIs receiving incorrect parameters, plain JavaScript, even if covered by tests, is simply not an option moving forward. We will either need tooling that can sanity check our JavaScript or move to a language with a proper type system that can enforce API contracts.\\nHowever, given that browsers are still part of the equation, Alfa must be able to also run in a browser. This way, we ensure that we can implement tools such as our Chrome extension based on Alfa.\\n","Decision":"We will use [TypeScript](https:\/\/github.com\/Microsoft\/TypeScript) for implementing all of Alfa. Being a superset of JavaScript, TypeScript has a low learning curve for people already familiar with JavaScript while providing a solid type system. We will enforce API contracts through generation of [declaration files](https:\/\/www.typescriptlang.org\/docs\/handbook\/declaration-files\/introduction.html) which will dictate the API surface that consumers can access. To the extent possible, we will keep a strict TypeScript configuration in order to catch as many issues as possible at compile time. In particular, this entails strict `undefined` and `null` checking in order to get rid of a previously common type of bug.\\n","tokens":195,"id":3153,"text":"## Context\\nHaving decided on [ADR 2](adr-002.md), we foresee that the Alfa code base will be both significantly larger and more complex than the code base of our proprietary engine. This is due to the fact that we will have to implement a great deal of APIs that we have previously relied on the browser implementations of. Coupled with the fact that the most common type of bug we have encountered in the past has been stray `undefined` or `null` values and APIs receiving incorrect parameters, plain JavaScript, even if covered by tests, is simply not an option moving forward. We will either need tooling that can sanity check our JavaScript or move to a language with a proper type system that can enforce API contracts.\\nHowever, given that browsers are still part of the equation, Alfa must be able to also run in a browser. This way, we ensure that we can implement tools such as our Chrome extension based on Alfa.\\n\n\n##Decision\nWe will use [TypeScript](https:\/\/github.com\/Microsoft\/TypeScript) for implementing all of Alfa. Being a superset of JavaScript, TypeScript has a low learning curve for people already familiar with JavaScript while providing a solid type system. We will enforce API contracts through generation of [declaration files](https:\/\/www.typescriptlang.org\/docs\/handbook\/declaration-files\/introduction.html) which will dictate the API surface that consumers can access. To the extent possible, we will keep a strict TypeScript configuration in order to catch as many issues as possible at compile time. In particular, this entails strict `undefined` and `null` checking in order to get rid of a previously common type of bug.\\n","MatchedIds":[2290,2652,2687,3156,4761],"Predicted":"## Decision\nUse TypeScript as the primary language for the Alfa code base. TypeScript provides a strong type system that can help catch common bugs like `undefined` or `null` values and incorrect parameters at compile time. It also seamlessly compiles down to JavaScript, ensuring compatibility with browsers. This enables us to maintain the ability to implement tools such as our Chrome extension based on Alfa while enforcing API contracts and reducing runtime errors.","GenTime":"2024-08-28 14:20:58"}
{"File Name":"frontend\/001-spa.md","Context":"## Context\\nManaging playing audio between page loads is impossible. This means that either, we use a pop-up player that plays audio in another window or we build a Single Page Application that doesn't perform page loads.\\nBuilding a single page application will add complexity and other issues to work around like SEO. Pop-up players are clunky, feel dis-jointed from the rest of the site and can be annoying to use in mobile browsers.\\n","Decision":"Build a SPA.\\n","tokens":89,"id":1096,"text":"## Context\\nManaging playing audio between page loads is impossible. This means that either, we use a pop-up player that plays audio in another window or we build a Single Page Application that doesn't perform page loads.\\nBuilding a single page application will add complexity and other issues to work around like SEO. Pop-up players are clunky, feel dis-jointed from the rest of the site and can be annoying to use in mobile browsers.\\n\n\n##Decision\nBuild a SPA.\\n","MatchedIds":[2843,847,1576,1894,2221],"Predicted":"## Decision\nWe decided to build a Single Page Application (SPA) to provide a seamless audio playback experience across the site. This decision is based on the following considerations:\n\n1. **User Experience**: An SPA will deliver a smooth and continuous audio playback experience without interruptions from page loads, which is essential for user satisfaction.\n\n2. **Technical Feasibility**: Although building an SPA will introduce some additional complexity, modern frameworks like React, Vue, or Angular can manage this efficiently. We can mitigate SEO issues by implementing server-side rendering (SSR) or utilizing services like prerender.io.\n\n3. **Mobile Compatibility**: Pop-up players are not mobile-friendly and can be detrimental to the overall user experience. SPAs, on the other hand, can provide a consistent and integrated feel on mobile devices.\n\n4. **Future Extendability**: An SPA architecture will be more flexible in the long run, allowing for easier implementation of new features and enhancements without the need for full page reloads.\n\n### Implementation Plan\n1. Use a modern JavaScript framework (React, Vue, Angular, etc.) to develop the SPA.\n2. Implement server-side rendering (SSR) to address SEO concerns.\n3. Ensure that the audio player component is persistent and can maintain its state across different views within the SPA.\n4. Optimize the SPA for performance to minimize initial load times and enhance user experience.\n5. Conduct thorough testing, especially on mobile devices, to ensure a seamless and user-friendly experience.\n\nThis approach ensures that our application can provide an optimal audio playback experience while staying user-friendly and future-proof.","GenTime":"2024-08-28 14:21:03"}
{"File Name":"operational-data-hub\/0039-least-privilege-access.md","Context":"## Context\\nThe principle of least privilege means that every module (such as a process, a user, or a program, depending on the subject) must be able to access only the information and resources that are necessary for its legitimate purpose.\\nThe principle of least privilege works by allowing only enough access to perform the required job. In an IT environment, adhering to the principle of least privilege reduces the risk of attackers gaining access to critical systems or sensitive data by compromising a low-level user account, device, or application. Implementing the principle helps contain compromises to their area of origin, stopping them from spreading to the system at large.\\n","Decision":"We will apply the principle of least privilege.\\n","tokens":128,"id":2737,"text":"## Context\\nThe principle of least privilege means that every module (such as a process, a user, or a program, depending on the subject) must be able to access only the information and resources that are necessary for its legitimate purpose.\\nThe principle of least privilege works by allowing only enough access to perform the required job. In an IT environment, adhering to the principle of least privilege reduces the risk of attackers gaining access to critical systems or sensitive data by compromising a low-level user account, device, or application. Implementing the principle helps contain compromises to their area of origin, stopping them from spreading to the system at large.\\n\n\n##Decision\nWe will apply the principle of least privilege.\\n","MatchedIds":[4525,4554,3950,3890,2256],"Predicted":"## Decision\nTo adhere to the principle of least privilege, we will implement Role-Based Access Control (RBAC) across all systems and applications. Each role will be defined with the minimal set of permissions necessary to perform its required tasks. \n\n1. **Role Definition and Assignment**: Each user, process, or service will be assigned to specific roles based on their legitimate purposes and permissions required to fulfill their responsibilities. These roles will be rigorously reviewed to ensure that they have no more than the necessary permissions.\n\n2. **Dynamic Permission Granting**: Permissions will be granted dynamically and temporarily, where possible. For instance, elevated permissions can be given just-in-time and revoked automatically after the task is completed.\n\n3. **Segregation of Duties**: Implement segregation of duties to ensure that critical tasks or access to sensitive data are split between multiple roles, thereby minimizing the risk of any single compromised account causing systemic issues.\n\n4. **Periodic Audits and Reviews**: Conduct regular audits and reviews of all roles, permissions, and access logs. This helps in identifying and rectifying unnecessary access rights, thereby reinforcing the principle of least privilege.\n\n5. **Automation and Monitoring**: Utilize automated tools to monitor and enforce the principle of least privilege at all times. Alerting mechanisms will be in place to identify any anomaly in access patterns or privilege escalations.\n\n6. **Training and Awareness**: Provide training to all users on the importance of adhering to the principle of least privilege and the potential risks associated with excessive permissions.\n\nBy implementing these measures, we ensure that every user, process, and application has just enough permissions to perform its job, thereby significantly reducing the attack surface and enhancing the overall security posture of our IT environment.","GenTime":"2024-08-28 14:21:08"}
{"File Name":"aspan_micro_front_ends\/0002-derived-attributes.md","Context":"## Context\\nDerived attributes are read-only.\\n","Decision":"Following attributes are derived from file system:\\n1. name - file name without extension\\n2. contentType - file extension without dot, lowercased\\n","tokens":10,"id":3192,"text":"## Context\\nDerived attributes are read-only.\\n\n\n##Decision\nFollowing attributes are derived from file system:\\n1. name - file name without extension\\n2. contentType - file extension without dot, lowercased\\n","MatchedIds":[4497,2543,5070,1855,2215],"Predicted":"## Decision\nThe system will enforce the read-only nature of derived attributes by implementing the following rules:\n\n1. **Read-Only Enforcement:** All derived attributes will be decorated with a read-only property decorator in the codebase to prevent any direct modification.\n2. **Immutable Data Structures:** Use immutable data structures to represent derived attributes where possible, ensuring that the values cannot be altered after initial computation.\n3. **Access Control:** Integrate access control mechanisms at the API layer to disallow update operations on derived attributes.\n4. **Static Analysis & Linting:** Configure static analysis tools and linters to detect any attempts to modify derived attributes in the code during the development process.\n5. **Documentation:** Clearly document the read-only status of derived attributes in the API documentation and developer guidelines, including examples of their proper use.\n6. **Testing:** Implement comprehensive test cases to ensure that derived attributes remain immutable throughout the application lifecycle. This includes unit tests and integration tests to verify that derived attributes cannot be updated.\n7. **Code Reviews:** Enforce rigorous code review practices to ensure adherence to the above rules and prevent inadvertent modifications to derived attributes.\n\nBy combining these strategies, the system will effectively maintain the integrity and immutability of derived attributes, ensuring they remain read-only as intended.","GenTime":"2024-08-28 14:21:13"}
{"File Name":"status-react\/0002-extension-uri.md","Context":"## Context\\nExtensions are defined by an EDN file accessed from decentralized storage. An URI schema allows to identify those files.\\n","Decision":"URI follows ethereum URI schema as specified in [EIP 831](https:\/\/github.com\/ethereum\/EIPs\/blob\/master\/EIPS\/eip-831.md)\\n```\\nuri                     = \"ethereum:\" path\\npath                    = \"status:extension\" \":\" storage \"@\" id\\nstorage                 = STRING (e.g. IPFS)\\nid                      = STRING (e.g. IPFS hash)\\n```\\nAn example of an extension available on IPFS is: `ethereum:status:extension:ipfs@QmTeW79w7QQ6Npa3b1d5tANreCDxF2iDaAPsDvW6KtLmfB`\\n","tokens":26,"id":2963,"text":"## Context\\nExtensions are defined by an EDN file accessed from decentralized storage. An URI schema allows to identify those files.\\n\n\n##Decision\nURI follows ethereum URI schema as specified in [EIP 831](https:\/\/github.com\/ethereum\/EIPs\/blob\/master\/EIPS\/eip-831.md)\\n```\\nuri                     = \"ethereum:\" path\\npath                    = \"status:extension\" \":\" storage \"@\" id\\nstorage                 = STRING (e.g. IPFS)\\nid                      = STRING (e.g. IPFS hash)\\n```\\nAn example of an extension available on IPFS is: `ethereum:status:extension:ipfs@QmTeW79w7QQ6Npa3b1d5tANreCDxF2iDaAPsDvW6KtLmfB`\\n","MatchedIds":[2763,2740,2710,2616,2215],"Predicted":"## Decision\nWe will use the [magnet URI scheme](https:\/\/en.wikipedia.org\/wiki\/Magnet_URI_scheme) to identify EDN files in decentralized storage. This allows us to leverage content-based addressing, ensuring that each URI uniquely corresponds to the specific content of an EDN file.","GenTime":"2024-08-28 14:21:15"}
{"File Name":"modernisation-platform\/0015-use-aws-image-builder-for-managing-amis.md","Context":"## Context\\nThere is likely to  be a common requirement across Modernisation Platform consumers to utilise the benefits of using customised AMIs, such as managed consistency of configuration and speed of EC2 instance deployment.\\nSome options to tackle AMI generation and management were considered\\n* A custom approach - based on a reference architecture (see <https:\/\/aws.amazon.com\/blogs\/awsmarketplace\/announcing-the-golden-ami-pipeline\/>) that brings together a number of tools and techniques to manage AMIs through their lifecycle. This approach brings with it a fair amount of complexity and arguably extra operational overhead.\\n* [Packer](https:\/\/www.packer.io\/) - a common approach across private and public cloud platforms, using this Hashicorp tool to programmatically build AMIs. Such a solution would also need to manage the distribution of AMIs across accounts and lifecycle management of AMIs\\n* [AWS Image Builder](https:\/\/docs.aws.amazon.com\/imagebuilder\/latest\/userguide\/what-is-image-builder.html) - a managed AWS service incorporating concepts such as pipelines, recipes and components, and even a marketplace of build and test components. Image builder is based on the use of AWS Systems Manager (so no ssh connections and exposed ports). A solution based on this (at the time of writing) would need to also handle the lifecycle management of AMIs (as it the case with Packer)\\n","Decision":"We will create an AMI generation and management capability based on AWS Image Builder.\\n","tokens":287,"id":3145,"text":"## Context\\nThere is likely to  be a common requirement across Modernisation Platform consumers to utilise the benefits of using customised AMIs, such as managed consistency of configuration and speed of EC2 instance deployment.\\nSome options to tackle AMI generation and management were considered\\n* A custom approach - based on a reference architecture (see <https:\/\/aws.amazon.com\/blogs\/awsmarketplace\/announcing-the-golden-ami-pipeline\/>) that brings together a number of tools and techniques to manage AMIs through their lifecycle. This approach brings with it a fair amount of complexity and arguably extra operational overhead.\\n* [Packer](https:\/\/www.packer.io\/) - a common approach across private and public cloud platforms, using this Hashicorp tool to programmatically build AMIs. Such a solution would also need to manage the distribution of AMIs across accounts and lifecycle management of AMIs\\n* [AWS Image Builder](https:\/\/docs.aws.amazon.com\/imagebuilder\/latest\/userguide\/what-is-image-builder.html) - a managed AWS service incorporating concepts such as pipelines, recipes and components, and even a marketplace of build and test components. Image builder is based on the use of AWS Systems Manager (so no ssh connections and exposed ports). A solution based on this (at the time of writing) would need to also handle the lifecycle management of AMIs (as it the case with Packer)\\n\n\n##Decision\nWe will create an AMI generation and management capability based on AWS Image Builder.\\n","MatchedIds":[923,3917,3147,4041,3604],"Predicted":"## Decision\nWe will use AWS Image Builder for AMI generation and management. This managed AWS service provides built-in capabilities for creating, testing, and distributing AMIs with reduced operational overhead compared to custom approaches. AWS Image Builder simplifies the creation pipeline by leveraging AWS Systems Manager, which enhances security by eliminating the need for SSH connections and exposed ports. \n\nAlthough lifecycle management of AMIs will still need to be handled (as is the case with Packer), the integration of pipelines, recipes, and the availability of marketplace components in AWS Image Builder provides a robust, scalable, and less complex solution suitable for meeting the consistent configuration and rapid deployment needs of the Modernisation Platform.","GenTime":"2024-08-28 14:21:17"}
{"File Name":"nada-kafkarator\/0004-combine-topic-creation-and-credentials-management-in-same-app.md","Context":"## Context\\nThe project requires dealing with two relatively separate concerns:\\n1. Create topics when needed\\n2. Supply credentials for working with topics.\\nIf we were to strictly follow the Single Responsibility Principle, these should be in separate apps.\\nHowever, the two concerns are conceptually quite connected, even if they are separate in implementation,\\nso it makes sense to keep them in the same application.\\n","Decision":"We will ignore the SRP in this instance, and keep the two concerns in the same application.\\n","tokens":83,"id":866,"text":"## Context\\nThe project requires dealing with two relatively separate concerns:\\n1. Create topics when needed\\n2. Supply credentials for working with topics.\\nIf we were to strictly follow the Single Responsibility Principle, these should be in separate apps.\\nHowever, the two concerns are conceptually quite connected, even if they are separate in implementation,\\nso it makes sense to keep them in the same application.\\n\n\n##Decision\nWe will ignore the SRP in this instance, and keep the two concerns in the same application.\\n","MatchedIds":[4021,1792,3950,2834,2397],"Predicted":"## Decision\nWe will consolidate both concerns into a single application while maintaining a clear modular structure within the application. \n\n- **Modules**: We will create two distinct modules within the application: \n  - `TopicCreationModule` which will handle the creation of topics.\n  - `CredentialSupplyModule` which will supply the necessary credentials for working with those topics.\n  \n- **Interfaces**: Each module will have well-defined interfaces to ensure they can operate independently if needed in the future.\n\n- **Dependency Injection**: To maintain loose coupling and flexibility, we will utilize dependency injection to manage dependencies between these modules.\n\n- **Single Responsibility Adherence**: While both modules reside within the same application, each will be designed in a way that adheres to the Single Responsibility Principle internally.\n\n- **Configuration Management**: Centralized configuration will be employed to ensure that settings for both modules are easily manageable and alterations affect the whole application uniformly.\n\nThis approach balances the need to conceptually link the two concerns while maintaining clear boundaries and separation within the codebase, facilitating ease of maintenance and potential future scalability.","GenTime":"2024-08-28 14:21:22"}
{"File Name":"operational-data-hub\/0009-uri-identifies-data.md","Context":"## Context\\nA Uniform Resource Identifier (URI) is a string of characters that unambiguously identifies a particular resource. To guarantee uniformity, all URIs follow a predefined set of syntax rules, but also maintain extensibility through a separately defined hierarchical naming scheme (e.g. http:\/\/).\\nTo be able to unambiguously identify any element of data, a URI will be defined for every element of data on the ODH platform. Building this URI from meaningful attributes instead of technical randomly generated ids makes it recognizable and easier to use. However, selecting the right attributes and context information to assure uniqueness is challenging.\\nThe URI scheme to use is [tag](https:\/\/tools.ietf.org\/html\/rfc4151). Tag URIs are quite human readable, but, unlike http URIs, not resolvable. This makes them suitable for identifying real-life things (entities) that have a representation on the ODH. A URI can be defined from the hierarchy of the organisation, forming a path to the resource. For example, if mycompany has a catalog with articles and one of the articles has article# 313, an article URI could be defined as `tag:vwt.digital,2020:mycompany\/catalog\/articles\/313`\\n","Decision":"We define a URI according to the [tag scheme](https:\/\/tools.ietf.org\/html\/rfc4151) for each entity that has a representation on the ODH platform.\\n","tokens":252,"id":2740,"text":"## Context\\nA Uniform Resource Identifier (URI) is a string of characters that unambiguously identifies a particular resource. To guarantee uniformity, all URIs follow a predefined set of syntax rules, but also maintain extensibility through a separately defined hierarchical naming scheme (e.g. http:\/\/).\\nTo be able to unambiguously identify any element of data, a URI will be defined for every element of data on the ODH platform. Building this URI from meaningful attributes instead of technical randomly generated ids makes it recognizable and easier to use. However, selecting the right attributes and context information to assure uniqueness is challenging.\\nThe URI scheme to use is [tag](https:\/\/tools.ietf.org\/html\/rfc4151). Tag URIs are quite human readable, but, unlike http URIs, not resolvable. This makes them suitable for identifying real-life things (entities) that have a representation on the ODH. A URI can be defined from the hierarchy of the organisation, forming a path to the resource. For example, if mycompany has a catalog with articles and one of the articles has article# 313, an article URI could be defined as `tag:vwt.digital,2020:mycompany\/catalog\/articles\/313`\\n\n\n##Decision\nWe define a URI according to the [tag scheme](https:\/\/tools.ietf.org\/html\/rfc4151) for each entity that has a representation on the ODH platform.\\n","MatchedIds":[2763,2963,1823,2183,2757],"Predicted":"## Decision\nWe will adopt the `tag` URI scheme for the ODH platform as it ensures human readability and proper identification of real-life entities represented on the platform. The URI will be constructed using meaningful attributes to enhance recognition and usability, thereby avoiding the complexity of technical randomly generated IDs.\n\nA standardized format will be followed for constructing the URIs which will include:\n1. The `tag` keyword.\n2. The domain name with the year of creation.\n3. A hierarchical path representing the organizational structure leading to the resource.\n\nFor example:\n- For an article with article# 313 in the catalog of `mycompany`, the URI will be: `tag:odh.data,2023:mycompany\/catalog\/articles\/313`.\n\nThis approach will ensure that URIs remain consistent, recognizable, and maintain their uniqueness across the ODH platform.","GenTime":"2024-08-28 14:21:26"}
{"File Name":"docs\/0012-winery-topology-modeler.md","Context":"## Context and Problem Statement\\nWe want to use a component to graphically model nodes and edges in a web application.\\n## Decision Drivers <!-- optional -->\\n* usable with Angular\\n* number of dependecies\\n* features\\n+ create Nodes\\n+ create Connections\\n* usability\\n","Decision":"* usable with Angular\\n* number of dependecies\\n* features\\n+ create Nodes\\n+ create Connections\\n* usability\\nChosen option: Choosen a Combination of ngx-graph and plain d3.js, because ngx-graph might miss some usefull features but has good usability which is worth to have a look at\\n### Positive Consequences <!-- optional -->\\n* easy to use graph modeler\\n* only a few dependencies\\n* full control over features and behavior\\n### Negative consequences <!-- optional -->\\n* needs to be implemented\\n","tokens":59,"id":4689,"text":"## Context and Problem Statement\\nWe want to use a component to graphically model nodes and edges in a web application.\\n## Decision Drivers <!-- optional -->\\n* usable with Angular\\n* number of dependecies\\n* features\\n+ create Nodes\\n+ create Connections\\n* usability\\n\n\n##Decision\n* usable with Angular\\n* number of dependecies\\n* features\\n+ create Nodes\\n+ create Connections\\n* usability\\nChosen option: Choosen a Combination of ngx-graph and plain d3.js, because ngx-graph might miss some usefull features but has good usability which is worth to have a look at\\n### Positive Consequences <!-- optional -->\\n* easy to use graph modeler\\n* only a few dependencies\\n* full control over features and behavior\\n### Negative consequences <!-- optional -->\\n* needs to be implemented\\n","MatchedIds":[1119,1151,1240,1659,3718],"Predicted":"## Decision\nChosen option: \"ngx-graph\", because\n- It is specifically designed for Angular, ensuring smooth integration with our existing stack.\n- It has a reasonable number of dependencies, reducing the likelihood of conflicts and simplifying maintenance.\n- It provides robust features for creating nodes and connections, allowing for comprehensive graphical modeling.\n- It is well-documented and has an active community, which enhances usability and support.\n\n### Positive Consequences\n- We can quickly start implementing node and edge modeling features in our web application.\n- The integration with Angular will be seamless, minimizing potential issues.\n- The component's active community and documentation will help our developers get up to speed swiftly.\n\n### Negative Consequences\n- As with any new dependency, there is an initial learning curve.\n- Future updates of ngx-graph could introduce breaking changes that we need to manage.","GenTime":"2024-08-28 14:21:29"}
{"File Name":"dlp-lux\/0003-Shibboleth.md","Context":"## Context\\nBoth Lux and Curate need to authenticate users to enforce authorization. Emory uses Shibboleth to manage\\nauthentication for University applications. Curate has gone through the lengthy process of requesting access to\\nShibboleth, and implemented user management through it. Part of the request was enumerating the \"attributes\" or\\nmetadata Curate needs to receive when a person logs in. Curate currently has three \"endpoints\", or applications,\\nthat can use its access to Shibboleth (prod, test, and arch).\\nLux also has access to Shibboleth, with the same three configured endpoints, and the same attributes.\\nLux does need to allow new users to sign in, where Curate does not.\\nIn production, neither application needs to allow users to authenticate from another source.\\n","Decision":"Lux will not use endpoints from Curate's Shibboleth access, instead using its own config. From the Lux side,\\na configuration option will be available to use database auth instead, for development, and to ease rollout.\\nIf the configuration option is not present, database auth will be used.\\n","tokens":167,"id":3214,"text":"## Context\\nBoth Lux and Curate need to authenticate users to enforce authorization. Emory uses Shibboleth to manage\\nauthentication for University applications. Curate has gone through the lengthy process of requesting access to\\nShibboleth, and implemented user management through it. Part of the request was enumerating the \"attributes\" or\\nmetadata Curate needs to receive when a person logs in. Curate currently has three \"endpoints\", or applications,\\nthat can use its access to Shibboleth (prod, test, and arch).\\nLux also has access to Shibboleth, with the same three configured endpoints, and the same attributes.\\nLux does need to allow new users to sign in, where Curate does not.\\nIn production, neither application needs to allow users to authenticate from another source.\\n\n\n##Decision\nLux will not use endpoints from Curate's Shibboleth access, instead using its own config. From the Lux side,\\na configuration option will be available to use database auth instead, for development, and to ease rollout.\\nIf the configuration option is not present, database auth will be used.\\n","MatchedIds":[2582,2578,2041,2397,5211],"Predicted":"## Decision\nBoth Lux and Curate will continue to utilize Shibboleth for user authentication, leveraging the same three configured endpoints (prod, test, and arch) and the same attributes. This will maintain consistency across University applications and centralize the authentication process.\n\nFor Lux, additional functionality will be implemented to facilitate the sign-up process for new users. This will be managed through the existing Shibboleth infrastructure, ensuring that all user data is securely handled and consistent with University standards.\n\nIn production environments, both Lux and Curate will strictly enforce authentication through Shibboleth and will not allow authentication from any other sources to maintain a unified and secure authentication process.\n\nThis decision aligns with Emory University's established authentication framework, simplifying maintenance and support while ensuring secure and consistent user management for Lux and Curate.","GenTime":"2024-08-28 14:21:32"}
{"File Name":"ea-talk\/0008-define-appropriate-schema-types.md","Context":"## Context\\nIn the course of trying to standardize how we do database development, we have had lots of discussion around schemas (much of the conversation around how oracle specifically views schemas, but the conversation *may* be relavant to other databases). This conversation has been mostly around how do do our database development, and how do we provide appropriate access to the required data.\\nThrougout this discussion (mostly happening in our DB Working Group), we have agreed on a set of definitions for different types of schemas.\\nHere are the notes from the original discussion:\\n","Decision":"We will use the following definitions for the different types of schemas in our databases:\\n### System Schemas\\nThese are schemas in the database that are completely outside of our control, used by the database itself as necessary. Even though we don't create or manage these, we are including them here for completness and categorization.\\n### DBA User Schemas\\nDBA User Schemas will exist for the DBAs to perform necessary functions in our databases.\\nThese schemas will have the highest level of access in our systems, and thus need to be the most careful about credentials and access.\\nDBA User Schemas should follow this naming convention:\\n```\\n{}_DBA\\n```\\nWhere {} is some useful identifier (i.e. `EXAMPLE_DBA`).\\nUsually, we have only one of these per database, `EXAMPLE_DBA`. If more are necessary, they should follow this convention.\\n### Application User Schemas\\nApplication user schemas will exist for each application that needs to access data in our databases.\\nApplication users should be granted the appropriate roles to access the necessary data. They should not be granted individual object grants.\\nApplication users should not have any object creation permissions (i.e. they should not be able to perform DDL operations).\\nApplication User Schemas should follow this naming convention:\\n```\\n{}_APP\\n```\\nWhere {} is the application name (i.e. `CALENDAR_APP`).\\n### Individual User Schemas\\nIndividual user schemas represent unique individual people that need access to database systems.\\nThese types of schemas are primarly used by developers and people in the organization that perform data analytics functions.\\nIndividual users should be granted the appropriate roles to access the necessary data. They should not be granted individual object grants.\\nIndividual users should have object creation permissions only for their own user schema.\\nIndividual User Schemas should be named the same as the user's email address. We prefer this to the AD account credentials because it follows a human-readable format, and better allows us to understand who the user is.\\n### Domain Schemas\\nDomain schemas are where the actual objects in the database exist (i.e. tables, views, functions, packages).\\nDomain schemas should have object creation permissions only for their own schema.\\nWhen domain schemas are created, the appropriate roles should also be created to be able to control access to the objects.\\nNote that these might exist in multiple database instances, e.g. custorder might have extract logic in POSP and tables in DSSP.\\nDomain Schemas should be named logically so their purpose can at least be guessed at.\\n### Replicated Schemas\\nReplicated Schmeas are similar to Domain schemas, except they will *only* include data whose system of record is elsewhere. They should not include any objects that aren't represented in the \"parent\" schema.\\nLike Domain schemas, the appropriate roles should also be created to be able to control access to the objects.\\nReplicated Schemas should follow this naming convention:\\n```\\n{}_REPL\\n```\\nWhere {} is the application name (i.e. `CALENDAR_REPL`).\\n","tokens":117,"id":1106,"text":"## Context\\nIn the course of trying to standardize how we do database development, we have had lots of discussion around schemas (much of the conversation around how oracle specifically views schemas, but the conversation *may* be relavant to other databases). This conversation has been mostly around how do do our database development, and how do we provide appropriate access to the required data.\\nThrougout this discussion (mostly happening in our DB Working Group), we have agreed on a set of definitions for different types of schemas.\\nHere are the notes from the original discussion:\\n\n\n##Decision\nWe will use the following definitions for the different types of schemas in our databases:\\n### System Schemas\\nThese are schemas in the database that are completely outside of our control, used by the database itself as necessary. Even though we don't create or manage these, we are including them here for completness and categorization.\\n### DBA User Schemas\\nDBA User Schemas will exist for the DBAs to perform necessary functions in our databases.\\nThese schemas will have the highest level of access in our systems, and thus need to be the most careful about credentials and access.\\nDBA User Schemas should follow this naming convention:\\n```\\n{}_DBA\\n```\\nWhere {} is some useful identifier (i.e. `EXAMPLE_DBA`).\\nUsually, we have only one of these per database, `EXAMPLE_DBA`. If more are necessary, they should follow this convention.\\n### Application User Schemas\\nApplication user schemas will exist for each application that needs to access data in our databases.\\nApplication users should be granted the appropriate roles to access the necessary data. They should not be granted individual object grants.\\nApplication users should not have any object creation permissions (i.e. they should not be able to perform DDL operations).\\nApplication User Schemas should follow this naming convention:\\n```\\n{}_APP\\n```\\nWhere {} is the application name (i.e. `CALENDAR_APP`).\\n### Individual User Schemas\\nIndividual user schemas represent unique individual people that need access to database systems.\\nThese types of schemas are primarly used by developers and people in the organization that perform data analytics functions.\\nIndividual users should be granted the appropriate roles to access the necessary data. They should not be granted individual object grants.\\nIndividual users should have object creation permissions only for their own user schema.\\nIndividual User Schemas should be named the same as the user's email address. We prefer this to the AD account credentials because it follows a human-readable format, and better allows us to understand who the user is.\\n### Domain Schemas\\nDomain schemas are where the actual objects in the database exist (i.e. tables, views, functions, packages).\\nDomain schemas should have object creation permissions only for their own schema.\\nWhen domain schemas are created, the appropriate roles should also be created to be able to control access to the objects.\\nNote that these might exist in multiple database instances, e.g. custorder might have extract logic in POSP and tables in DSSP.\\nDomain Schemas should be named logically so their purpose can at least be guessed at.\\n### Replicated Schemas\\nReplicated Schmeas are similar to Domain schemas, except they will *only* include data whose system of record is elsewhere. They should not include any objects that aren't represented in the \"parent\" schema.\\nLike Domain schemas, the appropriate roles should also be created to be able to control access to the objects.\\nReplicated Schemas should follow this naming convention:\\n```\\n{}_REPL\\n```\\nWhere {} is the application name (i.e. `CALENDAR_REPL`).\\n","MatchedIds":[4167,5244,2710,3776,2828],"Predicted":"## Context\nIn the course of trying to standardize how we do database development, we have had lots of discussion around schemas (much of the conversation around how Oracle specifically views schemas, but the conversation *may* be relevant to other databases). This conversation has been mostly around how to do our database development, and how to provide appropriate access to the required data.\nThroughout this discussion (mostly happening in our DB Working Group), we have agreed on a set of definitions for different types of schemas.\nHere are the notes from the original discussion:\n\n## Decision Drivers\n* Standardization of database development practices.\n* Providing appropriate and secure access to data.\n* Compatibility with various database management systems.\n* Ease of maintenance and scalability.\n\n## Decision\nWe have decided to adopt a standardized schema strategy with the following definitions and guidelines for different types of schemas:\n\n1. **Primary Schema**:\n   - The primary schema will contain the core tables and objects that are essential for application functionality.\n   - This schema will be version-controlled and changes to it will follow a strict review process.\n\n2. **Auxiliary Schemas**:\n   - These schemas will contain non-core supplemental data that supports the functionality of the primary schema but is not essential for critical application operations.\n   - Examples include logging, auditing, and metadata.\n\n3. **Access Control Schema**:\n   - This schema is dedicated to managing access control rules, roles, and permissions.\n   - This allows for a centralized and clear separation of security-related data from application data.\n\n4. **Development and Testing Schemas**:\n   - Separate schemas will be provided for development, testing, and staging environments.\n   - These schemas will mirror the primary schema structure but will be isolated to ensure that changes can be tested without affecting production data.\n\n5. **Data Warehouse Schema**:\n   - A specialized schema for reporting and data analysis and structured for optimized read performance.\n   - This schema will source data from the primary schema but will be designed with additional indexed tables and aggregates to facilitate analytical queries.\n\n### Positive Consequences\n* **Standardization**: Provides a consistent approach across different projects and teams, reducing confusion and improving collaboration.\n* **Security**: Clear separation of access control rules improves the security posture by reducing the risk of accidental data exposure.\n* **Maintainability**: Separate schemas for different purposes (e.g., development, testing, production) make it easier to manage and maintain code and data.\n* **Performance**: Optimized data warehouse schema enhances performance for analytical queries.\n\n### Negative Consequences\n* **Increased Complexity**: Multiple schemas may introduce additional complexity in terms of management and understanding for new team members.\n* **Oversight Required**: Strict control and review processes for the primary schema may slow down development speed.\n* **Replication Overhead**: Maintaining separate development, testing, and production schemas might introduce additional overhead in terms of data replication and synchronization efforts.\n\nBy adhering to these definitions and guidelines, we aim to achieve a more organized, secure, and scalable approach to database development and management across our organization.","GenTime":"2024-08-28 14:21:42"}
{"File Name":"sexual-health-service-finder\/0006-use-in-house-postcodes-search-rather-than-postcodes-io.md","Context":"## Context\\nThe application has been using https:\/\/postcodes.io\/. Postcodes.io is available\\nfor free and does not require any relationship between consumer and supplier.\\nOne of the drawbacks to this is that although support is available when using\\nthe free tier, there is no provision for any specific support relationship.\\nMore formal relationships are available but this is through the paid product\\nhttps:\/\/ideal-postcodes.co.uk\/.  Given the number of requests and the fact the\\nfree tier was supplying the information required, it had previously been\\ndecided there was no need to migrate to a paid product. However, the strategic\\ndirection has been set such that high value APIs, usable across many products\\nshould be supplied in-house. To that end, there is now an in-house postcode\\nsearch available.\\n","Decision":"The decision is to replace the use of postcodes.io with the in-house postcode\\nlookup, as per the strategic direction.\\n","tokens":165,"id":4276,"text":"## Context\\nThe application has been using https:\/\/postcodes.io\/. Postcodes.io is available\\nfor free and does not require any relationship between consumer and supplier.\\nOne of the drawbacks to this is that although support is available when using\\nthe free tier, there is no provision for any specific support relationship.\\nMore formal relationships are available but this is through the paid product\\nhttps:\/\/ideal-postcodes.co.uk\/.  Given the number of requests and the fact the\\nfree tier was supplying the information required, it had previously been\\ndecided there was no need to migrate to a paid product. However, the strategic\\ndirection has been set such that high value APIs, usable across many products\\nshould be supplied in-house. To that end, there is now an in-house postcode\\nsearch available.\\n\n\n##Decision\nThe decision is to replace the use of postcodes.io with the in-house postcode\\nlookup, as per the strategic direction.\\n","MatchedIds":[2204,2410,4378,5213,5211],"Predicted":"## Decision\nMigrate from using https:\/\/postcodes.io\/ to the newly available in-house postcode search service. This decision aligns with the strategic direction of supplying high-value APIs in-house, thereby ensuring better control and support for critical integrations. \n\n### Positive Consequences\n* Full control over the service with the ability to implement custom features and optimizations as needed.\n* Established support processes and SLAs to handle any issues or outages, improving service reliability.\n* Reduced dependency on external third-party services, minimizing the risk associated with sudden changes in third-party pricing or service availability.\n* Potential for tighter integration with other in-house services, enabling richer feature sets and improved data consistency.\n\n### Negative Consequences\n* Initial migration effort required to switch from https:\/\/postcodes.io\/ to the in-house service.\n* Possible initial teething issues as the new in-house service might need to stabilize and mature.\n* Resource allocation needed for ongoing maintenance and support of the in-house service.","GenTime":"2024-08-28 14:21:45"}
{"File Name":"linshare-mobile-flutter-app\/0006-download-file-manager.md","Context":"## Context\\nIn LinShare Flutter application, we can download files one by one from \"My Space\", \"Received Shares\" or \"Shared Spaces\" with the library `flutter_downloader` by adding it to the queue :\\n```\\nTaskId enqueue(String url, String saveDir, String header...)\\n```\\nWe can also clear or cancel file queue with dedicated functions.\\n","Decision":"We decided that LinShare could currently download a file one by one.\\n","tokens":80,"id":3297,"text":"## Context\\nIn LinShare Flutter application, we can download files one by one from \"My Space\", \"Received Shares\" or \"Shared Spaces\" with the library `flutter_downloader` by adding it to the queue :\\n```\\nTaskId enqueue(String url, String saveDir, String header...)\\n```\\nWe can also clear or cancel file queue with dedicated functions.\\n\n\n##Decision\nWe decided that LinShare could currently download a file one by one.\\n","MatchedIds":[1647,1094,1091,1643,4533],"Predicted":"## Decision\nWe will utilize the `flutter_downloader` library for managing file downloads in the LinShare Flutter application. This involves adding files to the download queue using the `enqueue` method, which allows us to specify the URL, the directory where the file should be saved, and any necessary headers. Additionally, we will leverage the library's functions to clear or cancel the file queue as needed. This decision supports efficient background downloads and provides flexibility in managing download tasks, ensuring a smooth user experience.","GenTime":"2024-08-28 14:21:47"}
{"File Name":"ockam\/0005-routing-data-formats.md","Context":"## Context\\nWe need a standard format for messages to be exchanged by the routing protocol.\\nThis format would be used by routers on different implementations.\\n","Decision":"We use the following formats:\\nFor a message:\\n```\\n{\\nonward_route: Route,\\nreturn_route: Route,\\npayload: Any\\n}\\n```\\nWhere\\n`Route` - an ordered list of addresses.\\nFor an address:\\n```\\n{\\ntype: Integer,\\ndata: Any\\n}\\n```\\n","tokens":31,"id":4538,"text":"## Context\\nWe need a standard format for messages to be exchanged by the routing protocol.\\nThis format would be used by routers on different implementations.\\n\n\n##Decision\nWe use the following formats:\\nFor a message:\\n```\\n{\\nonward_route: Route,\\nreturn_route: Route,\\npayload: Any\\n}\\n```\\nWhere\\n`Route` - an ordered list of addresses.\\nFor an address:\\n```\\n{\\ntype: Integer,\\ndata: Any\\n}\\n```\\n","MatchedIds":[2181,1180,1442,4097,3746],"Predicted":"## Decision\nWe use the [Protocol Buffers](https:\/\/developers.google.com\/protocol-buffers) (protobuf) format for message serialization and exchange in the routing protocol. Protocol Buffers are widely used for defining and serializing structured data, providing a language-neutral, platform-neutral extensible mechanism for serializing structured data.\n\n### Message Structure\nThe message structure will follow a predefined schema defined in `.proto` files, which will be shared across different router implementations to ensure compatibility.\n\n#### Example `proto` file\n```proto\nsyntax = \"proto3\";\n\npackage routing;\n\n\/\/ The RouterMessage is a message that can be exchanged between routers.\nmessage RouterMessage {\n  string id = 1; \/\/ Unique identifier for the message\n  string type = 2; \/\/ Type of message (e.g., \"update\", \"query\")\n  bytes payload = 3; \/\/ Serialized payload data\n  int64 timestamp = 4; \/\/ Timestamp of the message\n}\n\n\/\/ An example of a specific message payload\nmessage UpdatePayload {\n  string route = 1; \/\/ Route information\n  map<string, string> attributes = 2; \/\/ Additional attributes\n}\n```\n\n### Integration\nAll routers will implement the protobuf serialization and deserialization for routing protocol messages. This ensures that messages can be interpreted correctly by any router in the network, regardless of the implementation language or platform.\n\n### Benefits\n- **Language Neutrality:** Protobuf supports generating code for multiple programming languages.\n- **Efficiency:** Protobuf messages are compact, reducing the overhead of data transmission.\n- **Extensibility:** Protobuf allows for backward and forward compatibility when adding new fields to the message.\n\n### Example Usage\n#### Serialization\n```go\nimport (\n  \"github.com\/golang\/protobuf\/proto\"\n  \"log\"\n)\n\nupdatePayload := &UpdatePayload{\n  Route: \"192.168.1.0\/24\",\n  Attributes: map[string]string{\n    \"next_hop\": \"192.168.1.1\",\n    \"metric\": \"100\",\n  },\n}\n\npayloadBytes, err := proto.Marshal(updatePayload)\nif err != nil {\n  log.Fatal(\"Failed to serialize payload: \", err)\n}\n\nrouterMessage := &RouterMessage{\n  Id: \"12345\",\n  Type: \"update\",\n  Payload: payloadBytes,\n  Timestamp: time.Now().Unix(),\n}\n\nmessageBytes, err := proto.Marshal(routerMessage)\nif err != nil {\n  log.Fatal(\"Failed to serialize router message: \", err)\n}\n\n\/\/ messageBytes can now be transmitted over the network\n```\n\n#### Deserialization\n```go\nreceivedBytes := \/\/ bytes received from the network\n\nrouterMessage := &RouterMessage{}\nerr := proto.Unmarshal(receivedBytes, routerMessage)\nif err != nil {\n  log.Fatal(\"Failed to deserialize router message: \", err)\n}\n\nupdatePayload := &UpdatePayload{}\nerr = proto.Unmarshal(routerMessage.Payload, updatePayload)\nif err != nil {\n  log.Fatal(\"Failed to deserialize payload: \", err)\n}\n\n\/\/ Now updatePayload contains the deserialized data\n```\n\nBy adopting Protocol Buffers for message serialization, we ensure a consistent, efficient, and extensible communication format across different routing protocol implementations.","GenTime":"2024-08-28 14:21:56"}
{"File Name":"linshare-mobile-android-app\/0008-download-with-downloadmanager-service.md","Context":"## Context\\nWe have some ways to perform downloading stable in the background, but system exposed a service called `DownloadManager`.\\nClient may request that a URI be downloaded to a particular destination file. The download manager will conduct the\\ndownload in the background, taking care of HTTP interactions and retrying downloads after failures or across connectivity changes and system reboot.\\nApps that request downloads through this API can register a broadcast receiver to handle when the download is progress, failure, completed.\\n","Decision":"Instead of implementing a `Worker` like `Upload`, we will delegate downloading task to `DownloadManager` system service.\\n","tokens":99,"id":1647,"text":"## Context\\nWe have some ways to perform downloading stable in the background, but system exposed a service called `DownloadManager`.\\nClient may request that a URI be downloaded to a particular destination file. The download manager will conduct the\\ndownload in the background, taking care of HTTP interactions and retrying downloads after failures or across connectivity changes and system reboot.\\nApps that request downloads through this API can register a broadcast receiver to handle when the download is progress, failure, completed.\\n\n\n##Decision\nInstead of implementing a `Worker` like `Upload`, we will delegate downloading task to `DownloadManager` system service.\\n","MatchedIds":[1643,3297,4974,2574,2763],"Predicted":"## Decision\nWe will use the system's `DownloadManager` service to handle background downloads. This approach ensures reliability in handling HTTP interactions, retrying downloads after failures, and maintaining persistence across connectivity changes and system reboots. Additionally, we will implement a broadcast receiver to manage download progress, failure, and completion notifications, providing a robust solution for background downloading requirements.","GenTime":"2024-08-28 14:21:58"}
{"File Name":"libelektra\/capabilities.md","Context":"## Problem\\nOnly plugins like `dump` and `quickdump` are able to represent any KeySet\\n(as they are designed to do so). Limitations of other storage plugins are\\ne.g., that not every structure of configuration is allowed.\\nSome of these limitations were documented `infos\/status`, others were not.\\n","Decision":"Add `infos\/features\/storage` to document limitations of storage plugins.\\nIdeally, storage plugins should throw an error in `kdbSet` for\\nunrepresentable KeySets.\\nElektra cannot guarantee that any configuration file format can\\nbe mounted anywhere.\\nDevelopers, maintainers and administrators are responsible for what\\nthey mount. They need to test the setup.\\n- [Base Name](base_name.md)\\n","tokens":66,"id":1318,"text":"## Problem\\nOnly plugins like `dump` and `quickdump` are able to represent any KeySet\\n(as they are designed to do so). Limitations of other storage plugins are\\ne.g., that not every structure of configuration is allowed.\\nSome of these limitations were documented `infos\/status`, others were not.\\n\n\n##Decision\nAdd `infos\/features\/storage` to document limitations of storage plugins.\\nIdeally, storage plugins should throw an error in `kdbSet` for\\nunrepresentable KeySets.\\nElektra cannot guarantee that any configuration file format can\\nbe mounted anywhere.\\nDevelopers, maintainers and administrators are responsible for what\\nthey mount. They need to test the setup.\\n- [Base Name](base_name.md)\\n","MatchedIds":[1278,1271,1310,1289,1309],"Predicted":"## Decision\n- Clearly document the limitations of each storage plugin within `infos\/status`.\n- Update the documentation to reflect the structural constraints and specific features that each plugin either supports or does not.\n- Implement runtime checks within the plugins to enforce their documented constraints. If a KeySet structure is not supported, an informative error message will be generated.\n- Ensure that developers are informed up-front about the limitations of each storage plugin, thus guiding them to choose the appropriate plugin for their use case.\n- Extend the existing plugins where feasible to accommodate more structures, or document the need for such features where implementation is impractical.\n- Provide a summary of structures supported by each plugin as a high-level guide in the documentation, facilitating quicker decisions for developers.\n- Integrate tests that validate the limitations and document these tests to serve both as verification and examples.","GenTime":"2024-08-28 14:22:02"}
{"File Name":"govuk-docker\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\nThe [original govuk-docker repo](https:\/\/github.com\/benthorner\/govuk-docker) was written outside of GOV.UK. Major architectural decisions, as well as more minor decisions, were written as [documentation in the repo][DECISIONS], together with their associated user [NEEDS]. While these documents have historical value, they are not being maintained and increasingly differ from the current state of the repo. As part of adopting an ADR approach, we should clearly deprecate these historical documents to avoid confusion.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":127,"id":1669,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\nThe [original govuk-docker repo](https:\/\/github.com\/benthorner\/govuk-docker) was written outside of GOV.UK. Major architectural decisions, as well as more minor decisions, were written as [documentation in the repo][DECISIONS], together with their associated user [NEEDS]. While these documents have historical value, they are not being maintained and increasingly differ from the current state of the repo. As part of adopting an ADR approach, we should clearly deprecate these historical documents to avoid confusion.\\n\n\n##Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","MatchedIds":[2091,4428,3119,1436,294],"Predicted":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions). The existing documentation in the repo, including both major and minor architectural decisions along with their associated user needs, will be formally deprecated to avoid confusion. This deprecation will be clearly indicated in the repo, and moving forward, all architectural decisions will be recorded as ADRs using the new standardized format.","GenTime":"2024-08-28 14:22:04"}
{"File Name":"celestia-core\/adr-018-ABCI-Validators.md","Context":"## Context\\nADR 009 introduced major improvements to the ABCI around validators and the use\\nof Amino. Here we follow up with some additional changes to improve the naming\\nand expected use of Validator messages.\\n","Decision":"### Validator\\nCurrently a Validator contains `address` and `pub_key`, and one or the other is\\noptional\/not-sent depending on the use case. Instead, we should have a\\n`Validator` (with just the address, used for RequestBeginBlock)\\nand a `ValidatorUpdate` (with the pubkey, used for ResponseEndBlock):\\n```\\nmessage Validator {\\nbytes address\\nint64 power\\n}\\nmessage ValidatorUpdate {\\nPubKey pub_key\\nint64 power\\n}\\n```\\nAs noted in [ADR-009](adr-009-ABCI-design.md),\\nthe `Validator` does not contain a pubkey because quantum public keys are\\nquite large and it would be wasteful to send them all over ABCI with every block.\\nThus, applications that want to take advantage of the information in BeginBlock\\nare _required_ to store pubkeys in state (or use much less efficient lazy means\\nof verifying BeginBlock data).\\n### RequestBeginBlock\\nLastCommitInfo currently has an array of `SigningValidator` that contains\\ninformation for each validator in the entire validator set.\\nInstead, this should be called `VoteInfo`, since it is information about the\\nvalidator votes.\\nNote that all votes in a commit must be from the same round.\\n```\\nmessage LastCommitInfo {\\nint64 round\\nrepeated VoteInfo commit_votes\\n}\\nmessage VoteInfo {\\nValidator validator\\nbool signed_last_block\\n}\\n```\\n### ResponseEndBlock\\nUse ValidatorUpdates instead of Validators. Then it's clear we don't need an\\naddress, and we do need a pubkey.\\nWe could require the address here as well as a sanity check, but it doesn't seem\\nnecessary.\\n### InitChain\\nUse ValidatorUpdates for both Request and Response. InitChain\\nis about setting\/updating the initial validator set, unlike BeginBlock\\nwhich is just informational.\\n","tokens":44,"id":4119,"text":"## Context\\nADR 009 introduced major improvements to the ABCI around validators and the use\\nof Amino. Here we follow up with some additional changes to improve the naming\\nand expected use of Validator messages.\\n\n\n##Decision\n### Validator\\nCurrently a Validator contains `address` and `pub_key`, and one or the other is\\noptional\/not-sent depending on the use case. Instead, we should have a\\n`Validator` (with just the address, used for RequestBeginBlock)\\nand a `ValidatorUpdate` (with the pubkey, used for ResponseEndBlock):\\n```\\nmessage Validator {\\nbytes address\\nint64 power\\n}\\nmessage ValidatorUpdate {\\nPubKey pub_key\\nint64 power\\n}\\n```\\nAs noted in [ADR-009](adr-009-ABCI-design.md),\\nthe `Validator` does not contain a pubkey because quantum public keys are\\nquite large and it would be wasteful to send them all over ABCI with every block.\\nThus, applications that want to take advantage of the information in BeginBlock\\nare _required_ to store pubkeys in state (or use much less efficient lazy means\\nof verifying BeginBlock data).\\n### RequestBeginBlock\\nLastCommitInfo currently has an array of `SigningValidator` that contains\\ninformation for each validator in the entire validator set.\\nInstead, this should be called `VoteInfo`, since it is information about the\\nvalidator votes.\\nNote that all votes in a commit must be from the same round.\\n```\\nmessage LastCommitInfo {\\nint64 round\\nrepeated VoteInfo commit_votes\\n}\\nmessage VoteInfo {\\nValidator validator\\nbool signed_last_block\\n}\\n```\\n### ResponseEndBlock\\nUse ValidatorUpdates instead of Validators. Then it's clear we don't need an\\naddress, and we do need a pubkey.\\nWe could require the address here as well as a sanity check, but it doesn't seem\\nnecessary.\\n### InitChain\\nUse ValidatorUpdates for both Request and Response. InitChain\\nis about setting\/updating the initial validator set, unlike BeginBlock\\nwhich is just informational.\\n","MatchedIds":[4656,21,3159,405,1949],"Predicted":"## Decision\n### Validator Update Naming\nTo improve clarity and consistency in Validator messages, we recommend revisiting and updating the naming conventions around Validator messages. Specifically, the term \"Validator\" should be used consistently across both naming conventions and data structures.\n\n#### ValidatorSetUpdate\nRename the current `Validator` message to `ValidatorUpdate` to better convey its purpose as an update to the validator set:\n```protobuf\nmessage ValidatorUpdate {\n    ValidatorAddress address = 1;\n    PubKey pub_key = 2;\n    int64 power = 3;\n}\n```\n\n#### ValidatorSet\nIntroduce a `ValidatorSet` message to encapsulate a set of validators. This can be used where multiple validators need to be represented together, such as in the `EndBlock` response:\n```protobuf\nmessage ValidatorSet {\n    repeated ValidatorUpdates validators = 1;\n}\n```\n\n### Expected Use of Validator Messages\n#### Validator Creation\nValidators are initialized at genesis using a `ValidatorSet` message:\n```protobuf\nmessage Genesis {\n    ValidatorSet initial_validator_set = 1;\n    ...\n}\n```\n\n#### Validator Updates\nDuring normal operation, validator updates should be done through the `ValidatorUpdate` messages, typically submitted in the `EndBlock` response:\n```protobuf\nmessage EndBlockResponse {\n    ValidatorSet validator_updates = 1;\n    ...\n}\n```\n\nTo ensure compatibility and facilitate robust testing, a migration tool should be supplied to translate existing `Validator` messages into the new `ValidatorUpdate` and `ValidatorSet` structures. This way, existing systems can gradually adopt the new formats without breaking compatibility.\n\n### Adoption Timeline\nWe propose a phased adoption of the new Validator messages:\n1. **Phase 1: Initial Implementation**: Introduce new messages and update documentation.\n2. **Phase 2: Deprecation Warning**: Add deprecation warnings to the old `Validator` messages.\n3. **Phase 3: Full Migration**: Remove old `Validator` messages and enforce the use of `ValidatorUpdate` and `ValidatorSet`.\n\nAdopting these changes will not only improve the clarity and intent of the validator-related messages but also streamline interactions with them, paving the way for easier maintenance and future enhancements.","GenTime":"2024-08-28 14:22:12"}
{"File Name":"HES_pipeline\/data_storage_and_access.md","Context":"## Context\\nThe raw HES data is large, complex, unweildly and potentially dirty. A way to\\nclean, store and later easily access the data for analysis required, which means\\nthat repeated cleaning and prep of the raw data is avoided.\\nThere are multiple datasets (with different variables), which once cleaned, will\\nbe accompanied by additional derived variables, and additional derived datasets.\\n","Decision":"The raw data will be read into a SQLite database through R. R is The Health\\nFoundation's open-source language of choice, and is well equiped to\\nhandle later analysis of said data. An SQLite database is not stored in memory,\\navoiding holding very large datasets an R session. It is a standard database\\nformat which can be easily queried with SQL which can in turn be written within\\nmost other common languagses, including R.\\nIn an SQLite database the database consistents of all the data, and within that\\nthere are tables; here each table corresponds to a HES dataset or a derived\\ndataset.\\n","tokens":83,"id":3253,"text":"## Context\\nThe raw HES data is large, complex, unweildly and potentially dirty. A way to\\nclean, store and later easily access the data for analysis required, which means\\nthat repeated cleaning and prep of the raw data is avoided.\\nThere are multiple datasets (with different variables), which once cleaned, will\\nbe accompanied by additional derived variables, and additional derived datasets.\\n\n\n##Decision\nThe raw data will be read into a SQLite database through R. R is The Health\\nFoundation's open-source language of choice, and is well equiped to\\nhandle later analysis of said data. An SQLite database is not stored in memory,\\navoiding holding very large datasets an R session. It is a standard database\\nformat which can be easily queried with SQL which can in turn be written within\\nmost other common languagses, including R.\\nIn an SQLite database the database consistents of all the data, and within that\\nthere are tables; here each table corresponds to a HES dataset or a derived\\ndataset.\\n","MatchedIds":[1016,1863,4234,443,914],"Predicted":"## Decision\nWe will create an ETL (Extract, Transform, Load) pipeline to handle the raw HES data. The pipeline will:\n\n1. **Extract** the raw HES data from its source.\n2. **Transform** the data by cleaning and preparing it, including:\n   - Handling missing values,\n   - Standardizing variable formats,\n   - Adding derived variables,\n   - Creating derived datasets as needed.\n3. **Load** the cleaned and prepared data into a cloud-based data storage solution such as Amazon Redshift or Google BigQuery.\n\nWe will also implement a scheduling tool (e.g., Apache Airflow) to automate the ETL process, ensuring the data remains updated without repeated manual intervention. This approach will ensure that all subsequent analytical tasks can access a standardized, clean version of the data, thus streamlining the analysis process and ensuring data quality and consistency.","GenTime":"2024-08-28 14:22:16"}
{"File Name":"crispy-dragon\/ADR-2-Frontend-Framework.md","Context":"## Context\\nTo make it easy for users to interact with our API we need a frontend that can be used by modern devices.\\n","Decision":"The frontend will be build with [React](https:\/\/reactjs.org\/) and [Snowpack](https:\/\/www.snowpack.dev) will be used as build tool.\\nThe application will be styled with [Tailwind CSS](https:\/\/tailwindcss.com)\\nWe also considered the following alternative solutions:\\n* Vanilla HTML would work for the scope of this application.\\n* Vanilla CSS would work, but take more time to generate nice layouts.\\n","tokens":27,"id":847,"text":"## Context\\nTo make it easy for users to interact with our API we need a frontend that can be used by modern devices.\\n\n\n##Decision\nThe frontend will be build with [React](https:\/\/reactjs.org\/) and [Snowpack](https:\/\/www.snowpack.dev) will be used as build tool.\\nThe application will be styled with [Tailwind CSS](https:\/\/tailwindcss.com)\\nWe also considered the following alternative solutions:\\n* Vanilla HTML would work for the scope of this application.\\n* Vanilla CSS would work, but take more time to generate nice layouts.\\n","MatchedIds":[1240,3715,3982,1180,1243],"Predicted":"## Decision\nWe use React for the frontend because it provides a component-based architecture that is easy to maintain and extend. Additionally, React has strong community support, comprehensive documentation, and is well-suited for building responsive interfaces that work seamlessly on modern devices. To ensure a consistent and user-friendly design, we will use a modern UI library like Material-UI along with React.","GenTime":"2024-08-28 14:22:19"}
{"File Name":"monocle\/0010-choice-of-protobuf.md","Context":"## Context and Problem Statement\\n* The current API implementations is spread across multiple modules (such as webapp, config, db and query) without a formal definition of the inputs\/outputs. This induces inconsistencies and we foresee technical difficulties to maintain and evole the service.\\n* We would like to use an interface description language to define the API between the various component (such as between the webapi and webclients, worker and database).\\n* How do we define and maintain the interfaces?\\n","Decision":"Chosen option: \"Protobuf\", because it comes out best (see below).\\n### Positive Consequences\\n- We need to start defining interfaces outside of their implementations.\\n- We need to use a simpler HTTP api (e.g. the path for the action function name and the body for its input), instead of encoding the inputs with a mix of path, querystring and body components.\\nFor example, use `POST \/get_projects BODY { \"index\": \"name\" }` instead of `GET \/projects?index=name`.\\n### Negative Consequences\\n- The team needs to learn a new language.\\n- Follow-up decisions required for the actual implementation.\\n","tokens":100,"id":4644,"text":"## Context and Problem Statement\\n* The current API implementations is spread across multiple modules (such as webapp, config, db and query) without a formal definition of the inputs\/outputs. This induces inconsistencies and we foresee technical difficulties to maintain and evole the service.\\n* We would like to use an interface description language to define the API between the various component (such as between the webapi and webclients, worker and database).\\n* How do we define and maintain the interfaces?\\n\n\n##Decision\nChosen option: \"Protobuf\", because it comes out best (see below).\\n### Positive Consequences\\n- We need to start defining interfaces outside of their implementations.\\n- We need to use a simpler HTTP api (e.g. the path for the action function name and the body for its input), instead of encoding the inputs with a mix of path, querystring and body components.\\nFor example, use `POST \/get_projects BODY { \"index\": \"name\" }` instead of `GET \/projects?index=name`.\\n### Negative Consequences\\n- The team needs to learn a new language.\\n- Follow-up decisions required for the actual implementation.\\n","MatchedIds":[693,1180,2181,4307,2834],"Predicted":"## Decision\nWe will use **Protocol Buffers (protobuf)**, a language-neutral, platform-neutral extensible mechanism for serializing structured data, to define and maintain the interfaces between the different components.\n\n### Positive Consequences\n* **Clear and Formal API Definitions**: Protocol Buffers will provide a clear and formal definition of the APIs, minimizing inconsistencies.\n* **Backward and Forward Compatibility**: Protocol Buffers support backward and forward compatibility which helps with evolving the services without breaking existing functionality.\n* **Improved Performance**: Protobuf is highly efficient in terms of both serialization and deserialization, which could improve the performance of inter-component communication.\n* **Multi-language Support**: Protocol Buffers support multiple programming languages, allowing us to use the same definitions across different services regardless of the language they are implemented in.\n\n### Negative Consequences\n* **Learning Curve**: Developers will need to learn Protocol Buffers and integrate it into their workflows, which might entail some initial overhead.\n* **Interfacing Complexity**: Maintaining proto files and evolving them might add to the complexity of the development process.\n* **Tooling Dependency**: Dependency on specific tools and compilers for generating code from proto definitions.\n\n### Implementation Plan\n1. **Define Proto Files**: Define .proto files for each service's API, clearly specifying the request and response messages.\n2. **Generate Code**: Utilize the protoc compiler to generate client and server code for the respective languages used in the services.\n3. **Integrate into Build Processes**: Integrate the code generation step into the build processes of the respective services.\n4. **Version Management**: Implement a versioning strategy for the proto files to handle backward and forward compatibility.\n5. **Documentation and Training**: Provide documentation and training for developers to facilitate the transition to using Protocol Buffers.\n\nBy adopting Protocol Buffers, we aim to consistently and efficiently define APIs, thus facilitating easier maintenance and evolution of the service.","GenTime":"2024-08-28 14:22:25"}
{"File Name":"hee-web-blueprint\/0003-use-github-actions-for-automation.md","Context":"## Context\\nWe should determine the platform we use to automate the build, test and deployment of the HEE National Website Platform.\\n","Decision":"Having determined to use Github to manage the source code of the platform, the simplest answer to this question was to look at Github actions. We determined after some investigative work to prove out our ability to deploy to the BloomReach cloud that we should use Github Actions to manage platform automation.\\n","tokens":27,"id":1196,"text":"## Context\\nWe should determine the platform we use to automate the build, test and deployment of the HEE National Website Platform.\\n\n\n##Decision\nHaving determined to use Github to manage the source code of the platform, the simplest answer to this question was to look at Github actions. We determined after some investigative work to prove out our ability to deploy to the BloomReach cloud that we should use Github Actions to manage platform automation.\\n","MatchedIds":[1239,4041,3722,469,2393],"Predicted":"## Decision\nWe will use GitHub Actions to automate the build, test, and deployment of the HEE National Website Platform. GitHub Actions integrates seamlessly with our version control system, offers robust CI\/CD capabilities, and supports Docker, which aligns with our need to containerize our application. Additionally, it provides built-in support for multiple environments and simplifies the management of secrets and dependencies.","GenTime":"2024-08-28 14:22:27"}
{"File Name":"texas-holdem-code-challenge\/0008-use-typescript.md","Context":"## Context\\nThe approach I have chosen (see docs\/model.md and docs\/algorithm.md) leads to\\nmany small unit-testable components and a main function that ties them together.\\nThe options I can see for the main function are:\\n1. Write it with unit tests, heavily mocking the dependencies\\n2. Use a type system to check that the plumbing matches up, don't unit test the\\nmain function\\n3. Don't test the main function, don't use types. My guess is I'll make some\\nmistake along the way and have to debug it.\\nIn all cases I'll run integration tests, but without (1) or (2) I won't know\\nwhere bugs are when the integration test fails.\\nMy guess is that (2) will be more efficient than (1), and either of them will be\\nmore efficient than (3).\\nFor JavaScript static typing, I'm familiar with TypeScript.\\n","Decision":"Use TypeScript.\\n","tokens":193,"id":2290,"text":"## Context\\nThe approach I have chosen (see docs\/model.md and docs\/algorithm.md) leads to\\nmany small unit-testable components and a main function that ties them together.\\nThe options I can see for the main function are:\\n1. Write it with unit tests, heavily mocking the dependencies\\n2. Use a type system to check that the plumbing matches up, don't unit test the\\nmain function\\n3. Don't test the main function, don't use types. My guess is I'll make some\\nmistake along the way and have to debug it.\\nIn all cases I'll run integration tests, but without (1) or (2) I won't know\\nwhere bugs are when the integration test fails.\\nMy guess is that (2) will be more efficient than (1), and either of them will be\\nmore efficient than (3).\\nFor JavaScript static typing, I'm familiar with TypeScript.\\n\n\n##Decision\nUse TypeScript.\\n","MatchedIds":[2652,3153,4785,1472,4940],"Predicted":"## Decision\nWe will use a type system to check that the plumbing matches up and not unit test the main function (option 2). Given that we are familiar with TypeScript for JavaScript static typing, we'll utilize TypeScript to enforce contract compliance and type safety for the main function and its dependencies. This approach ensures that the small, unit-testable components remain thoroughly tested while efficiently catching plumbing errors in the main function through compile-time checks. Integration tests will complement this strategy to identify any overarching issues. This method balances efficiency and reliability, providing a safeguard against potential mistakes without excessive mocking.","GenTime":"2024-08-28 14:22:30"}
{"File Name":"dapr\/API-009-bidirectional-bindings.md","Context":"## Context\\nAs we want to provide bi-directional capabilities for bindings to allow for cases such as getting a blob from a storage account,\\nAn API change is needed to account for the requested type of operation.\\n","Decision":"### Naming\\nIt was decided to keep the bindings name as is. Alternative proposals were included changing bindings to connectors, but a strong case couldn't be made in favor of connectors to justify the breaking change it would cause.\\n### Types\\nIt was decided to keep the same YAML format for both input bindings and bi-directional bindings as it is today.\\nAfter careful inspection, splitting to two types (for example, trigger bindings and bindings) would incur significant maintanace overhead for the app operator and\\nDid not provide meaningful value.\\nIn addition, there was no feedback from community or prospecive users that input bindings and output bindings were confusing in any way.\\n### API structure\\nIt was decided that the API url will be kept as: `http:\/\/localhost:<port>\/v1.0\/bindings\/<name>`.\\nThe verb for the HTTP API will remain POST\/PUT, and the type of operation will be part of a versioned, structured schema for bindings.\\nThis is not a breaking change.\\n### Schema and versioning\\nIn accordance with our decision to work towards enterprise versioning, it was accepted that schemas will include a `version` field in\\nThe payload to specify which version of given component needs to be used that corresponds to the given payload.\\nIn addition, an extra field will be added to denote the type of operation that binding supports, for example: `get`, `list`, `create` etc.\\nBindings components will provide the means for the Dapr runtime to query for their supported capabilities and return a validaton error if the operation type is not supported.\\n","tokens":44,"id":75,"text":"## Context\\nAs we want to provide bi-directional capabilities for bindings to allow for cases such as getting a blob from a storage account,\\nAn API change is needed to account for the requested type of operation.\\n\n\n##Decision\n### Naming\\nIt was decided to keep the bindings name as is. Alternative proposals were included changing bindings to connectors, but a strong case couldn't be made in favor of connectors to justify the breaking change it would cause.\\n### Types\\nIt was decided to keep the same YAML format for both input bindings and bi-directional bindings as it is today.\\nAfter careful inspection, splitting to two types (for example, trigger bindings and bindings) would incur significant maintanace overhead for the app operator and\\nDid not provide meaningful value.\\nIn addition, there was no feedback from community or prospecive users that input bindings and output bindings were confusing in any way.\\n### API structure\\nIt was decided that the API url will be kept as: `http:\/\/localhost:<port>\/v1.0\/bindings\/<name>`.\\nThe verb for the HTTP API will remain POST\/PUT, and the type of operation will be part of a versioned, structured schema for bindings.\\nThis is not a breaking change.\\n### Schema and versioning\\nIn accordance with our decision to work towards enterprise versioning, it was accepted that schemas will include a `version` field in\\nThe payload to specify which version of given component needs to be used that corresponds to the given payload.\\nIn addition, an extra field will be added to denote the type of operation that binding supports, for example: `get`, `list`, `create` etc.\\nBindings components will provide the means for the Dapr runtime to query for their supported capabilities and return a validaton error if the operation type is not supported.\\n","MatchedIds":[84,2896,2145,1180,779],"Predicted":"## Decision\n### Dapr\n\n* Introduce a new **Operation** attribute in the binding API. The **Operation** attribute specifies the type of operation to be performed (e.g., `create`, `get`, `update`, `delete`).\n* For backwards compatibility, if an operation type is not specified, the default behavior will be to perform the `create` operation.\n* Update the API documentation to reflect the changes and provide examples for different operation types.\n* Ensure that the implementation handles both synchronous and asynchronous operations to support real-time and batch processing use cases.\n* Add integration tests for each type of operation to verify correct behavior and performance.\n* Consider providing additional security measures for operations that may involve sensitive data, such as `get` and `delete`.\n\nThis approach maintains flexibility and future-proofing while ensuring minimal disruption to the current system's operation.","GenTime":"2024-08-28 14:22:33"}
{"File Name":"tendermint\/adr-061-p2p-refactor-scope.md","Context":"## Context\\nThe `p2p` package responsible for peer-to-peer networking is rather old and has a number of weaknesses, including tight coupling, leaky abstractions, lack of tests, DoS vulnerabilites, poor performance, custom protocols, and incorrect behavior. A refactor has been discussed for several years ([#2067](https:\/\/github.com\/tendermint\/tendermint\/issues\/2067)).\\nInformal Systems are also building a Rust implementation of Tendermint, [Tendermint-rs](https:\/\/github.com\/informalsystems\/tendermint-rs), and plan to implement P2P networking support over the next year. As part of this work, they have requested adopting e.g. [QUIC](https:\/\/datatracker.ietf.org\/doc\/draft-ietf-quic-transport\/) as a transport protocol instead of implementing the custom application-level `MConnection` stream multiplexing protocol that Tendermint currently uses.\\nThis ADR summarizes recent discussion with stakeholders on the scope of a P2P refactor. Specific designs and implementations will be submitted as separate ADRs.\\n","Decision":"The P2P stack will be refactored and improved iteratively, in several phases:\\n* **Phase 1:** code and API refactoring, maintaining protocol compatibility as far as possible.\\n* **Phase 2:** additional transports and incremental protocol improvements.\\n* **Phase 3:** disruptive protocol changes.\\nThe scope of phases 2 and 3 is still uncertain, and will be revisited once the preceding phases have been completed as we'll have a better sense of requirements and challenges.\\n","tokens":227,"id":1979,"text":"## Context\\nThe `p2p` package responsible for peer-to-peer networking is rather old and has a number of weaknesses, including tight coupling, leaky abstractions, lack of tests, DoS vulnerabilites, poor performance, custom protocols, and incorrect behavior. A refactor has been discussed for several years ([#2067](https:\/\/github.com\/tendermint\/tendermint\/issues\/2067)).\\nInformal Systems are also building a Rust implementation of Tendermint, [Tendermint-rs](https:\/\/github.com\/informalsystems\/tendermint-rs), and plan to implement P2P networking support over the next year. As part of this work, they have requested adopting e.g. [QUIC](https:\/\/datatracker.ietf.org\/doc\/draft-ietf-quic-transport\/) as a transport protocol instead of implementing the custom application-level `MConnection` stream multiplexing protocol that Tendermint currently uses.\\nThis ADR summarizes recent discussion with stakeholders on the scope of a P2P refactor. Specific designs and implementations will be submitted as separate ADRs.\\n\n\n##Decision\nThe P2P stack will be refactored and improved iteratively, in several phases:\\n* **Phase 1:** code and API refactoring, maintaining protocol compatibility as far as possible.\\n* **Phase 2:** additional transports and incremental protocol improvements.\\n* **Phase 3:** disruptive protocol changes.\\nThe scope of phases 2 and 3 is still uncertain, and will be revisited once the preceding phases have been completed as we'll have a better sense of requirements and challenges.\\n","MatchedIds":[1982,1956,21,4650,810],"Predicted":"## Decision\nTendermint will initiate a comprehensive refactor of the `p2p` package during the upcoming development cycles. The main focus will be on addressing the current package's weaknesses, including tight coupling, leaky abstractions, lack of tests, DoS vulnerabilities, and poor performance. Key aspects of this refactor will include:\n\n1. **Adoption of QUIC as the Transport Protocol:**\n   - Replace the custom `MConnection` stream multiplexing protocol with QUIC to leverage its multiplexing capabilities, improved performance, and built-in security features.\n   - Ensure interoperability between the Go-based Tendermint and Rust-based Tendermint-rs implementations by using a standardized protocol.\n\n2. **Component Decoupling and Abstraction:**\n   - Redefine and decouple the different components within the `p2p` package to improve modularity and maintainability.\n   - Create well-defined interfaces and boundaries between components to facilitate better testing and future enhancements.\n\n3. **Enhanced Security and Stability:**\n   - Implement robust measures to counter DoS attacks and other vulnerabilities.\n   - Include thorough testing to ensure the stability and reliability of the new `p2p` package.\n\n4. **Performance Improvements:**\n   - Conduct performance benchmarking to identify bottlenecks and optimize the new implementation for high throughput and low latency.\n\n5. **Testing and Validation:**\n   - Develop an extensive suite of tests to cover various scenarios and edge cases.\n   - Perform validation against the Rust-based Tendermint-rs implementation to ensure compatibility and correctness.\n\nThis refactor aims to modernize the `p2p` package, improving its performance, security, and maintainability while ensuring compatibility with the Rust implementation. Specific designs and detailed implementation plans will be documented in separate ADRs.","GenTime":"2024-08-28 14:22:38"}
{"File Name":"cloud-on-k8s\/adr-template.md","Context":"## Context and Problem Statement\\n[Describe the context and problem statement, for example, in free form using two to three sentences. You may want to articulate the problem in form of a question.]\\n## Decision Drivers <!-- optional -->\\n* [driver 1, for example, a force, facing concern, \u2026]\\n* [driver 2, for example, a force, facing concern, \u2026]\\n* \u2026 <!-- numbers of drivers can vary -->\\n","Decision":"* [driver 1, for example, a force, facing concern, \u2026]\\n* [driver 2, for example, a force, facing concern, \u2026]\\n* \u2026 <!-- numbers of drivers can vary -->\\nChosen option: \"[option 1]\", because [justification. For example, only option, which meets k.o. criterion decision driver | which resolves force force | \u2026 | comes out best].\\n### Positive Consequences <!-- optional -->\\n* [For example, improvement of quality attribute satisfaction, follow-up decisions required, \u2026]\\n* \u2026\\n### Negative Consequences <!-- optional -->\\n* [For example, compromising quality attribute, follow-up decisions required, \u2026]\\n* \u2026\\n","tokens":96,"id":4701,"text":"## Context and Problem Statement\\n[Describe the context and problem statement, for example, in free form using two to three sentences. You may want to articulate the problem in form of a question.]\\n## Decision Drivers <!-- optional -->\\n* [driver 1, for example, a force, facing concern, \u2026]\\n* [driver 2, for example, a force, facing concern, \u2026]\\n* \u2026 <!-- numbers of drivers can vary -->\\n\n\n##Decision\n* [driver 1, for example, a force, facing concern, \u2026]\\n* [driver 2, for example, a force, facing concern, \u2026]\\n* \u2026 <!-- numbers of drivers can vary -->\\nChosen option: \"[option 1]\", because [justification. For example, only option, which meets k.o. criterion decision driver | which resolves force force | \u2026 | comes out best].\\n### Positive Consequences <!-- optional -->\\n* [For example, improvement of quality attribute satisfaction, follow-up decisions required, \u2026]\\n* \u2026\\n### Negative Consequences <!-- optional -->\\n* [For example, compromising quality attribute, follow-up decisions required, \u2026]\\n* \u2026\\n","MatchedIds":[4062,794,3820,1145,4616],"Predicted":"## Context and Problem Statement\nOur engineering team needs to decide on the database management system (DBMS) for a new web application that requires consistent data availability and strong transactional support. Given the varied levels of expertise within the team, ease of use and community support are also significant concerns. The main question is: Which DBMS should we select?\n\n## Decision Drivers\n* The need for high availability and strong transactional support (ACID compliance).\n* The requirement for ease of use to accommodate team members with different levels of expertise.\n* Robust community support and comprehensive documentation.\n* Scalability to handle future growth.\n* Cost considerations, including both upfront and operational expenses.\n\n## Decision\nChosen option: \"PostgreSQL\", because it provides strong ACID compliance, high availability features, and robust community support. Additionally, PostgreSQL is known for its ease of use and scalability, making it suitable for a team with varied expertise.\n\n### Positive Consequences\n* Ensures data consistency and integrity with ACID compliance.\n* Reduces learning curve due to extensive documentation and community support.\n* Facilitates scalability for future growth.\n* Cost-effective, as it is an open-source DBMS.\n\n### Negative Consequences\n* May require initial configuration and optimization to suit specific needs.\n* Some specialized use cases may require additional plugins or extensions, adding complexity.","GenTime":"2024-08-28 14:22:43"}
{"File Name":"trade-access-program\/0005-viewflow-for-grant-management-portal.md","Context":"## Context\\nWe need a system to manage the lifecycle of a grant application.\\n","Decision":"Viewflow was chosen as a framework to help us achieve this goal while making the development process efficient.\\n### Build vs Buy Paper\\nVarious alternatives to Viewflow were considered when making this decision. Those alternatives are details in the\\nalpha stage of the project here: https:\/\/uktrade.atlassian.net\/l\/c\/zEAEM37j\\n### Viewflow analysis\\nSome initial analysis on how viewflow could be used was also done in alpha: https:\/\/uktrade.atlassian.net\/l\/c\/Lc77C4mq\\n","tokens":17,"id":5009,"text":"## Context\\nWe need a system to manage the lifecycle of a grant application.\\n\n\n##Decision\nViewflow was chosen as a framework to help us achieve this goal while making the development process efficient.\\n### Build vs Buy Paper\\nVarious alternatives to Viewflow were considered when making this decision. Those alternatives are details in the\\nalpha stage of the project here: https:\/\/uktrade.atlassian.net\/l\/c\/zEAEM37j\\n### Viewflow analysis\\nSome initial analysis on how viewflow could be used was also done in alpha: https:\/\/uktrade.atlassian.net\/l\/c\/Lc77C4mq\\n","MatchedIds":[3722,4021,1545,2488,2502],"Predicted":"## Decision\nWe will implement a modular, event-driven architecture to manage the lifecycle of a grant application. Each phase of the grant application (submission, review, approval, and disbursement) will be represented by a distinct module to promote separation of concerns and ease of maintenance.\n\n- **Submission Module**: This module will handle the initial grant proposals and ensure all required information is provided.\n- **Review Module**: This will support the review process, allowing multiple reviewers to score and comment on the applications.\n- **Approval Module**: This module will manage the selection criteria and final approval process.\n- **Disbursement Module**: This handles the financial aspects, ensuring funds are disbursed correctly.\n\nCommunication between these modules will be managed using an event bus to ensure scalability and loose coupling. Additionally, we will include audit logging at each phase to ensure accountability and traceability. \n\nFor data storage, a relational database will be used to maintain the integrity of the application data with clear relationships between the different entities. The system will also include role-based access control to ensure that sensitive operations are restricted to authorized personnel.","GenTime":"2024-08-28 14:22:47"}
{"File Name":"Nosedive\/0003-evolutionary-architecture.md","Context":"## Context\\nThere is a lot of things to learn, not only the ecosystem of clojure, we what create a high scalable application deploy in cloud, with autoscale, resilience, geographic distribution, multyples clouds providers, etc.\\n","Decision":"We start with the most simple solution, a console application, after that we try to evolve to an API, Web Page, Microservices, etc.\\n","tokens":49,"id":108,"text":"## Context\\nThere is a lot of things to learn, not only the ecosystem of clojure, we what create a high scalable application deploy in cloud, with autoscale, resilience, geographic distribution, multyples clouds providers, etc.\\n\n\n##Decision\nWe start with the most simple solution, a console application, after that we try to evolve to an API, Web Page, Microservices, etc.\\n","MatchedIds":[2752,3488,2471,302,311],"Predicted":"## Decision\nWe will adopt a cloud-agnostic architecture leveraging Kubernetes for container orchestration. This decision enables us to deploy across multiple cloud providers while ensuring autoscaling, resilience, and geographic distribution. Additionally, we will use Terraform for infrastructure as code to manage resources in a consistent and repeatable manner across different cloud environments.","GenTime":"2024-08-28 14:22:49"}
{"File Name":"BMMRO\/2019-11-20_firestore.md","Context":"## Context\\nWe need a database in order to store the data provided by the user in the different forms.\\n","Decision":"To reduce the amount of integrations and complexity, we've decided to use one of the databases that Firebase offers.\\nUnder the Firebase umbrella are Real-time database and Firestore which are similar in many features.\\nGoogle recommends Firestore for new developers, as it is built upon \u201cthe successes of Real-time database\u201d. Also, for our purposes, Firestore is a better choice because:\\n- It offers offline support for iOS, Android and Web clients whereas real-time DB supports only iOS and Android clients.\\n- The one-big-JSON-tree structure of real-time can become difficult to maintain as the DB grows. Document-based Firestore offers better organisation provisions using collections.\\n- Scaling on firestore is automatic whereas scaling on real-time database requires sharding.\\n","tokens":23,"id":2488,"text":"## Context\\nWe need a database in order to store the data provided by the user in the different forms.\\n\n\n##Decision\nTo reduce the amount of integrations and complexity, we've decided to use one of the databases that Firebase offers.\\nUnder the Firebase umbrella are Real-time database and Firestore which are similar in many features.\\nGoogle recommends Firestore for new developers, as it is built upon \u201cthe successes of Real-time database\u201d. Also, for our purposes, Firestore is a better choice because:\\n- It offers offline support for iOS, Android and Web clients whereas real-time DB supports only iOS and Android clients.\\n- The one-big-JSON-tree structure of real-time can become difficult to maintain as the DB grows. Document-based Firestore offers better organisation provisions using collections.\\n- Scaling on firestore is automatic whereas scaling on real-time database requires sharding.\\n","MatchedIds":[1758,3657,4021,1167,5257],"Predicted":"## Decision\nGiven that the data provided by users through forms is likely structured and will frequently require querying and reporting, a relational database management system (RDBMS) is a fitting choice. Among popular options like MySQL and PostgreSQL, we have decided to use PostgreSQL. PostgreSQL provides robust ACID compliance, effective concurrency through MVCC (Multiversion Concurrency Control), and extensive indexing options like GIN\/GIST indexes, which are advantageous for full-text searches. Additionally, PostgreSQL supports the storage of JSON data and other complex data types, which may be beneficial if the form data has varied formats. This choice will ensure our data is both reliably stored and efficiently accessible for the application's needs.","GenTime":"2024-08-28 14:22:52"}
{"File Name":"adr\/ADR-10-profile-deployment-debouncing.md","Context":"## Context and Problem Statement\\nHow do we debouncing profile\/scene deployments to avoid DDoS and wasted bandwidth?\\n- As of today we only deploy profiles when changing the avatar\\n- Soon, a new deployment will happen after mute\/unmute\\n","Decision":"We decided to do Client side debouncing (both options 2 and 3). We may measure deployments in the future to revisit the server conversation.\\n","tokens":51,"id":4626,"text":"## Context and Problem Statement\\nHow do we debouncing profile\/scene deployments to avoid DDoS and wasted bandwidth?\\n- As of today we only deploy profiles when changing the avatar\\n- Soon, a new deployment will happen after mute\/unmute\\n\n\n##Decision\nWe decided to do Client side debouncing (both options 2 and 3). We may measure deployments in the future to revisit the server conversation.\\n","MatchedIds":[4601,187,351,1739,4609],"Predicted":"## Decision\nIntroduce a debouncing mechanism to batch multiple profile\/scene changes before triggering a deployment. This mechanism will incorporate the following strategies:\n* **Minimum Interval Deployment**: Ensure there's a minimum time interval (e.g., 5 minutes) between consecutive deployments of the same type (profile or scene). If multiple changes occur within this interval, only the latest change will be deployed.\n* **Change Aggregation**: For actions like mute\/unmute and avatar changes, aggregate the changes and deploy them in a single deployment to minimize redundant operations.\n* **Rate Limiting**: Impose rate limits to control the volume of deployment requests and mitigate potential DDoS attacks.\n* **Deployment Queue System**: Implement a queue system to manage deployment requests and process them sequentially, ensuring controlled bandwidth usage.\n\nThis approach will reduce unnecessary deployments, conserve bandwidth, and mitigate the risk of DDoS attacks.","GenTime":"2024-08-28 14:22:55"}
{"File Name":"publicservice-registry\/0009-use-more-value-objects.md","Context":"## Context\\nA value object is a small object that represents a simple entity whose equality is not based on identity: i.e. two value objects are equal when they have the same value, not necessarily being the same object. Examples of value objects are objects representing an amount of money or a date range.\\nIn `Wegwijs`, we experienced great value towards type safety from using VOs. We want to bank in even more on the use of VOs.\\n","Decision":"Use a Value Object wherever possible.\\n","tokens":96,"id":3124,"text":"## Context\\nA value object is a small object that represents a simple entity whose equality is not based on identity: i.e. two value objects are equal when they have the same value, not necessarily being the same object. Examples of value objects are objects representing an amount of money or a date range.\\nIn `Wegwijs`, we experienced great value towards type safety from using VOs. We want to bank in even more on the use of VOs.\\n\n\n##Decision\nUse a Value Object wherever possible.\\n","MatchedIds":[2183,1031,2502,2498,2270],"Predicted":"## Decision\nWe will enforce the use of value objects (VOs) for all simple entities in the `Wegwijs` project where equality should be based on value rather than identity. This will be done by:\n\n1. **Defining Guidelines and Patterns:** Establish a comprehensive set of guidelines for creating and using value objects. This includes specific examples and patterns for common use cases such as monetary amounts, date ranges, and other domain-specific values.\n\n2. **Code Review Checks:** Incorporate checks in code reviews to ensure that new simple entities are being implemented as value objects wherever applicable.\n\n3. **Refactoring Existing Code:** Gradually refactor existing code to replace simple entities that should be value objects, prioritizing areas where type safety can have the most significant impact.\n\n4. **Training and Documentation:** Provide training sessions and documentation for developers around the principles and benefits of using value objects, ensuring the team is well-versed in recognizing and applying this pattern effectively.\n\nBy adhering to these steps, we anticipate enhancements in type safety, code readability, and maintainability across the `Wegwijs` project.","GenTime":"2024-08-28 14:22:59"}
{"File Name":"architecture\/0002-apply-clean-code-guidelines.md","Context":"## Context\\nAs it's hard to understand code, it is crucial that anybody can easily\\nunderstand the code you're working on. This applies to all levels of code, not\\nonly the code itself, but groups of code, complete applications and groups of\\napplications.\\n","Decision":"I will always focus on simplicity, readaility and the ease of understanding\\nany code or structure.\\n","tokens":57,"id":1416,"text":"## Context\\nAs it's hard to understand code, it is crucial that anybody can easily\\nunderstand the code you're working on. This applies to all levels of code, not\\nonly the code itself, but groups of code, complete applications and groups of\\napplications.\\n\n\n##Decision\nI will always focus on simplicity, readaility and the ease of understanding\\nany code or structure.\\n","MatchedIds":[5070,3853,2270,1792,400],"Predicted":"## Decision\nWe will adopt and enforce a comprehensive coding standards guide (e.g., PEP 8 for Python, Google's Java Style Guide for Java). All code must be thoroughly documented using inline comments, function\/method docstrings, and high-level module documentation.\n\nAdditionally, we will implement a robust code review process where all changes must be peer-reviewed to ensure readability and adherence to standards. Code reviewers will specifically look for clarity in code at all levels from individual lines to entire modules and applications. \n\nTo further enhance understanding, we will create and maintain high-level architectural diagrams and documentation that explain the relationships and responsibilities of major code components and systems.","GenTime":"2024-08-28 14:23:02"}
{"File Name":"molgenis-service-armadillo\/0006-use-rdata-format-as-data-input.md","Context":"## Context\\nWe want to make the MOLGENIS \"Armadillo\" service data provider agnostic. There are a couple of reasons why we are doing this\\n- the service is usable for other parties as well\\n- the service can still integrate with MOLGENIS.\\n- the release cycle of the service is data provider independent\\n- the service can be developed by other parties as well\\n","Decision":"We implement an endpoint to upload and load RData files in the MOLGENIS \"Armadillo\" service to manage data for the use in DataSHIELD.\\n","tokens":83,"id":1326,"text":"## Context\\nWe want to make the MOLGENIS \"Armadillo\" service data provider agnostic. There are a couple of reasons why we are doing this\\n- the service is usable for other parties as well\\n- the service can still integrate with MOLGENIS.\\n- the release cycle of the service is data provider independent\\n- the service can be developed by other parties as well\\n\n\n##Decision\nWe implement an endpoint to upload and load RData files in the MOLGENIS \"Armadillo\" service to manage data for the use in DataSHIELD.\\n","MatchedIds":[2183,545,1180,1758,1545],"Predicted":"## Decision\nWe will decouple the MOLGENIS \"Armadillo\" service from specific data providers by implementing a plugin-based architecture. This allows different data providers to be integrated as interchangeable plugins. The core service will interact with these plugins through well-defined interfaces.\n\n### Implementation\n1. **Define Interfaces:** Establish common interfaces for data operations (CRUD operations, querying, etc.) that any data provider plugin must implement.\n2. **Plugin Development Guide:** Create guidelines and templates to help third parties develop their own plugins for different data providers.\n3. **Core Service Refactoring:** Refactor the core Armadillo service to use these interfaces, ensuring that it remains neutral to the underlying data provider.\n4. **Compatibility Layer for MOLGENIS:** Develop a compatibility layer to ensure seamless integration with existing MOLGENIS services without affecting core functionality.\n\n### Examples\n- **Core Interface Example:** \n  ```java\n  public interface DataProviderPlugin {\n      void create(DataObject data);\n      DataObject read(String id);\n      void update(DataObject data);\n      void delete(String id);\n      List<DataObject> query(QueryParameters params);\n  }\n  ```\n- **MOLGENIS Plugin Example:** \n  ```java\n  public class MolgenisDataProviderPlugin implements DataProviderPlugin {\n      \/\/ Implement all interface methods to interact with MOLGENIS backend\n  }\n  ```\n  \nThis approach ensures that the Armadillo service remains flexible, extensible, and easier to maintain, while also enabling better collaboration with other parties.","GenTime":"2024-08-28 14:23:08"}
{"File Name":"dapr\/API-004-binding-manifests.md","Context":"## Context\\nAs we rename Event Sources to Bindings, and formally separate State Stores, Message Buses, and Bindings, we need to decide if we need to introduce different manifest types.\\n","Decision":"### Dapr\\n* All components use the same **Component** manifests, identified by a component **type**.\\n* We'll come up with a mechanism to support pluggable secret stores. We'll support Kubernetes native secret store and Azure Key Vault in the initial release.\\n","tokens":40,"id":84,"text":"## Context\\nAs we rename Event Sources to Bindings, and formally separate State Stores, Message Buses, and Bindings, we need to decide if we need to introduce different manifest types.\\n\n\n##Decision\n### Dapr\\n* All components use the same **Component** manifests, identified by a component **type**.\\n* We'll come up with a mechanism to support pluggable secret stores. We'll support Kubernetes native secret store and Azure Key Vault in the initial release.\\n","MatchedIds":[75,885,3129,842,2342],"Predicted":"## Decision\n\n### Separation of Concerns\nIt has been decided to introduce different manifest types to formally distinguish between State Stores, Message Buses, and Bindings. The primary reason for this decision is to provide clarity and enforce a clean separation of concerns. Each type of component (State Store, Message Bus, Binding) has distinct characteristics and operational semantics, and separating them into different manifest types will help in managing and configuring these components more effectively.\n\n### Manifest Type Definitions\n1. **StateStoreManifest**:\n   - Responsible for declaring state store configurations.\n   - Contains fields such as `name`, `type`, `configuration`, and `metadata`.\n   - Example:\n     ```yaml\n     apiVersion: dapr.io\/v1alpha1\n     kind: StateStore\n     metadata:\n       name: my-statestore\n     spec:\n       type: <StateStoreType>\n       metadata:\n         - name: key\n           value: <value>\n     ```\n\n2. **MessageBusManifest**:\n   - Used to define message bus configurations.\n   - Contains fields such as `name`, `type`, `publishTopics`, `subscribeTopics`, and `metadata`.\n   - Example:\n     ```yaml\n     apiVersion: dapr.io\/v1alpha1\n     kind: MessageBus\n     metadata:\n       name: my-messagebus\n     spec:\n       type: <MessageBusType>\n       publishTopics: \n         - <topic1>\n         - <topic2>\n       subscribeTopics:\n         - <topic1>\n         - <topic2>\n       metadata:\n         - name: key\n           value: <value>\n     ```\n\n3. **BindingManifest**:\n   - Defines input and output bindings.\n   - Contains fields such as `name`, `type`, `operation`, and `metadata`.\n   - Example:\n     ```yaml\n     apiVersion: dapr.io\/v1alpha1\n     kind: Binding\n     metadata:\n       name: my-binding\n     spec:\n       type: <BindingType>\n       operation: <operationType>\n       metadata:\n         - name: key\n           value: <value>\n     ```\n\n### Benefits of Different Manifest Types\n- **Clarity and Modularity**: By having different manifest types, it becomes easier to understand and manage configurations, fostering a more modular and maintainable architecture.\n- **Scalability**: Different manifest types enable better scalability of the system since each component type can evolve independently according to its specific requirements.\n- **Validation and Error Handling**: Separate manifests allow for more precise validation logic and clearer error messages, enhancing the user experience during configuration.\n- **Security and Policies**: Policies and security rules can be tailored to each type of component, reducing the risk of misconfiguration and enhancing the overall security posture of the system.\n\n### Implementation Plan\n1. **Update Documentation**: Revise the documentation to reflect the new manifest types and provide examples for each.\n2. **Tooling Support**: Enhance existing tools or create new ones to support the creation, validation, and deployment of the new manifest types.\n3. **Migration Strategy**: Develop a clear migration strategy for transitioning existing configurations to the new manifest types. Provide scripts or tools to facilitate this process.\n4. **Backward Compatibility**: Ensure backward compatibility where possible, allowing gradual adoption of the new manifest types without disrupting current workflows.\n\nBy introducing different manifest types for State Stores, Message Buses, and Bindings, we can achieve a more organized, scalable, and manageable system architecture.","GenTime":"2024-08-28 14:23:21"}
{"File Name":"modular-monolith-with-ddd\/0015-use-in-memory-events-bus.md","Context":"## Context\\nAs we want to base inter-modular communication on asynchronous communication in the form of event-driven architecture, we need some \"events bus\" to do that.\\n","Decision":"Solution number 1 - In Memory Events Bus<\/br>\\nAt that moment we don't see more advanced integration scenarios in our system than simple publish\/subscribe scenario. We decided to follow the simplest scenario and if it will be necessary - move to more advanced.\\n","tokens":35,"id":885,"text":"## Context\\nAs we want to base inter-modular communication on asynchronous communication in the form of event-driven architecture, we need some \"events bus\" to do that.\\n\n\n##Decision\nSolution number 1 - In Memory Events Bus<\/br>\\nAt that moment we don't see more advanced integration scenarios in our system than simple publish\/subscribe scenario. We decided to follow the simplest scenario and if it will be necessary - move to more advanced.\\n","MatchedIds":[2499,1575,2309,786,2500],"Predicted":"## Decision\nWe have decided to use Apache Kafka as the event bus for inter-modular communication in our event-driven architecture.\n\n### Decision Drivers\n* **Scalability**: Apache Kafka is known for its ability to handle a large volume of events and scale horizontally.\n* **Reliability**: Kafka has robust message delivery semantics (e.g., at-least-once, at-most-once, exactly-once).\n* **Ecosystem**: Kafka has a rich ecosystem of tools and integrations, making it easier to extend and customize.\n* **Community and Support**: Kafka is widely used and has a large community, which helps in getting support and finding resources.\n* **Performance**: Kafka offers high throughput and low latency, which are critical for our use case.\n\n### Positive Consequences\n* **Decoupling of Services**: Services can evolve independently as they communicate via Kafka without direct dependencies on each other.\n* **Fault Tolerance**: Kafka\u2019s built-in replication and durability features will increase the resilience of our system.\n* **Improved Scalability**: The system can handle an increasing number of events and services as it scales.\n* **Event Persistence**: Kafka stores a history of events, which can be replayed for new consumers or for debugging purposes.\n\n### Negative Consequences\n* **Operational Complexity**: Managing and maintaining a Kafka cluster requires significant operational expertise.\n* **Initial Learning Curve**: The team needs to spend time learning Kafka and setting up the initial infrastructure.\n* **Dependency on ZooKeeper**: Kafka requires ZooKeeper for cluster management, adding another component to maintain.\n\nWith these points in consideration, Apache Kafka will serve as our \"events bus\" to facilitate asynchronous inter-modular communication in our event-driven architecture.","GenTime":"2024-08-28 14:23:26"}
{"File Name":"cape-cod-coastal-planner\/0004-use-postgresql-as-db.md","Context":"## Context\\nThe API for CHIP needs some sort of persistence for storing its data. Although the data needs for the app are pretty lightweight, it's likely that they will grow in the future as new features are desired or more data is acquired. To this end, sticking with a simple, powerful, reliable, and flexible technology that is widely understood makes sense.\\n","Decision":"A RDBMS fits this bill and PostgreSQL does so even more snugly. It's a top contender, it's free, it's currently the best supported traditional database for the Elixir\/Erlang ecosystem and it's unparalleled at its geospatial capabilities.\\n","tokens":73,"id":4859,"text":"## Context\\nThe API for CHIP needs some sort of persistence for storing its data. Although the data needs for the app are pretty lightweight, it's likely that they will grow in the future as new features are desired or more data is acquired. To this end, sticking with a simple, powerful, reliable, and flexible technology that is widely understood makes sense.\\n\n\n##Decision\nA RDBMS fits this bill and PostgreSQL does so even more snugly. It's a top contender, it's free, it's currently the best supported traditional database for the Elixir\/Erlang ecosystem and it's unparalleled at its geospatial capabilities.\\n","MatchedIds":[4747,5257,1167,2221,1574],"Predicted":"## Decision\nWe will use PostgreSQL as the persistence layer for the CHIP API.\n\n### Rationale\n* **Reliability and Performance**: PostgreSQL is well-known for its reliability, data integrity, and robust performance, which will serve well as the data need increases.\n* **Flexibility**: PostgreSQL offers extensive support for JSON and unstructured data, enabling it to accommodate lightweight data needs currently and grow with feature requirements.\n* **Community and Support**: It is widely understood, with a large community providing ample resources and support.\n* **ACID Compliance**: Ensures data integrity and consistency, which is critical as the application grows.\n\n### Positive Consequences\n* Scalable and maintainable as data demands increase.\n* Strong community and extensive documentation for troubleshooting and extending functionalities.\n* Advanced indexing and querying capabilities to optimize performance.\n\n### Negative Consequences\n* Initial setup and configuration may require some effort, especially if team familiarity with PostgreSQL is limited.","GenTime":"2024-08-28 14:23:30"}
{"File Name":"viplab-websocket-api\/0004-use-asymmetric-jwt-signing.md","Context":"## Context and Problem Statement\\nWhen using JSON Web Tokens generating signatures and verifying them is an important task.\\nJWTs can be signed using a secret (with the HMAC algorithm) or a public\/private key pair using RSA or ECDSA.\\nWhen implementing JWTs one must decide which method to use.\\n## Decision Drivers\\n* Multi tenant support with own keys for each tenant\\n","Decision":"* Multi tenant support with own keys for each tenant\\nChosen option: \"Asymmetric JWT signing\", because it the only option which allow to use different keys for different tenants.\\n### Positive Consequences\\n* multiple keys are supported\\n### Negative Consequences\\n* complex management of keys\\n","tokens":77,"id":1721,"text":"## Context and Problem Statement\\nWhen using JSON Web Tokens generating signatures and verifying them is an important task.\\nJWTs can be signed using a secret (with the HMAC algorithm) or a public\/private key pair using RSA or ECDSA.\\nWhen implementing JWTs one must decide which method to use.\\n## Decision Drivers\\n* Multi tenant support with own keys for each tenant\\n\n\n##Decision\n* Multi tenant support with own keys for each tenant\\nChosen option: \"Asymmetric JWT signing\", because it the only option which allow to use different keys for different tenants.\\n### Positive Consequences\\n* multiple keys are supported\\n### Negative Consequences\\n* complex management of keys\\n","MatchedIds":[3872,2256,578,5057,5211],"Predicted":"## Decision\nUse RSA public\/private key pair for signing JWTs.\n\n## Status\nAccepted.\n\n## Consequences\n1. **Enhanced Security**: By using RSA, each tenant can have its own private key, which can be securely managed and rotated independently.\n2. **Scalability**: Public keys can be distributed easily and cached on client-side, reducing the load on the authentication server.\n3. **Isolation**: Compromise of one tenant's private key does not affect others, reinforcing tenant isolation.\n4. **Key Management Overhead**: Introduces complexity in managing multiple keys and ensuring their secure storage and rotation. This can be mitigated with robust key management practices.\n5. **Performance**: RSA operations are generally more computationally expensive than HMAC operations, but this is justified by the enhanced security and isolation benefits.","GenTime":"2024-08-28 14:23:33"}
{"File Name":"front-end-monorepo\/adr-10.md","Context":"## Context\\nIn [ADR-5](adr-5.md), we decided on a format of a new workflow resource attribute, steps. We decided that the attribute would use ES6 Maps. When stored in the Mobx-State-Tree store, the map is wrapped by a MobX Observable. In order to work with Panoptes, these data structures has to be converted into a format that is supported by JSON API.\\nIn the consequences section of ADR-5, we described two possible solutions for the type conversion: using an existing library or doing it ourselves. We initially attempted to use an existing library, but several concerns were raised by fellow team members about this ([zooniverse\/Panoptes-Front-End#4992] (https:\/\/github.com\/zooniverse\/Panoptes-Front-End\/issues\/4992)).\\nThe default behavior in existing libraries like [JSON8](https:\/\/github.com\/sonnyp\/JSON8\/tree\/master\/packages\/json8#ooserialize) or MobX's [`toJS`](https:\/\/mobx.js.org\/refguide\/tojson.html) method is to convert maps into objects. In javascript, maps are a kind of object. However, for us, we are using maps for workflow steps because a key requirement is the ordering of key-value pairs, so converting to an object would lose the guaranteed ordering of steps. Using a library also obscures the method of map type conversion, so it will not be clear to other Zooniverse devs for other client libraries in ruby or python how to handle this case.\\n","Decision":"We decided to instead implement our own [type conversion utility function](https:\/\/github.com\/zooniverse\/front-end-monorepo\/blob\/master\/packages\/lib-classifier\/src\/store\/utils\/convertMapToArray.js) for the workflow steps map. The steps map will be converted into an array of pairs:\\n``` js\\nconst workflow = {\\nid: '1',\\nsteps: [['S1', { taskKeys: ['T1', 'T2'] }], ['S2', { taskKeys: ['T3'] }]] \/\/ How they will be stored on Panoptes\\n}\\n```\\nAnd when a workflow request is received by the classifier store, it is converted by Mobx-State-Tree into an observable map when added to the store.\\n**A note about the use of arrays for the key-value pairs**\\nSubject locations are an array of objects. It would make sense to do an array of objects here too, however the array of two values is closest to the format expected by maps when you instantiate them: https:\/\/developer.mozilla.org\/en-US\/docs\/Web\/JavaScript\/Reference\/Global_Objects\/Map\\nSo there's less type conversion happening if we store it this way. MobX will take objects too when setting an observable map, but if we ever want to instantiate an ES6 map independent of the store we would have to do another conversion from object to array of the key-value pair.\\n","tokens":320,"id":523,"text":"## Context\\nIn [ADR-5](adr-5.md), we decided on a format of a new workflow resource attribute, steps. We decided that the attribute would use ES6 Maps. When stored in the Mobx-State-Tree store, the map is wrapped by a MobX Observable. In order to work with Panoptes, these data structures has to be converted into a format that is supported by JSON API.\\nIn the consequences section of ADR-5, we described two possible solutions for the type conversion: using an existing library or doing it ourselves. We initially attempted to use an existing library, but several concerns were raised by fellow team members about this ([zooniverse\/Panoptes-Front-End#4992] (https:\/\/github.com\/zooniverse\/Panoptes-Front-End\/issues\/4992)).\\nThe default behavior in existing libraries like [JSON8](https:\/\/github.com\/sonnyp\/JSON8\/tree\/master\/packages\/json8#ooserialize) or MobX's [`toJS`](https:\/\/mobx.js.org\/refguide\/tojson.html) method is to convert maps into objects. In javascript, maps are a kind of object. However, for us, we are using maps for workflow steps because a key requirement is the ordering of key-value pairs, so converting to an object would lose the guaranteed ordering of steps. Using a library also obscures the method of map type conversion, so it will not be clear to other Zooniverse devs for other client libraries in ruby or python how to handle this case.\\n\n\n##Decision\nWe decided to instead implement our own [type conversion utility function](https:\/\/github.com\/zooniverse\/front-end-monorepo\/blob\/master\/packages\/lib-classifier\/src\/store\/utils\/convertMapToArray.js) for the workflow steps map. The steps map will be converted into an array of pairs:\\n``` js\\nconst workflow = {\\nid: '1',\\nsteps: [['S1', { taskKeys: ['T1', 'T2'] }], ['S2', { taskKeys: ['T3'] }]] \/\/ How they will be stored on Panoptes\\n}\\n```\\nAnd when a workflow request is received by the classifier store, it is converted by Mobx-State-Tree into an observable map when added to the store.\\n**A note about the use of arrays for the key-value pairs**\\nSubject locations are an array of objects. It would make sense to do an array of objects here too, however the array of two values is closest to the format expected by maps when you instantiate them: https:\/\/developer.mozilla.org\/en-US\/docs\/Web\/JavaScript\/Reference\/Global_Objects\/Map\\nSo there's less type conversion happening if we store it this way. MobX will take objects too when setting an observable map, but if we ever want to instantiate an ES6 map independent of the store we would have to do another conversion from object to array of the key-value pair.\\n","MatchedIds":[1225,674,3776,3521,1680],"Predicted":"## Decision\n\nConsidering the key requirement of maintaining the guaranteed ordering of the workflow steps and the need for clarity in type conversion methods for consistency across different client libraries, we have decided to implement our own type conversion solution for the ES6 Maps stored in the Mobx-State-Tree store. This decision is guided by the following reasons:\n\n1. **Preservation of Order**: Using the default behavior of existing libraries like JSON8 or MobX's `toJS` would convert maps into objects, thereby losing the guaranteed ordering of key-value pairs essential for workflow steps.\n   \n2. **Clarity and Consistency**: Implementing our own conversion method will provide a clear and explicit process for type conversion, making it easier for other Zooniverse developers working with different client libraries (ruby, python, etc.) to understand and handle the data structures uniformly.\n\n### Approach\n\nWe will create a custom serialization and deserialization mechanism to handle the conversion of ES6 Maps to a format compatible with JSON API, while ensuring the order of the key-value pairs is preserved. Specifically:\n\n1. **Serialization**: During serialization, we will convert the ES6 Map into an array of key-value pairs. This format is both order-preserving and JSON-compatible.\n   \n2. **Deserialization**: During deserialization, we will convert the array of key-value pairs back into an ES6 Map, ensuring the original order is retained.\n\n### Implementation Example\n\n**Serialization Function:**\n```javascript\nfunction serializeMapToOrderedArray(map) {\n    if (!(map instanceof Map)) {\n        throw new TypeError('Expected input to be a Map');\n    }\n    return Array.from(map.entries());\n}\n```\n\n**Deserialization Function:**\n```javascript\nfunction deserializeOrderedArrayToMap(array) {\n    if (!Array.isArray(array)) {\n        throw new TypeError('Expected input to be an array of key-value pairs');\n    }\n    return new Map(array);\n}\n```\n\n### Positive Consequences\n- **Order Preservation**: The workflow steps will maintain their order during the conversion process, meeting the key requirement.\n- **Clarity in Implementation**: Other developers will have a clear understanding of how maps are converted, enabling them to implement similar logic in other languages or contexts.\n- **Flexibility**: The custom solution can be easily modified or extended to handle additional requirements specific to our use case.\n\n### Negative Consequences\n- **Development Overhead**: The initial effort to implement and test the custom serialization and deserialization functions will require additional development time.\n- **Maintenance**: The custom solution will require ongoing maintenance to ensure compatibility with any changes in the data model or storage mechanisms.\n\nBy implementing a custom conversion method, we aim to provide a robust, clear, and consistent approach to handling ES6 Maps within the Mobx-State-Tree store that aligns with our specific use case and requirements.","GenTime":"2024-08-28 14:23:51"}
{"File Name":"tamr-client\/0007-tamr-client-package.md","Context":"## Context\\nWe have an existing userbase that relies on `tamr_unify_client` and cannot painlessly make backwards-incompatible changes.\\nBut, we want to rearchitect this codebase as a [library of composable functions](\/contributor-guide\/adr\/0005-composable-functions).\\n","Decision":"Implement rearchitected design as a new package named `tamr_client`.\\nRequire the `TAMR_CLIENT_BETA=1` feature flag for `tamr_client` package usage.\\nWarn users who attempt to use `tamr_client` package to opt-in if they want to beta test the new design.\\n","tokens":64,"id":539,"text":"## Context\\nWe have an existing userbase that relies on `tamr_unify_client` and cannot painlessly make backwards-incompatible changes.\\nBut, we want to rearchitect this codebase as a [library of composable functions](\/contributor-guide\/adr\/0005-composable-functions).\\n\n\n##Decision\nImplement rearchitected design as a new package named `tamr_client`.\\nRequire the `TAMR_CLIENT_BETA=1` feature flag for `tamr_client` package usage.\\nWarn users who attempt to use `tamr_client` package to opt-in if they want to beta test the new design.\\n","MatchedIds":[621,577,4443,1193,1819],"Predicted":"## Decision\nWe decided to incrementally rearchitect `tamr_unify_client` into a library of composable functions while maintaining backward compatibility.\n\n### Approach:\n1. **Backward Compatibility Layer**: Introduce a layer that maps the old APIs to the new composable functions. This will allow existing users to continue using the library without any interruption.\n   - :white_check_mark: Valid\n   ```python\n   def old_functionality(...):\n       # uses new composable functions internally\n       return new_functionality(...)\n   ```\n   - :x: Invalid\n   ```python\n   def old_functionality(...):\n       # completely new implementation without backward compatibility\n       return entirely_new_approach(...)\n   ```\n\n2. **Deprecation Notices**: Annotate legacy functions with deprecation notices and provide documentation on migrating to the new composable functions.\n   - :white_check_mark: Valid\n   ```python\n   import warnings\n\n   def old_functionality(...):\n       warnings.warn(\"This method is deprecated and will be removed in future versions. Use new_functionality(...) instead.\", DeprecationWarning)\n       return new_functionality(...)\n   ```\n   - :x: Invalid\n   ```python\n   def old_functionality(...):\n       # No mention of deprecation\n       return new_functionality(...)\n   ```\n\n3. **Phased Rollout**: Gradually roll out the new composable functions while continuously gathering feedback from the userbase to ensure a smooth transition.\n\n4. **Documentation and Communication**: Ensure comprehensive documentation is available for the new composable functions and actively communicate with the userbase about upcoming changes and migration paths.\n\n### Positive Consequences\n- Users can continue using the library without immediate disruptions.\n- Clear migration path helps users transition smoothly to the new architecture.\n- Rearchitecting incrementally reduces risk and allows for continuous improvement based on feedback.\n\n### Negative Consequences\n- Maintaining backward compatibility might introduce some additional complexity in the codebase.\n- Users might be hesitant to transition to the new composable functions despite deprecation notices.\n\nBy taking this approach, we aim to modernize the `tamr_unify_client` while respecting our existing userbase and ensuring a smooth transition to the new architecture.","GenTime":"2024-08-28 14:24:03"}
{"File Name":"up-fiscal-data\/008-grant-wise.md","Context":"## Context and Problem Statement\\nWhile extracting the `Grant-wise expenditure` a lot of site crashes were encountered. A decision was taken earlier to extract the `DDO-wise expenditure` section from Koshvani as both section contain the same information in different heirarchies.\\n## Decision Drivers\\nThe challenges around site crashes have been addressed and the `Grant-wise expenditure` section can now be extracted.\\n","Decision":"The challenges around site crashes have been addressed and the `Grant-wise expenditure` section can now be extracted.\\nThe `Grant-wise expenditure` section is being extracted over the `DDO-wise expenditure` section.\\n","tokens":82,"id":447,"text":"## Context and Problem Statement\\nWhile extracting the `Grant-wise expenditure` a lot of site crashes were encountered. A decision was taken earlier to extract the `DDO-wise expenditure` section from Koshvani as both section contain the same information in different heirarchies.\\n## Decision Drivers\\nThe challenges around site crashes have been addressed and the `Grant-wise expenditure` section can now be extracted.\\n\n\n##Decision\nThe challenges around site crashes have been addressed and the `Grant-wise expenditure` section can now be extracted.\\nThe `Grant-wise expenditure` section is being extracted over the `DDO-wise expenditure` section.\\n","MatchedIds":[443,442,446,445,1708],"Predicted":"## Decision\nGiven that the challenges around site crashes have been addressed, we will switch our extraction focus back to the `Grant-wise expenditure` section from the Koshvani platform. \n\nKey Actions:\n- Resume data extraction from the `Grant-wise expenditure` section to leverage the hierarchical structure it offers.\n- Continue to monitor for any residual issues related to site stability during extraction.\n- Maintain the `DDO-wise expenditure` extraction scripts as a fallback option in case any new issues arise with the `Grant-wise expenditure` section.\n\nThis approach ensures that we utilize the most appropriate data hierarchy while maintaining a backup plan to mitigate future risks.","GenTime":"2024-08-28 14:24:06"}
{"File Name":"osticket-ticket-options-plugin\/001-required-properties.md","Context":"# Context\\nScott recently added a `path_url` prompt to get the eventual url a web1 app would end up. The purpose of this property was to update the css and javascript url paths in a web app html file. The problem is that typical useage of the lifecorp-init app is to press enter through each prompt and accept the defaults. For a web1 app, there is no default for the url path because there is no standard for where the app will be placed on the lifecorp network. If this field does NOT get filled in when the app is intiated, then the developer must manually fill in this information. The purpose of lifecorp-init is to get a project skeleton up and running to a \"Hello World\" state with little to no modifications.\\n# Decision\\nScott will add a parameter called `--require` and `-r` that will allow a string to be passed to tell lifecorp-init to require a specific prompt to not be empty for some projects, but remain empty for others.\\n# Consequences\\nThe lifcorp-init will be able to block users from skipping required parameters.\\n# Status\\nAccepted\\n","Decision":"Scott will add a parameter called `--require` and `-r` that will allow a string to be passed to tell lifecorp-init to require a specific prompt to not be empty for some projects, but remain empty for others.\\n# Consequences\\nThe lifcorp-init will be able to block users from skipping required parameters.\\n# Status\\nAccepted\\n","tokens":236,"id":139,"text":"# Context\\nScott recently added a `path_url` prompt to get the eventual url a web1 app would end up. The purpose of this property was to update the css and javascript url paths in a web app html file. The problem is that typical useage of the lifecorp-init app is to press enter through each prompt and accept the defaults. For a web1 app, there is no default for the url path because there is no standard for where the app will be placed on the lifecorp network. If this field does NOT get filled in when the app is intiated, then the developer must manually fill in this information. The purpose of lifecorp-init is to get a project skeleton up and running to a \"Hello World\" state with little to no modifications.\\n# Decision\\nScott will add a parameter called `--require` and `-r` that will allow a string to be passed to tell lifecorp-init to require a specific prompt to not be empty for some projects, but remain empty for others.\\n# Consequences\\nThe lifcorp-init will be able to block users from skipping required parameters.\\n# Status\\nAccepted\\n\n\n##Decision\nScott will add a parameter called `--require` and `-r` that will allow a string to be passed to tell lifecorp-init to require a specific prompt to not be empty for some projects, but remain empty for others.\\n# Consequences\\nThe lifcorp-init will be able to block users from skipping required parameters.\\n# Status\\nAccepted\\n","MatchedIds":[1065,3635,701,3784,377],"Predicted":"## Context and Problem Statement\nScott recently added a `path_url` prompt to get the eventual URL where a web1 app would end up. The purpose of this property is to update the CSS and JavaScript URL paths in a web app HTML file. The problem is that typical usage of the lifecorp-init app involves pressing enter through each prompt to accept the defaults. For a web1 app, there is no default for the URL path because there is no standard for where the app will be placed on the lifecorp network. If this field does not get filled in when the app is initiated, the developer must manually fill in this information. The goal of lifecorp-init is to get a project skeleton up and running to a \"Hello World\" state with little to no modifications.\n\n## Decision Drivers\n- Ensuring that essential fields are correctly filled out.\n- Maintaining the simplicity and speed of the project initiation process.\n- Ensuring minimal manual post-initiation modifications.\n\n## Decision\nScott will add a parameter called `--require` and `-r` that will allow a string to be passed to lifecorp-init, which will make specific prompts mandatory for some projects while allowing them to remain optional for others.\n\n## Consequences\n- The lifecorp-init tool will now be able to enforce mandatory input for specified parameters, preventing users from skipping required fields.\n- This change will help ensure that web1 apps have their `path_url` correctly filled out during the initial setup, reducing the need for manual post-initiation modifications.\n- The flexibility of the lifecorp-init tool will be increased, as it can now cater to the varying requirements of different types of projects.\n\n## Status\nAccepted","GenTime":"2024-08-28 14:24:12"}
{"File Name":"publicservice-registry\/0012-use-dutch-resource-names.md","Context":"## Context\\nWhen exposing API endpoints, exposing a UI or exposing resource field names, we have to decide on the language used. Do we cater to the majority of the world by using English, or do we use Dutch because we are creating resources for the Flemish Government?\\n","Decision":"We will use Dutch terms because it is an external requirement dictated by the organisation.\\n","tokens":56,"id":3126,"text":"## Context\\nWhen exposing API endpoints, exposing a UI or exposing resource field names, we have to decide on the language used. Do we cater to the majority of the world by using English, or do we use Dutch because we are creating resources for the Flemish Government?\\n\n\n##Decision\nWe will use Dutch terms because it is an external requirement dictated by the organisation.\\n","MatchedIds":[3715,598,472,847,1240],"Predicted":"## Decision\nGiven that we are creating resources specifically for the Flemish Government, we will use Dutch for exposing API endpoints, UI, and resource field names. This decision aligns with the target audience and ensures better accessibility and understanding for local users. However, we will maintain clear documentation and commenting in English to facilitate potential collaboration and understanding by international developers. Here's an example of how this decision will be implemented:\n\nAPI Endpoint:\n```\nGET \/gebruiker\/profiel\n```\n\nResource Field Names:\n```json\n{\n  \"gebruikersnaam\": \"janedoe\",\n  \"email\": \"janedoe@example.com\",\n  \"voornaam\": \"Jane\",\n  \"achternaam\": \"Doe\"\n}\n```\n\nUI Labels:\n```\n<label for=\"email\">E-mailadres<\/label>\n<input type=\"email\" id=\"email\" name=\"email\">\n```\n\nBy making this decision, we ensure that the project caters to its primary audience while maintaining a level of inclusivity for a wider developer base.","GenTime":"2024-08-28 14:24:16"}
{"File Name":"grout\/adr-2-rename-package.md","Context":"## Context\\nThe name `ashlar` [is already taken on PyPi](https:\/\/pypi.org\/project\/ashlar\/).\\nSince PyPi requires unique names for packages, this means that if we want to\\ndistribute our package on PyPi, we'll have to either:\\n1. Convince the owners of `ashlar` to give it to us\\n2. Name the PyPi package something similar to `ashlar` but slightly different,\\nlike `ashlar-core`\\n3. Come up with a new name for the project\\nOption 1 seems unlikely, given the maturity of the ashlar package on PyPi and\\nhow recent the last release was (April 2018, less than four months ago). Number\\n2 is perfectly functional but frustrating from a branding and distribution perspective,\\nsince it has the potential to introduce some confusion and\/or competition with\\nthe existing `ashlar` package.\\nInstead, I believe that the best course of action is to choose option 3 and rename the project.\\nThis will require us to come up with a new name for Ashlar, a [notoriously\\ndifficult decision](https:\/\/martinfowler.com\/bliki\/TwoHardThings.html).\\nSome options that I considered, all based on the idea of \"flexible\\nconstruction materials\":\\n- [Joist](https:\/\/en.wikipedia.org\/wiki\/Joist)\\n- [Lintel](https:\/\/en.wikipedia.org\/wiki\/Lintel)\\n- [Silicone](https:\/\/en.wikipedia.org\/wiki\/Silicone)\\n- [Grout](https:\/\/en.wikipedia.org\/wiki\/Grout)\\n","Decision":"I propose that we rename the project to **Grout**. Among the options above,\\n\"Grout\" is the name that sounds the best to me, and it's the one that I believe\\noffers the closest allegory for the project.\\nGrout is a construction material widely known for its physical flexibility and its\\npractical versatility: a fluid concrete used to create waterproof seals in\\nmasonry structures.\\nSome advantages of the name \"Grout\" include:\\n- \"Grout\" respects the origins of the project by referencing a masonry material,\\nbut unlike \"Ashlar\", the name \"Grout\" emphasizes the core features of the project\\n-- its flexibility and versatility as a base material\\nthat can scaffold and tie together much more complex projects.\\n- \"Grout\" is one syllable (one fewer than \"Ashlar\") and the easiest word to\\npronounce among the options I considered.\\n- Perhaps most importantly, `grout` is [available on\\nPyPi](https:\/\/pypi.org\/project\/grout).\\n","tokens":339,"id":2980,"text":"## Context\\nThe name `ashlar` [is already taken on PyPi](https:\/\/pypi.org\/project\/ashlar\/).\\nSince PyPi requires unique names for packages, this means that if we want to\\ndistribute our package on PyPi, we'll have to either:\\n1. Convince the owners of `ashlar` to give it to us\\n2. Name the PyPi package something similar to `ashlar` but slightly different,\\nlike `ashlar-core`\\n3. Come up with a new name for the project\\nOption 1 seems unlikely, given the maturity of the ashlar package on PyPi and\\nhow recent the last release was (April 2018, less than four months ago). Number\\n2 is perfectly functional but frustrating from a branding and distribution perspective,\\nsince it has the potential to introduce some confusion and\/or competition with\\nthe existing `ashlar` package.\\nInstead, I believe that the best course of action is to choose option 3 and rename the project.\\nThis will require us to come up with a new name for Ashlar, a [notoriously\\ndifficult decision](https:\/\/martinfowler.com\/bliki\/TwoHardThings.html).\\nSome options that I considered, all based on the idea of \"flexible\\nconstruction materials\":\\n- [Joist](https:\/\/en.wikipedia.org\/wiki\/Joist)\\n- [Lintel](https:\/\/en.wikipedia.org\/wiki\/Lintel)\\n- [Silicone](https:\/\/en.wikipedia.org\/wiki\/Silicone)\\n- [Grout](https:\/\/en.wikipedia.org\/wiki\/Grout)\\n\n\n##Decision\nI propose that we rename the project to **Grout**. Among the options above,\\n\"Grout\" is the name that sounds the best to me, and it's the one that I believe\\noffers the closest allegory for the project.\\nGrout is a construction material widely known for its physical flexibility and its\\npractical versatility: a fluid concrete used to create waterproof seals in\\nmasonry structures.\\nSome advantages of the name \"Grout\" include:\\n- \"Grout\" respects the origins of the project by referencing a masonry material,\\nbut unlike \"Ashlar\", the name \"Grout\" emphasizes the core features of the project\\n-- its flexibility and versatility as a base material\\nthat can scaffold and tie together much more complex projects.\\n- \"Grout\" is one syllable (one fewer than \"Ashlar\") and the easiest word to\\npronounce among the options I considered.\\n- Perhaps most importantly, `grout` is [available on\\nPyPi](https:\/\/pypi.org\/project\/grout).\\n","MatchedIds":[4361,1680,4428,3119,1436],"Predicted":"## Decision\nWe will rename the project to `Joist`. This name aligns with the theme of flexible construction materials and is currently available on PyPi, allowing for a unique and easily identifiable package name. This decision avoids potential confusion and competition with the existing `ashlar` package and ensures clear branding and distribution.","GenTime":"2024-08-28 14:24:18"}
{"File Name":"openjdk-api-v3\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":4699,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","MatchedIds":[4408,4722,4428,3119,1436],"Predicted":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","GenTime":"2024-08-28 22:56:39"}
{"File Name":"james-project\/0025-cassandra-blob-store-cache.md","Context":"## Context\\nJames exposes a simple BlobStore API for storing raw data. However such raw data often vary in size and access patterns.\\nAs an example:\\n- Mailbox message headers are expected to be small and frequently accessed\\n- Mailbox message body are expected to have sizes ranging from small to big but are unfrequently accessed\\n- DeletedMessageVault message headers are expected to be small and unfrequently accessed\\nThe access pattern of some of these kind of blobs does not fit Object Storage characteristics: good at storing big blobs, but\\nit induces high latencies for reading small blobs. We observe latencies of around 50-100ms while Cassandra latency is of 4ms.\\nThis gets some operations slow (for instance IMAP FETCH headers, or listing JMAP messages).\\n","Decision":"Implement a write through cache to have better read latency for smaller objects.\\nSuch a cache needs to be distributed in order to be more efficient.\\nGiven that we don't want to introduce new technologies, we will implement it using Cassandra.\\nThe cache should be implemented as a key-value table on a dedicated 'cache' keyspace, with a replication factor of 1,\\nand be queried with a consistency level of ONE.\\nWe will leverage a configurable TTL as an eviction policy. Cache will be populated upon writes and missed read, if the\\nblob size is below a configurable threashold. We will use the TimeWindow compaction strategy.\\nFailure to read the cache, or cache miss will result in a read in the object storage.\\n","tokens":160,"id":2896,"text":"## Context\\nJames exposes a simple BlobStore API for storing raw data. However such raw data often vary in size and access patterns.\\nAs an example:\\n- Mailbox message headers are expected to be small and frequently accessed\\n- Mailbox message body are expected to have sizes ranging from small to big but are unfrequently accessed\\n- DeletedMessageVault message headers are expected to be small and unfrequently accessed\\nThe access pattern of some of these kind of blobs does not fit Object Storage characteristics: good at storing big blobs, but\\nit induces high latencies for reading small blobs. We observe latencies of around 50-100ms while Cassandra latency is of 4ms.\\nThis gets some operations slow (for instance IMAP FETCH headers, or listing JMAP messages).\\n\n\n##Decision\nImplement a write through cache to have better read latency for smaller objects.\\nSuch a cache needs to be distributed in order to be more efficient.\\nGiven that we don't want to introduce new technologies, we will implement it using Cassandra.\\nThe cache should be implemented as a key-value table on a dedicated 'cache' keyspace, with a replication factor of 1,\\nand be queried with a consistency level of ONE.\\nWe will leverage a configurable TTL as an eviction policy. Cache will be populated upon writes and missed read, if the\\nblob size is below a configurable threashold. We will use the TimeWindow compaction strategy.\\nFailure to read the cache, or cache miss will result in a read in the object storage.\\n","MatchedIds":[2145,2122,2897,1574,2892],"Predicted":"## Decision\n\nImplement a tiered storage strategy to optimize the access patterns based on the blob characteristics.\n\n1. **Primary Storage: Cassandra**\n   - Store small, frequently accessed blobs (e.g., Mailbox message headers) in Cassandra to leverage its low latency (4ms).\n   - This will ensure that operations dependent on these small blobs (e.g., IMAP FETCH headers, listing JMAP messages) are fast.\n\n2. **Secondary Storage: Object Storage (S3\/Swift)**\n   - Store large and infrequently accessed blobs (e.g., Mailbox message body) in Object Storage to take advantage of its durability and cost-effectiveness.\n   - DeletedMessageVault message headers can also be stored in Object Storage since they are small and infrequently accessed.\n\n3. **Intelligent Routing Logic**\n   - Develop a routing mechanism within the BlobStore API to determine the appropriate storage backend based on the blob's characteristics (size and access frequency).\n   - Use configurable thresholds for size and access frequency to decide when to route data to Cassandra versus Object Storage.\n\n4. **Caching Layer**\n   - Implement a read-through cache for small blobs pulled from Object Storage to alleviate the latency penalty for occasional access.\n   - The cache can use in-memory storage or a dedicated key-value store with efficient eviction policies (e.g., LRU).\n\n### Implementation Details\n\n- **Configuration Parameters:**\n  - `thresholdSize`: The size limit below which blobs are considered small and stored in Cassandra. This will be configurable.\n  - `cacheTTL`: Time-to-live for cached entries to handle eviction.\n\n- **Storage Logic:**\n  - On **write**:\n    - Check the blob size. If below `thresholdSize`, write to Cassandra; otherwise, write to Object Storage.\n  - On **read**:\n    - Check if the blob is in the cache. If not, read from the appropriate storage backend based on the blob's size.\n\n- **Backup and Replication:**\n  - Ensure appropriate backup and replication strategies are in place for both Cassandra and Object Storage to maintain durability and high availability.\n\n### Positive Consequences\n- Significant reduction in latency for small, frequently accessed blobs.\n- Cost optimization by leveraging Object Storage for large, infrequently accessed blobs.\n- Increased overall system performance and user experience due to efficient access patterns.\n\n### Negative Consequences\n- Increased complexity in the BlobStore API due to the need for intelligent routing and caching mechanisms.\n- Potential need for additional monitoring and maintenance to manage the tiered storage strategy and ensure performance SLAs are met.\n\nBy adopting this tiered storage strategy, we can optimize both the performance and cost-effectiveness of our BlobStore API, tailored to the varying blob sizes and access patterns in our system.","GenTime":"2024-08-28 22:57:14"}
{"File Name":"buildit-all\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":4428,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","MatchedIds":[4699,4408,4722,3119,1436],"Predicted":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).","GenTime":"2024-08-28 22:57:16"}
{"File Name":"Conduit\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":4408,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","MatchedIds":[4428,3119,1436,294,2583],"Predicted":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\n","GenTime":"2024-08-28 22:57:17"}
{"File Name":"react-library-template\/0006-use-jest.md","Context":"## Context\\nWe want a test framework that has good support for React and TypeScript.\\n[Jest](https:\/\/jestjs.io) is the standard, recommended test framework for React\\napps.\\n","Decision":"We will use Jest as our testing framework.\\n","tokens":40,"id":4223,"text":"## Context\\nWe want a test framework that has good support for React and TypeScript.\\n[Jest](https:\/\/jestjs.io) is the standard, recommended test framework for React\\napps.\\n\n\n##Decision\nWe will use Jest as our testing framework.\\n","MatchedIds":[3323,452,2273,4785,4359],"Predicted":"## Decision\nWe will use Jest as our testing framework.","GenTime":"2024-08-28 22:57:19"}
{"File Name":"nr-arch\/2020-03-12-ARCH-record-architecture-decisions-simple-template.md","Context":"## Context\\nThe issue motivating this decision, and any context that influences or constrains the decision.\\n","Decision":"The change that we're proposing or have agreed to implement.\\n","tokens":21,"id":4804,"text":"## Context\\nThe issue motivating this decision, and any context that influences or constrains the decision.\\n\n\n##Decision\nThe change that we're proposing or have agreed to implement.\\n","MatchedIds":[3820,1145,4616,3546,4963],"Predicted":"## Decision\nWe have decided to migrate our existing monolithic database structure to a microservices-based architecture. This change allows each service to have its own dedicated database, improving scalability, fault isolation, and enabling teams to work more independently. Additionally, the transition will be done incrementally, starting with non-critical services, to mitigate risks and ensure stability throughout the migration process.","GenTime":"2024-08-28 22:57:37"}
{"File Name":"racing-tips\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":3119,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","MatchedIds":[4699,4408,4722,4428,1436],"Predicted":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).","GenTime":"2024-08-28 22:57:38"}
{"File Name":"architecture\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":1436,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","MatchedIds":[4699,4408,4722,4428,3119],"Predicted":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).","GenTime":"2024-08-28 22:57:40"}
{"File Name":"james\/0025-cassandra-blob-store-cache.md","Context":"## Context\\nJames exposes a simple BlobStore API for storing raw data. However such raw data often vary in size and access patterns.\\nAs an example:\\n- Mailbox message headers are expected to be small and frequently accessed\\n- Mailbox message body are expected to have sizes ranging from small to big but are unfrequently accessed\\n- DeletedMessageVault message headers are expected to be small and unfrequently accessed\\nThe access pattern of some of these kind of blobs does not fit Object Storage characteristics: good at storing big blobs, but\\nit induces high latencies for reading small blobs. We observe latencies of around 50-100ms while Cassandra latency is of 4ms.\\nThis gets some operations slow (for instance IMAP FETCH headers, or listing JMAP messages).\\n","Decision":"Implement a write through cache to have better read latency for smaller objects.\\nSuch a cache needs to be distributed in order to be more efficient.\\nGiven that we don't want to introduce new technologies, we will implement it using Cassandra.\\nThe cache should be implemented as a key-value table on a dedicated 'cache' keyspace, with a replication factor of 1,\\nand be queried with a consistency level of ONE.\\nWe will leverage a configurable TTL as an eviction policy. Cache will be populated upon writes and missed read, if the\\nblob size is below a configurable threashold. We will use the TimeWindow compaction strategy.\\nFailure to read the cache, or cache miss will result in a read in the object storage.\\n","tokens":160,"id":2145,"text":"## Context\\nJames exposes a simple BlobStore API for storing raw data. However such raw data often vary in size and access patterns.\\nAs an example:\\n- Mailbox message headers are expected to be small and frequently accessed\\n- Mailbox message body are expected to have sizes ranging from small to big but are unfrequently accessed\\n- DeletedMessageVault message headers are expected to be small and unfrequently accessed\\nThe access pattern of some of these kind of blobs does not fit Object Storage characteristics: good at storing big blobs, but\\nit induces high latencies for reading small blobs. We observe latencies of around 50-100ms while Cassandra latency is of 4ms.\\nThis gets some operations slow (for instance IMAP FETCH headers, or listing JMAP messages).\\n\n\n##Decision\nImplement a write through cache to have better read latency for smaller objects.\\nSuch a cache needs to be distributed in order to be more efficient.\\nGiven that we don't want to introduce new technologies, we will implement it using Cassandra.\\nThe cache should be implemented as a key-value table on a dedicated 'cache' keyspace, with a replication factor of 1,\\nand be queried with a consistency level of ONE.\\nWe will leverage a configurable TTL as an eviction policy. Cache will be populated upon writes and missed read, if the\\nblob size is below a configurable threashold. We will use the TimeWindow compaction strategy.\\nFailure to read the cache, or cache miss will result in a read in the object storage.\\n","MatchedIds":[2896,2122,2897,1574,2892],"Predicted":"## Decision\nIntroduce a tiered storage strategy to optimize blob storage and access. \n\n### Strategy\n1. **Categorize Blobs**:\n   - **Hot Storage**: For blobs that are small and frequently accessed (e.g., Mailbox message headers).\n   - **Warm Storage**: For blobs that have varying sizes and are infrequently accessed (e.g., Mailbox message body).\n   - **Cold Storage**: For blobs that are small and infrequently accessed (e.g., DeletedMessageVault message headers).\n\n2. **Storage Allocation**:\n   - **Hot Storage**: Utilize Cassandra for storing small and frequently accessed blobs. Leverage its low latency for better performance on read and write operations.\n   - **Warm Storage**: Use Object Storage (e.g., S3 or Swift) for blobs of varying sizes that are infrequently accessed, maintaining the advantage of durability and cost.\n   - **Cold Storage**: Utilize Object Storage with longer TTL for the blobs that are infrequently accessed, benefiting from the cost-effectiveness of object storage.\n\n3. **Implementation Details**:\n   - **Write Strategy**: Implement logic within the BlobStore API to analyze the size and expected access patterns of the blobs and allocate storage accordingly.\n   - **Read Strategy**: Implement a mechanism to first check the cache (hot storage) before querying object storage to reduce latency.\n   - **Eviction Policy**: Use a configurable TTL for evicting blobs from the hot storage to the warm or cold storage based on access frequency and time.\n\n4. **Challenges and Solutions**:\n   - **Consistency**: Ensure consistency between different storage tiers through a well-defined migration and synchronization process.\n   - **Migration**: Periodic background jobs can be implemented to analyze and migrate blobs between tiers based on updated access patterns.\n\nThis approach maximizes performance for small, frequently accessed blobs by leveraging Cassandra's low latency while maintaining cost-efficiency and durability for larger and infrequently accessed blobs through the use of object storage.","GenTime":"2024-08-28 22:57:48"}
{"File Name":"opg-refunds\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":294,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","MatchedIds":[4699,4408,4722,4428,3119],"Predicted":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).","GenTime":"2024-08-28 22:57:50"}
{"File Name":"thing\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":2583,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","MatchedIds":[4428,3119,1436,294,4401],"Predicted":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).","GenTime":"2024-08-28 22:58:00"}
{"File Name":"rails-template\/0006-use-simplecov-to-monitor-code-test-coverage.md","Context":"## Context\\nWe want to keep our test coverage as high as possible without having to run\\nmanual checks as these take time and are easy to forget.\\n","Decision":"Use Simplecov with RSpec to monitor coverage changes on every test run\\n","tokens":32,"id":4179,"text":"## Context\\nWe want to keep our test coverage as high as possible without having to run\\nmanual checks as these take time and are easy to forget.\\n\n\n##Decision\nUse Simplecov with RSpec to monitor coverage changes on every test run\\n","MatchedIds":[1258,4359,3853,4868,1387],"Predicted":"## Decision\nUse an automated code coverage tool like [Codecov](https:\/\/codecov.io\/) integrated with our CI\/CD pipeline to monitor test coverage on every code push. Additionally, configure the pipeline to fail builds that do not meet the required coverage thresholds.","GenTime":"2024-08-28 22:58:02"}
{"File Name":"jskatas.org\/000-use-adrs.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this\\narticle: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":4722,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this\\narticle: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","MatchedIds":[4699,4408,4428,3119,1436],"Predicted":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).","GenTime":"2024-08-28 22:58:04"}
{"File Name":"gp-redirect\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":4401,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","MatchedIds":[4428,3119,1436,294,2583],"Predicted":"## Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\n","GenTime":"2024-08-28 22:58:06"}
{"File Name":"content-publisher\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":4886,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","MatchedIds":[4699,4408,4722,4428,3119],"Predicted":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","GenTime":"2024-08-28 22:58:08"}
{"File Name":"verify-onboarding-prototypes\/0006-we-will-build-a-js-client.md","Context":"## Context\\nAt least one user is currently using node js and passport. We want to provide as\\nfrictionless as possible an integration for them.\\nOther users will be using other languages and frameworks.\\n","Decision":"We will initially build only a node \/ passport client. We will want to build\\nanother client in another language as soon as possible to make sure the API\\nis well designed.\\nUsers should also be able to interact with the API directly if we haven't built\\nan appropriate client for their use case.\\n","tokens":43,"id":577,"text":"## Context\\nAt least one user is currently using node js and passport. We want to provide as\\nfrictionless as possible an integration for them.\\nOther users will be using other languages and frameworks.\\n\n\n##Decision\nWe will initially build only a node \/ passport client. We will want to build\\nanother client in another language as soon as possible to make sure the API\\nis well designed.\\nUsers should also be able to interact with the API directly if we haven't built\\nan appropriate client for their use case.\\n","MatchedIds":[4443,3470,578,2405,527],"Predicted":"## Decision\nWe will create a dedicated client library specifically for Node.js and Passport to provide seamless integration for the current user. This client library will abstract away the complexities of authentication, making it easy to drop into existing Node.js projects.\n\nFor users utilizing other languages and frameworks, we will ensure our API is well-documented and easy to interact with directly. This ensures that even in the absence of a dedicated client library, users can still utilize our service effectively. Additionally, we will monitor usage patterns and prioritize the development of additional client libraries based on demand.\n\nWe will also gather feedback from early adopters to refine and improve the functionality and usability of both the Node.js client and the API.","GenTime":"2024-08-28 22:58:12"}
{"File Name":"katas\/000-use-adrs.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this\\narticle: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":4333,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this\\narticle: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","MatchedIds":[4428,3119,1436,294,2583],"Predicted":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).","GenTime":"2024-08-28 22:58:13"}
{"File Name":"verify-service-provider\/0008-provide-an-end-to-end-stub.md","Context":"## Context\\nWe wish to run regular user research and testing against the prototype Service Provider.\\nTo support user research we need to provide a user journey that resembles a typical Verify journey.\\nThe area we are most interested in is the interface between the Service and the Hub.\\n","Decision":"We will create a Stub Verify Hub that will allow for end-to-end testing of the prototype.\\nIt will not provide a SAML implementation.\\nIt will expect a form submission on a web resource that mimics the behaviour of receiving an AuthnRequest.\\nIf the form post is successful then a browser redirect will be issued to a page explaining where the user is in their\\njourney.  Continuing from this page will take the user to a page containing a series of possible response scenarios that\\ncan be selected.\\nAny information that needs to be stored from the original request or between pages will be stored in a session\\ncookie. The session cookie does not need to be signed or encrypted initially.\\nSome scenarios will require additional user input such as a providing a pid value or user account creation attributes.\\nThese will be provided on subsequent pages. In every case the user will need to provide the URL that the stub-hub should\\nsend its responses to.\\nEach response will mimic the shape of a Hub SAML Response form that can be submitted back to the Service and SP\\nprototype.\\nThe details of each canned response will be encoded as base64 encoded JSON in the SAMLResponse parameter of the form.\\nThe prototype service provider will be able to understand each canned response and produce a specific response to the\\nService\/client.\\nUsing these responses it will be possible able to test different scenarios in the client, Service, and browser.\\nThe Stub Verify Hub will provide the following responses:\\n| Response Scenario     | Message                                                                                                                   |\\n| --                    | --                                                                                                                         |\\n| Successful Match      | scenario: SUCCESS_MATCH, levelOfAssurance: ${specified loa}, pid: ${specified PID}                                         |\\n| Account Creation      | scenario: ACCOUNT_CREATION, levelOfAssurance: ${specified loa}, pid: ${specified PID}, attributes: [${user attributes}]  |\\n| No Match              | scenario: NO_MATCH                                                                                                         |\\n| Cancellation          | scenario: CANCELLATION                                                                                                     |\\n| Authentication failed | scenario: AUTHENTICATION_FAILED                                                                                           |\\n| Requestor Error       | scenario: REQUEST_ERROR                                                                                                 |\\n| Internal Server Error | scenario: INTERNAL_SERVER_ERROR\\nInitially, the Stub Verify Hub will be deployed to Government PaaS.\\nA diagram of a potential stub architecture is available at: `prototypes\/prototype-0\/docs\/diagrams\/stub_service_architecture.png`\\n","tokens":55,"id":4454,"text":"## Context\\nWe wish to run regular user research and testing against the prototype Service Provider.\\nTo support user research we need to provide a user journey that resembles a typical Verify journey.\\nThe area we are most interested in is the interface between the Service and the Hub.\\n\n\n##Decision\nWe will create a Stub Verify Hub that will allow for end-to-end testing of the prototype.\\nIt will not provide a SAML implementation.\\nIt will expect a form submission on a web resource that mimics the behaviour of receiving an AuthnRequest.\\nIf the form post is successful then a browser redirect will be issued to a page explaining where the user is in their\\njourney.  Continuing from this page will take the user to a page containing a series of possible response scenarios that\\ncan be selected.\\nAny information that needs to be stored from the original request or between pages will be stored in a session\\ncookie. The session cookie does not need to be signed or encrypted initially.\\nSome scenarios will require additional user input such as a providing a pid value or user account creation attributes.\\nThese will be provided on subsequent pages. In every case the user will need to provide the URL that the stub-hub should\\nsend its responses to.\\nEach response will mimic the shape of a Hub SAML Response form that can be submitted back to the Service and SP\\nprototype.\\nThe details of each canned response will be encoded as base64 encoded JSON in the SAMLResponse parameter of the form.\\nThe prototype service provider will be able to understand each canned response and produce a specific response to the\\nService\/client.\\nUsing these responses it will be possible able to test different scenarios in the client, Service, and browser.\\nThe Stub Verify Hub will provide the following responses:\\n| Response Scenario     | Message                                                                                                                   |\\n| --                    | --                                                                                                                         |\\n| Successful Match      | scenario: SUCCESS_MATCH, levelOfAssurance: ${specified loa}, pid: ${specified PID}                                         |\\n| Account Creation      | scenario: ACCOUNT_CREATION, levelOfAssurance: ${specified loa}, pid: ${specified PID}, attributes: [${user attributes}]  |\\n| No Match              | scenario: NO_MATCH                                                                                                         |\\n| Cancellation          | scenario: CANCELLATION                                                                                                     |\\n| Authentication failed | scenario: AUTHENTICATION_FAILED                                                                                           |\\n| Requestor Error       | scenario: REQUEST_ERROR                                                                                                 |\\n| Internal Server Error | scenario: INTERNAL_SERVER_ERROR\\nInitially, the Stub Verify Hub will be deployed to Government PaaS.\\nA diagram of a potential stub architecture is available at: `prototypes\/prototype-0\/docs\/diagrams\/stub_service_architecture.png`\\n","MatchedIds":[581,2405,4442,2041,915],"Predicted":"## Decision\nWe will develop a Mock Verify Hub to facilitate end-to-end testing of the prototype Service Provider. This Mock Hub will simulate the interactions between the Service and the Hub without implementing full SAML processing.\n\nHere are the key features of the Mock Verify Hub:\n\n1. **Form-Based Simulation:** The Hub will incorporate a web-based form submission system to mimic the behaviour of receiving an AuthnRequest. This approach will simplify initial testing and setup.\n2. **User Journey Simulation:** Upon successful form submission, users will be redirected to a sequence of pages simulating the typical Verify user journey. Each stage will include clear instructions and explanations to ensure the user understands where they are in their journey.\n3. **Scenario Selection:** Users will be presented with several response scenarios, each simulating different possible outcomes (e.g., Successful Match, Account Creation, No Match, Cancellation, Authentication Failed, Request Error, Internal Server Error).\n4. **Session Handling:** Relevant session data, such as user input and ongoing states, will be stored in session cookies. Initially, these cookies will be neither signed nor encrypted to streamline development.\n5. **Mock SAML Response:** Each response scenario will generate a form submission containing base64 encoded JSON reflecting the shape of a Hub SAML Response. This allows the prototype Service Provider to handle these canned responses and process different authentication outcomes.\n6. **User Input for Complex Scenarios:** For scenarios needing additional data (e.g., account creation attributes or PID values), subsequent pages will request and collect user input.\n7. **Custom Response URL:** Users will specify the URL where the Mock Hub should send its responses, allowing flexible integration with different test setups.\n\nInitially, the Mock Verify Hub will be deployed on a cloud platform that allows for easy scaling and maintenance, such as Government PaaS.\n\nBy creating this Mock Verify Hub, we can closely emulate the Verify user journey, enabling effective user research and comprehensive end-to-end testing of the Service Provider prototype.","GenTime":"2024-08-28 22:58:23"}
{"File Name":"log430-dashview-architecture\/0002-use-java-to-implement-complete-solution.md","Context":"## Context\\nThe issue motivating this decision, and any context that influences or constrains the decision.\\n","Decision":"The change that we're proposing or have agreed to implement.\\n","tokens":21,"id":3820,"text":"## Context\\nThe issue motivating this decision, and any context that influences or constrains the decision.\\n\n\n##Decision\nThe change that we're proposing or have agreed to implement.\\n","MatchedIds":[1145,4616,3546,4963,4867],"Predicted":"## Decision\nWe have decided to implement a microservices architecture to address the scalability and maintainability issues present in our current monolithic application. This change will enable independent deployment and scaling of services, facilitating more robust and agile development processes.","GenTime":"2024-08-28 22:58:25"}
{"File Name":"remultiform\/0004-use-prettier-to-format-code.md","Context":"## Context\\nWe want to ensure we're all using one code style, that is familiar across\\nprojects. [Prettier](https:\/\/prettier.io\/) is an opinionated code formatter with\\nsupport for most, if not all, of the languages in the JavaScript ecosystem. As\\nof writing, it is used by over\\n[1 million repositories](https:\/\/github.com\/prettier\/prettier\/network\/dependents?package_id=UGFja2FnZS00OTAwMTEyNTI%3D)\\non GitHub, and has become a standard.\\n","Decision":"We will enforce that everything supported by Prettier has its style enforced by\\nit.\\nWe will set up Git hooks to automatically run the formatter before committing.\\nWe will set continuous integration up to reject commits that are not correctly\\nformatted.\\n","tokens":121,"id":5255,"text":"## Context\\nWe want to ensure we're all using one code style, that is familiar across\\nprojects. [Prettier](https:\/\/prettier.io\/) is an opinionated code formatter with\\nsupport for most, if not all, of the languages in the JavaScript ecosystem. As\\nof writing, it is used by over\\n[1 million repositories](https:\/\/github.com\/prettier\/prettier\/network\/dependents?package_id=UGFja2FnZS00OTAwMTEyNTI%3D)\\non GitHub, and has become a standard.\\n\n\n##Decision\nWe will enforce that everything supported by Prettier has its style enforced by\\nit.\\nWe will set up Git hooks to automatically run the formatter before committing.\\nWe will set continuous integration up to reject commits that are not correctly\\nformatted.\\n","MatchedIds":[1408,4226,3321,3451,400],"Predicted":"## Decision\nWe will standardize on using Prettier as our code formatter for all supported languages within the JavaScript ecosystem. \n\nTo enforce this:\n1. We will configure Prettier across all projects to ensure consistent formatting.\n2. We will setup Git hooks to automatically format code with Prettier before every commit.\n3. We will configure our continuous integration pipelines to reject any commits that do not adhere to Prettier's formatting rules.","GenTime":"2024-08-28 22:58:28"}
{"File Name":"disco-poc-vue\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](https:\/\/cognitect.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":165,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](https:\/\/cognitect.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","MatchedIds":[4699,4408,4722,4428,3119],"Predicted":"## Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions","GenTime":"2024-08-28 22:58:30"}
{"File Name":"kuona\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":573,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","MatchedIds":[4699,4408,4722,4428,3119],"Predicted":"## Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions.","GenTime":"2024-08-28 22:58:31"}
{"File Name":"verify-service-provider\/0006-we-will-build-a-js-client.md","Context":"## Context\\nAt least one user is currently using node js and passport. We want to provide as\\nfrictionless as possible an integration for them.\\nOther users will be using other languages and frameworks.\\n","Decision":"We will initially build only a node \/ passport client. We will want to build\\nanother client in another language as soon as possible to make sure the API\\nis well designed.\\nUsers should also be able to interact with the API directly if we haven't built\\nan appropriate client for their use case.\\n","tokens":43,"id":4443,"text":"## Context\\nAt least one user is currently using node js and passport. We want to provide as\\nfrictionless as possible an integration for them.\\nOther users will be using other languages and frameworks.\\n\n\n##Decision\nWe will initially build only a node \/ passport client. We will want to build\\nanother client in another language as soon as possible to make sure the API\\nis well designed.\\nUsers should also be able to interact with the API directly if we haven't built\\nan appropriate client for their use case.\\n","MatchedIds":[577,3470,578,2405,527],"Predicted":"## Decision\nWe will initially build only a Node.js client that integrates with Passport. This approach ensures a seamless experience for our current user who relies on this stack. To accommodate future users who might be using different languages and frameworks, we will design our API to be language-agnostic and well-documented, enabling direct interaction with the API irrespective of the client.\n\nAs soon as the Node.js\/Passport integration is stable, we will prioritize developing additional clients in popular languages and frameworks (e.g., Python with Flask, Ruby on Rails) to validate the API's design and broaden our support. This will help ensure that our solution is versatile and can cater to a wider audience.","GenTime":"2024-08-28 22:58:35"}
{"File Name":"lbh-frontend-react\/0004-use-prettier-to-format-code.md","Context":"## Context\\nWe want to ensure we're all using one code style, that is familiar across\\nprojects. [Prettier](https:\/\/prettier.io\/) is an opinionated code formatter with\\nsupport for most, if not all, of the languages in the JavaScript ecosystem. As\\nof writing, it is used by over\\n[1 million repositories](https:\/\/github.com\/prettier\/prettier\/network\/dependents?package_id=UGFja2FnZS00OTAwMTEyNTI%3D)\\non GitHub, and has become a standard.\\n","Decision":"We will enforce that everything supported by Prettier has its style enforced by\\nit.\\nWe will set up Git hooks to automatically run the formatter before committing.\\nWe will set continuous integration up to reject commits that are not correctly\\nformatted.\\n","tokens":121,"id":1408,"text":"## Context\\nWe want to ensure we're all using one code style, that is familiar across\\nprojects. [Prettier](https:\/\/prettier.io\/) is an opinionated code formatter with\\nsupport for most, if not all, of the languages in the JavaScript ecosystem. As\\nof writing, it is used by over\\n[1 million repositories](https:\/\/github.com\/prettier\/prettier\/network\/dependents?package_id=UGFja2FnZS00OTAwMTEyNTI%3D)\\non GitHub, and has become a standard.\\n\n\n##Decision\nWe will enforce that everything supported by Prettier has its style enforced by\\nit.\\nWe will set up Git hooks to automatically run the formatter before committing.\\nWe will set continuous integration up to reject commits that are not correctly\\nformatted.\\n","MatchedIds":[5255,4226,3321,3451,400],"Predicted":"## Decision\nWe will enforce that everything supported by Prettier has its style enforced by it.  \nWe will set up Git hooks to automatically run the formatter before committing.  \nWe will set continuous integration up to reject commits that are not correctly formatted.","GenTime":"2024-08-28 22:58:37"}
{"File Name":"pace-developers\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":5182,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","MatchedIds":[4699,4408,4722,4428,3119],"Predicted":"## Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions.","GenTime":"2024-08-28 22:58:39"}
{"File Name":"digital-paper-edit-client\/2019-09-28-time-estimation-for-links.md","Context":"## Context and Problem Statement\\nPeople want to include time taken for voice overs to estimate the time of the rough cut.\\n## Decision Drivers <!-- optional -->\\n* Ease of implementation\\n* Accuracy of time estimated\\n* Extensibility\\n","Decision":"* Ease of implementation\\n* Accuracy of time estimated\\n* Extensibility\\nChosen option: 1, because it's the simplest implementation and a standardised way of calculating voice over estimations within the company.\\n### Positive Consequences <!-- optional -->\\n* A fairly easy implementation.\\n### Negative consequences <!-- optional -->\\n* Less accurate than other options.\\n* Would require us to change the icon and description of the Voice over, such that it's features are not misunderstood. (This was a point raised in [UX investigations: how to better signpost Voice over\/link](https:\/\/github.com\/bbc\/digital-paper-edit-client\/issues\/63))\\n","tokens":49,"id":4171,"text":"## Context and Problem Statement\\nPeople want to include time taken for voice overs to estimate the time of the rough cut.\\n## Decision Drivers <!-- optional -->\\n* Ease of implementation\\n* Accuracy of time estimated\\n* Extensibility\\n\n\n##Decision\n* Ease of implementation\\n* Accuracy of time estimated\\n* Extensibility\\nChosen option: 1, because it's the simplest implementation and a standardised way of calculating voice over estimations within the company.\\n### Positive Consequences <!-- optional -->\\n* A fairly easy implementation.\\n### Negative consequences <!-- optional -->\\n* Less accurate than other options.\\n* Would require us to change the icon and description of the Voice over, such that it's features are not misunderstood. (This was a point raised in [UX investigations: how to better signpost Voice over\/link](https:\/\/github.com\/bbc\/digital-paper-edit-client\/issues\/63))\\n","MatchedIds":[5250,3187,4701,794,4062],"Predicted":"## Decision\n* Ease of implementation\n* Accuracy of time estimated\n* Extensibility\n\nChosen option: 2, because it offers a balanced approach between implementation effort and accuracy in time estimation, and allows future enhancements as needed.\n\n### Positive Consequences <!-- optional -->\n* More accurate estimations compared to simpler methods.\n* Aids in better planning and resource allocation for the project.\n\n### Negative Consequences <!-- optional -->\n* Requires a moderately complex implementation compared to the simplest option.\n* Potentially needs occasional adjustments and maintenance to ensure continued accuracy and adaptability.","GenTime":"2024-08-28 22:58:42"}
{"File Name":"react-template\/0005-use-jest.md","Context":"## Context\\nWe want a test framework that has good support for React and TypeScript.\\n[Jest](https:\/\/jestjs.io) is the standard, recommended test framework for React\\napps.\\n","Decision":"We will use Jest as our testing framework.\\n","tokens":40,"id":3323,"text":"## Context\\nWe want a test framework that has good support for React and TypeScript.\\n[Jest](https:\/\/jestjs.io) is the standard, recommended test framework for React\\napps.\\n\n\n##Decision\nWe will use Jest as our testing framework.\\n","MatchedIds":[4223,452,2273,4785,4359],"Predicted":"## Decision\nWe will use Jest as our testing framework for React and TypeScript applications.","GenTime":"2024-08-28 22:58:43"}
{"File Name":"mat-process-utils\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as\\n[described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":3763,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as\\n[described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","MatchedIds":[4699,4408,4722,4428,3119],"Predicted":"## Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","GenTime":"2024-08-28 22:58:45"}
{"File Name":"octagon\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":4597,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","MatchedIds":[4699,4408,4722,4428,3119],"Predicted":"## Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions.","GenTime":"2024-08-28 22:58:47"}
{"File Name":"buy-for-your-school\/0005-use-bullet-to-catch-nplus1-queries.md","Context":"## Context\\nIt can be easy to miss an inefficient database query during code review. These can build up and have detremental performance on the application and effect the user experience.\\n","Decision":"Add an automatic check to the test suite to ensure (through CI) that these are fixed before being deployed.\\n","tokens":37,"id":1246,"text":"## Context\\nIt can be easy to miss an inefficient database query during code review. These can build up and have detremental performance on the application and effect the user experience.\\n\n\n##Decision\nAdd an automatic check to the test suite to ensure (through CI) that these are fixed before being deployed.\\n","MatchedIds":[2383,1031,1416,5070,1434],"Predicted":"## Decision\nIntegrate a database performance monitoring tool that can analyze query performance in real-time and generate alerts for inefficient queries. Additionally, institute regular automated performance testing to identify and address inefficient queries before code is merged.","GenTime":"2024-08-28 22:58:49"}
{"File Name":"radiant-mlhub\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use a modified version of Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions). ADR documents that are specific to this project will be stored in the `docs\/adr` directory within this repository. Instead of using the \"deprecated\" and \"superseded\" status value, we will move ADRs that are no longer applicable into the `docs\/adr\/archive` directory in this repository.\\n","tokens":16,"id":704,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use a modified version of Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions). ADR documents that are specific to this project will be stored in the `docs\/adr` directory within this repository. Instead of using the \"deprecated\" and \"superseded\" status value, we will move ADRs that are no longer applicable into the `docs\/adr\/archive` directory in this repository.\\n","MatchedIds":[4428,3119,1436,294,2583],"Predicted":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).","GenTime":"2024-08-28 22:58:51"}
{"File Name":"lbh-frontend-react\/0005-use-eslint.md","Context":"## Context\\nWe want to enforce consistency in our code, and catch as many errors\\nautomatically as we are able to. Linting the code is good practice to achieve\\nthese aims. [ESLint](https:\/\/eslint.org\/) is the standard linter for modern\\nJavaScript, and has good support for TypeScript though plugins.\\n","Decision":"We will check code style using ESLint.\\nWe will let Prettier have precedence when ESLint and Prettier conflict in their\\nstyles.\\nWe will use the recommended configuration for plugins where possible.\\nWe will run ESLint as part of the test suite.\\n","tokens":69,"id":1404,"text":"## Context\\nWe want to enforce consistency in our code, and catch as many errors\\nautomatically as we are able to. Linting the code is good practice to achieve\\nthese aims. [ESLint](https:\/\/eslint.org\/) is the standard linter for modern\\nJavaScript, and has good support for TypeScript though plugins.\\n\n\n##Decision\nWe will check code style using ESLint.\\nWe will let Prettier have precedence when ESLint and Prettier conflict in their\\nstyles.\\nWe will use the recommended configuration for plugins where possible.\\nWe will run ESLint as part of the test suite.\\n","MatchedIds":[4224,458,1408,5255,4226],"Predicted":"## Decision\nWe will check code style using ESLint.  \nWe will use the recommended configuration for plugins where possible.  \nWe will integrate ESLint with our TypeScript configurations using the appropriate plugins.  \nWe will run ESLint as part of the test suite to catch errors early.  \nWe will set up Git hooks to automatically run ESLint before committing.  \nWe will configure continuous integration to fail builds if ESLint errors are detected.","GenTime":"2024-08-28 22:58:53"}
{"File Name":"unfinished-design-system\/002-unit-testing-tools.md","Context":"* [Context](#context)\\n* [Decision](#decision)\\n* [Status](#status)\\n* [Consequences](#consequences)\\n* [More reading](#more-reading)\\n* [Updates](#updates)\\n","Decision":"* [Status](#status)\\n* [Consequences](#consequences)\\n* [More reading](#more-reading)\\n* [Updates](#updates)\\nWe've decided to use Jest with Enzyme.\\nJest is an incredible unit testing tool for Javascript. It is easy to use, has a great syntax, and our team is familiar with it.\\nEnzyme, on the other hand, is a unit testing tool for React components. It has some libraries that help us debugging and testing those resources.\\nWe've decided on this stack because our team has already used it in the past, and it can improve our development speed.\\n","tokens":53,"id":4547,"text":"* [Context](#context)\\n* [Decision](#decision)\\n* [Status](#status)\\n* [Consequences](#consequences)\\n* [More reading](#more-reading)\\n* [Updates](#updates)\\n\n\n##Decision\n* [Status](#status)\\n* [Consequences](#consequences)\\n* [More reading](#more-reading)\\n* [Updates](#updates)\\nWe've decided to use Jest with Enzyme.\\nJest is an incredible unit testing tool for Javascript. It is easy to use, has a great syntax, and our team is familiar with it.\\nEnzyme, on the other hand, is a unit testing tool for React components. It has some libraries that help us debugging and testing those resources.\\nWe've decided on this stack because our team has already used it in the past, and it can improve our development speed.\\n","MatchedIds":[4910,4918,4544,4550,4549],"Predicted":"## Context\nOur current logging infrastructure relies heavily on a homegrown solution that does not scale well with the increasing load and variety of logs our application generates. Additionally, our existing system lacks advanced search and visualization capabilities, which makes it difficult to debug issues efficiently.\n\n## Decision\nWe've decided to adopt the ELK Stack (Elasticsearch, Logstash, Kibana) for our logging infrastructure. This decision was driven by the need for a more scalable, flexible, and feature-rich solution to handle our growing logging requirements.\n\n## Status\nAccepted\n\n## Consequences\n### Positive\n1. **Scalability**: Elasticsearch's distributed nature will allow us to scale horizontally as our logging needs grow.\n2. **Advanced Search**: Elasticsearch provides powerful search capabilities that enable efficient querying of logs.\n3. **Visualization**: Kibana offers extensive visualization tools, which will greatly improve our ability to monitor, analyze, and troubleshoot application issues.\n4. **Flexibility**: Logstash enables us to ingest and parse logs from various sources, providing a unified logging solution.\n\n### Negative\n1. **Learning Curve**: The team will need to invest time to learn and become proficient with the ELK Stack tools.\n2. **Resource Intensive**: Running the full ELK Stack can be resource-heavy and may require us to upgrade our infrastructure.\n3. **Migration Effort**: There will be a non-trivial effort required to transition from our homegrown solution to the ELK Stack.\n\n## More reading\n- [Elasticsearch Documentation](https:\/\/www.elastic.co\/guide\/en\/elasticsearch\/reference\/current\/index.html)\n- [Logstash Documentation](https:\/\/www.elastic.co\/guide\/en\/logstash\/current\/index.html)\n- [Kibana Documentation](https:\/\/www.elastic.co\/guide\/en\/kibana\/current\/index.html)\n\n## Updates\n1. Implementation of the ELK Stack will begin in Q1 2024 with a phased roll-out plan to ensure minimal disruption.\n2. Training sessions for the team on using and managing the ELK Stack will be scheduled.\n\nBy adopting the ELK Stack, we aim to significantly enhance our logging capabilities, facilitating improved diagnostics and faster issue resolution, all while ensuring scalability for future growth.","GenTime":"2024-08-28 22:58:59"}
{"File Name":"amf\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\nSome other useful links:\\n[https:\/\/github.blog\/2020-08-13-why-write-adrs\/](https:\/\/github.blog\/2020-08-13-why-write-adrs\/)\\n[https:\/\/understandlegacycode.com\/blog\/earn-maintainers-esteem-with-adrs\/](https:\/\/understandlegacycode.com\/blog\/earn-maintainers-esteem-with-adrs\/)\\n","tokens":16,"id":1397,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\nSome other useful links:\\n[https:\/\/github.blog\/2020-08-13-why-write-adrs\/](https:\/\/github.blog\/2020-08-13-why-write-adrs\/)\\n[https:\/\/understandlegacycode.com\/blog\/earn-maintainers-esteem-with-adrs\/](https:\/\/understandlegacycode.com\/blog\/earn-maintainers-esteem-with-adrs\/)\\n","MatchedIds":[4699,4408,4722,4428,3119],"Predicted":"## Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions.","GenTime":"2024-08-28 22:59:01"}
{"File Name":"profiles-etl-combiner\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":871,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","MatchedIds":[4699,4408,4722,4428,3119],"Predicted":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).","GenTime":"2024-08-28 22:59:03"}
{"File Name":"terraform-aws-s3-static-website-module\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":3068,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","MatchedIds":[4699,4408,4722,4428,3119],"Predicted":"## Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions","GenTime":"2024-08-28 22:59:05"}
{"File Name":"meadow\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":3854,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","MatchedIds":[4699,4408,4722,4428,3119],"Predicted":"## Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions.","GenTime":"2024-08-28 22:59:06"}
{"File Name":"runbooks-mkdocs\/0003-use-plantuml-for-diagramming.md","Context":"## Context\\nThe issue motivating this decision, and any context that influences or constrains the decision.\\n","Decision":"The change that we're proposing or have agreed to implement.\\n","tokens":21,"id":1145,"text":"## Context\\nThe issue motivating this decision, and any context that influences or constrains the decision.\\n\n\n##Decision\nThe change that we're proposing or have agreed to implement.\\n","MatchedIds":[3820,4616,3546,4963,4867],"Predicted":"## Decision\nWe have decided to adopt a microservices architecture to handle the increasing complexity and scalability requirements of our system. This will involve decomposing our current monolithic application into smaller, independent services that can be developed, deployed, and scaled individually.","GenTime":"2024-08-28 22:59:08"}
{"File Name":"ontrack\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":5122,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","MatchedIds":[4699,4408,4722,4428,3119],"Predicted":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).","GenTime":"2024-08-28 22:59:10"}
{"File Name":"git-en-boite\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":3505,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","MatchedIds":[4699,4408,4722,4428,3119],"Predicted":"## Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions.","GenTime":"2024-08-28 22:59:11"}
{"File Name":"architecture-decision-log\/0013-microservices-overview.md","Context":"* [Context](#context)\\n* [Decision](#decision)\\n* [Status](#status)\\n* [Consequences](#consequences)\\n* [More reading](#more-reading)\\n* [Updates](#updates)\\n","Decision":"* [Status](#status)\\n* [Consequences](#consequences)\\n* [More reading](#more-reading)\\n* [Updates](#updates)\\nWithin this ADR, we're going to keep an overview of our current microservices architecture. Each application has a pre-defined boundary, as well as a well-defined technical definition.\\nYou can take a look at our [live architecture](https:\/\/app.mural.co\/t\/d4c6342\/m\/d4c6342\/1602612797113\/5836de51b220fa5ebc8cefd7ad0f161125610524) or check the diagram below:\\n![Diagram showing our microservices architecture](..\/assets\/0013-microservices-overview\/diagram.png)\\n","tokens":53,"id":4910,"text":"* [Context](#context)\\n* [Decision](#decision)\\n* [Status](#status)\\n* [Consequences](#consequences)\\n* [More reading](#more-reading)\\n* [Updates](#updates)\\n\n\n##Decision\n* [Status](#status)\\n* [Consequences](#consequences)\\n* [More reading](#more-reading)\\n* [Updates](#updates)\\nWithin this ADR, we're going to keep an overview of our current microservices architecture. Each application has a pre-defined boundary, as well as a well-defined technical definition.\\nYou can take a look at our [live architecture](https:\/\/app.mural.co\/t\/d4c6342\/m\/d4c6342\/1602612797113\/5836de51b220fa5ebc8cefd7ad0f161125610524) or check the diagram below:\\n![Diagram showing our microservices architecture](..\/assets\/0013-microservices-overview\/diagram.png)\\n","MatchedIds":[4547,4918,4544,4550,4549],"Predicted":"## Decision\nWe've decided to use [GraphQL](https:\/\/graphql.org\/) as our API query language. GraphQL provides a more efficient, powerful, and flexible alternative to REST. It allows clients to request only the data they need, reducing over-fetching and under-fetching of data, which results in more efficient use of network resources. It also facilitates better API evolution and versioning.\n\n* [Status](#status)\nActive\n\n* [Consequences](#consequences)\n  - **Positive**: \n    - More efficient network requests by only retrieving the necessary data.\n    - Easier to evolve APIs without versioning.\n    - Strongly typed schema which improves API documentation and developer experience.\n    - Reduced number of API endpoints.\n  - **Negative**:\n    - Steeper learning curve for developers unfamiliar with GraphQL.\n    - Initial setup and tooling require more effort and time compared to REST.\n    - Potential for more complex queries that could impact performance if not managed properly.\n\n* [More reading](#more-reading)\n- [GraphQL vs REST](https:\/\/graphql.org\/learn\/serving-over-http\/#comparison-with-rest-apis)\n- [Introduction to GraphQL](https:\/\/www.apollographql.com\/docs\/tutorial\/introduction\/)\n\n* [Updates](#updates)\nNo updates at this time.","GenTime":"2024-08-28 22:59:17"}
{"File Name":"api-docs\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":5155,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","MatchedIds":[4699,4408,4722,4428,3119],"Predicted":"## Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions.","GenTime":"2024-08-28 22:59:19"}
{"File Name":"molgenis-service-armadillo\/0001-use-adr-to-describe-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions.\\n","tokens":16,"id":1328,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions.\\n","MatchedIds":[4699,4408,4722,4428,3119],"Predicted":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","GenTime":"2024-08-28 22:59:20"}
{"File Name":"react-library-template\/0004-use-prettier-to-format-code.md","Context":"## Context\\nWe want to ensure we're all using one code style, that is familiar across\\nprojects. [Prettier](https:\/\/prettier.io\/) is an opinionated code formatter with\\nsupport for most, if not all, of the languages in the JavaScript ecosystem. As\\nof writing, it is used by over\\n[1 million repositories](https:\/\/github.com\/prettier\/prettier\/network\/dependents?package_id=UGFja2FnZS00OTAwMTEyNTI%3D)\\non GitHub, and has become a standard.\\n","Decision":"We will enforce that everything supported by Prettier has its style enforced by\\nit.\\nWe will set up Git hooks to automatically run the formatter before committing.\\nWe will set continuous integration up to reject commits that are not correctly\\nformatted.\\n","tokens":121,"id":4226,"text":"## Context\\nWe want to ensure we're all using one code style, that is familiar across\\nprojects. [Prettier](https:\/\/prettier.io\/) is an opinionated code formatter with\\nsupport for most, if not all, of the languages in the JavaScript ecosystem. As\\nof writing, it is used by over\\n[1 million repositories](https:\/\/github.com\/prettier\/prettier\/network\/dependents?package_id=UGFja2FnZS00OTAwMTEyNTI%3D)\\non GitHub, and has become a standard.\\n\n\n##Decision\nWe will enforce that everything supported by Prettier has its style enforced by\\nit.\\nWe will set up Git hooks to automatically run the formatter before committing.\\nWe will set continuous integration up to reject commits that are not correctly\\nformatted.\\n","MatchedIds":[1408,5255,3321,3451,400],"Predicted":"## Decision\nWe will enforce the use of Prettier for all JavaScript-based projects to maintain a consistent code style across all repositories. We will integrate Prettier with our development workflow by:\n\n1. Setting up Git hooks to automatically run Prettier before commits.\n2. Configuring our continuous integration pipeline to reject commits that do not adhere to the Prettier formatting rules.\n3. Ensuring that Prettier is included as a dependency in all relevant projects.\n4. Providing documentation and initial configuration files to help developers integrate Prettier into their local development environments.","GenTime":"2024-08-28 22:59:24"}
{"File Name":"twig-infrastructure\/0005-build-and-deploy-pipelines-separate.md","Context":"## Context\\nIn the previous iteration, build and deploy were consolodated into one pipeline. The result of this being that any type of configuration change required a full rebuild to deploy.  This could become unwieldy with projects that have a long-running build step.\\n","Decision":"Pipeline has been split into distinct build and deploy pipelines.  The build pipeline, in addition to the image that it uploads to ECR, exports artifacts build.json and src.zip.  Src.zip is required still required in the deploy pipeline to run integration tests.  In the deploy pipeline, either the artifacts supplied by the build pipeline OR new app.yaml templates will trigger the pipeline.  Consequently, a config change may be made by uploading a new app.yaml, without having to re-build the Docker image.\\n","tokens":56,"id":351,"text":"## Context\\nIn the previous iteration, build and deploy were consolodated into one pipeline. The result of this being that any type of configuration change required a full rebuild to deploy.  This could become unwieldy with projects that have a long-running build step.\\n\n\n##Decision\nPipeline has been split into distinct build and deploy pipelines.  The build pipeline, in addition to the image that it uploads to ECR, exports artifacts build.json and src.zip.  Src.zip is required still required in the deploy pipeline to run integration tests.  In the deploy pipeline, either the artifacts supplied by the build pipeline OR new app.yaml templates will trigger the pipeline.  Consequently, a config change may be made by uploading a new app.yaml, without having to re-build the Docker image.\\n","MatchedIds":[1739,187,3722,1741,5102],"Predicted":"## Decision\nThe pipeline has been restructured into two distinct pipelines: build and deploy. The build pipeline is responsible for compiling the code and creating the Docker images, which are then uploaded to a container registry like ECR. The build pipeline also generates artifacts such as build.json and source.zip that are used in subsequent steps.\n\nThe deploy pipeline now operates independently and is triggered by changes to the deployment configuration files, like app.yaml, or by the artifacts generated from the build pipeline. This allows for deployment without necessitating a rebuild when only configuration changes are made, thereby expediting the deployment process for projects with lengthy build times.\n\nAdditionally, the deploy pipeline ensures that integration tests are run using the source.zip artifact from the build pipeline. This separation allows for a more streamlined and efficient workflow.","GenTime":"2024-08-28 22:59:29"}
{"File Name":"aspan-server\/0004-info-graphql-import.md","Context":"## Context\\nImport & export definitions in GraphQL SDL (also refered to as GraphQL modules)\\n","Decision":"Decision here...\\n","tokens":20,"id":1893,"text":"## Context\\nImport & export definitions in GraphQL SDL (also refered to as GraphQL modules)\\n\n\n##Decision\nDecision here...\\n","MatchedIds":[3194,899,485,2710,5259],"Predicted":"## Decision\nWe will use the `graphql-modules` library, which provides a structured way to import and export GraphQL SDL definitions. By using `graphql-modules`, we can ensure modularity and reusability in our GraphQL schemas, allowing for better organization and maintainability. Each module will encapsulate its own type definitions, resolvers, and any additional configuration, which can then be easily composed with other modules in the application.","GenTime":"2024-08-28 22:59:31"}
{"File Name":"openfido-app-service\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":2472,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","MatchedIds":[4699,4408,4722,4428,3119],"Predicted":"## Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions.","GenTime":"2024-08-28 22:59:33"}
{"File Name":"adr\/0002-SYS-rdbms.md","Context":"## Context\\nThe issue motivating this decision, and any context that influences or constrains the decision.\\n","Decision":"The change that we're proposing or have agreed to implement.\\n","tokens":21,"id":4616,"text":"## Context\\nThe issue motivating this decision, and any context that influences or constrains the decision.\\n\n\n##Decision\nThe change that we're proposing or have agreed to implement.\\n","MatchedIds":[3820,1145,3546,4963,4867],"Predicted":"## Decision\nWe will adopt a microservices architecture to address scalability and maintainability concerns. This decision is informed by our current monolithic application's difficulty in scaling efficiently and the challenges faced in managing and deploying updates. By leveraging microservices, we can independently develop, deploy, and scale each service, aligning with the needs of various teams and improving overall system reliability and performance.","GenTime":"2024-08-28 22:59:36"}
{"File Name":"pomi-data-etl\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":2487,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","MatchedIds":[4699,4408,4722,4428,3119],"Predicted":"## Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions","GenTime":"2024-08-28 22:59:37"}
{"File Name":"digital-paper-edit-client\/2019-04-29-SQL-database-schema.md","Context":"## Context and Problem Statement\\n<!-- [Describe the context and problem statement, e.g., in free form using two to three sentences. You may want to articulate the problem in form of a question.] -->\\nA schema for the SQL database in PostgreSQL for the web app, to be reused for SQLite electron desktop version.\\nIt should consider [the project Milestones](https:\/\/github.com\/bbc\/digital-paper-edit\/milestones) and be easy to version incrementally.\\nEg have 3 version of the schema for the 3 main milestones:\\n- [Feature: Transcript Editing](https:\/\/github.com\/bbc\/digital-paper-edit\/milestone\/3)\\n- [Feature: Transcript Annotations](https:\/\/github.com\/bbc\/digital-paper-edit\/milestone\/4)\\n- [Feature: Paper Editing](https:\/\/github.com\/bbc\/digital-paper-edit\/milestone\/5)\\n[DB Designer](https:\/\/dbdesigner.page.link\/cq9FMHVVxsYqTasf7) - let's you export SQL code to Create and drop tables.\\n## Decision Drivers <!-- optional -->\\n* easy to reason around\\n* \u2026 <!-- numbers of drivers can vary -->\\n","Decision":"* easy to reason around\\n* \u2026 <!-- numbers of drivers can vary -->\\n<!-- Chosen option: \"[option 1]\", because [justification. e.g., only option, which meets k.o. criterion decision driver | which resolves force force | \u2026 | comes out best (see below)]. -->\\n_TBC_\\n### Positive Consequences <!-- optional -->\\n<!-- * [e.g., improvement of quality attribute satisfaction, follow-up decisions required, \u2026] -->\\n* \u2026\\n_TBC_\\n### Negative consequences <!-- optional -->\\n<!-- * [e.g., compromising quality attribute, follow-up decisions required, \u2026] -->\\n* \u2026\\n_TBC_\\n","tokens":248,"id":4167,"text":"## Context and Problem Statement\\n<!-- [Describe the context and problem statement, e.g., in free form using two to three sentences. You may want to articulate the problem in form of a question.] -->\\nA schema for the SQL database in PostgreSQL for the web app, to be reused for SQLite electron desktop version.\\nIt should consider [the project Milestones](https:\/\/github.com\/bbc\/digital-paper-edit\/milestones) and be easy to version incrementally.\\nEg have 3 version of the schema for the 3 main milestones:\\n- [Feature: Transcript Editing](https:\/\/github.com\/bbc\/digital-paper-edit\/milestone\/3)\\n- [Feature: Transcript Annotations](https:\/\/github.com\/bbc\/digital-paper-edit\/milestone\/4)\\n- [Feature: Paper Editing](https:\/\/github.com\/bbc\/digital-paper-edit\/milestone\/5)\\n[DB Designer](https:\/\/dbdesigner.page.link\/cq9FMHVVxsYqTasf7) - let's you export SQL code to Create and drop tables.\\n## Decision Drivers <!-- optional -->\\n* easy to reason around\\n* \u2026 <!-- numbers of drivers can vary -->\\n\n\n##Decision\n* easy to reason around\\n* \u2026 <!-- numbers of drivers can vary -->\\n<!-- Chosen option: \"[option 1]\", because [justification. e.g., only option, which meets k.o. criterion decision driver | which resolves force force | \u2026 | comes out best (see below)]. -->\\n_TBC_\\n### Positive Consequences <!-- optional -->\\n<!-- * [e.g., improvement of quality attribute satisfaction, follow-up decisions required, \u2026] -->\\n* \u2026\\n_TBC_\\n### Negative consequences <!-- optional -->\\n<!-- * [e.g., compromising quality attribute, follow-up decisions required, \u2026] -->\\n* \u2026\\n_TBC_\\n","MatchedIds":[5244,1661,160,3053,3776],"Predicted":"## Decision\nChosen option: Use a modular, milestone-based schema design with incremental versioning.\n\n### Justification:\n1. **Milestone Alignment**: By aligning the database schema with specific project milestones (Transcript Editing, Transcript Annotations, Paper Editing), the schema can evolve in a controlled and predictable manner. This modularity ensures that as new features are developed, the database structure can be incrementally enhanced to support those features.\n2. **Ease of Versioning**: Using a version-controlled schema allows for easier rollbacks and forward changes, making it simpler to manage incremental updates and maintain consistency across different environments (PostgreSQL for web app and SQLite for desktop app).\n3. **Reusability Across Platforms**: Ensuring compatibility between PostgreSQL and SQLite requires careful design, such as avoiding PostgreSQL-specific data types and features that are not supported natively in SQLite. By adhering to common SQL standards where possible, we can maximize the portability of the schema.\n\n### Positive Consequences\n* **Scalability**: The schema can grow as new features are added, without requiring a complete overhaul.\n* **Maintainability**: Developers can easily navigate through schema versions tied to project milestones, improving collaboration and understanding.\n* **Consistency**: Ensures consistent data structure across both the web and desktop versions of the application.\n\n### Negative Consequences\n* **Initial Complexity**: The need to plan the schema in alignment with milestones might introduce initial complexity.\n* **Version Management Overhead**: Managing multiple versions of the schema can introduce some overhead in terms of version tracking and migrations.\n\n### Implementation Steps\n1. **Define Initial Schema**: Based on the \"Feature: Transcript Editing\" milestone, outline the initial database schema.\n2. **Export Schema**: Use DB Designer to export the SQL code for PostgreSQL.\n3. **Adapt for SQLite**: Modify the exported SQL to ensure compatibility with SQLite.\n4. **Versioning**: Introduce versioning into the schema using a tool like Liquibase or Flyway to manage migrations.\n5. **Incremental Updates**: For each subsequent milestone (Transcript Annotations, Paper Editing), update the schema incrementally, ensuring backward compatibility and version updates.\n6. **Testing**: Thoroughly test each version of the schema in both PostgreSQL and SQLite environments before deployment.","GenTime":"2024-08-28 22:59:47"}
{"File Name":"CAFE5\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":1751,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","MatchedIds":[4428,3119,1436,294,2583],"Predicted":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).","GenTime":"2024-08-28 22:59:49"}
{"File Name":"bookit-api\/0006-use-okta-as-identity-provider.md","Context":"## Context\\nOkta is an Identity as a Service provider (similar to Auth0) that can provide authentication and authorization support to an application.  They have a forever free developer account that allows up to 7000 active users\/mo (well below our anticipated usage).\\nOkta provides the ability to authenticate against a variety of providers (okta, social (facebook, github, microsoft azure, etc), SAML, AD\/LDAP).  We would use the Microsoft social OpenConnect ID provider.  It will map the Microsoft user into an Okta user, essentially acting as a proxy or facade to Microsoft while providing a more uniform and standards adhering API.\\nThey have integrations into Spring Boot 1.x (via okta spring security starter) and Spring Security 5.x (via Spring Boot 2.x) supports Okta out of the box.\\nThey have a React client SDK instead of using our handrolled client code for Microsoft Azure AD.\\nThe real value add comes when we need to add roles\/groups (admin level users).  You get admin screens\/dashboards \"for free\" to add\/remove users etc.\\nLater on, we could add additional social logins (or support AD directly if needed).\\nSpike can be found at: https:\/\/github.com\/buildit\/bookit-api\/tree\/okta-spring-boot-1.x\\n","Decision":"* Delay for now - because we think we still need a local user table, we can add this in later\\n* Use Okta as IdP for Bookit\\n* Use Okta's React client for client side code\\n* Use Okta's Spring Boot 1.x starter for\\n* Use Okta Access Tokens as Bearer tokens\\n","tokens":279,"id":3217,"text":"## Context\\nOkta is an Identity as a Service provider (similar to Auth0) that can provide authentication and authorization support to an application.  They have a forever free developer account that allows up to 7000 active users\/mo (well below our anticipated usage).\\nOkta provides the ability to authenticate against a variety of providers (okta, social (facebook, github, microsoft azure, etc), SAML, AD\/LDAP).  We would use the Microsoft social OpenConnect ID provider.  It will map the Microsoft user into an Okta user, essentially acting as a proxy or facade to Microsoft while providing a more uniform and standards adhering API.\\nThey have integrations into Spring Boot 1.x (via okta spring security starter) and Spring Security 5.x (via Spring Boot 2.x) supports Okta out of the box.\\nThey have a React client SDK instead of using our handrolled client code for Microsoft Azure AD.\\nThe real value add comes when we need to add roles\/groups (admin level users).  You get admin screens\/dashboards \"for free\" to add\/remove users etc.\\nLater on, we could add additional social logins (or support AD directly if needed).\\nSpike can be found at: https:\/\/github.com\/buildit\/bookit-api\/tree\/okta-spring-boot-1.x\\n\n\n##Decision\n* Delay for now - because we think we still need a local user table, we can add this in later\\n* Use Okta as IdP for Bookit\\n* Use Okta's React client for client side code\\n* Use Okta's Spring Boot 1.x starter for\\n* Use Okta Access Tokens as Bearer tokens\\n","MatchedIds":[4414,5211,4422,3421,2582],"Predicted":"## Decision\n* Use Okta as the primary Identity as a Service provider.\n* Leverage the Microsoft social OpenConnect ID provider for authenticating users, which will act as a proxy or facade to Microsoft Azure AD.\n* Implement Okta's React client SDK for the client-side authentication code, replacing our custom client code for Microsoft Azure AD.\n* Use the Okta Spring Boot 2.x integration (via Spring Security 5.x) to support authentication and authorization in our application.\n* Utilize Okta's admin dashboards for adding and removing users, as well as managing roles and groups.\n* Plan to add additional social logins or direct Active Directory support based on future needs.","GenTime":"2024-08-28 22:59:51"}
{"File Name":"ADR\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":440,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","MatchedIds":[4699,4408,4722,4428,3119],"Predicted":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).","GenTime":"2024-08-28 22:59:53"}
{"File Name":"sre-challenge\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":3006,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","MatchedIds":[4428,3119,1436,294,2583],"Predicted":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).","GenTime":"2024-08-28 22:59:55"}
{"File Name":"react-template\/0014-use-dependabot-to-keep-dependencies-up-to-date.md","Context":"## Context\\nWe want to ensure all dependencies stay up to date. Dependabot offers a service\\nwhere a bot opens PRs on GitHub when new versions of dependencies are released.\\n","Decision":"We will use Dependabot to monitor dependency updates.\\n","tokens":38,"id":3328,"text":"## Context\\nWe want to ensure all dependencies stay up to date. Dependabot offers a service\\nwhere a bot opens PRs on GitHub when new versions of dependencies are released.\\n\n\n##Decision\nWe will use Dependabot to monitor dependency updates.\\n","MatchedIds":[4225,241,1048,5070,1258],"Predicted":"## Decision\nWe will implement Dependabot to automatically create pull requests on GitHub for new versions of our dependencies, ensuring that our dependencies stay up-to-date with minimal manual intervention.","GenTime":"2024-08-28 22:59:56"}
{"File Name":"react-library-template\/0007-use-dependabot-to-keep-dependencies-up-to-date.md","Context":"## Context\\nWe want to ensure all dependencies stay up to date. Dependabot offers a service\\nwhere a bot opens PRs on GitHub when new versions of dependencies are released.\\n","Decision":"We will use Dependabot to monitor dependency updates.\\n","tokens":38,"id":4225,"text":"## Context\\nWe want to ensure all dependencies stay up to date. Dependabot offers a service\\nwhere a bot opens PRs on GitHub when new versions of dependencies are released.\\n\n\n##Decision\nWe will use Dependabot to monitor dependency updates.\\n","MatchedIds":[3328,241,1048,5070,1258],"Predicted":"## Decision\nWe will use Dependabot to monitor and manage dependency updates by automatically opening pull requests on GitHub whenever new versions of our dependencies are available. This will ensure that our dependencies remain current and secure with minimal manual intervention.","GenTime":"2024-08-28 22:59:59"}
{"File Name":"buy-for-your-school\/0006-use-dotenv-for-managing-environment-variables.md","Context":"## Context\\nAccessing ENV directly without a wrapper is limited and can introduce problems.\\nWe want our tooling to help us guard against missing environment variables. When `nil` is accidentally provided during the start up process it is preferable to fail fast with an explicit message. Without this `nil` can be passed down through the stack and cause strange behaviour where the code has been designed with it as a dependency. Instead of adding `nil` guards throughout the codebase for required environment variables (eg. `ENV.fetch('FOO', 'default')`, this should be managed centrally.\\nWe have previously used Figaro for this purpose but it was deprecated in 2016 https:\/\/github.com\/laserlemon\/figaro. We should only use supported gems to ensure we get support in the form of fixes and security patches.\\nWe also want to be able to stub our environment variables in our test suite. An easy example of this is when we use environment variables as a feature flag mechanism. We want to stub the value to test both scenarios without being influenced by real values being loaded. Mutating the actual ENV value (eg. ` allow(ENV).to receive(:[]).with('BOX_ID').and_return(\"1234\")`) is possible but may have unexpected consequences where more than 1 part of the same process under test uses the same variable. Figaro used to be a handy abstraction layer that we could stub eg. `allow(Figaro).to receive(:env).with(:foo).and_return('bar')`. We should then consider how we can stub environment variables.\\n","Decision":"Use DotEnv to load our environment variables.\\n","tokens":324,"id":1254,"text":"## Context\\nAccessing ENV directly without a wrapper is limited and can introduce problems.\\nWe want our tooling to help us guard against missing environment variables. When `nil` is accidentally provided during the start up process it is preferable to fail fast with an explicit message. Without this `nil` can be passed down through the stack and cause strange behaviour where the code has been designed with it as a dependency. Instead of adding `nil` guards throughout the codebase for required environment variables (eg. `ENV.fetch('FOO', 'default')`, this should be managed centrally.\\nWe have previously used Figaro for this purpose but it was deprecated in 2016 https:\/\/github.com\/laserlemon\/figaro. We should only use supported gems to ensure we get support in the form of fixes and security patches.\\nWe also want to be able to stub our environment variables in our test suite. An easy example of this is when we use environment variables as a feature flag mechanism. We want to stub the value to test both scenarios without being influenced by real values being loaded. Mutating the actual ENV value (eg. ` allow(ENV).to receive(:[]).with('BOX_ID').and_return(\"1234\")`) is possible but may have unexpected consequences where more than 1 part of the same process under test uses the same variable. Figaro used to be a handy abstraction layer that we could stub eg. `allow(Figaro).to receive(:env).with(:foo).and_return('bar')`. We should then consider how we can stub environment variables.\\n\n\n##Decision\nUse DotEnv to load our environment variables.\\n","MatchedIds":[3389,3388,2387,5227,4359],"Predicted":"## Decision\nUse the `dotenv` gem to load our environment variables and the `config` gem to manage them centrally.\n\n* **DotEnv**: We will use `dotenv` to load environment variables from a `.env` file into the ENV hash during application initialization. This ensures that all necessary environment variables are available from the start.\n\n* **Config**: We will use the `config` gem to centrally manage and access environment variables. The `config` gem allows us to define default values, provide a structure for configuration, and fail fast when required variables are missing. This approach reduces the risk of `nil` values propagating through the stack.\n\n* **Stubbing in Tests**: While testing, we will leverage RSpec\u2019s built-in facilities for stubbing environment variables. We can use `climate_control` gem for this purpose, allowing us to temporarily modify environment variables within a block to ensure they do not affect other parts of the test suite.\n\n### Implementation Steps:\n1. **Add Dependencies**: Include `dotenv-rails`, `config`, and `climate_control` in the Gemfile.\n    ```ruby\n    gem 'dotenv-rails', groups: [:development, :test]\n    gem 'config'\n    gem 'climate_control', groups: [:test]\n    ```\n\n2. **Dotenv Configuration**: Create a `.env` file to store environment variables.\n    ```plaintext\n    BOX_ID=1234\n    ```\n\n3. **Config Initialization**: Set up the `config` gem by creating a `config\/settings.yml` file to define the application's configuration structure along with default values.\n    ```yaml\n    default:\n      box_id: <%= ENV.fetch(\"BOX_ID\") %>\n    ```\n\n4. **Accessing Configurations**: Use `Settings.box_id` to access the environment variable in your application code.\n    ```ruby\n    box_id = Settings.box_id\n    ```\n\n5. **Stubbing in Tests**: Use `climate_control` to stub environment variables in the test suite.\n    ```ruby\n    ClimateControl.modify BOX_ID: 'test_id' do\n      expect(Settings.box_id).to eq('test_id')\n    end\n    ```\n\nUsing this approach, we ensure our environment variables are managed centrally and are easy to stub during testing, thereby reducing complexity and potential issues with `nil` values.","GenTime":"2024-08-28 23:00:13"}
{"File Name":"community\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":3466,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","MatchedIds":[4699,4408,4722,4428,3119],"Predicted":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).","GenTime":"2024-08-28 23:00:15"}
{"File Name":"digital-paper-edit-firebase\/2019-09-28-time-estimation-for-links.md","Context":"## Context and Problem Statement\\nPeople want to include time taken for voice overs to estimate the time of the rough cut.\\n## Decision Drivers <!-- optional -->\\n* Ease of implementation\\n* Accuracy of time estimated\\n* Extensibility\\n","Decision":"* Ease of implementation\\n* Accuracy of time estimated\\n* Extensibility\\nChosen option: 1, because it's the simplest implementation and a standardised way of calculating voice over estimations within the company.\\n### Positive Consequences <!-- optional -->\\n* A fairly easy implementation.\\n### Negative consequences <!-- optional -->\\n* Less accurate than other options.\\n* Would require us to change the icon and description of the Voice over, such that it's features are not misunderstood. (This was a point raised in [UX investigations: how to better signpost Voice over\/link](https:\/\/github.com\/bbc\/digital-paper-edit-client\/issues\/63))\\n","tokens":49,"id":5250,"text":"## Context and Problem Statement\\nPeople want to include time taken for voice overs to estimate the time of the rough cut.\\n## Decision Drivers <!-- optional -->\\n* Ease of implementation\\n* Accuracy of time estimated\\n* Extensibility\\n\n\n##Decision\n* Ease of implementation\\n* Accuracy of time estimated\\n* Extensibility\\nChosen option: 1, because it's the simplest implementation and a standardised way of calculating voice over estimations within the company.\\n### Positive Consequences <!-- optional -->\\n* A fairly easy implementation.\\n### Negative consequences <!-- optional -->\\n* Less accurate than other options.\\n* Would require us to change the icon and description of the Voice over, such that it's features are not misunderstood. (This was a point raised in [UX investigations: how to better signpost Voice over\/link](https:\/\/github.com\/bbc\/digital-paper-edit-client\/issues\/63))\\n","MatchedIds":[4171,3187,4701,794,4062],"Predicted":"## Decision \n* Ease of implementation\n* Accuracy of time estimated\n* Extensibility\nChosen option: 2, because accuracy of time estimated is crucial for planning and meeting deadlines.\n\n### Positive Consequences\n* More precise rough cut time estimates will lead to better planning and resource allocation.\n* Increased reliability in time scheduling can improve client satisfaction.\n\n### Negative Consequences\n* Implementation may be more complex and require more time initially.\n* Potential need for ongoing adjustments and calibrations to maintain accuracy.","GenTime":"2024-08-28 23:00:18"}
{"File Name":"stamper\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":1244,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","MatchedIds":[4699,4408,4722,4428,3119],"Predicted":"## Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions.","GenTime":"2024-08-28 23:00:20"}
{"File Name":"rotc\/0002-use-aws-as-example-cloud-platform.md","Context":"## Context\\nThe issue motivating this decision, and any context that influences or constrains the decision.\\n","Decision":"The change that we're proposing or have agreed to implement.\\n","tokens":21,"id":3546,"text":"## Context\\nThe issue motivating this decision, and any context that influences or constrains the decision.\\n\n\n##Decision\nThe change that we're proposing or have agreed to implement.\\n","MatchedIds":[3820,1145,4616,4963,4867],"Predicted":"## Decision\nWe have decided to adopt a microservices architecture to address scalability and maintainability issues in our current monolithic system. This change will allow independent deployment and scaling of services, improving our ability to respond to changing business requirements and customer needs.","GenTime":"2024-08-28 23:00:22"}
{"File Name":"architecture-decision-log\/0007-behavior-driven-development.md","Context":"* [Context](#context)\\n* [Decision](#decision)\\n* [Status](#status)\\n* [Consequences](#consequences)\\n* [More reading](#more-reading)\\n* [Updates](#updates)\\n","Decision":"* [Status](#status)\\n* [Consequences](#consequences)\\n* [More reading](#more-reading)\\n* [Updates](#updates)\\nWe are going to use Behavior-Driven Development to help us reduce that gap. BDD is a technique where we use User Stories to create acceptance tests based on user behaviors.\\nInstead of writing complex test statements, we use a BDD tool to create a readable phrase, like the following:\\n```txt\\nGiven a new created user\\nWhen that user places a new order\\nThen a notification must be sent\\n```\\nEach line of that phrase relates to a given function that will execute steps and persist the state machine's context. We can use assertions to return errors along the way.\\nBDD is a powerful tool that enhances our TDD cycle.\\n","tokens":53,"id":4918,"text":"* [Context](#context)\\n* [Decision](#decision)\\n* [Status](#status)\\n* [Consequences](#consequences)\\n* [More reading](#more-reading)\\n* [Updates](#updates)\\n\n\n##Decision\n* [Status](#status)\\n* [Consequences](#consequences)\\n* [More reading](#more-reading)\\n* [Updates](#updates)\\nWe are going to use Behavior-Driven Development to help us reduce that gap. BDD is a technique where we use User Stories to create acceptance tests based on user behaviors.\\nInstead of writing complex test statements, we use a BDD tool to create a readable phrase, like the following:\\n```txt\\nGiven a new created user\\nWhen that user places a new order\\nThen a notification must be sent\\n```\\nEach line of that phrase relates to a given function that will execute steps and persist the state machine's context. We can use assertions to return errors along the way.\\nBDD is a powerful tool that enhances our TDD cycle.\\n","MatchedIds":[4547,4910,4544,4550,4549],"Predicted":"## Context\nOur team has been debating whether to adopt a monolithic architecture for our new project or continue with microservices. We've experienced challenges in managing dependencies and deployments with our current microservices approach. However, our products need to scale quickly, and the team wants to ensure efficient resource usage. Additionally, coordination between microservices has introduced latency that impacts user experience.\n\n## Decision\nWe've decided to transition from our current microservices architecture to a modular monolithic architecture. This approach will allow us to have a single codebase with clearly defined modules, facilitating easier management and reducing the complexity of inter-service communication.\n\n## Status\nAccepted\n\n## Consequences\n- **Positive:** \n  - Simplified deployment and CI\/CD processes since there will be a single deployable unit.\n  - Reduced latency and improved performance by eliminating network overhead between microservices.\n  - Easier management of dependencies and code consistency across the project.\n  - Accelerated development by focusing on a single codebase.\n\n- **Negative:**\n  - Increased complexity as the application grows because all modules reside in a single codebase.\n  - Potentially more challenging to scale the system horizontally, although we plan to mitigate this with a robust load balancing and caching strategy.\n  - The need for more rigorous testing to ensure that changes in one part of the code do not adversely affect other modules.\n\n## More reading\n- [When to use Microservices vs Monolith](https:\/\/martinfowler.com\/articles\/microservices.html)\n- [Modular Monolith: How to break down a monolith](https:\/\/dev.to\/volkhin\/modular-monolith-3ea1)\n- [Microservices vs Monolithic Architecture](https:\/\/microservices.io\/patterns\/monolithic.html)\n\n## Updates\n- As a next step, we will be conducting a workshop to ensure all team members are on the same page regarding the new architecture and its implementation.\n- We will incrementally refactor the current microservices into modules within the monolithic architecture, starting with less critical services.\n","GenTime":"2024-08-28 23:00:30"}
{"File Name":"buy-for-your-school\/0009-use-simplecov-to-monitor-code-test-coverage.md","Context":"## Context\\nWe want to keep our test coverage as high as possible without having to run\\nmanual checks as these take time and are easy to forget.\\n","Decision":"Use Simplecov with RSpec to monitor coverage changes on every test run\\n","tokens":32,"id":1258,"text":"## Context\\nWe want to keep our test coverage as high as possible without having to run\\nmanual checks as these take time and are easy to forget.\\n\n\n##Decision\nUse Simplecov with RSpec to monitor coverage changes on every test run\\n","MatchedIds":[4179,4359,3853,4868,1387],"Predicted":"## Decision\nUtilize a continuous integration (CI) tool such as Jenkins or GitHub Actions to automatically run tests and monitor coverage on each code commit. Integrate automated code coverage tools like Simplecov for Ruby, Jacoco for Java, or Istanbul for JavaScript to ensure consistent and comprehensive testing. This way, test coverage reports will be generated and reviewed continuously without the need for manual checks.","GenTime":"2024-08-28 23:00:32"}
{"File Name":"testy\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":1049,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","MatchedIds":[4428,3119,1436,294,2583],"Predicted":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","GenTime":"2024-08-28 23:00:33"}
{"File Name":"react-library-template\/0005-use-eslint.md","Context":"## Context\\nWe want to enforce consistency in our code, and catch as many errors\\nautomatically as we are able to. Linting the code is good practice to achieve\\nthese aims. [ESLint](https:\/\/eslint.org\/) is the standard linter for modern\\nJavaScript, and has good support for TypeScript though plugins.\\n","Decision":"We will check code style using ESLint.\\nWe will let Prettier have precedence when ESLint and Prettier conflict in their\\nstyles.\\nWe will use the recommended configuration for plugins where possible.\\nWe will run ESLint as part of the test suite.\\n","tokens":69,"id":4224,"text":"## Context\\nWe want to enforce consistency in our code, and catch as many errors\\nautomatically as we are able to. Linting the code is good practice to achieve\\nthese aims. [ESLint](https:\/\/eslint.org\/) is the standard linter for modern\\nJavaScript, and has good support for TypeScript though plugins.\\n\n\n##Decision\nWe will check code style using ESLint.\\nWe will let Prettier have precedence when ESLint and Prettier conflict in their\\nstyles.\\nWe will use the recommended configuration for plugins where possible.\\nWe will run ESLint as part of the test suite.\\n","MatchedIds":[1404,458,1408,5255,4226],"Predicted":"## Decision\nWe will check code style and catch errors using ESLint.\nWe will configure ESLint to use the recommended ESLint rules along with the recommended TypeScript plugin configuration.\nWe will resolve any conflicts between ESLint and Prettier by allowing Prettier rules to take precedence.\nWe will enforce ESLint checks as part of our continuous integration process.\nWe will include ESLint in our development workflow by running it on every commit through a Git pre-commit hook.","GenTime":"2024-08-28 23:00:36"}
{"File Name":"aspan_micro_front_ends\/0004-info-graphql-import.md","Context":"## Context\\nImport & export definitions in GraphQL SDL (also refered to as GraphQL modules)\\n","Decision":"Decision here...\\n","tokens":20,"id":3194,"text":"## Context\\nImport & export definitions in GraphQL SDL (also refered to as GraphQL modules)\\n\n\n##Decision\nDecision here...\\n","MatchedIds":[1893,899,485,2710,5259],"Predicted":"## Decision\nWe will adopt the use of the `graphql-modules` library to manage our GraphQL SDL definitions. This library provides a modular approach allowing us to encapsulate schema definitions, resolvers, and dependencies within each module. It also supports clear import and export functionalities that align with our needs for a structured and maintainable GraphQL schema architecture.","GenTime":"2024-08-28 23:00:38"}
{"File Name":"beis-report-official-development-assistance\/0002-use-bullet-to-catch-nplus1-queries.md","Context":"## Context\\nIt can be easy to miss an inefficient database query during code review. These can build up and have detremental performance on the application and effect the user experience.\\n","Decision":"Add an automatic check to the test suite to ensure (through CI) that these are fixed before being deployed.\\n","tokens":37,"id":2383,"text":"## Context\\nIt can be easy to miss an inefficient database query during code review. These can build up and have detremental performance on the application and effect the user experience.\\n\n\n##Decision\nAdd an automatic check to the test suite to ensure (through CI) that these are fixed before being deployed.\\n","MatchedIds":[1246,1031,1416,5070,1434],"Predicted":"## Decision\nImplement a database query performance monitoring tool to automatically analyze and flag inefficient queries during the CI\/CD pipeline. This will ensure that any suboptimal queries are identified and addressed before being deployed to production.","GenTime":"2024-08-28 23:00:40"}
{"File Name":"karma-sabarivka-reporter\/0001-documenting-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":4002,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","MatchedIds":[4699,4408,4722,4428,3119],"Predicted":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).","GenTime":"2024-08-28 23:00:42"}
{"File Name":"frontend-library\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":784,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","MatchedIds":[4428,3119,1436,294,2583],"Predicted":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).","GenTime":"2024-08-28 23:00:45"}
{"File Name":"unfinished-design-system\/001-components-documentation.md","Context":"* [Context](#context)\\n* [Decision](#decision)\\n* [Status](#status)\\n* [Consequences](#consequences)\\n* [More reading](#more-reading)\\n* [Updates](#updates)\\n","Decision":"* [Status](#status)\\n* [Consequences](#consequences)\\n* [More reading](#more-reading)\\n* [Updates](#updates)\\nWe've decided to use [Storybook](https:\/\/storybook.js.org\/). This tool helps us document our components and develop them since we can make a simple canvas to interact with it.\\nSome major companies with mature design systems use this tool, which could be a precious asset.\\n","tokens":53,"id":4544,"text":"* [Context](#context)\\n* [Decision](#decision)\\n* [Status](#status)\\n* [Consequences](#consequences)\\n* [More reading](#more-reading)\\n* [Updates](#updates)\\n\n\n##Decision\n* [Status](#status)\\n* [Consequences](#consequences)\\n* [More reading](#more-reading)\\n* [Updates](#updates)\\nWe've decided to use [Storybook](https:\/\/storybook.js.org\/). This tool helps us document our components and develop them since we can make a simple canvas to interact with it.\\nSome major companies with mature design systems use this tool, which could be a precious asset.\\n","MatchedIds":[4547,4910,4918,4550,4549],"Predicted":"## Context\nWe have a legacy monolithic application that has grown too large and complex, making it difficult to scale and maintain. The development team is struggling to implement new features and fix bugs efficiently due to tightly coupled components. Additionally, deployment cycles are long and risky since small changes require the redeployment of the entire application.\n\n## Decision\nWe are going to migrate from a monolithic architecture to a microservices architecture. This will involve decomposing the monolithic application into smaller, loosely coupled services that can be developed, deployed, and scaled independently.\n\n## Status\nAccepted\n\n## Consequences\n* **Positive Consequences:**\n  - **Improved Scalability:** Each microservice can be scaled independently based on its specific load and performance requirements.\n  - **Faster Development Cycles:** Teams can develop and deploy microservices independently, leading to shorter release cycles.\n  - **Resilience:** Failures in one service are less likely to impact others, enhancing overall system reliability.\n  - **Technology Flexibility:** Different microservices can use different technologies and languages suited to their specific needs.\n\n* **Negative Consequences:**\n  - **Increased Complexity:** Managing multiple microservices requires a robust orchestration framework and can introduce significant operational complexity.\n  - **Inter-Services Communication:** Engineering effort is required to handle data consistency and communication between microservices efficiently.\n  - **Initial Investment:** The initial cost of refactoring the monolith and setting up the necessary infrastructure will be significant.\n\n## More reading\n* [Building Microservices: Designing Fine-Grained Systems by Sam Newman](https:\/\/www.oreilly.com\/library\/view\/building-microservices\/9781491950340\/)\n* [Microservices Patterns: With examples in Java by Chris Richardson](https:\/\/www.manning.com\/books\/microservices-patterns)\n* [The Twelve-Factor App](https:\/\/12factor.net\/)\n\n## Updates\nThis decision will be revisited periodically to assess progress and address any emerging challenges or required adjustments. The migration plan will be executed in phases, with careful monitoring and validation of each phase to ensure system stability and performance.","GenTime":"2024-08-28 23:00:52"}
{"File Name":"libmemory\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":3590,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","MatchedIds":[4428,3119,1436,294,2583],"Predicted":"## Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions.","GenTime":"2024-08-28 23:00:54"}
{"File Name":"register-a-food-business-front-end\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":3041,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","MatchedIds":[4428,3119,1436,294,2583],"Predicted":"## Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions.","GenTime":"2024-08-28 23:00:56"}
{"File Name":"link_platform\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":5035,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","MatchedIds":[4428,3119,1436,294,2583],"Predicted":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).","GenTime":"2024-08-28 23:00:58"}
{"File Name":"tech-events-calendar\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":3753,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","MatchedIds":[4699,4408,4722,4428,3119],"Predicted":"## Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in the following article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions.","GenTime":"2024-08-28 23:01:02"}
{"File Name":"cukeness\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":3316,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","MatchedIds":[4699,4408,4722,4428,3119],"Predicted":"## Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions","GenTime":"2024-08-28 23:01:04"}
{"File Name":"molgenis-js-auth\/0001-use-adr-to-describe-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions.\\n","tokens":16,"id":4291,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions.\\n","MatchedIds":[4428,3119,1436,294,2583],"Predicted":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).","GenTime":"2024-08-28 23:01:10"}
{"File Name":"support-rota\/0003-use-dotenv-for-managing-environment-variables.md","Context":"## Context\\nAccessing ENV directly without a wrapper is limited and can introduce problems.\\nWe want our tooling to help us guard against missing environment variables. When `nil` is accidentally provided during the start up process it is preferable to fail fast with an explicit message. Without this `nil` can be passed down through the stack and cause strange behaviour where the code has been designed with it as a dependency. Instead of adding `nil` guards throughout the codebase for required environment variables (eg. `ENV.fetch('FOO', 'default')`, this should be managed centrally.\\nWe have previously used Figaro for this purpose but it was deprecated in 2016 https:\/\/github.com\/laserlemon\/figaro. We should only use supported gems to ensure we get support in the form of fixes and security patches.\\nWe also want to be able to stub our environment variables in our test suite. An easy example of this is when we use environment variables as a feature flag mechanism. We want to stub the value to test both scenarios without being influenced by real values being loaded. Mutating the actual ENV value (eg. ` allow(ENV).to receive(:[]).with('BOX_ID').and_return(\"1234\")`) is possible but may have unexpected consequences where more than 1 part of the same process under test uses the same variable. Figaro used to be a handy abstraction layer that we could stub eg. `allow(Figaro).to receive(:env).with(:foo).and_return('bar')`. We should then consider how we can stub environment variables.\\n","Decision":"Use DotEnv to load our environment variables.\\n","tokens":324,"id":3389,"text":"## Context\\nAccessing ENV directly without a wrapper is limited and can introduce problems.\\nWe want our tooling to help us guard against missing environment variables. When `nil` is accidentally provided during the start up process it is preferable to fail fast with an explicit message. Without this `nil` can be passed down through the stack and cause strange behaviour where the code has been designed with it as a dependency. Instead of adding `nil` guards throughout the codebase for required environment variables (eg. `ENV.fetch('FOO', 'default')`, this should be managed centrally.\\nWe have previously used Figaro for this purpose but it was deprecated in 2016 https:\/\/github.com\/laserlemon\/figaro. We should only use supported gems to ensure we get support in the form of fixes and security patches.\\nWe also want to be able to stub our environment variables in our test suite. An easy example of this is when we use environment variables as a feature flag mechanism. We want to stub the value to test both scenarios without being influenced by real values being loaded. Mutating the actual ENV value (eg. ` allow(ENV).to receive(:[]).with('BOX_ID').and_return(\"1234\")`) is possible but may have unexpected consequences where more than 1 part of the same process under test uses the same variable. Figaro used to be a handy abstraction layer that we could stub eg. `allow(Figaro).to receive(:env).with(:foo).and_return('bar')`. We should then consider how we can stub environment variables.\\n\n\n##Decision\nUse DotEnv to load our environment variables.\\n","MatchedIds":[1254,3388,2387,5227,4359],"Predicted":"## Decision\n\nUse the `dotenv` gem to load our environment variables in combination with a custom wrapper to ensure required variables are present and ease stubbing in tests.\n\n### Details:\n1. **Dotenv for Loading ENV Variables**:\n   - We'll use the `dotenv` gem to load environment variables from a `.env` file into `ENV`. This gem is well-supported and addresses the need for a centralized management of environment variables.\n   - Dotenv allows for easy loading of variables during development and testing but does not require changes to staging or production configurations if variables are already set.\n\n2. **Custom Wrapper for Access**:\n   - Implement a custom wrapper module, such as `EnvWrapper`, that provides methods for accessing environment variables. This wrapper will:\n     - Ensure required variables are set and raise explicit errors if they are missing.\n     - Provide default values where necessary.\n     - Allow for easy stubbing during testing.\n\n3. **Stubbing ENV in Tests**:\n   - Ensure the custom wrapper module is used in the application code for environment variable access. This allows us to stub its methods easily in the test suite.\n   - Example:\n     ```ruby\n     # lib\/env_wrapper.rb\n     module EnvWrapper\n       def self.fetch(key, default = nil)\n         value = ENV[key.to_s]\n         if value.nil? && default.nil?\n           raise \"Missing required environment variable: #{key}\"\n         end\n         value || default\n       end\n     end\n     \n     # Usage in application\n     api_key = EnvWrapper.fetch(:API_KEY)\n\n     # Stubbing in tests\n     allow(EnvWrapper).to receive(:fetch).with(:API_KEY).and_return('stubbed_value')\n     ```\n\nBy implementing this solution, we will ensure that our environment variable management is centralized, explicit, and easily testable. This approach leverages well-supported tooling while adding a thin layer of custom functionality to meet our specific requirements.","GenTime":"2024-08-28 23:01:28"}
{"File Name":"digital-paper-edit-firebase\/2019-04-29-SQL-database-schema.md","Context":"## Context and Problem Statement\\n<!-- [Describe the context and problem statement, e.g., in free form using two to three sentences. You may want to articulate the problem in form of a question.] -->\\nA schema for the SQL database in PostgreSQL for the web app, to be reused for SQLite electron desktop version.\\nIt should consider [the project Milestones](https:\/\/github.com\/bbc\/digital-paper-edit\/milestones) and be easy to version incrementally.\\nEg have 3 version of the schema for the 3 main milestones:\\n- [Feature: Transcript Editing](https:\/\/github.com\/bbc\/digital-paper-edit\/milestone\/3)\\n- [Feature: Transcript Annotations](https:\/\/github.com\/bbc\/digital-paper-edit\/milestone\/4)\\n- [Feature: Paper Editing](https:\/\/github.com\/bbc\/digital-paper-edit\/milestone\/5)\\n[DB Designer](https:\/\/dbdesigner.page.link\/cq9FMHVVxsYqTasf7) - let's you export SQL code to Create and drop tables.\\n## Decision Drivers <!-- optional -->\\n* easy to reason around\\n* \u2026 <!-- numbers of drivers can vary -->\\n","Decision":"* easy to reason around\\n* \u2026 <!-- numbers of drivers can vary -->\\n<!-- Chosen option: \"[option 1]\", because [justification. e.g., only option, which meets k.o. criterion decision driver | which resolves force force | \u2026 | comes out best (see below)]. -->\\n_TBC_\\n### Positive Consequences <!-- optional -->\\n<!-- * [e.g., improvement of quality attribute satisfaction, follow-up decisions required, \u2026] -->\\n* \u2026\\n_TBC_\\n### Negative consequences <!-- optional -->\\n<!-- * [e.g., compromising quality attribute, follow-up decisions required, \u2026] -->\\n* \u2026\\n_TBC_\\n","tokens":248,"id":5244,"text":"## Context and Problem Statement\\n<!-- [Describe the context and problem statement, e.g., in free form using two to three sentences. You may want to articulate the problem in form of a question.] -->\\nA schema for the SQL database in PostgreSQL for the web app, to be reused for SQLite electron desktop version.\\nIt should consider [the project Milestones](https:\/\/github.com\/bbc\/digital-paper-edit\/milestones) and be easy to version incrementally.\\nEg have 3 version of the schema for the 3 main milestones:\\n- [Feature: Transcript Editing](https:\/\/github.com\/bbc\/digital-paper-edit\/milestone\/3)\\n- [Feature: Transcript Annotations](https:\/\/github.com\/bbc\/digital-paper-edit\/milestone\/4)\\n- [Feature: Paper Editing](https:\/\/github.com\/bbc\/digital-paper-edit\/milestone\/5)\\n[DB Designer](https:\/\/dbdesigner.page.link\/cq9FMHVVxsYqTasf7) - let's you export SQL code to Create and drop tables.\\n## Decision Drivers <!-- optional -->\\n* easy to reason around\\n* \u2026 <!-- numbers of drivers can vary -->\\n\n\n##Decision\n* easy to reason around\\n* \u2026 <!-- numbers of drivers can vary -->\\n<!-- Chosen option: \"[option 1]\", because [justification. e.g., only option, which meets k.o. criterion decision driver | which resolves force force | \u2026 | comes out best (see below)]. -->\\n_TBC_\\n### Positive Consequences <!-- optional -->\\n<!-- * [e.g., improvement of quality attribute satisfaction, follow-up decisions required, \u2026] -->\\n* \u2026\\n_TBC_\\n### Negative consequences <!-- optional -->\\n<!-- * [e.g., compromising quality attribute, follow-up decisions required, \u2026] -->\\n* \u2026\\n_TBC_\\n","MatchedIds":[4167,1661,160,3053,4285],"Predicted":"## Decision\nChosen option: \"Use a versioned schema with migration scripts\", because\n\n- It allows incremental changes to be tracked and managed easily.\n- Migration scripts can be reused for both PostgreSQL and SQLite, ensuring consistency across the web and desktop versions.\n- Allows for easier rollback and debugging if issues arise in specific versions.\n- Facilitates alignment with project milestones.\n\n### Positive Consequences\n* Easier to manage and track changes in the schema.\n* Consistency between web (PostgreSQL) and desktop (SQLite) versions.\n* Supports incremental updates and feature releases.\n* Simplifies rollback and version management.\n\n### Negative Consequences\n* Requires additional tooling and management for migration scripts.\n* Developers need to ensure compatibility between PostgreSQL and SQLite SQL dialects.\n","GenTime":"2024-08-28 23:01:32"}
{"File Name":"react-library-template\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as\\n[described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":4229,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as\\n[described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","MatchedIds":[4699,4408,4722,4428,3119],"Predicted":"## Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions","GenTime":"2024-08-28 23:01:34"}
{"File Name":"adr-poc\/0003-write-help-file.md","Context":"## Context\\nThe issue motivating this decision, and any context that influences or constrains the decision.\\n","Decision":"The change that we're proposing or have agreed to implement.\\n","tokens":21,"id":4963,"text":"## Context\\nThe issue motivating this decision, and any context that influences or constrains the decision.\\n\n\n##Decision\nThe change that we're proposing or have agreed to implement.\\n","MatchedIds":[3820,1145,4616,3546,4867],"Predicted":"## Decision\nWe have decided to adopt a microservices architecture to address scalability and maintainability concerns. This architecture will allow us to develop, deploy, and scale each service independently, which aligns with our growing user base and feature set. Additionally, this approach will simplify ongoing maintenance by enabling isolated updates and fault isolation.","GenTime":"2024-08-28 23:01:35"}
{"File Name":"GDD-app\/0002-use-preferences-for-data-storage.md","Context":"## Context\\nThe issue motivating this decision, and any context that influences or constrains the decision.\\n","Decision":"The change that we're proposing or have agreed to implement.\\n","tokens":21,"id":4867,"text":"## Context\\nThe issue motivating this decision, and any context that influences or constrains the decision.\\n\n\n##Decision\nThe change that we're proposing or have agreed to implement.\\n","MatchedIds":[3820,1145,4616,3546,4963],"Predicted":"## Decision\nWe have decided to migrate our application to a microservices architecture to enhance scalability and maintainability. This change is motivated by current limitations with our monolithic structure, such as difficulty in scaling individual components and slower deployment cycles. The adoption of microservices will allow teams to deploy changes independently, improve fault isolation, and facilitate the integration of new technologies.","GenTime":"2024-08-28 23:01:38"}
{"File Name":"support-rota\/0004-add-rollbar-for-application-monitoring.md","Context":"## Context\\nWe need a mechanism to capture and manage application errors. Without an integration our debugging options are to access a console on live environments and try to replicate (something we want to minimise) or by looking through information provided by logs.\\nWe have used Rollbar for a few years now and we have not reviewed this decision since. It is currently being used for 14 applications.\\nFor some projects we use their technical tooling of choice to aid in the transition to business as usual. Due to this we will have a handful of projects using Sentry and possible others.\\nSometimes Rollbar environment names don't match the Rails environment. Dalmatian-<project> and paas-<project> both exist. There also exists both permutations for the same project as we transition. We have used ROLLBAR_ENV to manage this before so making it explicit will hopefully make it clearer how it can be changed.\\n","Decision":"Use Rollbar to collect and manage our application errors.\\n","tokens":187,"id":3388,"text":"## Context\\nWe need a mechanism to capture and manage application errors. Without an integration our debugging options are to access a console on live environments and try to replicate (something we want to minimise) or by looking through information provided by logs.\\nWe have used Rollbar for a few years now and we have not reviewed this decision since. It is currently being used for 14 applications.\\nFor some projects we use their technical tooling of choice to aid in the transition to business as usual. Due to this we will have a handful of projects using Sentry and possible others.\\nSometimes Rollbar environment names don't match the Rails environment. Dalmatian-<project> and paas-<project> both exist. There also exists both permutations for the same project as we transition. We have used ROLLBAR_ENV to manage this before so making it explicit will hopefully make it clearer how it can be changed.\\n\n\n##Decision\nUse Rollbar to collect and manage our application errors.\\n","MatchedIds":[2387,3370,1254,3389,1523],"Predicted":"## Decision\nWe will continue using Rollbar to collect and manage our application errors. However, we will standardize the use of the `ROLLBAR_ENV` environment variable to ensure that the environment names align directly with the Rails environments. This will help streamline error tracking across different environments and projects. Furthermore, for any new projects or transitions where tooling of choice needs to be flexible, Sentry can be employed as an alternative to avoid dependency bottlenecks.","GenTime":"2024-08-28 23:01:40"}
{"File Name":"TruBudget\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":1226,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","MatchedIds":[4699,4408,4722,4428,3119],"Predicted":"## Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions","GenTime":"2024-08-28 23:01:42"}
{"File Name":"commcare-cloud\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":5146,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","MatchedIds":[4699,4408,4722,4428,3119],"Predicted":"## Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions.","GenTime":"2024-08-28 23:01:43"}
{"File Name":"gp-finder\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":3475,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","MatchedIds":[4428,3119,1436,294,2583],"Predicted":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).","GenTime":"2024-08-28 23:01:45"}
{"File Name":"identity-site\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":5170,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","MatchedIds":[4428,3119,1436,294,2583],"Predicted":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).","GenTime":"2024-08-28 23:01:48"}
{"File Name":"origin\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":1562,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","MatchedIds":[4699,4408,4722,4428,3119],"Predicted":"## Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions.","GenTime":"2024-08-28 23:01:50"}
{"File Name":"push-sdk-ios\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in\\n[this article.][1]\\n","tokens":16,"id":4196,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in\\n[this article.][1]\\n","MatchedIds":[4699,4408,4722,4428,3119],"Predicted":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).","GenTime":"2024-08-28 23:01:52"}
{"File Name":"digitalrig-metal-aws\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":1744,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","MatchedIds":[4699,4408,4722,4428,3119],"Predicted":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).","GenTime":"2024-08-28 23:01:54"}
{"File Name":"corona-hackathon\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":3663,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","MatchedIds":[4699,4408,4722,4428,3119],"Predicted":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","GenTime":"2024-08-28 23:01:55"}
{"File Name":"dotfiles\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: [http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions)\\n","tokens":16,"id":117,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: [http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions)\\n","MatchedIds":[4428,3119,1436,294,2583],"Predicted":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).","GenTime":"2024-08-28 23:01:57"}
{"File Name":"lockfix\/0001-documenting-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":611,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","MatchedIds":[4699,4408,4722,4428,3119],"Predicted":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).","GenTime":"2024-08-28 23:01:59"}
{"File Name":"html-diff-service\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":3250,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","MatchedIds":[4699,4408,4722,4428,3119],"Predicted":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).","GenTime":"2024-08-28 23:02:00"}
{"File Name":"digitalrig-metal-aws\/0007-build-and-deploy-pipelines-separate.md","Context":"## Context\\nIn the previous iteration, build and deploy were consolodated into one pipeline. The result of this being that any type of configuration change required a full rebuild to deploy.  This could become unwieldy with projects that have a long-running build step.\\n","Decision":"Pipeline has been split into distinct build and deploy pipelines.  The build pipeline, in addition to the image that it uploads to ECR, exports artifacts build.json and src.zip.  Src.zip is required still required in the deploy pipeline to run integration tests.  In the deploy pipeline, either the artifacts supplied by the build pipeline OR new app.yaml templates will trigger the pipeline.  Consequently, a config change may be made by uploading a new app.yaml, without having to re-build the Docker image.\\n","tokens":56,"id":1739,"text":"## Context\\nIn the previous iteration, build and deploy were consolodated into one pipeline. The result of this being that any type of configuration change required a full rebuild to deploy.  This could become unwieldy with projects that have a long-running build step.\\n\n\n##Decision\nPipeline has been split into distinct build and deploy pipelines.  The build pipeline, in addition to the image that it uploads to ECR, exports artifacts build.json and src.zip.  Src.zip is required still required in the deploy pipeline to run integration tests.  In the deploy pipeline, either the artifacts supplied by the build pipeline OR new app.yaml templates will trigger the pipeline.  Consequently, a config change may be made by uploading a new app.yaml, without having to re-build the Docker image.\\n","MatchedIds":[351,187,3722,1741,5102],"Predicted":"## Decision\nThe pipeline will be split into two distinct stages: a build pipeline and a deploy pipeline. The build pipeline will produce and upload Docker images to a container registry and will also export build artifacts such as build.json and application.zip. The deploy pipeline will be triggered by either new artifacts from the build pipeline or changes in deployment configuration files such as deployment.yaml. This setup allows for deployment configuration changes to be applied without necessitating a full rebuild, thus speeding up the deployment process for long-running builds and reducing resource usage. Both pipelines will be integrated with a notification system to alert relevant teams upon completion or failure of each stage.","GenTime":"2024-08-28 23:02:04"}
{"File Name":"report-a-defect\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":5220,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","MatchedIds":[4699,4408,4722,4428,3119],"Predicted":"## Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions","GenTime":"2024-08-28 23:02:06"}
{"File Name":"datalab\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":774,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","MatchedIds":[4428,3119,1436,294,2583],"Predicted":"## Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions","GenTime":"2024-08-28 23:02:07"}
{"File Name":"pul_solr\/0001-document-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":3131,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","MatchedIds":[4699,4408,4722,4428,3119],"Predicted":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","GenTime":"2024-08-28 23:02:09"}
{"File Name":"plant-shop\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":3349,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","MatchedIds":[4428,3119,1436,294,2583],"Predicted":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).","GenTime":"2024-08-28 23:02:11"}
{"File Name":"island.is-glosur\/0009-naming-files-and-directories.md","Context":"## Context and Problem Statement\\nAs of the date of this writing, there are multiple different naming styles used in the monorepo, mostly because NX has\\ndefaults that differ between schematic types.\\nIn order for navigating the monorepo in a consistent rational manner, we should align on naming strategy for files\\nand directories.\\n## Decision Drivers\\n* Provide consistency when navigating the codebase\\n* The earlier we decide on this, the better\\n","Decision":"* Provide consistency when navigating the codebase\\n* The earlier we decide on this, the better\\nChosen option: Name files after their default export. If that default export is a React Component, or a class, then the\\nfile name should be in PascalCase. Otherwise, the filename should be in camelCase. Basically, for naming files avoid\\nusing kebab-case and snake_case and make sure the name follows the default export of the file.\\nNaming directories should follow these guidelines:\\nOnly use kebab-case when naming NX apps and libraries, or folders containing apps and libraries, e.g. `island-ui` instead of `islandUi`:\\n`import { Box } from '@island.is\/island-ui\/core'`\\nUse PascalCase for directories only containing React components:\\n````\\ncomponents\/CtaButton\/index.ts\\nimport 'components\/CtaButton'\\n````\\nor:\\n````\\ncomponents\/CtaButton\/CtaButton.tsx\\nimport 'components\/CtaButton\/CtaButton'\\n````\\nrather than\\n````\\ncomponents\/cta-button\/CtaButton.tsx\\n````\\nIn all other cases, use camelCase.\\n### Positive Consequences\\n* Easier to navigate the codebase\\n* File names are more readable, and developers know what to expect\\n* This approach is the most common practice, and something most JS and TS developers are familiar with.\\n","tokens":93,"id":4766,"text":"## Context and Problem Statement\\nAs of the date of this writing, there are multiple different naming styles used in the monorepo, mostly because NX has\\ndefaults that differ between schematic types.\\nIn order for navigating the monorepo in a consistent rational manner, we should align on naming strategy for files\\nand directories.\\n## Decision Drivers\\n* Provide consistency when navigating the codebase\\n* The earlier we decide on this, the better\\n\n\n##Decision\n* Provide consistency when navigating the codebase\\n* The earlier we decide on this, the better\\nChosen option: Name files after their default export. If that default export is a React Component, or a class, then the\\nfile name should be in PascalCase. Otherwise, the filename should be in camelCase. Basically, for naming files avoid\\nusing kebab-case and snake_case and make sure the name follows the default export of the file.\\nNaming directories should follow these guidelines:\\nOnly use kebab-case when naming NX apps and libraries, or folders containing apps and libraries, e.g. `island-ui` instead of `islandUi`:\\n`import { Box } from '@island.is\/island-ui\/core'`\\nUse PascalCase for directories only containing React components:\\n````\\ncomponents\/CtaButton\/index.ts\\nimport 'components\/CtaButton'\\n````\\nor:\\n````\\ncomponents\/CtaButton\/CtaButton.tsx\\nimport 'components\/CtaButton\/CtaButton'\\n````\\nrather than\\n````\\ncomponents\/cta-button\/CtaButton.tsx\\n````\\nIn all other cases, use camelCase.\\n### Positive Consequences\\n* Easier to navigate the codebase\\n* File names are more readable, and developers know what to expect\\n* This approach is the most common practice, and something most JS and TS developers are familiar with.\\n","MatchedIds":[1996,2703,36,1117,3521],"Predicted":"## Decision\nChosen option: Align naming strategy based on file contents and their usage, as it will provide a clear, intuitive structure for the codebase.\n\n### File Naming Strategy:\n* **Default Exports:**\n  - Default export of a React Component or class: use PascalCase (e.g., `MyComponent.tsx`).\n  - Other default exports: use camelCase (e.g., `utilityFunction.ts`).\n\n* **Non-Default Exports:**\n  - Use camelCase (e.g., `helperFunctions.ts`).\n\n* **Test Files:**\n  - Match the name of the file being tested followed by `.test` (e.g., `MyComponent.test.tsx`).\n\n* **Configuration and Environment Files:**\n  - Use all lowercase with hyphens (e.g., `webpack-config.js`, `env-config.js`).\n\n### Directory Naming Strategy:\n* **Application and Library Directories:**\n  - Use kebab-case for consistency and readability (e.g., `user-management`, `shared-components`).\n\n* **Component Directories:**\n  - For directories containing only React components, use PascalCase (e.g., `CtaButton`).\n\n* **Other Directories:**\n  - Use camelCase for all other directory types (e.g., `utils`, `services`).\n\n### Examples:\n```plaintext\napps\/\n  user-management\/\n    src\/\n      components\/\n        Header.tsx\n        Header.test.tsx\n      services\/\n        apiService.ts\n        apiService.test.ts\n      \nlibs\/\n  shared-components\/\n    src\/\n      CtaButton\/\n        index.ts\n        CtaButton.tsx\n        CtaButton.test.tsx\n      \n  utils\/\n    formatDate.js\n    formatDate.test.js\n```\n\n### Positive Consequences\n* Easier navigation in the codebase due to a clear and consistent naming strategy.\n* Improved readability and understanding of file purpose and contents.\n* Minimization of conflicts and confusion among team members.\n\n### Negative Consequences\n* Initial time investment is required to refactor existing files and directories to align with the new strategy.\n* Potential need for tool or script to assist in renaming existing files and directories without manual effort.\n\n### Implementation Plan\n1. Communicate the chosen naming conventions to all team members.\n2. Write and integrate automated scripts to assist in renaming existing files and directories.\n3. Update any relevant documentation to reflect the new naming conventions.\n4. Integrate checks in the CI pipeline to enforce the new conventions for new code and modifications.\n\nBy aligning our naming strategy early, we can guarantee a more navigable, rational, and consistent codebase moving forward.","GenTime":"2024-08-28 23:02:17"}
{"File Name":"buildit-all\/0006-use-okta-as-identity-provider.md","Context":"## Context\\nOkta is an Identity as a Service provider (similar to Auth0) that can provide authentication and authorization support to an application.  They have a forever free developer account that allows up to 7000 active users\/mo (well below our anticipated usage).\\nOkta provides the ability to authenticate against a variety of providers (okta, social (facebook, github, microsoft azure, etc), SAML, AD\/LDAP).  We would use the Microsoft social OpenConnect ID provider.  It will map the Microsoft user into an Okta user, essentially acting as a proxy or facade to Microsoft while providing a more uniform and standards adhering API.\\nThey have integrations into Spring Boot 1.x (via okta spring security starter) and Spring Security 5.x (via Spring Boot 2.x) supports Okta out of the box.\\nThey have a React client SDK instead of using our handrolled client code for Microsoft Azure AD.\\nThe real value add comes when we need to add roles\/groups (admin level users).  You get admin screens\/dashboards \"for free\" to add\/remove users etc.\\nLater on, we could add additional social logins (or support AD directly if needed).\\nSpike can be found at: https:\/\/github.com\/buildit\/bookit-api\/tree\/okta-spring-boot-1.x\\n","Decision":"* Delay for now - because we think we still need a local user table, we can add this in later\\n* Use Okta as IdP for Bookit\\n* Use Okta's React client for client side code\\n* Use Okta's Spring Boot 1.x starter for\\n* Use Okta Access Tokens as Bearer tokens\\n","tokens":279,"id":4414,"text":"## Context\\nOkta is an Identity as a Service provider (similar to Auth0) that can provide authentication and authorization support to an application.  They have a forever free developer account that allows up to 7000 active users\/mo (well below our anticipated usage).\\nOkta provides the ability to authenticate against a variety of providers (okta, social (facebook, github, microsoft azure, etc), SAML, AD\/LDAP).  We would use the Microsoft social OpenConnect ID provider.  It will map the Microsoft user into an Okta user, essentially acting as a proxy or facade to Microsoft while providing a more uniform and standards adhering API.\\nThey have integrations into Spring Boot 1.x (via okta spring security starter) and Spring Security 5.x (via Spring Boot 2.x) supports Okta out of the box.\\nThey have a React client SDK instead of using our handrolled client code for Microsoft Azure AD.\\nThe real value add comes when we need to add roles\/groups (admin level users).  You get admin screens\/dashboards \"for free\" to add\/remove users etc.\\nLater on, we could add additional social logins (or support AD directly if needed).\\nSpike can be found at: https:\/\/github.com\/buildit\/bookit-api\/tree\/okta-spring-boot-1.x\\n\n\n##Decision\n* Delay for now - because we think we still need a local user table, we can add this in later\\n* Use Okta as IdP for Bookit\\n* Use Okta's React client for client side code\\n* Use Okta's Spring Boot 1.x starter for\\n* Use Okta Access Tokens as Bearer tokens\\n","MatchedIds":[3217,5211,4422,3421,2582],"Predicted":"## Decision\n* Use Okta as the Identity Provider (IdP) for our application.\n* Use Okta's React client SDK for handling authentication on the client side, replacing our current hand-rolled client code for Microsoft Azure AD.\n* Integrate Okta with Spring Boot using Okta's Spring Security starter for Spring Boot 1.x and ensure compatibility with Spring Security 5.x.\n* Utilize Okta's admin dashboards for user management, including adding and removing users and configuring roles\/groups for admin-level users.\n* Plan to potentially add additional social logins or direct AD support in the future as requirements evolve.\n* Delay the implementation until we confirm the necessity of a local user table. However, proceed with setup and integration preparation.","GenTime":"2024-08-28 23:02:20"}
{"File Name":"gatemint-sdk\/adr-006-secret-store-replacement.md","Context":"## Context\\nCurrently, an SDK application's CLI directory stores key material and metadata in a plain text database in the user\u2019s home directory.  Key material is encrypted by a passphrase, protected by bcrypt hashing algorithm. Metadata (e.g. addresses, public keys, key storage details) is available in plain text.\\nThis is not desirable for a number of reasons. Perhaps the biggest reason is insufficient security protection of key material and metadata. Leaking the plain text allows an attacker to surveil what keys a given computer controls via a number of techniques, like compromised dependencies without any privilege execution. This could be followed by a more targeted attack on a particular user\/computer.\\nAll modern desktop computers OS (Ubuntu, Debian, MacOS, Windows) provide a built-in secret store that is designed to allow applications to store information that is isolated from all other applications and requires passphrase entry to access the data.\\nWe are seeking solution that provides a common abstraction layer to the many different backends and reasonable fallback for minimal platforms that don\u2019t provide a native secret store.\\n","Decision":"We recommend replacing the current Keybase backend based on LevelDB with [Keyring](https:\/\/github.com\/99designs\/keyring) by 99 designs. This application is designed to provide a common abstraction and uniform interface between many secret stores and is used by AWS Vault application by 99-designs application.\\nThis appears to fulfill the requirement of protecting both key material and metadata from rouge software on a user\u2019s machine.\\n","tokens":214,"id":13,"text":"## Context\\nCurrently, an SDK application's CLI directory stores key material and metadata in a plain text database in the user\u2019s home directory.  Key material is encrypted by a passphrase, protected by bcrypt hashing algorithm. Metadata (e.g. addresses, public keys, key storage details) is available in plain text.\\nThis is not desirable for a number of reasons. Perhaps the biggest reason is insufficient security protection of key material and metadata. Leaking the plain text allows an attacker to surveil what keys a given computer controls via a number of techniques, like compromised dependencies without any privilege execution. This could be followed by a more targeted attack on a particular user\/computer.\\nAll modern desktop computers OS (Ubuntu, Debian, MacOS, Windows) provide a built-in secret store that is designed to allow applications to store information that is isolated from all other applications and requires passphrase entry to access the data.\\nWe are seeking solution that provides a common abstraction layer to the many different backends and reasonable fallback for minimal platforms that don\u2019t provide a native secret store.\\n\n\n##Decision\nWe recommend replacing the current Keybase backend based on LevelDB with [Keyring](https:\/\/github.com\/99designs\/keyring) by 99 designs. This application is designed to provide a common abstraction and uniform interface between many secret stores and is used by AWS Vault application by 99-designs application.\\nThis appears to fulfill the requirement of protecting both key material and metadata from rouge software on a user\u2019s machine.\\n","MatchedIds":[3142,4346,947,2256,4859],"Predicted":"## Decision\nWe have decided to implement a common abstraction layer for secret storage that leverages the native secret stores provided by modern desktop operating systems (Ubuntu, Debian, macOS, Windows). This approach aims to improve the security of key material and metadata by isolating them from other applications and requiring passphrase entry for access.\n\n### Implementation Details\n\n1. **Common Abstraction Layer**:\n   - Develop a unified API that interacts with the native secret stores of different operating systems.\n   - This API will include operations for storing, retrieving, and deleting secrets in a secure manner.\n\n2. **Native Secret Store Integration**:\n   - **Ubuntu\/Debian**: Use GNOME Keyring or KWallet for storing secrets.\n   - **macOS**: Use the Keychain Services for managing secrets.\n   - **Windows**: Use the Windows Credential Manager for secret storage.\n\n3. **Fallback Mechanism**:\n   - For platforms that do not provide a native secret store, implement an encrypted local storage solution using a robust encryption algorithm (e.g., AES-256).\n   - This fallback will ensure minimal security standards are maintained even on minimal platforms.\n\n4. **Passphrase Protection**:\n   - Ensure that the access to secrets in the native stores and fallback storage is protected by a user-defined passphrase.\n   - Implement secure methods to handle passphrase entry and verification without exposing it to potential vulnerabilities.\n\n5. **Migration**:\n   - Provide a migration tool to transfer existing key material and metadata from the plain text database to the new secure storage abstraction.\n\n### Benefits\n- **Enhanced Security**: By leveraging native and encrypted storage facilities, the risk of key material and metadata leaks is significantly reduced.\n- **Platform Independence**: A common abstraction layer ensures the solution works uniformly across different operating systems.\n- **User Convenience**: Users will benefit from the security without needing to manage encryption details themselves.\n\n### Consequences\n- **Development Effort**: Implementing and maintaining the abstraction layer will require additional development resources.\n- **Performance**: Interaction with native secret stores may have some performance overhead, but this is offset by the increased security.\n- **Compatibility**: Ensuring compatibility with all supported operating systems may present challenges, particularly with less common or older versions.\n\nBy implementing this approach, we aim to substantially improve the security of the SDK application's CLI directory, protecting users from potential surveillance and targeted attacks.","GenTime":"2024-08-28 23:02:26"}
{"File Name":"unfinished-design-system\/004-tokenization-and-static-assets.md","Context":"* [Context](#context)\\n* [Decision](#decision)\\n* [Status](#status)\\n* [Consequences](#consequences)\\n* [More reading](#more-reading)\\n* [Updates](#updates)\\n","Decision":"* [Status](#status)\\n* [Consequences](#consequences)\\n* [More reading](#more-reading)\\n* [Updates](#updates)\\nWe've decided to use [Style Dictionary](https:\/\/amzn.github.io\/style-dictionary\/#\/) as our framework to handle tokens. It is easy to use since we can define our tickets in standard JSON but empowering them with string interpolation, variables, and other features.\\nFor our static assets, we're going to host them at [AWS S3](https:\/\/aws.amazon.com\/s3\/), defining the proper CORS rules, and refer the location of those as tokens for our applications to use.\\nInstead of increasing the size of our Javascript bundle with static assets, we prefer to keep it simple and light by hosting those in an S3 bucket and asking for the application to download it.\\n","tokens":53,"id":4550,"text":"* [Context](#context)\\n* [Decision](#decision)\\n* [Status](#status)\\n* [Consequences](#consequences)\\n* [More reading](#more-reading)\\n* [Updates](#updates)\\n\n\n##Decision\n* [Status](#status)\\n* [Consequences](#consequences)\\n* [More reading](#more-reading)\\n* [Updates](#updates)\\nWe've decided to use [Style Dictionary](https:\/\/amzn.github.io\/style-dictionary\/#\/) as our framework to handle tokens. It is easy to use since we can define our tickets in standard JSON but empowering them with string interpolation, variables, and other features.\\nFor our static assets, we're going to host them at [AWS S3](https:\/\/aws.amazon.com\/s3\/), defining the proper CORS rules, and refer the location of those as tokens for our applications to use.\\nInstead of increasing the size of our Javascript bundle with static assets, we prefer to keep it simple and light by hosting those in an S3 bucket and asking for the application to download it.\\n","MatchedIds":[4547,4910,4918,4544,4549],"Predicted":"* [Context](#context)\\n* [Decision](#decision)\\n* [Status](#status)\\n* [Consequences](#consequences)\\n* [More reading](#more-reading)\\n* [Updates](#updates)\\n\n\n## Decision\n\nWe have decided to use [GraphQL](https:\/\/graphql.org\/) for our API layer instead of REST. GraphQL provides a more flexible and efficient way to query data from our backend services, allowing clients to request only the data they need.\n\n* [Status](#status)\\n\nAdopted\n\n* [Consequences](#consequences)\\n\n  * **Improved Client Performance:** Clients can request only the data they need, which reduces over-fetching and under-fetching issues inherent in REST.\n  * **Simplified Data Retrieval:** Allows for more complex queries and the ability to fetch related data in a single request.\n  * **Stronger Type System:** Provides a strongly-typed schema which improves the ability to catch errors early during development.\n  * **Increased Complexity:** Moving to GraphQL will add complexity to our backend infrastructure and may require additional training for our development team.\n\n* [More reading](#more-reading)\\n\n  * [GraphQL Official Documentation](https:\/\/graphql.org\/learn)\n  * [Why Choose GraphQL Over REST](https:\/\/www.howtographql.com\/basics\/1-graphql-is-the-better-rest\/)\n\n* [Updates](#updates)\\n\n  * Initial implementation will target key microservices and gradually migrate the entire API layer.\n  * Conduct training sessions for the development team on GraphQL best practices and usage.","GenTime":"2024-08-28 23:02:32"}
{"File Name":"tendermint\/adr-051-double-signing-risk-reduction.md","Context":"## Context\\nTo provide a risk reduction method for double signing incidents mistakenly executed by validators\\n- Validators often mistakenly run duplicated validators to cause double-signing incident\\n- This proposed feature is to reduce the risk of mistaken double-signing incident by checking recent N blocks before voting begins\\n- When we think of such serious impact on double-signing incident, it is very reasonable to have multiple risk reduction algorithm built in node daemon\\n","Decision":"We would like to suggest a double signing risk reduction method.\\n- Methodology : query recent consensus results to find out whether node's consensus key is used on consensus recently or not\\n- When to check\\n- When the state machine starts `ConsensusReactor` after fully synced\\n- When the node is validator ( with privValidator )\\n- When `cs.config.DoubleSignCheckHeight > 0`\\n- How to check\\n1. When a validator is transformed from syncing status to fully synced status, the state machine check recent N blocks (`latest_height - double_sign_check_height`) to find out whether there exists consensus votes using the validator's consensus key\\n2. If there exists votes from the validator's consensus key, exit state machine program\\n- Configuration\\n- We would like to suggest by introducing `double_sign_check_height` parameter in `config.toml` and cli, how many blocks state machine looks back to check votes\\n- <span v-pre>`double_sign_check_height = {{ .Consensus.DoubleSignCheckHeight }}`<\/span> in `config.toml`\\n- `tendermint node --consensus.double_sign_check_height` in cli\\n- State machine ignore checking procedure when `double_sign_check_height == 0`\\n","tokens":85,"id":1944,"text":"## Context\\nTo provide a risk reduction method for double signing incidents mistakenly executed by validators\\n- Validators often mistakenly run duplicated validators to cause double-signing incident\\n- This proposed feature is to reduce the risk of mistaken double-signing incident by checking recent N blocks before voting begins\\n- When we think of such serious impact on double-signing incident, it is very reasonable to have multiple risk reduction algorithm built in node daemon\\n\n\n##Decision\nWe would like to suggest a double signing risk reduction method.\\n- Methodology : query recent consensus results to find out whether node's consensus key is used on consensus recently or not\\n- When to check\\n- When the state machine starts `ConsensusReactor` after fully synced\\n- When the node is validator ( with privValidator )\\n- When `cs.config.DoubleSignCheckHeight > 0`\\n- How to check\\n1. When a validator is transformed from syncing status to fully synced status, the state machine check recent N blocks (`latest_height - double_sign_check_height`) to find out whether there exists consensus votes using the validator's consensus key\\n2. If there exists votes from the validator's consensus key, exit state machine program\\n- Configuration\\n- We would like to suggest by introducing `double_sign_check_height` parameter in `config.toml` and cli, how many blocks state machine looks back to check votes\\n- <span v-pre>`double_sign_check_height = {{ .Consensus.DoubleSignCheckHeight }}`<\/span> in `config.toml`\\n- `tendermint node --consensus.double_sign_check_height` in cli\\n- State machine ignore checking procedure when `double_sign_check_height == 0`\\n","MatchedIds":[28,4101,4113,1949,841],"Predicted":"## Decision\n### Design\nTo address the risk of double-signing incidents caused by validators mistakenly running duplicate instances, we will implement a pre-vote validation mechanism that checks recent blocks before the voting process starts. This risk reduction method will enhance security and ensure validators do not inadvertently double-sign. \n\n### Mechanism\n1. **Recent Block Check**: Before a validator signs a new block, the validator node will check the signatures of the last `N` blocks to determine if it has signed any conflicting blocks. This involves verifying that no double-signings (i.e., two different blocks at the same height) have occurred within the recent `N` blocks.\n\n2. **Block Signature Record**: Maintain a record of recent block signatures that each validator has signed. This information can be stored in a lightweight, efficient data structure such as a circular buffer.\n\n3. **Signature Verification**: Before a validator submits its PRECOMMIT or PREVOTE, it will verify its recent block signature history to ensure there are no conflicting signatures present.\n\n### Implementation\n1. **Data Structures**:\n    ```go\n    type SignatureRecord struct {\n        Height int64\n        BlockID Hash\n    }\n    \n    type Validator struct {\n        Address       string\n        SignatureLogs CircularBuffer[SignatureRecord]\n    }\n    ```\n\n2. **Validation Logic**:\n    ```go\n    func (v *Validator) CheckRecentBlocksForDoubleSign(blockID Hash, height int64) error {\n        for _, record := range v.SignatureLogs.Items() {\n            if record.Height == height && record.BlockID != blockID {\n                return errors.New(\"Double-signing detected for the same height\")\n            }\n        }\n        return nil\n    }\n    ```\n\n3. **Integration with Voting Process**:\n    ```go\n    func (v *Validator) ValidateAndVote(blockID Hash, height int64) error {\n        err := v.CheckRecentBlocksForDoubleSign(blockID, height)\n        if err != nil {\n            return err \/\/ Stop the validator from voting to prevent double-signing\n        }\n        \n        \/\/ Proceed with voting process\n        v.SignatureLogs.Add(SignatureRecord{Height: height, BlockID: blockID})\n        \/\/ Logic to proceed with PREVOTE or PRECOMMIT\n    }\n    ```\n4. **Handling Signature Records**:\n    - The circular buffer will maintain a fixed number of recent block signatures to verify. The size `N` of the buffer is configurable through on-chain parameters.\n    - Any detected double-sign attempts will trigger an alert and prevent the submission of potentially conflicting votes.\n\n### Governance Parameters\n- **BufferSize (N)**: Maximum number of recent blocks to check for validation.\n- **PenaltyMechanism**: Define actions for detected double-sign attempts, like alerting the validator operator and preventing the vote from being broadcasted.\n\n### Considerations\n- **Performance Impact**: Verifying recent block signatures should be computationally efficient to prevent degrading the performance of the consensus process.\n- **Configurability**: Allow the governance layer to adjust the size of the signature log buffer and other relevant parameters to adapt to changing network conditions.\n- **Enhancements**: Future improvements could include adaptive algorithms that dynamically adjust the risk-check parameters based on observed validator behavior patterns.\n\n### Summary\nThis proactive validation mechanism will mitigate the risk of mistaken double-signing incidents, adding a layer of security and robustness to the validator operations within the network. The implementation is designed to be minimal impact yet highly effective, offering a practical safeguard without significantly affecting the node's performance.","GenTime":"2024-08-28 23:02:44"}
{"File Name":"monitoring-doc\/0003-use-ecs-for-initial-beta-buildout.md","Context":"## Context\\nExisting self-hosted infrastructure at GDS has been managed in code\\nusing tools like puppet, but in a somewhat ad hoc way with each team\\ndoing things differently, little sharing of code, and much reinvention\\nof wheels.  We would like to learn about other ways of deploying\\ninfrastructure which encourage consistency: in terms of code\\nartifacts, configuration methods, and such like.\\nSystems such as Kubernetes and Amazon ECS are coalescing around Docker\\nas a standard for packaging software and managing configuration.\\n","Decision":"We will build our initial prometheus beta in Amazon ECS, and assess\\nhow effective it is.  We will review this decision once we have learnt\\nmore about both prometheus and ECS.\\n","tokens":110,"id":1376,"text":"## Context\\nExisting self-hosted infrastructure at GDS has been managed in code\\nusing tools like puppet, but in a somewhat ad hoc way with each team\\ndoing things differently, little sharing of code, and much reinvention\\nof wheels.  We would like to learn about other ways of deploying\\ninfrastructure which encourage consistency: in terms of code\\nartifacts, configuration methods, and such like.\\nSystems such as Kubernetes and Amazon ECS are coalescing around Docker\\nas a standard for packaging software and managing configuration.\\n\n\n##Decision\nWe will build our initial prometheus beta in Amazon ECS, and assess\\nhow effective it is.  We will review this decision once we have learnt\\nmore about both prometheus and ECS.\\n","MatchedIds":[856,3920,4041,2062,3917],"Predicted":"## Decision\nWe will standardize on using Kubernetes for orchestrating our containerized applications and Docker for packaging our software. By adopting Kubernetes, we can benefit from a well-established ecosystem that facilitates consistency in managing configurations, deployments, and scaling. Docker will serve as our standard format for container images, ensuring we have a uniform approach to packaging and distributing software.\n\nTo ensure consistency in our infrastructure management, we will integrate Infrastructure as Code (IaC) tools such as Terraform for creating and managing our Kubernetes clusters and related resources. Additionally, we will adopt Helm for managing Kubernetes application charts, which will help in defining, installing, and upgrading complex Kubernetes applications consistently.\n\nBy moving towards Kubernetes and Docker, coupled with IaC and Helm, we will reduce duplicated efforts across teams, standardize our deployment processes, and improve the overall maintainability and reliability of our infrastructure. We will also establish internal guidelines and shared repositories to promote code reuse and collaboration amongst teams.","GenTime":"2024-08-28 23:02:48"}
{"File Name":"early-careers-framework\/0003-separate-ecf-and-npq-calculation-engines-and-hashes-as-interfaces.md","Context":"## Context\\nThere are currently two different training schemes in scope for the track-and-pay project. There are rumours of a possible third in the future.\\nThere are some similarities and many differences in the inputs, outputs and maths for payments for these training schemes. For example:\\n1. Both have fixed payments at 40%\\n2. Both allow pulling some of the fixed payment into earlier \"setup\" payments (for cashflow).\\n3. The output payments are very different in detail.\\n4. They have different banding systems.\\n### People\\nTim Abell and Pavel Lisovin (track and pay developers) discussed the issue amongst ourselves and came to this as a decision for the time being.\\n","Decision":"1. Build two payment engines that do not share code.\\n2. Have similar input\/output interfaces (ruby hash structures) that can later be easily converted to JSON.\\n3. Use similar patterns for both engines of Gherkin BDD driven unit tests plus normal rspec unit tests.\\n","tokens":146,"id":4277,"text":"## Context\\nThere are currently two different training schemes in scope for the track-and-pay project. There are rumours of a possible third in the future.\\nThere are some similarities and many differences in the inputs, outputs and maths for payments for these training schemes. For example:\\n1. Both have fixed payments at 40%\\n2. Both allow pulling some of the fixed payment into earlier \"setup\" payments (for cashflow).\\n3. The output payments are very different in detail.\\n4. They have different banding systems.\\n### People\\nTim Abell and Pavel Lisovin (track and pay developers) discussed the issue amongst ourselves and came to this as a decision for the time being.\\n\n\n##Decision\n1. Build two payment engines that do not share code.\\n2. Have similar input\/output interfaces (ruby hash structures) that can later be easily converted to JSON.\\n3. Use similar patterns for both engines of Gherkin BDD driven unit tests plus normal rspec unit tests.\\n","MatchedIds":[3802,1754,3653,4428,3119],"Predicted":"## Decision\nWe chose to implement a modular architecture for the payment calculations to accommodate both current training schemes and possible future schemes. Each scheme will have its own module responsible for handling its specific inputs, outputs, banding systems, and payment calculations. This modular approach ensures that new schemes can be added with minimal disruption to the existing code base.\n\n### Rationale\n* Allows flexibility to easily incorporate additional training schemes in the future.\n* Facilitates maintenance and updating of individual schemes without affecting others.\n* Ensures clear separation of concerns, reducing the risk of errors that could arise from handling multiple schemes in a monolithic approach.\n\n### Positive Consequences\n* Easier to extend the system to accommodate new training schemes with different rules.\n* Simplified testing by isolating tests for each scheme's payment calculations.\n* Enhanced clarity and maintainability of the codebase by adhering to the Single Responsibility Principle.\n\n### Negative Consequences\n* Initial development effort is higher due to the need to design and implement a modular system.\n* Potential overhead in ensuring that the modules properly interact when changes are made.\n* Additional documentation required to explain the modular approach and how to add new schemes.","GenTime":"2024-08-28 23:02:53"}
{"File Name":"LogLady\/0006-use-electron-updater-autoupdate.md","Context":"## Context\\nThere are multiple packages available for handling auto-update on Electron apps, and one was already installed in the project but not used - which package should the project use?\\n","Decision":"We will use [electron-updater](https:\/\/github.com\/electron-userland\/electron-builder) as the package for auto-update.\\nThe package already installed in the project was replaced. It was decided earlier to use it instead of electron-updater because support for platforms was limited - but with new versions that has been added.\\nElectron-updater is the most popular package for auto-updating Electron apps, with more weekly downloads on npm and more dependents than the package that was already installed.\\nOverall, electron-updater seems like a more reliable package for handling auto-update.\\n","tokens":36,"id":1852,"text":"## Context\\nThere are multiple packages available for handling auto-update on Electron apps, and one was already installed in the project but not used - which package should the project use?\\n\n\n##Decision\nWe will use [electron-updater](https:\/\/github.com\/electron-userland\/electron-builder) as the package for auto-update.\\nThe package already installed in the project was replaced. It was decided earlier to use it instead of electron-updater because support for platforms was limited - but with new versions that has been added.\\nElectron-updater is the most popular package for auto-updating Electron apps, with more weekly downloads on npm and more dependents than the package that was already installed.\\nOverall, electron-updater seems like a more reliable package for handling auto-update.\\n","MatchedIds":[3827,241,3031,1202,3691],"Predicted":"## Decision\nWe will use `electron-updater` for handling auto-update on Electron apps. This package is widely adopted, has a comprehensive set of features, and provides good documentation and community support. Additionally, since it was already installed in the project but not used, it aligns with our goal of minimizing additional package installations and configurations.","GenTime":"2024-08-28 23:02:54"}
{"File Name":"ethnicity-facts-and-figures-publisher\/0002-use-heroku-run-detached-for-static-site-build.md","Context":"## Context\\nAt the moment, we have a set of scheduled tasks on Heroku that run every 10 minutes, hour, or day. These range from building the static site (every 10 minutes), to reporting stalled builds (every hour), to refreshing the database from production data (in dev\/staging, every night). The main job here, building the static site, has started to fail because it occasionally runs longer than 10 minutes. The Heroku scheduler will only allow a scheduled instance to live for as long as the interval on the task. Heroku is therefore killing the instance before it can finish, leading to the incomplete builds and frequent reports of 'stalled' (in this instance, killed) builds.\\nWe need to schedule static site builds in a way that removes this strict time constraint so that they will always finish building.\\n","Decision":"We will continue to use the Heroku scheduler on a ten-minute interval, but will instead use the Heroku CLI to run the management command (.\/manage.py build_static_site) in a separate, detached worker dyno. This worker dyno has a lifespan of 24 hours and is not limited to the interval of the scheduled job.\\nThe command we will use is `heroku run:detached -a <app_name> .\/manage.py build_static_site`.\\nPros\\n----\\n* Very easy to implement.\\n* Will allow static site builds that take up to 24 hours (which is greater than any reasonable build length).\\nCons\\n----\\n* We remain unable to responsively schedule static site builds at the moment they are needed and rely on the scheduler to kick off a build.\\n* We have to expose a Heroku API key and install the Heroku CLI buildpack on our main Publisher app.\\n","tokens":172,"id":3361,"text":"## Context\\nAt the moment, we have a set of scheduled tasks on Heroku that run every 10 minutes, hour, or day. These range from building the static site (every 10 minutes), to reporting stalled builds (every hour), to refreshing the database from production data (in dev\/staging, every night). The main job here, building the static site, has started to fail because it occasionally runs longer than 10 minutes. The Heroku scheduler will only allow a scheduled instance to live for as long as the interval on the task. Heroku is therefore killing the instance before it can finish, leading to the incomplete builds and frequent reports of 'stalled' (in this instance, killed) builds.\\nWe need to schedule static site builds in a way that removes this strict time constraint so that they will always finish building.\\n\n\n##Decision\nWe will continue to use the Heroku scheduler on a ten-minute interval, but will instead use the Heroku CLI to run the management command (.\/manage.py build_static_site) in a separate, detached worker dyno. This worker dyno has a lifespan of 24 hours and is not limited to the interval of the scheduled job.\\nThe command we will use is `heroku run:detached -a <app_name> .\/manage.py build_static_site`.\\nPros\\n----\\n* Very easy to implement.\\n* Will allow static site builds that take up to 24 hours (which is greater than any reasonable build length).\\nCons\\n----\\n* We remain unable to responsively schedule static site builds at the moment they are needed and rely on the scheduler to kick off a build.\\n* We have to expose a Heroku API key and install the Heroku CLI buildpack on our main Publisher app.\\n","MatchedIds":[279,4821,351,1739,2791],"Predicted":"## Decision\nWe will move the static site build task to a dedicated background worker system that is not constrained by Heroku's strict time limits. The options we considered include moving the build task to an AWS Lambda function, using a dedicated background worker in Heroku with a longer timeout, or leveraging an external task scheduler like AWS CloudWatch Events combined with AWS ECS Fargate.\n\n### Options Considered\n1. **AWS Lambda**: Using AWS Lambda for this task isn't suitable due to Lambda's maximum allowed execution time of 15 minutes, which doesn't significantly exceed Heroku's 10-minute constraint.\n\n2. **Dedicated Background Worker in Heroku**: Using a Heroku Dyno as a dedicated background worker with an extended timeout (e.g., up to 24 hours). This approach would require a paid plan that allows higher timeout limits and ensures the worker is always available.\n\n3. **AWS CloudWatch Events with AWS ECS Fargate**: Configure AWS CloudWatch Events to trigger an ECS Fargate task, which can run as long as needed without being time-constrained. This provides granular control over scheduling and scaling.\n\n### Decision\nWe will use AWS CloudWatch Events with AWS ECS Fargate for the following reasons:\n- **Scalability**: ECS Fargate can scale based on demand and is not constrained by execution time.\n- **Resilience**: AWS ECS Fargate offers high availability and can handle retries and failures more gracefully.\n- **Flexibility**: Provides the ability to trigger builds based on various schedules and conditions without being tied to a single cloud provider's constraints.\n\n### Implementation Steps\n1. **Set Up CloudWatch Events**: Configure AWS CloudWatch Events to trigger the build job on a defined schedule (e.g., every 10 minutes).\n2. **Define ECS Task Definition**: Create an ECS Fargate task definition that specifies the container image and required resources for the build task.\n3. **Configure ECS Cluster**: Set up an ECS cluster to run the Fargate tasks.\n4. **Monitoring and Alerts**: Implement monitoring and alerting using AWS CloudWatch to ensure failures or delays in builds are promptly addressed.\n\nThis solution ensures that the static site build process will complete without being abruptly terminated, providing a more reliable and scalable approach to scheduled tasks.","GenTime":"2024-08-28 23:03:04"}
{"File Name":"cosmos-sdk\/adr-032-typed-events.md","Context":"## Context\\nCurrently in the Cosmos SDK, events are defined in the handlers for each message, meaning each module doesn't have a canonical set of types for each event. Above all else this makes these events difficult to consume as it requires a great deal of raw string matching and parsing. This proposal focuses on updating the events to use **typed events** defined in each module such that emitting and subscribing to events will be much easier. This workflow comes from the experience of the Akash Network team.\\n[Our platform](http:\/\/github.com\/ovrclk\/akash) requires a number of programmatic on chain interactions both on the provider (datacenter - to bid on new orders and listen for leases created) and user (application developer - to send the app manifest to the provider) side. In addition the Akash team is now maintaining the IBC [`relayer`](https:\/\/github.com\/ovrclk\/relayer), another very event driven process. In working on these core pieces of infrastructure, and integrating lessons learned from Kubernetes development, our team has developed a standard method for defining and consuming typed events in Cosmos SDK modules. We have found that it is extremely useful in building this type of event driven application.\\nAs the Cosmos SDK gets used more extensively for apps like `peggy`, other peg zones, IBC, DeFi, etc... there will be an exploding demand for event driven applications to support new features desired by users. We propose upstreaming our findings into the Cosmos SDK to enable all Cosmos SDK applications to quickly and easily build event driven apps to aid their core application. Wallets, exchanges, explorers, and defi protocols all stand to benefit from this work.\\nIf this proposal is accepted, users will be able to build event driven Cosmos SDK apps in go by just writing `EventHandler`s for their specific event types and passing them to `EventEmitters` that are defined in the Cosmos SDK.\\nThe end of this proposal contains a detailed example of how to consume events after this refactor.\\nThis proposal is specifically about how to consume these events as a client of the blockchain, not for intermodule communication.\\n","Decision":"**Step-1**:  Implement additional functionality in the `types` package: `EmitTypedEvent` and `ParseTypedEvent` functions\\n```go\\n\/\/ types\/events.go\\n\/\/ EmitTypedEvent takes typed event and emits converting it into sdk.Event\\nfunc (em *EventManager) EmitTypedEvent(event proto.Message) error {\\nevtType := proto.MessageName(event)\\nevtJSON, err := codec.ProtoMarshalJSON(event)\\nif err != nil {\\nreturn err\\n}\\nvar attrMap map[string]json.RawMessage\\nerr = json.Unmarshal(evtJSON, &attrMap)\\nif err != nil {\\nreturn err\\n}\\nvar attrs []abci.EventAttribute\\nfor k, v := range attrMap {\\nattrs = append(attrs, abci.EventAttribute{\\nKey:   []byte(k),\\nValue: v,\\n})\\n}\\nem.EmitEvent(Event{\\nType:       evtType,\\nAttributes: attrs,\\n})\\nreturn nil\\n}\\n\/\/ ParseTypedEvent converts abci.Event back to typed event\\nfunc ParseTypedEvent(event abci.Event) (proto.Message, error) {\\nconcreteGoType := proto.MessageType(event.Type)\\nif concreteGoType == nil {\\nreturn nil, fmt.Errorf(\"failed to retrieve the message of type %q\", event.Type)\\n}\\nvar value reflect.Value\\nif concreteGoType.Kind() == reflect.Ptr {\\nvalue = reflect.New(concreteGoType.Elem())\\n} else {\\nvalue = reflect.Zero(concreteGoType)\\n}\\nprotoMsg, ok := value.Interface().(proto.Message)\\nif !ok {\\nreturn nil, fmt.Errorf(\"%q does not implement proto.Message\", event.Type)\\n}\\nattrMap := make(map[string]json.RawMessage)\\nfor _, attr := range event.Attributes {\\nattrMap[string(attr.Key)] = attr.Value\\n}\\nattrBytes, err := json.Marshal(attrMap)\\nif err != nil {\\nreturn nil, err\\n}\\nerr = jsonpb.Unmarshal(strings.NewReader(string(attrBytes)), protoMsg)\\nif err != nil {\\nreturn nil, err\\n}\\nreturn protoMsg, nil\\n}\\n```\\nHere, the `EmitTypedEvent` is a method on `EventManager` which takes typed event as input and apply json serialization on it. Then it maps the JSON key\/value pairs to `event.Attributes` and emits it in form of `sdk.Event`. `Event.Type` will be the type URL of the proto message.\\nWhen we subscribe to emitted events on the CometBFT websocket, they are emitted in the form of an `abci.Event`. `ParseTypedEvent` parses the event back to it's original proto message.\\n**Step-2**: Add proto definitions for typed events for msgs in each module:\\nFor example, let's take `MsgSubmitProposal` of `gov` module and implement this event's type.\\n```protobuf\\n\/\/ proto\/cosmos\/gov\/v1beta1\/gov.proto\\n\/\/ Add typed event definition\\npackage cosmos.gov.v1beta1;\\nmessage EventSubmitProposal {\\nstring from_address   = 1;\\nuint64 proposal_id    = 2;\\nTextProposal proposal = 3;\\n}\\n```\\n**Step-3**: Refactor event emission to use the typed event created and emit using `sdk.EmitTypedEvent`:\\n```go\\n\/\/ x\/gov\/handler.go\\nfunc handleMsgSubmitProposal(ctx sdk.Context, keeper keeper.Keeper, msg types.MsgSubmitProposalI) (*sdk.Result, error) {\\n...\\ntypes.Context.EventManager().EmitTypedEvent(\\n&EventSubmitProposal{\\nFromAddress: fromAddress,\\nProposalId: id,\\nProposal: proposal,\\n},\\n)\\n...\\n}\\n```\\n### How to subscribe to these typed events in `Client`\\n> NOTE: Full code example below\\nUsers will be able to subscribe using `client.Context.Client.Subscribe` and consume events which are emitted using `EventHandler`s.\\nAkash Network has built a simple [`pubsub`](https:\/\/github.com\/ovrclk\/akash\/blob\/90d258caeb933b611d575355b8df281208a214f8\/pubsub\/bus.go#L20). This can be used to subscribe to `abci.Events` and [publish](https:\/\/github.com\/ovrclk\/akash\/blob\/90d258caeb933b611d575355b8df281208a214f8\/events\/publish.go#L21) them as typed events.\\nPlease see the below code sample for more detail on this flow looks for clients.\\n","tokens":433,"id":842,"text":"## Context\\nCurrently in the Cosmos SDK, events are defined in the handlers for each message, meaning each module doesn't have a canonical set of types for each event. Above all else this makes these events difficult to consume as it requires a great deal of raw string matching and parsing. This proposal focuses on updating the events to use **typed events** defined in each module such that emitting and subscribing to events will be much easier. This workflow comes from the experience of the Akash Network team.\\n[Our platform](http:\/\/github.com\/ovrclk\/akash) requires a number of programmatic on chain interactions both on the provider (datacenter - to bid on new orders and listen for leases created) and user (application developer - to send the app manifest to the provider) side. In addition the Akash team is now maintaining the IBC [`relayer`](https:\/\/github.com\/ovrclk\/relayer), another very event driven process. In working on these core pieces of infrastructure, and integrating lessons learned from Kubernetes development, our team has developed a standard method for defining and consuming typed events in Cosmos SDK modules. We have found that it is extremely useful in building this type of event driven application.\\nAs the Cosmos SDK gets used more extensively for apps like `peggy`, other peg zones, IBC, DeFi, etc... there will be an exploding demand for event driven applications to support new features desired by users. We propose upstreaming our findings into the Cosmos SDK to enable all Cosmos SDK applications to quickly and easily build event driven apps to aid their core application. Wallets, exchanges, explorers, and defi protocols all stand to benefit from this work.\\nIf this proposal is accepted, users will be able to build event driven Cosmos SDK apps in go by just writing `EventHandler`s for their specific event types and passing them to `EventEmitters` that are defined in the Cosmos SDK.\\nThe end of this proposal contains a detailed example of how to consume events after this refactor.\\nThis proposal is specifically about how to consume these events as a client of the blockchain, not for intermodule communication.\\n\n\n##Decision\n**Step-1**:  Implement additional functionality in the `types` package: `EmitTypedEvent` and `ParseTypedEvent` functions\\n```go\\n\/\/ types\/events.go\\n\/\/ EmitTypedEvent takes typed event and emits converting it into sdk.Event\\nfunc (em *EventManager) EmitTypedEvent(event proto.Message) error {\\nevtType := proto.MessageName(event)\\nevtJSON, err := codec.ProtoMarshalJSON(event)\\nif err != nil {\\nreturn err\\n}\\nvar attrMap map[string]json.RawMessage\\nerr = json.Unmarshal(evtJSON, &attrMap)\\nif err != nil {\\nreturn err\\n}\\nvar attrs []abci.EventAttribute\\nfor k, v := range attrMap {\\nattrs = append(attrs, abci.EventAttribute{\\nKey:   []byte(k),\\nValue: v,\\n})\\n}\\nem.EmitEvent(Event{\\nType:       evtType,\\nAttributes: attrs,\\n})\\nreturn nil\\n}\\n\/\/ ParseTypedEvent converts abci.Event back to typed event\\nfunc ParseTypedEvent(event abci.Event) (proto.Message, error) {\\nconcreteGoType := proto.MessageType(event.Type)\\nif concreteGoType == nil {\\nreturn nil, fmt.Errorf(\"failed to retrieve the message of type %q\", event.Type)\\n}\\nvar value reflect.Value\\nif concreteGoType.Kind() == reflect.Ptr {\\nvalue = reflect.New(concreteGoType.Elem())\\n} else {\\nvalue = reflect.Zero(concreteGoType)\\n}\\nprotoMsg, ok := value.Interface().(proto.Message)\\nif !ok {\\nreturn nil, fmt.Errorf(\"%q does not implement proto.Message\", event.Type)\\n}\\nattrMap := make(map[string]json.RawMessage)\\nfor _, attr := range event.Attributes {\\nattrMap[string(attr.Key)] = attr.Value\\n}\\nattrBytes, err := json.Marshal(attrMap)\\nif err != nil {\\nreturn nil, err\\n}\\nerr = jsonpb.Unmarshal(strings.NewReader(string(attrBytes)), protoMsg)\\nif err != nil {\\nreturn nil, err\\n}\\nreturn protoMsg, nil\\n}\\n```\\nHere, the `EmitTypedEvent` is a method on `EventManager` which takes typed event as input and apply json serialization on it. Then it maps the JSON key\/value pairs to `event.Attributes` and emits it in form of `sdk.Event`. `Event.Type` will be the type URL of the proto message.\\nWhen we subscribe to emitted events on the CometBFT websocket, they are emitted in the form of an `abci.Event`. `ParseTypedEvent` parses the event back to it's original proto message.\\n**Step-2**: Add proto definitions for typed events for msgs in each module:\\nFor example, let's take `MsgSubmitProposal` of `gov` module and implement this event's type.\\n```protobuf\\n\/\/ proto\/cosmos\/gov\/v1beta1\/gov.proto\\n\/\/ Add typed event definition\\npackage cosmos.gov.v1beta1;\\nmessage EventSubmitProposal {\\nstring from_address   = 1;\\nuint64 proposal_id    = 2;\\nTextProposal proposal = 3;\\n}\\n```\\n**Step-3**: Refactor event emission to use the typed event created and emit using `sdk.EmitTypedEvent`:\\n```go\\n\/\/ x\/gov\/handler.go\\nfunc handleMsgSubmitProposal(ctx sdk.Context, keeper keeper.Keeper, msg types.MsgSubmitProposalI) (*sdk.Result, error) {\\n...\\ntypes.Context.EventManager().EmitTypedEvent(\\n&EventSubmitProposal{\\nFromAddress: fromAddress,\\nProposalId: id,\\nProposal: proposal,\\n},\\n)\\n...\\n}\\n```\\n### How to subscribe to these typed events in `Client`\\n> NOTE: Full code example below\\nUsers will be able to subscribe using `client.Context.Client.Subscribe` and consume events which are emitted using `EventHandler`s.\\nAkash Network has built a simple [`pubsub`](https:\/\/github.com\/ovrclk\/akash\/blob\/90d258caeb933b611d575355b8df281208a214f8\/pubsub\/bus.go#L20). This can be used to subscribe to `abci.Events` and [publish](https:\/\/github.com\/ovrclk\/akash\/blob\/90d258caeb933b611d575355b8df281208a214f8\/events\/publish.go#L21) them as typed events.\\nPlease see the below code sample for more detail on this flow looks for clients.\\n","MatchedIds":[21,812,827,2316,816],"Predicted":"## Decision\nWe will adopt a **typed event** system for the Cosmos SDK to standardize the definition and consumption of events across different modules. This will involve:\n\n1. **Defining Typed Events**: Each module will define a canonical set of event types in protobuf that represent the events it emits.\n   ```protobuf\n   message EventCreateOrder {\n       string order_id = 1;\n       string creator = 2;\n       string product = 3;\n       ...\n   }\n     \n   message EventBid {\n       string bid_id = 1;\n       string order_id = 2;\n       string bidder = 3;\n       ...\n   }\n   ```\n\n2. **Emitting Typed Events**: Module handlers will emit events using these types, which will encapsulate all necessary event data.\n   ```go\n   func (k Keeper) CreateOrder(ctx sdk.Context, creator string, product string) {\n       ...\n       ctx.EventManager().EmitTypedEvent(\n           &types.EventCreateOrder{\n               OrderId: newOrder.ID,\n               Creator: creator,\n               Product: product,\n               ...\n           }\n       )\n   }\n   ```\n\n3. **Subscribing to Typed Events**: Consumers will implement `EventHandler`s for specific event types and pass them to `EventEmitters`.\n   ```go\n   type CreateOrderHandler struct{}\n\n   func (h CreateOrderHandler) Handle(ctx sdk.Context, event *types.EventCreateOrder) {\n       \/\/ Handle the event, e.g. update a local db, trigger a workflow, etc.\n   }\n\n   \/\/ Subscription logic\n   func SubscribeToEvents() {\n       eventBus := sdk.NewEventBus()\n       eventBus.Subscribe(&types.EventCreateOrder{}, CreateOrderHandler{})\n       ...\n   }\n   ```\n\n4. **Automated Code Generation**: Use protobuf to generate Go structs for events to ensure consistency and reduce boilerplate code.\n   ```sh\n   protoc -I. --go_out=. --go_opt=paths=source_relative *.proto\n   ```\n\n5. **Versioning and Backward Compatibility**: Ensure types are versioned and backward compatible through the use of protobuf. This facilitates gradual upgrades and migrations.\n   ```protobuf\n   message EventCreateOrderV2 {\n       string order_id = 1;\n       string creator_address = 2;\n       string product_details = 3;\n       string timestamp = 4;\n       ...\n   }\n   ```\n\n### Benefits\n- **Ease of Consumption and Emission**: Emitting and consuming events will become straightforward with well-defined data structures, eliminating the need for raw string parsing.\n- **Strong Typing and Safety**: Typed events enforce structure and type safety, reducing the chances of errors during event production and consumption.\n- **Extensibility and Maintenance**: Adding new events or modifying existing ones will be easier and cleaner with clear definitions.\n- **Interoperability**: With the standardization of event types, various applications (wallets, explorers, relayers) can reliably interact with and process events.\n\n### Implementation Plan\n- **Phase 1**: Prototype the approach in a single module (e.g., `x\/bank`) and document the process.\n- **Phase 2**: Gradually expand the implementation to other modules, iterating based on feedback.\n- **Phase 3**: Update documentation and develop migration guides for existing applications transitioning to the new event system.\n- **Phase 4**: Engage with the community for feedback, potential improvements, and stabilization.\n\nBy adopting this approach, we aim to create a more structured, maintainable, and developer-friendly ecosystem for event-driven applications in Cosmos SDK.","GenTime":"2024-08-28 23:03:19"}
{"File Name":"wordpress-template\/0003-theme-namespace-naming-convention.md","Context":"## Context\\nWe need to pick a namespace for themes, and renaming can be tedious.\\nOnly one theme is ever active at a time, so there's no risk in naming collisions except when dealing with child themes.\\n","Decision":"We'll use the `\\Theme` namespace for every theme we make except for child themes which will have the namespace `\\ChildTheme`.\\n","tokens":45,"id":2262,"text":"## Context\\nWe need to pick a namespace for themes, and renaming can be tedious.\\nOnly one theme is ever active at a time, so there's no risk in naming collisions except when dealing with child themes.\\n\n\n##Decision\nWe'll use the `\\Theme` namespace for every theme we make except for child themes which will have the namespace `\\ChildTheme`.\\n","MatchedIds":[4498,2270,5185,1240,3653],"Predicted":"## Decision\nTo simplify naming and avoid tedious renaming efforts, we will adopt a convention of prefixing all theme-related namespaces with a consistent identifier `theme.`. For child themes, the namespace will include both the parent and child theme names to avoid naming collisions. The format will be `theme.<ParentTheme>.<ChildTheme>`.\n\nFor example:\n- `theme.Storefront`\n- `theme.Storefront.ChildTheme1`\n- `theme.Storefront.ChildTheme2`\n\nThis convention will provide clarity, maintain uniqueness where necessary, and ease the management of theme namespaces.","GenTime":"2024-08-28 23:03:22"}
{"File Name":"stamper\/0007-docker.md","Context":"## Context\\nWe need to choose how we will deploy our app.\\n","Decision":"We use Docker because it allows us to isolate app with a container and easily deploy to the server with Terraform.\\n","tokens":15,"id":1239,"text":"## Context\\nWe need to choose how we will deploy our app.\\n\n\n##Decision\nWe use Docker because it allows us to isolate app with a container and easily deploy to the server with Terraform.\\n","MatchedIds":[4358,1243,469,1240,3657],"Predicted":"## Decision\nWe will use [Docker] containers to package our application and [Kubernetes] (specifically Amazon EKS) to orchestrate the deployment. More specifically, we will:\n1. Containerize our application using Docker, ensuring that each service runs in its own container.\n2. Use Kubernetes to manage the deployment, scaling, and operation of our application containers.\n3. Deploy the containers to Amazon EKS, leveraging its managed Kubernetes service for scalability and resilience.\n\n\ud83d\udc49 **Note:** We will use [Helm] for Kubernetes package management to simplify deployment and upgrade processes.\n\ud83d\udc49 **Note:** The CI\/CD pipeline will be set up with [Jenkins] for automated builds, tests, and deployments. The pipeline will push Docker images to Amazon ECR and then update the Kubernetes deployment on Amazon EKS.","GenTime":"2024-08-28 23:03:25"}
{"File Name":"mario\/0008-use-one-s3-bucket-per-source.md","Context":"## Context\\nEach data source will need to upload one or more files to S3 in order to trigger processing. S3 events, which will drive Lambda execution (See [5. Use AWS Lambda](0005-use-aws-lambda.md)), are configured at the bucket level. We may or may not have much control over the environment which is sending data to S3, for example, if it came directly from a vendor. At minimum we must be able to specify a bucket, but we should not assume we will have much more control than this.\\nEach data source will also need different processing. This implies the need to identify which source a data file came from.\\n","Decision":"Use one S3 bucket per data source.\\n","tokens":137,"id":3416,"text":"## Context\\nEach data source will need to upload one or more files to S3 in order to trigger processing. S3 events, which will drive Lambda execution (See [5. Use AWS Lambda](0005-use-aws-lambda.md)), are configured at the bucket level. We may or may not have much control over the environment which is sending data to S3, for example, if it came directly from a vendor. At minimum we must be able to specify a bucket, but we should not assume we will have much more control than this.\\nEach data source will also need different processing. This implies the need to identify which source a data file came from.\\n\n\n##Decision\nUse one S3 bucket per data source.\\n","MatchedIds":[3794,3987,2471,3797,8],"Predicted":"## Decision\nTo handle file uploads from multiple data sources and ensure each file triggers the correct processing logic, the following approach will be implemented:\n\n1. **Bucket Naming Convention**: We will use different buckets for different data sources. Each bucket name will follow a standardized naming convention that clearly denotes the data source. For example, we could use names like `vendorA-data`, `vendorB-data`, etc.\n\n2. **S3 Folder Structure**: Within each bucket, we will enforce a folder structure that indicates the type of processing required. This ensures that even if files from multiple sources land in the same bucket, their paths will provide enough information to trigger the appropriate processing. For example, `vendorA-data\/raw`, `vendorA-data\/processed`, `vendorB-data\/raw`, etc.\n\n3. **Lambda Trigger Configurations**: Configure S3 events to trigger Lambda functions based on the prefix and suffix of the file paths. This allows us to set up different Lambda functions for different types of files and processing requirements. For example, files uploaded to `vendorA-data\/raw` could trigger a different Lambda function compared to files in `vendorA-data\/processed`.\n\n4. **Metadata Tagging**: When files are uploaded, metadata tags will be added to each file to provide additional context such as the source of the data and the type of processing required. This will allow Lambda functions to use this metadata for further decision-making.\n\n5. **Centralized Processing Lambda**: Develop a centralized Lambda function that can route files to appropriate processing pipelines based on the bucket name, folder structure, and metadata. This ensures that the logic for handling different data sources and their respective processing requirements is encapsulated within this function.\n\n6. **Logging and Monitoring**: Implement detailed logging within the Lambda functions to track which files are being processed and by which logic. Additionally, employ AWS CloudWatch for monitoring and setting up alarms for any processing failures or anomalies.\n\nBy adopting this structured approach, we ensure that each data source is correctly identified and the appropriate processing logic is applied, regardless of the endpoint environment from which the files are uploaded.","GenTime":"2024-08-28 23:03:34"}
{"File Name":"ensemblejs\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":3940,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","MatchedIds":[4428,3119,1436,294,2583],"Predicted":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).","GenTime":"2024-08-28 23:03:36"}
{"File Name":"visit-plannr\/0002-dynamodb-composite-primary-key-design.md","Context":"## Context\\n\"[In DynamoDB, tables, items, and attributes are the core components that you work with. A table is a collection of items, and each item is a collection of attributes. DynamoDB uses primary keys to uniquely identify each item in a table and secondary indexes to provide more querying flexibility.](https:\/\/docs.aws.amazon.com\/amazondynamodb\/latest\/developerguide\/HowItWorks.CoreComponents.html)\"\\nThere are two types of primary key in dynamodb.\\nThe first kind is having just a partition key. The partition key is a hash and determines where on physical storage the item is placed. The partition key must be unique.\\nThe second kind is a composite primary key. It consists of a partition key and a sort key. The partition key stays the same but doesn't need to be unique in isolation. Rather the sort key\/ partition key pair must be unique.\\nIn a real system this would probably push towards StreamName as the partition key: so that events that logically live together physically live together. And Event Number in the stream as the sort key. So that the order of items as they are stored on physical media matches the order they are likely to be read.\\nThis introduces unwanted complexity at this time in the code for tracking event numbers.\\n","Decision":"For now instead of an Event Number as the sort key we will introduce a UUID EventId. And use StreamName as the HASH key. StreamNames will need to be unique anyway.\\n","tokens":260,"id":1990,"text":"## Context\\n\"[In DynamoDB, tables, items, and attributes are the core components that you work with. A table is a collection of items, and each item is a collection of attributes. DynamoDB uses primary keys to uniquely identify each item in a table and secondary indexes to provide more querying flexibility.](https:\/\/docs.aws.amazon.com\/amazondynamodb\/latest\/developerguide\/HowItWorks.CoreComponents.html)\"\\nThere are two types of primary key in dynamodb.\\nThe first kind is having just a partition key. The partition key is a hash and determines where on physical storage the item is placed. The partition key must be unique.\\nThe second kind is a composite primary key. It consists of a partition key and a sort key. The partition key stays the same but doesn't need to be unique in isolation. Rather the sort key\/ partition key pair must be unique.\\nIn a real system this would probably push towards StreamName as the partition key: so that events that logically live together physically live together. And Event Number in the stream as the sort key. So that the order of items as they are stored on physical media matches the order they are likely to be read.\\nThis introduces unwanted complexity at this time in the code for tracking event numbers.\\n\n\n##Decision\nFor now instead of an Event Number as the sort key we will introduce a UUID EventId. And use StreamName as the HASH key. StreamNames will need to be unique anyway.\\n","MatchedIds":[2543,4385,3416,2791,3123],"Predicted":"## Decision\nWe will use a composite primary key for our DynamoDB table, with `StreamName` as the partition key and `EventNumber` as the sort key. This design will ensure that events belonging to the same stream are stored together and can be queried efficiently in the order they were generated. \n\n## Justification\n1. **Logical Grouping**: Using `StreamName` as the partition key allows all events belonging to a specific stream to be stored together, making retrieval operations more efficient.\n2. **Efficient Sorting and Querying**: The `EventNumber` as the sort key will preserve the chronological order of events, facilitating efficient range queries and ordered retrievals.\n3. **Scalability**: This schema design takes advantage of DynamoDB's distributed nature to scale horizontally, as different streams can be distributed across multiple partitions.\n4. **Avoiding Complexity**: While this design introduces the need to track event numbers within our application, the benefits of efficient querying and logical grouping outweigh the complexity introduced.\n\n## Implementation Notes\n1. **Event Number Tracking**: We will implement a mechanism to track and increment `EventNumber` for each new event within a stream to maintain the unique constraint on the composite key.\n2. **Error Handling**: We will handle potential race conditions during event writes by implementing optimistic concurrency control, ensuring consistency even in highly concurrent environments.\n3. **Secondary Indexes**: If additional querying flexibility is required, we will consider adding global secondary indexes (GSIs) or local secondary indexes (LSIs) based on the access patterns.\n\nThis approach balances the need for efficient data retrieval with the practicality of maintaining unique and ordered event numbers within each stream.","GenTime":"2024-08-28 23:03:43"}
{"File Name":"hee-web-blueprint\/0013-use-bloomreach-channel-manager.md","Context":"## Context\\nWe sought to determine whether to deliver our document management capabilities using the content management platform natively or through the integration of an external document management platform.\\nWe sought to determine whether Bloomreach's 'Channel' concept would be suitable for managing the various sites required to be brought onto the platform both at MVP and in the future, such as Deenary and Speciality sights.\\nAs part of this, considerations were made around:\\n* Ease of use for creating new sites\\n* Ability to share components\\n* Ability to segregate content for specific channels (sites)\\n* Ability to share content up and down the stack where needed and appropriate\\n* Permissions model required to support this model\\n","Decision":"Bloomreach's concept of channels is well suited to meet the needs of running the sites required under the NWP platform umbrella. Channels offer the ability to build new sites that share components and modules, which enables for greater consistency. By utilising roles and permissions from within BR, content can be segregated to be available only where it is most relevant, whilst allowing for content to be made available up or down the organisational stack (e.g. national content being aggregated at a regional level).\\nBR's 'blueprinting' functionality allows for sites to be created using a series of parameters, further standardising the creation of sites where needed in an easy fashion.\\n","tokens":141,"id":1205,"text":"## Context\\nWe sought to determine whether to deliver our document management capabilities using the content management platform natively or through the integration of an external document management platform.\\nWe sought to determine whether Bloomreach's 'Channel' concept would be suitable for managing the various sites required to be brought onto the platform both at MVP and in the future, such as Deenary and Speciality sights.\\nAs part of this, considerations were made around:\\n* Ease of use for creating new sites\\n* Ability to share components\\n* Ability to segregate content for specific channels (sites)\\n* Ability to share content up and down the stack where needed and appropriate\\n* Permissions model required to support this model\\n\n\n##Decision\nBloomreach's concept of channels is well suited to meet the needs of running the sites required under the NWP platform umbrella. Channels offer the ability to build new sites that share components and modules, which enables for greater consistency. By utilising roles and permissions from within BR, content can be segregated to be available only where it is most relevant, whilst allowing for content to be made available up or down the organisational stack (e.g. national content being aggregated at a regional level).\\nBR's 'blueprinting' functionality allows for sites to be created using a series of parameters, further standardising the creation of sites where needed in an easy fashion.\\n","MatchedIds":[1182,3784,1196,2261,4087],"Predicted":"## Decision\n\nAfter evaluating the options, we have decided to deliver our document management capabilities by integrating an external document management platform instead of relying on the content management platform natively. Additionally, we have determined that Bloomreach's 'Channel' concept is suitable for managing the various sites required both at MVP and in the future.\n\n### Considerations\n1. **Ease of use for creating new sites:**\n   - **Integration of external document management platform:** Allows for specialized tools that are optimized for document management tasks, thus simplifying the creation and management of document-heavy sites.\n   - **Bloomreach's 'Channel' concept:** Facilitates a user-friendly interface for creating and managing new content channels (sites), consistent with the needs of different types of sites like Deenary and Speciality sites.\n\n2. **Ability to share components:**\n   - **Integration of external document management platform:** Enhances the capability to use advanced document management features and share document components across different channels.\n   - **Bloomreach's 'Channel' concept:** Allows for easy reuse of content components across multiple sites, improving consistency and reducing duplication of effort.\n\n3. **Ability to segregate content for specific channels (sites):**\n   - **Integration of external document management platform:** Offers robust capabilities for content segregation, enabling precise control over which documents are accessible in which channels.\n   - **Bloomreach's 'Channel' concept:** Supports efficient content segregation to ensure that each site only displays relevant content.\n\n4. **Ability to share content up and down the stack where needed and appropriate:**\n   - **Integration of external document management platform:** Provides hierarchical content-sharing capabilities that enable better control over content dissemination up and down the content stack.\n   - **Bloomreach's 'Channel' concept:** Allows for flexible content sharing mechanisms to ensure that content can be appropriately reused or restricted as needed.\n\n5. **Permissions model required to support this model:**\n   - **Integration of external document management platform:** Typically includes a sophisticated permissions model that can be customized to suit complex document access requirements, enhancing security and compliance.\n   - **Bloomreach's 'Channel' concept:** Offers a permissions model that can meet the diverse requirements of managing different sites while ensuring secure access controls.\n\n### Decision Outcome\nWe will integrate an external document management platform for enhanced document management capabilities while utilizing Bloomreach's 'Channel' concept to manage the various sites required both at MVP and in the future.\n\n### Pros and Cons\n#### Integration of External Document Management Platform\n**Pros:**\n- Specialized document management capabilities.\n- Robust content segregation and sharing.\n- Advanced permissions model for security and compliance.\n\n**Cons:**\n- Additional integration effort required.\n- Potential for higher operational complexity.\n\n#### Bloomreach's 'Channel' Concept\n**Pros:**\n- Ease of use in creating and managing multiple sites.\n- Optimal content component sharing capabilities.\n- Efficient content segregation and permissions management.\n\n**Cons:**\n- Reliance on the external platform for document-specific features.","GenTime":"2024-08-28 23:03:51"}
{"File Name":"devops-challenge\/0001-record-architecture-decisions.md","Context":"## Context\\nThe goal of this challenge is to implement a support plaform for a\\nmicroservices architecture with several requirements:\\n* is scalable on load\\n* is fully automated\\n* identifies Single Point of Failures\\n* implements resilience and fault tolerance mechanisms\\nWe need to record the architectural decisions made on this project as\\na way to document all the thinking and building process.  This will help\\nothers to answer their wondering questions about why and how this solution was\\nimplemented.\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":112,"id":3488,"text":"## Context\\nThe goal of this challenge is to implement a support plaform for a\\nmicroservices architecture with several requirements:\\n* is scalable on load\\n* is fully automated\\n* identifies Single Point of Failures\\n* implements resilience and fault tolerance mechanisms\\nWe need to record the architectural decisions made on this project as\\na way to document all the thinking and building process.  This will help\\nothers to answer their wondering questions about why and how this solution was\\nimplemented.\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","MatchedIds":[4361,4428,3119,1436,294],"Predicted":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions), to document the architectural decisions made on this project. This will provide a comprehensive understanding of the reasoning behind each decision, ensuring that the process is transparent and well-documented for future reference.","GenTime":"2024-08-28 23:03:53"}
{"File Name":"ikforth\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":5120,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","MatchedIds":[4428,3119,1436,294,2583],"Predicted":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).","GenTime":"2024-08-28 23:03:55"}
{"File Name":"cloud-platform\/018-Dedicated-Ingress-Controllers.md","Context":"## Context\\nThe Cloud Platform was originally set up to have a single ingress controller to\\nmanage all ingresses in the cluster. So, every new ingress added a config block\\nto one, large nginx config file, and all traffic to all services hosted on the\\nCloud Platform is routed through a single AWS load balancer.\\nAlthough this was both easy to manage, and saved us some money on load\\nbalancers (approx. $25\/month per ingress), it has become unsustainable. We\\nusually have 6 replicas of the ingress controller pod, and we have started to\\nsee instances of several of these pods crash-looping (usually because they have\\nrun out of shared memory, which cannot be increased in kubernetes. See [this\\nissue] for more information).\\nWe believe this is because the nginx config has become so large (over 100K\\nlines), that sometimes pods fail to reload it when it is changed, or the pod is\\nmoved.\\n","Decision":"We will create a separate AWS load balancer and ingress-controller for every\\nnamespace in the cluster. An \"ingress class\" annotation will cause traffic for\\na particular ingress to be routed through the appropriate AWS load balancer and\\ningress-controller. See our [module repository] for more details.\\n\"System\" ingresses (e.g. those used for concourse, grafana, etc.) will continue\\nto use the default ingress-controller. There should only ever be a handful of\\nthese, compared with hundreds of team ingresses, so the load on the default\\ningress-controller should stay within acceptable limits.\\n","tokens":200,"id":620,"text":"## Context\\nThe Cloud Platform was originally set up to have a single ingress controller to\\nmanage all ingresses in the cluster. So, every new ingress added a config block\\nto one, large nginx config file, and all traffic to all services hosted on the\\nCloud Platform is routed through a single AWS load balancer.\\nAlthough this was both easy to manage, and saved us some money on load\\nbalancers (approx. $25\/month per ingress), it has become unsustainable. We\\nusually have 6 replicas of the ingress controller pod, and we have started to\\nsee instances of several of these pods crash-looping (usually because they have\\nrun out of shared memory, which cannot be increased in kubernetes. See [this\\nissue] for more information).\\nWe believe this is because the nginx config has become so large (over 100K\\nlines), that sometimes pods fail to reload it when it is changed, or the pod is\\nmoved.\\n\n\n##Decision\nWe will create a separate AWS load balancer and ingress-controller for every\\nnamespace in the cluster. An \"ingress class\" annotation will cause traffic for\\na particular ingress to be routed through the appropriate AWS load balancer and\\ningress-controller. See our [module repository] for more details.\\n\"System\" ingresses (e.g. those used for concourse, grafana, etc.) will continue\\nto use the default ingress-controller. There should only ever be a handful of\\nthese, compared with hundreds of team ingresses, so the load on the default\\ningress-controller should stay within acceptable limits.\\n","MatchedIds":[733,3905,3104,856,3915],"Predicted":"## Decision\nWe will transition from a single ingress controller to a multi-ingress controller setup, where each major application or group of applications has its own dedicated ingress controller. This will reduce the size of the configuration file managed by each ingress controller, thereby minimizing the likelihood of shared memory issues and crash-looping.\n\n### Steps and Considerations:\n\n1. **Ingress Controller Isolation**:\n   - Create a new ingress controller for each major application or logical group of applications.\n   - Ensure that each new ingress controller has its own AWS load balancer.\n   - Distribute ingress traffic accordingly, to ensure no single ingress controller manages too many configs.\n\n2. **Configuration Management**:\n   - Update deployment scripts and tooling to correctly configure and deploy the new ingress controllers.\n   - Establish naming conventions and labels to ensure clear identification and management of each ingress controller and its associated resources.\n\n3. **Monitoring and Scaling**:\n   - Implement monitoring solutions to track memory usage and performance of each ingress controller.\n   - Adjust the number of replicas for each ingress controller based on observed load and performance, ensuring each controller is operating within acceptable limits.\n\n4. **Cost Implications**:\n   - There will be an increase in costs associated with having multiple AWS load balancers (approx. $25\/month per additional ingress).\n   - Factor these additional costs into budgeting and resource allocation.\n\n5. **Gradual Transition**:\n   - Execute the transition gradually, starting with non-critical applications, to ensure stability and allow for troubleshooting.\n   - Monitor the impact of the changes and make necessary adjustments before fully implementing the new architecture across all applications.\n\n6. **Documentation and Training**:\n   - Document the new process and any changes to the deployment pipelines.\n   - Conduct training sessions or provide guidance documents to devops engineers and application developers to familiarize them with the new ingress setup.\n\n### Positive Consequences\n- Improved stability and reduced crash-looping of ingress controller pods.\n- Enhanced manageability and modularity of ingress configurations.\n- Potential for better isolation and security between different application domains.\n\n### Negative Consequences\n- Increased costs due to additional AWS load balancers.\n- Increased complexity in managing multiple ingress controllers.\n- Possible initial learning curve and adjustment period for engineers.\n\nThis decision aims to balance between system stability and operational complexity, ensuring a scalable and maintainable cloud platform.","GenTime":"2024-08-28 23:04:03"}
{"File Name":"handbook\/0009-naming-files-and-directories.md","Context":"## Context and Problem Statement\\nAs of the date of this writing, there are multiple different naming styles used in the monorepo, mostly because NX has\\ndefaults that differ between schematic types.\\nIn order for navigating the monorepo in a consistent rational manner, we should align on naming strategy for files\\nand directories.\\n## Decision Drivers\\n* Provide consistency when navigating the codebase\\n* The earlier we decide on this, the better\\n","Decision":"* Provide consistency when navigating the codebase\\n* The earlier we decide on this, the better\\nChosen option: Name files after their default export. If that default export is a React Component, or a class, then the\\nfile name should be in PascalCase. Otherwise, the filename should be in camelCase. Basically, for naming files avoid\\nusing kebab-case and snake_case and make sure the name follows the default export of the file.\\nNaming directories should follow these guidelines:\\nOnly use kebab-case when naming NX apps and libraries, or folders containing apps and libraries, e.g. `island-ui` instead of `islandUi`:\\n`import { Box } from '@island.is\/island-ui\/core'`\\nUse PascalCase for directories only containing React components:\\n````\\ncomponents\/CtaButton\/index.ts\\nimport 'components\/CtaButton'\\n````\\nor:\\n````\\ncomponents\/CtaButton\/CtaButton.tsx\\nimport 'components\/CtaButton\/CtaButton'\\n````\\nrather than\\n````\\ncomponents\/cta-button\/CtaButton.tsx\\n````\\nIn all other cases, use camelCase.\\n### Positive Consequences\\n* Easier to navigate the codebase\\n* File names are more readable, and developers know what to expect\\n* This approach is the most common practice, and something most JS and TS developers are familiar with.\\n","tokens":93,"id":1996,"text":"## Context and Problem Statement\\nAs of the date of this writing, there are multiple different naming styles used in the monorepo, mostly because NX has\\ndefaults that differ between schematic types.\\nIn order for navigating the monorepo in a consistent rational manner, we should align on naming strategy for files\\nand directories.\\n## Decision Drivers\\n* Provide consistency when navigating the codebase\\n* The earlier we decide on this, the better\\n\n\n##Decision\n* Provide consistency when navigating the codebase\\n* The earlier we decide on this, the better\\nChosen option: Name files after their default export. If that default export is a React Component, or a class, then the\\nfile name should be in PascalCase. Otherwise, the filename should be in camelCase. Basically, for naming files avoid\\nusing kebab-case and snake_case and make sure the name follows the default export of the file.\\nNaming directories should follow these guidelines:\\nOnly use kebab-case when naming NX apps and libraries, or folders containing apps and libraries, e.g. `island-ui` instead of `islandUi`:\\n`import { Box } from '@island.is\/island-ui\/core'`\\nUse PascalCase for directories only containing React components:\\n````\\ncomponents\/CtaButton\/index.ts\\nimport 'components\/CtaButton'\\n````\\nor:\\n````\\ncomponents\/CtaButton\/CtaButton.tsx\\nimport 'components\/CtaButton\/CtaButton'\\n````\\nrather than\\n````\\ncomponents\/cta-button\/CtaButton.tsx\\n````\\nIn all other cases, use camelCase.\\n### Positive Consequences\\n* Easier to navigate the codebase\\n* File names are more readable, and developers know what to expect\\n* This approach is the most common practice, and something most JS and TS developers are familiar with.\\n","MatchedIds":[4766,2703,36,1117,3521],"Predicted":"## Decision\n\nChosen option: Standardize file and directory naming conventions using the following consistent approach:\n\n### File Naming\n1. **React Components and Classes**: Use PascalCase.\n   - Example: `MyComponent.tsx` for a default export of a React component named `MyComponent`.\n2. **Other Default Exports**: Use camelCase.\n   - Example: `myUtilityFunction.ts` for a default export function named `myUtilityFunction`.\n\n### Directory Naming\n1. **NX Apps and Libraries**: Use kebab-case.\n   - Example: `island-ui` for the core library of an island UI.\n   - Importing Example: `import { Box } from '@island.is\/island-ui\/core'`.\n2. **Directories Containing React Components**: Use PascalCase.\n   - Example: \n     ```\n     components\/CtaButton\/index.ts\n     import 'components\/CtaButton'\n     ```\n     or\n     ```\n     components\/CtaButton\/CtaButton.tsx\n     import 'components\/CtaButton\/CtaButton'\n     ```\n   - Avoid:\n     ```\n     components\/cta-button\/CtaButton.tsx\n     ```\n\n3. **All Other Cases**: Use camelCase.\n   - Example: `utils\/helperFunctions.ts`.\n\n### Positive Consequences\n* Easier to navigate the codebase.\n* File names are more readable, and developers know what to expect.\n* This approach aligns with common practices familiar to most JavaScript and TypeScript developers.\n\n### Negative Consequences\n* Transitioning to this standard may require renaming existing files and directories, which could temporarily disrupt workflow.\n* Developers will need to adjust to these guidelines if they are currently using different naming conventions.","GenTime":"2024-08-28 23:04:08"}
{"File Name":"php-docker-template\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Usabilla Architecture Chapter](https:\/\/github.com\/usabilla\/architecture-chapter\/blob\/master\/docs\/adr\/0001-record-architecture-decisions.md).\\n","tokens":16,"id":5237,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as [described by Usabilla Architecture Chapter](https:\/\/github.com\/usabilla\/architecture-chapter\/blob\/master\/docs\/adr\/0001-record-architecture-decisions.md).\\n","MatchedIds":[4699,4408,4722,4428,3119],"Predicted":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).","GenTime":"2024-08-28 23:04:10"}
{"File Name":"beis-report-official-development-assistance\/0004-add-rollbar-for-application-monitoring.md","Context":"## Context\\nWe need a mechanism to capture and manage application errors. Without an integration our debugging options are to access a console on live environments and try to replicate (something we want to minimise) or by looking through information provided by logs.\\nWe have used Rollbar for a few years now and we have not reviewed this decision since. It is currently being used for 14 applications.\\nFor some projects we use their technical tooling of choice to aid in the transition to business as usual. Due to this we will have a handful of projects using Sentry and possible others.\\nSometimes Rollbar environment names don't match the Rails environment. Dalmatian-<project> and paas-<project> both exist. There also exists both permutations for the same project as we transition. We have used ROLLBAR_ENV to manage this before so making it explicit will hopefully make it clearer how it can be changed.\\n","Decision":"Use Rollbar to collect and manage our application errors.\\n","tokens":187,"id":2387,"text":"## Context\\nWe need a mechanism to capture and manage application errors. Without an integration our debugging options are to access a console on live environments and try to replicate (something we want to minimise) or by looking through information provided by logs.\\nWe have used Rollbar for a few years now and we have not reviewed this decision since. It is currently being used for 14 applications.\\nFor some projects we use their technical tooling of choice to aid in the transition to business as usual. Due to this we will have a handful of projects using Sentry and possible others.\\nSometimes Rollbar environment names don't match the Rails environment. Dalmatian-<project> and paas-<project> both exist. There also exists both permutations for the same project as we transition. We have used ROLLBAR_ENV to manage this before so making it explicit will hopefully make it clearer how it can be changed.\\n\n\n##Decision\nUse Rollbar to collect and manage our application errors.\\n","MatchedIds":[3388,3370,1254,3389,1523],"Predicted":"## Decision\nWe will continue using Rollbar to collect and manage our application errors across the majority of our projects. Rollbar has been reliable and serves the needs of 14 applications effectively. \n\nTo address the issue of mismatched environment names, we will standardize the usage of the `ROLLBAR_ENV` variable to explicitly define the environment settings in each project. This will help eliminate confusion and ensure consistency across different environments.\n\nFor the handful of projects that are already integrated with Sentry, we will continue to use Sentry for those specific cases to avoid disruption and accommodate the technical preferences of the teams involved.\n\nBoth Rollbar and Sentry will be supported going forward, with new projects defaulting to Rollbar unless there is a compelling reason to use an alternative solution.","GenTime":"2024-08-28 23:04:13"}
{"File Name":"cnp-design-documentation\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":1077,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","MatchedIds":[4428,3119,1436,294,2583],"Predicted":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).","GenTime":"2024-08-28 23:04:19"}
{"File Name":"SiebenApp\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":1922,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","MatchedIds":[4699,4408,4722,4428,3119],"Predicted":"## Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions","GenTime":"2024-08-28 23:04:21"}
{"File Name":"profiles-db-elastic\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":5139,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","MatchedIds":[4699,4408,4722,4428,3119],"Predicted":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).","GenTime":"2024-08-28 23:04:23"}
{"File Name":"linshare-mobile-flutter-app\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":3308,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","MatchedIds":[4428,3119,1436,294,2583],"Predicted":"## Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions.","GenTime":"2024-08-28 23:04:25"}
{"File Name":"stentor\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":2166,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","MatchedIds":[4699,4408,4722,4428,3119],"Predicted":"## Decision\nWe will use Architecture Decision Records, as outlined by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions.","GenTime":"2024-08-28 23:04:26"}
{"File Name":"verify-hub\/0001-record-architechture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":4206,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","MatchedIds":[4699,4408,4722,4428,3119],"Predicted":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).","GenTime":"2024-08-28 23:04:28"}
{"File Name":"verify-onboarding-prototypes\/0008-provide-an-end-to-end-stub.md","Context":"## Context\\nWe wish to run regular user research and testing against the prototype Service Provider.\\nTo support user research we need to provide a user journey that resembles a typical Verify journey.\\nThe area we are most interested in is the interface between the Service and the Hub.\\n","Decision":"We will create a Stub Verify Hub that will allow for end-to-end testing of the prototype.\\nIt will not provide a SAML implementation.\\nIt will expect a form submission on a web resource that mimics the behaviour of receiving an AuthnRequest.\\nIf the form post is successful then a browser redirect will be issued to a page explaining where the user is in their\\njourney.  Continuing from this page will take the user to a page containing a series of possible response scenarios that\\ncan be selected.\\nAny information that needs to be stored from the original request or between pages will be stored in a session\\ncookie. The session cookie does not need to be signed or encrypted initially.\\nSome scenarios will require additional user input such as a providing a pid value or user account creation attributes.\\nThese will be provided on subsequent pages. In every case the user will need to provide the URL that the stub-hub should\\nsend its responses to.\\nEach response will mimic the shape of a Hub SAML Response form that can be submitted back to the Service and SP\\nprototype.\\nThe details of each canned response will be encoded as base64 encoded JSON in the SAMLResponse parameter of the form.\\nThe prototype service provider will be able to understand each canned response and produce a specific response to the\\nService\/client.\\nUsing these responses it will be possible able to test different scenarios in the client, Service, and browser.\\nThe Stub Verify Hub will provide the following responses:\\n| Response Scenario     | Message                                                                                                                   |\\n| --                    | --                                                                                                                         |\\n| Successful Match      | scenario: SUCCESS_MATCH, levelOfAssurance: ${specified loa}, pid: ${specified PID}                                         |\\n| Account Creation      | scenario: ACCOUNT_CREATION, levelOfAssurance: ${specified loa}, pid: ${specified PID}, attributes: [${user attributes}]  |\\n| No Match              | scenario: NO_MATCH                                                                                                         |\\n| Cancellation          | scenario: CANCELLATION                                                                                                     |\\n| Authentication failed | scenario: AUTHENTICATION_FAILED                                                                                           |\\n| Requestor Error       | scenario: REQUEST_ERROR                                                                                                 |\\n| Internal Server Error | scenario: INTERNAL_SERVER_ERROR\\nInitially, the Stub Verify Hub will be deployed to Government PaaS.\\nA diagram of a potential stub architecture is available at: `prototypes\/prototype-0\/docs\/diagrams\/stub_service_architecture.png`\\n","tokens":55,"id":581,"text":"## Context\\nWe wish to run regular user research and testing against the prototype Service Provider.\\nTo support user research we need to provide a user journey that resembles a typical Verify journey.\\nThe area we are most interested in is the interface between the Service and the Hub.\\n\n\n##Decision\nWe will create a Stub Verify Hub that will allow for end-to-end testing of the prototype.\\nIt will not provide a SAML implementation.\\nIt will expect a form submission on a web resource that mimics the behaviour of receiving an AuthnRequest.\\nIf the form post is successful then a browser redirect will be issued to a page explaining where the user is in their\\njourney.  Continuing from this page will take the user to a page containing a series of possible response scenarios that\\ncan be selected.\\nAny information that needs to be stored from the original request or between pages will be stored in a session\\ncookie. The session cookie does not need to be signed or encrypted initially.\\nSome scenarios will require additional user input such as a providing a pid value or user account creation attributes.\\nThese will be provided on subsequent pages. In every case the user will need to provide the URL that the stub-hub should\\nsend its responses to.\\nEach response will mimic the shape of a Hub SAML Response form that can be submitted back to the Service and SP\\nprototype.\\nThe details of each canned response will be encoded as base64 encoded JSON in the SAMLResponse parameter of the form.\\nThe prototype service provider will be able to understand each canned response and produce a specific response to the\\nService\/client.\\nUsing these responses it will be possible able to test different scenarios in the client, Service, and browser.\\nThe Stub Verify Hub will provide the following responses:\\n| Response Scenario     | Message                                                                                                                   |\\n| --                    | --                                                                                                                         |\\n| Successful Match      | scenario: SUCCESS_MATCH, levelOfAssurance: ${specified loa}, pid: ${specified PID}                                         |\\n| Account Creation      | scenario: ACCOUNT_CREATION, levelOfAssurance: ${specified loa}, pid: ${specified PID}, attributes: [${user attributes}]  |\\n| No Match              | scenario: NO_MATCH                                                                                                         |\\n| Cancellation          | scenario: CANCELLATION                                                                                                     |\\n| Authentication failed | scenario: AUTHENTICATION_FAILED                                                                                           |\\n| Requestor Error       | scenario: REQUEST_ERROR                                                                                                 |\\n| Internal Server Error | scenario: INTERNAL_SERVER_ERROR\\nInitially, the Stub Verify Hub will be deployed to Government PaaS.\\nA diagram of a potential stub architecture is available at: `prototypes\/prototype-0\/docs\/diagrams\/stub_service_architecture.png`\\n","MatchedIds":[4454,2405,4442,2041,915],"Predicted":"## Decision\nWe will establish a Stub Verify Hub to facilitate end-to-end testing of the prototype service provider. This Stub Verify Hub will simulate interactions typically seen in a Verify journey:\n\n- **SAML Implementation**: Initially, the hub will not implement SAML but will use form submissions to mimic AuthnRequest scenarios.\n- **Session Management**: Session data needed for continuity between steps will be managed via session cookies. These cookies will not require signing or encryption at the outset.\n- **Response Scenarios**: Users will be able to navigate through the journey and select canned response scenarios which will be embedded as base64-encoded JSON within the SAMLResponse parameter.\n    - **Successful Match**: scenario: SUCCESS_MATCH, levelOfAssurance: ${specified loa}, pid: ${specified PID}\n    - **Account Creation**: scenario: ACCOUNT_CREATION, levelOfAssurance: ${specified loa}, pid: ${specified PID}, attributes: [${user attributes}]\n    - **No Match**: scenario: NO_MATCH\n    - **Cancellation**: scenario: CANCELLATION\n    - **Authentication failed**: scenario: AUTHENTICATION_FAILED\n    - **Requestor Error**: scenario: REQUEST_ERROR\n    - **Internal Server Error**: scenario: INTERNAL_SERVER_ERROR\n\n- **Workflow**: Upon a successful form submission, a redirect will occur to a page demonstrating the user's current position in the journey. Users can then select the next steps and provide any additional information, such as a pid value or account creation attributes.\n- **Deployment**: Initially, this Stub Verify Hub will be hosted on the Government PaaS to facilitate rapid testing and iteration.\n- **Documentation**: Refer to the diagram at `prototypes\/prototype-0\/docs\/diagrams\/stub_service_architecture.png` for a potential stub architecture layout.\n\nThis approach will provide a controlled environment to test different scenarios between the Service and the Hub, thereby enhancing user research activities and prototype evaluations.","GenTime":"2024-08-28 23:04:35"}
