File Name,Context,Decision,tokens,id,text
volley-management/0007-follow-optimistic-concurrency.md,"## Context and Problem Statement\nAs any modern system, Volley Management faces a problem of concurrent changes to data and we need to support such scenario.\nWe explicitly do not consider an option to go without concurrency checks - time will tell if it is a good decision).\n## Decision Drivers <!-- optional -->\n* Performance - decision should support high throughput scenarios\n* Maintainability - amount of code needed to write should be minimized\n","* Performance - decision should support high throughput scenarios\n* Maintainability - amount of code needed to write should be minimized\nChosen option: ""Optimistic concurrency"", because it is a better from the perfromance perspective and it will support our choice to [use Azure Table Storage](0006-use-azure-table-storage-for-persistence.md).\n### Positive Consequences <!-- optional -->\n* Concurrency check will fail very rarely so it will have almost no overhead\n* It is aligned with Azure Table storage Optimistic concurrency\n### Negative Consequences <!-- optional -->\n* Whole application will have a responsibility to manage versions properly\n* If we change a datastore we might need to have a larger change\n",86,2526,"## Context and Problem Statement\nAs any modern system, Volley Management faces a problem of concurrent changes to data and we need to support such scenario.\nWe explicitly do not consider an option to go without concurrency checks - time will tell if it is a good decision).\n## Decision Drivers <!-- optional -->\n* Performance - decision should support high throughput scenarios\n* Maintainability - amount of code needed to write should be minimized\n

##Decision
* Performance - decision should support high throughput scenarios\n* Maintainability - amount of code needed to write should be minimized\nChosen option: ""Optimistic concurrency"", because it is a better from the perfromance perspective and it will support our choice to [use Azure Table Storage](0006-use-azure-table-storage-for-persistence.md).\n### Positive Consequences <!-- optional -->\n* Concurrency check will fail very rarely so it will have almost no overhead\n* It is aligned with Azure Table storage Optimistic concurrency\n### Negative Consequences <!-- optional -->\n* Whole application will have a responsibility to manage versions properly\n* If we change a datastore we might need to have a larger change\n"
paas-team-manual/ADR010-postgres-bind-behaviour.html.md,"## Context\nWe use RDS for tenant databases. Tenants can bind more than one application to a database instance created via the services console. Database migrations were broken when a binding was removed and re-added, or when another bound application other than the one that created the database tables attempted to modify them.\nPreviously the RDS broker PostgreSQL engine copied the MySQL engine and granted all rights on the database to the newly created user. In PostgreSQL this will give the user rights to create tables, but because it has a more finely-grained permission model than MySQL this does not give the user rights on existing tables, or tables newly created by other users.\nOnly the owner of a table can alter/drop it, and you cannot grant this permission to other users. Users who are the owners of tables cannot be removed until the table ownership is changed.\nWe attempted to work around the PostgreSQL permissions system in the following ways:\n* Using [`ALTER DEFAULT PRIVILEGES`](https://www.postgresql.org/docs/9.5/static/sql-alterdefaultprivileges.html) on every newly created user to `GRANT ALL ON PUBLIC` - this means that every user can `SELECT`, `INSERT`, and `DELETE`, but because only the table owner can `ALTER` or `DROP` this will not allow other bound users to run migrations. This is also limited to the `PUBLIC` (default) schema, so would fail to work for any applications that have custom schemas.\n* Making the group the owner of the `PUBLIC` schema. This allowed members of the group to `DROP` tables within the schema, but still did not allow them to `ALTER` these tables.\n* Creating a ""parent"" group that is given ownership of all tables, sequences, and functions. New users are granted rights on this group and thus can carry out all owner actions on group-owned items. A [DDL event trigger](https://www.postgresql.org/docs/9.5/static/event-trigger-definition.html) using the built-in `REASSIGN OWNED BY` command handed over ownership of all resources owned by the current user to the parent `$DBNAME_owner` group. This worked perfectly on all the PostgreSQL databases we tested with, however RDS does not allow use of event triggers so it was unworkable in production.\n","We decided the simplest solution was to issue the same credentials to all applications bound to the database. This means that the same user will be used by each application, and they will not suffer ownership problems when issuing `DROP` or `ALTER` commands.\nRather than deterministically deriving the credentials we decided to store them in the backing database itself meaning they can be changed at any time in case of a breach. To protect against database breaches that may leak stored credentials we also decided to encrypt the credentials using a symmetric key held by the broker.\nWe accepted that in the future we may wish to extend this by allowing the broker to issue read-only credentials, or credentials that are suitable for `SELECT`, `INSERT`, and `DELETE` operations, but not schema modifications.\n",470,224,"## Context\nWe use RDS for tenant databases. Tenants can bind more than one application to a database instance created via the services console. Database migrations were broken when a binding was removed and re-added, or when another bound application other than the one that created the database tables attempted to modify them.\nPreviously the RDS broker PostgreSQL engine copied the MySQL engine and granted all rights on the database to the newly created user. In PostgreSQL this will give the user rights to create tables, but because it has a more finely-grained permission model than MySQL this does not give the user rights on existing tables, or tables newly created by other users.\nOnly the owner of a table can alter/drop it, and you cannot grant this permission to other users. Users who are the owners of tables cannot be removed until the table ownership is changed.\nWe attempted to work around the PostgreSQL permissions system in the following ways:\n* Using [`ALTER DEFAULT PRIVILEGES`](https://www.postgresql.org/docs/9.5/static/sql-alterdefaultprivileges.html) on every newly created user to `GRANT ALL ON PUBLIC` - this means that every user can `SELECT`, `INSERT`, and `DELETE`, but because only the table owner can `ALTER` or `DROP` this will not allow other bound users to run migrations. This is also limited to the `PUBLIC` (default) schema, so would fail to work for any applications that have custom schemas.\n* Making the group the owner of the `PUBLIC` schema. This allowed members of the group to `DROP` tables within the schema, but still did not allow them to `ALTER` these tables.\n* Creating a ""parent"" group that is given ownership of all tables, sequences, and functions. New users are granted rights on this group and thus can carry out all owner actions on group-owned items. A [DDL event trigger](https://www.postgresql.org/docs/9.5/static/event-trigger-definition.html) using the built-in `REASSIGN OWNED BY` command handed over ownership of all resources owned by the current user to the parent `$DBNAME_owner` group. This worked perfectly on all the PostgreSQL databases we tested with, however RDS does not allow use of event triggers so it was unworkable in production.\n

##Decision
We decided the simplest solution was to issue the same credentials to all applications bound to the database. This means that the same user will be used by each application, and they will not suffer ownership problems when issuing `DROP` or `ALTER` commands.\nRather than deterministically deriving the credentials we decided to store them in the backing database itself meaning they can be changed at any time in case of a breach. To protect against database breaches that may leak stored credentials we also decided to encrypt the credentials using a symmetric key held by the broker.\nWe accepted that in the future we may wish to extend this by allowing the broker to issue read-only credentials, or credentials that are suitable for `SELECT`, `INSERT`, and `DELETE` operations, but not schema modifications.\n"
qc-atlas/0008-testcontainers.md,## Context and Problem Statement\nRunning Service Tests requires PostgreSQL. To ensure an instance is always up and running a container has to be manually launched or started automaticially.\n,"Chosen option: [Testcontainers](https://www.testcontainers.org/), because it simplifies the test execution process\n### Positive Consequences <!-- optional -->\n* It is not mandatory to ensure postgres is running before starting the tests\n",34,699,"## Context and Problem Statement\nRunning Service Tests requires PostgreSQL. To ensure an instance is always up and running a container has to be manually launched or started automaticially.\n

##Decision
Chosen option: [Testcontainers](https://www.testcontainers.org/), because it simplifies the test execution process\n### Positive Consequences <!-- optional -->\n* It is not mandatory to ensure postgres is running before starting the tests\n"
trade-access-program/0005-viewflow-for-grant-management-portal.md,## Context\nWe need a system to manage the lifecycle of a grant application.\n,Viewflow was chosen as a framework to help us achieve this goal while making the development process efficient.\n### Build vs Buy Paper\nVarious alternatives to Viewflow were considered when making this decision. Those alternatives are details in the\nalpha stage of the project here: https://uktrade.atlassian.net/l/c/zEAEM37j\n### Viewflow analysis\nSome initial analysis on how viewflow could be used was also done in alpha: https://uktrade.atlassian.net/l/c/Lc77C4mq\n,17,5009,"## Context\nWe need a system to manage the lifecycle of a grant application.\n

##Decision
Viewflow was chosen as a framework to help us achieve this goal while making the development process efficient.\n### Build vs Buy Paper\nVarious alternatives to Viewflow were considered when making this decision. Those alternatives are details in the\nalpha stage of the project here: https://uktrade.atlassian.net/l/c/zEAEM37j\n### Viewflow analysis\nSome initial analysis on how viewflow could be used was also done in alpha: https://uktrade.atlassian.net/l/c/Lc77C4mq\n"
celestia-core/adr-026-general-merkle-proof.md,"## Context\nWe are using raw `[]byte` for merkle proofs in `abci.ResponseQuery`. It makes hard to handle multilayer merkle proofs and general cases. Here, new interface `ProofOperator` is defined. The users can defines their own Merkle proof format and layer them easily.\nGoals:\n- Layer Merkle proofs without decoding/reencoding\n- Provide general way to chain proofs\n- Make the proof format extensible, allowing thirdparty proof types\n","### ProofOperator\n`type ProofOperator` is an interface for Merkle proofs. The definition is:\n```go\ntype ProofOperator interface {\nRun([][]byte) ([][]byte, error)\nGetKey() []byte\nProofOp() ProofOp\n}\n```\nSince a proof can treat various data type, `Run()` takes `[][]byte` as the argument, not `[]byte`. For example, a range proof's `Run()` can take multiple key-values as its argument. It will then return the root of the tree for the further process, calculated with the input value.\n`ProofOperator` does not have to be a Merkle proof - it can be a function that transforms the argument for intermediate process e.g. prepending the length to the `[]byte`.\n### ProofOp\n`type ProofOp` is a protobuf message which is a triple of `Type string`, `Key []byte`, and `Data []byte`. `ProofOperator` and `ProofOp`are interconvertible, using `ProofOperator.ProofOp()` and `OpDecoder()`, where `OpDecoder` is a function that each proof type can register for their own encoding scheme. For example, we can add an byte for encoding scheme before the serialized proof, supporting JSON decoding.\n",99,4130,"## Context\nWe are using raw `[]byte` for merkle proofs in `abci.ResponseQuery`. It makes hard to handle multilayer merkle proofs and general cases. Here, new interface `ProofOperator` is defined. The users can defines their own Merkle proof format and layer them easily.\nGoals:\n- Layer Merkle proofs without decoding/reencoding\n- Provide general way to chain proofs\n- Make the proof format extensible, allowing thirdparty proof types\n

##Decision
### ProofOperator\n`type ProofOperator` is an interface for Merkle proofs. The definition is:\n```go\ntype ProofOperator interface {\nRun([][]byte) ([][]byte, error)\nGetKey() []byte\nProofOp() ProofOp\n}\n```\nSince a proof can treat various data type, `Run()` takes `[][]byte` as the argument, not `[]byte`. For example, a range proof's `Run()` can take multiple key-values as its argument. It will then return the root of the tree for the further process, calculated with the input value.\n`ProofOperator` does not have to be a Merkle proof - it can be a function that transforms the argument for intermediate process e.g. prepending the length to the `[]byte`.\n### ProofOp\n`type ProofOp` is a protobuf message which is a triple of `Type string`, `Key []byte`, and `Data []byte`. `ProofOperator` and `ProofOp`are interconvertible, using `ProofOperator.ProofOp()` and `OpDecoder()`, where `OpDecoder` is a function that each proof type can register for their own encoding scheme. For example, we can add an byte for encoding scheme before the serialized proof, supporting JSON decoding.\n"
sepa-customer-platform/0004-authenticate-backstage-users-azure-ad.md,"## Context\nAzure Cloud Services and Microsoft Dynamics 365 have been proposed to support the CCP case & contact management, therefore, to maximise vendor reuse\n& interoperability, Azure's identity management system will be used to provide authentication for the frontend application (back stage/SEPA users).\n## Decision Drivers\nSEPA uses Active Directory for it's existing internal account management so providing one unified single sign on login would improve security and usability.\n","SEPA uses Active Directory for it's existing internal account management so providing one unified single sign on login would improve security and usability.\n[Option 1] SEPA should look to integrate Azure Active Directory with Dynamics to provide a shared identity 'federation', where backstage (SEPA) users are able to access Dynamics data via the WebAPI & perform CRUD operations using their existing single sign on login.\nDynamics users would therefore match the users within the Active Directory, and SEPA would require a Dynamics license for each SEPA user.\nIf SEPA's Active Directory is managed on premises, Azure AD connect can be used to auto sync existing Active Directory accounts to the Azure setup.\n### Positive Consequences\n* Reuse of existing SEPA logins\n* Better backstage user experience\n### Negative Consequences\n* If Azure Active Directory connect is used to sync accounts, this would need to be monitored & managed by SEPA.\n",90,5157,"## Context\nAzure Cloud Services and Microsoft Dynamics 365 have been proposed to support the CCP case & contact management, therefore, to maximise vendor reuse\n& interoperability, Azure's identity management system will be used to provide authentication for the frontend application (back stage/SEPA users).\n## Decision Drivers\nSEPA uses Active Directory for it's existing internal account management so providing one unified single sign on login would improve security and usability.\n

##Decision
SEPA uses Active Directory for it's existing internal account management so providing one unified single sign on login would improve security and usability.\n[Option 1] SEPA should look to integrate Azure Active Directory with Dynamics to provide a shared identity 'federation', where backstage (SEPA) users are able to access Dynamics data via the WebAPI & perform CRUD operations using their existing single sign on login.\nDynamics users would therefore match the users within the Active Directory, and SEPA would require a Dynamics license for each SEPA user.\nIf SEPA's Active Directory is managed on premises, Azure AD connect can be used to auto sync existing Active Directory accounts to the Azure setup.\n### Positive Consequences\n* Reuse of existing SEPA logins\n* Better backstage user experience\n### Negative Consequences\n* If Azure Active Directory connect is used to sync accounts, this would need to be monitored & managed by SEPA.\n"
architecture/0002-apply-clean-code-guidelines.md,"## Context\nAs it's hard to understand code, it is crucial that anybody can easily\nunderstand the code you're working on. This applies to all levels of code, not\nonly the code itself, but groups of code, complete applications and groups of\napplications.\n","I will always focus on simplicity, readaility and the ease of understanding\nany code or structure.\n",57,1416,"## Context\nAs it's hard to understand code, it is crucial that anybody can easily\nunderstand the code you're working on. This applies to all levels of code, not\nonly the code itself, but groups of code, complete applications and groups of\napplications.\n

##Decision
I will always focus on simplicity, readaility and the ease of understanding\nany code or structure.\n"
karma-sabarivka-reporter/0001-documenting-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",16,4002,"## Context\nWe need to record the architectural decisions made on this project.\n

##Decision
We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n"
digitalrig-metal-aws/0006-create-reference-implementation-repository.md,"## Context\nThe rig defined at [Bookit Infrastructure](https://github.com/buildit/bookit-infrastructure) is an instance of the AWS Bare Metal Rig.\nWhilst it's rather generic as it is, it is specific to Bookit's needs.\nThe AWS Bare Metal Rig is also intended to offer choices for the different components (Compute - ECS EC2 vs ECS Fargate vs EKS, RDS - Aurora MySQL vs Aurora Postgres vs Aurora Serverless, etc).\nThe only way to capture that is via branches which can be hard to discover.\nFinally, there is not a single repo that represents the latest and greatest version of the AWS Bare Metal Rig.  As instances of Rigs diverge, it is difficult to instantiate a new one that includes all of the latest features\n",Create a digitalrig-metal-aws repo (https://github.com/buildit/digitalrig-metal-aws) that demonstrates all of the options and latest features of the AWS Bare Metal Rig and removes any Bookit specific wording/concepts.\n,162,1742,"## Context\nThe rig defined at [Bookit Infrastructure](https://github.com/buildit/bookit-infrastructure) is an instance of the AWS Bare Metal Rig.\nWhilst it's rather generic as it is, it is specific to Bookit's needs.\nThe AWS Bare Metal Rig is also intended to offer choices for the different components (Compute - ECS EC2 vs ECS Fargate vs EKS, RDS - Aurora MySQL vs Aurora Postgres vs Aurora Serverless, etc).\nThe only way to capture that is via branches which can be hard to discover.\nFinally, there is not a single repo that represents the latest and greatest version of the AWS Bare Metal Rig.  As instances of Rigs diverge, it is difficult to instantiate a new one that includes all of the latest features\n

##Decision
Create a digitalrig-metal-aws repo (https://github.com/buildit/digitalrig-metal-aws) that demonstrates all of the options and latest features of the AWS Bare Metal Rig and removes any Bookit specific wording/concepts.\n"
opg-lpa/0006-modernise-the-code-base.md,"## Context\nWe have inherited a relatively large and complex legacy code base, mostly written in PHP.\nPHP [appears to be on a downwards trend as a language](https://pypl.github.io/PYPL.html?country=GB),\nespecially in contrast with Python. It's likely it will become increasingly difficult\nto find good PHP developers in future.\nAnecdotally, PHP is not seen as a desirable language for developers to work with. It doesn't\nhave the cool factor of newer languages like golang; nor the clean syntax and API of\nlanguages of similar pedigree, such as Python.\nOur code base is also showing its age somewhat. Some of the libraries are starting to rot.\nA mix of contractors and developers working on the code base over several years has\nresulted in a mix of styles and approaches. While we have already cleared out a lot\nof unused and/or broken code, there is likely to be more we haven't found yet.\nWe are also lagging behind the latest Design System guidelines, as our application was one\nof the first to go live, before the current iteration of the Design System existed.\nThis means that any changes to design have to be done piecemeal and manually: we can't\nsimply import the newest version of the design system and have everything magically update.\nThis combination of factors means that the code base can be difficult to work with:\nresistant to change and easy to break.\n","We have decided to modernise the code base to make it easier to work with and better\naligned with modern web architecture and standards. This is not a small job, but\nthe guiding principles we've decided on, shown below, should help us achieve our aims.\n(""Modernising the code base"" is not to be confused with ""modernising LPAs"". Here\nwe're just talking about modernising the code base for the Make an LPA tool.)\n* **Don't rewrite everything at once**\nWhere possible, migrate part of an application to a new\ncomponent and split traffic coming into the domain so that some paths are diverted to that\ncomponent. This will typically use nginx in dev, but may be done at the AWS level if\nappropriate (e.g in a load balancer or application gateway).\nThis is challenging, but means that we don't have to do a ""big bang"" release of the new\nversion of the tool. Our aim is to gradually replace existing components with new\nones, which are (hopefully) simpler, future-proofed, more efficient, and don't rely on PHP.\n* **Use Python for new work**\nWe considered golang, but don't have the experience in the team to build applications with it.\nWe felt that learning a new language + frameworks would only reduce our ability to deliver, with\nminimal benefits: our application is not under heavy load and responds in an\nacceptable amount of time, so golang's super efficiency isn't essential.\nWe feel that we could scale horizontally if necessary and have not had any major issues\nwith capacity in the past.\n* **Choose containers or lambdas as appropriate**\nUse a container for components which stay up most of the time, and lambdas for\n""bursty"" applications (e.g. background processes like PDF generation, daily statistics aggregation).\n* **Choose the right lambda for the job**\nUse ""pure"" lambdas where possible. This is only the case where an application has simple dependencies\nwhich don't require unusual native libraries outside the\n[stock AWS Docker images for lambdas](https://gallery.ecr.aws/lambda/python)).\nIf a component is problematic to run as a pure lambda, use a lambda running a Docker image based\non one of the stock AWS Docker images for lambdas.\n* **Choose the right Docker image**\nWhen using Docker images, prefer the following:\n* Images based on AWS Lambda images (if writing a component which will run as a Docker lambda).\n* Images based on Alpine (for other cases).\n* Images based on a non-Alpine foundation like Ubuntu, but only if an Alpine image is not available.\n* **Use Flask and gunicorn**\nUse [Flask](https://flask.palletsprojects.com/) for new Python web apps, fronted by\n[gunicorn](https://gunicorn.org/) for the WSGI implementation.\n* **Use the latest Design System**\nUse the [Government Design System](https://design-system.service.gov.uk/) guidelines for new UI. In\nparticular, use the\n[Land Registry's Python implementation of the design system](https://github.com/LandRegistry/govuk-frontend-jinja),\nwritten as [Jinja2 templates](https://jinja.palletsprojects.com/).\nOur aim should be to utilise it without modification as far as possible, so that we can easily upgrade\nif it is changed by developers at the Land Registry.\n* **Migrate legacy code to PHP 8**\nWhere possible, upgrade PHP applications to PHP 8, when supported by [Laminas](https://getlaminas.org/).\nAt the time of writing, Laminas support for PHP 8 is only partial, so we are stuck with PHP 7 for now,\nas large parts of our stack are implemented on top of Laminas.\n* **Specify new APIs with OpenAPI**\nSpecify new APIs using [OpenAPI](https://swagger.io/specification/). Ideally, use tooling\nwhich enables an API to be automatically built from an OpenAPI specification, binding to\ncode only when necessary, to avoid repetitive boilerplate.\n* **Controlled, incremental releases**\nProvision new infrastructure behind a feature flag wherever possible. This allows us to\nwork on new components, moving them into the live environment as they are ready, but hidden\nfrom the outside world. When ready for delivery, we switch the flag over to make that piece\nof infrastructure live.\n* **Follow good practices for web security**\nBe aware of the [OWASP Top Ten](https://owasp.org/www-project-top-ten/) and code to avoid those\nissues. Use tools like [Talisman](https://github.com/GoogleCloudPlatform/flask-talisman) to\nimprove security.\n* **Be mindful of accessibility**\nConsider accessibility requirements at every step of the design and coding phases. Aim to\ncomply with [WCAG 2.1 Level AA](https://www.w3.org/WAI/WCAG22/quickref/) as a minimum. While the\nDesign System helps a lot with this, always bear accessibility in mind when building workflows\nand custom components it doesn't cover.\n* **Be properly open source**\nMake the code base properly open source. While our code is open, there are still barriers to entry\nfor developers outside the Ministry of Justice, such as the requirement to have access to AWS secrets,\nS3, postcode API, the Government payment gateway and SendGrid for the system to work correctly. We\nwill work towards removing these barriers so that onboarding of new developers (internally and\nexternally) is seamless, and to enable potentially anyone to fully contribute to the project.\n* **Improve test coverage everywhere**\nAs we work on the code, be aware of gaps in testing and plug them as they arise. Don't wait for\nan opportunity to fix everything at once: make refactoring and adding unit tests part of the\nwork on an issue (unless it's going to take longer than working on the issue!).\nWhere a whole category of testing is missing, add it (for example, we\nhave recently implemented the foundations for load testing; see\n[0004-implement-load-testing](./0004-implement-load-testing.md)).\n* **Automate code quality metrics**\nIntroduce tools to lint and scan code as we go, to ensure consistent, easy-to-follow code. See\n[0003-linting-and-scanning](./0003-linting-and-scanning.md)) for a starting point.\n* **Peer review everything**\nAll commits to the code base must go through peer review before merging. No lone wolf developers.\n* **Be pragmatic**\nSee the [pragmatic quick reference](https://www.ccs.neu.edu/home/lieber/courses/csg110/sp08/Pragmatic%20Quick%20Reference.htm)\nfor a summary. These are generally good principles for software engineering.\n",306,1652,"## Context\nWe have inherited a relatively large and complex legacy code base, mostly written in PHP.\nPHP [appears to be on a downwards trend as a language](https://pypl.github.io/PYPL.html?country=GB),\nespecially in contrast with Python. It's likely it will become increasingly difficult\nto find good PHP developers in future.\nAnecdotally, PHP is not seen as a desirable language for developers to work with. It doesn't\nhave the cool factor of newer languages like golang; nor the clean syntax and API of\nlanguages of similar pedigree, such as Python.\nOur code base is also showing its age somewhat. Some of the libraries are starting to rot.\nA mix of contractors and developers working on the code base over several years has\nresulted in a mix of styles and approaches. While we have already cleared out a lot\nof unused and/or broken code, there is likely to be more we haven't found yet.\nWe are also lagging behind the latest Design System guidelines, as our application was one\nof the first to go live, before the current iteration of the Design System existed.\nThis means that any changes to design have to be done piecemeal and manually: we can't\nsimply import the newest version of the design system and have everything magically update.\nThis combination of factors means that the code base can be difficult to work with:\nresistant to change and easy to break.\n

##Decision
We have decided to modernise the code base to make it easier to work with and better\naligned with modern web architecture and standards. This is not a small job, but\nthe guiding principles we've decided on, shown below, should help us achieve our aims.\n(""Modernising the code base"" is not to be confused with ""modernising LPAs"". Here\nwe're just talking about modernising the code base for the Make an LPA tool.)\n* **Don't rewrite everything at once**\nWhere possible, migrate part of an application to a new\ncomponent and split traffic coming into the domain so that some paths are diverted to that\ncomponent. This will typically use nginx in dev, but may be done at the AWS level if\nappropriate (e.g in a load balancer or application gateway).\nThis is challenging, but means that we don't have to do a ""big bang"" release of the new\nversion of the tool. Our aim is to gradually replace existing components with new\nones, which are (hopefully) simpler, future-proofed, more efficient, and don't rely on PHP.\n* **Use Python for new work**\nWe considered golang, but don't have the experience in the team to build applications with it.\nWe felt that learning a new language + frameworks would only reduce our ability to deliver, with\nminimal benefits: our application is not under heavy load and responds in an\nacceptable amount of time, so golang's super efficiency isn't essential.\nWe feel that we could scale horizontally if necessary and have not had any major issues\nwith capacity in the past.\n* **Choose containers or lambdas as appropriate**\nUse a container for components which stay up most of the time, and lambdas for\n""bursty"" applications (e.g. background processes like PDF generation, daily statistics aggregation).\n* **Choose the right lambda for the job**\nUse ""pure"" lambdas where possible. This is only the case where an application has simple dependencies\nwhich don't require unusual native libraries outside the\n[stock AWS Docker images for lambdas](https://gallery.ecr.aws/lambda/python)).\nIf a component is problematic to run as a pure lambda, use a lambda running a Docker image based\non one of the stock AWS Docker images for lambdas.\n* **Choose the right Docker image**\nWhen using Docker images, prefer the following:\n* Images based on AWS Lambda images (if writing a component which will run as a Docker lambda).\n* Images based on Alpine (for other cases).\n* Images based on a non-Alpine foundation like Ubuntu, but only if an Alpine image is not available.\n* **Use Flask and gunicorn**\nUse [Flask](https://flask.palletsprojects.com/) for new Python web apps, fronted by\n[gunicorn](https://gunicorn.org/) for the WSGI implementation.\n* **Use the latest Design System**\nUse the [Government Design System](https://design-system.service.gov.uk/) guidelines for new UI. In\nparticular, use the\n[Land Registry's Python implementation of the design system](https://github.com/LandRegistry/govuk-frontend-jinja),\nwritten as [Jinja2 templates](https://jinja.palletsprojects.com/).\nOur aim should be to utilise it without modification as far as possible, so that we can easily upgrade\nif it is changed by developers at the Land Registry.\n* **Migrate legacy code to PHP 8**\nWhere possible, upgrade PHP applications to PHP 8, when supported by [Laminas](https://getlaminas.org/).\nAt the time of writing, Laminas support for PHP 8 is only partial, so we are stuck with PHP 7 for now,\nas large parts of our stack are implemented on top of Laminas.\n* **Specify new APIs with OpenAPI**\nSpecify new APIs using [OpenAPI](https://swagger.io/specification/). Ideally, use tooling\nwhich enables an API to be automatically built from an OpenAPI specification, binding to\ncode only when necessary, to avoid repetitive boilerplate.\n* **Controlled, incremental releases**\nProvision new infrastructure behind a feature flag wherever possible. This allows us to\nwork on new components, moving them into the live environment as they are ready, but hidden\nfrom the outside world. When ready for delivery, we switch the flag over to make that piece\nof infrastructure live.\n* **Follow good practices for web security**\nBe aware of the [OWASP Top Ten](https://owasp.org/www-project-top-ten/) and code to avoid those\nissues. Use tools like [Talisman](https://github.com/GoogleCloudPlatform/flask-talisman) to\nimprove security.\n* **Be mindful of accessibility**\nConsider accessibility requirements at every step of the design and coding phases. Aim to\ncomply with [WCAG 2.1 Level AA](https://www.w3.org/WAI/WCAG22/quickref/) as a minimum. While the\nDesign System helps a lot with this, always bear accessibility in mind when building workflows\nand custom components it doesn't cover.\n* **Be properly open source**\nMake the code base properly open source. While our code is open, there are still barriers to entry\nfor developers outside the Ministry of Justice, such as the requirement to have access to AWS secrets,\nS3, postcode API, the Government payment gateway and SendGrid for the system to work correctly. We\nwill work towards removing these barriers so that onboarding of new developers (internally and\nexternally) is seamless, and to enable potentially anyone to fully contribute to the project.\n* **Improve test coverage everywhere**\nAs we work on the code, be aware of gaps in testing and plug them as they arise. Don't wait for\nan opportunity to fix everything at once: make refactoring and adding unit tests part of the\nwork on an issue (unless it's going to take longer than working on the issue!).\nWhere a whole category of testing is missing, add it (for example, we\nhave recently implemented the foundations for load testing; see\n[0004-implement-load-testing](./0004-implement-load-testing.md)).\n* **Automate code quality metrics**\nIntroduce tools to lint and scan code as we go, to ensure consistent, easy-to-follow code. See\n[0003-linting-and-scanning](./0003-linting-and-scanning.md)) for a starting point.\n* **Peer review everything**\nAll commits to the code base must go through peer review before merging. No lone wolf developers.\n* **Be pragmatic**\nSee the [pragmatic quick reference](https://www.ccs.neu.edu/home/lieber/courses/csg110/sp08/Pragmatic%20Quick%20Reference.htm)\nfor a summary. These are generally good principles for software engineering.\n"
we-be-best/1-use-both-pointer-and-mouse-events.md,"## Context\n- Originally we wanted to use just PointerEvents API for capturing events because the API covers mouse and pointer inputs.\n- Unfortunately Safari and iOS have very limited support. Safari does currently have an experimental API for it but the movementX and movementY properties are always 0.\n- MovementX/Y is a clean browser only method for determining distance without having to track previous coordinates.\n- Mobile is a secondary concern for this app, but pointer events on desktop Safari is also unsupported :(\n",All pointer event interactions have a mouse event fallback.\n,102,873,"## Context\n- Originally we wanted to use just PointerEvents API for capturing events because the API covers mouse and pointer inputs.\n- Unfortunately Safari and iOS have very limited support. Safari does currently have an experimental API for it but the movementX and movementY properties are always 0.\n- MovementX/Y is a clean browser only method for determining distance without having to track previous coordinates.\n- Mobile is a secondary concern for this app, but pointer events on desktop Safari is also unsupported :(\n

##Decision
All pointer event interactions have a mouse event fallback.\n"
dapr/CLI-001-cli-and-runtime-versioning.md,"## Context\nAs we formally establish Dapr component version, we need to decide if we want to couple CLI versions with runtime versions.\n",* We'll keep CLI versioning and runtime versioning separate.\n* CLI will pull down latest runtime binary during the *init()* command.\n* Version scheme is: *major.minor.revision.build* for both CLI and runtime.\n,28,78,"## Context\nAs we formally establish Dapr component version, we need to decide if we want to couple CLI versions with runtime versions.\n

##Decision
* We'll keep CLI versioning and runtime versioning separate.\n* CLI will pull down latest runtime binary during the *init()* command.\n* Version scheme is: *major.minor.revision.build* for both CLI and runtime.\n"
TOSCAna/0018-cloudfoundry-no-deployment-in-runtime.md,## Problem\nMost of the credentials/information of the environment (like a service address) are only available as soon the application is deployed.\n,* Chosen Alternative: creating scripts\n,29,1745,"## Problem\nMost of the credentials/information of the environment (like a service address) are only available as soon the application is deployed.\n

##Decision
* Chosen Alternative: creating scripts\n"
bfi-discovery/0002-choice-of-auth-provider.md,"## Context and Problem Statement\nIn order to deliver BFI's IIIF Universal Viewer auditing platform, an\nidentity and access management solution must be provisioned which\nsupports user creation and registration, user management, and\nauthentication and authorisation.\n## Decision Drivers\n* Ease of initial deployment and configuration of the solution.\n* Ongoing hosting and maintenance costs of the solution.\n* Availability of core features to satisfy the project requirements with\nno / minimal costs.\n","* Ease of initial deployment and configuration of the solution.\n* Ongoing hosting and maintenance costs of the solution.\n* Availability of core features to satisfy the project requirements with\nno / minimal costs.\nAuth0 is selected as the solution, due to its managed nature requiring\nzero deployment and effort. After an evaluation of features compared\nagainst the project requirements, it has been determined that Auth0's\nfree tier will suffice and as a result there are no ongoing costs\neither.\n### Positive Consequences\n* As a managed solution, there are no initial deployment or ongoing\nhosting / infrastructure costs.\n* Patching and maintenance of the solution is provided by Auth0 at no\nadditional cost / interruption to the customer.\n* It is expected that (at least initially) the free tier will suffice to\nmeet the requirements of the project.\n### Negative Consequences\n* The features and customisations available are limited without\nupgrading to a paid tier:\n* Email templates cannot be customised on the free plan.\n* A custom domain cannot be used to serve the Auth0 tenant.\n* Enterprise integrations (AD, LDAP, etc) are not available.\n* Multifactor authentication is not available.\n* Auth0 hosted UI's can be customised but only to a limited extent.\n* Limit of 7000 monthly active users.\n",96,4857,"## Context and Problem Statement\nIn order to deliver BFI's IIIF Universal Viewer auditing platform, an\nidentity and access management solution must be provisioned which\nsupports user creation and registration, user management, and\nauthentication and authorisation.\n## Decision Drivers\n* Ease of initial deployment and configuration of the solution.\n* Ongoing hosting and maintenance costs of the solution.\n* Availability of core features to satisfy the project requirements with\nno / minimal costs.\n

##Decision
* Ease of initial deployment and configuration of the solution.\n* Ongoing hosting and maintenance costs of the solution.\n* Availability of core features to satisfy the project requirements with\nno / minimal costs.\nAuth0 is selected as the solution, due to its managed nature requiring\nzero deployment and effort. After an evaluation of features compared\nagainst the project requirements, it has been determined that Auth0's\nfree tier will suffice and as a result there are no ongoing costs\neither.\n### Positive Consequences\n* As a managed solution, there are no initial deployment or ongoing\nhosting / infrastructure costs.\n* Patching and maintenance of the solution is provided by Auth0 at no\nadditional cost / interruption to the customer.\n* It is expected that (at least initially) the free tier will suffice to\nmeet the requirements of the project.\n### Negative Consequences\n* The features and customisations available are limited without\nupgrading to a paid tier:\n* Email templates cannot be customised on the free plan.\n* A custom domain cannot be used to serve the Auth0 tenant.\n* Enterprise integrations (AD, LDAP, etc) are not available.\n* Multifactor authentication is not available.\n* Auth0 hosted UI's can be customised but only to a limited extent.\n* Limit of 7000 monthly active users.\n"
digitalrig-metal-aws/0005-use-aws-rds-aurora-mysql-for-database-persistence.md,"## Context\nBookit needs a persistence mechanism.  There are many to choose from that fit an application's needs.  Currently, we believe a SQL/RDBMS approach fits better than NoSQL.  There's not a lot of context to add to that, just a quick poll of the engineers when we kicked off the project.  With that in mind, we wanted something hosted/PaaS.\nGiven we're in AWS, RDS is an obvious choice.  We don't currently have a preference for DB vendor/implementation, but are drawn to open source and free.  MySql and PostgreSql fit that criteria.\nFurther, AWS RDS has their own MySql implementation which provides much better performance and up to the minute backups with no degredation for fractions of a penny/hr more than the standard MySql over RDS.  And while Bookit's usage might not warrant the need for higher performance, there is always a need for high availability and Aurora provides that in a very hands off way.  There is also an Aurora implentation for PostgreSql but at the time of this decision, that is in Preview so we decided to skip it.\n",Use AWS RDS Aurora MySql implemntation for our database persistence mechansim.  Create via Riglet CloudFormation stacks.  Production will run with 1 primary and 1 replica for HA.  Staging & Integration will run with only 1 Primary to keep costs down.\n,238,1740,"## Context\nBookit needs a persistence mechanism.  There are many to choose from that fit an application's needs.  Currently, we believe a SQL/RDBMS approach fits better than NoSQL.  There's not a lot of context to add to that, just a quick poll of the engineers when we kicked off the project.  With that in mind, we wanted something hosted/PaaS.\nGiven we're in AWS, RDS is an obvious choice.  We don't currently have a preference for DB vendor/implementation, but are drawn to open source and free.  MySql and PostgreSql fit that criteria.\nFurther, AWS RDS has their own MySql implementation which provides much better performance and up to the minute backups with no degredation for fractions of a penny/hr more than the standard MySql over RDS.  And while Bookit's usage might not warrant the need for higher performance, there is always a need for high availability and Aurora provides that in a very hands off way.  There is also an Aurora implentation for PostgreSql but at the time of this decision, that is in Preview so we decided to skip it.\n

##Decision
Use AWS RDS Aurora MySql implemntation for our database persistence mechansim.  Create via Riglet CloudFormation stacks.  Production will run with 1 primary and 1 replica for HA.  Staging & Integration will run with only 1 Primary to keep costs down.\n"
origin/0005-simplify-off-chain-storage.md,## Context\nStoring data in hybrid storage (off-chain and on-chain) presented problems with data inconsistency when some data was written off-chain but it's on-chain reference hasn't been committed yet. This would cause some data being corrupted under certain conditions.\n,"We decided to store off-chain data in a way which doesn't reference the ID of any of the entities, but purely stores the precise proof data under the hash of the off-chain data.\n",52,1558,"## Context\nStoring data in hybrid storage (off-chain and on-chain) presented problems with data inconsistency when some data was written off-chain but it's on-chain reference hasn't been committed yet. This would cause some data being corrupted under certain conditions.\n

##Decision
We decided to store off-chain data in a way which doesn't reference the ID of any of the entities, but purely stores the precise proof data under the hash of the off-chain data.\n"
libelektra/capabilities.md,"## Problem\nOnly plugins like `dump` and `quickdump` are able to represent any KeySet\n(as they are designed to do so). Limitations of other storage plugins are\ne.g., that not every structure of configuration is allowed.\nSome of these limitations were documented `infos/status`, others were not.\n","Add `infos/features/storage` to document limitations of storage plugins.\nIdeally, storage plugins should throw an error in `kdbSet` for\nunrepresentable KeySets.\nElektra cannot guarantee that any configuration file format can\nbe mounted anywhere.\nDevelopers, maintainers and administrators are responsible for what\nthey mount. They need to test the setup.\n- [Base Name](base_name.md)\n",66,1318,"## Problem\nOnly plugins like `dump` and `quickdump` are able to represent any KeySet\n(as they are designed to do so). Limitations of other storage plugins are\ne.g., that not every structure of configuration is allowed.\nSome of these limitations were documented `infos/status`, others were not.\n

##Decision
Add `infos/features/storage` to document limitations of storage plugins.\nIdeally, storage plugins should throw an error in `kdbSet` for\nunrepresentable KeySets.\nElektra cannot guarantee that any configuration file format can\nbe mounted anywhere.\nDevelopers, maintainers and administrators are responsible for what\nthey mount. They need to test the setup.\n- [Base Name](base_name.md)\n"
cena/0014-adopt-commmand-query-separation.md,"## Context\n[Command-query separation](https://martinfowler.com/bliki/CommandQuerySeparation.html) states that every method should\neither be a command that performs an action, or a query that returns data to the caller, but not both.\nAdopting command-query separation makes a clear separation of methods that change state from those that don't. So we can\nuse queries with much more confidence, and only be careful with commands orchestration.\nCommands and queries terminology is already used in the `menu-generation` application.\n","Command-query separation will be enforced in the [core hexagon](./0003-adopt-hexagonal-architecture.md), especially in\napplication services.\n",113,647,"## Context\n[Command-query separation](https://martinfowler.com/bliki/CommandQuerySeparation.html) states that every method should\neither be a command that performs an action, or a query that returns data to the caller, but not both.\nAdopting command-query separation makes a clear separation of methods that change state from those that don't. So we can\nuse queries with much more confidence, and only be careful with commands orchestration.\nCommands and queries terminology is already used in the `menu-generation` application.\n

##Decision
Command-query separation will be enforced in the [core hexagon](./0003-adopt-hexagonal-architecture.md), especially in\napplication services.\n"
connecting-to-services/0015-add-info-page.md,"## Context\nWhen debugging it is useful to be sure of what the deployed version of the application is. In addition, when debugging date and time issues (e.g openness) then it is useful to know the time in UTC from the point of view of the application.\n",The application will provide an info page.\n,55,2198,"## Context\nWhen debugging it is useful to be sure of what the deployed version of the application is. In addition, when debugging date and time issues (e.g openness) then it is useful to know the time in UTC from the point of view of the application.\n

##Decision
The application will provide an info page.\n"
ftd-scratch3-offline/0004-use-multiple-projects.md,## Context\nThe software could be developed in one big (Gradle) project.\nThis would make integration easier.\nAt the same time this would make re-use of the code outside of this project harder.\nOne big project would probably lead to worse code since there would not be the need to have defined API boundaries.\n,We will try to modularize the software and will use multiple projects to achieve this goal.\n,67,2626,"## Context\nThe software could be developed in one big (Gradle) project.\nThis would make integration easier.\nAt the same time this would make re-use of the code outside of this project harder.\nOne big project would probably lead to worse code since there would not be the need to have defined API boundaries.\n

##Decision
We will try to modularize the software and will use multiple projects to achieve this goal.\n"
openchs-adr/0012-create-a-generic-relationship-framework-to-link-between-mother-and-child.md,"## Context\nDuring a delivery for a mother in the mother program, we will need to create one or more new children. Filling in details during delivery and during PNC visits will need switching between them easy.\nAt the same time, we are also thinking of creating the concept of a family. Here, the individuals of a family will be linked to the head of the household through a relationship.\nWe need the modeling of a relationship to be a generic structure that can support both these use cases.\n","Create an option to map relationships between individuals, with relationship being a concept orthogonal to families. Relationships between individuals can be anything (family relationships, or even relationships to ASHA worker etc if required).\nRelationships will be two-way with different values between the two. We will not build (atleast for now) the ability to automatically deduce transitive relationships.\n",103,2829,"## Context\nDuring a delivery for a mother in the mother program, we will need to create one or more new children. Filling in details during delivery and during PNC visits will need switching between them easy.\nAt the same time, we are also thinking of creating the concept of a family. Here, the individuals of a family will be linked to the head of the household through a relationship.\nWe need the modeling of a relationship to be a generic structure that can support both these use cases.\n

##Decision
Create an option to map relationships between individuals, with relationship being a concept orthogonal to families. Relationships between individuals can be anything (family relationships, or even relationships to ASHA worker etc if required).\nRelationships will be two-way with different values between the two. We will not build (atleast for now) the ability to automatically deduce transitive relationships.\n"
link_platform/0016-use-devise-for-admin-authentication.md,## Context\nWe need a some way to authenticate and manage Link Platform Adminstrators.  Administrators will need to log in to their Link Instances to manage data and configuration.\n,[Devise](https://github.com/plataformatec/devise#starting-with-rails) is a very popular gem that integrates well with ActiveRecord.  It provides support for [a ridiculous amount of authentication providers](https://github.com/omniauth/omniauth/wiki/List-of-Strategies) through Omniauth as well as a variety of features such as password reset.\n,37,5024,"## Context\nWe need a some way to authenticate and manage Link Platform Adminstrators.  Administrators will need to log in to their Link Instances to manage data and configuration.\n

##Decision
[Devise](https://github.com/plataformatec/devise#starting-with-rails) is a very popular gem that integrates well with ActiveRecord.  It provides support for [a ridiculous amount of authentication providers](https://github.com/omniauth/omniauth/wiki/List-of-Strategies) through Omniauth as well as a variety of features such as password reset.\n"
paas-csls-splunk-broker/ADR007-paas-taking-ownership.md,"## Context\nThe RE Autom8 team originally wrote and maintained this broker, in collaboration with Cyber Security. It is configured as a service broker in the Ireland and London regions of GOV.UK PaaS, and enabled for a few GDS-internal tenants. The code lives in a subdirectory of the `alphagov/tech-ops` repository, and the pipeline which builds and deploys it lives in the Tech Ops multi-tenant Concourse.\n","The GOV.UK PaaS team decided that they were happy to take ownership of the broker, because it requires knowledge of the platform to maintain, and they maintain all the other brokers on the platform.\n",94,3291,"## Context\nThe RE Autom8 team originally wrote and maintained this broker, in collaboration with Cyber Security. It is configured as a service broker in the Ireland and London regions of GOV.UK PaaS, and enabled for a few GDS-internal tenants. The code lives in a subdirectory of the `alphagov/tech-ops` repository, and the pipeline which builds and deploys it lives in the Tech Ops multi-tenant Concourse.\n

##Decision
The GOV.UK PaaS team decided that they were happy to take ownership of the broker, because it requires knowledge of the platform to maintain, and they maintain all the other brokers on the platform.\n"
nr-arch/2020-03-12-ARCH-record-architecture-decisions-simple-template.md,"## Context\nThe issue motivating this decision, and any context that influences or constrains the decision.\n",The change that we're proposing or have agreed to implement.\n,21,4804,"## Context\nThe issue motivating this decision, and any context that influences or constrains the decision.\n

##Decision
The change that we're proposing or have agreed to implement.\n"
kafkarator/0004-only-for-aiven.md,"## Context\nNAV currently has Kafka running on-premise, but have plans to move everything to the cloud.\nKafkarator will create an integration between applications and Kafka, removing some manual work and putting more configuration in code.\nThe plan is hosted Kafka from Aiven, eliminating the need for maintaining our own Kafka cluster on-premise.\n","Kafkarator will only deal with the future solution using Aiven, and not work for on-premise Kafka.\n",70,2837,"## Context\nNAV currently has Kafka running on-premise, but have plans to move everything to the cloud.\nKafkarator will create an integration between applications and Kafka, removing some manual work and putting more configuration in code.\nThe plan is hosted Kafka from Aiven, eliminating the need for maintaining our own Kafka cluster on-premise.\n

##Decision
Kafkarator will only deal with the future solution using Aiven, and not work for on-premise Kafka.\n"
digital-paper-edit-firebase/2020-03-12-integrating-with-new-pstt-module.md,## Context and Problem Statement\nWe want to connect to Newslabs' shared STT service (named Newslabs PSTT).\n,"Ashley is building a service that will have an API Gateway and an S3 bucket. Uploading to the S3 bucket will trigger the STT event. We will be uploading to a bucket, as the previous architecture, with limitations that will define how we will communicate with Newslabs' PSTT service.\nThere will only be one shared bucket per environment for services using this service. The name of the bucket is ""newslabs-stt-media-to-transcribe"" and ""newslabs-stt-media-to-transcribe-test"".\n### Limitations\n- Newslabs pstt will only handle audio files (`mp3`, `mp4`, `wav`, `flac`)\n- the client (dpe) need to ensure they upload audio not video\n- do not use the eTag as the reference, use object key instead: `<service_name>/<object_key>.<ext>`\n- assume that the client is sending a unique object key\n### Example\n#### Upload\nWhen uploading a file with Object Key: `280612.mp3`, the Object Key should be prepended with the service name: `dpe/280612.mp3`\n#### Status\nThe endpoint for requesting the status of a transcription uses [this lambda](https://github.com/bbc/newslabs-stt/tree/master/newslabs-stt-check-transcription), which returns the transcription status.\nThere is an example response in the README.\nMake a request to an API Gateway endpoint (please ask) with something like this in the request body:\n```json\n{\n""objectKey"": ""dpe/uuid.ext""\n}\n```\n",28,5243,"## Context and Problem Statement\nWe want to connect to Newslabs' shared STT service (named Newslabs PSTT).\n

##Decision
Ashley is building a service that will have an API Gateway and an S3 bucket. Uploading to the S3 bucket will trigger the STT event. We will be uploading to a bucket, as the previous architecture, with limitations that will define how we will communicate with Newslabs' PSTT service.\nThere will only be one shared bucket per environment for services using this service. The name of the bucket is ""newslabs-stt-media-to-transcribe"" and ""newslabs-stt-media-to-transcribe-test"".\n### Limitations\n- Newslabs pstt will only handle audio files (`mp3`, `mp4`, `wav`, `flac`)\n- the client (dpe) need to ensure they upload audio not video\n- do not use the eTag as the reference, use object key instead: `<service_name>/<object_key>.<ext>`\n- assume that the client is sending a unique object key\n### Example\n#### Upload\nWhen uploading a file with Object Key: `280612.mp3`, the Object Key should be prepended with the service name: `dpe/280612.mp3`\n#### Status\nThe endpoint for requesting the status of a transcription uses [this lambda](https://github.com/bbc/newslabs-stt/tree/master/newslabs-stt-check-transcription), which returns the transcription status.\nThere is an example response in the README.\nMake a request to an API Gateway endpoint (please ask) with something like this in the request body:\n```json\n{\n""objectKey"": ""dpe/uuid.ext""\n}\n```\n"
PIMS/database.md,"## Context\nPIMS requires a database to store all property information.\nThe data is relational, requiring constraints and must run within a Linux docker container on OpenShift.\nAdditionally it must be supported by Entity Framework Core 3.1.\n","Originally the database generated for the SWU was with PostgreSQL, after further consideration it made more sense to tightly couple both MS-SQL with .NET Core.\nThis will give us better performance and tighter integration with Entity Framework Core.\nIt was decided to create a Linux docker container to host the MS-SQL 2019 database.\n",49,3966,"## Context\nPIMS requires a database to store all property information.\nThe data is relational, requiring constraints and must run within a Linux docker container on OpenShift.\nAdditionally it must be supported by Entity Framework Core 3.1.\n

##Decision
Originally the database generated for the SWU was with PostgreSQL, after further consideration it made more sense to tightly couple both MS-SQL with .NET Core.\nThis will give us better performance and tighter integration with Entity Framework Core.\nIt was decided to create a Linux docker container to host the MS-SQL 2019 database.\n"
uniffi-rs/0004-only-threadsafe-interfaces.md,"## Context and Problem Statement\n[ADR-0003](0003-threadsafe-interfaces.md) introduced support for ""thread-safe\ninterfaces"" - possibly leading to the impression that there is such a thing as\nnon-threadsafe interfaces and confusion about exactly what the attribute means.\nHowever, the entire concept of non-threadsafe interfaces is a misconception -\nthe Rust compiler insists that everything wrapped by uniffi is thread-safe -\nthe only question is who manages this thread-safety. Interfaces which are not\nmarked as thread-safe cause uniffi to wrap the interface in a mutex which is\nhidden in the generated code and therefore not obvious to the casual reader.\nThe `[Threadsafe]` marker acts as a way for the component author to opt out of\nthe overhead and blocking behaviour of this mutex, at the cost of opting in to\nmanaging their own locking internally. This ADR proposes that uniffi forces\ncomponent authors to explicitly manage that locking in all cases - or to put\nthis in Rust terms, that all structs supported by uniffi must already be\n`Send+Sync`\nNote that this ADR will hence-forth use the term `Send+Sync` instead of\n""Threadsafe"" because it more accurately describes the actual intent and avoids\nany misunderstandings that might be caused by using the somewhat broad and\ngeneric ""Threadsafe"".\n## Decision Drivers\n* Supporting non-`Send+Sync` structs means uniffi must add hidden locking to make\nthem `Send+Sync`. We consider this a ""foot-gun"" as it may lead to accidentally\nhaving method calls unexpectedly block for long periods, such as\n[this Fenix bug](https://github.com/mozilla-mobile/fenix/issues/17086)\n(with more details available in [this JIRA ticket](https://jira.mozilla.com/browse/SDK-157)).\n* Supporting such structs will hinder uniffi growing in directions that we've\nfound are desired in practice, such as allowing structs to use [alternative\nmethod receivers](https://github.com/mozilla/uniffi-rs/issues/417) or to\n[pass interface references over the FFI](https://github.com/mozilla/uniffi-rs/issues/419).\n","* Supporting non-`Send+Sync` structs means uniffi must add hidden locking to make\nthem `Send+Sync`. We consider this a ""foot-gun"" as it may lead to accidentally\nhaving method calls unexpectedly block for long periods, such as\n[this Fenix bug](https://github.com/mozilla-mobile/fenix/issues/17086)\n(with more details available in [this JIRA ticket](https://jira.mozilla.com/browse/SDK-157)).\n* Supporting such structs will hinder uniffi growing in directions that we've\nfound are desired in practice, such as allowing structs to use [alternative\nmethod receivers](https://github.com/mozilla/uniffi-rs/issues/417) or to\n[pass interface references over the FFI](https://github.com/mozilla/uniffi-rs/issues/419).\nChosen option:\n* **[Option 2] Immediately deprecate, then remove entirely, support for\nnon-`Send+Sync` interfaces.**\nThis decision was taken because our real world experience tells us that\nnon-`Send+Sync` interfaces are only useful in toy or example applications (eg,\nthe nimbus and autofill projects didn't get very far before needing these\ncapabilities), so the extra ongoing work in supporting these interfaces cannot\nbe justified.\n### Positive Consequences\n* The locking in all uniffi supported components will be more easily\ndiscoverable - it will be in hand-written rust code and not hidden inside\ngenerated code. This is a benefit to the developers of the uniffi supported\ncomponent rather than to the consumers of it; while we are considering other\nfeatures to help communicate the lock semantics to such consumers, that is\nbeyond the scope of this ADR.\n* Opens the door to enhancements that would be impossible for non-`Send+Sync`\ninterfaces, and simpler to implement for `Send+Sync` interfaces if support\nfor non-`Send+Sync` interfaces did not exist.\n* Simpler implementation and documentation.\n### Negative Consequences\n* All consumers (both inside Mozilla and external) will need to change their\ninterfaces to be `Send+Sync`. As an example of what this entails,\nsee [this commit](https://github.com/mozilla/uniffi-rs/commit/454dfff6aa560dffad980a9258853108a44d5985)\nwhich converts the `todolist` example.\n* Simple, toy applications may be more difficult to wrap - consumers will not\nbe able to defer decisions about `Send+Sync` support and will instead need to\nimplement simple locking as demonstrated in [this commit](\nhttps://github.com/mozilla/uniffi-rs/commit/454dfff6aa560dffad980a9258853108a44d5985).\n* Existing applications that are yet to consider how to make their\nimplementations `Send+Sync` cannot be wrapped until they have.\n* The examples which aren't currently marked with the `[Threadsafe]` attribute\nwill become more complex as they will all need to implement and explain how\nthey achieve being `Send+Sync`.\n* The perception that its more difficult to wrap interfaces will lead to less\nadoption of the tool.\n",468,4952,"## Context and Problem Statement\n[ADR-0003](0003-threadsafe-interfaces.md) introduced support for ""thread-safe\ninterfaces"" - possibly leading to the impression that there is such a thing as\nnon-threadsafe interfaces and confusion about exactly what the attribute means.\nHowever, the entire concept of non-threadsafe interfaces is a misconception -\nthe Rust compiler insists that everything wrapped by uniffi is thread-safe -\nthe only question is who manages this thread-safety. Interfaces which are not\nmarked as thread-safe cause uniffi to wrap the interface in a mutex which is\nhidden in the generated code and therefore not obvious to the casual reader.\nThe `[Threadsafe]` marker acts as a way for the component author to opt out of\nthe overhead and blocking behaviour of this mutex, at the cost of opting in to\nmanaging their own locking internally. This ADR proposes that uniffi forces\ncomponent authors to explicitly manage that locking in all cases - or to put\nthis in Rust terms, that all structs supported by uniffi must already be\n`Send+Sync`\nNote that this ADR will hence-forth use the term `Send+Sync` instead of\n""Threadsafe"" because it more accurately describes the actual intent and avoids\nany misunderstandings that might be caused by using the somewhat broad and\ngeneric ""Threadsafe"".\n## Decision Drivers\n* Supporting non-`Send+Sync` structs means uniffi must add hidden locking to make\nthem `Send+Sync`. We consider this a ""foot-gun"" as it may lead to accidentally\nhaving method calls unexpectedly block for long periods, such as\n[this Fenix bug](https://github.com/mozilla-mobile/fenix/issues/17086)\n(with more details available in [this JIRA ticket](https://jira.mozilla.com/browse/SDK-157)).\n* Supporting such structs will hinder uniffi growing in directions that we've\nfound are desired in practice, such as allowing structs to use [alternative\nmethod receivers](https://github.com/mozilla/uniffi-rs/issues/417) or to\n[pass interface references over the FFI](https://github.com/mozilla/uniffi-rs/issues/419).\n

##Decision
* Supporting non-`Send+Sync` structs means uniffi must add hidden locking to make\nthem `Send+Sync`. We consider this a ""foot-gun"" as it may lead to accidentally\nhaving method calls unexpectedly block for long periods, such as\n[this Fenix bug](https://github.com/mozilla-mobile/fenix/issues/17086)\n(with more details available in [this JIRA ticket](https://jira.mozilla.com/browse/SDK-157)).\n* Supporting such structs will hinder uniffi growing in directions that we've\nfound are desired in practice, such as allowing structs to use [alternative\nmethod receivers](https://github.com/mozilla/uniffi-rs/issues/417) or to\n[pass interface references over the FFI](https://github.com/mozilla/uniffi-rs/issues/419).\nChosen option:\n* **[Option 2] Immediately deprecate, then remove entirely, support for\nnon-`Send+Sync` interfaces.**\nThis decision was taken because our real world experience tells us that\nnon-`Send+Sync` interfaces are only useful in toy or example applications (eg,\nthe nimbus and autofill projects didn't get very far before needing these\ncapabilities), so the extra ongoing work in supporting these interfaces cannot\nbe justified.\n### Positive Consequences\n* The locking in all uniffi supported components will be more easily\ndiscoverable - it will be in hand-written rust code and not hidden inside\ngenerated code. This is a benefit to the developers of the uniffi supported\ncomponent rather than to the consumers of it; while we are considering other\nfeatures to help communicate the lock semantics to such consumers, that is\nbeyond the scope of this ADR.\n* Opens the door to enhancements that would be impossible for non-`Send+Sync`\ninterfaces, and simpler to implement for `Send+Sync` interfaces if support\nfor non-`Send+Sync` interfaces did not exist.\n* Simpler implementation and documentation.\n### Negative Consequences\n* All consumers (both inside Mozilla and external) will need to change their\ninterfaces to be `Send+Sync`. As an example of what this entails,\nsee [this commit](https://github.com/mozilla/uniffi-rs/commit/454dfff6aa560dffad980a9258853108a44d5985)\nwhich converts the `todolist` example.\n* Simple, toy applications may be more difficult to wrap - consumers will not\nbe able to defer decisions about `Send+Sync` support and will instead need to\nimplement simple locking as demonstrated in [this commit](\nhttps://github.com/mozilla/uniffi-rs/commit/454dfff6aa560dffad980a9258853108a44d5985).\n* Existing applications that are yet to consider how to make their\nimplementations `Send+Sync` cannot be wrapped until they have.\n* The examples which aren't currently marked with the `[Threadsafe]` attribute\nwill become more complex as they will all need to implement and explain how\nthey achieve being `Send+Sync`.\n* The perception that its more difficult to wrap interfaces will lead to less\nadoption of the tool.\n"
disco-poc-vue/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as [described by Michael Nygard](https://cognitect.com/blog/2011/11/15/documenting-architecture-decisions).\n",16,165,"## Context\nWe need to record the architectural decisions made on this project.\n

##Decision
We will use Architecture Decision Records, as [described by Michael Nygard](https://cognitect.com/blog/2011/11/15/documenting-architecture-decisions).\n"
tracking-consent-frontend/0007-use-eslint-for-linting-and-formatting.md,## Context and Problem Statement\nWe found that without a standardised format our javascript files ended up with different\nformats in different files or even multiple formats in the same file.  We also found that\nour IDEs had different configurations which meant that using an autoformat tool would give\ndifferent results when each of us do it.\n## Decision Drivers\n* We wanted to spend less time doing manual formatting\n* We wanted to spend less time undoing autoformatting which had been applied to unchanged lines\n* We wanted to see easily which lines had actually changed when reviewing PRs\n* We wanted to avoid discussions about individual's preferences for particular\n,"* We wanted to spend less time doing manual formatting\n* We wanted to spend less time undoing autoformatting which had been applied to unchanged lines\n* We wanted to see easily which lines had actually changed when reviewing PRs\n* We wanted to avoid discussions about individual's preferences for particular\nChosen option: ESLint + Airbnb\nWe decided to use ESLint with Airbnb because the Airbnb approach is [well documented](https://github.com/airbnb/javascript)\nand adopting this meant we wouldn't have to make each decision ourselves.  We decided not to use Prettier as well, there\nwas a discussion about the possibility of conflicting format changes between the two tools - we encountered one of these,\nit was overcome with config updates.\nESLint is compatible with both Javascript and Typescript projects which made it seem particularly suitable for us as we\nchose to use Typescript for Tracking Consent but we usually use Javascript.\nEach of the current team members use JetBrains IDEs and that has a built-in plugin which supports auto formatting on save,\nthis is also a common feature which most IDEs have.\nWe decided to add pre-commit and pre-push hooks which ensure that the style rules have been met but do not mutate code.\nWe discussed the option of formatting the code on commit but we felt that this introduced an uncertainty about what we\nwere committing.\n### Positive Consequences\n* Code style within a file will be consistent\n* Files within a project will be consistent with each other\n* When widely applied this Javascript/Typescript files between projects will be consistent\n* IDE will reformat for us which takes away the need for us to do it ourselves\n* Code reviews will focus less on code format and more on the details of the changes made\n* Builds will fail if format isn't followed which guarentees consistency\n### Negative Consequences\n* Builds will fail if format isn't followed which can disrupt focus/workflow (mitigated by pre-commit hooks)\n",133,3521,"## Context and Problem Statement\nWe found that without a standardised format our javascript files ended up with different\nformats in different files or even multiple formats in the same file.  We also found that\nour IDEs had different configurations which meant that using an autoformat tool would give\ndifferent results when each of us do it.\n## Decision Drivers\n* We wanted to spend less time doing manual formatting\n* We wanted to spend less time undoing autoformatting which had been applied to unchanged lines\n* We wanted to see easily which lines had actually changed when reviewing PRs\n* We wanted to avoid discussions about individual's preferences for particular\n

##Decision
* We wanted to spend less time doing manual formatting\n* We wanted to spend less time undoing autoformatting which had been applied to unchanged lines\n* We wanted to see easily which lines had actually changed when reviewing PRs\n* We wanted to avoid discussions about individual's preferences for particular\nChosen option: ESLint + Airbnb\nWe decided to use ESLint with Airbnb because the Airbnb approach is [well documented](https://github.com/airbnb/javascript)\nand adopting this meant we wouldn't have to make each decision ourselves.  We decided not to use Prettier as well, there\nwas a discussion about the possibility of conflicting format changes between the two tools - we encountered one of these,\nit was overcome with config updates.\nESLint is compatible with both Javascript and Typescript projects which made it seem particularly suitable for us as we\nchose to use Typescript for Tracking Consent but we usually use Javascript.\nEach of the current team members use JetBrains IDEs and that has a built-in plugin which supports auto formatting on save,\nthis is also a common feature which most IDEs have.\nWe decided to add pre-commit and pre-push hooks which ensure that the style rules have been met but do not mutate code.\nWe discussed the option of formatting the code on commit but we felt that this introduced an uncertainty about what we\nwere committing.\n### Positive Consequences\n* Code style within a file will be consistent\n* Files within a project will be consistent with each other\n* When widely applied this Javascript/Typescript files between projects will be consistent\n* IDE will reformat for us which takes away the need for us to do it ourselves\n* Code reviews will focus less on code format and more on the details of the changes made\n* Builds will fail if format isn't followed which guarentees consistency\n### Negative Consequences\n* Builds will fail if format isn't followed which can disrupt focus/workflow (mitigated by pre-commit hooks)\n"
dl4j-dev-tools/0006-op_specific_enums.md,## Context\nSome ops have an ordinal parameter which switches between a few possible modes. Giving those modes a proper name\nmakes usage and documentation easier.\n,"We allow `Arg` sections to have an `ENUM` data type and add a `possibleValues` property to define the possible values\nfor this arg. The ordinal number of the enum is the same as its position within the `possibleValues` list starting from\n`0`.\nA runtime check on op construction, will ensure that each enum arg has one or more possible values, and that default\nvalues match one of the possible values (if applicable).\nOn code generation, an appropriate representation of this enum will be generated in the target language. The name of\nthe generated enum will be derived from the name of the arg.\n### Example\n```kotlin\nArg(ENUM, ""padMode""){\npossibleValues = listOf(""CONSTANT"", ""REFLECT"", ""SYMMETRIC"")\ndescription = ""padding mode""\n}\n```\n",31,994,"## Context\nSome ops have an ordinal parameter which switches between a few possible modes. Giving those modes a proper name\nmakes usage and documentation easier.\n

##Decision
We allow `Arg` sections to have an `ENUM` data type and add a `possibleValues` property to define the possible values\nfor this arg. The ordinal number of the enum is the same as its position within the `possibleValues` list starting from\n`0`.\nA runtime check on op construction, will ensure that each enum arg has one or more possible values, and that default\nvalues match one of the possible values (if applicable).\nOn code generation, an appropriate representation of this enum will be generated in the target language. The name of\nthe generated enum will be derived from the name of the arg.\n### Example\n```kotlin\nArg(ENUM, ""padMode""){\npossibleValues = listOf(""CONSTANT"", ""REFLECT"", ""SYMMETRIC"")\ndescription = ""padding mode""\n}\n```\n"
alfa/adr-003.md,"## Context\nHaving decided on [ADR 2](adr-002.md), we foresee that the Alfa code base will be both significantly larger and more complex than the code base of our proprietary engine. This is due to the fact that we will have to implement a great deal of APIs that we have previously relied on the browser implementations of. Coupled with the fact that the most common type of bug we have encountered in the past has been stray `undefined` or `null` values and APIs receiving incorrect parameters, plain JavaScript, even if covered by tests, is simply not an option moving forward. We will either need tooling that can sanity check our JavaScript or move to a language with a proper type system that can enforce API contracts.\nHowever, given that browsers are still part of the equation, Alfa must be able to also run in a browser. This way, we ensure that we can implement tools such as our Chrome extension based on Alfa.\n","We will use [TypeScript](https://github.com/Microsoft/TypeScript) for implementing all of Alfa. Being a superset of JavaScript, TypeScript has a low learning curve for people already familiar with JavaScript while providing a solid type system. We will enforce API contracts through generation of [declaration files](https://www.typescriptlang.org/docs/handbook/declaration-files/introduction.html) which will dictate the API surface that consumers can access. To the extent possible, we will keep a strict TypeScript configuration in order to catch as many issues as possible at compile time. In particular, this entails strict `undefined` and `null` checking in order to get rid of a previously common type of bug.\n",195,3153,"## Context\nHaving decided on [ADR 2](adr-002.md), we foresee that the Alfa code base will be both significantly larger and more complex than the code base of our proprietary engine. This is due to the fact that we will have to implement a great deal of APIs that we have previously relied on the browser implementations of. Coupled with the fact that the most common type of bug we have encountered in the past has been stray `undefined` or `null` values and APIs receiving incorrect parameters, plain JavaScript, even if covered by tests, is simply not an option moving forward. We will either need tooling that can sanity check our JavaScript or move to a language with a proper type system that can enforce API contracts.\nHowever, given that browsers are still part of the equation, Alfa must be able to also run in a browser. This way, we ensure that we can implement tools such as our Chrome extension based on Alfa.\n

##Decision
We will use [TypeScript](https://github.com/Microsoft/TypeScript) for implementing all of Alfa. Being a superset of JavaScript, TypeScript has a low learning curve for people already familiar with JavaScript while providing a solid type system. We will enforce API contracts through generation of [declaration files](https://www.typescriptlang.org/docs/handbook/declaration-files/introduction.html) which will dictate the API surface that consumers can access. To the extent possible, we will keep a strict TypeScript configuration in order to catch as many issues as possible at compile time. In particular, this entails strict `undefined` and `null` checking in order to get rid of a previously common type of bug.\n"
jabref/0015-support-an-abstract-query-syntax-for-query-conversion.md,"## Context and Problem Statement\nAll libraries use their own query syntax for advanced search options. To increase usability, users should be able to formulate their (abstract) search queries in a query syntax that can be mapped to the library specific search queries. To achieve this, the query has to be parsed into an AST.\nWhich query syntax should be used for the abstract queries?\nWhich features should the syntax support?\n","Chosen option: ""Use a syntax that is derived of the lucene query syntax"", because only option that is already known, and easy to implement.\nFurthermore parsers for lucene already exist and are tested.\nFor simplicity, and lack of universal capabilities across fetchers, only basic query features and therefor syntax is supported:\n* All terms in the query are whitespace separated and will be ANDed\n* Default and certain fielded terms are supported\n* Fielded Terms:\n* `author`\n* `title`\n* `journal`\n* `year` (for single year)\n* `year-range` (for range e.g. `year-range:2012-2015`)\n* The `journal`, `year`, and `year-range` fields should only be populated once in each query\n* The `year` and `year-range` fields are mutually exclusive\n* Example:\n* `author:""Igor Steinmacher"" author:""Christoph Treude"" year:2017` will be converted to\n* `author:""Igor Steinmacher"" AND author:""Christoph Treude"" AND year:2017`\nThe supported syntax can be expressed in EBNF as follows:\nQuery := {Clause} \\nClause:= \[Field\] Term \\nField := author: | title: | journal: | year: | year-range: | default:\\nTerm  := Word | Phrase \\nWord can be derived to any series of non-whitespace characters.\nPhrases are multiple words wrapped in quotes and may contain white-space characters within the quotes.\\nNote: Even though this EBNF syntactically allows the creation of queries with year and year-range fields,\nsuch a query does not make sense semantically and therefore will not be executed.\n### Positive Consequences\n* Already tested\n* Well known\n* Easy to implement\n* Can use an existing parser\n",84,4738,"## Context and Problem Statement\nAll libraries use their own query syntax for advanced search options. To increase usability, users should be able to formulate their (abstract) search queries in a query syntax that can be mapped to the library specific search queries. To achieve this, the query has to be parsed into an AST.\nWhich query syntax should be used for the abstract queries?\nWhich features should the syntax support?\n

##Decision
Chosen option: ""Use a syntax that is derived of the lucene query syntax"", because only option that is already known, and easy to implement.\nFurthermore parsers for lucene already exist and are tested.\nFor simplicity, and lack of universal capabilities across fetchers, only basic query features and therefor syntax is supported:\n* All terms in the query are whitespace separated and will be ANDed\n* Default and certain fielded terms are supported\n* Fielded Terms:\n* `author`\n* `title`\n* `journal`\n* `year` (for single year)\n* `year-range` (for range e.g. `year-range:2012-2015`)\n* The `journal`, `year`, and `year-range` fields should only be populated once in each query\n* The `year` and `year-range` fields are mutually exclusive\n* Example:\n* `author:""Igor Steinmacher"" author:""Christoph Treude"" year:2017` will be converted to\n* `author:""Igor Steinmacher"" AND author:""Christoph Treude"" AND year:2017`\nThe supported syntax can be expressed in EBNF as follows:\nQuery := {Clause} \\nClause:= \[Field\] Term \\nField := author: | title: | journal: | year: | year-range: | default:\\nTerm  := Word | Phrase \\nWord can be derived to any series of non-whitespace characters.\nPhrases are multiple words wrapped in quotes and may contain white-space characters within the quotes.\\nNote: Even though this EBNF syntactically allows the creation of queries with year and year-range fields,\nsuch a query does not make sense semantically and therefore will not be executed.\n### Positive Consequences\n* Already tested\n* Well known\n* Easy to implement\n* Can use an existing parser\n"
tdr-dev-documentation/0015-govuk-notify-staging-configuration.md,"## Context\nGovUK Notify is used as the email provider for Keycloak: [0012 Keycloak Email Provider](0012-keycloak-email-provider.md)\nIt was decided to use a separate GovUK Notify service for each TDR environment as GovUK Notify does not have the concept of environments: [0014 GovUK Notify Multi-environment Configuration](0014-govuk-notify-multi-environment-configuration.md)\nGovUK services have a ""trial mode"", and a ""live mode"".\nThe trial mode has limits placed on who can receive emails, and the number of emails that can be sent.\n","The ""staging"" GovUK Notify service is to be live to allow for user testing on this environment.\nAlso, it will mirror the TDR production environment as closely as possible.\n",126,1784,"## Context\nGovUK Notify is used as the email provider for Keycloak: [0012 Keycloak Email Provider](0012-keycloak-email-provider.md)\nIt was decided to use a separate GovUK Notify service for each TDR environment as GovUK Notify does not have the concept of environments: [0014 GovUK Notify Multi-environment Configuration](0014-govuk-notify-multi-environment-configuration.md)\nGovUK services have a ""trial mode"", and a ""live mode"".\nThe trial mode has limits placed on who can receive emails, and the number of emails that can be sent.\n

##Decision
The ""staging"" GovUK Notify service is to be live to allow for user testing on this environment.\nAlso, it will mirror the TDR production environment as closely as possible.\n"
Head-Start-TTADP/0011-monitoring.md,## Context\nTTA Smart Hub requires a continuous monitoring solution to ensure uptime and error resolution.\n,"TTA Smart Hub will utilize New Relic for Monitoring.\n* Integrates easily with front end and backend code\n* Track performance metrics and errors\n* FedRAMP approved\n* Handles alerting both by itself, and via integrations with more flexible alerting platforms as we grow.\n",20,1171,"## Context\nTTA Smart Hub requires a continuous monitoring solution to ensure uptime and error resolution.\n

##Decision
TTA Smart Hub will utilize New Relic for Monitoring.\n* Integrates easily with front end and backend code\n* Track performance metrics and errors\n* FedRAMP approved\n* Handles alerting both by itself, and via integrations with more flexible alerting platforms as we grow.\n"
hmpps-interventions-ui/0010-use-stylelint-for-linting-styles.md,"## Context\nWe want to enforce consistency in our code, and catch as many errors\nautomatically as we are able to. Linting the code is good practice to achieve\nthese aims. [Stylelint](https://stylelint.io/) is one of the more popular CSS\nlinters with support for SASS, and is easily configurable for our purposes.\n",We will check SASS syntax using Stylelint.\nWe will use the recommended configuration for plugins where possible.\nWe will use Stylelint to automatically fix linting errors in a pre-commit hook.\n,75,458,"## Context\nWe want to enforce consistency in our code, and catch as many errors\nautomatically as we are able to. Linting the code is good practice to achieve\nthese aims. [Stylelint](https://stylelint.io/) is one of the more popular CSS\nlinters with support for SASS, and is easily configurable for our purposes.\n

##Decision
We will check SASS syntax using Stylelint.\nWe will use the recommended configuration for plugins where possible.\nWe will use Stylelint to automatically fix linting errors in a pre-commit hook.\n"
scholarsphere/0009-acl-actor-permissions.md,"## Context\nPermissions on works and collections can come from two sources: 1) the person who authored the resource, such as the\ndepositor or the proxy depositor; and 2) access controls (ACLs) that grant permissions based on user or group identity.\nWhen determining who has access to a given resource, both these sources may need to be consulted.\n","Access controls and depositor or proxy depositor rights are independent from one another.\nAccess controls should not include permissions granted by the Actor-to-resource arrangement, such as edit rights of the\ndepositor. They are a separate form of permission structure and therefore independent of one another. Likewise,\npermissions that come from a depositor should have no bearing on what access controls may be applied to a resource.\n",79,4554,"## Context\nPermissions on works and collections can come from two sources: 1) the person who authored the resource, such as the\ndepositor or the proxy depositor; and 2) access controls (ACLs) that grant permissions based on user or group identity.\nWhen determining who has access to a given resource, both these sources may need to be consulted.\n

##Decision
Access controls and depositor or proxy depositor rights are independent from one another.\nAccess controls should not include permissions granted by the Actor-to-resource arrangement, such as edit rights of the\ndepositor. They are a separate form of permission structure and therefore independent of one another. Likewise,\npermissions that come from a depositor should have no bearing on what access controls may be applied to a resource.\n"
beis-report-official-development-assistance/0030-run-data-migrations-manually.md,"## Context\nThe Data Migrate gem has caused us a number of issues in the past, it runs\nsilently as part of a deploy, and this can result in surprising errors\nduring a deploy. We've also had issues with the gem itself - most recently\na bug in a new version causing strange errors in deployment.\n","With this in mind, we've decided to drop the use of the Data Migrate gem,\nand instead run any data migrations manually. We have easy access to the\nconsole via GOV.UK PaaS, so this is relatively painless.\nWe have added a generator which creates an empty Ruby file with a timestamp and the name of the migration as the filename. Once the code is\ndeployed, we run the migration on the production server.\nAs the code is plain old Ruby, it's also easier for us to write tests, if\nwe think it's worth doing (for example, the migrations code is suitably\ncomplex).\n",69,2396,"## Context\nThe Data Migrate gem has caused us a number of issues in the past, it runs\nsilently as part of a deploy, and this can result in surprising errors\nduring a deploy. We've also had issues with the gem itself - most recently\na bug in a new version causing strange errors in deployment.\n

##Decision
With this in mind, we've decided to drop the use of the Data Migrate gem,\nand instead run any data migrations manually. We have easy access to the\nconsole via GOV.UK PaaS, so this is relatively painless.\nWe have added a generator which creates an empty Ruby file with a timestamp and the name of the migration as the filename. Once the code is\ndeployed, we run the migration on the production server.\nAs the code is plain old Ruby, it's also easier for us to write tests, if\nwe think it's worth doing (for example, the migrations code is suitably\ncomplex).\n"
linshare-mobile-android-app/0008-download-with-downloadmanager-service.md,"## Context\nWe have some ways to perform downloading stable in the background, but system exposed a service called `DownloadManager`.\nClient may request that a URI be downloaded to a particular destination file. The download manager will conduct the\ndownload in the background, taking care of HTTP interactions and retrying downloads after failures or across connectivity changes and system reboot.\nApps that request downloads through this API can register a broadcast receiver to handle when the download is progress, failure, completed.\n","Instead of implementing a `Worker` like `Upload`, we will delegate downloading task to `DownloadManager` system service.\n",99,1647,"## Context\nWe have some ways to perform downloading stable in the background, but system exposed a service called `DownloadManager`.\nClient may request that a URI be downloaded to a particular destination file. The download manager will conduct the\ndownload in the background, taking care of HTTP interactions and retrying downloads after failures or across connectivity changes and system reboot.\nApps that request downloads through this API can register a broadcast receiver to handle when the download is progress, failure, completed.\n

##Decision
Instead of implementing a `Worker` like `Upload`, we will delegate downloading task to `DownloadManager` system service.\n"
holochain-rust/0003-redux-architecture-pattern.md,"## Context\nWe are doing a rewrite.\nHolochain Go code shows many implicit dependencies between different modules and stateful objects. In conjunction with the complexity of a p2p network of agents, this leads to a level of overall complexity that feels too much to manage. A clean and fitting architecture for this Rust rebuild is needed.\nHaving a single global state within the agent feels appropriate and even balancing the distributed nature of the network of agents.\n","The new holochain architecture will follow a redux architecture in order for an agent to have one global state.\nWe will apply nested state objects which represent a state tree, with sub states for each module.\nWe use reference counting smart pointers for the sub\nstates such that it is possible for each module's\nreducer to decide if the sub state is to be mutated or reused.\n",92,1495,"## Context\nWe are doing a rewrite.\nHolochain Go code shows many implicit dependencies between different modules and stateful objects. In conjunction with the complexity of a p2p network of agents, this leads to a level of overall complexity that feels too much to manage. A clean and fitting architecture for this Rust rebuild is needed.\nHaving a single global state within the agent feels appropriate and even balancing the distributed nature of the network of agents.\n

##Decision
The new holochain architecture will follow a redux architecture in order for an agent to have one global state.\nWe will apply nested state objects which represent a state tree, with sub states for each module.\nWe use reference counting smart pointers for the sub\nstates such that it is possible for each module's\nreducer to decide if the sub state is to be mutated or reused.\n"
libelektra/warning_array.md,## Problem\nCurrently multiple warnings are saved in an elektra non-conforming array\nnotation which is limited to 100 entries. The notation of `#00` is against\nthe design [decision made](array.md).\n,"The format should be aligned with the correct array notation,\nstarting with `#0`. The maximum number of warnings will stay at\n100 entries (`#0` - `#_99`).\n- [Array](array.md)\n",46,1303,"## Problem\nCurrently multiple warnings are saved in an elektra non-conforming array\nnotation which is limited to 100 entries. The notation of `#00` is against\nthe design [decision made](array.md).\n

##Decision
The format should be aligned with the correct array notation,\nstarting with `#0`. The maximum number of warnings will stay at\n100 entries (`#0` - `#_99`).\n- [Array](array.md)\n"
buy-for-your-school/0013-use-dfe-sign-in-as-auth-provider.md,## Context\nThe service needs a way to authenticate trusted school buying professionals and to restrict the majority of access to the public.\nWe believe a simpler password-less authentication mechanism would be all that's required. This service does not need any of the school and user information held within DfE Sign-in (DSI). DfE governance has reviewed our concern and decided this service should use DSI.\nThere is currently no formal recommendation for a tool of choice in the technical guidance https://github.com/DFE-Digital/technical-guidance.\nWe want a tool that provides an open and modern security standard.\n,We are going to use DSI as our single sign-on provider using the OIDC standard.\n,126,1252,"## Context\nThe service needs a way to authenticate trusted school buying professionals and to restrict the majority of access to the public.\nWe believe a simpler password-less authentication mechanism would be all that's required. This service does not need any of the school and user information held within DfE Sign-in (DSI). DfE governance has reviewed our concern and decided this service should use DSI.\nThere is currently no formal recommendation for a tool of choice in the technical guidance https://github.com/DFE-Digital/technical-guidance.\nWe want a tool that provides an open and modern security standard.\n

##Decision
We are going to use DSI as our single sign-on provider using the OIDC standard.\n"
jupyter-nbrequirements/0000-dependencies-management-jupyter-notebooks.md,"## Context and Problem Statement\nHow to guarantee reproducibility of Jupyter Notebooks?\nIn order to allow any user to re run the notebook with similar behaviour, it's important that each notebook is shipped with dependencies requirements\nthat include direct and transitive dependencies. This would also enforce and support security, reproducibility, traecability.\nNotebooks should be treated as component/service that use their own dependencies, therefore when storing notebooks,\nthey should be stored with dependencies so that an image can be built to run them or they can be shared and reused by others.\n## Decision Drivers <!-- optional -->\n* user prospective\n* reproducibility\n* traecability\n","* user prospective\n* reproducibility\n* traecability\nThe option selected is 3. because:\n* enforce reproducibility\n* enforce traceability between notebook\n### Positive Consequences <!-- optional -->\n* Satisfy reproducibility, traecability, shareability.\n* Notebooks are coupled with dependencies in their metadata.\n* If more notebooks are present, a common Pipfile can be created with a button that can automatically extract from all notebook dependencies and new common Pipfile.lock will be created. This would allow creation of an image that can run the notebooks.\n",140,1228,"## Context and Problem Statement\nHow to guarantee reproducibility of Jupyter Notebooks?\nIn order to allow any user to re run the notebook with similar behaviour, it's important that each notebook is shipped with dependencies requirements\nthat include direct and transitive dependencies. This would also enforce and support security, reproducibility, traecability.\nNotebooks should be treated as component/service that use their own dependencies, therefore when storing notebooks,\nthey should be stored with dependencies so that an image can be built to run them or they can be shared and reused by others.\n## Decision Drivers <!-- optional -->\n* user prospective\n* reproducibility\n* traecability\n

##Decision
* user prospective\n* reproducibility\n* traecability\nThe option selected is 3. because:\n* enforce reproducibility\n* enforce traceability between notebook\n### Positive Consequences <!-- optional -->\n* Satisfy reproducibility, traecability, shareability.\n* Notebooks are coupled with dependencies in their metadata.\n* If more notebooks are present, a common Pipfile can be created with a button that can automatically extract from all notebook dependencies and new common Pipfile.lock will be created. This would allow creation of an image that can run the notebooks.\n"
ockam/0001-record-architectural-decisions.md,## Context\nWe need to record the architectural decisions that we make as we develop Ockam.\n,"We will keep a collection of records for ""architecturally significant"" decisions: those that\naffect the structure, non-functional characteristics, dependencies, interfaces, or construction\ntechniques.\nWe will use Architecture Decision Records, as [described by Michael Nygard](1).\n",21,4539,"## Context\nWe need to record the architectural decisions that we make as we develop Ockam.\n

##Decision
We will keep a collection of records for ""architecturally significant"" decisions: those that\naffect the structure, non-functional characteristics, dependencies, interfaces, or construction\ntechniques.\nWe will use Architecture Decision Records, as [described by Michael Nygard](1).\n"
rotc/0002-use-aws-as-example-cloud-platform.md,"## Context\nThe issue motivating this decision, and any context that influences or constrains the decision.\n",The change that we're proposing or have agreed to implement.\n,21,3546,"## Context\nThe issue motivating this decision, and any context that influences or constrains the decision.\n

##Decision
The change that we're proposing or have agreed to implement.\n"
nr-arch/2020-04-24-ARCH-OpenJDK-Versions.md,"## Context\nThis is a response from IITD Architecture to the IITD Infrastructure Team, and others, for an Official Statement on OpenJDK positioning.\n* Status: proposed\n* Deciders: Licence Change\n* Date: 2020-08-27\nTechnical Story: [description | <https://apps.nrs.gov.bc.ca/int/jira/browse/ARCH-62]>\n## Context and Problem Statement\nThis is a response from IITD Architecture to the IITD Infrastructure Team, and others, for an Official Statement on OpenJDK positioning.\n## Decision Drivers\n* An official Information Innovation & Technology Division (IITD) statement is needed on the direction forward for target versions of Java/JDKs for all IITD hosted java applications.  Oracle has changed its licensing model, and will now be using a subscription model for charging for the use of its Oracle Java SE product releases.\n* Java SE 8 is the end of the legacy versioning and release cadence model; Java 9 was the new beginning.\n* Oracle extended the public updates of its Oracle JDK to January 2019 for commercial production use, and at least the end of 2020 for individual desktop use.\n* Oracle JDK should be, for the most part, interchangeable with Oracle’s OpenJDK builds for those who want to move up to the next releases, as they did for Java 6->7 and Java 7->8.\n* Java 10 is the suggested release by Oracle\n* Oracle has posted more information on the roadmap for the Java Client (including Applets and Web Start).\n* Oracle plans to discontinue contributions to the JDK 8 Updates project until January 2019.\n* For security reasons, IITD Architecture encourages upgrade/migration of all java applications to at least JDK 11.\n* Scope - IITD all server and client side applications owned by IITD that run Java.\n","* An official Information Innovation & Technology Division (IITD) statement is needed on the direction forward for target versions of Java/JDKs for all IITD hosted java applications.  Oracle has changed its licensing model, and will now be using a subscription model for charging for the use of its Oracle Java SE product releases.\n* Java SE 8 is the end of the legacy versioning and release cadence model; Java 9 was the new beginning.\n* Oracle extended the public updates of its Oracle JDK to January 2019 for commercial production use, and at least the end of 2020 for individual desktop use.\n* Oracle JDK should be, for the most part, interchangeable with Oracle’s OpenJDK builds for those who want to move up to the next releases, as they did for Java 6->7 and Java 7->8.\n* Java 10 is the suggested release by Oracle\n* Oracle has posted more information on the roadmap for the Java Client (including Applets and Web Start).\n* Oracle plans to discontinue contributions to the JDK 8 Updates project until January 2019.\n* For security reasons, IITD Architecture encourages upgrade/migration of all java applications to at least JDK 11.\n* Scope - IITD all server and client side applications owned by IITD that run Java.\nJDK 8 & Later\nOracles OpenJDK JDK binaries for Windows, macOS, and Linux are available on release-specific pages of jdk.java.net as .tar.gz or .zip archives.\nAs an example, the archives for JDK 13 may be found on jdk.java.net/13 and may be extracted on the command line using\n$ tar xvf openjdk-13*_bin.tar.gz\nor\n$ unzip openjdk-13*_bin.zip\ndepending on the archive type.\n### Positive Consequences\n* removes the dependencies on Oracle JDK Licensing\n* reduces security vulnerabilities of older JDK versions\nJava 7 is still in predominant use. It goes without saying that any version of Java below 7 should be updated immediately even version 7 needs significant remediation for its fleet of vulnerabilities.\nFurther vulnerabilities -\n* <https://www.cvedetails.com/product/19117/Oracle-JRE.html?vendor_id=93>\n*	<https://www.cvedetails.com/product/23642/Oracle-Openjdk.html?vendor_id=93>\n### Negative Consequences\n* slow performance may occur\n* migration issues will need to be addressed\n* Migrate all Java JDK dependencies from Oracle JDK to OpenJDK.\n* Upgrade all older versions to at least JDK 8, preference is to encourage teams to target move to JDK 11.\n",403,4802,"## Context\nThis is a response from IITD Architecture to the IITD Infrastructure Team, and others, for an Official Statement on OpenJDK positioning.\n* Status: proposed\n* Deciders: Licence Change\n* Date: 2020-08-27\nTechnical Story: [description | <https://apps.nrs.gov.bc.ca/int/jira/browse/ARCH-62]>\n## Context and Problem Statement\nThis is a response from IITD Architecture to the IITD Infrastructure Team, and others, for an Official Statement on OpenJDK positioning.\n## Decision Drivers\n* An official Information Innovation & Technology Division (IITD) statement is needed on the direction forward for target versions of Java/JDKs for all IITD hosted java applications.  Oracle has changed its licensing model, and will now be using a subscription model for charging for the use of its Oracle Java SE product releases.\n* Java SE 8 is the end of the legacy versioning and release cadence model; Java 9 was the new beginning.\n* Oracle extended the public updates of its Oracle JDK to January 2019 for commercial production use, and at least the end of 2020 for individual desktop use.\n* Oracle JDK should be, for the most part, interchangeable with Oracle’s OpenJDK builds for those who want to move up to the next releases, as they did for Java 6->7 and Java 7->8.\n* Java 10 is the suggested release by Oracle\n* Oracle has posted more information on the roadmap for the Java Client (including Applets and Web Start).\n* Oracle plans to discontinue contributions to the JDK 8 Updates project until January 2019.\n* For security reasons, IITD Architecture encourages upgrade/migration of all java applications to at least JDK 11.\n* Scope - IITD all server and client side applications owned by IITD that run Java.\n

##Decision
* An official Information Innovation & Technology Division (IITD) statement is needed on the direction forward for target versions of Java/JDKs for all IITD hosted java applications.  Oracle has changed its licensing model, and will now be using a subscription model for charging for the use of its Oracle Java SE product releases.\n* Java SE 8 is the end of the legacy versioning and release cadence model; Java 9 was the new beginning.\n* Oracle extended the public updates of its Oracle JDK to January 2019 for commercial production use, and at least the end of 2020 for individual desktop use.\n* Oracle JDK should be, for the most part, interchangeable with Oracle’s OpenJDK builds for those who want to move up to the next releases, as they did for Java 6->7 and Java 7->8.\n* Java 10 is the suggested release by Oracle\n* Oracle has posted more information on the roadmap for the Java Client (including Applets and Web Start).\n* Oracle plans to discontinue contributions to the JDK 8 Updates project until January 2019.\n* For security reasons, IITD Architecture encourages upgrade/migration of all java applications to at least JDK 11.\n* Scope - IITD all server and client side applications owned by IITD that run Java.\nJDK 8 & Later\nOracles OpenJDK JDK binaries for Windows, macOS, and Linux are available on release-specific pages of jdk.java.net as .tar.gz or .zip archives.\nAs an example, the archives for JDK 13 may be found on jdk.java.net/13 and may be extracted on the command line using\n$ tar xvf openjdk-13*_bin.tar.gz\nor\n$ unzip openjdk-13*_bin.zip\ndepending on the archive type.\n### Positive Consequences\n* removes the dependencies on Oracle JDK Licensing\n* reduces security vulnerabilities of older JDK versions\nJava 7 is still in predominant use. It goes without saying that any version of Java below 7 should be updated immediately even version 7 needs significant remediation for its fleet of vulnerabilities.\nFurther vulnerabilities -\n* <https://www.cvedetails.com/product/19117/Oracle-JRE.html?vendor_id=93>\n*	<https://www.cvedetails.com/product/23642/Oracle-Openjdk.html?vendor_id=93>\n### Negative Consequences\n* slow performance may occur\n* migration issues will need to be addressed\n* Migrate all Java JDK dependencies from Oracle JDK to OpenJDK.\n* Upgrade all older versions to at least JDK 8, preference is to encourage teams to target move to JDK 11.\n"
lbh-adrs/Validation.md,"## **Context**\nValidation is an important function within APIs, to ensure that data that is submitted via API calls is properly checked to ensure it meets the requirements set by the business.\nWe will look at two options for validation:\n- **Data Annotations**\nThis involves ""annotating"" each class model with specific validation, such as\n- `[Required(ErrorMessage = ""This field is required"")]`\n- `[MaxLength(20)]`\nThere are a number of issues with this approach:\n- Validation is scattered throughout the codebase as attributes on data model classes\n- Testing is not straightforward\n- Error messages are part of the compiled code and it is not possible to decouple this, e.g. to allow for customisable error messages\n- Does not allow for conditional validation\n- **Fluent Validation**\nFluent Validation solves a number of the issues that DataAnnotation cannot be solved by Data Annotations. It:\n- Is easy to configure with minimal, unintrusive setup in `Startup.cs`\n- Lives outside of the data model classes\n- Very easy to test validation in isolation\n- Error messaging can be externalised using dependency injection\n- Allows for chaining of validators and conditional validation\n- Has a lot of built-in validation already (*if **x** exists, then **y** must also exist*)\n","**Fluent Validation**\nFluent Valdation is widely used, offers a lot of flexibility and allows for a clean, customisable and testable approach to validation.\n",276,2307,"## **Context**\nValidation is an important function within APIs, to ensure that data that is submitted via API calls is properly checked to ensure it meets the requirements set by the business.\nWe will look at two options for validation:\n- **Data Annotations**\nThis involves ""annotating"" each class model with specific validation, such as\n- `[Required(ErrorMessage = ""This field is required"")]`\n- `[MaxLength(20)]`\nThere are a number of issues with this approach:\n- Validation is scattered throughout the codebase as attributes on data model classes\n- Testing is not straightforward\n- Error messages are part of the compiled code and it is not possible to decouple this, e.g. to allow for customisable error messages\n- Does not allow for conditional validation\n- **Fluent Validation**\nFluent Validation solves a number of the issues that DataAnnotation cannot be solved by Data Annotations. It:\n- Is easy to configure with minimal, unintrusive setup in `Startup.cs`\n- Lives outside of the data model classes\n- Very easy to test validation in isolation\n- Error messaging can be externalised using dependency injection\n- Allows for chaining of validators and conditional validation\n- Has a lot of built-in validation already (*if **x** exists, then **y** must also exist*)\n

##Decision
**Fluent Validation**\nFluent Valdation is widely used, offers a lot of flexibility and allows for a clean, customisable and testable approach to validation.\n"
james/0002-make-taskmanager-distributed.md,"## Context\nIn order to have a distributed version of James we need to have an homogeneous way to deal with `Task`.\nCurrently, every James nodes of a cluster have their own instance of `TaskManager` and they have no knowledge of others, making it impossible to orchestrate task execution at the cluster level.\nTasks are scheduled and ran on the same node they are scheduled.\nWe are also unable to list or access to the details of all the `Task`s of a cluster.\n",Create a distribution-aware implementation of `TaskManager`.\n,102,2127,"## Context\nIn order to have a distributed version of James we need to have an homogeneous way to deal with `Task`.\nCurrently, every James nodes of a cluster have their own instance of `TaskManager` and they have no knowledge of others, making it impossible to orchestrate task execution at the cluster level.\nTasks are scheduled and ran on the same node they are scheduled.\nWe are also unable to list or access to the details of all the `Task`s of a cluster.\n

##Decision
Create a distribution-aware implementation of `TaskManager`.\n"
gp-redirect/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\n",16,4401,"## Context\nWe need to record the architectural decisions made on this project.\n

##Decision
We will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\n"
cljdoc/0008-use-circleci-as-analysis-sandbox.md,"## Context\nAnalyzing untrusted Clojure code means loading it which should only be done in some kind of\nsandboxed environment. A Docker image has been created to help with this but this still\nrequires us to run and monitor job execution. Bad actors could still trigger many builds\nto run Bitcoin miners and other compute-stealing stuff.\nAlternatives to running Docker ourselves are AWS Lambda (probably similar compute-stealing\nissues) and ""hacking"" a continous integration service to do the job for us. More detail can\nbe found in the [notes on Isolation](https://github.com/martinklepsch/cljdoc/blob/72da65055ab94942f33fb63b29b732e81b559508/doc/isolation.md)\n",For a first version of cljdoc we will use CircleCI to run analysis for us. The result of\nthis analysis will be made available as a build artifact which can then be laoded in\na trusted environment to import data into Grimoire and build HTML (or other) documentation\nfrontends.\n,160,2601,"## Context\nAnalyzing untrusted Clojure code means loading it which should only be done in some kind of\nsandboxed environment. A Docker image has been created to help with this but this still\nrequires us to run and monitor job execution. Bad actors could still trigger many builds\nto run Bitcoin miners and other compute-stealing stuff.\nAlternatives to running Docker ourselves are AWS Lambda (probably similar compute-stealing\nissues) and ""hacking"" a continous integration service to do the job for us. More detail can\nbe found in the [notes on Isolation](https://github.com/martinklepsch/cljdoc/blob/72da65055ab94942f33fb63b29b732e81b559508/doc/isolation.md)\n

##Decision
For a first version of cljdoc we will use CircleCI to run analysis for us. The result of\nthis analysis will be made available as a build artifact which can then be laoded in\na trusted environment to import data into Grimoire and build HTML (or other) documentation\nfrontends.\n"
hee-web-blueprint/0013-use-bloomreach-channel-manager.md,"## Context\nWe sought to determine whether to deliver our document management capabilities using the content management platform natively or through the integration of an external document management platform.\nWe sought to determine whether Bloomreach's 'Channel' concept would be suitable for managing the various sites required to be brought onto the platform both at MVP and in the future, such as Deenary and Speciality sights.\nAs part of this, considerations were made around:\n* Ease of use for creating new sites\n* Ability to share components\n* Ability to segregate content for specific channels (sites)\n* Ability to share content up and down the stack where needed and appropriate\n* Permissions model required to support this model\n","Bloomreach's concept of channels is well suited to meet the needs of running the sites required under the NWP platform umbrella. Channels offer the ability to build new sites that share components and modules, which enables for greater consistency. By utilising roles and permissions from within BR, content can be segregated to be available only where it is most relevant, whilst allowing for content to be made available up or down the organisational stack (e.g. national content being aggregated at a regional level).\nBR's 'blueprinting' functionality allows for sites to be created using a series of parameters, further standardising the creation of sites where needed in an easy fashion.\n",141,1205,"## Context\nWe sought to determine whether to deliver our document management capabilities using the content management platform natively or through the integration of an external document management platform.\nWe sought to determine whether Bloomreach's 'Channel' concept would be suitable for managing the various sites required to be brought onto the platform both at MVP and in the future, such as Deenary and Speciality sights.\nAs part of this, considerations were made around:\n* Ease of use for creating new sites\n* Ability to share components\n* Ability to segregate content for specific channels (sites)\n* Ability to share content up and down the stack where needed and appropriate\n* Permissions model required to support this model\n

##Decision
Bloomreach's concept of channels is well suited to meet the needs of running the sites required under the NWP platform umbrella. Channels offer the ability to build new sites that share components and modules, which enables for greater consistency. By utilising roles and permissions from within BR, content can be segregated to be available only where it is most relevant, whilst allowing for content to be made available up or down the organisational stack (e.g. national content being aggregated at a regional level).\nBR's 'blueprinting' functionality allows for sites to be created using a series of parameters, further standardising the creation of sites where needed in an easy fashion.\n"
fundraising-application/003_Validation.md,"## Context\nWhen the team for the re-write of the Fundraising formed in 2016, we discovered that team members had different approaches to do validation:\n* Use an established library, like [Symfony Validation](https://symfony.com/doc/current/validation.html).\n* Write our own validation logic.\nThe arguments in favor of writing our own logic were:\n* We don't want to bind our domain layer to a concrete validation library implementation.\n* The individual validations - checking for required fields in most cases - are so simple that using an external library would make the validation more complicated.\n* We don't know the ""maintenance cycles"" of the library, either we need to constantly update or the library is not maintained properly.\n* Every developer would have to learn the API of the external library.\nAt the start of the project we did not know where we should put the validation logic:\n* At the framework/presentation layer, forcing us to create valid, fully formed domain objects as input for use cases.\n* At the use case layer, making validation part of the use case.\n","For each use case we write a validator class that checks the `Request` value object of that use case. The validator class must ensure that the use case can create valid domain objects from the request object. The validator class uses simple `if` checks and no external framework.\nWe return result data structures from validation classes. The result data structures that have some way of communicating to the framework layer what input caused the validation error. If necessary, one input can have more than one validation error.\nValidation error names are language-independent unique strings in `snake_case`. When we need to translate those error codes, we put the translations in the file [`validations.js`](https://github.com/wmde/fundraising-frontend-content/blob/test/i18n/de_DE/messages/validations.json) in the [content repository](https://github.com/wmde/fundraising-frontend-content). We don't  map every error to a translation, we can write frontend layer code that summarizes the errors or maps them in a different way.\n",225,1532,"## Context\nWhen the team for the re-write of the Fundraising formed in 2016, we discovered that team members had different approaches to do validation:\n* Use an established library, like [Symfony Validation](https://symfony.com/doc/current/validation.html).\n* Write our own validation logic.\nThe arguments in favor of writing our own logic were:\n* We don't want to bind our domain layer to a concrete validation library implementation.\n* The individual validations - checking for required fields in most cases - are so simple that using an external library would make the validation more complicated.\n* We don't know the ""maintenance cycles"" of the library, either we need to constantly update or the library is not maintained properly.\n* Every developer would have to learn the API of the external library.\nAt the start of the project we did not know where we should put the validation logic:\n* At the framework/presentation layer, forcing us to create valid, fully formed domain objects as input for use cases.\n* At the use case layer, making validation part of the use case.\n

##Decision
For each use case we write a validator class that checks the `Request` value object of that use case. The validator class must ensure that the use case can create valid domain objects from the request object. The validator class uses simple `if` checks and no external framework.\nWe return result data structures from validation classes. The result data structures that have some way of communicating to the framework layer what input caused the validation error. If necessary, one input can have more than one validation error.\nValidation error names are language-independent unique strings in `snake_case`. When we need to translate those error codes, we put the translations in the file [`validations.js`](https://github.com/wmde/fundraising-frontend-content/blob/test/i18n/de_DE/messages/validations.json) in the [content repository](https://github.com/wmde/fundraising-frontend-content). We don't  map every error to a translation, we can write frontend layer code that summarizes the errors or maps them in a different way.\n"
monocle/0009-changes-extended-with-task-data.md,"## Context and Problem Statement\nAs a user, I want to get Changes metrics related to tasks defined\nin a task tracker. A simple usecase example is to get insight of the\nratio of changes related to Feature Requests vs Bug fixing.\n## Decision Drivers\n* Simple implementation\n* No assumption about the tasks tracker\n* Support of a set of generic fields related to a task\n","* Simple implementation\n* No assumption about the tasks tracker\n* Support of a set of generic fields related to a task\nChosen option: ""Monocle API provides an interface for external task data crawlers"", because it will ease integration between Monocle and the\nvarious task trackers available in the market.\nA Monocle operator will need to write the crawler for its\nown task tracker. However, over the time, operators might have written crawlers for most popular systems and released them under a open source license.\nFrom the Monocle side, we provide a clear API for a task tracker\ncrawler to push task related data to Monocle. Each task data sent to Monocle must at least set a predifined set of generic attributes like\n""severity"", ""change_url"", ""title"", ... The monocle API is then able to\nfind corresponding Changes in the database that match the ""change_url"" field.\nEach changes get a new attribute called ""task_data"" that is a list of\nrelated task data records. Indeed multiple tasks might be related to a\nsingle change.\nAlso, Monocle keeps track of task data that do not match any Changes in the\ndatabase. The Monocle change crawlers engine triggers a search for adoption of orphaned task data.\n",79,4636,"## Context and Problem Statement\nAs a user, I want to get Changes metrics related to tasks defined\nin a task tracker. A simple usecase example is to get insight of the\nratio of changes related to Feature Requests vs Bug fixing.\n## Decision Drivers\n* Simple implementation\n* No assumption about the tasks tracker\n* Support of a set of generic fields related to a task\n

##Decision
* Simple implementation\n* No assumption about the tasks tracker\n* Support of a set of generic fields related to a task\nChosen option: ""Monocle API provides an interface for external task data crawlers"", because it will ease integration between Monocle and the\nvarious task trackers available in the market.\nA Monocle operator will need to write the crawler for its\nown task tracker. However, over the time, operators might have written crawlers for most popular systems and released them under a open source license.\nFrom the Monocle side, we provide a clear API for a task tracker\ncrawler to push task related data to Monocle. Each task data sent to Monocle must at least set a predifined set of generic attributes like\n""severity"", ""change_url"", ""title"", ... The monocle API is then able to\nfind corresponding Changes in the database that match the ""change_url"" field.\nEach changes get a new attribute called ""task_data"" that is a list of\nrelated task data records. Indeed multiple tasks might be related to a\nsingle change.\nAlso, Monocle keeps track of task data that do not match any Changes in the\ndatabase. The Monocle change crawlers engine triggers a search for adoption of orphaned task data.\n"
gsp/ADR041-service-operated-policies.md,"## Context\nOur service-operator allows service teams to provision various AWS services by\ndeclaratively defining resources and submitting them via the kubernetes api.\nSome of these resources require IAM to authorise how the provisioned service\ncan be used. The types of actions that can be performed on.\n#### Example\nThe service operator allows provisioning of S3 buckets and bucket configuration such as:\n```\n---\napiVersion: storage.govsvc.uk/v1beta1\nkind: S3Bucket\nmetadata:\nname: s3-bucket-sample\nspec:\naws:\nLifecycleRules:\n- Expiration: 90days\nVersioning:\nEnabled: true\n```\nIn order to access a provisioned bucket via the the AWS SDK users will require\nan IAM role/policy that allows access.\nWe want things like bucket ACL, versioning configuration and lifecycle policy\nto be defined declaratively via the resource manifest (see example above), and continuously managed\nby the service operator.\nWe want users of the provisioned bucket to be able to read back all\nconfiguration, and be able to fully utilise the specific bucket for reading,\nwriting and managing their objects within the provisioned bucket, but we want\nto avoid giving permissions to users that could cause conflicts with the\nproperties that are managed by the service operator's reconcile loop.\nFor example, given the example manifest above, we would like to avoid giving\npermissions that would allow a user to alter the Expiration LifeCycleRules,\nsince any changes the user made would be periodically overwritten by the\nservice operator's reconciliation.\n","* We will provision policy that gives full access for users to _use_ the\nprovisioned service.\n* We will avoid provisioning policy that allows users to create, destroy or\nconfigure the provisioned service, so that this can remain the declarative\ndomain of the service-operator.\n",335,3907,"## Context\nOur service-operator allows service teams to provision various AWS services by\ndeclaratively defining resources and submitting them via the kubernetes api.\nSome of these resources require IAM to authorise how the provisioned service\ncan be used. The types of actions that can be performed on.\n#### Example\nThe service operator allows provisioning of S3 buckets and bucket configuration such as:\n```\n---\napiVersion: storage.govsvc.uk/v1beta1\nkind: S3Bucket\nmetadata:\nname: s3-bucket-sample\nspec:\naws:\nLifecycleRules:\n- Expiration: 90days\nVersioning:\nEnabled: true\n```\nIn order to access a provisioned bucket via the the AWS SDK users will require\nan IAM role/policy that allows access.\nWe want things like bucket ACL, versioning configuration and lifecycle policy\nto be defined declaratively via the resource manifest (see example above), and continuously managed\nby the service operator.\nWe want users of the provisioned bucket to be able to read back all\nconfiguration, and be able to fully utilise the specific bucket for reading,\nwriting and managing their objects within the provisioned bucket, but we want\nto avoid giving permissions to users that could cause conflicts with the\nproperties that are managed by the service operator's reconcile loop.\nFor example, given the example manifest above, we would like to avoid giving\npermissions that would allow a user to alter the Expiration LifeCycleRules,\nsince any changes the user made would be periodically overwritten by the\nservice operator's reconciliation.\n

##Decision
* We will provision policy that gives full access for users to _use_ the\nprovisioned service.\n* We will avoid provisioning policy that allows users to create, destroy or\nconfigure the provisioned service, so that this can remain the declarative\ndomain of the service-operator.\n"
buy-for-your-school/0020-use-accessible-autocomplete-for-autocomplete-fields.md,## Context\nIt is necessary to provide autocomplete functionality to make certain fields quicker to enter by suggesting potential results to the user.\n,We will use [accessible-autocomplete](https://github.com/alphagov/accessible-autocomplete) to provide the autocomplete capability in our pages.\nThis package has been chosen because accessibility has been carefully considered when developing the package.\nAlso it is designed to be used with `govuk` form styles so it will be in keeping with other form fields\nand not be jarring to the user.\n,26,1255,"## Context\nIt is necessary to provide autocomplete functionality to make certain fields quicker to enter by suggesting potential results to the user.\n

##Decision
We will use [accessible-autocomplete](https://github.com/alphagov/accessible-autocomplete) to provide the autocomplete capability in our pages.\nThis package has been chosen because accessibility has been carefully considered when developing the package.\nAlso it is designed to be used with `govuk` form styles so it will be in keeping with other form fields\nand not be jarring to the user.\n"
react-transcript-editor/2018-10-05-components-comunication.md,"## Context and Problem Statement\nDeciding how to have the internal components of the Transcript Editor communicate with each other.\n## Decision Drivers <!-- optional -->\n* Simple and straightforward way to reason around passing data between components\n* Extensible anticipating use cases when using the component ""in the wild"" and having internal info accessible/when if needed.\n","* Simple and straightforward way to reason around passing data between components\n* Extensible anticipating use cases when using the component ""in the wild"" and having internal info accessible/when if needed.\n<!-- Chosen option: ""[option 1]"", because [justification. e.g., only option, which meets k.o. criterion decision driver | which resolves force force | … | comes out best (see below)]. -->\nStill evaluating, leaning torwards some light refactoring to enable parent component, option 1 to keep things simple.\n<!-- ### Positive Consequences\n* [e.g., improvement of quality attribute satisfaction, follow-up decisions required, …]\n* …\n### Negative consequences\n* [e.g., compromising quality attribute, follow-up decisions required, …]\n* … -->\n",71,3187,"## Context and Problem Statement\nDeciding how to have the internal components of the Transcript Editor communicate with each other.\n## Decision Drivers <!-- optional -->\n* Simple and straightforward way to reason around passing data between components\n* Extensible anticipating use cases when using the component ""in the wild"" and having internal info accessible/when if needed.\n

##Decision
* Simple and straightforward way to reason around passing data between components\n* Extensible anticipating use cases when using the component ""in the wild"" and having internal info accessible/when if needed.\n<!-- Chosen option: ""[option 1]"", because [justification. e.g., only option, which meets k.o. criterion decision driver | which resolves force force | … | comes out best (see below)]. -->\nStill evaluating, leaning torwards some light refactoring to enable parent component, option 1 to keep things simple.\n<!-- ### Positive Consequences\n* [e.g., improvement of quality attribute satisfaction, follow-up decisions required, …]\n* …\n### Negative consequences\n* [e.g., compromising quality attribute, follow-up decisions required, …]\n* … -->\n"
bananatabs/0002-react-testing-library.md,"## Context\nWe want to avoid testing implementation details in our integration tests.\nWe want to use `react-testing-library` which makes it easier to make assertions on the rendered DOM rather than assert implementation details. But mostly because it enable us to find and trigger click events on different UI elements.\ne.g. toggling the visibility of different tabs and window groups.\nBut when it comes to asserting the rendered DOM, in most cases we trust the view will render the model properly.\nIt could be more sensible to only verify the state/model.\n","We will assert against the session in the provider which is accessible through the context. (the BananaContext instance, not a React Context)\n",111,2816,"## Context\nWe want to avoid testing implementation details in our integration tests.\nWe want to use `react-testing-library` which makes it easier to make assertions on the rendered DOM rather than assert implementation details. But mostly because it enable us to find and trigger click events on different UI elements.\ne.g. toggling the visibility of different tabs and window groups.\nBut when it comes to asserting the rendered DOM, in most cases we trust the view will render the model properly.\nIt could be more sensible to only verify the state/model.\n

##Decision
We will assert against the session in the provider which is accessible through the context. (the BananaContext instance, not a React Context)\n"
PIMS/geocoder.md,## Context\nPresently when submitting new properties or editing existing properties within inventory the only way to set the latitude and longitude values is manually.\nIdeally the inventory would use GIS location values that are pulled from Data BC (better source of truth).\nProviding a way through the property address to pull valid GIS coordinates from Data BC Geocoder would improve the data and the user experience.\nAdditionally Geocoder can be used to verify addresses that are manually entered.\n- [Geocoder](https://www2.gov.bc.ca/gov/content/data/geographic-data-services/location-services/geocoder)\n- [Data BC](https://catalogue.data.gov.bc.ca/dataset/bc-address-geocoder-web-service)\n- [API Swagger](https://catalogue.data.gov.bc.ca/dataset/bc-address-geocoder-web-service/resource/40d6411e-ab98-4df9-a24e-67f81c45f6fa/view/1d3c42fc-53dc-4aab-ae3b-f4d056cb00e0)\n- [Developer API Keys](https://github.com/bcgov/gwa/wiki/Developer-Guide#developer-api-keys)\n- API Host = `https://geocoder.api.gov.bc.ca`\n,"Integrate with Data BC Geocoder API.\nWhen a user types an address a list of viable matches will be displayed.\nIf the user selects one of the matches it will be used to set the address and GIS coordinates,.\n",262,3968,"## Context\nPresently when submitting new properties or editing existing properties within inventory the only way to set the latitude and longitude values is manually.\nIdeally the inventory would use GIS location values that are pulled from Data BC (better source of truth).\nProviding a way through the property address to pull valid GIS coordinates from Data BC Geocoder would improve the data and the user experience.\nAdditionally Geocoder can be used to verify addresses that are manually entered.\n- [Geocoder](https://www2.gov.bc.ca/gov/content/data/geographic-data-services/location-services/geocoder)\n- [Data BC](https://catalogue.data.gov.bc.ca/dataset/bc-address-geocoder-web-service)\n- [API Swagger](https://catalogue.data.gov.bc.ca/dataset/bc-address-geocoder-web-service/resource/40d6411e-ab98-4df9-a24e-67f81c45f6fa/view/1d3c42fc-53dc-4aab-ae3b-f4d056cb00e0)\n- [Developer API Keys](https://github.com/bcgov/gwa/wiki/Developer-Guide#developer-api-keys)\n- API Host = `https://geocoder.api.gov.bc.ca`\n

##Decision
Integrate with Data BC Geocoder API.\nWhen a user types an address a list of viable matches will be displayed.\nIf the user selects one of the matches it will be used to set the address and GIS coordinates,.\n"
up-fiscal-data/008-grant-wise.md,## Context and Problem Statement\nWhile extracting the `Grant-wise expenditure` a lot of site crashes were encountered. A decision was taken earlier to extract the `DDO-wise expenditure` section from Koshvani as both section contain the same information in different heirarchies.\n## Decision Drivers\nThe challenges around site crashes have been addressed and the `Grant-wise expenditure` section can now be extracted.\n,The challenges around site crashes have been addressed and the `Grant-wise expenditure` section can now be extracted.\nThe `Grant-wise expenditure` section is being extracted over the `DDO-wise expenditure` section.\n,82,447,"## Context and Problem Statement\nWhile extracting the `Grant-wise expenditure` a lot of site crashes were encountered. A decision was taken earlier to extract the `DDO-wise expenditure` section from Koshvani as both section contain the same information in different heirarchies.\n## Decision Drivers\nThe challenges around site crashes have been addressed and the `Grant-wise expenditure` section can now be extracted.\n

##Decision
The challenges around site crashes have been addressed and the `Grant-wise expenditure` section can now be extracted.\nThe `Grant-wise expenditure` section is being extracted over the `DDO-wise expenditure` section.\n"
latis3/0005-Operations-defined-in-FDML-schema-as-elements.md,"## Context and Problem Statement\nOperations can be defined in the FDML as elements and attributes or simply as elements.  For example the operation take can be described as:\n```\n<xs:element name=""take"" type=""xs:integer"">\n```\nor with attributes as:\n```\n<xs:element name=""take"">\n<xs:complexType>\n<xs:attribute name=""value""/>\n</xs:complexType>\n</xs:element>\n```\n## Decision Drivers\n* consistency\n* expressiveness\n",* consistency\n* expressiveness\nChosen option: Try to describe operations as elements only\n### Positive Consequences\n* consistency\n### Negative Consequences\n* future operations may not be definable\n,115,4713,"## Context and Problem Statement\nOperations can be defined in the FDML as elements and attributes or simply as elements.  For example the operation take can be described as:\n```\n<xs:element name=""take"" type=""xs:integer"">\n```\nor with attributes as:\n```\n<xs:element name=""take"">\n<xs:complexType>\n<xs:attribute name=""value""/>\n</xs:complexType>\n</xs:element>\n```\n## Decision Drivers\n* consistency\n* expressiveness\n

##Decision
* consistency\n* expressiveness\nChosen option: Try to describe operations as elements only\n### Positive Consequences\n* consistency\n### Negative Consequences\n* future operations may not be definable\n"
event-routing-backends/0005-PII-leakage-prevention.rst,"Context\n-------\n``Event-routing-backends`` transforms and emits edx events that may contain PII which is not meant to be shared with learning record consumers. New xAPI and Caliper transformers are expected to be added in ``Event-routing-backends`` and therefore, a mechanism needs to be put in place to reduce chances of PII leakage via these transformers.\nDecision\n--------\n#. An accessor `method`_ will be developed to get values from open edx events for a specified key. Information inside the open edx event will only be accessed using this accessor method in base transformers and in event transformers.\n#. If a key, required by the specification (xAPI or Caliper), is not found in open edx event, this method will throw an exception.\n#. If a key, deemed optional by the specification (xAPI or Caliper), is not found in open edx event, this method will log this instance and return None.\n#. The efficacy of fuzzy comparison between keys in transformed events and potential PII keys will be evaluated in future and incorporated if found useful.\nBenefits\n---------\n#. Only the instances where this accessor is called, will need to be reviewed during code review, in order to check for potential PII leakage.\n.. _method: https://github.com/openedx/event-routing-backends/blob/f430d4cf58bdab01e42fcc944241898606873d82/event_routing_backends/processors/mixins/base_transformer.py#L139\n","--------\n#. An accessor `method`_ will be developed to get values from open edx events for a specified key. Information inside the open edx event will only be accessed using this accessor method in base transformers and in event transformers.\n#. If a key, required by the specification (xAPI or Caliper), is not found in open edx event, this method will throw an exception.\n#. If a key, deemed optional by the specification (xAPI or Caliper), is not found in open edx event, this method will log this instance and return None.\n#. The efficacy of fuzzy comparison between keys in transformed events and potential PII keys will be evaluated in future and incorporated if found useful.\nBenefits\n---------\n#. Only the instances where this accessor is called, will need to be reviewed during code review, in order to check for potential PII leakage.\n.. _method: https://github.com/openedx/event-routing-backends/blob/f430d4cf58bdab01e42fcc944241898606873d82/event_routing_backends/processors/mixins/base_transformer.py#L139\n",307,4477,"Context\n-------\n``Event-routing-backends`` transforms and emits edx events that may contain PII which is not meant to be shared with learning record consumers. New xAPI and Caliper transformers are expected to be added in ``Event-routing-backends`` and therefore, a mechanism needs to be put in place to reduce chances of PII leakage via these transformers.\nDecision\n--------\n#. An accessor `method`_ will be developed to get values from open edx events for a specified key. Information inside the open edx event will only be accessed using this accessor method in base transformers and in event transformers.\n#. If a key, required by the specification (xAPI or Caliper), is not found in open edx event, this method will throw an exception.\n#. If a key, deemed optional by the specification (xAPI or Caliper), is not found in open edx event, this method will log this instance and return None.\n#. The efficacy of fuzzy comparison between keys in transformed events and potential PII keys will be evaluated in future and incorporated if found useful.\nBenefits\n---------\n#. Only the instances where this accessor is called, will need to be reviewed during code review, in order to check for potential PII leakage.\n.. _method: https://github.com/openedx/event-routing-backends/blob/f430d4cf58bdab01e42fcc944241898606873d82/event_routing_backends/processors/mixins/base_transformer.py#L139\n

##Decision
--------\n#. An accessor `method`_ will be developed to get values from open edx events for a specified key. Information inside the open edx event will only be accessed using this accessor method in base transformers and in event transformers.\n#. If a key, required by the specification (xAPI or Caliper), is not found in open edx event, this method will throw an exception.\n#. If a key, deemed optional by the specification (xAPI or Caliper), is not found in open edx event, this method will log this instance and return None.\n#. The efficacy of fuzzy comparison between keys in transformed events and potential PII keys will be evaluated in future and incorporated if found useful.\nBenefits\n---------\n#. Only the instances where this accessor is called, will need to be reviewed during code review, in order to check for potential PII leakage.\n.. _method: https://github.com/openedx/event-routing-backends/blob/f430d4cf58bdab01e42fcc944241898606873d82/event_routing_backends/processors/mixins/base_transformer.py#L139\n"
backdrop/adr-002-persistent-storage.md,"# Context\nThe way the Government Digital Service (GDS) [makes technology choices is\ndescribed in the service manual](https://www.gov.uk/service-manual/making-software/choosing-technology). We are selecting which technology will to use to provide\npersistence for the Performance Platform.\nGDS has experience in running MongoDB and MySQL in production.\nWe envisage the Performance Platform as taking in unstructured data from a\nvariety of data sources (spreadsheets, analytics, logs, other databases and\napplications) and allowing people to collect this data in a single place. This\nshould enable service managers to:\n- make comparisons\n- see how well their service is performing\n- see how the performance changes over time, as they iterate the service\nSo we want a persistent data store that will store unstructured data, and\nallow us to apply a structure either by post-processing the data, or at query\ntime.\nThe volume of the data that we are envisaging at this stage is pretty small.\nWe will be building a small thing to start; as we learn more about the\nuser needs and problem space, then we will revisit this decision. Since the\nvolume is small, it does not seem likely that we need Hadoop / HDFS or\nCassandra.\nWe are not the canonical source of this data. We are an aggregator; the\ncanonical source remains the data sources which will be providing feeds or\npushing the data into the Performance Platform.\nBecause of this position, we do not need ACID properties for this data, nor\nneed worry about the CAP theorem in any detail.\n# Decision\nWe will use MongoDB. We are comfortable operating it in production,\nit will allow unstructured data (in the form of JSON documents) and we can\napply structure at query time.\n# Status\nAccepted.\n# Consequences\nUse MongoDB with an appropriate replica-set configuration.\n","We will use MongoDB. We are comfortable operating it in production,\nit will allow unstructured data (in the form of JSON documents) and we can\napply structure at query time.\n# Status\nAccepted.\n# Consequences\nUse MongoDB with an appropriate replica-set configuration.\n",403,4266,"# Context\nThe way the Government Digital Service (GDS) [makes technology choices is\ndescribed in the service manual](https://www.gov.uk/service-manual/making-software/choosing-technology). We are selecting which technology will to use to provide\npersistence for the Performance Platform.\nGDS has experience in running MongoDB and MySQL in production.\nWe envisage the Performance Platform as taking in unstructured data from a\nvariety of data sources (spreadsheets, analytics, logs, other databases and\napplications) and allowing people to collect this data in a single place. This\nshould enable service managers to:\n- make comparisons\n- see how well their service is performing\n- see how the performance changes over time, as they iterate the service\nSo we want a persistent data store that will store unstructured data, and\nallow us to apply a structure either by post-processing the data, or at query\ntime.\nThe volume of the data that we are envisaging at this stage is pretty small.\nWe will be building a small thing to start; as we learn more about the\nuser needs and problem space, then we will revisit this decision. Since the\nvolume is small, it does not seem likely that we need Hadoop / HDFS or\nCassandra.\nWe are not the canonical source of this data. We are an aggregator; the\ncanonical source remains the data sources which will be providing feeds or\npushing the data into the Performance Platform.\nBecause of this position, we do not need ACID properties for this data, nor\nneed worry about the CAP theorem in any detail.\n# Decision\nWe will use MongoDB. We are comfortable operating it in production,\nit will allow unstructured data (in the form of JSON documents) and we can\napply structure at query time.\n# Status\nAccepted.\n# Consequences\nUse MongoDB with an appropriate replica-set configuration.\n

##Decision
We will use MongoDB. We are comfortable operating it in production,\nit will allow unstructured data (in the form of JSON documents) and we can\napply structure at query time.\n# Status\nAccepted.\n# Consequences\nUse MongoDB with an appropriate replica-set configuration.\n"
play-frontend-hmrc/0012-create-an-endpoint-in-play-frontend-hmrc-for-surfacing-session-metadata.md,"## Context and Problem Statement\nTax users are sometimes, without warning, unnecessarily signed out when accessing\nMDTP services using multiple browser tabs or windows. This is a violation of [WCAG 2.1\nsuccess criterion 2.1.1 (Timing adjustable)](https://www.w3.org/WAI/WCAG21/Understanding/timing-adjustable.html).\nThis problem is a consequence of the fact that a user's session can be refreshed via user activity in any\ntab or window belonging to the same browser profile. However, the Javascript-powered [hmrcTimeoutDialog](https://github.com/hmrc/play-frontend-hmrc#warning-users-before-timing-them-out)\ncurrently has no way of knowing about this activity following initial page load.\nSolving this issue requires providing the timeout dialog component with knowledge of the actual time\nremaining on the user's active session via an endpoint that is itself excluded from\nsession management.\nHow can we achieve this cost-effectively while minimising impact for service teams, limiting duplication of\nknowledge and avoiding introducing additional coupling between frontend microservices?\n## Decision Drivers\n* The need to minimise code changes for service teams other than a library upgrade.\n* The avoidance of requiring service teams to add any additional routing rules.\n* The avoidance of requiring service teams to add any additional configuration.\n* The need to limit duplication of knowledge between SessionTimeoutFilter in bootstrap-play and\nany other library or service.\n* The preference for maintaining loose coupling of frontend services and avoiding adding single points\nof failure between frontend microservices.\n* The need for the endpoint used to interrogate the session to not itself affect the session.\n","* The need to minimise code changes for service teams other than a library upgrade.\n* The avoidance of requiring service teams to add any additional routing rules.\n* The avoidance of requiring service teams to add any additional configuration.\n* The need to limit duplication of knowledge between SessionTimeoutFilter in bootstrap-play and\nany other library or service.\n* The preference for maintaining loose coupling of frontend services and avoiding adding single points\nof failure between frontend microservices.\n* The need for the endpoint used to interrogate the session to not itself affect the session.\nChosen option: option 9, because it is an option PlatUI and PlatOps agree on, is technically feasible,\nsatisfies most of the decision drivers and is the smallest possible change with the least impact to service teams. The intention would be\nto revisit option 7 (play-session) at a later date in order to address any outstanding concerns\naround knowledge duplication.\n",348,560,"## Context and Problem Statement\nTax users are sometimes, without warning, unnecessarily signed out when accessing\nMDTP services using multiple browser tabs or windows. This is a violation of [WCAG 2.1\nsuccess criterion 2.1.1 (Timing adjustable)](https://www.w3.org/WAI/WCAG21/Understanding/timing-adjustable.html).\nThis problem is a consequence of the fact that a user's session can be refreshed via user activity in any\ntab or window belonging to the same browser profile. However, the Javascript-powered [hmrcTimeoutDialog](https://github.com/hmrc/play-frontend-hmrc#warning-users-before-timing-them-out)\ncurrently has no way of knowing about this activity following initial page load.\nSolving this issue requires providing the timeout dialog component with knowledge of the actual time\nremaining on the user's active session via an endpoint that is itself excluded from\nsession management.\nHow can we achieve this cost-effectively while minimising impact for service teams, limiting duplication of\nknowledge and avoiding introducing additional coupling between frontend microservices?\n## Decision Drivers\n* The need to minimise code changes for service teams other than a library upgrade.\n* The avoidance of requiring service teams to add any additional routing rules.\n* The avoidance of requiring service teams to add any additional configuration.\n* The need to limit duplication of knowledge between SessionTimeoutFilter in bootstrap-play and\nany other library or service.\n* The preference for maintaining loose coupling of frontend services and avoiding adding single points\nof failure between frontend microservices.\n* The need for the endpoint used to interrogate the session to not itself affect the session.\n

##Decision
* The need to minimise code changes for service teams other than a library upgrade.\n* The avoidance of requiring service teams to add any additional routing rules.\n* The avoidance of requiring service teams to add any additional configuration.\n* The need to limit duplication of knowledge between SessionTimeoutFilter in bootstrap-play and\nany other library or service.\n* The preference for maintaining loose coupling of frontend services and avoiding adding single points\nof failure between frontend microservices.\n* The need for the endpoint used to interrogate the session to not itself affect the session.\nChosen option: option 9, because it is an option PlatUI and PlatOps agree on, is technically feasible,\nsatisfies most of the decision drivers and is the smallest possible change with the least impact to service teams. The intention would be\nto revisit option 7 (play-session) at a later date in order to address any outstanding concerns\naround knowledge duplication.\n"
architecture/0007-use-static-code-analysis.md,## Context\nYou never think of everything. Sticking to standards is a very good\nthing to prevent you from doing things that can go bad. Those also\nhelps making the code be more readable and structured.\n,Use Static Code Analysis to find violations of standards.\n,45,1418,"## Context\nYou never think of everything. Sticking to standards is a very good\nthing to prevent you from doing things that can go bad. Those also\nhelps making the code be more readable and structured.\n

##Decision
Use Static Code Analysis to find violations of standards.\n"
raster-foundry/adr-0006-workflow-manager.md,"## Context\nThis ADR has been superseded by `ADR-0019`.\nSome actions and features of Raster Foundry require a way to manage asynchronous tasks and workflows.\nFor instance, user uploads of imagery or tools may start workflows in an ad hoc manner, while in\ncontrast imports of imagery from NASA or partners may need to happen on a schedule. The nature of\nthese tasks could vary from bash scripts and python functions to spark jobs and ECS tasks.\nThe ideal tool will provide some means to monitor task progress, retry on some failures, and\nnotify personnel if necessary. There are a few options of tools we can use: celery, SWF, Luigi, and Airflow.\nAzavea has experience working with both celery and SWF; however, due to our past experience with these\ntools it seemed prudent to explore additional options as well.\n| Workflow Tool   | Pros | Cons |\n|-----------------|------|------|\n| Celery          | Familiar, written in python, flexible | Provides poor primitives for workflows, many open issues, difficult to monitor workflows |\n| SWF (botoflow)) | Familiar, now written in python, maintaining state is not our responsibility (HA by default), great primitives for workflows and tasks | Difficult to monitor, relatively immature tools and projects, not many others using it |\n| Luigi           | Mature, seems to be stable, written in python | Unfamiliar execution model, primarily designed for scheduled, recurring task |\n| Airflow         | Mature, stable, fits into our execution model, written in python, excellent UI | Requires celery (for what we want to do)), requires managing the scheduler and a cache |\n","Raster Foundry will use Airflow as a task manager. There are a number of advantages over some of the\nalternatives. First, Airflow has a large, active, user base that have used it in production. The\nproject itself is in the Apache incubator, providing a strong signal that the project is of high quality\nSecond, Airflow's UI for monitoring task and workflow progress is great. It provides\nhigh-level relevant information that will enable us to diagnose issues quickly. Additionally, it\nprovides a means to view log output of tasks directly in the admin interface. Third, Airflow\nsupports both scheduled and ad hoc tasks. Lastly, Airflow's architecture would re-use many\ncomponents that will already be a part of Raster Foundry's infrastructure - including a Postgres\ndatabase and a redis cache.\n",353,1688,"## Context\nThis ADR has been superseded by `ADR-0019`.\nSome actions and features of Raster Foundry require a way to manage asynchronous tasks and workflows.\nFor instance, user uploads of imagery or tools may start workflows in an ad hoc manner, while in\ncontrast imports of imagery from NASA or partners may need to happen on a schedule. The nature of\nthese tasks could vary from bash scripts and python functions to spark jobs and ECS tasks.\nThe ideal tool will provide some means to monitor task progress, retry on some failures, and\nnotify personnel if necessary. There are a few options of tools we can use: celery, SWF, Luigi, and Airflow.\nAzavea has experience working with both celery and SWF; however, due to our past experience with these\ntools it seemed prudent to explore additional options as well.\n| Workflow Tool   | Pros | Cons |\n|-----------------|------|------|\n| Celery          | Familiar, written in python, flexible | Provides poor primitives for workflows, many open issues, difficult to monitor workflows |\n| SWF (botoflow)) | Familiar, now written in python, maintaining state is not our responsibility (HA by default), great primitives for workflows and tasks | Difficult to monitor, relatively immature tools and projects, not many others using it |\n| Luigi           | Mature, seems to be stable, written in python | Unfamiliar execution model, primarily designed for scheduled, recurring task |\n| Airflow         | Mature, stable, fits into our execution model, written in python, excellent UI | Requires celery (for what we want to do)), requires managing the scheduler and a cache |\n

##Decision
Raster Foundry will use Airflow as a task manager. There are a number of advantages over some of the\nalternatives. First, Airflow has a large, active, user base that have used it in production. The\nproject itself is in the Apache incubator, providing a strong signal that the project is of high quality\nSecond, Airflow's UI for monitoring task and workflow progress is great. It provides\nhigh-level relevant information that will enable us to diagnose issues quickly. Additionally, it\nprovides a means to view log output of tasks directly in the admin interface. Third, Airflow\nsupports both scheduled and ad hoc tasks. Lastly, Airflow's architecture would re-use many\ncomponents that will already be a part of Raster Foundry's infrastructure - including a Postgres\ndatabase and a redis cache.\n"
taxonomy-manager/001-graph-database.md,"## Context and Problem Statement\nPersistence and retrieval of SKOS taxonomies require a storage layer that supports storing rich, free-form linked data.\nSuch a data model could be represented in a traditional RDBMS, however, doing so would require a specialized serialization and deserialization implementation whereas graph databases can typically store RDF natively.\n## Decision Drivers <!-- optional -->\n* High availability/Fault tolerance\n* Learning curve\n* Maintenance overhead\n* Vendor lock-in\n","* High availability/Fault tolerance\n* Learning curve\n* Maintenance overhead\n* Vendor lock-in\nChosen option: Apache Jena with underlying PostgreSQL persistence store and a custom persistence layer, because it provides a highly available database persistence that is widely available as a managed service. We opted against using Apache Jena's SDB to achieve this as it has been in a state of ""maintenance only"" since June 2013.\n### Positive Consequences\n* PostgreSQL can be clustered, making it highly available\n* PostgreSQL is broadly available as a managed service\n* We can utilise the same PostgreSQL database for storing additional data (e.g. users, roles, etc)\n### Negative Consequences\n* We have to design our own schema and code for interacting with the PostgreSQL database\n",95,1510,"## Context and Problem Statement\nPersistence and retrieval of SKOS taxonomies require a storage layer that supports storing rich, free-form linked data.\nSuch a data model could be represented in a traditional RDBMS, however, doing so would require a specialized serialization and deserialization implementation whereas graph databases can typically store RDF natively.\n## Decision Drivers <!-- optional -->\n* High availability/Fault tolerance\n* Learning curve\n* Maintenance overhead\n* Vendor lock-in\n

##Decision
* High availability/Fault tolerance\n* Learning curve\n* Maintenance overhead\n* Vendor lock-in\nChosen option: Apache Jena with underlying PostgreSQL persistence store and a custom persistence layer, because it provides a highly available database persistence that is widely available as a managed service. We opted against using Apache Jena's SDB to achieve this as it has been in a state of ""maintenance only"" since June 2013.\n### Positive Consequences\n* PostgreSQL can be clustered, making it highly available\n* PostgreSQL is broadly available as a managed service\n* We can utilise the same PostgreSQL database for storing additional data (e.g. users, roles, etc)\n### Negative Consequences\n* We have to design our own schema and code for interacting with the PostgreSQL database\n"
gp-finder/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\n",16,3475,"## Context\nWe need to record the architectural decisions made on this project.\n

##Decision
We will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\n"
nhsuk-prototype-kit-version-one/0003-use-npm-scripts-and-gulp-for-running-tasks.md,"## Context\nThere are lots of different tasks that need processed in order to get the prototype kit up and running. Tasks such as; installing dependencies, moving files from dependencies into the app file structure, and most importantly - running the application.\n","We will use a mixture on NPM scripts and [Gulp](https://gulpjs.com) to run our tasks. NPM scripts give us the core installation and start tasks as well as the ability to run Gulp tasks. Gulp, written in javascript, is very extensible and will allow us to have complete control over compilation and assembly of the applications assets.\n",49,3682,"## Context\nThere are lots of different tasks that need processed in order to get the prototype kit up and running. Tasks such as; installing dependencies, moving files from dependencies into the app file structure, and most importantly - running the application.\n

##Decision
We will use a mixture on NPM scripts and [Gulp](https://gulpjs.com) to run our tasks. NPM scripts give us the core installation and start tasks as well as the ability to run Gulp tasks. Gulp, written in javascript, is very extensible and will allow us to have complete control over compilation and assembly of the applications assets.\n"
james-project/0031-distributed-mail-queue.md,"## Context\nMailQueue is a central component of SMTP infrastructure allowing asynchronous mail processing. This enables a short\nSMTP reply time despite a potentially longer mail processing time. It also works as a buffer during SMTP peak workload\nto not overload a server.\nFurthermore, when used as a Mail Exchange server (MX), the ability to add delays to be observed before dequeing elements\nallows, among others:\n- Delaying retries upon MX delivery failure to a remote site.\n- Throttling, which could be helpful for not being considered a spammer.\nA mailqueue also enables advanced administration operations like traffic review, discarding emails, resetting wait\ndelays, purging the queue, etc.\nSpring implementation and non distributed implementations rely on an embedded ActiveMQ to implement the MailQueue.\nEmails are being stored in a local file system. An administrator wishing to administrate the mailQueue will thus need\nto interact with all its James servers, which is not friendly in a distributed setup.\nDistributed James relies on the following third party softwares (among other):\n- **RabbitMQ** for messaging. Good at holding a queue, however some advanced administrative operations can't be\nimplemented with this component alone. This is the case for `browse`, `getSize` and `arbitrary mail removal`.\n- **Cassandra** is the metadata database. Due to **tombstone** being used for delete, queue is a well known anti-pattern.\n- **ObjectStorage** (Swift or S3) holds byte content.\n","Distributed James should ship a distributed MailQueue composing the following softwares with the following\nresponsibilities:\n- **RabbitMQ** for messaging. A rabbitMQ consumer will trigger dequeue operations.\n- A time series projection of the queue content (order by time list of mail metadata) will be maintained in **Cassandra** (see later). Time series avoid the\naforementioned tombstone anti-pattern, and no polling is performed on this projection.\n- **ObjectStorage** (Swift or S3) holds large byte content. This avoids overwhelming other softwares which do not scale\nas well in term of Input/Output operation per seconds.\nHere are details of the tables composing Cassandra MailQueue View data-model:\n- **enqueuedMailsV3** holds the time series. The primary key holds the queue name, the (rounded) time of enqueue\ndesigned as a slice, and a bucketCount. Slicing enables listing a large amount of items from a given point in time, in an\nfashion that is not achievable with a classic partition approach. The bucketCount enables sharding and avoids all writes\nat a given point in time to go to the same Cassandra partition. The clustering key is composed of an enqueueId - a\nunique identifier. The content holds the metadata of the email. This table enables, from a starting date, to load all of\nthe emails that have ever been in the mailQueue. Its content is never deleted.\n- **deletedMailsV2** tells wether a mail stored in *enqueuedMailsV3* had been deleted or not. The queueName and\nenqueueId are used as primary key. This table is updated upon dequeue and deletes. This table is queried upon dequeue\nto filter out deleted/purged items.\n- **browseStart** store the latest known point in time from which all previous emails had been deleted/dequeued. It\nenables to skip most deleted items upon browsing/deleting queue content. Its update is probability based and\nasynchronously piggy backed on dequeue.\nHere are the main mail operation sequences:\n- Upon **enqueue** mail content is stored in the *object storage*, an entry is added in *enqueuedMailsV3* and a message\nis fired on *rabbitMQ*.\n- **dequeue** is triggered by a rabbitMQ message to be received. *deletedMailsV2* is queried to know if the message had\nalready been deleted. If not, the mail content is retrieved from the *object storage*, then an entry is added in\n*deletedMailsV2* to notice the email had been dequeued. A dequeue has a random probability to trigger a browse start\nupdate. If so, from current browse start, *enqueuedMailsV3* content is iterated, and checked against *deletedMailsV2*\nuntil the first non deleted / dequeued email is found. This point becomes the new browse start. BrowseStart can never\npoint after the start of the current slice. A grace period upon browse start update is left to tolerate clock skew.\nUpdate of the browse start is done randomly as it is a simple way to avoid synchronisation in a distributed system: we\nensure liveness while uneeded browseStart updates being triggered would simply waste a few resources.\n- Upon **browse**, *enqueuedMailsV3* content is iterated, and checked against *deletedMailsV2*, starting from the\ncurrent browse start.\n- Upon **delete/purge**, *enqueuedMailsV3* content is iterated, and checked against *deletedMailsV2*. Mails matching\nthe condition are marked as deleted in *enqueuedMailsV3*.\n- Upon **getSize**, we perform a browse and count the returned elements.\nThe distributed mail queue requires a fine tuned configuration, which mostly depends of the count of Cassandra servers,\nand of the mailQueue throughput:\n- **sliceWindow** is the time period of a slice. All the elements of **enqueuedMailsV3** sharing the same slice are\nretrieved at once. The bigger, the more elements are going to be read at once, the less frequent browse start update\nwill be. Lower values might result in many almost empty slices to be read, generating higher read load. We recommend\n**sliceWindow** to be chosen from users maximum throughput so that approximately 10.000 emails be contained in a slice.\nOnly values dividing the current *sliceWindow* are allowed as new values (otherwize previous slices might not be found).\n- **bucketCount** enables spreading the writes in your Cassandra cluster using a bucketting strategy. Low values will\nlead to workload not to be spread evenly, higher values might result in uneeded reads upon browse. The count of Cassandra\nservers should be a good starting value. Only increasing the count of buckets is supported as a configuration update as\ndecreasing the bucket count might result in some buckets to be lost.\n- **updateBrowseStartPace** governs the probability of updating browseStart upon dequeue/deletes. We recommend choosing\na value guarantying a reasonable probability of updating the browse start every few slices. Too big values will lead to\nuneeded update of not yet finished slices. Too low values will end up in a more expensive browseStart update and browse\niterating through slices with all their content deleted. This value can be changed freely.\nWe rely on eventSourcing to validate the mailQueue configuration changes upon James start following the aforementioned rules.\n",321,2892,"## Context\nMailQueue is a central component of SMTP infrastructure allowing asynchronous mail processing. This enables a short\nSMTP reply time despite a potentially longer mail processing time. It also works as a buffer during SMTP peak workload\nto not overload a server.\nFurthermore, when used as a Mail Exchange server (MX), the ability to add delays to be observed before dequeing elements\nallows, among others:\n- Delaying retries upon MX delivery failure to a remote site.\n- Throttling, which could be helpful for not being considered a spammer.\nA mailqueue also enables advanced administration operations like traffic review, discarding emails, resetting wait\ndelays, purging the queue, etc.\nSpring implementation and non distributed implementations rely on an embedded ActiveMQ to implement the MailQueue.\nEmails are being stored in a local file system. An administrator wishing to administrate the mailQueue will thus need\nto interact with all its James servers, which is not friendly in a distributed setup.\nDistributed James relies on the following third party softwares (among other):\n- **RabbitMQ** for messaging. Good at holding a queue, however some advanced administrative operations can't be\nimplemented with this component alone. This is the case for `browse`, `getSize` and `arbitrary mail removal`.\n- **Cassandra** is the metadata database. Due to **tombstone** being used for delete, queue is a well known anti-pattern.\n- **ObjectStorage** (Swift or S3) holds byte content.\n

##Decision
Distributed James should ship a distributed MailQueue composing the following softwares with the following\nresponsibilities:\n- **RabbitMQ** for messaging. A rabbitMQ consumer will trigger dequeue operations.\n- A time series projection of the queue content (order by time list of mail metadata) will be maintained in **Cassandra** (see later). Time series avoid the\naforementioned tombstone anti-pattern, and no polling is performed on this projection.\n- **ObjectStorage** (Swift or S3) holds large byte content. This avoids overwhelming other softwares which do not scale\nas well in term of Input/Output operation per seconds.\nHere are details of the tables composing Cassandra MailQueue View data-model:\n- **enqueuedMailsV3** holds the time series. The primary key holds the queue name, the (rounded) time of enqueue\ndesigned as a slice, and a bucketCount. Slicing enables listing a large amount of items from a given point in time, in an\nfashion that is not achievable with a classic partition approach. The bucketCount enables sharding and avoids all writes\nat a given point in time to go to the same Cassandra partition. The clustering key is composed of an enqueueId - a\nunique identifier. The content holds the metadata of the email. This table enables, from a starting date, to load all of\nthe emails that have ever been in the mailQueue. Its content is never deleted.\n- **deletedMailsV2** tells wether a mail stored in *enqueuedMailsV3* had been deleted or not. The queueName and\nenqueueId are used as primary key. This table is updated upon dequeue and deletes. This table is queried upon dequeue\nto filter out deleted/purged items.\n- **browseStart** store the latest known point in time from which all previous emails had been deleted/dequeued. It\nenables to skip most deleted items upon browsing/deleting queue content. Its update is probability based and\nasynchronously piggy backed on dequeue.\nHere are the main mail operation sequences:\n- Upon **enqueue** mail content is stored in the *object storage*, an entry is added in *enqueuedMailsV3* and a message\nis fired on *rabbitMQ*.\n- **dequeue** is triggered by a rabbitMQ message to be received. *deletedMailsV2* is queried to know if the message had\nalready been deleted. If not, the mail content is retrieved from the *object storage*, then an entry is added in\n*deletedMailsV2* to notice the email had been dequeued. A dequeue has a random probability to trigger a browse start\nupdate. If so, from current browse start, *enqueuedMailsV3* content is iterated, and checked against *deletedMailsV2*\nuntil the first non deleted / dequeued email is found. This point becomes the new browse start. BrowseStart can never\npoint after the start of the current slice. A grace period upon browse start update is left to tolerate clock skew.\nUpdate of the browse start is done randomly as it is a simple way to avoid synchronisation in a distributed system: we\nensure liveness while uneeded browseStart updates being triggered would simply waste a few resources.\n- Upon **browse**, *enqueuedMailsV3* content is iterated, and checked against *deletedMailsV2*, starting from the\ncurrent browse start.\n- Upon **delete/purge**, *enqueuedMailsV3* content is iterated, and checked against *deletedMailsV2*. Mails matching\nthe condition are marked as deleted in *enqueuedMailsV3*.\n- Upon **getSize**, we perform a browse and count the returned elements.\nThe distributed mail queue requires a fine tuned configuration, which mostly depends of the count of Cassandra servers,\nand of the mailQueue throughput:\n- **sliceWindow** is the time period of a slice. All the elements of **enqueuedMailsV3** sharing the same slice are\nretrieved at once. The bigger, the more elements are going to be read at once, the less frequent browse start update\nwill be. Lower values might result in many almost empty slices to be read, generating higher read load. We recommend\n**sliceWindow** to be chosen from users maximum throughput so that approximately 10.000 emails be contained in a slice.\nOnly values dividing the current *sliceWindow* are allowed as new values (otherwize previous slices might not be found).\n- **bucketCount** enables spreading the writes in your Cassandra cluster using a bucketting strategy. Low values will\nlead to workload not to be spread evenly, higher values might result in uneeded reads upon browse. The count of Cassandra\nservers should be a good starting value. Only increasing the count of buckets is supported as a configuration update as\ndecreasing the bucket count might result in some buckets to be lost.\n- **updateBrowseStartPace** governs the probability of updating browseStart upon dequeue/deletes. We recommend choosing\na value guarantying a reasonable probability of updating the browse start every few slices. Too big values will lead to\nuneeded update of not yet finished slices. Too low values will end up in a more expensive browseStart update and browse\niterating through slices with all their content deleted. This value can be changed freely.\nWe rely on eventSourcing to validate the mailQueue configuration changes upon James start following the aforementioned rules.\n"
api-catalogue/0001-remove-tech-docs-gem-dependency.md,"## Context\nThe main data source of the API catalogue website was originally a collection of\nmarkdown files. The build process uses the Middleman static site generator\nconfigured by the [Tech Docs Gem](https://github.com/alphagov/tech-docs-gem)\n('TDG').\nThe TDG provides additional functionality including search, sidebar\nnavigation ('Table of Contents'), the layout, and styling.\nThe TDG is not necessarily a good fit for the API catalogue because the project\nisn't purely documentation, and our data source is now a CSV.\nIn particular it is difficult to override templates inherited from the gem, to\nadjust the layout on a particular page or add page-specific JavaScript for\nexample.\nUsing TDG to render the Table of Contents is slow for our site because\nby design every page is re-rendered multiple times to pull out the headings\n(adding over a minute to build times).\nThe TDG also requires specific dependency versions. These version\nrestrictions prevent us being in control of version upgrades which are necessary\nto remain on support versions and receive security patches.\n",Remove the TDG as a dependency by vendoring the code relevant to\nthe API catalogue directly into the project itself.\n,228,3586,"## Context\nThe main data source of the API catalogue website was originally a collection of\nmarkdown files. The build process uses the Middleman static site generator\nconfigured by the [Tech Docs Gem](https://github.com/alphagov/tech-docs-gem)\n('TDG').\nThe TDG provides additional functionality including search, sidebar\nnavigation ('Table of Contents'), the layout, and styling.\nThe TDG is not necessarily a good fit for the API catalogue because the project\nisn't purely documentation, and our data source is now a CSV.\nIn particular it is difficult to override templates inherited from the gem, to\nadjust the layout on a particular page or add page-specific JavaScript for\nexample.\nUsing TDG to render the Table of Contents is slow for our site because\nby design every page is re-rendered multiple times to pull out the headings\n(adding over a minute to build times).\nThe TDG also requires specific dependency versions. These version\nrestrictions prevent us being in control of version upgrades which are necessary\nto remain on support versions and receive security patches.\n

##Decision
Remove the TDG as a dependency by vendoring the code relevant to\nthe API catalogue directly into the project itself.\n"
adr/ADR-40-ui-dependencies-upgrades.md,"## Context and Problem Statement\nThe organization has several UI apps and libraries and of them have different React versions, causing issues whenever we want to consume them. To remove these problems, and to keep every app updated, we need to move to React 17 in every UI app and lib, specially in the UI repository that contains most of our shared UI components.\nUpdating the UI repository to the latest version of React implies updating `react-semantic-ui` to its latest version, ending up in [a major change that removed the `Responsive` component](https://github.com/Semantic-Org/Semantic-UI-React/pull/4008), a widely used component dedicated to conditionally rendering different components based on their display. Removing this component will cause a breaking change in our current [UI library](https://github.com/decentraland/ui) and will imply everyone to get on board of this breaking change, but a different strategy can be chosen by keeping the `Responsive` component by copying it from the library until everyone gets on board with an alternative.\nWe need to provide, alongside this update, an alternative library to the `Responsive` component, providing a similar or a better API for rendering components according to device sizes.\n- The `@artsy/fresnel` works by using a ContextProvider component that wraps the whole application, coupling the media query solution to this library.\n- Doesn't have hooks support.\n#### Second alternative (react-semantic-ui)\n##### Advantages\n- The libary doesn't require a provider or something previously set in an application to use it (non-coupling dependency).\n- Provides hooks and component solutions for rendering components with different media queries, providing a versatile that allows us to render different components or part of the components by using the hooks.\n##### Disadvantages\n- Bad SSR support.\n","The option to keep the an exact copy of the `Responsive` component (from the old `react-semantic-ui` lib version) was chosen in order to have a frictionless upgrade of the library.\nThe procedure in which we'll be handling the upgrade is the following:\n1. A non breaking change upgrade will be provided to our [UI library](https://github.com/decentraland/ui), keeping the `Responsive` component as a deprecated component and an alternative (describe below) will be provided to replace it.\n2. A breaking change upgrade will be applied to our [UI library](https://github.com/decentraland/ui), whenever all of our dependencies are updated, removing the `Responsive` component.\nWe’ll be providing, alongside the `Responsive` component a set of components and hooks to replace it, using the `react-responsive`, library. This library was chosen in favor of the recommended `@artsy/fresnel` mainly because of its versatility. The need of having to set a provider at the application's root level, (coupling the users of this dependency to `@artsy/fresnel`) to have better SSR support that we don't currently need, made us decide not to go with it.\nThe components built with the `react-responsive` and exposed to the consumers of our [UI library](https://github.com/decentraland/ui) will be the following:\n- **Desktop** (for devices with `min width: 992`)\n- **Tablet** (for devices with `min width: 768 and max width: 991`)\n- **TabletAndBelow** (for devices with `max width: 991`, that is taking into consideration tablets and mobile devices)\n- **Mobile** (for devices with `max width: 767`)\n- **NotMobile** (for devices that don't comply with the requirements specified in Mobile)\nThese components describe a conditional rendering based on the media the page in being rendered.\nWhere we had:\n```tsx\n<Responsive\nas={Menu}\nsecondary\nstackable\nminWidth={Responsive.onlyTablet.minWidth}\n>\n<a className=""dcl navbar-logo"" href=""https://decentraland.org"">\n<Logo />\n</a>\n{this.renderLeftMenu()}\n</Responsive>\n<Responsive\n{...Responsive.onlyMobile}\nclassName=""dcl navbar-mobile-menu""\n>\n<a className=""dcl navbar-logo"" href=""https://decentraland.org"">\n<Logo />\n</a>\n<Header\nsize=""small""\nclassName={`dcl active-page ${\nthis.state.toggle ? 'caret-up' : 'caret-down'\n}`}\nonClick={this.handleToggle}\n>\n{activePage}\n</Header>\n</Responsive>\n```\nWe now have:\n```tsx\n<NotMobile>\n<Menu secondary stackable>\n<a className=""dcl navbar-logo"" href=""https://decentraland.org"">\n<Logo />\n</a>\n{this.renderLeftMenu()}\n</Menu>\n</NotMobile>\n<Mobile>\n<div className=""dcl navbar-mobile-menu"">\n<a className=""dcl navbar-logo"" href=""https://decentraland.org"">\n<Logo />\n</a>\n<Header\nsize=""small""\nclassName={`dcl active-page ${\nthis.state.toggle ? 'caret-up' : 'caret-down'\n}`}\nonClick={this.handleToggle}\n>\n{activePage}\n</Header>\n</div>\n</Mobile>\n```\nAnd, alongside these components, as explained before, we're exposing the following set of hooks:\n- **useDesktopMediaQuery**\n- **useTabletMediaQuery**\n- **useTabletAndBelowMediaQuery**\n- **useMobileMediaQuery**\n- **useNotMobileMediaQuery**\nWhich return true if the device is the one defined as the name of the hook.\nThese types of hooks will provide us with newer functionality, being able to customize small portions of our code instead of forking our components into two.\nAs an example, we can apply certain styles by simply:\n```tsx\nconst isMobile = useMobileMediaQuery()\nconst classes = isMobile ? ""dcl mobile"" : ""dcl""\n<div className={classes}>\n...\n</div>\n```\n",379,4613,"## Context and Problem Statement\nThe organization has several UI apps and libraries and of them have different React versions, causing issues whenever we want to consume them. To remove these problems, and to keep every app updated, we need to move to React 17 in every UI app and lib, specially in the UI repository that contains most of our shared UI components.\nUpdating the UI repository to the latest version of React implies updating `react-semantic-ui` to its latest version, ending up in [a major change that removed the `Responsive` component](https://github.com/Semantic-Org/Semantic-UI-React/pull/4008), a widely used component dedicated to conditionally rendering different components based on their display. Removing this component will cause a breaking change in our current [UI library](https://github.com/decentraland/ui) and will imply everyone to get on board of this breaking change, but a different strategy can be chosen by keeping the `Responsive` component by copying it from the library until everyone gets on board with an alternative.\nWe need to provide, alongside this update, an alternative library to the `Responsive` component, providing a similar or a better API for rendering components according to device sizes.\n- The `@artsy/fresnel` works by using a ContextProvider component that wraps the whole application, coupling the media query solution to this library.\n- Doesn't have hooks support.\n#### Second alternative (react-semantic-ui)\n##### Advantages\n- The libary doesn't require a provider or something previously set in an application to use it (non-coupling dependency).\n- Provides hooks and component solutions for rendering components with different media queries, providing a versatile that allows us to render different components or part of the components by using the hooks.\n##### Disadvantages\n- Bad SSR support.\n

##Decision
The option to keep the an exact copy of the `Responsive` component (from the old `react-semantic-ui` lib version) was chosen in order to have a frictionless upgrade of the library.\nThe procedure in which we'll be handling the upgrade is the following:\n1. A non breaking change upgrade will be provided to our [UI library](https://github.com/decentraland/ui), keeping the `Responsive` component as a deprecated component and an alternative (describe below) will be provided to replace it.\n2. A breaking change upgrade will be applied to our [UI library](https://github.com/decentraland/ui), whenever all of our dependencies are updated, removing the `Responsive` component.\nWe’ll be providing, alongside the `Responsive` component a set of components and hooks to replace it, using the `react-responsive`, library. This library was chosen in favor of the recommended `@artsy/fresnel` mainly because of its versatility. The need of having to set a provider at the application's root level, (coupling the users of this dependency to `@artsy/fresnel`) to have better SSR support that we don't currently need, made us decide not to go with it.\nThe components built with the `react-responsive` and exposed to the consumers of our [UI library](https://github.com/decentraland/ui) will be the following:\n- **Desktop** (for devices with `min width: 992`)\n- **Tablet** (for devices with `min width: 768 and max width: 991`)\n- **TabletAndBelow** (for devices with `max width: 991`, that is taking into consideration tablets and mobile devices)\n- **Mobile** (for devices with `max width: 767`)\n- **NotMobile** (for devices that don't comply with the requirements specified in Mobile)\nThese components describe a conditional rendering based on the media the page in being rendered.\nWhere we had:\n```tsx\n<Responsive\nas={Menu}\nsecondary\nstackable\nminWidth={Responsive.onlyTablet.minWidth}\n>\n<a className=""dcl navbar-logo"" href=""https://decentraland.org"">\n<Logo />\n</a>\n{this.renderLeftMenu()}\n</Responsive>\n<Responsive\n{...Responsive.onlyMobile}\nclassName=""dcl navbar-mobile-menu""\n>\n<a className=""dcl navbar-logo"" href=""https://decentraland.org"">\n<Logo />\n</a>\n<Header\nsize=""small""\nclassName={`dcl active-page ${\nthis.state.toggle ? 'caret-up' : 'caret-down'\n}`}\nonClick={this.handleToggle}\n>\n{activePage}\n</Header>\n</Responsive>\n```\nWe now have:\n```tsx\n<NotMobile>\n<Menu secondary stackable>\n<a className=""dcl navbar-logo"" href=""https://decentraland.org"">\n<Logo />\n</a>\n{this.renderLeftMenu()}\n</Menu>\n</NotMobile>\n<Mobile>\n<div className=""dcl navbar-mobile-menu"">\n<a className=""dcl navbar-logo"" href=""https://decentraland.org"">\n<Logo />\n</a>\n<Header\nsize=""small""\nclassName={`dcl active-page ${\nthis.state.toggle ? 'caret-up' : 'caret-down'\n}`}\nonClick={this.handleToggle}\n>\n{activePage}\n</Header>\n</div>\n</Mobile>\n```\nAnd, alongside these components, as explained before, we're exposing the following set of hooks:\n- **useDesktopMediaQuery**\n- **useTabletMediaQuery**\n- **useTabletAndBelowMediaQuery**\n- **useMobileMediaQuery**\n- **useNotMobileMediaQuery**\nWhich return true if the device is the one defined as the name of the hook.\nThese types of hooks will provide us with newer functionality, being able to customize small portions of our code instead of forking our components into two.\nAs an example, we can apply certain styles by simply:\n```tsx\nconst isMobile = useMobileMediaQuery()\nconst classes = isMobile ? ""dcl mobile"" : ""dcl""\n<div className={classes}>\n...\n</div>\n```\n"
CCS-Architecture-Decision-Records/0008-cmp-use-shared-store-for-capturing-buyer-needs.md,"## Context\nEach commercial agreement will ask the buyer questions in order to ascertain what outcomes and supplier offers are applicable to the buyer. Some of these questions will be the same across agreements. Many will have common patterns and data, even if they differ in detail.\nIn the future we will want to more automatically match buyers’ articulations of need so as to work out which products CCS can offer to meet their need.\nThe buyer needs data needs to be stored during a buying journey, but final agreement details, when the buyer agrees a call-off for instance, will need to be stored in the agreement records.\n",Use a shared component to store user tasks in a question and answer format during buying journeys\nThe Q&A definitions should be defined from a common repository defining the agreements\nAgreed outcomes must be stored in the agreement records\n,123,1001,"## Context\nEach commercial agreement will ask the buyer questions in order to ascertain what outcomes and supplier offers are applicable to the buyer. Some of these questions will be the same across agreements. Many will have common patterns and data, even if they differ in detail.\nIn the future we will want to more automatically match buyers’ articulations of need so as to work out which products CCS can offer to meet their need.\nThe buyer needs data needs to be stored during a buying journey, but final agreement details, when the buyer agrees a call-off for instance, will need to be stored in the agreement records.\n

##Decision
Use a shared component to store user tasks in a question and answer format during buying journeys\nThe Q&A definitions should be defined from a common repository defining the agreements\nAgreed outcomes must be stored in the agreement records\n"
digitalrig-metal-aws/0003-use-aws-codepipeline-and-codebuild-instead-of-travis.md,## Context\nTravis has some limitations about what stages you can use to create a pipleine.  We still desire to have a hosted/PaaS CI/CD solution\n,* Use AWS CodePipeline and CodeBuild instead of Travis\n* We will aim to create a new Pipeline/Build and potentially execution environment per branch.\n* This will be manual at first and later could be automated via webhooks and lambda functions\n,36,1741,"## Context\nTravis has some limitations about what stages you can use to create a pipleine.  We still desire to have a hosted/PaaS CI/CD solution\n

##Decision
* Use AWS CodePipeline and CodeBuild instead of Travis\n* We will aim to create a new Pipeline/Build and potentially execution environment per branch.\n* This will be manual at first and later could be automated via webhooks and lambda functions\n"
documents-api/0002-use-base64-content-for-file-uploading.md,"## Context\nThere are a number of ways an API could allow clients to upload files to S3, the popular ones:\n- Allow the API to accept Base 64 encoded files in a JSON POST request and subsequently send this blob to S3\n- Allow the API to accept multipart form uploads, compile the parts on the server then send the file to S3\n- Use the S3 Presigned URL functionality, which allows the client to act as the IAM which created the URL for a single operation, and upload the file directly to S3 themselves\n","We decided to use the first option (base 64 encoded uploads), for the following reason:\n- We do not want to expose any AWS links to outside parties. This ensures that all access to AWS resources is controlled by Hackney authentication mechanisms.\n",113,3987,"## Context\nThere are a number of ways an API could allow clients to upload files to S3, the popular ones:\n- Allow the API to accept Base 64 encoded files in a JSON POST request and subsequently send this blob to S3\n- Allow the API to accept multipart form uploads, compile the parts on the server then send the file to S3\n- Use the S3 Presigned URL functionality, which allows the client to act as the IAM which created the URL for a single operation, and upload the file directly to S3 themselves\n

##Decision
We decided to use the first option (base 64 encoded uploads), for the following reason:\n- We do not want to expose any AWS links to outside parties. This ensures that all access to AWS resources is controlled by Hackney authentication mechanisms.\n"
smarthub/0002-use-lerna-for-package-versioning.md,## Context\nSmarthub project consist of multiple packages which are the part of Smarthub SDK. Complex dependency graph forces us to update dependent packages manually every time dependency has changed.\n,Migrate code base to monorepo structure and use `lerna` for versioning management.\n,39,3691,"## Context\nSmarthub project consist of multiple packages which are the part of Smarthub SDK. Complex dependency graph forces us to update dependent packages manually every time dependency has changed.\n

##Decision
Migrate code base to monorepo structure and use `lerna` for versioning management.\n"
js-sdk/0003-allow-only-tft-token.md,"## Context\nDrop FreeTFT, TFTA tokens from SDK\n","Completely drop FreeTFT, TFTA from currency options and don't ask for it at all. should always be TFT in the UI\n",14,5198,"## Context\nDrop FreeTFT, TFTA tokens from SDK\n

##Decision
Completely drop FreeTFT, TFTA from currency options and don't ask for it at all. should always be TFT in the UI\n"
bosh-bootloader/0002-replace-go-bindata-with-packr2.md,"## Context\nThe issue motivating this decision, and any context that influences or constrains the decision.\nThe original author of go-bindata delete their account and then the repo was\nrecreated under a different owner. The dependency has shifted around enough that\nwe have lost faith in the intention of the maintainers\n[more details here](https://twitter.com/francesc/status/961249107020001280?lang=en)\nAlso, some of the development use cases around go-bindata (like what is bundled into the code\nduring a test run or final build) made it hard to reason about.\n",Use [Packr2](https://github.com/gobuffalo/packr/tree/master/v2) instead.\n,126,3873,"## Context\nThe issue motivating this decision, and any context that influences or constrains the decision.\nThe original author of go-bindata delete their account and then the repo was\nrecreated under a different owner. The dependency has shifted around enough that\nwe have lost faith in the intention of the maintainers\n[more details here](https://twitter.com/francesc/status/961249107020001280?lang=en)\nAlso, some of the development use cases around go-bindata (like what is bundled into the code\nduring a test run or final build) made it hard to reason about.\n

##Decision
Use [Packr2](https://github.com/gobuffalo/packr/tree/master/v2) instead.\n"
community/dr-012-Prettier_as_JavaScript_code_formatter.md,"## Context\nThe Code Style Guide is a set of conventions on how to write the source code. It covers many areas, including the use of `camelCase` vs `PascalCase` for variable identifiers, whether or not to use a semicolon as a statement terminator, or the use of tabs or spaces for indentation.\nIt is obvious that an enforced, unified code style in a project is beneficial. Not only does it improve the readability, but it also saves you from a lot of noise while looking at diffs, caused by unadjusted whitespaces, different break line settings, and other issues. It also ends discussions around the style itself. Once applied, everyone can write code the way they want, and let the formatter do the work. In the end, it saves time and energy.\n[Prettier](https://prettier.io) is one of the solutions dedicated to code formatting. It does not enforce such code quality rules as the use of globally scoped variables or naming. It only enforces the formatting rules. It can be used as a plugin for selected IDEs, as a pre-commit `git` hook, or as a standalone CLI tool. No matter which option you choose, it produces the same output given its configuration.\nIt was chosen because of its simplicity, ease of configuration, small amount of available options, and support for JavaScript, TypeScript, GraphQL, CSS, SCSS, and JSON, all of which are used in Kyma projects.\n",The decision is to use Prettier as the only code formatter for JavaScript-based projects.\n,301,3451,"## Context\nThe Code Style Guide is a set of conventions on how to write the source code. It covers many areas, including the use of `camelCase` vs `PascalCase` for variable identifiers, whether or not to use a semicolon as a statement terminator, or the use of tabs or spaces for indentation.\nIt is obvious that an enforced, unified code style in a project is beneficial. Not only does it improve the readability, but it also saves you from a lot of noise while looking at diffs, caused by unadjusted whitespaces, different break line settings, and other issues. It also ends discussions around the style itself. Once applied, everyone can write code the way they want, and let the formatter do the work. In the end, it saves time and energy.\n[Prettier](https://prettier.io) is one of the solutions dedicated to code formatting. It does not enforce such code quality rules as the use of globally scoped variables or naming. It only enforces the formatting rules. It can be used as a plugin for selected IDEs, as a pre-commit `git` hook, or as a standalone CLI tool. No matter which option you choose, it produces the same output given its configuration.\nIt was chosen because of its simplicity, ease of configuration, small amount of available options, and support for JavaScript, TypeScript, GraphQL, CSS, SCSS, and JSON, all of which are used in Kyma projects.\n

##Decision
The decision is to use Prettier as the only code formatter for JavaScript-based projects.\n"
corona-hackathon/0010-branching-strategy.md,## Context\nWe have to make a decision on the branching strategy for development.\n,[Git Flow](https://danielkummer.github.io/git-flow-cheatsheet/) it will be.\n,17,3653,"## Context\nWe have to make a decision on the branching strategy for development.\n

##Decision
[Git Flow](https://danielkummer.github.io/git-flow-cheatsheet/) it will be.\n"
konfetti/0004-use-lazy-loading.md,## Context\nWe need to avoid side effects on configuration loading and prevent the need to fully configure the settings to run a subset of tests in projects using `konfetti`.\n,"We will use a lazy evaluation approach, similar to [implemented in Django](https://github.com/django/django/blob/master/django/conf/__init__.py#L42)\n",37,3572,"## Context\nWe need to avoid side effects on configuration loading and prevent the need to fully configure the settings to run a subset of tests in projects using `konfetti`.\n

##Decision
We will use a lazy evaluation approach, similar to [implemented in Django](https://github.com/django/django/blob/master/django/conf/__init__.py#L42)\n"
embvm-core/0014-refactor-driver-interfaces-to-use-namespaces.md,"## Context\nDriver interfaces were previously defined in the global namespace, and associated types were defined as `struct`s in the global namespace with a generic name like `tof`:\n```\nstruct tof\n{\nusing distance_t = uint16_t;\nusing cb_t = stdext::inplace_function<void(distance_t)>;\nstatic const distance_t INVALID_RANGE = UINT16_MAX;\nenum class mode\n{\ndefaultRange = 0,\nshortRange,\nmedRange,\nlongRange,\n};\n}''\n```\nInterface classes would inherit from these structs:\n```\nclass TimeOfFlight : public embvm::DriverBase, public embvm::tof\n```\nAnd the effect was similar to namespacing (`embvm::tof::mode`):\n```\nvirtual embvm::tof::mode mode(embvm::tof::mode m) = 0;\n```\nEssentially, we are recreating a feature that `namespace` already provides and complicating our inheritance chains.\n","Each driver interface class and any related types are to be stored in a separate namespace per driver type. This keeps related types tied together, improves our generated documentation, and provides improved names for classes and types.\n",225,3027,"## Context\nDriver interfaces were previously defined in the global namespace, and associated types were defined as `struct`s in the global namespace with a generic name like `tof`:\n```\nstruct tof\n{\nusing distance_t = uint16_t;\nusing cb_t = stdext::inplace_function<void(distance_t)>;\nstatic const distance_t INVALID_RANGE = UINT16_MAX;\nenum class mode\n{\ndefaultRange = 0,\nshortRange,\nmedRange,\nlongRange,\n};\n}''\n```\nInterface classes would inherit from these structs:\n```\nclass TimeOfFlight : public embvm::DriverBase, public embvm::tof\n```\nAnd the effect was similar to namespacing (`embvm::tof::mode`):\n```\nvirtual embvm::tof::mode mode(embvm::tof::mode m) = 0;\n```\nEssentially, we are recreating a feature that `namespace` already provides and complicating our inheritance chains.\n

##Decision
Each driver interface class and any related types are to be stored in a separate namespace per driver type. This keeps related types tied together, improves our generated documentation, and provides improved names for classes and types.\n"
infection/0004-PHPUnit-expect-exception-over-try-catch.md,"### Context\nWhen executing code that is expected to fail in a test case, there is two ways to do this:\n```php\nfunction test_something(): void {\n// ...\ntry {\n// the statement that fail\n$this->fail();\n} catch (Exception $e) {\n// ...\n}\n}\n```\nOr:\n```php\nfunction test_something(): void {\n// ...\n$this->expectException($exception)\n// the statement that fail\n}\n```\n### Decision\nAs recommended by [Sebastian Bergmann][sebastian-bergmann] in\n[this article][phpunit-exception-best-practices], since in both cases a PHPUnit specific API is\nnecessary, the decision taken is to leverage the `expectException*()` API when possible.\nA pull request to fix this practice in the whole codebase may be done but has not been made\nmandatory. New pull requests though should stick to this practice.\n### Status\nAccepted ([#1090][1090])\n[sebastian-bergmann]: https://thephp.cc/company/consultants/sebastian-bergmann\n[phpunit-exception-best-practices]: https://thephp.cc/news/2016/02/questioning-phpunit-best-practices\n[1090]: https://github.com/infection/infection/pull/1061\n","As recommended by [Sebastian Bergmann][sebastian-bergmann] in\n[this article][phpunit-exception-best-practices], since in both cases a PHPUnit specific API is\nnecessary, the decision taken is to leverage the `expectException*()` API when possible.\nA pull request to fix this practice in the whole codebase may be done but has not been made\nmandatory. New pull requests though should stick to this practice.\n### Status\nAccepted ([#1090][1090])\n[sebastian-bergmann]: https://thephp.cc/company/consultants/sebastian-bergmann\n[phpunit-exception-best-practices]: https://thephp.cc/news/2016/02/questioning-phpunit-best-practices\n[1090]: https://github.com/infection/infection/pull/1061\n",285,68,"### Context\nWhen executing code that is expected to fail in a test case, there is two ways to do this:\n```php\nfunction test_something(): void {\n// ...\ntry {\n// the statement that fail\n$this->fail();\n} catch (Exception $e) {\n// ...\n}\n}\n```\nOr:\n```php\nfunction test_something(): void {\n// ...\n$this->expectException($exception)\n// the statement that fail\n}\n```\n### Decision\nAs recommended by [Sebastian Bergmann][sebastian-bergmann] in\n[this article][phpunit-exception-best-practices], since in both cases a PHPUnit specific API is\nnecessary, the decision taken is to leverage the `expectException*()` API when possible.\nA pull request to fix this practice in the whole codebase may be done but has not been made\nmandatory. New pull requests though should stick to this practice.\n### Status\nAccepted ([#1090][1090])\n[sebastian-bergmann]: https://thephp.cc/company/consultants/sebastian-bergmann\n[phpunit-exception-best-practices]: https://thephp.cc/news/2016/02/questioning-phpunit-best-practices\n[1090]: https://github.com/infection/infection/pull/1061\n

##Decision
As recommended by [Sebastian Bergmann][sebastian-bergmann] in\n[this article][phpunit-exception-best-practices], since in both cases a PHPUnit specific API is\nnecessary, the decision taken is to leverage the `expectException*()` API when possible.\nA pull request to fix this practice in the whole codebase may be done but has not been made\nmandatory. New pull requests though should stick to this practice.\n### Status\nAccepted ([#1090][1090])\n[sebastian-bergmann]: https://thephp.cc/company/consultants/sebastian-bergmann\n[phpunit-exception-best-practices]: https://thephp.cc/news/2016/02/questioning-phpunit-best-practices\n[1090]: https://github.com/infection/infection/pull/1061\n"
pace-developers/0007-developer-scripts-storage-location.md,"## Context\nWhen developing new algorithms and features for PACE-related software,\ndevelopers often create useful demo/visualisation scripts for their own use.\nThese scripts could be useful or interesting for other developers, and are\nimportant for reproducibility or justifying design decisions. They should be\nstored somewhere in version control so that they can be easily accessed by any\ndevelopers and referred to later. However, they are not intended for general\nuse and will not be actively maintained or tested. There are 2 main options:\n* Store them in a `dev_scripts` directory in each separate project repository\n* Store them in a `scripts` directory in `pace-developers`\nIf they're in the `dev_scripts` directory for each project repository:\n+ All in one place\n+ Scripts will be close to the code they are used for\n- Scripts may not work with the version of the code they are distributed with\n- It's unclear where scripts that use more than one project would go\n- Despite the folder being called `dev_scripts` people might expect the scripts\nto actually work as they're in the main project repository\nIf they're in a `scripts` directory in `pace-developers`:\n+ They can be kept close to the decision-making developer documentation that\nthey support\n+ A version can be specified for any project dependencies\n","Developer scripts will be stored in an appropriately placed `scripts`\ndirectory in the `pace-developers` repository. Depending on whether the\nscript is tied to a particular software, or general algorithm development\nit could be stored in `pace-developers/euphonic/scripts` or\n`pace-developers/powder_averaging/scripts` for example.\n",283,5176,"## Context\nWhen developing new algorithms and features for PACE-related software,\ndevelopers often create useful demo/visualisation scripts for their own use.\nThese scripts could be useful or interesting for other developers, and are\nimportant for reproducibility or justifying design decisions. They should be\nstored somewhere in version control so that they can be easily accessed by any\ndevelopers and referred to later. However, they are not intended for general\nuse and will not be actively maintained or tested. There are 2 main options:\n* Store them in a `dev_scripts` directory in each separate project repository\n* Store them in a `scripts` directory in `pace-developers`\nIf they're in the `dev_scripts` directory for each project repository:\n+ All in one place\n+ Scripts will be close to the code they are used for\n- Scripts may not work with the version of the code they are distributed with\n- It's unclear where scripts that use more than one project would go\n- Despite the folder being called `dev_scripts` people might expect the scripts\nto actually work as they're in the main project repository\nIf they're in a `scripts` directory in `pace-developers`:\n+ They can be kept close to the decision-making developer documentation that\nthey support\n+ A version can be specified for any project dependencies\n

##Decision
Developer scripts will be stored in an appropriately placed `scripts`\ndirectory in the `pace-developers` repository. Depending on whether the\nscript is tied to a particular software, or general algorithm development\nit could be stored in `pace-developers/euphonic/scripts` or\n`pace-developers/powder_averaging/scripts` for example.\n"
google-cloud-cpp/2019-01-04-error-reporting-with-statusor.md,"**Context**: We know there will be users of these C++ libraries who want to use\nC++ exceptions as well as those who are not able to. Our C++ libraries must work\nfor all of our users, regardless of their ability to use exceptions.\n**Decision**: None of our APIs will throw exceptions to indicate errors.\nInstead, our APIs will typically report errors to callers by returning a\n`Status` or `StatusOr<T>` object, unless the library we're using has another\nnon-throwing way to report errors (e.g., [badbit][badbit-link] in the standard\nI/O library).\n**Consequences**: This decision will result in a single set of APIs and a\nconsistent vocabulary for all users, whether or not they choose to compile with\nexceptions. This decision does not prevent callers from using exceptions in\ntheir own code.\nA downside of this decision is that our APIs will not be natural or idiomatic\nfor the [50+%][survey-link] of users who might prefer exceptions for error\nreporting.\nChanging existing APIs from throwing exceptions to returning `StatusOr<T>` is a\nbreaking change. As of this writing (Jan 2019), this project has a\n[Google Cloud Storage][gcs-link] component that is at the Alpha quality level,\nand a [Google Cloud Bigtable][bigtable-link] that is already at the Beta quality\nlevel. We plan to immediately change the API for Google Cloud Storage. We have\nno immediate plans to change the API for Cloud Bigtable. We will communicate a\ntimeline to change this API in a separate document.\n[badbit-link]: https://en.cppreference.com/w/cpp/io/ios_base/iostate\n[bigtable-link]: https://github.com/googleapis/google-cloud-cpp/tree/main/google/cloud/bigtable\n[gcs-link]: https://github.com/googleapis/google-cloud-cpp/tree/main/google/cloud/storage\n[survey-link]: https://isocpp.org/blog/2018/03/results-summary-cpp-foundation-developer-survey-lite-2018-02\n","Instead, our APIs will typically report errors to callers by returning a\n`Status` or `StatusOr<T>` object, unless the library we're using has another\nnon-throwing way to report errors (e.g., [badbit][badbit-link] in the standard\nI/O library).\n**Consequences**: This decision will result in a single set of APIs and a\nconsistent vocabulary for all users, whether or not they choose to compile with\nexceptions. This decision does not prevent callers from using exceptions in\ntheir own code.\nA downside of this decision is that our APIs will not be natural or idiomatic\nfor the [50+%][survey-link] of users who might prefer exceptions for error\nreporting.\nChanging existing APIs from throwing exceptions to returning `StatusOr<T>` is a\nbreaking change. As of this writing (Jan 2019), this project has a\n[Google Cloud Storage][gcs-link] component that is at the Alpha quality level,\nand a [Google Cloud Bigtable][bigtable-link] that is already at the Beta quality\nlevel. We plan to immediately change the API for Google Cloud Storage. We have\nno immediate plans to change the API for Cloud Bigtable. We will communicate a\ntimeline to change this API in a separate document.\n[badbit-link]: https://en.cppreference.com/w/cpp/io/ios_base/iostate\n[bigtable-link]: https://github.com/googleapis/google-cloud-cpp/tree/main/google/cloud/bigtable\n[gcs-link]: https://github.com/googleapis/google-cloud-cpp/tree/main/google/cloud/storage\n[survey-link]: https://isocpp.org/blog/2018/03/results-summary-cpp-foundation-developer-survey-lite-2018-02\n",436,2467,"**Context**: We know there will be users of these C++ libraries who want to use\nC++ exceptions as well as those who are not able to. Our C++ libraries must work\nfor all of our users, regardless of their ability to use exceptions.\n**Decision**: None of our APIs will throw exceptions to indicate errors.\nInstead, our APIs will typically report errors to callers by returning a\n`Status` or `StatusOr<T>` object, unless the library we're using has another\nnon-throwing way to report errors (e.g., [badbit][badbit-link] in the standard\nI/O library).\n**Consequences**: This decision will result in a single set of APIs and a\nconsistent vocabulary for all users, whether or not they choose to compile with\nexceptions. This decision does not prevent callers from using exceptions in\ntheir own code.\nA downside of this decision is that our APIs will not be natural or idiomatic\nfor the [50+%][survey-link] of users who might prefer exceptions for error\nreporting.\nChanging existing APIs from throwing exceptions to returning `StatusOr<T>` is a\nbreaking change. As of this writing (Jan 2019), this project has a\n[Google Cloud Storage][gcs-link] component that is at the Alpha quality level,\nand a [Google Cloud Bigtable][bigtable-link] that is already at the Beta quality\nlevel. We plan to immediately change the API for Google Cloud Storage. We have\nno immediate plans to change the API for Cloud Bigtable. We will communicate a\ntimeline to change this API in a separate document.\n[badbit-link]: https://en.cppreference.com/w/cpp/io/ios_base/iostate\n[bigtable-link]: https://github.com/googleapis/google-cloud-cpp/tree/main/google/cloud/bigtable\n[gcs-link]: https://github.com/googleapis/google-cloud-cpp/tree/main/google/cloud/storage\n[survey-link]: https://isocpp.org/blog/2018/03/results-summary-cpp-foundation-developer-survey-lite-2018-02\n

##Decision
Instead, our APIs will typically report errors to callers by returning a\n`Status` or `StatusOr<T>` object, unless the library we're using has another\nnon-throwing way to report errors (e.g., [badbit][badbit-link] in the standard\nI/O library).\n**Consequences**: This decision will result in a single set of APIs and a\nconsistent vocabulary for all users, whether or not they choose to compile with\nexceptions. This decision does not prevent callers from using exceptions in\ntheir own code.\nA downside of this decision is that our APIs will not be natural or idiomatic\nfor the [50+%][survey-link] of users who might prefer exceptions for error\nreporting.\nChanging existing APIs from throwing exceptions to returning `StatusOr<T>` is a\nbreaking change. As of this writing (Jan 2019), this project has a\n[Google Cloud Storage][gcs-link] component that is at the Alpha quality level,\nand a [Google Cloud Bigtable][bigtable-link] that is already at the Beta quality\nlevel. We plan to immediately change the API for Google Cloud Storage. We have\nno immediate plans to change the API for Cloud Bigtable. We will communicate a\ntimeline to change this API in a separate document.\n[badbit-link]: https://en.cppreference.com/w/cpp/io/ios_base/iostate\n[bigtable-link]: https://github.com/googleapis/google-cloud-cpp/tree/main/google/cloud/bigtable\n[gcs-link]: https://github.com/googleapis/google-cloud-cpp/tree/main/google/cloud/storage\n[survey-link]: https://isocpp.org/blog/2018/03/results-summary-cpp-foundation-developer-survey-lite-2018-02\n"
android-guidelines/0009-gradle-rules.md,## Context and Problem Statement\nNumber of module count has been increased our build times. Enabling unused plugins and using **gradle.kts** is causing to longer build times.\n,* Disable generating BuildConfig file if its not needed in module.\n* Only enable *databinding* if you're going to use DataBinding in that module.\n* Do not apply *kapt* plugin if you're not going to use.\n* Do not create new variants other than *debug* and *release*.\n* Use groovy scripts on *build.gradle* files.\n,37,3860,"## Context and Problem Statement\nNumber of module count has been increased our build times. Enabling unused plugins and using **gradle.kts** is causing to longer build times.\n

##Decision
* Disable generating BuildConfig file if its not needed in module.\n* Only enable *databinding* if you're going to use DataBinding in that module.\n* Do not apply *kapt* plugin if you're not going to use.\n* Do not create new variants other than *debug* and *release*.\n* Use groovy scripts on *build.gradle* files.\n"
tracking-consent-frontend/0002-use-data-attribute-for-gtm-container.md,"## Context and Problem Statement\nIn order to simplify the tracking consent build and deploy process and\nmake integrating with tracking consent less surprising, should\nthe configuration of the GTM container used by tracking consent be via\ndata attributes rather than separate bundles?\n## Decision Drivers\n* The need to keep things simple for service developers\n* The need to improve the operability of tracking consent\n","* The need to keep things simple for service developers\n* The need to improve the operability of tracking consent\nChosen option: ""Use a data attribute"", because based on the benefits listed below the team\nbelieves this is the best way forward.\n### Positive Consequences\n* The Javascript bundle creation process is simplified.\n* The Scala Play routing is simplified\n* Only one endpoint needs to be managed in production\n* Future additional containers can be supported more easily\n* Service developers only have to configure a single URL to tracking consent and use\na data attribute to configure the container.\n* The central common configuration repo (owned by a separate team) only requires a single URL to\ntracking consent defining for each environment, rather than one for each GTM container.\n### Negative Consequences\n* Service developers need to add id=""tracking-consent-script-tag"" to the SCRIPT\ntag when integrating.\n",78,3529,"## Context and Problem Statement\nIn order to simplify the tracking consent build and deploy process and\nmake integrating with tracking consent less surprising, should\nthe configuration of the GTM container used by tracking consent be via\ndata attributes rather than separate bundles?\n## Decision Drivers\n* The need to keep things simple for service developers\n* The need to improve the operability of tracking consent\n

##Decision
* The need to keep things simple for service developers\n* The need to improve the operability of tracking consent\nChosen option: ""Use a data attribute"", because based on the benefits listed below the team\nbelieves this is the best way forward.\n### Positive Consequences\n* The Javascript bundle creation process is simplified.\n* The Scala Play routing is simplified\n* Only one endpoint needs to be managed in production\n* Future additional containers can be supported more easily\n* Service developers only have to configure a single URL to tracking consent and use\na data attribute to configure the container.\n* The central common configuration repo (owned by a separate team) only requires a single URL to\ntracking consent defining for each environment, rather than one for each GTM container.\n### Negative Consequences\n* Service developers need to add id=""tracking-consent-script-tag"" to the SCRIPT\ntag when integrating.\n"
aspan-server/0003-replacing-ramda-with-lodash.md,## Context\nArrow functions are a much more natural way to reduce visual noise in most contexts in JavaScript.\n,Decision here...\n,22,1890,"## Context\nArrow functions are a much more natural way to reduce visual noise in most contexts in JavaScript.\n

##Decision
Decision here...\n"
report-a-defect/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",16,5220,"## Context\nWe need to record the architectural decisions made on this project.\n

##Decision
We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n"
jskatas.org/000-use-adrs.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as described by Michael Nygard in this\narticle: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\n",16,4722,"## Context\nWe need to record the architectural decisions made on this project.\n

##Decision
We will use Architecture Decision Records, as described by Michael Nygard in this\narticle: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\n"
webwritertechandhumanity.com/0002-how-to-implement-special-pages.md,"## Context\nI need to implement two special pages, the privacy policy page and the about me page.\nThey can't be in the map, I don't want them there because the user would\nbe forced to read them when scanning the spiral.\n",I'll implement solution 1 because solution 2 is complicated and I want to prioritise\nthe release of a first working website.\n,52,5001,"## Context\nI need to implement two special pages, the privacy policy page and the about me page.\nThey can't be in the map, I don't want them there because the user would\nbe forced to read them when scanning the spiral.\n

##Decision
I'll implement solution 1 because solution 2 is complicated and I want to prioritise\nthe release of a first working website.\n"
cloud-sdk-js/0021-odata-url-builder.md,"## Context\nThe current request builder APIs are not able to handle some odata requests like:\n- query navigation properties `GET /People('scottketchum')/Friends`\n- getting ""raw value"" of a property `/People('scottketchum')/$value`\n","- Implement A for now as a powerful workaround.\n- Proposal B/C/variant will be a `2.0` task, where it seems C might be the winner and we might review the decision later as they close to each other.\nAt least, implement it as a separate task so we have a workaround for custom URL.\n### Proposal B\n```ts\n// Problem 1\n// /People('russellwhyte')/Friends\nTripPinService.entity(People, 'russellwhyte') // single item can continue linking\n.navigationProp(People.Friends)\n.buildGetRequest() //xxxRequestBuilder, which can be called by single item/multi items and others\n.customHeaders(headers)\n.execute(destination);\n```\n```ts\n// Problem 2,3,4\n// /People('russellwhyte')/Friends('scottketchum')/BestFriend/BestFriend\nTripPinService.entity(People, 'russellwhyte') // single item can continue linking\n.navigationProp(People.Friends, 'scottketchum') // single item can continue linking\n.navigationProp(People.BestFriend) // single item can continue linking\n.navigationProp(People.BestFriend); // single item can continue linking\n```\n#### Pros and cons:\n##### Pros:\n- Better fluent API (compared to `asChildOf`) with builder pattern.\n- Can be extended for supporting problem 5-7.\n- Typed.\n##### Cons:\n- Lots of effort to build the new structure, which seems to be a `2.0` task.\n### Proposal C\nBasically, the same idea but with different API in terms of reaching single items.(e.g., ""getByKey"" and 1-to-1 navigation properties)\n```ts\n// Problem 1\n// /People('russellwhyte')/Friends\nTripPinService.entity(People) // multi item can call ""key"" to become a single item\n.key('russellwhyte') // single item can continue linking\n.navigationProp(People.Friends);\n```\n```ts\n// Problem 2,3,4\n// /People('russellwhyte')/Friends('scottketchum')/BestFriend/BestFriend\nTripPinService.entity(People) // multi item can call ""key"" to become a single item\n.key('russellwhyte') // single item can continue linking\n.navigationProp(People.Friends) // multi item can call ""key"" to become a single item\n.key('scottketchum') // single item can continue linking\n.navigationProp(People.BestFriend)\n.navigationProp(People.BestFriend)\n.buildGetRequest() //xxxRequestBuilder, which can be called by single item/multi items and others\n.customHeaders(headers)\n.execute(destination);\n```\n### Proposal C variants\n```\n//frank\nPeople.requestBuilder()\n.getByKey('key') // xxxRequestBuilder\n.toFriend('abc')\n.toBestFriend()\n.toFriends()\n.getBuilder()//create\n//marika\nPeople.requestBuilder()\n.key('scottketchum')\n.navigationProp(People.BestFriend)\n.navigationProp(People.BestFriend)\n.buildGetRequest() //xxxRequestBuilder, which can be called by single item/multi items and others\n.customHeaders(headers)\n.execute(destination)\n```\n#### Pros and cons:\nSame as `Proposal B`, but with more methods instead of overloading functions with more parameters.\n### Proposal D\nUse the similar API like `asChildOf`\n```ts\n// /People(personKey)/Friends\nFriends.requestBuilder().getAll().asChildOf(person, People.Friends);\n```\n```ts\n// /People(personKey)/Friends(friendKey)\nFriends.requestBuilder().getByKey(friendKey).asChildOf(person, People.Friends);\n```\n#### Pros and cons:\n##### Pros:\n- Consistent with `asChildOf` for generating same URL.\n- Medium complexity\n- Typed.\n##### Cons:\n- Cannot be extended for supporting problem 5-7, so we need to find solution for them.\n- The ugly API `asChildOf` is used with additional use cases.\n- Different order: `Entity -> NavigationProp` (url) V.S. `NavigationProp -> Entity` (API usage)\n#### Decision:\nNot chosen due to the lack of extension and confusing API.\n### previous docs\nFind related discussion [here](../implementation-documentation/api-improvements.md)\n",61,3630,"## Context\nThe current request builder APIs are not able to handle some odata requests like:\n- query navigation properties `GET /People('scottketchum')/Friends`\n- getting ""raw value"" of a property `/People('scottketchum')/$value`\n

##Decision
- Implement A for now as a powerful workaround.\n- Proposal B/C/variant will be a `2.0` task, where it seems C might be the winner and we might review the decision later as they close to each other.\nAt least, implement it as a separate task so we have a workaround for custom URL.\n### Proposal B\n```ts\n// Problem 1\n// /People('russellwhyte')/Friends\nTripPinService.entity(People, 'russellwhyte') // single item can continue linking\n.navigationProp(People.Friends)\n.buildGetRequest() //xxxRequestBuilder, which can be called by single item/multi items and others\n.customHeaders(headers)\n.execute(destination);\n```\n```ts\n// Problem 2,3,4\n// /People('russellwhyte')/Friends('scottketchum')/BestFriend/BestFriend\nTripPinService.entity(People, 'russellwhyte') // single item can continue linking\n.navigationProp(People.Friends, 'scottketchum') // single item can continue linking\n.navigationProp(People.BestFriend) // single item can continue linking\n.navigationProp(People.BestFriend); // single item can continue linking\n```\n#### Pros and cons:\n##### Pros:\n- Better fluent API (compared to `asChildOf`) with builder pattern.\n- Can be extended for supporting problem 5-7.\n- Typed.\n##### Cons:\n- Lots of effort to build the new structure, which seems to be a `2.0` task.\n### Proposal C\nBasically, the same idea but with different API in terms of reaching single items.(e.g., ""getByKey"" and 1-to-1 navigation properties)\n```ts\n// Problem 1\n// /People('russellwhyte')/Friends\nTripPinService.entity(People) // multi item can call ""key"" to become a single item\n.key('russellwhyte') // single item can continue linking\n.navigationProp(People.Friends);\n```\n```ts\n// Problem 2,3,4\n// /People('russellwhyte')/Friends('scottketchum')/BestFriend/BestFriend\nTripPinService.entity(People) // multi item can call ""key"" to become a single item\n.key('russellwhyte') // single item can continue linking\n.navigationProp(People.Friends) // multi item can call ""key"" to become a single item\n.key('scottketchum') // single item can continue linking\n.navigationProp(People.BestFriend)\n.navigationProp(People.BestFriend)\n.buildGetRequest() //xxxRequestBuilder, which can be called by single item/multi items and others\n.customHeaders(headers)\n.execute(destination);\n```\n### Proposal C variants\n```\n//frank\nPeople.requestBuilder()\n.getByKey('key') // xxxRequestBuilder\n.toFriend('abc')\n.toBestFriend()\n.toFriends()\n.getBuilder()//create\n//marika\nPeople.requestBuilder()\n.key('scottketchum')\n.navigationProp(People.BestFriend)\n.navigationProp(People.BestFriend)\n.buildGetRequest() //xxxRequestBuilder, which can be called by single item/multi items and others\n.customHeaders(headers)\n.execute(destination)\n```\n#### Pros and cons:\nSame as `Proposal B`, but with more methods instead of overloading functions with more parameters.\n### Proposal D\nUse the similar API like `asChildOf`\n```ts\n// /People(personKey)/Friends\nFriends.requestBuilder().getAll().asChildOf(person, People.Friends);\n```\n```ts\n// /People(personKey)/Friends(friendKey)\nFriends.requestBuilder().getByKey(friendKey).asChildOf(person, People.Friends);\n```\n#### Pros and cons:\n##### Pros:\n- Consistent with `asChildOf` for generating same URL.\n- Medium complexity\n- Typed.\n##### Cons:\n- Cannot be extended for supporting problem 5-7, so we need to find solution for them.\n- The ugly API `asChildOf` is used with additional use cases.\n- Different order: `Entity -> NavigationProp` (url) V.S. `NavigationProp -> Entity` (API usage)\n#### Decision:\nNot chosen due to the lack of extension and confusing API.\n### previous docs\nFind related discussion [here](../implementation-documentation/api-improvements.md)\n"
snippets-service/0005-frequency-capping.md,"## Context\nFrequency Capping allows Content Managers to limit the number of\nimpressions or interactions users have with content. Is a widely\navailable tool in Publishing Platforms.\nIt's usually developed on the server side where the system can decide\nhow many times to serve the content to the requesting users which we\ncall ""Global Frequency Capping"". Additionally the system may be able\nto limit the number of impressions per user which we call ""Local"" or\n""User Frequency Capping"".\nFor example a Content Piece can be set to 1,000,000 Global Impressions\nand 1 Impression per User, thus indirectly driving 1,000,000 different\nusers to this Content.\nThis functionality has been lacking from the Snippet Service due to\ntechnical limitations imposed by the way metrics were collected and\ncontent selection was handled on the client side. The latest\ndevelopments in Firefox Messaging Center and the Firefox Telemetry\nPipeline unblock this capability. [0]\n","We decide to implement the Frequency Capping functionality into our\nplatform to allow Content Managers to limit the number of Impressions,\nClicks and Blocks per Job.\nLocal or User Frequency Capping will be handled on the Browser level\nby the Firefox Messaging Platform. The later supports only Impression\nFrequency Capping.\nThe Snippets Service will provide an interface (UI) for the Content\nManagers to set upper limits on the number of Impressions a Job gets\nper Hour, Day, Week, Fortnight, Month or for the complete Browser\nProfile Lifetime. This information is included in the JSON generated\nfor each Job.\nFor Global Frequency Capping the Snippets Service will provide an\ninterface (UI) for the Content Managers to set the limits on total\nworldwide number of Impressions, Clicks and Blocks per Job.\nSnippets Service will query Mozilla's Redash for Telemetry data every\nten minutes and will fetch current impressions, clicks, blocks for\neach Job with set limits.\nWhen the reported numbers exceed the set limits then, the Job will be\nmarked COMPLETE and will be pulled out of the Bundles on the next run\nof `update_jobs` cron job.\nThe Frequency Capping functionality is additional to the Date\nPublishing controls, therefore a Job can end on a specific Date and\nTime or when its Global Frequency Capping Limits are met.\n### Monitoring and Handling of Errors\nSince Global Frequency Capping depends on an external system for\nMetrics (Redash / Telemetry) it is possible that the latest numbers are\nnot always available to the Snippets Service to make a decision. Such\ncases include scheduled or unplanned service interruptions or network\nerrors.\nIn co-ordination with Snippet Content Owner we decided that for cases\nwhere the Snippets Service cannot get the latest numbers for more than\n24 hours, Jobs with Global Frequency Capping will get canceled. The\ncancellation reason will state that the Jobs where prematurely\nterminated due to missing metrics.\nThe cron job responsible for fetching the Data from Telemetry is\nmonitored by a Dead Man's Snitch.\n",208,3229,"## Context\nFrequency Capping allows Content Managers to limit the number of\nimpressions or interactions users have with content. Is a widely\navailable tool in Publishing Platforms.\nIt's usually developed on the server side where the system can decide\nhow many times to serve the content to the requesting users which we\ncall ""Global Frequency Capping"". Additionally the system may be able\nto limit the number of impressions per user which we call ""Local"" or\n""User Frequency Capping"".\nFor example a Content Piece can be set to 1,000,000 Global Impressions\nand 1 Impression per User, thus indirectly driving 1,000,000 different\nusers to this Content.\nThis functionality has been lacking from the Snippet Service due to\ntechnical limitations imposed by the way metrics were collected and\ncontent selection was handled on the client side. The latest\ndevelopments in Firefox Messaging Center and the Firefox Telemetry\nPipeline unblock this capability. [0]\n

##Decision
We decide to implement the Frequency Capping functionality into our\nplatform to allow Content Managers to limit the number of Impressions,\nClicks and Blocks per Job.\nLocal or User Frequency Capping will be handled on the Browser level\nby the Firefox Messaging Platform. The later supports only Impression\nFrequency Capping.\nThe Snippets Service will provide an interface (UI) for the Content\nManagers to set upper limits on the number of Impressions a Job gets\nper Hour, Day, Week, Fortnight, Month or for the complete Browser\nProfile Lifetime. This information is included in the JSON generated\nfor each Job.\nFor Global Frequency Capping the Snippets Service will provide an\ninterface (UI) for the Content Managers to set the limits on total\nworldwide number of Impressions, Clicks and Blocks per Job.\nSnippets Service will query Mozilla's Redash for Telemetry data every\nten minutes and will fetch current impressions, clicks, blocks for\neach Job with set limits.\nWhen the reported numbers exceed the set limits then, the Job will be\nmarked COMPLETE and will be pulled out of the Bundles on the next run\nof `update_jobs` cron job.\nThe Frequency Capping functionality is additional to the Date\nPublishing controls, therefore a Job can end on a specific Date and\nTime or when its Global Frequency Capping Limits are met.\n### Monitoring and Handling of Errors\nSince Global Frequency Capping depends on an external system for\nMetrics (Redash / Telemetry) it is possible that the latest numbers are\nnot always available to the Snippets Service to make a decision. Such\ncases include scheduled or unplanned service interruptions or network\nerrors.\nIn co-ordination with Snippet Content Owner we decided that for cases\nwhere the Snippets Service cannot get the latest numbers for more than\n24 hours, Jobs with Global Frequency Capping will get canceled. The\ncancellation reason will state that the Jobs where prematurely\nterminated due to missing metrics.\nThe cron job responsible for fetching the Data from Telemetry is\nmonitored by a Dead Man's Snitch.\n"
connecting-to-services/0003-use-postcodes-io-for-postcode-lookup.md,## Context\nThe application is about finding services closest to the search point. All\nservices have a co-ordinate in lat/lon format. Currently the application\nrequests either an out-code or a postcode as the search point. The submitted\npostcode needs to be resolved to a lat/lon that can be used to query against.\n,"[Postcodes.io](https://postcodes.io/) provides both an out-code and a postcode\nlookup. The results of which return latitude and longitude. The service is\nfree, [supported](https://postcodes.io/about),\n[monitored](https://status.ideal-postcodes.co.uk/) and contains the full set of\nactive postcodes for the UK. Supplied by the\n[ONS](https://data.gov.uk/dataset/national-statistics-postcode-lookup-uk).\n",68,2204,"## Context\nThe application is about finding services closest to the search point. All\nservices have a co-ordinate in lat/lon format. Currently the application\nrequests either an out-code or a postcode as the search point. The submitted\npostcode needs to be resolved to a lat/lon that can be used to query against.\n

##Decision
[Postcodes.io](https://postcodes.io/) provides both an out-code and a postcode\nlookup. The results of which return latitude and longitude. The service is\nfree, [supported](https://postcodes.io/about),\n[monitored](https://status.ideal-postcodes.co.uk/) and contains the full set of\nactive postcodes for the UK. Supplied by the\n[ONS](https://data.gov.uk/dataset/national-statistics-postcode-lookup-uk).\n"
adr/ADR-6-git-style-guide.md,## Context or problem to solve\n* How should git histories look like?\n* What format should we use to write commit messages?\n,"We agreed on normalizing commit messages to master branches to avoid things like `Merge pull request #15 from client/menduz-patch-10` in persuit of more semantic messages like `fix: commit style guide, closes #15`. That is particularly helpful in repositories with several contributors and fosters professionalism in open source repositories.\n### Branches\nWhen you work on a branch on a specific issue, we keep the spirit of [semantic branch naming](https://medium.com/@hanuman_95739/how-to-integrate-branch-naming-commit-message-and-push-rules-in-gitlab-fe9cd642cc1a). Think of this as writing what is and what you are doing in a three word sentence The first one must be oune of the list. For instance:\n```\nfix/wrong_host\n^  ^^------------^\n|  ||\n|  |+----> Summary in present tense.\n|  +-----> Slash\n+--------> Type: chore, docs, feat, fix, refactor, style, or test.\n```\nOther examples are:\n```\ndocs/update_readme\nrefactor/new_welcome_message\n```\nLook for the *Examples* in section *Commit messages* for a description of the allowed branch types.\nIt's OK to use hyphens (`-`) or underscores (`_`) to replace spaces. Avoid any other special characters, like `#` or `$`, as they might lead to problems, for example, when deploying the content using the branch name as part of the URL. The branch name should match this regexp: `(chore|docs|feat|fix|refactor|style|test)/[0-9a-zA-Z_-]+`\n#### Exceptions:\nThis rules are ignored when you work on *environment* branches like `master`, `development`, `staging` and so on.\n",28,4600,"## Context or problem to solve\n* How should git histories look like?\n* What format should we use to write commit messages?\n

##Decision
We agreed on normalizing commit messages to master branches to avoid things like `Merge pull request #15 from client/menduz-patch-10` in persuit of more semantic messages like `fix: commit style guide, closes #15`. That is particularly helpful in repositories with several contributors and fosters professionalism in open source repositories.\n### Branches\nWhen you work on a branch on a specific issue, we keep the spirit of [semantic branch naming](https://medium.com/@hanuman_95739/how-to-integrate-branch-naming-commit-message-and-push-rules-in-gitlab-fe9cd642cc1a). Think of this as writing what is and what you are doing in a three word sentence The first one must be oune of the list. For instance:\n```\nfix/wrong_host\n^  ^^------------^\n|  ||\n|  |+----> Summary in present tense.\n|  +-----> Slash\n+--------> Type: chore, docs, feat, fix, refactor, style, or test.\n```\nOther examples are:\n```\ndocs/update_readme\nrefactor/new_welcome_message\n```\nLook for the *Examples* in section *Commit messages* for a description of the allowed branch types.\nIt's OK to use hyphens (`-`) or underscores (`_`) to replace spaces. Avoid any other special characters, like `#` or `$`, as they might lead to problems, for example, when deploying the content using the branch name as part of the URL. The branch name should match this regexp: `(chore|docs|feat|fix|refactor|style|test)/[0-9a-zA-Z_-]+`\n#### Exceptions:\nThis rules are ignored when you work on *environment* branches like `master`, `development`, `staging` and so on.\n"
octagon/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\n",16,4597,"## Context\nWe need to record the architectural decisions made on this project.\n

##Decision
We will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\n"
insight/0001-python.md,## Context\nWe need to have a base programming language as our first choice to implement the various functionalities. Other languages may be used as needed with proper justification.\n,We choose Python as our base programming language.\n,33,4761,"## Context\nWe need to have a base programming language as our first choice to implement the various functionalities. Other languages may be used as needed with proper justification.\n

##Decision
We choose Python as our base programming language.\n"
alfresco-anaxes-shipyard/0002-docker-registry-for-internal-and-protected-images.md,"## Context\nAs part of our efforts around containerized deployment of the Alfresco Digital Business Platform we need to standardize on a Docker Image Registry that Alfresco engineering teams, other internal groups in the organization, and customers and partners can use to publish and consume Docker images.  We can describe those as 'internal' and 'protected' tiers.\nThe Nexus3 implementation in use at the time of writing does not meet our requirements around access control, security scanning, scalability and global performance, usability, or maintainability.\nOur IT resources are currently stretched very thin and we should avoid adding another system for them to deploy and maintain if possible.\n","We will use [Quay.io](https://quay.io) for the internal and protected tiers of access and use Docker Hub for public repositories (images of community versions and/or enterprise artifacts with trail licenses).\nWe’d like to limit the introduction of additional deployments (particularly customer-facing) that our IT staff has to maintain, so we'd prefer a SaaS solution.\nThe REST API of Quay.io allows our organization to potentially automate user provisioning/invitation and user/group management which is not available for Docker Cloud at this time.\nAdditionally, Quay / CoreOS seems strongly committed to their SaaS offering while Docker seems entirely focused on their Enterprise ‘on-prem’ product.\nThe summary [![report of the comparison](https://img.shields.io/badge/report%20of%20the%20comparison-PRIVATE-red.svg)](https://ts.alfresco.com/share/s/mVAV1sGIReC_iqgMN0GGnQ) also contains reference links to the full investigation.\n",130,3598,"## Context\nAs part of our efforts around containerized deployment of the Alfresco Digital Business Platform we need to standardize on a Docker Image Registry that Alfresco engineering teams, other internal groups in the organization, and customers and partners can use to publish and consume Docker images.  We can describe those as 'internal' and 'protected' tiers.\nThe Nexus3 implementation in use at the time of writing does not meet our requirements around access control, security scanning, scalability and global performance, usability, or maintainability.\nOur IT resources are currently stretched very thin and we should avoid adding another system for them to deploy and maintain if possible.\n

##Decision
We will use [Quay.io](https://quay.io) for the internal and protected tiers of access and use Docker Hub for public repositories (images of community versions and/or enterprise artifacts with trail licenses).\nWe’d like to limit the introduction of additional deployments (particularly customer-facing) that our IT staff has to maintain, so we'd prefer a SaaS solution.\nThe REST API of Quay.io allows our organization to potentially automate user provisioning/invitation and user/group management which is not available for Docker Cloud at this time.\nAdditionally, Quay / CoreOS seems strongly committed to their SaaS offering while Docker seems entirely focused on their Enterprise ‘on-prem’ product.\nThe summary [![report of the comparison](https://img.shields.io/badge/report%20of%20the%20comparison-PRIVATE-red.svg)](https://ts.alfresco.com/share/s/mVAV1sGIReC_iqgMN0GGnQ) also contains reference links to the full investigation.\n"
qc-atlas/0005-use-OpenAPI.md,"## Context and Problem Statement\nThe API has to be documented in human and machine readable form. If the API is documented in machine readable form, the automatic generation of client services is possible.\n## Decision Drivers\n* readable API documentation\n* effort of manually creating client services\n","* readable API documentation\n* effort of manually creating client services\nChosen option: Use OpenAPI, because the API is described in a standardized format which is human and machine readable.\n### Positive Consequences\n* Standardized documentation of the API\n* Automatic service generation for clients is possible\n### Negative Consequences <!-- optional -->\n* OpenAPI annotations have to be maintained\n",56,693,"## Context and Problem Statement\nThe API has to be documented in human and machine readable form. If the API is documented in machine readable form, the automatic generation of client services is possible.\n## Decision Drivers\n* readable API documentation\n* effort of manually creating client services\n

##Decision
* readable API documentation\n* effort of manually creating client services\nChosen option: Use OpenAPI, because the API is described in a standardized format which is human and machine readable.\n### Positive Consequences\n* Standardized documentation of the API\n* Automatic service generation for clients is possible\n### Negative Consequences <!-- optional -->\n* OpenAPI annotations have to be maintained\n"
fundraising-application/020_PayPal_IPN_Queue.md,"## Context and Problem Statement\nThe PayPal IPNs on Fundraising Frontend started to fail after a deployment\nand was only noticed some days after when the Project Manager needed to\nexport the data. Upon investigation, it was discovered that:\n* Error logging was inactive on the application. (Now fixed)\n* We can’t debug using the responses our system returned to PayPal as\nwe don’t have access to the IPN log.\nThis led to a situation where we couldn't get the information required\nto debug the error. It was suggested we queue all incoming requests from\nPayPal on our own system for processing by our system.\n## Decision Drivers\n* **Transparency**: If our system fails we would have a stored queue to\nuse for debugging.\n* **Automation**: The IPNs wouldn't need to be fired again once an error\nbecomes fixed as our system would resume processing the queue.\n","* **Transparency**: If our system fails we would have a stored queue to\nuse for debugging.\n* **Automation**: The IPNs wouldn't need to be fired again once an error\nbecomes fixed as our system would resume processing the queue.\nSince this was the first occurrence of the problem, and the Fundraising\nApplication system is now running well again, we decided against introducing\nthe IPN queue feature.\n",190,1523,"## Context and Problem Statement\nThe PayPal IPNs on Fundraising Frontend started to fail after a deployment\nand was only noticed some days after when the Project Manager needed to\nexport the data. Upon investigation, it was discovered that:\n* Error logging was inactive on the application. (Now fixed)\n* We can’t debug using the responses our system returned to PayPal as\nwe don’t have access to the IPN log.\nThis led to a situation where we couldn't get the information required\nto debug the error. It was suggested we queue all incoming requests from\nPayPal on our own system for processing by our system.\n## Decision Drivers\n* **Transparency**: If our system fails we would have a stored queue to\nuse for debugging.\n* **Automation**: The IPNs wouldn't need to be fired again once an error\nbecomes fixed as our system would resume processing the queue.\n

##Decision
* **Transparency**: If our system fails we would have a stored queue to\nuse for debugging.\n* **Automation**: The IPNs wouldn't need to be fired again once an error\nbecomes fixed as our system would resume processing the queue.\nSince this was the first occurrence of the problem, and the Fundraising\nApplication system is now running well again, we decided against introducing\nthe IPN queue feature.\n"
paas-csls-splunk-broker/ADR004-deploy-broker-as-lambda.md,"## Context\nWe need to deploy the Broker somewhere.\nThe Broker implements the service broker API to generate per-application syslog\ndrain URLs (Adapter URLs).\nThe Adapter is written in Go.\nThe Broker is written in Go.\nThe Adapter runs as a lambda in AWS alongside the CSLS infrastructure.\nWe have a pipeline to continuously build, test, deploy the Adapter to lambda.\n",We will deploy the Broker as an AWS Lambda\n,77,3295,"## Context\nWe need to deploy the Broker somewhere.\nThe Broker implements the service broker API to generate per-application syslog\ndrain URLs (Adapter URLs).\nThe Adapter is written in Go.\nThe Broker is written in Go.\nThe Adapter runs as a lambda in AWS alongside the CSLS infrastructure.\nWe have a pipeline to continuously build, test, deploy the Adapter to lambda.\n

##Decision
We will deploy the Broker as an AWS Lambda\n"
