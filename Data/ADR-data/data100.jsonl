{"File Name":"cdh-adrs\/0010-etl_tenancy_relationship.md","Context":"## Context\\nThe PriveXMl results generated by ETL on the processing of data feed files must be capable of categorization based on specific tenants (Company, Client & Account) data. Due to the fact such data will change overtime based on client requests, every sections of PriveXML Contents (Ticker + Position + Transaction) section should generally be able to be connected to giving tenant. But using just the tenant Id or primary key as a relationship marker isn't suitable, as during migration and over-all during the life of our archives, seperation of the tenant changes and the PriveXML events will import insertion but increase complexity on the read\/consumption end as we may need to re-read all changes for tenant before being able to consume the PriveXML to generate current state.\\n","Decision":"We increase complexity during insertion by including all necessary tenant information as part of a PriveXML event and also provide event objects which could only just contain specific changes to client information within the same topic. This then enforces us to think of this topic as a general topic for ETL which may contain an event to process a new PriveXML result or process tenant data change.\\nMore so, the topics for each tenant must be tenant specific due to necessary policies and requirements to create seperation of data on physical levels.\\n","tokens":160,"id":0}
{"File Name":"cdh-adrs\/0011-pentaho.md","Context":"## Context\\nWe currently have certain specific feeds being parsed using pentaho, these are legacy feeds which transform data feed files into PriveXMl format. These feed parsers will require migration into new CDH ETL feeds parsers based on defined standard library.\\n","Decision":"Due to limited resources and time constraints, we will defer the feed migration from pentaho to the CDH ETL till adequate human resources are available, more so an indept understanding of exactly how the parsers work is required, which will require talking with Christopher on this.\\n","tokens":52,"id":1}
{"File Name":"cdh-adrs\/0009-etl_feed_synchronization_and_delivery.md","Context":"## Context\\nCurrently, we move files around in S3 from Archive folders into bulkupload folders which is cumbersome and error prone,\\nmore so this is being done to allow us trigger data feed processing from the dev admin.\\nWe need a means of reducing the flow steps to go from files to processing and reduce surface level access for the ETL team.\\n","Decision":"- ETL will have a dedicated database tables which will serve the need to both register and retrieve data feed files in the\\nmost optimized manner necessary on a per feed basis. This database tables will be appropriately updated by the ETL service\\nbased on update events from either the uploader service, or the windows-specific uploader jars.\\nThe following operations then becomes possible:\\n1. Ability to query for specific files based on upload date (the date it was uploaded to S3).\\n2. Ability to query for a specific batch of files related to specific data feeds.\\n3. Ability to get data files specific to a given owner and\/or provider.\\n4. Ability to get data files specific to a given provider.\\n- Due to we creating a database to optimize a query to retrieve and manage data feed files, then we require a way to keep\\nthe ETL database tables up to date with new files from uploader service. Therefore, the ETL system will listen for events coming\\nfrom both a lambda function which will be called by S3 when new files are added to the specific bucket, and the new uploader\\nservice which will house necessary logic for retrieving such data feed files from their sources. Once all uploading logic\\nhave being migrated to the uploader, we will de-commission the lambda function and directly have the uploader service inform the\\nETL service as regards new files.\\nSee\\n![Data Feed Delivery](..\/assets\/images\/workflows\/image8.png)\\n","tokens":73,"id":2}
{"File Name":"cdh-adrs\/0004-cdh-tenants.md","Context":"## Context\\nMonolith will undergo segmentation where we plan to pull out the Client Data Hub out of the monolith as a separate service for better scaling and API management. The functionality of the client data hub to provided tenant, transaction and position related data will stay the same, but the data feed parsing within it's perview will also be moved out into an [ETL Service](.\/etl.md).\\nThe client data hub provides the following services:\\n1. Delivery of tenant information (Accounts, Company, and Client data).\\n2. Delivery of Tenant Transaction (Data Feed and Systems) for specified periods of time.\\n3. Delivery of Tenant Positions for their related transactions for porfolio tracking.\\n4. Processing of reconciliation request on tenant accounts and positions.\\nThis requires us to consider migration procedures for moving existing data from the monolith into the new CDH and ETL related databases.\\n","Decision":"Create a specific services (ETL and CDH) where an ETL service and CDH service will be responsible for the processing of delivered PriveXML for consumption and delivery of client transactions + positions into data tables. Both services will communicate across a pubsub event bus.\\nThe following is expected:\\n- CDH will expose an API by which it will handle all tenant and transaction related requests from monolith.\\n- CDH will listen on specified event topic on message bus from which all tenant update requests will be received as events\\n- CDH will publish to the monolith events on updated tenant data by which monolith will update it's records.\\n- CDH will have all tenant data moved from monolith into it's control (to be discussed in CDH Tech).\\n- CDH will consume all PriveXML events to update it's records of transactions + tickers + positions.\\n![CDH Architecture](..\/assets\/images\/aab_workshop\/aab_5.jpg)\\n","tokens":183,"id":3}
{"File Name":"cdh-adrs\/0006-etl_cdh_communication.md","Context":"## Context\\nWe wish to segment into separate processes where the data feed files processing is handled by the ETL service and the CDH service is reponsible for consuming these produced output which then are materialized into records which is used in response to request to the CDH service. This means ETL service must be able to communicate to the CDH service loosely without direct connection or dependence between either.\\n","Decision":"We have chosen an event based communication where the CDH and ETL service communicate results between each other over an event queue based on specified topics (deployed onsite within geozone of CDH and ETL services).\\n![Event Queue](..\/assets\/images\/workflows\/image3.png)\\n","tokens":81,"id":4}
{"File Name":"cdh-adrs\/0005-etl-monolith-migration.md","Context":"## Context\\nCurrently, the parsing process for datafeeds is still part of the existing monolith codebase, which brings it's own share of problems with scaling and feed parsing managemnent. Monolith also has a hard dependency on AWS S3 which must be broken out to allow us specfically move these dependencies outside as they are not relatively core to the ETL processing architecture but are generally how we organize and move input into desired place for access and delivery.\\nA hard requirement for the ETL service is the ability to ensure processed datafeed consistently have relational information with their tenants as specific feeds\\nhave specific constraints on how they are accessed and stored.\\nAnother hard requirements is to organize how data feed files are accessed and retrieved from S3, this currently has issues due to the need to directly access S3, and move files and directories into the bulkupload directories for processing by the monolith. Whilst the alternative of storing both file, metadata and file binary into the database simplifies these access issues, we create more problems in the management of the database files (without using FileStream optimization in SQLServer), increasing cost of backup and replication.\\n","Decision":"ETL will be moved into an external service of it's own with the following responsibilities:\\n- Embodiment of all parser logic.\\n- Delivery of agreed parser format (currently PriveXML) into message queues.\\n- Standardized library for parsing delivery and logic.\\n- Standardized database tables for data feed file delivery and access.\\n- Standardized database tables for tenant data (Company, Client, Accounts).\\n- Creates tenant specific events for delivery for tenant specific datafeed.\\nAs regards data feed file access problem\\n- Manage synchronization of uploaded files events into database from uploader service.\\n","tokens":233,"id":5}
{"File Name":"cdh-adrs\/0001-feed-parsers.md","Context":"## Context\\nWe need a clearly defined way for handling data feed files for processing, what are the expected inputs and outputs from the parsers and how will this feed into the whole parsing process for ETL ?\\n","Decision":"1. A EAM Parser Factory: this produces a content reader which will be used by all written parsers for reading the contents of a giving data source.\\n1. A Processing Adapter per EAM data feed type which has registered different parsers which handle the retrieval of different types of data out of giving data feed source (e.g CreditSuisse XML).\\n1. Custom Data Extractors (e.g IncomeCashFlowParsers, OrderBroker) which are responsible for extracting different data types from the ContentReader, these are then accumulated by the data feed ProcessingAdapter into a unified format which can be transformed into portions of the expected PriveXML format.\\n1. The custom data extractors will have rights to define specific errors for their data extraction process and how that will affect that specific extraction or for a giving set of files. We will have errors which may be critical and cause immediate failure or which can be considered non-critical and only stop giving feed extraction or ensure it is logged and continued from. The key is that such details should not be the responsibility of the core and as far as only specific errors which the core is concerned with towards stopping immediately for that giving source or a set of sources.\\n![Target Parser Flow](..\/assets\/images\/workflows\/image1.png)\\n","tokens":43,"id":6}
{"File Name":"cdh-adrs\/0003-uploaders.md","Context":"## Context\\nMonolith and a few deployed jars handling delivery of data feed files into S3 for the existing parsing processes powering the monolith client data hub system, this increases cost on the monolith's systems which require more vertical scaling of resources to manage, more so, due to the monolith lock, any fix or update is locked to the monolith release SDLC.\\nConsidering these functions serve to move files from source to destination we need to migrate them as external services to both CDH and the monolith, these are then bundled into a single service responsible for the delivery of new data feed files into the S3 Archives and CDH feed data stores.\\n","Decision":"Migration of all monolith related uploading logic into external service which is responsible for the timely retreival, delivery and storage of data feed files from their respective sources. The service is responsible for ensuring the ETL service database is always up to date, by deliverying events on file additions into the archive storage regardless of what storage is being used by event delivery.\\n![Data Feed Delivery](..\/assets\/images\/workflows\/image8.png)\\n","tokens":135,"id":7}
{"File Name":"cdh-adrs\/0002-cdh-etl-logic-boundaries.md","Context":"## Context\\nTo ensure a clear separation as regards what logic resides within CDH and ETL related services where there exists feed specific\\nrequirements during onboarding and processing of feed files, the following issues where considered:\\n- Will such logic require specialized implementation across feeds?\\n- Are such logic generic and require one time implementation or will require continous change\/update?\\n- What are the benefits of moving such logic into ETL instead of CDH.\\n","Decision":"The most important point agreed on was that CDH will remain focused on defined object and data models as possible and CDH will\\nrun with the expectation that all inputs received are completed. This means CDH should not have domain specific knowledge in regards\\nspecific intricacies about how specific feeds are reconcilied into complete Positions, Transactions and Ticker data.\\nSuch specificity will reside within the ETL service and be housed based on each feed parsing logic within the ETL service.\\nThe benefits of such a system is that only ETL needs to change to accomodate new and changing requirements of old and new feeds\\nensuring the final result is always consistent to march what the CDH service requires.\\n","tokens":92,"id":8}
{"File Name":"cdh-adrs\/0008-etl_feed_parsers.md","Context":"## Context\\nWe need a clearly defined way for handling data feed files for processing, what are the expected inputs and outputs from the parsers and how will this feed into the whole parsing process for ETL ?\\n","Decision":"1. A EAM Parser Factory: this produces a content reader which will be used by all written parsers for reading the contents of a giving data source.\\n1. A Processing Adapter per EAM data feed type which has registered different parsers which handle the retrieval of different types of data out of giving data feed source (e.g CreditSuisse XML).\\n1. Custom Data Extractors (e.g IncomeCashFlowParsers, OrderBroker) which are responsible for extracting different data types from the ContentReader, these are then accumulated by the data feed ProcessingAdapter into a unified format which can be transformed into portions of the expected PriveXML format.\\n1. The custom data extractors will have rights to define specific errors for their data extraction process and how that will affect that specific extraction or for a giving set of files. We will have errors which may be critical and cause immediate failure or which can be considered non-critical and only stop giving feed extraction or ensure it is logged and continued from. The key is that such details should not be the responsibility of the core and as far as only specific errors which the core is concerned with towards stopping immediately for that giving source or a set of sources.\\n![Target Parser Flow](..\/assets\/images\/workflows\/image1.png)\\n","tokens":43,"id":9}
{"File Name":"gatemint-sdk\/adr-024-coin-metadata.md","Context":"## Context\\nAssets in the Cosmos SDK are represented via a `Coins` type that consists of an `amount` and a `denom`,\\nwhere the `amount` can be any arbitrarily large or small value. In addition, the Cosmos SDK uses an\\naccount-based model where there are two types of primary accounts -- basic accounts and module accounts.\\nAll account types have a set of balances that are composed of `Coins`. The `x\/bank` module keeps\\ntrack of all balances for all accounts and also keeps track of the total supply of balances in an\\napplication.\\nWith regards to a balance `amount`, the Cosmos SDK assumes a static and fixed unit of denomination,\\nregardless of the denomination itself. In other words, clients and apps built atop a Cosmos-SDK-based\\nchain may choose to define and use arbitrary units of denomination to provide a richer UX, however, by\\nthe time a tx or operation reaches the Cosmos SDK state machine, the `amount` is treated as a single\\nunit. For example, for the Cosmos Hub (Gaia), clients assume 1 ATOM = 10^6 uatom, and so all txs and\\noperations in the Cosmos SDK work off of units of 10^6.\\nThis clearly provides a poor and limited UX especially as interoperability of networks increases and\\nas a result the total amount of asset types increases. We propose to have `x\/bank` additionally keep\\ntrack of metadata per `denom` in order to help clients, wallet providers, and explorers improve their\\nUX and remove the requirement for making any assumptions on the unit of denomination.\\n","Decision":"The `x\/bank` module will be updated to store and index metadata by `denom`, specifically the \"base\" or\\nsmallest unit -- the unit the Cosmos SDK state-machine works with.\\nMetadata may also include a non-zero length list of denominations. Each entry containts the name of\\nthe denomination `denom`, the exponent to the base and a list of aliases. An entry is to be\\ninterpreted as `1 denom = 10^exponent base_denom` (e.g. `1 ETH = 10^18 wei` and `1 uatom = 10^0 uatom`).\\nThere are two denominations that are of high importance for clients: the `base`, which is the smallest\\npossible unit and the `display`, which is the unit that is commonly referred to in human communication\\nand on exchanges. The values in those fields link to an entry in the list of denominations.\\nThe list in `denom_units` and the `display` entry may be changed via governance.\\nAs a result, we can define the type as follows:\\n```protobuf\\nmessage DenomUnit {\\nstring denom    = 1;\\nuint32 exponent = 2;\\nrepeated string aliases = 3;\\n}\\nmessage Metadata {\\nstring description = 1;\\nrepeated DenomUnit denom_units = 2;\\nstring base = 3;\\nstring display = 4;\\n}\\n```\\nAs an example, the ATOM's metadata can be defined as follows:\\n```json\\n{\\n\"description\": \"The native staking token of the Cosmos Hub.\",\\n\"denom_units\": [\\n{\\n\"denom\": \"uatom\",\\n\"exponent\": 0,\\n\"aliases\": [\\n\"microatom\"\\n],\\n},\\n{\\n\"denom\": \"matom\",\\n\"exponent\": 3,\\n\"aliases\": [\\n\"milliatom\"\\n]\\n},\\n{\\n\"denom\": \"atom\",\\n\"exponent\": 6,\\n}\\n],\\n\"base\": \"uatom\",\\n\"display\": \"atom\",\\n}\\n```\\nGiven the above metadata, a client may infer the following things:\\n- 4.3atom = 4.3 * (10^6) = 4,300,000uatom\\n- The string \"atom\" can be used as a display name in a list of tokens.\\n- The balance 4300000 can be displayed as 4,300,000uatom or 4,300matom or 4.3atom.\\nThe `display` denomination 4.3atom is a good default if the authors of the client don't make\\nan explicit decision to choose a different representation.\\nA client should be able to query for metadata by denom both via the CLI and REST interfaces. In\\naddition, we will add handlers to these interfaces to convert from any unit to another given unit,\\nas the base framework for this already exists in the Cosmos SDK.\\nFinally, we need to ensure metadata exists in the `GenesisState` of the `x\/bank` module which is also\\nindexed by the base `denom`.\\n```go\\ntype GenesisState struct {\\nSendEnabled   bool        `json:\"send_enabled\" yaml:\"send_enabled\"`\\nBalances      []Balance   `json:\"balances\" yaml:\"balances\"`\\nSupply        sdk.Coins   `json:\"supply\" yaml:\"supply\"`\\nDenomMetadata []Metadata  `json:\"denom_metadata\" yaml:\"denom_metadata\"`\\n}\\n```\\n","tokens":336,"id":11}
{"File Name":"gatemint-sdk\/adr-007-specialization-groups.md","Context":"## Context\\nThis idea was first conceived of in order to fulfill the use case of the\\ncreation of a decentralized Computer Emergency Response Team (dCERT), whose\\nmembers would be elected by a governing community and would fulfill the role of\\ncoordinating the community under emergency situations. This thinking\\ncan be further abstracted into the conception of \"blockchain specialization\\ngroups\".\\nThe creation of these groups are the beginning of specialization capabilities\\nwithin a wider blockchain community which could be used to enable a certain\\nlevel of delegated responsibilities. Examples of specialization which could be\\nbeneficial to a blockchain community include: code auditing, emergency response,\\ncode development etc. This type of community organization paves the way for\\nindividual stakeholders to delegate votes by issue type, if in the future\\ngovernance proposals include a field for issue type.\\n","Decision":"A specialization group can be broadly broken down into the following functions\\n(herein containing examples):\\n- Membership Admittance\\n- Membership Acceptance\\n- Membership Revocation\\n- (probably) Without Penalty\\n- member steps down (self-Revocation)\\n- replaced by new member from governance\\n- (probably) With Penalty\\n- due to breach of soft-agreement (determined through governance)\\n- due to breach of hard-agreement (determined by code)\\n- Execution of Duties\\n- Special transactions which only execute for members of a specialization\\ngroup (for example, dCERT members voting to turn off transaction routes in\\nan emergency scenario)\\n- Compensation\\n- Group compensation (further distribution decided by the specialization group)\\n- Individual compensation for all constituents of a group from the\\ngreater community\\nMembership admittance to a specialization group could take place over a wide\\nvariety of mechanisms. The most obvious example is through a general vote among\\nthe entire community, however in certain systems a community may want to allow\\nthe members already in a specialization group to internally elect new members,\\nor maybe the community may assign a permission to a particular specialization\\ngroup to appoint members to other 3rd party groups. The sky is really the limit\\nas to how membership admittance can be structured. We attempt to capture\\nsome of these possiblities in a common interface dubbed the `Electionator`. For\\nits initial implementation as a part of this ADR we recommend that the general\\nelection abstraction (`Electionator`) is provided as well as a basic\\nimplementation of that abstraction which allows for a continuous election of\\nmembers of a specialization group.\\n``` golang\\n\/\/ The Electionator abstraction covers the concept space for\\n\/\/ a wide variety of election kinds.\\ntype Electionator interface {\\n\/\/ is the election object accepting votes.\\nActive() bool\\n\/\/ functionality to execute for when a vote is cast in this election, here\\n\/\/ the vote field is anticipated to be marshalled into a vote type used\\n\/\/ by an election.\\n\/\/\\n\/\/ NOTE There are no explicit ids here. Just votes which pertain specifically\\n\/\/ to one electionator. Anyone can create and send a vote to the electionator item\\n\/\/ which will presumably attempt to marshal those bytes into a particular struct\\n\/\/ and apply the vote information in some arbitrary way. There can be multiple\\n\/\/ Electionators within the Cosmos-Hub for multiple specialization groups, votes\\n\/\/ would need to be routed to the Electionator upstream of here.\\nVote(addr sdk.AccAddress, vote []byte)\\n\/\/ here lies all functionality to authenticate and execute changes for\\n\/\/ when a member accepts being elected\\nAcceptElection(sdk.AccAddress)\\n\/\/ Register a revoker object\\nRegisterRevoker(Revoker)\\n\/\/ No more revokers may be registered after this function is called\\nSealRevokers()\\n\/\/ register hooks to call when an election actions occur\\nRegisterHooks(ElectionatorHooks)\\n\/\/ query for the current winner(s) of this election based on arbitrary\\n\/\/ election ruleset\\nQueryElected() []sdk.AccAddress\\n\/\/ query metadata for an address in the election this\\n\/\/ could include for example position that an address\\n\/\/ is being elected for within a group\\n\/\/\\n\/\/ this metadata may be directly related to\\n\/\/ voting information and\/or privileges enabled\\n\/\/ to members within a group.\\nQueryMetadata(sdk.AccAddress) []byte\\n}\\n\/\/ ElectionatorHooks, once registered with an Electionator,\\n\/\/ trigger execution of relevant interface functions when\\n\/\/ Electionator events occur.\\ntype ElectionatorHooks interface {\\nAfterVoteCast(addr sdk.AccAddress, vote []byte)\\nAfterMemberAccepted(addr sdk.AccAddress)\\nAfterMemberRevoked(addr sdk.AccAddress, cause []byte)\\n}\\n\/\/ Revoker defines the function required for a membership revocation rule-set\\n\/\/ used by a specialization group. This could be used to create self revoking,\\n\/\/ and evidence based revoking, etc. Revokers types may be created and\\n\/\/ reused for different election types.\\n\/\/\\n\/\/ When revoking the \"cause\" bytes may be arbitrarily marshalled into evidence,\\n\/\/ memos, etc.\\ntype Revoker interface {\\nRevokeName() string      \/\/ identifier for this revoker type\\nRevokeMember(addr sdk.AccAddress, cause []byte) error\\n}\\n```\\nCertain level of commonality likely exists between the existing code within\\n`x\/governance` and required functionality of elections. This common\\nfunctionality should be abstracted during implementation. Similarly for each\\nvote implementation client CLI\/REST functionality should be abstracted\\nto be reused for multiple elections.\\nThe specialization group abstraction firstly extends the `Electionator`\\nbut also further defines traits of the group.\\n``` golang\\ntype SpecializationGroup interface {\\nElectionator\\nGetName() string\\nGetDescription() string\\n\/\/ general soft contract the group is expected\\n\/\/ to fulfill with the greater community\\nGetContract() string\\n\/\/ messages which can be executed by the members of the group\\nHandler(ctx sdk.Context, msg sdk.Msg) sdk.Result\\n\/\/ logic to be executed at endblock, this may for instance\\n\/\/ include payment of a stipend to the group members\\n\/\/ for participation in the security group.\\nEndBlocker(ctx sdk.Context)\\n}\\n```\\n","tokens":173,"id":12}
{"File Name":"gatemint-sdk\/adr-006-secret-store-replacement.md","Context":"## Context\\nCurrently, an SDK application's CLI directory stores key material and metadata in a plain text database in the user\u2019s home directory.  Key material is encrypted by a passphrase, protected by bcrypt hashing algorithm. Metadata (e.g. addresses, public keys, key storage details) is available in plain text.\\nThis is not desirable for a number of reasons. Perhaps the biggest reason is insufficient security protection of key material and metadata. Leaking the plain text allows an attacker to surveil what keys a given computer controls via a number of techniques, like compromised dependencies without any privilege execution. This could be followed by a more targeted attack on a particular user\/computer.\\nAll modern desktop computers OS (Ubuntu, Debian, MacOS, Windows) provide a built-in secret store that is designed to allow applications to store information that is isolated from all other applications and requires passphrase entry to access the data.\\nWe are seeking solution that provides a common abstraction layer to the many different backends and reasonable fallback for minimal platforms that don\u2019t provide a native secret store.\\n","Decision":"We recommend replacing the current Keybase backend based on LevelDB with [Keyring](https:\/\/github.com\/99designs\/keyring) by 99 designs. This application is designed to provide a common abstraction and uniform interface between many secret stores and is used by AWS Vault application by 99-designs application.\\nThis appears to fulfill the requirement of protecting both key material and metadata from rouge software on a user\u2019s machine.\\n","tokens":214,"id":13}
{"File Name":"gatemint-sdk\/adr-026-ibc-client-recovery-mechanisms.md","Context":"## Context\\n### Summary\\nAt launch, IBC will be a novel protocol, without an experienced user-base. At the protocol layer, it is not possible to distinguish between client expiry or misbehaviour due to genuine faults (Byzantine behavior) and client expiry or misbehaviour due to user mistakes (failing to update a client, or accidentally double-signing). In the base IBC protocol and ICS 20 fungible token transfer implementation, if a client can no longer be updated, funds in that channel will be permanently locked and can no longer be transferred. To the degree that it is safe to do so, it would be preferable to provide users with a recovery mechanism which can be utilised in these exceptional cases.\\n### Exceptional cases\\nThe state of concern is where a client associated with connection(s) and channel(s) can no longer be updated. This can happen for several reasons:\\n1. The chain which the client is following has halted and is no longer producing blocks\/headers, so no updates can be made to the client\\n1. The chain which the client is following has continued to operate, but no relayer has submitted a new header within the unbonding period, and the client has expired\\n1. This could be due to real misbehaviour (intentional Byzantine behaviour) or merely a mistake by validators, but the client cannot distinguish these two cases\\n1. The chain which the client is following has experienced a misbehaviour event, and the client has been frozen & thus can no longer be updated\\n### Security model\\nTwo-thirds of the validator set (the quorum for governance, module participation) can already sign arbitrary data, so allowing governance to manually force-update a client with a new header after a delay period does not substantially alter the security model.\\n","Decision":"We elect not to deal with chains which have actually halted, which is necessarily Byzantine behaviour and in which case token recovery is not likely possible anyways (in-flight packets cannot be timed-out, but the relative impact of that is minor).\\n1. Require Tendermint light clients (ICS 07) to be created with the following additional flags\\n1. `allow_governance_override_after_expiry` (boolean, default false)\\n1. Require Tendermint light clients (ICS 07) to expose the following additional internal query functions\\n1. `Expired() boolean`, which returns whether or not the client has passed the trusting period since the last update (in which case no headers can be validated)\\n1. Require Tendermint light clients (ICS 07) to expose the following additional state mutation functions\\n1. `Unfreeze()`, which unfreezes a light client after misbehaviour and clears any frozen height previously set\\n1. Require Tendermint light clients (ICS 07) & solo machine clients (ICS 06) to be created with the following additional flags\\n1. `allow_governance_override_after_misbehaviour` (boolean, default false)\\n1. Add a new governance proposal type, `ClientUpdateProposal`, in the `x\/ibc` module\\n1. Extend the base `Proposal` with a client identifier (`string`) and a header (`bytes`, encoded in a client-type-specific format)\\n1. If this governance proposal passes, the client is updated with the provided header, if and only if:\\n1. `allow_governance_override_after_expiry` is true and the client has expired (`Expired()` returns true)\\n1. `allow_governance_override_after_misbehaviour` is true and the client has been frozen (`Frozen()` returns true)\\n1. In this case, additionally, the client is unfrozen by calling `Unfreeze()`\\nNote additionally that the header submitted by governance must be new enough that it will be possible to update the light client after the new header is inserted into the client state (which will only happen after the governance proposal has passed).\\nThis ADR does not address planned upgrades, which are handled separately as per the [specification](https:\/\/github.com\/cosmos\/ics\/tree\/master\/spec\/ics-007-tendermint-client#upgrades).\\n","tokens":370,"id":15}
{"File Name":"gatemint-sdk\/adr-021-protobuf-query-encoding.md","Context":"## Context\\nThis ADR is a continuation of the motivation, design, and context established in\\n[ADR 019](.\/adr-019-protobuf-state-encoding.md) and\\n[ARD 020](.\/adr-019-protobuf-transaction-encoding.md), namely, we aim to design the\\nProtocol Buffer migration path for the client-side of the Cosmos SDK.\\nThis ADR continues from [ARD 020](.\/adr-020-protobuf-transaction-encoding.md)\\nto specify the encoding of queries.\\n","Decision":"### Custom Query Definition\\nModules define custom queries through a protocol buffers `service` definition.\\nThese `service` definitions are generally associated with and used by the\\nGRPC protocol. However, the protocol buffers specification indicates that\\nthey can be used more generically by any request\/response protocol that uses\\nprotocol buffer encoding. Thus, we can use `service` definitions for specifying\\ncustom ABCI queries and even reuse a substantial amount of the GRPC infrastructure.\\nEach module with custom queries should define a service canonically named `Query`:\\n```proto\\n\/\/ x\/bank\/types\/types.proto\\nservice Query {\\nrpc QueryBalance(QueryBalanceParams) returns (cosmos_sdk.v1.Coin) { }\\nrpc QueryAllBalances(QueryAllBalancesParams) returns (QueryAllBalancesResponse) { }\\n}\\n```\\n#### Handling of Interface Types\\nModules that use interface types and need true polymorphism generally force a\\n`oneof` up to the app-level that provides the set of concrete implementations of\\nthat interface that the app supports. While app's are welcome to do the same for\\nqueries and implement an app-level query service, it is recommended that modules\\nprovide query methods that expose these interfaces via `google.protobuf.Any`.\\nThere is a concern on the transaction level that the overhead of `Any` is too\\nhigh to justify its usage. However for queries this is not a concern, and\\nproviding generic module-level queries that use `Any` does not preclude apps\\nfrom also providing app-level queries that return use the app-level `oneof`s.\\nA hypothetical example for the `gov` module would look something like:\\n```proto\\n\/\/ x\/gov\/types\/types.proto\\nimport \"google\/protobuf\/any.proto\";\\nservice Query {\\nrpc GetProposal(GetProposalParams) returns (AnyProposal) { }\\n}\\nmessage AnyProposal {\\nProposalBase base = 1;\\ngoogle.protobuf.Any content = 2;\\n}\\n```\\n### Custom Query Implementation\\nIn order to implement the query service, we can reuse the existing [gogo protobuf](https:\/\/github.com\/gogo\/protobuf)\\ngrpc plugin, which for a service named `Query` generates an interface named\\n`QueryServer` as below:\\n```go\\ntype QueryServer interface {\\nQueryBalance(context.Context, *QueryBalanceParams) (*types.Coin, error)\\nQueryAllBalances(context.Context, *QueryAllBalancesParams) (*QueryAllBalancesResponse, error)\\n}\\n```\\nThe custom queries for our module are implemented by implementing this interface.\\nThe first parameter in this generated interface is a generic `context.Context`,\\nwhereas querier methods generally need an instance of `sdk.Context` to read\\nfrom the store. Since arbitrary values can be attached to `context.Context`\\nusing the `WithValue` and `Value` methods, the SDK should provide a function\\n`sdk.UnwrapSDKContext` to retrieve the `sdk.Context` from the provided\\n`context.Context`.\\nAn example implementation of `QueryBalance` for the bank module as above would\\nlook something like:\\n```go\\ntype Querier struct {\\nKeeper\\n}\\nfunc (q Querier) QueryBalance(ctx context.Context, params *types.QueryBalanceParams) (*sdk.Coin, error) {\\nbalance := q.GetBalance(sdk.UnwrapSDKContext(ctx), params.Address, params.Denom)\\nreturn &balance, nil\\n}\\n```\\n### Custom Query Registration and Routing\\nQuery server implementations as above would be registered with `AppModule`s using\\na new method `RegisterQueryServer(grpc.Server)` which could be implemented simply\\nas below:\\n```go\\n\/\/ x\/bank\/module.go\\nfunc (am AppModule) RegisterQueryServer(server grpc.Server) {\\ntypes.RegisterQueryServer(server, keeper.Querier{am.keeper})\\n}\\n```\\nUnderneath the hood, a new method `RegisterService(sd *grpc.ServiceDesc, handler interface{})`\\nwill be added to the existing `baseapp.QueryRouter` to add the queries to the custom\\nquery routing table (with the routing method being described below).\\nThe signature for this method matches the existing\\n`RegisterServer` method on the GRPC `Server` type where `handler` is the custom\\nquery server implementation described above.\\nGRPC-like requests are routed by the service name (ex. `cosmos_sdk.x.bank.v1.Query`)\\nand method name (ex. `QueryBalance`) combined with `\/`s to form a full\\nmethod name (ex. `\/cosmos_sdk.x.bank.v1.Query\/QueryBalance`). This gets translated\\ninto an ABCI query as `custom\/cosmos_sdk.x.bank.v1.Query\/QueryBalance`. Service handlers\\nregistered with `QueryRouter.RegisterService` will be routed this way.\\nBeyond the method name, GRPC requests carry a protobuf encoded payload, which maps naturally\\nto `RequestQuery.Data`, and receive a protobuf encoded response or error. Thus\\nthere is a quite natural mapping of GRPC-like rpc methods to the existing\\n`sdk.Query` and `QueryRouter` infrastructure.\\nThis basic specification allows us to reuse protocol buffer `service` definitions\\nfor ABCI custom queries substantially reducing the need for manual decoding and\\nencoding in query methods.\\n### GRPC Protocol Support\\nIn addition to providing an ABCI query pathway, we can easily provide a GRPC\\nproxy server that routes requests in the GRPC protocol to ABCI query requests\\nunder the hood. In this way, clients could use their host languages' existing\\nGRPC implementations to make direct queries against Cosmos SDK app's using\\nthese `service` definitions. In order for this server to work, the `QueryRouter`\\non `BaseApp` will need to expose the service handlers registered with\\n`QueryRouter.RegisterService` to the proxy server implementation. Nodes could\\nlaunch the proxy server on a separate port in the same process as the ABCI app\\nwith a command-line flag.\\n### REST Queries and Swagger Generation\\n[grpc-gateway](https:\/\/github.com\/grpc-ecosystem\/grpc-gateway) is a project that\\ntranslates REST calls into GRPC calls using special annotations on service\\nmethods. Modules that want to expose REST queries should add `google.api.http`\\nannotations to their `rpc` methods as in this example below.\\n```proto\\n\/\/ x\/bank\/types\/types.proto\\nservice Query {\\nrpc QueryBalance(QueryBalanceParams) returns (cosmos_sdk.v1.Coin) {\\noption (google.api.http) = {\\nget: \"\/x\/bank\/v1\/balance\/{address}\/{denom}\"\\n};\\n}\\nrpc QueryAllBalances(QueryAllBalancesParams) returns (QueryAllBalancesResponse) {\\noption (google.api.http) = {\\nget: \"\/x\/bank\/v1\/balances\/{address}\"\\n};\\n}\\n}\\n```\\ngrpc-gateway will work direcly against the GRPC proxy described above which will\\ntranslate requests to ABCI queries under the hood. grpc-gateway can also\\ngenerate Swagger definitions automatically.\\nIn the current implementation of REST queries, each module needs to implement\\nREST queries manually in addition to ABCI querier methods. Using the grpc-gateway\\napproach, there will be no need to generate separate REST query handlers, just\\nquery servers as described above as grpc-gateway handles the translation of protobuf\\nto REST as well as Swagger definitions.\\nThe SDK should provide CLI commands for apps to start GRPC gateway either in\\na separate process or the same process as the ABCI app, as well as provide a\\ncommand for generating grpc-gateway proxy `.proto` files and the `swagger.json`\\nfile.\\n### Client Usage\\nThe gogo protobuf grpc plugin generates client interfaces in addition to server\\ninterfaces. For the `Query` service defined above we would get a `QueryClient`\\ninterface like:\\n```go\\ntype QueryClient interface {\\nQueryBalance(ctx context.Context, in *QueryBalanceParams, opts ...grpc.CallOption) (*types.Coin, error)\\nQueryAllBalances(ctx context.Context, in *QueryAllBalancesParams, opts ...grpc.CallOption) (*QueryAllBalancesResponse, error)\\n}\\n```\\nVia a small patch to gogo protobuf ([gogo\/protobuf#675](https:\/\/github.com\/gogo\/protobuf\/pull\/675))\\nwe have tweaked the grpc codegen to use an interface rather than concrete type\\nfor the generated client struct. This allows us to also reuse the GRPC infrastructure\\nfor ABCI client queries.\\n1Context` will receive a new method `QueryConn` that returns a `ClientConn`\\nthat routes calls to ABCI queries\\nClients (such as CLI methods) will then be able to call query methods like this:\\n```go\\nclientCtx := client.NewContext()\\nqueryClient := types.NewQueryClient(clientCtx.QueryConn())\\nparams := &types.QueryBalanceParams{addr, denom}\\nresult, err := queryClient.QueryBalance(gocontext.Background(), params)\\n```\\n### Testing\\nTests would be able to create a query client directly from keeper and `sdk.Context`\\nreferences using a `QueryServerTestHelper` as below:\\n```go\\nqueryHelper := baseapp.NewQueryServerTestHelper(ctx)\\ntypes.RegisterQueryServer(queryHelper, keeper.Querier{app.BankKeeper})\\nqueryClient := types.NewQueryClient(queryHelper)\\n```\\n","tokens":108,"id":16}
{"File Name":"gatemint-sdk\/adr-009-evidence-module.md","Context":"## Context\\nIn order to support building highly secure, robust and interoperable blockchain\\napplications, it is vital for the Cosmos SDK to expose a mechanism in which arbitrary\\nevidence can be submitted, evaluated and verified resulting in some agreed upon\\npenalty for any misbehavior committed by a validator, such as equivocation (double-voting),\\nsigning when unbonded, signing an incorrect state transition (in the future), etc.\\nFurthermore, such a mechanism is paramount for any\\n[IBC](https:\/\/github.com\/cosmos\/ics\/blob\/master\/ibc\/2_IBC_ARCHITECTURE.md) or\\ncross-chain validation protocol implementation in order to support the ability\\nfor any misbehavior to be relayed back from a collateralized chain to a primary\\nchain so that the equivocating validator(s) can be slashed.\\n","Decision":"We will implement an evidence module in the Cosmos SDK supporting the following\\nfunctionality:\\n- Provide developers with the abstractions and interfaces necessary to define\\ncustom evidence messages, message handlers, and methods to slash and penalize\\naccordingly for misbehavior.\\n- Support the ability to route evidence messages to handlers in any module to\\ndetermine the validity of submitted misbehavior.\\n- Support the ability, through governance, to modify slashing penalties of any\\nevidence type.\\n- Querier implementation to support querying params, evidence types, params, and\\nall submitted valid misbehavior.\\n### Types\\nFirst, we define the `Evidence` interface type. The `x\/evidence` module may implement\\nits own types that can be used by many chains (e.g. `CounterFactualEvidence`).\\nIn addition, other modules may implement their own `Evidence` types in a similar\\nmanner in which governance is extensible. It is important to note any concrete\\ntype implementing the `Evidence` interface may include arbitrary fields such as\\nan infraction time. We want the `Evidence` type to remain as flexible as possible.\\nWhen submitting evidence to the `x\/evidence` module, the concrete type must provide\\nthe validator's consensus address, which should be known by the `x\/slashing`\\nmodule (assuming the infraction is valid), the height at which the infraction\\noccurred and the validator's power at same height in which the infraction occurred.\\n```go\\ntype Evidence interface {\\nRoute() string\\nType() string\\nString() string\\nHash() HexBytes\\nValidateBasic() error\\n\/\/ The consensus address of the malicious validator at time of infraction\\nGetConsensusAddress() ConsAddress\\n\/\/ Height at which the infraction occurred\\nGetHeight() int64\\n\/\/ The total power of the malicious validator at time of infraction\\nGetValidatorPower() int64\\n\/\/ The total validator set power at time of infraction\\nGetTotalPower() int64\\n}\\n```\\n### Routing & Handling\\nEach `Evidence` type must map to a specific unique route and be registered with\\nthe `x\/evidence` module. It accomplishes this through the `Router` implementation.\\n```go\\ntype Router interface {\\nAddRoute(r string, h Handler) Router\\nHasRoute(r string) bool\\nGetRoute(path string) Handler\\nSeal()\\n}\\n```\\nUpon successful routing through the `x\/evidence` module, the `Evidence` type\\nis passed through a `Handler`. This `Handler` is responsible for executing all\\ncorresponding business logic necessary for verifying the evidence as valid. In\\naddition, the `Handler` may execute any necessary slashing and potential jailing.\\nSince slashing fractions will typically result from some form of static functions,\\nallow the `Handler` to do this provides the greatest flexibility. An example could\\nbe `k * evidence.GetValidatorPower()` where `k` is an on-chain parameter controlled\\nby governance. The `Evidence` type should provide all the external information\\nnecessary in order for the `Handler` to make the necessary state transitions.\\nIf no error is returned, the `Evidence` is considered valid.\\n```go\\ntype Handler func(Context, Evidence) error\\n```\\n### Submission\\n`Evidence` is submitted through a `MsgSubmitEvidence` message type which is internally\\nhandled by the `x\/evidence` module's `SubmitEvidence`.\\n```go\\ntype MsgSubmitEvidence struct {\\nEvidence\\n}\\nfunc handleMsgSubmitEvidence(ctx Context, keeper Keeper, msg MsgSubmitEvidence) Result {\\nif err := keeper.SubmitEvidence(ctx, msg.Evidence); err != nil {\\nreturn err.Result()\\n}\\n\/\/ emit events...\\nreturn Result{\\n\/\/ ...\\n}\\n}\\n```\\nThe `x\/evidence` module's keeper is responsible for matching the `Evidence` against\\nthe module's router and invoking the corresponding `Handler` which may include\\nslashing and jailing the validator. Upon success, the submitted evidence is persisted.\\n```go\\nfunc (k Keeper) SubmitEvidence(ctx Context, evidence Evidence) error {\\nhandler := keeper.router.GetRoute(evidence.Route())\\nif err := handler(ctx, evidence); err != nil {\\nreturn ErrInvalidEvidence(keeper.codespace, err)\\n}\\nkeeper.setEvidence(ctx, evidence)\\nreturn nil\\n}\\n```\\n### Genesis\\nFinally, we need to represent the genesis state of the `x\/evidence` module. The\\nmodule only needs a list of all submitted valid infractions and any necessary params\\nfor which the module needs in order to handle submitted evidence. The `x\/evidence`\\nmodule will naturally define and route native evidence types for which it'll most\\nlikely need slashing penalty constants for.\\n```go\\ntype GenesisState struct {\\nParams       Params\\nInfractions  []Evidence\\n}\\n```\\n","tokens":176,"id":17}
{"File Name":"gatemint-sdk\/adr-013-metrics.md","Context":"## Context\\nTelemetry is paramount into debugging and understanding what the application is doing and how it is\\nperforming. We aim to expose metrics from modules and other core parts of the Cosmos SDK.\\nIn addition, we should aim to support multiple configurable sinks that an operator may choose from.\\nBy default, when telemetry is enabled, the application should track and expose metrics that are\\nstored in-memory. The operator may choose to enable additional sinks, where we support only\\n[Prometheus](https:\/\/prometheus.io\/) for now, as it's battle-tested, simple to setup, open source,\\nand is rich with ecosystem tooling.\\nWe must also aim to integrate metrics into the Cosmos SDK in the most seamless way possible such that\\nmetrics may be added or removed at will and without much friction. To do this, we will use the\\n[go-metrics](https:\/\/github.com\/armon\/go-metrics) library.\\nFinally, operators may enable telemetry along with specific configuration options. If enabled, metrics\\nwill be exposed via `\/metrics?format={text|prometheus}` via the API server.\\n","Decision":"We will add an additional configuration block to `app.toml` that defines telemetry settings:\\n```toml\\n###############################################################################\\n###                         Telemetry Configuration                         ###\\n###############################################################################\\n[telemetry]\\n# Prefixed with keys to separate services\\nservice-name = {{ .Telemetry.ServiceName }}\\n# Enabled enables the application telemetry functionality. When enabled,\\n# an in-memory sink is also enabled by default. Operators may also enabled\\n# other sinks such as Prometheus.\\nenabled = {{ .Telemetry.Enabled }}\\n# Enable prefixing gauge values with hostname\\nenable-hostname = {{ .Telemetry.EnableHostname }}\\n# Enable adding hostname to labels\\nenable-hostname-label = {{ .Telemetry.EnableHostnameLabel }}\\n# Enable adding service to labels\\nenable-service-label = {{ .Telemetry.EnableServiceLabel }}\\n# PrometheusRetentionTime, when positive, enables a Prometheus metrics sink.\\nprometheus-retention-time = {{ .Telemetry.PrometheusRetentionTime }}\\n```\\nThe given configuration allows for two sinks -- in-memory and Prometheus. We create a `Metrics`\\ntype that performs all the bootstrapping for the operator, so capturing metrics becomes seamless.\\n```go\\n\/\/ Metrics defines a wrapper around application telemetry functionality. It allows\\n\/\/ metrics to be gathered at any point in time. When creating a Metrics object,\\n\/\/ internally, a global metrics is registered with a set of sinks as configured\\n\/\/ by the operator. In addition to the sinks, when a process gets a SIGUSR1, a\\n\/\/ dump of formatted recent metrics will be sent to STDERR.\\ntype Metrics struct {\\nmemSink           *metrics.InmemSink\\nprometheusEnabled bool\\n}\\n\/\/ Gather collects all registered metrics and returns a GatherResponse where the\\n\/\/ metrics are encoded depending on the type. Metrics are either encoded via\\n\/\/ Prometheus or JSON if in-memory.\\nfunc (m *Metrics) Gather(format string) (GatherResponse, error) {\\nswitch format {\\ncase FormatPrometheus:\\nreturn m.gatherPrometheus()\\ncase FormatText:\\nreturn m.gatherGeneric()\\ncase FormatDefault:\\nreturn m.gatherGeneric()\\ndefault:\\nreturn GatherResponse{}, fmt.Errorf(\"unsupported metrics format: %s\", format)\\n}\\n}\\n```\\nIn addition, `Metrics` allows us to gather the current set of metrics at any given point in time. An\\noperator may also choose to send a signal, SIGUSR1, to dump and print formatted metrics to STDERR.\\nDuring an application's bootstrapping and construction phase, if `Telemetry.Enabled` is `true`, the\\nAPI server will create an instance of a reference to `Metrics` object and will register a metrics\\nhandler accordingly.\\n```go\\nfunc (s *Server) Start(cfg config.Config) error {\\n\/\/ ...\\nif cfg.Telemetry.Enabled {\\nm, err := telemetry.New(cfg.Telemetry)\\nif err != nil {\\nreturn err\\n}\\ns.metrics = m\\ns.registerMetrics()\\n}\\n\/\/ ...\\n}\\nfunc (s *Server) registerMetrics() {\\nmetricsHandler := func(w http.ResponseWriter, r *http.Request) {\\nformat := strings.TrimSpace(r.FormValue(\"format\"))\\ngr, err := s.metrics.Gather(format)\\nif err != nil {\\nrest.WriteErrorResponse(w, http.StatusBadRequest, fmt.Sprintf(\"failed to gather metrics: %s\", err))\\nreturn\\n}\\nw.Header().Set(\"Content-Type\", gr.ContentType)\\n_, _ = w.Write(gr.Metrics)\\n}\\ns.Router.HandleFunc(\"\/metrics\", metricsHandler).Methods(\"GET\")\\n}\\n```\\nApplication developers may track counters, gauges, summaries, and key\/value metrics. There is no\\nadditional lifting required by modules to leverage profiling metrics. To do so, it's as simple as:\\n```go\\nfunc (k BaseKeeper) MintCoins(ctx sdk.Context, moduleName string, amt sdk.Coins) error {\\ndefer metrics.MeasureSince(time.Now(), \"MintCoins\")\\n\/\/ ...\\n}\\n```\\n","tokens":229,"id":18}
{"File Name":"gatemint-sdk\/adr-002-docs-structure.md","Context":"## Context\\nThere is a need for a scalable structure of the SDK documentation. Current documentation includes a lot of non-related SDK material, is difficult to maintain and hard to follow as a user.\\nIdeally, we would have:\\n- All docs related to dev frameworks or tools live in their respective github repos (sdk repo would contain sdk docs, hub repo would contain hub docs, lotion repo would contain lotion docs, etc.)\\n- All other docs (faqs, whitepaper, high-level material about Cosmos) would live on the website.\\n","Decision":"Re-structure the `\/docs` folder of the SDK github repo as follows:\\n```\\ndocs\/\\n\u251c\u2500\u2500 README\\n\u251c\u2500\u2500 intro\/\\n\u251c\u2500\u2500 concepts\/\\n\u2502   \u251c\u2500\u2500 baseapp\\n\u2502   \u251c\u2500\u2500 types\\n\u2502   \u251c\u2500\u2500 store\\n\u2502   \u251c\u2500\u2500 server\\n\u2502   \u251c\u2500\u2500 modules\/\\n\u2502   \u2502   \u251c\u2500\u2500 keeper\\n\u2502   \u2502   \u251c\u2500\u2500 handler\\n\u2502   \u2502   \u251c\u2500\u2500 cli\\n\u2502   \u251c\u2500\u2500 gas\\n\u2502   \u2514\u2500\u2500 commands\\n\u251c\u2500\u2500 clients\/\\n\u2502   \u251c\u2500\u2500 lite\/\\n\u2502   \u251c\u2500\u2500 service-providers\\n\u251c\u2500\u2500 modules\/\\n\u251c\u2500\u2500 spec\/\\n\u251c\u2500\u2500 translations\/\\n\u2514\u2500\u2500 architecture\/\\n```\\nThe files in each sub-folders do not matter and will likely change. What matters is the sectioning:\\n- `README`: Landing page of the docs.\\n- `intro`: Introductory material. Goal is to have a short explainer of the SDK and then channel people to the resource they need. The [sdk-tutorial](https:\/\/github.com\/cosmos\/sdk-application-tutorial\/) will be highlighted, as well as the `godocs`.\\n- `concepts`: Contains high-level explanations of the abstractions of the SDK. It does not contain specific code implementation and does not need to be updated often. **It is not an API specification of the interfaces**. API spec is the `godoc`.\\n- `clients`: Contains specs and info about the various SDK clients.\\n- `spec`: Contains specs of modules, and others.\\n- `modules`: Contains links to `godocs` and the spec of the modules.\\n- `architecture`: Contains architecture-related docs like the present one.\\n- `translations`: Contains different translations of the documentation.\\nWebsite docs sidebar will only include the following sections:\\n- `README`\\n- `intro`\\n- `concepts`\\n- `clients`\\n`architecture` need not be displayed on the website.\\n","tokens":113,"id":19}
{"File Name":"gatemint-sdk\/adr-003-dynamic-capability-store.md","Context":"## Context\\nFull implementation of the [IBC specification](https:\/\/github.com\/cosmos\/ics) requires the ability to create and authenticate object-capability keys at runtime (i.e., during transaction execution),\\nas described in [ICS 5](https:\/\/github.com\/cosmos\/ics\/tree\/master\/spec\/ics-005-port-allocation#technical-specification). In the IBC specification, capability keys are created for each newly initialised\\nport & channel, and are used to authenticate future usage of the port or channel. Since channels and potentially ports can be initialised during transaction execution, the state machine must be able to create\\nobject-capability keys at this time.\\nAt present, the Cosmos SDK does not have the ability to do this. Object-capability keys are currently pointers (memory addresses) of `StoreKey` structs created at application initialisation in `app.go` ([example](https:\/\/github.com\/cosmos\/gaia\/blob\/dcbddd9f04b3086c0ad07ee65de16e7adedc7da4\/app\/app.go#L132))\\nand passed to Keepers as fixed arguments ([example](https:\/\/github.com\/cosmos\/gaia\/blob\/dcbddd9f04b3086c0ad07ee65de16e7adedc7da4\/app\/app.go#L160)). Keepers cannot create or store capability keys during transaction execution \u2014 although they could call `NewKVStoreKey` and take the memory address\\nof the returned struct, storing this in the Merklised store would result in a consensus fault, since the memory address will be different on each machine (this is intentional \u2014 were this not the case, the keys would be predictable and couldn't serve as object capabilities).\\nKeepers need a way to keep a private map of store keys which can be altered during transaction execution, along with a suitable mechanism for regenerating the unique memory addresses (capability keys) in this map whenever the application is started or restarted, along with a mechanism to revert capability creation on tx failure.\\nThis ADR proposes such an interface & mechanism.\\n","Decision":"The SDK will include a new `CapabilityKeeper` abstraction, which is responsible for provisioning,\\ntracking, and authenticating capabilities at runtime. During application initialisation in `app.go`,\\nthe `CapabilityKeeper` will be hooked up to modules through unique function references\\n(by calling `ScopeToModule`, defined below) so that it can identify the calling module when later\\ninvoked.\\nWhen the initial state is loaded from disk, the `CapabilityKeeper`'s `Initialise` function will create\\nnew capability keys for all previously allocated capability identifiers (allocated during execution of\\npast transactions and assigned to particular modes), and keep them in a memory-only store while the\\nchain is running.\\nThe `CapabilityKeeper` will include a persistent `KVStore`, a `MemoryStore`, and an in-memory map.\\nThe persistent `KVStore` tracks which capability is owned by which modules.\\nThe `MemoryStore` stores a forward mapping that map from module name, capability tuples to capability names and\\na reverse mapping that map from module name, capability name to the capability index.\\nSince we cannot marshal the capability into a `KVStore` and unmarshal without changing the memory location of the capability,\\nthe reverse mapping in the KVStore will simply map to an index. This index can then be used as a key in the ephemeral\\ngo-map to retrieve the capability at the original memory location.\\nThe `CapabilityKeeper` will define the following types & functions:\\nThe `Capability` is similar to `StoreKey`, but has a globally unique `Index()` instead of\\na name. A `String()` method is provided for debugging.\\nA `Capability` is simply a struct, the address of which is taken for the actual capability.\\n```golang\\ntype Capability struct {\\nindex uint64\\n}\\n```\\nA `CapabilityKeeper` contains a persistent store key, memory store key, and mapping of allocated module names.\\n```golang\\ntype CapabilityKeeper struct {\\npersistentKey StoreKey\\nmemKey        StoreKey\\ncapMap        map[uint64]*Capability\\nmoduleNames   map[string]interface{}\\nsealed        bool\\n}\\n```\\nThe `CapabilityKeeper` provides the ability to create *scoped* sub-keepers which are tied to a\\nparticular module name. These `ScopedCapabilityKeeper`s must be created at application initialisation\\nand passed to modules, which can then use them to claim capabilities they receive and retrieve\\ncapabilities which they own by name, in addition to creating new capabilities & authenticating capabilities\\npassed by other modules.\\n```golang\\ntype ScopedCapabilityKeeper struct {\\npersistentKey StoreKey\\nmemKey        StoreKey\\ncapMap        map[uint64]*Capability\\nmoduleName    string\\n}\\n```\\n`ScopeToModule` is used to create a scoped sub-keeper with a particular name, which must be unique.\\nIt MUST be called before `InitialiseAndSeal`.\\n```golang\\nfunc (ck CapabilityKeeper) ScopeToModule(moduleName string) ScopedCapabilityKeeper {\\nif k.sealed {\\npanic(\"cannot scope to module via a sealed capability keeper\")\\n}\\nif _, ok := k.scopedModules[moduleName]; ok {\\npanic(fmt.Sprintf(\"cannot create multiple scoped keepers for the same module name: %s\", moduleName))\\n}\\nk.scopedModules[moduleName] = struct{}{}\\nreturn ScopedKeeper{\\ncdc:      k.cdc,\\nstoreKey: k.storeKey,\\nmemKey:   k.memKey,\\ncapMap:   k.capMap,\\nmodule:   moduleName,\\n}\\n}\\n```\\n`InitialiseAndSeal` MUST be called exactly once, after loading the initial state and creating all\\nnecessary `ScopedCapabilityKeeper`s, in order to populate the memory store with newly-created\\ncapability keys in accordance with the keys previously claimed by particular modules and prevent the\\ncreation of any new `ScopedCapabilityKeeper`s.\\n```golang\\nfunc (ck CapabilityKeeper) InitialiseAndSeal(ctx Context) {\\nif ck.sealed {\\npanic(\"capability keeper is sealed\")\\n}\\npersistentStore := ctx.KVStore(ck.persistentKey)\\nmap := ctx.KVStore(ck.memKey)\\n\/\/ initialise memory store for all names in persistent store\\nfor index, value := range persistentStore.Iter() {\\ncapability = &CapabilityKey{index: index}\\nfor moduleAndCapability := range value {\\nmoduleName, capabilityName := moduleAndCapability.Split(\"\/\")\\nmemStore.Set(moduleName + \"\/fwd\/\" + capability, capabilityName)\\nmemStore.Set(moduleName + \"\/rev\/\" + capabilityName, index)\\nck.capMap[index] = capability\\n}\\n}\\nck.sealed = true\\n}\\n```\\n`NewCapability` can be called by any module to create a new unique, unforgeable object-capability\\nreference. The newly created capability is automatically persisted; the calling module need not\\ncall `ClaimCapability`.\\n```golang\\nfunc (sck ScopedCapabilityKeeper) NewCapability(ctx Context, name string) (Capability, error) {\\n\/\/ check name not taken in memory store\\nif capStore.Get(\"rev\/\" + name) != nil {\\nreturn nil, errors.New(\"name already taken\")\\n}\\n\/\/ fetch the current index\\nindex := persistentStore.Get(\"index\")\\n\/\/ create a new capability\\ncapability := &CapabilityKey{index: index}\\n\/\/ set persistent store\\npersistentStore.Set(index, Set.singleton(sck.moduleName + \"\/\" + name))\\n\/\/ update the index\\nindex++\\npersistentStore.Set(\"index\", index)\\n\/\/ set forward mapping in memory store from capability to name\\nmemStore.Set(sck.moduleName + \"\/fwd\/\" + capability, name)\\n\/\/ set reverse mapping in memory store from name to index\\nmemStore.Set(sck.moduleName + \"\/rev\/\" + name, index)\\n\/\/ set the in-memory mapping from index to capability pointer\\ncapMap[index] = capability\\n\/\/ return the newly created capability\\nreturn capability\\n}\\n```\\n`AuthenticateCapability` can be called by any module to check that a capability\\ndoes in fact correspond to a particular name (the name can be untrusted user input)\\nwith which the calling module previously associated it.\\n```golang\\nfunc (sck ScopedCapabilityKeeper) AuthenticateCapability(name string, capability Capability) bool {\\n\/\/ return whether forward mapping in memory store matches name\\nreturn memStore.Get(sck.moduleName + \"\/fwd\/\" + capability) === name\\n}\\n```\\n`ClaimCapability` allows a module to claim a capability key which it has received from another module\\nso that future `GetCapability` calls will succeed.\\n`ClaimCapability` MUST be called if a module which receives a capability wishes to access it by name\\nin the future. Capabilities are multi-owner, so if multiple modules have a single `Capability` reference,\\nthey will all own it.\\n```golang\\nfunc (sck ScopedCapabilityKeeper) ClaimCapability(ctx Context, capability Capability, name string) error {\\npersistentStore := ctx.KVStore(sck.persistentKey)\\n\/\/ set forward mapping in memory store from capability to name\\nmemStore.Set(sck.moduleName + \"\/fwd\/\" + capability, name)\\n\/\/ set reverse mapping in memory store from name to capability\\nmemStore.Set(sck.moduleName + \"\/rev\/\" + name, capability)\\n\/\/ update owner set in persistent store\\nowners := persistentStore.Get(capability.Index())\\nowners.add(sck.moduleName + \"\/\" + name)\\npersistentStore.Set(capability.Index(), owners)\\n}\\n```\\n`GetCapability` allows a module to fetch a capability which it has previously claimed by name.\\nThe module is not allowed to retrieve capabilities which it does not own.\\n```golang\\nfunc (sck ScopedCapabilityKeeper) GetCapability(ctx Context, name string) (Capability, error) {\\n\/\/ fetch the index of capability using reverse mapping in memstore\\nindex := memStore.Get(sck.moduleName + \"\/rev\/\" + name)\\n\/\/ fetch capability from go-map using index\\ncapability := capMap[index]\\n\/\/ return the capability\\nreturn capability\\n}\\n```\\n`ReleaseCapability` allows a module to release a capability which it had previously claimed. If no\\nmore owners exist, the capability will be deleted globally.\\n```golang\\nfunc (sck ScopedCapabilityKeeper) ReleaseCapability(ctx Context, capability Capability) err {\\npersistentStore := ctx.KVStore(sck.persistentKey)\\nname := capStore.Get(sck.moduleName + \"\/fwd\/\" + capability)\\nif name == nil {\\nreturn error(\"capability not owned by module\")\\n}\\n\/\/ delete forward mapping in memory store\\nmemoryStore.Delete(sck.moduleName + \"\/fwd\/\" + capability, name)\\n\/\/ delete reverse mapping in memory store\\nmemoryStore.Delete(sck.moduleName + \"\/rev\/\" + name, capability)\\n\/\/ update owner set in persistent store\\nowners := persistentStore.Get(capability.Index())\\nowners.remove(sck.moduleName + \"\/\" + name)\\nif owners.size() > 0 {\\n\/\/ there are still other owners, keep the capability around\\npersistentStore.Set(capability.Index(), owners)\\n} else {\\n\/\/ no more owners, delete the capability\\npersistentStore.Delete(capability.Index())\\ndelete(capMap[capability.Index()])\\n}\\n}\\n```\\n### Usage patterns\\n#### Initialisation\\nAny modules which use dynamic capabilities must be provided a `ScopedCapabilityKeeper` in `app.go`:\\n```golang\\nck := NewCapabilityKeeper(persistentKey, memoryKey)\\nmod1Keeper := NewMod1Keeper(ck.ScopeToModule(\"mod1\"), ....)\\nmod2Keeper := NewMod2Keeper(ck.ScopeToModule(\"mod2\"), ....)\\n\/\/ other initialisation logic ...\\n\/\/ load initial state...\\nck.InitialiseAndSeal(initialContext)\\n```\\n#### Creating, passing, claiming and using capabilities\\nConsider the case where `mod1` wants to create a capability, associate it with a resource (e.g. an IBC channel) by name, then pass it to `mod2` which will use it later:\\nModule 1 would have the following code:\\n```golang\\ncapability := scopedCapabilityKeeper.NewCapability(ctx, \"resourceABC\")\\nmod2Keeper.SomeFunction(ctx, capability, args...)\\n```\\n`SomeFunction`, running in module 2, could then claim the capability:\\n```golang\\nfunc (k Mod2Keeper) SomeFunction(ctx Context, capability Capability) {\\nk.sck.ClaimCapability(ctx, capability, \"resourceABC\")\\n\/\/ other logic...\\n}\\n```\\nLater on, module 2 can retrieve that capability by name and pass it to module 1, which will authenticate it against the resource:\\n```golang\\nfunc (k Mod2Keeper) SomeOtherFunction(ctx Context, name string) {\\ncapability := k.sck.GetCapability(ctx, name)\\nmod1.UseResource(ctx, capability, \"resourceABC\")\\n}\\n```\\nModule 1 will then check that this capability key is authenticated to use the resource before allowing module 2 to use it:\\n```golang\\nfunc (k Mod1Keeper) UseResource(ctx Context, capability Capability, resource string) {\\nif !k.sck.AuthenticateCapability(name, capability) {\\nreturn errors.New(\"unauthenticated\")\\n}\\n\/\/ do something with the resource\\n}\\n```\\nIf module 2 passed the capability key to module 3, module 3 could then claim it and call module 1 just like module 2 did\\n(in which case module 1, module 2, and module 3 would all be able to use this capability).\\n","tokens":434,"id":20}
{"File Name":"gatemint-sdk\/adr-020-protobuf-transaction-encoding.md","Context":"## Context\\nThis ADR is a continuation of the motivation, design, and context established in\\n[ADR 019](.\/adr-019-protobuf-state-encoding.md), namely, we aim to design the\\nProtocol Buffer migration path for the client-side of the Cosmos SDK.\\nSpecifically, the client-side migration path primarily includes tx generation and\\nsigning, message construction and routing, in addition to CLI & REST handlers and\\nbusiness logic (i.e. queriers).\\nWith this in mind, we will tackle the migration path via two main areas, txs and\\nquerying. However, this ADR solely focuses on transactions. Querying should be\\naddressed in a future ADR, but it should build off of these proposals.\\nBased on detailed discussions ([\\#6030](https:\/\/github.com\/cosmos\/cosmos-sdk\/issues\/6030)\\nand [\\#6078](https:\/\/github.com\/cosmos\/cosmos-sdk\/issues\/6078)), the original\\ndesign for transactions was changed substantially from an `oneof` \/JSON-signing\\napproach to the approach described below.\\n","Decision":"### Transactions\\nSince interface values are encoded with `google.protobuf.Any` in state (see [ADR 019](adr-019-protobuf-state-encoding.md)),\\n`sdk.Msg`s are encoding with `Any` in transactions.\\nOne of the main goals of using `Any` to encode interface values is to have a\\ncore set of types which is reused by apps so that\\nclients can safely be compatible with as many chains as possible.\\nIt is one of the goals of this specification to provide a flexible cross-chain transaction\\nformat that can serve a wide variety of use cases without breaking client\\ncompatibility.\\nIn order to facilitate signing, transactions are separated into `TxBody`,\\nwhich will be re-used by `SignDoc` below, and `signatures`:\\n```proto\\n\/\/ types\/types.proto\\npackage cosmos_sdk.v1;\\nmessage Tx {\\nTxBody body = 1;\\nAuthInfo auth_info = 2;\\n\/\/ A list of signatures that matches the length and order of AuthInfo's signer_infos to\\n\/\/ allow connecting signature meta information like public key and signing mode by position.\\nrepeated bytes signatures = 3;\\n}\\n\/\/ A variant of Tx that pins the signer's exact binary represenation of body and\\n\/\/ auth_info. This is used for signing, broadcasting and verification. The binary\\n\/\/ `serialize(tx: TxRaw)` is stored in Tendermint and the hash `sha256(serialize(tx: TxRaw))`\\n\/\/ becomes the \"txhash\", commonly used as the transaction ID.\\nmessage TxRaw {\\n\/\/ A protobuf serialization of a TxBody that matches the representation in SignDoc.\\nbytes body = 1;\\n\/\/ A protobuf serialization of an AuthInfo that matches the representation in SignDoc.\\nbytes auth_info = 2;\\n\/\/ A list of signatures that matches the length and order of AuthInfo's signer_infos to\\n\/\/ allow connecting signature meta information like public key and signing mode by position.\\nrepeated bytes signatures = 3;\\n}\\nmessage TxBody {\\n\/\/ A list of messages to be executed. The required signers of those messages define\\n\/\/ the number and order of elements in AuthInfo's signer_infos and Tx's signatures.\\n\/\/ Each required signer address is added to the list only the first time it occurs.\\n\/\/\\n\/\/ By convention, the first required signer (usually from the first message) is referred\\n\/\/ to as the primary signer and pays the fee for the whole transaction.\\nrepeated google.protobuf.Any messages = 1;\\nstring memo = 2;\\nint64 timeout_height = 3;\\nrepeated google.protobuf.Any extension_options = 1023;\\n}\\nmessage AuthInfo {\\n\/\/ This list defines the signing modes for the required signers. The number\\n\/\/ and order of elements must match the required signers from TxBody's messages.\\n\/\/ The first element is the primary signer and the one which pays the fee.\\nrepeated SignerInfo signer_infos = 1;\\n\/\/ The fee can be calculated based on the cost of evaluating the body and doing signature verification of the signers. This can be estimated via simulation.\\nFee fee = 2;\\n}\\nmessage SignerInfo {\\n\/\/ The public key is optional for accounts that already exist in state. If unset, the\\n\/\/ verifier can use the required signer address for this position and lookup the public key.\\nPublicKey public_key = 1;\\n\/\/ ModeInfo describes the signing mode of the signer and is a nested\\n\/\/ structure to support nested multisig pubkey's\\nModeInfo mode_info = 2;\\n\/\/ sequence is the sequence of the account, which describes the\\n\/\/ number of committed transactions signed by a given address. It is used to prevent\\n\/\/ replay attacks.\\nuint64 sequence = 3;\\n}\\nmessage ModeInfo {\\noneof sum {\\nSingle single = 1;\\nMulti multi = 2;\\n}\\n\/\/ Single is the mode info for a single signer. It is structured as a message\\n\/\/ to allow for additional fields such as locale for SIGN_MODE_TEXTUAL in the future\\nmessage Single {\\nSignMode mode = 1;\\n}\\n\/\/ Multi is the mode info for a multisig public key\\nmessage Multi {\\n\/\/ bitarray specifies which keys within the multisig are signing\\nCompactBitArray bitarray = 1;\\n\/\/ mode_infos is the corresponding modes of the signers of the multisig\\n\/\/ which could include nested multisig public keys\\nrepeated ModeInfo mode_infos = 2;\\n}\\n}\\nenum SignMode {\\nSIGN_MODE_UNSPECIFIED = 0;\\nSIGN_MODE_DIRECT = 1;\\nSIGN_MODE_TEXTUAL = 2;\\nSIGN_MODE_LEGACY_AMINO_JSON = 127;\\n}\\n```\\nAs will be discussed below, in order to include as much of the `Tx` as possible\\nin the `SignDoc`, `SignerInfo` is separated from signatures so that only the\\nraw signatures themselves live outside of what is signed over.\\nBecause we are aiming for a flexible, extensible cross-chain transaction\\nformat, new transaction processing options should be added to `TxBody` as soon\\nthose use cases are discovered, even if they can't be implemented yet.\\nBecause there is coordination overhead in this, `TxBody` includes an\\n`extension_options` field which can be used for any transaction processing\\noptions that are not already covered. App developers should, nevertheless,\\nattempt to upstream important improvements to `Tx`.\\n### Signing\\nAll of the signing modes below aim to provide the following guarantees:\\n- **No Malleability**: `TxBody` and `AuthInfo` cannot change once the transaction\\nis signed\\n- **Predictable Gas**: if I am signing a transaction where I am paying a fee,\\nthe final gas is fully dependent on what I am signing\\nThese guarantees give the maximum amount confidence to message signers that\\nmanipulation of `Tx`s by intermediaries can't result in any meaningful changes.\\n#### `SIGN_MODE_DIRECT`\\nThe \"direct\" signing behavior is to sign the raw `TxBody` bytes as broadcast over\\nthe wire. This has the advantages of:\\n- requiring the minimum additional client capabilities beyond a standard protocol\\nbuffers implementation\\n- leaving effectively zero holes for transaction malleability (i.e. there are no\\nsubtle differences between the signing and encoding formats which could\\npotentially be exploited by an attacker)\\nSignatures are structured using the `SignDoc` below which reuses the serialization of\\n`TxBody` and `AuthInfo` and only adds the fields which are needed for signatures:\\n```proto\\n\/\/ types\/types.proto\\nmessage SignDoc {\\n\/\/ A protobuf serialization of a TxBody that matches the representation in TxRaw.\\nbytes body = 1;\\n\/\/ A protobuf serialization of an AuthInfo that matches the representation in TxRaw.\\nbytes auth_info = 2;\\nstring chain_id = 3;\\nuint64 account_number = 4;\\n}\\n```\\nIn order to sign in the default mode, clients take the following steps:\\n1. Serialize `TxBody` and `AuthInfo` using any valid protobuf implementation.\\n2. Create a `SignDoc` and serialize it using [ADR 027](.\/adr-027-deterministic-protobuf-serialization.md).\\n3. Sign the encoded `SignDoc` bytes.\\n4. Build a `TxRaw` and serialize it for broadcasting.\\nSignature verification is based on comparing the raw `TxBody` and `AuthInfo`\\nbytes encoded in `TxRaw` not based on any [\"canonicalization\"](https:\/\/github.com\/regen-network\/canonical-proto3)\\nalgorithm which creates added complexity for clients in addition to preventing\\nsome forms of upgradeability (to be addressed later in this document).\\nSignature verifiers do:\\n1. Deserialize a `TxRaw` and pull out `body` and `auth_info`.\\n2. Create a list of required signer addresses from the messages.\\n3. For each required signer:\\n- Pull account number and sequence from the state.\\n- Obtain the public key either from state or `AuthInfo`'s `signer_infos`.\\n- Create a `SignDoc` and serialize it using [ADR 027](.\/adr-027-deterministic-protobuf-serialization.md).\\n- Verify the signature at the the same list position against the serialized `SignDoc`.\\n#### `SIGN_MODE_LEGACY_AMINO`\\nIn order to support legacy wallets and exchanges, Amino JSON will be temporarily\\nsupported transaction signing. Once wallets and exchanges have had a\\nchance to upgrade to protobuf based signing, this option will be disabled. In\\nthe meantime, it is foreseen that disabling the current Amino signing would cause\\ntoo much breakage to be feasible. Note that this is mainly a requirement of the\\nCosmos Hub and other chains may choose to disable Amino signing immediately.\\nLegacy clients will be able to sign a transaction using the current Amino\\nJSON format and have it encoded to protobuf using the REST `\/tx\/encode`\\nendpoint before broadcasting.\\n#### `SIGN_MODE_TEXTUAL`\\nAs was discussed extensively in [\\#6078](https:\/\/github.com\/cosmos\/cosmos-sdk\/issues\/6078),\\nthere is a desire for a human-readable signing encoding, especially for hardware\\nwallets like the [Ledger](https:\/\/www.ledger.com) which display\\ntransaction contents to users before signing. JSON was an attempt at this but\\nfalls short of the ideal.\\n`SIGN_MODE_TEXTUAL` is intended as a placeholder for a human-readable\\nencoding which will replace Amino JSON. This new encoding should be even more\\nfocused on readability than JSON, possibly based on formatting strings like\\n[MessageFormat](http:\/\/userguide.icu-project.org\/formatparse\/messages).\\nIn order to ensure that the new human-readable format does not suffer from\\ntransaction malleability issues, `SIGN_MODE_TEXTUAL`\\nrequires that the _human-readable bytes are concatenated with the raw `SignDoc`_\\nto generate sign bytes.\\nMultiple human-readable formats (maybe even localized messages) may be supported\\nby `SIGN_MODE_TEXTUAL` when it is implemented.\\n### Unknown Field Filtering\\nUnknown fields in protobuf messages should generally be rejected by transaction\\nprocessors because:\\n- important data may be present in the unknown fields, that if ignored, will\\ncause unexpected behavior for clients\\n- they present a malleability vulnerability where attackers can bloat tx size\\nby adding random uninterpreted data to unsigned content (i.e. the master `Tx`,\\nnot `TxBody`)\\nThere are also scenarios where we may choose to safely ignore unknown fields\\n(https:\/\/github.com\/cosmos\/cosmos-sdk\/issues\/6078#issuecomment-624400188) to\\nprovide graceful forwards compatibility with newer clients.\\nWe propose that field numbers with bit 11 set (for most use cases this is\\nthe range of 1024-2047) be considered non-critical fields that can safely be\\nignored if unknown.\\nTo handle this we will need a unknown field filter that:\\n- always rejects unknown fields in unsigned content (i.e. top-level `Tx` and\\nunsigned parts of `AuthInfo` if present based on the signing mode)\\n- rejects unknown fields in all messages (including nested `Any`s) other than\\nfields with bit 11 set\\nThis will likely need to be a custom protobuf parser pass that takes message bytes\\nand `FileDescriptor`s and returns a boolean result.\\n### Public Key Encoding\\nPublic keys in the Cosmos SDK implement Tendermint's `crypto.PubKey` interface,\\nso a natural solution might be to use `Any` as we are doing for other interfaces.\\nThere are, however, a limited number of public keys in existence and new ones\\naren't created overnight. The proposed solution is to use a `oneof` that:\\n- attempts to catalog all known key types even if a given app can't use them all\\n- has an `Any` member that can be used when a key type isn't present in the `oneof`\\nEx:\\n```proto\\nmessage PublicKey {\\noneof sum {\\nbytes secp256k1 = 1;\\nbytes ed25519 = 2;\\n...\\ngoogle.protobuf.Any any_pubkey = 15;\\n}\\n}\\n```\\nApps should only attempt to handle a registered set of public keys that they\\nhave tested. The provided signature verification ante handler decorators will\\nenforce this.\\n### CLI & REST\\nCurrently, the REST and CLI handlers encode and decode types and txs via Amino\\nJSON encoding using a concrete Amino codec. Being that some of the types dealt with\\nin the client can be interfaces, similar to how we described in [ADR 019](.\/adr-019-protobuf-state-encoding.md),\\nthe client logic will now need to take a codec interface that knows not only how\\nto handle all the types, but also knows how to generate transactions, signatures,\\nand messages.\\n```go\\ntype AccountRetriever interface {\\nEnsureExists(clientCtx client.Context, addr sdk.AccAddress) error\\nGetAccountNumberSequence(clientCtx client.Context, addr sdk.AccAddress) (uint64, uint64, error)\\n}\\ntype Generator interface {\\nNewTx() TxBuilder\\nNewFee() ClientFee\\nNewSignature() ClientSignature\\nMarshalTx(tx types.Tx) ([]byte, error)\\n}\\ntype TxBuilder interface {\\nGetTx() sdk.Tx\\nSetMsgs(...sdk.Msg) error\\nGetSignatures() []sdk.Signature\\nSetSignatures(...sdk.Signature)\\nGetFee() sdk.Fee\\nSetFee(sdk.Fee)\\nGetMemo() string\\nSetMemo(string)\\n}\\n```\\nWe then update `Context` to have new fields: `JSONMarshaler`, `TxGenerator`,\\nand `AccountRetriever`, and we update `AppModuleBasic.GetTxCmd` to take\\na `Context` which should have all of these fields pre-populated.\\nEach client method should then use one of the `Init` methods to re-initialize\\nthe pre-populated `Context`. `tx.GenerateOrBroadcastTx` can be used to\\ngenerate or broadcast a transaction. For example:\\n```go\\nimport \"github.com\/spf13\/cobra\"\\nimport \"github.com\/cosmos\/cosmos-sdk\/client\"\\nimport \"github.com\/cosmos\/cosmos-sdk\/client\/tx\"\\nfunc NewCmdDoSomething(clientCtx client.Context) *cobra.Command {\\nreturn &cobra.Command{\\nRunE: func(cmd *cobra.Command, args []string) error {\\nclientCtx := ctx.InitWithInput(cmd.InOrStdin())\\nmsg := NewSomeMsg{...}\\ntx.GenerateOrBroadcastTx(clientCtx, msg)\\n},\\n}\\n}\\n```\\n","tokens":234,"id":21}
{"File Name":"gatemint-sdk\/adr-018-extendable-voting-period.md","Context":"## Context\\nCurrently the voting period for all governance proposals is the same.  However, this is suboptimal as all governance proposals do not require the same time period.  For more non-contentious proposals, they can be dealt with more efficently with a faster period, while more contentious or complex proposals may need a longer period for extended discussion\/consideration.\\n","Decision":"We would like to design a mechanism for making the voting period of a governance proposal variable based on the demand of voters.  We would like it to be based on the view of the governance participants, rather than just the proposer of a governance proposal (thus, allowing the proposer to select the voting period length is not sufficient).\\nHowever, we would like to avoid the creation of an entire second voting process to determine the length of the voting period, as it just pushed the problem to determining the length of that first voting period.\\nThus, we propose the following mechanism:\\n### Params:\\n- The current gov param `VotingPeriod` is to be replaced by a `MinVotingPeriod` param.  This is the the default voting period that all governance proposal voting periods start with.\\n- There is a new gov param called `MaxVotingPeriodExtension`.\\n### Mechanism\\nThere is a new `Msg` type called `MsgExtendVotingPeriod`, which can be sent by any staked account during a proposal's voting period.  It allows the sender to unilaterally extend the length of the voting period by `MaxVotingPeriodExtension * sender's share of voting power`.  Every address can only call `MsgExtendVotingPeriod` once per proposal.\\nSo for example, if the `MaxVotingPeriodExtension` is set to 100 Days, then anyone with 1% of voting power can extend the voting power by 1 day.  If 33% of voting power has sent the message, the voting period will be extended by 33 days.  Thus, if absolutely everyone chooses to extend the voting period, the absolute maximum voting period will be `MinVotingPeriod + MaxVotingPeriodExtension`.\\nThis system acts as a sort of distributed coordination, where individual stakers choosing to extend or not, allows the system the guage the conentiousness\/complexity of the proposal.  It is extremely unlikely that many stakers will choose to extend at the exact same time, it allows stakers to view how long others have already extended thus far, to decide whether or not to extend further.\\n### Dealing with Unbonding\/Redelegation\\nThere is one thing that needs to be addressed.  How to deal with redelegation\/unbonding during the voting period.  If a staker of 5% calls `MsgExtendVotingPeriod` and then unbonds, does the voting period then decrease by 5 days again?  This is not good as it can give people a false sense of how long they have to make their decision.  For this reason, we want to design it such that the voting period length can only be extended, not shortened.  To do this, the current extension amount is based on the highest percent that voted extension at any time.  This is best explained by example:\\n1. Let's say 2 stakers of voting power 4% and 3% respectively vote to extend.  The voting period will be extended by 7 days.\\n2. Now the staker of 3% decides to unbond before the end of the voting period.  The voting period extension remains 7 days.\\n3. Now, let's say another staker of 2% voting power decides to extend voting period.  There is now 6% of active voting power choosing the extend.  The voting power remains 7 days.\\n4. If a fourth staker of 10% chooses to extend now, there is a total of 16% of active voting power wishing to extend.  The voting period will be extended to 16 days.\\n### Delegators\\nJust like votes in the actual voting period, delegators automatically inherit the extension of their validators.  If their validator chooses to extend, their voting power will be used in the validator's extension.  However, the delegator is unable to override their validator and \"unextend\" as that would contradict the \"voting power length can only be ratcheted up\" principle described in the previous section.  However, a delegator may choose the extend using their personal voting power, if their validator has not done so.\\n","tokens":75,"id":22}
{"File Name":"gatemint-sdk\/adr-017-historical-header-module.md","Context":"## Context\\nIn order for the Cosmos SDK to implement the [IBC specification](https:\/\/github.com\/cosmos\/ics), modules within the SDK must have the ability to introspect recent consensus states (validator sets & commitment roots) as proofs of these values on other chains must be checked during the handshakes.\\n","Decision":"The application MUST store the most recent `n` headers in a persistent store. At first, this store MAY be the current Merklised store. A non-Merklised store MAY be used later as no proofs are necessary.\\nThe application MUST store this information by storing new headers immediately when handling `abci.RequestBeginBlock`:\\n```golang\\nfunc BeginBlock(ctx sdk.Context, keeper HistoricalHeaderKeeper, req abci.RequestBeginBlock) abci.ResponseBeginBlock {\\ninfo := HistoricalInfo{\\nHeader: ctx.BlockHeader(),\\nValSet: keeper.StakingKeeper.GetAllValidators(ctx), \/\/ note that this must be stored in a canonical order\\n}\\nkeeper.SetHistoricalInfo(ctx, ctx.BlockHeight(), info)\\nn := keeper.GetParamRecentHeadersToStore()\\nkeeper.PruneHistoricalInfo(ctx, ctx.BlockHeight() - n)\\n\/\/ continue handling request\\n}\\n```\\nAlternatively, the application MAY store only the hash of the validator set.\\nThe application MUST make these past `n` committed headers available for querying by SDK modules through the `Keeper`'s `GetHistoricalInfo` function. This MAY be implemented in a new module, or it MAY also be integrated into an existing one (likely `x\/staking` or `x\/ibc`).\\n`n` MAY be configured as a parameter store parameter, in which case it could be changed by `ParameterChangeProposal`s, although it will take some blocks for the stored information to catch up if `n` is increased.\\n","tokens":65,"id":23}
{"File Name":"gatemint-sdk\/adr-019-protobuf-state-encoding.md","Context":"## Context\\nCurrently, the Cosmos SDK utilizes [go-amino](https:\/\/github.com\/tendermint\/go-amino\/) for binary\\nand JSON object encoding over the wire bringing parity between logical objects and persistence objects.\\nFrom the Amino docs:\\n> Amino is an object encoding specification. It is a subset of Proto3 with an extension for interface\\n> support. See the [Proto3 spec](https:\/\/developers.google.com\/protocol-buffers\/docs\/proto3) for more\\n> information on Proto3, which Amino is largely compatible with (but not with Proto2).\\n>\\n> The goal of the Amino encoding protocol is to bring parity into logic objects and persistence objects.\\nAmino also aims to have the following goals (not a complete list):\\n- Binary bytes must be decode-able with a schema.\\n- Schema must be upgradeable.\\n- The encoder and decoder logic must be reasonably simple.\\nHowever, we believe that Amino does not fulfill these goals completely and does not fully meet the\\nneeds of a truly flexible cross-language and multi-client compatible encoding protocol in the Cosmos SDK.\\nNamely, Amino has proven to be a big pain-point in regards to supporting object serialization across\\nclients written in various languages while providing virtually little in the way of true backwards\\ncompatibility and upgradeability. Furthermore, through profiling and various benchmarks, Amino has\\nbeen shown to be an extremely large performance bottleneck in the Cosmos SDK <sup>1<\/sup>. This is\\nlargely reflected in the performance of simulations and application transaction throughput.\\nThus, we need to adopt an encoding protocol that meets the following criteria for state serialization:\\n- Language agnostic\\n- Platform agnostic\\n- Rich client support and thriving ecosystem\\n- High performance\\n- Minimal encoded message size\\n- Codegen-based over reflection-based\\n- Supports backward and forward compatibility\\nNote, migrating away from Amino should be viewed as a two-pronged approach, state and client encoding.\\nThis ADR focuses on state serialization in the Cosmos SDK state machine. A corresponding ADR will be\\nmade to address client-side encoding.\\n","Decision":"We will adopt [Protocol Buffers](https:\/\/developers.google.com\/protocol-buffers) for serializing\\npersisted structured data in the Cosmos SDK while providing a clean mechanism and developer UX for\\napplications wishing to continue to use Amino. We will provide this mechanism by updating modules to\\naccept a codec interface, `Marshaler`, instead of a concrete Amino codec. Furthermore, the Cosmos SDK\\nwill provide three concrete implementations of the `Marshaler` interface: `AminoCodec`, `ProtoCodec`,\\nand `HybridCodec`.\\n- `AminoCodec`: Uses Amino for both binary and JSON encoding.\\n- `ProtoCodec`: Uses Protobuf for or both binary and JSON encoding.\\n- `HybridCodec`: Uses Amino for JSON encoding and Protobuf for binary encoding.\\nUntil the client migration landscape is fully understood and designed, modules will use a `HybridCodec`\\nas the concrete codec it accepts and\/or extends. This means that all client JSON encoding, including\\ngenesis state, will still use Amino. The ultimate goal will be to replace Amino JSON encoding with\\nProtbuf encoding and thus have modules accept and\/or extend `ProtoCodec`.\\n### Module Codecs\\nModules that do not require the ability to work with and serialize interfaces, the path to Protobuf\\nmigration is pretty straightforward. These modules are to simply migrate any existing types that\\nare encoded and persisted via their concrete Amino codec to Protobuf and have their keeper accept a\\n`Marshaler` that will be a `HybridCodec`. This migration is simple as things will just work as-is.\\nNote, any business logic that needs to encode primitive types like `bool` or `int64` should use\\n[gogoprotobuf](https:\/\/github.com\/gogo\/protobuf) Value types.\\nExample:\\n```go\\nts, err := gogotypes.TimestampProto(completionTime)\\nif err != nil {\\n\/\/ ...\\n}\\nbz := cdc.MustMarshalBinaryBare(ts)\\n```\\nHowever, modules can vary greatly in purpose and design and so we must support the ability for modules\\nto be able to encode and work with interfaces (e.g. `Account` or `Content`). For these modules, they\\nmust define their own codec interface that extends `Marshaler`. These specific interfaces are unique\\nto the module and will contain method contracts that know how to serialize the needed interfaces.\\nExample:\\n```go\\n\/\/ x\/auth\/types\/codec.go\\ntype Codec interface {\\ncodec.Marshaler\\nMarshalAccount(acc exported.Account) ([]byte, error)\\nUnmarshalAccount(bz []byte) (exported.Account, error)\\nMarshalAccountJSON(acc exported.Account) ([]byte, error)\\nUnmarshalAccountJSON(bz []byte) (exported.Account, error)\\n}\\n```\\n### Usage of `Any` to encode interfaces\\nIn general, module-level .proto files should define messages which encode interfaces\\nusing [`google.protobuf.Any`](https:\/\/github.com\/protocolbuffers\/protobuf\/blob\/master\/src\/google\/protobuf\/any.proto).\\nAfter [extension discussion](https:\/\/github.com\/cosmos\/cosmos-sdk\/issues\/6030),\\nthis was chosen as the preferred alternative to application-level `oneof`s\\nas in our original protobuf design. The arguments in favor of `Any` can be\\nsummarized as follows:\\n* `Any` provides a simpler, more consistent client UX for dealing with\\ninterfaces than app-level `oneof`s that will need to be coordinated more\\ncarefully across applications. Creating a generic transaction\\nsigning library using `oneof`s may be cumbersome and critical logic may need\\nto be reimplemented for each chain\\n* `Any` provides more resistance against human error than `oneof`\\n* `Any` is generally simpler to implement for both modules and apps\\nThe main counter-argument to using `Any` centers around its additional space\\nand possibly performance overhead. The space overhead could be dealt with using\\ncompression at the persistence layer in the future and the performance impact\\nis likely to be small. Thus, not using `Any` is seem as a pre-mature optimization,\\nwith user experience as the higher order concern.\\nNote, that given the SDK's decision to adopt the `Codec` interfaces described\\nabove, apps can still choose to use `oneof` to encode state and transactions\\nbut it is not the recommended approach. If apps do choose to use `oneof`s\\ninstead of `Any` they will likely lose compatibility with client apps that\\nsupport multiple chains. Thus developers should think carefully about whether\\nthey care more about what is possibly a pre-mature optimization or end-user\\nand client developer UX.\\n### Safe usage of `Any`\\nBy default, the [gogo protobuf implementation of `Any`](https:\/\/godoc.org\/github.com\/gogo\/protobuf\/types)\\nuses [global type registration]( https:\/\/github.com\/gogo\/protobuf\/blob\/master\/proto\/properties.go#L540)\\nto decode values packed in `Any` into concrete\\ngo types. This introduces a vulnerability where any malicious module\\nin the dependency tree could registry a type with the global protobuf registry\\nand cause it to be loaded and unmarshaled by a transaction that referenced\\nit in the `type_url` field.\\nTo prevent this, we introduce a type registration mechanism for decoding `Any`\\nvalues into concrete types through the `InterfaceRegistry` interface which\\nbears some similarity to type registration with Amino:\\n```go\\ntype InterfaceRegistry interface {\\n\/\/ RegisterInterface associates protoName as the public name for the\\n\/\/ interface passed in as iface\\n\/\/ Ex:\\n\/\/   registry.RegisterInterface(\"cosmos_sdk.Msg\", (*sdk.Msg)(nil))\\nRegisterInterface(protoName string, iface interface{})\\n\/\/ RegisterImplementations registers impls as a concrete implementations of\\n\/\/ the interface iface\\n\/\/ Ex:\\n\/\/  registry.RegisterImplementations((*sdk.Msg)(nil), &MsgSend{}, &MsgMultiSend{})\\nRegisterImplementations(iface interface{}, impls ...proto.Message)\\n}\\n```\\nIn addition to serving as a whitelist, `InterfaceRegistry` can also serve\\nto communicate the list of concrete types that satisfy an interface to clients.\\nIn .proto files:\\n* fields which accept interfaces should be annotated with `cosmos_proto.accepts_interface`\\nusing the same full-qualified name passed as `protoName` to `InterfaceRegistry.RegisterInterface`\\n* interface implementations should be annotated with `cosmos_proto.implements_interface`\\nusing the same full-qualified name passed as `protoName` to `InterfaceRegistry.RegisterInterface`\\nIn the future, `protoName`, `cosmos_proto.accepts_interface`, `cosmos_proto.implements_interface`\\nmay be used via code generation, reflection &\/or static linting.\\nThe same struct that implements `InterfaceRegistry` will also implement an\\ninterface `InterfaceUnpacker` to be used for unpacking `Any`s:\\n```go\\ntype InterfaceUnpacker interface {\\n\/\/ UnpackAny unpacks the value in any to the interface pointer passed in as\\n\/\/ iface. Note that the type in any must have been registered with\\n\/\/ RegisterImplementations as a concrete type for that interface\\n\/\/ Ex:\\n\/\/    var msg sdk.Msg\\n\/\/    err := ctx.UnpackAny(any, &msg)\\n\/\/    ...\\nUnpackAny(any *Any, iface interface{}) error\\n}\\n```\\nNote that `InterfaceRegistry` usage does not deviate from standard protobuf\\nusage of `Any`, it just introduces a security and introspection layer for\\ngolang usage.\\n`InterfaceRegistry` will be a member of `ProtoCodec` and `HybridCodec` as\\ndescribed above. In order for modules to register interface types, app modules\\ncan optionally implement the following interface:\\n```go\\ntype InterfaceModule interface {\\nRegisterInterfaceTypes(InterfaceRegistry)\\n}\\n```\\nThe module manager will include a method to call `RegisterInterfaceTypes` on\\nevery module that implements it in order to populate the `InterfaceRegistry`.\\n### Using `Any` to encode state\\nThe SDK will provide support methods `MarshalAny` and `UnmarshalAny` to allow\\neasy encoding of state to `Any` in `Codec` implementations. Ex:\\n```go\\nimport \"github.com\/cosmos\/cosmos-sdk\/codec\"\\nfunc (c *Codec) MarshalEvidence(evidenceI eviexported.Evidence) ([]byte, error) {\\nreturn codec.MarshalAny(evidenceI)\\n}\\nfunc (c *Codec) UnmarshalEvidence(bz []byte) (eviexported.Evidence, error) {\\nvar evi eviexported.Evidence\\nerr := codec.UnmarshalAny(c.interfaceContext, &evi, bz)\\nif err != nil {\\nreturn nil, err\\n}\\nreturn evi, nil\\n}\\n```\\n### Using `Any` in `sdk.Msg`s\\nA similar concept is to be applied for messages that contain interfaces fields.\\nFor example, we can define `MsgSubmitEvidence` as follows where `Evidence` is\\nan interface:\\n```protobuf\\n\/\/ x\/evidence\/types\/types.proto\\nmessage MsgSubmitEvidence {\\nbytes submitter = 1\\n[\\n(gogoproto.casttype) = \"github.com\/cosmos\/cosmos-sdk\/types.AccAddress\"\\n];\\ngoogle.protobuf.Any evidence = 2;\\n}\\n```\\nNote that in order to unpack the evidence from `Any` we do need a reference to\\n`InterfaceRegistry`. In order to reference evidence in methods like\\n`ValidateBasic` which shouldn't have to know about the `InterfaceRegistry`, we\\nintroduce an `UnpackInterfaces` phase to deserialization which unpacks\\ninterfaces before they're needed.\\n### Unpacking Interfaces\\nTo implement the `UnpackInterfaces` phase of deserialization which unpacks\\ninterfaces wrapped in `Any` before they're needed, we create an interface\\nthat `sdk.Msg`s and other types can implement:\\n```go\\ntype UnpackInterfacesMessage interface {\\nUnpackInterfaces(InterfaceUnpacker) error\\n}\\n```\\nWe also introduce a private `cachedValue interface{}` field onto the `Any`\\nstruct itself with a public getter `GetCachedValue() interface{}`.\\nThe `UnpackInterfaces` method is to be invoked during message deserialization right\\nafter `Unmarshal` and any interface values packed in `Any`s will be decoded\\nand stored in `cachedValue` for reference later.\\nThen unpacked interface values can safely be used in any code afterwards\\nwithout knowledge of the `InterfaceRegistry`\\nand messages can introduce a simple getter to cast the cached value to the\\ncorrect interface type.\\nThis has the added benefit that unmarshaling of `Any` values only happens once\\nduring initial deserialization rather than every time the value is read. Also,\\nwhen `Any` values are first packed (for instance in a call to\\n`NewMsgSubmitEvidence`), the original interface value is cached so that\\nunmarshaling isn't needed to read it again.\\n`MsgSubmitEvidence` could implement `UnpackInterfaces`, plus a convenience getter\\n`GetEvidence` as follows:\\n```go\\nfunc (msg MsgSubmitEvidence) UnpackInterfaces(ctx sdk.InterfaceRegistry) error {\\nvar evi eviexported.Evidence\\nreturn ctx.UnpackAny(msg.Evidence, *evi)\\n}\\nfunc (msg MsgSubmitEvidence) GetEvidence() eviexported.Evidence {\\nreturn msg.Evidence.GetCachedValue().(eviexported.Evidence)\\n}\\n```\\n### Amino Compatibility\\nOur custom implementation of `Any` can be used transparently with Amino if used\\nwith the proper codec instance. What this means is that interfaces packed within\\n`Any`s will be amino marshaled like regular Amino interfaces (assuming they\\nhave been registered properly with Amino).\\nIn order for this functionality to work:\\n- **all legacy code must use `*codec.LegacyAmino` instead of `*amino.Codec` which is\\nnow a wrapper which properly handles `Any`**\\n- **all new code should use `Marshaler` which is compatible with both amino and\\nprotobuf**\\n- Also, before v0.39, `codec.LegacyAmino` will be renamed to `codec.LegacyAmino`.\\n### Why Wasn't X Chosen Instead\\nFor a more complete comparison to alternative protocols, see [here](https:\/\/codeburst.io\/json-vs-protocol-buffers-vs-flatbuffers-a4247f8bda6f).\\n### Cap'n Proto\\nWhile [Cap\u2019n Proto](https:\/\/capnproto.org\/) does seem like an advantageous alternative to Protobuf\\ndue to it's native support for interfaces\/generics and built in canonicalization, it does lack the\\nrich client ecosystem compared to Protobuf and is a bit less mature.\\n### FlatBuffers\\n[FlatBuffers](https:\/\/google.github.io\/flatbuffers\/) is also a potentially viable alternative, with the\\nprimary difference being that FlatBuffers does not need a parsing\/unpacking step to a secondary\\nrepresentation before you can access data, often coupled with per-object memory allocation.\\nHowever, it would require great efforts into research and full understanding the scope of the migration\\nand path forward -- which isn't immediately clear. In addition, FlatBuffers aren't designed for\\nuntrusted inputs.\\n","tokens":439,"id":24}
{"File Name":"gatemint-sdk\/adr-011-generalize-genesis-accounts.md","Context":"## Context\\nCurrently, the SDK allows for custom account types; the `auth` keeper stores any type fulfilling its `Account` interface. However `auth` does not handle exporting or loading accounts to\/from a genesis file, this is done by `genaccounts`, which only handles one of 4 concrete account types (`BaseAccount`, `ContinuousVestingAccount`, `DelayedVestingAccount` and `ModuleAccount`).\\nProjects desiring to use custom accounts (say custom vesting accounts) need to fork and modify `genaccounts`.\\n","Decision":"In summary, we will (un)marshal all accounts (interface types) directly using amino, rather than converting to `genaccounts`\u2019s `GenesisAccount` type. Since doing this removes the majority of `genaccounts`'s code, we will merge `genaccounts` into `auth`. Marshalled accounts will be stored in `auth`'s genesis state.\\nDetailed changes:\\n### 1) (Un)Marshal accounts directly using amino\\nThe `auth` module's `GenesisState` gains a new field `Accounts`. Note these aren't of type `exported.Account` for reasons outlined in section 3.\\n```go\\n\/\/ GenesisState - all auth state that must be provided at genesis\\ntype GenesisState struct {\\nParams   Params           `json:\"params\" yaml:\"params\"`\\nAccounts []GenesisAccount `json:\"accounts\" yaml:\"accounts\"`\\n}\\n```\\nNow `auth`'s `InitGenesis` and `ExportGenesis` (un)marshal accounts as well as the defined params.\\n```go\\n\/\/ InitGenesis - Init store state from genesis data\\nfunc InitGenesis(ctx sdk.Context, ak AccountKeeper, data GenesisState) {\\nak.SetParams(ctx, data.Params)\\n\/\/ load the accounts\\nfor _, a := range data.Accounts {\\nacc := ak.NewAccount(ctx, a) \/\/ set account number\\nak.SetAccount(ctx, acc)\\n}\\n}\\n\/\/ ExportGenesis returns a GenesisState for a given context and keeper\\nfunc ExportGenesis(ctx sdk.Context, ak AccountKeeper) GenesisState {\\nparams := ak.GetParams(ctx)\\nvar genAccounts []exported.GenesisAccount\\nak.IterateAccounts(ctx, func(account exported.Account) bool {\\ngenAccount := account.(exported.GenesisAccount)\\ngenAccounts = append(genAccounts, genAccount)\\nreturn false\\n})\\nreturn NewGenesisState(params, genAccounts)\\n}\\n```\\n### 2) Register custom account types on the `auth` codec\\nThe `auth` codec must have all custom account types registered to marshal them. We will follow the pattern established in `gov` for proposals.\\nAn example custom account definition:\\n```go\\nimport authtypes \"github.com\/cosmos\/cosmos-sdk\/x\/auth\/types\"\\n\/\/ Register the module account type with the auth module codec so it can decode module accounts stored in a genesis file\\nfunc init() {\\nauthtypes.RegisterAccountTypeCodec(ModuleAccount{}, \"cosmos-sdk\/ModuleAccount\")\\n}\\ntype ModuleAccount struct {\\n...\\n```\\nThe `auth` codec definition:\\n```go\\nvar ModuleCdc *codec.LegacyAmino\\nfunc init() {\\nModuleCdc = codec.NewLegacyAmino()\\n\/\/ register module msg's and Account interface\\n...\\n\/\/ leave the codec unsealed\\n}\\n\/\/ RegisterAccountTypeCodec registers an external account type defined in another module for the internal ModuleCdc.\\nfunc RegisterAccountTypeCodec(o interface{}, name string) {\\nModuleCdc.RegisterConcrete(o, name, nil)\\n}\\n```\\n### 3) Genesis validation for custom account types\\nModules implement a `ValidateGenesis` method. As `auth` does not know of account implementations, accounts will need to validate themselves.\\nWe will unmarshal accounts into a `GenesisAccount` interface that includes a `Validate` method.\\n```go\\ntype GenesisAccount interface {\\nexported.Account\\nValidate() error\\n}\\n```\\nThen the `auth` `ValidateGenesis` function becomes:\\n```go\\n\/\/ ValidateGenesis performs basic validation of auth genesis data returning an\\n\/\/ error for any failed validation criteria.\\nfunc ValidateGenesis(data GenesisState) error {\\n\/\/ Validate params\\n...\\n\/\/ Validate accounts\\naddrMap := make(map[string]bool, len(data.Accounts))\\nfor _, acc := range data.Accounts {\\n\/\/ check for duplicated accounts\\naddrStr := acc.GetAddress().String()\\nif _, ok := addrMap[addrStr]; ok {\\nreturn fmt.Errorf(\"duplicate account found in genesis state; address: %s\", addrStr)\\n}\\naddrMap[addrStr] = true\\n\/\/ check account specific validation\\nif err := acc.Validate(); err != nil {\\nreturn fmt.Errorf(\"invalid account found in genesis state; address: %s, error: %s\", addrStr, err.Error())\\n}\\n}\\nreturn nil\\n}\\n```\\n### 4) Move add-genesis-account cli to `auth`\\nThe `genaccounts` module contains a cli command to add base or vesting accounts to a genesis file.\\nThis will be moved to `auth`. We will leave it to projects to write their own commands to add custom accounts. An extensible cli handler, similar to `gov`, could be created but it is not worth the complexity for this minor use case.\\n### 5) Update module and vesting accounts\\nUnder the new scheme, module and vesting account types need some minor updates:\\n- Type registration on `auth`'s codec (shown above)\\n- A `Validate` method for each `Account` concrete type\\n","tokens":111,"id":25}
{"File Name":"gatemint-sdk\/adr-015-ibc-packet-receiver.md","Context":"## Context\\n[ICS 26 - Routing Module](https:\/\/github.com\/cosmos\/ics\/tree\/master\/spec\/ics-026-routing-module) defines a function [`handlePacketRecv`](https:\/\/github.com\/cosmos\/ics\/tree\/master\/spec\/ics-026-routing-module#packet-relay).\\nIn ICS 26, the routing module is defined as a layer above each application module\\nwhich verifies and routes messages to the destination modules. It is possible to\\nimplement it as a separate module, however, we already have functionality to route\\nmessages upon the destination identifiers in the baseapp. This ADR suggests\\nto utilize existing `baseapp.router` to route packets to application modules.\\nGenerally, routing module callbacks have two separate steps in them,\\nverification and execution. This corresponds to the `AnteHandler`-`Handler`\\nmodel inside the SDK. We can do the verification inside the `AnteHandler`\\nin order to increase developer ergonomics by reducing boilerplate\\nverification code.\\nFor atomic multi-message transaction, we want to keep the IBC related\\nstate modification to be preserved even the application side state change\\nreverts. One of the example might be IBC token sending message following with\\nstake delegation which uses the tokens received by the previous packet message.\\nIf the token receiving fails for any reason, we might not want to keep\\nexecuting the transaction, but we also don't want to abort the transaction\\nor the sequence and commitment will be reverted and the channel will be stuck.\\nThis ADR suggests new `CodeType`, `CodeTxBreak`, to fix this problem.\\n","Decision":"`PortKeeper` will have the capability key that is able to access only the\\nchannels bound to the port. Entities that hold a `PortKeeper` will be\\nable to call the methods on it which are corresponding with the methods with\\nthe same names on the `ChannelKeeper`, but only with the\\nallowed port. `ChannelKeeper.Port(string, ChannelChecker)` will be defined to\\neasily construct a capability-safe `PortKeeper`. This will be addressed in\\nanother ADR and we will use insecure `ChannelKeeper` for now.\\n`baseapp.runMsgs` will break the loop over the messages if one of the handlers\\nreturns `!Result.IsOK()`. However, the outer logic will write the cached\\nstore if `Result.IsOK() || Result.Code.IsBreak()`. `Result.Code.IsBreak()` if\\n`Result.Code == CodeTxBreak`.\\n```go\\nfunc (app *BaseApp) runTx(tx Tx) (result Result) {\\nmsgs := tx.GetMsgs()\\n\/\/ AnteHandler\\nif app.anteHandler != nil {\\nanteCtx, msCache := app.cacheTxContext(ctx)\\nnewCtx, err := app.anteHandler(anteCtx, tx)\\nif !newCtx.IsZero() {\\nctx = newCtx.WithMultiStore(ms)\\n}\\nif err != nil {\\n\/\/ error handling logic\\nreturn res\\n}\\nmsCache.Write()\\n}\\n\/\/ Main Handler\\nrunMsgCtx, msCache := app.cacheTxContext(ctx)\\nresult = app.runMsgs(runMsgCtx, msgs)\\n\/\/ BEGIN modification made in this ADR\\nif result.IsOK() || result.IsBreak() {\\n\/\/ END\\nmsCache.Write()\\n}\\nreturn result\\n}\\n```\\nThe Cosmos SDK will define an `AnteDecorator` for IBC packet receiving. The\\n`AnteDecorator` will iterate over the messages included in the transaction, type\\n`switch` to check whether the message contains an incoming IBC packet, and if so\\nverify the Merkle proof.\\n```go\\ntype ProofVerificationDecorator struct {\\nclientKeeper ClientKeeper\\nchannelKeeper ChannelKeeper\\n}\\nfunc (pvr ProofVerificationDecorator) AnteHandle(ctx Context, tx Tx, simulate bool, next AnteHandler) (Context, error) {\\nfor _, msg := range tx.GetMsgs() {\\nvar err error\\nswitch msg := msg.(type) {\\ncase client.MsgUpdateClient:\\nerr = pvr.clientKeeper.UpdateClient(msg.ClientID, msg.Header)\\ncase channel.MsgPacket:\\nerr = pvr.channelKeeper.RecvPacket(msg.Packet, msg.Proofs, msg.ProofHeight)\\ncase chanel.MsgAcknowledgement:\\nerr = pvr.channelKeeper.AcknowledgementPacket(msg.Acknowledgement, msg.Proof, msg.ProofHeight)\\ncase channel.MsgTimeoutPacket:\\nerr = pvr.channelKeeper.TimeoutPacket(msg.Packet, msg.Proof, msg.ProofHeight, msg.NextSequenceRecv)\\ncase channel.MsgChannelOpenInit;\\nerr = pvr.channelKeeper.CheckOpen(msg.PortID, msg.ChannelID, msg.Channel)\\ndefault:\\ncontinue\\n}\\nif err != nil {\\nreturn ctx, err\\n}\\n}\\nreturn next(ctx, tx, simulate)\\n}\\n```\\nWhere `MsgUpdateClient`, `MsgPacket`, `MsgAcknowledgement`, `MsgTimeoutPacket`\\nare `sdk.Msg` types correspond to `handleUpdateClient`, `handleRecvPacket`,\\n`handleAcknowledgementPacket`, `handleTimeoutPacket` of the routing module,\\nrespectively.\\nThe side effects of `RecvPacket`, `VerifyAcknowledgement`,\\n`VerifyTimeout` will be extracted out into separated functions,\\n`WriteAcknowledgement`, `DeleteCommitment`, `DeleteCommitmentTimeout`, respectively,\\nwhich will be called by the application handlers after the execution.\\n`WriteAcknowledgement` writes the acknowledgement to the state that can be\\nverified by the counter-party chain and increments the sequence to prevent\\ndouble execution. `DeleteCommitment` will delete the commitment stored,\\n`DeleteCommitmentTimeout` will delete the commitment and close channel in case\\nof ordered channel.\\n```go\\nfunc (keeper ChannelKeeper) WriteAcknowledgement(ctx Context, packet Packet, ack []byte) {\\nkeeper.SetPacketAcknowledgement(ctx, packet.GetDestPort(), packet.GetDestChannel(), packet.GetSequence(), ack)\\nkeeper.SetNextSequenceRecv(ctx, packet.GetDestPort(), packet.GetDestChannel(), packet.GetSequence())\\n}\\nfunc (keeper ChannelKeeper) DeleteCommitment(ctx Context, packet Packet) {\\nkeeper.deletePacketCommitment(ctx, packet.GetSourcePort(), packet.GetSourceChannel(), packet.GetSequence())\\n}\\nfunc (keeper ChannelKeeper) DeleteCommitmentTimeout(ctx Context, packet Packet) {\\nk.deletePacketCommitment(ctx, packet.GetSourcePort(), packet.GetSourceChannel(), packet.GetSequence())\\nif channel.Ordering == types.ORDERED [\\nchannel.State = types.CLOSED\\nk.SetChannel(ctx, packet.GetSourcePort(), packet.GetSourceChannel(), channel)\\n}\\n}\\n```\\nEach application handler should call respective finalization methods on the `PortKeeper`\\nin order to increase sequence (in case of packet) or remove the commitment\\n(in case of acknowledgement and timeout).\\nCalling those functions implies that the application logic has successfully executed.\\nHowever, the handlers can return `Result` with `CodeTxBreak` after calling those methods\\nwhich will persist the state changes that has been already done but prevent any further\\nmessages to be executed in case of semantically invalid packet. This will keep the sequence\\nincreased in the previous IBC packets(thus preventing double execution) without\\nproceeding to the following messages.\\nIn any case the application modules should never return state reverting result,\\nwhich will make the channel unable to proceed.\\n`ChannelKeeper.CheckOpen` method will be introduced. This will replace `onChanOpen*` defined\\nunder the routing module specification. Instead of define each channel handshake callback\\nfunctions, application modules can provide `ChannelChecker` function with the `AppModule`\\nwhich will be injected to `ChannelKeeper.Port()` at the top level application.\\n`CheckOpen` will find the correct `ChennelChecker` using the\\n`PortID` and call it, which will return an error if it is unacceptable by the application.\\nThe `ProofVerificationDecorator` will be inserted to the top level application.\\nIt is not safe to make each module responsible to call proof verification\\nlogic, whereas application can misbehave(in terms of IBC protocol) by\\nmistake.\\nThe `ProofVerificationDecorator` should come right after the default sybil attack\\nresistent layer from the current `auth.NewAnteHandler`:\\n```go\\n\/\/ add IBC ProofVerificationDecorator to the Chain of\\nfunc NewAnteHandler(\\nak keeper.AccountKeeper, supplyKeeper types.SupplyKeeper, ibcKeeper ibc.Keeper,\\nsigGasConsumer SignatureVerificationGasConsumer) sdk.AnteHandler {\\nreturn sdk.ChainAnteDecorators(\\nNewSetUpContextDecorator(), \/\/ outermost AnteDecorator. SetUpContext must be called first\\n...\\nNewIncrementSequenceDecorator(ak),\\nibcante.ProofVerificationDecorator(ibcKeeper.ClientKeeper, ibcKeeper.ChannelKeeper), \/\/ innermost AnteDecorator\\n)\\n}\\n```\\nThe implementation of this ADR will also create a `Data` field of the `Packet` of type `[]byte`, which can be deserialised by the receiving module into its own private type. It is up to the application modules to do this according to their own interpretation, not by the IBC keeper.  This is crucial for dynamic IBC.\\nExample application-side usage:\\n```go\\ntype AppModule struct {}\\n\/\/ CheckChannel will be provided to the ChannelKeeper as ChannelKeeper.Port(module.CheckChannel)\\nfunc (module AppModule) CheckChannel(portID, channelID string, channel Channel) error {\\nif channel.Ordering != UNORDERED {\\nreturn ErrUncompatibleOrdering()\\n}\\nif channel.CounterpartyPort != \"bank\" {\\nreturn ErrUncompatiblePort()\\n}\\nif channel.Version != \"\" {\\nreturn ErrUncompatibleVersion()\\n}\\nreturn nil\\n}\\nfunc NewHandler(k Keeper) Handler {\\nreturn func(ctx Context, msg Msg) Result {\\nswitch msg := msg.(type) {\\ncase MsgTransfer:\\nreturn handleMsgTransfer(ctx, k, msg)\\ncase ibc.MsgPacket:\\nvar data PacketDataTransfer\\nif err := types.ModuleCodec.UnmarshalBinaryBare(msg.GetData(), &data); err != nil {\\nreturn err\\n}\\nreturn handlePacketDataTransfer(ctx, k, msg, data)\\ncase ibc.MsgTimeoutPacket:\\nvar data PacketDataTransfer\\nif err := types.ModuleCodec.UnmarshalBinaryBare(msg.GetData(), &data); err != nil {\\nreturn err\\n}\\nreturn handleTimeoutPacketDataTransfer(ctx, k, packet)\\n\/\/ interface { PortID() string; ChannelID() string; Channel() ibc.Channel }\\n\/\/ MsgChanInit, MsgChanTry implements ibc.MsgChannelOpen\\ncase ibc.MsgChannelOpen:\\nreturn handleMsgChannelOpen(ctx, k, msg)\\n}\\n}\\n}\\nfunc handleMsgTransfer(ctx Context, k Keeper, msg MsgTransfer) Result {\\nerr := k.SendTransfer(ctx,msg.PortID, msg.ChannelID, msg.Amount, msg.Sender, msg.Receiver)\\nif err != nil {\\nreturn sdk.ResultFromError(err)\\n}\\nreturn sdk.Result{}\\n}\\nfunc handlePacketDataTransfer(ctx Context, k Keeper, packet Packet, data PacketDataTransfer) Result {\\nerr := k.ReceiveTransfer(ctx, packet.GetSourcePort(), packet.GetSourceChannel(), packet.GetDestinationPort(), packet.GetDestinationChannel(), data)\\nif err != nil {\\n\/\/ TODO: Source chain sent invalid packet, shutdown channel\\n}\\nk.ChannelKeeper.WriteAcknowledgement([]byte{0x00}) \/\/ WriteAcknowledgement increases the sequence, preventing double spending\\nreturn sdk.Result{}\\n}\\nfunc handleCustomTimeoutPacket(ctx Context, k Keeper, packet CustomPacket) Result {\\nerr := k.RecoverTransfer(ctx, packet.GetSourcePort(), packet.GetSourceChannel(), packet.GetDestinationPort(), packet.GetDestinationChannel(), data)\\nif err != nil {\\n\/\/ This chain sent invalid packet or cannot recover the funds\\npanic(err)\\n}\\nk.ChannelKeeper.DeleteCommitmentTimeout(ctx, packet)\\n\/\/ packet timeout should not fail\\nreturn sdk.Result{}\\n}\\nfunc handleMsgChannelOpen(sdk.Context, k Keeper, msg MsgOpenChannel) Result {\\nk.AllocateEscrowAddress(ctx, msg.ChannelID())\\nreturn sdk.Result{}\\n}\\n```\\n","tokens":337,"id":26}
{"File Name":"gatemint-sdk\/adr-008-dCERT-group.md","Context":"## Context\\nIn order to reduce the number of parties involved with handling sensitive\\ninformation in an emergency scenario, we propose the creation of a\\nspecialization group named The Decentralized Computer Emergency Response Team\\n(dCERT).  Initially this group's role is intended to serve as coordinators\\nbetween various actors within a blockchain community such as validators,\\nbug-hunters, and developers.  During a time of crisis, the dCERT group would\\naggregate and relay input from a variety of stakeholders to the developers who\\nare actively devising a patch to the software, this way sensitive information\\ndoes not need to be publicly disclosed while some input from the community can\\nstill be gained.\\nAdditionally, a special privilege is proposed for the dCERT group: the capacity\\nto \"circuit-break\" (aka. temporarily disable)  a particular message path. Note\\nthat this privilege should be enabled\/disabled globally with a governance\\nparameter such that this privilege could start disabled and later be enabled\\nthrough a parameter change proposal, once a dCERT group has been established.\\nIn the future it is foreseeable that the community may wish to expand the roles\\nof dCERT with further responsibilities such as the capacity to \"pre-approve\" a\\nsecurity update on behalf of the community prior to a full community\\nwide vote whereby the sensitive information would be revealed prior to a\\nvulnerability being patched on the live network.\\n","Decision":"The dCERT group is proposed to include an implementation of a `SpecializationGroup`\\nas defined in [ADR 007](.\/adr-007-specialization-groups.md). This will include the\\nimplementation of:\\n- continuous voting\\n- slashing due to breach of soft contract\\n- revoking a member due to breach of soft contract\\n- emergency disband of the entire dCERT group (ex. for colluding maliciously)\\n- compensation stipend from the community pool or other means decided by\\ngovernance\\nThis system necessitates the following new parameters:\\n- blockly stipend allowance per dCERT member\\n- maximum number of dCERT members\\n- required staked slashable tokens for each dCERT member\\n- quorum for suspending a particular member\\n- proposal wager for disbanding the dCERT group\\n- stabilization period for dCERT member transition\\n- circuit break dCERT privileges enabled\\nThese parameters are expected to be implemented through the param keeper such\\nthat governance may change them at any given point.\\n### Continuous Voting Electionator\\nAn `Electionator` object is to be implemented as continuous voting and with the\\nfollowing specifications:\\n- All delegation addresses may submit votes at any point which updates their\\npreferred representation on the dCERT group.\\n- Preferred representation may be arbitrarily split between addresses (ex. 50%\\nto John, 25% to Sally, 25% to Carol)\\n- In order for a new member to be added to the dCERT group they must\\nsend a transaction accepting their admission at which point the validity of\\ntheir admission is to be confirmed.\\n- A sequence number is assigned when a member is added to dCERT group.\\nIf a member leaves the dCERT group and then enters back, a new sequence number\\nis assigned.\\n- Addresses which control the greatest amount of preferred-representation are\\neligible to join the dCERT group (up the _maximum number of dCERT members_).\\nIf the dCERT group is already full and new member is admitted, the existing\\ndCERT member with the lowest amount of votes is kicked from the dCERT group.\\n- In the split situation where the dCERT group is full but a vying candidate\\nhas the same amount of vote as an existing dCERT member, the existing\\nmember should maintain its position.\\n- In the split situation where somebody must be kicked out but the two\\naddresses with the smallest number of votes have the same number of votes,\\nthe address with the smallest sequence number maintains its position.\\n- A stabilization period can be optionally included to reduce the\\n\"flip-flopping\" of the dCERT membership tail members. If a stabilization\\nperiod is provided which is greater than 0, when members are kicked due to\\ninsufficient support, a queue entry is created which documents which member is\\nto replace which other member. While this entry is in the queue, no new entries\\nto kick that same dCERT member can be made. When the entry matures at the\\nduration of the  stabilization period, the new member is instantiated, and old\\nmember kicked.\\n### Staking\/Slashing\\nAll members of the dCERT group must stake tokens _specifically_ to maintain\\neligibility as a dCERT member. These tokens can be staked directly by the vying\\ndCERT member or out of the good will of a 3rd party (who shall gain no on-chain\\nbenefits for doing so). This staking mechanism should use the existing global\\nunbonding time of tokens staked for network validator security. A dCERT member\\ncan _only be_ a member if it has the required tokens staked under this\\nmechanism. If those tokens are unbonded then the dCERT member must be\\nautomatically kicked from the group.\\nSlashing of a particular dCERT member due to soft-contract breach should be\\nperformed by governance on a per member basis based on the magnitude of the\\nbreach.  The process flow is anticipated to be that a dCERT member is suspended\\nby the dCERT group prior to being slashed by governance.\\nMembership suspension by the dCERT group takes place through a voting procedure\\nby the dCERT group members. After this suspension has taken place, a governance\\nproposal to slash the dCERT member must be submitted, if the proposal is not\\napproved by the time the rescinding member has completed unbonding their\\ntokens, then the tokens are no longer staked and unable to be slashed.\\nAdditionally in the case of an emergency situation of a colluding and malicious\\ndCERT group, the community needs the capability to disband the entire dCERT\\ngroup and likely fully slash them. This could be achieved though a special new\\nproposal type (implemented as a general governance proposal) which would halt\\nthe functionality of the dCERT group until the proposal was concluded. This\\nspecial proposal type would likely need to also have a fairly large wager which\\ncould be slashed if the proposal creator was malicious. The reason a large\\nwager should be required is because as soon as the proposal is made, the\\ncapability of the dCERT group to halt message routes is put on temporarily\\nsuspended, meaning that a malicious actor who created such a proposal could\\nthen potentially exploit a bug during this period of time, with no dCERT group\\ncapable of shutting down the exploitable message routes.\\n### dCERT membership transactions\\nActive dCERT members\\n- change of the description of the dCERT group\\n- circuit break a message route\\n- vote to suspend a dCERT member.\\nHere circuit-breaking refers to the capability to disable a groups of messages,\\nThis could for instance mean: \"disable all staking-delegation messages\", or\\n\"disable all distribution messages\". This could be accomplished by verifying\\nthat the message route has not been \"circuit-broken\" at CheckTx time (in\\n`baseapp\/baseapp.go`).\\n\"unbreaking\" a circuit is anticipated only to occur during a hard fork upgrade\\nmeaning that no capability to unbreak a message route on a live chain is\\nrequired.\\nNote also, that if there was a problem with governance voting (for instance a\\ncapability to vote many times) then governance would be broken and should be\\nhalted with this mechanism, it would be then up to the validator set to\\ncoordinate and hard-fork upgrade to a patched version of the software where\\ngovernance is re-enabled (and fixed). If the dCERT group abuses this privilege\\nthey should all be severely slashed.\\n","tokens":292,"id":27}
{"File Name":"gatemint-sdk\/adr-014-proportional-slashing.md","Context":"## Context\\nIn Proof of Stake-based chains, centralization of consensus power amongst a small set of validators can cause harm to the network due to increased risk of censorship, liveness failure, fork attacks, etc.  However, while this centralization causes a negative externality to the network, it is not directly felt by the delegators contributing towards delegating towards already large validators.  We would like a way to pass on the negative externality cost of centralization onto those large validators and their delegators.\\n","Decision":"### Design\\nTo solve this problem, we will implement a procedure called Proportional Slashing.  The desire is that the larger a validator is, the more they should be slashed.  The first naive attempt is to make a validator's slash percent proportional to their share of consensus voting power.\\n```\\nslash_amount = k * power \/\/ power is the faulting validator's voting power and k is some on-chain constant\\n```\\nHowever, this will incentivize validators with large amounts of stake to split up their voting power amongst accounts, so that if they fault, they all get slashed at a lower percent.  The solution to this is to take into account not just a validator's own voting percentage, but also the voting percentage of all the other validators who get slashed in a specified time frame.\\n```\\nslash_amount = k * (power_1 + power_2 + ... + power_n) \/\/ where power_i is the voting power of the ith validator faulting in the specified time frame and k is some on-chain constant\\n```\\nNow, if someone splits a validator of 10% into two validators of 5% each which both fault, then they both fault in the same time frame, they both will still get slashed at the sum 10% amount.\\nHowever, an operator might still choose to split up their stake across multiple accounts with hopes that if any of them fault independently, they will not get slashed at the full amount.  In the case that the validators do fault together, they will get slashed the same amount as if they were one entity.  There is no con to splitting up.  However, if operators are going to split up their stake without actually decorrelating their setups, this also causes a negative externality to the network as it fills up validator slots that could have gone to others or increases the commit size.  In order to disincentivize this, we want it to be the case such that splitting up a validator into multiple validators and they fault together is punished more heavily that keeping it as a single validator that faults.\\nWe can achieve this by not only taking into account the sum of the percentages of the validators that faulted, but also the *number* of validators that faulted in the window.  One general form for an equation that fits this desired property looks like this:\\n```\\nslash_amount = k * ((power_1)^(1\/r) + (power_2)^(1\/r) + ... + (power_n)^(1\/r))^r \/\/ where k and r are both on-chain constants\\n```\\nSo now, for example, assuming k=1 and r=2, if one validator of 10% faults, it gets a 10% slash, while if two validators of 5% each fault together, they both get a 20% slash ((sqrt(0.05)+sqrt(0.05))^2).\\n#### Correlation across non-sybil validators\\nOne will note, that this model doesn't differentiate between multiple validators run by the same operators vs validators run by different operators.  This can be seen as an additional benefit in fact.  It incentivizes validators to differentiate their setups from other validators, to avoid having correlated faults with them or else they risk a higher slash.  So for example, operators should avoid using the same popular cloud hosting platforms or using the same Staking as a Service providers.  This will lead to a more resilient and decentralized network.\\n#### Parameterization\\nThe value of k and r can be different for different types of slashable faults.  For example, we may want to punish liveness faults 10% as severely as double signs.\\nThere can also be minimum and maximums put in place in order to bound the size of the slash percent.\\n#### Griefing\\nGriefing, the act of intentionally being slashed to make another's slash worse, could be a concern here.  However, using the protocol described here, the attacker could not substantially grief without getting slashed a substantial amount themselves.  The larger the validator is, the more heavily it can impact the slash, it needs to be non-trivial to have a significant impact on the slash percent.  Furthermore, the larger the grief, the griefer loses quadratically more.\\nIt may also be possible to, rather than the k and r factors being constants, perhaps using an inverse gini coefficient may mitigate some griefing attacks, but this an area for future research.\\n### Implementation\\nIn the slashing module, we will add two queues that will track all of the recent slash events.  For double sign faults, we will define \"recent slashes\" as ones that have occured within the last `unbonding period`.  For liveness faults, we will define \"recent slashes\" as ones that have occured withing the last `jail period`.\\n```\\ntype SlashEvent struct {\\nAddress                     sdk.ValAddress\\nSqrtValidatorVotingPercent  sdk.Dec\\nSlashedSoFar                sdk.Dec\\n}\\n```\\nThese slash events will be pruned from the queue once they are older than their respective \"recent slash period\".\\nWhenever a new slash occurs, a `SlashEvent` struct is created with the faulting validator's voting percent and a `SlashedSoFar` of 0.  Because recent slash events are pruned before the unbonding period and unjail period expires, it should not be possible for the same validator to have multiple SlashEvents in the same Queue at the same time.\\nWe then will iterate over all the SlashEvents in the queue, adding their `SqrtValidatorVotingPercent` and squaring the result to calculate the new percent to slash all the validators in the queue at, using the \"Square of Sum of Roots\" formula introduced above.\\nOnce we have the `NewSlashPercent`, we then iterate over all the `SlashEvent`s in the queue once again, and if `NewSlashPercent > SlashedSoFar` for that SlashEvent, we call the `staking.Slash(slashEvent.Address, slashEvent.Power, Math.Min(Math.Max(minSlashPercent, NewSlashPercent - SlashedSoFar), maxSlashPercent)` (we pass in the power of the validator before any slashes occured, so that we slash the right amount of tokens).  We then set `SlashEvent.SlashedSoFar` amount to `NewSlashPercent`.\\n","tokens":104,"id":28}
{"File Name":"gatemint-sdk\/adr-012-state-accessors.md","Context":"## Context\\nSDK modules currently use the `KVStore` interface and `Codec` to access their respective state. While\\nthis provides a large degree of freedom to module developers, it is hard to modularize and the UX is\\nmediocre.\\nFirst, each time a module tries to access the state, it has to marshal the value and set or get the\\nvalue and finally unmarshal. Usually this is done by declaring `Keeper.GetXXX` and `Keeper.SetXXX` functions,\\nwhich are repetitive and hard to maintain.\\nSecond, this makes it harder to align with the object capability theorem: the right to access the\\nstate is defined as a `StoreKey`, which gives full access on the entire Merkle tree, so a module cannot\\nsend the access right to a specific key-value pair (or a set of key-value pairs) to another module safely.\\nFinally, because the getter\/setter functions are defined as methods of a module's `Keeper`, the reviewers\\nhave to consider the whole Merkle tree space when they reviewing a function accessing any part of the state.\\nThere is no static way to know which part of the state that the function is accessing (and which is not).\\n","Decision":"We will define a type named `Value`:\\n```go\\ntype Value struct {\\nm   Mapping\\nkey []byte\\n}\\n```\\nThe `Value` works as a reference for a key-value pair in the state, where `Value.m` defines the key-value\\nspace it will access and `Value.key` defines the exact key for the reference.\\nWe will define a type named `Mapping`:\\n```go\\ntype Mapping struct {\\nstoreKey sdk.StoreKey\\ncdc      *codec.LegacyAmino\\nprefix   []byte\\n}\\n```\\nThe `Mapping` works as a reference for a key-value space in the state, where `Mapping.storeKey` defines\\nthe IAVL (sub-)tree and `Mapping.prefix` defines the optional subspace prefix.\\nWe will define the following core methods for the `Value` type:\\n```go\\n\/\/ Get and unmarshal stored data, noop if not exists, panic if cannot unmarshal\\nfunc (Value) Get(ctx Context, ptr interface{}) {}\\n\/\/ Get and unmarshal stored data, return error if not exists or cannot unmarshal\\nfunc (Value) GetSafe(ctx Context, ptr interface{}) {}\\n\/\/ Get stored data as raw byte slice\\nfunc (Value) GetRaw(ctx Context) []byte {}\\n\/\/ Marshal and set a raw value\\nfunc (Value) Set(ctx Context, o interface{}) {}\\n\/\/ Check if a raw value exists\\nfunc (Value) Exists(ctx Context) bool {}\\n\/\/ Delete a raw value value\\nfunc (Value) Delete(ctx Context) {}\\n```\\nWe will define the following core methods for the `Mapping` type:\\n```go\\n\/\/ Constructs key-value pair reference corresponding to the key argument in the Mapping space\\nfunc (Mapping) Value(key []byte) Value {}\\n\/\/ Get and unmarshal stored data, noop if not exists, panic if cannot unmarshal\\nfunc (Mapping) Get(ctx Context, key []byte, ptr interface{}) {}\\n\/\/ Get and unmarshal stored data, return error if not exists or cannot unmarshal\\nfunc (Mapping) GetSafe(ctx Context, key []byte, ptr interface{})\\n\/\/ Get stored data as raw byte slice\\nfunc (Mapping) GetRaw(ctx Context, key []byte) []byte {}\\n\/\/ Marshal and set a raw value\\nfunc (Mapping) Set(ctx Context, key []byte, o interface{}) {}\\n\/\/ Check if a raw value exists\\nfunc (Mapping) Has(ctx Context, key []byte) bool {}\\n\/\/ Delete a raw value value\\nfunc (Mapping) Delete(ctx Context, key []byte) {}\\n```\\nEach method of the `Mapping` type that is passed the arugments `ctx`, `key`, and `args...` will proxy\\nthe call to `Mapping.Value(key)` with arguments `ctx` and `args...`.\\nIn addition, we will define and provide a common set of types derived from the `Value` type:\\n```go\\ntype Boolean struct { Value }\\ntype Enum struct { Value }\\ntype Integer struct { Value; enc IntEncoding }\\ntype String struct { Value }\\n\/\/ ...\\n```\\nWhere the encoding schemes can be different, `o` arguments in core methods are typed, and `ptr` arguments\\nin core methods are replaced by explicit return types.\\nFinally, we will define a family of types derived from the `Mapping` type:\\n```go\\ntype Indexer struct {\\nm   Mapping\\nenc IntEncoding\\n}\\n```\\nWhere the `key` argument in core method is typed.\\nSome of the properties of the accessor types are:\\n- State access happens only when a function which takes a `Context` as an argument is invoked\\n- Accessor type structs give rights to access the state only that the struct is referring, no other\\n- Marshalling\/Unmarshalling happens implicitly within the core methods\\n","tokens":248,"id":30}
{"File Name":"gatemint-sdk\/adr-023-protobuf-naming.md","Context":"## Context\\nProtocol Buffers provide a basic [style guide](https:\/\/developers.google.com\/protocol-buffers\/docs\/style)\\nand [Buf](https:\/\/buf.build\/docs\/style-guide) builds upon that. To the\\nextent possible, we want to follow industry accepted guidelines and wisdom for\\nthe effective usage of protobuf, deviating from those only when there is clear\\nrationale for our use case.\\n### Adoption of `Any`\\nThe adoption of `google.protobuf.Any` as the recommended approach for encoding\\ninterface types (as opposed to `oneof`) makes package naming a central part\\nof the encoding as fully-qualified message names now appear in encoded\\nmessages.\\n### Current Directory Organization\\nThus far we have mostly followed [Buf's](https:\/\/buf.build) [DEFAULT](https:\/\/buf.build\/docs\/lint-checkers#default)\\nrecommendations, with the minor deviation of disabling [`PACKAGE_DIRECTORY_MATCH`](https:\/\/buf.build\/docs\/lint-checkers#file_layout)\\nwhich although being convenient for developing code comes with the warning\\nfrom Buf that:\\n> you will have a very bad time with many Protobuf plugins across various languages if you do not do this\\n### Adoption of gRPC Queries\\nIn [ADR 021](adr-021-protobuf-query-encoding.md), gRPC was adopted for Protobuf\\nnative queries. The full gRPC service path thus becomes a key part of ABCI query\\npath. In the future, gRPC queries may be allowed from within persistent scripts\\nby technologies such as CosmWasm and these query routes would be stored within\\nscript binaries.\\n","Decision":"The goal of this ADR is to provide thoughtful naming conventions that:\\n* encourage a good user experience for when users interact directly with\\n.proto files and fully-qualified protobuf names\\n* balance conciseness against the possibility of either over-optimizing (making\\nnames too short and cryptic) or under-optimizing (just accepting bloated names\\nwith lots of redundant information)\\nThese guidelines are meant to act as a style guide for both the SDK and\\nthird-party modules.\\nAs a starting point, we should adopt all of the [DEFAULT](https:\/\/buf.build\/docs\/lint-checkers#default)\\ncheckers in [Buf's](https:\/\/buf.build) including [`PACKAGE_DIRECTORY_MATCH`](https:\/\/buf.build\/docs\/lint-checkers#file_layout),\\nexcept:\\n* [PACKAGE_VERSION_SUFFIX](https:\/\/buf.build\/docs\/lint-checkers#package_version_suffix)\\n* [SERVICE_SUFFIX](https:\/\/buf.build\/docs\/lint-checkers#service_suffix)\\nFurther guidelines to be described below.\\n### Principles\\n#### Concise and Descriptive Names\\nNames should be descriptive enough to convey their meaning and distinguish\\nthem from other names.\\nGiven that we are using fully-qualifed names within\\n`google.protobuf.Any` as well as within gRPC query routes, we should aim to\\nkeep names concise, without going overboard. The general rule of thumb should\\nbe if a shorter name would convey more or else the same thing, pick the shorter\\nname.\\nFor instance, `cosmos.bank.MsgSend` (19 bytes) conveys roughly the same information\\nas `cosmos_sdk.x.bank.v1.MsgSend` (28 bytes) but is more concise.\\nSuch conciseness makes names both more pleasant to work with and take up less\\nspace within transactions and on the wire.\\nWe should also resist the temptation to over-optimize, by making names\\ncryptically short with abbreviations. For instance, we shouldn't try to\\nreduce `cosmos.bank.MsgSend` to `csm.bk.MSnd` just to save a few bytes.\\nThe goal is to make names **_concise but not cryptic_**.\\n#### Names are for Clients First\\nPackage and type names should be chosen for the benefit of users, not\\nnecessarily because of legacy concerns related to the go code-base.\\n#### Plan for Longevity\\nIn the interests of long-term support, we should plan on the names we do\\nchoose to be in usage for a long time, so now is the opportunity to make\\nthe best choices for the future.\\n### Versioning\\n#### Don't Allow Breaking Changes in Stable Packages\\nAlways use a breaking change detector such as [Buf](https:\/\/buf.build) to prevent\\nbreaking changes in stable (non-alpha or beta) packages. Breaking changes can\\nbreak smart contracts\/persistent scripts and generally provide a bad UX for\\nclients. With protobuf, there should usually be ways to extend existing\\nfunctionality instead of just breaking it.\\n#### Omit v1 suffix\\nInstead of using [Buf's recommended version suffix](https:\/\/buf.build\/docs\/lint-checkers#package_version_suffix),\\nwe can omit `v1` for packages that don't actually have a second version. This\\nallows for more concise names for common use cases like `cosmos.bank.Send`.\\nPackages that do have a second or third version can indicate that with `.v2`\\nor `.v3`.\\n#### Use `alpha` or `beta` to Denote Non-stable Packages\\n[Buf's recommended version suffix](https:\/\/buf.build\/docs\/lint-checkers#package_version_suffix)\\n(ex. `v1alpha1`) _should_ be used for non-stable packages. These packages should\\nlikely be excluded from breaking change detection and _should_ generally\\nbe blocked from usage by smart contracts\/persistent scripts to prevent them\\nfrom breaking. The SDK _should_ mark any packages as alpha or beta where the\\nAPI is likely to change significantly in the near future.\\n### Package Naming\\n#### Adopt a short, unique top-level package name\\nTop-level packages should adopt a short name that is known to not collide with\\nother names in common usage within the Cosmos ecosystem. In the near future, a\\nregistry should be created to reserve and index top-level package names used\\nwithin the Cosmos ecosystem. Because the Cosmos SDK is intended to provide\\nthe top-level types for the Cosmos project, the top-level package name `cosmos`\\nis recommended for usage within the Cosmos SDK instead of the longer `cosmos_sdk`.\\n[ICS](https:\/\/github.com\/cosmos\/ics) specifications could consider a\\nshort top-level package like `ics23` based upon the standard number.\\n#### Limit sub-package depth\\nSub-package depth should be increased with caution. Generally a single\\nsub-package is needed for a module or a library. Even though `x` or `modules`\\nis used in source code to denote modules, this is often unnecessary for .proto\\nfiles as modules are the primary thing sub-packages are used for. Only items which\\nare known to be used infrequently should have deep sub-package depths.\\nFor the Cosmos SDK, it is recommended that that we simply write `cosmos.bank`,\\n`cosmos.gov`, etc. rather than `cosmos.x.bank`. In practice, most non-module\\ntypes can go straight in the `cosmos` package or we can introduce a\\n`cosmos.base` package if needed. Note that this naming _will not_ change\\ngo package names, i.e. the `cosmos.bank` protobuf package will still live in\\n`x\/bank`.\\n### Message Naming\\nMessage type names should be as concise possible without losing clarity. `sdk.Msg`\\ntypes which are used in transactions will retain the `Msg` prefix as that provides\\nhelpful context.\\n### Service and RPC Naming\\n[ADR 021](adr-021-protobuf-query-encoding.md) specifies that modules should\\nimplement a gRPC query service. We should consider the principle of conciseness\\nfor query service and RPC names as these may be called from persistent script\\nmodules such as CosmWasm. Also, users may use these query paths from tools like\\n[gRPCurl](https:\/\/github.com\/fullstorydev\/grpcurl). As an example, we can shorten\\n`\/cosmos_sdk.x.bank.v1.QueryService\/QueryBalance` to\\n`\/cosmos.bank.Query\/Balance` without losing much useful information.\\nRPC request and response types _should_ follow the `ServiceNameMethodNameRequest`\/\\n`ServiceNameMethodNameResponse` naming convention. i.e. for an RPC method named `Balance`\\non the `Query` service, the request and response types would be `QueryBalanceRequest`\\nand `QueryBalanceResponse`. This will be more self-explanatory than `BalanceRequest`\\nand `BalanceResponse`.\\n#### Use just `Query` for the query service\\nInstead of [Buf's default service suffix recommendation](https:\/\/github.com\/cosmos\/cosmos-sdk\/pull\/6033),\\nwe should simply use the shorter `Query` for query services.\\nFor other types of gRPC services, we should consider sticking with Buf's\\ndefault recommendation.\\n#### Omit `Get` and `Query` from query service RPC names\\n`Get` and `Query` should be omitted from `Query` service names because they are\\nredundant in the fully-qualified name. For instance, `\/cosmos.bank.Query\/QueryBalance`\\njust says `Query` twice without any new information.\\n","tokens":330,"id":31}
{"File Name":"gatemint-sdk\/adr-027-deterministic-protobuf-serialization.md","Context":"## Context\\n[Protobuf](https:\/\/developers.google.com\/protocol-buffers\/docs\/proto3)\\nseralization is not unique (i.e. there exist a practically unlimited number of\\nvalid binary representations for a protobuf document)<sup>1<\/sup>. For signature\\nverification in Cosmos SDK, signer and verifier need to agree on the same\\nserialization of a SignDoc as defined in\\n[ADR-020](.\/adr-020-protobuf-transaction-encoding.md) without transmitting the\\nserialization. This document describes a deterministic serialization scheme for\\na subset of protobuf documents, that covers this use case but can be reused in\\nother cases as well.\\n","Decision":"The following encoding scheme is proposed to be used by other ADRs.\\n### Scope\\nThis ADR defines a protobuf3 serializer. The output is a valid protobuf\\nserialization, such that every protobuf parser can parse it.\\nNo maps are supported in version 1 due to the complexity of defining a\\nderterministic serialization. This might change in future. Implementations must\\nreject documents containing maps as invalid input.\\n### Serialization rules\\nThe serialization is based on the\\n[protobuf 3 encoding](https:\/\/developers.google.com\/protocol-buffers\/docs\/encoding)\\nwith the following additions:\\n1. Fields must be serialized only once in ascending order\\n2. Extra fields or any extra data must not be added\\n3. [Default values](https:\/\/developers.google.com\/protocol-buffers\/docs\/proto3#default)\\nmust be omitted\\n4. `repeated` fields of scalar numeric types must use\\n[packed encoding](https:\/\/developers.google.com\/protocol-buffers\/docs\/encoding#packed)\\nby default.\\n5. Variant encoding of integers must not be longer than needed.\\nWhile rule number 1. and 2. should be pretty straight forward and describe the\\ndefault behaviour of all protobuf encoders the author is aware of, the 3rd rule\\nis more interesting. After a protobuf 3 deserialization you cannot differentiate\\nbetween unset fields and fields set to the default value<sup>2<\/sup>. At\\nserialization level however, it is possible to set the fields with an empty\\nvalue or omitting them entirely. This is a significant difference to e.g. JSON\\nwhere a property can be empty (`\"\"`, `0`), `null` or undefined, leading to 3\\ndifferent documents.\\nOmitting fields set to default values is valid because the parser must assign\\nthe default value to fields missing in the serialization<sup>3<\/sup>. For scalar\\ntypes, omitting defaults is required by the spec<sup>4<\/sup>. For `repeated`\\nfields, not serializing them is the only way to express empty lists. Enums must\\nhave a first element of numeric value 0, which is the default<sup>5<\/sup>. And\\nmessage fields default to unset<sup>6<\/sup>.\\nOmitting defaults allows for some amount of forward compatibility: users of\\nnewer versions of a protobuf schema produce the same serialization as users of\\nolder versions as long as newly added fields are not used (i.e. set to their\\ndefault value).\\n### Implementation\\nThere are three main implementation strategies, ordered from the least to the\\nmost custom development:\\n- **Use a protobuf serializer that follows the above rules by default.** E.g.\\n[gogoproto](https:\/\/pkg.go.dev\/github.com\/gogo\/protobuf\/gogoproto) is known to\\nbe compliant by in most cases, but not when certain annotations such as\\n`nullable = false` are used. It might also be an option to configure an\\nexisting serializer accordingly.\\n- **Normalize default values before encoding them.** If your serializer follows\\nrule 1. and 2. and allows you to explicitly unset fields for serialization,\\nyou can normalize default values to unset. This can be done when working with\\n[protobuf.js](https:\/\/www.npmjs.com\/package\/protobufjs):\\n```js\\nconst bytes = SignDoc.encode({\\nbodyBytes: body.length > 0 ? body : null, \/\/ normalize empty bytes to unset\\nauthInfoBytes: authInfo.length > 0 ? authInfo : null, \/\/ normalize empty bytes to unset\\nchainId: chainId || null, \/\/ normalize \"\" to unset\\naccountNumber: accountNumber || null, \/\/ normalize 0 to unset\\naccountSequence: accountSequence || null, \/\/ normalize 0 to unset\\n}).finish();\\n```\\n- **Use a hand-written serializer for the types you need.** If none of the above\\nways works for you, you can write a serializer yourself. For SignDoc this\\nwould look something like this in Go, building on existing protobuf utilities:\\n```go\\nif !signDoc.body_bytes.empty() {\\nbuf.WriteUVarInt64(0xA) \/\/ wire type and field number for body_bytes\\nbuf.WriteUVarInt64(signDoc.body_bytes.length())\\nbuf.WriteBytes(signDoc.body_bytes)\\n}\\nif !signDoc.auth_info.empty() {\\nbuf.WriteUVarInt64(0x12) \/\/ wire type and field number for auth_info\\nbuf.WriteUVarInt64(signDoc.auth_info.length())\\nbuf.WriteBytes(signDoc.auth_info)\\n}\\nif !signDoc.chain_id.empty() {\\nbuf.WriteUVarInt64(0x1a) \/\/ wire type and field number for chain_id\\nbuf.WriteUVarInt64(signDoc.chain_id.length())\\nbuf.WriteBytes(signDoc.chain_id)\\n}\\nif signDoc.account_number != 0 {\\nbuf.WriteUVarInt64(0x20) \/\/ wire type and field number for account_number\\nbuf.WriteUVarInt(signDoc.account_number)\\n}\\nif signDoc.account_sequence != 0 {\\nbuf.WriteUVarInt64(0x28) \/\/ wire type and field number for account_sequence\\nbuf.WriteUVarInt(signDoc.account_sequence)\\n}\\n```\\n### Test vectors\\nGiven the protobuf definition `Article.proto`\\n```protobuf\\npackage blog;\\nsyntax = \"proto3\";\\nenum Type {\\nUNSPECIFIED = 0;\\nIMAGES = 1;\\nNEWS = 2;\\n};\\nenum Review {\\nUNSPECIFIED = 0;\\nACCEPTED = 1;\\nREJECTED = 2;\\n};\\nmessage Article {\\nstring title = 1;\\nstring description = 2;\\nuint64 created = 3;\\nuint64 updated = 4;\\nbool public = 5;\\nbool promoted = 6;\\nType type = 7;\\nReview review = 8;\\nrepeated string comments = 9;\\nrepeated string backlinks = 10;\\n};\\n```\\nserializing the values\\n```yaml\\ntitle: \"The world needs change \ud83c\udf33\"\\ndescription: \"\"\\ncreated: 1596806111080\\nupdated: 0\\npublic: true\\npromoted: false\\ntype: Type.NEWS\\nreview: Review.UNSPECIFIED\\ncomments: [\"Nice one\", \"Thank you\"]\\nbacklinks: []\\n```\\nmust result in the serialization\\n```\\n0a1b54686520776f726c64206e65656473206368616e676520f09f8cb318e8bebec8bc2e280138024a084e696365206f6e654a095468616e6b20796f75\\n```\\nWhen inspecting the serialized document, you see that every second field is\\nomitted:\\n```\\n$ echo 0a1b54686520776f726c64206e65656473206368616e676520f09f8cb318e8bebec8bc2e280138024a084e696365206f6e654a095468616e6b20796f75 | xxd -r -p | protoc --decode_raw\\n1: \"The world needs change \\360\\237\\214\\263\"\\n3: 1596806111080\\n5: 1\\n7: 2\\n9: \"Nice one\"\\n9: \"Thank you\"\\n```\\n","tokens":135,"id":32}
{"File Name":"gatemint-sdk\/adr-004-split-denomination-keys.md","Context":"## Context\\nWith permissionless IBC, anyone will be able to send arbitrary denominations to any other account. Currently, all non-zero balances are stored along with the account in an `sdk.Coins` struct, which creates a potential denial-of-service concern, as too many denominations will become expensive to load & store each time the account is modified. See issues [5467](https:\/\/github.com\/cosmos\/cosmos-sdk\/issues\/5467) and [4982](https:\/\/github.com\/cosmos\/cosmos-sdk\/issues\/4982) for additional context.\\nSimply rejecting incoming deposits after a denomination count limit doesn't work, since it opens up a griefing vector: someone could send a user lots of nonsensical coins over IBC, and then prevent the user from receiving real denominations (such as staking rewards).\\n","Decision":"Balances shall be stored per-account & per-denomination under a denomination- and account-unique key, thus enabling O(1) read & write access to the balance of a particular account in a particular denomination.\\n### Account interface (x\/auth)\\n`GetCoins()` and `SetCoins()` will be removed from the account interface, since coin balances will\\nnow be stored in & managed by the bank module.\\nThe vesting account interface will replace `SpendableCoins` in favor of `LockedCoins` which does\\nnot require the account balance anymore. In addition, `TrackDelegation()`  will now accept the\\naccount balance of all tokens denominated in the vesting balance instead of loading the entire\\naccount balance.\\nVesting accounts will continue to store original vesting, delegated free, and delegated\\nvesting coins (which is safe since these cannot contain arbitrary denominations).\\n### Bank keeper (x\/bank)\\nThe following APIs will be added to the `x\/bank` keeper:\\n- `GetAllBalances(ctx Context, addr AccAddress) Coins`\\n- `GetBalance(ctx Context, addr AccAddress, denom string) Coin`\\n- `SetBalance(ctx Context, addr AccAddress, coin Coin)`\\n- `LockedCoins(ctx Context, addr AccAddress) Coins`\\n- `SpendableCoins(ctx Context, addr AccAddress) Coins`\\nAdditional APIs may be added to facilitate iteration and auxiliary functionality not essential to\\ncore functionality or persistence.\\nBalances will be stored first by the address, then by the denomination (the reverse is also possible,\\nbut retrieval of all balances for a single account is presumed to be more frequent):\\n```golang\\nvar BalancesPrefix = []byte(\"balances\")\\nfunc (k Keeper) SetBalance(ctx Context, addr AccAddress, balance Coin) error {\\nif !balance.IsValid() {\\nreturn err\\n}\\nstore := ctx.KVStore(k.storeKey)\\nbalancesStore := prefix.NewStore(store, BalancesPrefix)\\naccountStore := prefix.NewStore(balancesStore, addr.Bytes())\\nbz := Marshal(balance)\\naccountStore.Set([]byte(balance.Denom), bz)\\nreturn nil\\n}\\n```\\nThis will result in the balances being indexed by the byte representation of\\n`balances\/{address}\/{denom}`.\\n`DelegateCoins()` and `UndelegateCoins()` will be altered to only load each individual\\naccount balance by denomination found in the (un)delegation amount. As a result,\\nany mutations to the account balance by will made by denomination.\\n`SubtractCoins()` and `AddCoins()` will be altered to read & write the balances\\ndirectly instead of calling `GetCoins()` \/ `SetCoins()` (which no longer exist).\\n`trackDelegation()` and `trackUndelegation()` will be altered to no longer update\\naccount balances.\\nExternal APIs will need to scan all balances under an account to retain backwards-compatibility. It\\nis advised that these APIs use `GetBalance` and `SetBalance` instead of `GetAllBalances` when\\npossible as to not load the entire account balance.\\n### Supply module\\nThe supply module, in order to implement the total supply invariant, will now need\\nto scan all accounts & call `GetAllBalances` using the `x\/bank` Keeper, then sum\\nthe balances and check that they match the expected total supply.\\n","tokens":175,"id":33}
{"File Name":"gatemint-sdk\/adr-022-custom-panic-handling.md","Context":"## Context\\nThe current implementation of BaseApp does not allow developers to write custom error handlers during panic recovery\\n[runTx()](https:\/\/github.com\/cosmos\/cosmos-sdk\/blob\/bad4ca75f58b182f600396ca350ad844c18fc80b\/baseapp\/baseapp.go#L539)\\nmethod. We think that this method can be more flexible and can give SDK users more options for customizations without\\nthe need to rewrite whole BaseApp. Also there's one special case for `sdk.ErrorOutOfGas` error handling, that case\\nmight be handled in a \"standard\" way (middleware) alongside the others.\\nWe propose middleware-solution, which could help developers implement the following cases:\\n* add external logging (let's say sending reports to external services like [Sentry](https:\/\/sentry.io));\\n* call panic for specific error cases;\\nIt will also make `OutOfGas` case and `default` case one of the middlewares.\\n`Default` case wraps recovery object to an error and logs it ([example middleware implementation](#Recovery-middleware)).\\nOur project has a sidecar service running alongside the blockchain node (smart contracts virtual machine). It is\\nessential that node <-> sidecar connectivity stays stable for TXs processing. So when the communication breaks we need\\nto crash the node and reboot it once the problem is solved. That behaviour makes node's state machine execution\\ndeterministic. As all keeper panics are caught by runTx's `defer()` handler, we have to adjust the BaseApp code\\nin order to customize it.\\n","Decision":"### Design\\n#### Overview\\nInstead of hardcoding custom error handling into BaseApp we suggest using set of middlewares which can be customized\\nexternally and will allow developers use as many custom error handlers as they want. Implementation with tests\\ncan be found [here](https:\/\/github.com\/cosmos\/cosmos-sdk\/pull\/6053).\\n#### Implementation details\\n##### Recovery handler\\nNew `RecoveryHandler` type added. `recoveryObj` input argument is an object returned by the standard Go function\\n`recover()` from the `builtin` package.\\n```go\\ntype RecoveryHandler func(recoveryObj interface{}) error\\n```\\nHandler should type assert (or other methods) an object to define if object should be handled.\\n`nil` should be returned if input object can't be handled by that `RecoveryHandler` (not a handler's target type).\\nNot `nil` error should be returned if input object was handled and middleware chain execution should be stopped.\\nAn example:\\n```go\\nfunc exampleErrHandler(recoveryObj interface{}) error {\\nerr, ok := recoveryObj.(error)\\nif !ok { return nil }\\nif someSpecificError.Is(err) {\\npanic(customPanicMsg)\\n} else {\\nreturn nil\\n}\\n}\\n```\\nThis example breaks the application execution, but it also might enrich the error's context like the `OutOfGas` handler.\\n##### Recovery middleware\\nWe also add a middleware type (decorator). That function type wraps `RecoveryHandler` and returns the next middleware in\\nexecution chain and handler's `error`. Type is used to separate actual `recovery()` object handling from middleware\\nchain processing.\\n```go\\ntype recoveryMiddleware func(recoveryObj interface{}) (recoveryMiddleware, error)\\nfunc newRecoveryMiddleware(handler RecoveryHandler, next recoveryMiddleware) recoveryMiddleware {\\nreturn func(recoveryObj interface{}) (recoveryMiddleware, error) {\\nif err := handler(recoveryObj); err != nil {\\nreturn nil, err\\n}\\nreturn next, nil\\n}\\n}\\n```\\nFunction receives a `recoveryObj` object and returns:\\n* (next `recoveryMiddleware`, `nil`) if object wasn't handled (not a target type) by `RecoveryHandler`;\\n* (`nil`, not nil `error`) if input object was handled and other middlewares in the chain should not be executed;\\n* (`nil`, `nil`) in case of invalid behavior. Panic recovery might not have been properly handled;\\nthis can be avoided by always using a `default` as a rightmost middleware in the chain (always returns an `error`');\\n`OutOfGas` middleware example:\\n```go\\nfunc newOutOfGasRecoveryMiddleware(gasWanted uint64, ctx sdk.Context, next recoveryMiddleware) recoveryMiddleware {\\nhandler := func(recoveryObj interface{}) error {\\nerr, ok := recoveryObj.(sdk.ErrorOutOfGas)\\nif !ok { return nil }\\nreturn sdkerrors.Wrap(\\nsdkerrors.ErrOutOfGas, fmt.Sprintf(\\n\"out of gas in location: %v; gasWanted: %d, gasUsed: %d\", err.Descriptor, gasWanted, ctx.GasMeter().GasConsumed(),\\n),\\n)\\n}\\nreturn newRecoveryMiddleware(handler, next)\\n}\\n```\\n`Default` middleware example:\\n```go\\nfunc newDefaultRecoveryMiddleware() recoveryMiddleware {\\nhandler := func(recoveryObj interface{}) error {\\nreturn sdkerrors.Wrap(\\nsdkerrors.ErrPanic, fmt.Sprintf(\"recovered: %v\\nstack:\\n%v\", recoveryObj, string(debug.Stack())),\\n)\\n}\\nreturn newRecoveryMiddleware(handler, nil)\\n}\\n```\\n##### Recovery processing\\nBasic chain of middlewares processing would look like:\\n```go\\nfunc processRecovery(recoveryObj interface{}, middleware recoveryMiddleware) error {\\nif middleware == nil { return nil }\\nnext, err := middleware(recoveryObj)\\nif err != nil { return err }\\nif next == nil { return nil }\\nreturn processRecovery(recoveryObj, next)\\n}\\n```\\nThat way we can create a middleware chain which is executed from left to right, the rightmost middleware is a\\n`default` handler which must return an `error`.\\n##### BaseApp changes\\nThe `default` middleware chain must exist in a `BaseApp` object. `Baseapp` modifications:\\n```go\\ntype BaseApp struct {\\n\/\/ ...\\nrunTxRecoveryMiddleware recoveryMiddleware\\n}\\nfunc NewBaseApp(...) {\\n\/\/ ...\\napp.runTxRecoveryMiddleware = newDefaultRecoveryMiddleware()\\n}\\nfunc (app *BaseApp) runTx(...) {\\n\/\/ ...\\ndefer func() {\\nif r := recover(); r != nil {\\nrecoveryMW := newOutOfGasRecoveryMiddleware(gasWanted, ctx, app.runTxRecoveryMiddleware)\\nerr, result = processRecovery(r, recoveryMW), nil\\n}\\ngInfo = sdk.GasInfo{GasWanted: gasWanted, GasUsed: ctx.GasMeter().GasConsumed()}\\n}()\\n\/\/ ...\\n}\\n```\\nDevelopers can add their custom `RecoveryHandler`s by providing `AddRunTxRecoveryHandler` as a BaseApp option parameter to the `NewBaseapp` constructor:\\n```go\\nfunc (app *BaseApp) AddRunTxRecoveryHandler(handlers ...RecoveryHandler) {\\nfor _, h := range handlers {\\napp.runTxRecoveryMiddleware = newRecoveryMiddleware(h, app.runTxRecoveryMiddleware)\\n}\\n}\\n```\\nThis method would prepend handlers to an existing chain.\\n","tokens":337,"id":34}
{"File Name":"gatemint-sdk\/adr-016-validator-consensus-key-rotation.md","Context":"## Context\\nValidator consensus key rotation feature has been discussed and requested for a long time, for the sake of safer validator key management policy (e.g. https:\/\/github.com\/tendermint\/tendermint\/issues\/1136). So, we suggest one of the simplest form of validator consensus key rotation implementation mostly onto Cosmos-SDK.\\nWe don't need to make any update on consensus logic in Tendermint because Tendermint does not have any mapping information of consensus key and validator operator key, meaning that from Tendermint point of view, a consensus key rotation of a validator is simply a replacement of a consensus key to another.\\nAlso, it should be noted that this ADR includes only the simplest form of consensus key rotation without considering multiple consensus keys concept. Such multiple consensus keys concept shall remain a long term goal of Tendermint and Cosmos-SDK.\\n","Decision":"### Pseudo procedure for consensus key rotation\\n- create new random consensus key.\\n- create and broadcast a transaction with a `MsgRotateConsPubKey` that states the new consensus key is now coupled with the validator operator with signature from the validator's operator key.\\n- old consensus key becomes unable to participate on consensus immediately after the update of key mapping state on-chain.\\n- start validating with new consensus key.\\n- validators using HSM and KMS should update the consensus key in HSM to use the new rotated key after the height `h` when `MsgRotateConsPubKey` committed to the blockchain.\\n### Considerations\\n- consensus key mapping information management strategy\\n- store history of each key mapping changes in the kvstore.\\n- the state machine can search corresponding consensus key paired with given validator operator for any arbitrary height in a recent unbonding period.\\n- the state machine does not need any historical mapping information which is past more than unbonding period.\\n- key rotation costs related to LCD and IBC\\n- LCD and IBC will have traffic\/computation burden when there exists frequent power changes\\n- In current Tendermint design, consensus key rotations are seen as power changes from LCD or IBC perspective\\n- Therefore, to minimize unnecessary frequent key rotation behavior, we limited maximum number of rotation in recent unbonding period and also applied exponentially increasing rotation fee\\n- limits\\n- a validator cannot rotate its consensus key more than `MaxConsPubKeyRotations` time for any unbonding period, to prevent spam.\\n- parameters can be decided by governance and stored in genesis file.\\n- key rotation fee\\n- a validator should pay `KeyRotationFee` to rotate the consensus key which is calculated as below\\n- `KeyRotationFee` = (max(`VotingPowerPercentage` * 100, 1) * `InitialKeyRotationFee`) * 2^(number of rotations in `ConsPubKeyRotationHistory` in recent unbonding period)\\n- evidence module\\n- evidence module can search corresponding consensus key for any height from slashing keeper so that it can decide which consensus key is supposed to be used for given height.\\n- abci.ValidatorUpdate\\n- tendermint already has ability to change a consensus key by ABCI communication(`ValidatorUpdate`).\\n- validator consensus key update can be done via creating new + delete old by change the power to zero.\\n- therefore, we expect we even do not need to change tendermint codebase at all to implement this feature.\\n- new genesis parameters in `staking` module\\n- `MaxConsPubKeyRotations` : maximum number of rotation can be executed by a validator in recent unbonding period. default value 10 is suggested(11th key rotation will be rejected)\\n- `InitialKeyRotationFee` : the initial key rotation fee when no key rotation has happened in recent unbonding period. default value 1atom is suggested(1atom fee for the first key rotation in recent unbonding period)\\n### Workflow\\n1. The validator generates a new consensus keypair.\\n2. The validator generates and signs a `MsgRotateConsPubKey` tx with their operator key and new ConsPubKey\\n```go\\ntype MsgRotateConsPubKey struct {\\nValidatorAddress  sdk.ValAddress\\nNewPubKey         crypto.PubKey\\n}\\n```\\n3. `handleMsgRotateConsPubKey` gets `MsgRotateConsPubKey`, calls `RotateConsPubKey` with emits event\\n4. `RotateConsPubKey`\\n- checks if `NewPubKey` is not duplicated on `ValidatorsByConsAddr`\\n- checks if the validator is does not exceed parameter `MaxConsPubKeyRotations` by iterating `ConsPubKeyRotationHistory`\\n- checks if the signing account has enough balance to pay `KeyRotationFee`\\n- pays `KeyRotationFee` to community fund\\n- overwrites `NewPubKey` in `validator.ConsPubKey`\\n- deletes old `ValidatorByConsAddr`\\n- `SetValidatorByConsAddr` for `NewPubKey`\\n- Add `ConsPubKeyRotationHistory` for tracking rotation\\n```go\\ntype ConsPubKeyRotationHistory struct {\\nOperatorAddress         sdk.ValAddress\\nOldConsPubKey           crypto.PubKey\\nNewConsPubKey           crypto.PubKey\\nRotatedHeight           int64\\n}\\n```\\n5. `ApplyAndReturnValidatorSetUpdates` checks if there is `ConsPubKeyRotationHistory` with `ConsPubKeyRotationHistory.RotatedHeight == ctx.BlockHeight()` and if so, generates 2 `ValidatorUpdate` , one for a remove validator and one for create new validator\\n```go\\nabci.ValidatorUpdate{\\nPubKey: tmtypes.TM2PB.PubKey(OldConsPubKey),\\nPower:  0,\\n}\\nabci.ValidatorUpdate{\\nPubKey: tmtypes.TM2PB.PubKey(NewConsPubKey),\\nPower:  v.ConsensusPower(),\\n}\\n```\\n6. at `previousVotes` Iteration logic of `AllocateTokens`,  `previousVote` using `OldConsPubKey` match up with `ConsPubKeyRotationHistory`, and replace validator for token allocation\\n7. Migrate `ValidatorSigningInfo` and `ValidatorMissedBlockBitArray` from `OldConsPubKey` to `NewConsPubKey`\\n- Note : All above features shall be implemented in `staking` module.\\n","tokens":173,"id":35}
{"File Name":"digital-paper-edit-storybook\/adr-28-08.md","Context":"## Context and Problem Statement\\nWe needed to clarify the relationship between the DPE Client repository, where the components we are using to populate the Storybook repo have already been written, and the Storybook repository.\\n- From which repo would components be published?\\n- Which repos would consume components from NPM?\\n- Should the Storybook live inside the Client repo?\\n## Decision Drivers\\nN\/A\\n","Decision":"N\/A\\nChosen option: Option 2, because this allows us to refactor components' code and preview changes within the Storybook locally \u2014 before publishing the component to the hosted Storybook and NPM.\\nThis means that our workflow for populating the Storybook and refactoring the Client code is as follows:\\n1. Duplicate component code to Storybook repo\\n2. Publish completed components to NPM\\n3. Remove the original component code from the Client and import via NPM\\n### Positive Consequences\\n### Negative consequences\\nCaveat: If more than one person is working on the Storybook and DPE Client, they'll need to sync up to ensure that details in code refactors are not lost due to overlapping work.\\nIf possible, also avoid having people working simultaneously on a component that consumes \/ is consumed by another component (i.e., one person working on a card component and another person working on a list component that consumes card components).\\n","tokens":83,"id":36}
{"File Name":"Marain.Tenancy\/0001-synthetic-root-tenant.md","Context":"## Context\\nCorvus Tenancy, which provides the underpinnings of the Marain Tenancy service, has always had a concept of a root tenant (dating from its earlier, pre-open-source incarnation). This ADR captures aspects of this root tenant that are non-obvious. (We learned that it was non-obvious because some code has been written that was unaware of the special status of the root tenant.)\\n","Decision":"There is a special tenant known as the Root Tenant. It has a well-known id, `f26450ab1668784bb327951c8b08f347`. It is special in three respects:\\n* tenants are hierarchical, and the root tenant forms the root of that hierarchy\\n* the tenanted storage mechanisms will all fall back to the root tenant to find default connection settings if the tenant being used has not defined tenant-specific settings\\n* within Marain services, the root tenant is always represented by a special in-memory instance of the `RootTenant` type, whereas all other tenants are managed by the tenancy service\\nThat third item is there to support the second: because each service puts its own `RootTenant` into the DI service collection, as a singleton, it becomes possible for the service to attach whatever service-specific fallback settings it requires. We describe the root tenant as \"synthetic\" because each service creates its own object to represent the root tenant, whereas all other objects representing tenants are obtained via the `Marain.Tenancy` service, typically through the `ClientTenantProvider`.\\nWe contemplated separating out the first two concerns (which might enable us not to need the third characteristic above) because it has been a source of confusion in the past. However, for the time being we are planning to keep it this way because that alternative approach would require us to introduce an extra mechanism to support these kinds of defaults.\\n","tokens":88,"id":37}
{"File Name":"Marain.Tenancy\/0002-separation-of-read-and-modify.md","Context":"## Context\\nOur initial design for tenancy in Corvus (which necessarily affected Marain.Tenancy) comingled read and write behaviour. The model was similar to the .NET Entity Framework: if you wanted to modify a tenant, you would first fetch an object representing that tenant, then make changes to that object, and then invoke an operation indicating that you wanted those changes to be written back.\\nWe made various changes to the Property Bag system that tenancy uses to store tenant properties to disassociate the API from any particular JSON serialization framework. We had previously forced a dependency on Json.NET, but we wanted to be able to move onto `System.Text.Json`, so we wanted to introduce a Property Bag abstraction that was independent of serialization mechanism (although still with a presumption that it must be possible for the properties to be serialized as JSON).\\nOne of the basic principles of efficient JSON parsing in the new world is that you don't build an object model representing the JSON unless you really need to. Ideally, you leave the JSON in its raw UTF-8 state, referred to via one or more `IMemory<byte>` values, and extract what data you need only as you need it. This can dramatically reduce GC pressure, particularly in cases where most of the data in question is not used most of the time. However, this model does not fit well with the \"modifiable entities\" approach to updates. If anything is free to modify the properties at any time, this implies an ability to edit or regenerate the JSON.\\nIn practice, modification of tenant properties is the exception, not the rule. Most Marain services will only ever fetch tenant properties. Only the Marain.Tenancy service should normally directly edit these properties. So the \"modifiable entities\" approach is not really necessary, and causes problems for migration to allocation-efficient strategies.\\n","Decision":"Since `Corvus.Json.Abstractions` separates out read and update operations for `IPropertyBag`, and `Corvus.Tenancy` therefore does the same (since it uses property bags), Marain.Tenancy will follow suit.\\nThe web API presented by Marain.Tenancy for modifying tenants uses JSON Patch. So instead of this procedure:\\n* fetch a serialized representation of an ITenant from the web API\\n* modify that representation to reflect the changes you wish to make\\n* PUT that serialized representation of an ITenant back to the web API\\nwe now use this procedure instead:\\n* send a PATCH request in describing the changes required in JSON Patch format\\nFor example, to rename a tenant, you would send this PATCH to the Marain.Tenancy service, using the URL representing the tenant (the same URL from which you would fetch the tenant if reading) with an `application\/json-patch+json` content type:\\n```json\\n[{\\n\"path\": \"\/name\",\\n\"op\": \"replace\",\\n\"value\": \"NewTenantName\"\\n}]\\n```\\nJSON Patch supports multiple changes in a single request, e.g.:\\n```json\\n[\\n{\\n\"op\": \"add\",\\n\"path\": \"\/properties\/StorageConfiguration__corvustenancy\",\\n\"value\": {\\n\"AccountName\": \"mardevtenancy\",\\n\"Container\": null,\\n\"KeyVaultName\": \"mardevkv\",\\n\"AccountKeySecretName\": \"mardevtenancystore\",\\n\"DisableTenantIdPrefix\": false\\n}\\n},\\n{\\n\"op\": \"add\",\\n\"path\": \"\/properties\/Foo__bar\",\\n\"value\": \"Some string\"\\n},\\n{\\n\"op\": \"add\",\\n\"path\": \"\/properties\/Foo__spong\",\\n\"value\": 42\\n}\\n]\\n```\\nThe `op` can be set to `remove` to delete properties.\\nClients will not typically build these PATCH requests themselves, because the `ClientTenantStore` type contains the relevant code. `ClientTenantStore` provides an implementation of `ITenantStore` that works by using the web API provided by Marain.Tenancy. So in practice, updating the name of a tenant is as simple as:\\n```csharp\\nawait tenantStore.UpdateTenantAsync(tenantId, name: \"NewTenantName\");\\n```\\nAdding or changing a property looks like this:\\nawait tenantStore.UpdateTenantAsync(\\ntenantId,\\npropertiesToSetOrAdd: new Dictionary<string, object>()\\n{\\n{ \"StorageConfiguration__corvustenancy\", myStorageConfig },\\n{ \"SomeOtherSetting\": 42 },\\n});\\n","tokens":373,"id":39}
{"File Name":"Menes\/0001-menes-exceptions-have-specialized-constructors.md","Context":"## Context\\nSwitching on the C# 8.0 nullable references feature for Menes has revealed some ambiguities around whether certain properties of exceptions are meant to be nullable.\\nIn many cases, the only reason for ambiguity is that we have followed a pattern of defining various \"standard constructors\", such as default constructors, exception-message-only constructors, and deserializing constructors.\\n","Decision":"Menes exceptions will not have any of these standard exceptions except in cases where there are no required properties (e.g., the exception's type tells you everything you need to know).\\nProperties that always have non-null values in practice will declare this formally by having non-nullable types.\\nWe will remove all deserializing constructors, and remove the `[Serializable]` attribute from all exceptions that have them. This has been motivated by the use of nullable references, because deserializing constructors cause some challenges there, but this is a distinct issue. Menes exceptions are all designed for use within a Menes-based service. Menes is designed to implement service boundaries, and by definition, if we ever attempt to throw a Menes-defined exception across a process boundary, we've made a mistake.\\n","tokens":77,"id":40}
{"File Name":"Menes\/0002-multitargeting-.net-standard-2.0-and-2.1.md","Context":"## Context\\nMenes supports C# 8.0's nullable references feature. In most cases, libraries need to use some of the attributes from the `System.Diagnostics.CodeAnalysis` namespace that enable to you provide sufficient information for the compiler's null analysis to do a good job.\\nThese attributes are not available in `netstandard2.0`. However, there is a standard workaround: define your own copies of these attributes and use those. We are using the `Nullable` NuGet package to do this for us. This works nicely, enabling applications targeting older runtimes still to enable nullable references.\\nThe problem is that you don't want to use this workaround unless you have to. Newer versions of .NET Core and .NET Standard have these attributes, so it's just a waste of space to define your own.\\n","Decision":"Menes will target both .NET Standard 2.0 and .NET Standard 2.1. The .NET Standard 2.0 version brings its own copies of the attributes, the .NET Standard 2.1 version relies on the ones built into the framework.\\n","tokens":167,"id":41}
{"File Name":"Menes\/0003-allowing-implicit-object-type.md","Context":"## Context\\nIn real-world OpenAPI schema, we have discovered that people sometimes omit the `type: object` from their object definitions. We believe that this *is* valid Open API schema.\\n```yaml\\nPet:\\nrequired:\\n- id\\n- name\\nproperties:\\nid:\\ntype: integer\\nformat: int64\\nname:\\ntype: string\\ntag:\\ntype: string\\n```\\nHowever, there are other scenarios where you are *not* expected to supply the `type` property. Specifically, the `anyOf`, `oneOf`, `allOf` cases.\\n```yaml\\nsomeEntity:\\nanyOf:\\n- type: string\\n- type: object\\n- type: array\\n- type: boolean\\n- type: integer\\n- type: number\\n```\\n","Decision":"Menes will support these semantics. We have updated our schema validation to support this by translating the missing `type` element into the internal schema type `None`, rather than translating to `Object`.\\n","tokens":170,"id":42}
{"File Name":"modiapersonoversikt\/0002-selvstendig-visittkort-som-library.md","Context":"## Context\\nModiapersonoversikt (denne frontenden) blir utviklet som en selvstendig frontend som p\u00e5 sikt skal erstatte frontenden i dagens modiabrukerdialog. For \u00e5 kunne levere fortl\u00f8pende ny funksjonalitet til saksbehandlerene, \u00f8nsker vi \u00e5 levere ofte og sm\u00e5tt.\\n","Decision":"Visittkortet dras inn som en enkeltst\u00e5ende react-komponent inn til modiabrukerdialog.\\n","tokens":81,"id":43}
{"File Name":"modiapersonoversikt\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":44}
{"File Name":"bunyan-logger\/0001-replace-travis-ci-with-github-actions.md","Context":"## Context\\nGitHub Actions are the new shiny and have been in Beta for sometime. They are\\ngoing to be generally available on\\n[2019-11-13](https:\/\/github.blog\/2019-08-08-github-actions-now-supports-ci-cd\/).\\nGitHub Actions will have a long term future. It is likely GitHub Actions\\nwill become the default CI mechanism (and possibly more) for projects hosted on\\nGitHub. Using them in this repo, which has a basic use case will provide some\\nexposure to the service.\\n","Decision":"The decision is to replace Travis CI with GitHub Actions.\\n","tokens":114,"id":45}
{"File Name":"mokka\/0003-upgrade-to-spring-boot-2-2-1.md","Context":"## Context\\nCurrently used: 2.1.0\\nThe latest stable version of Spring Boot is 2.2.1:\\nhttps:\/\/github.com\/spring-projects\/spring-boot\/wiki\/Spring-Boot-2.2-Release-Notes\\nMore frequent but smaller upgrades are recommended.\\n","Decision":"Spring Boot will be upgraded to 2.2.1.\\nAccording to release notes no migration needed on Mokka side.\\n","tokens":63,"id":46}
{"File Name":"mokka\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":48}
{"File Name":"copilot\/0003-use-mcp-api-for-pilot-config.md","Context":"## Context\\nMesh Configuration Protocol (MCP) is a [protocol](https:\/\/github.com\/istio\/api\/tree\/master\/mcp) for transferring configuration among Istio components during runtime. MCP is meant to defer all the logics and complexities back to the server (copilot) as oppose to the original design which all the logic was embeded in the client (Pilot). Another goal of MCP is to create a unified contract for all the Custom Resource Definitions and Service Discovery and the way they are communicated with Pilot.\\n","Decision":"Copilot will implement a MCP server to send configuration to Pilot. We will be sending definitions for Gateways, VirtualServices and DestinationRules over bi-directional GRPC.\\n","tokens":106,"id":49}
{"File Name":"copilot\/0006-do-something-for-one-way-tls.md","Context":"## Context\\nEnable one way TLS between front-end and envoy per host+domain that is\\nspecified via gateway config.\\n","Decision":"#### Gateway Configuration\\nThis is achieved by sending the following config from copilot.\\n```\\napiVersion: networking.istio.io\/v1alpha3\\nkind: Gateway\\nmetadata:\\nname: mygateway\\nspec:\\nselector:\\nistio: ingressgateway # use istio default ingress gateway\\nservers:\\n- port:\\nnumber: 443\\nname: https-httpbin\\nprotocol: HTTPS\\ntls:\\nmode: SIMPLE\\nserverCertificate: \/etc\/istio\/ingressgateway-certs\/tls.crt\\nprivateKey: \/etc\/istio\/ingressgateway-certs\/tls.key\\nhosts:\\n- \"httpbin.example.com\"\\n- port:\\nnumber: 443\\nname: https-bookinfo\\nprotocol: HTTPS\\ntls:\\nmode: SIMPLE\\nserverCertificate: \/etc\/istio\/ingressgateway-bookinfo-certs\/tls.crt\\nprivateKey: \/etc\/istio\/ingressgateway-bookinfo-certs\/tls.key\\nhosts:\\n- \"bookinfo.com\"\\n```\\nIn the config above each cert and key in the array of servers represent a\\nhost+domain and the path to each cert and the key is arbitrarily chosen.\\nCopilot extracts the domain information from the cert chains provided in the bosh spec properties:\\n```\\nfrontend_tls_keypairs:\\nexample:\\n- cert_chain: |\\n-----BEGIN CERTIFICATE-----\\n-----END CERTIFICATE-----\\n-----BEGIN CERTIFICATE-----\\n-----END CERTIFICATE-----\\nprivate_key: |\\n-----BEGIN RSA PRIVATE KEY-----\\n-----END RSA PRIVATE KEY-----\\n- cert_chain: |\\n-----BEGIN CERTIFICATE-----\\n-----END CERTIFICATE-----\\n-----BEGIN CERTIFICATE-----\\n-----END CERTIFICATE-----\\nprivate_key: |\\n-----BEGIN RSA PRIVATE KEY-----\\n-----END RSA PRIVATE KEY-----\\n```\\n#### Cert Storage\\nThe placement of the certs and keys on the envoy VM is done using a separate\\nprocess specific to this purpose. This process will be in charge of knowing\\nwhere the certs are located and placing the certs on the correct paths. It is\\nimportant for the envoy VM and copilot to agree on a path where the cert and the keys\\nare stored, and having a specific process to manage this will reduce duplication\\nand mitigate skew.\\n","tokens":25,"id":50}
{"File Name":"copilot\/0004-check-in-copilot-dependencies-that-were-already-vendored.md","Context":"## Context\\nPrevious packaging of Copilot in istio release relied on the fact that you would\\nbe building copilot on the local machine (bosh pre-packaging).  This meant that\\nyou could reliably fetch all of your dependencies using dep (which was included\\nas a blob in the release).\\nWhen we moved to get rid of pre-packaging and instead do all packaging on a bosh\\nvm (just known as packaging) we ended up missing one key external dependency for\\ndep to work (git). Including git as part of release would have meant adding\\nanother blob and packaging step just for git.\\n","Decision":"We removed the .gitignore of the vendor directory and checked-in all of the\\nsource code that dep was placing in that directory at build time.\\n","tokens":128,"id":51}
{"File Name":"copilot\/0005-enable-grpc-gzip-compression-between-copilot-and-route-syncer.md","Context":"## Context\\nThe GRPC default message size of 4 mb currently causing a bottleneck between cc-route-syncer and copilot. As our message sizes increased with scale this prevents us from sending messages to copilot.\\n","Decision":"We have decided to reduce the message size by enabling GRPC's GZIP compression between cc-route-syncer and copilot.\\n","tokens":44,"id":52}
{"File Name":"copilot\/0002-use-event-streaming-model-for-diego-actuallrp-syncing.md","Context":"## Context\\nThe diego ActualLRP syncing model as currently implemented will fetch all LRPs\\nacross all diego cells at a specified time interval (at the time of writing 10\\nseconds). As the ActualLRP count grows on a cloudfoundry deployment this could\\nimpact the performance of the BBS (large response sets coming back).\\n","Decision":"We want to use the [Event package](https:\/\/github.com\/cloudfoundry\/bbs\/blob\/master\/doc\/events.md)\\nto get the event stream for each ActualLRP. We will also use a bulk sync every\\n60 seconds to catch any events that were missed.\\n","tokens":75,"id":53}
{"File Name":"copilot\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":54}
{"File Name":"ELEN4010\/ADR Maps API choice.md","Context":"## Context\\nFor the project, we need an API for a map: interface, search, marker placement, satellite or road map imagery. There are several options for maps, will be a primary mode of interacting with the site.\\n* Google maps\\n* OpenLayers\\n* TomTom\\n* MapBox\\n* HERE\\n* Mapfit\\nMain factor are cost, and ease of use (documentation for the API)\\nGoogle maps are highly customizable in style and appearance, and configerable for marker placement, information windows, and interface\/controls.\\n","Decision":"Upon examining the options, Google Maps was considered the most mature, easy-to-use and well-supported option. The API has excellent documentation and example code. The interface will be familiar to the majority of site users.\\n","tokens":112,"id":55}
{"File Name":"ELEN4010\/ADR Testing.md","Context":"## Context\\nIn order to holistically test the core functionality of the website, a combination of unit testing, end-to-end testing, and manual testing is used.\\nUnit tests are used on back-end models and database-related code in order to validate the functionality of each essential unit of the code (which, in most cases, are functions).\\nOn the front-end, various user actions are performed by automated testing software. During that process, key aspects relating to the front-end side of the website are tested.\\nHigh-level functionality is exclusively assessed and confirmed via manual user testing. This includes testing the following aspects of the website:\\n- Marker placement on maps\\n- Destinations being correctly added and drawn\\n- Trips being correctly written to and received from session storage\\n","Decision":"The testing framework chosen for automated testing is Jest. This framework is used because:\\n- It has a simple installation and configuration process for Node.js\\n- Due to its popularity as a javascript testing framework, it has a large developer-community which produces many articles, documents and forum threads (amongst many other sources of documentation and support)\\n- It has a wide variety of built-in assertion abilities (which means that there is no need for the installation of a third-party assertion library)\\nIn order to simulate in-browser user-interactions with the website, Selenium WebDriver is used. Front-end testing is performed on the https:\/\/testawaywego.azurewebsites.net website since it is the website used for development.\\nUltimately, it was decided that all automated front-end user testing will be performed using Google Chrome as the browser. The reason for this is due to the fact that Google Chrome has the highest browser market share (more than 60%) globally - meaning that a majrity of the website's users will be using Google Chrome.\\nAt multiple stages throughout the development process, manual testing on other major browsers (i.e. FireFox, Safari and Microsoft Edge) was also performed in order to ensure the cross-browser compatibility of the website. Manual testing was also used to ensure that the website is mobile-friendly.\\n","tokens":156,"id":56}
{"File Name":"ELEN4010\/ADR Sprint Planning and timeline.md","Context":"## Context\\n16-25 April, Thabang is away, must work remotely. Major submission period over 29th April - 10th May, will impact productivity on software project.\\n","Decision":"4 Sprints planned, consecutively. Will only start on 17th April, but have a \"Sprint 0\" from 9th to 16 April, with initial planning, research and folder structure creation, setup of Azure and Travis. Will not count towards actual number of sprints.\\n* Sprints will begin on Wednesdays, with a 1 hr review session in the morning.\\n* There will be a three hour sprint planning session in the afternooon each Wednesday.\\n* Release will be every Tuesday, by 8pm\\n* Product release window will be from 2pm - 8pm, with all pull requests done before 6pm to give time for any required last minute code review and testing\\n* Friday coding sessions together from 12pm - 4pm\\n* Standups via Whatsapp, or between lectures. Preferable to do in person but may not be possible.\\nRebecca: Product Manager\\nTyson: SCRUM Master\\nTheese roles will be alternated throughout the project each week.\\n","tokens":40,"id":57}
{"File Name":"ELEN4010\/ADR Email invites.md","Context":"## Context\\nIn order to join a group, a potential member must be invited (it's not correct to add a person to a group without asking permission.) A person invited can either be a member of the website (have an account) or be a new user (no account registered yet). In order to cover both of these scenarios, and to avoid the website being a \"walled garden\" with a tiny set of users, and to encourage potential future growt, a mechanism to invite users could be an email, sent by an existing member, to any valid email address with an invitation to join. This could be in the form of a token with a payload, or more simply, an extra table in the DB, linking the invited person's email to a trip ID.\\n","Decision":"The mechanism of an external invitation with a specific link requires the ability to send an email (prefereably attractively  styled and clearly phrased, to avoid being rejected as unsolicited or junk email). The node module 'nodemailer' was selected as appropriate, for its wide support, mature development and ease of use, and 0 dependecies.\\n","tokens":158,"id":58}
{"File Name":"ELEN4010\/ADR Reordering Destinations.md","Context":"## Context\\nA list of destinations should be reorderable, not fixed\\n","Decision":"A trip is made up of a list of destinations. This list should be able to be reordered, on the main site or the mobile version of the site. Draggable would be the best, but a button for moving an extry up and down will also work.\\n","tokens":15,"id":59}
{"File Name":"ELEN4010\/ADR Entering destinations for a trip.md","Context":"## Context\\nDestinations need to be entered into a trip somehow. The two most obvious choices seem to be by typing (some kind of auto-completion feature) or by clicking directly on a map, to set markers. These paradigms are the dominant ones in most existing APIs and site\/map websites.\\n","Decision":"We will aim to support both autocomplete AND clicking on the map. This would be the most convenient for users of the site.\\n","tokens":63,"id":60}
{"File Name":"ELEN4010\/ADR Login API Choice.md","Context":"## Context\\nIt would be convenient to use the Google Login API as an alternative method for users to login. This would provide a template for our own login details stored in the DB, as well as a quick way to get the Sprint 1 User story related to login completed ASAP.\\n","Decision":"Using a well known and widely known\/supported login mechanism such as Google's OAuth2 will allow more rapid development of an appropriate security setup for the site. We will apply for an API key and start implementing the login\/registration page through the Google Login API\\n","tokens":58,"id":61}
{"File Name":"ELEN4010\/ADR GitHub Project Board as KanBan.md","Context":"## Context\\nA SCRUM-based agile devlopment workflow would benefit from a central KANBAN board to keep track of userstories that have been written, are in progress, and are complete. This will help identify the sprint backlog, and the current focus of the sprint. Labels could be used to indicate size\/priority\/difficuly or value to the project, to help calculate the sprint velocity and determine what can get done inside a single sprint.\\n","Decision":"Using the GitHib Project page with a single project for the repo, and using Issues labelled as User Stories, with columns for \"To Do\", \"In progress\", and \"Completed\".  We can leverage some of the automatic rules in Git to help automate some of the completetion of tasks ties to Milestones for each sprint:\\nhttps:\/\/github.com\/witseie-elen4010\/2019-005-project\/projects\/1\\n","tokens":94,"id":62}
{"File Name":"ELEN4010\/ADR Trunk-Based Development.md","Context":"## Context\\nTo perform Continual Integration and development, with weekly releases, it would be convenient and useful to have a testing branch as well. Accidental pull requests into the main branch may introduce features that have not been tested from the interfac\/front-end. It is difficult to automate these front-end interface tests, and there may be factors not present in a localhost\/express server that only become apparent in an online scanario.\\nThe use of **master** branch as the release branch is useful, as 'master' is usually the most protected on GitHub, with the most warnings about deleting, modifying, etc.\\nCode reviews ar essential from all developers, to become familiar with each other's code, and to learn about javascript, and web-development. THis way we all learn from each other, and also learn good review and communicaton practice.\\n","Decision":"**master** will be the release branch\\n**development** will be the main development\/test branch. This will also be made into the \"default\" branch for all pull requests, to avoid accidentaly PR into master\\n**feature** branches must be made off development, with unique names. All pull requests for completed features to be made into \"development\".\\n* All PRs must be reviewed by at least two developers to merge into \"development\"\\n* All PRs must be reviewed by at the three other developers to merge into \"master\"\\n* All PRs must pass all tests (Jest, Travis, and Coveralls) in order to be considered valid for a merge\\n* Stale reviews will be automatically dismissed if a new commit is pushed to the same branch\\n* Accepted PRs for completed features (User Stories) should be deleted after sucessfully merging\\n","tokens":172,"id":63}
{"File Name":"ELEN4010\/ADR Folder structure.md","Context":"## Context\\nHaving a fixed structure for a project has may advantages, limiting spread of files across multiple folders and contraining locations to known places. THere is an advantage is letting a folder strucute emerge oganically, but also a large risk, as things can break when low-level file locations change, necesitating logs of bug fixing and refactoring. Having a rigid initial structure canb lead to later restrictions, or imposed complexity.\\n","Decision":"The following folder strucure is adopted:\\n.\\n\u251c\u2500\u2500 app\\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 controllers\\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 models\\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 public\\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 css\\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 img\\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 js\\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 routes\\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 views\\n\u251c\u2500\u2500 docs\\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 adr\\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 misc\\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 project_artifacts\\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 templates\\n\u251c\u2500\u2500 node_modules\\n\u251c\u2500\u2500 test\\n\u2514\u2500\u2500 local_only\\n**Update** Removed folders originally specified that were found to not be required during project development: 'log' and 'utility'\\n","tokens":90,"id":64}
{"File Name":"infection\/0002-@covers-annotations.md","Context":"### Context\\nPHPUnit offers a range of `@covers` annotations with the possible to enforce a strict mode or to\\nenforce them. The question is when should those annotations be enforced and\/or if we need to enable\\nanother settings as well?\\n### Decision\\nSince we are using the [`symfony\/phpunit-bridge`][phpunit-bridge], we decide to leverage the\\n[`Symfony\\Bridge\\PhpUnit\\CoverageListener`][code-coverage-listener] in `phpunit.xml.dist` in order to avoid to require the\\n`@covers` annotations whilst still benefit from it.\\nThis however does not allow to completely forgo its usage due to the following cases:\\n- A test testing more than one class, requiring multiple `@covers` annotations\\n- A test case testing a \"test class\", i.e. code reserved for testing purposes\\nFor this reason, the proposal to remove the `@covers` annotations via the [PHP-CS-Fixer][php-cs-fixer]\\nsetting `general_phpdoc_annotation_remove` has been refused.\\nSince no one came up with an easy or acceptable proposal to automate the process of whether a\\n`@covers` annotation is necessary or not, no further action has been voted for automating this\\nprocess.\\n### Status\\nAccepted ([#1060][1060])\\n[code-coverage-listener]: https:\/\/symfony.com\/doc\/current\/components\/phpunit_bridge.html#code-coverage-listener\\n[phpunit-bridge]: https:\/\/packagist.org\/packages\/symfony\/phpunit-bridge\\n[php-cs-fixer]: https:\/\/github.com\/FriendsOfPHP\/PHP-CS-Fixer\\n[1060]: https:\/\/github.com\/infection\/infection\/pull\/1060\\n","Decision":"Since we are using the [`symfony\/phpunit-bridge`][phpunit-bridge], we decide to leverage the\\n[`Symfony\\Bridge\\PhpUnit\\CoverageListener`][code-coverage-listener] in `phpunit.xml.dist` in order to avoid to require the\\n`@covers` annotations whilst still benefit from it.\\nThis however does not allow to completely forgo its usage due to the following cases:\\n- A test testing more than one class, requiring multiple `@covers` annotations\\n- A test case testing a \"test class\", i.e. code reserved for testing purposes\\nFor this reason, the proposal to remove the `@covers` annotations via the [PHP-CS-Fixer][php-cs-fixer]\\nsetting `general_phpdoc_annotation_remove` has been refused.\\nSince no one came up with an easy or acceptable proposal to automate the process of whether a\\n`@covers` annotation is necessary or not, no further action has been voted for automating this\\nprocess.\\n### Status\\nAccepted ([#1060][1060])\\n[code-coverage-listener]: https:\/\/symfony.com\/doc\/current\/components\/phpunit_bridge.html#code-coverage-listener\\n[phpunit-bridge]: https:\/\/packagist.org\/packages\/symfony\/phpunit-bridge\\n[php-cs-fixer]: https:\/\/github.com\/FriendsOfPHP\/PHP-CS-Fixer\\n[1060]: https:\/\/github.com\/infection\/infection\/pull\/1060\\n","tokens":361,"id":65}
{"File Name":"infection\/0003-PHPUnit-this-over-self.md","Context":"### Context\\nPHPUnit assertions are static methods, yet in our code base we call them with `$this` instead of\\n`self`.\\nWhilst \"incorrect\", this usage does not break anything. Besides:\\n- [PHUnit documentation][phpunit-doc] itself uses this by default\\n- `$this` is much more widely used than `self` in this context in the community\\n- all Infection code uses `$this`\\nThere is not much shortcomings from using this other than the \"incorrectness\" of using a static\\nmethod as a non-static one.\\n### Decision\\nSince there is no clear benefits of adopting `self` over `$this` and given the context of its usage,\\nthe decision is to keep the usage of `$this` over `self` in the codebase.\\n### Status\\nAccepted ([#1061][1061])\\n[phpunit-doc]: https:\/\/phpunit.de\/manual\/6.5\/en\/appendixes.assertions.html\\n[1061]: https:\/\/github.com\/infection\/infection\/pull\/1061\\n","Decision":"Since there is no clear benefits of adopting `self` over `$this` and given the context of its usage,\\nthe decision is to keep the usage of `$this` over `self` in the codebase.\\n### Status\\nAccepted ([#1061][1061])\\n[phpunit-doc]: https:\/\/phpunit.de\/manual\/6.5\/en\/appendixes.assertions.html\\n[1061]: https:\/\/github.com\/infection\/infection\/pull\/1061\\n","tokens":221,"id":66}
{"File Name":"infection\/0001-inheritdoc.md","Context":"### Context\\nUsing `@inheritdoc` was done inconsistently across the codebase so the decision of whether we use it\\nsystematically or remove it systematically had to be done.\\nA number of points:\\n- [PHPDoc][phpdoc-inheritance] provides inheritance of the docblocks by default when appropriate\\n- Static analysers such as PHPStan or Psalm can do without at the time of writing\\nAlso it has a very limited value.\\n### Decision\\nDo not use `@inheritdoc` tags or any of its variants. The `@inheritdoc` tags and its variants must\\nbe removed when submitting pull requests.\\n### Status\\nAccepted ([#860][860])\\n[phpdoc-inheritance]: https:\/\/docs.phpdoc.org\/guides\/inheritance.html\\n[860]: https:\/\/github.com\/infection\/infection\/issues\/860\\n","Decision":"Do not use `@inheritdoc` tags or any of its variants. The `@inheritdoc` tags and its variants must\\nbe removed when submitting pull requests.\\n### Status\\nAccepted ([#860][860])\\n[phpdoc-inheritance]: https:\/\/docs.phpdoc.org\/guides\/inheritance.html\\n[860]: https:\/\/github.com\/infection\/infection\/issues\/860\\n","tokens":175,"id":67}
{"File Name":"infection\/0004-PHPUnit-expect-exception-over-try-catch.md","Context":"### Context\\nWhen executing code that is expected to fail in a test case, there is two ways to do this:\\n```php\\nfunction test_something(): void {\\n\/\/ ...\\ntry {\\n\/\/ the statement that fail\\n$this->fail();\\n} catch (Exception $e) {\\n\/\/ ...\\n}\\n}\\n```\\nOr:\\n```php\\nfunction test_something(): void {\\n\/\/ ...\\n$this->expectException($exception)\\n\/\/ the statement that fail\\n}\\n```\\n### Decision\\nAs recommended by [Sebastian Bergmann][sebastian-bergmann] in\\n[this article][phpunit-exception-best-practices], since in both cases a PHPUnit specific API is\\nnecessary, the decision taken is to leverage the `expectException*()` API when possible.\\nA pull request to fix this practice in the whole codebase may be done but has not been made\\nmandatory. New pull requests though should stick to this practice.\\n### Status\\nAccepted ([#1090][1090])\\n[sebastian-bergmann]: https:\/\/thephp.cc\/company\/consultants\/sebastian-bergmann\\n[phpunit-exception-best-practices]: https:\/\/thephp.cc\/news\/2016\/02\/questioning-phpunit-best-practices\\n[1090]: https:\/\/github.com\/infection\/infection\/pull\/1061\\n","Decision":"As recommended by [Sebastian Bergmann][sebastian-bergmann] in\\n[this article][phpunit-exception-best-practices], since in both cases a PHPUnit specific API is\\nnecessary, the decision taken is to leverage the `expectException*()` API when possible.\\nA pull request to fix this practice in the whole codebase may be done but has not been made\\nmandatory. New pull requests though should stick to this practice.\\n### Status\\nAccepted ([#1090][1090])\\n[sebastian-bergmann]: https:\/\/thephp.cc\/company\/consultants\/sebastian-bergmann\\n[phpunit-exception-best-practices]: https:\/\/thephp.cc\/news\/2016\/02\/questioning-phpunit-best-practices\\n[1090]: https:\/\/github.com\/infection\/infection\/pull\/1061\\n","tokens":285,"id":68}
{"File Name":"dapr\/API-002-actor-api-design.md","Context":"## Context\\nGiven Dapr is going out with language specific Actor SDKs, we formally introduced an Actor API into Dapr to make Actors are first-class citizen in Dapr. The goal of this review was to ensure Dapr can provide strong support of Service Fabric stateful actors programming model so that we can offer a migration path to the majority of existing actor users.\\n","Decision":"### Dapr\\n* A separate Actor interface is defined.\\n* Actors should support multiple reminders and timers.\\n* Actor state access methods are encapsulated in the Actor interface itself.\\n* Actor interface shall support updating a group of key-value states in a single operation.\\n* Actor interface shall support deletion of an actor. If the actor is activated when the method is called, the in-flight transaction is allowed to complete, then the actor is deactivated, deleted, with associated state removed.\\n### Non-Dapr\\n* Transaction across multiple API calls is left for future versions, if proven necessary. Due to single-threaded guarantee, such transaction scope might be unnecessary. However, if developer expects an Actor code to behave atomically (in an implied transaction scope), we may have to implement this.\\n","tokens":75,"id":69}
{"File Name":"dapr\/ENG-003-test-infrastructure.md","Context":"## Context\\nE2E tests ensure the functional correctness in an e2e environment in order to make sure Dapr works with the user code deployments. The tests will be run before \/ after PR is merged or by a scheduler.\\nDapr E2E tests require the test infrastructure in order to not only test Dapr functionalities, but also show these test results in a consistent way. This document will decide how to bring up the test cluster, run the test, and report the test results.\\n","Decision":"### Test environments\\nAlthough Dapr is designed for multi cloud environments, e2e tests will be run under Kubernetes environments for now. We will support two different options to run e2e tests with local machine and CI on the pre-built Kubernetes cluster.\\n* **Local machine**. contributors or developers will use [Minikube](https:\/\/github.com\/kubernetes\/minikube) to validate their changes and run new tests before creating Pull Request.\\n* **Continuous Integration**. E2E tests will be run in the pre-built [Azure Kubernetes Service](https:\/\/azure.microsoft.com\/en-us\/services\/kubernetes-service\/) before\/after PR is merged or by a scheduler. Even if we will use [Azure Kubernetes Service](https:\/\/azure.microsoft.com\/en-us\/services\/kubernetes-service\/) in our test infrastructure, contributors should run e2e tests in any  RBAC-enabled Kubernetes clusters.\\n### Bring up test cluster\\nWe will provide the manual instruction or simple script to bring up test infrastructure unlike the other Kubernetes projects using [kubetest](https:\/\/github.com\/kubernetes\/test-infra\/tree\/master\/kubetest). Dapr E2E tests will clean up and revert all configurations in the cluster once the test is done. Without kubetest, we can create e2e tests simpler without the dependency of the 3rd party test frameworks, such as ginkgo, gomega.\\n### CI\/CD and test result report for tests\\nMany Kubernetes-related projects use [Prow](https:\/\/github.com\/kubernetes\/test-infra\/tree\/master\/prow), and [Testgrid](https:\/\/github.com\/kubernetes\/test-infra\/tree\/master\/testgrid) for Test CI, PR, and test result management. However, we will not use them to run Dapr E2E tests and share the test result since we need to self-host them on Google cloud platform.\\nInstead, Dapr will use [Azure Pipeline](https:\/\/azure.microsoft.com\/en-us\/services\/devops\/pipelines\/) to run e2e tests and its [test report feature](https:\/\/docs.microsoft.com\/en-us\/azure\/devops\/pipelines\/test\/review-continuous-test-results-after-build?view=azure-devops) without self-hosted CI and test report services. Even contributors can get their own azure pipelines accounts **for free** without self-hosting them.\\n","tokens":102,"id":70}
{"File Name":"dapr\/API-006-universal-namespace.md","Context":"## Context\\nFor cloud-edge hybrid scenarios and multie-region deployment scenarios, we need the ability to facilitate communications cross clusters. Specifically, it's desirable to have services scoped by cluster names so that a service in one cluster can address and invoke services on another trusted cluster through fully qualified names in a universal namespace, such as cluster1.serviceb.\\n","Decision":"We should consider adding universal namespace capabilities to Dapr.\\n","tokens":70,"id":71}
{"File Name":"dapr\/ENG-004-signing.md","Context":"## Context\\nAuthenticode signing of binaries.\\n","Decision":"* Binaries will not be signed with Microsoft keys. In future we can revisit to sign the binaries with dapr.io keys.\\n","tokens":11,"id":72}
{"File Name":"dapr\/SDK-002-java-jdk-versions.md","Context":"## Context\\nDapr offers a Java SDK. Java 11 is the latest LTS version. Java 8 is the previous LTS version but still the mainly used version by the Java community in 2019. What should be the minimum Java version supported by Dapr's Java SDK?\\nSee https:\/\/github.com\/dapr\/java-sdk\/issues\/17\\n","Decision":"* Java 8 should be the minimum version supported for Dapr's Java SDK.\\n* Java 11 should be used in samples and user documentation to encourage adoption.\\n* Java 8's commercial support ends in 2022. Dapr's Java SDK shoud migrate to Java 11 prior to that. The timeline still not decided.\\n","tokens":71,"id":73}
{"File Name":"dapr\/ENG-001-tagging.md","Context":"## Context\\nAs we embraced using Docker repositories to store our images, and keeping in mind we support multiple repositories along with versioning of images and different architectures,\\nWe needed a way to construct an accepted and constant way of naming our Docker images.\\n","Decision":"* An image will conform to the following format: \\<namespace>\/\\<repository>:\\<tag>\\n* A valid tag conforms to the following format: \\<version>-\\<architecture>, or just \\<version>, then arch is assumed Linux\\n","tokens":51,"id":74}
{"File Name":"dapr\/API-009-bidirectional-bindings.md","Context":"## Context\\nAs we want to provide bi-directional capabilities for bindings to allow for cases such as getting a blob from a storage account,\\nAn API change is needed to account for the requested type of operation.\\n","Decision":"### Naming\\nIt was decided to keep the bindings name as is. Alternative proposals were included changing bindings to connectors, but a strong case couldn't be made in favor of connectors to justify the breaking change it would cause.\\n### Types\\nIt was decided to keep the same YAML format for both input bindings and bi-directional bindings as it is today.\\nAfter careful inspection, splitting to two types (for example, trigger bindings and bindings) would incur significant maintanace overhead for the app operator and\\nDid not provide meaningful value.\\nIn addition, there was no feedback from community or prospecive users that input bindings and output bindings were confusing in any way.\\n### API structure\\nIt was decided that the API url will be kept as: `http:\/\/localhost:<port>\/v1.0\/bindings\/<name>`.\\nThe verb for the HTTP API will remain POST\/PUT, and the type of operation will be part of a versioned, structured schema for bindings.\\nThis is not a breaking change.\\n### Schema and versioning\\nIn accordance with our decision to work towards enterprise versioning, it was accepted that schemas will include a `version` field in\\nThe payload to specify which version of given component needs to be used that corresponds to the given payload.\\nIn addition, an extra field will be added to denote the type of operation that binding supports, for example: `get`, `list`, `create` etc.\\nBindings components will provide the means for the Dapr runtime to query for their supported capabilities and return a validaton error if the operation type is not supported.\\n","tokens":44,"id":75}
{"File Name":"dapr\/API-001-state-store-api-design.md","Context":"## Context\\nWe reviewed storage API design for completeness and consistency.\\n","Decision":"* All requests\/responses use a single parameter that represents the request\/response object. This allows us to extend\/update request\/response object without changing the API.\\n* Add Delete() method\\n* Support bulk operations: BulkDelete() and BulkSet(). All operations in the bulk are expected to be completed within a single transaction scope.\\n* Support a generic BulkOperation() method, which is carried out as a single transaction.\\n* Transaction across multiple API requests is postponed to future versions.\\n* Actor state operations are moved to a new Actor interface. Please see [API-002-actor-api-design](.\/API-002-actor-api-design.md).\\n","tokens":14,"id":76}
{"File Name":"dapr\/API-012-content-type.md","Context":"## Context\\nNot adding content-type to state store, pubsub and bindings.\\n","Decision":"* We will not add content-type since it is a persisted metadata and it can cause problems such as:\\n* Long term support since metadata persisted previously would need to be supported indefinitely.\\n* Added requirement for components to implement, leading to potentially hacky implementations to persist metadata side-by-side with data.\\nOriginal issue and discussion: https:\/\/github.com\/dapr\/dapr\/issues\/2026\\n","tokens":17,"id":77}
{"File Name":"dapr\/CLI-001-cli-and-runtime-versioning.md","Context":"## Context\\nAs we formally establish Dapr component version, we need to decide if we want to couple CLI versions with runtime versions.\\n","Decision":"* We'll keep CLI versioning and runtime versioning separate.\\n* CLI will pull down latest runtime binary during the *init()* command.\\n* Version scheme is: *major.minor.revision.build* for both CLI and runtime.\\n","tokens":28,"id":78}
{"File Name":"dapr\/API-010-appcallback-versioning.md","Context":"## Context\\nThere was a proposal to introducing versioning for HTTP App Callbacks. The goal of this review was to understand if a versioning was required and how it could handle situations post v1.0 of DAPR\\n","Decision":"- Introducing versioning to app callback APIs would require changes to the user applications which is not feasible\\n- There would be no way for DAPR runtime to find out the app callback version before hand\\nWe decided not to introduce such a versioning scheme on the app callback APIs. Post v1.0, if required, the versioning could be implemented inside the payload but not on the API itself. A missing version in the payload could imply v1.0.\\n","tokens":47,"id":79}
{"File Name":"dapr\/API-008-multi-state-store-api-design.md","Context":"## Context\\nThis decision record is to support multiple state stores support in Dapr. We agreed on the decision to introduce the breaking change in API\\nto support multi state store with no backward compatibility.\\nWith this change , the state API allows the app to target a specific state store by store-name, for example:\\nv1.0\/state\/storeA\/\\nv1.0\/state\/storeB\/\\nEarlier this breaking change, the API is v1.0\/state\/`<key>`\\nWe have reviewed multi storage API design for completeness and consistency.\\n","Decision":"*  New state store API is v1.0\/state\/`<store-name>`\/\\n*  If user is using actors and like to persist the state then user must provide actorStateStore: true in the configuration yaml.\\nIf the attribute is not specified or multiple actor state stores are configured, Dapr runtime will log warning.\\nThe actor API to save the state will fail in both these scenarios where actorStore is not specified or multiple actor stores\\nare specified.\\n*  It is noted that after this breaking change, actor state store has to be specified unlike earlier where first state store is picked up by default.\\n* It is noted that this breaking change will also require a CLI change to generate the state store YAML for redis with actorStateStore.\\n* To provide multiple stores, user has to provide separate YAML for each store and giving unique name for the store.\\n* It is noted that the param's keyPrefix represents state key prefix, it's value included ${appid} is the microservice appid, ${name} is the CRDs component's unique name, ${none} is non key prefix and the custom key prefix\\nFor example, below are the 2 sample yaml files in which redis store is used as actor state store while mongodb store is not used as actor state store.\\n```\\napiVersion: dapr.io\/v1alpha1\\nkind: Component\\nmetadata:\\nname: myStore1  # Required. This is the unique name of the store.\\nspec:\\ntype: state.redis\\nmetadata:\\n- name: keyPrefix\\nvalue: none # Optional. default appid. such as: appid, none, name and custom key prefix\\n- name: <KEY>\\nvalue: <VALUE>\\n- name: <KEY>\\nvalue: <VALUE>\\n- name: actorStateStore  # Optional. default: false\\nvalue : true\\n```\\n```\\napiVersion: dapr.io\/v1alpha1\\nkind: Component\\nmetadata:\\nname: myStore2 # Required. This is the unique name of the store.\\nspec:\\ntype: state.mongodb\\nmetadata:\\n- name: keyPrefix\\nvalue: none # Optional. default appid. such as: appid, none, name and custom key prefix\\n- name: <KEY>\\nvalue: <VALUE>\\n- name: <KEY>\\nvalue: <VALUE>\\n```\\nSo with the above example, the state APIs will be : v1.0\/state\/myStore1\/`<key>`\\nand v1.0\/state\/myStore2\/`<key>`\\n","tokens":115,"id":80}
{"File Name":"dapr\/ENG-002-Dapr-Release.md","Context":"## Context\\nThis record descibes how to safely release new dapr binaries and the corresponding configurations without any blockers to users.\\n","Decision":"### Integration build release\\nIntegration build refers to the build from `master` branch once we merge PullRequest to master branch. This build will be used for development purposes and must not be released to users and impact their environments.\\n### Official build release\\n#### Pre-release build\\nPre-release build will be built from `release-<major>.<minor>` branch and versioned by git version tag suffix e.g. `-alpha.0`, `-alpha.1`, etc. This build is not released to users who use the latest stable version.\\n**Pre-release process**\\n1. Create branch `release-<major>.<minor>` from master and push the branch. e.g. `release-0.1`\\n2. Add pre-release version tag(with suffix -alpha.0 e.g. v0.1.0-alpha.0) and push the tag\\n```\\n$ git tag \"v0.1.0-alpha.0\" -m \"v0.1.0-alpha.0\"\\n$ git push --tags\\n```\\n3. CI creates new build and push the images with only version tag\\n4. Test and validate the functionalities with the specific version\\n5. If there are regressions and bugs, fix them in release-* branch and merge back to master\\n6. Create new pre-release version tag(with suffix -alpha.1, -alpha.2, etc)\\n7. Repeat from 4 to 6 until all bugs are fixed\\n#### Release the stable version to users\\nOnce all bugs are fixed, we will create the release note under [.\/docs\/release_notes](https:\/\/github.com\/dapr\/dapr\/tree\/master\/docs\/release_notes) and run CI release manually in order to deliver the stable version to users.\\n### Release Patch version\\nWe will work on the existing `release-<major>.<minor>` branch to release patch version. Once all bugs are fixed, we will add new patch version tag, such as `v0.1.1-alpha.0`, and then release the build manually.\\n","tokens":27,"id":81}
{"File Name":"dapr\/API-005-state-store-behavior.md","Context":"## Context\\nAs we continue to solidify our API spec, we need to explicitly define component behaviors in the spec and make sure those are implemented in our implementation. This document captures our decisions on state store behaviors. It's expected that we'll create more of such documents to capture explicit component behavior decisions.\\n","Decision":"### Concurrency model\\n* Dapr supports two flavors of optimistic concurrency: first-write wins and last-write wins. First-write wins is implemented through ETag.\\n* User code can express concurrency intention with a *config* annotation attached to a request. See **Config annotation** for details.\\n* Future version of Dapr may support call throttling through application channel.\\n* We'll choose last-write wins as the default.\\n### Consistency model\\n* Dapr supports both eventual consistency and strong consistency.\\n* Actors always use strong consistency.\\n* We'll choose eventual consistency as default for services other than actors.\\n### Actor Transaction\\n* Dapr-compatible Actor state stores shall support ACID transaction.\\n* Dapr doesn't mandate specific transaction isolation level at this point. However, when deemed necessary, we can easily add those to **Config annotation** as needed.\\n### Config annotation\\n* User payload can contain an optional **config** annotation\/element that expresses various constraints and policies to be applied to the call, including:\\n* Concurrency model: first-write or last-write\\n* Consistency model: strong or eventual\\n* Retry policies:\\n* Interval\\n* Pattern: linear, expotential\\n* Circuit-breaker Timeout (before an open circuit-breaker is reset)\\n### State store configuration probe\\n* An Dapr-compatible state store shall provide an endpoint that answers to configuration probe and returns (among others):\\n* Supported concurrency model\\n* Supported consistency model\\n* A state store instance shall return the specific configuration of the current instance.\\n* It's considered out of scope to require state store to dynamically apply new configurations.\\n### Dapr\\n* Update state store API spec to reflect above decisions\\n* Create backlog of issues to implement above decisions\\n","tokens":62,"id":82}
{"File Name":"dapr\/ARC-003-grpc-protobuf-coding-convention.md","Context":"## Context\\nWe have defined gRPC services and protobuf messages without convention, which results in the duplicated protobuf definitions and inconsistent names of services and messages. Thus, this record defines the minimum-level coding convention for Protobuf message to improve the quality of grpc\/protobuf message definitions.\\n","Decision":"* Use `google.protobuf.Any` data field only if the message field conveys serialized protobuf message with type url. Otherwise, use the explicit data type or protobuf message.\\n* Use `Request` suffix for gRPC request message name and `Response` suffix for gRPC response message name\\n* Do not use `Client` and `Service` suffix for gRPC service name e.g. (x) DaprClient, DaprService\\n* Avoid the duplicated protobuf message definitions by defining the messages in shared proto\\n* Define and use enum type if field accepts only predefined values.\\n","tokens":56,"id":83}
{"File Name":"dapr\/API-004-binding-manifests.md","Context":"## Context\\nAs we rename Event Sources to Bindings, and formally separate State Stores, Message Buses, and Bindings, we need to decide if we need to introduce different manifest types.\\n","Decision":"### Dapr\\n* All components use the same **Component** manifests, identified by a component **type**.\\n* We'll come up with a mechanism to support pluggable secret stores. We'll support Kubernetes native secret store and Azure Key Vault in the initial release.\\n","tokens":40,"id":84}
{"File Name":"dapr\/CLI-002-self-hosted-init-and-uninstall-behaviors.md","Context":"## Context\\nChanges in behavior of `init` and `uninstall` on Self Hosted mode for. Discussed in this [issue](https:\/\/github.com\/dapr\/cli\/issues\/411).\\n","Decision":"* Calling `dapr init` will\\n* Install `daprd` binary in `$HOME\/.dapr\/bin` for Linux\/MacOS and `%USERPROFILE%\\.dapr\\bin` for Windows.\\n* Set up the `dapr_placement`, `dapr_redis` and `dapr_zipkin` containers.\\n* Create the default `components` folder in `$HOME\/.dapr\/bin` for Linux\/MacOS or `%USERPROFILE\\.dapr\\bin` for Windows.\\n* Create the default components configurations for `pubsub.yaml`, `statestore.yaml` and `zipkin.yaml` in the default `components` folder.\\n* Create a default configuration file in `$HOME\/.dapr\/config.yaml` for Linx\/MacOS and `%USERPROFILE%\\.dapr\\config.yaml` for Windows for enabling tracing by default.\\n* Calling `dapr init --slim` will\\n* Install the binaries `daprd` and `placement` in `$HOME\/.dapr\/bin` for Linux\/MacOS and `%USERPROFILE%\\.dapr\\bin` for Windows.\\n* Create an empty default `components` folder in `$HOME\/.dapr\/bin` for Linux\/MacOS or `%USERPROFILE\\.dapr\\bin` for Windows.\\n* Calling `dapr uninstall` will\\n* Remove the folder containing binaries `bin` the default path `$HOME\/.dapr` for Linux\/MacOS and `%USERPROFILE%\\.dapr` for Windows.\\n* Remove the docker dapr_placement if Docker is installed.\\n* Calling `dapr uninstall --all`\\n* Remove the folder containing binaries `bin` the default path `$HOME\/.dapr` for Linux\/MacOS and `%USERPROFILE%\\.dapr` for Windows.\\n* Remove the docker containers dapr_placement, dapr_redis and dapr_zipkin if Docker is installed.\\n* Remove the default folder `$HOME\/.dapr` in Linux\/MacOS and `%USERPROFILE%\\.dapr` in Windows.\\n* CLI on the init command will fail if a prior installtion exists in the default path `$HOME\/.dapr` for Linux\/MacOS and `%USERPROFILE%\\.dapr` for Windows.\\n* **There will no longer be an option for `--install-path` during init or during uninstall.**\\n* The `dapr` CLI by default will expect the `daprd` in `$HOME\/.dapr\/bin` for Linux\/MacOS and `%USERPROFILE%\\.dapr\\bin` for Windows. The command `dapr run` will not expect the `daprd` binary to be in the `PATH` variable, it will launch the binary from the default path.\\n","tokens":41,"id":85}
{"File Name":"dapr\/ARC-004-http-server.md","Context":"## Context\\nGo community has the multiple http server implementations, such as go net\/http, fasthttp, gin, to serve HTTP Restful API. This decision records describes which http server implementation uses in Dapr.\\n","Decision":"* Use [fasthttp server](https:\/\/github.com\/valyala\/fasthttp) implementation because fasthttp offers [the best performance and lowest resource usages](https:\/\/github.com\/valyala\/fasthttp#http-server-performance-comparison-with-nethttp) for the existing HTTP 1.1 server\\n* Use [fasthttpadaptor](https:\/\/godoc.org\/github.com\/valyala\/fasthttp\/fasthttpadaptor) if you need to convert fasthttp request context to net\/http context.\\n","tokens":44,"id":86}
{"File Name":"dapr\/ARC-001-refactor-for-modularity-and-testability.md","Context":"## Context\\nAs we keep building up Dapr features, it becomes apparent that we need to refactor the existing code base to reinforce component modularity. This will improve testability and maintainability in long run. And this refactor also lays the foundation of opening up extensible points (such as Bindings) to the community.\\n","Decision":"### Dapr\\n* Formally separate hosting and API implementations. Hosting provides communication protocols (HTTP\/gRPC) as different access heads to the same Dapr API implementation.\\n* Ensure consistency between gRPC and HTTP interface.\\n* Separate binding implementations to a separate repository.\\n* Use smart defaults for configurable parameters.\\n* Rename Dapr runtime binary from **dapr** to **daprd**.\\n### Non-Dapr\\n* We may consider allowing Dapr to dynamically load bindings during runtime. However, we are not going to implement this unless it's justified by customer asks.\\n* A unified configuration file that includes paths to individual configuration files.\\n* Provide a Discovery building block with hopefully pluggable discovery mechanisms (such as a custom DNS).\\n","tokens":66,"id":87}
{"File Name":"dapr\/SDK-001-releases.md","Context":"## Context\\nDapr exposes APIs for building blocks which can be invoked over http\/gRPC by the user code. Making raw http\/gRPC calls from user code works but it doesn't provide a good strongly typed experience for developers.\\n","Decision":"* Dapr provides language specific SDKs for developers for C#, Java, Javascript, Python, Go, Rust, C++. There may be others in the future\\n- For the current release, the SDKs are auto-generated from the Dapr proto specifications using gRPC tools.\\n- In future releases, we will work on creating and releasing strongly typed SDKs for the languages, which are wrappers on top of the auto-generated gRPC SDKs (e.g. C# SDK shipped for state management APIs with the 0.1.0 release.) This is the preferred approach. Creating purely handcrafted SDKs is discouraged.\\n* For Actors, language specific SDKs are written as Actor specific handcrafted code is preferred since this greatly simplifies the user experience. e.g. The C# Actor SDK shipped with the 0.1.0 release.\\n","tokens":47,"id":88}
{"File Name":"dapr\/API-003-messaging-api-names.md","Context":"## Context\\nOur existing messaging interface names lack of clarity. This review was to make sure messaging interfaces were named appropriately to avoid possible confusions.\\n","Decision":"### Dapr\\n* All messaging APIs are grouped under a **messaging** namespace\/package.\\n* We define three distinct messaging interfaces:\\n- **direct**\\nOne-to-one messaging between two parties: a sender sending message to a recipient.\\n- **broadcast**\\nOne-to-many messaging: a sender sending message to a list of recipients.\\n- **pub-sub**\\nMessaging through pub-sub: a publisher publishing to a topic, to which subscribers subscribe.\\n* We distinguish message and direct invocation. For messaging, we guarantee at-least-once delivery. For direct invocation, we provide best-attempt delivery.\\n","tokens":30,"id":89}
{"File Name":"dapr\/API-011-state-store-api-parity.md","Context":"## Context\\nWe reviewed parity of state store APIs .\\n","Decision":"* GetState APIs continue to have Single Key Get and Bulk Get APIs behaviour as current 0.10.0 version.\\n* SaveState API will continue to have one SaveState API endpoint. If user wants to save single key, same save state API will be used\\nfor passing single item in the bulk set.\\nPotential issues arises if following new single key save state API is introduced:\\n`Post : state\/{storeName}\/{key}`\\nThis will conflict with\\n- State Transaction API, if the key is \"transaction\"\\n- GetBulkState API, if the key is \"bulk\"\\nSo the decision is to continue the Save State API behaviour as current 0.10.0 version.\\n* Bulk Delete API might come in future versions based on the scenarios.\\n","tokens":13,"id":90}
{"File Name":"dapr\/API-007-tracing-endpoint.md","Context":"## Context\\nWe now support distributed tracing across Dapr sidecars, and we inject correlation id to HTTP headers and gRPC metadata before we hand the requests to user code. However, it's up to the user code to configure and implement proper tracing themselves.\\n","Decision":"We should consider adding a tracing endpoint that user code can call in to log traces and telemetries.\\n","tokens":53,"id":91}
{"File Name":"nicoprj\/adr-001.md","Context":"## Context\\nWhile we were considering how to ship the CSSK, the question of how it\\nshould be organized came up several times. The crux of the issue was\\nwhether the more reusable parts (for example, the event dispatch bits)\\nshould be separated into a library, or whether it should all just live\\nin one big pile of code. Currently, the CSSK is organized in the\\nlatter manner.\\nSeparating the infrastructure code out into a separate library would\\nmake it more obvious which parts of the code are intended to be edited\\nby the developer. Right now, it\u2019s not immediately obvious where to\\nmake changes, and what bits are there to support the development\\nexperience.\\nSeparating the infrastructure code out into a separate library would\\nalso make it far more inconvenient to make changes to it. Developers\\nwould need to update the lib, possibly creating their own fork, and\\nthen ensure that the app code was using the appropriate version. This\\nis extremely likely to happen because the infrastructure code is\\nneither comprehensive nor mature.\\n","Decision":"We have decided to keep it all together, but clearly separate the\\n\u201clibrary\u201d code from the \u201capp\u201d code by using separate directories for\\nthose two types of code: \u201clib\u201d and \u201capp\u201d.\\n","tokens":219,"id":92}
{"File Name":"nucleus\/0002-push-data-changes-to-github.md","Context":"## Context\\nThe API for publishing release notes is not advanced and is just a giant blob of JSON containing every release in the database. A GitLab Job runs on a schedule and reads this blob, splits it into a file per release, and commits those changes to a GitHub repo. This job is slow and is something else to maintain and monitor separate from Nucleus. So the decision was between improving the API to only send the releases that had changed since the last sync, or to push changes to GitHub as soon as the're made. The latter has the advantages of happening very quickly after the change is saved, and having the context of the Nucleus user who made the change which can also be recorded in the Git commit.\\n","Decision":"We've decided to go with pushing changes directly to GitHub via the GitHub API and using an async worker system to do it. The async system chosen was [Spinach][].\\n","tokens":149,"id":93}
{"File Name":"nucleus\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":94}
{"File Name":"CopperBend\/D002_unify_command_flow_for_player_and_creatures.md","Context":"## Context\\nThe 'core mechanic' of this game is the player being in control of allied plants.  There may be other cases of player control of other in-game entities as dev continues, but this is core.\\nGiving creatures and the player character pluggable command\/control is more dev overhead.  As a spare time project, any choice for extra complexity deserves extra scrutiny.  On the same wave, though, the outcomes which can justify the extra effort are a larger set:  Not just important functionality, but significant learning and straight pleasure in craftsmanship can be enough reason.\\nThis offers options for reuse.\\nIf our guy is terrified, the player's InputCommandSource can be temporarily replaced with a FleeingCommandSource (or some such).  When the player is in charge of other entities, they get an InputCommandSource themselves, and will naturally be controlled by the player when their moments to act arrive in the schedule.\\n","Decision":"I'll do this.  It feels correct, and it trends toward the sort of wins and frustrations I'm hoping for in this project.  That is, where my decisions, rather than the guts of frameworks, are what I'm wrestling most often.\\n","tokens":191,"id":95}
{"File Name":"CopperBend\/D001_keep_ADRs_in_git_and_freely_reformat.md","Context":"## Context\\nI feel that ADRs in some form may be very beneficial at Envisage, but two things make me reluctant to push the practice right now.  First, ignorance--I've never done this, so I can't talk personally about the experience.  Second, the enthusiasm of first encounter.  Both of these are addressed by actually doing it for a while.\\nCopper Bend is a learning\/entertainment project.  Trying new techniques and technologies is a primary goal.\\nThe Envisage products are very different from this one.  What works well here won't necessarily cross over.\\n","Decision":"I will avoid tooling, and simply handcraft the documents.  I'll keep them in the main project repo.  I'll review periodically, trimming the present-but-useless and adding the missing-yet-promising.\\nI won't worry about synchronizing the format between ADR docs.\\n","tokens":125,"id":96}
{"File Name":"nozama\/adr-001-simulator-webserver.md","Context":"## Context\\nSince we decided to design the simulator as a separated component we didn't thought how it would communicate with the WebApp. So, the simulator is a simple java program that can't do anything to communicate with Nozama since it is a web application and has a different ecosystem (spring framework).\\n","Decision":"As of now we decided to create a simple webserver to expose simulator to our main WebApp. As it main functionality is to just pass some data when required and send notifications to Nozama's backend when some task is done.\\n","tokens":63,"id":97}
{"File Name":"react\/adr-XXX-file-structure.md","Context":"## Context\\nComponents might be more grokable if they were structured consistently. This ADR proposes conventions\\n","Decision":"TL;DR:\\n```\\nprimer-react\/\\n\u251c\u2500 src\/\\n\u2502  \u251c\u2500 Breadcrumbs\/\\n\u2502  \u2502  \u251c\u2500 index.ts                    \/\/ Just re-exporting?\\n\u2502  \u2502  \u251c\u2500 Breadcrumbs.tsx             \/\/ Primary component\\n\u2502  \u2502  \u251c\u2500 BreadcrumbsItem.tsx         \/\/ Subcomponent (include parent component name to increase findability in most IDEs)\\n\u2502  \u2502  \u251c\u2500 Breadcrumbs.mdx             \/\/ Documentation. Always .mdx, not .md\\n\u2502  \u2502  \u251c\u2500 Breadcrumbs.stories.tsx\\n\u2502  \u2502  \u251c\u2500 Breadcrumbs.test.tsx        \/\/ Unit tests\\n\u2502  \u2502  \u251c\u2500 Breadcrumbs.types.test.tsx  \/\/ Type tests\\n\u2502  \u2502  \u251c\u2500 Breadcrumbs.yml             \/\/ Component metadata (Possible future)\\n\u2502  \u2502  \u2514\u2500 __snapshots__\/\\n\u2506  \u2506\\n```\\n### Rules\\n- Every component should have its own PascalCased directory directly under `src\/`\\n- Subcomponents should be properties of the exported parent component (e.g., `Breadcrumbs.Item`)\\n- Replacements of existing components should use an incrementing number (e.g., `Breadcrumbs2` rather than `NewBreadcrumbs`)\\n","tokens":22,"id":98}
{"File Name":"react\/adr-003-prop-norms.md","Context":"## Context\\nOur component prop APIs have, at times been a bit of a mess. We've seen:\\n- Implicit conventions not documented anywhere but consistently reflected in our code (e.g., the type of the `sx` prop)\\n- Explicit plans to change some of those (e.g., the deprecation of Styled System props)\\n- Inconsistencies in our implementation (e.g., when components accept a `ref` prop)\\nThis ADR aims to unify some of these conversations about prop APIs, codify our decisions, and sequence the work to get there.\\n","Decision":"### \ud83d\udfe2 `sx`\\nAll components that ultimately render to the DOM should accept an `sx` prop.\\nThe `sx` prop (of type `SystemStyleObject`) should generally set styles for the root HTML element rendered by the component. An exception would be components like `<Dialog>`, whose outermost HTML element is a backdrop. In that case, it would be appropriate for `sx` styles to apply to child of the backdrop that is more likely to need styling overrides.\\n### \ud83d\udfe2 `ref`\\nAll components that ultimately render to the DOM should accept a `ref` prop. That `ref` prop should most often be passed to the root HTMLElement rendered by the component, although occasionally a different descendent node may make more sense.\\nSee also: [Discussion on `ref` props (internal)](https:\/\/github.com\/github\/primer\/discussions\/131)\\n### \ud83d\udfe1 `as`\\nOnly components with a clear need for polymorphism should accept an `as` prop. Reasonable cases include:\\n- Components that need functionality from the component passed to the `as` prop, like a `<Button>` that renders a React Router link.\\n- Components whose accessibility are improved by using semantically appropriate HTML elements, like an ActionList\\nWhen a Primer component user passes an `as` prop to a component, it should be done in a way that is consistent with the component\u2019s intended use. In some situations we can enforce that with a narrowed type for our `as` prop.\\nSee also: [Discussion on `as` props (internal)](https:\/\/github.com\/github\/primer\/discussions\/130)\\n### \ud83d\udfe1 DOM props: Limited\\nAll components that accept an `as` prop should accept props en masse for the element specified by the `as` prop (excluding props of the same name already used by the component). _Additionally_, some other elements that do _not_ accept an `as` prop should accept the props for their root HTML element when those props are fundamental to the component\u2019s function (e.g., `<TextInput>` should accept DOM props for its underlying `<input>`).\\n### \ud83d\udd34 Styled System props\\nComponents should not accept Styled System props (except our utility components: `Box` and `Text`)\\n_Reasoning:_ Utility components are meant to provide a convenient API for writing styles (including styles that reference theme and other context managed within Primer). Non-utility components implement specific design patterns where additional styling is available for exceptional cases.\\nSee also: [Discussion on the deprecation of styled-system props (internal)](https:\/\/github.com\/github\/primer\/discussions\/132)\\n### \ud83d\udd34 `theme`\\nComponents should not accept a `theme` prop (with the exception of `ThemeProvider`).\\n_Reasoning:_ The `theme` prop doesn't enable anything that can't be done with `<ThemeProvider>`, and promotes the anti-pattern of per-component theme overrides.\\n### `children`\\nI'm intentionally withholding advocacy about `children` prop types because I expect that topic will be covered by a future ADR.\\n### Sequencing\\n1. Deprecate remaining unwanted Styled System props (should be done? Let's verify.)\\n1. Release an eslint rule to disallow Styled System props\\n1. Release an eslint rule to disallow `theme`\\n1. Migrate all usage within PRC\\n1. Assist GitHub projects with migration\\n1. Remove support for unwanted props\\n1. Update docs to reflect the standards in this ADR\\n","tokens":120,"id":100}
{"File Name":"react\/adr-001-typescript.md","Context":"## Context\\nPrimer React components was originally released without TypeScript type definitions, making it difficult for engineers to consume the library in TypeScript applications. In [July 2019](https:\/\/github.com\/primer\/react\/commit\/2983c935ea9ad600c04078adb25e40c3624c11fa#diff-7aa4473ede4abd9ec099e87fec67fd57afafaf39e05d493ab4533acc38547eb8), we created an [ambient declaration](https:\/\/www.geeksforgeeks.org\/typescript-ambients-declaration\/) file (`index.d.ts`) file to provide type definitions for TypeScript applications without having to rewrite Primer React components in TypeScript.\\n`index.d.ts` has been an effective stopgap, enabling teams to build complex applications with Primer React components and TypeScript. However, because `index.d.ts` is disconnected from the implementation code, we've struggled to keep the type definitions up-to-date and accurate, as evidenced by [many](https:\/\/github.com\/primer\/react\/issues\/906) [TypeScript](https:\/\/github.com\/primer\/react\/issues\/540) [bug](https:\/\/github.com\/primer\/react\/issues\/520) [reports](https:\/\/github.com\/primer\/react\/issues\/534). As the library continues to grow in size and complexity, manually maintaining type definitions will become unsustainable.\\n","Decision":"We will rewrite Primer React components in TypeScript.\\n","tokens":284,"id":101}
{"File Name":"kitsune\/0002-es-l10n-content.md","Context":"## Context\\nKitsune supports many locales,\\nand has content which we want to be searchable in those locales.\\nElasticsearch has support for many language-specific analyzers:\\nhttps:\/\/www.elastic.co\/guide\/en\/elasticsearch\/reference\/7.9\/analysis-lang-analyzer.html\\nSearch v1 used per-document analyzers,\\nthat is to say, within the same index:\\n```\\ndoc_1: { \"content\": \"Hello world\" }\\ndoc_2: { \"content\": \"Hallo Welt\" }\\n```\\n`doc_1.content` could be analyzed using an english analyzer,\\nand `doc_2.content` could be analyzed using a german analyzer.\\nWell before version 7 ES removed this feature,\\nand now all fields of the same name across an index must be analyzed the same,\\nso we must take a different approach with the current Search implementation.\\nWe can either place separate locales in their own index,\\nand set up locale-specific analyzers for the same field name across indices.\\nOr we can keep separate locales within the same index,\\nand define unique field names for each field which needs to be analyzed under a specific locale.\\n","Decision":"Heavily influenced by: https:\/\/www.elastic.co\/blog\/multilingual-search-using-language-identification-in-elasticsearch\\nWe will store all documents within the same index and use an Object field for fields which need to use locale-specific analyzers.\\nWe will call this field `SumoLocaleAwareTextField` and will have a key for each locale,\\nwith the appropriate analyzer defined on that key,\\nsuch that:\\n```\\ndoc_1: { \"content\": { \"en-US\": \"Hello world\" }}\\ndoc_2: { \"content\": { \"de\": \"Hallo Welt\" }}\\n```\\n`doc_1.content.en-US` is analyzed using an english analyzer,\\nand `doc_2.content.de` is analyzed using a german analyzer.\\n","tokens":249,"id":102}
{"File Name":"kitsune\/0003-es-aaq-documents.md","Context":"## Context\\nAs we are re-implementing our search in ElasticSearch v7,\\nwe must re-implement Ask a Question (AAQ) search.\\nThere is one primary use-case for storing AAQ documents in ES which Search v1 supports,\\nwhich we must continue to be able to do in the redesigned Search:\\nsearching for an AAQ thread as a unit.\\nThere are other secondary use-cases which we may want to support when storing AAQ documents in ES.\\nA non-exhaustive list of these are:\\n-   Searching within AAQ threads\\n-   Searching within questions and their solutions\\n-   Aggregating answers to create contribution data\\nWe also want search to be _fast_,\\nso should model our data to avoid nested fields and parent-child relationships,\\nand use de-normalization wherever possible:\\nhttps:\/\/www.elastic.co\/guide\/en\/elasticsearch\/reference\/7.9\/tune-for-search-speed.html#_document_modeling\\n","Decision":"We will model our data in ES based on what makes most sense for our expected use-cases,\\nand what will make those fast and efficient,\\nrather than feeling like we must have a 1:1 copy of our data structure in our database.\\nIn this vein, we will use a structure of two document \"types\" within one index,\\n`QuestionDocument` and `AnswerDocument`,\\nwhere a `QuestionDocument` will exist for each `Question` which exists in the database,\\nand an `AnswerDocument` will exist for each `Answer` which exists in the database.\\n`AnswerDocument` will be a subclass of `QuestionDocument` so will inherit all of its fields,\\nand we will set the value of those fields to the value of the `Question` associated with its `Answer`.\\nFor instance, if in database:\\n```\\nanswer.created => 2020-10-27\\nanswer.question.created => 2020-11-01\\n```\\nin elastic:\\n```\\nanswer_document.created => 2020-10-27\\nanswer_document.question_created => 2020-11-01\\n```\\n`QuestionDocument` will also have an `answer_content` field,\\nwhich contains the content of all a Question's Answers.\\nWe will set this to null in the `AnswerDocument`.\\n","tokens":205,"id":103}
{"File Name":"kitsune\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":104}
{"File Name":"Nosedive\/0006-use-of-migratus-library.md","Context":"## Context\\nDirect form the [jdbc documentation](http:\/\/clojure-doc.org\/articles\/ecosystem\/java_jdbc\/home.html)\\nAnother common need with SQL is for database migration libraries. Some of the more popular options are:\\n* Drift\\n* Migratus\\n* Ragtime\\n","Decision":"After a quick read of the documentation, and see that all solutions are similar, I select Migratus, by the comodity of have a lein plugin\\n","tokens":60,"id":105}
