{"File Name":"cdh-adrs\/0010-etl_tenancy_relationship.md","Context":"## Context\\nThe PriveXMl results generated by ETL on the processing of data feed files must be capable of categorization based on specific tenants (Company, Client & Account) data. Due to the fact such data will change overtime based on client requests, every sections of PriveXML Contents (Ticker + Position + Transaction) section should generally be able to be connected to giving tenant. But using just the tenant Id or primary key as a relationship marker isn't suitable, as during migration and over-all during the life of our archives, seperation of the tenant changes and the PriveXML events will import insertion but increase complexity on the read\/consumption end as we may need to re-read all changes for tenant before being able to consume the PriveXML to generate current state.\\n","Decision":"We increase complexity during insertion by including all necessary tenant information as part of a PriveXML event and also provide event objects which could only just contain specific changes to client information within the same topic. This then enforces us to think of this topic as a general topic for ETL which may contain an event to process a new PriveXML result or process tenant data change.\\nMore so, the topics for each tenant must be tenant specific due to necessary policies and requirements to create seperation of data on physical levels.\\n","tokens":160,"id":0}
{"File Name":"cdh-adrs\/0011-pentaho.md","Context":"## Context\\nWe currently have certain specific feeds being parsed using pentaho, these are legacy feeds which transform data feed files into PriveXMl format. These feed parsers will require migration into new CDH ETL feeds parsers based on defined standard library.\\n","Decision":"Due to limited resources and time constraints, we will defer the feed migration from pentaho to the CDH ETL till adequate human resources are available, more so an indept understanding of exactly how the parsers work is required, which will require talking with Christopher on this.\\n","tokens":52,"id":1}
{"File Name":"cdh-adrs\/0009-etl_feed_synchronization_and_delivery.md","Context":"## Context\\nCurrently, we move files around in S3 from Archive folders into bulkupload folders which is cumbersome and error prone,\\nmore so this is being done to allow us trigger data feed processing from the dev admin.\\nWe need a means of reducing the flow steps to go from files to processing and reduce surface level access for the ETL team.\\n","Decision":"- ETL will have a dedicated database tables which will serve the need to both register and retrieve data feed files in the\\nmost optimized manner necessary on a per feed basis. This database tables will be appropriately updated by the ETL service\\nbased on update events from either the uploader service, or the windows-specific uploader jars.\\nThe following operations then becomes possible:\\n1. Ability to query for specific files based on upload date (the date it was uploaded to S3).\\n2. Ability to query for a specific batch of files related to specific data feeds.\\n3. Ability to get data files specific to a given owner and\/or provider.\\n4. Ability to get data files specific to a given provider.\\n- Due to we creating a database to optimize a query to retrieve and manage data feed files, then we require a way to keep\\nthe ETL database tables up to date with new files from uploader service. Therefore, the ETL system will listen for events coming\\nfrom both a lambda function which will be called by S3 when new files are added to the specific bucket, and the new uploader\\nservice which will house necessary logic for retrieving such data feed files from their sources. Once all uploading logic\\nhave being migrated to the uploader, we will de-commission the lambda function and directly have the uploader service inform the\\nETL service as regards new files.\\nSee\\n![Data Feed Delivery](..\/assets\/images\/workflows\/image8.png)\\n","tokens":73,"id":2}
{"File Name":"cdh-adrs\/0004-cdh-tenants.md","Context":"## Context\\nMonolith will undergo segmentation where we plan to pull out the Client Data Hub out of the monolith as a separate service for better scaling and API management. The functionality of the client data hub to provided tenant, transaction and position related data will stay the same, but the data feed parsing within it's perview will also be moved out into an [ETL Service](.\/etl.md).\\nThe client data hub provides the following services:\\n1. Delivery of tenant information (Accounts, Company, and Client data).\\n2. Delivery of Tenant Transaction (Data Feed and Systems) for specified periods of time.\\n3. Delivery of Tenant Positions for their related transactions for porfolio tracking.\\n4. Processing of reconciliation request on tenant accounts and positions.\\nThis requires us to consider migration procedures for moving existing data from the monolith into the new CDH and ETL related databases.\\n","Decision":"Create a specific services (ETL and CDH) where an ETL service and CDH service will be responsible for the processing of delivered PriveXML for consumption and delivery of client transactions + positions into data tables. Both services will communicate across a pubsub event bus.\\nThe following is expected:\\n- CDH will expose an API by which it will handle all tenant and transaction related requests from monolith.\\n- CDH will listen on specified event topic on message bus from which all tenant update requests will be received as events\\n- CDH will publish to the monolith events on updated tenant data by which monolith will update it's records.\\n- CDH will have all tenant data moved from monolith into it's control (to be discussed in CDH Tech).\\n- CDH will consume all PriveXML events to update it's records of transactions + tickers + positions.\\n![CDH Architecture](..\/assets\/images\/aab_workshop\/aab_5.jpg)\\n","tokens":183,"id":3}
{"File Name":"cdh-adrs\/0006-etl_cdh_communication.md","Context":"## Context\\nWe wish to segment into separate processes where the data feed files processing is handled by the ETL service and the CDH service is reponsible for consuming these produced output which then are materialized into records which is used in response to request to the CDH service. This means ETL service must be able to communicate to the CDH service loosely without direct connection or dependence between either.\\n","Decision":"We have chosen an event based communication where the CDH and ETL service communicate results between each other over an event queue based on specified topics (deployed onsite within geozone of CDH and ETL services).\\n![Event Queue](..\/assets\/images\/workflows\/image3.png)\\n","tokens":81,"id":4}
{"File Name":"cdh-adrs\/0005-etl-monolith-migration.md","Context":"## Context\\nCurrently, the parsing process for datafeeds is still part of the existing monolith codebase, which brings it's own share of problems with scaling and feed parsing managemnent. Monolith also has a hard dependency on AWS S3 which must be broken out to allow us specfically move these dependencies outside as they are not relatively core to the ETL processing architecture but are generally how we organize and move input into desired place for access and delivery.\\nA hard requirement for the ETL service is the ability to ensure processed datafeed consistently have relational information with their tenants as specific feeds\\nhave specific constraints on how they are accessed and stored.\\nAnother hard requirements is to organize how data feed files are accessed and retrieved from S3, this currently has issues due to the need to directly access S3, and move files and directories into the bulkupload directories for processing by the monolith. Whilst the alternative of storing both file, metadata and file binary into the database simplifies these access issues, we create more problems in the management of the database files (without using FileStream optimization in SQLServer), increasing cost of backup and replication.\\n","Decision":"ETL will be moved into an external service of it's own with the following responsibilities:\\n- Embodiment of all parser logic.\\n- Delivery of agreed parser format (currently PriveXML) into message queues.\\n- Standardized library for parsing delivery and logic.\\n- Standardized database tables for data feed file delivery and access.\\n- Standardized database tables for tenant data (Company, Client, Accounts).\\n- Creates tenant specific events for delivery for tenant specific datafeed.\\nAs regards data feed file access problem\\n- Manage synchronization of uploaded files events into database from uploader service.\\n","tokens":233,"id":5}
{"File Name":"cdh-adrs\/0001-feed-parsers.md","Context":"## Context\\nWe need a clearly defined way for handling data feed files for processing, what are the expected inputs and outputs from the parsers and how will this feed into the whole parsing process for ETL ?\\n","Decision":"1. A EAM Parser Factory: this produces a content reader which will be used by all written parsers for reading the contents of a giving data source.\\n1. A Processing Adapter per EAM data feed type which has registered different parsers which handle the retrieval of different types of data out of giving data feed source (e.g CreditSuisse XML).\\n1. Custom Data Extractors (e.g IncomeCashFlowParsers, OrderBroker) which are responsible for extracting different data types from the ContentReader, these are then accumulated by the data feed ProcessingAdapter into a unified format which can be transformed into portions of the expected PriveXML format.\\n1. The custom data extractors will have rights to define specific errors for their data extraction process and how that will affect that specific extraction or for a giving set of files. We will have errors which may be critical and cause immediate failure or which can be considered non-critical and only stop giving feed extraction or ensure it is logged and continued from. The key is that such details should not be the responsibility of the core and as far as only specific errors which the core is concerned with towards stopping immediately for that giving source or a set of sources.\\n![Target Parser Flow](..\/assets\/images\/workflows\/image1.png)\\n","tokens":43,"id":6}
{"File Name":"cdh-adrs\/0003-uploaders.md","Context":"## Context\\nMonolith and a few deployed jars handling delivery of data feed files into S3 for the existing parsing processes powering the monolith client data hub system, this increases cost on the monolith's systems which require more vertical scaling of resources to manage, more so, due to the monolith lock, any fix or update is locked to the monolith release SDLC.\\nConsidering these functions serve to move files from source to destination we need to migrate them as external services to both CDH and the monolith, these are then bundled into a single service responsible for the delivery of new data feed files into the S3 Archives and CDH feed data stores.\\n","Decision":"Migration of all monolith related uploading logic into external service which is responsible for the timely retreival, delivery and storage of data feed files from their respective sources. The service is responsible for ensuring the ETL service database is always up to date, by deliverying events on file additions into the archive storage regardless of what storage is being used by event delivery.\\n![Data Feed Delivery](..\/assets\/images\/workflows\/image8.png)\\n","tokens":135,"id":7}
{"File Name":"cdh-adrs\/0002-cdh-etl-logic-boundaries.md","Context":"## Context\\nTo ensure a clear separation as regards what logic resides within CDH and ETL related services where there exists feed specific\\nrequirements during onboarding and processing of feed files, the following issues where considered:\\n- Will such logic require specialized implementation across feeds?\\n- Are such logic generic and require one time implementation or will require continous change\/update?\\n- What are the benefits of moving such logic into ETL instead of CDH.\\n","Decision":"The most important point agreed on was that CDH will remain focused on defined object and data models as possible and CDH will\\nrun with the expectation that all inputs received are completed. This means CDH should not have domain specific knowledge in regards\\nspecific intricacies about how specific feeds are reconcilied into complete Positions, Transactions and Ticker data.\\nSuch specificity will reside within the ETL service and be housed based on each feed parsing logic within the ETL service.\\nThe benefits of such a system is that only ETL needs to change to accomodate new and changing requirements of old and new feeds\\nensuring the final result is always consistent to march what the CDH service requires.\\n","tokens":92,"id":8}
{"File Name":"cdh-adrs\/0008-etl_feed_parsers.md","Context":"## Context\\nWe need a clearly defined way for handling data feed files for processing, what are the expected inputs and outputs from the parsers and how will this feed into the whole parsing process for ETL ?\\n","Decision":"1. A EAM Parser Factory: this produces a content reader which will be used by all written parsers for reading the contents of a giving data source.\\n1. A Processing Adapter per EAM data feed type which has registered different parsers which handle the retrieval of different types of data out of giving data feed source (e.g CreditSuisse XML).\\n1. Custom Data Extractors (e.g IncomeCashFlowParsers, OrderBroker) which are responsible for extracting different data types from the ContentReader, these are then accumulated by the data feed ProcessingAdapter into a unified format which can be transformed into portions of the expected PriveXML format.\\n1. The custom data extractors will have rights to define specific errors for their data extraction process and how that will affect that specific extraction or for a giving set of files. We will have errors which may be critical and cause immediate failure or which can be considered non-critical and only stop giving feed extraction or ensure it is logged and continued from. The key is that such details should not be the responsibility of the core and as far as only specific errors which the core is concerned with towards stopping immediately for that giving source or a set of sources.\\n![Target Parser Flow](..\/assets\/images\/workflows\/image1.png)\\n","tokens":43,"id":9}
