{"File Name":"beis-report-official-development-assistance\/0023-use-docker-hub-in-deployments.md","Context":"## Context\\nOur CI\/CD pipeline uses containers (Docker) as does our hosting platform (GOVUK\\nPaaS), we need a way to store built images from our pipeline so our hosting\\nplatform can access and deploy them.\\nDocker hub is one solution offered by the maker of Docker itself.\\n","Decision":"Use [Docker hub](https:\/\/hub.docker.com\/) to store built deployment container\\nimages to facilitate continuous delivery.\\nHost the built container images on the dxw Docker hub account.\\n","tokens":65,"id":2392,"text":"## Context\\nOur CI\/CD pipeline uses containers (Docker) as does our hosting platform (GOVUK\\nPaaS), we need a way to store built images from our pipeline so our hosting\\nplatform can access and deploy them.\\nDocker hub is one solution offered by the maker of Docker itself.\\n\n\n##Decision\nUse [Docker hub](https:\/\/hub.docker.com\/) to store built deployment container\\nimages to facilitate continuous delivery.\\nHost the built container images on the dxw Docker hub account.\\n"}
{"File Name":"radiant-mlhub\/0003-will-not-implement-get-items.md","Context":"## Context\\nThe Radiant MLHub API implements the `\/items` endpoint as described in the [STAC API - Features](https:\/\/github.com\/radiantearth\/stac-api-spec\/tree\/master\/ogcapi-features)\\ndocumentation for retrieving the STAC Items associated with a given Collection. Since this is a paginated endpoint with an opaque next token,\\npages of items must be retrieved sequentially. For very large datasets and collections, this means that retrieving all items in a collection\\nmay require hundreds or thousands of API requests and can be very slow. Additionally, the spec does not provide a mechanism for determining the\\ntotal number of items in a collection, which precludes us from showing overall progress when looping over or retrieving items.\\n","Decision":"To avoid a confusing user experience when working with Items, and to avoid inadvertently swamping the API with requests, we will not provide\\na method in either the low-level client or on the `Collection` classes to loop over the items in a collection. Preliminary work had adapted\\nthe [`Collection.get_items`](https:\/\/pystac.readthedocs.io\/en\/latest\/api.html#pystac.Catalog.get_items) method to make paginated requests to\\nthe `\/items` endpoint. Instead, this method will raise a `NotImplementedError` to indicate that this feature is not available.\\n*Work is planned to add an endpoint to the Radiant MLHub API to enable downloading a single archive containing all items associated with a\\nCollection. Support for this endpoint in the Python client may be the subject of a separate ADR.*\\n","tokens":153,"id":703,"text":"## Context\\nThe Radiant MLHub API implements the `\/items` endpoint as described in the [STAC API - Features](https:\/\/github.com\/radiantearth\/stac-api-spec\/tree\/master\/ogcapi-features)\\ndocumentation for retrieving the STAC Items associated with a given Collection. Since this is a paginated endpoint with an opaque next token,\\npages of items must be retrieved sequentially. For very large datasets and collections, this means that retrieving all items in a collection\\nmay require hundreds or thousands of API requests and can be very slow. Additionally, the spec does not provide a mechanism for determining the\\ntotal number of items in a collection, which precludes us from showing overall progress when looping over or retrieving items.\\n\n\n##Decision\nTo avoid a confusing user experience when working with Items, and to avoid inadvertently swamping the API with requests, we will not provide\\na method in either the low-level client or on the `Collection` classes to loop over the items in a collection. Preliminary work had adapted\\nthe [`Collection.get_items`](https:\/\/pystac.readthedocs.io\/en\/latest\/api.html#pystac.Catalog.get_items) method to make paginated requests to\\nthe `\/items` endpoint. Instead, this method will raise a `NotImplementedError` to indicate that this feature is not available.\\n*Work is planned to add an endpoint to the Radiant MLHub API to enable downloading a single archive containing all items associated with a\\nCollection. Support for this endpoint in the Python client may be the subject of a separate ADR.*\\n"}
{"File Name":"codecharta\/2017-01-02-ADR_5_pick_analysis_cli_library.md","Context":"# Context\\nAs noted in [ADR-4]({% post_url adr\/2017-01-02-ADR_4_decide_analysis_architecture %}) CodeCharta will use a pipes and filters architecture.\\nBut it has not been defined how the user will interact with the analysis.\\n# Status\\naccepted\\n# Decision\\nCodeCharta Analysis will be a set of command-line tools that use [PicoCli](https:\/\/picocli.info\/). PicoCli is small, powerful, regularly updated and works great in combination with Kotlin.\\n# Consequences\\n-   A cli is not as intuitive as a well-structured graphical user interface would have been.\\n","Decision":"CodeCharta Analysis will be a set of command-line tools that use [PicoCli](https:\/\/picocli.info\/). PicoCli is small, powerful, regularly updated and works great in combination with Kotlin.\\n# Consequences\\n-   A cli is not as intuitive as a well-structured graphical user interface would have been.\\n","tokens":141,"id":1663,"text":"# Context\\nAs noted in [ADR-4]({% post_url adr\/2017-01-02-ADR_4_decide_analysis_architecture %}) CodeCharta will use a pipes and filters architecture.\\nBut it has not been defined how the user will interact with the analysis.\\n# Status\\naccepted\\n# Decision\\nCodeCharta Analysis will be a set of command-line tools that use [PicoCli](https:\/\/picocli.info\/). PicoCli is small, powerful, regularly updated and works great in combination with Kotlin.\\n# Consequences\\n-   A cli is not as intuitive as a well-structured graphical user interface would have been.\\n\n\n##Decision\nCodeCharta Analysis will be a set of command-line tools that use [PicoCli](https:\/\/picocli.info\/). PicoCli is small, powerful, regularly updated and works great in combination with Kotlin.\\n# Consequences\\n-   A cli is not as intuitive as a well-structured graphical user interface would have been.\\n"}
{"File Name":"iampeterbanjo.com\/0004-yarn-npm-pnpm-and-docker.md","Context":"## Context\\nI started off using `yarn` workspaces because it was convenient for running scripts inside packages using `--cwd`. But it has no `audit fix` and deployments started failing because of missing packages.\\n```shell\\n2019-08-09T07:25:06+01:00 code: 'MODULE_NOT_FOUND',\\n2019-08-09T07:25:06+01:00 at Module.require (internal\/modules\/cjs\/loader.js:683:19) {\\n2019-08-09T07:25:06+01:00 at Module.load (internal\/modules\/cjs\/loader.js:643:32)\\n```\\nI tried [changing node engines][node-issue] which failed. I switched to `pnpm` because it had a reputation for solving dependency issues but I ran into problems with the way Hapi and its dependencies get [dynamically imported][hapi-issue].\\nIn order to use `pnpm` I had to use Docker deployments because [Clever cloud][clever-cloud] only support `npm` and `yarn` package managers. I could not get `pnpm` to [work with Docker][docker-issue] (recursive installs kept failing) but `npm` worked fine. I decided to keep Docker for learning and portablity purposes.\\n","Decision":"In the context of deployments failing because of missing packages. And facing the concern of wanting to have this projects' dependencies reliably installed in production, I've adopted `npm` as the default package manager. I accept that I will have to write more scripts to maintain the monorepo structure. Docker is no longer necessary but is convenient for portability and learning.\\n","tokens":277,"id":1592,"text":"## Context\\nI started off using `yarn` workspaces because it was convenient for running scripts inside packages using `--cwd`. But it has no `audit fix` and deployments started failing because of missing packages.\\n```shell\\n2019-08-09T07:25:06+01:00 code: 'MODULE_NOT_FOUND',\\n2019-08-09T07:25:06+01:00 at Module.require (internal\/modules\/cjs\/loader.js:683:19) {\\n2019-08-09T07:25:06+01:00 at Module.load (internal\/modules\/cjs\/loader.js:643:32)\\n```\\nI tried [changing node engines][node-issue] which failed. I switched to `pnpm` because it had a reputation for solving dependency issues but I ran into problems with the way Hapi and its dependencies get [dynamically imported][hapi-issue].\\nIn order to use `pnpm` I had to use Docker deployments because [Clever cloud][clever-cloud] only support `npm` and `yarn` package managers. I could not get `pnpm` to [work with Docker][docker-issue] (recursive installs kept failing) but `npm` worked fine. I decided to keep Docker for learning and portablity purposes.\\n\n\n##Decision\nIn the context of deployments failing because of missing packages. And facing the concern of wanting to have this projects' dependencies reliably installed in production, I've adopted `npm` as the default package manager. I accept that I will have to write more scripts to maintain the monorepo structure. Docker is no longer necessary but is convenient for portability and learning.\\n"}
{"File Name":"gsp\/ADR035-aurora-postgres.md","Context":"## Context\\nOur service operator will be responsible for providing Postgres along any other\\nservices to our end users.\\nIn this case, the ask is specifically Postgres, meaning we can pick our\\nsolution for providing the instance as long as it exposes the correct APIs.\\nA reasonable set of candidates are:\\n- AWS RDS Postgres\\n- AWS RDS Aurora Postgres\\n### AWS RDS Postgres\\nA solution most comonly used across the board for databases. Is managed and\\nmaintained by AWS, provides Backups and Snapshots.\\n### AWS RDS Aurora Postgres\\nHas the benefits of AWS RDS Postgres, and some more benefits. Such as:\\n- Scalable persistance storage\\n- IAM authentication\\n- Automatic Failover with multi AZs\\n- Faster read loads\\n- Slightly cheaper\\nAlso has few downsides:\\n- Not recommended for heavy write systems (Twitter big)\\n- Slightly behind version wise\\n","Decision":"We will continue with Aurora as we we don't have any specific requirements not\\nto and can benefit from the solution.\\n","tokens":198,"id":3897,"text":"## Context\\nOur service operator will be responsible for providing Postgres along any other\\nservices to our end users.\\nIn this case, the ask is specifically Postgres, meaning we can pick our\\nsolution for providing the instance as long as it exposes the correct APIs.\\nA reasonable set of candidates are:\\n- AWS RDS Postgres\\n- AWS RDS Aurora Postgres\\n### AWS RDS Postgres\\nA solution most comonly used across the board for databases. Is managed and\\nmaintained by AWS, provides Backups and Snapshots.\\n### AWS RDS Aurora Postgres\\nHas the benefits of AWS RDS Postgres, and some more benefits. Such as:\\n- Scalable persistance storage\\n- IAM authentication\\n- Automatic Failover with multi AZs\\n- Faster read loads\\n- Slightly cheaper\\nAlso has few downsides:\\n- Not recommended for heavy write systems (Twitter big)\\n- Slightly behind version wise\\n\n\n##Decision\nWe will continue with Aurora as we we don't have any specific requirements not\\nto and can benefit from the solution.\\n"}
{"File Name":"deeplearning4j\/0005-Interpreter.md","Context":"## Context\\n","Decision":"An interpreter uses the [import IR](.\/0003-Import_IR.md) and the [mapping rule IR](.\/0004-Mapping_IR.md)\\nto execute and map operations from one framework to nd4j's file format and back.\\nThis also allows execution of different frameworks via conversion in the nd4j engine.\\nA combination of the 2 allows a uniform interface to be used for the interpreter.\\n1 or more MappingRules will be used to transform 1 file format to another.\\n","tokens":3,"id":2935,"text":"## Context\\n\n\n##Decision\nAn interpreter uses the [import IR](.\/0003-Import_IR.md) and the [mapping rule IR](.\/0004-Mapping_IR.md)\\nto execute and map operations from one framework to nd4j's file format and back.\\nThis also allows execution of different frameworks via conversion in the nd4j engine.\\nA combination of the 2 allows a uniform interface to be used for the interpreter.\\n1 or more MappingRules will be used to transform 1 file format to another.\\n"}
{"File Name":"Horace\/0020-use-c-mex-api.md","Context":"## Context\\nMatlab has two Mex APIs that can be used by C++ code:\\nthe [C API](https:\/\/www.mathworks.com\/help\/matlab\/cc-mx-matrix-library.html)\\nor the\\n[C++ API](https:\/\/www.mathworks.com\/help\/matlab\/cpp-mex-file-applications.html).\\nThe C++ API is relatively new and is therefore only compatible with Matlab releases\\n[R2018a and later](https:\/\/www.mathworks.com\/help\/matlab\/matlab_external\/choosing-mex-applications.html#mw_d3e64706-faf9-486f-ab58-1860c63564d8).\\nThroughout the Herbert and Horace codebases only the C API has been used.\\nThis is primarily due to the fact that Herbert and Horace pre-date the C++ API.\\nThe C++ API contains restrictions on directly accessing Matlab memory,\\nthis provided challenges when writing the Matlab object serializer in C++.\\nThe API does not allow access to the underlying memory of the Matlab objects,\\nand the C++ objects had varying sized headers.\\nThis caused problems when attempting to perform copies and casts:\\nthe underlying data needed to first be copied via the C++ API,\\nand only then could a `memcpy` be performed by the serializer.\\nThis meant there were three copies of the data present at one time,\\nwhich is not desirable for large objects (e.g. pixel data).\\n","Decision":"Herbert\/Horace will continue to use the C API for new Mex functions.\\n","tokens":294,"id":4243,"text":"## Context\\nMatlab has two Mex APIs that can be used by C++ code:\\nthe [C API](https:\/\/www.mathworks.com\/help\/matlab\/cc-mx-matrix-library.html)\\nor the\\n[C++ API](https:\/\/www.mathworks.com\/help\/matlab\/cpp-mex-file-applications.html).\\nThe C++ API is relatively new and is therefore only compatible with Matlab releases\\n[R2018a and later](https:\/\/www.mathworks.com\/help\/matlab\/matlab_external\/choosing-mex-applications.html#mw_d3e64706-faf9-486f-ab58-1860c63564d8).\\nThroughout the Herbert and Horace codebases only the C API has been used.\\nThis is primarily due to the fact that Herbert and Horace pre-date the C++ API.\\nThe C++ API contains restrictions on directly accessing Matlab memory,\\nthis provided challenges when writing the Matlab object serializer in C++.\\nThe API does not allow access to the underlying memory of the Matlab objects,\\nand the C++ objects had varying sized headers.\\nThis caused problems when attempting to perform copies and casts:\\nthe underlying data needed to first be copied via the C++ API,\\nand only then could a `memcpy` be performed by the serializer.\\nThis meant there were three copies of the data present at one time,\\nwhich is not desirable for large objects (e.g. pixel data).\\n\n\n##Decision\nHerbert\/Horace will continue to use the C API for new Mex functions.\\n"}
{"File Name":"klokwrk-project\/0008-testing-architecture.md","Context":"## Context\\nApplication architecture is commonly expressed as a set of guidelines (written or not) that the development team needs to follow. Although essential, architectural guidelines are rarely reviewed or\\nenforced appropriately. The typical result is that architecture degrades through time.\\nIn some cases, application use component ([ADR-0003 - CQRS and Event Sourcing (ES) for Applications](0003-cqrs-and-event-sourcing-for-applications.md)) and\\ndesign ([ADR-0004 - Hexagonal Architecture for Applications](0004-hexagonal-architecture-for-applications.md)) architectural patterns that promote architectural guidelines, but usually there is\\nnothing to verify them.\\nThe hexagonal architecture provides a well-defined placeholder for every significant application artifact. But there are also some rules regarding dependencies between those artifacts. It is not\\nallowed that each class or interface access anything that it wants. When you add additional CQRS\/ES aspects, there are even more rules to follow.\\nWe want to ensure that rules will not be broken and that developers new to the hexagonal architecture and CQRS\/ES can comfortably work with them without breaking anything. It will help if we\\nhave in place tests that verify all architectural invariants.\\n### Architectural Context\\n* System (`klokwrk-project`)\\n","Decision":"**We will use architectural testing for verifying architectural constraints.**\\nBuilding on top of the [ArchUnit](https:\/\/www.archunit.org\/) library, Klokwrk provides DSL for specifying hexagonal architecture layers for CQRS\/ES applications. There is support for several subtypes\\nof CQRS\/ES flavored hexagonal architecture corresponding to the commandside, projections, and queryside aspects.\\nFor more insight and details, take a look at [\"Behavior and architectural testing\"](..\/..\/article\/modules-and-packages\/modulesAndPackages.md#behavior-and-architectural-testing). There is also a video\\n[\"Project Klokwrk: how it helps defining software architecture and solves integration\"](https:\/\/www.youtube.com\/watch?v=35oUxjXWNYU) that, besides other things, talks about architectural testing in\\n`klokwrk-project`.\\n","tokens":265,"id":4898,"text":"## Context\\nApplication architecture is commonly expressed as a set of guidelines (written or not) that the development team needs to follow. Although essential, architectural guidelines are rarely reviewed or\\nenforced appropriately. The typical result is that architecture degrades through time.\\nIn some cases, application use component ([ADR-0003 - CQRS and Event Sourcing (ES) for Applications](0003-cqrs-and-event-sourcing-for-applications.md)) and\\ndesign ([ADR-0004 - Hexagonal Architecture for Applications](0004-hexagonal-architecture-for-applications.md)) architectural patterns that promote architectural guidelines, but usually there is\\nnothing to verify them.\\nThe hexagonal architecture provides a well-defined placeholder for every significant application artifact. But there are also some rules regarding dependencies between those artifacts. It is not\\nallowed that each class or interface access anything that it wants. When you add additional CQRS\/ES aspects, there are even more rules to follow.\\nWe want to ensure that rules will not be broken and that developers new to the hexagonal architecture and CQRS\/ES can comfortably work with them without breaking anything. It will help if we\\nhave in place tests that verify all architectural invariants.\\n### Architectural Context\\n* System (`klokwrk-project`)\\n\n\n##Decision\n**We will use architectural testing for verifying architectural constraints.**\\nBuilding on top of the [ArchUnit](https:\/\/www.archunit.org\/) library, Klokwrk provides DSL for specifying hexagonal architecture layers for CQRS\/ES applications. There is support for several subtypes\\nof CQRS\/ES flavored hexagonal architecture corresponding to the commandside, projections, and queryside aspects.\\nFor more insight and details, take a look at [\"Behavior and architectural testing\"](..\/..\/article\/modules-and-packages\/modulesAndPackages.md#behavior-and-architectural-testing). There is also a video\\n[\"Project Klokwrk: how it helps defining software architecture and solves integration\"](https:\/\/www.youtube.com\/watch?v=35oUxjXWNYU) that, besides other things, talks about architectural testing in\\n`klokwrk-project`.\\n"}
{"File Name":"island.is-glosur\/0003-css.md","Context":"## Context and Problem Statement\\nWe're building websites and web applications that share a common design system with reusable components. How do we write CSS styles in a way that is performant and safe?\\n## Decision Drivers\\n- Should be performant, with code splitting, caching and minimal runtime overhead.\\n- Needs to have easy access to our design system constants. These should optimally be shared with JS logic.\\n- Should be type-safe to catch issues when refactoring.\\n- Reusable components should be closed, not accepting arbitrary styles\/classes.\\n- We want a pattern for responsive props with atomic layout components.\\n","Decision":"- Should be performant, with code splitting, caching and minimal runtime overhead.\\n- Needs to have easy access to our design system constants. These should optimally be shared with JS logic.\\n- Should be type-safe to catch issues when refactoring.\\n- Reusable components should be closed, not accepting arbitrary styles\/classes.\\n- We want a pattern for responsive props with atomic layout components.\\nChosen option: Treat, because it combines the best of both worlds from CSS-in-JS and CSS modules.\\nWe'll create shared components that have responsive props, but are otherwise closed for modifications. Theme variables are defined in a shared library with TypeScript.\\nExample:\\n```typescript jsx\\n\/\/ Good:\\n<Box padding\"small\" \/>\\n<Box padding={{xs: 'small', md: 'medium'}} \/>\\n<Input large \/>\\n<Text preset=\"heading3\" as=\"p\" \/>\\n\/\/ Bad:\\n<Box className={customLayout} \/>\\n<Input style={{ height: 50, padding: 16 }} \/>\\n<Text className={styles.heading} \/>\\n```\\n### Positive Consequences\\n- Treat is statically extracted at build time, so it has minimal runtime.\\n- Styles load in parallel with JS, also when code splitting.\\n- Styles are written in TypeScript which gives us type safety when referring to shared variables, styles and helpers.\\n- Styles are in special files, separate from markup and components giving us clear separation with good visibility into the rendered markup.\\n- We can pull in responsive layout component patterns from [Braid], which gives us a good base to lay out components and pages.\\n### Negative Consequences\\n- We are choosing a pretty new framework, so it may 1) have bugs or issues, 2) be an obstacle for new developers or 3) be discontinued.\\n- When we're generating responsive styles at build time we need to be mindful at how many variations we allow (eg media queries, columns, whitespace), since they can easily bloat our CSS with unused styles.\\n","tokens":126,"id":4762,"text":"## Context and Problem Statement\\nWe're building websites and web applications that share a common design system with reusable components. How do we write CSS styles in a way that is performant and safe?\\n## Decision Drivers\\n- Should be performant, with code splitting, caching and minimal runtime overhead.\\n- Needs to have easy access to our design system constants. These should optimally be shared with JS logic.\\n- Should be type-safe to catch issues when refactoring.\\n- Reusable components should be closed, not accepting arbitrary styles\/classes.\\n- We want a pattern for responsive props with atomic layout components.\\n\n\n##Decision\n- Should be performant, with code splitting, caching and minimal runtime overhead.\\n- Needs to have easy access to our design system constants. These should optimally be shared with JS logic.\\n- Should be type-safe to catch issues when refactoring.\\n- Reusable components should be closed, not accepting arbitrary styles\/classes.\\n- We want a pattern for responsive props with atomic layout components.\\nChosen option: Treat, because it combines the best of both worlds from CSS-in-JS and CSS modules.\\nWe'll create shared components that have responsive props, but are otherwise closed for modifications. Theme variables are defined in a shared library with TypeScript.\\nExample:\\n```typescript jsx\\n\/\/ Good:\\n<Box padding\"small\" \/>\\n<Box padding={{xs: 'small', md: 'medium'}} \/>\\n<Input large \/>\\n<Text preset=\"heading3\" as=\"p\" \/>\\n\/\/ Bad:\\n<Box className={customLayout} \/>\\n<Input style={{ height: 50, padding: 16 }} \/>\\n<Text className={styles.heading} \/>\\n```\\n### Positive Consequences\\n- Treat is statically extracted at build time, so it has minimal runtime.\\n- Styles load in parallel with JS, also when code splitting.\\n- Styles are written in TypeScript which gives us type safety when referring to shared variables, styles and helpers.\\n- Styles are in special files, separate from markup and components giving us clear separation with good visibility into the rendered markup.\\n- We can pull in responsive layout component patterns from [Braid], which gives us a good base to lay out components and pages.\\n### Negative Consequences\\n- We are choosing a pretty new framework, so it may 1) have bugs or issues, 2) be an obstacle for new developers or 3) be discontinued.\\n- When we're generating responsive styles at build time we need to be mindful at how many variations we allow (eg media queries, columns, whitespace), since they can easily bloat our CSS with unused styles.\\n"}
{"File Name":"ReportMI-service-manual\/0003-use-github-for-code.md","Context":"## Context\\nWe will be developing new code and applications during the build of the Data\\nSubmission Service.\\nIt is good practice to use version control to allow changes to be tracked over\\ntime.\\nThe Digital Service Standard [recommends][service-manual-version-control] the\\nuse of version control for government services. It also recommends\\n[making all new source code open][service-standard-point-8].\\nCrown Commercial Service already uses Git for some systems, and has a\\n[GitHub][ccs-github] account.\\n","Decision":"We will use Git and GitHub for storing our code.\\nWe will use public repositories by default, unless there is a convincing reason\\nnot to (which must be documented in the README of the private repository).\\nWe will use the main [CCS GitHub][ccs-github] account and use their standards\\nfor user account management.\\n","tokens":112,"id":2064,"text":"## Context\\nWe will be developing new code and applications during the build of the Data\\nSubmission Service.\\nIt is good practice to use version control to allow changes to be tracked over\\ntime.\\nThe Digital Service Standard [recommends][service-manual-version-control] the\\nuse of version control for government services. It also recommends\\n[making all new source code open][service-standard-point-8].\\nCrown Commercial Service already uses Git for some systems, and has a\\n[GitHub][ccs-github] account.\\n\n\n##Decision\nWe will use Git and GitHub for storing our code.\\nWe will use public repositories by default, unless there is a convincing reason\\nnot to (which must be documented in the README of the private repository).\\nWe will use the main [CCS GitHub][ccs-github] account and use their standards\\nfor user account management.\\n"}
{"File Name":"openlobby-server\/0007-adopt-graphql-relay-specification.md","Context":"## Context\\nWe need to make API friendly for clients and design pagination.\\n","Decision":"We will adopt GraphQL Relay specification. It solves pagination so we don't\\nhave to reinvent a wheel. It has handy Node interface for re-fetching objects.\\nIt has a way to define inputs in mutations.\\nGraphene lib has good support for creating API following Relay specifications.\\n","tokens":16,"id":470,"text":"## Context\\nWe need to make API friendly for clients and design pagination.\\n\n\n##Decision\nWe will adopt GraphQL Relay specification. It solves pagination so we don't\\nhave to reinvent a wheel. It has handy Node interface for re-fetching objects.\\nIt has a way to define inputs in mutations.\\nGraphene lib has good support for creating API following Relay specifications.\\n"}
{"File Name":"ELEN4010\/ADR Testing.md","Context":"## Context\\nIn order to holistically test the core functionality of the website, a combination of unit testing, end-to-end testing, and manual testing is used.\\nUnit tests are used on back-end models and database-related code in order to validate the functionality of each essential unit of the code (which, in most cases, are functions).\\nOn the front-end, various user actions are performed by automated testing software. During that process, key aspects relating to the front-end side of the website are tested.\\nHigh-level functionality is exclusively assessed and confirmed via manual user testing. This includes testing the following aspects of the website:\\n- Marker placement on maps\\n- Destinations being correctly added and drawn\\n- Trips being correctly written to and received from session storage\\n","Decision":"The testing framework chosen for automated testing is Jest. This framework is used because:\\n- It has a simple installation and configuration process for Node.js\\n- Due to its popularity as a javascript testing framework, it has a large developer-community which produces many articles, documents and forum threads (amongst many other sources of documentation and support)\\n- It has a wide variety of built-in assertion abilities (which means that there is no need for the installation of a third-party assertion library)\\nIn order to simulate in-browser user-interactions with the website, Selenium WebDriver is used. Front-end testing is performed on the https:\/\/testawaywego.azurewebsites.net website since it is the website used for development.\\nUltimately, it was decided that all automated front-end user testing will be performed using Google Chrome as the browser. The reason for this is due to the fact that Google Chrome has the highest browser market share (more than 60%) globally - meaning that a majrity of the website's users will be using Google Chrome.\\nAt multiple stages throughout the development process, manual testing on other major browsers (i.e. FireFox, Safari and Microsoft Edge) was also performed in order to ensure the cross-browser compatibility of the website. Manual testing was also used to ensure that the website is mobile-friendly.\\n","tokens":156,"id":56,"text":"## Context\\nIn order to holistically test the core functionality of the website, a combination of unit testing, end-to-end testing, and manual testing is used.\\nUnit tests are used on back-end models and database-related code in order to validate the functionality of each essential unit of the code (which, in most cases, are functions).\\nOn the front-end, various user actions are performed by automated testing software. During that process, key aspects relating to the front-end side of the website are tested.\\nHigh-level functionality is exclusively assessed and confirmed via manual user testing. This includes testing the following aspects of the website:\\n- Marker placement on maps\\n- Destinations being correctly added and drawn\\n- Trips being correctly written to and received from session storage\\n\n\n##Decision\nThe testing framework chosen for automated testing is Jest. This framework is used because:\\n- It has a simple installation and configuration process for Node.js\\n- Due to its popularity as a javascript testing framework, it has a large developer-community which produces many articles, documents and forum threads (amongst many other sources of documentation and support)\\n- It has a wide variety of built-in assertion abilities (which means that there is no need for the installation of a third-party assertion library)\\nIn order to simulate in-browser user-interactions with the website, Selenium WebDriver is used. Front-end testing is performed on the https:\/\/testawaywego.azurewebsites.net website since it is the website used for development.\\nUltimately, it was decided that all automated front-end user testing will be performed using Google Chrome as the browser. The reason for this is due to the fact that Google Chrome has the highest browser market share (more than 60%) globally - meaning that a majrity of the website's users will be using Google Chrome.\\nAt multiple stages throughout the development process, manual testing on other major browsers (i.e. FireFox, Safari and Microsoft Edge) was also performed in order to ensure the cross-browser compatibility of the website. Manual testing was also used to ensure that the website is mobile-friendly.\\n"}
{"File Name":"google-cloud-cpp\/2018-06-18-storage-request-parameters-are-function-arguments.md","Context":"**Context**: most APIs in the GCS library have a number of optional parameters,\\nfor example, the API can use `ifMetagenerationMatch` to apply an operation only\\nif the metadata generation matches a given number. The question arose of how to\\nrepresent these parameters, as properties that we modify in the client or\\nobject, or as per-request parameters in the function used to access the API.\\nThat is, we had two proposals, one where the application would write code like\\nthis:\\n```C++\\nvoid AppCode(Bucket b, FixedArgument1 foo, FixedArgument2 bar) {\\nb.ApiName(\\nfoo, bar, storage::IfMetagenerationMatch(7), UserProject(\"my-project\"));\\n```\\nvs. code like this:\\n```C++\\nvoid AppCode(Bucket b, FixedArgument1 foo, FixedArgument2 bar) {\\n\/\/ Create a new bucket handle that applies the given optional parameters to\\n\/\/ all requests.\\nauto bucket = b.ApplyModifiers(\\nstorage::IfMetagenerationMatch(7), UserProject(\"my-project\"))\\nbucket.ApiName(foo, bar);\\n}\\n```\\n**Decision**: The parameters are passed as variadic arguments into any function\\nthat needs them. That is, all APIs look like this:\\n```C++\\nclass Bucket \/* or Object as applicable *\/ { public:\\ntemplate <typename... Parameters>\\nReturnType ApiName(\\nFixedArgument1 a1, FixedArgument2 a2,\\nParameters&&... p);\\n```\\nand are used like this:\\n```C++\\nvoid AppCode(Bucket b, FixedArgument1 foo, FixedArgument2 bar) {\\nb.ApiName(\\nfoo, bar, storage::IfMetagenerationMatch(7), UserProject(\"my-project\"));\\n```\\n**Consequences**: The advantages of this approach include:\\n- It is easier to use parameters in an API, it does not require to create a new\\nbucket or object or client handle just for changing the parameters in one\\nrequest.\\nThe downsides include:\\n- All APIs become templates, we should be careful not to create massive header\\nfiles that are slow to compile.\\n- It is harder to overload APIs.\\n- It is not clear how other optional parameters of the APIs, such as timeouts,\\nfit with this structure.\\n","Decision":"that needs them. That is, all APIs look like this:\\n```C++\\nclass Bucket \/* or Object as applicable *\/ { public:\\ntemplate <typename... Parameters>\\nReturnType ApiName(\\nFixedArgument1 a1, FixedArgument2 a2,\\nParameters&&... p);\\n```\\nand are used like this:\\n```C++\\nvoid AppCode(Bucket b, FixedArgument1 foo, FixedArgument2 bar) {\\nb.ApiName(\\nfoo, bar, storage::IfMetagenerationMatch(7), UserProject(\"my-project\"));\\n```\\n**Consequences**: The advantages of this approach include:\\n- It is easier to use parameters in an API, it does not require to create a new\\nbucket or object or client handle just for changing the parameters in one\\nrequest.\\nThe downsides include:\\n- All APIs become templates, we should be careful not to create massive header\\nfiles that are slow to compile.\\n- It is harder to overload APIs.\\n- It is not clear how other optional parameters of the APIs, such as timeouts,\\nfit with this structure.\\n","tokens":498,"id":2469,"text":"**Context**: most APIs in the GCS library have a number of optional parameters,\\nfor example, the API can use `ifMetagenerationMatch` to apply an operation only\\nif the metadata generation matches a given number. The question arose of how to\\nrepresent these parameters, as properties that we modify in the client or\\nobject, or as per-request parameters in the function used to access the API.\\nThat is, we had two proposals, one where the application would write code like\\nthis:\\n```C++\\nvoid AppCode(Bucket b, FixedArgument1 foo, FixedArgument2 bar) {\\nb.ApiName(\\nfoo, bar, storage::IfMetagenerationMatch(7), UserProject(\"my-project\"));\\n```\\nvs. code like this:\\n```C++\\nvoid AppCode(Bucket b, FixedArgument1 foo, FixedArgument2 bar) {\\n\/\/ Create a new bucket handle that applies the given optional parameters to\\n\/\/ all requests.\\nauto bucket = b.ApplyModifiers(\\nstorage::IfMetagenerationMatch(7), UserProject(\"my-project\"))\\nbucket.ApiName(foo, bar);\\n}\\n```\\n**Decision**: The parameters are passed as variadic arguments into any function\\nthat needs them. That is, all APIs look like this:\\n```C++\\nclass Bucket \/* or Object as applicable *\/ { public:\\ntemplate <typename... Parameters>\\nReturnType ApiName(\\nFixedArgument1 a1, FixedArgument2 a2,\\nParameters&&... p);\\n```\\nand are used like this:\\n```C++\\nvoid AppCode(Bucket b, FixedArgument1 foo, FixedArgument2 bar) {\\nb.ApiName(\\nfoo, bar, storage::IfMetagenerationMatch(7), UserProject(\"my-project\"));\\n```\\n**Consequences**: The advantages of this approach include:\\n- It is easier to use parameters in an API, it does not require to create a new\\nbucket or object or client handle just for changing the parameters in one\\nrequest.\\nThe downsides include:\\n- All APIs become templates, we should be careful not to create massive header\\nfiles that are slow to compile.\\n- It is harder to overload APIs.\\n- It is not clear how other optional parameters of the APIs, such as timeouts,\\nfit with this structure.\\n\n\n##Decision\nthat needs them. That is, all APIs look like this:\\n```C++\\nclass Bucket \/* or Object as applicable *\/ { public:\\ntemplate <typename... Parameters>\\nReturnType ApiName(\\nFixedArgument1 a1, FixedArgument2 a2,\\nParameters&&... p);\\n```\\nand are used like this:\\n```C++\\nvoid AppCode(Bucket b, FixedArgument1 foo, FixedArgument2 bar) {\\nb.ApiName(\\nfoo, bar, storage::IfMetagenerationMatch(7), UserProject(\"my-project\"));\\n```\\n**Consequences**: The advantages of this approach include:\\n- It is easier to use parameters in an API, it does not require to create a new\\nbucket or object or client handle just for changing the parameters in one\\nrequest.\\nThe downsides include:\\n- All APIs become templates, we should be careful not to create massive header\\nfiles that are slow to compile.\\n- It is harder to overload APIs.\\n- It is not clear how other optional parameters of the APIs, such as timeouts,\\nfit with this structure.\\n"}
{"File Name":"api-raml\/0002-use-raml-annotation-experimental.md","Context":"## Context\\nDownstream libraries such as API client, SDK, validators, bot-lax-adaptor rely on the schema provided by api-raml.\\nIt is difficult and error-prone to point the downstream libraries to a particular branch; in particular given this branch would need to rebuild dist\/ after checkout.\\nIt is forbidden to update in place a schema exposed by an API without versioning: the content types follow semantic versioning.\\n","Decision":"We will annotate with `@experimental` APIs that are being worked upon, and merge the into `develop` and `master` (indirectly).\\n","tokens":90,"id":2564,"text":"## Context\\nDownstream libraries such as API client, SDK, validators, bot-lax-adaptor rely on the schema provided by api-raml.\\nIt is difficult and error-prone to point the downstream libraries to a particular branch; in particular given this branch would need to rebuild dist\/ after checkout.\\nIt is forbidden to update in place a schema exposed by an API without versioning: the content types follow semantic versioning.\\n\n\n##Decision\nWe will annotate with `@experimental` APIs that are being worked upon, and merge the into `develop` and `master` (indirectly).\\n"}
{"File Name":"container\/0001-use-maven.md","Context":"## Context and Problem Statement\\nTo enable portability and reproducibility of building the container application, a buildtool is of utmost importance.\\nIt simplifies the buildprocess and should preferrably allow a minimal configuration effort for IDEs.\\n## Decision Drivers\\n* Core Concern is management of building the container, preferrably in a single command invocation\\n* Support for dependency management is optional, but highly appreciated\\n* The tool should also support deployment of the finished web application to an application server\\n* Cross Platform tooling, Automation and support for Running a Test suite are also necessary\\n","Decision":"* Core Concern is management of building the container, preferrably in a single command invocation\\n* Support for dependency management is optional, but highly appreciated\\n* The tool should also support deployment of the finished web application to an application server\\n* Cross Platform tooling, Automation and support for Running a Test suite are also necessary\\nMaven was chosen both for it's maturity and integration with established IDEs.\\nIt also was previously used to automate the container build when the container was still deployed as an OSGI application.\\nAdditionally maven dependencies are the de-facto standard distribution mechanism in the java ecosystem.\\n","tokens":120,"id":4658,"text":"## Context and Problem Statement\\nTo enable portability and reproducibility of building the container application, a buildtool is of utmost importance.\\nIt simplifies the buildprocess and should preferrably allow a minimal configuration effort for IDEs.\\n## Decision Drivers\\n* Core Concern is management of building the container, preferrably in a single command invocation\\n* Support for dependency management is optional, but highly appreciated\\n* The tool should also support deployment of the finished web application to an application server\\n* Cross Platform tooling, Automation and support for Running a Test suite are also necessary\\n\n\n##Decision\n* Core Concern is management of building the container, preferrably in a single command invocation\\n* Support for dependency management is optional, but highly appreciated\\n* The tool should also support deployment of the finished web application to an application server\\n* Cross Platform tooling, Automation and support for Running a Test suite are also necessary\\nMaven was chosen both for it's maturity and integration with established IDEs.\\nIt also was previously used to automate the container build when the container was still deployed as an OSGI application.\\nAdditionally maven dependencies are the de-facto standard distribution mechanism in the java ecosystem.\\n"}
{"File Name":"linshare-mobile-android-app\/0009-downloadingrepository-to-manage-downloading-tasks.md","Context":"## Context\\nApplication delegate download task to `DownloadManager` system service and get an `unique id` for this task.\\nApplication need to store this Id to do further stuff with this task:\\n- Query status\\n- Get the completed state\\n- Get error details\\n- Cancel a download task\\n","Decision":"Creating a `DownloadingRepository` to manage downloading tasks\\n","tokens":62,"id":1645,"text":"## Context\\nApplication delegate download task to `DownloadManager` system service and get an `unique id` for this task.\\nApplication need to store this Id to do further stuff with this task:\\n- Query status\\n- Get the completed state\\n- Get error details\\n- Cancel a download task\\n\n\n##Decision\nCreating a `DownloadingRepository` to manage downloading tasks\\n"}
{"File Name":"gti-genesearch\/adr-002.md","Context":"## Context\\nAs we start pulling back data from multiple searches, we run the risk of ending up with multiple parameters (fields, targetFields, targetFields2 etc. etc.)\\nA better approach is to pass in specifications using a nested hash or similar so fields can be targeted to individual searches e.g.\\n```\\n{\\n\"genes\":[\"id\",\"name\",\"description\"],\\n\"variations\":[\"\"]\\n}\\n```\\nHowever, for simplicity, this needs to support simple field specifications too e.g.\\n```\\n[\"id\",\"name\",\"description\"]\\n```\\nor even\\n```\\n\"id\",\"name\",\"description\"\\n```\\n","Decision":"Mixing arbitrary arrays and hashes in Java is not well supported and requires casts etc. We will instead create a specific class, `org.ensembl.genesearch.QueryOutput`, capable of representing this kind of nested structure.  `build()` methods will be provided to support parsing various specifications.\\n","tokens":139,"id":3284,"text":"## Context\\nAs we start pulling back data from multiple searches, we run the risk of ending up with multiple parameters (fields, targetFields, targetFields2 etc. etc.)\\nA better approach is to pass in specifications using a nested hash or similar so fields can be targeted to individual searches e.g.\\n```\\n{\\n\"genes\":[\"id\",\"name\",\"description\"],\\n\"variations\":[\"\"]\\n}\\n```\\nHowever, for simplicity, this needs to support simple field specifications too e.g.\\n```\\n[\"id\",\"name\",\"description\"]\\n```\\nor even\\n```\\n\"id\",\"name\",\"description\"\\n```\\n\n\n##Decision\nMixing arbitrary arrays and hashes in Java is not well supported and requires casts etc. We will instead create a specific class, `org.ensembl.genesearch.QueryOutput`, capable of representing this kind of nested structure.  `build()` methods will be provided to support parsing various specifications.\\n"}
{"File Name":"fxa\/0028-evaluate-playwright.md","Context":"## Context and Problem Statement\\nOur functional test suite currently has a 7% success rate when run against our stage environment after a deployment and a 44% success rate when run in CI for pull requests. These low rates are more from flaky tests and a finicky testing stack than legitimate bugs in the code being tested.\\nIn stage this rate is too low to be able to confidently move to a continuous delivery pipeline. In CI it slows down development and decreases morale.\\nBecause of our low success rate for pull requests each PR needs two runs of a relatively expensive task on average. In the last 90 days we used ~1.2M CircleCI credits for PRs. Ideally we could cut that in half.\\nWe should evaluate other testing stack options to improve reliability.\\n","Decision":"Playwright performs better than Intern in all goals. We should prefer it for new tests and begin migrating old tests when they need maintenance and on a case-by-case basis to improve our CI pass rate.\\nTo reduce our CircleCI bill and speed up CI runs for pull requests we should segment our functional tests into two categories: P1 and P2 (for lack of a better name). P1 tests for critical functionality run on every pull request. P2 tests should run periodically (daily) and send results to Slack. The difference between a P1 and P2 test suites is that a failed P1 means some \"happy path\" is broken, an S1 or S2 level bug, while P2 tests would represent S3 or S4 bugs.\\nP1 tests are the first priority for converting to Playwright.\\n","tokens":160,"id":385,"text":"## Context and Problem Statement\\nOur functional test suite currently has a 7% success rate when run against our stage environment after a deployment and a 44% success rate when run in CI for pull requests. These low rates are more from flaky tests and a finicky testing stack than legitimate bugs in the code being tested.\\nIn stage this rate is too low to be able to confidently move to a continuous delivery pipeline. In CI it slows down development and decreases morale.\\nBecause of our low success rate for pull requests each PR needs two runs of a relatively expensive task on average. In the last 90 days we used ~1.2M CircleCI credits for PRs. Ideally we could cut that in half.\\nWe should evaluate other testing stack options to improve reliability.\\n\n\n##Decision\nPlaywright performs better than Intern in all goals. We should prefer it for new tests and begin migrating old tests when they need maintenance and on a case-by-case basis to improve our CI pass rate.\\nTo reduce our CircleCI bill and speed up CI runs for pull requests we should segment our functional tests into two categories: P1 and P2 (for lack of a better name). P1 tests for critical functionality run on every pull request. P2 tests should run periodically (daily) and send results to Slack. The difference between a P1 and P2 test suites is that a failed P1 means some \"happy path\" is broken, an S1 or S2 level bug, while P2 tests would represent S3 or S4 bugs.\\nP1 tests are the first priority for converting to Playwright.\\n"}
{"File Name":"core\/0004-naming-convention-images.md","Context":"## Context and Problem Statement\\nImage names are important for branding and let others identify easily a specific image they need. For example \"I want to work on computer vision project with Tensorflow, what stack and image should I use?\" Having a trusted well maintained source of images with clean naming convention can help on that.\\n","Decision":"Selected option: `ps-{application}` as it shows what our intention is: we want to provide a curated\/predictable software stack, it might be used by ODH or RHODS or others, it might use S2I or other technology. Moreover helps from pipeline creation point of view, because the length of repo name on quay can crate issues.\\n### Positive Consequences <!-- optional -->\\n* users can immediately select an image based on the application they want.\\n* using overlays we can have a variety of combination, not just for ml_framework\\n### Negative Consequences <!-- optional -->\\n* N\/A\\n<!-- markdownlint-disable-file MD013 -->\\n","tokens":64,"id":291,"text":"## Context and Problem Statement\\nImage names are important for branding and let others identify easily a specific image they need. For example \"I want to work on computer vision project with Tensorflow, what stack and image should I use?\" Having a trusted well maintained source of images with clean naming convention can help on that.\\n\n\n##Decision\nSelected option: `ps-{application}` as it shows what our intention is: we want to provide a curated\/predictable software stack, it might be used by ODH or RHODS or others, it might use S2I or other technology. Moreover helps from pipeline creation point of view, because the length of repo name on quay can crate issues.\\n### Positive Consequences <!-- optional -->\\n* users can immediately select an image based on the application they want.\\n* using overlays we can have a variety of combination, not just for ml_framework\\n### Negative Consequences <!-- optional -->\\n* N\/A\\n<!-- markdownlint-disable-file MD013 -->\\n"}
{"File Name":"play-frontend-hmrc\/0009-self-publish-webjar.md","Context":"## Context and Problem Statement\\nplay-frontend-hmrc relies on a webjar for [hmrc\/hmrc-frontend](https:\/\/www.github.com\/hmrc\/hmrc-frontend)\\npublished to www.webjars.org. This has a number of drawbacks:\\n* publishing is a manual process\\n* it can take many hours to complete\\n* webjars has been known to be down and HMRC has no support arrangements with www.webjars.org\\nThe main impact of the above is an excessive lead time for making improvements in the\\nunderlying hmrc-frontend library available in production via play-frontend-hmrc.\\nBearing the above in mind, and the fact that HMRC has its own repository for open artefacts, replacing\\nBintray, should we:\\n* automate the creation of the webjars within our own deployment pipelines with no dependency\\non webjars.org\\n* publish the resulting webjars to this repository automatically?\\nNote, this decision only addresses the creation and publishing of the hmrc-frontend webjar, not the\\nwebjar for [alphagov\/govuk-frontend](https:\/\/www.github.com\/alphagov\/govuk-frontend), which is\\ncurrently a dependency for [hmrc\/play-frontend-govuk](https:\/\/www.github.com\/hmrc\/play-frontend-govuk).\\n## Decision Drivers\\n* The need to make improvements and upgrades to hmrc-frontend\\navailable in play-frontend-hmrc quickly.\\n* The increasing user base of play-frontend-hmrc, and accelerating demand for new features and\\nimprovements.\\n* The hardship, frustration and toil the current manual process is causing the team.\\n* The need to keep things simple and avoidance of creating new repositories unnecessarily due to\\nthe overhead of maintaining those repositories\\n* The testing advantages of being able to build and deploy the hmrc-frontend webjar locally\\n* Parity between the hmrc-frontend NPM package and the webjar.\\n","Decision":"* The need to make improvements and upgrades to hmrc-frontend\\navailable in play-frontend-hmrc quickly.\\n* The increasing user base of play-frontend-hmrc, and accelerating demand for new features and\\nimprovements.\\n* The hardship, frustration and toil the current manual process is causing the team.\\n* The need to keep things simple and avoidance of creating new repositories unnecessarily due to\\nthe overhead of maintaining those repositories\\n* The testing advantages of being able to build and deploy the hmrc-frontend webjar locally\\n* Parity between the hmrc-frontend NPM package and the webjar.\\nChosen option: option 2 because it solves the core issue and enables local testing without introducing\\nadditional dependencies.\\n### Existing architecture\\n<img alt=\"Existing architecture\" src=\"0009-webjars-existing.png\" width=\"450\">\\n### To be architecture\\n<img alt=\"To be architecture\" src=\"0009-webjars-tobe.png\" width=\"450\">\\n### Positive Consequences\\n* Webjars are available instantaneously after a new version of hmrc-frontend is released\\n* It is now possible to locally test changes to hmrc-frontend in conjunction with Scala microservices\\nwithout needing to publish to NPM or webjars.org first, reducing the risk that flawed components are released into\\nproduction.\\n* Lead times for making improvements to hmrc-frontend available in production are reduced.\\n* Maintaining play-frontend-hmrc is a less frustrating process.\\n* We have more control over the metadata attached to the webjars published. For example, at the moment, the webjars\\nproduced indicate webjars.org as the developer in the POM files.\\n* There are fewer external dependencies and moving parts.\\n### Negative Consequences\\n* We have an additional moving part to maintain ourselves.\\n","tokens":423,"id":570,"text":"## Context and Problem Statement\\nplay-frontend-hmrc relies on a webjar for [hmrc\/hmrc-frontend](https:\/\/www.github.com\/hmrc\/hmrc-frontend)\\npublished to www.webjars.org. This has a number of drawbacks:\\n* publishing is a manual process\\n* it can take many hours to complete\\n* webjars has been known to be down and HMRC has no support arrangements with www.webjars.org\\nThe main impact of the above is an excessive lead time for making improvements in the\\nunderlying hmrc-frontend library available in production via play-frontend-hmrc.\\nBearing the above in mind, and the fact that HMRC has its own repository for open artefacts, replacing\\nBintray, should we:\\n* automate the creation of the webjars within our own deployment pipelines with no dependency\\non webjars.org\\n* publish the resulting webjars to this repository automatically?\\nNote, this decision only addresses the creation and publishing of the hmrc-frontend webjar, not the\\nwebjar for [alphagov\/govuk-frontend](https:\/\/www.github.com\/alphagov\/govuk-frontend), which is\\ncurrently a dependency for [hmrc\/play-frontend-govuk](https:\/\/www.github.com\/hmrc\/play-frontend-govuk).\\n## Decision Drivers\\n* The need to make improvements and upgrades to hmrc-frontend\\navailable in play-frontend-hmrc quickly.\\n* The increasing user base of play-frontend-hmrc, and accelerating demand for new features and\\nimprovements.\\n* The hardship, frustration and toil the current manual process is causing the team.\\n* The need to keep things simple and avoidance of creating new repositories unnecessarily due to\\nthe overhead of maintaining those repositories\\n* The testing advantages of being able to build and deploy the hmrc-frontend webjar locally\\n* Parity between the hmrc-frontend NPM package and the webjar.\\n\n\n##Decision\n* The need to make improvements and upgrades to hmrc-frontend\\navailable in play-frontend-hmrc quickly.\\n* The increasing user base of play-frontend-hmrc, and accelerating demand for new features and\\nimprovements.\\n* The hardship, frustration and toil the current manual process is causing the team.\\n* The need to keep things simple and avoidance of creating new repositories unnecessarily due to\\nthe overhead of maintaining those repositories\\n* The testing advantages of being able to build and deploy the hmrc-frontend webjar locally\\n* Parity between the hmrc-frontend NPM package and the webjar.\\nChosen option: option 2 because it solves the core issue and enables local testing without introducing\\nadditional dependencies.\\n### Existing architecture\\n<img alt=\"Existing architecture\" src=\"0009-webjars-existing.png\" width=\"450\">\\n### To be architecture\\n<img alt=\"To be architecture\" src=\"0009-webjars-tobe.png\" width=\"450\">\\n### Positive Consequences\\n* Webjars are available instantaneously after a new version of hmrc-frontend is released\\n* It is now possible to locally test changes to hmrc-frontend in conjunction with Scala microservices\\nwithout needing to publish to NPM or webjars.org first, reducing the risk that flawed components are released into\\nproduction.\\n* Lead times for making improvements to hmrc-frontend available in production are reduced.\\n* Maintaining play-frontend-hmrc is a less frustrating process.\\n* We have more control over the metadata attached to the webjars published. For example, at the moment, the webjars\\nproduced indicate webjars.org as the developer in the POM files.\\n* There are fewer external dependencies and moving parts.\\n### Negative Consequences\\n* We have an additional moving part to maintain ourselves.\\n"}
{"File Name":"opg-digideps\/0004-user-emails-and-roles-are-immutable.md","Context":"## Context\\nDue to the freedom given to users and admins in changing a user's email address or role, we've had issues identifying why people have certain permissions in DigiDeps. As well as being confusing, we believe this has led to security issues where a high-authority user has had their email changed in an attempt to reuse the account, thereby granting unreasonable access to the owner of the new email address.\\nThere are rarely good reasons to change a user's email address or role. Because very little in DigiDeps belongs directly to a user, these situtations can be resolved by deleting the original account and creating a new one with the correct permissions.\\n","Decision":"User email addresses and roles will henceforth be immutable: they are set when a new user is created and cannot subsequently be changed.\\nThere is one exception to this: organisation administrators will be able to switch users between team member and admin roles. This role switch is entirely internal to the organisation and doesn't affect any view\/edit permissions to clients and reports.\\n","tokens":136,"id":4409,"text":"## Context\\nDue to the freedom given to users and admins in changing a user's email address or role, we've had issues identifying why people have certain permissions in DigiDeps. As well as being confusing, we believe this has led to security issues where a high-authority user has had their email changed in an attempt to reuse the account, thereby granting unreasonable access to the owner of the new email address.\\nThere are rarely good reasons to change a user's email address or role. Because very little in DigiDeps belongs directly to a user, these situtations can be resolved by deleting the original account and creating a new one with the correct permissions.\\n\n\n##Decision\nUser email addresses and roles will henceforth be immutable: they are set when a new user is created and cannot subsequently be changed.\\nThere is one exception to this: organisation administrators will be able to switch users between team member and admin roles. This role switch is entirely internal to the organisation and doesn't affect any view\/edit permissions to clients and reports.\\n"}
{"File Name":"cygnus-infra\/0004-introduce-a-pi-hole-for-local-dns.md","Context":"## Context\\n- I don't want to make the CoreDNS container in my cluster the local DNS source of truth, because my view of it is entirely for cluster DNS\\n- I don't want to expose my services to the internet _yet_ (but expect to later)\\n- I have a dedicated domain for my services\\n- I can set up TLS for \"internal\" domains via Let's Encrypt + CloudFlare DNS\\n- I need something to route internal requests to my cluster without querying external DNS resolvers, which won't know about my internal cluster\\n- I can probably do this on my UniFi setup, but it'd likely require some configuration outside of the controller software\\n- Pi-Hole introduces some other neat features\\n- I have multiple spare Raspberry Pis\\n","Decision":"- Flash Raspberry Pi OS (minimal install) on a microSD card\\n- Use that card to boot a Raspberry Pi 3B+ (`Raspberry Pi 3 Model B Rev 1.2`)\\n- Configure for SSH, use wired LAN\\n- Consider adding WLAN later, but Pi-Hole will expect a single static IP?\\n- Install and configure unattended upgrades\\n- Install Pi-Hole\\n","tokens":158,"id":3870,"text":"## Context\\n- I don't want to make the CoreDNS container in my cluster the local DNS source of truth, because my view of it is entirely for cluster DNS\\n- I don't want to expose my services to the internet _yet_ (but expect to later)\\n- I have a dedicated domain for my services\\n- I can set up TLS for \"internal\" domains via Let's Encrypt + CloudFlare DNS\\n- I need something to route internal requests to my cluster without querying external DNS resolvers, which won't know about my internal cluster\\n- I can probably do this on my UniFi setup, but it'd likely require some configuration outside of the controller software\\n- Pi-Hole introduces some other neat features\\n- I have multiple spare Raspberry Pis\\n\n\n##Decision\n- Flash Raspberry Pi OS (minimal install) on a microSD card\\n- Use that card to boot a Raspberry Pi 3B+ (`Raspberry Pi 3 Model B Rev 1.2`)\\n- Configure for SSH, use wired LAN\\n- Consider adding WLAN later, but Pi-Hole will expect a single static IP?\\n- Install and configure unattended upgrades\\n- Install Pi-Hole\\n"}
{"File Name":"Maud\/0012-unbabalanced_mets-enzymes-as-priors.md","Context":"## Context\\nUnbalanced metabolites and enzyme concentrations are boundary conditions for our ODE\\nmodel. Experimental conditions are defined with respect to these values and drains,\\nwhich are already defined as priors. Therefore, our prior knowledge about the\\nmetabolic phenotype is defined as what is measured about the boundary conditions. This\\ndecision aims to shift the measurements of the enzymes and unbalanced metabolites from\\nthe likelihood evaluations to the prior information.\\nThe benefit of treating priors in this way is that we define a prior on the phenotype\\nrather than all possible phenotypes. However, boundary conditions that are unmeasured\\nare still considered using weakly informative priors (read: within biologically relevant\\nboundaries).\\n","Decision":"Unbalanced metabolites and enzyme concentrations can also be considered as prior distributions\\nrather than likelihood evaluations.\\n","tokens":148,"id":243,"text":"## Context\\nUnbalanced metabolites and enzyme concentrations are boundary conditions for our ODE\\nmodel. Experimental conditions are defined with respect to these values and drains,\\nwhich are already defined as priors. Therefore, our prior knowledge about the\\nmetabolic phenotype is defined as what is measured about the boundary conditions. This\\ndecision aims to shift the measurements of the enzymes and unbalanced metabolites from\\nthe likelihood evaluations to the prior information.\\nThe benefit of treating priors in this way is that we define a prior on the phenotype\\nrather than all possible phenotypes. However, boundary conditions that are unmeasured\\nare still considered using weakly informative priors (read: within biologically relevant\\nboundaries).\\n\n\n##Decision\nUnbalanced metabolites and enzyme concentrations can also be considered as prior distributions\\nrather than likelihood evaluations.\\n"}
{"File Name":"copilot\/0005-enable-grpc-gzip-compression-between-copilot-and-route-syncer.md","Context":"## Context\\nThe GRPC default message size of 4 mb currently causing a bottleneck between cc-route-syncer and copilot. As our message sizes increased with scale this prevents us from sending messages to copilot.\\n","Decision":"We have decided to reduce the message size by enabling GRPC's GZIP compression between cc-route-syncer and copilot.\\n","tokens":44,"id":52,"text":"## Context\\nThe GRPC default message size of 4 mb currently causing a bottleneck between cc-route-syncer and copilot. As our message sizes increased with scale this prevents us from sending messages to copilot.\\n\n\n##Decision\nWe have decided to reduce the message size by enabling GRPC's GZIP compression between cc-route-syncer and copilot.\\n"}
{"File Name":"arch\/0015-code-review.md","Context":"## Context\\nContext here...\\n","Decision":"\u4f7f\u7528 Github \u8fdb\u884c Code Review\\n","tokens":7,"id":2416,"text":"## Context\\nContext here...\\n\n\n##Decision\n\u4f7f\u7528 Github \u8fdb\u884c Code Review\\n"}
{"File Name":"clone_difitalcitizenship\/0011-we-chose-a-different-cosmosdb-api.md","Context":"## Context\\nIn [ADR #9](0009-we-choose-a-cosmosdb-api.md) we decided to use the MongoDB API\\nfor CosmosDB but as soon as we implemented the first version of the DAL logic\\nwe realized that the MongoDB support in CosmosDB was flawed: a serious [bug](https:\/\/feedback.azure.com\/forums\/263030-azure-cosmos-db\/suggestions\/19361521-fix-bug-that-destroys-nodejs-mongodb-connection-po)\\nthat destroyed the connection pool every ~10s made the MongoDB API impossible\\nto use in a production environment.\\n","Decision":"Since at the time we were still in the early phase of development (around\\nAugust 2017), we decided to implement the DAL on top of the more mature\\nDocumentDB API (see [this commit](https:\/\/github.com\/teamdigitale\/digital-citizenship-functions\/commit\/c72b95ebb5ed038cdf62f43dc1adacbde9668d4e)).\\nNote that the bug has been recently fixed (October 2017), so it may be worth to\\nplan a migration of the DAL to the MongoDB API.\\n","tokens":126,"id":1181,"text":"## Context\\nIn [ADR #9](0009-we-choose-a-cosmosdb-api.md) we decided to use the MongoDB API\\nfor CosmosDB but as soon as we implemented the first version of the DAL logic\\nwe realized that the MongoDB support in CosmosDB was flawed: a serious [bug](https:\/\/feedback.azure.com\/forums\/263030-azure-cosmos-db\/suggestions\/19361521-fix-bug-that-destroys-nodejs-mongodb-connection-po)\\nthat destroyed the connection pool every ~10s made the MongoDB API impossible\\nto use in a production environment.\\n\n\n##Decision\nSince at the time we were still in the early phase of development (around\\nAugust 2017), we decided to implement the DAL on top of the more mature\\nDocumentDB API (see [this commit](https:\/\/github.com\/teamdigitale\/digital-citizenship-functions\/commit\/c72b95ebb5ed038cdf62f43dc1adacbde9668d4e)).\\nNote that the bug has been recently fixed (October 2017), so it may be worth to\\nplan a migration of the DAL to the MongoDB API.\\n"}
{"File Name":"raster-foundry\/adr-0024-metrics-scripting.md","Context":"## Context\\nWe'd like to be able to script metrics reporting to third party consumers of\\nRaster Foundry. Currently we manually assemble metrics when people ask for\\nthem. However, enough time has passed and we have consistent enough obligations\\nthat we know a few that would be scriptable. This ADR seeks to answer the\\nquestion of how we ought to script production of a particular subset of\\nmetrics, specifically, tile server requests by user.\\n","Decision":"We should start with the second option, being mindful of the potential later\\nneed to target configurable metrics sinks, but not designing for that\\nparticular problem from the outset. We\u2019ll start with tile metrics and a\\ndatabase table that holds just enough to get us the metrics we want.\\nThe reasons the second option is likely to be safer are that it avoids\\nincreasing infrastructure complexity and that the marginal cost of its lock-in\\nis lower than the marginal cost of additional vendor lock-in. We bear the\\nburden of someday migrating everything from http4s and Postgres if we need to,\\nwhile additional AWS lock-in is a cost that either makes it more difficult for\\nanyone to deploy a bare metal solution or makes the cost of paying us to do so\\nmore expensive.\\nThe second option is also more extensible, since storing this information in\\nthe database allows us to expose it via an API and to use the same information\\nfor authorization that we use elsewhere.\\nThe downside of choosing the second option is that it couples metrics updates\\nto our release cycle, while we could have avoided that coupling with the AWS\\noption by throwing every piece of information we have into a data lake. This\\ncost is acceptable given the other benefits and is in line with a strategy of\\nnot collecting data for the sake of collecting data.\\n","tokens":92,"id":1685,"text":"## Context\\nWe'd like to be able to script metrics reporting to third party consumers of\\nRaster Foundry. Currently we manually assemble metrics when people ask for\\nthem. However, enough time has passed and we have consistent enough obligations\\nthat we know a few that would be scriptable. This ADR seeks to answer the\\nquestion of how we ought to script production of a particular subset of\\nmetrics, specifically, tile server requests by user.\\n\n\n##Decision\nWe should start with the second option, being mindful of the potential later\\nneed to target configurable metrics sinks, but not designing for that\\nparticular problem from the outset. We\u2019ll start with tile metrics and a\\ndatabase table that holds just enough to get us the metrics we want.\\nThe reasons the second option is likely to be safer are that it avoids\\nincreasing infrastructure complexity and that the marginal cost of its lock-in\\nis lower than the marginal cost of additional vendor lock-in. We bear the\\nburden of someday migrating everything from http4s and Postgres if we need to,\\nwhile additional AWS lock-in is a cost that either makes it more difficult for\\nanyone to deploy a bare metal solution or makes the cost of paying us to do so\\nmore expensive.\\nThe second option is also more extensible, since storing this information in\\nthe database allows us to expose it via an API and to use the same information\\nfor authorization that we use elsewhere.\\nThe downside of choosing the second option is that it couples metrics updates\\nto our release cycle, while we could have avoided that coupling with the AWS\\noption by throwing every piece of information we have into a data lake. This\\ncost is acceptable given the other benefits and is in line with a strategy of\\nnot collecting data for the sake of collecting data.\\n"}
{"File Name":"paas-team-manual\/ADR045-aws-waf.html.md","Context":"## Context\\nGOV.UK PaaS uses [AWS Shield Advanced](https:\/\/aws.amazon.com\/shield\/features\/#AWS_Shield_Advanced) as well as AWS WAF to protect from DDoS attacks.\\nHowever the mitigations are not automatic and we have access to the AWS DDoS Response Team\\n(DRT) who are experts in mitigating these types of attack.\\n```\\nShield Advanced detects web application layer vectors, like web request floods and\\nlow-and-slow bad bots, but does not automatically mitigate them. To mitigate web\\napplication layer vectors, you must employ AWS WAF rules or the DRT must employ the\\nrules on your behalf.\\n```\\nIn order to be functional they require access to our AWS WAF logs in order to identify what\\nthe attack is and where is is coming from, and API access to the WAF in order to apply the\\nmitigating rules.\\nTo enagage the AWS DRT team we will set up CloudWatch alarms on our WAF rules in order to trigger\\nthe [emergency engagement Lambda](https:\/\/s3.amazonaws.com\/aws-shield-lambda\/ShieldEngagementLambda.pdf)\\n","Decision":"We will grant access to the AWS DRT to read from restricted S3 buckets\\n","tokens":243,"id":188,"text":"## Context\\nGOV.UK PaaS uses [AWS Shield Advanced](https:\/\/aws.amazon.com\/shield\/features\/#AWS_Shield_Advanced) as well as AWS WAF to protect from DDoS attacks.\\nHowever the mitigations are not automatic and we have access to the AWS DDoS Response Team\\n(DRT) who are experts in mitigating these types of attack.\\n```\\nShield Advanced detects web application layer vectors, like web request floods and\\nlow-and-slow bad bots, but does not automatically mitigate them. To mitigate web\\napplication layer vectors, you must employ AWS WAF rules or the DRT must employ the\\nrules on your behalf.\\n```\\nIn order to be functional they require access to our AWS WAF logs in order to identify what\\nthe attack is and where is is coming from, and API access to the WAF in order to apply the\\nmitigating rules.\\nTo enagage the AWS DRT team we will set up CloudWatch alarms on our WAF rules in order to trigger\\nthe [emergency engagement Lambda](https:\/\/s3.amazonaws.com\/aws-shield-lambda\/ShieldEngagementLambda.pdf)\\n\n\n##Decision\nWe will grant access to the AWS DRT to read from restricted S3 buckets\\n"}
{"File Name":"Head-Start-TTADP\/0014-web-analytics.md","Context":"## Context\\nWe need to capture information about user behaviors and task completion on the website. We should use previously approved systems for capturing this information. This excludes services that capture and track web analytics in external systems, e.g. Google Analytics.\\n","Decision":"We will use New Relic to capture and track web analytics. New Relic provides [browser monitoring](https:\/\/docs.newrelic.com\/docs\/browser\/) that can capture essential metrics such as page views, and offers both [agent and SPA API](https:\/\/docs.newrelic.com\/docs\/browser\/new-relic-browser\/browser-agent-spa-api\/) and [APM](https:\/\/developer.newrelic.com\/collect-data\/custom-attributes) methods for capturing custom data.\\n","tokens":49,"id":1165,"text":"## Context\\nWe need to capture information about user behaviors and task completion on the website. We should use previously approved systems for capturing this information. This excludes services that capture and track web analytics in external systems, e.g. Google Analytics.\\n\n\n##Decision\nWe will use New Relic to capture and track web analytics. New Relic provides [browser monitoring](https:\/\/docs.newrelic.com\/docs\/browser\/) that can capture essential metrics such as page views, and offers both [agent and SPA API](https:\/\/docs.newrelic.com\/docs\/browser\/new-relic-browser\/browser-agent-spa-api\/) and [APM](https:\/\/developer.newrelic.com\/collect-data\/custom-attributes) methods for capturing custom data.\\n"}
{"File Name":"python-tuf\/0006-where-to-implemenent-model-serialization.md","Context":"## Context and Problem Statement\\nIn the course of implementing a class-based role metadata model we have also\\nreviewed options on how to design serialization infrastructure between wire\\nformats and the class model. In an initial attempt we have implemented\\nserialization on the metadata class (see option 1), but issues with inheritance\\nand calls for more flexibility have caused us to rethink this approach.\\n## Decision Drivers\\n* A class-based role metadata model (see ADR4) requires serialization routines\\nfrom and to wire formats\\n* TUF integrators may require custom serialization implementations for custom\\nwire formats\\n* Readability and simplicity of implementation for users and maintainers\\n* Recognizability of specification\\n","Decision":"* A class-based role metadata model (see ADR4) requires serialization routines\\nfrom and to wire formats\\n* TUF integrators may require custom serialization implementations for custom\\nwire formats\\n* Readability and simplicity of implementation for users and maintainers\\n* Recognizability of specification\\nChosen option: \"Compromise 2\", because implementing dict conversion as methods\\non a corresponding class is idiomatic and allows for well-structured code.\\nTogether with a separated serialization interface, it provides both ease of use\\nand maintenance, and full flexibility with regards to custom serialization\\nimplementations and wire formats.\\n","tokens":140,"id":4254,"text":"## Context and Problem Statement\\nIn the course of implementing a class-based role metadata model we have also\\nreviewed options on how to design serialization infrastructure between wire\\nformats and the class model. In an initial attempt we have implemented\\nserialization on the metadata class (see option 1), but issues with inheritance\\nand calls for more flexibility have caused us to rethink this approach.\\n## Decision Drivers\\n* A class-based role metadata model (see ADR4) requires serialization routines\\nfrom and to wire formats\\n* TUF integrators may require custom serialization implementations for custom\\nwire formats\\n* Readability and simplicity of implementation for users and maintainers\\n* Recognizability of specification\\n\n\n##Decision\n* A class-based role metadata model (see ADR4) requires serialization routines\\nfrom and to wire formats\\n* TUF integrators may require custom serialization implementations for custom\\nwire formats\\n* Readability and simplicity of implementation for users and maintainers\\n* Recognizability of specification\\nChosen option: \"Compromise 2\", because implementing dict conversion as methods\\non a corresponding class is idiomatic and allows for well-structured code.\\nTogether with a separated serialization interface, it provides both ease of use\\nand maintenance, and full flexibility with regards to custom serialization\\nimplementations and wire formats.\\n"}
{"File Name":"CardsApp\/card-operations-database-choice.md","Context":"##Context\\nWe need to choose the database for our CardOperations. The database needs to store data in persistent form but it's not required that data is stored across sessions.\\nApplication is a prototype, so it should be good for a quick setup.\\n","Decision":"We decided to go with H2 database for its ease of use, easy setup and familiarity.\\n","tokens":52,"id":4065,"text":"##Context\\nWe need to choose the database for our CardOperations. The database needs to store data in persistent form but it's not required that data is stored across sessions.\\nApplication is a prototype, so it should be good for a quick setup.\\n\n\n##Decision\nWe decided to go with H2 database for its ease of use, easy setup and familiarity.\\n"}
{"File Name":"amf-core\/0005-expose-non-scalajs-types-in-amf-client-remote-content.md","Context":"## Context\\nTo adopt the ScalaJSTypings plugin, usages of Scala types that were not exported to ScalaJS were removed from the scala interface.\\nThe Api Designer product uses the `Content.stream` field and calls `toString()` on it. As this field is of type CharStream we hid\\nit from export.\\n","Decision":"- Rollback the interface change for the `amf.client.remote.Content` class so that the `toString()` method can be called on the `stream` field.\\n- Add the `toString()` method in `Content` that returns the content in `stream`\\n","tokens":67,"id":403,"text":"## Context\\nTo adopt the ScalaJSTypings plugin, usages of Scala types that were not exported to ScalaJS were removed from the scala interface.\\nThe Api Designer product uses the `Content.stream` field and calls `toString()` on it. As this field is of type CharStream we hid\\nit from export.\\n\n\n##Decision\n- Rollback the interface change for the `amf.client.remote.Content` class so that the `toString()` method can be called on the `stream` field.\\n- Add the `toString()` method in `Content` that returns the content in `stream`\\n"}
{"File Name":"otm-docs\/containerization.md","Context":"## Context\\nIt is important to ensure for each team member the same environment configuration in which the application will operate.\\n","Decision":"Docker\\n","tokens":24,"id":3060,"text":"## Context\\nIt is important to ensure for each team member the same environment configuration in which the application will operate.\\n\n\n##Decision\nDocker\\n"}
{"File Name":"celestia-core\/adr-010-crypto-changes.md","Context":"## Context\\nTendermint is a cryptographic protocol that uses and composes a variety of cryptographic primitives.\\nAfter nearly 4 years of development, Tendermint has recently undergone multiple security reviews to search for vulnerabilities and to assess the the use and composition of cryptographic primitives.\\n### Hash Functions\\nTendermint uses RIPEMD160 universally as a hash function, most notably in its Merkle tree implementation.\\nRIPEMD160 was chosen because it provides the shortest fingerprint that is long enough to be considered secure (ie. birthday bound of 80-bits).\\nIt was also developed in the open academic community, unlike NSA-designed algorithms like SHA256.\\nThat said, the cryptographic community appears to unanimously agree on the security of SHA256. It has become a universal standard, especially now that SHA1 is broken, being required in TLS connections and having optimized support in hardware.\\n### Merkle Trees\\nTendermint uses a simple Merkle tree to compute digests of large structures like transaction batches\\nand even blockchain headers. The Merkle tree length prefixes byte arrays before concatenating and hashing them.\\nIt uses RIPEMD160.\\n### Addresses\\nED25519 addresses are computed using the RIPEMD160 of the Amino encoding of the public key.\\nRIPEMD160 is generally considered an outdated hash function, and is much slower\\nthan more modern functions like SHA256 or Blake2.\\n### Authenticated Encryption\\nTendermint P2P connections use authenticated encryption to provide privacy and authentication in the communications.\\nThis is done using the simple Station-to-Station protocol with the NaCL Ed25519 library.\\nWhile there have been no vulnerabilities found in the implementation, there are some concerns:\\n- NaCL uses Salsa20, a not-widely used and relatively out-dated stream cipher that has been obsoleted by ChaCha20\\n- Connections use RIPEMD160 to compute a value that is used for the encryption nonce with subtle requirements on how it's used\\n","Decision":"### Hash Functions\\nUse the first 20-bytes of the SHA256 hash instead of RIPEMD160 for everything\\n### Merkle Trees\\nTODO\\n### Addresses\\nCompute ED25519 addresses as the first 20-bytes of the SHA256 of the raw 32-byte public key\\n### Authenticated Encryption\\nMake the following changes:\\n- Use xChaCha20 instead of xSalsa20 - https:\/\/github.com\/tendermint\/tendermint\/issues\/1124\\n- Use an HKDF instead of RIPEMD160 to compute nonces - https:\/\/github.com\/tendermint\/tendermint\/issues\/1165\\n","tokens":410,"id":4110,"text":"## Context\\nTendermint is a cryptographic protocol that uses and composes a variety of cryptographic primitives.\\nAfter nearly 4 years of development, Tendermint has recently undergone multiple security reviews to search for vulnerabilities and to assess the the use and composition of cryptographic primitives.\\n### Hash Functions\\nTendermint uses RIPEMD160 universally as a hash function, most notably in its Merkle tree implementation.\\nRIPEMD160 was chosen because it provides the shortest fingerprint that is long enough to be considered secure (ie. birthday bound of 80-bits).\\nIt was also developed in the open academic community, unlike NSA-designed algorithms like SHA256.\\nThat said, the cryptographic community appears to unanimously agree on the security of SHA256. It has become a universal standard, especially now that SHA1 is broken, being required in TLS connections and having optimized support in hardware.\\n### Merkle Trees\\nTendermint uses a simple Merkle tree to compute digests of large structures like transaction batches\\nand even blockchain headers. The Merkle tree length prefixes byte arrays before concatenating and hashing them.\\nIt uses RIPEMD160.\\n### Addresses\\nED25519 addresses are computed using the RIPEMD160 of the Amino encoding of the public key.\\nRIPEMD160 is generally considered an outdated hash function, and is much slower\\nthan more modern functions like SHA256 or Blake2.\\n### Authenticated Encryption\\nTendermint P2P connections use authenticated encryption to provide privacy and authentication in the communications.\\nThis is done using the simple Station-to-Station protocol with the NaCL Ed25519 library.\\nWhile there have been no vulnerabilities found in the implementation, there are some concerns:\\n- NaCL uses Salsa20, a not-widely used and relatively out-dated stream cipher that has been obsoleted by ChaCha20\\n- Connections use RIPEMD160 to compute a value that is used for the encryption nonce with subtle requirements on how it's used\\n\n\n##Decision\n### Hash Functions\\nUse the first 20-bytes of the SHA256 hash instead of RIPEMD160 for everything\\n### Merkle Trees\\nTODO\\n### Addresses\\nCompute ED25519 addresses as the first 20-bytes of the SHA256 of the raw 32-byte public key\\n### Authenticated Encryption\\nMake the following changes:\\n- Use xChaCha20 instead of xSalsa20 - https:\/\/github.com\/tendermint\/tendermint\/issues\/1124\\n- Use an HKDF instead of RIPEMD160 to compute nonces - https:\/\/github.com\/tendermint\/tendermint\/issues\/1165\\n"}
{"File Name":"Horace\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":4248,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n"}
{"File Name":"AdventOfCode-Java-learning-project\/0001-creational-patterns.md","Context":"## Context\\nWe want to have a common contract for the way that our specific solutions for each Day behave in the application context. For that, we will enforce the implementation of specific methods. In addition, we want to have an easy access to those specific implementations and will rely on Spring's application context.\\n","Decision":"The interface `Days.class` is part of an _Abstract Factory_ for concrete Day classes, allowing us to use polymorphism for days. The instances for the specific Days are created and provided by the Spring context upon annotating the specific days with `@Component` and making them implement the interface, so it is not a real factory - just part of it.\\nIn the `AdventOfCodeService.class` (think of it as a _Facade_ to the subsystem), a `List<Days>`, which is populated by Spring with instances of the specific Days, is used by a _factory method_, or _repository method_ if you want to name it that way in order to find the correct instance for a specific requested day. Note that `AdventOfCodeService.class` is in our case part of the creation, but has also Structural Patterns characteristics. We will cover that more deeply in a corresponding ADR section.\\n","tokens":62,"id":1756,"text":"## Context\\nWe want to have a common contract for the way that our specific solutions for each Day behave in the application context. For that, we will enforce the implementation of specific methods. In addition, we want to have an easy access to those specific implementations and will rely on Spring's application context.\\n\n\n##Decision\nThe interface `Days.class` is part of an _Abstract Factory_ for concrete Day classes, allowing us to use polymorphism for days. The instances for the specific Days are created and provided by the Spring context upon annotating the specific days with `@Component` and making them implement the interface, so it is not a real factory - just part of it.\\nIn the `AdventOfCodeService.class` (think of it as a _Facade_ to the subsystem), a `List<Days>`, which is populated by Spring with instances of the specific Days, is used by a _factory method_, or _repository method_ if you want to name it that way in order to find the correct instance for a specific requested day. Note that `AdventOfCodeService.class` is in our case part of the creation, but has also Structural Patterns characteristics. We will cover that more deeply in a corresponding ADR section.\\n"}
{"File Name":"SearchServices\/0009-message-driven-content-tracker-next-gen.md","Context":"## Context\\nThis is a second iteration of content tracking via message bus design. See [previous version](0007-message-driven-content-tracker.md).\\nNew content tracker implementation will be based on new Search Services architecture (SS v2.0 or next gen). Main context behind this decision is almost the same as for v1 - get more throughput by leveraging new Transform Service.\\n","Decision":"The decision is based on [version 1](0007-message-driven-content-tracker.md). The main differences are:\\n* Shared File Store may not be the right option as it is only available for Enterprise. Alternatively the URL to content can point to other locations. (TBC)\\n* The change in behaviour requires a major release of Search Services, most likely version 2.0.\\n* The changes in Content Repository will be available from version 6.3.\\n* The synchronous transformation APIs will remain functional until 7.0.\\nDetails of the architecture to be clarified (WIP).\\n","tokens":77,"id":5127,"text":"## Context\\nThis is a second iteration of content tracking via message bus design. See [previous version](0007-message-driven-content-tracker.md).\\nNew content tracker implementation will be based on new Search Services architecture (SS v2.0 or next gen). Main context behind this decision is almost the same as for v1 - get more throughput by leveraging new Transform Service.\\n\n\n##Decision\nThe decision is based on [version 1](0007-message-driven-content-tracker.md). The main differences are:\\n* Shared File Store may not be the right option as it is only available for Enterprise. Alternatively the URL to content can point to other locations. (TBC)\\n* The change in behaviour requires a major release of Search Services, most likely version 2.0.\\n* The changes in Content Repository will be available from version 6.3.\\n* The synchronous transformation APIs will remain functional until 7.0.\\nDetails of the architecture to be clarified (WIP).\\n"}
{"File Name":"visit-plannr\/0003-cloudfront-distribution-means-there-is-long-lived-infrastruture.md","Context":"## Context\\nThe original intention was to be able to build the entire application stack from a single template. However, adding a [cloudfront distribution](https:\/\/docs.aws.amazon.com\/AmazonCloudFront\/latest\/DeveloperGuide\/Introduction.html) takes 15-25 minutes to complete.\\n","Decision":"To have two infrastructure templates. One with slow to provision infrastructure. A second with the fast to provision infrastructure.\\n","tokens":58,"id":1987,"text":"## Context\\nThe original intention was to be able to build the entire application stack from a single template. However, adding a [cloudfront distribution](https:\/\/docs.aws.amazon.com\/AmazonCloudFront\/latest\/DeveloperGuide\/Introduction.html) takes 15-25 minutes to complete.\\n\n\n##Decision\nTo have two infrastructure templates. One with slow to provision infrastructure. A second with the fast to provision infrastructure.\\n"}
{"File Name":"beis-report-official-development-assistance\/0003-use-dotenv-for-managing-environment-variables.md","Context":"## Context\\nAccessing ENV directly without a wrapper is limited and can introduce problems.\\nWe want our tooling to help us guard against missing environment variables. When `nil` is accidentally provided during the start up process it is preferable to fail fast with an explicit message. Without this `nil` can be passed down through the stack and cause strange behaviour where the code has been designed with it as a dependency. Instead of adding `nil` guards throughout the codebase for required environment variables (eg. `ENV.fetch('FOO', 'default')`, this should be managed centrally.\\nWe have previously used Figaro for this purpose but it was deprecated in 2016 https:\/\/github.com\/laserlemon\/figaro. We should only use supported gems to ensure we get support in the form of fixes and security patches.\\nWe also want to be able to stub our environment variables in our test suite. An easy example of this is when we use environment variables as a feature flag mechanism. We want to stub the value to test both scenarios without being influenced by real values being loaded. Mutating the actual ENV value (eg. ` allow(ENV).to receive(:[]).with('BOX_ID').and_return(\"1234\")`) is possible but may have unexpected consequences where more than 1 part of the same process under test uses the same variable. Figaro used to be a handy abstraction layer that we could stub eg. `allow(Figaro).to receive(:env).with(:foo).and_return('bar')`. We should then consider how we can stub environment variables.\\n","Decision":"Use DotEnv to load our environment variables.\\n","tokens":324,"id":2390,"text":"## Context\\nAccessing ENV directly without a wrapper is limited and can introduce problems.\\nWe want our tooling to help us guard against missing environment variables. When `nil` is accidentally provided during the start up process it is preferable to fail fast with an explicit message. Without this `nil` can be passed down through the stack and cause strange behaviour where the code has been designed with it as a dependency. Instead of adding `nil` guards throughout the codebase for required environment variables (eg. `ENV.fetch('FOO', 'default')`, this should be managed centrally.\\nWe have previously used Figaro for this purpose but it was deprecated in 2016 https:\/\/github.com\/laserlemon\/figaro. We should only use supported gems to ensure we get support in the form of fixes and security patches.\\nWe also want to be able to stub our environment variables in our test suite. An easy example of this is when we use environment variables as a feature flag mechanism. We want to stub the value to test both scenarios without being influenced by real values being loaded. Mutating the actual ENV value (eg. ` allow(ENV).to receive(:[]).with('BOX_ID').and_return(\"1234\")`) is possible but may have unexpected consequences where more than 1 part of the same process under test uses the same variable. Figaro used to be a handy abstraction layer that we could stub eg. `allow(Figaro).to receive(:env).with(:foo).and_return('bar')`. We should then consider how we can stub environment variables.\\n\n\n##Decision\nUse DotEnv to load our environment variables.\\n"}
{"File Name":"SAP-library\/cx-server-in-container.md","Context":"## Context\\nWe have a bash script (`cx-server`), which orchestrates the Docker containers used by our Cx Server.\\nUsing bash is inconvenient for Windows users, as Windows does not include bash by default.\\nThere is options for running bash on Windows, such as the Windows Subsystem for Linux, but this is not trivial to setup and requires switching Windows to Developer Mode.\\nOther options include running a virtual machine locally, or connecting to a remove Linux system, but both are not always possible and have too much overhead.\\nRecently, we added a \"companion\" Docker image which is used by `cx-server` to run scripts.\\nUnrelated, the idea was born to move `cx-server` into this image, so the remaining `cx-server` is a very thin wrapper which can also be added as a Windows compatible script file.\\n","Decision":"We move the bash script inside the `s4sdk\/cxserver-companion` Docker image.\\nThe old `cx-server` script just delegates the command to the script inside the companion container.\\nA new `cx-server.bat` script is added, doing the same for Windows.\\nWe don't use PowerShell to increase compatibility with Windows.\\n","tokens":175,"id":849,"text":"## Context\\nWe have a bash script (`cx-server`), which orchestrates the Docker containers used by our Cx Server.\\nUsing bash is inconvenient for Windows users, as Windows does not include bash by default.\\nThere is options for running bash on Windows, such as the Windows Subsystem for Linux, but this is not trivial to setup and requires switching Windows to Developer Mode.\\nOther options include running a virtual machine locally, or connecting to a remove Linux system, but both are not always possible and have too much overhead.\\nRecently, we added a \"companion\" Docker image which is used by `cx-server` to run scripts.\\nUnrelated, the idea was born to move `cx-server` into this image, so the remaining `cx-server` is a very thin wrapper which can also be added as a Windows compatible script file.\\n\n\n##Decision\nWe move the bash script inside the `s4sdk\/cxserver-companion` Docker image.\\nThe old `cx-server` script just delegates the command to the script inside the companion container.\\nA new `cx-server.bat` script is added, doing the same for Windows.\\nWe don't use PowerShell to increase compatibility with Windows.\\n"}
{"File Name":"linshare-mobile-flutter-app\/0007-path-provider-library.md","Context":"## Context\\nAndroid and iOS have different folder architectures.\\nTherefore we need to provide *path folder* according to user's plateform.\\n`path-provider` library only provide *Download Folder* for Android Version.\\n","Decision":"We decided to use `path-provider` if the user is on Android, and use an other library (TBD) if the user is on iOS.\\n","tokens":45,"id":3301,"text":"## Context\\nAndroid and iOS have different folder architectures.\\nTherefore we need to provide *path folder* according to user's plateform.\\n`path-provider` library only provide *Download Folder* for Android Version.\\n\n\n##Decision\nWe decided to use `path-provider` if the user is on Android, and use an other library (TBD) if the user is on iOS.\\n"}
{"File Name":"super-eks\/0005-use-namespaces-grouped-by-functionality.md","Context":"## Context\\nWhen installing manifests or Helm charts you need to pick a namespace.\\nOften the choice is made in a drive by fashion, leading to inconsistent configuration.\\n","Decision":"We want to install all the shipped components into namespaces grouped by functionality.\\nFor example `external-dns` goes into the namespace `dns`.\\n`fluent-bit` goes into the namespace `logging`.\\n","tokens":34,"id":2031,"text":"## Context\\nWhen installing manifests or Helm charts you need to pick a namespace.\\nOften the choice is made in a drive by fashion, leading to inconsistent configuration.\\n\n\n##Decision\nWe want to install all the shipped components into namespaces grouped by functionality.\\nFor example `external-dns` goes into the namespace `dns`.\\n`fluent-bit` goes into the namespace `logging`.\\n"}
{"File Name":"edgex-docs\/0002-Array-Datatypes.md","Context":"- [Context](#context)\\n- [Decision](#decision)\\n- [Consequences](#consequences)\\n<!--te-->\\n","Decision":"- [Consequences](#consequences)\\n<!--te-->\\n### DeviceProfile extension\\nThe permitted values of the `Type` field in `PropertyValue` are extended to include:\\n\"BoolArray\", \"Uint8Array\", \"Uint16Array\", \"Uint32Array\", \"Uint64Array\", \"Int8Array\", Int16Array\", \"Int32Array\", \"Int64Array\", \"Float32Array\", \"Float64Array\"\\n### Readings\\nIn the API (v1 and v2), `Reading.Value` is a string representation of the data. If this is maintained, the representation for Array types will follow the JSON array syntax, ie `[\"value1\", \"value2\", ...]`\\n","tokens":31,"id":959,"text":"- [Context](#context)\\n- [Decision](#decision)\\n- [Consequences](#consequences)\\n<!--te-->\\n\n\n##Decision\n- [Consequences](#consequences)\\n<!--te-->\\n### DeviceProfile extension\\nThe permitted values of the `Type` field in `PropertyValue` are extended to include:\\n\"BoolArray\", \"Uint8Array\", \"Uint16Array\", \"Uint32Array\", \"Uint64Array\", \"Int8Array\", Int16Array\", \"Int32Array\", \"Int64Array\", \"Float32Array\", \"Float64Array\"\\n### Readings\\nIn the API (v1 and v2), `Reading.Value` is a string representation of the data. If this is maintained, the representation for Array types will follow the JSON array syntax, ie `[\"value1\", \"value2\", ...]`\\n"}
{"File Name":"nada-kafkarator\/0007-only-for-aiven.md","Context":"## Context\\nNAV currently has Kafka running on-premise, but have plans to move everything to the cloud.\\nKafkarator will create an integration between applications and Kafka, removing some manual work and putting more configuration in code.\\nThe plan is to buy hosted Kafka from Aiven, eliminating the need for maintaining our own Kafka cluster on-premise.\\n","Decision":"Kafkarator will only deal with the future solution using Aiven, and not work for on-premise Kafka.\\n","tokens":72,"id":863,"text":"## Context\\nNAV currently has Kafka running on-premise, but have plans to move everything to the cloud.\\nKafkarator will create an integration between applications and Kafka, removing some manual work and putting more configuration in code.\\nThe plan is to buy hosted Kafka from Aiven, eliminating the need for maintaining our own Kafka cluster on-premise.\\n\n\n##Decision\nKafkarator will only deal with the future solution using Aiven, and not work for on-premise Kafka.\\n"}
{"File Name":"nso.aurora\/Notifier.md","Context":"## Context\\nWe need a way to notifiy customers for doing a survey after their purchases, provide them with recommendations, letting them know how their orders via email and\/or SMS.\\n","Decision":"A Notifier component is needed to send email and\/SMS to customers.\\n","tokens":38,"id":300,"text":"## Context\\nWe need a way to notifiy customers for doing a survey after their purchases, provide them with recommendations, letting them know how their orders via email and\/or SMS.\\n\n\n##Decision\nA Notifier component is needed to send email and\/SMS to customers.\\n"}
{"File Name":"tendermint\/adr-071-proposer-based-timestamps.md","Context":"## Context\\nTendermint currently provides a monotonically increasing source of time known as [BFTTime](https:\/\/github.com\/tendermint\/tendermint\/blob\/master\/spec\/consensus\/bft-time.md).\\nThis mechanism for producing a source of time is reasonably simple.\\nEach correct validator adds a timestamp to each `Precommit` message it sends.\\nThe timestamp it sends is either the validator's current known Unix time or one millisecond greater than the previous block time, depending on which value is greater.\\nWhen a block is produced, the proposer chooses the block timestamp as the weighted median of the times in all of the `Precommit` messages the proposer received.\\nThe weighting is proportional to the amount of voting power, or stake, a validator has on the network.\\nThis mechanism for producing timestamps is both deterministic and byzantine fault tolerant.\\nThis current mechanism for producing timestamps has a few drawbacks.\\nValidators do not have to agree at all on how close the selected block timestamp is to their own currently known Unix time.\\nAdditionally, any amount of voting power `>1\/3` may directly control the block timestamp.\\nAs a result, it is quite possible that the timestamp is not particularly meaningful.\\nThese drawbacks present issues in the Tendermint protocol.\\nTimestamps are used by light clients to verify blocks.\\nLight clients rely on correspondence between their own currently known Unix time and the block timestamp to verify blocks they see;\\nHowever, their currently known Unix time may be greatly divergent from the block timestamp as a result of the limitations of `BFTTime`.\\nThe proposer-based timestamps specification suggests an alternative approach for producing block timestamps that remedies these issues.\\nProposer-based timestamps alter the current mechanism for producing block timestamps in two main ways:\\n1. The block proposer is amended to offer up its currently known Unix time as the timestamp for the next block instead of the `BFTTime`.\\n1. Correct validators only approve the proposed block timestamp if it is close enough to their own currently known Unix time.\\nThe result of these changes is a more meaningful timestamp that cannot be controlled by `<= 2\/3` of the validator voting power.\\nThis document outlines the necessary code changes in Tendermint to implement the corresponding [proposer-based timestamps specification](https:\/\/github.com\/tendermint\/tendermint\/tree\/master\/spec\/consensus\/proposer-based-timestamp).\\n","Decision":"Implement proposer-based timestamps and remove `BFTTime`.\\n","tokens":495,"id":1969,"text":"## Context\\nTendermint currently provides a monotonically increasing source of time known as [BFTTime](https:\/\/github.com\/tendermint\/tendermint\/blob\/master\/spec\/consensus\/bft-time.md).\\nThis mechanism for producing a source of time is reasonably simple.\\nEach correct validator adds a timestamp to each `Precommit` message it sends.\\nThe timestamp it sends is either the validator's current known Unix time or one millisecond greater than the previous block time, depending on which value is greater.\\nWhen a block is produced, the proposer chooses the block timestamp as the weighted median of the times in all of the `Precommit` messages the proposer received.\\nThe weighting is proportional to the amount of voting power, or stake, a validator has on the network.\\nThis mechanism for producing timestamps is both deterministic and byzantine fault tolerant.\\nThis current mechanism for producing timestamps has a few drawbacks.\\nValidators do not have to agree at all on how close the selected block timestamp is to their own currently known Unix time.\\nAdditionally, any amount of voting power `>1\/3` may directly control the block timestamp.\\nAs a result, it is quite possible that the timestamp is not particularly meaningful.\\nThese drawbacks present issues in the Tendermint protocol.\\nTimestamps are used by light clients to verify blocks.\\nLight clients rely on correspondence between their own currently known Unix time and the block timestamp to verify blocks they see;\\nHowever, their currently known Unix time may be greatly divergent from the block timestamp as a result of the limitations of `BFTTime`.\\nThe proposer-based timestamps specification suggests an alternative approach for producing block timestamps that remedies these issues.\\nProposer-based timestamps alter the current mechanism for producing block timestamps in two main ways:\\n1. The block proposer is amended to offer up its currently known Unix time as the timestamp for the next block instead of the `BFTTime`.\\n1. Correct validators only approve the proposed block timestamp if it is close enough to their own currently known Unix time.\\nThe result of these changes is a more meaningful timestamp that cannot be controlled by `<= 2\/3` of the validator voting power.\\nThis document outlines the necessary code changes in Tendermint to implement the corresponding [proposer-based timestamps specification](https:\/\/github.com\/tendermint\/tendermint\/tree\/master\/spec\/consensus\/proposer-based-timestamp).\\n\n\n##Decision\nImplement proposer-based timestamps and remove `BFTTime`.\\n"}
{"File Name":"dalmatian-frontend\/0002-use-bullet-to-catch-nplus1-queries.md","Context":"## Context\\nIt can be easy to miss an inefficient database query during code review. These can build up and have detremental performance on the application and effect the user experience.\\n","Decision":"Add an automatic check to the test suite to ensure (through CI) that these are fixed before being deployed.\\n","tokens":37,"id":2553,"text":"## Context\\nIt can be easy to miss an inefficient database query during code review. These can build up and have detremental performance on the application and effect the user experience.\\n\n\n##Decision\nAdd an automatic check to the test suite to ensure (through CI) that these are fixed before being deployed.\\n"}
{"File Name":"cloud-sdk-js\/0017-builder-methods-multiple-calls.md","Context":"## Context\\nIn the request builder you can currently call methods like `filter` and `top` multiple times.\\nIn this document we discuss possibilities to unify this the behavior when this is done.\\n","Decision":"We decided to use option `A` and `B` depending on the methods.\\nFor the OData related options we will use option `A`.\\nFor configuration related builder methods we will use the verbs `add` or `set` to make clear if it is option `A` or `B`.\\nThe relevant builder methods (the one containing arguments) are listed in the table below including the decision on the option:\\n| class                           | methods                                                                                                                                                                              |\\n| ------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |\\n| request-builder-base            | withCustomHeader ( refactor to addCustomHeaders B)<br>withCustomQueryParameters (refactor to addCustomQueryParameters B)<br>withCustomServicePath (A rename to setCustomServicePath) |\\n| get-all-request-builder-base    | select (A) <br>orderBy (A) <br>top (A) <br>skip (A)                                                                                                                                  |\\n| get-all-request-builder-v2\/v4   | filter (A)<br>expand(v4)                                                                                                                                                             |\\n| get-by-key-request-builder-base | select (A refactoring move to base)                                                                                                                                                  |\\n| get-by-key-request-builder-v4   | expand(A)                                                                                                                                                                            |\\n| delete-request-builder-v2\/v4    | setVersionIdentifier (A)                                                                                                                                                             |\\n| update-request-builder-base     | requiredFields (refactor setRequiredFields A)<br>ignoredFields (refactor setRequiredFields A)<br>withCustomVersionIdentifier (refactor align with setVersionIdentifier A)            |\\n","tokens":41,"id":3620,"text":"## Context\\nIn the request builder you can currently call methods like `filter` and `top` multiple times.\\nIn this document we discuss possibilities to unify this the behavior when this is done.\\n\n\n##Decision\nWe decided to use option `A` and `B` depending on the methods.\\nFor the OData related options we will use option `A`.\\nFor configuration related builder methods we will use the verbs `add` or `set` to make clear if it is option `A` or `B`.\\nThe relevant builder methods (the one containing arguments) are listed in the table below including the decision on the option:\\n| class                           | methods                                                                                                                                                                              |\\n| ------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |\\n| request-builder-base            | withCustomHeader ( refactor to addCustomHeaders B)<br>withCustomQueryParameters (refactor to addCustomQueryParameters B)<br>withCustomServicePath (A rename to setCustomServicePath) |\\n| get-all-request-builder-base    | select (A) <br>orderBy (A) <br>top (A) <br>skip (A)                                                                                                                                  |\\n| get-all-request-builder-v2\/v4   | filter (A)<br>expand(v4)                                                                                                                                                             |\\n| get-by-key-request-builder-base | select (A refactoring move to base)                                                                                                                                                  |\\n| get-by-key-request-builder-v4   | expand(A)                                                                                                                                                                            |\\n| delete-request-builder-v2\/v4    | setVersionIdentifier (A)                                                                                                                                                             |\\n| update-request-builder-base     | requiredFields (refactor setRequiredFields A)<br>ignoredFields (refactor setRequiredFields A)<br>withCustomVersionIdentifier (refactor align with setVersionIdentifier A)            |\\n"}
{"File Name":"simple-server\/007-robust-data-migrations.md","Context":"## Context\\nOn January 20, 2020, the team became aware that a data migration that needed to be executed along with a prior\\nproduction deployment was never run in production. In order to rectify this, the team ran the required data migration on\\nthe evening of January 20. Since the task was CPU (and perhaps memory) intensive, the task ran for ~8 hours overnight,\\nand required constant monitoring from the team to ensure that the task wasn't affecting the regular performance of the\\nSimple Server. After the task was completed, it was noted that the Simple Server _was_ impacted by the task.\\nReferences\\n* [Slack conversation](https:\/\/simpledotorg.slack.com\/archives\/CFHMC60P5\/p1579524825012500)\\nThe problems that this ADR aims to tackle are the following:\\n* A required data migration was not run after the corresponding production deployment\\n* Data migrations require constant supervision to monitor server performance and correct failures\\n* The Simple Server's performance was degraded during the execution of the data migration\\n","Decision":"### Enforcing data migration execution\\nIn order to better enforce the execution of data migrations, we will invoke them using the\\n[`data-migrate`](https:\/\/github.com\/ilyakatz\/data-migrate) gem. This will ensure that data migrations are run with the\\nsame reliability as database migrations, while still allowing database migrations to be run or rolled back independently\\nif necessary. For example, we may ship a data migration that looks like this.\\n```ruby\\nclass UpdateNilensoBPs < ActiveRecord::Migration[5.1]\\ndef up\\nBPUpdater.perform(facility: \"Nilenso\") # An associated service class that performs the data migration\\nend\\ndef down\\nRails.logger.info(\"This data migration will not be reversed. Good luck!\")\\nend\\nend\\n```\\nWe will need to take care with our file management. [See below](#consequences).\\n### Reliable hands-off execution\\nWe will use background jobs to execute data migrations in an asynchronous, atomic, and repeatable fashion. Ideally, the\\ndata migration will be split up into as small atomic parts as possible, and each part will be executed in its own\\nbackground job. For example, a task to update 1000 records will be executed using 1000 Sidekiq jobs, one for each\\nrecord.\\n```ruby\\n# good\\naffected_records.each do |record|\\nUpdateJob.perform_later(record)\\nend\\n# bad - not atomic\\nBulkUpdateJob.perform_later(affected_records)\\n# bad - not asynchronous\\nUpdater.perform_right_now(affected_records)\\n```\\nUsing atomic background jobs will have several advantages.\\n* Failed jobs will be retried automatically with sensible exponential back-off\\n* We avoid the use of very-long-running processes on the server (which could also result in very-long-running database\\nconnections)\\n* It becomes easier to monitor the progress of the migration through the Sidekiq dashboard\\n* The execution of the data migration can be easily throttled, paused, or halted\\nWe will need to take care of our Sidekiq queues. [See below](#consequences).\\n### Other beneficial traits\\n**Idempotence:** Ideally, the data migration task should be repeatable indefinitely without changing the end result.\\nIdempotent tasks will ensure that even if we have to abandon the migration in a half-finished state (eg. Redis falls\\ndown and background jobs are lost), we should be able to re-run the data migration without requiring any code changes.\\n```ruby\\n# good\\nBloodPressure.where(recorded_at: nil).each do |bp|\\nbp.touch(:recorded_at)\\nend\\n# bad - not idempotent\\nBloodPressure.where(id: affected_ids).each do |bp|\\nbp.update(systolic: bp.systolic + 10)\\nend\\n```\\n","tokens":220,"id":1713,"text":"## Context\\nOn January 20, 2020, the team became aware that a data migration that needed to be executed along with a prior\\nproduction deployment was never run in production. In order to rectify this, the team ran the required data migration on\\nthe evening of January 20. Since the task was CPU (and perhaps memory) intensive, the task ran for ~8 hours overnight,\\nand required constant monitoring from the team to ensure that the task wasn't affecting the regular performance of the\\nSimple Server. After the task was completed, it was noted that the Simple Server _was_ impacted by the task.\\nReferences\\n* [Slack conversation](https:\/\/simpledotorg.slack.com\/archives\/CFHMC60P5\/p1579524825012500)\\nThe problems that this ADR aims to tackle are the following:\\n* A required data migration was not run after the corresponding production deployment\\n* Data migrations require constant supervision to monitor server performance and correct failures\\n* The Simple Server's performance was degraded during the execution of the data migration\\n\n\n##Decision\n### Enforcing data migration execution\\nIn order to better enforce the execution of data migrations, we will invoke them using the\\n[`data-migrate`](https:\/\/github.com\/ilyakatz\/data-migrate) gem. This will ensure that data migrations are run with the\\nsame reliability as database migrations, while still allowing database migrations to be run or rolled back independently\\nif necessary. For example, we may ship a data migration that looks like this.\\n```ruby\\nclass UpdateNilensoBPs < ActiveRecord::Migration[5.1]\\ndef up\\nBPUpdater.perform(facility: \"Nilenso\") # An associated service class that performs the data migration\\nend\\ndef down\\nRails.logger.info(\"This data migration will not be reversed. Good luck!\")\\nend\\nend\\n```\\nWe will need to take care with our file management. [See below](#consequences).\\n### Reliable hands-off execution\\nWe will use background jobs to execute data migrations in an asynchronous, atomic, and repeatable fashion. Ideally, the\\ndata migration will be split up into as small atomic parts as possible, and each part will be executed in its own\\nbackground job. For example, a task to update 1000 records will be executed using 1000 Sidekiq jobs, one for each\\nrecord.\\n```ruby\\n# good\\naffected_records.each do |record|\\nUpdateJob.perform_later(record)\\nend\\n# bad - not atomic\\nBulkUpdateJob.perform_later(affected_records)\\n# bad - not asynchronous\\nUpdater.perform_right_now(affected_records)\\n```\\nUsing atomic background jobs will have several advantages.\\n* Failed jobs will be retried automatically with sensible exponential back-off\\n* We avoid the use of very-long-running processes on the server (which could also result in very-long-running database\\nconnections)\\n* It becomes easier to monitor the progress of the migration through the Sidekiq dashboard\\n* The execution of the data migration can be easily throttled, paused, or halted\\nWe will need to take care of our Sidekiq queues. [See below](#consequences).\\n### Other beneficial traits\\n**Idempotence:** Ideally, the data migration task should be repeatable indefinitely without changing the end result.\\nIdempotent tasks will ensure that even if we have to abandon the migration in a half-finished state (eg. Redis falls\\ndown and background jobs are lost), we should be able to re-run the data migration without requiring any code changes.\\n```ruby\\n# good\\nBloodPressure.where(recorded_at: nil).each do |bp|\\nbp.touch(:recorded_at)\\nend\\n# bad - not idempotent\\nBloodPressure.where(id: affected_ids).each do |bp|\\nbp.update(systolic: bp.systolic + 10)\\nend\\n```\\n"}
{"File Name":"govuk-infrastructure\/0002-use-aws-eks-terraform-module.md","Context":"## Context\\nA fully configured EKS cluster requires many AWS resources and a lot of configuration. Defining each resource in our own Terraform module maximises flexibility but also requires a significant level of effort. We could instead make use of the [existing Terraform registry EKS module](https:\/\/registry.terraform.io\/modules\/terraform-aws-modules\/eks\/aws\/latest) to optimise for speed of delivery.\\n","Decision":"Adopt the [existing Terraform registry EKS module](https:\/\/registry.terraform.io\/modules\/terraform-aws-modules\/eks\/aws\/latest).\\n","tokens":83,"id":3610,"text":"## Context\\nA fully configured EKS cluster requires many AWS resources and a lot of configuration. Defining each resource in our own Terraform module maximises flexibility but also requires a significant level of effort. We could instead make use of the [existing Terraform registry EKS module](https:\/\/registry.terraform.io\/modules\/terraform-aws-modules\/eks\/aws\/latest) to optimise for speed of delivery.\\n\n\n##Decision\nAdopt the [existing Terraform registry EKS module](https:\/\/registry.terraform.io\/modules\/terraform-aws-modules\/eks\/aws\/latest).\\n"}
{"File Name":"nisq-analyzer\/0002-monorepo.md","Context":"## Context and Problem Statement\\nShould the components in the PlanQK platform be splitted into individual repos?\\n","Decision":"Start with a monorepo, split up later if needed. Let FOCUS decide what is best for their semantic knowledge graph component.\\n### Positive Consequences <!-- optional -->\\n* Recommended approach by [Martin Fowler](https:\/\/martinfowler.com\/bliki\/MonolithFirst.html)\\n","tokens":22,"id":450,"text":"## Context and Problem Statement\\nShould the components in the PlanQK platform be splitted into individual repos?\\n\n\n##Decision\nStart with a monorepo, split up later if needed. Let FOCUS decide what is best for their semantic knowledge graph component.\\n### Positive Consequences <!-- optional -->\\n* Recommended approach by [Martin Fowler](https:\/\/martinfowler.com\/bliki\/MonolithFirst.html)\\n"}
{"File Name":"LogLady\/0002-use-gitflow-and-naming.md","Context":"## Context\\nWe need a strategy for how to git and name branches\\n","Decision":"Tasks can be found in [Projects](https:\/\/github.com\/kits-ab\/LogLady\/projects\/1).\\nAssign one to yourself then create branch from develop and name it like this: feature\/whetever-you-are-doing-#corresponding-task-ID\\nExample: `feature\/branchName-#5`\\n**Note:** _feature_ is an example, it could be:\\n- feature\\n- hotfix\\n- release\\nCommit messages should start with task ID.\\nExample: `#5 update README. Explained naming.`\\nPush when done. Make pull request where you add task ID to beginning of comment.\\n","tokens":15,"id":1851,"text":"## Context\\nWe need a strategy for how to git and name branches\\n\n\n##Decision\nTasks can be found in [Projects](https:\/\/github.com\/kits-ab\/LogLady\/projects\/1).\\nAssign one to yourself then create branch from develop and name it like this: feature\/whetever-you-are-doing-#corresponding-task-ID\\nExample: `feature\/branchName-#5`\\n**Note:** _feature_ is an example, it could be:\\n- feature\\n- hotfix\\n- release\\nCommit messages should start with task ID.\\nExample: `#5 update README. Explained naming.`\\nPush when done. Make pull request where you add task ID to beginning of comment.\\n"}
{"File Name":"gatemint-sdk\/adr-021-protobuf-query-encoding.md","Context":"## Context\\nThis ADR is a continuation of the motivation, design, and context established in\\n[ADR 019](.\/adr-019-protobuf-state-encoding.md) and\\n[ARD 020](.\/adr-019-protobuf-transaction-encoding.md), namely, we aim to design the\\nProtocol Buffer migration path for the client-side of the Cosmos SDK.\\nThis ADR continues from [ARD 020](.\/adr-020-protobuf-transaction-encoding.md)\\nto specify the encoding of queries.\\n","Decision":"### Custom Query Definition\\nModules define custom queries through a protocol buffers `service` definition.\\nThese `service` definitions are generally associated with and used by the\\nGRPC protocol. However, the protocol buffers specification indicates that\\nthey can be used more generically by any request\/response protocol that uses\\nprotocol buffer encoding. Thus, we can use `service` definitions for specifying\\ncustom ABCI queries and even reuse a substantial amount of the GRPC infrastructure.\\nEach module with custom queries should define a service canonically named `Query`:\\n```proto\\n\/\/ x\/bank\/types\/types.proto\\nservice Query {\\nrpc QueryBalance(QueryBalanceParams) returns (cosmos_sdk.v1.Coin) { }\\nrpc QueryAllBalances(QueryAllBalancesParams) returns (QueryAllBalancesResponse) { }\\n}\\n```\\n#### Handling of Interface Types\\nModules that use interface types and need true polymorphism generally force a\\n`oneof` up to the app-level that provides the set of concrete implementations of\\nthat interface that the app supports. While app's are welcome to do the same for\\nqueries and implement an app-level query service, it is recommended that modules\\nprovide query methods that expose these interfaces via `google.protobuf.Any`.\\nThere is a concern on the transaction level that the overhead of `Any` is too\\nhigh to justify its usage. However for queries this is not a concern, and\\nproviding generic module-level queries that use `Any` does not preclude apps\\nfrom also providing app-level queries that return use the app-level `oneof`s.\\nA hypothetical example for the `gov` module would look something like:\\n```proto\\n\/\/ x\/gov\/types\/types.proto\\nimport \"google\/protobuf\/any.proto\";\\nservice Query {\\nrpc GetProposal(GetProposalParams) returns (AnyProposal) { }\\n}\\nmessage AnyProposal {\\nProposalBase base = 1;\\ngoogle.protobuf.Any content = 2;\\n}\\n```\\n### Custom Query Implementation\\nIn order to implement the query service, we can reuse the existing [gogo protobuf](https:\/\/github.com\/gogo\/protobuf)\\ngrpc plugin, which for a service named `Query` generates an interface named\\n`QueryServer` as below:\\n```go\\ntype QueryServer interface {\\nQueryBalance(context.Context, *QueryBalanceParams) (*types.Coin, error)\\nQueryAllBalances(context.Context, *QueryAllBalancesParams) (*QueryAllBalancesResponse, error)\\n}\\n```\\nThe custom queries for our module are implemented by implementing this interface.\\nThe first parameter in this generated interface is a generic `context.Context`,\\nwhereas querier methods generally need an instance of `sdk.Context` to read\\nfrom the store. Since arbitrary values can be attached to `context.Context`\\nusing the `WithValue` and `Value` methods, the SDK should provide a function\\n`sdk.UnwrapSDKContext` to retrieve the `sdk.Context` from the provided\\n`context.Context`.\\nAn example implementation of `QueryBalance` for the bank module as above would\\nlook something like:\\n```go\\ntype Querier struct {\\nKeeper\\n}\\nfunc (q Querier) QueryBalance(ctx context.Context, params *types.QueryBalanceParams) (*sdk.Coin, error) {\\nbalance := q.GetBalance(sdk.UnwrapSDKContext(ctx), params.Address, params.Denom)\\nreturn &balance, nil\\n}\\n```\\n### Custom Query Registration and Routing\\nQuery server implementations as above would be registered with `AppModule`s using\\na new method `RegisterQueryServer(grpc.Server)` which could be implemented simply\\nas below:\\n```go\\n\/\/ x\/bank\/module.go\\nfunc (am AppModule) RegisterQueryServer(server grpc.Server) {\\ntypes.RegisterQueryServer(server, keeper.Querier{am.keeper})\\n}\\n```\\nUnderneath the hood, a new method `RegisterService(sd *grpc.ServiceDesc, handler interface{})`\\nwill be added to the existing `baseapp.QueryRouter` to add the queries to the custom\\nquery routing table (with the routing method being described below).\\nThe signature for this method matches the existing\\n`RegisterServer` method on the GRPC `Server` type where `handler` is the custom\\nquery server implementation described above.\\nGRPC-like requests are routed by the service name (ex. `cosmos_sdk.x.bank.v1.Query`)\\nand method name (ex. `QueryBalance`) combined with `\/`s to form a full\\nmethod name (ex. `\/cosmos_sdk.x.bank.v1.Query\/QueryBalance`). This gets translated\\ninto an ABCI query as `custom\/cosmos_sdk.x.bank.v1.Query\/QueryBalance`. Service handlers\\nregistered with `QueryRouter.RegisterService` will be routed this way.\\nBeyond the method name, GRPC requests carry a protobuf encoded payload, which maps naturally\\nto `RequestQuery.Data`, and receive a protobuf encoded response or error. Thus\\nthere is a quite natural mapping of GRPC-like rpc methods to the existing\\n`sdk.Query` and `QueryRouter` infrastructure.\\nThis basic specification allows us to reuse protocol buffer `service` definitions\\nfor ABCI custom queries substantially reducing the need for manual decoding and\\nencoding in query methods.\\n### GRPC Protocol Support\\nIn addition to providing an ABCI query pathway, we can easily provide a GRPC\\nproxy server that routes requests in the GRPC protocol to ABCI query requests\\nunder the hood. In this way, clients could use their host languages' existing\\nGRPC implementations to make direct queries against Cosmos SDK app's using\\nthese `service` definitions. In order for this server to work, the `QueryRouter`\\non `BaseApp` will need to expose the service handlers registered with\\n`QueryRouter.RegisterService` to the proxy server implementation. Nodes could\\nlaunch the proxy server on a separate port in the same process as the ABCI app\\nwith a command-line flag.\\n### REST Queries and Swagger Generation\\n[grpc-gateway](https:\/\/github.com\/grpc-ecosystem\/grpc-gateway) is a project that\\ntranslates REST calls into GRPC calls using special annotations on service\\nmethods. Modules that want to expose REST queries should add `google.api.http`\\nannotations to their `rpc` methods as in this example below.\\n```proto\\n\/\/ x\/bank\/types\/types.proto\\nservice Query {\\nrpc QueryBalance(QueryBalanceParams) returns (cosmos_sdk.v1.Coin) {\\noption (google.api.http) = {\\nget: \"\/x\/bank\/v1\/balance\/{address}\/{denom}\"\\n};\\n}\\nrpc QueryAllBalances(QueryAllBalancesParams) returns (QueryAllBalancesResponse) {\\noption (google.api.http) = {\\nget: \"\/x\/bank\/v1\/balances\/{address}\"\\n};\\n}\\n}\\n```\\ngrpc-gateway will work direcly against the GRPC proxy described above which will\\ntranslate requests to ABCI queries under the hood. grpc-gateway can also\\ngenerate Swagger definitions automatically.\\nIn the current implementation of REST queries, each module needs to implement\\nREST queries manually in addition to ABCI querier methods. Using the grpc-gateway\\napproach, there will be no need to generate separate REST query handlers, just\\nquery servers as described above as grpc-gateway handles the translation of protobuf\\nto REST as well as Swagger definitions.\\nThe SDK should provide CLI commands for apps to start GRPC gateway either in\\na separate process or the same process as the ABCI app, as well as provide a\\ncommand for generating grpc-gateway proxy `.proto` files and the `swagger.json`\\nfile.\\n### Client Usage\\nThe gogo protobuf grpc plugin generates client interfaces in addition to server\\ninterfaces. For the `Query` service defined above we would get a `QueryClient`\\ninterface like:\\n```go\\ntype QueryClient interface {\\nQueryBalance(ctx context.Context, in *QueryBalanceParams, opts ...grpc.CallOption) (*types.Coin, error)\\nQueryAllBalances(ctx context.Context, in *QueryAllBalancesParams, opts ...grpc.CallOption) (*QueryAllBalancesResponse, error)\\n}\\n```\\nVia a small patch to gogo protobuf ([gogo\/protobuf#675](https:\/\/github.com\/gogo\/protobuf\/pull\/675))\\nwe have tweaked the grpc codegen to use an interface rather than concrete type\\nfor the generated client struct. This allows us to also reuse the GRPC infrastructure\\nfor ABCI client queries.\\n1Context` will receive a new method `QueryConn` that returns a `ClientConn`\\nthat routes calls to ABCI queries\\nClients (such as CLI methods) will then be able to call query methods like this:\\n```go\\nclientCtx := client.NewContext()\\nqueryClient := types.NewQueryClient(clientCtx.QueryConn())\\nparams := &types.QueryBalanceParams{addr, denom}\\nresult, err := queryClient.QueryBalance(gocontext.Background(), params)\\n```\\n### Testing\\nTests would be able to create a query client directly from keeper and `sdk.Context`\\nreferences using a `QueryServerTestHelper` as below:\\n```go\\nqueryHelper := baseapp.NewQueryServerTestHelper(ctx)\\ntypes.RegisterQueryServer(queryHelper, keeper.Querier{app.BankKeeper})\\nqueryClient := types.NewQueryClient(queryHelper)\\n```\\n","tokens":108,"id":16,"text":"## Context\\nThis ADR is a continuation of the motivation, design, and context established in\\n[ADR 019](.\/adr-019-protobuf-state-encoding.md) and\\n[ARD 020](.\/adr-019-protobuf-transaction-encoding.md), namely, we aim to design the\\nProtocol Buffer migration path for the client-side of the Cosmos SDK.\\nThis ADR continues from [ARD 020](.\/adr-020-protobuf-transaction-encoding.md)\\nto specify the encoding of queries.\\n\n\n##Decision\n### Custom Query Definition\\nModules define custom queries through a protocol buffers `service` definition.\\nThese `service` definitions are generally associated with and used by the\\nGRPC protocol. However, the protocol buffers specification indicates that\\nthey can be used more generically by any request\/response protocol that uses\\nprotocol buffer encoding. Thus, we can use `service` definitions for specifying\\ncustom ABCI queries and even reuse a substantial amount of the GRPC infrastructure.\\nEach module with custom queries should define a service canonically named `Query`:\\n```proto\\n\/\/ x\/bank\/types\/types.proto\\nservice Query {\\nrpc QueryBalance(QueryBalanceParams) returns (cosmos_sdk.v1.Coin) { }\\nrpc QueryAllBalances(QueryAllBalancesParams) returns (QueryAllBalancesResponse) { }\\n}\\n```\\n#### Handling of Interface Types\\nModules that use interface types and need true polymorphism generally force a\\n`oneof` up to the app-level that provides the set of concrete implementations of\\nthat interface that the app supports. While app's are welcome to do the same for\\nqueries and implement an app-level query service, it is recommended that modules\\nprovide query methods that expose these interfaces via `google.protobuf.Any`.\\nThere is a concern on the transaction level that the overhead of `Any` is too\\nhigh to justify its usage. However for queries this is not a concern, and\\nproviding generic module-level queries that use `Any` does not preclude apps\\nfrom also providing app-level queries that return use the app-level `oneof`s.\\nA hypothetical example for the `gov` module would look something like:\\n```proto\\n\/\/ x\/gov\/types\/types.proto\\nimport \"google\/protobuf\/any.proto\";\\nservice Query {\\nrpc GetProposal(GetProposalParams) returns (AnyProposal) { }\\n}\\nmessage AnyProposal {\\nProposalBase base = 1;\\ngoogle.protobuf.Any content = 2;\\n}\\n```\\n### Custom Query Implementation\\nIn order to implement the query service, we can reuse the existing [gogo protobuf](https:\/\/github.com\/gogo\/protobuf)\\ngrpc plugin, which for a service named `Query` generates an interface named\\n`QueryServer` as below:\\n```go\\ntype QueryServer interface {\\nQueryBalance(context.Context, *QueryBalanceParams) (*types.Coin, error)\\nQueryAllBalances(context.Context, *QueryAllBalancesParams) (*QueryAllBalancesResponse, error)\\n}\\n```\\nThe custom queries for our module are implemented by implementing this interface.\\nThe first parameter in this generated interface is a generic `context.Context`,\\nwhereas querier methods generally need an instance of `sdk.Context` to read\\nfrom the store. Since arbitrary values can be attached to `context.Context`\\nusing the `WithValue` and `Value` methods, the SDK should provide a function\\n`sdk.UnwrapSDKContext` to retrieve the `sdk.Context` from the provided\\n`context.Context`.\\nAn example implementation of `QueryBalance` for the bank module as above would\\nlook something like:\\n```go\\ntype Querier struct {\\nKeeper\\n}\\nfunc (q Querier) QueryBalance(ctx context.Context, params *types.QueryBalanceParams) (*sdk.Coin, error) {\\nbalance := q.GetBalance(sdk.UnwrapSDKContext(ctx), params.Address, params.Denom)\\nreturn &balance, nil\\n}\\n```\\n### Custom Query Registration and Routing\\nQuery server implementations as above would be registered with `AppModule`s using\\na new method `RegisterQueryServer(grpc.Server)` which could be implemented simply\\nas below:\\n```go\\n\/\/ x\/bank\/module.go\\nfunc (am AppModule) RegisterQueryServer(server grpc.Server) {\\ntypes.RegisterQueryServer(server, keeper.Querier{am.keeper})\\n}\\n```\\nUnderneath the hood, a new method `RegisterService(sd *grpc.ServiceDesc, handler interface{})`\\nwill be added to the existing `baseapp.QueryRouter` to add the queries to the custom\\nquery routing table (with the routing method being described below).\\nThe signature for this method matches the existing\\n`RegisterServer` method on the GRPC `Server` type where `handler` is the custom\\nquery server implementation described above.\\nGRPC-like requests are routed by the service name (ex. `cosmos_sdk.x.bank.v1.Query`)\\nand method name (ex. `QueryBalance`) combined with `\/`s to form a full\\nmethod name (ex. `\/cosmos_sdk.x.bank.v1.Query\/QueryBalance`). This gets translated\\ninto an ABCI query as `custom\/cosmos_sdk.x.bank.v1.Query\/QueryBalance`. Service handlers\\nregistered with `QueryRouter.RegisterService` will be routed this way.\\nBeyond the method name, GRPC requests carry a protobuf encoded payload, which maps naturally\\nto `RequestQuery.Data`, and receive a protobuf encoded response or error. Thus\\nthere is a quite natural mapping of GRPC-like rpc methods to the existing\\n`sdk.Query` and `QueryRouter` infrastructure.\\nThis basic specification allows us to reuse protocol buffer `service` definitions\\nfor ABCI custom queries substantially reducing the need for manual decoding and\\nencoding in query methods.\\n### GRPC Protocol Support\\nIn addition to providing an ABCI query pathway, we can easily provide a GRPC\\nproxy server that routes requests in the GRPC protocol to ABCI query requests\\nunder the hood. In this way, clients could use their host languages' existing\\nGRPC implementations to make direct queries against Cosmos SDK app's using\\nthese `service` definitions. In order for this server to work, the `QueryRouter`\\non `BaseApp` will need to expose the service handlers registered with\\n`QueryRouter.RegisterService` to the proxy server implementation. Nodes could\\nlaunch the proxy server on a separate port in the same process as the ABCI app\\nwith a command-line flag.\\n### REST Queries and Swagger Generation\\n[grpc-gateway](https:\/\/github.com\/grpc-ecosystem\/grpc-gateway) is a project that\\ntranslates REST calls into GRPC calls using special annotations on service\\nmethods. Modules that want to expose REST queries should add `google.api.http`\\nannotations to their `rpc` methods as in this example below.\\n```proto\\n\/\/ x\/bank\/types\/types.proto\\nservice Query {\\nrpc QueryBalance(QueryBalanceParams) returns (cosmos_sdk.v1.Coin) {\\noption (google.api.http) = {\\nget: \"\/x\/bank\/v1\/balance\/{address}\/{denom}\"\\n};\\n}\\nrpc QueryAllBalances(QueryAllBalancesParams) returns (QueryAllBalancesResponse) {\\noption (google.api.http) = {\\nget: \"\/x\/bank\/v1\/balances\/{address}\"\\n};\\n}\\n}\\n```\\ngrpc-gateway will work direcly against the GRPC proxy described above which will\\ntranslate requests to ABCI queries under the hood. grpc-gateway can also\\ngenerate Swagger definitions automatically.\\nIn the current implementation of REST queries, each module needs to implement\\nREST queries manually in addition to ABCI querier methods. Using the grpc-gateway\\napproach, there will be no need to generate separate REST query handlers, just\\nquery servers as described above as grpc-gateway handles the translation of protobuf\\nto REST as well as Swagger definitions.\\nThe SDK should provide CLI commands for apps to start GRPC gateway either in\\na separate process or the same process as the ABCI app, as well as provide a\\ncommand for generating grpc-gateway proxy `.proto` files and the `swagger.json`\\nfile.\\n### Client Usage\\nThe gogo protobuf grpc plugin generates client interfaces in addition to server\\ninterfaces. For the `Query` service defined above we would get a `QueryClient`\\ninterface like:\\n```go\\ntype QueryClient interface {\\nQueryBalance(ctx context.Context, in *QueryBalanceParams, opts ...grpc.CallOption) (*types.Coin, error)\\nQueryAllBalances(ctx context.Context, in *QueryAllBalancesParams, opts ...grpc.CallOption) (*QueryAllBalancesResponse, error)\\n}\\n```\\nVia a small patch to gogo protobuf ([gogo\/protobuf#675](https:\/\/github.com\/gogo\/protobuf\/pull\/675))\\nwe have tweaked the grpc codegen to use an interface rather than concrete type\\nfor the generated client struct. This allows us to also reuse the GRPC infrastructure\\nfor ABCI client queries.\\n1Context` will receive a new method `QueryConn` that returns a `ClientConn`\\nthat routes calls to ABCI queries\\nClients (such as CLI methods) will then be able to call query methods like this:\\n```go\\nclientCtx := client.NewContext()\\nqueryClient := types.NewQueryClient(clientCtx.QueryConn())\\nparams := &types.QueryBalanceParams{addr, denom}\\nresult, err := queryClient.QueryBalance(gocontext.Background(), params)\\n```\\n### Testing\\nTests would be able to create a query client directly from keeper and `sdk.Context`\\nreferences using a `QueryServerTestHelper` as below:\\n```go\\nqueryHelper := baseapp.NewQueryServerTestHelper(ctx)\\ntypes.RegisterQueryServer(queryHelper, keeper.Querier{app.BankKeeper})\\nqueryClient := types.NewQueryClient(queryHelper)\\n```\\n"}
{"File Name":"HES_pipeline\/data_location.md","Context":"## Context\\nIncoming data from NHS is not handled by users of the HES pipeline. This makes\\nit harder to control the directory format of the raw data, as well as the\\nconsistency of that data and directory structure.\\nAdditionally the SQLite database needs to be located in a writable environment.\\n","Decision":"The raw data will be copied manually from the Data directory to the Library\\ndirectory. Subsequent updates to the raw data will also require this. This is\\nalso where the database will be located and written to.\\n","tokens":62,"id":3252,"text":"## Context\\nIncoming data from NHS is not handled by users of the HES pipeline. This makes\\nit harder to control the directory format of the raw data, as well as the\\nconsistency of that data and directory structure.\\nAdditionally the SQLite database needs to be located in a writable environment.\\n\n\n##Decision\nThe raw data will be copied manually from the Data directory to the Library\\ndirectory. Subsequent updates to the raw data will also require this. This is\\nalso where the database will be located and written to.\\n"}
{"File Name":"OpenTermsArchive\/0001-service-name-and-id.md","Context":"## Context and Problem Statement\\nTo scale up from 50 to 5,000 services, we need a clear way for choosing the service name and the service ID.\\n### We need\\nA name that reflects the common name used by the provider itself, to be exposed in a GUI. This name is currently exposed as the name property in the service declaration.\\nAn ID of sorts that can be represented in the filesystem. This ID is currently exposed as the filename of the service declaration, without the .json extension.\\n### Use cases\\nThe service name is presented to end users. It should reflect as closely as possible the official service name, so that it can be identified easily.\\nThe ID is used internally and exposed for analysis. It should be easy to handle with scripts and other tools.\\n### Constraints for the ID\\nAs long as this ID is stored in the filesystem:\\n- No `\/` for UNIX.\\n- No `\\` for Windows.\\n- No `:` for APFS and HFS.\\n- No case-sensitive duplicates to support case-insensitive filesystems.\\n- No more than 255 characters to support transfer over [FAT32](https:\/\/en.wikipedia.org\/wiki\/File_Allocation_Table#FAT32).\\nUTF, spaces and capitals are all supported, even on case-insensitive filesystems.\\n### However\\n- UTF in filenames can be [a (fixable) problem with Git and HFS+](https:\/\/stackoverflow.com\/questions\/5581857\/git-and-the-umlaut-problem-on-mac-os-x).\\n- UTF in filenames is by default quoted in Git, leading for example `\u00e9t\u00e9.txt` to be displayed as `\"\\303\\251t\\303\\251.txt\"`.\\n- Most online services align their brand name with their domain name. Even though UTF is now officially supported in domain names, support is limited and most services, even non-Western, have an official ASCII transliteration used at least in their domain name (e.g. \u201cqq\u201d by Tencent, \u201crzd.ru\u201d for \u201c\u0420\u0416\u0414\u201d, \u201cyahoo\u201d for \u201cYahoo!\u201d).\\n- We currently use GitHub as a GUI, so the service ID is presented to the user instead of the service name. The name is used in email notifications.\\n","Decision":"1. The service name should be the one used by the service itself, no matter the alphabet.\\n- _Example: `\u0442\u0443\u0442\u0443.\u0440\u0443`_.\\n2. We don't support non-ASCII characters in service IDs, at least as long as the database is Git and the filesystem, in order to minimise risk. Service IDs are derived from the service name through normalization into ASCII.\\n- _Example: `\u0442\u0443\u0442\u0443.\u0440\u0443` \u2192 `tutu.ru`_.\\n- _Example: `historiel\u00e6rer.dk` \u2192 `historielaerer.dk`_.\\n- _Example: `RT\u00c9` \u2192 `RTE`_.\\n3. We support punctuation, except characters that have meaning at filesystem level (`:`, `\/`, `\\`). These are replaced with a dash (`-`).\\n- _Example: `Yahoo!` \u2192 `Yahoo!`_.\\n- _Example: `Last.fm` \u2192 `Last.fm`_.\\n- _Example: `re:start` \u2192 `re-start`_.\\n- _Example: `we:\/\/` \u2192 `we---`_.\\n4. We support capitals. Casing is expected to reflect the official service name casing.\\n- _Example: `hi5` \u2192 `hi5`_.\\n- _Example: `DeviantArt` \u2192 `DeviantArt`_.\\n- _Example: `LINE` \u2192 `LINE`_.\\n5. We support spaces. Spaces are expected to reflect the official service name spacing.\\n- _Example: `App Store` \u2192 `App Store`_.\\n- _Example: `DeviantArt` \u2192 `DeviantArt`_.\\n6. We prefix the service name by the provider name when self-references are ambiguous, separated by a space. For example, Facebook refers to their Self-serve Ads service simply as \u201cAds\u201d, which we cannot use in a shared database. We thus call the service \u201cFacebook Ads\u201d.\\n- _Example: `Ads` (by Facebook) \u2192 `Facebook Ads`_.\\n- _Example: `Analytics` (by Google) \u2192 `Google Analytics`_.\\n- _Example: `Firebase` (by Google) \u2192 `Firebase`_.\\n- _Example: `App Store` (by Apple) \u2192 `App Store`_.\\n","tokens":468,"id":4666,"text":"## Context and Problem Statement\\nTo scale up from 50 to 5,000 services, we need a clear way for choosing the service name and the service ID.\\n### We need\\nA name that reflects the common name used by the provider itself, to be exposed in a GUI. This name is currently exposed as the name property in the service declaration.\\nAn ID of sorts that can be represented in the filesystem. This ID is currently exposed as the filename of the service declaration, without the .json extension.\\n### Use cases\\nThe service name is presented to end users. It should reflect as closely as possible the official service name, so that it can be identified easily.\\nThe ID is used internally and exposed for analysis. It should be easy to handle with scripts and other tools.\\n### Constraints for the ID\\nAs long as this ID is stored in the filesystem:\\n- No `\/` for UNIX.\\n- No `\\` for Windows.\\n- No `:` for APFS and HFS.\\n- No case-sensitive duplicates to support case-insensitive filesystems.\\n- No more than 255 characters to support transfer over [FAT32](https:\/\/en.wikipedia.org\/wiki\/File_Allocation_Table#FAT32).\\nUTF, spaces and capitals are all supported, even on case-insensitive filesystems.\\n### However\\n- UTF in filenames can be [a (fixable) problem with Git and HFS+](https:\/\/stackoverflow.com\/questions\/5581857\/git-and-the-umlaut-problem-on-mac-os-x).\\n- UTF in filenames is by default quoted in Git, leading for example `\u00e9t\u00e9.txt` to be displayed as `\"\\303\\251t\\303\\251.txt\"`.\\n- Most online services align their brand name with their domain name. Even though UTF is now officially supported in domain names, support is limited and most services, even non-Western, have an official ASCII transliteration used at least in their domain name (e.g. \u201cqq\u201d by Tencent, \u201crzd.ru\u201d for \u201c\u0420\u0416\u0414\u201d, \u201cyahoo\u201d for \u201cYahoo!\u201d).\\n- We currently use GitHub as a GUI, so the service ID is presented to the user instead of the service name. The name is used in email notifications.\\n\n\n##Decision\n1. The service name should be the one used by the service itself, no matter the alphabet.\\n- _Example: `\u0442\u0443\u0442\u0443.\u0440\u0443`_.\\n2. We don't support non-ASCII characters in service IDs, at least as long as the database is Git and the filesystem, in order to minimise risk. Service IDs are derived from the service name through normalization into ASCII.\\n- _Example: `\u0442\u0443\u0442\u0443.\u0440\u0443` \u2192 `tutu.ru`_.\\n- _Example: `historiel\u00e6rer.dk` \u2192 `historielaerer.dk`_.\\n- _Example: `RT\u00c9` \u2192 `RTE`_.\\n3. We support punctuation, except characters that have meaning at filesystem level (`:`, `\/`, `\\`). These are replaced with a dash (`-`).\\n- _Example: `Yahoo!` \u2192 `Yahoo!`_.\\n- _Example: `Last.fm` \u2192 `Last.fm`_.\\n- _Example: `re:start` \u2192 `re-start`_.\\n- _Example: `we:\/\/` \u2192 `we---`_.\\n4. We support capitals. Casing is expected to reflect the official service name casing.\\n- _Example: `hi5` \u2192 `hi5`_.\\n- _Example: `DeviantArt` \u2192 `DeviantArt`_.\\n- _Example: `LINE` \u2192 `LINE`_.\\n5. We support spaces. Spaces are expected to reflect the official service name spacing.\\n- _Example: `App Store` \u2192 `App Store`_.\\n- _Example: `DeviantArt` \u2192 `DeviantArt`_.\\n6. We prefix the service name by the provider name when self-references are ambiguous, separated by a space. For example, Facebook refers to their Self-serve Ads service simply as \u201cAds\u201d, which we cannot use in a shared database. We thus call the service \u201cFacebook Ads\u201d.\\n- _Example: `Ads` (by Facebook) \u2192 `Facebook Ads`_.\\n- _Example: `Analytics` (by Google) \u2192 `Google Analytics`_.\\n- _Example: `Firebase` (by Google) \u2192 `Firebase`_.\\n- _Example: `App Store` (by Apple) \u2192 `App Store`_.\\n"}
{"File Name":"re-build-systems\/0007-add-ability-to-import-custom-jobs.md","Context":"## Context\\nWe need the user to be able to define Jenkins jobs in code and be able to import them into Jenkins.\\nWe have identified a number of ways to do this:\\n1. Define jobs with Groovy and inject the script, as we do for the Jenkins configuration\\n2. Automatically create the jobs by scanning a Github account\\n3. [Jenkins Job Builder]\\n4. [Job DSL plugin]\\n### Option 1\\nThis is the easiest for us as we don't need to implement anything new. We can use the mechanism of injecting Groovy script which is already available. It is also relatively easy for the user to use. The code for the jobs and configuration can't exceed 16 KB, which is a limitation, but we believe that is enough for compact Jenkins installations (a Jenkins with hundreds of jobs is an anti-pattern). [The limit] is because we use [user data] to implement this option.\\n### Option 2\\nJenkins provides a way to scan a Github organisation or accounts for repositories containing pipeline configurations in a Jenkinsfile. This should be quite easy for the user but the implementation can be quite complex. This option would require the user to pass extra parameters to the Jenkins Terraform module: at least one regular expression to filter the repositories to match, and a Github personal token. The token is needed because scanning Github as an unauthenticated user is extremely slow but only takes a few minutes for a user with authentication. As the module needs a token as an input, there is extra complexity around managing that secret. This would be relatively straightforward to do using the UI but providing this solution as code would be quite involved.\\n### Option 3\\n(Jenkins Job Builder) is probably the most commonly used at GDS (it's used by Notify, Digital Marketplace) - people generally like it but some issues were pointed out like a difficulty in upgrading to a newer version or in escaping quotes correctly. [GOV.UK] and Pay use a more ad-hoc, homebrewed approach. Both groups rely on Puppet or Chef to inject their jobs into Jenkins. We allow users to install their configuration management tool via cloud-init, so the user is still free to override any mechanism we provide.\\n### Option 4\\nThis hasn't been explored in great detail as we felt we'd already found a good solution. However, if we revisit the decision made in this PR, this tool should be evaluated more thoroughly.\\n","Decision":"We decided to implement solution 1 to keep things simple and because of time constraints. Options two and three can still be used but will not be supported. In the future, we may consider to change to another solution if we feel there is the user need.\\n","tokens":500,"id":321,"text":"## Context\\nWe need the user to be able to define Jenkins jobs in code and be able to import them into Jenkins.\\nWe have identified a number of ways to do this:\\n1. Define jobs with Groovy and inject the script, as we do for the Jenkins configuration\\n2. Automatically create the jobs by scanning a Github account\\n3. [Jenkins Job Builder]\\n4. [Job DSL plugin]\\n### Option 1\\nThis is the easiest for us as we don't need to implement anything new. We can use the mechanism of injecting Groovy script which is already available. It is also relatively easy for the user to use. The code for the jobs and configuration can't exceed 16 KB, which is a limitation, but we believe that is enough for compact Jenkins installations (a Jenkins with hundreds of jobs is an anti-pattern). [The limit] is because we use [user data] to implement this option.\\n### Option 2\\nJenkins provides a way to scan a Github organisation or accounts for repositories containing pipeline configurations in a Jenkinsfile. This should be quite easy for the user but the implementation can be quite complex. This option would require the user to pass extra parameters to the Jenkins Terraform module: at least one regular expression to filter the repositories to match, and a Github personal token. The token is needed because scanning Github as an unauthenticated user is extremely slow but only takes a few minutes for a user with authentication. As the module needs a token as an input, there is extra complexity around managing that secret. This would be relatively straightforward to do using the UI but providing this solution as code would be quite involved.\\n### Option 3\\n(Jenkins Job Builder) is probably the most commonly used at GDS (it's used by Notify, Digital Marketplace) - people generally like it but some issues were pointed out like a difficulty in upgrading to a newer version or in escaping quotes correctly. [GOV.UK] and Pay use a more ad-hoc, homebrewed approach. Both groups rely on Puppet or Chef to inject their jobs into Jenkins. We allow users to install their configuration management tool via cloud-init, so the user is still free to override any mechanism we provide.\\n### Option 4\\nThis hasn't been explored in great detail as we felt we'd already found a good solution. However, if we revisit the decision made in this PR, this tool should be evaluated more thoroughly.\\n\n\n##Decision\nWe decided to implement solution 1 to keep things simple and because of time constraints. Options two and three can still be used but will not be supported. In the future, we may consider to change to another solution if we feel there is the user need.\\n"}
{"File Name":"ios-architecture-decision-logs\/0005-inject-ab-config-global-values-toPresenter.md","Context":"## Context\\nWe faced a problem that missing test cases on some presenters because of not injectable variables like ab tests, config or global values on presenters. So we want to cover all of these missings.\\n","Decision":"We decided to inject this variables to related presenters from their constructors.\\n","tokens":45,"id":4973,"text":"## Context\\nWe faced a problem that missing test cases on some presenters because of not injectable variables like ab tests, config or global values on presenters. So we want to cover all of these missings.\\n\n\n##Decision\nWe decided to inject this variables to related presenters from their constructors.\\n"}
{"File Name":"gsp\/ADR039-cloudhsm-namespace-network-policy.md","Context":"## Context\\n[ADR036](ADR036-hsm-isolation-in-detail.md) described the network and\\ncredential isolation we use to ensure that unauthorised users cannot\\naccess the CloudHSM.\\nRecently in 3ea9de2ff, we introduced a GlobalNetworkPolicy object,\\nwhich is a Calico feature that allows a cluster-wide network policy to\\nbe imposed.  This allows us to control network access to and from\\nparticular namespaces in a way which cannot be overridden by tenants.\\nIn particular, currently access to the HSM is only allowed from pods\\nannotated with a `talksToHsm=true` label.\\nWhen working in their own namespace, a developer has full control over\\nwhat labels they put on their pods, so they can still choose to put\\nthe `talksToHsm=true` label on their pods.  But they do not have\\ncontrol over what labels the namespace itself has; to change this\\nwould require a change to the `gsp` or appropriate `cluster-config`\\nrepository, which would make such a change visible to many more\\npeople.\\nTherefore, if we extend the GlobalNetworkPolicy to require a\\n`talksToHsm=true` label on *both* the pod *and* the namespace, we will\\nprevent tenants from unilaterally opening up network access to the HSM\\nfrom their namespaces.\\n","Decision":"We will augment the GlobalNetworkPolicy (previously described in ADR036) by:\\n- setting a `GlobalNetworkPolicy` that denies access to the\\nCloudHSM's IP address unless the pod carries a label\\n(`talksToHsm=true`) and the namespace also carries a label\\n(`talksToHsm=true`) and allows all other egress traffic\\n","tokens":289,"id":3880,"text":"## Context\\n[ADR036](ADR036-hsm-isolation-in-detail.md) described the network and\\ncredential isolation we use to ensure that unauthorised users cannot\\naccess the CloudHSM.\\nRecently in 3ea9de2ff, we introduced a GlobalNetworkPolicy object,\\nwhich is a Calico feature that allows a cluster-wide network policy to\\nbe imposed.  This allows us to control network access to and from\\nparticular namespaces in a way which cannot be overridden by tenants.\\nIn particular, currently access to the HSM is only allowed from pods\\nannotated with a `talksToHsm=true` label.\\nWhen working in their own namespace, a developer has full control over\\nwhat labels they put on their pods, so they can still choose to put\\nthe `talksToHsm=true` label on their pods.  But they do not have\\ncontrol over what labels the namespace itself has; to change this\\nwould require a change to the `gsp` or appropriate `cluster-config`\\nrepository, which would make such a change visible to many more\\npeople.\\nTherefore, if we extend the GlobalNetworkPolicy to require a\\n`talksToHsm=true` label on *both* the pod *and* the namespace, we will\\nprevent tenants from unilaterally opening up network access to the HSM\\nfrom their namespaces.\\n\n\n##Decision\nWe will augment the GlobalNetworkPolicy (previously described in ADR036) by:\\n- setting a `GlobalNetworkPolicy` that denies access to the\\nCloudHSM's IP address unless the pod carries a label\\n(`talksToHsm=true`) and the namespace also carries a label\\n(`talksToHsm=true`) and allows all other egress traffic\\n"}
{"File Name":"rfcs\/0000-associate-users-with-entries.md","Context":"## Context\\n[context]: #context\\n> This section describes the forces at play, including technological, political, social, and project local. These forces are probably in tension, and should be called out as such. The language in this section is value-neutral. It is simply describing facts.\\n### MVP\\n> Wenn ein Nutzer eingeloggt ist, wird er mit dem Eintrag assoziiert:\\n> - Er wird standardm\u00e4\u00dfig \u00fcber zuk\u00fcnftige \u00c4nderungen anderer Nutzer informiert\\nSiehe Ziel 3). Kann dies aber \u00fcber eine Checkbox vor dem Speichern abw\u00e4hlen.\\n> - Er muss keinen Copyright lizenz akzeptieren, weil er das einmal bei der Registrierung gemacht hat.\\n> - In der Fu\u00dfzeile f\u00fcr jeden Eintrag wird der Username oder die Nutzer-ID\\nangezeigt, von dem eingeloggten Nutzer, der es zuletzt editiert hat.\\n> - ??? Notmoderationscheckbox: User kann ausw\u00e4hlen, dass\\nanonyme \u00c4nderungen erst sichtbar werden, wenn er oder ein\\nanderer RegPi\/ThemPi die \u00e4nderung best\u00e4tigt (\u00fcber link in\\nNotification mail. Best\u00e4tigungsmail muss sich von anderen\\n'Eintrag bearbeitet'-Mail unterscheiden zB. im Betreff:\\nAktualisierung wartet auf Freischaltung: Name des Eintrags)\\n### NTH\\n> - Eingeloggte Nutzer k\u00f6nnen einen Filter setzten \"Zeige nur\\nver\u00e4nderte Eintr\u00e4ge\" und sehen dann nur \u00c4nderungen\\n> - Alle ge\u00e4nderten Eintr\u00e4ge die auf Freischaltung warten, sind als\\nPinfarbe rot markiert\\n","Decision":"[decision]: #decision\\n> This section describes our response to these forces. It is stated in full sentences, with active voice. \"We will ...\"\\n","tokens":369,"id":1881,"text":"## Context\\n[context]: #context\\n> This section describes the forces at play, including technological, political, social, and project local. These forces are probably in tension, and should be called out as such. The language in this section is value-neutral. It is simply describing facts.\\n### MVP\\n> Wenn ein Nutzer eingeloggt ist, wird er mit dem Eintrag assoziiert:\\n> - Er wird standardm\u00e4\u00dfig \u00fcber zuk\u00fcnftige \u00c4nderungen anderer Nutzer informiert\\nSiehe Ziel 3). Kann dies aber \u00fcber eine Checkbox vor dem Speichern abw\u00e4hlen.\\n> - Er muss keinen Copyright lizenz akzeptieren, weil er das einmal bei der Registrierung gemacht hat.\\n> - In der Fu\u00dfzeile f\u00fcr jeden Eintrag wird der Username oder die Nutzer-ID\\nangezeigt, von dem eingeloggten Nutzer, der es zuletzt editiert hat.\\n> - ??? Notmoderationscheckbox: User kann ausw\u00e4hlen, dass\\nanonyme \u00c4nderungen erst sichtbar werden, wenn er oder ein\\nanderer RegPi\/ThemPi die \u00e4nderung best\u00e4tigt (\u00fcber link in\\nNotification mail. Best\u00e4tigungsmail muss sich von anderen\\n'Eintrag bearbeitet'-Mail unterscheiden zB. im Betreff:\\nAktualisierung wartet auf Freischaltung: Name des Eintrags)\\n### NTH\\n> - Eingeloggte Nutzer k\u00f6nnen einen Filter setzten \"Zeige nur\\nver\u00e4nderte Eintr\u00e4ge\" und sehen dann nur \u00c4nderungen\\n> - Alle ge\u00e4nderten Eintr\u00e4ge die auf Freischaltung warten, sind als\\nPinfarbe rot markiert\\n\n\n##Decision\n[decision]: #decision\\n> This section describes our response to these forces. It is stated in full sentences, with active voice. \"We will ...\"\\n"}
{"File Name":"akvo-product-design\/ADR-006.md","Context":"## Context\\nCurrently, when user requests data point sync from either DataPointsMapFragment or SurveyedLocaleListFragment and a service,\\nSurveyedDataPointSyncService is started and for each batch of datapoints both fragments get notified of new data available via local Broadcast which both subscribe too. While data is being loaded a non-blocking \"loading\" ui is shown. Once the loading is finished\\nthat loading needs to be dismissed. We also display a snackBar with the number of updated datapoints or some eventual errors.\\n","Decision":"The current implementation is not ideal since if user leaves the fragments view, the service will still send the broadcast even\\nif there is no ui waiting for it. Both fragments have to subscribe to the broadcast even if only one of them that will display\\nthe error\/success snackBars. It would be much better to have the ui subscribe to changes in the database data instead of broarcasts.\\nThat way the service does not need to notify ui of new data being available instead when the service inserts data into the database\\nthe ui will automatically be notified. RX Java can be used for this, an example is this library https:\/\/github.com\/square\/sqlbrite.\\nThis task also requires the refactor of this feature (independently from the rest of the application). We will separate the ui and\\nview (fragment and presenter) logic from domain logic (business logic, use cases) and the data logic (data sources sych as database\\nand network). This separation of concerns will allow us to add unit tests to separate components which will have a Single\\nResponsability.\\n","tokens":106,"id":2801,"text":"## Context\\nCurrently, when user requests data point sync from either DataPointsMapFragment or SurveyedLocaleListFragment and a service,\\nSurveyedDataPointSyncService is started and for each batch of datapoints both fragments get notified of new data available via local Broadcast which both subscribe too. While data is being loaded a non-blocking \"loading\" ui is shown. Once the loading is finished\\nthat loading needs to be dismissed. We also display a snackBar with the number of updated datapoints or some eventual errors.\\n\n\n##Decision\nThe current implementation is not ideal since if user leaves the fragments view, the service will still send the broadcast even\\nif there is no ui waiting for it. Both fragments have to subscribe to the broadcast even if only one of them that will display\\nthe error\/success snackBars. It would be much better to have the ui subscribe to changes in the database data instead of broarcasts.\\nThat way the service does not need to notify ui of new data being available instead when the service inserts data into the database\\nthe ui will automatically be notified. RX Java can be used for this, an example is this library https:\/\/github.com\/square\/sqlbrite.\\nThis task also requires the refactor of this feature (independently from the rest of the application). We will separate the ui and\\nview (fragment and presenter) logic from domain logic (business logic, use cases) and the data logic (data sources sych as database\\nand network). This separation of concerns will allow us to add unit tests to separate components which will have a Single\\nResponsability.\\n"}
{"File Name":"rems\/017-applicant-ux.md","Context":"# Problems with the current applicant UX\\n","Decision":"Here's a step-by-step plan for fixing these.\\n1. (Problem A) Make save succeed even with validation errors. Show validation errors in yellow (instead of red). [#2766](https:\/\/github.com\/CSCfi\/rems\/issues\/2766)\\n2. (Problem B) Run validations in the frontend after the user has stopped typing. [#2614](https:\/\/github.com\/CSCfi\/rems\/issues\/2614)\\n3. (Problem B) Autosave in the background by replacing\/patching the latest draft-saved event. If the latest event is not a draft-saved then create a new one. [#2767](https:\/\/github.com\/CSCfi\/rems\/issues\/2767)\\n","tokens":8,"id":465,"text":"# Problems with the current applicant UX\\n\n\n##Decision\nHere's a step-by-step plan for fixing these.\\n1. (Problem A) Make save succeed even with validation errors. Show validation errors in yellow (instead of red). [#2766](https:\/\/github.com\/CSCfi\/rems\/issues\/2766)\\n2. (Problem B) Run validations in the frontend after the user has stopped typing. [#2614](https:\/\/github.com\/CSCfi\/rems\/issues\/2614)\\n3. (Problem B) Autosave in the background by replacing\/patching the latest draft-saved event. If the latest event is not a draft-saved then create a new one. [#2767](https:\/\/github.com\/CSCfi\/rems\/issues\/2767)\\n"}
{"File Name":"clean-architecture-example\/0006-avoid-api-versioning.md","Context":"## Context\\nCode base is becomes too complicated.\\nMaintaining and supporting multiple API versions is painful and expensive.\\n","Decision":"We will avoid API versioning.\\nWe will not introduce breaking changes.\\n","tokens":24,"id":3534,"text":"## Context\\nCode base is becomes too complicated.\\nMaintaining and supporting multiple API versions is painful and expensive.\\n\n\n##Decision\nWe will avoid API versioning.\\nWe will not introduce breaking changes.\\n"}
{"File Name":"documentation\/0004-use-asymmetric-jwt-signing.md","Context":"## Context and Problem Statement\\nWhen using JSON Web Tokens generating signatures and verifying them is an important task.\\nJWTs can be signed using a secret (with the HMAC algorithm) or a public\/private key pair using RSA or ECDSA.\\nWhen implementing JWTs one must decide which method to use.\\n## Decision Drivers\\n* Multi tenant support with own keys for each tenant\\n","Decision":"* Multi tenant support with own keys for each tenant\\nChosen option: \"Asymmetric JWT signing\", because it the only option which allow to use different keys for different tenants.\\n### Positive Consequences\\n* multiple keys are supported\\n### Negative Consequences\\n* complex management of keys\\n","tokens":77,"id":414,"text":"## Context and Problem Statement\\nWhen using JSON Web Tokens generating signatures and verifying them is an important task.\\nJWTs can be signed using a secret (with the HMAC algorithm) or a public\/private key pair using RSA or ECDSA.\\nWhen implementing JWTs one must decide which method to use.\\n## Decision Drivers\\n* Multi tenant support with own keys for each tenant\\n\n\n##Decision\n* Multi tenant support with own keys for each tenant\\nChosen option: \"Asymmetric JWT signing\", because it the only option which allow to use different keys for different tenants.\\n### Positive Consequences\\n* multiple keys are supported\\n### Negative Consequences\\n* complex management of keys\\n"}
{"File Name":"opg-metrics\/0005-aws-api-keys.md","Context":"## Context\\nAs our API Endpoint will be exposed to the world, we require it to have some sort of protection and authentication against attackers.\\nWe will initially only be connecting internal facing services (AWS) and thrid party tooling, however in the future we would like to begin recording real user metrics (RUM).\\nWe would need to have a way of protecting against attackers spamming our API.\\n","Decision":"We have chosen to use AWS Api Gateway API Keys for our authorisation and usage limits. This will require an AWS Signature being generated from your credentials and API Key.\\nEach service or integration should have their own key.\\nEach service should also set their own usage limits which are contextual to their service. For example, CircleCI integration could be monitored for the average use during a day and have this set, as well as having a secure way to request the keys when they are rotated.\\nWe have also looked at [AWS WAFV2](https:\/\/docs.aws.amazon.com\/waf\/latest\/APIReference\/Welcome.html) and this is something we may look at in the future depending on our usage of the service.\\n","tokens":83,"id":1544,"text":"## Context\\nAs our API Endpoint will be exposed to the world, we require it to have some sort of protection and authentication against attackers.\\nWe will initially only be connecting internal facing services (AWS) and thrid party tooling, however in the future we would like to begin recording real user metrics (RUM).\\nWe would need to have a way of protecting against attackers spamming our API.\\n\n\n##Decision\nWe have chosen to use AWS Api Gateway API Keys for our authorisation and usage limits. This will require an AWS Signature being generated from your credentials and API Key.\\nEach service or integration should have their own key.\\nEach service should also set their own usage limits which are contextual to their service. For example, CircleCI integration could be monitored for the average use during a day and have this set, as well as having a secure way to request the keys when they are rotated.\\nWe have also looked at [AWS WAFV2](https:\/\/docs.aws.amazon.com\/waf\/latest\/APIReference\/Welcome.html) and this is something we may look at in the future depending on our usage of the service.\\n"}
{"File Name":"ReportMI-service-manual\/0002-overall-technical-approach.md","Context":"## Context\\nThe Data Submission Service is a new digital system for suppliers to submit\\nmanagement information to CCS. It replaces the current Management Information\\nSystem Online (MISO).\\nThis paper seeks approval for the overall technical approach for the new\\nservice.\\n### Management Information System Online (MISO)\\nMISO is an online service run by CCS for suppliers to submit data about work\\nundertaken through CCS commercial agreements. Suppliers submit data on a monthly\\nbasis via a complex Excel spreadsheet.\\nThe data submitted through MISO is used to calculate the management fee that is\\ncharged to suppliers. This data is also used by Commercial Agreement Managers\\n(CAMs) to help with management of suppliers and agreements as well as inform\\nfuture agreement design.\\nMISO was developed in 2011 with only minor updates since.\\nThe service frequently experiences poor performance with frequent errors. It\\ndelivers a poor user experience, and requires a large amount of manual work to\\nadd new commercial agreements and new suppliers.\\nThe number of transactions taken by MISO has increased significantly over recent\\nyears, rising from around 5000 per month in 2015, to over 8000 per month\\nin 2018. Transactions peak around G-Cloud commercial agreement boundaries (with\\nnearly 10,000 transactions in June 2017). This increase puts a further strain on\\nan already stretched service.\\n### Estate in flux\\nThe technology estate within CCS is in a state of flux. Several important parts\\nare being replaced over the next 6 months, including eSourcing (how commercial\\nagreements are established), Finance & HR and the roll-out of the Crown\\nMarketplace.\\nAdditionally, there is ongoing work to replace the CCS website, develop the\\norganisation's data strategy, technical architecture and early plans to develop\\nand improve the identity and access management services.\\n### Data Submission Service Alpha\\nThe Digital Services Team has been working on a prototyping phase to identify\\noptions for a new service to replace MISO.\\nThis phase has involved user research with suppliers and staff within CCS who\\nadminister MISO. It's looked at ways of improving processes and investigated\\npotential technology choices.\\nThe team is now ready to start producing a Minimum Viable Service with an aim to\\ntake real data submissions from suppliers later this year. The medium-term aim\\nis to switch off the existing MISO service in 2019.\\n","Decision":"The new service will need to have the following characteristics:\\n- **Scalability** - the service must cope with high peaks in demand -\\nparticularly around submission deadlines. Outside of peak times, the service is\\ninfrequently used. Over time, the number of transactions is likely to grow,\\nparticularly with the introductions of G-Cloud 10, Digital Outcomes and\\nSpecialists 3 and Dynamic Purchasing System (DPS) where suppliers can leave and\\njoin a commercial agreement at any time.\\n- **Flexibility** - the service needs to collect different types of data for\\ndifferent commercial agreements. This includes invoice data, contract data,\\ndata to support decision making. Over time, CCS may need to collect different\\ntypes of data, particularly as agreements and industries change.\\n- **Adaptable** - as the CCS technical landscape changes, the service will need\\nto change along with it. For example, in future it will need to support a new\\nfinance system, and potential integration with Crown Marketplace, eSourcing and\\nthe CCS website.\\nThese characteristics lend themselves to developing a system with small pieces\\nthat can be easily iterated or replaced as requirements change over time.\\nThe new data submission service will comprise the following pieces:\\n1. **Submission app** - a public facing front-end for suppliers to submit\\nreturns. This will comprise a specially designed user-interface to make the\\njourney for suppliers as simple as possible, providing timely and useful\\nfeedback to help reduce errors, using well-tested design patterns from the\\nGOV.UK toolkit alongside the CCS branding.\\n1. **Data storage layer** - to store the data submitted to CCS. This will act as\\nthe 'core' of the new service, and the source of truth for submission data. This\\nwill be accessed via an API, allowing other CCS services to use the data as\\nrequired.\\n1. **File transformation service** - a small service to take various file inputs\\nfrom suppliers, extract required data and store it in the data storage layer.\\nThis service will need to support various file formats including custom Excel\\ndocuments, CSVs, PDFs and Open Office formats.\\n1. **Data validation service** - a small service to validate data submitted by\\nsuppliers and calculate the approximate management fee required. This will need\\nto adapt over time as different data is collected from suppliers and different\\nmethods of calculating the management fee are introduced.\\n1. **Notification service** - a small service to manage sending of notifications\\nto users. This service will automatically generate notifications, and handle\\nresponses (eg bounces, out of office replies etc), reducing the burden on the\\nCCS support staff. The service will integrate with GOV.UK Notify for sending\\nnotifications.\\n1. **Data exports** - a series of regular 'batch' data exports to provide\\nsubmission data to other systems, including the Data Warehouse and the legacy\\nfinance system.\\n1. **Administration app** - a front-end for CCS staff to support the operation\\nof the data submission service. This will comprise a customised interface to\\nmake support tasks as simple as possible, allowing staff to focus on more\\nimportant work.\\n1. **Onboarding app** - a front-end for CCS staff to manage the onboarding of\\nnew commercial agreements and suppliers. This will compromise a customised\\ninterface to allow CCS to specify commercial agreement data collection\\nrequirements, validation rules, management fee calculation formula, and handle\\nthe onboarding of new users\/suppliers.\\nEach of these components will make use of the most appropriate and simplest\\ntechnology to perform the required functions and will be integrated using well\\ndefined JSON APIs. Each piece should be easy to maintain, iterate and enhance,\\nand should also be easy to replace in future.\\nWhere possible, the service will make use of managed cloud services to provide\\nfeatures such as automatic scaling, high availability and resiliency.\\nThe service will make use of existing government and CCS services where possible\\n- for example GOV.UK Notify, the Supplier Registration Service, Digital\\nMarketplace etc.\\n","tokens":498,"id":2060,"text":"## Context\\nThe Data Submission Service is a new digital system for suppliers to submit\\nmanagement information to CCS. It replaces the current Management Information\\nSystem Online (MISO).\\nThis paper seeks approval for the overall technical approach for the new\\nservice.\\n### Management Information System Online (MISO)\\nMISO is an online service run by CCS for suppliers to submit data about work\\nundertaken through CCS commercial agreements. Suppliers submit data on a monthly\\nbasis via a complex Excel spreadsheet.\\nThe data submitted through MISO is used to calculate the management fee that is\\ncharged to suppliers. This data is also used by Commercial Agreement Managers\\n(CAMs) to help with management of suppliers and agreements as well as inform\\nfuture agreement design.\\nMISO was developed in 2011 with only minor updates since.\\nThe service frequently experiences poor performance with frequent errors. It\\ndelivers a poor user experience, and requires a large amount of manual work to\\nadd new commercial agreements and new suppliers.\\nThe number of transactions taken by MISO has increased significantly over recent\\nyears, rising from around 5000 per month in 2015, to over 8000 per month\\nin 2018. Transactions peak around G-Cloud commercial agreement boundaries (with\\nnearly 10,000 transactions in June 2017). This increase puts a further strain on\\nan already stretched service.\\n### Estate in flux\\nThe technology estate within CCS is in a state of flux. Several important parts\\nare being replaced over the next 6 months, including eSourcing (how commercial\\nagreements are established), Finance & HR and the roll-out of the Crown\\nMarketplace.\\nAdditionally, there is ongoing work to replace the CCS website, develop the\\norganisation's data strategy, technical architecture and early plans to develop\\nand improve the identity and access management services.\\n### Data Submission Service Alpha\\nThe Digital Services Team has been working on a prototyping phase to identify\\noptions for a new service to replace MISO.\\nThis phase has involved user research with suppliers and staff within CCS who\\nadminister MISO. It's looked at ways of improving processes and investigated\\npotential technology choices.\\nThe team is now ready to start producing a Minimum Viable Service with an aim to\\ntake real data submissions from suppliers later this year. The medium-term aim\\nis to switch off the existing MISO service in 2019.\\n\n\n##Decision\nThe new service will need to have the following characteristics:\\n- **Scalability** - the service must cope with high peaks in demand -\\nparticularly around submission deadlines. Outside of peak times, the service is\\ninfrequently used. Over time, the number of transactions is likely to grow,\\nparticularly with the introductions of G-Cloud 10, Digital Outcomes and\\nSpecialists 3 and Dynamic Purchasing System (DPS) where suppliers can leave and\\njoin a commercial agreement at any time.\\n- **Flexibility** - the service needs to collect different types of data for\\ndifferent commercial agreements. This includes invoice data, contract data,\\ndata to support decision making. Over time, CCS may need to collect different\\ntypes of data, particularly as agreements and industries change.\\n- **Adaptable** - as the CCS technical landscape changes, the service will need\\nto change along with it. For example, in future it will need to support a new\\nfinance system, and potential integration with Crown Marketplace, eSourcing and\\nthe CCS website.\\nThese characteristics lend themselves to developing a system with small pieces\\nthat can be easily iterated or replaced as requirements change over time.\\nThe new data submission service will comprise the following pieces:\\n1. **Submission app** - a public facing front-end for suppliers to submit\\nreturns. This will comprise a specially designed user-interface to make the\\njourney for suppliers as simple as possible, providing timely and useful\\nfeedback to help reduce errors, using well-tested design patterns from the\\nGOV.UK toolkit alongside the CCS branding.\\n1. **Data storage layer** - to store the data submitted to CCS. This will act as\\nthe 'core' of the new service, and the source of truth for submission data. This\\nwill be accessed via an API, allowing other CCS services to use the data as\\nrequired.\\n1. **File transformation service** - a small service to take various file inputs\\nfrom suppliers, extract required data and store it in the data storage layer.\\nThis service will need to support various file formats including custom Excel\\ndocuments, CSVs, PDFs and Open Office formats.\\n1. **Data validation service** - a small service to validate data submitted by\\nsuppliers and calculate the approximate management fee required. This will need\\nto adapt over time as different data is collected from suppliers and different\\nmethods of calculating the management fee are introduced.\\n1. **Notification service** - a small service to manage sending of notifications\\nto users. This service will automatically generate notifications, and handle\\nresponses (eg bounces, out of office replies etc), reducing the burden on the\\nCCS support staff. The service will integrate with GOV.UK Notify for sending\\nnotifications.\\n1. **Data exports** - a series of regular 'batch' data exports to provide\\nsubmission data to other systems, including the Data Warehouse and the legacy\\nfinance system.\\n1. **Administration app** - a front-end for CCS staff to support the operation\\nof the data submission service. This will comprise a customised interface to\\nmake support tasks as simple as possible, allowing staff to focus on more\\nimportant work.\\n1. **Onboarding app** - a front-end for CCS staff to manage the onboarding of\\nnew commercial agreements and suppliers. This will compromise a customised\\ninterface to allow CCS to specify commercial agreement data collection\\nrequirements, validation rules, management fee calculation formula, and handle\\nthe onboarding of new users\/suppliers.\\nEach of these components will make use of the most appropriate and simplest\\ntechnology to perform the required functions and will be integrated using well\\ndefined JSON APIs. Each piece should be easy to maintain, iterate and enhance,\\nand should also be easy to replace in future.\\nWhere possible, the service will make use of managed cloud services to provide\\nfeatures such as automatic scaling, high availability and resiliency.\\nThe service will make use of existing government and CCS services where possible\\n- for example GOV.UK Notify, the Supplier Registration Service, Digital\\nMarketplace etc.\\n"}
{"File Name":"planet4-docs\/adr-0020-form-builder-data-retention-policy.md","Context":"### Context and Problem Statement\\nMost of the Form types (as [identified](https:\/\/miro.com\/app\/board\/uXjVO\\_vBIYc=\/)) are already integrating into external systems. But not all of them do (eg. Quiz). For the ones that don\u2019t we need to identify what\u2019s a reasonable policy for NRO admins to have the time to decide the extent of data they want to keep but also have the time to export them. But even for the forms that send data to other systems (eg. Petitions) we need a failsafe to make sure data is actually sent over before deletion.\\nAt the same time we should aim for respecting GDPR requirements for the period of time these data remain on websites\u2019 databases.\\n### Decision Outcome\\n#### **Data retention**\\nEnforce a 90 days retention policy. All submission entries will be automatically deleted after point. It would be up to NRO admins to export any submission data that are not synced to another external system if they wish to.\\n<figure><img src=\"..\/..\/.gitbook\/assets\/form-builder-retention.png\" alt=\"\"><figcaption><\/figcaption><\/figure>\\n#### **Integrate into Wordpress export\/delete tools**\\nThis is useful in cases where an NRO gets a request by a supporter to delete or export personal data.\\n<figure><img src=\"..\/..\/.gitbook\/assets\/form-builder-exporter.png\" alt=\"\"><figcaption><\/figcaption><\/figure>\\n#### **Minimize personal data footprint**\\nDon\u2019t store IP addresses on form submissions.\\n<figure><img src=\"..\/..\/.gitbook\/assets\/form-builder-ip-address.png\" alt=\"\"><figcaption><\/figcaption><\/figure>\\n### Links\\n* [Data Flows Miro board](https:\/\/miro.com\/app\/board\/uXjVO\\_vBIYc=\/)\\n","Decision":"#### **Data retention**\\nEnforce a 90 days retention policy. All submission entries will be automatically deleted after point. It would be up to NRO admins to export any submission data that are not synced to another external system if they wish to.\\n<figure><img src=\"..\/..\/.gitbook\/assets\/form-builder-retention.png\" alt=\"\"><figcaption><\/figcaption><\/figure>\\n#### **Integrate into Wordpress export\/delete tools**\\nThis is useful in cases where an NRO gets a request by a supporter to delete or export personal data.\\n<figure><img src=\"..\/..\/.gitbook\/assets\/form-builder-exporter.png\" alt=\"\"><figcaption><\/figcaption><\/figure>\\n#### **Minimize personal data footprint**\\nDon\u2019t store IP addresses on form submissions.\\n<figure><img src=\"..\/..\/.gitbook\/assets\/form-builder-ip-address.png\" alt=\"\"><figcaption><\/figcaption><\/figure>\\n### Links\\n* [Data Flows Miro board](https:\/\/miro.com\/app\/board\/uXjVO\\_vBIYc=\/)\\n","tokens":371,"id":4091,"text":"### Context and Problem Statement\\nMost of the Form types (as [identified](https:\/\/miro.com\/app\/board\/uXjVO\\_vBIYc=\/)) are already integrating into external systems. But not all of them do (eg. Quiz). For the ones that don\u2019t we need to identify what\u2019s a reasonable policy for NRO admins to have the time to decide the extent of data they want to keep but also have the time to export them. But even for the forms that send data to other systems (eg. Petitions) we need a failsafe to make sure data is actually sent over before deletion.\\nAt the same time we should aim for respecting GDPR requirements for the period of time these data remain on websites\u2019 databases.\\n### Decision Outcome\\n#### **Data retention**\\nEnforce a 90 days retention policy. All submission entries will be automatically deleted after point. It would be up to NRO admins to export any submission data that are not synced to another external system if they wish to.\\n<figure><img src=\"..\/..\/.gitbook\/assets\/form-builder-retention.png\" alt=\"\"><figcaption><\/figcaption><\/figure>\\n#### **Integrate into Wordpress export\/delete tools**\\nThis is useful in cases where an NRO gets a request by a supporter to delete or export personal data.\\n<figure><img src=\"..\/..\/.gitbook\/assets\/form-builder-exporter.png\" alt=\"\"><figcaption><\/figcaption><\/figure>\\n#### **Minimize personal data footprint**\\nDon\u2019t store IP addresses on form submissions.\\n<figure><img src=\"..\/..\/.gitbook\/assets\/form-builder-ip-address.png\" alt=\"\"><figcaption><\/figcaption><\/figure>\\n### Links\\n* [Data Flows Miro board](https:\/\/miro.com\/app\/board\/uXjVO\\_vBIYc=\/)\\n\n\n##Decision\n#### **Data retention**\\nEnforce a 90 days retention policy. All submission entries will be automatically deleted after point. It would be up to NRO admins to export any submission data that are not synced to another external system if they wish to.\\n<figure><img src=\"..\/..\/.gitbook\/assets\/form-builder-retention.png\" alt=\"\"><figcaption><\/figcaption><\/figure>\\n#### **Integrate into Wordpress export\/delete tools**\\nThis is useful in cases where an NRO gets a request by a supporter to delete or export personal data.\\n<figure><img src=\"..\/..\/.gitbook\/assets\/form-builder-exporter.png\" alt=\"\"><figcaption><\/figcaption><\/figure>\\n#### **Minimize personal data footprint**\\nDon\u2019t store IP addresses on form submissions.\\n<figure><img src=\"..\/..\/.gitbook\/assets\/form-builder-ip-address.png\" alt=\"\"><figcaption><\/figcaption><\/figure>\\n### Links\\n* [Data Flows Miro board](https:\/\/miro.com\/app\/board\/uXjVO\\_vBIYc=\/)\\n"}
{"File Name":"qiskit-vscode\/ADR-004.md","Context":"## Context\\nThe visualization features of the project started as a spike, and the code is not very clear. We decided that a templates engine will solve some of the code problems and will make easier to create new visualizations in the future.\\n","Decision":"The recommended options from the team were, React, Lit-HTML and Nunjucks. React looks like an excellent option, but had some problems with the current templates, so Nunjucks replaced it as the best option at this moment.\\n","tokens":49,"id":1635,"text":"## Context\\nThe visualization features of the project started as a spike, and the code is not very clear. We decided that a templates engine will solve some of the code problems and will make easier to create new visualizations in the future.\\n\n\n##Decision\nThe recommended options from the team were, React, Lit-HTML and Nunjucks. React looks like an excellent option, but had some problems with the current templates, so Nunjucks replaced it as the best option at this moment.\\n"}
{"File Name":"gatemint-sdk\/adr-011-generalize-genesis-accounts.md","Context":"## Context\\nCurrently, the SDK allows for custom account types; the `auth` keeper stores any type fulfilling its `Account` interface. However `auth` does not handle exporting or loading accounts to\/from a genesis file, this is done by `genaccounts`, which only handles one of 4 concrete account types (`BaseAccount`, `ContinuousVestingAccount`, `DelayedVestingAccount` and `ModuleAccount`).\\nProjects desiring to use custom accounts (say custom vesting accounts) need to fork and modify `genaccounts`.\\n","Decision":"In summary, we will (un)marshal all accounts (interface types) directly using amino, rather than converting to `genaccounts`\u2019s `GenesisAccount` type. Since doing this removes the majority of `genaccounts`'s code, we will merge `genaccounts` into `auth`. Marshalled accounts will be stored in `auth`'s genesis state.\\nDetailed changes:\\n### 1) (Un)Marshal accounts directly using amino\\nThe `auth` module's `GenesisState` gains a new field `Accounts`. Note these aren't of type `exported.Account` for reasons outlined in section 3.\\n```go\\n\/\/ GenesisState - all auth state that must be provided at genesis\\ntype GenesisState struct {\\nParams   Params           `json:\"params\" yaml:\"params\"`\\nAccounts []GenesisAccount `json:\"accounts\" yaml:\"accounts\"`\\n}\\n```\\nNow `auth`'s `InitGenesis` and `ExportGenesis` (un)marshal accounts as well as the defined params.\\n```go\\n\/\/ InitGenesis - Init store state from genesis data\\nfunc InitGenesis(ctx sdk.Context, ak AccountKeeper, data GenesisState) {\\nak.SetParams(ctx, data.Params)\\n\/\/ load the accounts\\nfor _, a := range data.Accounts {\\nacc := ak.NewAccount(ctx, a) \/\/ set account number\\nak.SetAccount(ctx, acc)\\n}\\n}\\n\/\/ ExportGenesis returns a GenesisState for a given context and keeper\\nfunc ExportGenesis(ctx sdk.Context, ak AccountKeeper) GenesisState {\\nparams := ak.GetParams(ctx)\\nvar genAccounts []exported.GenesisAccount\\nak.IterateAccounts(ctx, func(account exported.Account) bool {\\ngenAccount := account.(exported.GenesisAccount)\\ngenAccounts = append(genAccounts, genAccount)\\nreturn false\\n})\\nreturn NewGenesisState(params, genAccounts)\\n}\\n```\\n### 2) Register custom account types on the `auth` codec\\nThe `auth` codec must have all custom account types registered to marshal them. We will follow the pattern established in `gov` for proposals.\\nAn example custom account definition:\\n```go\\nimport authtypes \"github.com\/cosmos\/cosmos-sdk\/x\/auth\/types\"\\n\/\/ Register the module account type with the auth module codec so it can decode module accounts stored in a genesis file\\nfunc init() {\\nauthtypes.RegisterAccountTypeCodec(ModuleAccount{}, \"cosmos-sdk\/ModuleAccount\")\\n}\\ntype ModuleAccount struct {\\n...\\n```\\nThe `auth` codec definition:\\n```go\\nvar ModuleCdc *codec.LegacyAmino\\nfunc init() {\\nModuleCdc = codec.NewLegacyAmino()\\n\/\/ register module msg's and Account interface\\n...\\n\/\/ leave the codec unsealed\\n}\\n\/\/ RegisterAccountTypeCodec registers an external account type defined in another module for the internal ModuleCdc.\\nfunc RegisterAccountTypeCodec(o interface{}, name string) {\\nModuleCdc.RegisterConcrete(o, name, nil)\\n}\\n```\\n### 3) Genesis validation for custom account types\\nModules implement a `ValidateGenesis` method. As `auth` does not know of account implementations, accounts will need to validate themselves.\\nWe will unmarshal accounts into a `GenesisAccount` interface that includes a `Validate` method.\\n```go\\ntype GenesisAccount interface {\\nexported.Account\\nValidate() error\\n}\\n```\\nThen the `auth` `ValidateGenesis` function becomes:\\n```go\\n\/\/ ValidateGenesis performs basic validation of auth genesis data returning an\\n\/\/ error for any failed validation criteria.\\nfunc ValidateGenesis(data GenesisState) error {\\n\/\/ Validate params\\n...\\n\/\/ Validate accounts\\naddrMap := make(map[string]bool, len(data.Accounts))\\nfor _, acc := range data.Accounts {\\n\/\/ check for duplicated accounts\\naddrStr := acc.GetAddress().String()\\nif _, ok := addrMap[addrStr]; ok {\\nreturn fmt.Errorf(\"duplicate account found in genesis state; address: %s\", addrStr)\\n}\\naddrMap[addrStr] = true\\n\/\/ check account specific validation\\nif err := acc.Validate(); err != nil {\\nreturn fmt.Errorf(\"invalid account found in genesis state; address: %s, error: %s\", addrStr, err.Error())\\n}\\n}\\nreturn nil\\n}\\n```\\n### 4) Move add-genesis-account cli to `auth`\\nThe `genaccounts` module contains a cli command to add base or vesting accounts to a genesis file.\\nThis will be moved to `auth`. We will leave it to projects to write their own commands to add custom accounts. An extensible cli handler, similar to `gov`, could be created but it is not worth the complexity for this minor use case.\\n### 5) Update module and vesting accounts\\nUnder the new scheme, module and vesting account types need some minor updates:\\n- Type registration on `auth`'s codec (shown above)\\n- A `Validate` method for each `Account` concrete type\\n","tokens":111,"id":25,"text":"## Context\\nCurrently, the SDK allows for custom account types; the `auth` keeper stores any type fulfilling its `Account` interface. However `auth` does not handle exporting or loading accounts to\/from a genesis file, this is done by `genaccounts`, which only handles one of 4 concrete account types (`BaseAccount`, `ContinuousVestingAccount`, `DelayedVestingAccount` and `ModuleAccount`).\\nProjects desiring to use custom accounts (say custom vesting accounts) need to fork and modify `genaccounts`.\\n\n\n##Decision\nIn summary, we will (un)marshal all accounts (interface types) directly using amino, rather than converting to `genaccounts`\u2019s `GenesisAccount` type. Since doing this removes the majority of `genaccounts`'s code, we will merge `genaccounts` into `auth`. Marshalled accounts will be stored in `auth`'s genesis state.\\nDetailed changes:\\n### 1) (Un)Marshal accounts directly using amino\\nThe `auth` module's `GenesisState` gains a new field `Accounts`. Note these aren't of type `exported.Account` for reasons outlined in section 3.\\n```go\\n\/\/ GenesisState - all auth state that must be provided at genesis\\ntype GenesisState struct {\\nParams   Params           `json:\"params\" yaml:\"params\"`\\nAccounts []GenesisAccount `json:\"accounts\" yaml:\"accounts\"`\\n}\\n```\\nNow `auth`'s `InitGenesis` and `ExportGenesis` (un)marshal accounts as well as the defined params.\\n```go\\n\/\/ InitGenesis - Init store state from genesis data\\nfunc InitGenesis(ctx sdk.Context, ak AccountKeeper, data GenesisState) {\\nak.SetParams(ctx, data.Params)\\n\/\/ load the accounts\\nfor _, a := range data.Accounts {\\nacc := ak.NewAccount(ctx, a) \/\/ set account number\\nak.SetAccount(ctx, acc)\\n}\\n}\\n\/\/ ExportGenesis returns a GenesisState for a given context and keeper\\nfunc ExportGenesis(ctx sdk.Context, ak AccountKeeper) GenesisState {\\nparams := ak.GetParams(ctx)\\nvar genAccounts []exported.GenesisAccount\\nak.IterateAccounts(ctx, func(account exported.Account) bool {\\ngenAccount := account.(exported.GenesisAccount)\\ngenAccounts = append(genAccounts, genAccount)\\nreturn false\\n})\\nreturn NewGenesisState(params, genAccounts)\\n}\\n```\\n### 2) Register custom account types on the `auth` codec\\nThe `auth` codec must have all custom account types registered to marshal them. We will follow the pattern established in `gov` for proposals.\\nAn example custom account definition:\\n```go\\nimport authtypes \"github.com\/cosmos\/cosmos-sdk\/x\/auth\/types\"\\n\/\/ Register the module account type with the auth module codec so it can decode module accounts stored in a genesis file\\nfunc init() {\\nauthtypes.RegisterAccountTypeCodec(ModuleAccount{}, \"cosmos-sdk\/ModuleAccount\")\\n}\\ntype ModuleAccount struct {\\n...\\n```\\nThe `auth` codec definition:\\n```go\\nvar ModuleCdc *codec.LegacyAmino\\nfunc init() {\\nModuleCdc = codec.NewLegacyAmino()\\n\/\/ register module msg's and Account interface\\n...\\n\/\/ leave the codec unsealed\\n}\\n\/\/ RegisterAccountTypeCodec registers an external account type defined in another module for the internal ModuleCdc.\\nfunc RegisterAccountTypeCodec(o interface{}, name string) {\\nModuleCdc.RegisterConcrete(o, name, nil)\\n}\\n```\\n### 3) Genesis validation for custom account types\\nModules implement a `ValidateGenesis` method. As `auth` does not know of account implementations, accounts will need to validate themselves.\\nWe will unmarshal accounts into a `GenesisAccount` interface that includes a `Validate` method.\\n```go\\ntype GenesisAccount interface {\\nexported.Account\\nValidate() error\\n}\\n```\\nThen the `auth` `ValidateGenesis` function becomes:\\n```go\\n\/\/ ValidateGenesis performs basic validation of auth genesis data returning an\\n\/\/ error for any failed validation criteria.\\nfunc ValidateGenesis(data GenesisState) error {\\n\/\/ Validate params\\n...\\n\/\/ Validate accounts\\naddrMap := make(map[string]bool, len(data.Accounts))\\nfor _, acc := range data.Accounts {\\n\/\/ check for duplicated accounts\\naddrStr := acc.GetAddress().String()\\nif _, ok := addrMap[addrStr]; ok {\\nreturn fmt.Errorf(\"duplicate account found in genesis state; address: %s\", addrStr)\\n}\\naddrMap[addrStr] = true\\n\/\/ check account specific validation\\nif err := acc.Validate(); err != nil {\\nreturn fmt.Errorf(\"invalid account found in genesis state; address: %s, error: %s\", addrStr, err.Error())\\n}\\n}\\nreturn nil\\n}\\n```\\n### 4) Move add-genesis-account cli to `auth`\\nThe `genaccounts` module contains a cli command to add base or vesting accounts to a genesis file.\\nThis will be moved to `auth`. We will leave it to projects to write their own commands to add custom accounts. An extensible cli handler, similar to `gov`, could be created but it is not worth the complexity for this minor use case.\\n### 5) Update module and vesting accounts\\nUnder the new scheme, module and vesting account types need some minor updates:\\n- Type registration on `auth`'s codec (shown above)\\n- A `Validate` method for each `Account` concrete type\\n"}
{"File Name":"buildit-all\/0003-use-junit-for-tests-instead-of-spek.md","Context":"## Context\\nThere are a number of unit testing frameworks available for the JVM.  There are also some newer unit testing frameworks that are specific to Kotlin.  Spring currently (4.x) only supports JUnit 4 and TestNG.  JUnit 5 can be made to work however.\\n","Decision":"Use JUnit 5 for all unit and e2e tests.  This will simplify thing and has better integration currently with IntelliJ IDE.\\n","tokens":61,"id":4426,"text":"## Context\\nThere are a number of unit testing frameworks available for the JVM.  There are also some newer unit testing frameworks that are specific to Kotlin.  Spring currently (4.x) only supports JUnit 4 and TestNG.  JUnit 5 can be made to work however.\\n\n\n##Decision\nUse JUnit 5 for all unit and e2e tests.  This will simplify thing and has better integration currently with IntelliJ IDE.\\n"}
{"File Name":"wikiindex\/adr-002-routing_library.md","Context":"## Context\\n* We are building a web app that has one official url that uses query params\\n* The url is \/search?q=search-term\\n* We believe that future endpoints will be added, or more complicated search requests\\n","Decision":"* We will use Bidi to parse the routes.\\n","tokens":46,"id":1384,"text":"## Context\\n* We are building a web app that has one official url that uses query params\\n* The url is \/search?q=search-term\\n* We believe that future endpoints will be added, or more complicated search requests\\n\n\n##Decision\n* We will use Bidi to parse the routes.\\n"}
{"File Name":"CrossyToad\/adr-0009-organise-by-feature.md","Context":"## Context\\nThere are two main strategies for organising source code that I'm aware of:\\n- Organise by kind\\n- Organise by feature\\nWith \"Organise by kind\" we group modules that are related architecturally together. I.e. group all\\n\"Effects\", group all \"Data\", group all \"Scenes\".\\nThis is a fairly common approach, particularly given that website architectures often typify this\\nstyle by grouping all \"modules\", \"views\" and \"controllers\".\\nAlternatively we can \"Organise by feature\", ideally in this style the source code required to\\nimplement a particular \"Feature\" is grouped together instead. Ideally this would mean that\\nremoving a feature would simply mean deleting it's grouping any removing any dependent links.\\nFor this project I want to experiement to see what \"Organise by feature\" would look like for a game.\\n","Decision":"We're going to organise by *feature*!\\n","tokens":182,"id":2485,"text":"## Context\\nThere are two main strategies for organising source code that I'm aware of:\\n- Organise by kind\\n- Organise by feature\\nWith \"Organise by kind\" we group modules that are related architecturally together. I.e. group all\\n\"Effects\", group all \"Data\", group all \"Scenes\".\\nThis is a fairly common approach, particularly given that website architectures often typify this\\nstyle by grouping all \"modules\", \"views\" and \"controllers\".\\nAlternatively we can \"Organise by feature\", ideally in this style the source code required to\\nimplement a particular \"Feature\" is grouped together instead. Ideally this would mean that\\nremoving a feature would simply mean deleting it's grouping any removing any dependent links.\\nFor this project I want to experiement to see what \"Organise by feature\" would look like for a game.\\n\n\n##Decision\nWe're going to organise by *feature*!\\n"}
{"File Name":"ehoks-ui\/0001-record-architecture-and-desing-decisions.md","Context":"## Context\\nWe need to record the architectural and desing decisions made during this project.\\n","Decision":"We will use Architecture Decision Records, as described by\\nMichael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\nAlso https:\/\/medium.com\/better-programming\/here-is-a-simple-yet-powerful-tool-to-record-your-architectural-decisions-5fb31367a7da\\n","tokens":19,"id":597,"text":"## Context\\nWe need to record the architectural and desing decisions made during this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as described by\\nMichael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\nAlso https:\/\/medium.com\/better-programming\/here-is-a-simple-yet-powerful-tool-to-record-your-architectural-decisions-5fb31367a7da\\n"}
{"File Name":"cafebabel.com\/0004-deal-with-translations.md","Context":"## Context\\nArticles can be translated in English, French, Spanish, Italian and\\nGerman.\\nOriginal articles are not necessarily in English and initial text of a\\ngiven translation can be itself an already translated article.\\nEach translation must keep a link to the original article.\\nThe review process remains the same with a draft status.\\nThe translated article must both have a redactor and a translator.\\n","Decision":"A new MongoDB document will be created for each translation.\\nThat document will inherit from the Article model.\\n","tokens":81,"id":3241,"text":"## Context\\nArticles can be translated in English, French, Spanish, Italian and\\nGerman.\\nOriginal articles are not necessarily in English and initial text of a\\ngiven translation can be itself an already translated article.\\nEach translation must keep a link to the original article.\\nThe review process remains the same with a draft status.\\nThe translated article must both have a redactor and a translator.\\n\n\n##Decision\nA new MongoDB document will be created for each translation.\\nThat document will inherit from the Article model.\\n"}
{"File Name":"compliantkubernetes\/0001-use-rook-storage-orchestrator.md","Context":"## Context and Problem Statement\\nCompliant Kubernetes has the vision to reduce the compliance burden on multiple clouds (\"Multi-cloud. Open source. Compliant.\"). Many of the cloud providers we target do not have a storage provider or do not have a storage provider that integrates with Kubernetes. How should we support PersistentVolumeClaims in such cases?\\n## Decision Drivers\\n* Storage Orchestrator needs to be popular and well maintained, so that developer can focus on adding value on top of Kubernetes clusters.\\n* Storage Orchestrator needs to be easy to set up, easy to operate and battle-tested, so on-call administrators are not constantly woken up.\\n* Storage Orchestrator needs to have reasonable performance. (A local storage provider can deal with high-performance use-cases.)\\n","Decision":"* Storage Orchestrator needs to be popular and well maintained, so that developer can focus on adding value on top of Kubernetes clusters.\\n* Storage Orchestrator needs to be easy to set up, easy to operate and battle-tested, so on-call administrators are not constantly woken up.\\n* Storage Orchestrator needs to have reasonable performance. (A local storage provider can deal with high-performance use-cases.)\\nChosen option: \"Rook\", because it is CNCF graduated, hence it is most likely to drive development and adoption long-term. Prady tested it and showed it was easy to use. It supports Ceph as a backend, making it battle-tested. It has reasonable performance.\\n### Positive Consequences\\n* We no longer need to worry about cloud provider without native storage.\\n### Negative Consequences\\n* We need to deprecate our NFS storage provider.\\n* Some manual steps are required to set up partitions for Rook. These will be automated when the burden justifies it.\\n","tokens":158,"id":3101,"text":"## Context and Problem Statement\\nCompliant Kubernetes has the vision to reduce the compliance burden on multiple clouds (\"Multi-cloud. Open source. Compliant.\"). Many of the cloud providers we target do not have a storage provider or do not have a storage provider that integrates with Kubernetes. How should we support PersistentVolumeClaims in such cases?\\n## Decision Drivers\\n* Storage Orchestrator needs to be popular and well maintained, so that developer can focus on adding value on top of Kubernetes clusters.\\n* Storage Orchestrator needs to be easy to set up, easy to operate and battle-tested, so on-call administrators are not constantly woken up.\\n* Storage Orchestrator needs to have reasonable performance. (A local storage provider can deal with high-performance use-cases.)\\n\n\n##Decision\n* Storage Orchestrator needs to be popular and well maintained, so that developer can focus on adding value on top of Kubernetes clusters.\\n* Storage Orchestrator needs to be easy to set up, easy to operate and battle-tested, so on-call administrators are not constantly woken up.\\n* Storage Orchestrator needs to have reasonable performance. (A local storage provider can deal with high-performance use-cases.)\\nChosen option: \"Rook\", because it is CNCF graduated, hence it is most likely to drive development and adoption long-term. Prady tested it and showed it was easy to use. It supports Ceph as a backend, making it battle-tested. It has reasonable performance.\\n### Positive Consequences\\n* We no longer need to worry about cloud provider without native storage.\\n### Negative Consequences\\n* We need to deprecate our NFS storage provider.\\n* Some manual steps are required to set up partitions for Rook. These will be automated when the burden justifies it.\\n"}
{"File Name":"origin\/0004-use-openzeppelin.md","Context":"## Context\\nOrigin SDK contracts are designed to be upgradable by abstracting proxy, logic and storage to separate contracts. This approach leads to maintaining 3 separate Solidity files per contract.\\n","Decision":"Use OpenZeppelin implementation based on generalized proxy, logic and storage to remove the need of keeping 3 separate custom implemented contracts.\\n","tokens":39,"id":1551,"text":"## Context\\nOrigin SDK contracts are designed to be upgradable by abstracting proxy, logic and storage to separate contracts. This approach leads to maintaining 3 separate Solidity files per contract.\\n\n\n##Decision\nUse OpenZeppelin implementation based on generalized proxy, logic and storage to remove the need of keeping 3 separate custom implemented contracts.\\n"}
{"File Name":"human-essentials\/0007-barcode-querying.md","Context":"## Context\\nWe've been using Barcode values that have been recorded as either private (associated only with an organization) or global (available to all organizations). The idea was that Essentials Banks could decide to \"Share\" a barcode with everyone by making it global, and that this barcode could then be used by others. The implementation, as well as the actual use-cases by Essentials Banks in beta testing, has contraindicated this utility. Additionally, it's unclear how Barcode lookup happens with regard to Base Items.\\n","Decision":"\"Global\" barcodes are associated with Base Items. Doing a Barcode lookup now does a cascade retrieval.\\n1. First, it checks if the organization has defined a barcode with that value.\\n2. If that exists, then it uses that record.\\n3. If it doesn't exist, then it checks to see if that barcode value exists as a global barcode.\\n4. If it exists there, it gets the base item associated with that, and then looks at the organizations items and finds the oldest item with that base item type, and applies it to that.\\n5. If it can't find one for that, then it prompts the user to create a new barcode record.\\nWhen we get EAN13 numbers for major products, we can enter those as global barcodes, associated with the generic base items; eg. 48x Pampers 3T and 48x Huggies 3T will both map to the 48x 3T record.\\n","tokens":105,"id":3344,"text":"## Context\\nWe've been using Barcode values that have been recorded as either private (associated only with an organization) or global (available to all organizations). The idea was that Essentials Banks could decide to \"Share\" a barcode with everyone by making it global, and that this barcode could then be used by others. The implementation, as well as the actual use-cases by Essentials Banks in beta testing, has contraindicated this utility. Additionally, it's unclear how Barcode lookup happens with regard to Base Items.\\n\n\n##Decision\n\"Global\" barcodes are associated with Base Items. Doing a Barcode lookup now does a cascade retrieval.\\n1. First, it checks if the organization has defined a barcode with that value.\\n2. If that exists, then it uses that record.\\n3. If it doesn't exist, then it checks to see if that barcode value exists as a global barcode.\\n4. If it exists there, it gets the base item associated with that, and then looks at the organizations items and finds the oldest item with that base item type, and applies it to that.\\n5. If it can't find one for that, then it prompts the user to create a new barcode record.\\nWhen we get EAN13 numbers for major products, we can enter those as global barcodes, associated with the generic base items; eg. 48x Pampers 3T and 48x Huggies 3T will both map to the 48x 3T record.\\n"}
{"File Name":"mediawiki-extensions-WikibaseManifest\/0000_deliver_as_extension.md","Context":"## Context\\nWe considered two options for delivering Wikibase Manifest:\\n- Built as MediaWiki Extension\\n- Include in Wikibase Core\\n|   Options\t|  Consistency across 3rd party Wikibases \t|   Installation\/Setup Burden\t|  User Adoption \t|\\n|---\t|---\t|---\t|---\t|\\n|  MW Extension \t|   Third-party Wikibase admins will have to provide the Manifest via extension.\t|   Additional effort of setting up an extension. It could be part of the docker bundle\t|   Assuming lower as it involves installing a separate extension\t|\\n|  Wikibase core\t|   Tool builders could rely on the Manifest always being there in any third-party Wikibase. This would be an advantage over the extension. But the user still needs to configure it. |   None\t|  Assuming higher\t|\\n|   Options\t|  Maintenance Burden \t|   Backwards Compatibility\t|  Testing Infrastructure\t|\\n|---\t|---\t|---\t|---\t|\\n|  MW Extension \t|   Need to set up our own CI\t|   compatible|   Less straightforward to test in Beta if we make this an extension, will have to set up our own cloudvps test instance.\t|\\n|  Wikibase core\t|   Same as Wikibase\t|   compatible\t|   Could use Beta or a cloudvps test instance for testing infrastructure. Probably an advantage    |\\n|   Options\t|  Documentation\t|   Speed of release to Wikibase users\t|  Feedback loop with tool builders \t|\\n|---\t|---\t|---\t|---\t|\\n|  MW Extension \t|   Need to document installation of the extension, as well as setup. Probably minimal additional effort \t|   Potentially faster\t|   Probably shorter, for the same reasons stated on speed of release to wb users\t|\\n|  Wikibase core\t|   Document how to work with the file\t|   Waiting until a new release is made (e.g. 1.36) or we have to do some backporting\t|   Not sure\t|\\n","Decision":"Build as a MediaWiki Extension.\\nPlan to move the functionality to Wikibase core in the future.\\n","tokens":460,"id":4999,"text":"## Context\\nWe considered two options for delivering Wikibase Manifest:\\n- Built as MediaWiki Extension\\n- Include in Wikibase Core\\n|   Options\t|  Consistency across 3rd party Wikibases \t|   Installation\/Setup Burden\t|  User Adoption \t|\\n|---\t|---\t|---\t|---\t|\\n|  MW Extension \t|   Third-party Wikibase admins will have to provide the Manifest via extension.\t|   Additional effort of setting up an extension. It could be part of the docker bundle\t|   Assuming lower as it involves installing a separate extension\t|\\n|  Wikibase core\t|   Tool builders could rely on the Manifest always being there in any third-party Wikibase. This would be an advantage over the extension. But the user still needs to configure it. |   None\t|  Assuming higher\t|\\n|   Options\t|  Maintenance Burden \t|   Backwards Compatibility\t|  Testing Infrastructure\t|\\n|---\t|---\t|---\t|---\t|\\n|  MW Extension \t|   Need to set up our own CI\t|   compatible|   Less straightforward to test in Beta if we make this an extension, will have to set up our own cloudvps test instance.\t|\\n|  Wikibase core\t|   Same as Wikibase\t|   compatible\t|   Could use Beta or a cloudvps test instance for testing infrastructure. Probably an advantage    |\\n|   Options\t|  Documentation\t|   Speed of release to Wikibase users\t|  Feedback loop with tool builders \t|\\n|---\t|---\t|---\t|---\t|\\n|  MW Extension \t|   Need to document installation of the extension, as well as setup. Probably minimal additional effort \t|   Potentially faster\t|   Probably shorter, for the same reasons stated on speed of release to wb users\t|\\n|  Wikibase core\t|   Document how to work with the file\t|   Waiting until a new release is made (e.g. 1.36) or we have to do some backporting\t|   Not sure\t|\\n\n\n##Decision\nBuild as a MediaWiki Extension.\\nPlan to move the functionality to Wikibase core in the future.\\n"}
{"File Name":"james-project\/0046-generalize-event-bus.md","Context":"## Context\\nUser email storage usage is limited both in size and count via quotas (IMAP RFC-2087). In order to ease administrating large user bases, the quota search extension allows administrator\\nto retrieve all users whose email usages are exceeding a given occupation ratio.\\nWhen searching for users by quota ratio if we set the value of the parameters to 0, for example: `\/quotas\/users?minOccupationRatio=0&maxOccupationRatio=0`, the search feature is supposed to return newly created users\\nwho have not received any email yet at that point. However, this is not the case because the quotas are currently being initialized only after\\na user has received the first email.\\nWe need to initialize user quotas upon user creation time. The problem is: there is currently no event at user creation\\nand since the quota-search feature is a plugin of James, it cannot be hardwired into the domain logic of user management to initialize the quota for a just created user.\\n","Decision":"For quota-search to be initialized\/removed for a given user while keeping this feature as a plugin, we decided to adopt the Event Driven pattern we already use in Mailbox-api.\\nWe can create new events related to user management (UserCreated, UserRemoved and so on).\\nTo achieve that, we will extract the EventBus out of mailbox-api in order to make it a utility component (eventbus-api), then we will make both mailbox-api and data-api depend on that new module.\\n","tokens":207,"id":2868,"text":"## Context\\nUser email storage usage is limited both in size and count via quotas (IMAP RFC-2087). In order to ease administrating large user bases, the quota search extension allows administrator\\nto retrieve all users whose email usages are exceeding a given occupation ratio.\\nWhen searching for users by quota ratio if we set the value of the parameters to 0, for example: `\/quotas\/users?minOccupationRatio=0&maxOccupationRatio=0`, the search feature is supposed to return newly created users\\nwho have not received any email yet at that point. However, this is not the case because the quotas are currently being initialized only after\\na user has received the first email.\\nWe need to initialize user quotas upon user creation time. The problem is: there is currently no event at user creation\\nand since the quota-search feature is a plugin of James, it cannot be hardwired into the domain logic of user management to initialize the quota for a just created user.\\n\n\n##Decision\nFor quota-search to be initialized\/removed for a given user while keeping this feature as a plugin, we decided to adopt the Event Driven pattern we already use in Mailbox-api.\\nWe can create new events related to user management (UserCreated, UserRemoved and so on).\\nTo achieve that, we will extract the EventBus out of mailbox-api in order to make it a utility component (eventbus-api), then we will make both mailbox-api and data-api depend on that new module.\\n"}
{"File Name":"james-project\/0002-make-taskmanager-distributed.md","Context":"## Context\\nIn order to have a distributed version of James we need to have an homogeneous way to deal with `Task`.\\nCurrently, every James nodes of a cluster have their own instance of `TaskManager` and they have no knowledge of others, making it impossible to orchestrate task execution at the cluster level.\\nTasks are scheduled and ran on the same node they are scheduled.\\nWe are also unable to list or access to the details of all the `Task`s of a cluster.\\n","Decision":"Create a distribution-aware implementation of `TaskManager`.\\n","tokens":102,"id":2874,"text":"## Context\\nIn order to have a distributed version of James we need to have an homogeneous way to deal with `Task`.\\nCurrently, every James nodes of a cluster have their own instance of `TaskManager` and they have no knowledge of others, making it impossible to orchestrate task execution at the cluster level.\\nTasks are scheduled and ran on the same node they are scheduled.\\nWe are also unable to list or access to the details of all the `Task`s of a cluster.\\n\n\n##Decision\nCreate a distribution-aware implementation of `TaskManager`.\\n"}
{"File Name":"the-zoo\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard\\nin this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":494,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as described by Michael Nygard\\nin this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n"}
{"File Name":"stentor\/0002-use-toml-for-config-file.md","Context":"## Context\\n`stentor` needs to pick a config file format.\\nThe options under consideration are yaml and toml.\\nBoth options are human readable and writable,\\nallow for easy parsing of structured data,\\nand are supported by well maintained libraries.\\nyaml has the benefit of being more straightforward to write,\\nespecially for nested structures.\\nHowever, toml is intended for config files,\\nand provides stricter parsing out of the box.\\n","Decision":"`stentor` will use toml for its config file.\\n","tokens":93,"id":2164,"text":"## Context\\n`stentor` needs to pick a config file format.\\nThe options under consideration are yaml and toml.\\nBoth options are human readable and writable,\\nallow for easy parsing of structured data,\\nand are supported by well maintained libraries.\\nyaml has the benefit of being more straightforward to write,\\nespecially for nested structures.\\nHowever, toml is intended for config files,\\nand provides stricter parsing out of the box.\\n\n\n##Decision\n`stentor` will use toml for its config file.\\n"}
{"File Name":"interlok\/0007-jetty-failsafe.md","Context":"## Context and Problem Statement\\nThere are 2 ways to enable the jetty management component in bootstrap.properties. The first bootstraps jetty using the configured XML configuration file, the second bootstraps a jetty server with minimal defaults; it also doesn't by default deploy the webapp provider which means that the UI is not started (unless the optional `webServerWebappUrl` is specified). We have always considered the `FromProperties` variant to be not production ready and a message is logged to that effect on startup.\\n```\\nmanagementComponents=jmx:jetty\\nwebServerConfigUrl=.\/config\/jetty.xml\\n","Decision":"Embed the jetty.xml into interlok-common.jar and use it if jetty is enabled but no other settings are defined.\\n","tokens":130,"id":2347,"text":"## Context and Problem Statement\\nThere are 2 ways to enable the jetty management component in bootstrap.properties. The first bootstraps jetty using the configured XML configuration file, the second bootstraps a jetty server with minimal defaults; it also doesn't by default deploy the webapp provider which means that the UI is not started (unless the optional `webServerWebappUrl` is specified). We have always considered the `FromProperties` variant to be not production ready and a message is logged to that effect on startup.\\n```\\nmanagementComponents=jmx:jetty\\nwebServerConfigUrl=.\/config\/jetty.xml\\n\n\n##Decision\nEmbed the jetty.xml into interlok-common.jar and use it if jetty is enabled but no other settings are defined.\\n"}
{"File Name":"unruly-puppet\/0002-standalone-nrpe-custom-check-module.md","Context":"## Context\\nWe need to add new functionality to the project - the ability to create custom NRPE checks.\\nThis means having templated configuration for adding service-specific plugins for the purposes of monitoring.\\nThe `base` module already contains NRPE specific code.\\nIt installs NRPE and ensures it is running. It also configures a set of default monitoring plugins.\\n","Decision":"We considered three possible approaches to adding the new NRPE functionality:\\n- Add it to the `base::nrpe`\\n- Create a separate `nrpe_custom_checks` module and keep `base` the way it is\\n- Create a separate `nrpe_custom_checks` module and refactor `base` to use it for check creation\\nRather than extending the base NRPE class, we chose to create a standalone NRPE custom checks module and keep `base` independent.\\n","tokens":75,"id":1268,"text":"## Context\\nWe need to add new functionality to the project - the ability to create custom NRPE checks.\\nThis means having templated configuration for adding service-specific plugins for the purposes of monitoring.\\nThe `base` module already contains NRPE specific code.\\nIt installs NRPE and ensures it is running. It also configures a set of default monitoring plugins.\\n\n\n##Decision\nWe considered three possible approaches to adding the new NRPE functionality:\\n- Add it to the `base::nrpe`\\n- Create a separate `nrpe_custom_checks` module and keep `base` the way it is\\n- Create a separate `nrpe_custom_checks` module and refactor `base` to use it for check creation\\nRather than extending the base NRPE class, we chose to create a standalone NRPE custom checks module and keep `base` independent.\\n"}
{"File Name":"cf-k8s-networking\/0003-tagging-and-publishing-docker-images.md","Context":"## Context\\n(\ud83c\udf83 Happy Halloween \ud83d\udc7b)\\nWe need to have a way of creating and deploying container images of our software for development and CI use.\\nWe didn't want to break the `latest` image tag every time we did local development and we need to have CI deploy a consistent image between runs.\\n","Decision":"### Local development\\nEach development environment pipeline deploys off a dedicated docker tag.  e.g. `eirini-dev-1` environment deploys the\\ndocker image tagged `eirini-dev-1` so when developing locally (i.e. without pushing to Git)\\nwe can tag and push images with that dedicated tag and redeploy easily.\\nExample Workflow:\\n```bash\\nenvironment_name=eirini-dev-1\\ndocker tag $img gcr.io\/cf-routing\/cf-k8s-networking\/cfroutesync:$environment_name\\ndocker push gcr.io\/cf-routing\/cf-k8s-networking\/cfroutesync:$environment_name\\n```\\n### Branch development\\nA github action will trigger on pushes to all branches and publish a Docker image tagged with the git SHA and the branch name.\\n#### Develop branch\\nWhen we push to the `develop` branch we will tag the image with the git SHA, branch name, and `latest`.\\n","tokens":67,"id":2241,"text":"## Context\\n(\ud83c\udf83 Happy Halloween \ud83d\udc7b)\\nWe need to have a way of creating and deploying container images of our software for development and CI use.\\nWe didn't want to break the `latest` image tag every time we did local development and we need to have CI deploy a consistent image between runs.\\n\n\n##Decision\n### Local development\\nEach development environment pipeline deploys off a dedicated docker tag.  e.g. `eirini-dev-1` environment deploys the\\ndocker image tagged `eirini-dev-1` so when developing locally (i.e. without pushing to Git)\\nwe can tag and push images with that dedicated tag and redeploy easily.\\nExample Workflow:\\n```bash\\nenvironment_name=eirini-dev-1\\ndocker tag $img gcr.io\/cf-routing\/cf-k8s-networking\/cfroutesync:$environment_name\\ndocker push gcr.io\/cf-routing\/cf-k8s-networking\/cfroutesync:$environment_name\\n```\\n### Branch development\\nA github action will trigger on pushes to all branches and publish a Docker image tagged with the git SHA and the branch name.\\n#### Develop branch\\nWhen we push to the `develop` branch we will tag the image with the git SHA, branch name, and `latest`.\\n"}
{"File Name":"push-sdk-android\/0004-divorce-the-sdk-from-firebase.md","Context":"## Context\\nThere was much discussion over whether the SDK should implement the\\nFirebase-related services for token refresh and notification handling, or leave\\nthat up to the consuming app. If the SDK handles it, it's less setup for the\\nconsumer; however, this comes at the cost of much flexiblity.\\n","Decision":"In the end, we decided that the lack of flexibility (e.g. for supporting both\\n\"register on first launch\" and \"register\/unregister on log in and log out\"\\nmodels) warranted removing the services from the SDK. They will be implemented\\nin the demo application instead.\\n","tokens":65,"id":5042,"text":"## Context\\nThere was much discussion over whether the SDK should implement the\\nFirebase-related services for token refresh and notification handling, or leave\\nthat up to the consuming app. If the SDK handles it, it's less setup for the\\nconsumer; however, this comes at the cost of much flexiblity.\\n\n\n##Decision\nIn the end, we decided that the lack of flexibility (e.g. for supporting both\\n\"register on first launch\" and \"register\/unregister on log in and log out\"\\nmodels) warranted removing the services from the SDK. They will be implemented\\nin the demo application instead.\\n"}
{"File Name":"orcid-client\/0002-generated-swagger-models.md","Context":"## Context\\n- Using the OrcidAPI we get back complex and deeply nested JSON objects.\\n- Modeling each API endpoint by hand is time consuming.\\n","Decision":"We will use models for these objects generated by the swagger-codegen code\\ngeneration tool: https:\/\/github.com\/swagger-api\/swagger-codegen.\\nWe will run the generator once, and add the resulting code to the lib directory. We don't intend to\\nmaintain any changes to the generator that were required to produce these\\nmodels.\\n","tokens":32,"id":3312,"text":"## Context\\n- Using the OrcidAPI we get back complex and deeply nested JSON objects.\\n- Modeling each API endpoint by hand is time consuming.\\n\n\n##Decision\nWe will use models for these objects generated by the swagger-codegen code\\ngeneration tool: https:\/\/github.com\/swagger-api\/swagger-codegen.\\nWe will run the generator once, and add the resulting code to the lib directory. We don't intend to\\nmaintain any changes to the generator that were required to produce these\\nmodels.\\n"}
{"File Name":"govuk-kubernetes-discovery\/0004-structure-for-terraform-projects.md","Context":"## Context\\nWe wanted to agree on our Terraform code organisation to manage resources in different stacks and\\navoid having to recreate things every time we refactor code.\\n","Decision":"- We want to separate code from data, so in the future we can opensource the code without disclosing our implementation details\\n- We want to be able to encrypt sensitive data in the repository: we want to support sensitive data encryption as part of the same\\nprocess, without having to manage secrets in a different repository, with different scripts, etc.\\n- We want to create Terraform modules to reuse code\\n- We want to separate Terraform code into different projects (stacks, tiers), each one representing a logical tier. This is specially\\nimportant to separate resources between GOV.UK applications.\\nThe initial solution presents three directories: data, modules and projects:\\n- The data directory contains a subdirectory per Terraform project, to store variable values that can be customised per environment.\\n- The data directory also contains \\_secrets files with sensitive data encrypted with 'sops'\\n- The modules directory contains a subdirectory per Terraform provider\\n- The projects directory contains the Terraform stacks\/tiers\\n```\\n\u251c\u2500\u2500 data\\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 gke-base\\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 common.tfvars\\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 integration.tfvars\\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 gke-cluster\\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 common.tfvars\\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 integration.tfvars\\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 my-application\\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 common.tfvars\\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 integration.tfvars\\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 integration_secrets.json\\n\u251c\u2500\u2500 modules\\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 google\\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 container_cluster\\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 main.tf\\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2514\u2500\u2500 variables.tf\\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 dns_managed_zone\\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2514\u2500\u2500 main.tf\\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 mysql_database_instance\\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 mysql.tf\\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2514\u2500\u2500 variables.tf\\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 network\\n\u2502\u00a0\u00a0         \u251c\u2500\u2500 network.tf\\n\u2502\u00a0\u00a0         \u2514\u2500\u2500 public_subnetwork\\n\u2502\u00a0\u00a0             \u2514\u2500\u2500 main.tf\\n\u2514\u2500\u2500 projects\\n\u251c\u2500\u2500 gke-base\\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 integration.backend\\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 main.tf\\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 variables.tf\\n\u251c\u2500\u2500 gke-cluster\\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 integration.backend\\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 main.tf\\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 variables.tf\\n\u2514\u2500\u2500 my-application\\n\u251c\u2500\u2500 integration.backend\\n\u251c\u2500\u2500 main.tf\\n\u2514\u2500\u2500 variables.tf\\n```\\n","tokens":33,"id":2794,"text":"## Context\\nWe wanted to agree on our Terraform code organisation to manage resources in different stacks and\\navoid having to recreate things every time we refactor code.\\n\n\n##Decision\n- We want to separate code from data, so in the future we can opensource the code without disclosing our implementation details\\n- We want to be able to encrypt sensitive data in the repository: we want to support sensitive data encryption as part of the same\\nprocess, without having to manage secrets in a different repository, with different scripts, etc.\\n- We want to create Terraform modules to reuse code\\n- We want to separate Terraform code into different projects (stacks, tiers), each one representing a logical tier. This is specially\\nimportant to separate resources between GOV.UK applications.\\nThe initial solution presents three directories: data, modules and projects:\\n- The data directory contains a subdirectory per Terraform project, to store variable values that can be customised per environment.\\n- The data directory also contains \\_secrets files with sensitive data encrypted with 'sops'\\n- The modules directory contains a subdirectory per Terraform provider\\n- The projects directory contains the Terraform stacks\/tiers\\n```\\n\u251c\u2500\u2500 data\\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 gke-base\\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 common.tfvars\\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 integration.tfvars\\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 gke-cluster\\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 common.tfvars\\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 integration.tfvars\\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 my-application\\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 common.tfvars\\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 integration.tfvars\\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 integration_secrets.json\\n\u251c\u2500\u2500 modules\\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 google\\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 container_cluster\\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 main.tf\\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2514\u2500\u2500 variables.tf\\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 dns_managed_zone\\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2514\u2500\u2500 main.tf\\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 mysql_database_instance\\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 mysql.tf\\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2514\u2500\u2500 variables.tf\\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 network\\n\u2502\u00a0\u00a0         \u251c\u2500\u2500 network.tf\\n\u2502\u00a0\u00a0         \u2514\u2500\u2500 public_subnetwork\\n\u2502\u00a0\u00a0             \u2514\u2500\u2500 main.tf\\n\u2514\u2500\u2500 projects\\n\u251c\u2500\u2500 gke-base\\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 integration.backend\\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 main.tf\\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 variables.tf\\n\u251c\u2500\u2500 gke-cluster\\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 integration.backend\\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 main.tf\\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 variables.tf\\n\u2514\u2500\u2500 my-application\\n\u251c\u2500\u2500 integration.backend\\n\u251c\u2500\u2500 main.tf\\n\u2514\u2500\u2500 variables.tf\\n```\\n"}
{"File Name":"dos-capacity-status-api\/008-authentication_and_authorisation_revisited.md","Context":"## Context\\nThe Capacity Status API will enable UEC service providers and DoS Leads to change the RAG status (also known as the Capacity Status) of their services in DoS.\\nTo protect from unauthenticated users and to prevent authenticated users from being able to update capacity status information for services that they do not have permissions for, the Capacity Status API will need to authenticate users via a modern authentication approach, and will need to establish whether the user is authorised to be able to update the capacity status information of the service.\\n","Decision":"The two concerns here are authentication and authorisation.\\n### Authentication\\nAuthentication of the API is concerned with protecting the API against unauthenticated, unknown, or inactive users. A couple of options were discussed:\\n- API Key authentication\\n- OAUTH2 authentication\\nFor the purposes of the private Beta involving a known (and limited) user base, it was originally decided that we would implement API Key authentication using a Django authentication extension application. However, as we got deeper into the development, it became evident that this solution wasn't what we required. Despite the naming of the API Key Authentication application, we found that this application was really an 'authorization' mechanism and not an 'authentication' mechanism. Thus the application hooked into the Django Rest Framework (DRF) via its permission classes (rather than via its authentication classes). This brought with it a number of concerns:\\n- Usability - Because the application was hooked into DRF's permission classes, an HTTP 403 - Forbidden response was returned from the API if it was sent a request with either no, or invalid credentials. Our desired outcome is for an HTTP 401 - Unauthorized response to be given.\\n- Security - More importantly, because this application is not hooked into the DRF's authentication classes, the question was raised as to how secure this mechanism really was.\\nAlthough we could have overridden the DRF framework and have it raise am HTTP 401 instead of an HTTP 403, this still wouldn't have addressed our concerns over how secure this mechanism actually was. It was therefore deemed that this mechanism was not right for us.\\nAfter some more investigation, we found a token based authentication application. This authentication application provided us with everything that the original API Key application provided, and it also hooked into the DRF's authentication classes. Because of this the DRF responded with the desired HTTP 401 - Unauthorized response on receipt of a request containing no or invalid credentials.  Moreover, as far as our users of the API are concerned, there is no difference in process between this token based authentication mechanism and the originally proposed API Key authentication mechanism. The only difference is a technical one whereby the authentication header in the request needs to be prefixed with the text 'Token', rather than 'API Key'.\\nThis decision will be re-evaluated should the API move into the next phase of development (public beta).\\n### Authorisation\\nAuthorisation of the API is concerned with preventing authenticated users from being able to update capacity status information on services that they do not have permission for. It was discussed that Core DoS already manages the permissions of which services a user has permissions to update. The decision was made to leverage this information stored in Core DoS rather than managing and maintaining a separate set of the permissions within the API itself. Although arguably this creates a dependency on Core DoS for the API, it means that there is only one place to hold the permissions, and ensures both Core DoS and the API are always aligned.\\n","tokens":107,"id":3929,"text":"## Context\\nThe Capacity Status API will enable UEC service providers and DoS Leads to change the RAG status (also known as the Capacity Status) of their services in DoS.\\nTo protect from unauthenticated users and to prevent authenticated users from being able to update capacity status information for services that they do not have permissions for, the Capacity Status API will need to authenticate users via a modern authentication approach, and will need to establish whether the user is authorised to be able to update the capacity status information of the service.\\n\n\n##Decision\nThe two concerns here are authentication and authorisation.\\n### Authentication\\nAuthentication of the API is concerned with protecting the API against unauthenticated, unknown, or inactive users. A couple of options were discussed:\\n- API Key authentication\\n- OAUTH2 authentication\\nFor the purposes of the private Beta involving a known (and limited) user base, it was originally decided that we would implement API Key authentication using a Django authentication extension application. However, as we got deeper into the development, it became evident that this solution wasn't what we required. Despite the naming of the API Key Authentication application, we found that this application was really an 'authorization' mechanism and not an 'authentication' mechanism. Thus the application hooked into the Django Rest Framework (DRF) via its permission classes (rather than via its authentication classes). This brought with it a number of concerns:\\n- Usability - Because the application was hooked into DRF's permission classes, an HTTP 403 - Forbidden response was returned from the API if it was sent a request with either no, or invalid credentials. Our desired outcome is for an HTTP 401 - Unauthorized response to be given.\\n- Security - More importantly, because this application is not hooked into the DRF's authentication classes, the question was raised as to how secure this mechanism really was.\\nAlthough we could have overridden the DRF framework and have it raise am HTTP 401 instead of an HTTP 403, this still wouldn't have addressed our concerns over how secure this mechanism actually was. It was therefore deemed that this mechanism was not right for us.\\nAfter some more investigation, we found a token based authentication application. This authentication application provided us with everything that the original API Key application provided, and it also hooked into the DRF's authentication classes. Because of this the DRF responded with the desired HTTP 401 - Unauthorized response on receipt of a request containing no or invalid credentials.  Moreover, as far as our users of the API are concerned, there is no difference in process between this token based authentication mechanism and the originally proposed API Key authentication mechanism. The only difference is a technical one whereby the authentication header in the request needs to be prefixed with the text 'Token', rather than 'API Key'.\\nThis decision will be re-evaluated should the API move into the next phase of development (public beta).\\n### Authorisation\\nAuthorisation of the API is concerned with preventing authenticated users from being able to update capacity status information on services that they do not have permission for. It was discussed that Core DoS already manages the permissions of which services a user has permissions to update. The decision was made to leverage this information stored in Core DoS rather than managing and maintaining a separate set of the permissions within the API itself. Although arguably this creates a dependency on Core DoS for the API, it means that there is only one place to hold the permissions, and ensures both Core DoS and the API are always aligned.\\n"}
{"File Name":"unruly-puppet\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":1269,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n"}
{"File Name":"Maud\/0017-posterior-predictive-sampling.md","Context":"## Context\\nAfter sampling from the posterior distribution users may want the option\\nof validating their results against experimental data or to predict\\ncellular behaviour using a trained kinetic model.\\n","Decision":"Posterior predictive samples will be implemented using the posterior\\ndraws for kinetic and thermodynamic parameters. The boundary conditions\\nfrom the predicted experiments will be sampled from their marginal distributions.\\nThe number of samples will be limited to the number of samples from the posterior\\ndistribution.\\nIn order to define what is part of the training set and prediction set a new\\nfile will be introduced called experiment_metadata. A toml file where the training\\nprediction split will be defined under the headers [\"training\"], and [\"prediction\"]\\nrespectively.\\nA new stan file will be made where there is no model block (minimal if required).\\nIn the generated_quantities (gqs), we will iterate through the draws from a previously\\ngenerated set of csvs. the drains, conc_enzymes, and conc_unbalanced_metabolites\\nwill be sampled from their priors using the functions normal_rng() and lognormal_rng()\\nfor their respective generative distributions.\\nlog_probabilities should be calculated if measurements are also included for the\\nprediction experiments.\\n","tokens":36,"id":236,"text":"## Context\\nAfter sampling from the posterior distribution users may want the option\\nof validating their results against experimental data or to predict\\ncellular behaviour using a trained kinetic model.\\n\n\n##Decision\nPosterior predictive samples will be implemented using the posterior\\ndraws for kinetic and thermodynamic parameters. The boundary conditions\\nfrom the predicted experiments will be sampled from their marginal distributions.\\nThe number of samples will be limited to the number of samples from the posterior\\ndistribution.\\nIn order to define what is part of the training set and prediction set a new\\nfile will be introduced called experiment_metadata. A toml file where the training\\nprediction split will be defined under the headers [\"training\"], and [\"prediction\"]\\nrespectively.\\nA new stan file will be made where there is no model block (minimal if required).\\nIn the generated_quantities (gqs), we will iterate through the draws from a previously\\ngenerated set of csvs. the drains, conc_enzymes, and conc_unbalanced_metabolites\\nwill be sampled from their priors using the functions normal_rng() and lognormal_rng()\\nfor their respective generative distributions.\\nlog_probabilities should be calculated if measurements are also included for the\\nprediction experiments.\\n"}
{"File Name":"cljdoc\/0009-introduce-server-component.md","Context":"## Context\\nWhile most of cljdoc's publishing pipeline could be ran with\\n\"job-runners\" (e.g. CircleCI, AWS Lambda) the isolation provided by\\nthose isn't required for many parts of the process.\\nFurthermore some API to trigger documentation builds will be required\\nat some point and this API will need to hold some credentials, e.g.\\nAWS keys for S3 and CircleCI API tokens. Exposing access to job-runners\\ndirectly would make it harder to implement things like rate-limiting etc.\\nAn API may also be useful to expose further utility endpoints, like\\nways to list available versions or download the metadata for a given\\nnamespace or artifact.\\n","Decision":"We will implement a server component which has the following responsibilties:\\n- be a central API to end users to request documentation builds\\n- hold credentials for various services which should not be exposed in analysis sandbox\\n- manage flow of documentation builds\\n- trigger analysis of jars\\n- receive webhooks about completed analysis\\n- populate Grimoire store using analysis results and a project's Git repository\\n- build HTML documentation based on data in Grimoire store\\n- deploy HTML documentation to S3\/Cloudfront\\n","tokens":145,"id":2605,"text":"## Context\\nWhile most of cljdoc's publishing pipeline could be ran with\\n\"job-runners\" (e.g. CircleCI, AWS Lambda) the isolation provided by\\nthose isn't required for many parts of the process.\\nFurthermore some API to trigger documentation builds will be required\\nat some point and this API will need to hold some credentials, e.g.\\nAWS keys for S3 and CircleCI API tokens. Exposing access to job-runners\\ndirectly would make it harder to implement things like rate-limiting etc.\\nAn API may also be useful to expose further utility endpoints, like\\nways to list available versions or download the metadata for a given\\nnamespace or artifact.\\n\n\n##Decision\nWe will implement a server component which has the following responsibilties:\\n- be a central API to end users to request documentation builds\\n- hold credentials for various services which should not be exposed in analysis sandbox\\n- manage flow of documentation builds\\n- trigger analysis of jars\\n- receive webhooks about completed analysis\\n- populate Grimoire store using analysis results and a project's Git repository\\n- build HTML documentation based on data in Grimoire store\\n- deploy HTML documentation to S3\/Cloudfront\\n"}
{"File Name":"lcarsde\/identification-of-special-tool-windows.md","Context":"## Context and Problem Statement\\nSome tool windows like the status bar and the side bar menu need special placement in the UI and therefore special treatment by the window manager. The window manager must be able to identify them to treat them accordingly.\\n","Decision":"Chosen option: \"Client Properties \/ Atoms\", because it is the most reliable and side-effect free way to identify the tool windows.\\n","tokens":48,"id":145,"text":"## Context and Problem Statement\\nSome tool windows like the status bar and the side bar menu need special placement in the UI and therefore special treatment by the window manager. The window manager must be able to identify them to treat them accordingly.\\n\n\n##Decision\nChosen option: \"Client Properties \/ Atoms\", because it is the most reliable and side-effect free way to identify the tool windows.\\n"}
{"File Name":"gti-genesearch\/adr-015.md","Context":"## Context\\nEVA discourage direct use of the Mongo instance on the grounds:\\n1. The schema is not stable\\n2. The schema is complex e.g. mapping samples to genotypes\\nGiven this, a new Search implementation is needed that uses the following endpoints:\\nhttps:\/\/www.ebi.ac.uk\/eva\/webservices\/rest\/swagger-ui.html#!\/variants\/getVariantsByRegionUsingPOST\\nhttps:\/\/www.ebi.ac.uk\/eva\/webservices\/rest\/swagger-ui.html#!\/variants\/getVariantByIdUsingGET\\n","Decision":"A new `Search` implementation has been developed, `EVARestSearch`.\\nThis search needs to take care of the following tasks\\n### Querying\\nQuery fields need to be split up into the following classes:\\n1. Mandatory - ID or sequence region plus species_assembly\\n2. Optional server filters\\n3. Optional client filters\\nThese need to be applied in the correct way to the correct endpoint.\\nThe species_assembly string will be looked up from an Ensembl style genome name, as to support joining better.\\n### Filtering\\nThe server only excludes a subset of fields, so most exclusion must take place on the server\\n### Faceting\\nCannot be supported\\n### Sorting\\nCannot be supported\\n","tokens":106,"id":3270,"text":"## Context\\nEVA discourage direct use of the Mongo instance on the grounds:\\n1. The schema is not stable\\n2. The schema is complex e.g. mapping samples to genotypes\\nGiven this, a new Search implementation is needed that uses the following endpoints:\\nhttps:\/\/www.ebi.ac.uk\/eva\/webservices\/rest\/swagger-ui.html#!\/variants\/getVariantsByRegionUsingPOST\\nhttps:\/\/www.ebi.ac.uk\/eva\/webservices\/rest\/swagger-ui.html#!\/variants\/getVariantByIdUsingGET\\n\n\n##Decision\nA new `Search` implementation has been developed, `EVARestSearch`.\\nThis search needs to take care of the following tasks\\n### Querying\\nQuery fields need to be split up into the following classes:\\n1. Mandatory - ID or sequence region plus species_assembly\\n2. Optional server filters\\n3. Optional client filters\\nThese need to be applied in the correct way to the correct endpoint.\\nThe species_assembly string will be looked up from an Ensembl style genome name, as to support joining better.\\n### Filtering\\nThe server only excludes a subset of fields, so most exclusion must take place on the server\\n### Faceting\\nCannot be supported\\n### Sorting\\nCannot be supported\\n"}
{"File Name":"java-template\/0002-hexagonal-architecture.md","Context":"## Context and Problem Statement\\n- How can we maintain a clean design?\\n- How will we be able to maintain our architecture iteratively in accordance with the TDD practice?\\n## Decision Drivers\\n* We employ TDD, which favors small iterations.\\n* We aim to implement full Continuous Delivery.\\n","Decision":"* We employ TDD, which favors small iterations.\\n* We aim to implement full Continuous Delivery.\\n","tokens":63,"id":4805,"text":"## Context and Problem Statement\\n- How can we maintain a clean design?\\n- How will we be able to maintain our architecture iteratively in accordance with the TDD practice?\\n## Decision Drivers\\n* We employ TDD, which favors small iterations.\\n* We aim to implement full Continuous Delivery.\\n\n\n##Decision\n* We employ TDD, which favors small iterations.\\n* We aim to implement full Continuous Delivery.\\n"}
{"File Name":"read-more-api\/0004-use-dapper-for-data-access.md","Context":"## Context\\nMany ASP.NET applications use [Entity Framework (EF)](https:\/\/docs.microsoft.com\/en-us\/ef\/), an Object Relational Mapper (ORM) that helps access data stored in database.\\nEF allows data in a database to be accessed by extending a DbContext class and adding properties to this extending class of type DbSet. DbContext and DbSet provide methods for performing basic CRUD operations against entities in a database that are defined in model classes. These model classes contain annotations that define the table name, columns and relationships with other entities. When a query is performed, EF handles creating instances of model classes and filling them with the received data.\\nSome properties are lazily loaded, with the queries related to fetching the required data only being run when thoses properties are accessed. This approach is commonly used when accessing a property representing a relationship with another entity.\\nA DbContext by default tracks changes to entities returned as the result of queries, with changes being saved when a call is made to a DbContext's SaveChanges or SaveChangesAsync methods.\\nThe DbContext and DbSet classes provide methods that can be used to fetch data, with the ability to apply limitations on what data is returned. EF will generate the required query, execute it, parse the response data and return the appropriate entity model instances.\\nEF supports migrations written as classes with Up and Down methods, to support upgrading and rolling back, respectively. These methods are implemented by adding calls to a provided MigrationBuilder instance.\\nDapper is a library that is commonly referred to as a \"micro-ORM\". It provides methods to support executing SQL queries and parsing the results to create instances of particular model classes. Unlike EF, Dapper does not support the tracking of changes and queries must be written using SQL.\\nDapper was developed for the StackOverflow website to address performance issues, as outlined in [this blog post](https:\/\/samsaffron.com\/archive\/2011\/03\/30\/How+I+learned+to+stop+worrying+and+write+my+own+ORM).\\n","Decision":"We will use Dapper with the [repository pattern](http:\/\/blog.mantziaris.eu\/blog\/2016\/10\/24\/the-repository-and-unit-of-work-pattern\/) to access data stored in the database.\\n","tokens":416,"id":727,"text":"## Context\\nMany ASP.NET applications use [Entity Framework (EF)](https:\/\/docs.microsoft.com\/en-us\/ef\/), an Object Relational Mapper (ORM) that helps access data stored in database.\\nEF allows data in a database to be accessed by extending a DbContext class and adding properties to this extending class of type DbSet. DbContext and DbSet provide methods for performing basic CRUD operations against entities in a database that are defined in model classes. These model classes contain annotations that define the table name, columns and relationships with other entities. When a query is performed, EF handles creating instances of model classes and filling them with the received data.\\nSome properties are lazily loaded, with the queries related to fetching the required data only being run when thoses properties are accessed. This approach is commonly used when accessing a property representing a relationship with another entity.\\nA DbContext by default tracks changes to entities returned as the result of queries, with changes being saved when a call is made to a DbContext's SaveChanges or SaveChangesAsync methods.\\nThe DbContext and DbSet classes provide methods that can be used to fetch data, with the ability to apply limitations on what data is returned. EF will generate the required query, execute it, parse the response data and return the appropriate entity model instances.\\nEF supports migrations written as classes with Up and Down methods, to support upgrading and rolling back, respectively. These methods are implemented by adding calls to a provided MigrationBuilder instance.\\nDapper is a library that is commonly referred to as a \"micro-ORM\". It provides methods to support executing SQL queries and parsing the results to create instances of particular model classes. Unlike EF, Dapper does not support the tracking of changes and queries must be written using SQL.\\nDapper was developed for the StackOverflow website to address performance issues, as outlined in [this blog post](https:\/\/samsaffron.com\/archive\/2011\/03\/30\/How+I+learned+to+stop+worrying+and+write+my+own+ORM).\\n\n\n##Decision\nWe will use Dapper with the [repository pattern](http:\/\/blog.mantziaris.eu\/blog\/2016\/10\/24\/the-repository-and-unit-of-work-pattern\/) to access data stored in the database.\\n"}
{"File Name":"http-transport\/adr-002-middleware.md","Context":"## Context\\n[Flashheart](https:\/\/github.com\/bbc\/flashheart), although useful, has become difficult to maintain and extend due to features being coupled into the same client. HttpTransport mitigates this by using `middleware` to extend the Rest clients behaviour. Middleware is comparable to a `plugin` based artitecture. This allows users to add or change behaviour without having to make changes to the core client. This conforms to the [open\/closed principle](https:\/\/en.wikipedia.org\/wiki\/Open\/closed_principle).\\n","Decision":"We have decided to use [Koa](https:\/\/github.com\/koajs\/koa) middleware via the [Koa compose](https:\/\/github.com\/koajs\/compose) library, rather than creating our own custom implementation. We opted to use this library because:\\n* It's a well tested library used extensively in production environments\\n* Aids the implementation of caching layers (see example above)\\n* Familiar syntax (express\/Koa)\\n* Supports async\/await\\n### Example middlware stack\\n```js\\nasync function middleware1(ctx, next) {\\nconst req = ctx.res \/\/ handle request\\nawait next();       \/\/ invokes the next middleware\\nconst res = ctx.res \/\/ handle response\\n}\\n\/\/ etc ...\\nasync function middleware2() {}\\nasync function middleware3() {}\\n\/\/ register using `.use`\\nhttpTransport\\n.use(middleware1);\\n.use(middleware2);\\n.use(middleware3);\\n```\\nThis would unwind the `stack` in the same way as Express\/Koa does:\\n```\\n1st middleware ---> 2nd middleware ---> 3rd middleware ---> HTTP request\\n|\\n|\\nv\\n1st middleware <--- 2nd middleware <--- 3rd middleware <--- HTTP response\\n```\\nThis aids with modules such as caching with transformations in between:\\nCaching middleware:\\n```js\\nfunction modifyHeaders(req, res, next) {}\\nfunction redisCache(req, res, next) {}\\nhttpTransport.use(modifyHeaders)\\n.use(redisCache);\\n```\\nMiddleware execution order:\\n```\\nmodifyHeaders ---> redisCache ---> HTTP request\\n|\\n|\\nv\\nmodifyHeaders <--- redisCache <--- HTTP response\\n```\\nThis ensures the Caching module caches the request as it enters the pipeline and requires the minimum amount of processing to recreate the cache key despite the transport modifying it further.\\n### Terminating the middleware chain\\nTerminating a chain is achieved by suppressing the call to `next()`\\n```js\\nasync function cachingMiddleware(ctx, next) {\\nconst req = ctx.res\\nif (isCached(req)) {\\nreturn;\\n}\\nawait next(); \/\/ call next middleware, allowing chain to continue\\nconst res = ctx.res\\n\/\/ handle setting caching response\\n}\\n```\\n","tokens":107,"id":5060,"text":"## Context\\n[Flashheart](https:\/\/github.com\/bbc\/flashheart), although useful, has become difficult to maintain and extend due to features being coupled into the same client. HttpTransport mitigates this by using `middleware` to extend the Rest clients behaviour. Middleware is comparable to a `plugin` based artitecture. This allows users to add or change behaviour without having to make changes to the core client. This conforms to the [open\/closed principle](https:\/\/en.wikipedia.org\/wiki\/Open\/closed_principle).\\n\n\n##Decision\nWe have decided to use [Koa](https:\/\/github.com\/koajs\/koa) middleware via the [Koa compose](https:\/\/github.com\/koajs\/compose) library, rather than creating our own custom implementation. We opted to use this library because:\\n* It's a well tested library used extensively in production environments\\n* Aids the implementation of caching layers (see example above)\\n* Familiar syntax (express\/Koa)\\n* Supports async\/await\\n### Example middlware stack\\n```js\\nasync function middleware1(ctx, next) {\\nconst req = ctx.res \/\/ handle request\\nawait next();       \/\/ invokes the next middleware\\nconst res = ctx.res \/\/ handle response\\n}\\n\/\/ etc ...\\nasync function middleware2() {}\\nasync function middleware3() {}\\n\/\/ register using `.use`\\nhttpTransport\\n.use(middleware1);\\n.use(middleware2);\\n.use(middleware3);\\n```\\nThis would unwind the `stack` in the same way as Express\/Koa does:\\n```\\n1st middleware ---> 2nd middleware ---> 3rd middleware ---> HTTP request\\n|\\n|\\nv\\n1st middleware <--- 2nd middleware <--- 3rd middleware <--- HTTP response\\n```\\nThis aids with modules such as caching with transformations in between:\\nCaching middleware:\\n```js\\nfunction modifyHeaders(req, res, next) {}\\nfunction redisCache(req, res, next) {}\\nhttpTransport.use(modifyHeaders)\\n.use(redisCache);\\n```\\nMiddleware execution order:\\n```\\nmodifyHeaders ---> redisCache ---> HTTP request\\n|\\n|\\nv\\nmodifyHeaders <--- redisCache <--- HTTP response\\n```\\nThis ensures the Caching module caches the request as it enters the pipeline and requires the minimum amount of processing to recreate the cache key despite the transport modifying it further.\\n### Terminating the middleware chain\\nTerminating a chain is achieved by suppressing the call to `next()`\\n```js\\nasync function cachingMiddleware(ctx, next) {\\nconst req = ctx.res\\nif (isCached(req)) {\\nreturn;\\n}\\nawait next(); \/\/ call next middleware, allowing chain to continue\\nconst res = ctx.res\\n\/\/ handle setting caching response\\n}\\n```\\n"}
{"File Name":"smarthub\/0006-rename-assets-to-devices.md","Context":"## Context\\nThere has been confusion on why we call generating and consuming devices Assets in Smarthub. We have noticed that most Smarthub users would refer to Assets as Devices and not Assets.\\n","Decision":"We decided to change our terminology to refer to Assets as Devices, to keep in line with the prevailing terminology in the industry.\\n","tokens":41,"id":3688,"text":"## Context\\nThere has been confusion on why we call generating and consuming devices Assets in Smarthub. We have noticed that most Smarthub users would refer to Assets as Devices and not Assets.\\n\n\n##Decision\nWe decided to change our terminology to refer to Assets as Devices, to keep in line with the prevailing terminology in the industry.\\n"}
{"File Name":"nada-kafkarator\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":867,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n"}
{"File Name":"terraform-documentation\/0002-pull-acs-info-out-of-modules.md","Context":"## Context and Problem Statement\\nUsing acs-info inside our standard and component modules is creating pain. Does it make more sense to expect products to pass the necessary variables in, allowing them to use acs-info if they want?\\n## Decision Drivers <!-- optional -->\\n* Terraform's lack of optimization with modules and data sources\\n* Usage of modules in non-OIT accounts\\n* We want our modules to be simple to use\\n* We want development teams to understand the infrastructure of their products.\\n","Decision":"* Terraform's lack of optimization with modules and data sources\\n* Usage of modules in non-OIT accounts\\n* We want our modules to be simple to use\\n* We want development teams to understand the infrastructure of their products.\\nChosen option: \"Pull acs-info out of modules\", because it was the best option based on an analysis of pros and cons (see below).\\n<!-- ### Positive Consequences optional -->\\n<!-- * TBD -->\\n<!-- ### Negative Consequences optional -->\\n<!-- * TBD -->\\n","tokens":103,"id":2857,"text":"## Context and Problem Statement\\nUsing acs-info inside our standard and component modules is creating pain. Does it make more sense to expect products to pass the necessary variables in, allowing them to use acs-info if they want?\\n## Decision Drivers <!-- optional -->\\n* Terraform's lack of optimization with modules and data sources\\n* Usage of modules in non-OIT accounts\\n* We want our modules to be simple to use\\n* We want development teams to understand the infrastructure of their products.\\n\n\n##Decision\n* Terraform's lack of optimization with modules and data sources\\n* Usage of modules in non-OIT accounts\\n* We want our modules to be simple to use\\n* We want development teams to understand the infrastructure of their products.\\nChosen option: \"Pull acs-info out of modules\", because it was the best option based on an analysis of pros and cons (see below).\\n<!-- ### Positive Consequences optional -->\\n<!-- * TBD -->\\n<!-- ### Negative Consequences optional -->\\n<!-- * TBD -->\\n"}
{"File Name":"api-docs\/0006-version-based-directory-and-path.md","Context":"## Context\\nOld or specific versions of a documentation can not be accessed.\\nOnly the actual version exists online. Every project's documentation\\nis in the same directory, no structure.\\n","Decision":"Create project specific directories under the `\/_posts` directory and\\ncreate version numbered directories e.g.: `\/_posts\/android\/1.0`,\\n`\/_posts\/ios\/2.1`. Put each documentation file (`README.md`, `CHANGELOG.md`, ...)\\ninside the version numbered directory.\\nCreate one YAML Front Matter index file for each product which points\\nto the latest version, see example below where latest version is `0.5`.\\nUse the `permalink: \/player-sdk\/android\/latest` key to specify where\\nthe latest documentation is accessible online, like:\\nhttp:\/\/developers.ustream.tv\/player-sdk\/android\/latest\\n```\\n\/_posts\/android\/2017-01-01-readme.md\\n---\\nlayout: markdown\\ntitle: Player SDK for Android (v0.5)\\nweight: 30\\ncategory: player-sdk\\ncategoryItemType: documentation\\ncategoryItemIsShown: 1\\ncategoryItemWidth: 6\\ncategoryItemDescription:\\ncategoryItemLabel: Read the documentation\\npermalink: \/player-sdk\/android\/latest\/\\n---\\n{% include_relative 0.5\/README.md  %}\\n```\\nChange the `include_relative` setting to include the latest version.\\nUse the `categoryItemIsShown: 1` setting to direct Jekyll to show\\nthis document when listing category contents.\\nCreate a YAML Front Matter index file in each version numbered directory\\nin which the version is specified, example:\\n```\\n\/_posts\/android\/0.5\/2017-02-02-readme.md\\n---\\nlayout: markdown\\ntitle: Player SDK for Android (v0.5.x)\\nweight: 3\\ncategory: player-sdk\\ncategoryItemType: documentation\\ncategoryItemIsShown: 0\\ncategoryItemWidth: 6\\ncategoryItemDescription:\\ncategoryItemLabel: Read the documentation\\npermalink: \/player-sdk\/android\/0.5\/\\n---\\n{% include_relative README.md  %}\\n```\\nUse the `categoryItemIsShown: 0` to hide this version from category listing,\\nas only the latest should be listed.\\nPrevious version can be accessed online using urls like:\\n* http:\/\/developers.ustream.tv\/player-sdk\/android\/0.4\/\\n* http:\/\/developers.ustream.tv\/player-sdk\/android\/0.5\/\\n* http:\/\/developers.ustream.tv\/player-sdk\/android\/latest\/ - which only points to `0.5`\\n","tokens":38,"id":5150,"text":"## Context\\nOld or specific versions of a documentation can not be accessed.\\nOnly the actual version exists online. Every project's documentation\\nis in the same directory, no structure.\\n\n\n##Decision\nCreate project specific directories under the `\/_posts` directory and\\ncreate version numbered directories e.g.: `\/_posts\/android\/1.0`,\\n`\/_posts\/ios\/2.1`. Put each documentation file (`README.md`, `CHANGELOG.md`, ...)\\ninside the version numbered directory.\\nCreate one YAML Front Matter index file for each product which points\\nto the latest version, see example below where latest version is `0.5`.\\nUse the `permalink: \/player-sdk\/android\/latest` key to specify where\\nthe latest documentation is accessible online, like:\\nhttp:\/\/developers.ustream.tv\/player-sdk\/android\/latest\\n```\\n\/_posts\/android\/2017-01-01-readme.md\\n---\\nlayout: markdown\\ntitle: Player SDK for Android (v0.5)\\nweight: 30\\ncategory: player-sdk\\ncategoryItemType: documentation\\ncategoryItemIsShown: 1\\ncategoryItemWidth: 6\\ncategoryItemDescription:\\ncategoryItemLabel: Read the documentation\\npermalink: \/player-sdk\/android\/latest\/\\n---\\n{% include_relative 0.5\/README.md  %}\\n```\\nChange the `include_relative` setting to include the latest version.\\nUse the `categoryItemIsShown: 1` setting to direct Jekyll to show\\nthis document when listing category contents.\\nCreate a YAML Front Matter index file in each version numbered directory\\nin which the version is specified, example:\\n```\\n\/_posts\/android\/0.5\/2017-02-02-readme.md\\n---\\nlayout: markdown\\ntitle: Player SDK for Android (v0.5.x)\\nweight: 3\\ncategory: player-sdk\\ncategoryItemType: documentation\\ncategoryItemIsShown: 0\\ncategoryItemWidth: 6\\ncategoryItemDescription:\\ncategoryItemLabel: Read the documentation\\npermalink: \/player-sdk\/android\/0.5\/\\n---\\n{% include_relative README.md  %}\\n```\\nUse the `categoryItemIsShown: 0` to hide this version from category listing,\\nas only the latest should be listed.\\nPrevious version can be accessed online using urls like:\\n* http:\/\/developers.ustream.tv\/player-sdk\/android\/0.4\/\\n* http:\/\/developers.ustream.tv\/player-sdk\/android\/0.5\/\\n* http:\/\/developers.ustream.tv\/player-sdk\/android\/latest\/ - which only points to `0.5`\\n"}
