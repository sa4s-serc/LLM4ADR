{"File Name":"beis-report-official-development-assistance\/0023-use-docker-hub-in-deployments.md","Context":"## Context\\nOur CI\/CD pipeline uses containers (Docker) as does our hosting platform (GOVUK\\nPaaS), we need a way to store built images from our pipeline so our hosting\\nplatform can access and deploy them.\\nDocker hub is one solution offered by the maker of Docker itself.\\n","Decision":"Use [Docker hub](https:\/\/hub.docker.com\/) to store built deployment container\\nimages to facilitate continuous delivery.\\nHost the built container images on the dxw Docker hub account.\\n","tokens":65,"id":2392}
{"File Name":"radiant-mlhub\/0003-will-not-implement-get-items.md","Context":"## Context\\nThe Radiant MLHub API implements the `\/items` endpoint as described in the [STAC API - Features](https:\/\/github.com\/radiantearth\/stac-api-spec\/tree\/master\/ogcapi-features)\\ndocumentation for retrieving the STAC Items associated with a given Collection. Since this is a paginated endpoint with an opaque next token,\\npages of items must be retrieved sequentially. For very large datasets and collections, this means that retrieving all items in a collection\\nmay require hundreds or thousands of API requests and can be very slow. Additionally, the spec does not provide a mechanism for determining the\\ntotal number of items in a collection, which precludes us from showing overall progress when looping over or retrieving items.\\n","Decision":"To avoid a confusing user experience when working with Items, and to avoid inadvertently swamping the API with requests, we will not provide\\na method in either the low-level client or on the `Collection` classes to loop over the items in a collection. Preliminary work had adapted\\nthe [`Collection.get_items`](https:\/\/pystac.readthedocs.io\/en\/latest\/api.html#pystac.Catalog.get_items) method to make paginated requests to\\nthe `\/items` endpoint. Instead, this method will raise a `NotImplementedError` to indicate that this feature is not available.\\n*Work is planned to add an endpoint to the Radiant MLHub API to enable downloading a single archive containing all items associated with a\\nCollection. Support for this endpoint in the Python client may be the subject of a separate ADR.*\\n","tokens":153,"id":703}
{"File Name":"codecharta\/2017-01-02-ADR_5_pick_analysis_cli_library.md","Context":"# Context\\nAs noted in [ADR-4]({% post_url adr\/2017-01-02-ADR_4_decide_analysis_architecture %}) CodeCharta will use a pipes and filters architecture.\\nBut it has not been defined how the user will interact with the analysis.\\n# Status\\naccepted\\n# Decision\\nCodeCharta Analysis will be a set of command-line tools that use [PicoCli](https:\/\/picocli.info\/). PicoCli is small, powerful, regularly updated and works great in combination with Kotlin.\\n# Consequences\\n-   A cli is not as intuitive as a well-structured graphical user interface would have been.\\n","Decision":"CodeCharta Analysis will be a set of command-line tools that use [PicoCli](https:\/\/picocli.info\/). PicoCli is small, powerful, regularly updated and works great in combination with Kotlin.\\n# Consequences\\n-   A cli is not as intuitive as a well-structured graphical user interface would have been.\\n","tokens":141,"id":1663}
{"File Name":"iampeterbanjo.com\/0004-yarn-npm-pnpm-and-docker.md","Context":"## Context\\nI started off using `yarn` workspaces because it was convenient for running scripts inside packages using `--cwd`. But it has no `audit fix` and deployments started failing because of missing packages.\\n```shell\\n2019-08-09T07:25:06+01:00 code: 'MODULE_NOT_FOUND',\\n2019-08-09T07:25:06+01:00 at Module.require (internal\/modules\/cjs\/loader.js:683:19) {\\n2019-08-09T07:25:06+01:00 at Module.load (internal\/modules\/cjs\/loader.js:643:32)\\n```\\nI tried [changing node engines][node-issue] which failed. I switched to `pnpm` because it had a reputation for solving dependency issues but I ran into problems with the way Hapi and its dependencies get [dynamically imported][hapi-issue].\\nIn order to use `pnpm` I had to use Docker deployments because [Clever cloud][clever-cloud] only support `npm` and `yarn` package managers. I could not get `pnpm` to [work with Docker][docker-issue] (recursive installs kept failing) but `npm` worked fine. I decided to keep Docker for learning and portablity purposes.\\n","Decision":"In the context of deployments failing because of missing packages. And facing the concern of wanting to have this projects' dependencies reliably installed in production, I've adopted `npm` as the default package manager. I accept that I will have to write more scripts to maintain the monorepo structure. Docker is no longer necessary but is convenient for portability and learning.\\n","tokens":277,"id":1592}
{"File Name":"gsp\/ADR035-aurora-postgres.md","Context":"## Context\\nOur service operator will be responsible for providing Postgres along any other\\nservices to our end users.\\nIn this case, the ask is specifically Postgres, meaning we can pick our\\nsolution for providing the instance as long as it exposes the correct APIs.\\nA reasonable set of candidates are:\\n- AWS RDS Postgres\\n- AWS RDS Aurora Postgres\\n### AWS RDS Postgres\\nA solution most comonly used across the board for databases. Is managed and\\nmaintained by AWS, provides Backups and Snapshots.\\n### AWS RDS Aurora Postgres\\nHas the benefits of AWS RDS Postgres, and some more benefits. Such as:\\n- Scalable persistance storage\\n- IAM authentication\\n- Automatic Failover with multi AZs\\n- Faster read loads\\n- Slightly cheaper\\nAlso has few downsides:\\n- Not recommended for heavy write systems (Twitter big)\\n- Slightly behind version wise\\n","Decision":"We will continue with Aurora as we we don't have any specific requirements not\\nto and can benefit from the solution.\\n","tokens":198,"id":3897}
{"File Name":"deeplearning4j\/0005-Interpreter.md","Context":"## Context\\n","Decision":"An interpreter uses the [import IR](.\/0003-Import_IR.md) and the [mapping rule IR](.\/0004-Mapping_IR.md)\\nto execute and map operations from one framework to nd4j's file format and back.\\nThis also allows execution of different frameworks via conversion in the nd4j engine.\\nA combination of the 2 allows a uniform interface to be used for the interpreter.\\n1 or more MappingRules will be used to transform 1 file format to another.\\n","tokens":3,"id":2935}
{"File Name":"Horace\/0020-use-c-mex-api.md","Context":"## Context\\nMatlab has two Mex APIs that can be used by C++ code:\\nthe [C API](https:\/\/www.mathworks.com\/help\/matlab\/cc-mx-matrix-library.html)\\nor the\\n[C++ API](https:\/\/www.mathworks.com\/help\/matlab\/cpp-mex-file-applications.html).\\nThe C++ API is relatively new and is therefore only compatible with Matlab releases\\n[R2018a and later](https:\/\/www.mathworks.com\/help\/matlab\/matlab_external\/choosing-mex-applications.html#mw_d3e64706-faf9-486f-ab58-1860c63564d8).\\nThroughout the Herbert and Horace codebases only the C API has been used.\\nThis is primarily due to the fact that Herbert and Horace pre-date the C++ API.\\nThe C++ API contains restrictions on directly accessing Matlab memory,\\nthis provided challenges when writing the Matlab object serializer in C++.\\nThe API does not allow access to the underlying memory of the Matlab objects,\\nand the C++ objects had varying sized headers.\\nThis caused problems when attempting to perform copies and casts:\\nthe underlying data needed to first be copied via the C++ API,\\nand only then could a `memcpy` be performed by the serializer.\\nThis meant there were three copies of the data present at one time,\\nwhich is not desirable for large objects (e.g. pixel data).\\n","Decision":"Herbert\/Horace will continue to use the C API for new Mex functions.\\n","tokens":294,"id":4243}
{"File Name":"klokwrk-project\/0008-testing-architecture.md","Context":"## Context\\nApplication architecture is commonly expressed as a set of guidelines (written or not) that the development team needs to follow. Although essential, architectural guidelines are rarely reviewed or\\nenforced appropriately. The typical result is that architecture degrades through time.\\nIn some cases, application use component ([ADR-0003 - CQRS and Event Sourcing (ES) for Applications](0003-cqrs-and-event-sourcing-for-applications.md)) and\\ndesign ([ADR-0004 - Hexagonal Architecture for Applications](0004-hexagonal-architecture-for-applications.md)) architectural patterns that promote architectural guidelines, but usually there is\\nnothing to verify them.\\nThe hexagonal architecture provides a well-defined placeholder for every significant application artifact. But there are also some rules regarding dependencies between those artifacts. It is not\\nallowed that each class or interface access anything that it wants. When you add additional CQRS\/ES aspects, there are even more rules to follow.\\nWe want to ensure that rules will not be broken and that developers new to the hexagonal architecture and CQRS\/ES can comfortably work with them without breaking anything. It will help if we\\nhave in place tests that verify all architectural invariants.\\n### Architectural Context\\n* System (`klokwrk-project`)\\n","Decision":"**We will use architectural testing for verifying architectural constraints.**\\nBuilding on top of the [ArchUnit](https:\/\/www.archunit.org\/) library, Klokwrk provides DSL for specifying hexagonal architecture layers for CQRS\/ES applications. There is support for several subtypes\\nof CQRS\/ES flavored hexagonal architecture corresponding to the commandside, projections, and queryside aspects.\\nFor more insight and details, take a look at [\"Behavior and architectural testing\"](..\/..\/article\/modules-and-packages\/modulesAndPackages.md#behavior-and-architectural-testing). There is also a video\\n[\"Project Klokwrk: how it helps defining software architecture and solves integration\"](https:\/\/www.youtube.com\/watch?v=35oUxjXWNYU) that, besides other things, talks about architectural testing in\\n`klokwrk-project`.\\n","tokens":265,"id":4898}
{"File Name":"island.is-glosur\/0003-css.md","Context":"## Context and Problem Statement\\nWe're building websites and web applications that share a common design system with reusable components. How do we write CSS styles in a way that is performant and safe?\\n## Decision Drivers\\n- Should be performant, with code splitting, caching and minimal runtime overhead.\\n- Needs to have easy access to our design system constants. These should optimally be shared with JS logic.\\n- Should be type-safe to catch issues when refactoring.\\n- Reusable components should be closed, not accepting arbitrary styles\/classes.\\n- We want a pattern for responsive props with atomic layout components.\\n","Decision":"- Should be performant, with code splitting, caching and minimal runtime overhead.\\n- Needs to have easy access to our design system constants. These should optimally be shared with JS logic.\\n- Should be type-safe to catch issues when refactoring.\\n- Reusable components should be closed, not accepting arbitrary styles\/classes.\\n- We want a pattern for responsive props with atomic layout components.\\nChosen option: Treat, because it combines the best of both worlds from CSS-in-JS and CSS modules.\\nWe'll create shared components that have responsive props, but are otherwise closed for modifications. Theme variables are defined in a shared library with TypeScript.\\nExample:\\n```typescript jsx\\n\/\/ Good:\\n<Box padding\"small\" \/>\\n<Box padding={{xs: 'small', md: 'medium'}} \/>\\n<Input large \/>\\n<Text preset=\"heading3\" as=\"p\" \/>\\n\/\/ Bad:\\n<Box className={customLayout} \/>\\n<Input style={{ height: 50, padding: 16 }} \/>\\n<Text className={styles.heading} \/>\\n```\\n### Positive Consequences\\n- Treat is statically extracted at build time, so it has minimal runtime.\\n- Styles load in parallel with JS, also when code splitting.\\n- Styles are written in TypeScript which gives us type safety when referring to shared variables, styles and helpers.\\n- Styles are in special files, separate from markup and components giving us clear separation with good visibility into the rendered markup.\\n- We can pull in responsive layout component patterns from [Braid], which gives us a good base to lay out components and pages.\\n### Negative Consequences\\n- We are choosing a pretty new framework, so it may 1) have bugs or issues, 2) be an obstacle for new developers or 3) be discontinued.\\n- When we're generating responsive styles at build time we need to be mindful at how many variations we allow (eg media queries, columns, whitespace), since they can easily bloat our CSS with unused styles.\\n","tokens":126,"id":4762}
{"File Name":"ReportMI-service-manual\/0003-use-github-for-code.md","Context":"## Context\\nWe will be developing new code and applications during the build of the Data\\nSubmission Service.\\nIt is good practice to use version control to allow changes to be tracked over\\ntime.\\nThe Digital Service Standard [recommends][service-manual-version-control] the\\nuse of version control for government services. It also recommends\\n[making all new source code open][service-standard-point-8].\\nCrown Commercial Service already uses Git for some systems, and has a\\n[GitHub][ccs-github] account.\\n","Decision":"We will use Git and GitHub for storing our code.\\nWe will use public repositories by default, unless there is a convincing reason\\nnot to (which must be documented in the README of the private repository).\\nWe will use the main [CCS GitHub][ccs-github] account and use their standards\\nfor user account management.\\n","tokens":112,"id":2064}
{"File Name":"openlobby-server\/0007-adopt-graphql-relay-specification.md","Context":"## Context\\nWe need to make API friendly for clients and design pagination.\\n","Decision":"We will adopt GraphQL Relay specification. It solves pagination so we don't\\nhave to reinvent a wheel. It has handy Node interface for re-fetching objects.\\nIt has a way to define inputs in mutations.\\nGraphene lib has good support for creating API following Relay specifications.\\n","tokens":16,"id":470}
{"File Name":"ELEN4010\/ADR Testing.md","Context":"## Context\\nIn order to holistically test the core functionality of the website, a combination of unit testing, end-to-end testing, and manual testing is used.\\nUnit tests are used on back-end models and database-related code in order to validate the functionality of each essential unit of the code (which, in most cases, are functions).\\nOn the front-end, various user actions are performed by automated testing software. During that process, key aspects relating to the front-end side of the website are tested.\\nHigh-level functionality is exclusively assessed and confirmed via manual user testing. This includes testing the following aspects of the website:\\n- Marker placement on maps\\n- Destinations being correctly added and drawn\\n- Trips being correctly written to and received from session storage\\n","Decision":"The testing framework chosen for automated testing is Jest. This framework is used because:\\n- It has a simple installation and configuration process for Node.js\\n- Due to its popularity as a javascript testing framework, it has a large developer-community which produces many articles, documents and forum threads (amongst many other sources of documentation and support)\\n- It has a wide variety of built-in assertion abilities (which means that there is no need for the installation of a third-party assertion library)\\nIn order to simulate in-browser user-interactions with the website, Selenium WebDriver is used. Front-end testing is performed on the https:\/\/testawaywego.azurewebsites.net website since it is the website used for development.\\nUltimately, it was decided that all automated front-end user testing will be performed using Google Chrome as the browser. The reason for this is due to the fact that Google Chrome has the highest browser market share (more than 60%) globally - meaning that a majrity of the website's users will be using Google Chrome.\\nAt multiple stages throughout the development process, manual testing on other major browsers (i.e. FireFox, Safari and Microsoft Edge) was also performed in order to ensure the cross-browser compatibility of the website. Manual testing was also used to ensure that the website is mobile-friendly.\\n","tokens":156,"id":56}
{"File Name":"google-cloud-cpp\/2018-06-18-storage-request-parameters-are-function-arguments.md","Context":"**Context**: most APIs in the GCS library have a number of optional parameters,\\nfor example, the API can use `ifMetagenerationMatch` to apply an operation only\\nif the metadata generation matches a given number. The question arose of how to\\nrepresent these parameters, as properties that we modify in the client or\\nobject, or as per-request parameters in the function used to access the API.\\nThat is, we had two proposals, one where the application would write code like\\nthis:\\n```C++\\nvoid AppCode(Bucket b, FixedArgument1 foo, FixedArgument2 bar) {\\nb.ApiName(\\nfoo, bar, storage::IfMetagenerationMatch(7), UserProject(\"my-project\"));\\n```\\nvs. code like this:\\n```C++\\nvoid AppCode(Bucket b, FixedArgument1 foo, FixedArgument2 bar) {\\n\/\/ Create a new bucket handle that applies the given optional parameters to\\n\/\/ all requests.\\nauto bucket = b.ApplyModifiers(\\nstorage::IfMetagenerationMatch(7), UserProject(\"my-project\"))\\nbucket.ApiName(foo, bar);\\n}\\n```\\n**Decision**: The parameters are passed as variadic arguments into any function\\nthat needs them. That is, all APIs look like this:\\n```C++\\nclass Bucket \/* or Object as applicable *\/ { public:\\ntemplate <typename... Parameters>\\nReturnType ApiName(\\nFixedArgument1 a1, FixedArgument2 a2,\\nParameters&&... p);\\n```\\nand are used like this:\\n```C++\\nvoid AppCode(Bucket b, FixedArgument1 foo, FixedArgument2 bar) {\\nb.ApiName(\\nfoo, bar, storage::IfMetagenerationMatch(7), UserProject(\"my-project\"));\\n```\\n**Consequences**: The advantages of this approach include:\\n- It is easier to use parameters in an API, it does not require to create a new\\nbucket or object or client handle just for changing the parameters in one\\nrequest.\\nThe downsides include:\\n- All APIs become templates, we should be careful not to create massive header\\nfiles that are slow to compile.\\n- It is harder to overload APIs.\\n- It is not clear how other optional parameters of the APIs, such as timeouts,\\nfit with this structure.\\n","Decision":"that needs them. That is, all APIs look like this:\\n```C++\\nclass Bucket \/* or Object as applicable *\/ { public:\\ntemplate <typename... Parameters>\\nReturnType ApiName(\\nFixedArgument1 a1, FixedArgument2 a2,\\nParameters&&... p);\\n```\\nand are used like this:\\n```C++\\nvoid AppCode(Bucket b, FixedArgument1 foo, FixedArgument2 bar) {\\nb.ApiName(\\nfoo, bar, storage::IfMetagenerationMatch(7), UserProject(\"my-project\"));\\n```\\n**Consequences**: The advantages of this approach include:\\n- It is easier to use parameters in an API, it does not require to create a new\\nbucket or object or client handle just for changing the parameters in one\\nrequest.\\nThe downsides include:\\n- All APIs become templates, we should be careful not to create massive header\\nfiles that are slow to compile.\\n- It is harder to overload APIs.\\n- It is not clear how other optional parameters of the APIs, such as timeouts,\\nfit with this structure.\\n","tokens":498,"id":2469}
{"File Name":"api-raml\/0002-use-raml-annotation-experimental.md","Context":"## Context\\nDownstream libraries such as API client, SDK, validators, bot-lax-adaptor rely on the schema provided by api-raml.\\nIt is difficult and error-prone to point the downstream libraries to a particular branch; in particular given this branch would need to rebuild dist\/ after checkout.\\nIt is forbidden to update in place a schema exposed by an API without versioning: the content types follow semantic versioning.\\n","Decision":"We will annotate with `@experimental` APIs that are being worked upon, and merge the into `develop` and `master` (indirectly).\\n","tokens":90,"id":2564}
{"File Name":"container\/0001-use-maven.md","Context":"## Context and Problem Statement\\nTo enable portability and reproducibility of building the container application, a buildtool is of utmost importance.\\nIt simplifies the buildprocess and should preferrably allow a minimal configuration effort for IDEs.\\n## Decision Drivers\\n* Core Concern is management of building the container, preferrably in a single command invocation\\n* Support for dependency management is optional, but highly appreciated\\n* The tool should also support deployment of the finished web application to an application server\\n* Cross Platform tooling, Automation and support for Running a Test suite are also necessary\\n","Decision":"* Core Concern is management of building the container, preferrably in a single command invocation\\n* Support for dependency management is optional, but highly appreciated\\n* The tool should also support deployment of the finished web application to an application server\\n* Cross Platform tooling, Automation and support for Running a Test suite are also necessary\\nMaven was chosen both for it's maturity and integration with established IDEs.\\nIt also was previously used to automate the container build when the container was still deployed as an OSGI application.\\nAdditionally maven dependencies are the de-facto standard distribution mechanism in the java ecosystem.\\n","tokens":120,"id":4658}
{"File Name":"linshare-mobile-android-app\/0009-downloadingrepository-to-manage-downloading-tasks.md","Context":"## Context\\nApplication delegate download task to `DownloadManager` system service and get an `unique id` for this task.\\nApplication need to store this Id to do further stuff with this task:\\n- Query status\\n- Get the completed state\\n- Get error details\\n- Cancel a download task\\n","Decision":"Creating a `DownloadingRepository` to manage downloading tasks\\n","tokens":62,"id":1645}
{"File Name":"gti-genesearch\/adr-002.md","Context":"## Context\\nAs we start pulling back data from multiple searches, we run the risk of ending up with multiple parameters (fields, targetFields, targetFields2 etc. etc.)\\nA better approach is to pass in specifications using a nested hash or similar so fields can be targeted to individual searches e.g.\\n```\\n{\\n\"genes\":[\"id\",\"name\",\"description\"],\\n\"variations\":[\"\"]\\n}\\n```\\nHowever, for simplicity, this needs to support simple field specifications too e.g.\\n```\\n[\"id\",\"name\",\"description\"]\\n```\\nor even\\n```\\n\"id\",\"name\",\"description\"\\n```\\n","Decision":"Mixing arbitrary arrays and hashes in Java is not well supported and requires casts etc. We will instead create a specific class, `org.ensembl.genesearch.QueryOutput`, capable of representing this kind of nested structure.  `build()` methods will be provided to support parsing various specifications.\\n","tokens":139,"id":3284}
{"File Name":"fxa\/0028-evaluate-playwright.md","Context":"## Context and Problem Statement\\nOur functional test suite currently has a 7% success rate when run against our stage environment after a deployment and a 44% success rate when run in CI for pull requests. These low rates are more from flaky tests and a finicky testing stack than legitimate bugs in the code being tested.\\nIn stage this rate is too low to be able to confidently move to a continuous delivery pipeline. In CI it slows down development and decreases morale.\\nBecause of our low success rate for pull requests each PR needs two runs of a relatively expensive task on average. In the last 90 days we used ~1.2M CircleCI credits for PRs. Ideally we could cut that in half.\\nWe should evaluate other testing stack options to improve reliability.\\n","Decision":"Playwright performs better than Intern in all goals. We should prefer it for new tests and begin migrating old tests when they need maintenance and on a case-by-case basis to improve our CI pass rate.\\nTo reduce our CircleCI bill and speed up CI runs for pull requests we should segment our functional tests into two categories: P1 and P2 (for lack of a better name). P1 tests for critical functionality run on every pull request. P2 tests should run periodically (daily) and send results to Slack. The difference between a P1 and P2 test suites is that a failed P1 means some \"happy path\" is broken, an S1 or S2 level bug, while P2 tests would represent S3 or S4 bugs.\\nP1 tests are the first priority for converting to Playwright.\\n","tokens":160,"id":385}
{"File Name":"core\/0004-naming-convention-images.md","Context":"## Context and Problem Statement\\nImage names are important for branding and let others identify easily a specific image they need. For example \"I want to work on computer vision project with Tensorflow, what stack and image should I use?\" Having a trusted well maintained source of images with clean naming convention can help on that.\\n","Decision":"Selected option: `ps-{application}` as it shows what our intention is: we want to provide a curated\/predictable software stack, it might be used by ODH or RHODS or others, it might use S2I or other technology. Moreover helps from pipeline creation point of view, because the length of repo name on quay can crate issues.\\n### Positive Consequences <!-- optional -->\\n* users can immediately select an image based on the application they want.\\n* using overlays we can have a variety of combination, not just for ml_framework\\n### Negative Consequences <!-- optional -->\\n* N\/A\\n<!-- markdownlint-disable-file MD013 -->\\n","tokens":64,"id":291}
{"File Name":"play-frontend-hmrc\/0009-self-publish-webjar.md","Context":"## Context and Problem Statement\\nplay-frontend-hmrc relies on a webjar for [hmrc\/hmrc-frontend](https:\/\/www.github.com\/hmrc\/hmrc-frontend)\\npublished to www.webjars.org. This has a number of drawbacks:\\n* publishing is a manual process\\n* it can take many hours to complete\\n* webjars has been known to be down and HMRC has no support arrangements with www.webjars.org\\nThe main impact of the above is an excessive lead time for making improvements in the\\nunderlying hmrc-frontend library available in production via play-frontend-hmrc.\\nBearing the above in mind, and the fact that HMRC has its own repository for open artefacts, replacing\\nBintray, should we:\\n* automate the creation of the webjars within our own deployment pipelines with no dependency\\non webjars.org\\n* publish the resulting webjars to this repository automatically?\\nNote, this decision only addresses the creation and publishing of the hmrc-frontend webjar, not the\\nwebjar for [alphagov\/govuk-frontend](https:\/\/www.github.com\/alphagov\/govuk-frontend), which is\\ncurrently a dependency for [hmrc\/play-frontend-govuk](https:\/\/www.github.com\/hmrc\/play-frontend-govuk).\\n## Decision Drivers\\n* The need to make improvements and upgrades to hmrc-frontend\\navailable in play-frontend-hmrc quickly.\\n* The increasing user base of play-frontend-hmrc, and accelerating demand for new features and\\nimprovements.\\n* The hardship, frustration and toil the current manual process is causing the team.\\n* The need to keep things simple and avoidance of creating new repositories unnecessarily due to\\nthe overhead of maintaining those repositories\\n* The testing advantages of being able to build and deploy the hmrc-frontend webjar locally\\n* Parity between the hmrc-frontend NPM package and the webjar.\\n","Decision":"* The need to make improvements and upgrades to hmrc-frontend\\navailable in play-frontend-hmrc quickly.\\n* The increasing user base of play-frontend-hmrc, and accelerating demand for new features and\\nimprovements.\\n* The hardship, frustration and toil the current manual process is causing the team.\\n* The need to keep things simple and avoidance of creating new repositories unnecessarily due to\\nthe overhead of maintaining those repositories\\n* The testing advantages of being able to build and deploy the hmrc-frontend webjar locally\\n* Parity between the hmrc-frontend NPM package and the webjar.\\nChosen option: option 2 because it solves the core issue and enables local testing without introducing\\nadditional dependencies.\\n### Existing architecture\\n<img alt=\"Existing architecture\" src=\"0009-webjars-existing.png\" width=\"450\">\\n### To be architecture\\n<img alt=\"To be architecture\" src=\"0009-webjars-tobe.png\" width=\"450\">\\n### Positive Consequences\\n* Webjars are available instantaneously after a new version of hmrc-frontend is released\\n* It is now possible to locally test changes to hmrc-frontend in conjunction with Scala microservices\\nwithout needing to publish to NPM or webjars.org first, reducing the risk that flawed components are released into\\nproduction.\\n* Lead times for making improvements to hmrc-frontend available in production are reduced.\\n* Maintaining play-frontend-hmrc is a less frustrating process.\\n* We have more control over the metadata attached to the webjars published. For example, at the moment, the webjars\\nproduced indicate webjars.org as the developer in the POM files.\\n* There are fewer external dependencies and moving parts.\\n### Negative Consequences\\n* We have an additional moving part to maintain ourselves.\\n","tokens":423,"id":570}
{"File Name":"opg-digideps\/0004-user-emails-and-roles-are-immutable.md","Context":"## Context\\nDue to the freedom given to users and admins in changing a user's email address or role, we've had issues identifying why people have certain permissions in DigiDeps. As well as being confusing, we believe this has led to security issues where a high-authority user has had their email changed in an attempt to reuse the account, thereby granting unreasonable access to the owner of the new email address.\\nThere are rarely good reasons to change a user's email address or role. Because very little in DigiDeps belongs directly to a user, these situtations can be resolved by deleting the original account and creating a new one with the correct permissions.\\n","Decision":"User email addresses and roles will henceforth be immutable: they are set when a new user is created and cannot subsequently be changed.\\nThere is one exception to this: organisation administrators will be able to switch users between team member and admin roles. This role switch is entirely internal to the organisation and doesn't affect any view\/edit permissions to clients and reports.\\n","tokens":136,"id":4409}
{"File Name":"cygnus-infra\/0004-introduce-a-pi-hole-for-local-dns.md","Context":"## Context\\n- I don't want to make the CoreDNS container in my cluster the local DNS source of truth, because my view of it is entirely for cluster DNS\\n- I don't want to expose my services to the internet _yet_ (but expect to later)\\n- I have a dedicated domain for my services\\n- I can set up TLS for \"internal\" domains via Let's Encrypt + CloudFlare DNS\\n- I need something to route internal requests to my cluster without querying external DNS resolvers, which won't know about my internal cluster\\n- I can probably do this on my UniFi setup, but it'd likely require some configuration outside of the controller software\\n- Pi-Hole introduces some other neat features\\n- I have multiple spare Raspberry Pis\\n","Decision":"- Flash Raspberry Pi OS (minimal install) on a microSD card\\n- Use that card to boot a Raspberry Pi 3B+ (`Raspberry Pi 3 Model B Rev 1.2`)\\n- Configure for SSH, use wired LAN\\n- Consider adding WLAN later, but Pi-Hole will expect a single static IP?\\n- Install and configure unattended upgrades\\n- Install Pi-Hole\\n","tokens":158,"id":3870}
{"File Name":"Maud\/0012-unbabalanced_mets-enzymes-as-priors.md","Context":"## Context\\nUnbalanced metabolites and enzyme concentrations are boundary conditions for our ODE\\nmodel. Experimental conditions are defined with respect to these values and drains,\\nwhich are already defined as priors. Therefore, our prior knowledge about the\\nmetabolic phenotype is defined as what is measured about the boundary conditions. This\\ndecision aims to shift the measurements of the enzymes and unbalanced metabolites from\\nthe likelihood evaluations to the prior information.\\nThe benefit of treating priors in this way is that we define a prior on the phenotype\\nrather than all possible phenotypes. However, boundary conditions that are unmeasured\\nare still considered using weakly informative priors (read: within biologically relevant\\nboundaries).\\n","Decision":"Unbalanced metabolites and enzyme concentrations can also be considered as prior distributions\\nrather than likelihood evaluations.\\n","tokens":148,"id":243}
{"File Name":"copilot\/0005-enable-grpc-gzip-compression-between-copilot-and-route-syncer.md","Context":"## Context\\nThe GRPC default message size of 4 mb currently causing a bottleneck between cc-route-syncer and copilot. As our message sizes increased with scale this prevents us from sending messages to copilot.\\n","Decision":"We have decided to reduce the message size by enabling GRPC's GZIP compression between cc-route-syncer and copilot.\\n","tokens":44,"id":52}
{"File Name":"arch\/0015-code-review.md","Context":"## Context\\nContext here...\\n","Decision":"\u4f7f\u7528 Github \u8fdb\u884c Code Review\\n","tokens":7,"id":2416}
{"File Name":"clone_difitalcitizenship\/0011-we-chose-a-different-cosmosdb-api.md","Context":"## Context\\nIn [ADR #9](0009-we-choose-a-cosmosdb-api.md) we decided to use the MongoDB API\\nfor CosmosDB but as soon as we implemented the first version of the DAL logic\\nwe realized that the MongoDB support in CosmosDB was flawed: a serious [bug](https:\/\/feedback.azure.com\/forums\/263030-azure-cosmos-db\/suggestions\/19361521-fix-bug-that-destroys-nodejs-mongodb-connection-po)\\nthat destroyed the connection pool every ~10s made the MongoDB API impossible\\nto use in a production environment.\\n","Decision":"Since at the time we were still in the early phase of development (around\\nAugust 2017), we decided to implement the DAL on top of the more mature\\nDocumentDB API (see [this commit](https:\/\/github.com\/teamdigitale\/digital-citizenship-functions\/commit\/c72b95ebb5ed038cdf62f43dc1adacbde9668d4e)).\\nNote that the bug has been recently fixed (October 2017), so it may be worth to\\nplan a migration of the DAL to the MongoDB API.\\n","tokens":126,"id":1181}
{"File Name":"raster-foundry\/adr-0024-metrics-scripting.md","Context":"## Context\\nWe'd like to be able to script metrics reporting to third party consumers of\\nRaster Foundry. Currently we manually assemble metrics when people ask for\\nthem. However, enough time has passed and we have consistent enough obligations\\nthat we know a few that would be scriptable. This ADR seeks to answer the\\nquestion of how we ought to script production of a particular subset of\\nmetrics, specifically, tile server requests by user.\\n","Decision":"We should start with the second option, being mindful of the potential later\\nneed to target configurable metrics sinks, but not designing for that\\nparticular problem from the outset. We\u2019ll start with tile metrics and a\\ndatabase table that holds just enough to get us the metrics we want.\\nThe reasons the second option is likely to be safer are that it avoids\\nincreasing infrastructure complexity and that the marginal cost of its lock-in\\nis lower than the marginal cost of additional vendor lock-in. We bear the\\nburden of someday migrating everything from http4s and Postgres if we need to,\\nwhile additional AWS lock-in is a cost that either makes it more difficult for\\nanyone to deploy a bare metal solution or makes the cost of paying us to do so\\nmore expensive.\\nThe second option is also more extensible, since storing this information in\\nthe database allows us to expose it via an API and to use the same information\\nfor authorization that we use elsewhere.\\nThe downside of choosing the second option is that it couples metrics updates\\nto our release cycle, while we could have avoided that coupling with the AWS\\noption by throwing every piece of information we have into a data lake. This\\ncost is acceptable given the other benefits and is in line with a strategy of\\nnot collecting data for the sake of collecting data.\\n","tokens":92,"id":1685}
{"File Name":"paas-team-manual\/ADR045-aws-waf.html.md","Context":"## Context\\nGOV.UK PaaS uses [AWS Shield Advanced](https:\/\/aws.amazon.com\/shield\/features\/#AWS_Shield_Advanced) as well as AWS WAF to protect from DDoS attacks.\\nHowever the mitigations are not automatic and we have access to the AWS DDoS Response Team\\n(DRT) who are experts in mitigating these types of attack.\\n```\\nShield Advanced detects web application layer vectors, like web request floods and\\nlow-and-slow bad bots, but does not automatically mitigate them. To mitigate web\\napplication layer vectors, you must employ AWS WAF rules or the DRT must employ the\\nrules on your behalf.\\n```\\nIn order to be functional they require access to our AWS WAF logs in order to identify what\\nthe attack is and where is is coming from, and API access to the WAF in order to apply the\\nmitigating rules.\\nTo enagage the AWS DRT team we will set up CloudWatch alarms on our WAF rules in order to trigger\\nthe [emergency engagement Lambda](https:\/\/s3.amazonaws.com\/aws-shield-lambda\/ShieldEngagementLambda.pdf)\\n","Decision":"We will grant access to the AWS DRT to read from restricted S3 buckets\\n","tokens":243,"id":188}
{"File Name":"Head-Start-TTADP\/0014-web-analytics.md","Context":"## Context\\nWe need to capture information about user behaviors and task completion on the website. We should use previously approved systems for capturing this information. This excludes services that capture and track web analytics in external systems, e.g. Google Analytics.\\n","Decision":"We will use New Relic to capture and track web analytics. New Relic provides [browser monitoring](https:\/\/docs.newrelic.com\/docs\/browser\/) that can capture essential metrics such as page views, and offers both [agent and SPA API](https:\/\/docs.newrelic.com\/docs\/browser\/new-relic-browser\/browser-agent-spa-api\/) and [APM](https:\/\/developer.newrelic.com\/collect-data\/custom-attributes) methods for capturing custom data.\\n","tokens":49,"id":1165}
{"File Name":"python-tuf\/0006-where-to-implemenent-model-serialization.md","Context":"## Context and Problem Statement\\nIn the course of implementing a class-based role metadata model we have also\\nreviewed options on how to design serialization infrastructure between wire\\nformats and the class model. In an initial attempt we have implemented\\nserialization on the metadata class (see option 1), but issues with inheritance\\nand calls for more flexibility have caused us to rethink this approach.\\n## Decision Drivers\\n* A class-based role metadata model (see ADR4) requires serialization routines\\nfrom and to wire formats\\n* TUF integrators may require custom serialization implementations for custom\\nwire formats\\n* Readability and simplicity of implementation for users and maintainers\\n* Recognizability of specification\\n","Decision":"* A class-based role metadata model (see ADR4) requires serialization routines\\nfrom and to wire formats\\n* TUF integrators may require custom serialization implementations for custom\\nwire formats\\n* Readability and simplicity of implementation for users and maintainers\\n* Recognizability of specification\\nChosen option: \"Compromise 2\", because implementing dict conversion as methods\\non a corresponding class is idiomatic and allows for well-structured code.\\nTogether with a separated serialization interface, it provides both ease of use\\nand maintenance, and full flexibility with regards to custom serialization\\nimplementations and wire formats.\\n","tokens":140,"id":4254}
{"File Name":"CardsApp\/card-operations-database-choice.md","Context":"##Context\\nWe need to choose the database for our CardOperations. The database needs to store data in persistent form but it's not required that data is stored across sessions.\\nApplication is a prototype, so it should be good for a quick setup.\\n","Decision":"We decided to go with H2 database for its ease of use, easy setup and familiarity.\\n","tokens":52,"id":4065}
{"File Name":"amf-core\/0005-expose-non-scalajs-types-in-amf-client-remote-content.md","Context":"## Context\\nTo adopt the ScalaJSTypings plugin, usages of Scala types that were not exported to ScalaJS were removed from the scala interface.\\nThe Api Designer product uses the `Content.stream` field and calls `toString()` on it. As this field is of type CharStream we hid\\nit from export.\\n","Decision":"- Rollback the interface change for the `amf.client.remote.Content` class so that the `toString()` method can be called on the `stream` field.\\n- Add the `toString()` method in `Content` that returns the content in `stream`\\n","tokens":67,"id":403}
{"File Name":"otm-docs\/containerization.md","Context":"## Context\\nIt is important to ensure for each team member the same environment configuration in which the application will operate.\\n","Decision":"Docker\\n","tokens":24,"id":3060}
{"File Name":"celestia-core\/adr-010-crypto-changes.md","Context":"## Context\\nTendermint is a cryptographic protocol that uses and composes a variety of cryptographic primitives.\\nAfter nearly 4 years of development, Tendermint has recently undergone multiple security reviews to search for vulnerabilities and to assess the the use and composition of cryptographic primitives.\\n### Hash Functions\\nTendermint uses RIPEMD160 universally as a hash function, most notably in its Merkle tree implementation.\\nRIPEMD160 was chosen because it provides the shortest fingerprint that is long enough to be considered secure (ie. birthday bound of 80-bits).\\nIt was also developed in the open academic community, unlike NSA-designed algorithms like SHA256.\\nThat said, the cryptographic community appears to unanimously agree on the security of SHA256. It has become a universal standard, especially now that SHA1 is broken, being required in TLS connections and having optimized support in hardware.\\n### Merkle Trees\\nTendermint uses a simple Merkle tree to compute digests of large structures like transaction batches\\nand even blockchain headers. The Merkle tree length prefixes byte arrays before concatenating and hashing them.\\nIt uses RIPEMD160.\\n### Addresses\\nED25519 addresses are computed using the RIPEMD160 of the Amino encoding of the public key.\\nRIPEMD160 is generally considered an outdated hash function, and is much slower\\nthan more modern functions like SHA256 or Blake2.\\n### Authenticated Encryption\\nTendermint P2P connections use authenticated encryption to provide privacy and authentication in the communications.\\nThis is done using the simple Station-to-Station protocol with the NaCL Ed25519 library.\\nWhile there have been no vulnerabilities found in the implementation, there are some concerns:\\n- NaCL uses Salsa20, a not-widely used and relatively out-dated stream cipher that has been obsoleted by ChaCha20\\n- Connections use RIPEMD160 to compute a value that is used for the encryption nonce with subtle requirements on how it's used\\n","Decision":"### Hash Functions\\nUse the first 20-bytes of the SHA256 hash instead of RIPEMD160 for everything\\n### Merkle Trees\\nTODO\\n### Addresses\\nCompute ED25519 addresses as the first 20-bytes of the SHA256 of the raw 32-byte public key\\n### Authenticated Encryption\\nMake the following changes:\\n- Use xChaCha20 instead of xSalsa20 - https:\/\/github.com\/tendermint\/tendermint\/issues\/1124\\n- Use an HKDF instead of RIPEMD160 to compute nonces - https:\/\/github.com\/tendermint\/tendermint\/issues\/1165\\n","tokens":410,"id":4110}
{"File Name":"Horace\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":4248}
{"File Name":"AdventOfCode-Java-learning-project\/0001-creational-patterns.md","Context":"## Context\\nWe want to have a common contract for the way that our specific solutions for each Day behave in the application context. For that, we will enforce the implementation of specific methods. In addition, we want to have an easy access to those specific implementations and will rely on Spring's application context.\\n","Decision":"The interface `Days.class` is part of an _Abstract Factory_ for concrete Day classes, allowing us to use polymorphism for days. The instances for the specific Days are created and provided by the Spring context upon annotating the specific days with `@Component` and making them implement the interface, so it is not a real factory - just part of it.\\nIn the `AdventOfCodeService.class` (think of it as a _Facade_ to the subsystem), a `List<Days>`, which is populated by Spring with instances of the specific Days, is used by a _factory method_, or _repository method_ if you want to name it that way in order to find the correct instance for a specific requested day. Note that `AdventOfCodeService.class` is in our case part of the creation, but has also Structural Patterns characteristics. We will cover that more deeply in a corresponding ADR section.\\n","tokens":62,"id":1756}
{"File Name":"SearchServices\/0009-message-driven-content-tracker-next-gen.md","Context":"## Context\\nThis is a second iteration of content tracking via message bus design. See [previous version](0007-message-driven-content-tracker.md).\\nNew content tracker implementation will be based on new Search Services architecture (SS v2.0 or next gen). Main context behind this decision is almost the same as for v1 - get more throughput by leveraging new Transform Service.\\n","Decision":"The decision is based on [version 1](0007-message-driven-content-tracker.md). The main differences are:\\n* Shared File Store may not be the right option as it is only available for Enterprise. Alternatively the URL to content can point to other locations. (TBC)\\n* The change in behaviour requires a major release of Search Services, most likely version 2.0.\\n* The changes in Content Repository will be available from version 6.3.\\n* The synchronous transformation APIs will remain functional until 7.0.\\nDetails of the architecture to be clarified (WIP).\\n","tokens":77,"id":5127}
{"File Name":"visit-plannr\/0003-cloudfront-distribution-means-there-is-long-lived-infrastruture.md","Context":"## Context\\nThe original intention was to be able to build the entire application stack from a single template. However, adding a [cloudfront distribution](https:\/\/docs.aws.amazon.com\/AmazonCloudFront\/latest\/DeveloperGuide\/Introduction.html) takes 15-25 minutes to complete.\\n","Decision":"To have two infrastructure templates. One with slow to provision infrastructure. A second with the fast to provision infrastructure.\\n","tokens":58,"id":1987}
{"File Name":"beis-report-official-development-assistance\/0003-use-dotenv-for-managing-environment-variables.md","Context":"## Context\\nAccessing ENV directly without a wrapper is limited and can introduce problems.\\nWe want our tooling to help us guard against missing environment variables. When `nil` is accidentally provided during the start up process it is preferable to fail fast with an explicit message. Without this `nil` can be passed down through the stack and cause strange behaviour where the code has been designed with it as a dependency. Instead of adding `nil` guards throughout the codebase for required environment variables (eg. `ENV.fetch('FOO', 'default')`, this should be managed centrally.\\nWe have previously used Figaro for this purpose but it was deprecated in 2016 https:\/\/github.com\/laserlemon\/figaro. We should only use supported gems to ensure we get support in the form of fixes and security patches.\\nWe also want to be able to stub our environment variables in our test suite. An easy example of this is when we use environment variables as a feature flag mechanism. We want to stub the value to test both scenarios without being influenced by real values being loaded. Mutating the actual ENV value (eg. ` allow(ENV).to receive(:[]).with('BOX_ID').and_return(\"1234\")`) is possible but may have unexpected consequences where more than 1 part of the same process under test uses the same variable. Figaro used to be a handy abstraction layer that we could stub eg. `allow(Figaro).to receive(:env).with(:foo).and_return('bar')`. We should then consider how we can stub environment variables.\\n","Decision":"Use DotEnv to load our environment variables.\\n","tokens":324,"id":2390}
{"File Name":"SAP-library\/cx-server-in-container.md","Context":"## Context\\nWe have a bash script (`cx-server`), which orchestrates the Docker containers used by our Cx Server.\\nUsing bash is inconvenient for Windows users, as Windows does not include bash by default.\\nThere is options for running bash on Windows, such as the Windows Subsystem for Linux, but this is not trivial to setup and requires switching Windows to Developer Mode.\\nOther options include running a virtual machine locally, or connecting to a remove Linux system, but both are not always possible and have too much overhead.\\nRecently, we added a \"companion\" Docker image which is used by `cx-server` to run scripts.\\nUnrelated, the idea was born to move `cx-server` into this image, so the remaining `cx-server` is a very thin wrapper which can also be added as a Windows compatible script file.\\n","Decision":"We move the bash script inside the `s4sdk\/cxserver-companion` Docker image.\\nThe old `cx-server` script just delegates the command to the script inside the companion container.\\nA new `cx-server.bat` script is added, doing the same for Windows.\\nWe don't use PowerShell to increase compatibility with Windows.\\n","tokens":175,"id":849}
{"File Name":"linshare-mobile-flutter-app\/0007-path-provider-library.md","Context":"## Context\\nAndroid and iOS have different folder architectures.\\nTherefore we need to provide *path folder* according to user's plateform.\\n`path-provider` library only provide *Download Folder* for Android Version.\\n","Decision":"We decided to use `path-provider` if the user is on Android, and use an other library (TBD) if the user is on iOS.\\n","tokens":45,"id":3301}
{"File Name":"super-eks\/0005-use-namespaces-grouped-by-functionality.md","Context":"## Context\\nWhen installing manifests or Helm charts you need to pick a namespace.\\nOften the choice is made in a drive by fashion, leading to inconsistent configuration.\\n","Decision":"We want to install all the shipped components into namespaces grouped by functionality.\\nFor example `external-dns` goes into the namespace `dns`.\\n`fluent-bit` goes into the namespace `logging`.\\n","tokens":34,"id":2031}
{"File Name":"edgex-docs\/0002-Array-Datatypes.md","Context":"- [Context](#context)\\n- [Decision](#decision)\\n- [Consequences](#consequences)\\n<!--te-->\\n","Decision":"- [Consequences](#consequences)\\n<!--te-->\\n### DeviceProfile extension\\nThe permitted values of the `Type` field in `PropertyValue` are extended to include:\\n\"BoolArray\", \"Uint8Array\", \"Uint16Array\", \"Uint32Array\", \"Uint64Array\", \"Int8Array\", Int16Array\", \"Int32Array\", \"Int64Array\", \"Float32Array\", \"Float64Array\"\\n### Readings\\nIn the API (v1 and v2), `Reading.Value` is a string representation of the data. If this is maintained, the representation for Array types will follow the JSON array syntax, ie `[\"value1\", \"value2\", ...]`\\n","tokens":31,"id":959}
{"File Name":"nada-kafkarator\/0007-only-for-aiven.md","Context":"## Context\\nNAV currently has Kafka running on-premise, but have plans to move everything to the cloud.\\nKafkarator will create an integration between applications and Kafka, removing some manual work and putting more configuration in code.\\nThe plan is to buy hosted Kafka from Aiven, eliminating the need for maintaining our own Kafka cluster on-premise.\\n","Decision":"Kafkarator will only deal with the future solution using Aiven, and not work for on-premise Kafka.\\n","tokens":72,"id":863}
{"File Name":"nso.aurora\/Notifier.md","Context":"## Context\\nWe need a way to notifiy customers for doing a survey after their purchases, provide them with recommendations, letting them know how their orders via email and\/or SMS.\\n","Decision":"A Notifier component is needed to send email and\/SMS to customers.\\n","tokens":38,"id":300}
{"File Name":"tendermint\/adr-071-proposer-based-timestamps.md","Context":"## Context\\nTendermint currently provides a monotonically increasing source of time known as [BFTTime](https:\/\/github.com\/tendermint\/tendermint\/blob\/master\/spec\/consensus\/bft-time.md).\\nThis mechanism for producing a source of time is reasonably simple.\\nEach correct validator adds a timestamp to each `Precommit` message it sends.\\nThe timestamp it sends is either the validator's current known Unix time or one millisecond greater than the previous block time, depending on which value is greater.\\nWhen a block is produced, the proposer chooses the block timestamp as the weighted median of the times in all of the `Precommit` messages the proposer received.\\nThe weighting is proportional to the amount of voting power, or stake, a validator has on the network.\\nThis mechanism for producing timestamps is both deterministic and byzantine fault tolerant.\\nThis current mechanism for producing timestamps has a few drawbacks.\\nValidators do not have to agree at all on how close the selected block timestamp is to their own currently known Unix time.\\nAdditionally, any amount of voting power `>1\/3` may directly control the block timestamp.\\nAs a result, it is quite possible that the timestamp is not particularly meaningful.\\nThese drawbacks present issues in the Tendermint protocol.\\nTimestamps are used by light clients to verify blocks.\\nLight clients rely on correspondence between their own currently known Unix time and the block timestamp to verify blocks they see;\\nHowever, their currently known Unix time may be greatly divergent from the block timestamp as a result of the limitations of `BFTTime`.\\nThe proposer-based timestamps specification suggests an alternative approach for producing block timestamps that remedies these issues.\\nProposer-based timestamps alter the current mechanism for producing block timestamps in two main ways:\\n1. The block proposer is amended to offer up its currently known Unix time as the timestamp for the next block instead of the `BFTTime`.\\n1. Correct validators only approve the proposed block timestamp if it is close enough to their own currently known Unix time.\\nThe result of these changes is a more meaningful timestamp that cannot be controlled by `<= 2\/3` of the validator voting power.\\nThis document outlines the necessary code changes in Tendermint to implement the corresponding [proposer-based timestamps specification](https:\/\/github.com\/tendermint\/tendermint\/tree\/master\/spec\/consensus\/proposer-based-timestamp).\\n","Decision":"Implement proposer-based timestamps and remove `BFTTime`.\\n","tokens":495,"id":1969}
{"File Name":"dalmatian-frontend\/0002-use-bullet-to-catch-nplus1-queries.md","Context":"## Context\\nIt can be easy to miss an inefficient database query during code review. These can build up and have detremental performance on the application and effect the user experience.\\n","Decision":"Add an automatic check to the test suite to ensure (through CI) that these are fixed before being deployed.\\n","tokens":37,"id":2553}
{"File Name":"cloud-sdk-js\/0017-builder-methods-multiple-calls.md","Context":"## Context\\nIn the request builder you can currently call methods like `filter` and `top` multiple times.\\nIn this document we discuss possibilities to unify this the behavior when this is done.\\n","Decision":"We decided to use option `A` and `B` depending on the methods.\\nFor the OData related options we will use option `A`.\\nFor configuration related builder methods we will use the verbs `add` or `set` to make clear if it is option `A` or `B`.\\nThe relevant builder methods (the one containing arguments) are listed in the table below including the decision on the option:\\n| class                           | methods                                                                                                                                                                              |\\n| ------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |\\n| request-builder-base            | withCustomHeader ( refactor to addCustomHeaders B)<br>withCustomQueryParameters (refactor to addCustomQueryParameters B)<br>withCustomServicePath (A rename to setCustomServicePath) |\\n| get-all-request-builder-base    | select (A) <br>orderBy (A) <br>top (A) <br>skip (A)                                                                                                                                  |\\n| get-all-request-builder-v2\/v4   | filter (A)<br>expand(v4)                                                                                                                                                             |\\n| get-by-key-request-builder-base | select (A refactoring move to base)                                                                                                                                                  |\\n| get-by-key-request-builder-v4   | expand(A)                                                                                                                                                                            |\\n| delete-request-builder-v2\/v4    | setVersionIdentifier (A)                                                                                                                                                             |\\n| update-request-builder-base     | requiredFields (refactor setRequiredFields A)<br>ignoredFields (refactor setRequiredFields A)<br>withCustomVersionIdentifier (refactor align with setVersionIdentifier A)            |\\n","tokens":41,"id":3620}
{"File Name":"simple-server\/007-robust-data-migrations.md","Context":"## Context\\nOn January 20, 2020, the team became aware that a data migration that needed to be executed along with a prior\\nproduction deployment was never run in production. In order to rectify this, the team ran the required data migration on\\nthe evening of January 20. Since the task was CPU (and perhaps memory) intensive, the task ran for ~8 hours overnight,\\nand required constant monitoring from the team to ensure that the task wasn't affecting the regular performance of the\\nSimple Server. After the task was completed, it was noted that the Simple Server _was_ impacted by the task.\\nReferences\\n* [Slack conversation](https:\/\/simpledotorg.slack.com\/archives\/CFHMC60P5\/p1579524825012500)\\nThe problems that this ADR aims to tackle are the following:\\n* A required data migration was not run after the corresponding production deployment\\n* Data migrations require constant supervision to monitor server performance and correct failures\\n* The Simple Server's performance was degraded during the execution of the data migration\\n","Decision":"### Enforcing data migration execution\\nIn order to better enforce the execution of data migrations, we will invoke them using the\\n[`data-migrate`](https:\/\/github.com\/ilyakatz\/data-migrate) gem. This will ensure that data migrations are run with the\\nsame reliability as database migrations, while still allowing database migrations to be run or rolled back independently\\nif necessary. For example, we may ship a data migration that looks like this.\\n```ruby\\nclass UpdateNilensoBPs < ActiveRecord::Migration[5.1]\\ndef up\\nBPUpdater.perform(facility: \"Nilenso\") # An associated service class that performs the data migration\\nend\\ndef down\\nRails.logger.info(\"This data migration will not be reversed. Good luck!\")\\nend\\nend\\n```\\nWe will need to take care with our file management. [See below](#consequences).\\n### Reliable hands-off execution\\nWe will use background jobs to execute data migrations in an asynchronous, atomic, and repeatable fashion. Ideally, the\\ndata migration will be split up into as small atomic parts as possible, and each part will be executed in its own\\nbackground job. For example, a task to update 1000 records will be executed using 1000 Sidekiq jobs, one for each\\nrecord.\\n```ruby\\n# good\\naffected_records.each do |record|\\nUpdateJob.perform_later(record)\\nend\\n# bad - not atomic\\nBulkUpdateJob.perform_later(affected_records)\\n# bad - not asynchronous\\nUpdater.perform_right_now(affected_records)\\n```\\nUsing atomic background jobs will have several advantages.\\n* Failed jobs will be retried automatically with sensible exponential back-off\\n* We avoid the use of very-long-running processes on the server (which could also result in very-long-running database\\nconnections)\\n* It becomes easier to monitor the progress of the migration through the Sidekiq dashboard\\n* The execution of the data migration can be easily throttled, paused, or halted\\nWe will need to take care of our Sidekiq queues. [See below](#consequences).\\n### Other beneficial traits\\n**Idempotence:** Ideally, the data migration task should be repeatable indefinitely without changing the end result.\\nIdempotent tasks will ensure that even if we have to abandon the migration in a half-finished state (eg. Redis falls\\ndown and background jobs are lost), we should be able to re-run the data migration without requiring any code changes.\\n```ruby\\n# good\\nBloodPressure.where(recorded_at: nil).each do |bp|\\nbp.touch(:recorded_at)\\nend\\n# bad - not idempotent\\nBloodPressure.where(id: affected_ids).each do |bp|\\nbp.update(systolic: bp.systolic + 10)\\nend\\n```\\n","tokens":220,"id":1713}
{"File Name":"govuk-infrastructure\/0002-use-aws-eks-terraform-module.md","Context":"## Context\\nA fully configured EKS cluster requires many AWS resources and a lot of configuration. Defining each resource in our own Terraform module maximises flexibility but also requires a significant level of effort. We could instead make use of the [existing Terraform registry EKS module](https:\/\/registry.terraform.io\/modules\/terraform-aws-modules\/eks\/aws\/latest) to optimise for speed of delivery.\\n","Decision":"Adopt the [existing Terraform registry EKS module](https:\/\/registry.terraform.io\/modules\/terraform-aws-modules\/eks\/aws\/latest).\\n","tokens":83,"id":3610}
{"File Name":"nisq-analyzer\/0002-monorepo.md","Context":"## Context and Problem Statement\\nShould the components in the PlanQK platform be splitted into individual repos?\\n","Decision":"Start with a monorepo, split up later if needed. Let FOCUS decide what is best for their semantic knowledge graph component.\\n### Positive Consequences <!-- optional -->\\n* Recommended approach by [Martin Fowler](https:\/\/martinfowler.com\/bliki\/MonolithFirst.html)\\n","tokens":22,"id":450}
{"File Name":"LogLady\/0002-use-gitflow-and-naming.md","Context":"## Context\\nWe need a strategy for how to git and name branches\\n","Decision":"Tasks can be found in [Projects](https:\/\/github.com\/kits-ab\/LogLady\/projects\/1).\\nAssign one to yourself then create branch from develop and name it like this: feature\/whetever-you-are-doing-#corresponding-task-ID\\nExample: `feature\/branchName-#5`\\n**Note:** _feature_ is an example, it could be:\\n- feature\\n- hotfix\\n- release\\nCommit messages should start with task ID.\\nExample: `#5 update README. Explained naming.`\\nPush when done. Make pull request where you add task ID to beginning of comment.\\n","tokens":15,"id":1851}
{"File Name":"gatemint-sdk\/adr-021-protobuf-query-encoding.md","Context":"## Context\\nThis ADR is a continuation of the motivation, design, and context established in\\n[ADR 019](.\/adr-019-protobuf-state-encoding.md) and\\n[ARD 020](.\/adr-019-protobuf-transaction-encoding.md), namely, we aim to design the\\nProtocol Buffer migration path for the client-side of the Cosmos SDK.\\nThis ADR continues from [ARD 020](.\/adr-020-protobuf-transaction-encoding.md)\\nto specify the encoding of queries.\\n","Decision":"### Custom Query Definition\\nModules define custom queries through a protocol buffers `service` definition.\\nThese `service` definitions are generally associated with and used by the\\nGRPC protocol. However, the protocol buffers specification indicates that\\nthey can be used more generically by any request\/response protocol that uses\\nprotocol buffer encoding. Thus, we can use `service` definitions for specifying\\ncustom ABCI queries and even reuse a substantial amount of the GRPC infrastructure.\\nEach module with custom queries should define a service canonically named `Query`:\\n```proto\\n\/\/ x\/bank\/types\/types.proto\\nservice Query {\\nrpc QueryBalance(QueryBalanceParams) returns (cosmos_sdk.v1.Coin) { }\\nrpc QueryAllBalances(QueryAllBalancesParams) returns (QueryAllBalancesResponse) { }\\n}\\n```\\n#### Handling of Interface Types\\nModules that use interface types and need true polymorphism generally force a\\n`oneof` up to the app-level that provides the set of concrete implementations of\\nthat interface that the app supports. While app's are welcome to do the same for\\nqueries and implement an app-level query service, it is recommended that modules\\nprovide query methods that expose these interfaces via `google.protobuf.Any`.\\nThere is a concern on the transaction level that the overhead of `Any` is too\\nhigh to justify its usage. However for queries this is not a concern, and\\nproviding generic module-level queries that use `Any` does not preclude apps\\nfrom also providing app-level queries that return use the app-level `oneof`s.\\nA hypothetical example for the `gov` module would look something like:\\n```proto\\n\/\/ x\/gov\/types\/types.proto\\nimport \"google\/protobuf\/any.proto\";\\nservice Query {\\nrpc GetProposal(GetProposalParams) returns (AnyProposal) { }\\n}\\nmessage AnyProposal {\\nProposalBase base = 1;\\ngoogle.protobuf.Any content = 2;\\n}\\n```\\n### Custom Query Implementation\\nIn order to implement the query service, we can reuse the existing [gogo protobuf](https:\/\/github.com\/gogo\/protobuf)\\ngrpc plugin, which for a service named `Query` generates an interface named\\n`QueryServer` as below:\\n```go\\ntype QueryServer interface {\\nQueryBalance(context.Context, *QueryBalanceParams) (*types.Coin, error)\\nQueryAllBalances(context.Context, *QueryAllBalancesParams) (*QueryAllBalancesResponse, error)\\n}\\n```\\nThe custom queries for our module are implemented by implementing this interface.\\nThe first parameter in this generated interface is a generic `context.Context`,\\nwhereas querier methods generally need an instance of `sdk.Context` to read\\nfrom the store. Since arbitrary values can be attached to `context.Context`\\nusing the `WithValue` and `Value` methods, the SDK should provide a function\\n`sdk.UnwrapSDKContext` to retrieve the `sdk.Context` from the provided\\n`context.Context`.\\nAn example implementation of `QueryBalance` for the bank module as above would\\nlook something like:\\n```go\\ntype Querier struct {\\nKeeper\\n}\\nfunc (q Querier) QueryBalance(ctx context.Context, params *types.QueryBalanceParams) (*sdk.Coin, error) {\\nbalance := q.GetBalance(sdk.UnwrapSDKContext(ctx), params.Address, params.Denom)\\nreturn &balance, nil\\n}\\n```\\n### Custom Query Registration and Routing\\nQuery server implementations as above would be registered with `AppModule`s using\\na new method `RegisterQueryServer(grpc.Server)` which could be implemented simply\\nas below:\\n```go\\n\/\/ x\/bank\/module.go\\nfunc (am AppModule) RegisterQueryServer(server grpc.Server) {\\ntypes.RegisterQueryServer(server, keeper.Querier{am.keeper})\\n}\\n```\\nUnderneath the hood, a new method `RegisterService(sd *grpc.ServiceDesc, handler interface{})`\\nwill be added to the existing `baseapp.QueryRouter` to add the queries to the custom\\nquery routing table (with the routing method being described below).\\nThe signature for this method matches the existing\\n`RegisterServer` method on the GRPC `Server` type where `handler` is the custom\\nquery server implementation described above.\\nGRPC-like requests are routed by the service name (ex. `cosmos_sdk.x.bank.v1.Query`)\\nand method name (ex. `QueryBalance`) combined with `\/`s to form a full\\nmethod name (ex. `\/cosmos_sdk.x.bank.v1.Query\/QueryBalance`). This gets translated\\ninto an ABCI query as `custom\/cosmos_sdk.x.bank.v1.Query\/QueryBalance`. Service handlers\\nregistered with `QueryRouter.RegisterService` will be routed this way.\\nBeyond the method name, GRPC requests carry a protobuf encoded payload, which maps naturally\\nto `RequestQuery.Data`, and receive a protobuf encoded response or error. Thus\\nthere is a quite natural mapping of GRPC-like rpc methods to the existing\\n`sdk.Query` and `QueryRouter` infrastructure.\\nThis basic specification allows us to reuse protocol buffer `service` definitions\\nfor ABCI custom queries substantially reducing the need for manual decoding and\\nencoding in query methods.\\n### GRPC Protocol Support\\nIn addition to providing an ABCI query pathway, we can easily provide a GRPC\\nproxy server that routes requests in the GRPC protocol to ABCI query requests\\nunder the hood. In this way, clients could use their host languages' existing\\nGRPC implementations to make direct queries against Cosmos SDK app's using\\nthese `service` definitions. In order for this server to work, the `QueryRouter`\\non `BaseApp` will need to expose the service handlers registered with\\n`QueryRouter.RegisterService` to the proxy server implementation. Nodes could\\nlaunch the proxy server on a separate port in the same process as the ABCI app\\nwith a command-line flag.\\n### REST Queries and Swagger Generation\\n[grpc-gateway](https:\/\/github.com\/grpc-ecosystem\/grpc-gateway) is a project that\\ntranslates REST calls into GRPC calls using special annotations on service\\nmethods. Modules that want to expose REST queries should add `google.api.http`\\nannotations to their `rpc` methods as in this example below.\\n```proto\\n\/\/ x\/bank\/types\/types.proto\\nservice Query {\\nrpc QueryBalance(QueryBalanceParams) returns (cosmos_sdk.v1.Coin) {\\noption (google.api.http) = {\\nget: \"\/x\/bank\/v1\/balance\/{address}\/{denom}\"\\n};\\n}\\nrpc QueryAllBalances(QueryAllBalancesParams) returns (QueryAllBalancesResponse) {\\noption (google.api.http) = {\\nget: \"\/x\/bank\/v1\/balances\/{address}\"\\n};\\n}\\n}\\n```\\ngrpc-gateway will work direcly against the GRPC proxy described above which will\\ntranslate requests to ABCI queries under the hood. grpc-gateway can also\\ngenerate Swagger definitions automatically.\\nIn the current implementation of REST queries, each module needs to implement\\nREST queries manually in addition to ABCI querier methods. Using the grpc-gateway\\napproach, there will be no need to generate separate REST query handlers, just\\nquery servers as described above as grpc-gateway handles the translation of protobuf\\nto REST as well as Swagger definitions.\\nThe SDK should provide CLI commands for apps to start GRPC gateway either in\\na separate process or the same process as the ABCI app, as well as provide a\\ncommand for generating grpc-gateway proxy `.proto` files and the `swagger.json`\\nfile.\\n### Client Usage\\nThe gogo protobuf grpc plugin generates client interfaces in addition to server\\ninterfaces. For the `Query` service defined above we would get a `QueryClient`\\ninterface like:\\n```go\\ntype QueryClient interface {\\nQueryBalance(ctx context.Context, in *QueryBalanceParams, opts ...grpc.CallOption) (*types.Coin, error)\\nQueryAllBalances(ctx context.Context, in *QueryAllBalancesParams, opts ...grpc.CallOption) (*QueryAllBalancesResponse, error)\\n}\\n```\\nVia a small patch to gogo protobuf ([gogo\/protobuf#675](https:\/\/github.com\/gogo\/protobuf\/pull\/675))\\nwe have tweaked the grpc codegen to use an interface rather than concrete type\\nfor the generated client struct. This allows us to also reuse the GRPC infrastructure\\nfor ABCI client queries.\\n1Context` will receive a new method `QueryConn` that returns a `ClientConn`\\nthat routes calls to ABCI queries\\nClients (such as CLI methods) will then be able to call query methods like this:\\n```go\\nclientCtx := client.NewContext()\\nqueryClient := types.NewQueryClient(clientCtx.QueryConn())\\nparams := &types.QueryBalanceParams{addr, denom}\\nresult, err := queryClient.QueryBalance(gocontext.Background(), params)\\n```\\n### Testing\\nTests would be able to create a query client directly from keeper and `sdk.Context`\\nreferences using a `QueryServerTestHelper` as below:\\n```go\\nqueryHelper := baseapp.NewQueryServerTestHelper(ctx)\\ntypes.RegisterQueryServer(queryHelper, keeper.Querier{app.BankKeeper})\\nqueryClient := types.NewQueryClient(queryHelper)\\n```\\n","tokens":108,"id":16}
{"File Name":"HES_pipeline\/data_location.md","Context":"## Context\\nIncoming data from NHS is not handled by users of the HES pipeline. This makes\\nit harder to control the directory format of the raw data, as well as the\\nconsistency of that data and directory structure.\\nAdditionally the SQLite database needs to be located in a writable environment.\\n","Decision":"The raw data will be copied manually from the Data directory to the Library\\ndirectory. Subsequent updates to the raw data will also require this. This is\\nalso where the database will be located and written to.\\n","tokens":62,"id":3252}
{"File Name":"OpenTermsArchive\/0001-service-name-and-id.md","Context":"## Context and Problem Statement\\nTo scale up from 50 to 5,000 services, we need a clear way for choosing the service name and the service ID.\\n### We need\\nA name that reflects the common name used by the provider itself, to be exposed in a GUI. This name is currently exposed as the name property in the service declaration.\\nAn ID of sorts that can be represented in the filesystem. This ID is currently exposed as the filename of the service declaration, without the .json extension.\\n### Use cases\\nThe service name is presented to end users. It should reflect as closely as possible the official service name, so that it can be identified easily.\\nThe ID is used internally and exposed for analysis. It should be easy to handle with scripts and other tools.\\n### Constraints for the ID\\nAs long as this ID is stored in the filesystem:\\n- No `\/` for UNIX.\\n- No `\\` for Windows.\\n- No `:` for APFS and HFS.\\n- No case-sensitive duplicates to support case-insensitive filesystems.\\n- No more than 255 characters to support transfer over [FAT32](https:\/\/en.wikipedia.org\/wiki\/File_Allocation_Table#FAT32).\\nUTF, spaces and capitals are all supported, even on case-insensitive filesystems.\\n### However\\n- UTF in filenames can be [a (fixable) problem with Git and HFS+](https:\/\/stackoverflow.com\/questions\/5581857\/git-and-the-umlaut-problem-on-mac-os-x).\\n- UTF in filenames is by default quoted in Git, leading for example `\u00e9t\u00e9.txt` to be displayed as `\"\\303\\251t\\303\\251.txt\"`.\\n- Most online services align their brand name with their domain name. Even though UTF is now officially supported in domain names, support is limited and most services, even non-Western, have an official ASCII transliteration used at least in their domain name (e.g. \u201cqq\u201d by Tencent, \u201crzd.ru\u201d for \u201c\u0420\u0416\u0414\u201d, \u201cyahoo\u201d for \u201cYahoo!\u201d).\\n- We currently use GitHub as a GUI, so the service ID is presented to the user instead of the service name. The name is used in email notifications.\\n","Decision":"1. The service name should be the one used by the service itself, no matter the alphabet.\\n- _Example: `\u0442\u0443\u0442\u0443.\u0440\u0443`_.\\n2. We don't support non-ASCII characters in service IDs, at least as long as the database is Git and the filesystem, in order to minimise risk. Service IDs are derived from the service name through normalization into ASCII.\\n- _Example: `\u0442\u0443\u0442\u0443.\u0440\u0443` \u2192 `tutu.ru`_.\\n- _Example: `historiel\u00e6rer.dk` \u2192 `historielaerer.dk`_.\\n- _Example: `RT\u00c9` \u2192 `RTE`_.\\n3. We support punctuation, except characters that have meaning at filesystem level (`:`, `\/`, `\\`). These are replaced with a dash (`-`).\\n- _Example: `Yahoo!` \u2192 `Yahoo!`_.\\n- _Example: `Last.fm` \u2192 `Last.fm`_.\\n- _Example: `re:start` \u2192 `re-start`_.\\n- _Example: `we:\/\/` \u2192 `we---`_.\\n4. We support capitals. Casing is expected to reflect the official service name casing.\\n- _Example: `hi5` \u2192 `hi5`_.\\n- _Example: `DeviantArt` \u2192 `DeviantArt`_.\\n- _Example: `LINE` \u2192 `LINE`_.\\n5. We support spaces. Spaces are expected to reflect the official service name spacing.\\n- _Example: `App Store` \u2192 `App Store`_.\\n- _Example: `DeviantArt` \u2192 `DeviantArt`_.\\n6. We prefix the service name by the provider name when self-references are ambiguous, separated by a space. For example, Facebook refers to their Self-serve Ads service simply as \u201cAds\u201d, which we cannot use in a shared database. We thus call the service \u201cFacebook Ads\u201d.\\n- _Example: `Ads` (by Facebook) \u2192 `Facebook Ads`_.\\n- _Example: `Analytics` (by Google) \u2192 `Google Analytics`_.\\n- _Example: `Firebase` (by Google) \u2192 `Firebase`_.\\n- _Example: `App Store` (by Apple) \u2192 `App Store`_.\\n","tokens":468,"id":4666}
{"File Name":"re-build-systems\/0007-add-ability-to-import-custom-jobs.md","Context":"## Context\\nWe need the user to be able to define Jenkins jobs in code and be able to import them into Jenkins.\\nWe have identified a number of ways to do this:\\n1. Define jobs with Groovy and inject the script, as we do for the Jenkins configuration\\n2. Automatically create the jobs by scanning a Github account\\n3. [Jenkins Job Builder]\\n4. [Job DSL plugin]\\n### Option 1\\nThis is the easiest for us as we don't need to implement anything new. We can use the mechanism of injecting Groovy script which is already available. It is also relatively easy for the user to use. The code for the jobs and configuration can't exceed 16 KB, which is a limitation, but we believe that is enough for compact Jenkins installations (a Jenkins with hundreds of jobs is an anti-pattern). [The limit] is because we use [user data] to implement this option.\\n### Option 2\\nJenkins provides a way to scan a Github organisation or accounts for repositories containing pipeline configurations in a Jenkinsfile. This should be quite easy for the user but the implementation can be quite complex. This option would require the user to pass extra parameters to the Jenkins Terraform module: at least one regular expression to filter the repositories to match, and a Github personal token. The token is needed because scanning Github as an unauthenticated user is extremely slow but only takes a few minutes for a user with authentication. As the module needs a token as an input, there is extra complexity around managing that secret. This would be relatively straightforward to do using the UI but providing this solution as code would be quite involved.\\n### Option 3\\n(Jenkins Job Builder) is probably the most commonly used at GDS (it's used by Notify, Digital Marketplace) - people generally like it but some issues were pointed out like a difficulty in upgrading to a newer version or in escaping quotes correctly. [GOV.UK] and Pay use a more ad-hoc, homebrewed approach. Both groups rely on Puppet or Chef to inject their jobs into Jenkins. We allow users to install their configuration management tool via cloud-init, so the user is still free to override any mechanism we provide.\\n### Option 4\\nThis hasn't been explored in great detail as we felt we'd already found a good solution. However, if we revisit the decision made in this PR, this tool should be evaluated more thoroughly.\\n","Decision":"We decided to implement solution 1 to keep things simple and because of time constraints. Options two and three can still be used but will not be supported. In the future, we may consider to change to another solution if we feel there is the user need.\\n","tokens":500,"id":321}
{"File Name":"ios-architecture-decision-logs\/0005-inject-ab-config-global-values-toPresenter.md","Context":"## Context\\nWe faced a problem that missing test cases on some presenters because of not injectable variables like ab tests, config or global values on presenters. So we want to cover all of these missings.\\n","Decision":"We decided to inject this variables to related presenters from their constructors.\\n","tokens":45,"id":4973}
{"File Name":"gsp\/ADR039-cloudhsm-namespace-network-policy.md","Context":"## Context\\n[ADR036](ADR036-hsm-isolation-in-detail.md) described the network and\\ncredential isolation we use to ensure that unauthorised users cannot\\naccess the CloudHSM.\\nRecently in 3ea9de2ff, we introduced a GlobalNetworkPolicy object,\\nwhich is a Calico feature that allows a cluster-wide network policy to\\nbe imposed.  This allows us to control network access to and from\\nparticular namespaces in a way which cannot be overridden by tenants.\\nIn particular, currently access to the HSM is only allowed from pods\\nannotated with a `talksToHsm=true` label.\\nWhen working in their own namespace, a developer has full control over\\nwhat labels they put on their pods, so they can still choose to put\\nthe `talksToHsm=true` label on their pods.  But they do not have\\ncontrol over what labels the namespace itself has; to change this\\nwould require a change to the `gsp` or appropriate `cluster-config`\\nrepository, which would make such a change visible to many more\\npeople.\\nTherefore, if we extend the GlobalNetworkPolicy to require a\\n`talksToHsm=true` label on *both* the pod *and* the namespace, we will\\nprevent tenants from unilaterally opening up network access to the HSM\\nfrom their namespaces.\\n","Decision":"We will augment the GlobalNetworkPolicy (previously described in ADR036) by:\\n- setting a `GlobalNetworkPolicy` that denies access to the\\nCloudHSM's IP address unless the pod carries a label\\n(`talksToHsm=true`) and the namespace also carries a label\\n(`talksToHsm=true`) and allows all other egress traffic\\n","tokens":289,"id":3880}
{"File Name":"rfcs\/0000-associate-users-with-entries.md","Context":"## Context\\n[context]: #context\\n> This section describes the forces at play, including technological, political, social, and project local. These forces are probably in tension, and should be called out as such. The language in this section is value-neutral. It is simply describing facts.\\n### MVP\\n> Wenn ein Nutzer eingeloggt ist, wird er mit dem Eintrag assoziiert:\\n> - Er wird standardm\u00e4\u00dfig \u00fcber zuk\u00fcnftige \u00c4nderungen anderer Nutzer informiert\\nSiehe Ziel 3). Kann dies aber \u00fcber eine Checkbox vor dem Speichern abw\u00e4hlen.\\n> - Er muss keinen Copyright lizenz akzeptieren, weil er das einmal bei der Registrierung gemacht hat.\\n> - In der Fu\u00dfzeile f\u00fcr jeden Eintrag wird der Username oder die Nutzer-ID\\nangezeigt, von dem eingeloggten Nutzer, der es zuletzt editiert hat.\\n> - ??? Notmoderationscheckbox: User kann ausw\u00e4hlen, dass\\nanonyme \u00c4nderungen erst sichtbar werden, wenn er oder ein\\nanderer RegPi\/ThemPi die \u00e4nderung best\u00e4tigt (\u00fcber link in\\nNotification mail. Best\u00e4tigungsmail muss sich von anderen\\n'Eintrag bearbeitet'-Mail unterscheiden zB. im Betreff:\\nAktualisierung wartet auf Freischaltung: Name des Eintrags)\\n### NTH\\n> - Eingeloggte Nutzer k\u00f6nnen einen Filter setzten \"Zeige nur\\nver\u00e4nderte Eintr\u00e4ge\" und sehen dann nur \u00c4nderungen\\n> - Alle ge\u00e4nderten Eintr\u00e4ge die auf Freischaltung warten, sind als\\nPinfarbe rot markiert\\n","Decision":"[decision]: #decision\\n> This section describes our response to these forces. It is stated in full sentences, with active voice. \"We will ...\"\\n","tokens":369,"id":1881}
{"File Name":"akvo-product-design\/ADR-006.md","Context":"## Context\\nCurrently, when user requests data point sync from either DataPointsMapFragment or SurveyedLocaleListFragment and a service,\\nSurveyedDataPointSyncService is started and for each batch of datapoints both fragments get notified of new data available via local Broadcast which both subscribe too. While data is being loaded a non-blocking \"loading\" ui is shown. Once the loading is finished\\nthat loading needs to be dismissed. We also display a snackBar with the number of updated datapoints or some eventual errors.\\n","Decision":"The current implementation is not ideal since if user leaves the fragments view, the service will still send the broadcast even\\nif there is no ui waiting for it. Both fragments have to subscribe to the broadcast even if only one of them that will display\\nthe error\/success snackBars. It would be much better to have the ui subscribe to changes in the database data instead of broarcasts.\\nThat way the service does not need to notify ui of new data being available instead when the service inserts data into the database\\nthe ui will automatically be notified. RX Java can be used for this, an example is this library https:\/\/github.com\/square\/sqlbrite.\\nThis task also requires the refactor of this feature (independently from the rest of the application). We will separate the ui and\\nview (fragment and presenter) logic from domain logic (business logic, use cases) and the data logic (data sources sych as database\\nand network). This separation of concerns will allow us to add unit tests to separate components which will have a Single\\nResponsability.\\n","tokens":106,"id":2801}
{"File Name":"rems\/017-applicant-ux.md","Context":"# Problems with the current applicant UX\\n","Decision":"Here's a step-by-step plan for fixing these.\\n1. (Problem A) Make save succeed even with validation errors. Show validation errors in yellow (instead of red). [#2766](https:\/\/github.com\/CSCfi\/rems\/issues\/2766)\\n2. (Problem B) Run validations in the frontend after the user has stopped typing. [#2614](https:\/\/github.com\/CSCfi\/rems\/issues\/2614)\\n3. (Problem B) Autosave in the background by replacing\/patching the latest draft-saved event. If the latest event is not a draft-saved then create a new one. [#2767](https:\/\/github.com\/CSCfi\/rems\/issues\/2767)\\n","tokens":8,"id":465}
{"File Name":"clean-architecture-example\/0006-avoid-api-versioning.md","Context":"## Context\\nCode base is becomes too complicated.\\nMaintaining and supporting multiple API versions is painful and expensive.\\n","Decision":"We will avoid API versioning.\\nWe will not introduce breaking changes.\\n","tokens":24,"id":3534}
{"File Name":"documentation\/0004-use-asymmetric-jwt-signing.md","Context":"## Context and Problem Statement\\nWhen using JSON Web Tokens generating signatures and verifying them is an important task.\\nJWTs can be signed using a secret (with the HMAC algorithm) or a public\/private key pair using RSA or ECDSA.\\nWhen implementing JWTs one must decide which method to use.\\n## Decision Drivers\\n* Multi tenant support with own keys for each tenant\\n","Decision":"* Multi tenant support with own keys for each tenant\\nChosen option: \"Asymmetric JWT signing\", because it the only option which allow to use different keys for different tenants.\\n### Positive Consequences\\n* multiple keys are supported\\n### Negative Consequences\\n* complex management of keys\\n","tokens":77,"id":414}
{"File Name":"opg-metrics\/0005-aws-api-keys.md","Context":"## Context\\nAs our API Endpoint will be exposed to the world, we require it to have some sort of protection and authentication against attackers.\\nWe will initially only be connecting internal facing services (AWS) and thrid party tooling, however in the future we would like to begin recording real user metrics (RUM).\\nWe would need to have a way of protecting against attackers spamming our API.\\n","Decision":"We have chosen to use AWS Api Gateway API Keys for our authorisation and usage limits. This will require an AWS Signature being generated from your credentials and API Key.\\nEach service or integration should have their own key.\\nEach service should also set their own usage limits which are contextual to their service. For example, CircleCI integration could be monitored for the average use during a day and have this set, as well as having a secure way to request the keys when they are rotated.\\nWe have also looked at [AWS WAFV2](https:\/\/docs.aws.amazon.com\/waf\/latest\/APIReference\/Welcome.html) and this is something we may look at in the future depending on our usage of the service.\\n","tokens":83,"id":1544}
{"File Name":"ReportMI-service-manual\/0002-overall-technical-approach.md","Context":"## Context\\nThe Data Submission Service is a new digital system for suppliers to submit\\nmanagement information to CCS. It replaces the current Management Information\\nSystem Online (MISO).\\nThis paper seeks approval for the overall technical approach for the new\\nservice.\\n### Management Information System Online (MISO)\\nMISO is an online service run by CCS for suppliers to submit data about work\\nundertaken through CCS commercial agreements. Suppliers submit data on a monthly\\nbasis via a complex Excel spreadsheet.\\nThe data submitted through MISO is used to calculate the management fee that is\\ncharged to suppliers. This data is also used by Commercial Agreement Managers\\n(CAMs) to help with management of suppliers and agreements as well as inform\\nfuture agreement design.\\nMISO was developed in 2011 with only minor updates since.\\nThe service frequently experiences poor performance with frequent errors. It\\ndelivers a poor user experience, and requires a large amount of manual work to\\nadd new commercial agreements and new suppliers.\\nThe number of transactions taken by MISO has increased significantly over recent\\nyears, rising from around 5000 per month in 2015, to over 8000 per month\\nin 2018. Transactions peak around G-Cloud commercial agreement boundaries (with\\nnearly 10,000 transactions in June 2017). This increase puts a further strain on\\nan already stretched service.\\n### Estate in flux\\nThe technology estate within CCS is in a state of flux. Several important parts\\nare being replaced over the next 6 months, including eSourcing (how commercial\\nagreements are established), Finance & HR and the roll-out of the Crown\\nMarketplace.\\nAdditionally, there is ongoing work to replace the CCS website, develop the\\norganisation's data strategy, technical architecture and early plans to develop\\nand improve the identity and access management services.\\n### Data Submission Service Alpha\\nThe Digital Services Team has been working on a prototyping phase to identify\\noptions for a new service to replace MISO.\\nThis phase has involved user research with suppliers and staff within CCS who\\nadminister MISO. It's looked at ways of improving processes and investigated\\npotential technology choices.\\nThe team is now ready to start producing a Minimum Viable Service with an aim to\\ntake real data submissions from suppliers later this year. The medium-term aim\\nis to switch off the existing MISO service in 2019.\\n","Decision":"The new service will need to have the following characteristics:\\n- **Scalability** - the service must cope with high peaks in demand -\\nparticularly around submission deadlines. Outside of peak times, the service is\\ninfrequently used. Over time, the number of transactions is likely to grow,\\nparticularly with the introductions of G-Cloud 10, Digital Outcomes and\\nSpecialists 3 and Dynamic Purchasing System (DPS) where suppliers can leave and\\njoin a commercial agreement at any time.\\n- **Flexibility** - the service needs to collect different types of data for\\ndifferent commercial agreements. This includes invoice data, contract data,\\ndata to support decision making. Over time, CCS may need to collect different\\ntypes of data, particularly as agreements and industries change.\\n- **Adaptable** - as the CCS technical landscape changes, the service will need\\nto change along with it. For example, in future it will need to support a new\\nfinance system, and potential integration with Crown Marketplace, eSourcing and\\nthe CCS website.\\nThese characteristics lend themselves to developing a system with small pieces\\nthat can be easily iterated or replaced as requirements change over time.\\nThe new data submission service will comprise the following pieces:\\n1. **Submission app** - a public facing front-end for suppliers to submit\\nreturns. This will comprise a specially designed user-interface to make the\\njourney for suppliers as simple as possible, providing timely and useful\\nfeedback to help reduce errors, using well-tested design patterns from the\\nGOV.UK toolkit alongside the CCS branding.\\n1. **Data storage layer** - to store the data submitted to CCS. This will act as\\nthe 'core' of the new service, and the source of truth for submission data. This\\nwill be accessed via an API, allowing other CCS services to use the data as\\nrequired.\\n1. **File transformation service** - a small service to take various file inputs\\nfrom suppliers, extract required data and store it in the data storage layer.\\nThis service will need to support various file formats including custom Excel\\ndocuments, CSVs, PDFs and Open Office formats.\\n1. **Data validation service** - a small service to validate data submitted by\\nsuppliers and calculate the approximate management fee required. This will need\\nto adapt over time as different data is collected from suppliers and different\\nmethods of calculating the management fee are introduced.\\n1. **Notification service** - a small service to manage sending of notifications\\nto users. This service will automatically generate notifications, and handle\\nresponses (eg bounces, out of office replies etc), reducing the burden on the\\nCCS support staff. The service will integrate with GOV.UK Notify for sending\\nnotifications.\\n1. **Data exports** - a series of regular 'batch' data exports to provide\\nsubmission data to other systems, including the Data Warehouse and the legacy\\nfinance system.\\n1. **Administration app** - a front-end for CCS staff to support the operation\\nof the data submission service. This will comprise a customised interface to\\nmake support tasks as simple as possible, allowing staff to focus on more\\nimportant work.\\n1. **Onboarding app** - a front-end for CCS staff to manage the onboarding of\\nnew commercial agreements and suppliers. This will compromise a customised\\ninterface to allow CCS to specify commercial agreement data collection\\nrequirements, validation rules, management fee calculation formula, and handle\\nthe onboarding of new users\/suppliers.\\nEach of these components will make use of the most appropriate and simplest\\ntechnology to perform the required functions and will be integrated using well\\ndefined JSON APIs. Each piece should be easy to maintain, iterate and enhance,\\nand should also be easy to replace in future.\\nWhere possible, the service will make use of managed cloud services to provide\\nfeatures such as automatic scaling, high availability and resiliency.\\nThe service will make use of existing government and CCS services where possible\\n- for example GOV.UK Notify, the Supplier Registration Service, Digital\\nMarketplace etc.\\n","tokens":498,"id":2060}
{"File Name":"planet4-docs\/adr-0020-form-builder-data-retention-policy.md","Context":"### Context and Problem Statement\\nMost of the Form types (as [identified](https:\/\/miro.com\/app\/board\/uXjVO\\_vBIYc=\/)) are already integrating into external systems. But not all of them do (eg. Quiz). For the ones that don\u2019t we need to identify what\u2019s a reasonable policy for NRO admins to have the time to decide the extent of data they want to keep but also have the time to export them. But even for the forms that send data to other systems (eg. Petitions) we need a failsafe to make sure data is actually sent over before deletion.\\nAt the same time we should aim for respecting GDPR requirements for the period of time these data remain on websites\u2019 databases.\\n### Decision Outcome\\n#### **Data retention**\\nEnforce a 90 days retention policy. All submission entries will be automatically deleted after point. It would be up to NRO admins to export any submission data that are not synced to another external system if they wish to.\\n<figure><img src=\"..\/..\/.gitbook\/assets\/form-builder-retention.png\" alt=\"\"><figcaption><\/figcaption><\/figure>\\n#### **Integrate into Wordpress export\/delete tools**\\nThis is useful in cases where an NRO gets a request by a supporter to delete or export personal data.\\n<figure><img src=\"..\/..\/.gitbook\/assets\/form-builder-exporter.png\" alt=\"\"><figcaption><\/figcaption><\/figure>\\n#### **Minimize personal data footprint**\\nDon\u2019t store IP addresses on form submissions.\\n<figure><img src=\"..\/..\/.gitbook\/assets\/form-builder-ip-address.png\" alt=\"\"><figcaption><\/figcaption><\/figure>\\n### Links\\n* [Data Flows Miro board](https:\/\/miro.com\/app\/board\/uXjVO\\_vBIYc=\/)\\n","Decision":"#### **Data retention**\\nEnforce a 90 days retention policy. All submission entries will be automatically deleted after point. It would be up to NRO admins to export any submission data that are not synced to another external system if they wish to.\\n<figure><img src=\"..\/..\/.gitbook\/assets\/form-builder-retention.png\" alt=\"\"><figcaption><\/figcaption><\/figure>\\n#### **Integrate into Wordpress export\/delete tools**\\nThis is useful in cases where an NRO gets a request by a supporter to delete or export personal data.\\n<figure><img src=\"..\/..\/.gitbook\/assets\/form-builder-exporter.png\" alt=\"\"><figcaption><\/figcaption><\/figure>\\n#### **Minimize personal data footprint**\\nDon\u2019t store IP addresses on form submissions.\\n<figure><img src=\"..\/..\/.gitbook\/assets\/form-builder-ip-address.png\" alt=\"\"><figcaption><\/figcaption><\/figure>\\n### Links\\n* [Data Flows Miro board](https:\/\/miro.com\/app\/board\/uXjVO\\_vBIYc=\/)\\n","tokens":371,"id":4091}
{"File Name":"qiskit-vscode\/ADR-004.md","Context":"## Context\\nThe visualization features of the project started as a spike, and the code is not very clear. We decided that a templates engine will solve some of the code problems and will make easier to create new visualizations in the future.\\n","Decision":"The recommended options from the team were, React, Lit-HTML and Nunjucks. React looks like an excellent option, but had some problems with the current templates, so Nunjucks replaced it as the best option at this moment.\\n","tokens":49,"id":1635}
{"File Name":"gatemint-sdk\/adr-011-generalize-genesis-accounts.md","Context":"## Context\\nCurrently, the SDK allows for custom account types; the `auth` keeper stores any type fulfilling its `Account` interface. However `auth` does not handle exporting or loading accounts to\/from a genesis file, this is done by `genaccounts`, which only handles one of 4 concrete account types (`BaseAccount`, `ContinuousVestingAccount`, `DelayedVestingAccount` and `ModuleAccount`).\\nProjects desiring to use custom accounts (say custom vesting accounts) need to fork and modify `genaccounts`.\\n","Decision":"In summary, we will (un)marshal all accounts (interface types) directly using amino, rather than converting to `genaccounts`\u2019s `GenesisAccount` type. Since doing this removes the majority of `genaccounts`'s code, we will merge `genaccounts` into `auth`. Marshalled accounts will be stored in `auth`'s genesis state.\\nDetailed changes:\\n### 1) (Un)Marshal accounts directly using amino\\nThe `auth` module's `GenesisState` gains a new field `Accounts`. Note these aren't of type `exported.Account` for reasons outlined in section 3.\\n```go\\n\/\/ GenesisState - all auth state that must be provided at genesis\\ntype GenesisState struct {\\nParams   Params           `json:\"params\" yaml:\"params\"`\\nAccounts []GenesisAccount `json:\"accounts\" yaml:\"accounts\"`\\n}\\n```\\nNow `auth`'s `InitGenesis` and `ExportGenesis` (un)marshal accounts as well as the defined params.\\n```go\\n\/\/ InitGenesis - Init store state from genesis data\\nfunc InitGenesis(ctx sdk.Context, ak AccountKeeper, data GenesisState) {\\nak.SetParams(ctx, data.Params)\\n\/\/ load the accounts\\nfor _, a := range data.Accounts {\\nacc := ak.NewAccount(ctx, a) \/\/ set account number\\nak.SetAccount(ctx, acc)\\n}\\n}\\n\/\/ ExportGenesis returns a GenesisState for a given context and keeper\\nfunc ExportGenesis(ctx sdk.Context, ak AccountKeeper) GenesisState {\\nparams := ak.GetParams(ctx)\\nvar genAccounts []exported.GenesisAccount\\nak.IterateAccounts(ctx, func(account exported.Account) bool {\\ngenAccount := account.(exported.GenesisAccount)\\ngenAccounts = append(genAccounts, genAccount)\\nreturn false\\n})\\nreturn NewGenesisState(params, genAccounts)\\n}\\n```\\n### 2) Register custom account types on the `auth` codec\\nThe `auth` codec must have all custom account types registered to marshal them. We will follow the pattern established in `gov` for proposals.\\nAn example custom account definition:\\n```go\\nimport authtypes \"github.com\/cosmos\/cosmos-sdk\/x\/auth\/types\"\\n\/\/ Register the module account type with the auth module codec so it can decode module accounts stored in a genesis file\\nfunc init() {\\nauthtypes.RegisterAccountTypeCodec(ModuleAccount{}, \"cosmos-sdk\/ModuleAccount\")\\n}\\ntype ModuleAccount struct {\\n...\\n```\\nThe `auth` codec definition:\\n```go\\nvar ModuleCdc *codec.LegacyAmino\\nfunc init() {\\nModuleCdc = codec.NewLegacyAmino()\\n\/\/ register module msg's and Account interface\\n...\\n\/\/ leave the codec unsealed\\n}\\n\/\/ RegisterAccountTypeCodec registers an external account type defined in another module for the internal ModuleCdc.\\nfunc RegisterAccountTypeCodec(o interface{}, name string) {\\nModuleCdc.RegisterConcrete(o, name, nil)\\n}\\n```\\n### 3) Genesis validation for custom account types\\nModules implement a `ValidateGenesis` method. As `auth` does not know of account implementations, accounts will need to validate themselves.\\nWe will unmarshal accounts into a `GenesisAccount` interface that includes a `Validate` method.\\n```go\\ntype GenesisAccount interface {\\nexported.Account\\nValidate() error\\n}\\n```\\nThen the `auth` `ValidateGenesis` function becomes:\\n```go\\n\/\/ ValidateGenesis performs basic validation of auth genesis data returning an\\n\/\/ error for any failed validation criteria.\\nfunc ValidateGenesis(data GenesisState) error {\\n\/\/ Validate params\\n...\\n\/\/ Validate accounts\\naddrMap := make(map[string]bool, len(data.Accounts))\\nfor _, acc := range data.Accounts {\\n\/\/ check for duplicated accounts\\naddrStr := acc.GetAddress().String()\\nif _, ok := addrMap[addrStr]; ok {\\nreturn fmt.Errorf(\"duplicate account found in genesis state; address: %s\", addrStr)\\n}\\naddrMap[addrStr] = true\\n\/\/ check account specific validation\\nif err := acc.Validate(); err != nil {\\nreturn fmt.Errorf(\"invalid account found in genesis state; address: %s, error: %s\", addrStr, err.Error())\\n}\\n}\\nreturn nil\\n}\\n```\\n### 4) Move add-genesis-account cli to `auth`\\nThe `genaccounts` module contains a cli command to add base or vesting accounts to a genesis file.\\nThis will be moved to `auth`. We will leave it to projects to write their own commands to add custom accounts. An extensible cli handler, similar to `gov`, could be created but it is not worth the complexity for this minor use case.\\n### 5) Update module and vesting accounts\\nUnder the new scheme, module and vesting account types need some minor updates:\\n- Type registration on `auth`'s codec (shown above)\\n- A `Validate` method for each `Account` concrete type\\n","tokens":111,"id":25}
{"File Name":"buildit-all\/0003-use-junit-for-tests-instead-of-spek.md","Context":"## Context\\nThere are a number of unit testing frameworks available for the JVM.  There are also some newer unit testing frameworks that are specific to Kotlin.  Spring currently (4.x) only supports JUnit 4 and TestNG.  JUnit 5 can be made to work however.\\n","Decision":"Use JUnit 5 for all unit and e2e tests.  This will simplify thing and has better integration currently with IntelliJ IDE.\\n","tokens":61,"id":4426}
{"File Name":"wikiindex\/adr-002-routing_library.md","Context":"## Context\\n* We are building a web app that has one official url that uses query params\\n* The url is \/search?q=search-term\\n* We believe that future endpoints will be added, or more complicated search requests\\n","Decision":"* We will use Bidi to parse the routes.\\n","tokens":46,"id":1384}
{"File Name":"CrossyToad\/adr-0009-organise-by-feature.md","Context":"## Context\\nThere are two main strategies for organising source code that I'm aware of:\\n- Organise by kind\\n- Organise by feature\\nWith \"Organise by kind\" we group modules that are related architecturally together. I.e. group all\\n\"Effects\", group all \"Data\", group all \"Scenes\".\\nThis is a fairly common approach, particularly given that website architectures often typify this\\nstyle by grouping all \"modules\", \"views\" and \"controllers\".\\nAlternatively we can \"Organise by feature\", ideally in this style the source code required to\\nimplement a particular \"Feature\" is grouped together instead. Ideally this would mean that\\nremoving a feature would simply mean deleting it's grouping any removing any dependent links.\\nFor this project I want to experiement to see what \"Organise by feature\" would look like for a game.\\n","Decision":"We're going to organise by *feature*!\\n","tokens":182,"id":2485}
{"File Name":"ehoks-ui\/0001-record-architecture-and-desing-decisions.md","Context":"## Context\\nWe need to record the architectural and desing decisions made during this project.\\n","Decision":"We will use Architecture Decision Records, as described by\\nMichael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\nAlso https:\/\/medium.com\/better-programming\/here-is-a-simple-yet-powerful-tool-to-record-your-architectural-decisions-5fb31367a7da\\n","tokens":19,"id":597}
{"File Name":"cafebabel.com\/0004-deal-with-translations.md","Context":"## Context\\nArticles can be translated in English, French, Spanish, Italian and\\nGerman.\\nOriginal articles are not necessarily in English and initial text of a\\ngiven translation can be itself an already translated article.\\nEach translation must keep a link to the original article.\\nThe review process remains the same with a draft status.\\nThe translated article must both have a redactor and a translator.\\n","Decision":"A new MongoDB document will be created for each translation.\\nThat document will inherit from the Article model.\\n","tokens":81,"id":3241}
{"File Name":"compliantkubernetes\/0001-use-rook-storage-orchestrator.md","Context":"## Context and Problem Statement\\nCompliant Kubernetes has the vision to reduce the compliance burden on multiple clouds (\"Multi-cloud. Open source. Compliant.\"). Many of the cloud providers we target do not have a storage provider or do not have a storage provider that integrates with Kubernetes. How should we support PersistentVolumeClaims in such cases?\\n## Decision Drivers\\n* Storage Orchestrator needs to be popular and well maintained, so that developer can focus on adding value on top of Kubernetes clusters.\\n* Storage Orchestrator needs to be easy to set up, easy to operate and battle-tested, so on-call administrators are not constantly woken up.\\n* Storage Orchestrator needs to have reasonable performance. (A local storage provider can deal with high-performance use-cases.)\\n","Decision":"* Storage Orchestrator needs to be popular and well maintained, so that developer can focus on adding value on top of Kubernetes clusters.\\n* Storage Orchestrator needs to be easy to set up, easy to operate and battle-tested, so on-call administrators are not constantly woken up.\\n* Storage Orchestrator needs to have reasonable performance. (A local storage provider can deal with high-performance use-cases.)\\nChosen option: \"Rook\", because it is CNCF graduated, hence it is most likely to drive development and adoption long-term. Prady tested it and showed it was easy to use. It supports Ceph as a backend, making it battle-tested. It has reasonable performance.\\n### Positive Consequences\\n* We no longer need to worry about cloud provider without native storage.\\n### Negative Consequences\\n* We need to deprecate our NFS storage provider.\\n* Some manual steps are required to set up partitions for Rook. These will be automated when the burden justifies it.\\n","tokens":158,"id":3101}
{"File Name":"origin\/0004-use-openzeppelin.md","Context":"## Context\\nOrigin SDK contracts are designed to be upgradable by abstracting proxy, logic and storage to separate contracts. This approach leads to maintaining 3 separate Solidity files per contract.\\n","Decision":"Use OpenZeppelin implementation based on generalized proxy, logic and storage to remove the need of keeping 3 separate custom implemented contracts.\\n","tokens":39,"id":1551}
{"File Name":"human-essentials\/0007-barcode-querying.md","Context":"## Context\\nWe've been using Barcode values that have been recorded as either private (associated only with an organization) or global (available to all organizations). The idea was that Essentials Banks could decide to \"Share\" a barcode with everyone by making it global, and that this barcode could then be used by others. The implementation, as well as the actual use-cases by Essentials Banks in beta testing, has contraindicated this utility. Additionally, it's unclear how Barcode lookup happens with regard to Base Items.\\n","Decision":"\"Global\" barcodes are associated with Base Items. Doing a Barcode lookup now does a cascade retrieval.\\n1. First, it checks if the organization has defined a barcode with that value.\\n2. If that exists, then it uses that record.\\n3. If it doesn't exist, then it checks to see if that barcode value exists as a global barcode.\\n4. If it exists there, it gets the base item associated with that, and then looks at the organizations items and finds the oldest item with that base item type, and applies it to that.\\n5. If it can't find one for that, then it prompts the user to create a new barcode record.\\nWhen we get EAN13 numbers for major products, we can enter those as global barcodes, associated with the generic base items; eg. 48x Pampers 3T and 48x Huggies 3T will both map to the 48x 3T record.\\n","tokens":105,"id":3344}
{"File Name":"mediawiki-extensions-WikibaseManifest\/0000_deliver_as_extension.md","Context":"## Context\\nWe considered two options for delivering Wikibase Manifest:\\n- Built as MediaWiki Extension\\n- Include in Wikibase Core\\n|   Options\t|  Consistency across 3rd party Wikibases \t|   Installation\/Setup Burden\t|  User Adoption \t|\\n|---\t|---\t|---\t|---\t|\\n|  MW Extension \t|   Third-party Wikibase admins will have to provide the Manifest via extension.\t|   Additional effort of setting up an extension. It could be part of the docker bundle\t|   Assuming lower as it involves installing a separate extension\t|\\n|  Wikibase core\t|   Tool builders could rely on the Manifest always being there in any third-party Wikibase. This would be an advantage over the extension. But the user still needs to configure it. |   None\t|  Assuming higher\t|\\n|   Options\t|  Maintenance Burden \t|   Backwards Compatibility\t|  Testing Infrastructure\t|\\n|---\t|---\t|---\t|---\t|\\n|  MW Extension \t|   Need to set up our own CI\t|   compatible|   Less straightforward to test in Beta if we make this an extension, will have to set up our own cloudvps test instance.\t|\\n|  Wikibase core\t|   Same as Wikibase\t|   compatible\t|   Could use Beta or a cloudvps test instance for testing infrastructure. Probably an advantage    |\\n|   Options\t|  Documentation\t|   Speed of release to Wikibase users\t|  Feedback loop with tool builders \t|\\n|---\t|---\t|---\t|---\t|\\n|  MW Extension \t|   Need to document installation of the extension, as well as setup. Probably minimal additional effort \t|   Potentially faster\t|   Probably shorter, for the same reasons stated on speed of release to wb users\t|\\n|  Wikibase core\t|   Document how to work with the file\t|   Waiting until a new release is made (e.g. 1.36) or we have to do some backporting\t|   Not sure\t|\\n","Decision":"Build as a MediaWiki Extension.\\nPlan to move the functionality to Wikibase core in the future.\\n","tokens":460,"id":4999}
{"File Name":"james-project\/0046-generalize-event-bus.md","Context":"## Context\\nUser email storage usage is limited both in size and count via quotas (IMAP RFC-2087). In order to ease administrating large user bases, the quota search extension allows administrator\\nto retrieve all users whose email usages are exceeding a given occupation ratio.\\nWhen searching for users by quota ratio if we set the value of the parameters to 0, for example: `\/quotas\/users?minOccupationRatio=0&maxOccupationRatio=0`, the search feature is supposed to return newly created users\\nwho have not received any email yet at that point. However, this is not the case because the quotas are currently being initialized only after\\na user has received the first email.\\nWe need to initialize user quotas upon user creation time. The problem is: there is currently no event at user creation\\nand since the quota-search feature is a plugin of James, it cannot be hardwired into the domain logic of user management to initialize the quota for a just created user.\\n","Decision":"For quota-search to be initialized\/removed for a given user while keeping this feature as a plugin, we decided to adopt the Event Driven pattern we already use in Mailbox-api.\\nWe can create new events related to user management (UserCreated, UserRemoved and so on).\\nTo achieve that, we will extract the EventBus out of mailbox-api in order to make it a utility component (eventbus-api), then we will make both mailbox-api and data-api depend on that new module.\\n","tokens":207,"id":2868}
{"File Name":"james-project\/0002-make-taskmanager-distributed.md","Context":"## Context\\nIn order to have a distributed version of James we need to have an homogeneous way to deal with `Task`.\\nCurrently, every James nodes of a cluster have their own instance of `TaskManager` and they have no knowledge of others, making it impossible to orchestrate task execution at the cluster level.\\nTasks are scheduled and ran on the same node they are scheduled.\\nWe are also unable to list or access to the details of all the `Task`s of a cluster.\\n","Decision":"Create a distribution-aware implementation of `TaskManager`.\\n","tokens":102,"id":2874}
{"File Name":"the-zoo\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard\\nin this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":494}
{"File Name":"stentor\/0002-use-toml-for-config-file.md","Context":"## Context\\n`stentor` needs to pick a config file format.\\nThe options under consideration are yaml and toml.\\nBoth options are human readable and writable,\\nallow for easy parsing of structured data,\\nand are supported by well maintained libraries.\\nyaml has the benefit of being more straightforward to write,\\nespecially for nested structures.\\nHowever, toml is intended for config files,\\nand provides stricter parsing out of the box.\\n","Decision":"`stentor` will use toml for its config file.\\n","tokens":93,"id":2164}
{"File Name":"interlok\/0007-jetty-failsafe.md","Context":"## Context and Problem Statement\\nThere are 2 ways to enable the jetty management component in bootstrap.properties. The first bootstraps jetty using the configured XML configuration file, the second bootstraps a jetty server with minimal defaults; it also doesn't by default deploy the webapp provider which means that the UI is not started (unless the optional `webServerWebappUrl` is specified). We have always considered the `FromProperties` variant to be not production ready and a message is logged to that effect on startup.\\n```\\nmanagementComponents=jmx:jetty\\nwebServerConfigUrl=.\/config\/jetty.xml\\n","Decision":"Embed the jetty.xml into interlok-common.jar and use it if jetty is enabled but no other settings are defined.\\n","tokens":130,"id":2347}
{"File Name":"unruly-puppet\/0002-standalone-nrpe-custom-check-module.md","Context":"## Context\\nWe need to add new functionality to the project - the ability to create custom NRPE checks.\\nThis means having templated configuration for adding service-specific plugins for the purposes of monitoring.\\nThe `base` module already contains NRPE specific code.\\nIt installs NRPE and ensures it is running. It also configures a set of default monitoring plugins.\\n","Decision":"We considered three possible approaches to adding the new NRPE functionality:\\n- Add it to the `base::nrpe`\\n- Create a separate `nrpe_custom_checks` module and keep `base` the way it is\\n- Create a separate `nrpe_custom_checks` module and refactor `base` to use it for check creation\\nRather than extending the base NRPE class, we chose to create a standalone NRPE custom checks module and keep `base` independent.\\n","tokens":75,"id":1268}
{"File Name":"cf-k8s-networking\/0003-tagging-and-publishing-docker-images.md","Context":"## Context\\n(\ud83c\udf83 Happy Halloween \ud83d\udc7b)\\nWe need to have a way of creating and deploying container images of our software for development and CI use.\\nWe didn't want to break the `latest` image tag every time we did local development and we need to have CI deploy a consistent image between runs.\\n","Decision":"### Local development\\nEach development environment pipeline deploys off a dedicated docker tag.  e.g. `eirini-dev-1` environment deploys the\\ndocker image tagged `eirini-dev-1` so when developing locally (i.e. without pushing to Git)\\nwe can tag and push images with that dedicated tag and redeploy easily.\\nExample Workflow:\\n```bash\\nenvironment_name=eirini-dev-1\\ndocker tag $img gcr.io\/cf-routing\/cf-k8s-networking\/cfroutesync:$environment_name\\ndocker push gcr.io\/cf-routing\/cf-k8s-networking\/cfroutesync:$environment_name\\n```\\n### Branch development\\nA github action will trigger on pushes to all branches and publish a Docker image tagged with the git SHA and the branch name.\\n#### Develop branch\\nWhen we push to the `develop` branch we will tag the image with the git SHA, branch name, and `latest`.\\n","tokens":67,"id":2241}
{"File Name":"push-sdk-android\/0004-divorce-the-sdk-from-firebase.md","Context":"## Context\\nThere was much discussion over whether the SDK should implement the\\nFirebase-related services for token refresh and notification handling, or leave\\nthat up to the consuming app. If the SDK handles it, it's less setup for the\\nconsumer; however, this comes at the cost of much flexiblity.\\n","Decision":"In the end, we decided that the lack of flexibility (e.g. for supporting both\\n\"register on first launch\" and \"register\/unregister on log in and log out\"\\nmodels) warranted removing the services from the SDK. They will be implemented\\nin the demo application instead.\\n","tokens":65,"id":5042}
{"File Name":"orcid-client\/0002-generated-swagger-models.md","Context":"## Context\\n- Using the OrcidAPI we get back complex and deeply nested JSON objects.\\n- Modeling each API endpoint by hand is time consuming.\\n","Decision":"We will use models for these objects generated by the swagger-codegen code\\ngeneration tool: https:\/\/github.com\/swagger-api\/swagger-codegen.\\nWe will run the generator once, and add the resulting code to the lib directory. We don't intend to\\nmaintain any changes to the generator that were required to produce these\\nmodels.\\n","tokens":32,"id":3312}
{"File Name":"govuk-kubernetes-discovery\/0004-structure-for-terraform-projects.md","Context":"## Context\\nWe wanted to agree on our Terraform code organisation to manage resources in different stacks and\\navoid having to recreate things every time we refactor code.\\n","Decision":"- We want to separate code from data, so in the future we can opensource the code without disclosing our implementation details\\n- We want to be able to encrypt sensitive data in the repository: we want to support sensitive data encryption as part of the same\\nprocess, without having to manage secrets in a different repository, with different scripts, etc.\\n- We want to create Terraform modules to reuse code\\n- We want to separate Terraform code into different projects (stacks, tiers), each one representing a logical tier. This is specially\\nimportant to separate resources between GOV.UK applications.\\nThe initial solution presents three directories: data, modules and projects:\\n- The data directory contains a subdirectory per Terraform project, to store variable values that can be customised per environment.\\n- The data directory also contains \\_secrets files with sensitive data encrypted with 'sops'\\n- The modules directory contains a subdirectory per Terraform provider\\n- The projects directory contains the Terraform stacks\/tiers\\n```\\n\u251c\u2500\u2500 data\\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 gke-base\\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 common.tfvars\\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 integration.tfvars\\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 gke-cluster\\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 common.tfvars\\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 integration.tfvars\\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 my-application\\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 common.tfvars\\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 integration.tfvars\\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 integration_secrets.json\\n\u251c\u2500\u2500 modules\\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 google\\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 container_cluster\\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 main.tf\\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2514\u2500\u2500 variables.tf\\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 dns_managed_zone\\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2514\u2500\u2500 main.tf\\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 mysql_database_instance\\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 mysql.tf\\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2514\u2500\u2500 variables.tf\\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 network\\n\u2502\u00a0\u00a0         \u251c\u2500\u2500 network.tf\\n\u2502\u00a0\u00a0         \u2514\u2500\u2500 public_subnetwork\\n\u2502\u00a0\u00a0             \u2514\u2500\u2500 main.tf\\n\u2514\u2500\u2500 projects\\n\u251c\u2500\u2500 gke-base\\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 integration.backend\\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 main.tf\\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 variables.tf\\n\u251c\u2500\u2500 gke-cluster\\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 integration.backend\\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 main.tf\\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 variables.tf\\n\u2514\u2500\u2500 my-application\\n\u251c\u2500\u2500 integration.backend\\n\u251c\u2500\u2500 main.tf\\n\u2514\u2500\u2500 variables.tf\\n```\\n","tokens":33,"id":2794}
{"File Name":"dos-capacity-status-api\/008-authentication_and_authorisation_revisited.md","Context":"## Context\\nThe Capacity Status API will enable UEC service providers and DoS Leads to change the RAG status (also known as the Capacity Status) of their services in DoS.\\nTo protect from unauthenticated users and to prevent authenticated users from being able to update capacity status information for services that they do not have permissions for, the Capacity Status API will need to authenticate users via a modern authentication approach, and will need to establish whether the user is authorised to be able to update the capacity status information of the service.\\n","Decision":"The two concerns here are authentication and authorisation.\\n### Authentication\\nAuthentication of the API is concerned with protecting the API against unauthenticated, unknown, or inactive users. A couple of options were discussed:\\n- API Key authentication\\n- OAUTH2 authentication\\nFor the purposes of the private Beta involving a known (and limited) user base, it was originally decided that we would implement API Key authentication using a Django authentication extension application. However, as we got deeper into the development, it became evident that this solution wasn't what we required. Despite the naming of the API Key Authentication application, we found that this application was really an 'authorization' mechanism and not an 'authentication' mechanism. Thus the application hooked into the Django Rest Framework (DRF) via its permission classes (rather than via its authentication classes). This brought with it a number of concerns:\\n- Usability - Because the application was hooked into DRF's permission classes, an HTTP 403 - Forbidden response was returned from the API if it was sent a request with either no, or invalid credentials. Our desired outcome is for an HTTP 401 - Unauthorized response to be given.\\n- Security - More importantly, because this application is not hooked into the DRF's authentication classes, the question was raised as to how secure this mechanism really was.\\nAlthough we could have overridden the DRF framework and have it raise am HTTP 401 instead of an HTTP 403, this still wouldn't have addressed our concerns over how secure this mechanism actually was. It was therefore deemed that this mechanism was not right for us.\\nAfter some more investigation, we found a token based authentication application. This authentication application provided us with everything that the original API Key application provided, and it also hooked into the DRF's authentication classes. Because of this the DRF responded with the desired HTTP 401 - Unauthorized response on receipt of a request containing no or invalid credentials.  Moreover, as far as our users of the API are concerned, there is no difference in process between this token based authentication mechanism and the originally proposed API Key authentication mechanism. The only difference is a technical one whereby the authentication header in the request needs to be prefixed with the text 'Token', rather than 'API Key'.\\nThis decision will be re-evaluated should the API move into the next phase of development (public beta).\\n### Authorisation\\nAuthorisation of the API is concerned with preventing authenticated users from being able to update capacity status information on services that they do not have permission for. It was discussed that Core DoS already manages the permissions of which services a user has permissions to update. The decision was made to leverage this information stored in Core DoS rather than managing and maintaining a separate set of the permissions within the API itself. Although arguably this creates a dependency on Core DoS for the API, it means that there is only one place to hold the permissions, and ensures both Core DoS and the API are always aligned.\\n","tokens":107,"id":3929}
{"File Name":"unruly-puppet\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":1269}
{"File Name":"Maud\/0017-posterior-predictive-sampling.md","Context":"## Context\\nAfter sampling from the posterior distribution users may want the option\\nof validating their results against experimental data or to predict\\ncellular behaviour using a trained kinetic model.\\n","Decision":"Posterior predictive samples will be implemented using the posterior\\ndraws for kinetic and thermodynamic parameters. The boundary conditions\\nfrom the predicted experiments will be sampled from their marginal distributions.\\nThe number of samples will be limited to the number of samples from the posterior\\ndistribution.\\nIn order to define what is part of the training set and prediction set a new\\nfile will be introduced called experiment_metadata. A toml file where the training\\nprediction split will be defined under the headers [\"training\"], and [\"prediction\"]\\nrespectively.\\nA new stan file will be made where there is no model block (minimal if required).\\nIn the generated_quantities (gqs), we will iterate through the draws from a previously\\ngenerated set of csvs. the drains, conc_enzymes, and conc_unbalanced_metabolites\\nwill be sampled from their priors using the functions normal_rng() and lognormal_rng()\\nfor their respective generative distributions.\\nlog_probabilities should be calculated if measurements are also included for the\\nprediction experiments.\\n","tokens":36,"id":236}
{"File Name":"cljdoc\/0009-introduce-server-component.md","Context":"## Context\\nWhile most of cljdoc's publishing pipeline could be ran with\\n\"job-runners\" (e.g. CircleCI, AWS Lambda) the isolation provided by\\nthose isn't required for many parts of the process.\\nFurthermore some API to trigger documentation builds will be required\\nat some point and this API will need to hold some credentials, e.g.\\nAWS keys for S3 and CircleCI API tokens. Exposing access to job-runners\\ndirectly would make it harder to implement things like rate-limiting etc.\\nAn API may also be useful to expose further utility endpoints, like\\nways to list available versions or download the metadata for a given\\nnamespace or artifact.\\n","Decision":"We will implement a server component which has the following responsibilties:\\n- be a central API to end users to request documentation builds\\n- hold credentials for various services which should not be exposed in analysis sandbox\\n- manage flow of documentation builds\\n- trigger analysis of jars\\n- receive webhooks about completed analysis\\n- populate Grimoire store using analysis results and a project's Git repository\\n- build HTML documentation based on data in Grimoire store\\n- deploy HTML documentation to S3\/Cloudfront\\n","tokens":145,"id":2605}
{"File Name":"lcarsde\/identification-of-special-tool-windows.md","Context":"## Context and Problem Statement\\nSome tool windows like the status bar and the side bar menu need special placement in the UI and therefore special treatment by the window manager. The window manager must be able to identify them to treat them accordingly.\\n","Decision":"Chosen option: \"Client Properties \/ Atoms\", because it is the most reliable and side-effect free way to identify the tool windows.\\n","tokens":48,"id":145}
{"File Name":"gti-genesearch\/adr-015.md","Context":"## Context\\nEVA discourage direct use of the Mongo instance on the grounds:\\n1. The schema is not stable\\n2. The schema is complex e.g. mapping samples to genotypes\\nGiven this, a new Search implementation is needed that uses the following endpoints:\\nhttps:\/\/www.ebi.ac.uk\/eva\/webservices\/rest\/swagger-ui.html#!\/variants\/getVariantsByRegionUsingPOST\\nhttps:\/\/www.ebi.ac.uk\/eva\/webservices\/rest\/swagger-ui.html#!\/variants\/getVariantByIdUsingGET\\n","Decision":"A new `Search` implementation has been developed, `EVARestSearch`.\\nThis search needs to take care of the following tasks\\n### Querying\\nQuery fields need to be split up into the following classes:\\n1. Mandatory - ID or sequence region plus species_assembly\\n2. Optional server filters\\n3. Optional client filters\\nThese need to be applied in the correct way to the correct endpoint.\\nThe species_assembly string will be looked up from an Ensembl style genome name, as to support joining better.\\n### Filtering\\nThe server only excludes a subset of fields, so most exclusion must take place on the server\\n### Faceting\\nCannot be supported\\n### Sorting\\nCannot be supported\\n","tokens":106,"id":3270}
{"File Name":"java-template\/0002-hexagonal-architecture.md","Context":"## Context and Problem Statement\\n- How can we maintain a clean design?\\n- How will we be able to maintain our architecture iteratively in accordance with the TDD practice?\\n## Decision Drivers\\n* We employ TDD, which favors small iterations.\\n* We aim to implement full Continuous Delivery.\\n","Decision":"* We employ TDD, which favors small iterations.\\n* We aim to implement full Continuous Delivery.\\n","tokens":63,"id":4805}
{"File Name":"read-more-api\/0004-use-dapper-for-data-access.md","Context":"## Context\\nMany ASP.NET applications use [Entity Framework (EF)](https:\/\/docs.microsoft.com\/en-us\/ef\/), an Object Relational Mapper (ORM) that helps access data stored in database.\\nEF allows data in a database to be accessed by extending a DbContext class and adding properties to this extending class of type DbSet. DbContext and DbSet provide methods for performing basic CRUD operations against entities in a database that are defined in model classes. These model classes contain annotations that define the table name, columns and relationships with other entities. When a query is performed, EF handles creating instances of model classes and filling them with the received data.\\nSome properties are lazily loaded, with the queries related to fetching the required data only being run when thoses properties are accessed. This approach is commonly used when accessing a property representing a relationship with another entity.\\nA DbContext by default tracks changes to entities returned as the result of queries, with changes being saved when a call is made to a DbContext's SaveChanges or SaveChangesAsync methods.\\nThe DbContext and DbSet classes provide methods that can be used to fetch data, with the ability to apply limitations on what data is returned. EF will generate the required query, execute it, parse the response data and return the appropriate entity model instances.\\nEF supports migrations written as classes with Up and Down methods, to support upgrading and rolling back, respectively. These methods are implemented by adding calls to a provided MigrationBuilder instance.\\nDapper is a library that is commonly referred to as a \"micro-ORM\". It provides methods to support executing SQL queries and parsing the results to create instances of particular model classes. Unlike EF, Dapper does not support the tracking of changes and queries must be written using SQL.\\nDapper was developed for the StackOverflow website to address performance issues, as outlined in [this blog post](https:\/\/samsaffron.com\/archive\/2011\/03\/30\/How+I+learned+to+stop+worrying+and+write+my+own+ORM).\\n","Decision":"We will use Dapper with the [repository pattern](http:\/\/blog.mantziaris.eu\/blog\/2016\/10\/24\/the-repository-and-unit-of-work-pattern\/) to access data stored in the database.\\n","tokens":416,"id":727}
{"File Name":"http-transport\/adr-002-middleware.md","Context":"## Context\\n[Flashheart](https:\/\/github.com\/bbc\/flashheart), although useful, has become difficult to maintain and extend due to features being coupled into the same client. HttpTransport mitigates this by using `middleware` to extend the Rest clients behaviour. Middleware is comparable to a `plugin` based artitecture. This allows users to add or change behaviour without having to make changes to the core client. This conforms to the [open\/closed principle](https:\/\/en.wikipedia.org\/wiki\/Open\/closed_principle).\\n","Decision":"We have decided to use [Koa](https:\/\/github.com\/koajs\/koa) middleware via the [Koa compose](https:\/\/github.com\/koajs\/compose) library, rather than creating our own custom implementation. We opted to use this library because:\\n* It's a well tested library used extensively in production environments\\n* Aids the implementation of caching layers (see example above)\\n* Familiar syntax (express\/Koa)\\n* Supports async\/await\\n### Example middlware stack\\n```js\\nasync function middleware1(ctx, next) {\\nconst req = ctx.res \/\/ handle request\\nawait next();       \/\/ invokes the next middleware\\nconst res = ctx.res \/\/ handle response\\n}\\n\/\/ etc ...\\nasync function middleware2() {}\\nasync function middleware3() {}\\n\/\/ register using `.use`\\nhttpTransport\\n.use(middleware1);\\n.use(middleware2);\\n.use(middleware3);\\n```\\nThis would unwind the `stack` in the same way as Express\/Koa does:\\n```\\n1st middleware ---> 2nd middleware ---> 3rd middleware ---> HTTP request\\n|\\n|\\nv\\n1st middleware <--- 2nd middleware <--- 3rd middleware <--- HTTP response\\n```\\nThis aids with modules such as caching with transformations in between:\\nCaching middleware:\\n```js\\nfunction modifyHeaders(req, res, next) {}\\nfunction redisCache(req, res, next) {}\\nhttpTransport.use(modifyHeaders)\\n.use(redisCache);\\n```\\nMiddleware execution order:\\n```\\nmodifyHeaders ---> redisCache ---> HTTP request\\n|\\n|\\nv\\nmodifyHeaders <--- redisCache <--- HTTP response\\n```\\nThis ensures the Caching module caches the request as it enters the pipeline and requires the minimum amount of processing to recreate the cache key despite the transport modifying it further.\\n### Terminating the middleware chain\\nTerminating a chain is achieved by suppressing the call to `next()`\\n```js\\nasync function cachingMiddleware(ctx, next) {\\nconst req = ctx.res\\nif (isCached(req)) {\\nreturn;\\n}\\nawait next(); \/\/ call next middleware, allowing chain to continue\\nconst res = ctx.res\\n\/\/ handle setting caching response\\n}\\n```\\n","tokens":107,"id":5060}
{"File Name":"smarthub\/0006-rename-assets-to-devices.md","Context":"## Context\\nThere has been confusion on why we call generating and consuming devices Assets in Smarthub. We have noticed that most Smarthub users would refer to Assets as Devices and not Assets.\\n","Decision":"We decided to change our terminology to refer to Assets as Devices, to keep in line with the prevailing terminology in the industry.\\n","tokens":41,"id":3688}
{"File Name":"nada-kafkarator\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":867}
{"File Name":"terraform-documentation\/0002-pull-acs-info-out-of-modules.md","Context":"## Context and Problem Statement\\nUsing acs-info inside our standard and component modules is creating pain. Does it make more sense to expect products to pass the necessary variables in, allowing them to use acs-info if they want?\\n## Decision Drivers <!-- optional -->\\n* Terraform's lack of optimization with modules and data sources\\n* Usage of modules in non-OIT accounts\\n* We want our modules to be simple to use\\n* We want development teams to understand the infrastructure of their products.\\n","Decision":"* Terraform's lack of optimization with modules and data sources\\n* Usage of modules in non-OIT accounts\\n* We want our modules to be simple to use\\n* We want development teams to understand the infrastructure of their products.\\nChosen option: \"Pull acs-info out of modules\", because it was the best option based on an analysis of pros and cons (see below).\\n<!-- ### Positive Consequences optional -->\\n<!-- * TBD -->\\n<!-- ### Negative Consequences optional -->\\n<!-- * TBD -->\\n","tokens":103,"id":2857}
{"File Name":"api-docs\/0006-version-based-directory-and-path.md","Context":"## Context\\nOld or specific versions of a documentation can not be accessed.\\nOnly the actual version exists online. Every project's documentation\\nis in the same directory, no structure.\\n","Decision":"Create project specific directories under the `\/_posts` directory and\\ncreate version numbered directories e.g.: `\/_posts\/android\/1.0`,\\n`\/_posts\/ios\/2.1`. Put each documentation file (`README.md`, `CHANGELOG.md`, ...)\\ninside the version numbered directory.\\nCreate one YAML Front Matter index file for each product which points\\nto the latest version, see example below where latest version is `0.5`.\\nUse the `permalink: \/player-sdk\/android\/latest` key to specify where\\nthe latest documentation is accessible online, like:\\nhttp:\/\/developers.ustream.tv\/player-sdk\/android\/latest\\n```\\n\/_posts\/android\/2017-01-01-readme.md\\n---\\nlayout: markdown\\ntitle: Player SDK for Android (v0.5)\\nweight: 30\\ncategory: player-sdk\\ncategoryItemType: documentation\\ncategoryItemIsShown: 1\\ncategoryItemWidth: 6\\ncategoryItemDescription:\\ncategoryItemLabel: Read the documentation\\npermalink: \/player-sdk\/android\/latest\/\\n---\\n{% include_relative 0.5\/README.md  %}\\n```\\nChange the `include_relative` setting to include the latest version.\\nUse the `categoryItemIsShown: 1` setting to direct Jekyll to show\\nthis document when listing category contents.\\nCreate a YAML Front Matter index file in each version numbered directory\\nin which the version is specified, example:\\n```\\n\/_posts\/android\/0.5\/2017-02-02-readme.md\\n---\\nlayout: markdown\\ntitle: Player SDK for Android (v0.5.x)\\nweight: 3\\ncategory: player-sdk\\ncategoryItemType: documentation\\ncategoryItemIsShown: 0\\ncategoryItemWidth: 6\\ncategoryItemDescription:\\ncategoryItemLabel: Read the documentation\\npermalink: \/player-sdk\/android\/0.5\/\\n---\\n{% include_relative README.md  %}\\n```\\nUse the `categoryItemIsShown: 0` to hide this version from category listing,\\nas only the latest should be listed.\\nPrevious version can be accessed online using urls like:\\n* http:\/\/developers.ustream.tv\/player-sdk\/android\/0.4\/\\n* http:\/\/developers.ustream.tv\/player-sdk\/android\/0.5\/\\n* http:\/\/developers.ustream.tv\/player-sdk\/android\/latest\/ - which only points to `0.5`\\n","tokens":38,"id":5150}
{"File Name":"gp-finder\/0004-use-elasticsearch-for-search.md","Context":"## Context\\nFollowing user research a more comprehensive text search engine was needed in order to be able to\\nuse more complex searches (and\/or), full phrase search, stemming as well as geolocation searches.\\n","Decision":"A review of ElasticSearch and Funnelback was undertaken [here](https:\/\/docs.google.com\/document\/d\/10xK0on4gzrB1ImZHk3Z_PoCr7V_aUpiieuLdQDg4OwQ\/edit)\\nand [ElasticSearch](https:\/\/www.elastic.co\/products\/elasticsearch) was chosen as a candidate due to the fact that it was\\nopen source and had comprehensive documentation with an established community around it.\\n(Funnelback)[https:\/\/www.funnelback.com\/] was discounted due to it being closed-source and for\\nit's lack of community support.\\nA  Docker image of GP data ([profiles-db-elastic](https:\/\/hub.docker.com\/r\/nhsuk\/profiles-db-elastic\/))\\nrunning an ElasticSearch instance has been created for use by this application and possibly by other applications.\\n","tokens":42,"id":3472}
{"File Name":"embvm-core\/0010-dispatch-callbacks.md","Context":"## Context\\nBecause we are building an [event-driven framework], we need to think about how to handle callback functions. We want to use them without causing threads\/functions to block unexpectedly while executing callbacks.\\n","Decision":"We will dispatch callbacks to a global dispatch queue which will execute them as processing time is available.\\n","tokens":41,"id":3009}
{"File Name":"fixcity\/0002-sys-pick-documentation-language.md","Context":"## Context\\n- We have polish development team, so all business related communication is in polish,\\n- All team members prefer to write code in english,\\n- There is not much business facing documentation (BFD).\\n","Decision":"- All BFD and artifacts will be in english (ES sessions, System Context diagram, e.t.c),\\n- All developer facing documentation and artifacts will be in english (Code, ADR, C2-C3 diagrams, e.t.c).\\n","tokens":45,"id":1796}
{"File Name":"tdr-dev-documentation\/0012-keycloak-email-provider.md","Context":"## Context\\nSet up Keycloak email provider to be able to send transferring bodies emails for updating passwords, and other Keycloak based user actions.\\n","Decision":"Decided to go with GovUK Notify as the email provider because:\\n* Re-using existing UK Government service.\\n* Ability to leverage GovUK branding to create specific TDR branding for emails.\\n* Free service.\\n* 24 hour support.\\n* NCSC compliant service.\\n* Relatively easy to integrate into Keycloak.\\n* Supports SMS. Might make things easier if there is a need to implement MFA via text messages.\\n### Disadvantages to GovUK Notify\\n* Logging not integrated with TDR application logging on AWS.\\n* Another account to manage for TDR.\\n### Documentation\\nSee here for further information on GovUK Notify: https:\/\/www.notifications.service.gov.uk\\n","tokens":30,"id":1774}
{"File Name":"claim-additional-payments-for-teaching\/0006-use-dependabot-for-dependent-library-vulnerability-checking.md","Context":"## Context\\nWe want to ensure we are made aware of vulnerabilities that are discovered in\\nthird-party open source libraries the application uses and can quickly update\\ndependent libraries to secure versions so that we can be confident we are not\\nopen to known security threats.\\n","Decision":"We will run [Dependabot](https:\/\/dependabot.com), a third-party service that\\nautomatically checks for known vulnerabilities and automatically creates Pull\\nRequests against the codebase to update vulnerable dependencies.\\n","tokens":55,"id":2092}
{"File Name":"atlasdb\/0005-stop-allowing-embedded-lock-and-timestamp-services-in-production.md","Context":"## Context\\nCurrently a completely normal way to set up AtlasDB is to run an embedded lock and timestamp service that only your atlas client talks to in Java. If this happens with 2 different clients, then they can easily clash with each other and cause undefined issues in the KVS. It also stops CLIs that require knowledge of lock and timestamp being able to run against these atlas clients.\\nThere is no reason for this situation to arise in production. Lock and Timestamp services can still be deployed in a sensible fashion without spinning up a separate service by registering the lock and timestamp server endpoints in your application, and having your own nodes act as the servers. Alternatively, you can spin up an completely external lock and timestamp service whose whole reason for existence is to perform these roles.\\n","Decision":"1. Remove the ability to run embedded lock and timestamp services in configuration\\n2. Provide an easy to run Timelock Server which provides lock and timestamp services to applications\\n3. Provide a sensible development story for testing against a TimelockServer that can be started from Java.\\n","tokens":157,"id":3095}
{"File Name":"modular-monolith-with-ddd\/0012-use-domain-driven-design-tactical-patterns.md","Context":"## Context\\nWe decided to use the Clean Architecture ([ADR #10](0010-use-clean-architecture-for-writes.md)) and create Rich Domain Models ([ADR #11](0011-create-rich-domain-models.md)) for each module. We need to define or use some construction elements \/ building blocks to implement our architecture and business logic.\\n","Decision":"We decided to use **Domain-Driven Design** tactical patterns. They focus on the Domain Model implementation. Especially we will use the following building blocks:\\n- Command - public method on Aggregate (behavior)\\n- Domain Event - the immutable class which represents important fact occurred on a special point of time (behavior)\\n- Entity - class with identity (identity cannot change) with mutable attributes which represents concept from domain\\n- Value Object - immutable class without an identity which represents concept from domain\\n- Aggregate - cluster of domain objects (Entities, Value Objects) with one class entry point (Entity as Aggregate Root) which defines the boundary of transaction\/consistency and protects business rules and invariants\\n- Repository - collection-like abstraction to persist and load particular Aggregate\\n- Domain Service - stateless service to execute some business logic which does not belong to any of Entity\/Value Object\\n","tokens":70,"id":898}
{"File Name":"celestia-node\/adr-002-predevnet-core-to-full-communication.md","Context":"## Context\\nAfter the offsite, there was a bit of confusion on what the default behaviour for running a Celestia Full node should be in the devnet. Since we decided on an architecture where Core nodes will communicate with Celestia Full nodes *exclusively* via RPC (over an HTTP connection, for example), it is necessary that a certain percentage of Celestia Full nodes in the devnet run with either an embedded Core node process or are able to fetch block information from a remote Core endpoint, otherwise there would be no way for the two separate networks (Core network and Celestia network) to communicate.\\n","Decision":"Since the flow of information in devnet is unidirectional, where Core nodes provide block information to Celestia Full nodes, the default behaviour for running a Celestia Full node is to have an embedded Core node process running within the Full node itself. Not only will this ensure that at least some Celestia Full nodes in the network will be communicating with Core nodes, it also makes it easier for end users to spin up a Celestia Full node without having to worry about feeding the Celestia Full node a remote Core endpoint from which it would fetch information.\\nIt is also important to note that for devnet, it should also be possible to run Celestia Full nodes as `standalone` processes (without a trusted remote or embedded Core node) as Celestia Full nodes should also be capable of learning of block information on a P2P-level from other Celestia Full nodes.\\n","tokens":129,"id":2177}
{"File Name":"court-case-service\/0004-nginx-caching.md","Context":"## Context\\nThe performance of the case list page is currently reasonably slow on account of the complex database queries it entails. It's also central to the user journey and thus has high traffic whilst receiving few updates for most of the day. This makes it a good candidate for caching which should hopefully significantly improve UX whilst reducing load on the court-case-service.\\n","Decision":"An nginx reverse-proxy will be introduced in front of the court-case-service which will act as a cache. We will make a cheap fast query to determine the last modified date of a case list and returning this in a Last-Modified header, the client (in this case an nginx proxy) can provide this timestamp back to us in an If-Modified-Since header which court-case-service can then use to return a 304 Not Modified status in the case where no changes have been made to that case list. Nginx will then serve a cached response having validated it is still fresh.\\n","tokens":71,"id":3581}
{"File Name":"akvo-product-design\/ADR-003.md","Context":"# Context\\nAkvo DASH is a system that allows the user to connect to different _data\\nsources_ and import the data. After that user must be able to transform\\nit (clean, aggregate, etc).\\nWe have been looking at different open source ETL frameworks like:\\n* [Pentaho Data\\nIntegration](http:\/\/www.pentaho.com\/product\/data-integration)\\n* [Clover ETL](http:\/\/www.cloveretl.com\/)\\n* [Onyx Platform](http:\/\/www.onyxplatform.org\/about.html)\\n* [Bubbles](http:\/\/bubbles.databrewery.org\/)\\n* [Kiba ETL](http:\/\/www.kiba-etl.org\/)\\nSome of them provide a GUI to build transformations, others require\\ncoding.\\nThere are other ones that are based on Hadoop ecosystem, which is really\\ntoo much for our current needs:\\n* [Luigi](https:\/\/luigi.readthedocs.org\/en\/stable\/)\\n* [Oozie](https:\/\/oozie.apache.org\/)\\n* [Azkaban](https:\/\/azkaban.github.io\/)\\n# Decision\\nBased on the skills of the team (Clojure expertise) and the fact Clojure\\nexcels at data transformation. We have decided that a small ad-hoc\\nfunctions for handling the import and transformation is enough for our\\ncurrent needs.\\nDepending on requirements we'll use a scheduling library like\\n[Quarzite](https:\/\/github.com\/michaelklishin\/quartzite) for scheduling\\nimports.\\n# Status\\nAccepted\\n# Consequences\\n* The current approach will be to create ad-hoc functions to handle\\nimports _(extract)_ from different data sources\\n* If we need a HA setup, there is an easy transition to Onyx Platform.\\nSee the\\n[conversation](https:\/\/gist.github.com\/iperdomo\/7af984b9f32c117678de) with Onyx author\\n","Decision":"Based on the skills of the team (Clojure expertise) and the fact Clojure\\nexcels at data transformation. We have decided that a small ad-hoc\\nfunctions for handling the import and transformation is enough for our\\ncurrent needs.\\nDepending on requirements we'll use a scheduling library like\\n[Quarzite](https:\/\/github.com\/michaelklishin\/quartzite) for scheduling\\nimports.\\n# Status\\nAccepted\\n# Consequences\\n* The current approach will be to create ad-hoc functions to handle\\nimports _(extract)_ from different data sources\\n* If we need a HA setup, there is an easy transition to Onyx Platform.\\nSee the\\n[conversation](https:\/\/gist.github.com\/iperdomo\/7af984b9f32c117678de) with Onyx author\\n","tokens":418,"id":2803}
{"File Name":"docs\/0001-java-framework.md","Context":"## Context and Problem Statement\\nWe want to use a well established Java framework in order to bootstrap and boost our development process.\\n","Decision":"We chose option 1, Spring Boot, since existing knowledge and experience with this framework is available.\\nTo reduce training costs (time) we chose this option.\\n","tokens":26,"id":4670}
{"File Name":"beis-report-official-development-assistance\/0021-use-lograge-gem.md","Context":"## Context\\nRODA uses Papertrail for logging. Our Papertrail account has a log data\\ntransfer limit of 50 MB. Extending this limit means moving to another tier\\non Papertrail's platform, which we would prefer to avoid.\\n","Decision":"- Reduce the default logging level on production to `info` instead of `debug`\\n- Use the [lograge gem](https:\/\/github.com\/roidrage\/lograge) to turn Rails'\\ndefault multiline logs into a single line, without losing information or\\ncontext\\n","tokens":51,"id":2380}
{"File Name":"the-zoo\/0002-use-black-code-formatter.md","Context":"## Context\\nWe would like to use code formatter to standardize code look so we stop bothering about it\\nin code reviews and focus on how code actually works.\\n","Decision":"We will use Black: https:\/\/github.com\/ambv\/black\\n","tokens":34,"id":490}
{"File Name":"reading-todo\/004-kubernetes-with-docker-for-deployment.md","Context":"## Context\\nFor the project a deployment and development mechanism is need to be chosen and adopted.\\n","Decision":"- [Kubernetes](https:\/\/kubernetes.io\/) will be the deployment technology of choice.\\nThe project will be deployed into a kubernetes cluster using [helm](https:\/\/helm.sh\/).\\n- [Docker](https:\/\/www.docker.com\/) will be the container building technology of choice.\\nEvery thing will be directed through docker containers, from development to deployment. CI Pipeline will run inside a docker container as well.\\n","tokens":20,"id":1061}
{"File Name":"mercury-platform\/0004-mercury-platform-is-deliberately-a-polyglot-system-with-language-suggestions.md","Context":"## Context\\nMercury platform could be accomplished by a handful of python scripts run locally on a consumer system.\\nHowever, the project is intended to also be a learning platform that uses the strengths of individual languages and cloud native services\\n","Decision":"Mercury platform will use multiple languages to achieve it's goal.\\nThe proposed list:\\n- Data retrieval and manipulator (Wrangler): Elixir\\n- Data processor and analyzer: Python\\n- Controller for end user input: Go\\n- Infrastructure: Pulumi via TypeScript\\nLanguages will be changed out as we discover more about each language's limitation with regard to our needs.\\n","tokens":47,"id":3810}
{"File Name":"compliantkubernetes\/0017-persist-dex.md","Context":"## Context and Problem Statement\\n> Dex requires persisting state to perform various tasks such as track refresh tokens, preventing replays, and rotating keys.\\nWhat persistence option should we use?\\n## Decision Drivers\\n* CRDs add complexity\\n* Storage adds complexity\\n* We want to frequently reboot Nodes for security patching\\n* We want to deliver excellent user experience\\n","Decision":"* CRDs add complexity\\n* Storage adds complexity\\n* We want to frequently reboot Nodes for security patching\\n* We want to deliver excellent user experience\\nChosen option: \"use CRD-based storage\", because it improves user experience when Nodes are rebooted.\\nWith \"memory\" storage, Dex loses the OpenID keys when restarted, which leads to the user being forced to eventually re-login. Worst off, this forced re-login happens unexpectedly from the user's perspective, when the Kubernetes apiserver chooses to refresh the OpenID keys.\\nHere is the experiment to illustrate the issue:\\n```console\\n$ curl https:\/\/dex.$DOMAIN\/.well-known\/openid-configuration > before-openid-configuration.json\\n$ curl https:\/\/dex.$DOMAIN\/keys > before-keys.json\\n$ kubectl delete pods -n dex -l app.kubernetes.io\/instance=dex\\n$ curl https:\/\/dex.$DOMAIN\/.well-known\/openid-configuration > after-openid-configuration.json\\n$ curl https:\/\/dex.$DOMAIN\/keys > after-keys.json\\n$ diff -y before-openid-configuration.json after-openid-configuration.json\\n[empty output, no differences]\\n$ diff -y before-keys.json after-keys.json\\n[all keys are replaced]\\n```\\n### Positive Consequences\\n* Nodes which host Dex can be rebooted for security patching\\n* User experience is optimized\\n### Negative Consequences\\n* Dex will have a more permissions in the Service Cluster (see [`rbac.yaml`](https:\/\/github.com\/dexidp\/helm-charts\/blob\/dex-0.6.3\/charts\/dex\/templates\/rbac.yaml))\\n* We will need to closely monitor migration steps for Dex\\n","tokens":75,"id":3112}
{"File Name":"my-cv\/0003-single-source-of-truth.md","Context":"## Context\\nWe want the data to be in one place, be it internal (in the static assets) or external (served by an API or an S3 bucket...)\\nWe want the project to be data-driven, so that the document is re-rendered if any data changes.\\nWe want the data being spread across the Components using the latest technologies.\\n","Decision":"The source of data is kept internally. It will stay in the webapge, as a static asset from now on.\\nBut the project must be kept easily switchable to an external data source.\\nWe spread the data across Components using React Context API.\\n","tokens":76,"id":3945}
{"File Name":"wikibase-release-pipeline\/0009-non-WMDE-release-notes.md","Context":"## Context\\nWe intend to package and release software that is not maintained by WMDE. For example, the Wikidata Query Service (WDQS).\\nThis software comes from a variety of sources including software that is used and maintained by the WMF in Wikimedia production Wikis but also some from complete third parties.\\nSome of this software including ElasticSearch and WikibaseLocalMedia already have curated release notes for versions.\\nOther software such as WDQS and Mediawiki extensions do not have release notes. They may have notable changes documented either in git commit messages or in phabricator tickets linked to those commits. It could be possible to computationally extract a compile these with some effort.\\nSoftware such as QuickStatements may prove difficult to build release note from git. It may require inspecting the code changes by eye.\\nCompiling release notes for software we do not maintain would add a significant maintenance burden.\\n","Decision":"We will not write custom release notes for software that we do not maintain.\\nWe will attempt to forward already curated release notes from upstream maintainers.\\n","tokens":185,"id":4470}
{"File Name":"seda-frontend\/0003-application-configuration-setup.md","Context":"## Context\\nUp to the point of writing this ADR, the SIA application aims to service only the municipality of Amsterdam. However, because of the nature of the application, more municipalities have shown an interest in the functionality the application has to offer.\\nThis poses a challenge, because there is a lot in the application that is specific to the municipality of Amsterdam. To sum up, amongst others (in random order):\\n- Docker registry URLs\\n- URLs of API endpoints, machine learning service, Authz service and map server\\n- Nginx configuration\\n- HTML `<title \/>`\\n- [`Sentry` Package](https:\/\/www.npmjs.com\/package\/@sentry\/browser) dependency and configuration\\n- [`Matomo` Package](https:\/\/www.npmjs.com\/package\/@datapunt\/matomo-tracker-js) dependency and configuration\\n- PWA manifest\\n- Logo\\n- Favicon\\n- PWA icons\\n- Hardcoded strings\\n- Main menu items\\n- Theme configuration\\n- Maps settings\\n- [`amsterdam-stijl` Package](https:\/\/www.npmjs.com\/package\/amsterdam-stijl) Package dependency\\n- Package URL and description\\nAll of the above need to be configurable or should be taken out of the equation to be able to publish a white-label version of `signals-frontend`.\\n","Decision":"Taking the pros and cons, the application's architecture, the Datapunt infrastructure and upcoming client wishes into account, the decision is made to go for the [Server-side \/ container](#Server-side \/ container) option.\\nThe repository will contain default configuration options so that it can be run locally and still be deployed to (acc.)meldingen.amsterdam.nl without the application breaking.\\nConfiguration is injected in the `index.html` file so it can be read at run-time.\\n","tokens":274,"id":315}
{"File Name":"openchs-adr\/0013-handling-change-of-catchments-in-the-client.md","Context":"## Context\\nA change in catchment in a device can be caused by two reasons.\\n1. The user-catchment assignment has changed in the server\\n2. A new user has logged in.\\nIn both these conditions,\\n- It is possible that there is unsynced data in the device\\n- The new user might not have control over old data\\nA new catchment means that the last update date times are wrong, and the data is also wrong on the device, so this means a full sync from the server. We should do this, but not allow existing unsynced data to be wiped out.\\nAlso note that the catchment an individual belongs to is set when he\/she is created. Therefore, a sync of data will not affect any unsynced data.\\n","Decision":"When a change in catchment is detected by openchs-client, existing unsynced data will be synced to the server, all data deleted, and a fresh sync started.\\n","tokens":163,"id":2820}
{"File Name":"js-sdk\/0019-use-custom-activation-wallet-for-threebot-deployers.md","Context":"## Context\\nActivation service fails a lot and that lead to starting online threebot without a wallet\\n","Decision":"Add option to activate the 3bot wallet via custom activation wallet on the deployer in case threefold service activation fails and pass the secret in the secret env\\n","tokens":20,"id":5203}
{"File Name":"Horace\/0011-version-project-with-cmake.md","Context":"## Context\\nEvery instance of the project should carry an up-to-date version number. This\\nversion number should be accessible from within the Matlab and C++ code.\\n","Decision":"The version number will be defined in a top-level VERSION file. This file\\nshould contain a version number of format \\<major\\>.\\<minor\\>.\\<patch\\> and\\nnothing else, including whitespace.\\nCMake will read the VERSION file and formate template Matlab and C++ files.\\nThese templates will be copied into the Matlab\/C++ source tree by CMake at\\nconfigure time.\\nIf CMake is building a developer version (i.e. CMake variable\\n`Horace_RELEASE_TYPE` is not equal to `RELEASE`), CMake will append a Git SHA\\nto the end of the version number. The Git SHA will be excluded from builds\\ncreated via the release pipeline; builds generated locally by developers and in\\npull request\/nightly Jenkins jobs will include the SHA.\\nIf CMake has not been run and has not generated the Matlab file containing the\\nversion, Matlab will read the VERSION file and append `.dev` to the version.\\nThis will signify this is an un-built developer version.\\n","tokens":34,"id":4241}
{"File Name":"dogma\/0014-apply-historical-events-to-aggregates.md","Context":"## Context\\nEvent sourcing engines need to call `AggregateRoot.ApplyEvent()` with\\n\"historical\" event types. That is, event types that have already been recorded\\nagainst the instance but are no longer configured for production by that\\naggregate.\\nThe current specification language prohibits this, as per the `ApplyEvent()`\\ndocumentation:\\n> It MUST NOT be called with a message of any type that has not been\\n> configured for production by a prior call to Configure().\\nAdditionally, without adding some new features to `AggregateConfigurer` it is\\nimpossible to declare an event as historical, meaning that there is no way to\\ndiscover historical event types from the configuration.\\n","Decision":"We have chosen to relax the language in the specification to allow calling\\n`ApplyEvent()` with any historical event types in addition to those configured\\nfor production.\\n","tokens":143,"id":1619}
{"File Name":"golang-git-fritz\/0003-follow-conventional-commits.md","Context":"## Context\\nThe point of the tool is to enforce good commit hygiene, which is easy to compromise while trying to just get good code out. We should pick a standard that seems like it's popular or otherwise conventional, and not overly strict for flexibility.\\n","Decision":"Use [Conventional Commits v1.0.0-beta.4]. It's similar enough to [Semantic Commits] and its inspiration [Angular Commits], but loose enough and working on being an independent standard.\\n","tokens":52,"id":969}
{"File Name":"paas-team-manual\/ADR002-concourse-pool-resource.md","Context":"Context\\n=======\\nWhen building pipelines using concourse, we investigated using the [pool\\nresource](https:\/\/github.com\/concourse\/pool-resource) in order to control flow\\nthrough jobs. This was an alternative to the use of the\\n[semver resource](https:\/\/github.com\/concourse\/semver-resource).\\nThese 2 resources are both workarounds to solve the problem of triggering jobs\\nwhen we haven't made changes to a resource.\\nThe problem is that the pool resource relies on write access to a github repo,\\nwhich means we must pass public keys that allow this access into the pipeline\\nand deployed concourse instance - we want to minimise the number of credentials\\nwe pass, and the semver resource relies on AWS credentials that are already\\npassed.\\nDecision\\n========\\nWe will not use the pool resource for flow between jobs - instead we will use\\nthe semver resource\\nStatus\\n======\\nAccepted\\nConsequences\\n============\\nThis was an investigation into a different approach, so no consequences\\n","Decision":"========\\nWe will not use the pool resource for flow between jobs - instead we will use\\nthe semver resource\\nStatus\\n======\\nAccepted\\nConsequences\\n============\\nThis was an investigation into a different approach, so no consequences\\n","tokens":213,"id":193}
{"File Name":"tech-events-calendar\/0002-store-event-data-as-markdown-files-with-yaml-front-matter.md","Context":"## Context\\nAs more events are added:\\n- README file gets harder to change. Adding events become more cumbersome.\\n- Format is unnatural. It is supposed to be a simple Markdown file but not it has a lot of weird formatting.\\n- When adding events, there is a large diff in JSON file due to other events shifted their line position.\\nTherefore, a new format for storing events is needed.\\n","Decision":"Change the format to Markdown files, one event per file, with extra metadata in the YAML front matter.\\n- It is familiar to those who uses static site generators, like Jekyll.\\n- It is human-readable, and easily editable.\\n- Libraries exists to parse this kind of file.\\nFiles are organized in `data` directory, which is split into folders for each month. Markdown files can then be created inside.\\nRelated GitHub issue: https:\/\/github.com\/ThaiProgrammer\/tech-events-calendar\/issues\/42\\n","tokens":85,"id":3752}
{"File Name":"datalab\/0031-kubernetes-namespace-for-environment-isolation.md","Context":"## Context\\nWe need to run multiple instances of the Datalabs system to allow us to continue to\\ndevelop while giving early adopters access to the system. We intend to run both a test\\nand production environment and need to decide whether to do this as a completely separate\\nKubernetes cluster or to isolate the environments using Kubernetes namespaces.\\n","Decision":"We have decided to run both environments on the same Kubernetes cluster but with a\\nseparate reverse proxy to allow testing of the proxy configuration. This decision was\\ntaken to avoid the maintenance overhead of having two clusters.\\n","tokens":71,"id":743}
{"File Name":"acs-deployment-aws\/0002-use-aws-cloudformation-for-provisioning-aws-resources.md","Context":"## Context\\nAs part of the ACS with AWS Services initiative, we are looking to use more Amazon services around our Kubernetes deployment. To do this we have to find a way to provision Amazon services like S3 buckets, EKS Cluster, Amazon MQ, Aurora DB which are outside of our current helm deployment. We have investigated 3 options for doing this provisioning.\\nThe first option would be to use CloudFormation templates to do the provisioning.\\nCloudFormation would be in alignment with our AWS First company direction and it can allow us to provision all types of Amazon resources needed.\\nAn additional plus is that we have experience working with this tool within our team.\\nHowever, CloudFormation locks us to Amazon only services and makes us have separated tools for provisioning Alfresco Content Services and the adjacent resources.\\nThe second option is having Terraform as an outside of AWS provisioner.\\nTerraform allows us to provision and make use of services from different cloud providers in our solutions as well as totally unrelated services like Github, Consul, PagerDuty and more importantly Bare Metal on-prem provisioning. Terraform is also abstracting away a good part of the required metadata needed for the provisioning of resources.\\nHowever, we have limited experience in using terraform.\\nThe final option is using kubernetes controllers to deploy Amazon resources as part of the helm deployment for acs.\\nImplementing kubernetes controllers for dynamically provisioning resources along with the usual kubernetes deployment for Alfresco Content Services would make us more consistent in how we deploy our applications and would ease up maintenance in the future.\\nHowever, we would still need another way for provisioning the actual kubernetes cluster and our experience in developing custom resource definitions used in kubernetes controllers is inexistent.\\n","Decision":"We will use Amazon CloudFormation templates as it is in alignment with Alfresco's AWS First direction. Also, we have experience in developing and using this tool within the company. It also brings us closer to potentially having a quickstart template for deploying the Alfresco Digital Business Platform.\\n","tokens":351,"id":4221}
{"File Name":"drt-v2\/0002-use-scala.md","Context":"## Context\\nUse scala for the server side processes as it's an approved language for use in the Home Office,\\n","Decision":"Use scala\\n","tokens":24,"id":1911}
{"File Name":"superwerker\/guardduty.md","Context":"## Context\\nsuperwerker provides a secure baseline and configures security related services by default.\\nSince GuardDuty (GD) is a native AWS service to find possible security threats and breaches, superwerker enables it for all AWS accounts.\\n","Decision":"- Use delegated administrator feature\\n- Delegate Administrator into Control Tower `Audit` account, since Control Tower also delegates AWS Config Rules Compliance findings into the Audit Account\\n- Enable GD for existing Control Tower core accounts (Management, Audit, Log Archive) and all future member accounts\\n- Use Control Tower `Setup\/UpdateLandingZone` Lifecycle events to start the setup of Delegated Administrator\\n- Enable [S3 data protection](https:\/\/aws.amazon.com\/blogs\/aws\/new-using-amazon-guardduty-to-protect-your-s3-buckets\/) by default\\n","tokens":51,"id":3403}
{"File Name":"platform\/2021-11-02-preparing-data-for-rule-evaluation.md","Context":"## Context\\nWhen we are creating new `Rule` definitions, we need to consider how we are retrieving the data necessary for the evaluation.\\nSince rules could possibly be evaluated on any given request, the data for the evaluation must also be present at all times.\\nThis circumstance causes us to carefully consider the performance with regards to additional database queries or storing the data\\nbeforehand.\\n","Decision":"Instances of `Rule` should always be able to evaluate based on data retrieved from an instance of `RuleScope`. This instance\\nprovides getters for `Context`, `SalesChannelContext` and an instance of `\\DateTimeImmutable` as the current time. Everything\\nthat needs to be evaluated should be derived from methods of these instances.\\nWhen the data necessary for evaluation **can't** already be retrieved by the methods of `Context` and `SalesChannelContext`,\\nthe **least** favorable option should be to add additional associations to the `Criteria` of DAL searches, e.g. in the `SalesChannelContextFactory`.\\nUnless an additional association is needed anyways and in a much wider scope, the preferred option should always be to use indexers and updater services.\\nUsing the indexers, only the data absolutely necessary for evaluation can be stored as part of the target entity's definition.\\nAs the data is persisted asynchronously within the message queue, it should be kept up to date by background processes and we can avoid any additional database queries\\nduring storefront requests.\\n","tokens":78,"id":4506}
{"File Name":"FlowKit\/0004-http-framework.md","Context":"## Context\\nIn order to wrap the FlowKit toolkit in a single HTTP API, an HTTP framework is required. There are a variety of options for this purpose, e.g. Flask and Django.\\nBoth Flask & Django offer a significant plugin ecosystem, and are 'battle-tested'. However, both are on the heavy side and are bound by legacy design. Of the two, Flask has much less boilerplate overhead.\\nAn alternative option is Quart, which is considerably newer. Quart is compatible with the Flask ecosystem plugin, and built to follow the newer ASGI standard. It is lightweight, offers impressive performance, and takes full advantage of the recent addition of asyncio to Python.\\nQuart also supports websockets, which, while not an immediate priority, are likely to be very useful for future, more dynamic iterations of the API.\\nShould Quart become defunct, the close mapping to the Flask API provides a low-impact exit.\\n","Decision":"The FlowKit API will make use of the Quart framework.\\n","tokens":190,"id":5051}
{"File Name":"konfetti\/0003-support-python-2-7.md","Context":"## Context\\nWe need to help application developers to migrate their projects from Python 2.7 to 3.5+.\\n","Decision":"We will support Python 2.7 on the best effort level until 2020-01-01.\\n","tokens":27,"id":3574}
{"File Name":"once-ui\/0004-add-readme-for-each-component-for-documentation.md","Context":"## Context\\nComponents should be well documented so that the users of the library will be able to quickly start using the component.\\n","Decision":"For now, we will create a README.md file inside each component folder, adjacent to the code for the component. The readme file should have the same structure and highlight some use cases, code examples and have api reference.\\n","tokens":26,"id":1152}
{"File Name":"otrv4\/004-smp-v2.md","Context":"### Context\\nThe Socialist Millionaires Protocol (SMP) in OTRv3 uses Diffie Hellman with a\\n1536-bit group, which gives an estimated security level of 128 bits. In order to\\nreach OTRv4's target security level (~224bits), this algorithm needs to be\\nupdated.\\nSMP Message 1 and SMP Message 1Q are very similar except for the 'Question'\\nfield. In OTRv3, if the user does not define an SMP question, message 1 is\\nsent; if they do, message 1Q is sent. These two messages can be combined into\\none by making the 'Question' field optional.\\n### Decision\\nSMP in OTRv4 uses elliptic curves to upgrade its security level to ~224. It uses\\nthe same curve used for the rest of OTRv4: Ed448-Goldilocks.\\nSMP in OTRv4 combines SMP Message 1 and SMP Message 1Q into one (SMP Message 1),\\nby making the 'Question' field optional.\\n### Consequences\\nThe security level of SMP is upgraded in accordance with the whole protocol's\\ntarget security level.\\nImplementers will only need to support one TLV type (SMP Message 1) for SMP\\ninitialization, by making the 'Question' field optional (for when a user\\nasks for a question and when they do not).\\nThis version of the SMP is, thereby, referred as, SMPv2.\\n","Decision":"SMP in OTRv4 uses elliptic curves to upgrade its security level to ~224. It uses\\nthe same curve used for the rest of OTRv4: Ed448-Goldilocks.\\nSMP in OTRv4 combines SMP Message 1 and SMP Message 1Q into one (SMP Message 1),\\nby making the 'Question' field optional.\\n### Consequences\\nThe security level of SMP is upgraded in accordance with the whole protocol's\\ntarget security level.\\nImplementers will only need to support one TLV type (SMP Message 1) for SMP\\ninitialization, by making the 'Question' field optional (for when a user\\nasks for a question and when they do not).\\nThis version of the SMP is, thereby, referred as, SMPv2.\\n","tokens":320,"id":3695}
{"File Name":"uniprot-website\/0004-state-management.md","Context":"## Context\\nThe state of data and UI will grow in the application as the development goes on. There needs to be good state management mechanism to handle this, particularly when components will require interaction across the component hierarchy.\\n","Decision":"[Redux](https:\/\/redux.js.org\/) will be used for state management. It is a centralised state management container that can handle data and UI state.\\n","tokens":44,"id":2014}
{"File Name":"terraform-aws-msk-module\/0002-support-terraform-v0-11-and-v0-12.md","Context":"## Context\\nTerraform has recently released version v0.12. [Upgrading to Terraform\\nv0.12](https:\/\/www.terraform.io\/upgrade-guides\/0-12.html) is significantly more\\neffort than previous Terraform upgrades due to a number of\\n[incompatibilities](https:\/\/www.terraform.io\/upgrade-guides\/0-12.html).\\nHypr has clients using the v0.11 of modules at this time. Whilst these clients\\ncontinue to use v0.11 modules, any new modules created should also be available\\nto these clients.\\nTerraform v0.12 provides a significant number of improvements that enable\\nmodules to be less verbose, with less workarounds to achieve the same results as\\nthose written using v0.11.\\n","Decision":"This module will support both Terraform v0.11 and v0.12 module users.\\nThe `master` branch of this module will contain the v0.12 version of this\\nmodule, whilst a `0.11` branch will hold the v0.11 version. This is a common\\npattern prevalent in the Terraform Module community at this time.\\nUsers should still be encouraged to upgrade their Terraform to v0.12, but by\\nproviding a v0.11 version of the module they do not miss out on the\\nfunctionality provided by the module at this time.\\n","tokens":169,"id":2225}
{"File Name":"docs\/0020-configurable-service-dependencies.md","Context":"## Context and Problem Statement\\nServices currently have a static network of dependencies. This makes it impossible to have a service that depends on a SQL based database use a different database that is compatible with the service.\\n## Decision Drivers\\n* Should fit in the current domain model\\n* Easy to add afterwards\\n* Should support existing services with their static dependencies\\n","Decision":"* Should fit in the current domain model\\n* Easy to add afterwards\\n* Should support existing services with their static dependencies\\nTo be decided.\\nThis adr purely documents possible decisions for this problem.\\n","tokens":71,"id":4690}
{"File Name":"ReportMI-service-manual\/0004-use-ruby-on-rails-for-applications.md","Context":"## Context\\nIn [ADR-0002][adr-0002] we outlined the overall technical approach for the Data\\nSubmission Service, and highlighted the components which we expect will be\\ndeveloped.\\nThis ADR focusses on the technology choice for building a portion of the\\napplications. In particular:\\n1. Submission app - the public facing front-end for suppliers to submit returns\\n1. Administration app - the front-end for CCS staff to support the operation of\\nthe service\\n1. Onboarding app - the front-end for CCS staff to manage the onboarding of new\\nsuppliers and commercial agreements\\n1. Data storage API - the API which these three applications use to store and\\nretrieve data\\n### Language and framework choice\\nThere are many possible language and framework choices for the development\\nof the applications for this service.\\nThe existing MISO service is built using C# and ASP.Net. Other services in CCS\\nuse a mixture of PHP and Java\/Spring. Digital Marketplace uses Python and GOV.UK\\nuses a mixture of Ruby with Rails and Sinatra, Python and GO.\\nAny decision we make about languages and frameworks should consider:\\n- The skill of the current team we have available to build the service - what\\nare the current team comfortable using?\\n- The skill of a future team - is there a large enough pool of suppliers and\\ncontractors who could support and main the service in the future?\\n- External toolkits and modules - are there useful toolkits and modules which\\ncould reduce development effort (eg the\\n[GOV.UK Frontend Toolkit][govuk-frontend-toolkit])\\n- Cost - what is the cost of developing in this way? Are there licence costs?\\n- Hosting - would picking this language restrict hosting options?\\nWe should consider the Service Manual guide on\\n[choosing technology][service-manual-choosing-technology].\\n","Decision":"We will use Ruby on Rails for the applications in the Data Submission Service\\nThe development team is used to working with these choices, and there is a large\\ncommunity of developers both inside and outside government using Ruby and Rails\\nfor government digital services.\\n","tokens":394,"id":2048}
{"File Name":"disc-golf-statistics\/0002-use-es2016-modules.md","Context":"## Context\\nES2016 introduced native support for the concept of modules. These are scoped files that expose some public functions. Modules are a way of organizing and sharing code.\\n","Decision":"We will use ES2016 modules to organize and share code. More information can be found here: http:\/\/exploringjs.com\/es6\/ch_modules.html#sec_modules-in-javascript\\n","tokens":36,"id":4295}
{"File Name":"coloseo\/0002-use-docker-compose.md","Context":"## Context\\nThe solution has to be portable and lightweight and work without special infrastructure.\\n","Decision":"Docker-compose is used as multi-container solution.\\n","tokens":18,"id":4798}
{"File Name":"operational-data-hub\/0053-project-company-data.md","Context":"## Context\\nSpecifying the [data catalog](0012-data-catalog-specifies-all-data-components.md) requires a clear and specific way of documenting that is also suitable for automated processing.\\nA data catalog enables a publisher to describe datasets and data services in a catalog using a standard model and vocabulary that facilitates the consumption and aggregation of metadata from multiple catalogs. The challenge is to define and name standard metadata fields so that a data consumer has sufficient information to process and understand the described data. The more information that can be conveyed in a standardized regular format, the more valuable data becomes.\\n[Project company data](https:\/\/vwt-digital.github.io\/project-company-data.github.io\/) specifies a metadata format for data catalogs in a machine readable JSON format, covering the commonly required metadata attributes.\\n","Decision":"We will use the [Project company data](https:\/\/vwt-digital.github.io\/project-company-data.github.io\/) to describe the data catalog.\\n","tokens":159,"id":2738}
{"File Name":"gatemint-sdk\/adr-016-validator-consensus-key-rotation.md","Context":"## Context\\nValidator consensus key rotation feature has been discussed and requested for a long time, for the sake of safer validator key management policy (e.g. https:\/\/github.com\/tendermint\/tendermint\/issues\/1136). So, we suggest one of the simplest form of validator consensus key rotation implementation mostly onto Cosmos-SDK.\\nWe don't need to make any update on consensus logic in Tendermint because Tendermint does not have any mapping information of consensus key and validator operator key, meaning that from Tendermint point of view, a consensus key rotation of a validator is simply a replacement of a consensus key to another.\\nAlso, it should be noted that this ADR includes only the simplest form of consensus key rotation without considering multiple consensus keys concept. Such multiple consensus keys concept shall remain a long term goal of Tendermint and Cosmos-SDK.\\n","Decision":"### Pseudo procedure for consensus key rotation\\n- create new random consensus key.\\n- create and broadcast a transaction with a `MsgRotateConsPubKey` that states the new consensus key is now coupled with the validator operator with signature from the validator's operator key.\\n- old consensus key becomes unable to participate on consensus immediately after the update of key mapping state on-chain.\\n- start validating with new consensus key.\\n- validators using HSM and KMS should update the consensus key in HSM to use the new rotated key after the height `h` when `MsgRotateConsPubKey` committed to the blockchain.\\n### Considerations\\n- consensus key mapping information management strategy\\n- store history of each key mapping changes in the kvstore.\\n- the state machine can search corresponding consensus key paired with given validator operator for any arbitrary height in a recent unbonding period.\\n- the state machine does not need any historical mapping information which is past more than unbonding period.\\n- key rotation costs related to LCD and IBC\\n- LCD and IBC will have traffic\/computation burden when there exists frequent power changes\\n- In current Tendermint design, consensus key rotations are seen as power changes from LCD or IBC perspective\\n- Therefore, to minimize unnecessary frequent key rotation behavior, we limited maximum number of rotation in recent unbonding period and also applied exponentially increasing rotation fee\\n- limits\\n- a validator cannot rotate its consensus key more than `MaxConsPubKeyRotations` time for any unbonding period, to prevent spam.\\n- parameters can be decided by governance and stored in genesis file.\\n- key rotation fee\\n- a validator should pay `KeyRotationFee` to rotate the consensus key which is calculated as below\\n- `KeyRotationFee` = (max(`VotingPowerPercentage` * 100, 1) * `InitialKeyRotationFee`) * 2^(number of rotations in `ConsPubKeyRotationHistory` in recent unbonding period)\\n- evidence module\\n- evidence module can search corresponding consensus key for any height from slashing keeper so that it can decide which consensus key is supposed to be used for given height.\\n- abci.ValidatorUpdate\\n- tendermint already has ability to change a consensus key by ABCI communication(`ValidatorUpdate`).\\n- validator consensus key update can be done via creating new + delete old by change the power to zero.\\n- therefore, we expect we even do not need to change tendermint codebase at all to implement this feature.\\n- new genesis parameters in `staking` module\\n- `MaxConsPubKeyRotations` : maximum number of rotation can be executed by a validator in recent unbonding period. default value 10 is suggested(11th key rotation will be rejected)\\n- `InitialKeyRotationFee` : the initial key rotation fee when no key rotation has happened in recent unbonding period. default value 1atom is suggested(1atom fee for the first key rotation in recent unbonding period)\\n### Workflow\\n1. The validator generates a new consensus keypair.\\n2. The validator generates and signs a `MsgRotateConsPubKey` tx with their operator key and new ConsPubKey\\n```go\\ntype MsgRotateConsPubKey struct {\\nValidatorAddress  sdk.ValAddress\\nNewPubKey         crypto.PubKey\\n}\\n```\\n3. `handleMsgRotateConsPubKey` gets `MsgRotateConsPubKey`, calls `RotateConsPubKey` with emits event\\n4. `RotateConsPubKey`\\n- checks if `NewPubKey` is not duplicated on `ValidatorsByConsAddr`\\n- checks if the validator is does not exceed parameter `MaxConsPubKeyRotations` by iterating `ConsPubKeyRotationHistory`\\n- checks if the signing account has enough balance to pay `KeyRotationFee`\\n- pays `KeyRotationFee` to community fund\\n- overwrites `NewPubKey` in `validator.ConsPubKey`\\n- deletes old `ValidatorByConsAddr`\\n- `SetValidatorByConsAddr` for `NewPubKey`\\n- Add `ConsPubKeyRotationHistory` for tracking rotation\\n```go\\ntype ConsPubKeyRotationHistory struct {\\nOperatorAddress         sdk.ValAddress\\nOldConsPubKey           crypto.PubKey\\nNewConsPubKey           crypto.PubKey\\nRotatedHeight           int64\\n}\\n```\\n5. `ApplyAndReturnValidatorSetUpdates` checks if there is `ConsPubKeyRotationHistory` with `ConsPubKeyRotationHistory.RotatedHeight == ctx.BlockHeight()` and if so, generates 2 `ValidatorUpdate` , one for a remove validator and one for create new validator\\n```go\\nabci.ValidatorUpdate{\\nPubKey: tmtypes.TM2PB.PubKey(OldConsPubKey),\\nPower:  0,\\n}\\nabci.ValidatorUpdate{\\nPubKey: tmtypes.TM2PB.PubKey(NewConsPubKey),\\nPower:  v.ConsensusPower(),\\n}\\n```\\n6. at `previousVotes` Iteration logic of `AllocateTokens`,  `previousVote` using `OldConsPubKey` match up with `ConsPubKeyRotationHistory`, and replace validator for token allocation\\n7. Migrate `ValidatorSigningInfo` and `ValidatorMissedBlockBitArray` from `OldConsPubKey` to `NewConsPubKey`\\n- Note : All above features shall be implemented in `staking` module.\\n","tokens":173,"id":35}
{"File Name":"digitalrig-metal-aws\/0002-use-aws-bare-metal-rig-approach.md","Context":"## Context\\nWe need to create a riglet for our new bookit project so that we practice what we preach.\\n","Decision":"We will use the AWS Bare Metal Riglet from bookit-riglet as a starting point for our riglet.  We will keep the previous bookit-riglet and create a new bookit-infrastructure project\/repo.\\nTechnologies:\\n* AWS: CloudFormation, ECR, ECS, Route53, VPC, ALB\\n* Deployment Mechanism: Docker images\\n* Build: Travis\\n","tokens":25,"id":1738}
{"File Name":"hmpps-interventions-docs\/0001-split-ui-and-service.md","Context":"## Context\\n### Need for sustainable services\\nThe Ministry of Justice 2022 Digital Strategy defines \"Building sustainable services\" as a priority.\\nAs the domain model of HM Prisons and Probation Service (HMPPS) is big, we aim to build sustainable services\\naround [bounded contexts][bounded-context], making it necessary to integrate with other contexts.\\n### Domain and business logic\\nHistorically, many systems built in HMPPS did not expose an API for the business logic and\/or were managed by third\\nparties, leading to difficult integration: we see business APIs beneficial to have from the start.\\nExisting APIs that are thin layers over a database (entity services) are easy to write but hard to use as it pushes\\nthe need to handle business logic back to the clients.\\nWe have seen services with a frontend\/backend split where frontend clients accumulated business logic as the\\nbackend service fell back to provide a CRUD entity service over the database; this is not sustainable.\\n_Sam Newman_ writes in _Monolith to Microservices_ (O'Reilly, 2019):\\n> Fundamentally, in a system that consists of multiple independent services, there has to be some interaction between\\n> the participants. In a microservice architecture, _domain coupling_ is the result\u2014the interactions between services\\n> model the interactions in our real domain.\\nWe want to aim for domain-level coupling to avoid business logic drifting into current and future clients.\\n### Languages in \"adopt\"\\nHMPPS Digital is heading towards using Java\/Kotlin and node.js as their primary language choices.\\nThe [current HMPPS Digital tech radar][radar] elects the mentioned languages in _adopt_. It also mentions Ruby (for London);\\nhowever, the current direction is to reduce regional differences as it is difficult to hire developers who are comfortable\\nand happy working with multiple languages.\\nThe department's talent pool reflects this split; there are:\\n- many node-focussed frontend specialists,\\n- many Java\/Kotlin-focussed backend specialists,\\n- a few full-stack developers.\\n","Decision":"We will **create standalone business interfaces (business\/domain APIs) by default**.\\nWe will **split the user interface** from the API's component to enforce the use of the business API by default\\nand better utilise the talent we have.\\nWe realise this is an optimisation to build durable domain-coupled systems at the cost of some team autonomy.\\n","tokens":428,"id":1802}
{"File Name":"operational-data-hub\/0042-python-version-3.md","Context":"## Context\\nIn the past, there was a bit of a debate in the coding community about which Python version was the best one to learn: Python 2 vs Python 3 (or, specifically, Python 2.7 vs 3.5). Now, it's more of a no-brainer: Python 3 is the clear winner.\\n","Decision":"We will use python3  exclusively for all our python code.\\nMake sure you use version active maintained version (currently 3.6 or higher)\\n","tokens":72,"id":2713}
{"File Name":"cafebabel.com\/0006-tags.md","Context":"## Context\\nArticles need tags to ease classification and navigation across similar topics.\\nTags must have a name, slug, language, optional description and optional image.\\nGiven this complexity, we cannot use embedded documents.\\nThe relation between tags' translations is not useful.\\nWe want to be able to retrieve articles for a given tag.\\nWe want to suggest tags for a given language when an article is\\ncreated\/modified.\\nWe want to be able to modify the description\/image for a given tag.\\n","Decision":"Custom implementation of tags as a separate collection.\\n","tokens":103,"id":3238}
{"File Name":"tdr-dev-documentation\/0006-user-data-model.md","Context":"## Context\\nWe have [decided][user-auth-adr] to use the OAuth2 provider Keycloak for user\\nauthentication. This means that user account details will be stored in\\nKeycloak's database, separately to the main consignment database.\\nThere is some overlap between user data model and consignment data model:\\n* Users belong to transferring bodies, and series belong to transferring bodies\\n* Users are linked to consignments that they create, and (at least for the MVP)\\nshould only be able to see their own consignments\\nUsers will also eventually need roles to distinguish users who create their own\\nconsignments from users who need need to see other users consignments, such as\\nDepartmental Record Officers or possibly digital archivists within The National\\nArchives.\\nWe need to make it possible for TDR admins to add and update users, including\\neasily assigning them to departments and roles, and use that information in\\nother parts of the TDR system such as the API or any reporting system.\\n[user-auth-adr]: 0005-user-authentication.md\\n","Decision":"Store user information in Keycloak. Use the same user and transferring body IDs\\nin Keycloak and the consignment DB, but don't duplicate any other information.\\nWhen the consignment database needs to store a reference to a user (e.g. a\\nrecord of which user created a consignment), store the Keycloak user ID.\\nStore details of transferring bodies in the consignment DB, including a code\\nwhich is referenced in the user's data in Keycloak.\\nAdd the transferring body and any user roles to the access token. This token\\nwill be passed from the frontend to the consignment API, which can use the user\\nID or body in the token to authorize requests.\\nIf the consignment API or analytics tool needs more information, such as the\\nname of a user, it can query the Keycloak REST API.\\nA rough diagram of the data model is shown below. The data may not be stored\\nexactly like this in Keycloak, and field names may be different in the\\nconsignment DB, but the diagram does show which system is responsible for\\ndifferent fields, the relationship between the systems. Dotted lines represent\\nreferences between databases, rather than strict foreign keys.\\n![Diagram of tables in the Consignment DB and Keycloak](images\/user-data-model.png)\\n### Administration\\nData in Keycloak can be updated by a developer or admin through the Keycloak\\nadmin site.\\nWe will eventually have to build an admin interface to update Body and Series\\ndata, but for now we will build an import script which lets a developer add new\\nbodies and series.\\n","tokens":228,"id":1781}
{"File Name":"hmpps-interventions-ui\/0005-use-prettier-to-format-typescript-code.md","Context":"## Context\\nWe want to ensure we're all using one code style, that is familiar across\\nprojects. [Prettier](https:\/\/prettier.io\/) is an opinionated code formatter with\\nsupport for most, if not all, of the languages in the JavaScript ecosystem. As\\nof writing, it is used by over\\n[1 million repositories](https:\/\/github.com\/prettier\/prettier\/network\/dependents?package_id=UGFja2FnZS00OTAwMTEyNTI%3D)\\non GitHub, including React itself, and has become a standard.\\n","Decision":"We will enforce that everything supported by Prettier has its style enforced by\\nit.\\nWe will set up Git hooks to automatically run the formatter before committing.\\nWe will set continuous integration up to reject commits that are not correctly\\nformatted.\\n","tokens":125,"id":454}
{"File Name":"mario\/0015-use-a-single-bucket.md","Context":"## Context\\nThere are a couple reasons to switch from multiple buckets to a single bucket. The first is that it simplifies the infrastructure provisioning that needs to be done when a new source is added. The second is that there is a hard limit on the number of buckets an AWS account can have, and our current approach to bucket creation is unsustainable.\\n","Decision":"Use a single namespaced S3 bucket for all source data. The structure of the bucket should be:\\n```\\ns3:\/\/bucket\/<environment>\/<source>\/<files>\\n```\\nWhere `environment` would be either `prod` or `stage`, and `source` would be the source identifier. The source identifier used here should also be used as the prefix for the index name. No specific decisions are made here about how the files are structured within a source.\\nmario should ignore source identifiers that it does not know about.\\n","tokens":71,"id":3414}
{"File Name":"figgy\/0009-unlinked-files.md","Context":"## Context\\nWhen an ingest fails in the middle of a transaction which is adding files, the\\nFileSets will not get persisted. However, the files will have already been\\ncopied to the repository via `FileAppender`. This results in files in the\\nrepository which have no corresponding database record.\\nFixing this will require development of a transactional disk StorageAdapter\\nwhich moves files at the end of a metadata transaction.\\n","Decision":"1. We don't have time to implement a transactional disk StorageAdapter at this\\ntime.\\n2. Accept this situation, document it here, and know we can free up space in the\\nfuture by looking for unlinked files and deleting them.\\n","tokens":90,"id":4824}
{"File Name":"operational-data-hub\/0002-repo-naming-conventions.md","Context":"## Context\\nWe feel the need to use a naming convention for github repos.\\n","Decision":"We identify three kinds of repositories:\\n### 1. General rules:\\n* Use hyphens ('-') between words in the name because:\\n* words written together without something inbetween are unclear.\\n* \"\\_\" is harder to type than \"-\"  [stack overflow](ttps:\/\/stackoverflow.com\/a\/11947816).\\n* Make repo names not longer than needed. (Because GCP project names are also limited in length).\\n### 2. config VWT DAT repositories\\nConfig repositories are repositories containing configurations of a specific Google Cloud Project (GCP) project.\\n* Should have the same name as the GCP project they are connected to minus the customer, environment and location.\\n* Name ends with `-config`.\\n### 3.  solutions VWT DAT repositories\\nSolutions repositories are repositories containing solutions, they can belong to multiple domains.\\n* Their names should always start with the domain they belong to.\\n* If the repository will handle multiple facets of the service, the name should end in `-handlers`\\n* Sometimes, two repositories are connected because they are the frontend and backend of a service. Their names should be the same except for the ending. Frontend repositories should end in `-tool` and backend repositories should end in `-api`.\\n### 4. \"normal\" VWT DAT repositories\\n\"Normal\" repositories are repositories not belonging to a solution. They contain code used specifically for the Operational Data Hub (ODH).\\n* Repository naming is equal to naming convention for solution repositories. Domains for these reposiitories is limited to `dat` and `odh`.\\n* If the repository is forked from another repository, its name should contain the name of the repository it forked from.\\n","tokens":17,"id":2743}
{"File Name":"app-performance-summary\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":4192}
{"File Name":"operational-data-hub\/0010-event-sourcing-captures-every-change-to-business-state.md","Context":"## Context\\nMartin Fowler: \"Event Sourcing ensures that all changes to application state are stored as a sequence of events. Not just can we query these events, we can also use the event log to reconstruct past states, and as a foundation to automatically adjust the state to cope with retroactive changes.\"\\nEvent sourcing persists the state of a business entity such an Order or a Customer as a sequence of state-changing events. Whenever the state of a business entity changes, a new event is appended to the list of events. Since saving an event is a single operation, it is inherently atomic. The application reconstructs an entity\u2019s current state by replaying the events.\\nA system is eventsourced when\\n- the single source of truth is a persisted history of the system\u2019s events;\\n- and that history is taken into account for enforcing constraints on new events.\\n","Decision":"All data on the ODH is available as a series of events. Events are published on the ODH topics and applications can process these events by creating a subscription on the topic.\\n","tokens":175,"id":2758}
{"File Name":"libelektra\/unit_testing.md","Context":"## Problem\\nThe previous unit testing framework started as hack to have a bit more\\nthan simple asserts. It is not easy to use (needs explicit enumeration\\nof all test cases) and lacks really important features (e.g. output of\\nthe assertion that failed).\\n","Decision":"- Keep C framework for C tests and ABI tests\\n- Google Unit testing framework `gtest` with code downloaded by CMake for\\nsystems where no source is packaged (Debian Wheezy, Arch Linux,\\nFedora,...) for C++ tests\\n- [Script Testing](script_testing.md)\\n","tokens":55,"id":1304}
{"File Name":"DunkMe\/0004-game-database-fields.md","Context":"#### Context and Problem Statement\\nAll game records need to be persisted to the relational database with reporting in mind.\\n#### Considered Options\\n- dbo.game for each game record, dbo.game_line for each shot attempt.\\n#### Decision Outcome\\nChosen option: dbo.game with dbo.game_line\\n- Each shot attempt is recorded with a score of 0 for miss and 1 for a dunk\\n- Verbose logging like this will make reporting easier and leave nothing to interpretation\\n","Decision":"Chosen option: dbo.game with dbo.game_line\\n- Each shot attempt is recorded with a score of 0 for miss and 1 for a dunk\\n- Verbose logging like this will make reporting easier and leave nothing to interpretation\\n","tokens":97,"id":1662}
{"File Name":"where-away\/0008-use-pug-to-render-the-final-html.md","Context":"## Context\\nWe'll need to combine html, js, and css. The ways I've done this before are:\\n- all in one file\\n- this is gross, hard to write, and nigh impossible to unit test\\n- bash & sed to replace placeholders blocks with file content\\n- this is cheap, but gross, and can cause trouble with character escaping.\\nIt's effectively a polyglot and forces you to think about bash when writing\\nJavaScript or CSS. Painful\\n- webpack with inlining plugins\\n- it's possible to use webpack to produce a single standalon html file\\nincluding css and javascript, but it takes a lot of configuration &\\nfiddling. I've done it, it was super powerful, but took an afternoon to set\\nup.\\n- fiddling with plugins\\n- different plugins for CSS and JavaScript\\n- it felt like webpack was not the right tool\\n- use a template engine like Pug that supports inlining JavaScript out of the\\nbox\\n- if necessary, this can be coupled with webpack or equivalent -- use a JS\\ntool to combine the JavaScript; use Pug to pull the built JavaScript into\\nthe final HTML\\n","Decision":"Use Pug to produce the HTML from a template.\\n","tokens":245,"id":2277}
{"File Name":"dp\/0005.md","Context":"## Context\\nAdditional screens for the publishing workflow were designed given all the\\npossible fields regardless of how they are retrieved or edited from the API.\\nThese fields were broken down into suggested logical groupings for users to edit.\\nThis separation means multiple API calls could be needed to save updates on a\\nsingle screen, and that datasets will have to be added with versions depending\\non which fields are updated (going against [decision 0004](0004.md)).\\n","Decision":"Screens should be separated to update either the dataset or version metadata\\nbut not both in one.\\n","tokens":96,"id":5110}
{"File Name":"approvals\/001_structure_classes.md","Context":"## Context\\nThere are many ways to structure classes beyond the standard rails Model View Controller (MVC) structure, therefore to help make the project maintainable in the future we have come up with the following standards.\\n","Decision":"* Avoid Helpers\\n* The rational behind this decision is standard rails helpers are hard to test efficiently and are global so they can have unintended consequences.\\n* Use a Decorators directory `app\/decorators`\\n* Decorators should be used for read only operations that modify the data for viewing purposes.\\n* Use a ChangeSet directory `app\/change_sets`\\n* ChangeSet should be used for reformatting the data for purposes of showing it to user\/system for modification.\\n* We have kept the change set concept since these classes can also be useful for data import not just user facing formats.\\n* Use a Services directory `app\/services`\\n* Services should be used for encapsulating complex actions or actions that work across multiple models, like seeding or importing bulk data. These may change the underlying data structures.\\n* Use a Mailer directory `app\/mailer`\\n* Notifications will be put in the mailer directory so that we can stick with the standard defined by ActionMailer\\n### Possible Future needs\\nWe currently believe that the business logic operations that are required for the system will be small enough to live directly on their primary model.  If in the future the models become too bloated we recommend creating a BusinessRules directory `app\/business_rules`. BusinessRules would be a corollary to Services, for complex or model-spanning read-only logic.\\n","tokens":44,"id":3320}
{"File Name":"aocourts-api\/0003-python.md","Context":"## Context\\nTo support the E&I phase of 18F's work with the Administrative Office of the Courts we need to build a small API that is easy to maintain, easy to deploy, and flexible. Naturally, one of the first (and sometimes consequential) decisions we will make is the choice of programming language. Considerations for this choice include:\\n- Supported by cloud services without a lot of custom work\\n- Well-understood by people on the team with easily available documentation\\n- Mature enough to have all the tools and libraries we will need\\n- Sufficient concurrency model for designing and API\\n","Decision":"For the current work we will build the API using Python and related tools such as FastAPI, Pytest, and SQLAlchemy.\\n","tokens":123,"id":3395}
{"File Name":"decode-demo\/0002-implement-using-phoenix.md","Context":"## Context\\nIt is required to create a demo app to show the component parts for DECODE\\nworking together in order to be able to prove that the system works. An\\nearlier version of this was implemented in Python using Flask and\\nFlask-socketio. This largely worked but was prone to weird socket failures,\\nand I wasn't very happy with maintaining or extending it (which may be\\nrequired).\\nThe finished service should be easy to deploy somewhere public (e.g. Heroku).\\nThe system should be amenable to being extended should we need to add any\\nmore functionality (e.g. to support logging into the dashboard).\\n","Decision":"We will reimplement the demo service using Elixir\/Phoenix rather than the\\nPython\/Flask\/Socketio version previously worked on.\\n","tokens":132,"id":4000}
{"File Name":"amf-core\/0007-no-spec-mediatypes-in-amf-operations.md","Context":"## Context\\nOn AMF 5 Beta's first iteration \"domain+syntax\" mediatypes like \"application\/swagger20+yaml\" were\\nused to decide how the syntax and the domain were to be parsed. This was especially useful to\\nvalidate, transform and render units using compound configurationts (API, WebAPI, RAML, OAS)\\nThis is controversial as:\\n- Resulting mediatypes are strange to the end-user and are not standard. Besides we could\\nonly handle a specific ordering in domain and syntax. A mediatype formed by syntax+domain\\ncouldn't be parsed.\\n- Clients that used a specific configuration like RAML10 or OAS30 had to specify the mediatype\\nalthough the configuration they used already specified their intended domain and syntax.\\n","Decision":"Remove those compound mediatypes and instead only keep them for syntax purposes when needed.\\n","tokens":165,"id":408}
{"File Name":"verify-service-provider\/0012-we-will-use-the-full-profile.md","Context":"## Context\\nVerify's SAML profile specifies that Responses and Assertions should be signed.\\nResponses should be signed by the Verify Hub and Assertions should be signed by\\nthe Matching Service Adapter.\\nThis profile causes problems with some off-the-shelf SAML service providers,\\nwhich can't handle multiple signatures from different keys in the same message.\\nAs a workaround, Verify introduced a \"simple\" profile where we do not sign Responses.\\n","Decision":"We will use the \"full\" profile, not the \"simple\" profile. The hub will sign responses\\nand the service provider will validate them against the hub's metadata.\\n","tokens":89,"id":4441}
{"File Name":"infra\/0003-use-voyager-for-service-ingress.md","Context":"## Context\\nWe need to have load balancers between our CDNs and our services, in order to deal with k8s node failures (and for handing traffic, and as a place to log stuff, and as a way to block bad actors if needed).  We'd prefer to only pay AWS\/GCP for a single load balancer thing, while still having the ability to generate unique dns addresses for each of our services.  They must be able to host certs correctly.\\n","Decision":"We're currently managing our ELBs in aws with some out of band terraform that connects the nodes of each k8s cluster to a load balancer. Since this is out of band, upgrading clusters or services implies doing a bunch of k8s stuff, and then also running terraform.  We could possibly simplify the whole experience by moving the full definition of the load balancer, and dns, and certs to objects inside k8s.  Voyager + External Ingress seem like the most common way to do this.  Deploying one 'ingress' object per group of services you want to have behind an ALB, and listing all the DNS to point at those services solves the problem outlined above.\\nThe primary advantage of doing this work, is that it allows dynamic things (deployments created in response to events, such as pull requests and ephemeral 'demo' branches) to be created simply by writing the yaml and deploying it to kubernetes.\\n","tokens":98,"id":855}
{"File Name":"rfcs\/0000-awesome-frontend.md","Context":"## Context\\n[context]: #context\\nThe current frontend is in a bad state.\\nThe following issues need to be solved to change the situation:\\n- **Maintainability**\\nDue to the untyped programming language JavaScript most types of code refactoring are risky to realize.\\nThis gets emphasized by external dependencies that easily can break the web application.\\nOften a simple dependency update leads to multiple hours of fighting the dependency hell.\\nUnfortunately a handful of bugs don't occur during the refactoring process but during the\\nruntime (because JS can't check correctness at compile time).\\n- **Complexity**\\nThe current complexity of the codebase is grown due to a lot of dirty quick fixes.\\nThis not only makes it hard for the community to contribute to the project\\nbut it also makes it costly to introduce new features.\\n- **Tests**\\nThere are few tests but most of the code is untested.\\nThis makes it even harder to refactor and maintain the project.\\n- **File size**\\nThe packed JavaScript bundle size was 5.3 MB some time ago.\\nIn the meanwhile its shrunken to 1.5 MB but that's still a bunch of bytes that\\nneeds to be downloaded at once.\\n","Decision":"[decision]: #decision\\nWe will port the whole frontend to [Rust](https:\/\/rust-lang.org). The migration\\ncan be divided into the following stages:\\n- Port the existing admin views to WASM as the first use case. All new admin views\\nwill be implemented with the new technology. This first stage serves as test balloon\\nto verify the feasibility of the plan.\\n- Create a read-only view of the regular Kvm front-end that allows to browse\\nthe map as well as searching and selecting entries.\\n- Complete the front-end by addint the editing functionality, i.e. to create and\\nmodify entries. This may include includes extension like additional actions for\\nlogged in users depending on their role.\\nThe result will be a [WASM](https:\/\/en.wikipedia.org\/wiki\/WebAssembly) module.\\nWe made this decision based on following reasons.\\nWe expect to achieve\\n- a much better **maintainability** because:\\n- Rust is statically typed\\n- refactoring is easy (the compiler does a very good job)\\n- the package manager [Cargo](https:\/\/doc.rust-lang.org\/stable\/cargo\/) provides\\na reliable dependency management\\n- most mistakes will popup at compile-time\\n- a reduced **complexity** because we can get rid of most legacy code\\n- a more efficient **testing** because we can focus on business logic\\n(instead of testing trivial things like it is required in JavaScript)\\n- a mobile friendly **file size** because the app is packed in a binary format\\n(instead of text like it is with JavaScript)\\n- a better API integration because we can **share** modules with the backend (OpenFairDB)\\nthat is also written in Rust.\\n","tokens":258,"id":1872}
{"File Name":"pul_library_drupal\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":1150}
{"File Name":"Horace\/0010-package-dependencies-in-repo.md","Context":"## Context\\nThere are various C++ dependencies present in Horace. For example, the version\\nof MPICH used should match the version used by Matlab - this avoids incompatible\\nshared libraries being used when executing mex functions. These libraries may\\nbe old versions and may not be easily available for users to download.\\n","Decision":"Dependencies will be packaged within the repo to be built against. Matlab's own\\nshared libraries will be built against where possible, but the static libraries\\nand headers required will be within the repository. Libraries will be statically\\nlinked where possible so that the libraries do not need to be shipped to users.\\n","tokens":64,"id":4246}
{"File Name":"qc-atlas\/0006-model-assemblers.md","Context":"## Context and Problem Statement\\nSpring HATEOAS includes several classes that encapsulate domain objects, adding support for links.\\nConstructing such objects, as well as adding the desired links to them is a common operation that\\nrequires entity-specific boilerplate code.\\nHow can duplicate code in nearly all controller methods be avoided?\\n## Decision Drivers <!-- optional -->\\n* Avoid duplicate code to create HATEOAS models\\n* Decouple link creation from normal entity logic\\n","Decision":"* Avoid duplicate code to create HATEOAS models\\n* Decouple link creation from normal entity logic\\nSeparate model assemblers were chosen, as the former option would require us to have a deep coupling between HATEOAS types\\nand our DTO classes.\\nDue to the assembler classes being initially only used for links they all reside in the `linkassembler` package.\\n","tokens":97,"id":696}
{"File Name":"bananatabs\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\nadr new Implement as Unix shell scripts\\nThis will create a new, numbered ADR file and open it in your\\neditor of choice (as specified by the VISUAL or EDITOR environment\\nvariable).\\nTo create a new ADR that supercedes a previous one (ADR 9, for example),\\nuse the -s option.\\nadr new -s 9 Use Rust for performance-critical functionality\\nThis will create a new ADR file that is flagged as superceding\\nADR 9, and changes the status of ADR 9 to indicate that it is\\nsuperceded by the new ADR.  It then opens the new ADR in your\\neditor of choice.\\n3. For further information, use the built in help:\\nadr help\\n","tokens":16,"id":2817}
{"File Name":"gatemint-sdk\/adr-027-deterministic-protobuf-serialization.md","Context":"## Context\\n[Protobuf](https:\/\/developers.google.com\/protocol-buffers\/docs\/proto3)\\nseralization is not unique (i.e. there exist a practically unlimited number of\\nvalid binary representations for a protobuf document)<sup>1<\/sup>. For signature\\nverification in Cosmos SDK, signer and verifier need to agree on the same\\nserialization of a SignDoc as defined in\\n[ADR-020](.\/adr-020-protobuf-transaction-encoding.md) without transmitting the\\nserialization. This document describes a deterministic serialization scheme for\\na subset of protobuf documents, that covers this use case but can be reused in\\nother cases as well.\\n","Decision":"The following encoding scheme is proposed to be used by other ADRs.\\n### Scope\\nThis ADR defines a protobuf3 serializer. The output is a valid protobuf\\nserialization, such that every protobuf parser can parse it.\\nNo maps are supported in version 1 due to the complexity of defining a\\nderterministic serialization. This might change in future. Implementations must\\nreject documents containing maps as invalid input.\\n### Serialization rules\\nThe serialization is based on the\\n[protobuf 3 encoding](https:\/\/developers.google.com\/protocol-buffers\/docs\/encoding)\\nwith the following additions:\\n1. Fields must be serialized only once in ascending order\\n2. Extra fields or any extra data must not be added\\n3. [Default values](https:\/\/developers.google.com\/protocol-buffers\/docs\/proto3#default)\\nmust be omitted\\n4. `repeated` fields of scalar numeric types must use\\n[packed encoding](https:\/\/developers.google.com\/protocol-buffers\/docs\/encoding#packed)\\nby default.\\n5. Variant encoding of integers must not be longer than needed.\\nWhile rule number 1. and 2. should be pretty straight forward and describe the\\ndefault behaviour of all protobuf encoders the author is aware of, the 3rd rule\\nis more interesting. After a protobuf 3 deserialization you cannot differentiate\\nbetween unset fields and fields set to the default value<sup>2<\/sup>. At\\nserialization level however, it is possible to set the fields with an empty\\nvalue or omitting them entirely. This is a significant difference to e.g. JSON\\nwhere a property can be empty (`\"\"`, `0`), `null` or undefined, leading to 3\\ndifferent documents.\\nOmitting fields set to default values is valid because the parser must assign\\nthe default value to fields missing in the serialization<sup>3<\/sup>. For scalar\\ntypes, omitting defaults is required by the spec<sup>4<\/sup>. For `repeated`\\nfields, not serializing them is the only way to express empty lists. Enums must\\nhave a first element of numeric value 0, which is the default<sup>5<\/sup>. And\\nmessage fields default to unset<sup>6<\/sup>.\\nOmitting defaults allows for some amount of forward compatibility: users of\\nnewer versions of a protobuf schema produce the same serialization as users of\\nolder versions as long as newly added fields are not used (i.e. set to their\\ndefault value).\\n### Implementation\\nThere are three main implementation strategies, ordered from the least to the\\nmost custom development:\\n- **Use a protobuf serializer that follows the above rules by default.** E.g.\\n[gogoproto](https:\/\/pkg.go.dev\/github.com\/gogo\/protobuf\/gogoproto) is known to\\nbe compliant by in most cases, but not when certain annotations such as\\n`nullable = false` are used. It might also be an option to configure an\\nexisting serializer accordingly.\\n- **Normalize default values before encoding them.** If your serializer follows\\nrule 1. and 2. and allows you to explicitly unset fields for serialization,\\nyou can normalize default values to unset. This can be done when working with\\n[protobuf.js](https:\/\/www.npmjs.com\/package\/protobufjs):\\n```js\\nconst bytes = SignDoc.encode({\\nbodyBytes: body.length > 0 ? body : null, \/\/ normalize empty bytes to unset\\nauthInfoBytes: authInfo.length > 0 ? authInfo : null, \/\/ normalize empty bytes to unset\\nchainId: chainId || null, \/\/ normalize \"\" to unset\\naccountNumber: accountNumber || null, \/\/ normalize 0 to unset\\naccountSequence: accountSequence || null, \/\/ normalize 0 to unset\\n}).finish();\\n```\\n- **Use a hand-written serializer for the types you need.** If none of the above\\nways works for you, you can write a serializer yourself. For SignDoc this\\nwould look something like this in Go, building on existing protobuf utilities:\\n```go\\nif !signDoc.body_bytes.empty() {\\nbuf.WriteUVarInt64(0xA) \/\/ wire type and field number for body_bytes\\nbuf.WriteUVarInt64(signDoc.body_bytes.length())\\nbuf.WriteBytes(signDoc.body_bytes)\\n}\\nif !signDoc.auth_info.empty() {\\nbuf.WriteUVarInt64(0x12) \/\/ wire type and field number for auth_info\\nbuf.WriteUVarInt64(signDoc.auth_info.length())\\nbuf.WriteBytes(signDoc.auth_info)\\n}\\nif !signDoc.chain_id.empty() {\\nbuf.WriteUVarInt64(0x1a) \/\/ wire type and field number for chain_id\\nbuf.WriteUVarInt64(signDoc.chain_id.length())\\nbuf.WriteBytes(signDoc.chain_id)\\n}\\nif signDoc.account_number != 0 {\\nbuf.WriteUVarInt64(0x20) \/\/ wire type and field number for account_number\\nbuf.WriteUVarInt(signDoc.account_number)\\n}\\nif signDoc.account_sequence != 0 {\\nbuf.WriteUVarInt64(0x28) \/\/ wire type and field number for account_sequence\\nbuf.WriteUVarInt(signDoc.account_sequence)\\n}\\n```\\n### Test vectors\\nGiven the protobuf definition `Article.proto`\\n```protobuf\\npackage blog;\\nsyntax = \"proto3\";\\nenum Type {\\nUNSPECIFIED = 0;\\nIMAGES = 1;\\nNEWS = 2;\\n};\\nenum Review {\\nUNSPECIFIED = 0;\\nACCEPTED = 1;\\nREJECTED = 2;\\n};\\nmessage Article {\\nstring title = 1;\\nstring description = 2;\\nuint64 created = 3;\\nuint64 updated = 4;\\nbool public = 5;\\nbool promoted = 6;\\nType type = 7;\\nReview review = 8;\\nrepeated string comments = 9;\\nrepeated string backlinks = 10;\\n};\\n```\\nserializing the values\\n```yaml\\ntitle: \"The world needs change \ud83c\udf33\"\\ndescription: \"\"\\ncreated: 1596806111080\\nupdated: 0\\npublic: true\\npromoted: false\\ntype: Type.NEWS\\nreview: Review.UNSPECIFIED\\ncomments: [\"Nice one\", \"Thank you\"]\\nbacklinks: []\\n```\\nmust result in the serialization\\n```\\n0a1b54686520776f726c64206e65656473206368616e676520f09f8cb318e8bebec8bc2e280138024a084e696365206f6e654a095468616e6b20796f75\\n```\\nWhen inspecting the serialized document, you see that every second field is\\nomitted:\\n```\\n$ echo 0a1b54686520776f726c64206e65656473206368616e676520f09f8cb318e8bebec8bc2e280138024a084e696365206f6e654a095468616e6b20796f75 | xxd -r -p | protoc --decode_raw\\n1: \"The world needs change \\360\\237\\214\\263\"\\n3: 1596806111080\\n5: 1\\n7: 2\\n9: \"Nice one\"\\n9: \"Thank you\"\\n```\\n","tokens":135,"id":32}
{"File Name":"opg-use-an-lpa\/0006-continuous-delivery.md","Context":"## Context\\nFrequent small releases are preferred strategy to prevent accumulating risk and deliver benefits to users more quickly\\n","Decision":"Aim for continuous integration and continuous delivery. [Proposed Flow](..\/diagrams\/CI%20CD%20Pipelines.png)\\n","tokens":22,"id":4849}
{"File Name":"simple-server\/015-online-patient-lookup.md","Context":"## Context\\nWe want to ensure that a patient\u2019s medical records within the state\\nare always available to the nurse for treatment if they are connected\\nto the internet. See related\\n[PRD](https:\/\/docs.google.com\/document\/d\/1q6cppByQULfh3_mMXC4BJpiNN9Uc_awA6rreeEtUBaM\/edit#)\\nfor more details on the feature and related specifications.\\nThe lookup will be a new API on the server that the mobile app will\\ncall. The following aspects of the problem are addressed in this ADR:\\n- API contract\\n- Data retention\\n- Access restrictions\\n- Audit logging\\n- Rate limiting\\n","Decision":"### Data retention\\nThere are two broad kinds of data retention on the app at a facility:\\n1. Permanent: patient is within sync criteria: the same block, has an\\nappointment, or is assigned to the facility\\n2. Temporary: patient is not within the sync criteria\\nIn order to have a better control of the retention, we will send this\\ninformation as `retention type` from the server. We will also send a\\n`retention duration` in seconds, which will be a static number to\\nbegin with, but can later vary depending on the state, country or\\nother factors. See the API contract below for more details.\\nWe will implement temporary retention in the app with this\\nfeature. With every patient retrieved via the lookup API, we will\\nstore the time until which the record needs to be retained if the\\nretention type is temporary.\\n```\\nretain_until = sync_time (now) + retention duration\\n```\\nAfter a sync (that happens every 15 minutes), we will delete the\\nrecords that:\\n- should be retained temporarily\\n- and have passed their retention time period: `now > retain_until`\\nIf a `temporary` record is synced via the sync API, then the retention\\ntype should be set to `permanent`.\\nWe will treat manual and automatic syncs in the same way, and\\nconfigure the retention period to suit the needs of showing patients\\nin the recent list, etc.\\n~~_Alternatively_: we can choose to hard-code the retention period on\\nthe app.~~\\n### API contract\\nFor the request, we will use the endpoint `GET`:\\nhttps:\/\/api.simple.org\/api\/v4\/patients\/identifier\/, where `identifier`\\nis any valid patient business identifier.\\nThe type of the identifier will not be specified in the request because:\\n- the client might not be able to discern the type\\n- the same identifier might exist across different types (BP passport,\\nNHID, or a future type)\\nIn the response, will return a _list of patients_ that have that\\nidentifer. Note that it is possible for more than a single patient to\\nhave the same identifier. The response contract will be similar to\\n[the API used in the BP Passport\\nApp](https:\/\/api.simple.org\/api-docs#tag\/Patient\/paths\/~1patient\/get):\\n````\\n{ \"patients\": [{\\n\"id\": \"497f6eca-6276-4993-bfeb-53cbbbba6f08\",\\n\"full_name\": \"string\",\\n\"age\": 0,\\n\"gender\": \"male\",\\n\"status\": \"active\",\\n\"recorded_at\": \"2019-08-24T14:15:22Z\",\\n\"reminder_consent\": \"granted\",\\n\"phone_numbers\": [],\\n\"address\": {...},\\n\"registration_facility\": {...},\\n\"medical_history\": {...},\\n\"blood_pressures\": [],\\n\"blood_sugars\": [],\\n\"appointments\": [],\\n\"medications\": [],\\n\"business_identifiers\": [],\\n\"retention\": {\\n\"type\": \"temporary\", \/\/ or \"permanent\"\\n\"duration_seconds\": 3600\\n}\\n}]\\n}\\n````\\nThis API will have a 5s timeout from the android app to prevent delay\\nin patient care.\\n~~_Alternatively_: We could use a v5 prefix to disambiguate with the\\nPassport app lookup API more clearly.~~\\n### Access restrictions\\nAccess for this API will be restricted to the state that the user is\\nregistered in. Trying to lookup a patient that resides outside the\\nstate will return 404.\\nIf a patient has travelled across states, and has records in both\\nstates corresponding to the same identifier, then only the patients\\nbelonging to the requesting user's state will be returned in the API.\\n### Audit logging\\nSimilar to the sync API, we will create a _lookup audit log_ for\\nsuccessful lookups performed using this API. This is separate from the\\nsync audit log since looking up a specific patient is conceptually\\ndifferent from fetching the block's patients. This will have the\\nfollowing fields in it:\\n````\\n{\\nuser_id: user.id,\\nfacility_id: current_facility.id,\\nidentifier: identifier,\\npatient_ids: [patient_id],\\ntime: Time.current\\n}\\n````\\n~~_Alternatively_: we can try to repurpose the existing audit logs for\\nthis, while capturing all the information.~~\\n### Rate limiting\\nWe currently rate limit our authentication endpoints using\\n[rack-attack](https:\/\/github.com\/rack\/rack-attack). We will do the\\nsame for this API. See the [relevant section in the\\nPRD](https:\/\/docs.google.com\/document\/d\/1q6cppByQULfh3_mMXC4BJpiNN9Uc_awA6rreeEtUBaM\/edit#)\\nfor the rationale and the chosen rate limiting configuration.\\n","tokens":145,"id":1711}
{"File Name":"govuk-aws\/0010-terraform-directory-structure.md","Context":"## Context\\nWe should have a consistent directory structure for Terraform from the beginning.\\nSome initial requirements:\\n- We want to separate code from data, so in the future we can open source the code without disclosing our implementation details,\\nmaybe keeping the data in a private Github repository\\n- We want to be able to encrypt sensitive data in the repository: we want to support sensitive data encryption as part of the same\\nprocess, without having to manage secrets in a different repository, with different scripts, etc.\\n- We want to create Terraform modules to reuse code\\n- We want to separate Terraform code into different projects, each one managing an infrastructure tier or application stack. This\\nis especially important to separate resources between GOV.UK applications.\\n","Decision":"The initial solution presents three directories: data, modules and projects:\\n- The data directory contains a subdirectory per Terraform project, to store variable values that can be customised per environment.\\n- The data directory also contains \\_secrets files with sensitive data encrypted with 'sops'\\n- The modules directory contains a subdirectory per Terraform provider\\n- The projects directory contains the Terraform stacks\/tiers\\n- Projects will be named such that:\\n+ Those to deploy common infrastructure will have a `govuk` prefix (e.g. networking, DNS zones)\\n+ Those to deploy applications (or groups of applications) will have an `app` prefix (e.g. frontend, puppetmaster).\\n```\\n\u251c\u2500\u2500 data\\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 base\\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 common.tfvars\\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 integration.tfvars\\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 my-application\\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 common.tfvars\\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 integration.tfvars\\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 integration_secrets.json\\n\u251c\u2500\u2500 modules\\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 aws\\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 route53_zone\\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2514\u2500\u2500  ...\\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 mysql_database_instance\\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 ...\\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 network\\n\u2502\u00a0\u00a0         \u2514\u2500\u2500 ...\\n\u2514\u2500\u2500 projects\\n\u251c\u2500\u2500 base\\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 integration.backend\\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 main.tf\\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 variables.tf\\n\u2514\u2500\u2500 my-application\\n\u251c\u2500\u2500 integration.backend\\n\u251c\u2500\u2500 main.tf\\n\u2514\u2500\u2500 variables.tf\\n```\\n","tokens":155,"id":4054}
{"File Name":"xebikart-infra\/005-use-kubectl-and-helm-for-k8s-deployment.md","Context":"## Context and Problem Statement\\nWe want to **deploy** pods, services, and other resources on **Kubernetes**,\\nwhich is a GKE cluster for now.\\nWhat tool should we use to do this?\\n## Decision Drivers\\n- Ease of use\\n- How \"far\" is the tool from the Kubernetes lifecycle? - _\"Is it trailing\\nafter upstream and if so, how far away from it\"_\\n- Ability to express deployments in a descriptive \"as-code\" way\\n- Ease of use in a CI\/CD pipeline - _Does it require a local state? Weird\\nrequirements? Auth to other systems?_\\n- Reliability - _\"Will the tool usage often lead to messed up\\nservices\/cluster?\"_\\n","Decision":"- Ease of use\\n- How \"far\" is the tool from the Kubernetes lifecycle? - _\"Is it trailing\\nafter upstream and if so, how far away from it\"_\\n- Ability to express deployments in a descriptive \"as-code\" way\\n- Ease of use in a CI\/CD pipeline - _Does it require a local state? Weird\\nrequirements? Auth to other systems?_\\n- Reliability - _\"Will the tool usage often lead to messed up\\nservices\/cluster?\"_\\nChosen option: \"A mix of **kubectl** and **Helm**\", because:\\n- `kubectl` is the native way of deploying to Kubernetes and it thus the most\\ndocumented, widely-used, appropriate and up-to-date.\\n- Helm fits best for complex stacks deployment and we're eager to try things\\nsuch as Helm charts repositories!\\nHelm will be used as an alternative to `kubectl` in a second part, when we'll\\nramp up things and when we will need to deploy more complex stacks. The\\nbeginning will be `kubectl` for our own services, and Helm for\\ncomponents\/middleware\/databases with existing charts such as RabbitMQ!\\nThis imply that the provided Kubernetes cluster(s) for this project will need\\nto be **provisionned with Helm requirements such as Tiller**.\\n","tokens":155,"id":982}
{"File Name":"timdex\/0003-follow-twelve-factor-methodology.md","Context":"## Context\\nDesigning modern scalable cloud based applications requires intentionally\\ndesigning the architecture to take advantage of the cloud.\\nOne leading way to do that is\\n[The Twelve Factor](https:\/\/12factor.net) methodology.\\n","Decision":"We will follow Twelve Factor methodology.\\n","tokens":48,"id":2376}
{"File Name":"opg-data\/0014-shared-services.md","Context":"## Context\\nAs we move to a structure where products share services rather than being purely isolated systems, we are moving away from a model where the API Gateway acts solely as a pathway into the Sirius Backend API. Early integrations such as providing LPAs to __Use and LPA__ or passing files from the __Digital Deputies Reporting Service__ were thin layers of translation over the existing Sirius API.\\nThe next phase of our API work is to move to a model where multiple products share and use an API that owns all its data, and encapsulates a need in the OPG domain. Our first example is the Use an LPA codes service, which provides data for both Sirius Case Management and the Use an LPA service, but is independent of both.\\nThis is in parallel to work that is splitting Sirius into smaller services, with the expectation that the two streams of work will eventually converge as a set of isolated services that can be recomposed into new products, our \"Service Swarm\".\\n","Decision":"Shared services will sit behind the OPG Data API Gateway and follow the [0009-api-domain-structure.md](api domain structure decision). Shared services will be isolated and own a particular domain for our applications. That is, they own a particular business process or kind of data, for example the generation and expiry of access codes.\\nThese services currently live within the boundary of the \"Sirius\" account, but are deployed independently.\\n","tokens":201,"id":2187}
{"File Name":"tendermint\/adr-030-consensus-refactor.md","Context":"## Context\\nOne of the biggest challenges this project faces is to proof that the\\nimplementations of the specifications are correct, much like we strive to\\nformaly verify our alogrithms and protocols we should work towards high\\nconfidence about the correctness of our program code. One of those is the core\\nof Tendermint - Consensus - which currently resides in the `consensus` package.\\nOver time there has been high friction making changes to the package due to the\\nalgorithm being scattered in a side-effectful container (the current\\n`ConsensusState`). In order to test the algorithm a large object-graph needs to\\nbe set up and even than the non-deterministic parts of the container makes will\\nprevent high certainty. Where ideally we have a 1-to-1 representation of the\\n[spec](https:\/\/github.com\/tendermint\/spec), ready and easy to test for domain\\nexperts.\\nAddresses:\\n- [#1495](https:\/\/github.com\/tendermint\/tendermint\/issues\/1495)\\n- [#1692](https:\/\/github.com\/tendermint\/tendermint\/issues\/1692)\\n","Decision":"To remedy these issues we plan a gradual, non-invasive refactoring of the\\n`consensus` package. Starting of by isolating the consensus alogrithm into\\na pure function and a finite state machine to address the most pressuring issue\\nof lack of confidence. Doing so while leaving the rest of the package in tact\\nand have follow-up optional changes to improve the sepration of concerns.\\n### Implementation changes\\nThe core of Consensus can be modelled as a function with clear defined inputs:\\n* `State` - data container for current round, height, etc.\\n* `Event`- significant events in the network\\nproducing clear outputs;\\n* `State` - updated input\\n* `Message` - signal what actions to perform\\n```go\\ntype Event int\\nconst (\\nEventUnknown Event = iota\\nEventProposal\\nMajority23PrevotesBlock\\nMajority23PrecommitBlock\\nMajority23PrevotesAny\\nMajority23PrecommitAny\\nTimeoutNewRound\\nTimeoutPropose\\nTimeoutPrevotes\\nTimeoutPrecommit\\n)\\ntype Message int\\nconst (\\nMeesageUnknown Message = iota\\nMessageProposal\\nMessageVotes\\nMessageDecision\\n)\\ntype State struct {\\nheight      uint64\\nround       uint64\\nstep        uint64\\nlockedValue interface{} \/\/ TODO: Define proper type.\\nlockedRound interface{} \/\/ TODO: Define proper type.\\nvalidValue  interface{} \/\/ TODO: Define proper type.\\nvalidRound  interface{} \/\/ TODO: Define proper type.\\n\/\/ From the original notes: valid(v)\\nvalid       interface{} \/\/ TODO: Define proper type.\\n\/\/ From the original notes: proposer(h, r)\\nproposer    interface{} \/\/ TODO: Define proper type.\\n}\\nfunc Consensus(Event, State) (State, Message) {\\n\/\/ Consolidate implementation.\\n}\\n```\\nTracking of relevant information to feed `Event` into the function and act on\\nthe output is left to the `ConsensusExecutor` (formerly `ConsensusState`).\\nBenefits for testing surfacing nicely as testing for a sequence of events\\nagainst algorithm could be as simple as the following example:\\n``` go\\nfunc TestConsensusXXX(t *testing.T) {\\ntype expected struct {\\nmessage Message\\nstate   State\\n}\\n\/\/ Setup order of events, initial state and expectation.\\nvar (\\nevents = []struct {\\nevent Event\\nwant  expected\\n}{\\n\/\/ ...\\n}\\nstate = State{\\n\/\/ ...\\n}\\n)\\nfor _, e := range events {\\nsate, msg = Consensus(e.event, state)\\n\/\/ Test message expectation.\\nif msg != e.want.message {\\nt.Fatalf(\"have %v, want %v\", msg, e.want.message)\\n}\\n\/\/ Test state expectation.\\nif !reflect.DeepEqual(state, e.want.state) {\\nt.Fatalf(\"have %v, want %v\", state, e.want.state)\\n}\\n}\\n}\\n```\\nHeight           int64\\nRound            int\\nBlockID          BlockID\\n}\\ntype TriggerTimeout struct {\\nHeight           int64\\nRound            int\\nDuration         Duration\\n}\\ntype RoundStep int\\nconst (\\nRoundStepUnknown RoundStep = iota\\nRoundStepPropose\\nRoundStepPrevote\\nRoundStepPrecommit\\nRoundStepCommit\\n)\\ntype State struct {\\nHeight           int64\\nRound            int\\nStep             RoundStep\\nLockedValue      BlockID\\nLockedRound      int\\nValidValue       BlockID\\nValidRound       int\\nValidatorId      int\\nValidatorSetSize int\\n}\\nfunc proposer(height int64, round int) int {}\\nfunc getValue() BlockID {}\\nfunc Consensus(event Event, state State) (State, Message, TriggerTimeout) {\\nmsg = nil\\ntimeout = nil\\nswitch event := event.(type) {\\ncase EventNewHeight:\\nif event.Height > state.Height {\\nstate.Height = event.Height\\nstate.Round = -1\\nstate.Step = RoundStepPropose\\nstate.LockedValue = nil\\nstate.LockedRound = -1\\nstate.ValidValue = nil\\nstate.ValidRound = -1\\nstate.ValidatorId = event.ValidatorId\\n}\\nreturn state, msg, timeout\\ncase EventNewRound:\\nif event.Height == state.Height and event.Round > state.Round {\\nstate.Round = eventRound\\nstate.Step = RoundStepPropose\\nif proposer(state.Height, state.Round) == state.ValidatorId {\\nproposal = state.ValidValue\\nif proposal == nil {\\nproposal = getValue()\\n}\\nmsg =  MessageProposal { state.Height, state.Round, proposal, state.ValidRound }\\n}\\ntimeout = TriggerTimeout { state.Height, state.Round, timeoutPropose(state.Round) }\\n}\\nreturn state, msg, timeout\\ncase EventProposal:\\nif event.Height == state.Height and event.Round == state.Round and\\nevent.Sender == proposal(state.Height, state.Round) and state.Step == RoundStepPropose {\\nif event.POLRound >= state.LockedRound or event.BlockID == state.BlockID or state.LockedRound == -1 {\\nmsg = MessageVote { state.Height, state.Round, event.BlockID, Prevote }\\n}\\nstate.Step = RoundStepPrevote\\n}\\nreturn state, msg, timeout\\ncase TimeoutPropose:\\nif event.Height == state.Height and event.Round == state.Round and state.Step == RoundStepPropose {\\nmsg = MessageVote { state.Height, state.Round, nil, Prevote }\\nstate.Step = RoundStepPrevote\\n}\\nreturn state, msg, timeout\\ncase Majority23PrevotesBlock:\\nif event.Height == state.Height and event.Round == state.Round and state.Step >= RoundStepPrevote and event.Round > state.ValidRound {\\nstate.ValidRound = event.Round\\nstate.ValidValue = event.BlockID\\nif state.Step == RoundStepPrevote {\\nstate.LockedRound = event.Round\\nstate.LockedValue = event.BlockID\\nmsg = MessageVote { state.Height, state.Round, event.BlockID, Precommit }\\nstate.Step = RoundStepPrecommit\\n}\\n}\\nreturn state, msg, timeout\\ncase Majority23PrevotesAny:\\nif event.Height == state.Height and event.Round == state.Round and state.Step == RoundStepPrevote {\\ntimeout = TriggerTimeout { state.Height, state.Round, timeoutPrevote(state.Round) }\\n}\\nreturn state, msg, timeout\\ncase TimeoutPrevote:\\nif event.Height == state.Height and event.Round == state.Round and state.Step == RoundStepPrevote {\\nmsg = MessageVote { state.Height, state.Round, nil, Precommit }\\nstate.Step = RoundStepPrecommit\\n}\\nreturn state, msg, timeout\\ncase Majority23PrecommitBlock:\\nif event.Height == state.Height {\\nstate.Step = RoundStepCommit\\nstate.LockedValue = event.BlockID\\n}\\nreturn state, msg, timeout\\ncase Majority23PrecommitAny:\\nif event.Height == state.Height and event.Round == state.Round {\\ntimeout = TriggerTimeout { state.Height, state.Round, timeoutPrecommit(state.Round) }\\n}\\nreturn state, msg, timeout\\ncase TimeoutPrecommit:\\nif event.Height == state.Height and event.Round == state.Round {\\nstate.Round = state.Round + 1\\n}\\nreturn state, msg, timeout\\n}\\n}\\nfunc ConsensusExecutor() {\\nproposal = nil\\nvotes = HeightVoteSet { Height: 1 }\\nstate = State {\\nHeight:       1\\nRound:        0\\nStep:         RoundStepPropose\\nLockedValue:  nil\\nLockedRound:  -1\\nValidValue:   nil\\nValidRound:   -1\\n}\\nevent = EventNewHeight {1, id}\\nstate, msg, timeout = Consensus(event, state)\\nevent = EventNewRound {state.Height, 0}\\nstate, msg, timeout = Consensus(event, state)\\nif msg != nil {\\nsend msg\\n}\\nif timeout != nil {\\ntrigger timeout\\n}\\nfor {\\nselect {\\ncase message := <- msgCh:\\nswitch msg := message.(type) {\\ncase MessageProposal:\\ncase MessageVote:\\nif msg.Height == state.Height {\\nnewVote = votes.AddVote(msg)\\nif newVote {\\nswitch msg.Type {\\ncase Prevote:\\nprevotes = votes.Prevotes(msg.Round)\\nif prevotes.WeakCertificate() and msg.Round > state.Round {\\nevent = EventNewRound { msg.Height, msg.Round }\\nstate, msg, timeout = Consensus(event, state)\\nstate = handleStateChange(state, msg, timeout)\\n}\\nif blockID, ok = prevotes.TwoThirdsMajority(); ok and blockID != nil {\\nif msg.Round == state.Round and hasBlock(blockID) {\\nevent = Majority23PrevotesBlock { msg.Height, msg.Round, blockID }\\nstate, msg, timeout = Consensus(event, state)\\nstate = handleStateChange(state, msg, timeout)\\n}\\nif proposal != nil and proposal.POLRound == msg.Round and hasBlock(blockID) {\\nevent = EventProposal {\\nHeight: state.Height\\nRound:  state.Round\\nBlockID: blockID\\nPOLRound: proposal.POLRound\\nSender: message.Sender\\n}\\nstate, msg, timeout = Consensus(event, state)\\nstate = handleStateChange(state, msg, timeout)\\n}\\n}\\nif prevotes.HasTwoThirdsAny() and msg.Round == state.Round {\\nevent = Majority23PrevotesAny { msg.Height, msg.Round, blockID }\\nstate, msg, timeout = Consensus(event, state)\\nstate = handleStateChange(state, msg, timeout)\\n}\\ncase Precommit:\\n}\\n}\\n}\\ncase timeout := <- timeoutCh:\\ncase block := <- blockCh:\\n}\\n}\\n}\\nfunc handleStateChange(state, msg, timeout) State {\\nif state.Step == Commit {\\nstate = ExecuteBlock(state.LockedValue)\\n}\\nif msg != nil {\\nsend msg\\n}\\nif timeout != nil {\\ntrigger timeout\\n}\\n}\\n```\\n### Implementation roadmap\\n* implement proposed implementation\\n* replace currently scattered calls in `ConsensusState` with calls to the new\\n`Consensus` function\\n* rename `ConsensusState` to `ConsensusExecutor` to avoid confusion\\n* propose design for improved separation and clear information flow between\\n`ConsensusExecutor` and `ConsensusReactor`\\n","tokens":235,"id":1967}
{"File Name":"visit-plannr\/0004-event-contracts.md","Context":"## Context\\nWhen returning to development after a break and needing to create events in integration tests it was difficult to identify and construct events.\\nAlso, producers that only produce one event type were still able to set an event type so the event contract was very weak.\\n","Decision":"All events will be in a file with a name formatted `eventType.event.js` where the event type is in past tense and is duplicated in a property of the event with key `type`.\\nThe JS module exported by that file will be one or more functions that return the event\\nE.g. `decisionMade.event.js` would export a function that takes a decision and returns\\n```\\n{\\ntype: \"decisionMade\",\\ndecision: \"the provided decision\"\\n}\\n```\\n","tokens":54,"id":1989}
{"File Name":"airline-reservation-system\/0004-TCH-RES-use-document-database-as-a-aggregate-data-repository.md","Context":"## Context and Problem Statement\\n**What kind of database should be used to storage aggregates data ?**\\nConsidered options:\\n1. Document database (MongoDB)\\n2. Relational database\\nDrivers:\\n* Simple objects will be saved.\\n* There will be no relationship between objects .\\n* A Read Model combining data from different aggregates is required .\\n","Decision":"Option no 1 - document database - MongoDB.\\n","tokens":78,"id":608}
{"File Name":"mat-process-utils\/0002-use-typescript.md","Context":"## Context\\nWe want to be confident about the code we write, and for it to be\\nself-documenting as much as possible.\\n[TypeScript](https:\/\/www.typescriptlang.org\/) is a compiled language with\\noptional typing. It's a superset of JavaScript, so is familiar to developers who\\nknow JavaScript. It has wide editor support.\\nAs of writing, TypeScript is used by over\\n[1.4 million repositories](https:\/\/github.com\/microsoft\/TypeScript\/network\/dependents?package_id=UGFja2FnZS01MTE3ODUxNjg%3D)\\non GitHub.\\n","Decision":"We will use TypeScript.\\n","tokens":134,"id":3762}
{"File Name":"mario\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":3420}
{"File Name":"pottery\/0002-bootstrap-by-copying-the-adr-tools-source-code.md","Context":"## Context\\nI'm writing this very fast as part of a hackday.\\nI want the app to be like [adr-tools](https:\/\/github.com\/npryce\/adr-tools): a command line tool that stores data in a project's version control system.  I want it to have built-in help, like adr-tools.\\n","Decision":"Copy the source of adr-tools.  Delete the scripts that don't make sense for Pottery.  Then do a bulk find-and-replace of \"adr\" to \"pottery\".  Bish bosh!\\n","tokens":70,"id":2511}
{"File Name":"akvo-product-design\/ADR-002.md","Context":"# Context\\nWe need to store the imported data from the _Connect_ process in an\\nhomogeneous way for easy retrieval.\\n# Decision\\nAfter checking how other tools work (like Kibana) it seems that\\nElasticSearch is a good choice for storing the imported data.\\nElasticSearch has its own DSL for building complex queries and we can\\nreuse that part.\\n# Status\\nAccepted\\n# Consequences\\n* We need to know how to run and maintain a ElasticSearch cluster\\n* We need to address the security of the cluster ourselves. Elastic\\nhas a security plugin but is not open source.\\nExamples:\\n- http:\/\/floragunn.com\/searchguard\\n- http:\/\/keycloak.github.io\/docs\/userguide\/keycloak-server\/html\/proxy.html\\n","Decision":"After checking how other tools work (like Kibana) it seems that\\nElasticSearch is a good choice for storing the imported data.\\nElasticSearch has its own DSL for building complex queries and we can\\nreuse that part.\\n# Status\\nAccepted\\n# Consequences\\n* We need to know how to run and maintain a ElasticSearch cluster\\n* We need to address the security of the cluster ourselves. Elastic\\nhas a security plugin but is not open source.\\nExamples:\\n- http:\/\/floragunn.com\/searchguard\\n- http:\/\/keycloak.github.io\/docs\/userguide\/keycloak-server\/html\/proxy.html\\n","tokens":161,"id":2804}
{"File Name":"openlobby-server\/0010-replace-flask-with-django.md","Context":"## Context\\nFlask turned out to be poorly designed piece of software which relays on too\\nmuch magic like manipulations of global objects like `g`.\\nSeems like we will also decide to use relational database.\\n","Decision":"We will switch to Django. It's not only well written server but it has also\\n\"batteries included\" like a good ORM layer. And some other features like\\nmiddlewares will simplify things.\\n","tokens":47,"id":466}
{"File Name":"hmpps-interventions-ui\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael\\nNygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":463}
{"File Name":"modular-monolith-with-ddd\/0005-create-one-rest-api-module.md","Context":"## Context\\nWe need to expose the API of our application to the outside world. For now, we expect one client of our application - FrontEnd SPA application.\\n","Decision":"Solution 1.\\nCreating separate API projects for each module will add complexity and little value. Grouping endpoints for a particular business module in a special directory is enough. Another layer on top of the module is unnecessary.\\n","tokens":34,"id":892}
{"File Name":"corona-hackathon\/0007-knowledge-crunching.md","Context":"## Context\\nWe have to make a decision, how we describe the core use cases of the application.\\n","Decision":"We use [Domain Storytelling](https:\/\/domainstorytelling.org\/) to describe core use cases & interactions.\\nAll user stories can be found in the [`.\/doc\/user-stories`](.\/doc\/user-stories) directory.\\n","tokens":22,"id":3659}
{"File Name":"ehoks-ui\/0003-use-single-callerid-for-both-oppija-and-virkailija.md","Context":"## Context\\nEvery service is required to pass a Caller-Id header with its requests and previously eHOKS frontend had separate\\nids for oppija and virkailija. While the codebase is largely shared between oppija and virkailija the services themselves\\nare separate and hence the separate ids were created. While the headers were simple to add to requests made by\\ncomponents only used by either oppija or virkailija, dynamically figuring out which id should be used in the shared\\ncomponents at any given time proved harder.\\n","Decision":"The separate ids will be replaced by a single frontend Caller-Id. Since all the requests made by both oppija and\\nvirkailija frontends go through the eHOKS backend service and don't call any external services directly this\\nshould be sufficient. The requests from oppija and virkailija can be distinquished from each other via other means,\\neg. they use different backend APIs altogether.\\n","tokens":112,"id":596}
{"File Name":"reactive\/0003-no-readable.md","Context":"## Context\\nSvelte offers a very interesting `readable` API that could work with this library.\\nAn implementation might look similar to this:\\n```ts\\nexport function readable<T>(initialValue: T, updater?: Updater<T>) {\\nconst value = ref(initialValue);\\nupdater?.((newValue: T) => {\\nvalue.current = newValue;\\n});\\nreturn {\\nget current() {\\nreturn value.current;\\n},\\n};\\n}\\n```\\nThe idea behind it is that it would provide a readonly way to having changing content. Similar to what an Observable would provide.\\nOne of the major questions, however, is whether this API would be beneficial or whether we should aim for something else.\\n","Decision":"While the `readable` API on its own offers a very nice functionality, it does not add enough to make up for the required learning effort as it does not blend in\\nwell enough with the framework.\\n","tokens":154,"id":1029}
{"File Name":"adr-tools\/0003-single-command-with-subcommands.md","Context":"## Context\\nThe tool provides a number of related commands to create\\nand manipulate architecture decision records.\\nHow can the user find out about the commands that are available?\\n","Decision":"The tool defines a single command, called `adr`.\\nThe first argument to `adr` (the subcommand) specifies the\\naction to perform.  Further arguments are interpreted by the\\nsubcommand.\\nRunning `adr` without any arguments lists the available\\nsubcommands.\\nSubcommands are implemented as scripts in the same\\ndirectory as the `adr` script.  E.g. the subcommand `new` is\\nimplemented as the script `adr-new`, the subcommand `help`\\nas the script `adr-help` and so on.\\nHelper scripts that are part of the implementation but not\\nsubcommands follow a different naming convention, so that\\nsubcommands can be listed by filtering and transforming script\\nfile names.\\n","tokens":35,"id":3565}
{"File Name":"tendermint\/adr-047-handling-evidence-from-light-client.md","Context":"## Context\\nThe bisection method of header verification used by the light client exposes\\nitself to a potential attack if any block within the light clients trusted period has\\na malicious group of validators with power that exceeds the light clients trust level\\n(default is 1\/3). To improve light client (and overall network) security, the light\\nclient has a detector component that compares the verified header provided by the\\nprimary against witness headers. This ADR outlines the process of mitigating attacks\\non the light client by using witness nodes to cross reference with.\\n","Decision":"The light client will be divided into two components: a `Verifier` (either sequential or\\nskipping) and a `Detector` (see [Informal's Detector](https:\/\/github.com\/informalsystems\/tendermint-rs\/blob\/master\/docs\/spec\/lightclient\/detection\/detection.md))\\n. The detector will take the trace of headers from the primary and check it against all\\nwitnesses. For a witness with a diverging header, the detector will first verify the header\\nby bisecting through all the heights defined by the trace that the primary provided. If valid,\\nthe light client will trawl through both traces and find the point of bifurcation where it\\ncan proceed to extract any evidence (as is discussed in detail later).\\nUpon successfully detecting the evidence, the light client will send it to both primary and\\nwitness before halting. It will not send evidence to other peers nor continue to verify the\\nprimary's header against any other header.\\n","tokens":115,"id":1981}
{"File Name":"opg-digideps\/0002-use-amazon-aurora-for-application-database.md","Context":"## Context\\nDigiDeps uses a Postgres database to store persistent information. Since the project was first set up, we have been using an Amazon RDS hosted instance of Postgres for this. However, this hosting option lacks scalability so we have to pay for a full database host 24\/7 for each environment.\\nWe have several environments which do not need to be highly available. This includes \"ephemeral\" development environments, the \"main\" environment used in CI, and the training environment. We do not need to run a database for these environments outside of working hours, and often inside of them too.\\n","Decision":"We will use Amazon Aurora Serverless for environments which do not need to always be on. Aurora automatically scales with usage, including pausing completely if the database isn't in use.\\n","tokens":128,"id":4411}
{"File Name":"tacboard\/001-Selectors-and-Reselect.md","Context":"## Context\\nUsing [selectors](https:\/\/medium.com\/@matthew.holman\/what-is-a-redux-selector-a517acee1fe8) is a best practice for Redux-applications to derive information from a minimal state while keeping the `mapStateToProps` and  `mapDispatchToProps` functions small.\\nI will use this pattern, because I made good experience with them on our work-project.\\nI am considering to use the [Reselect](https:\/\/github.com\/reduxjs\/reselect) for this, because it seems to be the \"industry-standard\".\\nArguments for using Rselect:\\n- Reselect is very popular and has a lot of documentation on the official pages and on StackOverflow\\n- Reselect handles memoisation\\n- Adding reselect now is not much effort, because there are no selectors yet. Adding it later will require touching all selectors again.\\nArguments against using Reselect:\\n- Selectors can also be written as plain functions to achieve the same separation of state and derived data\\n- An additional library adds to the size of the bundle\\n- The state of the tacboard is not that complicate, I don't expect memoisation to become a problem soon (but who knows?)\\n- Many of the selectors can not be cachedefficiently, because they depend on component-props (e.g. whether a field is empty or not requires the field-index), see [Reselect-Docs](https:\/\/github.com\/reduxjs\/reselect#accessing-react-props-in-selectors)\\n","Decision":"I will use Reselect to rewrite the selectors. The only real downside seems to be the increased package-size, but that is not a big concern at this point.\\nIt seems more important to avoid having to rewrite everything later. There also is a way to get memoization for selectors that use component props (see [Reselect-Docs](https:\/\/github.com\/reduxjs\/reselect#sharing-selectors-with-props-across-multiple-component-instances)).\\n","tokens":313,"id":2592}
{"File Name":"stagecraft\/adr-001-security.md","Context":"# Context\\nAs someone responsible for ensuring accurate data is published by the\\ngovernment, I need to ensure that people authoring content can only\\nview and edit things that they own, so that there are no embarrassing\\nupdates published.\\nWe need to ensure that datasets and dashboards can only be edited by\\nthe right people.\\nWe currently have signonotron2 accounts for all of the people using\\nthis application. That will continue to be in place for the future.\\nWe need a way of restricting which signon accounts have access to\\ndifferent entities within the admin application.\\nWe have tried to model this in various ways.\\nAs someone working at the DVLA, should I have the ability to edit\\nall of the DVLAs dashboards? The answer to that is no.\\nWe've had some discussions with our user population, but what is\\nactually good still isn't known at this point. So we are going to\\nincrementally do this functionality, as a series of vertical slices,\\nand accept that we will need to refactor, rework and do data\\nmigrations as we learn more about the problem space and how best to\\nmeet those needs.\\nInitially, we proposed using custom Django Managers in stagecraft to\\nimplement this. We don't think this is the right approach; it smells\\ntoo complex.\\nA simpler start point will be to define the URLs (resources) and\\nhave permissions per resource.\\n# Decision\\nDefine the matrix of URLs, and map permissions per resource.\\n# Status\\nAccepted.\\n# Consequences\\nTrading shipping a thing with future technical debt for now.\\n","Decision":"Define the matrix of URLs, and map permissions per resource.\\n# Status\\nAccepted.\\n# Consequences\\nTrading shipping a thing with future technical debt for now.\\n","tokens":338,"id":3394}
{"File Name":"front-end-monorepo\/adr-34.md","Context":"## Context\\nOn PFE, workflows have a complex logic of programmatic workflow selection. [ADR 18](adr-18.md) has documented the original context. This ADR is a partial superseding of the decision on how to select the workflow when navigating to `\/classify`\\n","Decision":"We will continue to use the routing behavior defined by [ADR 18](adr-18.md)\\n### `\/classify` behaviour\\nWe will not programmatically select a workflow except for one case: when there is only one active workflow. For all other cases, we have a UI prompt for the volunteer to manually select which workflow they wish to contribute to.\\n### Error handling\\nCases when the workflow is not available are:\\n- The workflow does not actually exist, so this will 404.\\n- The workflow exists, but is in an inactive state. The activeness state effectively functions as a permissions mechanism since users with the project owner, collaborator, or expert role or Zooniverse admins can still request and load inactive workflows. Users with the correct role should be able to load an inactive workflow with a visual indication in the UI it is inactive. All other users will receive a 404.\\n- The workflow exists and is active, however, the project uses workflow assignment and the workflow has not been assigned to the volunteer yet. The classify page should load, the classifier itself doesn't, and the workflow selection prompt is rendered for the volunteer to choose between the workflows they have been assigned.\\n","tokens":58,"id":496}
{"File Name":"opg-use-an-lpa\/0014-library-for-application-views.md","Context":"## Context\\nWe need to use a common library for managing HTML views within the service.\\nTwo libraries were investigated; Plates and Twig.\\n","Decision":"To use Twig.\\n","tokens":29,"id":4838}
{"File Name":"blueprint\/0004-argocd-apps-of-apps-structure.md","Context":"## Context and Problem Statement\\nArgoCD `Applications` manifests are a [declarative way to manage](https:\/\/argoproj.github.io\/argo-cd\/operator-manual\/declarative-setup\/#applications) ArgoCD `Applications` in git. Often times these are manifests that are stored alongside ArgoCD deployment manifests.\\nThis has been fine in the past since we controlled the deployment of ArgoCD and had merge access to the repo where the applications were stored. So if we wanted to onboard a new app, we make a PR with the application manifest and someone on our team would merge it.\\nBut there can be a situation where another team, like cluster-admins or infra, store the ArgoCD deployments in their own repo.\\nIf we applied our current practice, we'd store our app manifests in this external repo. The problem is that we may not have merge access to this repo, and it wouldn't really make much sense for people who manage the infrastructure to also handle PR's that don't pertain directly to cluster management.\\n","Decision":"Chosen Option `(3)`. Problems with `(1)` have been outlined above. The issues with `(3)` is that there is no way to effectively enforce teams to ensure their App Projects belong to their team's project (this is further described below).\\nThe Proposed Solution is captured by this diagram:\\n![image](https:\/\/user-images.githubusercontent.com\/10904967\/99705533-d8aac380-2a67-11eb-88e9-b63582271994.png)\\nThe idea here is that all our operate-first\/team-1\/team-2\/...\/team-n ArgoCD `Applications` would go in the `opf-argocd-apps` repo. Then we'd have an App of Apps i.e. the `OPF Parent App` that manages all these apps. This way we can add new applications declaratively to ArgoCD without having to make PR's to the `Infra Repo` (e.g. `moc-cnv-sandbox`). Operate-first admins would manage the `opf-argocd-apps` repo. Any other ArgoCD `Applications` that manage cluster resources like `clusterrolebindings` or operator `subscriptions` etc. can remain in the infra repo since that's a concern for cluster admins. We would direct any _user_ that wants to use ArgoCD to manage their apps to add their ArgoCD `Applications` to the `opf-argocd-apps` repo.\\n### Positive Consequences\\n- Infrastructure\/cluster-admins are not bombarded with PR's for ArgoCD App onboarding\\n- OperateFirst maintainers can handle the PR's unhindered\\n- The `opf-argocd-apps` repo can be leveraged by CRC\/Quicklab\/Other OCP Clusters to quickly setup ArgoCD ODH\/Thoth\/etc. Applications.\\n### Negative Consequences\\nBiggest concern here is that there is no way to automatically enforce that Applications in `opf-argocd-apps` repo _belong_ to the `Operate First` ArgoCD project (see diagram).\\n_Why is this a problem?_ Because we use ArgoCD projects to restrict what types of resources applications _in that project_ can deploy. For example ArgoCD apps in the `Infra Apps` project in the diagram can deploy: `clusterrolebinding`, `operators`, etc. So while `OPF Parent App` cannot deploy `clusterrolebindings` because it belongs to the `Operate First` ArgoCD project, it could deploy another ArgoCD application that belongs to `Infra apps` and _that ArgoCD app_ could deploy clusterrolebindings.\\nYou can read more about this [issue here](https:\/\/github.com\/argoproj\/argo-cd\/issues\/3045). The individual there used admission hooks to get around this but I don't think we want to go there just yet. My suggestion is we begin by enforcing this at the PR level, and transition to maybe catching this in CI until there's a proper solution upstream.\\n","tokens":216,"id":3556}
{"File Name":"alfa\/adr-001.md","Context":"## Context\\nMajor decisions on architecture and design have so far primarily [been recorded in code comments](https:\/\/github.com\/Siteimprove\/alfa\/blob\/b92105fba4b961e8b67c91490aa1d7c6f13a8328\/packages\/alfa-dom\/src\/types.ts#L1-L17) or not recorded at all. While the former has been an acceptable approach, the latter has not as important information has resided only with the person who made the decision.\\nWe want to avoid siloing of information on the reasons for why important decisions were made and we want to do so in a way that is both transparent and easy to maintain. Documenting architecture decisions should be no different than documenting code in that the documentation is written as plain text and lives with the rest of the code base.\\n","Decision":"Moving forward, we will make use of so-called architecture decision reports (ADR) as described in [Documenting architecture decisions](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions) by Michael Nygard. The ADRs will live in the [`docs\/architecture\/decisions`](..\/decisions) directory and will follow the format outlined in [`docs\/architecture.md`](..\/..\/architecture.md).\\n","tokens":172,"id":3154}
{"File Name":"verify-event-store-schema\/0001-flyway.md","Context":"## Context\\nWe need a way of running database migrations to the verify event store database.\\n","Decision":"We have chosen to use [Flyway](https:\/\/flywaydb.org) since the team has some experince with it and it seems like a simple and lightweight option.\\n","tokens":19,"id":318}
{"File Name":"editions\/04-\u2705-paths.md","Context":"## Context\\nThe editions lambda needs to be able to identify specific versions of an issue.\\n","Decision":"To have two deployments of the backend, one for previewing, and a second for published issues.\\nThe published issues deployment will replace the issueid path parameter with source\/issueid.\\n`source` will identify which file in the published bucket will be retreived to form the issue on.\\n","tokens":19,"id":671}
{"File Name":"racing-tips\/0002-use-kubernetes-and-docker.md","Context":"## Context\\nA lot of options exist to create applications nowadays - serverless, PaaS, SaaS etc. The purpose of this project is to hone Kubernetes and Docker skills in particular which may mean use of these technologies seem over engineered.\\n","Decision":"* Use Kubernetes and Docker\\n* Use Amazon EKS\\n","tokens":49,"id":3118}
{"File Name":"mokka\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":48}
{"File Name":"iampeterbanjo.com\/0013-cypress-e2e-tests-in-typescript.md","Context":"## Context\\nI'm getting Typescript errors in the e2e tests. If I'm relying on Typescript for linting then keep errors clean is good hygiene. I'v tried ignoring the `e2e\/` or `cyperss\/` folders and that is not working. Why? Not sure.\\n","Decision":"In the context of getting Typescript errors in my e2e tests and facing the concern to keep errors clean, then use Typescript for e2e tests. Especially since it seems to be [well documented][cypress-ts] with examples. Therefore the burden should be light.\\n","tokens":67,"id":1600}
{"File Name":"external-service-operator\/0005-only-prober-package-is-responsible-for-marking-addresses-ready-or-no-ready.md","Context":"## Context\\nAs the Endpoint ressource has two controlling structures.\\n1. Endpoint Reconciler\\n2. Prober (ProbeManager and according Workers)\\n","Decision":"There is the need to seperate concerns in that resource. Therefore:\\n### Endpoint Reconciler\\nIs responsible for modifying:\\n* ObjectMeta\\n* General Structure for Subsets\\n* It makes sure exactly the same IPs in the Externalservice ressource exist also in the Endpoint\\n* If not it will add the missing IPs always to the NotReadyAddresses List\\n### Prober\\nThe Prober package is soley responsible for moving Addresses from Addresses to NotReadyAddresses and vice versa.\\n","tokens":33,"id":1732}
{"File Name":"digital-paper-edit-client\/2019-05-20-previewing-papercuts.md","Context":"## Context and Problem Statement\\nAs part of the Paper-edit view\\n![Screen Shot 2019-04-18 at 13 54 26](https:\/\/user-images.githubusercontent.com\/4661975\/56362368-8fd2bf00-61e1-11e9-9e87-edf71c0da030.png)\\n> As a user, I want to be able to get (watch or listen to) a preview of the paper-edit\/program script so that I can see how my program's paper-cuts (transcripts text selections) will \"render\" as audio and\/or video.\\nIn the sketch, this would be the \"canvas\" - _borrowing terminology from to the NLE video editors distinction between preview and canvas_\\nA React component that takes a sequence of audio or video clips as an input, with start and end time, and displays a player that can\\n- [ ] Seamlessly play this EDL (edit decision list) without buffer or load time in between clips\\n- [ ] Has player controls, such as\\n- [ ] progress bar\\n- [ ] play\/pause\\n- [ ] stop\\nNote that the component should be able to generate the preview if the sequence is made of audio, video or a mix of audio and video files.\\nIn first version it could just deal with sequences of media, but it be good if in subsequent versions it could handle things like text, titles (as a possible way to display place holders for voice over text?) - this second part needs to be flashed out more. But the general thing to keep in mind is the extensibility of the elements to display, if that makes sense.\\n## Decision Drivers <!-- optional -->\\n* easy to reason around\\n* performant\\n","Decision":"* easy to reason around\\n* performant\\nThere are other examples and indications in the repo on how to enable, play, pause, and a progress bar.\\nI think under the hood, Video Context, used the HTML5 canvas to concat the videos in the sequence and then provides a unified interface to treat it as a single media.\\nChosen option: option 5 `bbc\/VideoContext`, because it seems to be a performant way to display an EDL\/playlist of clips with start and end times options. It is also currently being mantained by BBC R&D.\\n","tokens":366,"id":4170}
{"File Name":"govuk-infrastructure\/0007-use-fluentbit-elasticsearch-and-kibana-for-application-logs.md","Context":"## Context\\n### Log collection and forwarding\\nApplication containers should output their logs as [`stdout`\/`stderr`](https:\/\/12factor.net\/logs) streams, which can then be forwarded by an aggregator to a single log store. These logs should be searchable, filterable and appropriately tagged with application\/component etc.\\nThere are two broad approaches to log collection and forwarding in Kubernetes\\n- Run a [sidecar container](https:\/\/kubernetes.io\/docs\/concepts\/cluster-administration\/logging\/#sidecar-container-with-logging-agent) with each `Pod` or container to pick up the application logs from a file or socket and forward on (this is the only available option with [EKS Fargate, although partially managed by AWS](https:\/\/docs.aws.amazon.com\/eks\/latest\/userguide\/fargate-logging.html))\\n- Run a log collection and forwarder agent on [each node as a `DaemonSet`](https:\/\/kubernetes.io\/docs\/concepts\/cluster-administration\/logging\/#using-a-node-logging-agent)\\nLogging should be provided as a service and enabled by default for all containers, so the sidecar pattern does not meet our needs.\\nFor application logs to be searchable and navigable we require the logging agent to annotate and index logs with kubernetes cluster metadata (namespace, pod name, labels etc).\\nThe most commonly used logging agents for Kubernetes are: [fluentd](https:\/\/www.fluentd.org) and [fluentbit](https:\/\/fluentbit.io). Fluentbit is a successor to fluentd, and aims to be more lightweight and offer higher performance, but has a smaller plugin ecosystem than fluentd.\\n### Log storage and UI\\nLogs must also be stored somewhere, with a browser-based UI for searching, filtering and browsing. There are many options in the space, but the most common option is [elasticsearch and kibana](https:\/\/www.elastic.co\/what-is\/elk-stack).\\nWe may ultimately push logs to a GDS-wide central log store, but should deploy something ourselves in the short term so that we can provide a usable logging service to users sooner rather than later, and get familiar with the details of logging within Kubernetes ourselves.\\nFor now, the obvious choice is Elasticsearch and Kibana, due to their extensive support for this use case and in this ecosystem.\\n","Decision":"Use a **fluentbit** Daemonset with AWS Elasticsearch Service.\\n","tokens":477,"id":3617}
{"File Name":"signals-frontend\/0011-embedded-application.md","Context":"## Context\\nWhile signals\/front-end is a React web app, the client wants to provide it to mobile app users as well.\\nFurthermore, some changes are requested specifically for the 'app version':\\n- The application header should be hidden\\n- The button 'Doe een melding' should be replaced with a 'Sluit venster' button which closes the window\\n- The possibility to initialize the map center location\\nThe above requirements are described in more detail in [Jira](https:\/\/datapunt.atlassian.net\/browse\/SIG-3933).\\nAt this moment, rebuilding the frontend in a native app architecture is not possible due to time and financial constraints.\\n","Decision":"Add a feature flag `appMode` to the application configuration, and deploy the application with `appMode=true` to a separate domain.\\nUsing the feature flag, the application conditionally applies the app-specific requirements.\\nThe mobile app embeds the web app (e.g. using [React Native WebView](https:\/\/github.com\/react-native-webview\/react-native-webview)),\\nand provides additional information via query parameters (e.g. map coordinates).\\nConversely, communication from the web-app to the mobile app occurs via the `postMessage()` API, but its use should be limited.\\nBecause an embedded application cannot close itself, the `postMessage()` can be used to notify the app that it should be closed.\\n","tokens":138,"id":3051}
{"File Name":"arch\/0023-application-performance-monitoring.md","Context":"## Context\\n1. \u670d\u52a1 A\uff0c\u7528\u6237\u8bbf\u95ee\u641c\u7d22\u9875\u9762\u65f6\uff0c\u8bf7\u6c42\u4e86 10 \u5206\u949f\uff0c\u8fd8\u6ca1\u8fd4\u56de\u5185\u5bb9\uff1b\\n2. \u670d\u52a1 B\uff0c\u8fd9\u4e2a\u9875\u9762\u8bf7\u6c42\u600e\u4e48\u6709 1000 \u6b21\u6570\u636e\u5e93\u67e5\u8be2\uff1b\\n3. \u670d\u52a1 C\uff0c\u8fd9\u6761\u6570\u636e\u5e93\u67e5\u8be2\u600e\u4e48\u8017\u65f6 5 \u5206\u949f\u3002\\n4. \u6211\u4eec\u7684\u8fd9\u4e2a\u9879\u76ee\u505a\u4e86\u534a\u5e74\uff0c\u4f7f\u7528\u6548\u679c\u5982\u4f55\u5440\uff0c\u54e6\uff0c\u9875\u9762\u5e73\u5747\u54cd\u5e94\u65f6\u95f4\u662f 5 \u79d2\uff0c\u554a\uff01\uff1b\\n5. \u7528\u6237\u53cd\u9988\u5e73\u5747\u9700\u8981\u9700\u8981 1 \u5929\u4ee5\u4e0a\u7684\u65f6\u95f4\uff1b\\n6. \u6d4b\u8bd5\u5f88\u96be\u5c06\u6240\u6709\u8fb9\u754c\u573a\u666f\u6d4b\u8bd5\u5230\u3002\\n","Decision":"\u4f7f\u7528 Newrelic \u5bf9\u6211\u4eec\u7ebf\u4e0a\u4e3b\u8981\u4e1a\u52a1\u505a\u6027\u80fd\u76d1\u63a7\uff0c\u5305\u62ec\u54cd\u5e94\u65f6\u957f\u3001\u541e\u5410\u91cf\u3001\u9519\u8bef\u7387\uff0c\u6162\u67e5\u8be2\uff0c\u67e5\u8be2\u5206\u6790\u7b49\uff0c\u4ed6\u53ef\u4ee5\u505a\u5230\u5b9e\u65f6\u5b9a\u4f4d\u548c\u901a\u77e5\uff0c\u5e76\u6307\u51fa\u5177\u4f53\u7684\u4ee3\u7801\u884c\u53ca SQL \u8bed\u53e5\u3002\\n","tokens":165,"id":2451}
{"File Name":"adr-viewer\/0003-use-same-colour-for-all-headers.md","Context":"## Context\\n`adr-viewer` presents all records with the same `lightgreen` header, even though records may be in different states.\\n","Decision":"We will keep the `lightgreen` colour for everything\\n","tokens":30,"id":3816}
{"File Name":"hmpps-interventions-ui\/0009-use-helmet-for-http-security.md","Context":"## Context\\nWe want to make sure we're setting the correct HTTP headers for security e.g.\\nContent Security Policy to protect against XSS attacks.\\n[Helmet](https:\/\/helmetjs.github.io\/) is a package that works well with Express\\nto make it easy to set various HTTP headers for secutiy.\\n","Decision":"We'll use Helmet to set secure HTTP headers.\\n","tokens":67,"id":462}
{"File Name":"docs\/0026-implementation-of-complex-eai-patterns-with-faas.md","Context":"## Context and Problem Statement\\nSome [Enterprise Integration Patterns](https:\/\/www.enterpriseintegrationpatterns.com) have a complex structure where parts of the behaviour can be implemented generically while some parts need to be modifiable by the end user (in our case the system admin using MICO).\\nWe have already [decided to use a FaaS platform](0023-faas.md) to provide this modifiability in form of code as configuration.\\nWhile this works well for most patterns, for some of the more complex patterns it is not easy to allow modifiability via FaaS.\\nThis is especially the case if the user want to write as little code as possible meaning the [generic part](0025-generic-component.md) of the component has to be implemented by the MICO team.\\n## Decision Drivers\\n* Modifiability of the patterns must be provided via a FaaS function\\n* The function should only have to contain as little code as possible\\n* Generic code for the pattern should be provided by the MICO platform either as a library to import into the FaaS function or in the component that calls said function\\n","Decision":"* Modifiability of the patterns must be provided via a FaaS function\\n* The function should only have to contain as little code as possible\\n* Generic code for the pattern should be provided by the MICO platform either as a library to import into the FaaS function or in the component that calls said function\\n**Where to implement logic**: To be decided\\n**State of configuration channels**: To be decided\\n**Stateful functions**: To be decided\\n**Routing**: We will support custom routing decisions in the FaaS function by always interpreting a routing slip if it is present.\\nThe routing slip has to support multiple destinations for one routing step.\\nThis will also allow us to make more patterns possible (everything that is not stateful) with a single generic kafka to FaaS connector.\\n**Intermediate requests**: To be decided\\n","tokens":231,"id":4669}
{"File Name":"island.is\/0003-css.md","Context":"## Context and Problem Statement\\nWe're building websites and web applications that share a common design system with reusable components. How do we write CSS styles in a way that is performant and safe?\\n## Decision Drivers\\n- Should be performant, with code splitting, caching and minimal runtime overhead.\\n- Needs to have easy access to our design system constants. These should optimally be shared with JS logic.\\n- Should be type-safe to catch issues when refactoring.\\n- Reusable components should be closed, not accepting arbitrary styles\/classes.\\n- We want a pattern for responsive props with atomic layout components.\\n","Decision":"- Should be performant, with code splitting, caching and minimal runtime overhead.\\n- Needs to have easy access to our design system constants. These should optimally be shared with JS logic.\\n- Should be type-safe to catch issues when refactoring.\\n- Reusable components should be closed, not accepting arbitrary styles\/classes.\\n- We want a pattern for responsive props with atomic layout components.\\nChosen option: Treat, because it combines the best of both worlds from CSS-in-JS and CSS modules.\\nWe'll create shared components that have responsive props, but are otherwise closed for modifications. Theme variables are defined in a shared library with TypeScript.\\nExample:\\n```typescript\\n\/\/ Good:\\n<Box padding\"small\" \/>\\n<Box padding={{xs: 'small', md: 'medium'}} \/>\\n<Input large \/>\\n<Text preset=\"heading3\" as=\"p\" \/>\\n```\\n```typescript\\n\/\/ Bad:\\n<Box className={customLayout} \/>\\n<Input style={{ height: 50, padding: 16 }} \/>\\n<Text className={styles.heading} \/>\\n```\\n### Positive Consequences\\n- Treat is statically extracted at build time, so it has minimal runtime.\\n- Styles load in parallel with JS, also when code splitting.\\n- Styles are written in TypeScript which gives us type safety when referring to shared variables, styles and helpers.\\n- Styles are in special files, separate from markup and components giving us clear separation with good visibility into the rendered markup.\\n- We can pull in responsive layout component patterns from Braid, which gives us a good base to lay out components and pages.\\n### Negative Consequences\\n- We are choosing a pretty new framework, so it may 1) have bugs or issues, 2) be an obstacle for new developers or 3) be discontinued.\\n- When we're generating responsive styles at build time we need to be mindful at how many variations we allow (eg media queries, columns, whitespace), since they can easily bloat our CSS with unused styles.\\n","tokens":126,"id":1108}
{"File Name":"Nosedive\/0006-use-of-migratus-library.md","Context":"## Context\\nDirect form the [jdbc documentation](http:\/\/clojure-doc.org\/articles\/ecosystem\/java_jdbc\/home.html)\\nAnother common need with SQL is for database migration libraries. Some of the more popular options are:\\n* Drift\\n* Migratus\\n* Ragtime\\n","Decision":"After a quick read of the documentation, and see that all solutions are similar, I select Migratus, by the comodity of have a lein plugin\\n","tokens":60,"id":105}
{"File Name":"pay-connector\/adr-002-decrypting-google-pay-token-not-required.md","Context":"## Context\\nThere are two possible tokenization methods when making a payment request to Google Pay: [PAYMENT_GATEWAY and DIRECT](https:\/\/developers.google.com\/pay\/api\/web\/guides\/tutorial#tokenization).\\nFeatures of PAYMENT_GATEWAY tokenization:\\n- Merchant id identifying GOV.UK Pay with Google Pay needs to be set up\\n- We simply pass the encrypted response from Google Pay through to the payment gateway as is\\n- No need of the operational overhead of key management as is already handled between Google Pay and the payment gateway\\nFeatures of DIRECT tokenization:\\n- No need for a Merchant id between GOV.UK Pay and Google Pay\\n- Need to be PCI DSS compliant (which we already are)\\n- Need to decrypt the response to get information to send to the payment gateway\\n- Need the operational overhead of key management as we need to [roll keys once a year and register these with Google Pay](https:\/\/developers.google.com\/pay\/api\/web\/guides\/resources\/payment-data-cryptography#key-management)\\nWith either tokenization method, we get a [number of data](https:\/\/developers.google.com\/pay\/api\/web\/reference\/object#response-objects), but the ones relevant to us are:\\n- Last 4 card digits\\n- Billing address (which includes name)\\n- Email address\\n- Card Network\\nWith DIRECT tokenization we decrypt to get:\\n- PAN\\n- expiration month and year\\n[and other things](https:\/\/developers.google.com\/pay\/api\/web\/guides\/resources\/payment-data-cryptography#encrypted-message).\\nChoosing whether to decrypt the payload from Google Pay or not is informed by the following two deal breakers:\\n- We need to be able to retrieve the cardholder name, email, and last 4 digits of a card\\n- We don't want services to have to manage any sort of keys\/certificates\\n","Decision":"Unlike the current implementation of Apple Pay, we have chosen to go with not decrypting the payload for the following reasons:\\n- We get all the information we require (Last 4 card digits, billing address name, email address, card network) in unencrypted form\\n- Services won't need to manage any keys\/certificates. This is because when a payment request is made to Google Pay we specify the payment gateway, e.g. \"Worldpay\".\\nGoogle Pay will return a response encrypted with the payment gateway's public key.\\nThis response is decryptable by the payment gateway as they have their private encryption key.\\nSo there is already a key management process between Google Pay and the payment gateway.\\nOur initial assumption that Google Pay encrypts with the merchant id's public key was incorrect.\\n- No operational process is needed on our part to manage keys\/certs.\\n","tokens":384,"id":2685}
{"File Name":"cljdoc\/0017-use-nomad-for-deployment.md","Context":"## Context\\ncljdoc's deployment story has been simplistic but effective. To recap:\\n- During CI a zip file is pushed to S3 that contains all files to run the application\\n- On the live server there is systemd service that will download an archive and run it. The\\nversion of the downloaded archive is specified via a file on the server.\\nUpdating simply required updating a file on the server and restarting the service.\\nThe issue with this approach however is that every time a new release was pushed to the server\\nthe restart of the systemd service would incur up to a minute of downtime. While this generally\\nisn't a huge deal it discourages certain development practices that may be desirable such as\\nContinuous Deployment.\\nOur existing deployment setup (and tools used by it) are poorly equipped to handle this kind of\\ndeployment scenario. A large amount of bash scripts would be required to start a new cljdoc server,\\nwait for it to become available, update nginx's upstream port, and kill old cljdoc server instances\\nin a repeatable, automated manner. Likely these bash scripts would be error-prone and turn into\\nsomething nobody likes to touch.\\n","Decision":"Implement a *canary deploy* mechanism for the cljdoc server application using\\n[Nomad](https:\/\/nomadproject.io) and [Traefik](https:\/\/traefik.io).\\nWhile both of these tools are probably aimed at much more complex workloads they provide the\\nfollowing benefits over the existing systemd\/Nginx setup:\\n- Automatic SSL certificates via Lets Encrypt\\n- Declarative specification of jobs and their desired update semantics\\n- APIs to schedule new jobs and cycle old\/new deployments\\n- Health checks to verify new deployments work as expected\\n- Machine images become much simpler since they only need Nomad, Consul, Docker\\nThis simplifies a lot of cljdoc's operational tasks while also enabling Continuous Deployment.\\n","tokens":243,"id":2610}
{"File Name":"interlok\/0005-remove-produce-destination.md","Context":"## Context and Problem Statement\\nIf we consider something like `jms-topic-producer` then we have to configure a produce destination what contains the topic. Traditionally, if we wanted a metadata driven destination, then it would have to be done via a `metadata-destination` but now with the `%message{metadata-key}` expression language; we would just use configured-produce-destination everywhere. However; this isn't necessarily obvious to the user. Additionally, we cannot have a destination that isn't a String.\\nThere are some producers that have 2 mandatory pieces of information that would be considered the _destination_. For instance the AWS Kinesis producer requires both a _stream-name_ and a _partition-key_. This kind of requirement doesn't lend itself very well to a `produce-destination`; we can't make the destination return a delimited string since both of those values are essentially arbitrary. So we either completely ignore produce-destination, or we use the destination for the stream name, and then something else entirely for the partition key.\\nThere are already some producers that ignore the destination entirely: SAP Idoc producer, Jetty Response Producer are the two that immediately spring to mind. So if isn't mandatory, then why have it?\\nEDIT: 2020-06-16 The proposal is to also remove `ConsumeDestination` as well as `ProduceDestination`\\n## Decision Drivers\\n* Less confusion for new users.\\n* Clean up configuration to take advantage of new features.\\n* Make things more predictable.\\n","Decision":"* Less confusion for new users.\\n* Clean up configuration to take advantage of new features.\\n* Make things more predictable.\\nDeprecate and Remove\\n### Status Quo\\nWe're in the army now.\\n#### Consequences\\n* Good, because there's no work to do, apart from new producers that don't fit the existing produce-destination paradigm.\\n* Bad, because we aren't simplifying the object model and configuration.\\n### Deprecate Destination\\nIf we deprecate Destination then we need to\\n* Deprecate destination as a member in all producers and consumers (AdaptrisMessageWorker as well?)\\n* Ensure that all producers and consumers provide an alternative method for specifying the destination\\n* For FsConsumer we might have a \"directory-name\" that is a String -> `%message{directory}`\\n* Ditto FsProducer\\n* It might be different for other producer\/consumer types\\n* Deprecate the produce\/request methods in the main producer interface.\\n* Default it to \"null\", and make sure it's not NotNull\\n* this means that the UI will no longer display the destination when you create a new producer in the UI; but if you have it configured, it will be present with the deprecated warnings.\\n#### JmsReplyToDestination\\nFinally there is a special `JmsReplyToDestination` that is in use; this uses object metadata to derive the `javax.jms.Destination` (and is handled specially by the JMS Producer Impls).\\n* Change the JMS Producer implementations so that if the object metadata exists (use-jms-reply-to-if-available=true), then it uses the JmsReplyTo value.\\n* Deprecate JmsReplyToDestination\\n#### Consequences\\n* Neutral, because there will be crossover period where both styles need to be valid; and how do we message this change to the existing users without a massive kerfuffle.\\n* Good, it ultimately simplifies the configuration, leaving us \"less work\" with ConfigBean translations.\\n* Good, because it allows producers to define their own configuration, in fact having a non-string based destination.\\n* __*Neutral\/Bad, because it's changes the produce\/request contract in AdaptrisMessageProducer*__\\n### Additional ProduceDestination implementations\\nFor the Kinesis produce we could introduce a new `KinesisProduceDestination` that contains an additional getPartitionKey() method (or we make it generic, and add a getQualifier() method).\\n#### Consequences\\n* Good, because we aren't modifying the object model\\n* Bad, because we have to cast ProduceDestination\\n* Bad, because we still have to support the situation where the user has only configured a ConfiguredProduceDestination or similar.\\n","tokens":312,"id":2339}
{"File Name":"tendermint-rs\/adr-007-light-client-supervisor-ergonomics.md","Context":"## Context\\nThe initial approach to use callbacks in order to capture async responses\\nfrom requests issued by the `Handle` to its `Supervisor` introduces a couple of\\ndrawbacks, i.e. try semantics, scope and borrow complexities. As the\\ngeneral pattern seems to be that a bounded channel is constructed to transport\\nthe response it could be reworked to be the actual value send with the event to\\nthe Supervisor. Which in turn send back the response on the typed channel to the\\nHandle. As an added side-effect this could remove the need for the Callback\\nabstraction all together and fully embraces CSP style concurrency.\\nFurthermore we have async versions of most `Handle` methods, but no real\\nuse-case driving it. At this point we can't anticipate the limitations of the\\ncurrent design with regards to concurrent access.\\n","Decision":"Remove the callback abstraction in favour of passing channels in the events\\nexchanged directly as the pattern is present already in the current concrete\\nimplementation of the `supervisor::Handle` anyway.\\nRemove async versions of the public methods until a use-case drives their\\nimplementation. We will leave concurrency to be informed by emerging use-cases\\nand field reports. With the first real integration likely happen in the RPC\\nserver and ibc relayer.\\n","tokens":174,"id":4311}
{"File Name":"rotc\/0007-use-standalone-kiali.md","Context":"## Context\\nThe issue motivating this decision, and any context that influences or constrains the decision.\\n","Decision":"The change that we're proposing or have agreed to implement.\\n","tokens":21,"id":3544}
{"File Name":"godspeed-you-blocked-developer\/005. Serverless-offline.md","Context":"## Context\\nI can't be doing a deployment to CloudFormation every time I want to test a new line of code. Serverless-offline is a plugin that allows local emulation of AWS Lambda and API Gateway. By running it via node --debug, you can [use it as a debugger from VS Code](https:\/\/medium.com\/@OneMuppet_\/debugging-lambada-functions-locally-in-vscode-with-actual-break-points-deee6235f590).\\n","Decision":"I will use the serverless-offline plugin to run a local copy of my Serverless stack.\\n","tokens":97,"id":4933}
{"File Name":"datalab\/0016-glusterfs-for-storage.md","Context":"## Context\\nContainers running on Kubernetes only provide ephemeral storage. We need to provide\\npersistent storage that allows volumes to be mounted into multiple containers. This\\nrestricts us to [Kubernetes Volume Types](https:\/\/kubernetes.io\/docs\/concepts\/storage\/persistent-volumes\/) that support \"Read Write Many\". Specifically, we are selecting\\nbetween [NFS](https:\/\/help.ubuntu.com\/lts\/serverguide\/network-file-system.html),\\n[GlusterFS](https:\/\/www.gluster.org\/) and [Rook](https:\/\/rook.io\/).\\n","Decision":"We have decided to use GlusterFS to provide distributed persistent storage.\\nWe have opted not to use Rook as it feels that it isn't yet ready for production usage. Also, while it would be easy to deploy hyper-converged we would need a second Kubernetes\\ncluster to run isolated storage as we require.\\nWe feel that simple NFS storage isn't sufficient as it won't give any data resilience.\\nGiven we have no backups, the data replication will give us limited disaster recover\\ncapability.\\n","tokens":116,"id":740}
{"File Name":"origin\/0006-rename-assets-to-devices.md","Context":"## Context\\nThere has been confusion on why we call generating and consuming devices Assets in Origin. We have noticed that most Origin users would refer to Assets as Devices and not Assets.\\n","Decision":"We decided to change our terminology to refer to Assets as Devices, to keep in line with the prevailing terminology in the industry.\\n","tokens":37,"id":1555}
{"File Name":"devops-tooling-docs\/0003-github-action-branch-strategy.md","Context":"## Context and Problem Statement\\nWe want to use [Semantic Versioning (SemVer)](https:\/\/semver.org\/) when publishing our GitHub Actions, but [we cannot use SemVer ranges](https:\/\/github.community\/t\/semver-ranges-docs-issues\/16723\/2) when consuming them.\\n<details>\\n<summary><b>More Context<\/b><\/summary><br>\\nWhen we do\\n```yaml\\nuses: byu-oit\/github-action-tf-plan-analyzer@v2\\n```\\nin GitHub Actions, we're telling GitHub to grab the repo at the _branch_ or _tag_ labelled `v2`.\\nThere's [currently no concept of SemVer](https:\/\/github.community\/t\/semver-ranges-docs-issues\/16723\/2) in GitHub Actions, but there are some conventions. A common convention is to cut \"releases\" (tags) using SemVer, e.g. `v2.0.1`, but that doesn't change the `v2` tag or branch by default.\\nWe would like consumers to be able to get `^2.0.0`, but currently that requires moving the `v2` tag or updating the `v2` branch.\\nGitHub for a while suggested using moving around tags like `v2`, but has [since suggested](https:\/\/docs.github.com\/en\/free-pro-team@latest\/actions\/creating-actions\/about-actions#using-tags-for-release-management) either using branches or tags for that purpose. It is otherwise very unusual to move tags on GitHub, so we prefer branches to convey mutability.\\n<\/details>\\n## Decision Drivers\\n* Versioning is necessary to communicate changes (especially breaking changes)\\n* We don't want to break deployments\\n* Breaking somebody's ability to deploy doesn't actually bring down production\\n* We want to make it easy to roll out updates\\n* Many consumers of our GitHub Actions are unfamiliar with how they work\\n","Decision":"* Versioning is necessary to communicate changes (especially breaking changes)\\n* We don't want to break deployments\\n* Breaking somebody's ability to deploy doesn't actually bring down production\\n* We want to make it easy to roll out updates\\n* Many consumers of our GitHub Actions are unfamiliar with how they work\\nWhen publishing actions, use branches corresponding to major versions (e.g. `v1`, `v2`) and keep them updated. Treat updates to the major-version branches as production changes. Continue to tag new releases according to SemVer (e.g. `v2.0.1`, `v2.1.0`).\\nWhen consuming [our actions](https:\/\/github.com\/search?q=org%3Abyu-oit+github-action+archived%3Afalse&type=Repositories), refer to the major version branches (e.g. `v2`). This will allow us to roll out most changes to the organization quickly and effectively.\\n","tokens":406,"id":4073}
{"File Name":"human-essentials\/0006-instantiating-items-from-base-items.md","Context":"## Context\\nWe encountered a bug where one Essentials Bank edited \/ deleted Items from the Item list (they weren't using specific Item types). Because Item types were shared globally, this deleted those types for everyone. That needs to not be the case.\\n","Decision":"We've decided to have all items be sandboxed on a per-organization basis. There is a list of templated starting items (\"Base Items\") that will be created for every new organization when they are first added. Each of those starting items (and all other items) will all be connected back to a Base Item (a generic type). This will allow the Essentials Banks to rename and customize the items to their heart's content without (a) affecting other Essentials Banks and (b) while keeping it possible for Partner Base to request items without needing to know specifically what kinds of items they have on-hand. An \"Other\" base item will accommodate any items we've not factored in already.\\n","tokens":51,"id":3341}
{"File Name":"registraties\/004-prevent-saga-name-clashes.md","Context":"## Context\\nContainer components can inject sagas and have them registered by a specific key. This key doesn't have to be unique. This, however, leads to unexpected problems when running on a production build. In development it's not an issue, because of hot-reloading, but in production, having more than one saga be registered by the same key, fails silently.\\nWhat happens is that the already registered saga is overwritten by another saga. The first sagas functionality will no longer be available to the application.\\n","Decision":"Injecting more than one saga with a key that has already been used, will throw an error and will prevent the application from running.\\n","tokens":105,"id":1123}
{"File Name":"terraform\/OS-Patching.md","Context":"## Context\\nAny operating systems that we use are likely to be patched for bug-fixes or security reasons at least once per month. Ideally we would pick up the latest stable release all the time, however this implies a great deal of churn. Such change would also come in spikes of activity e.g. around Windows patch tuesdays.\\n","Decision":"We will not patch our operating systems in AWS. Instead we will pin every instance to a specific version of an operating system and move the pin (via a PR) only when necessary.\\n","tokens":69,"id":924}
{"File Name":"viplab-websocket-api\/0003-transfere-hash-in-jwt-claim.md","Context":"## Context and Problem Statement\\nWe have to transfer json data and verify the integrity of the json data model.\\n[ADR-0002](0002-use-sha256-with-base64url-encoding.md) describes how to create a hash of the json.\\nThe hash must be transferred to from the authorization server to the WebSocket API secure.\\nThe validity of the hash must be verified.\\n## Decision Drivers <!-- optional -->\\n* JWT should be used\\n","Decision":"* JWT should be used\\nChosen option: \"Transfer hash in JWT Claim\", because it's the only option when using JWT.\\n### Positive Consequences <!-- optional -->\\n* multiple hashes for different json documents can be added in one JWT\\n","tokens":93,"id":1718}
{"File Name":"dogma\/0002-document-api-changes.md","Context":"## Context\\nWe need to advertise a meaningful history of changes to the Dogma API\\nspecification for both application developers and engine developers.\\nThe types of changes that have been made should be clearly identified, with\\nspecial attention drawn to changes that are not backwards compatible.\\n### Proposals\\n- Maintain a `CHANGELOG.md` as per the recommendations of [Keep a Changelog]\\n- Additionally, begin changelog entries that describe a BC break with `**[BC]**`\\n- Periodically tag releases, using [semantic versioning]\\n","Decision":"A changelog will be maintained as per [Keep a Changelog]. Unreleased changes\\nshould be added to the changelog as they are made.\\nGit tags will be named according to the rules of [semantic versioning].\\nAdditionally, tag names are to be prefixed with a `v` as required by [Go modules].\\n","tokens":112,"id":1616}
{"File Name":"offender-management-architecture-decisions\/0011-use-nomis-oauth-server2-for-allocation-api-authentication.md","Context":"## Context\\nWe need to protect the allocation API with authentication, but we'd rather not\\nhave to come up with an approach to do that ourselves from scratch.\\nThe new [NOMIS OAuth2 server](https:\/\/github.com\/ministryofjustice\/nomis-oauth2-server)\\nis already being used in production for authentication on almost all of the\\nNOMIS APIs and some other APIs built in Sheffield. We will need to use it to\\nauthenticate with the Custody API, and the other services which may need to use\\nthe allocation API are very likely to already be using this authentication\\nmethod for the other APIs they use.\\nClients can use one token (of a particular grant type) to authenticate with all\\nAPIs which use the NOMIS OAuth2 server, which makes things simpler for all\\nthose services - they don't have to work with multiple different authentication\\napproaches.\\nThe NOMIS OAuth2 server uses JWTs signed with a private key, so relying\\nservices can verify the integrity and authenticity of tokens presented by\\nclients using the corresponding public key.\\nWe've decided that the allocation manager will be entirely responsible for user\\naccess control and will call other APIs directly, and the allocation API will\\nbe a smaller interface onto its data (see [ADR 0010](0010-allocation-api-has-less-responsibility.md)).\\nThat means that the allocation API doesn't need to know which user it's\\nreturning data for, and we can use a system-to-system approach to\\nauthentication.\\nWe don't know of any other shared approaches to API authentication which are\\nused in the prison space.\\n","Decision":"We will use the NOMIS OAuth2 server for authentication on the allocation API.\\nWe will use the client credentials OAuth2 grant type for authentication on the\\nallocation API.\\nWe will verify signatures on presented tokens in the allocation API.\\nWe will respect expiration times on presented tokens in the allocation API.\\n","tokens":337,"id":257}
{"File Name":"embvm-core\/0005-provide-non-blocking-interfaces.md","Context":"## Context\\nBlocking interfaces are those that only return when an operation has been completed.\\nNon-blocking interfaces initiate a request and return when the request is submitted. Completion is annouced via a callback or by checking the status through another interface.\\nNon-blocking interfaces provide a more flexible implementation route.\\nBlocking implementations are typically avoided on embedded systems. Typically, a thread should sleep while waiting for an action to complete, rather than hogging processor resources.\\n","Decision":"Interfaces provided by the framework will be non-blocking. Users can write their own blocking wrappers if blocking code is needed.\\n","tokens":92,"id":3014}
{"File Name":"docker-compose-rule\/0002-move-waiting-code-into-a-seperate-concern.md","Context":"## Context\\nWe started with waiting code spread throughout the different layers of the DockerComposition stack:\\n* DockerComposition\\n* Container\\n* DockerPort\\nThis makes our custom waits depend on specific functionality rather than general functionality that external devs can also use. Now we're making all `waitForService` calls go through just a couple of API calls with flexible Healthchecks this is going to bite us.\\n","Decision":"Wait code (for the purposes of powering `waitForService`) does not go in the Composition stack. Instead, waits depend on external observations of this stack.\\n","tokens":81,"id":1538}
{"File Name":"monocle\/0005-design-of-es-documents.md","Context":"## Context and Problem Statement\\nWe want to support most of the existing Code Review systems. Monocle queries should be agnostic regarding the data source.\\n","Decision":"Choosen option: \"A crawler by data source and an unique and generic data schema\".\\nBecause, the addition of a data source support must not have any impact on the queries, the CLI or the WEB UI. The DB schema will be complete enough to fill the basic requirements of each source but will not implement specificities. The terminilogy will be generic for instance a Pull Request and a Review (Gerrit) will be called a \"Change\".\\n### Database objects\\n* Change: an object that describe the status of a Pull Request or Gerrit Review. The Change object will be attached attributes such as: the creation data, the author, the amount of commits within the Change, the changed files list, the title, ...\\n* ChangeCreatedEvent: an object that describe the creation event of a Change\\n* ChangeCommentedEvent: an object that describe a comment event on a given Change\\n* ChangeReviewedEvent: an object that describe a review event on a given Change\\n* ChangeCommitPushedEvent: an object that describe a commit push event on a given Change\\n* ChangeCommitForcePushedEvent: an object that describe a commit force push event on a given Change\\n* ChangeMergedEvent: an object that describe a Change merge event\\n* ChangeAbandonedEvent: an object that describe a Change abandoned event\\n","tokens":32,"id":4638}
{"File Name":"SoundCloudVisualizer\/0010-use-jest-and-cypress-for-tests.md","Context":"## Context\\nThere are currently no tests in this application. This was partly because there were a lot of options to choose from out of all the testing frameworks or libraries available for JavaScript, and because it felt a bit excessive for a side project.\\nSince then testing frameworks such as [Jest](https:\/\/jestjs.io\/) and [Cypress](https:\/\/www.cypress.io) have surfaced. Both of these promise a more developer-friendly experience. I figured I should get more practice with these tools and decided it'd be best to try them on an existing project.\\n","Decision":"Jest and Cypress will be used to test this application.\\n","tokens":116,"id":1581}
{"File Name":"openchs-adr\/0008-use-monorepo-for-client.md","Context":"## Context\\nDevelopment in health-modules often requires tandem work in openchs-android as well. It is much easier to have them in a single repo for ease of development. The repos will be set up using Lerna.\\n","Decision":"All client side libraries will be set up as a monorepo using Lerna.\\n","tokens":45,"id":2821}
{"File Name":"paas-team-manual\/ADR041-bosh-access-with-mtls.html.md","Context":"## Context\\nIn [ADR040 BOSH access without SOCKS](..\/ADR040-bosh-access-without-socks\/) we removed the requirement for using a SOCKS5 proxy or SSH tunnel to access the User Account and Authentication Service (UAA).\\nWe are moving towards a [zero trust network model](https:\/\/www.ncsc.gov.uk\/blog-post\/zero-trust-architecture-design-principles) and as part of this, we are removing the IP allow lists that have been in place.\\nWe discussed the proposed methods with IA and Cyber after reviewing the [RFC created as part of #169915408](https:\/\/docs.google.com\/document\/d\/1XZsrNp88tOSyC_bjy1mg3Yyv2TkpKgYSjoYResGAbps\/edit#heading=h.xscqoqxlc072)\\n","Decision":"We will remove the reliance on IP allow lists for all services on the BOSH instance.\\nMutual TLS will replace the allow lists.\\nCyber prefer this method, as it give a much stronger authentication to the platform. This is due to authenticating both the individual and the machine that are accessing critical services.\\n","tokens":180,"id":204}
{"File Name":"devops-challenge\/0013-feat-implement-helm.md","Context":"## Context\\nNow we have applications that get deployed into our microservices platform, we\\nneed to manage the configuration in an easy way.  YAML files are nice, but are\\ndifficult to reuse or to publish in order to manage vast amounts of\\ndeployments.  We need more tools.\\nHelm is a solution that eases distribution of kubernetes applications by using\\nthe concepts of Charts and Releases.\\nAs the manual states, a Chart is a Helm package. It defines the resources to\\nrun an application or tool inside a Kubernetes cluster.  A Release is an\\ninstance of a Chart running in a cluster.  Each time a Chart is installed, a\\nnew release is created.  So it's reusable code.\\nTo reuse this code and make it more accesible to others, there are repositories\\nlike in other package managers.  A Helm Chart can be published in a Repository\\nto use it widely.\\nThis provides huge advantages to the manageability of the applications deployed\\nin Kubernetes clusters.\\n","Decision":"I'm going to implement helm packages to replace the templates in the kube\\nsubfolder of each application.\\nBy now, I'm not going to use a remote repository, just the local definition of\\nthe Helm Chart.\\n","tokens":214,"id":3486}
{"File Name":"tech-team\/0004-supported-python-versions.md","Context":"## Context\\neLife has numerous projects written completely and partly with the [Python programming language](https:\/\/www.python.org\/).\\nIn order to provide language version consistency across projects we need to get a consensus on which versions we are going to support.\\nWe have only gone up to Python 3.5 due to the default Python versions pre installed on the Ubuntu distributions we use.\\n","Decision":"We will use Python >=2.7.14 as our default version for any project that solely uses or supports Python 2.\\nWe will use Python 3.5 as our default supported version for any project that solely uses or supports Python 3.\\n","tokens":79,"id":935}
{"File Name":"james\/0023-cassandra-mailbox-counters-inconsistencies.md","Context":"## Context\\nCassandra maintains a per mailbox projection for message count and unseen message count.\\nAs with any projection, it can go out of sync, leading to inconsistent results being returned to the client, which is not acceptable.\\nHere is the table organisation:\\n- `mailbox` Lists the mailboxes\\n- `messageIdTable` Holds mailbox and flags for each message, lookup by mailbox ID + UID\\n- `imapUidTable` Holds mailbox and flags for each message, lookup by message ID and serves as a source of truth\\n- `mailboxCounters` Holds messages count and unseen message count for each mailbox.\\nFailures during the denormalization process will lead to inconsistencies between the counts and the content of `imapUidTable`\\nThis can lead to the following user experience:\\n- Invalid message count can be reported in the Mail User Agent (IMAP & JMAP)\\n- Invalid message unseen count can be reported in the Mail User Agent (IMAP & JMAP)\\n","Decision":"Implement a webadmin exposed task to recompute mailbox counters.\\nThis endpoints will:\\n- List existing mailboxes\\n- List their messages using `messageIdTable`\\n- Check them against their source of truth `imapUidTable`\\n- Compute mailbox counter values\\n- And reset the value of the counter if needed in `mailboxCounters`\\n","tokens":204,"id":2154}
{"File Name":"opg-metrics\/0004-use-grafana-for-our-data-visualisation.md","Context":"## Context\\nWe need a way to visualise our time series data for users to view and analyse data.\\nWe need the ability for authenticating users with levels of permissions for creation and viewing. A way to organise dashboards by project or area of interest should also be available from the solution to help navigate to information.\\nIt is not to be used for debugging, we have other solutions that provide and handle this sort of functionality. This should be for analysing metrics across a spectrum of sources and be able to overlay key points on top of each other.\\n","Decision":"We have chosen Grafana for its ease of use, popularity, plugin support and Open Source status.\\n","tokens":114,"id":1543}
{"File Name":"bedrock\/0003-use-cloudflare-workers-and-convert-for-multi-variant-testing.md","Context":"## Context\\nOur current method for implementing multi-variant tests involves frequent, often non-trivial code changes to our most high traffic download pages. Prioritizing and running concurrent experiments on such pages is also often complex, increasing the risk of accidental breakage and making longer-term changes harder to roll out. Our current tool, [Traffic Cop](https:\/\/github.com\/mozmeao\/trafficcop\/), also requires significant custom code to accomodate these types of situations. Accurately measuring and reporting on the outcome of experiments is also a time consuming step of the process for our data science team, often requiring custom instrumentation and analysis.\\nWe would like to make our end-to-end experimentation process faster, with increased capacity, whilst also minimizing the performance impact and volume of code churn related to experiments running on our most important web pages.\\n","Decision":"We will use [Cloudflare Workers](https:\/\/www.cloudflare.com\/en-gb\/products\/cloudflare-workers\/) to redirect a small percentage of traffic to standalone, experimental versions of our download pages. The worker code will live in the [www-workers](https:\/\/github.com\/mozmeao\/www-workers) repository. We will implement a ([vetted and approved](https:\/\/bugzilla.mozilla.org\/show_bug.cgi?id=1565012)) third-party experimentation tool called [Convert](https:\/\/www.convert.com\/) for use on those experimental pages.\\n","tokens":169,"id":3997}
{"File Name":"james-project\/0006-task-serialization.md","Context":"## Context\\nBy switching the task manager to a distributed implementation, we need to be able to execute a `Task` on any node of the cluster.\\nWe need to have a way to describe the `Task` to be executed and serialize it in order to be able to store it in the `Created` event. Which will be persisted in the Event Store, and will be send in the event bus.\\nAt this point in time a `Task` can contain any arbitrary code. It's not an element of a finite set of actions.\\n","Decision":"* Create a `Factory` for one `Task`\\n* Inject a `Factory` `Registry` via a Guice Module\\n* The `Task` `Serialization` will be done in JSON, We will get inspired by `EventSerializer`\\n* Every `Task`s should have a specific integration test demonstrating that serialization works\\n* Each `Task` is responsible of eventually dealing with the different versions of the serialized information\\n","tokens":112,"id":2864}
{"File Name":"fundraising-application\/021_Single_or_Multi_Page_Application_Architecture.md","Context":"## Context and Problem Statement\\nThe original Fundraising Application was a classic web-based Multi-Page-Application with some JavaScript for progressive enhancement and state stored on the Server. The 2016 rewrite introduced some state on the client. The new skin we introduced in 2019 renders the template variables in an HTML data attribute, but the client-side markup is rendered with Vue. It is a \"hybrid\" Multi-Page-Application, where the client-side code still depends on the server side variables, but the server-side \"templating\" is obsolete.\\nThe current architecture has the following drawbacks:\\n* Switching pages does a full page reload\\n* Our naive client-side \"subpage\" mechanism breaks the browser history (see https:\/\/phabricator.wikimedia.org\/T285046 )\\n* Components shared across pages need to be compiled into the entry point\\nfor each page.\\n* We need \"Full Stack\" Developers who are familiar with backend\\ntechnologies (PHP, Symfony, databases, PHP CI tools, Docker) and frontend technologies\\n(TypeScript, Vue, SASS, bundler and CI tools).\\n## Decision Drivers\\n* User experience - fast page load times, browser history, immediate feedback of\\nwhat's happening, keep focus on current task.\\n* Developer experience (ease-of-use, fewest dependencies possible)\\n","Decision":"* User experience - fast page load times, browser history, immediate feedback of\\nwhat's happening, keep focus on current task.\\n* Developer experience (ease-of-use, fewest dependencies possible)\\nA separation into an API and Single-Page-Application looks like the best\\noption, regarding our decision drivers:\\n* It's improving on an already good user experience.\\n* It won't add complexity for developers beyond the status quo\\n* It's \"open\" enough to improve our technology stack, we don't tie\\nourselves to a specific library.\\nA Server-Rendered Multi-Page-Application with progressive enhancement\\nlooks like a promising architecture for the far future. We will keep\\nobserving the available technologies.\\n","tokens":274,"id":1524}
{"File Name":"wh_covid19_app\/ADR-0003 Use Flutter beta channel.md","Context":"## Context\\nThere are three different versions of flutter that you can build out: Stable, Beta and Dev. Each version will come with various trade-offs.\\nCurrently, there has been a missed stable release, and Beta is required to build iOS correctly.\\n","Decision":"We are using the beta channel.  Once some of the fixes land in stable, we may decide to revisit this decision.\\n","tokens":52,"id":298}
{"File Name":"amf-core\/0002-extracttolink-in-json-ld-expanded-emission.md","Context":"## Context\\nExpanded JSON-LD emission embeds node links. This means that whenever a node references another node, instead of using a\\nJSON-LD link we render the referenced node in place. This is because the advantage of the JSON-LD expanded emission is\\nto allow consumer applications (e.g. API Console) to consume JSON-LD as regular JSON; and for that we cannot use JSON-LD\\nlinks.\\n**The problem**: Embedding nodes causes the resulting JSON-LD to be very big (because the same node is rendered many times).\\n_Note: Flattened JSON-LD emission does not suffer this problem because it only renders each node once and then uses\\nJSON-LD links for every node reference._\\n","Decision":"`extractToLink` is a logic we developed for the expanded JSON-LD emission that introduces _AMF links_ (as opposed to\\nJSON-LD links) for some node references. Specifically, it introduces an AMF link and extracts the link target to a\\n_declaration_.\\n_AMF links_ are only available for `Linkable` domain elements, so we use it in only for certain elements. These links\\nare part of the model as opposed to JSON-LD links which are simple graph references.\\n","tokens":153,"id":410}
{"File Name":"scholarsphere\/0005-order-files-alphanumerically.md","Context":"## Context\\nFiles within a work should be displayed in a certain order. The ordering can be automatic or arbitrary.\\n","Decision":"Files will be ordered alphanumerically, according to their names. The application can now render them in the same order\\neverytime, without additional metadata.\\n","tokens":24,"id":4558}
{"File Name":"celestia-core\/adr-054-crypto-encoding-2.md","Context":"## Context\\nAmino has been a pain point of many users in the ecosystem. While Tendermint does not suffer greatly from the performance degradation introduced by amino, we are making an effort in moving the encoding format to a widely adopted format, [Protocol Buffers](https:\/\/developers.google.com\/protocol-buffers). With this migration a new standard is needed for the encoding of keys. This will cause ecosystem wide breaking changes.\\nCurrently amino encodes keys as `<PrefixBytes> <Length> <ByteArray>`.\\n","Decision":"Previously Tendermint defined all the key types for use in Tendermint and the Cosmos-SDK. Going forward the Cosmos-SDK will define its own protobuf type for keys. This will allow Tendermint to only define the keys that are being used in the codebase (ed25519).\\nThere is the the opportunity to only define the usage of ed25519 (`bytes`) and not have it be a `oneof`, but this would mean that the `oneof` work is only being postponed to a later date. When using the `oneof` protobuf type we will have to manually switch over the possible key types and then pass them to the interface which is needed.\\nThe approach that will be taken to minimize headaches for users is one where all encoding of keys will shift to protobuf and where amino encoding is relied on, there will be custom marshal and unmarshal functions.\\nProtobuf messages:\\n```proto\\nmessage PubKey {\\noneof key {\\nbytes ed25519 = 1;\\n}\\nmessage PrivKey {\\noneof sum {\\nbytes ed25519 = 1;\\n}\\n}\\n```\\n> Note: The places where backwards compatibility is needed is still unclear.\\nAll modules currently do not rely on amino encoded bytes and keys are not amino encoded for genesis, therefore a hardfork upgrade is what will be needed to adopt these changes.\\nThis work will be broken out into a few PRs, this work will be merged into a proto-breakage branch, all PRs will be reviewed prior to being merged:\\n1. Encoding of keys to protobuf and protobuf messages\\n2. Move Tendermint types to protobuf, mainly the ones that are being encoded.\\n3. Go one by one through the reactors and transition amino encoded messages to protobuf.\\n4. Test with cosmos-sdk and\/or testnets repo.\\n","tokens":104,"id":4143}
{"File Name":"mysql-monitoring-release\/0002-common-go-linter.md","Context":"## Context\\nThe `mysql-monitoring-release` comprises several self-contained `src` modules.\\nThe expectation is that each contains a `bin` directory, with a `test` bash\\nscript representing the set of unit tests for a given module.\\nThis approach makes sense, but is leading to some copied linting code.\\n","Decision":"We considered:\\n1. Making a separate module that each of the other modules' tests would source\\n1. Pulling the linting code out of the modules entirely, and putting it in CI\\nBoth options eliminate the redundant, difficult-to-maintain copypasta.\\nWe opted for option 1, as we felt that it enabled the quickest feedback loop on\\ncode formatting, because it still existed inside of the `bin\/test` scripts.\\n","tokens":67,"id":4863}
{"File Name":"datalab\/0041-user-driven-dask-and-spark.md","Context":"## Context\\nPreviously we have provisioned centralised Dask & Spark clusters which users can consume\\nfrom notebook environments. However since this decision a number of other options\\nhave emerged, specifically being able to use the native Kubernetes scheduler as Dask & Spark\\nschedulers.\\nWe are now moving to a pattern of users being able to spin up their own clusters\\nwhen required.\\n","Decision":"We have decided to collapse the centralised Dask & Spark clusters in favour of writing\\ndocumentation\/working with users to provision their own clusters using projects such\\nas [Dask Labextension](https:\/\/github.com\/dask\/dask-labextension), which is now supported\\nwithin the Labs environment.\\n","tokens":78,"id":732}
{"File Name":"helix-authentication-service\/0008-use-ecmascript-modules.md","Context":"## Context\\nSince 2015 JavaScript has had a standard for defining and importing modules, as defined in chapter 15 of the [ECMAScript 2015 Language Specification](https:\/\/262.ecma-international.org\/6.0\/) (also known as ECMAScript 6). [Node.js](https:\/\/nodejs.org\/) introduced support for ECMAScript Modules (ESM) in late 2020, with all major supported versions providing ESM support.\\nPrior to the use of ECMAScript Modules, this application was using the `require()` functionality provided by Node.js to import modules. While this worked without any issue, the dawn of a major problem appeared on the horizon: third party libraries started using ESM. The old `require()` module system cannot import modules defined using the new ECMAScript 6 syntax. While ESM is backward compatible, it is not forward compatible. As more libraries migrate to this standard, the application will gradually go stale, using old versions of libraries.\\nAs such, switching the entire application to ESM has become a necessity.\\n","Decision":"This application will use ECMAScript Modules because it is the only sensible choice going forward.\\n","tokens":222,"id":2023}
{"File Name":"texas-holdem-code-challenge\/0012-treat-this-as-a-programming-puzzle-not-a-client-deliverable.md","Context":"## Context\\nBased on the instructions, this is a \"Programming Exercise\" and the solution\\nshould be \"production level\".\\nIt is also in the format of a well specified programming puzzle: the\\n\"Specifications\" section is very precise.\\nIf this were real-world software, I would not ignore invalid inputs and the\\nlike. Even if it's not beautiful, some basic input validation would be expected\\n-- sooner or later someone's going to type in the wrong thing, and a messy stack\\ntrace is not a nice user experience. (On the other hand, if it was a programming\\ncompetition, I would not think twice about believing the specification.)\\nI would also review things like performance requirements (\"You described\\ninputting one game description at a time, is that right? Or will you need to\\nprocess batches quickly?\").\\nFurther, I would want to demo the application before delivering it to check that\\nI haven't made any silly assumptions.\\nHowever, this is neither a programming puzzle nor a client deliverable, it's a\\njob application. My guess is that Darryl isn't interested in rounds of iteration\\nto make sure I've built him just the right thing -- I'm guessing it's more about\\nseeing my process, the code I write, and the sorts of things I'm thinking about.\\nSo, in the interest of time and understanding the essence of this task:\\n","Decision":"I will treat this as a programming puzzle and not a client deliverable. I will\\ntake the instructions at face value and forsake the client feedback sessions.\\n","tokens":293,"id":2287}
{"File Name":"push-sdk-ios\/0002-avoid-runtime-dependencies.md","Context":"## Context\\nWe don't want to create an SDK that requires customers to add a slew of\\nexternal dependencies.\\n","Decision":"We will avoid external runtime dependencies. For instance, we will use the core\\nApple networking libraries (e.g. URLSession) instead of introducing a\\ndependency on something like [Alamofire][1].\\nExternal development dependencies (those necessary for contributing, but which\\nare not bundled with the SDK) will be added as necessary.\\n","tokens":24,"id":4195}
{"File Name":"web-mapviewer\/2021_04_14_drawing_library.md","Context":"## Context\\nThe application will support drawing, as its predecessor. The question is, can we achieve an approach that doesn't rely on (or isn't intertwined with) the mapping framework.\\nThe goal is also to pave the way to enable users to draw with some snapping help (on roads, or other geometries)\\n### Potential paths prospected\\n- No framework, implement a minimalist drawing library with only Javascript\\n- Use a library, such as PaperJS, to handle the drawing. Translate the output into geographic geometries.\\n- Use OpenLayers as a drawing tool (as in the viewer `mf-geoadmin3`)\\n","Decision":"After looking into all paths above, decision has been made to go with the OpenLayers approach. Here's reasons why :\\nPure Javascript approach could fit neatly in the current technology stack, but will require a lot of investment to achieve any kind of snapping\\nPaperJS has a nice toolbox for drawing, but will also require some work in order to support snapping. So the balance benefice (good drawing tools) vs. work needed isn't positive.\\nThis leaves us with the OpenLayers approach, that has already proven itself on the viewer `mf-geoadmin3`. We know that snapping is a possibility, as there's some snapping (only on the current drawing though).\\n","tokens":131,"id":4784}
{"File Name":"link_platform\/0011-use-rails-for-backend.md","Context":"## Context\\nAt the onset of the Link Platform project we needed to decide what framework to use for our backend. After some\\ndiscussion two choices presented themselves:\\n1. Serverless architecture using AWS (lamda, API Gateway, etc.)\\n1. Ruby on Rails\\n","Decision":"The decision was made to use Ruby on Rails for our backend.\\n","tokens":59,"id":5019}
{"File Name":"lockfix\/0002-typescript-as-project-language.md","Context":"## Context\\n* This is Open Source app which may be used by big variety of projects\\n* TypeScript is widely known nowadays, so there is high chance to get contribution from other peple if this technology is used\\n* Project's author and main contibutor (@kopach) is fluent in this technology, so no need in spending extra efort in learning something differrent\\n* TypeScript is type safe, so more secure and potentially should prevent from common mistakes\\n* TypeScript integrates well with JavaScript so all libraries from both ecosystems can be used easilly\\n","Decision":"Use TypeScript as main and only programming language in this project\\n","tokens":112,"id":612}
{"File Name":"akka-monitor\/0005-use-dropwizard-metrics.md","Context":"## Context\\nWe want to provide metrics about the status of a monitored service.\\n","Decision":"We will use _Dropwizard_ for creating the metrics.\\n","tokens":17,"id":1623}
{"File Name":"clone_difitalcitizenship\/0009-we-choose-a-cosmosdb-api.md","Context":"## Context\\n[CosmosDB](https:\/\/docs.microsoft.com\/en-us\/azure\/cosmos-db\/) provide two document-oriented APIs:\\n* [DocumentDB API](http:\/\/azure.github.io\/azure-documentdb-node\/DocumentClient.html)\\n* [MongoDB API](http:\/\/mongodb.github.io\/node-mongodb-native\/2.0\/)\\nFor the purpose of the Digital Citizenship project we will need the following capabilities:\\n* Single document CRUD operations\\n* Query capabilities on secondary indexes (with filtering, result limiting and ordering)\\n* Conflict detection during concurrency writes\\nWe also want to minimize the coupling with proprietary technologies and protocols.\\nComparison of the CosmosDB APIs:\\n|                    | DocumentDB API | MongoDB API |\\n| ------------------ | -------------- | ----------- |\\n| CRUD operations    | Yes            | Yes         |\\n| Query capabilities | Yes            | Yes         |\\n| Conflict detection | Yes            | Yes (1)     |\\n| Lock-in            | High           | Low         |\\n(1) Conflict detection is not supported by the MongoDB API but can be implemented at the application level via [MVCC](https:\/\/en.wikipedia.org\/wiki\/Multiversion_concurrency_control) with the addition of a `version` field to the documents.\\n","Decision":"We decide to implement all database operations via the MongoDB API.\\n","tokens":263,"id":1179}
{"File Name":"elife-base-images\/0005-provide-standard-bin.md","Context":"## Context\\nApplications often need to drop in binaries or scripts, either downloaded or picked up from other images such as `proofreader-php`.\\nThe binaries needed do not need `root` permissions or system-wide installation.\\nThe binaries may need to modify the application files.\\nA container image may be an alien environment, as it makes it difficult to find out which of the files were provided by the Dockerfile build process.\\n","Decision":"Every image should provide a standard `\/srv\/bin` folder, on the PATH, containing utilities owned by `elife`.\\n","tokens":86,"id":1505}
{"File Name":"scientific-thesis-template\/0005-use-pdflatex-as-default.md","Context":"## Context and Problem Statement\\nWhich latex compiler to use?\\n* pdflatex\\n* xelatex\\n* lualatex\\n","Decision":"Chosen option: \"pdflatex\", because compiles faster and correct ligatures are required at the final typesetting step only.\\n### Positive Consequences\\n* Faster compile time\\n### Negative Consequences\\n* Using lualatex just before publication might lead to a different layout and additional effort\\n* lualatex's power might not be used at all\\n","tokens":30,"id":250}
{"File Name":"unit-e-project\/ADR-0016.md","Context":"## Context\\nFor an open source project being able to distribute its code under the chosen\\nlicense it's important to make sure that all contributions are done under this\\nlicense and that contributors have the legal right to do so. There are three\\nways how this is usually handled:\\n* Rely on the implicit agreement by contributors when they submit code to a\\nproject with a known license.\\n* Require contributors to sign a contributor license agreement (CLA) which\\nexplicitly assigns rights to the project and gives guarantees from the\\ncontributor. There are a variety of CLAs used by open source projects which\\ndiffer in details such as what rights are given to the organization governing\\nthe project. For contributions being done by a company it's required that the\\ncompany has signed a corporate CLA.\\n* The [Developer Certificate of Origin\\n(DCO)](https:\/\/developercertificate.org\/), established by the Linux\\nFoundation, which is an explicit statement by contributors in each commit that\\nthey have the legal rights to contribute the code and are contributing the\\ncode under the license the project has chosen. This doesn't require signing\\nformal documents between project and contributor or their employer.\\nThere are two dimensions to consider in choosing which way to use, one is the\\nclarity about the terms of the contribution and the other is how much of a\\nbarrier is created by adding requirements for contributions.\\nThe DCO is an increasingly popular way of providing legal clarity while not\\nadding the overhead and barrier of a CLA. It works in a symmetric way, not\\ngiving the project additional rights such as to relicense the code under a\\nproprietary license which often is the case in CLAs. It doesn't require signing\\nan agreement which can be difficult if the employer of the developer has to do\\nthat. It is standardized and used by many open source projects such as the Linux\\nKernel, Docker, Chef, or GitLab, so that many developers are already familiar\\nwith it and know how to use it.\\nUsing the DCO sends a clear signal that the open source project is serious about\\nthe integrity of its code not only from a technical perspective but also from a\\nlegal one.\\n","Decision":"UnitE adopts the DCO. That means that all commits MUST be signed off by the\\nauthor by adding a statement at the end of the commit message in the form:\\nSigned-off-by: Jane Doe <jd@example.com>\\nThis statement declares that the author has the right to contribute the code and\\ndoes so under the license of the open source project, which in the case of UnitE\\nis MIT. The statement formally declares acceptance of the Developer Certificate\\nof Origin which is hosted at https:\/\/developercertificate.org\/:\\nDeveloper Certificate of Origin\\nVersion 1.1\\nCopyright (C) 2004, 2006 The Linux Foundation and its contributors.\\n1 Letterman Drive\\nSuite D4700\\nSan Francisco, CA, 94129\\nEveryone is permitted to copy and distribute verbatim copies of this\\nlicense document, but changing it is not allowed.\\nDeveloper's Certificate of Origin 1.1\\nBy making a contribution to this project, I certify that:\\n(a) The contribution was created in whole or in part by me and I\\nhave the right to submit it under the open source license\\nindicated in the file; or\\n(b) The contribution is based upon previous work that, to the best\\nof my knowledge, is covered under an appropriate open source\\nlicense and I have the right under that license to submit that\\nwork with modifications, whether created in whole or in part\\nby me, under the same open source license (unless I am\\npermitted to submit under a different license), as indicated\\nin the file; or\\n(c) The contribution was provided directly to me by some other\\nperson who certified (a), (b) or (c) and I have not modified\\nit.\\n(d) I understand and agree that this project and the contribution\\nare public and that a record of the contribution (including all\\npersonal information I submit with it, including my sign-off) is\\nmaintained indefinitely and may be redistributed consistent with\\nthis project or the open source license(s) involved.\\nOther people involved with creating the submission, such as co-authors or the\\ncommitter, SHOULD add their own `Signed-off-by` statement so that you might get\\nmultiple of these statements in a commit message.\\n","tokens":455,"id":3773}
{"File Name":"app-performance-summary\/0003-integrate-with-drive.md","Context":"## Context\\n- We're using [Google Data Studio](https:\/\/datastudio.google.com) to present KPIs to stakeholders.\\n- We're calculating application metrics on a periodic basis\\n- We need a way to get the data into a dashboard\\n","Decision":"Integrate with google drive so we can load KPI reports into a spreadsheet within the GOV.UK team drive.\\nUse the [Pygsheets](http:\/\/pygsheets.readthedocs.io\/) library for this.\\nThe data in the sheet can then be used as a data source within data studio.\\n","tokens":52,"id":4193}
{"File Name":"dos-server\/adr-9-airflow.md","Context":"## Context\\nA tool is needed that can automatically run ingest scripts on a pre-defined schedule and provide status updates on ingest operations.\\n","Decision":"The project will use Apache Airflow to run ingest scripts.\\n","tokens":27,"id":3381}
{"File Name":"bfi-discovery\/0001-choice-of-rdbms.md","Context":"## Context and Problem Statement\\nBFI's IIIF Univeral Viewer auditing platform requires that each event\\nwhich is subject to long term persistence is captured in a manner that\\nis secure, robust and performant.\\n## Decision Drivers\\n* Ability for internal BFI staff to leverage their existing experience\\nwith managing and maintaining the RDMS.\\n* Ease of deployment and configuration.\\n* Ease of ongoing maintenance, patching and support.\\n* Open source, mature and in active support \/ maintenance.\\n","Decision":"* Ability for internal BFI staff to leverage their existing experience\\nwith managing and maintaining the RDMS.\\n* Ease of deployment and configuration.\\n* Ease of ongoing maintenance, patching and support.\\n* Open source, mature and in active support \/ maintenance.\\nMySQL is selected as the solution as it aligns provides BFI with an RDMS\\nwith which they already have experience and are able to support and\\nmaintaining in the long term.\\n### Positive Consequences\\n* No initial or ongoing licensing costs associated with the use of\\nMySQL.\\n* Ease of ongoing maintenance for BFI owning to their existing\\nfamiliarity with MySQL.\\n* Availability of multiple storage engines (InnoDB, MyISAM, etc) and\\ndistributions (MySQL Community Edition, MariaDB, PerconaDB, etc).\\n* Simple deployment and configuration (e.g., MySQL publish official\\nDocker images).\\n### Negative Consequences\\n* Certain SQL features are not available (although it is not expected\\nthat these will be required).\\n","tokens":104,"id":4854}
{"File Name":"dalmatian-frontend\/0001-use-pull-request-templates.md","Context":"## Context\\nThe quality of information included in our pull requests varies greatly which can lead to code reviews which take longer and are harder for the person to understand the considerations, outcomes and consquences of a series of changes.\\nA couple of recent projects have found a GitHub pull request template to have been a positive change. Prompting what pull request descriptions should include has lead to better documented changes that have been easier to review on the whole.\\n","Decision":"Include a basic pull request template for GitHub so that every pull request prompts every author to fill it out.\\n","tokens":89,"id":2557}
{"File Name":"verify-service-provider\/0016-we-will-have-a-healthcheck-endpoint.md","Context":"## Context\\nIn various user research sessions we've observed users start the MSA or the verify service provider\\nand then want to check whether it's working correctly. There's also a need for users to be able to\\nmonitor the health of the system once it's deployed to their environment.\\nDropwizard allows you to configure an HTTP endpoint as a healthcheck. This can perform some arbitrary\\nactions that check the health of the system.\\n","Decision":"We will have a healthcheck endpoint that will check the verify-service-provider can read metadata from\\nthe hub and the MSA.\\n","tokens":90,"id":4455}
{"File Name":"libelektra\/base_name.md","Context":"## Problem\\nA key name is made out of a sequence of key part names, and can be constructed with `keyAddBaseName\/keySetBaseName`.\\nBoth applications and configuration file formats might need arbitrary strings to be encoded within a key name part.\\nFor example:\\n- an application uses names of internal components as sections within the configuration.\\n- a parser reads an empty string, to be encoded as base name.\\n","Decision":"`keyAddBaseName\/keySetBaseName` never fail with any argument, so any character sequence can be escaped except of NULL bytes.\\nThe argument goes unmodified to the unescaped key name.\\nFor arrays there is no escaping needed because an array is only an array if the metadata `array` is appended to the direct parent key.\\nSee [array](array.md).\\n- [Array](array.md)\\n","tokens":89,"id":1297}
{"File Name":"nhsuk-prototype-kit-version-one\/0004-use-basic-auth-to-secure-published-prototypes.md","Context":"## Context\\nGiven that prototypes built using the NHSUK prototype kit will use the same styling and branding as live NHSUK services, if a member of the public came across the prototype they may confuse it with a live service. It would be beneficial to secure these prototypes behind some level of authentication.\\n","Decision":"We will use the [basic-auth](https:\/\/www.npmjs.com\/package\/basic-auth) package to add a layer of authentication when the prototype application is run on a production environment. This will allow the creator of the prototype to set a username and password to access the prototype when published.\\n","tokens":60,"id":3683}
{"File Name":"react\/adr-XXX-file-structure.md","Context":"## Context\\nComponents might be more grokable if they were structured consistently. This ADR proposes conventions\\n","Decision":"TL;DR:\\n```\\nprimer-react\/\\n\u251c\u2500 src\/\\n\u2502  \u251c\u2500 Breadcrumbs\/\\n\u2502  \u2502  \u251c\u2500 index.ts                    \/\/ Just re-exporting?\\n\u2502  \u2502  \u251c\u2500 Breadcrumbs.tsx             \/\/ Primary component\\n\u2502  \u2502  \u251c\u2500 BreadcrumbsItem.tsx         \/\/ Subcomponent (include parent component name to increase findability in most IDEs)\\n\u2502  \u2502  \u251c\u2500 Breadcrumbs.mdx             \/\/ Documentation. Always .mdx, not .md\\n\u2502  \u2502  \u251c\u2500 Breadcrumbs.stories.tsx\\n\u2502  \u2502  \u251c\u2500 Breadcrumbs.test.tsx        \/\/ Unit tests\\n\u2502  \u2502  \u251c\u2500 Breadcrumbs.types.test.tsx  \/\/ Type tests\\n\u2502  \u2502  \u251c\u2500 Breadcrumbs.yml             \/\/ Component metadata (Possible future)\\n\u2502  \u2502  \u2514\u2500 __snapshots__\/\\n\u2506  \u2506\\n```\\n### Rules\\n- Every component should have its own PascalCased directory directly under `src\/`\\n- Subcomponents should be properties of the exported parent component (e.g., `Breadcrumbs.Item`)\\n- Replacements of existing components should use an incrementing number (e.g., `Breadcrumbs2` rather than `NewBreadcrumbs`)\\n","tokens":22,"id":98}
{"File Name":"tdr-dev-documentation\/0001-multi-account-terraform.md","Context":"## Context\\nWe have created multiple AWS accounts to host the different TDR environments,\\nand we're happy with the decision we made during the Alpha to use Terraform to\\nconfigure the infrastructure (see [terraform.md][alpha-terraform] and\\n[terraform_vs_cloudformation.md][tf-vs-cf]).\\nThe Alpha only ran in one AWS account. We used Terraform workspaces so that we\\ncould create multiple environments if we wanted, but in the end we only used one\\nenvironment for prototyping and user testing.\\nIn Alpha, we ran Terraform scripts from our dev machines. For Beta, we'd prefer\\nto run Terraform in a controlled environment like Jenkins. This has a few\\nadvantages:\\n- Makes it easier to control access to production infrastructure: a developer\\ncan have permission to run Jenkins jobs (and therefore make\\nversion-controlled, peer-reviewed changes to production) without having\\ncredentials giving them full access to the production account\\n- Makes it easier to deploy changes to each environment in turn\\n- Provides a record of (recent) Terraform runs, and who started them\\nBut there's a chicken-and-egg problem with running Terraform from a hosted\\nsystem like Jenkins, because it would be helpful if we could use Terraform to\\ncreate the Jenkins infrastructure itself.\\n[alpha-terraform]: ..\/technology-considerations\/terraform.md\\n[tf-vs-cf]: ..\/technology-considerations\/terraform_vs_cloudformation.md\\n","Decision":"Create multiple Terraform projects, which are run in different stages:\\n- A backend project which is run against the management account and bootstraps\\nthe rest of the infrastructure. It is run from a development machine and\\ncreates the IAM roles and Terraform state storage (S3 and DynamoDB) that will\\nbe used by the environment-specific Terraform scripts. This project's own\\nTerraform state is stored in a manually-created S3 bucket and DynamoDB table.\\n- A CI project which is run against the management account when the backend\\nbootstrap script has been run. It is also run from a development machine, but\\nthe Terraform state is saved in the storage set up by the bootstrap script.\\n- An environment project which is run against each TDR environment account\\n(integration, staging, production). This script is run from Jenkins.\\nSince the environment-specific Terraform is run from Jenkins, which is in AWS,\\nwe can use IAM roles to give Jenkins permission to run Terraform. This means\\nthat we don't need to generate an AWS secret key (which could be used from\\noutside our environment if it was stolen) for Jenkins to run Terraform.\\n","tokens":303,"id":1777}
{"File Name":"handbook\/0011-open-source-license.md","Context":"## Context and Problem Statement\\nIt is the offical policy of the Digital Iceland and stated in the Techical Direction that it is to be implemented as free and open source. Open source software by definition is open to anyone to use, modify, distribute and study. These permissions are enforced an open source license. There are a number of well-known and widely used open source licenses available and we need to choose a license that best fits the goals of digital iceland.\\nThere are two main types of open source licences:  more permissive licences that confer broad freedoms and minimal obligations (e.g., the MIT, BSD and the Apache 2.0 licences); and sharealike licences that require licensing adaptations with the same licence if they distribute them (e.g., the GNU GPL).\\nDevelopment for Digital Iceland will be open and free with minimum complications for development for all involved. Reuse and transparency will be promoted.\\n## Decision Drivers\\n* The primary motivation is to encourage, co-development, collabiration, transparency and reuse of the software.\\n* It is important to build on the experience of similar government led inititives in other countries.\\n* Digital Iceland has no patents or intellecatual property that needs to be protected or guarded by the license chosen.\\n* It is not a concern for Digital Iceland that the license restricts usage in other projects, be it open or closed source.\\n","Decision":"* The primary motivation is to encourage, co-development, collabiration, transparency and reuse of the software.\\n* It is important to build on the experience of similar government led inititives in other countries.\\n* Digital Iceland has no patents or intellecatual property that needs to be protected or guarded by the license chosen.\\n* It is not a concern for Digital Iceland that the license restricts usage in other projects, be it open or closed source.\\nThe MIT license was chosen, for the following reasons:\\n* It is the least restrictive of the licenses.\\n* It is very consise, simple and easy to understand and therefore should be clear to users and developers.\\n* Digital Iceland does not require protection of patents or existing intelletual property.\\n* Well known government lead initiatives like uk.gov and X-Road use the MIT license.\\n* The MIT license is the best known and most widely used free and open-source license in the world.\\n","tokens":285,"id":1994}
{"File Name":"princeton_ansible\/0003-cicognara-letsencrypt-path.md","Context":"## Context\\nThere are instances where we will be unable to use our InCommon Federation SSL certificate framework for certs. In instances where a dot org, etc., In those instances Let's Encrypt is an adequate solution.\\nThe Lets Encrypt expects the SSL certs to be under `\/etc\/letsencrypt\/` which differs from where we place the rest of our SSL certificated\\n","Decision":"make sure certificates are under `\/etc\/letsencrypt\/`\\ncreate a directory under `\/var\/www\/letsencrypt\/<servicename>` This directory which is configured as the `http:\/\/tld.domain\/.well_known` will be empty but is needed during the Let's Encrypt verification step.\\n","tokens":75,"id":2254}
{"File Name":"adr-j\/0005-help-comments.md","Context":"## Context\\nThe tool will have a help subcommand to provide documentation for users.\\nIt is usful to have the usage documentation in the code. When reading the code, that's the first place to look for information about how to run the command.\\n","Decision":"The command classes are annotated with usage documentation. This is actively read when usage documentation needs to be displayed, thus avoiding the use of separate help files etc..\\n","tokens":53,"id":4159}
{"File Name":"cljdoc\/0003-utilize-grimoire-for-storage.md","Context":"## Context\\nWe need to store documentation (API, articles, etc.) for many different projects, versions\\nand platforms.\\n","Decision":"[Grimoire](https:\/\/github.com\/clojure-grimoire\/lib-grimoire) has a storage design that\\nsupports those requirements and seems to be a well designed project overall.\\nGrimoire also supports implementing different storage backends which may be useful later.\\nFor now we will try to build a filesystem based storage for documentaton based on Grimoire's\\nformat and ideas.\\n","tokens":26,"id":2608}
{"File Name":"rufus\/nnnn-exported-identifiers.md","Context":"## Context and problem statement\\nA package should only export public modules, and a module should only export\\npublic types, constants and functions. It should not be possible for a user to\\naccess private identifiers in a package they're using. What facilities should\\nRufus provide for users to manage encapsulation?\\n","Decision":"* A user can tell whether an identifier is public or private at a glance.\\n* Works well for all users, including those that uses non-latin languages and\\nalphabets.\\n* It's not possible to access private modules from outside their package.\\n* It's not possible to access private types, constants, or functions from\\noutside their module.\\nChosen option: option 1, because it avoids Rufus source files in the same\\ndirectory have names that differ only by case, which is likely to lead to\\nconfusion.\\n","tokens":64,"id":3044}
{"File Name":"arch\/0046-knowledge-sharing.md","Context":"## Context\\nFor a long time, we only have a few tech sharing in our company.\\nSo we take a survey in October and get the willings of sharing details from the whole R&D team, subjects include: `Python`, `Java`, `database`, `spider`, `data analysis`, `automation`, `algorithm`, `project experience` and so on.\\nAbsolutely, People always have the passion to attend sharing.\\n","Decision":"There are a lot of ways to do sharing.\\n* Formal sharing\\n* sharer prepare a specific topic with well design ppt\\n* Casual talking\\n* people sit around a table and talk\\n* `Lean coffee`\\n* participants gather, build an agenda, and begin talking\\nFormal sharing need a long time for preparing, we have try, but have too few sharers.\\nCasual talking don\u2019t have good topics, as developer always too shy to talk.\\n`Lean Coffee` is a structured, but agenda-less meeting. Participants gather, build an agenda, and begin talking. Conversations are directed and productive because the agenda for the meeting was democratically generated.\\n`Lean Coffee` talk a little time(less than 10 minutes) to prepare, but have the chosen topics for talking.\\nWe decide make this every Monday from 7:30 to 9:00.\\n### How does Lean Coffee work?\\n1. Set up a simple Kanban;\\n2. What to Discuss;\\n3. Vote and Talk;\\n### Details\\n1. 3 columns: `To Discuss` `Discussion` `Discussed`;\\n2. Put topics in `To Discuss` with 1-2 sentences each other;\\n3. Vote (2 votes each) and order it;\\n4. Set an 8 minute timer;\\n5. Move the first item into `Discussion` and start talking;\\n6. Repeat;\\n7. 5 minutes left for 1-2 sentence takeaways each other.\\n","tokens":89,"id":2419}
{"File Name":"oasis-core\/0003-consensus-runtime-token-transfer.md","Context":"## Context\\nCurrently each runtime can define its own token (or none at all) and there is no\\nmechanism that would support transfer of consensus layer tokens into a runtime\\nand back out.\\nIntroducing such a mechanism would allow the consensus layer tokens to be used\\ninside runtimes for various functions. This ADR proposes such a mechanism.\\n","Decision":"On a high level, this proposal adds support for consensus\/runtime token\\ntransfers as follows:\\n- **Each staking account can set an allowance for beneficiaries.** Each staking\\naccount can set an allowance, a maximum amount a beneficiary can withdraw from\\nthe given account. Beneficiaries are identified by their address. This is\\nsimilar to approve\/transferFrom calls defined by the [ERC-20 Token Standard].\\nPreviously such functionality was already present but was removed in\\n[oasis-core#2021].\\n- **Each runtime itself has an account in the consensus layer.** This account\\ncontains the balance of tokens which are managed exclusively by the runtime\\nand do not belong to any specific regular account in the consensus layer.\\nIt is not possible to transfer directly into a runtime account and doing so\\nmay result in funds to be locked without a way to reclaim them.\\nThe only way to perform any operations on runtime accounts is through the use\\nof messages emitted by the runtime during each round. These messages are\\nsubject to discrepancy detection and instruct the consensus layer what to do.\\nCombined, the two mechanisms enable account holders to set an allowance in the\\nbenefit of runtimes so that the runtimes can withdraw up to the allowed amount\\nfrom the account holder's address.\\n### Addresses\\nThis proposal introduces the following new address context for the runtime\\naccounts:\\n```\\noasis-core\/address: runtime\\n```\\nInitial version for the address context is `0`. To derive the address, the\\nstandard address derivation scheme is used, with the runtime's 32-byte\\nidentifier used as the `data` part.\\n### State\\nThis proposal introduces\/updates the following consensus state in the staking\\nmodule:\\n#### General Accounts\\nThe general account data structure is modified to include an additional field\\nstoring the allowances as follows:\\n```golang\\ntype GeneralAccount struct {\\n\/\/ ... existing fields omitted ...\\nAllowances map[Address]quantity.Quantity `json:\"allowances,omitempty\"`\\n}\\n```\\n### Transaction Methods\\nThis proposal adds the following new transaction methods in the staking module:\\n#### Allow\\nAllow enables an account holder to set an allowance for a beneficiary.\\n**Method name:**\\n```\\nstaking.Allow\\n```\\n**Body:**\\n```golang\\ntype Allow struct {\\nBeneficiary  Address           `json:\"beneficiary\"`\\nNegative     bool              `json:\"negative,omitempty\"`\\nAmountChange quantity.Quantity `json:\"amount_change\"`\\n}\\n```\\n**Fields:**\\n- `beneficiary` specifies the beneficiary account address.\\n- `amount_change` specifies the absolute value of the amount of base units to\\nchange the allowance for.\\n- `negative` specifies whether the `amount_change` should be subtracted instead\\nof added.\\nThe transaction signer implicitly specifies the general account. Upon executing\\nthe allow the following actions are performed:\\n- If either the `disable_transfers` staking consensus parameter is set to `true`\\nor the `max_allowances` staking consensus parameter is set to zero, the method\\nfails with `ErrForbidden`.\\n- It is checked whether either the transaction signer address or the\\n`beneficiary` address are reserved. If any are reserved, the method fails with\\n`ErrForbidden`.\\n- Address specified by `beneficiary` is compared with the transaction signer\\naddress. If the addresses are the same, the method fails with\\n`ErrInvalidArgument`.\\n- The account indicated by the signer is loaded.\\n- If the allow would create a new allowance and the maximum number of allowances\\nfor an account has been reached, the method fails with `ErrTooManyAllowances`.\\n- The set of allowances is updated so that the allowance is updated as specified\\nby `amount_change`\/`negative`. In case the change would cause the allowance to\\nbe equal to zero or negative, the allowance is removed.\\n- The account is saved.\\n- The corresponding `AllowanceChangeEvent` is emitted with the following\\nstructure:\\n```golang\\ntype AllowanceChangeEvent struct {\\nOwner        Address           `json:\"owner\"`\\nBeneficiary  Address           `json:\"beneficiary\"`\\nAllowance    quantity.Quantity `json:\"allowance\"`\\nNegative     bool              `json:\"negative,omitempty\"`\\nAmountChange quantity.Quantity `json:\"amount_change\"`\\n}\\n```\\nWhere `allowance` contains the new total allowance, the `amount_change`\\ncontains the absolute amount the allowance has changed for and `negative`\\nspecifies whether the allowance has been reduced rather than increased. The\\nevent is emitted even if the new allowance is zero.\\n#### Withdraw\\nWithdraw enables a beneficiary to withdraw from the given account.\\n**Method name:**\\n```\\nstaking.Withdraw\\n```\\n**Body:**\\n```golang\\ntype Withdraw struct {\\nFrom   Address           `json:\"from\"`\\nAmount quantity.Quantity `json:\"amount\"`\\n}\\n```\\n**Fields:**\\n- `from` specifies the account address to withdraw from.\\n- `amount` specifies the amount of base units to withdraw.\\nThe transaction signer implicitly specifies the destination general account.\\nUpon executing the withdrawal the following actions are performed:\\n- If either the `disable_transfers` staking consensus parameter is set to `true`\\nor the `max_allowances` staking consensus parameter is set to zero, the method\\nfails with `ErrForbidden`.\\n- It is checked whether either the transaction signer address or the\\n`from` address are reserved. If any are reserved, the method fails with\\n`ErrForbidden`.\\n- Address specified by `from` is compared with the transaction signer address.\\nIf the addresses are the same, the method fails with `ErrInvalidArgument`.\\n- The source account indicated by `from` is loaded.\\n- The destination account indicated by the transaction signer is loaded.\\n- `amount` is deducted from the corresponding allowance in the source account.\\nIf this would cause the allowance to go negative, the method fails with\\n`ErrForbidden`.\\n- `amount` is deducted from the source general account balance. If this would\\ncause the balance to go negative, the method fails with\\n`ErrInsufficientBalance`.\\n- `amount` is added to the destination general account balance.\\n- Both source and destination accounts are saved.\\n- The corresponding `TransferEvent` is emitted.\\n- The corresponding `AllowanceChangeEvent` is emitted with the updated\\nallowance.\\n### Queries\\nThis proposal adds the following new query methods in the staking module by\\nupdating the `staking.Backend` interface as follows:\\n```golang\\ntype Backend interface {\\n\/\/ ... existing methods omitted ...\\n\/\/ Allowance looks up the allowance for the given owner\/beneficiary combination.\\nAllowance(ctx context.Context, query *AllowanceQuery) (*quantity.Quantity, error)\\n}\\n\/\/ AllowanceQuery is an allowance query.\\ntype AllowanceQuery struct {\\nHeight      int64   `json:\"height\"`\\nOwner       Address `json:\"owner\"`\\nBeneficiary Address `json:\"beneficiary\"`\\n}\\n```\\n### Messages\\nSince this is the first proposal that introduces a new runtime message type that\\ncan be emitted from a runtime during a round, it also defines some general\\nproperties of runtime messages and the dispatch mechanism:\\n- Each message has an associated gas cost that needs to be paid by the\\nsubmitter (e.g. as part of the `roothash.ExecutorCommit` method call). The gas\\ncost is split among the committee members.\\n- There is a maximum number of messages that can be emitted by a runtime during\\na given round. The limit is defined both globally (e.g. a roothash consensus\\nparameter) and per-runtime (which needs to be equal to or lower than the\\nglobal limit).\\n- Messages are serialized using a sum type describing all possible messages,\\nwhere each message type is assigned a _field name_:\\n```golang\\ntype Message struct {\\nMessage1 *Message1 `json:\"message1,omitempty\"`\\nMessage2 *Message2 `json:\"message2,omitempty\"`\\n\/\/ ...\\n}\\n```\\n- All messages are versioned by embeding the `cbor.Versioned` structure which\\nprovides a single `uint16` field `v`.\\n- A change is made to how messages are included in commitments, to reduce the\\nsize of submitted transactions.\\nThe `ComputeResultsHeader` is changed so that the `Messages` field is replaced\\nwith a `MessagesHash` field containing a hash of the CBOR-encoded messages\\nemitted by the runtime.\\nAt the same time `ComputeBody` is changed to include an additional field\\n`Messages` as follows:\\n```golang\\ntype ComputeBody struct {\\n\/\/ ... existing fields omitted ...\\nMessages []*block.Message `json:\"messages,omitempty\"`\\n}\\n```\\nThe `Messages` field must only be populated in the commitment by the\\ntransaction scheduler and must match the `MessagesHash`.\\n- If any of the included messages is deemed _malformed_, the round fails and the\\nruntime state is not updated.\\n- In order to support messages that fail to execute, a new roothash event is\\nemitted for each executed message:\\n```golang\\ntype MessageEvent struct {\\nIndex  uint32 `json:\"index,omitempty\"`\\nModule string `json:\"module,omitempty\"`\\nCode   uint32 `json:\"code,omitempty\"`\\n}\\n```\\nWhere the `index` specifies the index of the executed message and the `module`\\nand `code` specify the module and error code accoording to Oasis Core error\\nencoding convention (note that the usual human readable message field is not\\nincluded).\\nThis proposal introduces the following runtime messages:\\n#### Staking Method Call\\nThe staking method call message enables a runtime to call one of the supported\\nstaking module methods.\\n**Field name:**\\n```\\nstaking\\n```\\n**Body:**\\n```golang\\ntype StakingMessage struct {\\ncbor.Versioned\\nTransfer *staking.Transfer `json:\"transfer,omitempty\"`\\nWithdraw *staking.Withdraw `json:\"withdraw,omitempty\"`\\n}\\n```\\n**Fields:**\\n- `v` must be set to `0`.\\n- `transfer` indicates that the `staking.Transfer` method should be executed.\\n- `withdraw` indicates that the `staking.Withdraw` method should be executed.\\nExactly one of the supported method fields needs to be non-nil, otherwise the\\nmessage is considered malformed.\\n### Consensus Parameters\\n#### Staking\\nThis proposal introduces the following new consensus parameters in the staking\\nmodule:\\n- `max_allowances` (uint32) specifies the maximum number of allowances an\\naccount can store. Zero means that allowance functionality is disabled.\\n#### Roothash\\nThis proposal introduces the following new consensus parameters in the roothash\\nmodule:\\n- `max_runtime_messages` (uint32) specifies the global limit on the number of\\nmessages that can be emitted in each round by the runtime. The default value\\nof `0` disables the use of runtime messages.\\n### Runtime Host Protocol\\nThis proposal modifies the runtime host protocol as follows:\\n#### Host to Runtime: Initialization\\nThe existing `RuntimeInfoRequest` message body is updated to contain a field\\ndenoting the consensus backend used by the host and its consensus protocol\\nversion as follows:\\n```golang\\ntype RuntimeInfoRequest struct {\\nConsensusBackend         string `json:\"consensus_backend\"`\\nConsensusProtocolVersion uint64 `json:\"consensus_protocol_version\"`\\n\/\/ ... existing fields omitted ...\\n}\\n```\\nThis information can be used by the runtime to ensure that it supports the\\nconsensus layer used by the host. In case the backend and\/or protocol version is\\nnot supported, the runtime should return an error and terminate. In case the\\nruntime does not interact with the consensus layer it may ignore the consensus\\nlayer information.\\n#### Host to Runtime: Transaction Batch Dispatch\\nThe existing `RuntimeExecuteTxBatchRequest` and `RuntimeCheckTxBatchRequest`\\nmessage bodies are updated to include the consensus layer light block at the\\nlast finalized round height (specified in `.Block.Header.Round`) and the list of\\n`MessageEvent`s emitted while processing the runtime messages emitted in the\\nprevious round as follows:\\n```golang\\ntype RuntimeExecuteTxBatchRequest struct {\\n\/\/ ConsensusBlock is the consensus light block at the last finalized round\\n\/\/ height (e.g., corresponding to .Block.Header.Round).\\nConsensusBlock consensus.LightBlock `json:\"consensus_block\"`\\n\/\/ MessageResults are the results of executing messages emitted by the\\n\/\/ runtime in the previous round (sorted by .Index).\\nMessageResults []roothash.MessageEvent `json:\"message_results,omitempty\"`\\n\/\/ ... existing fields omitted ...\\n}\\ntype RuntimeCheckTxBatchRequest struct {\\n\/\/ ConsensusBlock is the consensus light block at the last finalized round\\n\/\/ height (e.g., corresponding to .Block.Header.Round).\\nConsensusBlock consensus.LightBlock `json:\"consensus_block\"`\\n\/\/ ... existing fields omitted ...\\n}\\n```\\nThe information from the light block can be used to access consensus layer\\nstate.\\n#### Runtime to Host: Read-only Storage Access\\nThe existing `HostStorageSyncRequest` message body is updated to include an\\nendpoint identifier as follows:\\n```golang\\ntype HostStorageSyncRequest struct {\\n\/\/ Endpoint is the storage endpoint to which this request should be routed.\\nEndpoint string `json:\"endpoint,omitempty\"`\\n\/\/ ... existing fields omitted ...\\n}\\n```\\nThe newly introduced `endpoint` field can take the following values:\\n- `runtime` (or empty string) denotes the runtime state endpoint. The empty\\nvalue is allowed for backwards compatibility as this was the only endpoint\\navailable before this proposal.\\n- `consensus` denotes the consensus state endpoint, providing access to\\nconsensus state.\\n### Rust Runtime Support Library\\nThe Rust runtime support library (`oasis-core-runtime`) must be updated to\\nsupport the updated message structures. Additionally, there needs to be basic\\nsupport for interpreting the data from the Tendermint consensus layer backend:\\n- Decoding light blocks.\\n- Decoding staking-related state structures.\\nThe Tendermint-specific functionality should be part of a separate crate.\\n### Expected User\/Consensus\/Runtime Flow\\n**Scenario:**\\nAccount holder has 100 tokens in her account in the consensus layer staking\\nledger and would like to spend 50 tokens to execute an action in runtime X.\\n**Flow:**\\n- Account holder sets an allowance of 50 tokens for runtime X by submitting an\\nallow transaction to the consensus layer.\\n- Account holder submits a runtime transaction that performs some action costing\\n50 tokens.\\n- Account holder's runtime transaction is executed in runtime X round R:\\n- Runtime X emits a message to transfer 50 tokens from the user's account to\\nthe runtime's own account.\\n_As an optimization runtime X can verify current consensus layer state and\\nreject the transaction early to prevent paying for needless consensus layer\\nmessage processing._\\n- Runtime X updates its state to indicate a pending transfer of 50 tokens from\\nthe user. It uses the index of the emitted message to be able to match the\\nmessage execution result once it arrives.\\n- Runtime X submits commitments to the consensus layer.\\n- When finalizing round R for runtime X, the consensus layer transfers 50 tokens\\nfrom the account holder's account to the runtime X account.\\n- Corresponding message result event is emitted, indicating success.\\n- When runtime X processes round R+1, the runtime receives the set of emitted\\nmessage result events.\\n- Runtime X processes message result events, using the index field to match the\\ncorresponding pending action and executes whatever action it queued.\\n- In case the message result event would indicate failure, the pending action\\ncan be pruned.\\n","tokens":73,"id":4349}
{"File Name":"holochain-rust\/0009-support-64bit-architectures-only.md","Context":"## Context\\nGoing forward with the rust implementation, we recognize that some 32bit architectures exist that people may want to run Holochain on.  Supporting 32bit architectures may have particular consequences in the realm of cryptography.  We have limited resources.\\n","Decision":"For now we will assume availability 64bit CPUs and not use our resources testing against 32bit targets.\\n","tokens":53,"id":1496}
{"File Name":"figgy\/0001-document-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":4823}
{"File Name":"ontrack\/0002-extension-indicators-identifiers.md","Context":"## Context\\nExample: take principles from an asciidoc source.\\nFor sync, having uuid might be an issue unless we use a \"source\" optional\\nattribute. That might make things more complex...\\nA source can be empty (when created from GUI) or set to the FQCN of the\\nprovisioner.\\nEven with a source attribute, having UUID does not make the automatic\\nidentification of a category or a type, based on name only, easy.\\nHence the need for a semantic id.\\nRisks of collision for provisioners remains and provisioned items will have\\nto prepend the id of their provisioner to their id.\\nA qualified source attribute still make sense to identify items which\\ncannot be edited. To fit with the usual way of Ontrack, the source must\\nbe a FQCN which maps to an `Extension`.\\n","Decision":"* do not use UUID for indicator items (categories, types & protfolios)\\n* for provisioned items, prepend the id of the provisioner to the item id\\n* introduce a source attribute and an indicator source model\\n","tokens":184,"id":5121}
{"File Name":"hee-web-blueprint\/0010-use-external-document-management-system.md","Context":"## Context\\nWe sought to determine whether to deliver our document management capabilities using the content management platform natively or through the integration of an external document management platform.\\n### Options\\n####\u00a0NHS Digital BloomReach\\nWe looked at the implementation of document management functionality in BloomReach delivered as part of the work for NHS Digital - https:\/\/github.com\/NHS-digital-website\/hippo. This project provides a good view of what native BloomReach document management and publishing looks like, it delivers a flexible content model for publishing to HTML documents.\\n#### Office 365\\nIn addition we should look at the current usage of Office 365 to determine whether the platform would be suitable for integration with BloomReach.\\n#### FutureNHS\\nFinally looked at the FutureNHS collaboration platform to see whether it would be suitable for integration with BloomReach. Whilst the platform in its current state would be a good candidate for integration, we discovered that at the time of writing, this platform is due to be rewritten.\\n","Decision":"We believe that Microsoft Office 365 and Sharepoint provide a good basis for the platforms document management capabilities and we have proved that it can be successfully integrated.\\nHaving looked at the requirements for LKS staff, particularly around the ability to support a range of document types such as spreadsheets and presentations, our belief is that we would be better placed to integrate an external document management system. The NHS Digital publishing platform provides an excellent HTML publishing model and workflow, however extending it to support a broader range of document types would be complex.\\nFutureNHS may provide a good candidate for integration in the future, however at the time of writing it is difficult to recommend as the product is in the process of being rewritten.\\n","tokens":204,"id":1195}
{"File Name":"cloud_controller_ng\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":1833}
{"File Name":"verify-service-provider\/0015-we-will-depend-on-a-minimal-set-of-verify-saml-libs.md","Context":"## Context\\nVerify have a number of pre-existing (closed source) libraries at various\\nlevels of abstraction for handling SAML behaviour. By depending on these\\nwe can benefit from all the work that's already been done making these easy\\nto use and secure by default.\\nThere's a plan in the medium term to open source a lot of these libraries.\\nBy depending on as few of them as possible we should be able to call the\\nverify-service-provider \"fully open\" sooner.\\nSome of the libraries at higher levels of abstraction are of questionable\\nvalue - although they make it easy to be consistent with the rest of verify\\nthey abstract away the exact nature of the SAML which makes the code hard to\\nread.\\n","Decision":"Good libraries that we should use for now:\\n* saml-serialisers\\n* ida-saml-extensions\\n* saml-security\\n* saml-metadata-bindings\\nLibraries we think we should ignore\\n* hub-saml\\n","tokens":151,"id":4451}
{"File Name":"news\/0009-on-testing-with-events.md","Context":"## Context\\nWe have had some flaky tests that are due to timing of events.\\nBecause the initial render is non-deterministic, firing test events may give the wrong results.\\n```js\\n\/\/ test\/integration\/web\/ui\/bookmarks-examples.js\\ndescribe(\"list items are added in response to 'bookmark-added' notification\", () => {\\nlet interactor, page, consoleMessages = null;\\nconst itemsSelector = 'div#application div#bookmarks .items';\\nbefore(async ()  =>\\n{\\ninteractor      = new WebInteractor({ headless: true });\\npage            = await interactor.page();\\nconsoleMessages = new ConsoleListener(page);\\n});\\nbeforeEach(async () => {\\nawait page.goto(`${baseUrl}?unplug=1&${feature}`, { waitUntil: 'domcontentloaded' });\\n});\\nafter(async () => await interactor.close());\\nit('for example', async () => {\\nawait page.once('load', async () => {\\nawait page.evaluate(() => {\\napplication.notify(\\n'bookmark-added',\\n{ id: 'id-1337', title: 'Title 1', url: 'http:\/\/abc\/def', source: '' }\\n);\\n});\\n});\\nawait page.waitForSelector(`${itemsSelector} li`);\\nconst listIds = await page.$$eval(`${itemsSelector} li`, items => items.map(it => ( it.id )));\\nexpect(listIds).to.eql([ 'bookmark-id-1337' ]);\\n});\\n});\\n```\\nHere if the `bookmark-added` notification is sent before the page has loaded, then the test will fail because\\n`div#application div#bookmarks .items` would have been overwritten.\\n","Decision":"Now trying introducing the idea that there is something in the html page called `view` that monitors load events:\\n```js\\nclass UIEvents {\\nconstructor(opts = { }) {\\nopts = { idlePeriod: 200, ...opts }\\nthis._idlePeriod = opts.idlePeriod;\\nthis._lastRender = this.now();\\n}\\nnotifyRendering() {\\nthis._lastRender = this.now();\\n}\\nasync waitUntilIdle(opts = { }) {\\nopts = { timeout: 500, ...opts };\\nconst startTime = new Date();\\nconst timedOut = () => new Date() - startTime >= opts.timeout;\\nconst delay = ms => new Promise(res => setTimeout(res, ms));\\nwhile(false == this.isIdle()) {\\nif (timedOut())\\nthrow new Error(`Timed out after <${opts.timeout}ms>`);\\nawait delay(10);\\n}\\n}\\nisIdle() {\\nreturn (this.now() - this._lastRender) > this._idlePeriod;\\n}\\nnow() { return new Date(); }\\n}\\nmodule.exports = { UIEvents }\\n```\\nThis *does* require views to add notifications like:\\n```js\\nimport { afterUpdate } from 'svelte';\\nafterUpdate(() => {\\nwindow.view.notifyRendering();\\n});\\n```\\n@todo: I guess we should add tests for that: make sure each view type raise these.\\n### Attempt 1\\nTried using [`page.once`](https:\/\/github.com\/puppeteer\/puppeteer\/blob\/main\/src\/common\/Page.ts), which is supposed to be a callback for pad load.\\nIt did work for a start but only by fluke I think.\\n### Decided against\\n* forcing views to implement a finished notification\\n* changing `Application` to notify when finished\\n","tokens":377,"id":4325}
{"File Name":"opg-refunds\/0002-default-service-key-for-dynamo-db-sessions.md","Context":"## Context\\nEncryption keys for frontend user sessions are provided in environment variables and cycled during every release.\\nThis has resulted in one incident of losing syncronisation, causing errors for users when services scale up and then scale down.\\n","Decision":"We will use a AWS owned Customer Master Key for the Sessions Dynamo DB tables to encrypt session tokens, and not push encryption keys into containers.\\nTable names                                       |\\n--------------------------------------------------|\\nrefunds-sessions-front-<opg_stackname>            |\\nrefunds-sessions-caseworker-front-<opg_stackname> |\\n","tokens":48,"id":295}
{"File Name":"disco-poc-vue\/0003-use-vuejs-3.md","Context":"## Context\\nWe reviewed various options for building our front end and decided Vue.js was\\nthe best fit. However, Vue is preparing for a major version upgrade. We could\\nchoose to use the current stable version (2) and migrate to the new version (3)\\nwhen it is released or start with the release candidates for the new version.\\n","Decision":"We will use Vue.js 3 releases candidates.\\n","tokens":72,"id":164}
{"File Name":"jabref\/0005-fully-support-utf8-only-for-latex-files.md","Context":"## Context and Problem Statement\\nThe feature [search for citations](https:\/\/github.com\/JabRef\/user-documentation\/issues\/210) displays the content of LaTeX files. The LaTeX files are text files and might be encoded arbitrarily.\\n","Decision":"Chosen option: \"Support UTF-8 encoding only\", because comes out best \\(see below\\).\\n### Positive Consequences\\n* All content of LaTeX files are displayed in JabRef\\n### Negative Consequences\\n* When a LaTeX files is encoded in another encoding, the user might see strange characters in JabRef\\n","tokens":47,"id":4749}
{"File Name":"architecture-decision-log\/0004-layered-architecture.md","Context":"* [Context](#context)\\n* [Decision](#decision)\\n* [Status](#status)\\n* [Consequences](#consequences)\\n","Decision":"* [Status](#status)\\n* [Consequences](#consequences)\\nWe are going to use Layered Architecture to help us solve those problems. In a nutshell, Layered Architecture breaks our system into four layers:\\n1. **Presentation:** This layer is responsible for showing the user all the elements for interaction.\\n2. **Application:** This layer is responsible for handling user interactions and processing them accordingly.\\n3. **Domain:** This layer is accountable for your core business. It contains all business logic to control your entities and required resources.\\n4. **Infrastructure:** As the name suggests, this layer is responsible for handling your infrastructure: interacting with databases, handling memory, and so forth.\\nThis pattern is like a Russian doll. If you drill-down inside a microservice from the presentation layer, it can have their Presentation, Application, Domain, and Infrastructure likewise.\\nA layer can also only talk with the layer below it (except for the Infrastructure layer). For example, the Presentation layer can only interact with the Application layer and the Infrastructure layer. And so forth.\\n![Diagram explaining the Layered Architecture hierarchy](..\/assets\/0004-layered-architecture\/hierarchy-diagram.png)\\n","tokens":34,"id":4906}
{"File Name":"documents-api\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":3988}
{"File Name":"openlobby-server\/0009-openid.md","Context":"## Context\\nWe need an authentication mechanism for users. It must be secure and\\nfrontend application independent.\\n","Decision":"We will use OpenID Connect. Open Lobby Server will provide all the hard stuff\\nfor a frontend applications. Ideally over the GraphQL API.\\n","tokens":22,"id":478}
{"File Name":"docs\/0025-generic-component.md","Context":"## Context and Problem Statement\\nWe want to realize the following user story: As a user, I want to connect two components of a distributed app. Both components write\/listen to different topics and\\nuse a different message format.\\nTo realize this, a message router and a message transformer ([EAI Pattern](https:\/\/www.enterpriseintegrationpatterns.com\/)) is needed as depicted in the following sketch.\\n![First scenario](images\/FirstScenario.png)\\nThe transformation and the routing logic is provided via functions which are hosted on a FaaS solution. This functions are user specific and could be provided via a function store. The transformer doesn't need to know how to transform\/split\/enrich\/filter the messages. It simply reads the message from a specific topic, hands it over to the function and sends the (transformed) result of the function to an output topic. Likewise, the router does not need to know how to route the messages (content or context based, e.g. if `itemPrice>50000` than route to high-value department). It reads the message from a topic, hands it over to a function and sends the message to a function determined topic. Both components have in common that they only read messages from a topic, hand the message to a function and act upon the result. We could draw the image above like this:\\n![generic component sketch](images\/genericComponent.png)\\n","Decision":"Chosen option: Kafka-faas-connector, because we don't want to mix messaging and business logic in the functions and need to have dynamic topics which are not supported by the OpenFaaS Kafka connector at the moment.\\n","tokens":285,"id":4671}
{"File Name":"fxa\/0003-event-broker-for-subscription-platform.md","Context":"## Context and Problem Statement\\nExternal OAuth Relying Parties (RPs) that Firefox Account users authenticate\\nto need to be kept up to date about whether the user still has an active\\nsubscription as well as knowing when to delete the user data. For internal\\nMozilla use, Mozilla provides [Firefox Service Notifications][] over SQS\\nqueues. These events are intended for internal trusted use and not suitable\\nfor external RPs.\\nA solution for these concerns is a new system referred to as the FxA Event\\nBroker which will store FxA related events and distribute them via webhooks\\nto relevant RPs that the user has accessed. The initial version described here applies primarily to external RPs which will only receive subscription status\\nchanges and account deletion events. This solution is being built with future\\nexpansion in mind which will require event stream storage per user and\\nnotification of internal Mozilla RPs.\\nDifficulties inherent in this solution lie in where to store the source of\\ntruth for what RPs a user has authenticated to, where the delivery functionality\\nshould reside, where the event streams for a user should reside, what set of\\ndata should be included in [Firefox Service Notifications][], and how this\\ndata should be communicated to the delivery system to avoid additional API\\nquery load on existing FxA services.\\n## Decision Drivers\\n- Subscription services deadlines\\n- Effort required and experience available for FxA changes\\n- Separation of concerns from existing FxA microservices\\n- Difficulty of schema migrations in existing FxA microservices\\n- Suitability of existing FxA databases for large-scale event storage\\n- Architectural desire to treat FxA Auth and OAuth as one (Merging\\nin-progress)\\n","Decision":"- Subscription services deadlines\\n- Effort required and experience available for FxA changes\\n- Separation of concerns from existing FxA microservices\\n- Difficulty of schema migrations in existing FxA microservices\\n- Suitability of existing FxA databases for large-scale event storage\\n- Architectural desire to treat FxA Auth and OAuth as one (Merging\\nin-progress)\\nChosen Option: C. Implementing RP notification as a new FxA Event Broker service\\nwith webhook and login activity stored in FxA Event Broker, because\\n- Less subscription platform timeline risk to store new data in new database vs.\\nmodify existing FxA OAuth database.\\n- Storing events at scale has database requirements that don't fit in well with\\nthe limitations with MySQL encountered in FxA Auth\/OAuth.\\n- Having FxA Auth be the only store of which RP to notify would require each\\nnotification to also include what RPs to notify, increasing the load on the\\nFxA Auth database.\\n","tokens":352,"id":363}
{"File Name":"uniprot-website\/0002-frontend-code.md","Context":"## Context\\nThe front end needs to provide a rich user experience, be modular and performant with large community support.\\n","Decision":"React.js is used as the frontend framework for building the new website. React.js is used with TypeScript, which is a superset of JavaScript.\\n","tokens":25,"id":2011}
{"File Name":"community\/dr-002-Traefik_as_an_API_Gateway.md","Context":"## Context\\nOne of the core components in Kyma is the API Gateway. This DR focuses on [Tr\u00e6fik](https:\/\/traefik.io), one of the candidates for an API Gateway.\\nTr\u00e6fik is an HTTP reverse proxy shipped with load-balancing, rate-limiting, monitoring, and several other useful features. In comparison to the other reverse proxies, Traefik stands out with its dynamic configuration abilities.\\nTr\u00e6fik supports the most common orchestration backends (for example: Docker, Kubernetes, Mesos, or Consul). It is written in Go. The evaluated version of Traefik is 1.4.0 [rc4].\\n","Decision":"During our evaluation process, it turned out that Traefik does not support one of the key features required in Kyma - JWT based authentication. Throughout the implementation, the lack of extensibility capabilities was also noticed. There is absolutely no notion of custom middlewares so introduction of additional features (like JWT-based authentication middleware) requires direct changes in the source code.\\nAdvantages:\\n- few dependencies\\n- dynamic configuration\\n- Kubernetes-based orchestration\\n- built in circuit breaker\\n- rate limiter\\nDisadvantages:\\n- poor extensibility\\nAlthough Traefik has a lot of interesting features, due to the poor extensibility capabilities, the decision is to not use it as an API Gateway in Kyma.\\n","tokens":144,"id":3464}
{"File Name":"cdk\/004-testing.md","Context":"## Context\\n<!--- What is the issue that we're seeing that is motivating this decision or change? -->\\nOne of the benefits of writing our infrastructure in a fully fledged programming language is the ability to write reusable components which can be tested. This library defines those components, both in the form of constructs and patterns (see [001-constructs-and-patterns](.\/001-constructs-and-patterns.md) for more details).\\nThere are two main strategies that can be used to unit test these components.\\nSnapshot tests synthesise a given stack and compare the output to a previous version that has been verified by the user and checked into the repository. If there are any changes, the test fails and the user is displayed the diff to either fix or update the stored snapshot.\\n[+] Quick and easy to write\\n[+] Pick up any changes to the output cloudformation (particularly useful for unintended side effects)\\n[-] A change to the component may cause many unrelated tests to also fail\\n[-] When testing different permutations of a component, a number of snapshots will be created, each of which contains a certain amount of information which is irrelevant to the particular test. This adds some extra effort to understand what is relevant to the test which you're looking at\\n[-] Snapshots are easy to update which, especially when multiple are affected by a change, makes it easy to accidentally update them incorrectly. Further to this, as the snapshot (which is essentially the assertion) is stored in a different file, it's not immediately obvious if the assertions are valid\\nDirect Assertions use the different assertions provided by the test framework to test specific functionality. For example, asserting that a property exists or that an array contains a particular value.\\n[+] Each test only contains the relevant assertions for the test, making it easier to understand the consequence of settings certain props\\n[+] Changes will only fail tests that cover that particular area\\n[-] More complex and time consuming to write\\n","Decision":"<!-- What is the change that we're proposing and\/or doing? -->\\nUse direct assertions for constructs and snapshots for patterns (and stacks)\\n_This decision is a recommendation for the general approach. There may be some cases where using a different approach is more applicable for a given test._\\n","tokens":401,"id":1192}
{"File Name":"Sylius\/2020_03_03_api_translations.md","Context":"### Problem to solve\\nProvide unified way to manage translations for translatable entities in API.\\n### Possible solutions\\nThe translations from collection could be embedded as objects within a `ProductOption`\/`ProductOptionValue` resource\\n**or** provided by [IRIs](https:\/\/en.wikipedia.org\/wiki\/Internationalized_Resource_Identifier)\\n### Decision and reasoning\\nTranslations should always be embedded as the collection of objects. They're irrelevant outside of the main object and\\ndo not provide any value alone.\\n","Decision":"Translations should always be embedded as the collection of objects. They're irrelevant outside of the main object and\\ndo not provide any value alone.\\n","tokens":103,"id":673}
{"File Name":"branchout\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":1473}
{"File Name":"content-publisher\/0007-minimal-model.md","Context":"## Context\\nWe currently have several classes that we think of as models.\\n* ActiveRecord classes that reflect persisted data in the database. These are conventionally found in `app\/models`.\\n* Example: `Document`\\n* Plain Ruby classes that reflect readonly data stored in configuration files. These currently have no place to live :-(.\\n* Example: `DocumentType`\\nWe also have other classes that resemble models, but aren't e.g.\\n* `DocumentUrl` is not a class we think of as a model e.g. because it has no public attributes\\n* `UserFacingState` is not a class we think of as a model e.g. because its instances have no unique ID\\nWe would like to make it clear which classes belong in `app\/models`.\\n","Decision":"We should treat a class as a model if it has the following properties.\\n* **The name of the class is a noun that represents a domain concept**\\n* Example: a `Document` represents all revisions of a piece of content, etc.\\n* Guideline: it should be difficult to rename the class to a verb\\n* Guideline: the class should be documented in the README nomenclature\\n* **Instances of the class have multiple attributes that encapsulate it**\\n* Example: a `DocumentType` encapsulates publishing metadata, tags, etc.\\n* Guideline: The attributes should be public and required in our code\\n* **Instances of the class should have a unique identifier attribute**\\n* Example: each `Document` has a unique `id` assigned by the database\\n* Example: each `DocumentType` has a unique `id` in its config file\\n* **Instances of the class support mass-assignment of their attributes**\\n* Example: a `Document` supports assignment as an ActiveRecord model\\n* Example: a `DocumentType` supports assignment on initialize (readonly)\\n","tokens":163,"id":4873}
{"File Name":"raster-foundry\/adr-0010-domain-layout.md","Context":"## Context\\nRaster Foundry's web presence needs a landing page for customer acquisition and a blog for curating content relevant to the project's objectives. Given that we are still early in the product development process, there is a desire to keep solutions to the landing page and blog simple, but also flexible so that they can easily be iterated on by all parties involved.\\n","Decision":"Building upon the experience gained by maintaining products like Cicero, HunchLab, and OpenTreeMap, the following decisions have been made regarding Raster Foundry\u2019s domain layout:\\n- Canonical domain is `www.rasterfoundry.com`\\n- Web application resides on `app.rasterfoundry.com`\\n- Blog resides on `blog.rasterfoundry.com`\\nAs far as the platforms that drive the landing page and blog, the former will be a static website built using [Jekyll](https:\/\/jekyllrb.com\/) and hosted via [GitHub Pages](https:\/\/pages.github.com\/) (with support from Amazon CloudFront for HTTPS) and the latter will be based on [Medium](https:\/\/medium.com\/).\\n","tokens":75,"id":1692}
{"File Name":"dos-server\/adr-10-aws-fargate.md","Context":"## Context\\nInfrastructure is needed to run Docker containers (the current choice of deployment packaging). As we are apparently using Amazon Fargate for running containers of other applications, the Dockerized application\\ncan be deployed to Amazon Fargate (until a replacement choice for running Docker containers is made by\\nthe infrastructure team.\\n","Decision":"Fargate platform will be used.\\n","tokens":66,"id":3376}
{"File Name":"Maud\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":249}
{"File Name":"mental-health-service-finder\/0005-use-local-data-for-tower-hamlets-ccg.md","Context":"## Context\\nTower Hamlets CCG's IAPT service does not have an ODS code. As a consequence it\\nis not possible to relate the CCG to the IAPT service. When a GP is providing\\nservices for Tower Hamlets CCG this will allow data to be displayed.\\n","Decision":"Rather than display no result we know the contact information for the IAPT\\nservice. We have decided to store this information locally and display it for\\nthe user.\\nThis is only a temporary measure and the change will be reverted once the data\\nhas been loaded into the central system.\\n","tokens":61,"id":4576}
{"File Name":"beis-report-official-development-assistance\/0008-use-heroku-for-hosting.md","Context":"## Context\\nDuring the set up phase of the beta we investigated what the long term hosting should be. This was agreed to be Government's Platform as a Service (GPaaS) as it was the strategic platform all other digital services within BEIS were moving to.\\nWe have been unable to get access to the BEIS GPaaS account to set up the platform.\\nWe value having a real service hosted as soon as possible and would like to have this done in the first few sprints so the team have a live product to iterate.\\n","Decision":"Use dxw's Heroku account to host a staging and production environment and migrate the service to GPaaS later.\\n","tokens":110,"id":2401}
{"File Name":"smjs\/2018121501-monorepo-and-directory-layout.md","Context":"## Context\\nFollowing from decision [2018120802](2018120802-implement-clean-architecture.md), we want to follow the tenet of [screaming architecture](https:\/\/blog.cleancoder.com\/uncle-bob\/2011\/09\/30\/Screaming-Architecture.html), stating that the first level directories of the application's source tree should represent the use cases, or features, of the application.\\nHowever, we cannot define domain modules inside application directories, because the domain cannot be mapped 1-to-1 to application features, since the domain is independent from the application. In practical terms, this means that it could happen that two different features will need the same domain module, and if this domain module is defined in feature `A`, and feature `B` requires those domain concepts as well, then feature `B` will have to know about feature `A`, even if this makes no sense as far as the semantic of those features are concerned.\\nAdditionally, we cannot define adapters, or \"infrastructure\" code, inside feature directories either, because application cannot know about adapters, and very often an adapter targets multiple features, like a Web adapter can target different features of a primary port, and even a secondary adapter could be the target of different features, if it acts like a mediator, for example with commands or events.\\nWe're currently using only one source code repository, and we don't want to go through the hassle of maintaining multiple repositories yet, because the project structure is still being actively changed. On top of that, we want to support a plugin architecture, where multiple different kinds of applications can be built by assembling together plugins. This is exactly the purpose of Clean Architecture, with maybe the only difference that the typical example of Clean Architecture is a Web application where there's only one application supporting different features, while we'll have different applications that can possibly be built from the same components: this, however, is just a deploying detail.\\n","Decision":"To avoid dealing with multiple source code repositories, we'll use the \"monorepo\" approach, meaning keeping everything inside a single source code repository.\\nAs far as the directory structure is concerned, this project is a bit unusual, since many different applications we'll be built from it, by assembling different plugins, instead of having only one possible application. This means that we cannot really display the application use cases at the first level of the directory tree, because there all applications and plugins will be located instead:\\n```\\nsloth-machine-framework\\ndomain\\nsmf\\nsloth-machine-architecture\\ndomain\\nsma\\nvirtual-machine\\napplication\\nvm\\narchitecture-loader\\napplication\\narcl\\nprogram-loader\\napplication\\npl\\nlocal-architecture-loader\\nadapters\\nlarcl\\nfile-program-loader\\nadapters\\nfpl\\nsloth-machine\\nadapters\\nsm\\n-> sloth-machine-framework\\n-> sloth-machine-architecture\\n-> virtual-machine\\n-> architecture-loader\\n-> program-loader\\n-> local-architecture-loader\\n-> file-program-loader\\n...\\n```\\nWe can argue that this directory layout is still quite \"screaming\", because it's displaying the features of our application, or we should better say \"toolkit\" since there are multiple applications: virtual machine, assemblers, compilers, debuggers, etc. This should state very clearly what's the purpose of these systems.\\nIt's interesting to point out, additionally, that here we have two different kinds of plugins: to the first one belong the plugins that are added during development of the application, because they're necessary for its structure, like `sloth_machine_core`, or all the `feature` ones; the second kind of plugins, instead, contains all the \"mods\" that can be added to an already deployed application, for example dropping them into some specific directory, to enhance the functionality of the virtual machine, without them being necessary for its structure. For example, the virtual machine supports many different architecture, and the end user, having a virtual machine application installed on his system, could add a new architecture by dropping an appropriate plugin archive to the correct folder, after which the application will pick up the new architecture at startup. This is handy because we don't want to require building and releasing a new version of the application only to add a new architecture, or a new supported assembler or compiler.\\n### References\\n- https:\/\/blog.cleancoder.com\/uncle-bob\/2012\/08\/13\/the-clean-architecture.html\\n- https:\/\/blog.cleancoder.com\/uncle-bob\/2011\/09\/30\/Screaming-Architecture.html\\n- https:\/\/trunkbaseddevelopment.com\/monorepos\/\\n- http:\/\/www.plainionist.net\/Implementing-Clean-Architecture-Scream\/\\n- https:\/\/news.ycombinator.com\/item?id=18808909\\n- https:\/\/medium.com\/@mattklein123\/monorepos-please-dont-e9a279be011b\\n- https:\/\/medium.com\/@adamhjk\/monorepo-please-do-3657e08a4b70\\n","tokens":404,"id":3980}
{"File Name":"ReportMI-service-manual\/0027-use-docker-in-development.md","Context":"## Context\\nThe issue motivating this decision, and any context that influences or constrains the decision.\\n- onboarding a fresh team has consumed a lot of time and we want to improve this for the future as this service moves from a build phase into the support phase.\\n- 2 ways to start and run the application were documented, depending on your preference you would use the ones you were most comfortable with. This led to docker documentation that fell behind and didn't work out of the box\\n- when supported by the dxw support team, a different developer will be on hand each week to fix issues. In order to effectively and confidently apply fixes we want to standardise and test the set up process, removing as much manual process as possible and allowing the developer to address problems quickly.\\n- in ADR 26 we discussed moving from AWS ECS (containers) to GOV.UK PaaS (no containers). Whilst we no longer use containers in production and there aren't the same advantages to parity we would normally fine, we believe there is still good value to using it to make consistent development environments.\\n","Decision":"We are going to use Docker in development exclusively for the frontend and the API.\\n","tokens":226,"id":2057}
{"File Name":"paas-team-manual\/ADR043-new-product-pages-language-and-framework.html.md","Context":"## Context\\n[PaaS product pages] have been reviewed and a number of accessibility issues\\nhave been identified. To resolve those we, would need to make an upgrade and\\nand review if any additional changes are needed to align with the GOV.UK Design System.\\nAs those pages are built in Ruby and in [ADR024] we've made the decision\\nto develop our user-facing applications on Node, it's a good opportunity to\\nlook at rebuilding the product pages.\\nWe've discussed user needs and it emerged that:\\n* anyone in the team (developer and non-developer) should be able to update pages\\nwith less effort\\n* pages should be performant for end users\\n* pages should be rendered by the server\\n* keeping pages up to date with GOV.UK Design System releases should be quicker and easier\\n* alignment of technologies for our user-facing web products should provide better\\ndeveloper experience and give us the option to have shared component libraries\\nWith the above in mind we researched options. Our admin interface is built in React,\\nso we narrowed the scope to React-based static site generators.\\nWe ended up comparing two: [NextJS] with static page export and [GatsbyJS]\\nwhich exports static pages by default.\\nFor page content we agreed that writing pages in [Markdown] is a good option,\\nso we tested both with [MDX] which can also embed React components inside content pages.\\n[NextJS] and [GatsbyJS] have different approaches to development and there are minor\\nperformance differences between them.\\nOur use case for now is narrow enough, and with the primary need of anyone in the team\\nbeing able to update pages, [NextJS] marginally gets more votes as Gatsby cannot be installed and run on\\nnon-developer machines.\\n","Decision":"We will use [NextJS] together with [MDX] to author PaaS product pages content in\\n[Markdown] and deliver them to users as static pages.\\n","tokens":375,"id":177}
{"File Name":"offender-management-architecture-decisions\/0006-use-the-custody-api-to-access-nomis-data.md","Context":"## Context\\nOur main source of data on prisoners and prison staff which we need for\\nallocations is NOMIS.\\nThere are now four APIs into NOMIS providing general data access, with varying\\napproaches to presenting the data and authentication. We do not want to add to\\nthis duplication.\\nThe APIs which have been developed more recently are more under the control of\\nHMPPS than the earlier ones (which were developed by a supplier). That gives us\\nmore flexibility around how we work with them and makes it possible to get\\nchanges into production more quickly. Using one of the newer APIs should mean\\nthat we are less blocked by delays around API changes than we have been on our\\nwork on Visit someone in prison.\\nIt has been agreed by the HMPPS technical community that we would like to move\\nall clients to use the Custody API in preference to the other APIs over time.\\nAlthough that work has not yet been prioritised, using the Custody API for new\\napplications will reduce the work needed in future to align our API usage.\\nThe Custody API has been designed to give a more direct view of the data in\\nNOMIS than the previous APIs have been - earlier approaches have favoured\\nimplementing specific endpoints to meet the needs of service teams rather than\\ngiving a more comprehensive view of all the data.\\n","Decision":"We will use the Custody API to access the NOMIS data we need.\\nWe will work with the team in Sheffield on development of the Custody API to\\nadd support for accessing the data we need.\\n","tokens":277,"id":267}
{"File Name":"gatemint-sdk\/adr-022-custom-panic-handling.md","Context":"## Context\\nThe current implementation of BaseApp does not allow developers to write custom error handlers during panic recovery\\n[runTx()](https:\/\/github.com\/cosmos\/cosmos-sdk\/blob\/bad4ca75f58b182f600396ca350ad844c18fc80b\/baseapp\/baseapp.go#L539)\\nmethod. We think that this method can be more flexible and can give SDK users more options for customizations without\\nthe need to rewrite whole BaseApp. Also there's one special case for `sdk.ErrorOutOfGas` error handling, that case\\nmight be handled in a \"standard\" way (middleware) alongside the others.\\nWe propose middleware-solution, which could help developers implement the following cases:\\n* add external logging (let's say sending reports to external services like [Sentry](https:\/\/sentry.io));\\n* call panic for specific error cases;\\nIt will also make `OutOfGas` case and `default` case one of the middlewares.\\n`Default` case wraps recovery object to an error and logs it ([example middleware implementation](#Recovery-middleware)).\\nOur project has a sidecar service running alongside the blockchain node (smart contracts virtual machine). It is\\nessential that node <-> sidecar connectivity stays stable for TXs processing. So when the communication breaks we need\\nto crash the node and reboot it once the problem is solved. That behaviour makes node's state machine execution\\ndeterministic. As all keeper panics are caught by runTx's `defer()` handler, we have to adjust the BaseApp code\\nin order to customize it.\\n","Decision":"### Design\\n#### Overview\\nInstead of hardcoding custom error handling into BaseApp we suggest using set of middlewares which can be customized\\nexternally and will allow developers use as many custom error handlers as they want. Implementation with tests\\ncan be found [here](https:\/\/github.com\/cosmos\/cosmos-sdk\/pull\/6053).\\n#### Implementation details\\n##### Recovery handler\\nNew `RecoveryHandler` type added. `recoveryObj` input argument is an object returned by the standard Go function\\n`recover()` from the `builtin` package.\\n```go\\ntype RecoveryHandler func(recoveryObj interface{}) error\\n```\\nHandler should type assert (or other methods) an object to define if object should be handled.\\n`nil` should be returned if input object can't be handled by that `RecoveryHandler` (not a handler's target type).\\nNot `nil` error should be returned if input object was handled and middleware chain execution should be stopped.\\nAn example:\\n```go\\nfunc exampleErrHandler(recoveryObj interface{}) error {\\nerr, ok := recoveryObj.(error)\\nif !ok { return nil }\\nif someSpecificError.Is(err) {\\npanic(customPanicMsg)\\n} else {\\nreturn nil\\n}\\n}\\n```\\nThis example breaks the application execution, but it also might enrich the error's context like the `OutOfGas` handler.\\n##### Recovery middleware\\nWe also add a middleware type (decorator). That function type wraps `RecoveryHandler` and returns the next middleware in\\nexecution chain and handler's `error`. Type is used to separate actual `recovery()` object handling from middleware\\nchain processing.\\n```go\\ntype recoveryMiddleware func(recoveryObj interface{}) (recoveryMiddleware, error)\\nfunc newRecoveryMiddleware(handler RecoveryHandler, next recoveryMiddleware) recoveryMiddleware {\\nreturn func(recoveryObj interface{}) (recoveryMiddleware, error) {\\nif err := handler(recoveryObj); err != nil {\\nreturn nil, err\\n}\\nreturn next, nil\\n}\\n}\\n```\\nFunction receives a `recoveryObj` object and returns:\\n* (next `recoveryMiddleware`, `nil`) if object wasn't handled (not a target type) by `RecoveryHandler`;\\n* (`nil`, not nil `error`) if input object was handled and other middlewares in the chain should not be executed;\\n* (`nil`, `nil`) in case of invalid behavior. Panic recovery might not have been properly handled;\\nthis can be avoided by always using a `default` as a rightmost middleware in the chain (always returns an `error`');\\n`OutOfGas` middleware example:\\n```go\\nfunc newOutOfGasRecoveryMiddleware(gasWanted uint64, ctx sdk.Context, next recoveryMiddleware) recoveryMiddleware {\\nhandler := func(recoveryObj interface{}) error {\\nerr, ok := recoveryObj.(sdk.ErrorOutOfGas)\\nif !ok { return nil }\\nreturn sdkerrors.Wrap(\\nsdkerrors.ErrOutOfGas, fmt.Sprintf(\\n\"out of gas in location: %v; gasWanted: %d, gasUsed: %d\", err.Descriptor, gasWanted, ctx.GasMeter().GasConsumed(),\\n),\\n)\\n}\\nreturn newRecoveryMiddleware(handler, next)\\n}\\n```\\n`Default` middleware example:\\n```go\\nfunc newDefaultRecoveryMiddleware() recoveryMiddleware {\\nhandler := func(recoveryObj interface{}) error {\\nreturn sdkerrors.Wrap(\\nsdkerrors.ErrPanic, fmt.Sprintf(\"recovered: %v\\nstack:\\n%v\", recoveryObj, string(debug.Stack())),\\n)\\n}\\nreturn newRecoveryMiddleware(handler, nil)\\n}\\n```\\n##### Recovery processing\\nBasic chain of middlewares processing would look like:\\n```go\\nfunc processRecovery(recoveryObj interface{}, middleware recoveryMiddleware) error {\\nif middleware == nil { return nil }\\nnext, err := middleware(recoveryObj)\\nif err != nil { return err }\\nif next == nil { return nil }\\nreturn processRecovery(recoveryObj, next)\\n}\\n```\\nThat way we can create a middleware chain which is executed from left to right, the rightmost middleware is a\\n`default` handler which must return an `error`.\\n##### BaseApp changes\\nThe `default` middleware chain must exist in a `BaseApp` object. `Baseapp` modifications:\\n```go\\ntype BaseApp struct {\\n\/\/ ...\\nrunTxRecoveryMiddleware recoveryMiddleware\\n}\\nfunc NewBaseApp(...) {\\n\/\/ ...\\napp.runTxRecoveryMiddleware = newDefaultRecoveryMiddleware()\\n}\\nfunc (app *BaseApp) runTx(...) {\\n\/\/ ...\\ndefer func() {\\nif r := recover(); r != nil {\\nrecoveryMW := newOutOfGasRecoveryMiddleware(gasWanted, ctx, app.runTxRecoveryMiddleware)\\nerr, result = processRecovery(r, recoveryMW), nil\\n}\\ngInfo = sdk.GasInfo{GasWanted: gasWanted, GasUsed: ctx.GasMeter().GasConsumed()}\\n}()\\n\/\/ ...\\n}\\n```\\nDevelopers can add their custom `RecoveryHandler`s by providing `AddRunTxRecoveryHandler` as a BaseApp option parameter to the `NewBaseapp` constructor:\\n```go\\nfunc (app *BaseApp) AddRunTxRecoveryHandler(handlers ...RecoveryHandler) {\\nfor _, h := range handlers {\\napp.runTxRecoveryMiddleware = newRecoveryMiddleware(h, app.runTxRecoveryMiddleware)\\n}\\n}\\n```\\nThis method would prepend handlers to an existing chain.\\n","tokens":337,"id":34}
{"File Name":"govuk-kubernetes-discovery\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":2797}
{"File Name":"adr\/ADR-4-collections-architecture-in-L1-L2.md","Context":"## Context and Problem Statement\\nCollections will be key in the Decentraland ecosystem. To bring more scalability and reduce costs the L2 will be seen as the main chain, and L1, as the gateway for collectors to use\/buy\/sell tokens in other platforms.\\nL1 could be seen as Ethereum mainnet and L2 any other sidechain EVM compatible as Matic or xDai.\\nEach collection follows the ERC721 standard. Every token transferred between layers will keep a reference of itself in the main chain:\\n- Collection address in L2. E.g: `0x2dac71c8c8a4b9547b53c1e1838152ca3277ce76`\\n- Token Id in L2. E.g: `1`\\n- Token URI in L2 (URI used for Decentraland and other platforms to know how the token looks like). E.g: `https:\/\/peer.decentraland.org\/lambdas\/collections\/standard\/erc721\/0x2dac71c8c8a4b9547b53c1e1838152ca3277ce76\/0`\\nThis document presents 2 alternatives on how to manage the collections between L1 & L2.\\n","Decision":"#### Alternative 1\\n#### Pros\\n- N\/A\\n#### Cons\\n- Moving the first token from a collection to L1, involves creating a contract using the factory (gas cost).\\n#### Implications\\n- Keep the same address and token ids between layers.\\n- Users will need to send a transaction for each collection to authorize the usage of tokens.\\n- Third party marketplaces (OpenSea) will not know without any effort if the contract is an official Decentraland collection in L1.\\n#### Alternative 2 \u2705\\n##### Pros\\n- Users will need to send only one transaction per marketplace to authorize the usage of tokens.\\n- We can use the unique collection as a valid and the official one for Decentraland in L1.\\n- Automatic and instant availability in marketplaces with the whitelisted collection.\\n- Fewer moving parts\\n##### Cons\\n- N\/A\\n##### Implications\\n- Collection addresses and token Ids will not be the same in L2 and L1.\\n# Open questions\\n- **Do the token URI changes in L2?**\\nToken URI of L2 is stored in the main collection in L1\\n```yaml\\nOpenSea (mainnet): opensea.com\/mainnet\/tokens\/{collection_l1}\/{hash(collection_l1_1, token)}\\nOpenSea (L2): opensea.com\/matic\/tokens\/{collection_l2_1}\/{token}\\n```\\n```python\\n# L2\\ncollection_l2_1.tokenURI(token_1) = https:\/\/peer.decentraland.org\/collection_l2_1\/token_1\\nmapping(tokenId => string) tokenURI\\n# L1\\ncollection_l1.tokenURI(hash(collection_l2_1, token_1)) = https:\/\/peer.decentraland.org\/collection_l2_1\/token_1\\nmapping(tokenId => string) tokenURI\\n```\\n- **Alt 1. Is it necessary to burn the token when L1->L2?**\\nIt could be not done. It is a common practice.\\n- **Alt 1. We could have the same collection address in L2 and L1**\\nThat was the idea.\\n- **How do we validate allowed\/denied items when moving from L2 to L1?**\\nValidate if the collection was created by factory.\\nTBD.\\n- **How do we prevent abuse on meta-transactions paid by DCL?**\\n.\\n- **What do we do if Matic is no _longer viable_ or _moving_ to another chain?**\\nWe can restore the state from a blockchain snapshot and copy the contracts.\\nUsers may need to manually send the tokens as a result of the migration.\\nThere are coordination risks with third parties (open sea selling items in the middle of the migration).\\nEnforce pause mechanisms? Upgradable contracts?\\n- **Who pays the tx fees when L2->L1?**\\nAs it is today, the user pays the TX.\\n- **How does the approval flow affect any of the options in L2?**\\n> If DAO is in L2 and it is cheap it would be good to vote everything.\\n_Reviewers will have to review several orders of magnitude of collections in L2_\\n# Participants\\n- Esteban Ordano\\n- Ignacio Mazzara\\n- Marcelo Alaniz\\n- Nicolas Santangelo\\n- Juan Cazala\\n- Agustin Mendez\\nDate: 2020-10-13\\n","tokens":260,"id":4627}
{"File Name":"openmrs-module-gpconnect\/0001-add-additional-data-to-resource.md","Context":"## Context\\nDeprecated design:\\n- For each resource, we extended the existing resource by creating a new table in the OpenMRS database called nhs_{resource} (e.g. nhs_patient and nhs_practitioner).\\nThe problem:\\n- When we identified a one-to-many relationship between a resource and a data field (between the Practitioner and the SDS Role Profile Id field), the current solution would have required making further changes to the OpenMRS model.\\n- These changes would have included creating a new table to hold instances of the SDS Role Profile Id field.\\n- However, this would have made the Practitioner resource more complex.\\*\\n\\* For context, a Practitioner resource in FHIR is equivalent to a provider in OpenMRS.\\n","Decision":"- We decided to use the existing OpenMRS model for creating attributes for each resource.\\n- There is a one-to-many relationship between a resource and its attributes.\\n- Each attribute has an attribute type, which is stored in the attribute_type table.\\n- For each new piece of data to be added to a resource:\\n- Create a new attribute_type that describes the data type - this should be included in the seed data.\\n- Create a new attribute with the attribute_type that you just created and link this new attribute to your resource.\\n- Update the translator for the resource (from FHIR to OpenMRS and vice versa) to ensure that the new piece of data is populated.\\n- Please see [the addition of the SDS Role Profile Id field to the Practitioner resource](https:\/\/github.com\/Open-GP\/openmrs-module-gpconnect\/commit\/f0ff30c18f0d386e338d87f22a847b0dc3dff5ff) to see an implementation of the following pattern:\\n![Provider Attribute Design](..\/images\/ProviderAttributeDesign2.png)\\n","tokens":159,"id":781}
{"File Name":"mongodb-updater\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":1128}
{"File Name":"dev\/00001-dot-prefix-for-configuration.md","Context":"## Context\\n1. The current implementation goes against a convention where global configuration files are prefixed with a period\\n2. I want to be able to do writebacks to the configuration, an exact example would be `dev add repo ${REPO_URL}`, and the current implementation makes it messy because the configuration files can be anywhere.\\n","Decision":"I have made the decision for configuration files to look like: `\\.dev(\\.[a-zA-Z\\-\\_\\.]+)?\\.yaml` and for the application to only search in the user home directory and current working directory. The `includes` property will be removed.\\n","tokens":67,"id":3335}
{"File Name":"read-more-api\/0009-service-layer.md","Context":"## Context\\nA Controller is responsible for receiving a request, executing it and returning an appropriate response.\\nA service layer can be added to remove knowledge of how an operation is performed from a Controller, allowing it to focus on the responsibilities mentioned above.\\n","Decision":"We will use a service layer to ensure that Controllers do not contain business logic.\\n","tokens":51,"id":720}
{"File Name":"datalab\/0008-weavenet-for-overlay-network.md","Context":"## Context\\nKubernetes does not provide an overlay network out of the box and we need to choose\\nwhich one to use from [here](https:\/\/kubernetes.io\/docs\/concepts\/cluster-administration\/networking\/).\\n","Decision":"We have decided to use the [WeaveNet](https:\/\/www.weave.works\/oss\/net\/) network as this\\nhas already been used by the JASMIN team. It also appears easy to use and there is good\\ndocumentation.\\n","tokens":46,"id":768}
{"File Name":"rails-template\/0011-use-prettier-to-format-js-code.md","Context":"## Context\\nWe want to ensure we're all using one code style, that is familiar across\\nprojects. [Prettier](https:\/\/prettier.io\/) is an opinionated code formatter with\\nsupport for most, if not all, of the languages in the JavaScript ecosystem. As\\nof writing, it is used by over\\n[1 million repositories](https:\/\/github.com\/prettier\/prettier\/network\/dependents?package_id=UGFja2FnZS00OTAwMTEyNTI%3D)\\non GitHub, including React itself, and has become a standard.\\n","Decision":"We will enforce that everything supported by Prettier has its style enforced by\\nit.\\nWe will set up Git hooks to automatically run the formatter before committing.\\nWe will set continuous integration up to reject commits that are not correctly\\nformatted.\\n","tokens":125,"id":4183}
{"File Name":"opg-data\/0003-opg-data-api-specification.md","Context":"## Context\\n### A pragmatic approach\\nWe need to arrive at an API spec that is fully formed, without spending an inordinate amount of time creating something from scratch, when other mature specifications exist and can be leveraged to suit our needs well.\\nMost major tech corporations have arrived at their own slightly differing proprietary implementations of a RESTful API spec, and none are completely awful or completely successful.\\nSome are more mature than others, such as JSON:API [https:\/\/jsonapi.org](https:\/\/jsonapi.org), which \"has been properly registered with the IANA. Its media type designation is application\/vnd.api+json.\"\\nIt is also [open-source](https:\/\/github.com\/json-api\/json-api).\\n> \"By following shared conventions, you can increase productivity, take advantage of generalised tooling, and focus on what matters: your application.\"\\n","Decision":"Our RESTful API implementation will be heavily based on JSON:API, and our documentation will focus mainly on hilighting those points where it differs.\\nWe will also summarise and elucidate key points as needed.\\n","tokens":176,"id":2178}
{"File Name":"interlok\/0011-annotation-aware-lifecycle.md","Context":"## Context and Problem Statement\\nIt's probably easiest to describe this using an example. Consider the changes that were implemented for [ADR-0008](0008-restful-failed-message-retrier.md). This added a new interface\\n```java\\npublic interface RetryStore extends ComponentLifecycle, ComponentLifecycleExtension {\\n\/\/ some methods.\\n}\\n```\\nalong with a configurable item that uses the new pluggable interface\\n```java\\npublic class RetryStoreWriteService extends ServiceImp {\\n\/**\\n* Where messages are stored for retries.\\n*\\n*\/\\n@Getter\\n@Setter\\n@NotNull\\n@NonNull\\nprivate RetryStore retryStore;\\n}\\n```\\nThis class requires a lot of boiler plate along the lines of overriding parent methods \/ implementing missing methods.\\n```java\\n@Override\\npublic void prepare() throws CoreException {\\nArgs.notNull(getRetryStore(), \"retry-store\");\\nLifecycleHelper.prepare(getRetryStore());\\n}\\n@Override\\nprotected void initService() throws CoreException {\\nLifecycleHelper.init(getRetryStore());\\n}\\n@Override\\npublic void start() throws CoreException {\\nLifecycleHelper.start(getRetryStore());\\nsuper.start();\\n}\\n@Override\\npublic void stop() {\\nLifecycleHelper.stop(getRetryStore());\\nsuper.stop();\\n}\\n@Override\\nprotected void closeService() {\\nLifecycleHelper.close(getRetryStore());\\n}\\n```\\nThis is prone to mistakes and we should be removing the boilerplate where possible.\\n## Decision Drivers\\n* Backwards compatible; i.e. doesn't break existing code.\\n* Low friction to uptake\\n","Decision":"* Backwards compatible; i.e. doesn't break existing code.\\n* Low friction to uptake\\nAnnotation + LifecycleHelper runtime behavioural change. Introduction of a `InterlokLifecycle` annotation.\\n","tokens":338,"id":2338}
{"File Name":"react-native-app\/0003-set-environment-variables.md","Context":"## Context\\nWe need to set some APIs keys without publishing them on GitHub.\\n","Decision":"We will add `config.js.example` and then explain in the README under a setup section that you need to copy that and call it `config.js` and add the secrets to it. `config.js` added to `.gitignore`.\\n","tokens":17,"id":4208}
{"File Name":"universal-remote-control-skill\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":922}
{"File Name":"Horace\/0005-use-gtest-in-cpp-tests.md","Context":"## Context\\nC++ tests are required for the project. For writing these, a testing framework\\nshould be used.\\nBelow is a shortlist of C++ testing frameworks:\\n* [GoogleTest](https:\/\/github.com\/google\/googletest)\\n* [Catch2](https:\/\/github.com\/catchorg\/Catch2)\\n* [CXXTest](https:\/\/cxxtest.com\/)\\n","Decision":"We will use GoogleTest as the framework for our C++ tests.\\nGoogleTest was chosen because it is:\\n* in active development\\n* used by other teams within ISIS (IBEX, Mantid use the mocking framework)\\n* required by [Google Benchmark](https:\/\/github.com\/google\/benchmark), which\\ncould become useful in the future.\\n* equipped with a mocking framework\\nCXXTest has not had any development for several years and Catch2, whilst being\\neasy to set-up (it's header only), is not used elsewhere within ISIS.\\n","tokens":84,"id":4232}
{"File Name":"once-ui\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":1161}
{"File Name":"qc-atlas\/0007-junit-for-testing.md","Context":"## Context and Problem Statement\\nSince the project started out with both JUnit4 and JUnit5, we only want one unit-testing framework in order to make testing writing consistent.\\n","Decision":"Chosen option: \"[JUnit5]\", because it is the newer version and therefore has a higher maturity and a wider feature-set.\\n### Positive Consequences <!-- optional -->\\n* Uniform tests\\n* More and newer features\\n","tokens":37,"id":692}
{"File Name":"imageclass-uplift\/0005-configuring-dlxs.md","Context":"## Context\\nDLXS has machinery to assemble an XSLT template via a path fallback mechanism.\\nBecause the uplift will not be immediately applied across all collections, DLXS will need to be extended to support an alternative assembly process.\\n","Decision":"Update DLXS:\\n* Configure a `collmgr` setting to flag a collection as being \"uplifted\"\\n* If detected, the fallback machinery will look in an uplift-specific path fallback to avoid any existing XSLT templates\\n","tokens":48,"id":4340}
{"File Name":"up-fiscal-data\/002-koshvani.md","Context":"## Context and Problem Statement\\nSections of the Koshvani platfrom to be considered in scope for scoping and analysing the data.\\n- Expenditure\\n- Receipts\\n","Decision":"Explore both sections to identify sub-sections from which data needs to be extracted and analysed.\\n","tokens":39,"id":448}
{"File Name":"atlasdb\/0010-use-partial-row-complete-cell-batching-in-gettimestampsbycell.md","Context":"## Context\\nAs of version 0.35.0 , our implementation of `DbKvs.getTimestampsByCell` was creating a MultiMap with all pairs of\\n(Cell, timestamp) from the range determined by the row batch size. In case of wide rows, or simply a row batch size\\nthat is too large, this could cause us to run out of memory; see [issue #982](https:\/\/github.com\/palantir\/atlasdb\/issues\/982).\\n","Decision":"We decided to use a block (a triple of row name, column name, and timestamp) batch size, defaulting to 1000000 to avoid\\nloading too many blocks into memory. The algorithm will fetch blocks by row, column, and increasing timestamp until the\\nbatch size is met. In that case:\\n1. If at least one row was fully processed, we return RowResults with all timestamps each of those rows, and continue\\nprocessing from the beginning of current row when more RowResults are necessary.\\n2. If no rows were fully processed, we continue processing blocks until the end of the current cell (row, column),\\nfetching all timestamps for that cell. We return a (partial) RowResult that contains all timestamps for that row\\nuntil and including the final cell processed. To create the next RowResult we then continue processing the row after\\nthe last cell processed, effectively splitting the wide row into several RowResults.\\nExample:\\nAssuming a batch size of 10 and the following table\\n|   |         1          |          2         |     3     |\\n|   | ------------------ | ------------------ | --------- |\\n| 1 |     (1, 2, 3)      |      (4, 5, 6)     | (7, 8, 9) |\\n| 2 |    (1, 2, 3, 4)    |    (4, 5, 6, 7)    | (7, 8, 9) |\\n| 3 | (1, 2, 3, 4, 5, 6) | (4, 5, 6, 7, 8, 9) | (7, 8, 9) |\\n| 4 |     (1, 2, 3)      |                    |           |\\nThe RowResults will be as follows:\\n- (1 -> (1 -> (1, 2, 3); 2 -> (4, 5, 6); 3 -> (7, 8, 9)))\\n- (2 -> (1 -> (1, 2, 3, 4); 2 -> (4, 5, 6, 7); 3 -> (7, 8, 9)))\\n- (3 -> (1 -> (1, 2, 3, 4, 5, 6); 2 -> (4, 5, 6, 7, 8, 9)))\\n- (3 -> (3 -> (7, 8, 9)))\\n- (4 -> (1 -> (1, 2, 3)))\\nOther options considered:\\n1. Partial row batching: fetch up to 1000000 blocks. If the block batch size is hit, stop immediately (cells can be split across multiple batches).\\n2. Full row batching: fetch up to 1000000 blocks. If the block batch size is hit in the middle of a row, continue fetching until the row is exhausted.\\n3. No batching, but throw an error when the block batch hint is reached\\n- Option 1 was considered, but was replaced by the modified version above. This is because sweep must ensure that all blocks\\nexcept for the most recent (before the immutable timestamp) are deleted. This can be achieved by repeating the last\\nblock from one batch in the next batch, or by keeping this last result in sweep's memory, so that it knows to remove it,\\nif further blocks for the same cell are encountered. Neither option is compelling: the first is hard to reason about,\\nand the second reduces the scope for parallelisation, and risks introducing a correctness bug.\\n- Option 2 was not chosen, because it does not guard well against wide rows (many cells) that have many overwrites.\\n- Option 3 was considered, but was ultimately discarded because it relies on properties of the code that calls getCellTimestamps.\\nIn particular, there needs to be retry logic that detects that an error was thrown, and reduces the batch size accordingly.\\n","tokens":100,"id":3097}
{"File Name":"mbed-tools\/0002-move-mbed-config-header-defines-to-cmake.md","Context":"## Context\\n`mbed_config.h` was created because the command line length limit on older versions of Windows prevented passing many \"-D\" defines to the compiler and linker.\\nAdditionally, passing many \"-D\" defines globally is a bad idea in itself. Therefore, `mbed_config.h` was a workaround to make a bad practice work.\\nOn modern versions of Windows, the command line limit isn't an issue. CMake can also do what's necessary to prevent exceeding the command line length limit where needed.\\n","Decision":"We will remove the generation of `mbed_config.h` and simply pass the \"-D\" defines using CMake directives. This is acheived by moving the \"-D\" definitions to the tools generated `mbed_config.cmake`.\\n","tokens":106,"id":4475}
{"File Name":"mercury-rust\/0002-choosing-capnproto.md","Context":"## Context\\nWe need something better than ProtoBuf\\n","Decision":"We checked the Cap'n'Proto documentation and examples on RPC.\\nSeems to be a perfect match for our needs, so we started integrating it to our SDK for communication between client\/server, replacing the previous ProtoBuf.\\n","tokens":11,"id":4902}
{"File Name":"Wikibase\/0016-use-github-actions-as-secondary-ci.md","Context":"## Context\\nThe Wikibase CI which runs on WMF Jenkins is currently \"augmented\" by running php unit tests in a variety of additional configurations (e.g. non-English wiki, repo\/client-only environment, etc) on legacy Travis CI infrastructure, via the GitHub mirror of our Gerrit code repository, see [https:\/\/travis-ci.org\/github\/wikimedia\/Wikibase](https:\/\/travis-ci.org\/github\/wikimedia\/Wikibase).\\nThe Travis CI features we currently use include:\\n* PHP installed (multiple versions),\\n* MySQL running,\\n* notifications via\\n* email,\\n* IRC,\\n* composer cache.\\nDue to a change in their business model, the travis-ci.org service is being phased out and replaced by (paid) travis-ci.com. Since we have no intention of dropping the extended CI testing, there are several options how to handle the situation:\\n* migrate additional CI for Wikibase to some other CI infrastructure, e.g. _GitHub Actions_,\\n* negotiate with the WMF changing the Travis CI plan to a paid one which would have unlimited\/less limited resources available,\\n* migrate additional CI for Wikibase to additional jobs on WMF Jenkins CI.\\n","Decision":"We will migrate the additional CI for Wikibase to the GitHub CI infrastructure, using _GitHub Actions_.\\nReasons: The GitHub mirror of the Wikibase repository exists already and all of the Travis CI features we have been using so far, are available on _GitHub Actions_:\\n* [setup-php](https:\/\/github.com\/shivammathur\/setup-php),\\n* MySQL pre-installed on the runner or use [MySQL service container](https:\/\/firefart.at\/post\/using-mysql-service-with-github-actions\/) (to be investigated),\\n* notifications via\\n* [email](https:\/\/docs.github.com\/en\/free-pro-team@latest\/github\/managing-subscriptions-and-notifications-on-github\/configuring-notifications) (built-in),\\n* [notify-irc](https:\/\/github.com\/rectalogic\/notify-irc) + [failure() condition](https:\/\/docs.github.com\/en\/free-pro-team@latest\/actions\/reference\/context-and-expression-syntax-for-github-actions#failure),\\n* [cache](https:\/\/github.com\/actions\/cache).\\n","tokens":252,"id":1342}
{"File Name":"dos-server\/adr-5-open-api-specification.md","Context":"## Context\\nIn accordance with our architecture principles (API first design), an API specification\\nis required to specify and share information about DOS API end points. OpenAPI is supported by most API design tools and it is already being used to specify other DLS APIs.\\n","Decision":"OpenAPI (Swagger) will be used.\\n","tokens":54,"id":3375}
{"File Name":"event-routing-backends\/0004-transformers-architecture.md","Context":"Context\\n-------\\nWe can develop event transformers either using the \u201cBackend\u201d architecture\\nor \u201cProcessor\u201d architecture. Making the transformers \u201cbackends\u201d will result\\nin relatively more nesting in the configurations as this \u201cbackend\u201d will have\\nits own configurations.\\nIf we decide to develop the event transformers as \u201cprocessors\u201d, it will result\\nin less complexity in the code since the transformer can be easily appended in\\nany backend\u2019s (router or logger) processors\u2019 pipeline.\\nDecision\\n--------\\nTransformers will be developed as event processors that can be added in\\nany backend\u2019s pipeline. Then the transformed events can be used for any purpose,\\neither for simply logging using the LoggerBackend or to route events using\\nEventRoutingBackend.\\nConsequences\\n------------\\nDeveloping transformers as processors will result in relatively less complex\\nconfigurations and it would provide us wider range of use cases for the transformers.\\n","Decision":"--------\\nTransformers will be developed as event processors that can be added in\\nany backend\u2019s pipeline. Then the transformed events can be used for any purpose,\\neither for simply logging using the LoggerBackend or to route events using\\nEventRoutingBackend.\\nConsequences\\n------------\\nDeveloping transformers as processors will result in relatively less complex\\nconfigurations and it would provide us wider range of use cases for the transformers.\\n","tokens":188,"id":4478}
{"File Name":"lbh-adrs\/Frontend-Tech-Stack.md","Context":"## **Context**\\nHackney has several frontends applications using different programming languages and frameworks:\\n- Angualr\/JS\\n- Ruby\\n- React\/TS\\nHackney has got a  microservices architecture exposing APIs to be used by different consumers and so each frontend can be developed with its own programming language and framework. Anyway, unless an application requires a particular programming language, it\u2019s still good to agree on a common language\/framework for mtfh-T&L workstream. This will make it easier to shift developers from different teams.\\n","Decision":"**React with TS (Type Script)**\\nReact with TypeScript is the most common framework\/language used for the majority of Hackney frontend application, it\u2019s the programming language that Hackney frontend developers are more familiar with, plus it has got the following well known advantages:\\n- It\u2019s open source\\n- Easy to learn because of its simple design and less time worrying about the framework specific code\\n- Better user experience and very fast performance compared with other FE frameworks\\n","tokens":113,"id":2308}
{"File Name":"register-a-food-business-front-end\/0007-switch-from-create-react-app-to-next-js.md","Context":"## Context\\nReact can be implemented from scratch, using the `create-react-app` CLI, or by using a 3rd-party framework such as Next.js.\\n","Decision":"We will re-start the scaffold.\\nWe will use Next.js as the basis for this project.\\nWe will extend and customise Next.js to fit our more bespoke requirements.\\n","tokens":34,"id":3039}
{"File Name":"paas-team-manual\/ADR026-DNS-layout-for-UK-hosting.html.md","Context":"## Context\\nWe are moving AWS hosting from Ireland to London. This ADR contains the decisions of the DNS names we will use for apps and system components that will be hosted in London.\\n","Decision":"We will use the following domain patterns for the London hosting:\\n* `(system_component).london.(system_domain)`\\n* `(app_name).london.(app_domain)`\\nWhere:\\n* (system_component) -- api, uaa, doppler, ssh, etc.\\n* (system_domain) -- _cloud.service.gov.uk_, _staging.cloudpipeline.digital_\\n* (app_domain) -- _cloudapps.digital_, _staging.cloudpipelineapps.digital_\\nThe reasons are:\\n* We should re-use the (system_component) first part to minimise the changes to the Cloud Foundry manifests.\\n* We should re-use the (system_domain) and (app_domain) last part, because these domains are assigned to GOV.UK PaaS as the public interface.\\n* The domain part `london` is preferrable to `uk`, because AWS may provide multiple-region hosting within the UK in the future.\\nThe domain structure for the dev and CI environments won't change. For the dev environments we will create a flag to choose where to create the deployment.\\n### Examples\\n#### Production\\n|Ireland|London|\\n|----|------|\\n|# _api.cloud.service.gov.uk_|_api.london.cloud.service.gov.uk_|\\n|# _sample-app.cloudapps.digital_|_sample-app.london.cloudapps.digital_|\\n#### Staging\\n|Ireland|London|\\n|----|------|\\n|# _api.staging.cloudpipeline.digital_|_api.london.staging.cloudpipeline.digital_|\\n|# _sample-app.staging.cloudpipelineapps.digital_|_sample-app.london.staging.cloudpipelineapps.digital_|\\n","tokens":39,"id":229}
{"File Name":"oasis-core\/0007-improved-random-beacon.md","Context":"## Context\\n> Any one who considers arithmetical methods of producing random digits\\n> is, of course, in a state of sin.\\n>\\n> --Dr. John von  Neumann\\nThe existing random beacon used by Oasis Core, is largely a placeholder\\nimplementation that naively uses the previous block's commit hash as the\\nentropy input.  As such it is clearly insecure as it is subject to\\nmanipulation.\\nA better random beacon which is harder for an adversary to manipulate\\nis required to provide entropy for secure committee elections.\\n","Decision":"At a high level, this ADR proposes implementing an on-chain random beacon\\nbased on \"SCRAPE: Scalabe Randomness Attested by Public Entities\" by\\nCascudo and David.  The new random beacon will use a commit-reveal scheme\\nbacked by a PVSS scheme so that as long as the threshold of participants\\nis met, and one participant is honest, secure entropy will be generated.\\nNote: This document assumes the reader understands SCRAPE. Details\\nregarding the underlying SCRAPE implementation are omitted for brevity.\\n### Node Descriptor\\nThe node descriptor of each node will be extended to include the following\\ndatastructure.\\n```golang\\ntype Node struct {\\n\/\/ ... existing fields omitted ...\\n\/\/ Beacon contains information for this node's participation\\n\/\/ in the random beacon protocol.\\n\/\/\\n\/\/ TODO: This is optional for now, make mandatory once enough\\n\/\/ nodes provide this field.\\nBeacon *BeaconInfo `json:\"beacon,omitempty\"`\\n}\\n\/\/ BeaconInfo contains information for this node's participation in\\n\/\/ the random beacon protocol.\\ntype BeaconInfo struct {\\n\/\/ Point is the elliptic curve point used for the PVSS algorithm.\\nPoint scrape.Point `json:\"point\"`\\n}\\n```\\nEach node will generate and maintain a long term elliptic curve point\\nand scalar pair (public\/private key pair), the point (public key) of\\nwhich will be included in the node descriptor.\\nFor the purposes of the initial implementation, the curve will be P-256.\\n### Consensus Parameters\\nThe beacon module will have the following consensus parameters that\\ncontrol behavior.\\n```golang\\ntype SCRAPEParameters struct {\\nParticipants  uint64 `json:\"participants\"`\\nThreshold     uint64 `json:\"threshold\"`\\nPVSSThreshold uint64 `json:\"pvss_threshold\"`\\nCommitInterval  int64 `json:\"commit_interval\"`\\nRevealInterval  int64 `json:\"reveal_interval\"`\\nTransitionDelay int64 `json:\"transition_delay\"`\\n}\\n```\\nFields:\\n- `Participants` - The number of participants to be selected for each\\nbeacon generation protocol round.\\n- `Threshold` - The minimum number of participants which must\\nsuccessfully contribute entropy for the final output to be\\nconsidered valid.\\n- `PVSSThreshold` - The minimum number of participants that are\\nrequired to reconstruct a PVSS secret from the corresponding\\ndecrypted shares (Note: This usually should just be set to\\n`Threshold`).\\n- `CommitInterval` - The duration of the Commit phase, in blocks.\\n- `RevealInterval` - The duration of the Reveal phase, in blocks.\\n- `TransitionDelay` - The duration of the post Reveal phase delay, in blocks.\\n### Consensus State and Events\\nThe on-chain beacon will maintain and make available the following consensus\\nstate.\\n```golang\\n\/\/ RoundState is a SCRAPE round state.\\ntype RoundState uint8\\nconst (\\nStateInvalid  RoundState = 0\\nStateCommit   RoundState = 1\\nStateReveal   RoundState = 2\\nStateComplete RoundState = 3\\n)\\n\/\/ SCRAPEState is the SCRAPE backend state.\\ntype SCRAPEState struct {\\nHeight int64 `json:\"height,omitempty\"`\\nEpoch EpochTime  `json:\"epoch,omitempty\"`\\nRound uint64     `json:\"round,omitempty\"`\\nState RoundState `json:\"state,omitempty\"`\\nInstance     *scrape.Instance      `json:\"instance,omitempty\"`\\nParticipants []signature.PublicKey `json:\"participants,omitempty\"`\\nEntropy      []byte                `json:\"entropy,omitempty\"`\\nBadParticipants map[signature.PublicKey]bool `json:\"bad_participants,omitempty\"`\\nCommitDeadline   int64 `json:\"commit_deadline,omitempty\"`\\nRevealDeadline   int64 `json:\"reveal_deadline,omitempty\"`\\nTransitionHeight int64 `json:\"transition_height,omitempty\"`\\nRuntimeDisableHeight int64 `json:\"runtime_disable_height,omitempty\"`\\n}\\n```\\nFields:\\n- `Height` - The block height at which the last event was emitted.\\n- `Epoch` - The epoch in which this beacon is being generated.\\n- `Round` - The epoch beacon generation round.\\n- `State` - The beacon generation step (commit\/reveal\/complete).\\n- `Instance` - The SCRAPE protocol state (encrypted\/decrypted shares of\\nall participants).\\n- `Participants` - The node IDs of the nodes selected to participate\\nin this beacon generation round.\\n- `Entropy` - The final raw entropy, if any.\\n- `BadParticipants` - A map of nodes that were selected, but have failed\\nto execute the protocol correctly.\\n- `CommitDeadline` - The height in blocks by which participants must\\nsubmit their encrypted shares.\\n- `RevealDeadline` - The height in blocks by which participants must\\nsubmit their decrypted shares.\\n- `TransitionHeight` - The height at which the epoch will transition\\nassuming this round completes successfully.\\n- `RuntimeDisableHeight` - The height at which, upon protocol failure,\\nruntime transactions will be disabled.  This height will be set to\\nthe transition height of the 0th round.\\nUpon transition to a next step of the protocol, the on-chain beacon will\\nemit the following event.\\n```golang\\n\/\/ SCRAPEEvent is a SCRAPE backend event.\\ntype SCRAPEEvent struct {\\nHeight int64 `json:\"height,omitempty\"`\\nEpoch EpochTime  `json:\"epoch,omitempty\"`\\nRound uint64     `json:\"round,omitempty\"`\\nState RoundState `json:\"state,omitempty\"`\\nParticipants []signature.PublicKey `json:\"participants,omitempty\"`\\n}\\n```\\nField definitions are identical to that of those in the `SCRAPEState`\\ndatastructure.\\n","tokens":116,"id":4351}
{"File Name":"caia\/0002-use-of-python.md","Context":"## Context\\nAn application has to written in some programming languge.\\n","Decision":"The following languages were considered for this application:\\n* Java\\n* Python\\n* Ruby\\nThe initial inclination was to use either Java or Ruby, based solely on\\ndeveloper experience.\\nAfter discussions with Ben Wallberg, it was decided to use Python because:\\n* The application is fairly simple, an unlikely to reach a complexity where\\nJava's more explicit typing and compile-time verification is needed.\\n* We have a number of developers familiar with Python, providing a broader\\nbase from which to draw support.\\n* While we are content with using Ruby (specifically Ruby on Rails) for website\\nprojects, this is a command-line application. We have fewer developers\\nfamiliar with Ruby.\\n","tokens":15,"id":3510}
{"File Name":"mental-health-service-finder\/0008-do-not-use-local-data-for-ccgs.md","Context":"## Context\\nAll current IAPT services should have an ODS code. Any future services must\\nhave an ODS code assigned. There for it is no longer necessary to use local\\ndata to relate CCGs to IAPT services.\\n","Decision":"Remove local data for all CCGs. The source for CCG to IAPT service relationship\\ndata will only be the central data store.\\n","tokens":50,"id":4575}
{"File Name":"react-template\/0007-use-eslint.md","Context":"## Context\\nWe want to enforce consistency in our code, and catch as many errors\\nautomatically as we are able to. Linting the code is good practice to achieve\\nthese aims. [ESLint](https:\/\/eslint.org\/) is the standard linter for modern\\nJavaScript, and has good support for TypeScript and React though plugins.\\n","Decision":"We will check code style using ESLint.\\nWe will let Prettier have precedence when ESLint and Prettier conflict in their\\nstyles.\\nWe will use the recommended configuration for plugins where possible.\\nWe will run ESLint as part of the test suite.\\n","tokens":71,"id":3330}
{"File Name":"html-integrations\/001-Generate--RFC-compliant--UUIDs--Telemetry.md","Context":"## Context (Discussion)\\nIn the context of the Telemetry project, we want to generate\\nvalid and consisten UUIDs since they are needed to ensure the uniqueness, randomeness and its validity.\\nUUID identifiers have an specification defined as the RFC-4122 standard  [A Universally Unique IDentifier (UUID) URN Namespace](https:\/\/tools.ietf.org\/html\/rfc4122) on ietf.org.\\nWe don't want to reinvent the wheel and it seems unwise to write our own library to generate this UUIDs since there are third party solutions with good support, small and secure.\\n","Decision":"We'll use [uuid package] to generate RFC4122 version 4 UUIDs to use on the Telemetry implementation. The code of the [uuid project] is available at github.\\nTherefore, **uuid** becomes the first functional dependency of the 'MathType Web Integration JavaScript SDK', known as npm package as '@wiris\/mathtype-html-integration-devkit'.\\n* [uuid project](https:\/\/github.com\/uuidjs\/uuid)\\n* [uuid package](https:\/\/www.npmjs.com\/package\/uuid)\\n### Pros and Cons of the Options\\n#### Implement our own Javascript library for that\\n- Bad, because Javascript Math.random function is not very good.\\n- Bad, because we'll need to maintain it.\\n- Bad, because more work to the backlog.\\n- Bad, because we're reinventing the wheel.\\n#### Using a third party library like github.com\/uuidjs\/uuid\\n- Good, because supports RFC4122 version 1, 3, 4, and 5 UUIDs.\\n- Good, because its well maintained, no issues and widely used; (26.085.977 downloads\/week).\\n- Good, because solves our problem immediately.\\n- Good, because it's secure, small and cross-platform.\\n- Bad, because we're adding a dependency to our core library, and therefore, to all our Javascript plugins.\\n","tokens":127,"id":337}
{"File Name":"phpadr\/0003-php-as-scripting-language.md","Context":"## Context\\nThe tool must be cross-platform and developed using a open source programming language in command-line interface to be used in any project independent of the tech stack.\\n","Decision":"Develop using [PHP](http:\/\/php.net\/) (PHP Hypertext Preprocessor) to work command line scripting.\\n","tokens":34,"id":5071}
{"File Name":"digital-paper-edit-client\/2019-06-20-environment-injection.md","Context":"## Context and Problem Statement\\nThis addresses the fact that we could not configure different environments to point to different API URLs as necessary (across Int, Test, and Live envs in Cosmos).\\nAt the point of deployment, `npm run build` is called in Jenkins to build the static  bundle and minify etc. Environment config (`REACT_APP_SERVER_URL` etc. variables declared in `.env`) get baked into the bundle and obfuscated at this point.\\nWe would need different Jenkins jobs or parameters to bake in the environment config (eg. the correct server URL) multiple times, which goes against the idea of having an _unconfigured \/ environment agnostic_ RPM in Cosmos.\\n## Decision Drivers <!-- optional -->\\n* Avoiding too much extra code overhead\\n* Trying to avoid security converns with proxy solutions (injection \/ DB manipulations)\\n* Avoiding having to re-build in Cosmos or on the box\\n","Decision":"* Avoiding too much extra code overhead\\n* Trying to avoid security converns with proxy solutions (injection \/ DB manipulations)\\n* Avoiding having to re-build in Cosmos or on the box\\nThis is a common problem with create-react-app and the need to configure it. We came across [this solution](https:\/\/github.com\/facebook\/create-react-app\/issues\/578#issuecomment-277843310) which detailed a simple way of consuming config with the `window` global.\\nIn source code this remains localhost:8080 as a local development fallback and it is passed in via Cosmos environment config.\\n","tokens":192,"id":4165}
{"File Name":"james-project\/0042-james-cli-based-on-webadmin.md","Context":"## Context\\nJames servers offer a command-line interface in order to interact with the server. However, it relies on the JMX protocol, which is known to be insecure. The JMX server embedded in Apache James, also used by the command line client is exposed to a java de-serialization issue according to [NVD-CVE-2017-12628 Detail](https:\/\/nvd.nist.gov\/vuln\/detail\/CVE-2017-12628), and thus can be used to execute arbitrary commands.\\nBesides, the current CLI interface is also not optimal for users. It places actions in front of entities with contiguous syntax, making it harder for the user to remember the command (for example, which entity the GET action command can interact with). If we design to place the entity first and the outgoing actions can interact with that entity afterward, the user will easily imagine what he\/she can do with each entity. This creates an intuitive interface that is easier to remember.\\nWebadmin APIs use HTTP protocol, which is more secure than JMX protocol to interact with James servers.\\nWebadmin command-line interface is an upcoming replacement for the outdated, security-vulnerable JMX command-line interface.\\n","Decision":"We decided to write a new CLI client, running on top of the JVM, communicating with James via the webadmin protocol, using http.\\n* What libraries will we use?\\n* http client: ***Feign library***. We used it as an http client in other parts of James so we continue to use it.\\n* CLI: ***Picocli library***. Picocli is a one-file command line parsing framework written in Java that allows us to create command line applications with almost no code. It allows mixing Options with positional Parameters (Eg: no need to the follow order Options then Parameters), [automatic type conversion](https:\/\/picocli.info\/#_strongly_typed_everything) of command line arguments to the type of the annotated field, provide Automatic Help and better Subcommand Support, easily handle Exceptions.\\n* How will we limit breaking changes this new CLI will cause?\\n* Work on a wrapper to adapt the old CLI API.\\n* Where will we locate this cli code?\\n* server\/protocols\/webadmin-cli\\n* Write a man page.\\n* Picocli generates beautiful documentation for our CLI (HTML, PDF and Unix man pages).\\n* We decided to adopt a more modern, modular CLI syntax:\\n```\\n$ .\/james-cli [OPTION] ENTITY ACTION {ARGUMENT}\\n```\\nwhere\\nOPTION: optional parameter when running the command line,\\nENTITY: represents the entity to perform action on,\\nACTION: name of the action to perform,\\nARGUMENT: arguments needed for the action.\\n#### Examples\\nAdd a domain to the domain list.\\n```\\n$ .\/james-cli --url http:\/\/127.0.0.1:9999 domain create domainNameToBeCreated\\n```\\nIn above command-line\\nOPTION: --url http:\/\/127.0.0.1:9999\\nENTITY: domain\\nACTION: create\\nARGUMENT: domainNameToBeCreated\\n","tokens":244,"id":2882}
{"File Name":"event-routing-backends\/0001-purpose-of-this-repo.rst","Context":"Context\\n-------\\n`OEP-26 <https:\/\/open-edx-proposals.readthedocs.io\/en\/latest\/oep-0026-arch-realtime-events.html>`__\\nconsists of the following components:\\n-  Asynchronous Routing Backend\\n-  Regular-expressions based filter processor\\n-  IMS Caliper transformer\\n-  xAPI transformer\\n-  Router to forward events\\nKeeping all of these components in one repo will make the repository\\nunnecessarily tangled since these additional components are not required\\nby the core app for its functionality.\\nDecision\\n--------\\nAmong the components listed above, Asynchronous Routing Backend and the\\nregular-expressions filter will be added in the core app (i.e.\\n`event-tracking <https:\/\/github.com\/openedx\/event-tracking>`__) while the\\nother components, i.e. Caliper transformer backend, xAPI transformer\\nbackend and router, will be added in the current repo.\\nBy keeping the concrete backends separate from the code, we can have\\nonly the core plugin interface for event tracking in its repository.\\nConsequences\\n------------\\nThe code will be decoupled and components can be used independently if\\nrequired.\\nRejected Alternatives\\n---------------------\\n**Add the routing backends to the event-tracking repository**\\nThis idea was rejected to keep the core event-tracking repository clean\\nand independent. The core repo is functional on its own and any\\npluggable extensions should be implemented separately.\\n","Decision":"--------\\nAmong the components listed above, Asynchronous Routing Backend and the\\nregular-expressions filter will be added in the core app (i.e.\\n`event-tracking <https:\/\/github.com\/openedx\/event-tracking>`__) while the\\nother components, i.e. Caliper transformer backend, xAPI transformer\\nbackend and router, will be added in the current repo.\\nBy keeping the concrete backends separate from the code, we can have\\nonly the core plugin interface for event tracking in its repository.\\nConsequences\\n------------\\nThe code will be decoupled and components can be used independently if\\nrequired.\\nRejected Alternatives\\n---------------------\\n**Add the routing backends to the event-tracking repository**\\nThis idea was rejected to keep the core event-tracking repository clean\\nand independent. The core repo is functional on its own and any\\npluggable extensions should be implemented separately.\\n","tokens":306,"id":4479}
{"File Name":"lightweight-architecture-decision-records\/0001-use-elasticsearch-for-search-api.md","Context":"## Context\\nThere is a need of having an API exposed which can be used to search enterprise wide common data model.\\nThe data currently resides in a RDBMS database, it is difficult to expose micro-services directly querying out of RDBMS databases since the application runs out the same environment.\\nThere are options like ElasticSearch or Solr where data can be replicated.\\n","Decision":"Use ElasticSearch for data indexing\\n","tokens":76,"id":3246}
{"File Name":"edgex-docs\/013-Device-Service-Events-Message-Bus.md","Context":"- [Context](#context)\\n- [Decision](#decision)\\n* [Which Message Bus implementations?](#which-message-bus-implementations)\\n* [Go Device SDK](#go-device-sdk)\\n* [C Device SDK](#c-device-sdk)\\n* [Core Data and Persistence](#core-data-and-persistence)\\n* [V2 Event DTO](#v2-event-dto)\\n+ [Validation](#validation)\\n* [Message Envelope](#message-envelope)\\n* [Application Services](#application-services)\\n* [MessageBus Topics](#messagebus-topics)\\n* [Configuration](#configuration)\\n+ [Device Services](#device-services)\\n- [[MessageQueue]](#messagequeue)\\n+ [Core Data](#core-data)\\n- [[MessageQueue]](#messagequeue)\\n+ [Application Services](#application-services)\\n- [[MessageBus]](#messagebus)\\n- [[Binding]](#binding)\\n* [Secure Connections](#secure-connections)\\n- [Consequences](#consequences)\\n","Decision":"* [Which Message Bus implementations?](#which-message-bus-implementations)\\n* [Go Device SDK](#go-device-sdk)\\n* [C Device SDK](#c-device-sdk)\\n* [Core Data and Persistence](#core-data-and-persistence)\\n* [V2 Event DTO](#v2-event-dto)\\n+ [Validation](#validation)\\n* [Message Envelope](#message-envelope)\\n* [Application Services](#application-services)\\n* [MessageBus Topics](#messagebus-topics)\\n* [Configuration](#configuration)\\n+ [Device Services](#device-services)\\n- [[MessageQueue]](#messagequeue)\\n+ [Core Data](#core-data)\\n- [[MessageQueue]](#messagequeue)\\n+ [Application Services](#application-services)\\n- [[MessageBus]](#messagebus)\\n- [[Binding]](#binding)\\n* [Secure Connections](#secure-connections)\\n- [Consequences](#consequences)\\n### Which Message Bus implementations?\\nMultiple Device Services may need to be publishing Events to the MessageBus concurrently.  `ZMQ` will not be a valid option if multiple Device Services are configured to publish. This is because `ZMQ` only allows for a single publisher. `ZMQ` will still be valid if only one Device Service is publishing Events. The `MQTT` and `Redis Streams` are valid options to use when multiple Device Services are required, as they both support multiple publishers. These are the only other implementations currently available for Go services. The C base device services do not yet have a MessageBus implementation.  See the [C Device SDK](#c-device-sdk) below for details.\\n> *Note: Documentation will need to be clear when `ZMQ` can be used and when it can not be used.*\\n### Go Device SDK\\nThe Go Device SDK will take advantage of the existing `go-mod-messaging` module to enable use of the EdgeX MessageBus. A new bootstrap handler will be created which initializes the MessageBus client based on configuration. See [Configuration](#configuration) section below for details.  The Go Device SDK will be enhanced to optionally publish Events to the MessageBus anywhere it currently POSTs Events to Core Data. This publish vs POST option will be controlled by configuration with publish as the default.  See [Configuration](#configuration) section below for details.\\n### C Device SDK\\nThe C Device SDK will implement its own MessageBus abstraction similar to the one in `go-mod-messaging`.  The first implementation type (MQTT or Redis Streams) is TBD. Using this abstraction allows for future implementations to be added when use cases warrant the additional implementations.  As with the Go SDK, the C SDK will be enhanced to optionally publish Events to the MessageBus anywhere it currently POSTs Events to Core Data. This publish vs POST option will be controlled by configuration with publish as the default.  See [Configuration](#configuration) section below for details.\\n### Core Data and Persistence\\nWith this design, Events will be sent directly to Application Services w\/o going through Core Data and thus will not be persisted unless changes are made to Core Data. To allow Events to optionally continue to be persisted, Core Data will become an additional or secondary (and optional) subscriber for the Events from the MessageBus. The Events will be persisted when they are received. Core Data will also retain the ability to receive Events via HTTP, persist them and publish them to the MessageBus as is done today. This allows for the flexibility to have some device services to be configured to POST Events and some to be configured to publish Events while we transition the Device Services to all have the capability to publishing Events. In the future, once this new `Publish` approach has been proven, we may decide to remove POSTing Events to Core Data from the Device SDKs.\\nThe existing `PersistData` setting will be ignored by the code path subscribing to Events since the only reason to do this is to persist the Events.\\nThere is a race condition for `Marked As Pushed` when Core Data is persisting Events received from the MessageBus. Core Data may not have finished persisting an Event before the Application Service has processed the Event and requested the Event be `Marked As Pushed`. It was decided to remove `Mark as Pushed` capability and just rely on time based scrubbing of old Events.\\n### V2 Event DTO\\nAs this development will be part of the Ireland release all Events published to the MessageBus will use the V2 Event DTO. This is already implemented in Core Data for the V2 AddEvent API.\\n#### Validation\\nServices receiving the Event DTO from the MessageBus will log validation errors and stop processing the Event.\\n### Message Envelope\\nEdgeX Go Services currently uses a custom Message Envelope for all data that is published to the MessageBus. This envelope wraps the data with metadata, which is `ContentType` (JSON or CBOR), `Correlation-Id` and the obsolete `Checksum`. The `Checksum` is used when the data is CBOR encoded to identify the Event in V1 API to be mark it as pushed. This checksum is no longer needed as the V2 Event DTO requires the ID be set by the Device Services which will always be used in the V2 API to mark the Events as pushed. The Message Envelope will be updated to remove this property.\\nThe C SDK will recreate this Message Envelope.\\n### Application Services\\nAs part of the V2 API consumption work in Ireland the App Services SDK will be changed to expect to receive V2 Event DTOs rather than the V1 Event model. It will also be updated to no longer expect or use the `Checksum` currently on the  Message Envelope. Note these changes must occur for the V2 consumption and are not directly tied to this effort.\\nThe App Service SDK will be enhanced for the secure MessageBus connection described below. See **[Secure Connections](#secure-connections)** for details\\n### MessageBus Topics\\n> *Note: The change recommended here is not required for this design, but it provides a good opportunity to adopt it.*\\nCurrently Core Data publishes Events to the simple `events` topic. All Application Services running receive every Event published, whether they want them or not. The Events can be filtered out using the `FilterByDeviceName` or `FilterByResourceName` pipeline functions, but the Application Services still receives every Event and process all the Events to some extent. This could cause load issues in a deployment with many devices and large volume of Events from various devices or a very verbose device that the Application Services is not interested in.\\n> *Note: The current `FilterByDeviceName` is only good if the device name is known statically and the only instance of the device defined by the `DeviceProfileName`. What we really need is `FilterByDeviceProfileName` which allows multiple instances of a device to be filtered for, rather than a single instance as it it now. The V2 API will be adding `DeviceProfileName` to the Events, so in Ireland this  filter will be possible.*\\nPub\/Sub systems have advanced topic schema, which we can take advantage of from Application Services to filter for just the Events the Application Service actual wants. Publishers of Events must add the `DeviceProfileName`, `DeviceName` and `SourceName` to the topic in the form `edgex\/events\/<device-profile-name>\/<device-name>\/<source-name>`. The `SourceName` is the `Resource` or `Command` name used to create the Event. This allows Application Services to filter for just the Events from the device(s) it wants by only subscribing to those `DeviceProfileNames` or the specific `DeviceNames` or just the specific `SourceNames`  Example subscribe topics if above schema is used:\\n- **edgex\/events\/#**\\n- All Events\\n- Core Data will subscribe using this topic schema\\n- **edgex\/events\/Random-Integer-Device\/#**\\n- Any Events from devices created from the **Random-Integer-Device** device profile\\n- **edgex\/events\/Random-Integer-Device\/Random-Integer-Device1**\\n- Only Events from the **Random-Integer-Device1** Device\\n- **edgex\/events\/Random-Integer-Device\/#\/Int16**\\n- Any Events with Readings from`Int16` device resource from devices created from the **Random-Integer-Device** device profile.\\n- **edgex\/events\/Modbus-Device\/#\/HVACValues\\n- Any Events with Readings from `HVACValues` device command from devices created from the **Modbus-Device** device profile.\\nThe MessageBus abstraction allows for multiple subscriptions, so an Application Service could specify to receive data from multiple specific device profiles or devices by creating multiple subscriptions. i.e.  `edgex\/Events\/Random-Integer-Device\/#` and  `edgex\/Events\/Random-Boolean-Device\/#`. Currently the App SDK only allows for a single subscription topic to be configured, but that could easily be expanded to handle a list of subscriptions. See [Configuration](#configuration) section below for details.\\nCore Data's existing publishing of Events would also need to be changed to use this new topic schema. One challenge with this is Core Data doesn't currently know the `DeviceProfileName` or `DeviceName` when it receives a CBOR encoded event. This is because it doesn't decode the Event until after it has published it to the MessageBus. Also, Core Data doesn't know of `SourceName` at all. The V2 API will be enhanced to change the AddEvent endpoint from `\/event` to `\/event\/{profile}\/{device}\/{source}` so that `DeviceProfileName`, `DeviceName`, and `SourceName` are always know no matter how the request is encoded.\\nThis new topic approach will be enabled via each publisher's `PublishTopic` having the `DeviceProfileName`, `DeviceName`and `SourceName`  added to the configured `PublishTopicPrefix`\\n```toml\\nPublishTopicPrefix = \"edgex\/events\" # \/<device-profile-name>\/<device-name>\/<source-name> will be added to this Publish Topic prefix\\n```\\nSee [Configuration](#configuration) section below for details.\\n### Configuration\\n#### Device Services\\nAll Device services will have the following additional configuration to allow connecting and publishing to the MessageBus. As describe above in the  [MessageBus Topics](#messagebus-topics) section, the `PublishTopic` will include the `DeviceProfileName` and `DeviceName`.\\n##### [MessageQueue]\\nA  MessageQueue section will be added, which is similar to that used in Core Data today, but with `PublishTopicPrefix` instead of `Topic`.To enable secure connections, the `Username` & `Password` have been replaced with ClientAuth & `SecretPath`, See **[Secure Connections](#secure-connections)** section below for details. The added `Enabled` property controls whether the Device Service publishes to the MessageBus or POSTs to Core Data.\\n```toml\\n[MessageQueue]\\nEnabled = true\\nProtocol = \"tcp\"\\nHost = \"localhost\"\\nPort = 1883\\nType = \"mqtt\"\\nPublishTopicPrefix = \"edgex\/events\" # \/<device-profile-name>\/<device-name>\/<source-name> will be added to this Publish Topic prefix\\n[MessageQueue.Optional]\\n# Default MQTT Specific options that need to be here to enable environment variable overrides of them\\n# Client Identifiers\\nClientId =\"<device service key>\"\\n# Connection information\\nQos          =  \"0\" # Quality of Sevice values are 0 (At most once), 1 (At least once) or 2 (Exactly once)\\nKeepAlive    =  \"10\" # Seconds (must be 2 or greater)\\nRetained     = \"false\"\\nAutoReconnect  = \"true\"\\nConnectTimeout = \"5\" # Seconds\\nSkipCertVerify = \"false\" # Only used if Cert\/Key file or Cert\/Key PEMblock are specified\\nClientAuth = \"none\" # Valid values are: `none`, `usernamepassword` or `clientcert`\\nSecretpath = \"messagebus\"  # Path in secret store used if ClientAuth not `none`\\n```\\n#### Core Data\\nCore data will also require additional configuration to be able to subscribe to receive Events from the MessageBus. As describe above in the  [MessageBus Topics](#messagebus-topics) section, the `PublishTopicPrefix` will have `DeviceProfileName` and `DeviceName` added to create the actual Public Topic.\\n##### [MessageQueue]\\nThe `MessageQueue` section will be  changed so that the `Topic` property changes to `PublishTopicPrefix` and `SubscribeEnabled` and `SubscribeTopic` will be added. As with device services configuration, the `Username` & `Password` have been replaced with `ClientAuth` & `SecretPath` for secure connections. See **[Secure Connections](#secure-connections)** section below for details. In addition, the Boolean `SubscribeEnabled` property will be used to control if the service subscribes to Events from the MessageBus or not.\\n```toml\\n[MessageQueue]\\nProtocol = \"tcp\"\\nHost = \"localhost\"\\nPort = 1883\\nType = \"mqtt\"\\nPublishTopicPrefix = \"edgex\/events\" # \/<device-profile-name>\/<device-name>\/<source-name> will be added to this Publish Topic prefix\\nSubscribeEnabled = true\\nSubscribeTopic = \"edgex\/events\/#\"\\n[MessageQueue.Optional]\\n# Default MQTT Specific options that need to be here to enable evnironment variable overrides of them\\n# Client Identifiers\\nClientId =\"edgex-core-data\"\\n# Connection information\\nQos          =  \"0\" # Quality of Sevice values are 0 (At most once), 1 (At least once) or 2 (Exactly once)\\nKeepAlive    =  \"10\" # Seconds (must be 2 or greater)\\nRetained     = \"false\"\\nAutoReconnect  = \"true\"\\nConnectTimeout = \"5\" # Seconds\\nSkipCertVerify = \"false\" # Only used if Cert\/Key file or Cert\/Key PEMblock are specified\\nClientAuth = \"none\" # Valid values are: `none`, `usernamepassword` or `clientcert`\\nSecretpath = \"messagebus\"  # Path in secret store used if ClientAuth not `none`\\n```\\n#### Application Services\\n##### [MessageBus]\\nSimilar to above, the Application Services `MessageBus` configuration will change to allow for secure connection to the MessageBus. The `Username` & `Password` have been replaced with `ClientAuth` & `SecretPath` for secure connections. See **[Secure Connections](#secure-connections)** section below for details.\\n```toml\\n[MessageBus.Optional]\\n# MQTT Specific options\\n# Client Identifiers\\nClientId =\"<app sevice key>\"\\n# Connection information\\nQos          =  \"0\" # Quality of Sevice values are 0 (At most once), 1 (At least once) or 2 (Exactly once)\\nKeepAlive    =  \"10\" # Seconds (must be 2 or greater)\\nRetained     = \"false\"\\nAutoReconnect  = \"true\"\\nConnectTimeout = \"5\" # Seconds\\nSkipCertVerify = \"false\" # Only used if Cert\/Key file or Cert\/Key PEMblock are specified\\nClientAuth = \"none\" # Valid values are: `none`, `usernamepassword` or `clientcert`\\nSecretpath = \"messagebus\"  # Path in secret store used if ClientAuth not `none`\\n```\\n##### [Binding]\\nThe `Binding` configuration section will require changes for the subscribe topics scheme described in the [MessageBus Topics](#messagebus-topics) section above to filter for Events from specific device profiles or devices. `SubscribeTopic` will change from a string property containing a single topic to the `SubscribeTopics` string property containing a comma separated list of topics. This allows for the flexibility for the property to be a single topic with the `#` wild card so the Application Service receives all Events as it does today.\\nReceive only Events from the `Random-Integer-Device` and `Random-Boolean-Device` profiles\\n```toml\\n[Binding]\\nType=\"messagebus\"\\nSubscribeTopics=\"edgex\/events\/Random-Integer-Device, edgex\/events\/Random-Boolean-Device\"\\n```\\nReceive only Events from the  `Random-Integer-Device1` from the `Random-Integer-Device` profile\\n```toml\\n[Binding]\\nType=\"messagebus\"\\nSubscribeTopics=\"edgex\/events\/Random-Integer-Device\/Random-Integer-Device1\"\\n```\\nor receives all Events:\\n```toml\\n[Binding]\\nType=\"messagebus\"\\nSubscribeTopics=\"edgex\/events\/#\"\\n```\\n### Secure Connections\\nAs stated earlier,  this ADR is dependent on the  **Secret Provider for All**(Link TBD) ADR to provide a common Secret Provider for all Edgex Services to access their secrets. Once this is available, the MessageBus connection can be secured via the following configurable client authentications modes which follows similar implementation for secure MQTT Export and secure MQTT Trigger used in Application Services.\\n- **none** - No authentication\\n- **usernamepassword** - Username & password authentication.\\n- **clientcert** - Client certificate and key for authentication.\\n- The secrets specified for the above options are pulled from the `Secret Provider` using the configured `SecretPath`.\\nHow the secrets are injected into the `Secret Provider` is out of scope for this ADR and covered in the **Secret Provider for All**( Link TBD) ADR.\\n","tokens":227,"id":954}
{"File Name":"gsp\/ADR045-dev-namespaces.md","Context":"## Context\\nFollowing the retirement of gsp-local the service teams have no way of testing\\nchanges before a merge to master. So the feedback cycle is very slow, and\\npotentially risky. We need to enable devs to get more rapid feedback through use\\nof namespaces that allow for deployments from arbitrary branches or from local\\n`kubectl apply` actions.\\nIn the past service teams have made use of the Sandbox cluster for some of this\\ntesting where we allow them Cluster Admin permissions. However the primary\\nfunction of the Sandbox cluster is to test changes to the platform itself and so\\nis often down or otherwise degraded, which also slows service team development\\ndown.\\nThis presents several security problems that need to be addressed.\\nThe in-cluster concourse:\\n* must not be able to create or edit daemonsets (as tenants should not need to\\ninteract with per-node pods that typically need elevated permissions to e.g.\\nmanipulate the network stack)\\n* must not be able to create pods that use host networking, or run in privileged\\nmode\\n* must not be authorised to create any \"cluster\" scoped resources (e.g.\\nclusterrole, custom resources etc.)\\nFor harbor:\\n* a dev in one namespace must not be authorised to push, edit or delete images\\nrelating to namespaces outside their own\\n* notary signing keys must be namespace-scoped\\nFor external-dns:\\n* a namespace must not be able to hijack the DNS entries of another namespace\\nFor istio:\\n* any istio resources deployed as part of a namespace must not have any impact\\non other namespaces\\nFor CloudHSM access:\\n* connectivity only enabled via `-cluster-config`, as with \"prod\" namespaces and\\npods\\n* credentials should be different for each namespace, and not the same as those\\nused for the \"production\" instances of the application\\n","Decision":"We will address the majority of the security concerns by implementing\\n[ADR043][].\\nWe will address the harbor concerns by creating namespace-scoped credentials\\nrelating to namespace-scoped harbor \"projects\" and provide these credentials via\\nsecrets in the namespace.\\nWe will address the DNS concerns by locking down each namespace instance of\\nexternal-dns to a dedicated zone.\\nWe will address the istio concerns through the use of gatekeeper constraints\\n(e.g. all istio resources that support it have `exportTo: [\".\"]` set).\\nWe will create separate CryptoUsers in the CloudHSM for each namespace that\\nrequires access. CloudHSM cryptographic keys\/operations can then be scoped to a\\nsingle user (& namespace). This will allow each namespace to effectively have\\nit's own virtual slice of the CloudHSM and enable controlling access on a\\nper-namespace basis.\\n","tokens":390,"id":3895}
{"File Name":"cloud-sdk-js\/0007-use-yarn-workspaces.md","Context":"## Context\\nLerna guidance on how to use the `bootstrap` command with or without hoisting and the resulting `package-lock.json`s is sparse.\\nCalling `lerna bootstrap --hoist` changes the root `package-lock.json`, calling `lerna bootstrap` created `package-lock.json`s in the packages, that do not contain references to other local packages and are therefore incorrect in terms of standard npm.\\nTherefore tools like dependabot cannot handle these files.\\nGenerally, there is quite some confusion about how lerna handles the package-locks, when they are updated and how to make them work with other tools.\\nFurther, because `lerna bootstrap --hoist` changes the root `package-lock.json` we are not using hoisting to ensure we respect the `build once` principle.\\nTherefore, we currently don't benefit from hoisting.\\n","Decision":"Use yarn workspaces with hoisting.\\n","tokens":178,"id":3624}
{"File Name":"Maud\/0016-validating-measured-fluxes.md","Context":"## Context\\nDefining measurements for independent fluxes isn't always clear.\\nThis can occur when you measure more fluxes than there are degrees\\nof freedom in a network.\\nAn example would be this simplified network:\\nA -> B -> C\\nwhere reaction 1 and reaction 2 are dependent, implying that\\nno additional information is achieved by including both.\\nAnother issue is knowing when you do not have enough fluxes\\nmeasured, resulting in an underdetermined system. Due to the\\nBayesian implementation of Maud, these systems are still theoretically\\nresolvable. However, supplementing as much information\\nas possible will likely be beneficial.\\n","Decision":"Identifying underdetermined systems is acomplished by first calculating\\nthe null space of the matrix. This gives the number of degrees of freedom\\nof the system as well. Then we calculate the reduced row echelon form of\\nthe transpose of the null space. The resulting matrix represents the\\nindependent flux pathways through the network as rows. If you take the\\nmeasured subset of reactions and there is a row containing no non-zero\\nentries then the system is not fully described using the current measurements.\\nDetermining if the system is overspecified is achieved by comparing the\\nnumber of measurements to the degrees of freedom. If the number of measurements\\nis larger than the degrees of freedom then the system is overdetermined.\\nIt is possible to both have an underdetermined system which is overspecified\\nby having multiple measurements on dependent paths. It is also possible to\\nrecieve the warning that the system is overspecified by independent measurements.\\nFor instance, a linear pathway where the influx and efflux are both measured.\\nThis is still valid as they are independent measurements.\\n","tokens":136,"id":242}
{"File Name":"GOG-Galaxy-Suggester\/0001-app-tech-stack.md","Context":"## Context and Problem Statement\\nI want to choose a technology stack that would allow us integrating into GoG Galaxy 2.0 ecosystem with ease, provide a rich modern-looking UI and be less familiar technology I could learn.\\n## Decision Drivers\\n* learning new technology\\n* should be a desktop standalone application\\n* being able to run GoG Galaxy 2.0 plugins\\n* being able to read GoG Galaxy 2.0 database (SQLite3)\\n* modern looking UI\\n","Decision":"* learning new technology\\n* should be a desktop standalone application\\n* being able to run GoG Galaxy 2.0 plugins\\n* being able to read GoG Galaxy 2.0 database (SQLite3)\\n* modern looking UI\\nChosen option: \"Python + React (embedded)\", because it allows for modern-looking UI while maintaining the Python connection to GoG Galaxy 2.0 ecosystem.\\n### Positive Consequences\\n* Rich UI tools.\\n* Ability to easily interact with (Python based) GoG Galaxy 2.0 plugins.\\n* Learning ability.\\n### Negative Consequences\\n* Additional complexity in repository, mixing frontend (React) and backend (Python) code.\\n* Might run into additional issues due to the need of running a server for a desktop app.\\n* Follow up: there needs to be some sort of launcher to run both backend and frontend.\\n","tokens":100,"id":2506}
{"File Name":"Head-Start-TTADP\/0009-security-scans.md","Context":"## Context\\n**Static Code Analysis**\\nThe code we write should be routinely and automatically scanned for programming and stylistic errors to ensure our code is highly readable, maintainable, and free of bugs. This scan should be performed in addition to developer code review. It will be most performant when run frequently and with little developer involvement.\\n**Static Security Audit of Package Dependencies**\\nEnsuring our node modules do not have security vulnerabilities prior to deployment gives us the opportunity to \"find and fix known vulnerabilities in dependencies that could cause data loss, service outages, unauthorized access to sensitive information, or other issues\" [ref NPM documentation](https:\/\/docs.npmjs.com\/auditing-package-dependencies-for-security-vulnerabilities). This audit is most helpful when performed at frequent intervals and with little developer involvement.\\n**Dynamic Security Scan**\\nA dynamic security scan is needed to identify application features that could leave website data vulnerable to loss, corruption, or unauthorized access by malicious actors. This scan is most valuable when performed at frequent intervals and with little developer involvement.\\n","Decision":"**Static Code Analysis with ESLint**\\nWe will use ESLint for static code analysis. This tool is easy to integrate into most text editors and implements some fixes automatically, which means errors are fixed before code is committed to our version control system. The tool can also run as part of the Continuous Integration (CI) pipeline. The pipeline job will fail when ESLint identifies errors. Code can only be deployed if pipeline jobs pass. This analysis combined with the gate on failing jobs prevents bad code from being deployed to users.\\n**Static Security Audit of Package Dependencies with Yarn Audit**\\nWe will use Yarn's `audit` command for static security audits. This tool checks for known security vulnerabilities in node modules and is frequently updated to detect new vulnerabilities. As we are already using Yarn for node module package management, using Yarn `audit` lets us avoid additional project complexity. Our CI pipeline will be configured to run this scan automatically each time code is committed to our version control system. The pipeline job will fail when vulnerabilities of a `MODERATE` or higher severity level are found, preventing node modules with known security vulnerabilities from being used in a deployed application.\\n**Dynamic Security Scan with Open Web Application Security Project Zed Attack Proxy (OWASP ZAP)**\\nWe will use OWASP ZAP's baseline scan to test application security. The tool is free, open-source, and updated and released weekly. Our CI pipeline will be configured to run this scan automatically each time code is committed to our version control system. The pipeline job will fail when the scanner finds vulnerabilities marked as `FAIL` in the ZAP configuration file. This pipeline failure prevents vulnerable code from being deployed.\\n","tokens":219,"id":1174}
{"File Name":"fxa\/0008-redis-lua-scripts.md","Context":"## Context and Problem Statement\\nWe currently use a fairly basic mechanism for doing transaction type operations against Redis. We may expand our use of Redis to replace some or all of our MySQL database for OAuth. The expansion will include use cases that require new indexing, join-like, and transactional operations. To support future use cases and make current ones more concise we should evaluate our implementation in `fxa-shared\/redis`.\\n## Decision Drivers\\n- Need for robust redis transactions\\n- Increased use of redis for oauth\\n- Updating module dependencies\\n- move from \"redis\" (last updated 2 years ago) to \"ioredis\" (actively maintained)\\n","Decision":"- Need for robust redis transactions\\n- Increased use of redis for oauth\\n- Updating module dependencies\\n- move from \"redis\" (last updated 2 years ago) to \"ioredis\" (actively maintained)\\nUse lua scripts. The proof-of-concept [PR](https:\/\/github.com\/mozilla\/fxa\/pull\/3278) showed lua as viable and an overall simpler option.\\n### Positive Consequences\\n- Future use cases should be simpler to implement\\n- Less custom Redis glue code and simpler stack traces\\n- Improved maintainability\\n### Negative Consequences\\n- Additional developer cost of understanding lua\\n- Changes to lua scripts require additional consideration with regard to performance. Performance implications should be a checklist item on any lua script change PR, similar to SQL stored procedures.\\n","tokens":134,"id":384}
{"File Name":"mymove\/0063-use-openapi-to-compile-api-specs.md","Context":"## Problem Statement\\nWe have a specification-first development cycle for our APIs. This means that editing our API - adding endpoints,\\nediting responses, changing functionality - starts in the YAML file that contains the API definition. From that, we use\\n`go-swagger` to read our specification and generate Go types for use in our backend.\\n**The good:** With this model, we can focus on the API design without worrying about how to convert that into usable Go\\ncode - `go-swagger` does this for us. Code is neatly organized into separate packages for each API, so they can function\\nindependently.\\n**The problem:** Our APIs are all concerned with the same data models, so even though they are _technically_\\nindependent, they are highly related. We're defining the same objects over and over again in our YAML specs. All APIs\\nhave a shipment, a move, an orders object, and the list goes on. When we make one change to these objects, we have to\\nmake changes to each and every YAML file.\\nThis means our YAML files quickly get out of sync. We've had to deal with bugs stemming from this disconnect many times.\\nThis is also hugely redundant - there are hundreds of lines that are essentially identical in each API.\\nWe have to do a lot of manual type conversions in the backend to turn the Swagger-generated Go types in our general\\nmodel types. These type conversions are also redundant, and they're another place where we can miss changes that add or\\nmodify fields. One possible negative is that having shared types between APIs would threaten their ability to function\\nindependently. However, our APIs are extremely interconnected on the backend and use many of the same services, so there\\nis a question of how independent they could possibly be regardless.\\nLastly, we struggle with maintaining the same standards in each API. Some are more resistant to change, and we don't\\nhave a good method for incrementally standardizing those APIs.\\n","Decision":"### Chosen Alternative: _Use the `openapi` CLI tool to compile shared API definitions (Option 3)_\\nThis looks like the most complicated solution by far. And for the initial implementation, it is. We have already\\nintroduced the `openapi` tool to the project so that we can preview our API documentation, but now we will be dependent\\non it for our development process. We will also have to work in a new folder, so all of our engineers will have to\\nacclimate to the development cycle.\\nHowever, the benefits are significant. The `openapi` compiler dictates a structure that is organized and fairly\\nintuitive, making it easy to create, find, and reference separate definition files. Like option 2, edits to one file can\\napply to all of our APIs. Furthermore, the compiler can handle our files as-is, so we can gradually split our\\ndefinitions as we move forward.\\nUnlike option 2, this method won't change the outward behavior of our APIs. External tools like Load Testing, and\\neventually the Prime integration, won't need to change the way they consume our content. This was ultimately the\\ndeciding factor because, even though this option _looks_ more complicated, the overall impact of the switch will be\\nminimal. Load Testing was also completely non-functional with option 2, and I have not yet figured out how to make it\\nwork.\\n","tokens":409,"id":3079}
{"File Name":"govuk-aws\/0014-launch-config-change-propagation-process.md","Context":"## Context\\nAn Auto-scaling Group's (ASG) instances are all created using a Launch Configuration (LC). The LC manages the image used to build instances as well as any user-data, security groups or tags.\\nWhen the LC of an ASG is changed those changes are only propagated to new instances. New instances are only created if the ASG scales-out or if an existing instance goes away (e.g. is terminated or fails).\\nHistorically many of our instances are long lived (e.g. production puppetmaster, at time of writing, has an uptime of 79 days). This means that changes to the LC may take a long time to propagate fully. In the case of important changes to the LC (e.g. new security groups or tags that add Puppet facts) this is undesirable.\\nThere are several possible ways to force propagation of LC changes within the existing Terraform code were considered ([issue](https:\/\/github.com\/hashicorp\/terraform\/issues\/1552)):\\n* Manual termination of instances\\n* Embedding CloudFormation in Terraform to more tightly manage instance life-cycle ([e.g. see this comment](https:\/\/github.com\/hashicorp\/terraform\/issues\/1552#issuecomment-191847434))\\n* Creating a new ASG for every LC ([e.g. see this post](https:\/\/groups.google.com\/forum\/#!msg\/terraform-tool\/7Gdhv1OAc80\/iNQ93riiLwAJ))\\n","Decision":"In order to propagate LC changes we will manually terminate effected instances.\\n","tokens":306,"id":4048}
{"File Name":"verify-frontend\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":1450}
{"File Name":"opg-digideps\/0003-use-aws-parameter-store-for-feature-flags.md","Context":"## Context\\nWe want to use feature flags in the service to allow us to easily enable and disable functionality. Feature flags should be easy to change, audited and able to take effect immediately.\\n","Decision":"We will use AWS Parameter Store to store our feature flags in a standardised format: `\/{environment}\/flag\/{flagName}`. This will make flags easy to identify, change and debug. Operators will have access to change the value of these flags.\\nResources inside our service can access feature flags either by having them passed in as environment variables, or by directly querying Parameter Store on-demand.\\nParameter Store values must be strings, so we will consistently use the values `0` (off) and `1` (on).\\n","tokens":40,"id":4410}
{"File Name":"generator-latex-template\/0006-use-single-quote-to-enquote.md","Context":"## Context and Problem Statement\\nIn a document, some words have to be put in quotes. How to direct latex to enquote a word?\\n## Decision Drivers\\n* Automatic correct typographical layout\\n* Less effort for the user\\n* Supported by prominent LaTeX editors (overleaf, vs.code, ...)\\n* Supported by standard LaTeX environments\\n","Decision":"* Automatic correct typographical layout\\n* Less effort for the user\\n* Supported by prominent LaTeX editors (overleaf, vs.code, ...)\\n* Supported by standard LaTeX environments\\nChosen option: \"Use single quote (\") to enquote text\", because resolves all forces.\\nWe accept that special hyphenation instructions such as `application\"=specific` do not work anymore.\\n<!-- markdownlint-disable-file MD013 -->\\n","tokens":71,"id":2333}
{"File Name":"external-service-operator\/0006-only-one-probe-of-http-tcp-is-possible.md","Context":"## Context\\nThe Kubernetes Probe Resource Structure would allow adding multiple types of Healthchecks at once. This would be:\\n* exec\\n* httpGet\\n* tcpSocket\\nHowever currently v1.18 v1.20 when one tries to add more than one Type, like:\\n```\\nreadinessProbe:\\nexec:\\ncommand:\\n- \"\/usr\/bin\/sh\"\\n- \"-c\"\\n- \"echo Hello World\"\\nhttpGet:\\nhost: localhost\\nport: 80\\ntcpSocket:\\nhost: localhost\\nport: 80\\n```\\nkubernetes will not validate the Resource with following message:\\n```\\n# pods \"test-probes\" was not valid:\\n# * spec.containers[0].readinessProbe.httpGet: Forbidden: may not specify more than 1 handler type\\n# * spec.containers[0].readinessProbe.tcpSocket: Forbidden: may not specify more than 1 handler type\\n```\\n","Decision":"It is decided to go with the same logic and ensure that only one type will be accepted, so the Operator is not supposed to handle the case more than one Types are given.\\nFurther it follows the same logic and will use and check HTTP first than TCP last. Exec healthchecks don't make sense and will be ignored completly\\n","tokens":202,"id":1734}
{"File Name":"iampeterbanjo.com\/0014-better-data-transformations.md","Context":"## Context\\nCreating new and updating existing data transformations, whether from APIs or the database, is still a pain point. After many refactors I feel that the abstractions I am using are not helping. The goal for the data transformations were:\\n- Take data from an API or database\\n- Modify or enrich it\\n- Save or display the results to the user\\nTo achieve this, there are two abstractions that I rely on:\\n- Hapi plugins: an interface defined by Hapi server that can add features to the server object.\\n- Plugin helpers: ad hoc functions that I use to decompose plugin actions.\\nThe problem with these are:\\n- Mixing of concerns: data transformation and server processes need different abstractions because they should be able to change independently. As it is, I have coupled them together.\\n- Broken interfaces: related to the first point, [Hapi plugins][hapi-plugins] have an interface that makes them useful for the server. Trying to use the same for creating a data pipeline is very \"hacky\" - open to frequent changes and bugs.\\n- There is a better way: data transformations will benefit from a different abstraction solving the current problem and opening up new possibilities.\\nTwo abstractions present themselves:\\n- Transducers\\n- Streams\\nThey both create programming interfaces that allow chaining of operations on a data set: e.g. `A |> B |> C` where the input for the next step is the output of the previous.\\nTransducers are an abstraction implemented by 3rd party libraries while streams are native to [NodeJs][nodejs-streams]. In practice, I would use a library for either approach to deal with [possible pain points][nodejs-streams-readable-streams].\\n","Decision":"In the context of lacking a well established method to build data pipelines in JavaScript, and facing the concern of fragile data transformations then use Streams. They can be used for large files since they have a low memory footprint and can have other uses e.g. request\/response streams.\\n","tokens":359,"id":1591}
{"File Name":"qc-atlas\/0009-joined-table-for-knowledge-artifact.md","Context":"## Context and Problem Statement\\nThe class `KnowledgeArtifact` is a helper base class that was not intended to have its own database table (see Issue [#182](https:\/\/github.com\/UST-QuAntiL\/qc-atlas\/issues\/182)).\\nThe current implementation as a joined table generates a database table.\\nShould we keep the current joined table implementation for `KnowledgeArtifact`?\\n## Decision Drivers <!-- optional -->\\n* The `KnowledgeArtifact` table is referenced by a foreign key from `DiscussionTopic`\\n","Decision":"* The `KnowledgeArtifact` table is referenced by a foreign key from `DiscussionTopic`\\nChosen option: \"[Joined table]\", because it is already implemented, allows for references in both ways and has no significant downside.\\n### Positive Consequences <!-- optional -->\\n* The current implementation can stay\\n","tokens":107,"id":691}
{"File Name":"handbook\/0010-cms.md","Context":"## Context and Problem Statement\\nisland.is will be maintaining and publishing content from many different government agencies and institutions. Their technical skill may vary a great deal, the content skill may also be lacking, therefore it is paramount for the system to be user friendly and intuitive.\\nAgencies and institutions should have enough autonomy with regards to editing content they are responsible for, to minimise the manual labour required by the island.is editors.\\nWhich CMS system would best suit the needs of island.is?\\n## Decision Drivers\\n- Content needs to be editable by non technical users\\n- Content needs to be accessible across multiple domains and platforms\\n- Setup should be simple for developers new to the project\\n- The system should manage flexible content structures to limit systems impact on design\\n- The system should be user friendly and easy to use for a non technical person\\n- The system needs to offer a suitable workflow option to ease content management once multiple agencies start to contribute\\n","Decision":"- Content needs to be editable by non technical users\\n- Content needs to be accessible across multiple domains and platforms\\n- Setup should be simple for developers new to the project\\n- The system should manage flexible content structures to limit systems impact on design\\n- The system should be user friendly and easy to use for a non technical person\\n- The system needs to offer a suitable workflow option to ease content management once multiple agencies start to contribute\\nDevs narrowed the choice down to two options Contentful and Contentstack.\\nBoth systems meet the required featureset.\\nA decision from management was made to use Contentful.\\nContentful is deemed to have a larger presence in the Icelandic dev community.\\nContentful is also believed to have a stronger funding base.\\nContentful is already implemented in some of our projects.\\n","tokens":191,"id":2000}
{"File Name":"lobiani\/0010-default-to-synchronous-command-handling-in-admin.md","Context":"## Context\\nIn [ADR 5](0005-use-cqrs-architectural-style.md) we decided to adopt CQRS architectural style. Therefore, we need a\\nstrategy, or a guideline for handling commands issued to admin sub-system. Couple of things we can take into the\\naccount:\\n- Admin tool is intended for \"internal use\" only, i.e. it won't be exposed to the wild (customers)\\n- There is no strict requirement about predictable and stable latency\\n","Decision":"Commands issued to admin will be handled synchronously by default. An asynchronous mode is still allowed though for\\nexceptional cases only.\\n","tokens":102,"id":1040}
{"File Name":"content-data-admin\/adr-000-document-architectural-decisions.md","Context":"## Context\\nWe aim to:\\n- Make it easier to understand the codebase and its status\\n- Reduce the number of meetings to handover information across teams\\n- Facilitate team rotations across GOV.UK\\n","Decision":"Track architectural decision that impact the status of the Content Data Admin, [following a lightweight format: ADR][1]\\n","tokens":46,"id":5133}
{"File Name":"cljdoc\/0016-prefer-polling-over-webhooks.md","Context":"## Context\\ncljdoc uses CircleCI as a sandbox to run analysis on projects as\\noutlined in [ADR #0008](0008-use-circleci-as-analysis-sandbox.md).\\nThe process currenlty works like this:\\n1. cljdoc queues a build\\n1. CircleCI eventually runs the build\\n1. cljdoc is notified via a webhook\\nThis approach is nice because we don't need to constantly check the status\\nof a build until it eventually finishes but it also has a few drawbacks:\\n- Testing anything involving webhooks in a local development\\nenvironment is a serious pain\\n- Code related to running the analysis and storing it's result are\\nspread accross multiple places making the code harder to follow and\\nmaintain\\n","Decision":"Move to a model where the respective analysis service (CircleCI or Local)\\nexposes a blocking interface for running the analysis.\\nFor CircleCI this blocking interface will simply poll the CircleCI build until\\nit reached [lifecycle `\"finished\"`](https:\/\/circleci.com\/docs\/api\/v1-reference\/#build).\\n","tokens":156,"id":2606}
{"File Name":"lbh-adrs\/Observability.md","Context":"## **Context**\\nProviding rich logging information will make it easier to investigate issues without making use of intrusive approaches (i.e: debug, memory dump), also making visible the behaviour of services by using monitoring tools to extract and\/or query these logs.\\nThe idea is to utilize services offered by AWS as they are comprehensive and can operate at scale with minimal administrative overhead.\\n","Decision":"**AWS x-ray**\\nAWS X-Ray is an AWS managed service that provides the functionality to debug and analyze distributed applications.\\n","tokens":76,"id":2320}
{"File Name":"js-sdk\/0009-add-logs-in-trc-and-nginx-container.md","Context":"## Context\\nAdd feature to stream logs from trc and nginx container for better debugging\\n","Decision":"Update the nginx and trc flists to use zinit and redirect logs to stdout to be streamed from redis\\n","tokens":18,"id":5192}
{"File Name":"unfinished-design-system\/005-lerna.md","Context":"* [Context](#context)\\n* [Decision](#decision)\\n* [Status](#status)\\n* [Consequences](#consequences)\\n* [More reading](#more-reading)\\n* [Updates](#updates)\\n","Decision":"* [Status](#status)\\n* [Consequences](#consequences)\\n* [More reading](#more-reading)\\n* [Updates](#updates)\\nWe've decided to use [Lerna](https:\/\/lerna.js.org\/) as the tool for our deployment. It has incredible features (such as automatic versioning), and it helped us a lot.\\nOur team has already used it in previous projects, so it will easy to set up and keep that tool.\\n","tokens":53,"id":4548}
{"File Name":"midashboard-infrastructure\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":344}
{"File Name":"react\/adr-001-typescript.md","Context":"## Context\\nPrimer React components was originally released without TypeScript type definitions, making it difficult for engineers to consume the library in TypeScript applications. In [July 2019](https:\/\/github.com\/primer\/react\/commit\/2983c935ea9ad600c04078adb25e40c3624c11fa#diff-7aa4473ede4abd9ec099e87fec67fd57afafaf39e05d493ab4533acc38547eb8), we created an [ambient declaration](https:\/\/www.geeksforgeeks.org\/typescript-ambients-declaration\/) file (`index.d.ts`) file to provide type definitions for TypeScript applications without having to rewrite Primer React components in TypeScript.\\n`index.d.ts` has been an effective stopgap, enabling teams to build complex applications with Primer React components and TypeScript. However, because `index.d.ts` is disconnected from the implementation code, we've struggled to keep the type definitions up-to-date and accurate, as evidenced by [many](https:\/\/github.com\/primer\/react\/issues\/906) [TypeScript](https:\/\/github.com\/primer\/react\/issues\/540) [bug](https:\/\/github.com\/primer\/react\/issues\/520) [reports](https:\/\/github.com\/primer\/react\/issues\/534). As the library continues to grow in size and complexity, manually maintaining type definitions will become unsustainable.\\n","Decision":"We will rewrite Primer React components in TypeScript.\\n","tokens":284,"id":101}
{"File Name":"buy-for-your-school\/0016-use-rubocop-govuk-for-code-linting.md","Context":"## Context\\nGov.uk projects should maintain consistent [code formatting](https:\/\/gds-way.cloudapps.digital\/manuals\/programming-languages\/ruby.html#code-formatting)\\n","Decision":"Change from [Standard.rb](https:\/\/github.com\/testdouble\/standard), which is a wrapper around [Rubocop](),\\nto the [GovUK maintained version](https:\/\/github.com\/alphagov\/rubocop-govuk).\\n","tokens":36,"id":1256}
{"File Name":"continuouspipe\/0003-merge-the-builder-micro-service-within-river.md","Context":"## Context\\nWe are running 12 micro-services, a lot of them are Symfony applications and we operate them differently.\\n","Decision":"We are going to merge some into this river application.\\n","tokens":25,"id":4267}
{"File Name":"cloud-platform\/002-Use-github-for-architecture-decision-record.md","Context":"## Context\\nThe cloud platforms team has had a number of discussions about where to hold documentation. We have tried using confluence for technical documentation but it has largely gone stale through lack of updates.\\nFor the development of the new platform we want to keep _technical_ documentation close to the code that implements that documentation. An example of this is the [kubernetes investigations](https:\/\/github.com\/ministryofjustice\/kubernetes-investigations) repo which holds our experiments into the use of kubernetes.\\nPutting technical documentation in GitHub has additional benefits:\\n* Using pull requests gives us a clear review and approval process\\n* It is part of the same workflow as other activities that we do on a day to day basis (e.g. writing code)\\n* The information can be held in the open and viewed by anyone\\n","Decision":"1. Our architecture decision log for the new cloud platform will be held in GitHub\\n","tokens":167,"id":626}
{"File Name":"cloud-sdk-js\/0001-circuit-breaker-options.md","Context":"## Context: What Triggered this Discussion?\\nWe recently decided to increase the default timeout of our circuit breakers to 10 seconds.\\nThis is pretty high for fail fast but necessary to prevent some requests from failing.\\nThe underlying issue is currently that we cannot simply make timeout into a parameter, because that would mean creating a new circuit breaker which would dismiss all of the state the previous one accumulated.\\nMore generally: circuit breakers inherently need state to decide when to e.g. open a circuit.\\n","Decision":"We currently see no customer demand for a feature like this.\\nWe will revisit this topic if necessary.\\n","tokens":103,"id":3623}
{"File Name":"dogma\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":1621}
{"File Name":"fxa\/0014-auth-for-settings-redesign.md","Context":"## Context and Problem Statement\\nThe Settings Redesign app needs to query & mutate the same protected user data as the existing settings app hosted on content-server. This will require some form of authentication & authorization to manage that data.\\n## Decision Drivers\\n- Smooth UX\\n- Security\\n- Development velocity\\n- Ease of integration\\n","Decision":"- Smooth UX\\n- Security\\n- Development velocity\\n- Ease of integration\\nChosen option: \"Authenticate via OAuth2 to use auth-server APIs\", because:\\n- It requires minimal changes to existing auth-server implementation and infrastructure.\\n- It relies on an authentication mechanism with relatively well-known security properties.\\n### Positive Consequences\\n- The OAuth2 access token mechanism is better suited to this purpose than a novel scheme to share session token credentials.\\n- The new Settings Redesign app can use existing auth-server APIs with minimal modifications to accept scoped OAuth2 access tokens. This can constrain most of the novelty in the project to the redesigned & reimplemented settings UX.\\n### Negative Consequences\\n- We will need to modify auth strategies in auth-server APIs, taking care not to affect existing usage in the production settings app.\\n- We don't have an entirely greenfield project, which could be a bit of a drag. But, rediscovering lessons learned in existing code can also be a drag.\\n","tokens":66,"id":382}
{"File Name":"devops-tooling-docs\/0002-pull-acs-info-out-of-modules.md","Context":"## Context and Problem Statement\\nUsing acs-info inside our standard and component modules is creating pain. Does it make more sense to expect products to pass the necessary variables in, allowing them to use acs-info if they want?\\n## Decision Drivers <!-- optional -->\\n* Terraform's lack of optimization with modules and data sources\\n* Usage of modules in non-OIT accounts\\n* We want our modules to be simple to use\\n* We want development teams to understand the infrastructure of their products.\\n","Decision":"* Terraform's lack of optimization with modules and data sources\\n* Usage of modules in non-OIT accounts\\n* We want our modules to be simple to use\\n* We want development teams to understand the infrastructure of their products.\\nChosen option: \"Pull acs-info out of modules\", because it was the best option based on an analysis of pros and cons (see below).\\n<!-- ### Positive Consequences optional -->\\n<!-- * TBD -->\\n<!-- ### Negative Consequences optional -->\\n<!-- * TBD -->\\n","tokens":103,"id":4074}
{"File Name":"buy-for-your-school\/0008-use-brakeman-for-security-analysis.md","Context":"## Context\\nWe need a mechanism for highlighting security vulnerabilities in our code before it reaches production environments\\n","Decision":"Use the [Brakeman](https:\/\/brakemanscanner.org\/) static security analysis tool to find vulnerabilities in development and test\\n","tokens":20,"id":1263}
{"File Name":"commercetools-adyen-integration\/0003-matching-adyen-notification-in-multitenancy.md","Context":"## Context\\nIn a multitenancy setup, there are multiple commercetools projects in one notification module instance.\\nWhen a notification comes from Adyen, the notification module needs to find the correct commercetools project for the notification.\\nThere are 2 possible options to tackle this problem:\\n1. The commercetools project name is a part of the notification callback URL (e.g. URL path) that is set up in Adyen Admin.\\n1. The commercetools project name is an attribute in the notification itself.\\nThe solution should be as less work as possible for the user of the integration. Additionally, it should work in a serverless environment where the URL to the function could be randomly generated.\\n","Decision":"- We will have an attribute in the notification itself. Its value will be the commercetools project name that the notification belongs to.\\n- In order to have the commercetools project name in every notification, we will use [`metadata` attribute](https:\/\/docs.adyen.com\/api-explorer\/#\/CheckoutService\/v66\/post\/payments__reqParam_metadata). This attribute will be set in the extension module in every request to Adyen.\\n- The value for `metadata` attribute will be taken from a custom field `custom.fields.commercetoolsProjectKey`. Its value will be set by the user of the integration.\\n","tokens":148,"id":1716}
{"File Name":"seda-frontend\/0002-using-local-state-in-settings-module.md","Context":"## Context\\nSome of the data that is required by the application is only needed in specific modules. Till now, `redux` has been relied on heavily and most of times for good reason. Some data, however, is only needed in specific parts of the application, but is still stored in the global store or is kept in a reducer on a per-component basis.\\nDifferent parts of the application have their own saga, reducer, actions and selectors which makes the application more difficult to understand, error prone and maintenance harder to keep up.\\nStoring all data in the global store requires a lot of (duplicate) boilerplate code, tests and mocking.\\n","Decision":"The structure of the application's state needs to reflect the data that is globally required. If specific data is only needed in specific parts of the application, that application's part should provide the data through a reducer and a context provider and not make use of the global (`redux`) store.\\nEssentially, the entire application's state can be provided by a context provider, but for now we'll take the bottom-up approach and gradually refactor and introduce the reducer\/context approach in favour of `redux` complexity.\\n","tokens":133,"id":314}
{"File Name":"my-notes\/stylelint.md","Context":"### Context\\n- linting for CSS\\n- After running it, it didn't pick up a single error\\n### Decision\\nDon't start using stylelint\\n","Decision":"Don't start using stylelint\\n","tokens":33,"id":5084}
{"File Name":"ftd-scratch3-offline\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":2642}
{"File Name":"drt-v2\/0012-serialize-akka-persistence-with-protobuf.md","Context":"## Context\\nFor akka persistence to be resilient to message schema changes you can't\/shouldn't rely on java  serialization.\\nThe big 3 serialization approaches would be:\\njson\\nprotobuf\\nthrift\\nhttp:\/\/blog.codeclimate.com\/blog\/2014\/06\/05\/choose-protocol-buffers\/\\n","Decision":"Use google's protobuf for serialization\\n","tokens":65,"id":1905}
{"File Name":"fxa\/0002-use-react-redux-and-typescript-for-subscription-management-pages.md","Context":"## Context and Problem Statement\\nA solution for isolating third-party payment widgets from the rest of account\\nmanagement can include building a separate web app on its own dedicated domain.\\nAs a side effect in building that app, we have the opportunity to choose\\ntechnologies for building it that don't necessarily follow the rest of FxA.\\n## Decision Drivers\\n* Opportunity for a fresh start with tech stack without rewriting\\n* Security in dealing with payment transactions\\n* Developer ergonomics\\n* Code quality & testing\\n* Subscription services deadlines\\n","Decision":"* Opportunity for a fresh start with tech stack without rewriting\\n* Security in dealing with payment transactions\\n* Developer ergonomics\\n* Code quality & testing\\n* Subscription services deadlines\\nChosen option: \"React, Redux, Typescript\", because\\n* Chance to start with a fresh stack\\n* More vibrant ecosystem\\n* Better tooling & developer ergonomics\\n","tokens":111,"id":372}
{"File Name":"meadow\/0003-terraform.md","Context":"## Context\\nPreviously we've kept Terraform code in a separate Github repository (nulterra) and this has had the effect of making changes more cumbersome.\\n","Decision":"We've decided to keep Meadow Terraform code inside the Meadow repository in a root directory called `terraform`.\\n","tokens":33,"id":3847}
{"File Name":"island.is\/0007-viskuausan-static-site-generator.md","Context":"## Context and Problem Statement\\nViskuausan is proving to be more complex and larger platform than just a simple documentation site from static content. Which React framework provides the most out-of-the-box features that we need?\\n## Decision Drivers\\n- Should use NodeJS and React as outlined in [S\u00cd technical direction](..\/..\/technical-direction.md)\\n- Should be able to support markdown content rendered to HTML\\n- Should be open source\\n- Should be customizable to island.is UI design\\n","Decision":"- Should use NodeJS and React as outlined in [S\u00cd technical direction](..\/..\/technical-direction.md)\\n- Should be able to support markdown content rendered to HTML\\n- Should be open source\\n- Should be customizable to island.is UI design\\nChosen option: NextJS + NestJS\\nNextJS is the chosen web framework for all island.is websites needing server side rendering. As Viskuausan will probably be merged with island.is main website, creating it using same frameworks makes it easy to merge later on. It is easier to reuse Island UI components using NextJS over Docusaurus. Docusaurus main advantage over Next is out-of-the-box markdown support but it is easy to add markdown support in NextJS using [Remark](https:\/\/github.com\/remarkjs\/remark) library.\\nNestJS is used to create backend services and Viskuausan needs few backend services related to the X-Road and API GW integrations. Provides functionalities like ORM, dependency injection, unit testing.\\n","tokens":99,"id":1112}
{"File Name":"testy\/0006-avoid-offensive-vocabulary.md","Context":"## Context\\nThere is an initiative about questioning the technical vocabulary we use, and avoiding some words that are widely used\\nand may offend people. This is a project that adheres to this initiative, therefore...\\n","Decision":"Remove all existing technical vocabulary that might be offensive, and prevent those terms to be added in the future.\\nFor instance, the use of \"master\/slave\" replaced by \"main\/replica\" (or similar), or \"whitelist\/blacklist\" by\\n\"safelist\/blocklist\" (or similar).\\n","tokens":43,"id":1047}
{"File Name":"libelektra\/functions_with_buffers.md","Context":"## Problem\\nCurrently the way functions like keyGetName() work is by passing a buffer with\\na maxSize and if the buffer is large enough, the value gets copied into the\\nbuffer. This leads to the user having to write a lot of surrounding boilerplate\\ncode, checking for the size of every value \/ name they want to copy into a buffer.\\n","Decision":"- Remove Functions:\\n- keyGetName()\\n- keyGetUnescapedName()\\n- keyGetBaseName()\\n- keyGetString()\\n- keyGetBinary()\\n- add documentation in API docu about life-time and add in release notes that you should use strncpy() \/ memcpy() instead:\\n```c\\n\/\/ str values\\nstrncpy(..., keyName (k), ...)\\n\/\/ binary values\\nmemcpy(..., keyValue (k), ...)\\n```\\n","tokens":74,"id":1313}
{"File Name":"tracking-consent-frontend\/0009-create-a-library-for-reading-the-user-consent-cookie.md","Context":"## Context and Problem Statement\\nTracking consent stores cookie consent information in a userConsent cookie as URL-encoded JSON. This information is available\\nto frontend microservices in Javascript running in a user's browser and in the request headers passed to the Scala\\nPlay controllers running on the server-side.\\nBefore frontend services can interpret the cookie, they need to decode and parse it using a Scala JSON library. They also need\\nto apply some business logic to ensure in cases of uncertainty, the user is, by default, assumed to have rejected cookies.\\nFor Javascript code, we propose to make the existing UserPreferences object available in the window object. This will mean\\nservices integrating with tracking consent will have a method available to them for parsing the cookie using the latest\\nbusiness logic. Additionally, they will have a method for subscribing and being notified about changes to cookie preferences in real-time.\\nFor Scala code, the situation is more problematic. Services will only have the raw userConsent cookie itself passed down in the `Cookie` header.\\nTo avoid teams having to copy and paste substantial quantities of Scala boilerplate replicating the existing Javascript logic\\nacross services, should we look to centralise this logic so it can be more easily maintained across the platform and if so how?\\nIdeally we would like a Scala controller to be able to do something like:\\n```scala\\nprivate val themeCookie = Cookie(name = \"theme\", value = \"dark\", maxAge = Some(365.days.toSeconds.toInt))\\nprivate def allowedCookies(implicit request: RequestHeader) =\\nif (userPreferences.preferences.settings) Seq(themeCookie) else Seq.empty\\nval termsAndConditions: Action[AnyContent] = Action { implicit request =>\\nOk(termsAndConditionsPage()).withCookies(allowedCookies: _*)\\n}\\n```\\n## Decision Drivers\\n* the need to keep repetitious boilerplate to a minimum, with changes needed of the order of a single line code change.\\n* the need to centralise the maintenance of the cookie reading logic, so it is easier to roll out changes\\n* the importance of having loosely coupled microservices and avoidance of unnecessarily introducing additional hard\\nnetworked dependencies into microservices\\n* the need for simplicity and for the fewest number of moving parts\\n* the preference for not introducing a requirement for microservices to add additional libraries\\n","Decision":"* the need to keep repetitious boilerplate to a minimum, with changes needed of the order of a single line code change.\\n* the need to centralise the maintenance of the cookie reading logic, so it is easier to roll out changes\\n* the importance of having loosely coupled microservices and avoidance of unnecessarily introducing additional hard\\nnetworked dependencies into microservices\\n* the need for simplicity and for the fewest number of moving parts\\n* the preference for not introducing a requirement for microservices to add additional libraries\\nChosen option 3: because it meets most if not all the above criteria.\\n### Positive Consequences\\n* Services can determine acceptance or rejection of non-essential cookies in a single line of code.\\n* Changes to the cookie parsing logic can be rolled out relatively easily by #team-plat-ui\\n* Consuming services do not rely on any additional networked dependencies\\n* No additional libraries would be needed (assuming the micro-library is added to play-frontend-hmrc or bootstrap-play)\\n### Negative Consequences\\n* Any version change to the userConsent cookie has the potential to break functionality relying on non-essential cookies\\nin consuming services. We may need to look at introducing the ability to have the concept of minor versions or only increment the version for breaking\\nchanges e.g. removal or change to the meaning of a cookie type.\\n","tokens":486,"id":3523}
{"File Name":"iampeterbanjo.com\/0003-hapijs-server.md","Context":"## Context\\n[ExpressJS][1] is the default server for most NodeJS projects. However I have found that it does not include the features that I would prefer -\\n- Robust and extensible architecture to decompose a project\\n- Handle common use cases like file uploads\\n- Nice error handling e.g. 404, route collisions\\n- Easy to test server behaviour including middleware\\n[HapiJS][2] has the above features and\\n- All essential dependencies are maintained by the same team\\n- Accessible and friendly community\\n[Read more][3] about HapiJS\\n","Decision":"In the context of a fullstack NodeJS project and facing the concern of making better architecture decisions, I decided to use HapiJS to achieve better quality measured by test coverage and composable architecture. I accept that I will have to rely on documentation and source code more since HapiJS is not as well written about because of its lower popularity.\\n","tokens":122,"id":1597}
{"File Name":"gsp\/ADR040-cluster-stability-node-replacement.md","Context":"## Context\\nSREs sometimes need to change various things about a cluster's worker nodes, and therefore replace all of the nodes:\\n* AMI (e.g. for an EKS update)\\n* instance type\\n* anything else in the launch template in some way (e.g. instance role)\\nIn addition to this, in our Terraform we get the latest AMI ID for our EKS version from AWS - this means that as soon as AWS releases a new AMI for our current EKS version, the next time the cluster's deployer step runs for Terraform it will also replace all the nodes.\\nWe have a CloudFormation UpdatePolicy on our worker node auto-scaling-groups that tells it to replace 1 node at a time in each ASG. This would be fine if we didn't have 3 ASGs with a small number of nodes in each - right now this practice terminates too many instances at once and results in the cluster becoming unstable. This in turn causes outages in the applications running in the cluster.\\nWithout such a policy we'd be able to update the launch template for the ASG and it wouldn't remove any existing nodes, just set up any new future nodes correctly.\\nThe reason we have 3 small ASGs is so the cluster auto-scaler can scale nodes independently in each AZ - it will know that a pod attempting (and failing) to schedule is tied to a persistent volume claim in a particular availability zone and can scale up the ASG for that zone.\\n","Decision":"We will do action 3 (lifecycle hooks) (and therefore not action 2 (some other node rolling process)).  This will mean that the ASG will know how to wait for nodes to be ready in the cluster when launching, and how to wait for nodes to be drained before terminating.  It also means we can choose our method of node rolling with relative freedom.  We will keep our existing CloudFormation UpdatePolicy as our actual mechanism of rolling nodes.\\nWe will do action 4 (turn off autoscaler while rolling nodes).  This will probably need to be a script run before we run terraform (to scale the autoscaler down) and afterwards (to scale the autoscaler back up).\\n","tokens":311,"id":3906}
{"File Name":"digital-paper-edit-infrastructure\/2019-04-23-transcript-architecture.md","Context":"## Context and Problem Statement\\nDigital Paper Edit has three milestones. Transcript, Paper-edit, and Annotation.\\nWe want to make sure that during the Transcript phase, we have a solid\\narchitecture to provide storage for uploaded content and transcription to be\\nused in later phases. We want to consider technologies from a resilience,\\nscalability, cost point of view, opensource compatibility and extensibility.\\n## Decision Drivers\\n* resilience\\n* scalability\\n* cost\\n* number of users\\n* opensource compatibility\\n* extensibility\\n","Decision":"* resilience\\n* scalability\\n* cost\\n* number of users\\n* opensource compatibility\\n* extensibility\\nWe are going for **Option 3**.\\nOption 1 (only EC2)\\n![Option 1 (only EC2)](.\/dpe-transcript-EC2.png)\\nOption 2 (EC2, Lambda and gateway)\\n![Option 2 (EC2, Lambda and gateway)](.\/dpe-transcript-EC2_Lambda.png)\\nOption 3 (EC2, Lambda, SNS, and SQS)\\n![Option 3 (EC2, Lambda, SNS, and SQS)](.\/dpe-transcript-fleshed-out.png)\\nAll options will have the advantages of:\\n* being maintainable,\\n* being transparent\\n* being transferable\\n* using existing pipelines\\n* It will have the benefits of using Cosmos, which means we will automatically\\nhave cert-based security and ELBs. We will also have a solid CI process around\\nthe project, including transparency around deployment issues.\\nAll options will have the disadvantages of:\\n* Initial learning curve around Cosmos (need to understand RHE7).\\n* Extra files for Cosmos will be in an opensource repository, which will require\\nadditional documentation and security checking.\\n* Cost of AWS - not entirely sure how much it will be right now, but it will be\\na minimum of 4 instances running in `t2.small` for Option 1, and 2\\n`t2.small` instances for Option 2, plus lambda executions as well as SQS and\\nS3.\\n* Locking in with AWS.\\n### Option 1 (only EC2)\\n#### Advantages\\n* Common pattern and will be easier to set up locally.\\nThis is good for the opensource developers - who will not need AWS accounts to\\ntest integration locally with Lambdas and Gateways.\\n#### Disadvantages\\n* Operational concerns that might be unnecessary\\n### Option 2 (EC2, Lambda and gateway)\\n#### Advantages\\n* Cheaper as there isn't a full instance running - Lambda.\\n* Better for the environment (less operational CO2 cost)\\n#### Disadvantages\\n* There could be a time-out issue for Lambda\\n* Difficulty in debugging due to Lambda\\n* Operating System is abstracted away (loss of control)\\n### Option 3 (EC2, Lambda, SNS, and SQS)\\nThis has been developed two months after. The architecture is fleshed out in this option.\\nIt shows, clearer responsibilities with defined interfaces.\\nWe've added:\\n1. SNS and SQS (fanout pattern) for reliable job delivery to microservices.\\n2. Each microservice to have a queue for asymmetrical queue growth.\\n3. S3 signed URL communication via API, rather than direct upload from API. This will improve data transmission from different components.\\n4. STT Queue jobs to be published by Audio FFMPEG Service, subscribed STT client (proxy), and pushed to STT service, treated as a black box.\\n5. On completion of STT task, the client will update the API directly.\\nThe video preview section is tinted with yellow as it is not currently in-scope.\\n#### Advantages\\n* Fault tolerance from using queues\\n* Avoiding timeout issues with Lambdas for running long jobs\\n* A uniform interface across the media processing microservices (polling queues, posting to the API)\\n* Upload is not via the API, which supposedly improves data transmission time.\\n... and benefits from the previous options.\\n#### Disadvantages\\n* Additional components and technologies\\n* Polling - although cost is negligible with optimisation.\\n","tokens":110,"id":3537}
{"File Name":"modular-monolith-with-ddd\/0008-allow-return-result-after-command-processing.md","Context":"## Context\\nThe theory of the CQRS and the CQS principle says that we should not return any information as the result of Command processing. The result should be always \"void\". However, sometimes we need to return some data immediately as part of the same request.\\n","Decision":"We decided to allow in some cases return results after command processing. Especially, when we create something and we need to return the ID of created object or don't know if request is Command or Query (like Authentication).\\n","tokens":55,"id":890}
{"File Name":"mongodb-updater\/0003-host-two-mongodb-updaters-in-stack.md","Context":"## Context\\nTwo different MongoDBs need to be updated on a daily basis.\\n","Decision":"Given the small number of databases to be updated both services will be hosted in the same stack, rather than manually create a stack for each database updater.\\n","tokens":17,"id":1127}
{"File Name":"ski-resort-manager\/0004-backend-for-frontend.md","Context":"## Context\\nThis application will have 2 front end and possibly a mobile app.\\nThese differents access will use common and specifics API. Somes datas will be use differently.\\n","Decision":"Every front end will have a specific backend to manage the differents needs.\\n","tokens":38,"id":2812}
{"File Name":"architecture-decision-log\/0012-linting.md","Context":"* [Context](#context)\\n* [Decision](#decision)\\n* [Status](#status)\\n* [Consequences](#consequences)\\n","Decision":"* [Status](#status)\\n* [Consequences](#consequences)\\nWe've decided to use a linter to avoid this problem, no matter the project size. Each language has its way of linting code, but every stack has at least a single tool to solve this problem.\\nAlso, alongside our linters, we're going to use [editorconfig](https:\/\/editorconfig.org\/), a great tool to standardize how we configure our text editors.\\nSome linters can extend properties. If your language linter can do so, we suggest creating a company-wide configuration and extend it in your application.\\nBelow, we've listed the linters we're using for each language:\\n* **Javascript (React, Node, Typescript, ...):** [ESLint](https:\/\/eslint.org\/) + [Prettier](https:\/\/prettier.io)\\n","tokens":34,"id":4912}
{"File Name":"beis-report-official-development-assistance\/0003-use-standard-rb.md","Context":"## Context\\nWe need to make sure our code is written in a standard style for clarity, consistency across a project and to avoid back and forth between developers about code style.\\n","Decision":"We will use [Standard.rb](https:\/\/github.com\/testdouble\/standard) and run the standard.rb rake task to lint the code as part of the test suite.\\n","tokens":36,"id":2382}
{"File Name":"io-pagopa-proxy\/0002-use-uuid-v1-for-sequential-requests.md","Context":"## Context\\nWe have to send requests to a server that requires unique Request IDs for each message.\\nSo, we need to generate uuids.\\n","Decision":"We decided to use uuid library and generate unique uuids based on timestamp (Version 1):\\nhttps:\/\/www.npmjs.com\/package\/uuid\\n","tokens":31,"id":634}
{"File Name":"polarbears\/ADR-2.md","Context":"## Context\\nWhen the delivery people deliver meals to the smart fridges or kiosks, they update the system with the delivery information (the location and the meals).\\nTo determine the inventory at a certain location, the Location Inventory component has to reconcile between the two sources of truths: 1) the information fed\\nto the system by the delivery people 2) the inventory reported by the external Fridge\/Toast APIs.\\nUpon receiving the delivery confirmation, the Location Inventory component can 1) query the Fridge\/Toast APIs and update its data upon synchronous\\nreconciliation 2) asynchronously query the Fridge\/Toast APIs and reconcile.\\n","Decision":"We will use asynchronous communication between the Location Inventory and the External Fridge\/Toast APIs for reconciliation.\\nWaiting for the synchronous reconciliation at a location will increase the time delivery people spend at a location and\\neven may not be possible if the External Fridge\/Toast Inventory APIs are down for some reason.\\n","tokens":134,"id":4648}
{"File Name":"static\/0002-add-enhanced-ecommerce.md","Context":"## Context\\nRendering apps track pageviews and events to help us measure the performance\\nof GOV.UK.\\nStatic uses an [abstraction layer](https:\/\/github.com\/alphagov\/govuk_frontend_toolkit\/blob\/master\/docs\/analytics.md) in GOV.UK Frontend Toolkit, which reports the data to Google Analytics (but the underlying provider could be changed).\\nFor analysing search performance, we want to be able to reliably track clickthrough rates for search result links.\\n[Enhanced E-Commerce](https:\/\/developers.google.com\/analytics\/devguides\/collection\/analyticsjs\/enhanced-ecommerce) is a plugin for google analytics that can track impressions as well as clicks. We model each search result page as a \"product list\", and each content item as a \"product\".\\n","Decision":"We added a plugin for E-Commerce to Static, so we could try using it\\nto measure search performance.\\nWe didn't add an abstraction layer to GOV.UK Frontend Toolkit, because we don't\\nknow if there is a wider need for it, and we wanted to validate our use case with GOV.UK data. The code could be moved to the Frontend Toolkit at a later date if necessary.\\nWe're sending the search query as a custom dimension. This is a workaround: without the custom dimension, Enhanced-Ecommerce\\ncan infer it from the path, but only for impression data. We couldn't get it to set the query for the click data.\\nFor each \"product\" we send the id, position, and list ('site search result'). To avoid unnecessary data transfer when users are interacting with the page, we use Query Time Imports to add\\nmetadata to products later.\\nWe use publishing platform `content_id` as the unique id where available, and use the path as a fallback.\\n","tokens":169,"id":1361}
{"File Name":"datalab\/0020-redux-for-state-store.md","Context":"## Context\\nBuilding a complex web application brings challenges around how to manage state. The\\nRedux website provides an excellent [motivation page](http:\/\/redux.js.org\/docs\/introduction\/Motivation.html) that discusses the challenges in detail.\\n","Decision":"We have decided to adopt the [Redux](http:\/\/redux.js.org\/) architecture to provide a clean separation between\\nour views, actions and state store.\\n","tokens":49,"id":762}
{"File Name":"wordpress-template\/0004-use-a-version-tag-to-make-updating-wordpress-easier.md","Context":"## Context\\nAs we move client sites to dalmatian we want an easy way to keep WordPress up\\nto date. We currently use whippet-racetrack but that doesn't understand github\\nand is still quite a manual process. We are likely to have 80 sites running on\\ndalmatian so we also need a scalable way to update these sites to the latest\\nversion of WordPress.\\nWe want to automatically update minor and patch versions but not major\\nversions. We have created a `v5` tag in our snapshot repo that will point to the\\nlatest released version of version 5.\\nWhen a new major version is released we will want to test it before updating\\ntherefore we are not going to just point at master branch on our snapshot\\nrepo.\\n","Decision":"Set the revision of WordPress in `config\/application.json` to be `v5`\\n","tokens":164,"id":2267}
{"File Name":"accessibility-monitoring\/ADR005-database-public-sector-websites.md","Context":"## Context\\nWe currently have a \"public sector domains\" table (along with an \"organisations\" table containing the owners of the domains).\\nHowever, many organisations have websites that are not on their own domain but in a subfolder of another domain. Moreover, many _services_ reside in subfolders or other logical locations.\\nEach of these needs to be treated as a separate \"website\" in the context of accessibility monitoring.\\nThus, we either change the \"public sector domains\" table to include all websites, not just unique domains, OR we create a new table specifically for public sector websites and initially copy over those domains that we can confirm are definitely websites.\\nThe domains table is useful in and of itself as a repository of registered domains. We should keep and maintain this as-is.\\n","Decision":"We will create and populate a new table in the PubSecDomains schema that contains:\\n* url\\n* name of the service\\n* the site's title from its HTML <head><title> element, where given\\n* the site's description from its <head><meta name=\"description\"> element, where given\\n* last_updated timestamp\\n* the website's sector (foreign key to sectors table)\\n* many-to-many join to the existing Organisations table (one site can, surprisingly, come under the auspices of more than one organisation; obviously one organisation can have multiple websites for their various services)\\n","tokens":162,"id":130}
{"File Name":"saas-platform-frontend\/0001-use-azure-pipeline-for-cicd.md","Context":"## Context\\nTo automated your development you need a CICD platform that automated the build, test and deploy steps.\\n","Decision":"I use Azure Pipeline to build, test and deploy.\\n","tokens":25,"id":3717}
{"File Name":"cf-k8s-networking\/0009-kubebuilder-controllers-dynamic-client-over-generated-clients.md","Context":"## Context\\n### Kubebuilder uses dynamic controller-runtime clients by default\\nKubebuilder uses the\\n[controller-runtime](https:\/\/github.com\/kubernetes-sigs\/controller-runtime)\\nlibrary. Controller Runtime has a dynamic\\n[client](https:\/\/pkg.go.dev\/sigs.k8s.io\/controller-runtime\/pkg\/client) that\\nis used by Kubebuilder controllers by default, rather than a generated\\n[client](https:\/\/github.com\/kubernetes\/community\/blob\/master\/contributors\/devel\/sig-api-machinery\/generating-clientset.md).\\nWhen you `kubebuilder create api`, it creates the api types and in order to\\ninteract with these api types the controller is supplied a controller-runtime\\nclient in it's controller scaffolding so you can CRUD that api type in the\\nKubernetes API.\\n### Problems with generated clients\\nUsing third party generated clients can also be problamatic because of the\\ntransitive dependency on the Kubernetes\\n[client-go](https:\/\/github.com\/kubernetes\/client-go) library and our own\\ndepdendency on client-go. When our controllers want to use a newer version of\\nthe client-go library, this can cause problems for our third party generated\\nclients because they will use a different version of client-go. This doesn't\\ncause problems if client-go keeps the same interface, but we have seen newer\\nversions of client-go break its public interface causing compilation issues.\\nIf the third party libraries ensured they updated their libraries to use the\\nsame version of client-go in a timely manner, this could be less of a\\nproblem. However, this puts a dependency on these third party libraries to\\nkeep their client-go libraries up-to-date.\\n","Decision":"We will only use the controller-runtime client to interact with Kubernetes\\nAPI objects instead of generated clients. This limits our dependency on third\\nparty libraries that can cause conflicts with the client-go library.\\n","tokens":345,"id":2246}
{"File Name":"helix-authentication-service\/0007-support-external-store.md","Context":"## Context\\nThis application handles several important pieces of information, including the\\nrequest\/user mapping, and the session state for each browser connection. By\\ndefault this information is stored in memory within the application process.\\nHowever, if the application fails and another instance begins to process\\nrequests (e.g. via a load balancer), then any login requests that are still in\\nprogress will be lost (the user will get an error and have to try again).\\nTo enable smooth failover and ensure high availability, an external data store\\noption should be provided, in which both the request\/user mapping and the\\nsession state will be stored. This store would be configured like any other\\naspect of the application, via environment variables. If not so configured, the\\nexisting in-memory data store should be used; this should be the default\\nbehavior.\\n","Decision":"Simply storing the data in a reliable fashion is easily done using disk files.\\nHowever, if the application is deployed to several machines, and one of those\\nmachines goes down, then the disk files will be unavailable to the other\\ninstance(s), assuming the use of local disk storage.\\nAlternatively, the data can be stored on a separate system, such as a database\\nor key\/value store. A database is often more complex to set up than is really\\nnecessary, and for the purpose of this application, a key\/value store is more\\nthan adequate. The question then becomes, which key\/value store to use. Ideally\\nit would be popular, mature, and well maintained. It would also need an actively\\nmaintained client library available as a Node package. The best solution would\\nbe one with easy setup in cloud computing services such as Azure.\\nThis application will offer support for the **Redis** key\/value store. There are\\nseveral reasons for this choice:\\n1. Redis is popular, well supported, and has been maintained for more than 10 years.\\n1. Redis can be configured to use memory-only, or file-based storage.\\n1. Nearly all cloud computing providers offer readily-available Redis instances.\\n","tokens":174,"id":2020}
{"File Name":"cdk\/005-package-structure.md","Context":"## Context\\n<!--- What is the issue that we're seeing that is motivating this decision or change? -->\\nThis project defines a library of components build on top of the AWS CDK and aiming to improve the user experience for managing infrastructure at the Guardian. As the library continues to grow, it is important that the library is structured sensibly, both for developers maintaining the library and those using it.\\n","Decision":"<!-- What is the change that we're proposing and\/or doing? -->\\nThe top level directories with the `constructs` directory should mirror the AWS CDK library names.\\nEach directory should contain an `index.ts` file which exports all of the classes within it.\\nFiles within these directories can either be at the top level or nested within directories. Where nested directories exist, they should only be used for grouping multiple implementations of the same underlying construct. For example, `GuLogShippingPolicy`, `GuSSMRunCommandPolicy`, `GuGetS3ObjectPolicy` could all be in seperate files within the `constrcuts\/iam\/policies` directory. These directories should also export all memebers in an `index.ts` file.\\nPatterns can all be defined at the top level within the `patterns` directory. They should all be exported in the `index.ts` file so that they can all be imported from `@guardian\/cdk`\\n","tokens":83,"id":1194}
{"File Name":"voroni-postals\/0003-switch-to-spring-boot.md","Context":"## Context\\nRapidoid was an interesting framework to try back in 2016, but since then I have wanted to switch this project back to Spring Boot for demonstration purposes.\\n","Decision":"This project will be switched to Spring Boot.\\n","tokens":37,"id":3268}
{"File Name":"golang-git-fritz\/0001-record-architecture-decisions.md","Context":"## Context\\nSome records are consistently followed and should be centralized for consistency.\\n","Decision":"Use jncmaguire\/adr.\\n","tokens":16,"id":970}
{"File Name":"james-project\/0049-deduplicated-blobs-gs-with-bloom-filters.md","Context":"## Context\\nThe body, headers, attachments of the mails are stored as blobs in a blob store.\\nIn order to save space in those stores, those blobs are de-duplicated using a hash of their content.\\nTo attain that the current blob store will read the content of the blob before saving it, and generate its id based on\\na hash of this content. This way two blobs with the same content will share the same id and thus be saved only once.\\nThis makes the safe deletion of one of those blobs a non-trivial problem as we can't delete one blob without ensuring\\nthat all references to it are themselves deleted. For example if two messages share the same blob, when we delete\\none message there is at the time being no way to tell if the blob is still referenced by another message.\\n","Decision":"To solve this, we will propose a simple two steps algorithms to provide a background deduplication job.\\nThe **first step** consists in building a bloom filter using the entities referencing the blobs.\\nIn the **second step** we iterate over blobs and check in the bloom filter to predict if they are referenced on not.\\n**Bloom filters** are probabilistic data structures. Here the reference prediction can produce false positives: we might\\nskip some non referenced blobs that should have been garbage collected. However, the associated probability can be tuned\\nand by adding a salt we can ensure subsequent runs will have different sets of false positives and thus that all blobs is\\neventually garbage collected.\\nTo avoid concurrency issues, where we could garbage collect a blob at the same time a new reference to it appear,\\na `reference generation` notion will be added. The de-duplicating id of the blobs which before where constructed\\nusing only the hash of their content,  will now include this `reference generation` too. To avoid synchronization\\nissues, the `generation` will be time based.\\nSo only blobs belonging to the `reference generation` `n-2` will be eligible for garbage collection to avoid\\nconcurrency issues, and allow for a clock skew.\\nFinally, we wish to offer the opportunity to configure, and reconfigure, the `generation` duration. In order to do so,\\nwe introduce a `generation family` part in the blobId. Incremented by the administrator on each configuration changes on\\nthe generation duration it allows avoiding conflicts in generations getting the same number before and after the change:\\nall blobIds with a different family are considered belonging to a distinct generation ready to be garbage collected. This\\nallows arbitrary changes in the generation duration.\\n","tokens":168,"id":2907}
{"File Name":"content-data-api\/adr-009-consume-from-publishing-api.md","Context":"## Context\\nWhen we created the ETL pipeline we originally used the publishing API messages just to flag\\nitems as updated. Then we would fetch items from the content store overnight.\\nWe now grow the `Item` dimension every time there is an update, instead of limiting it\\nto one record per day. Now that we do this, we rely on two sources of truth for this data:\\nthe publishing API and the content store. We should not need to make requests to the content\\nstore at all, because the content store is intended to support the frontend architecture, not ETL\/analytics.\\nWe are addressing this now because we want to change the data warehouse to store individual parts\\nof guides and travel advice, and this is difficult to implement due to the complexity of the message\\nprocessing.\\n","Decision":"We will use the publishing API messages to extract all content, rather than issuing an additional request to the content store.\\n### Data flow\\nWe are refactoring the publishing API message hander, so that it first transforms a message into a `ContentItem` class, which stores all the information we need.\\nThis is the input we use to present the content for that item, calculate a hash of that content, and (if neccessary) calculate quality metrics for that content. We have decided to do all this processing synchronously for the time being so the pipeline is more predictable and easy to debug.\\n### Growing the item dimension\\nWe subscribe to these kinds of updates from the publishing API:\\n- major\\n- minor\\n- links\\n- republish\\n- unpublish\\nA new item record is created if:\\n- we've never seen that content item before\\n- the content hash changes\\n- the links change (if links appear in a different order they should still be considered the same)\\nOtherwise we don't insert new records.\\nWhen creating a new item we \"promote\" it by setting `latest=false` on the existing latest item with that content ID, and setting `latest=true` on the new item.\\n### Impact of multi-part items\\nFor a multi-part item, we will transform a single message into multiple items, each with a unique base path but a shared content ID.\\nThe base path of a part-item is the `base_path` attribute of the publishing API content item plus the `slug` of the part in the `details` hash.\\nWhen we grow the dimension, we mark *all* parts with the same `content_id` with `latest=false`, and create new item records for *all* the part items from the new message.\\n![](images\/content_etl.png)\\n### Benefits:\\n- We can implement multi-part documents very easily\\n- The data warehouse doesn't depend on frontend infrastructure.\\n","tokens":165,"id":1860}
{"File Name":"cloud-platform\/012-One-cluster-for-dev-staging-prod.md","Context":"## Context\\nThe Cloud Platform needs to host both citizen-facing, production services, and development environments for service teams to iterate on their code, or just set up sandboxes for experimentation and learning.\\nTo support this, should we have separate clusters for production, development, and staging? Or, should we run a single cluster hosting all these different types of workload?\\n","Decision":"After consideration of the pros and cons of each approach we went with one cluster, using namespaces to partition different workloads.\\nSome important reasons behind this move were:\\n- A single k8s cluster can be made powerful enough to run all of our workloads\\n- Managing a single cluster keeps our operational overhead and costs to a minimum.\\n- Namespaces and RBAC keep different workloads isolated from each other.\\n- It would be very hard to keep multiple clusters (dev\/staging\/prod) from becoming too different to be representative environments\\nTo clarify the last point; to be useful, a development cluster must be as similar as possible to the production cluster. However, given multiple clusters, with different security and other constraints, some 'drift' is inevitable - e.g. the development cluster might be upgraded to a newer kubernetes version before staging and production, or it could have different connectivity into private networks, or different performance constraints from the production cluster.\\nBased on our past experience, these differences tend to increase over time, to the point where the development cluster is too far away from production to be representative. The extra work required to maintain multiple environments becomes wasted effort.\\nIf namespace segregation is sufficient to isolate one production service from another, then it is enough to isolate a team's development environment from a production service.\\nIf namespace segregation is not sufficient for this, then the whole cloud platform idea doesn't work.\\n","tokens":74,"id":622}
{"File Name":"govuk-aws\/0006-puppet-architecture.md","Context":"## Context\\nFor our initial iteration we should discuss whether we use a Puppetmaster architecture,\\nor use a masterless Puppet approach.\\nUsing the former approach, we would have to take the following into account:\\n- We would set up a Puppetmaster for the rest of the provisioned instances to connect to\\n- We would deploy new Puppet code to this machine only\\n- It matches our current architecture and approach\\n- We would need to take into account certificate management\\n- Each machine would \"check in\" to the Puppetmaster every 30 minutes, ensuring\\na consistent state\\nIf we used a masterless approach:\\n- We would need to deploy Puppet code and secrets to each instance that we provision\\n- We would run Puppet only when we deploy new code\\n- It differs from our current approach\\n- We would not have to manage a Puppetmaster which means no single point of failure\\n","Decision":"Use a Puppetmaster to not diverge from current architecture in the initial iteration.\\n","tokens":182,"id":4028}
{"File Name":"SAP-Cloud\/ignore-end-to-end-tests-on-non-produtive-branches.md","Context":"## Context\\nSAP S\/4HANA Cloud SDK Pipeline can execute end-to-end tests, which simulate how a human would test the application.\\nEnd-to-end tests tend to run quite long, which might impede how fast pull requests can be merged.\\n### Decision\\nWe allow to skip running end-to-end tests on non-productive branches.\\nWe do not allow skipping them on the productive branch.\\nThis feature is disabled by default.\\n","Decision":"We allow to skip running end-to-end tests on non-productive branches.\\nWe do not allow skipping them on the productive branch.\\nThis feature is disabled by default.\\n","tokens":93,"id":2929}
{"File Name":"dogma\/0007-location-of-examples.md","Context":"## Context\\nWe need to decide whether Dogma's examples should reside in the `dogma`\\nrepository itself, or a separate `examples` repository.\\n","Decision":"We've decided to move the examples to a separate repository, so that we can\\nprovide fully-functional examples that depend on modules\/packages that we would\\nnot want to have as dependants of `dogma` itself, such as `mysql`, etc.\\n","tokens":33,"id":1622}
{"File Name":"coloseo\/0003-use-mern-stack.md","Context":"## Context\\n","Decision":"The project is build with the MERN Stack.\\n","tokens":3,"id":4797}
{"File Name":"govuk-aws\/0007-puppet-cert-management.md","Context":"## Context\\nWhen machines connect to a Puppetmaster, they require a signed certificate to be\\npresent to allow interaction.\\nIf we lose a Puppetmaster (or it is recreated by an Auto Scaling Group) then we\\nwould have to re-sign all the certificates to allow all machines in the\\nenvironment to run Puppet again (ref: https:\/\/docs.puppet.com\/puppet\/3.8\/ssl_regenerate_certificates.html).\\nThis would involve manual intervention to allow machines to connect again\\nand we would be unable to deploy new Puppet code during this period.\\nIdeally we should back up our signed certificates, and a new machine would\\nbe able to restore them when it is recreated.\\n","Decision":"We will defer this until a later date while we're testing this with Integration\\nto concentrate on getting the stack up and running and accept the loss of certicates.\\nAt a later date we will re-visit this approach and implement a suitable solution.\\n","tokens":143,"id":4050}
{"File Name":"arch\/0005-replace-wechat-mail-sms-with-slack.md","Context":"## Context\\n1. \u5f53\u524d\u4f7f\u7528\u5fae\u4fe1\u3001\u90ae\u4ef6\u4f5c\u4e3a\u5de5\u4f5c\u4e2d\u7684\u6c9f\u901a\u5de5\u5177\uff0c\u5de5\u4f5c\u4e0e\u751f\u6d3b\u6df7\u5408\uff1b\\n2. \u5404\u79cd\u544a\u8b66\u4fe1\u606f\u9700\u5206\u522b\u67e5\u770b\u90ae\u4ef6\u3001\u77ed\u4fe1\u6216\u662f\u76d1\u63a7\u5e73\u53f0\uff1b\\n3. \u968f\u7740\u4efb\u52a1\u7ba1\u7406\u5e73\u53f0\u3001Bug \u7ba1\u7406\u5e73\u53f0\u3001wiki\u3001\u6587\u6863\u5206\u4eab\u7f51\u7ad9\u3001\u804a\u5929\u5de5\u5177\u7684\u6d41\u884c\uff0c\u90ae\u4ef6\u7684\u4f7f\u7528\u573a\u666f\u8d8a\u6765\u8d8a\u5c0f\uff0c\u5e76\u4e14\u90ae\u4ef6\u4fe1\u606f\u5e76\u4e0d\u53ca\u65f6\uff1b\\n4. \u901a\u8fc7\u5fae\u4fe1\u8fdb\u884c\u6c9f\u901a\uff0c\u65b0\u4eba\u65e0\u6cd5\u4e86\u89e3\u5386\u53f2\u4fe1\u606f\uff0c\u7ecf\u5e38\u9700\u8981\u628a\u53d1\u8fc7\u7684\u5185\u5bb9\u91cd\u590d\u53d1\u9001\u3002\\n","Decision":"### Slack \u652f\u6301\u7684\u529f\u80fd\\n1. **\u516c\u5f00\u4e0e\u79c1\u6709\u7684\u9891\u9053\uff0c\u9891\u9053\u53ef\u968f\u65f6\u52a0\u5165\u6216\u9000\u51fa**\uff1b\\n1. \u6309\u9879\u76ee\uff0cproject-crm, project-sms, \u7b49\uff1b\\n2. \u6309\u6280\u672f\u8bdd\u9898\uff0ctech-restful-api, tech-queue \u7b49\uff1b\\n3. \u53ef\u4ee5\u9080\u8bf7\u76f8\u5173\u4eba\uff0c\u4efb\u4f55\u4eba\u4e5f\u53ef\u968f\u610f\u52a0\u5165\u3002\\n2. **\u6d88\u606f\u652f\u6301\u8868\u60c5\u5feb\u901f\u56de\u590d**\uff1b\\n1. \u8868\u60c5\u662f\u9644\u52a0\u5728\u6d88\u606f\u4e0a\u7684\uff0c\u4e0a\u4e0b\u6587\u5185\u805a\u5f88\u9ad8\u3002\\n3. \u6d88\u606f\u652f\u6301\u6536\u85cf\uff1b\\n1. \u4e00\u4e9b\u4e0d\u9519\u7684\u91cd\u8981\u4fe1\u606f\uff0c\u53ef\u4ee5\u6536\u85cf\u8d77\u6765\u968f\u65f6\u67e5\u770b\u3002\\n4. \u652f\u6301\u5404\u79cd\u6587\u4ef6\u7684\u5206\u4eab\uff1b\\n1. pdf \u7b49\u6587\u4ef6\u5747\u53ef\u9884\u89c8\u3002\\n5. **\u5206\u4eab\u7684\u94fe\u63a5\u652f\u6301\u9884\u89c8**\uff1b\\n1. \u5206\u4eab\u7684\u94fe\u63a5\uff0c\u4e0d\u7528\u70b9\u5f00\u4e5f\u77e5\u9053\u5927\u4f53\u5185\u5bb9\u3002\\n6. \u641c\u7d22\u529f\u80fd\u5f3a\u5927\uff0c\u53ef\u901a\u8fc7\u5feb\u6377\u65b9\u5f0f\uff0c\u641c\u7d22\u540c\u4e8b\uff0c\u6d88\u606f\u8bb0\u5f55\uff0c\u6587\u4ef6\u7b49\uff1b\\n7. \u591a\u79cd\u7a0b\u5e8f\u4ee3\u7801\u652f\u6301\u9ad8\u4eae\uff1b\\n1. \u4ee3\u7801\u9ad8\u4eae\u9884\u89c8\uff0c\u81ea\u52a8\u6298\u53e0\uff0c\u4e0d\u5f71\u54cd\u6574\u4f53\u6548\u679c\u3002\\n8. **\u5f3a\u5927\u7684\u7b2c\u4e09\u65b9\u96c6\u6210\uff0c\u5c06\u6240\u6709\u6d88\u606f\u3001\u901a\u77e5\u6c47\u805a\u5728\u4e00\u5904**\uff0c\u4f8b\u5982\uff0cTrello\uff0cGithub\uff0cNewRelic, Sentry\uff0cJenkins \u7b49\uff1b\\n9. **\u65b0\u52a0\u5165\u8005\u53ef\u67e5\u770b\u7fa4\u7ec4\u5386\u53f2\u4fe1\u606f**\u3002\\n1. \u4fe1\u606f\u518d\u4e5f\u4e0d\u7528\u91cd\u590d\u53d1\u4e86\u3002\\n10. \u5f00\u653e\u6027\u975e\u5e38\u597d\\n1. \u4efb\u4f55\u4eba\u90fd\u53ef\u4ee5\u65b9\u4fbf\u7533\u8bf7\u5f00\u53d1\u8005 KEY\uff0c\u5efa\u7acb\u81ea\u5df1\u7684\u673a\u5668\u4eba\u3002\\n### \u7814\u53d1\u90e8\u9891\u9053\u8bbe\u8ba1\\n1. CI\/CD - \u7528\u4e8e\u63a5\u6536\u6d4b\u8bd5\u3001\u90e8\u7f72\u7ed3\u679c\uff0c\u6d4b\u8bd5\u8986\u76d6\u7387\uff0cPR \u7b49\u4fe1\u606f\uff0c\u4e5f\u7528\u4e8e\u53d1\u8d77\u6d4b\u8bd5\u3001\u90e8\u7f72\u7b49\uff1b\\n2. NewRelic - \u7528\u4e8e\u63a5\u6536\u5e94\u7528\u6027\u80fd\u62a5\u8b66\u7b49\u4fe1\u606f\uff1b\\n3. Sentry - \u7ebf\u4e0a\u5b9e\u65f6\u9519\u8bef\u4fe1\u606f\uff0c\u53ef\u6839\u636e\u9879\u76ee\u5355\u72ec\u62c6\u51fa\u6765\uff1b\\n4. Team-X - \u7528\u4e8e\u7ec4\u5185\u6c9f\u901a\uff0c\u4e00\u4e2a Scrum \u7ec4\uff0c\u5305\u62ec\u7814\u53d1\uff0c\u4ea7\u54c1\u53ca\u6d4b\u8bd5\uff1b\\n5. Knowledge - \u7528\u4e8e\u6240\u6709\u7814\u53d1\u4eba\u5458\u8fdb\u884c\u6c9f\u901a\u4e0e\u5206\u4eab\uff1b\\n6. Product - \u7528\u4e8e\u8de8 Team \u7684\u4ea7\u54c1\u8fdb\u884c\u6c9f\u901a\u4e0e\u5206\u4eab\uff1b\\n7. Backend - \u7528\u4e8e\u6240\u6709\u540e\u7aef\u4eba\u5458\u8fdb\u884c\u95ee\u9898\u54a8\u8be2\u53ca\u5206\u4eab\uff1b\\n8. Leads(\u79c1\u5bc6) - \u7528\u4e8e\u6240\u6709 leader \u8fdb\u884c\u6c9f\u901a\u3001\u5b89\u6392\u5468\u4f1a\u7b49\uff1b\\n9. Frontend, UI, Mobile, Devops, QA etc\\n### \u6211\u4eec\u7528\u4e86\u5982\u4e0b\u7b2c\u4e09\u65b9\u8f6f\u4ef6\\n* Sentry\\n* NewRelic\\n* RedMine\\n* Teambition\\n* Confluence\\n* Github\\n* Bugly\\n* FIR.im\\n* Jenkins\\n* etc\\n### Slack vs BearyChat\\n* Slack \u4f18\u7f3a\u70b9\uff1a\\n* \u9f3b\u7956\\n* \u51e0\u4e4e\u6240\u6709\u5e38\u7528\u8f6f\u4ef6(\u56fd\u5916)\u90fd\u6709\u96c6\u6210\\n* \u529f\u80fd\u5b8c\u5584\u5065\u58ee\uff0c\u516c\u53f8\u53ef\u4fe1\\n* \u7f51\u7edc\u4e0d\u7a33\u5b9a\\n* \u56fd\u5185\u5e94\u7528\u96c6\u6210\u5f88\u5c11\\n* BearChat \u4f18\u7f3a\u70b9\uff1a\\n* \u57fa\u672c\u6709 Slack \u7684\u6838\u5fc3\u529f\u80fd\\n* \u6570\u636e\u7684\u9690\u79c1\u65e0\u6cd5\u4fdd\u8bc1\\n* \u670d\u52a1\u7684\u53ef\u7528\u6027\u65e0\u6cd5\u4fdd\u8bc1\\n* \u7b2c\u4e09\u65b9\u96c6\u6210\u592a\u5c11\uff08must-read, Simple Poll, etc \u90fd\u6ca1\u6709\uff09\uff0c\u5012\u662f\u96c6\u6210\u4e86\u56fd\u5185\u7684 Teambiton, \u76d1\u63a7\u5b9d\u7b49\\n\u6700\u7ec8\uff0c\u6211\u4eec\u7ed9\u56fd\u4ea7\u8f6f\u4ef6(\u670d\u52a1)\u4e00\u4e2a\u673a\u4f1a\u3002\\n\u7ecf\u8fc7\u4e24\u5468\u7684 BearyChat \u8bd5\u7528\uff0c\u76ee\u524d\u5df2\u51c6\u5907\u8f6c\u7528 Slack - 2017-08-03\\n1. BearyChat \u7684 Teambition \u96c6\u6210\u5931\u8d25\uff0c\u6c9f\u901a\u540e\u8bf4\u662f TB \u7684\u63a5\u53e3\u8c03\u6574\uff1b\\n2. \u96c6\u6210\u6570\u7684\u9650\u5236\uff0c\u90fd\u662f 10 \u4e2a\uff0c\u4f46\u540c\u4e00\u4e2a\u7b2c\u4e09\u65b9\u591a\u6b21\u96c6\u6210\uff0cSlack \u7b97\u4e00\u4e2a\uff0cBearyChat \u7b97\u591a\u4e2a\uff0c\u6bd4\u5982 Github, Sentry\uff1b\\n3. \u7ecf\u8fc7\u5bf9\u5404\u4e2a\u56fd\u5185\u516c\u53f8\u7684\u4e86\u89e3\uff0c\u53d1\u73b0\u4f7f\u7528\u4e2d Slack \u7684\u56fd\u5185\u7a33\u5b9a\u6027\u8fd8\u597d\u3002\\n","tokens":166,"id":2448}
{"File Name":"paas-team-manual\/ADR031-services-core-pipeline-separate-jobs.html.md","Context":"# Context\\nWe have a single pipeline `create-cloudfoundry` which creates a Cloud Foundry\\ndeployment, and also deploys additional services to the platform.\\nThese services include:\\n- PaaS Accounts\\n- PaaS Admin\\n- PaaS Billing\\n- PaaS Metrics\\nWhich are core components to our platform, but not to Cloud Foundry.\\nCurrently these services are unnecessarily coupled in a couple of places:\\n- The `post-deploy` job\\n- The `custom-acceptance-tests` job\\nUnnecessarily coupling has resulted in flakey app tests blocking CVE\\nremediation from reaching production.\\n# Decision\\nMove, where possible, PaaS services into their own jobs (within the same\\n`create-cloudfoundry` pipeline) such that they do not impede progress of\\ndeployment to the core platform.\\n# Status\\nAccepted\\n# Consequences\\nThe pipeline will no longer be fully controlled by the `pipeline-lock` pool.\\nThe individual jobs in the pipeline will be less mysterious.\\n","Decision":"Move, where possible, PaaS services into their own jobs (within the same\\n`create-cloudfoundry` pipeline) such that they do not impede progress of\\ndeployment to the core platform.\\n# Status\\nAccepted\\n# Consequences\\nThe pipeline will no longer be fully controlled by the `pipeline-lock` pool.\\nThe individual jobs in the pipeline will be less mysterious.\\n","tokens":221,"id":220}
{"File Name":"paas-team-manual\/ADR005-pingdom-healthchecks.md","Context":"Context\\n=======\\nWe wanted to open up access to tenant applications in our production environment.\\nAs part of an earlier story, Pingdom checks were set up for a healthcheck application in CI, Staging, and Production. At this stage applications were not accessible from non-office IP addresses.\\nThe problem we faced was a need to test the code introduced to make our production environment accessible to the world. Initially, we suggested applying the same change to our staging environment. However, this approach means all applications in staging will be accessible from anywhere.\\nIf we use Pingdom to assert an application is accessible from the outside world then we need to remove the explicit rules (security groups) allowing Pingdom traffic. This means our CI environment would not be accessible to Pingdom probes.\\n* [#116104189 - set up Pingdom](https:\/\/www.pivotaltracker.com\/story\/show\/116104189)\\n* [#115347323 - allow public access to tenant applications](https:\/\/www.pivotaltracker.com\/story\/show\/115347323)\\nDecision\\n========\\nIt was decided we would make the staging environment accessible to the outside world as well as production, and define future work for removing the CI Pingdom check and security groups allowing Pingdom probes, and setting up tests from the pipeline which use the Pingdom API.\\nGiven that the advantages relate to the availability of our production environment, they outweigh not having an automated healthcheck on an application in our CI environment. However, we remain open to hearing solutions to providing healthchecks for CI in future.\\nStatus\\n======\\nProposed\\nConsequences\\n============\\nA story is now required to remove the Pingdom health check for our CI environment, and the security groups allowing Pingdom probes.\\n### Positive\\n* We are now able to test accessibility using the staging environment\\n* We are now able to use Pingdom to assert not just application health, but routing as well.\\n* We have maintained consistency between staging and production\\n### Negative\\n* Any applications in our staging environment need to be considered for whether they are suitable to be public.\\n* We would no longer have healthchecks via Pingdom for our CI environment.\\n","Decision":"========\\nIt was decided we would make the staging environment accessible to the outside world as well as production, and define future work for removing the CI Pingdom check and security groups allowing Pingdom probes, and setting up tests from the pipeline which use the Pingdom API.\\nGiven that the advantages relate to the availability of our production environment, they outweigh not having an automated healthcheck on an application in our CI environment. However, we remain open to hearing solutions to providing healthchecks for CI in future.\\nStatus\\n======\\nProposed\\nConsequences\\n============\\nA story is now required to remove the Pingdom health check for our CI environment, and the security groups allowing Pingdom probes.\\n### Positive\\n* We are now able to test accessibility using the staging environment\\n* We are now able to use Pingdom to assert not just application health, but routing as well.\\n* We have maintained consistency between staging and production\\n### Negative\\n* Any applications in our staging environment need to be considered for whether they are suitable to be public.\\n* We would no longer have healthchecks via Pingdom for our CI environment.\\n","tokens":449,"id":182}
{"File Name":"superwerker\/securityhub.md","Context":"## Context\\nSince Security Hub is a native AWS service for central security alert management, checks and mitigation, superwerker enables it for all AWS accounts.\\n","Decision":"- Enable SH for existing Control Tower core accounts (master, Audit, Log Archive) and all future member accounts\\n- Use Control Tower `Setup\/UpdateLandingZone` Lifecycle events to start the invite setup for SH\\n- The delegated administrator feature is currently not supported by Lambda and\/or SSM Automation runtimes - since upgrading the current mechanism to this feature as soon as it's available is officially supported we're postponing this (#70); this subsequently requires us to implement integrity protection\\n- SH out-of-the-box complains about a lot of security check issues - this has been scoped out from 1.0 (#99)\\n","tokens":32,"id":3401}
{"File Name":"gsp\/ADR033-nlb-for-mtls.md","Context":"## Context\\nVerify's [doc-checking service](https:\/\/github.com\/alphagov\/doc-checking) is\\nsecured in part using mTLS. Currently, our clusters are fronted by ALBs which\\ncannot provide mTLS.\\nThe doc-checking service currently runs an nginx that provides the mTLS\\nfunctionality. In order for GSP to be able to allow something within the\\ncluster to perform mTLS we must run a load balancer that forwards unaltered TCP\\npackets in addition to, or instead of an ALB.\\n","Decision":"We will optionally create and run an NLB in addition to the current ALB for\\nclusters that have a requirement to terminate their own TLS. This NLB will be\\navailable at `nlb.$CLUSTER_DOMAIN`.\\n","tokens":115,"id":3888}
{"File Name":"react-native-app\/0007-detach-from-expo.md","Context":"## Context\\nThe size for our Android app is 28mb even if it only has 2 screens with simple tabs and lists.\\nThis is because the size for an Expo app on iOS is approximately 33mb (download), and Android is about 20mb because Expo includes a bunch of APIs regardless of whether or not you are using them. Expo will make this customizable in the future but for now, there is no option to customize it.\\n","Decision":"Detach from Expo.\\n","tokens":92,"id":4209}
{"File Name":"geem\/0003-postgresql-database.md","Context":"## Context\\n[PostgreSQL](https:\/\/www.postgresql.org\/) provides a [native JSON field type](https:\/\/www.postgresql.org\/docs\/10\/static\/datatype-json.html) which could be useful for storing package contents.\\n","Decision":"TBD\\n","tokens":47,"id":2589}
{"File Name":"adr-playground\/0002-implement-as-unix-shell-scripts.md","Context":"## Context\\nThe issue motivating this decision, and any context that influences or constrains the decision.\\n","Decision":"The change that we're proposing or have agreed to implement.\\n","tokens":21,"id":2025}
{"File Name":"buy-for-your-school\/0019-use-loaf-for-breadcrumb-links.md","Context":"## Context\\nIt is necessary for breadcrumb navigation to be supported and configured in a consistent way across the application\\n","Decision":"The recommended solution is to use the `loaf` library - which is already used in several other DfE repositories as the go-to solution for breadcrumb management:\\n- https:\/\/github.com\/DFE-Digital\/academy-transfers-frontend\\n- https:\/\/github.com\/DFE-Digital\/get-into-teaching-app\\n- https:\/\/github.com\/DFE-Digital\/npd-find-and-explore\\n","tokens":22,"id":1265}
{"File Name":"opg-use-an-lpa\/0011-the-same-zend-application-will-be-used-for-both-viewer-and-actor-components.md","Context":"## Context\\nUse an LPA will be made up of two components - those for use by LPA _actors_, and those used by third\\nparty groups who are the _viewers_ of the LPA.\\nAt present it is expected that these two components will be hosted on two different domains.\\n","Decision":"That both `Viewer` and `Actor` will both be separate modules of the same Zend application.\\nNote: it is still expected that they will be deployed separately into two containers.\\n","tokens":63,"id":4850}
{"File Name":"bookit-api\/0004-security.md","Context":"## Context\\nBookit needs the concept of a user so it can identify who booked what bookable and allow certain users to overrule a booking.\\nThis involves both authentication (who you are) and authorization (what you're allowed to do).  For the MVP we are primarily focused on Authentication.\\nEmployees from different domains (Wipro, Designit, Cooper, etc) need to log into the system.\\nProtect all endpoints, optionally allow \/ping but, if authentication provided, return profile information as well.\\n","Decision":"* We will use Azure AD v2 OpenID Connect to exchange an id_token for an opaque Bookit API opaque access_token\\n","tokens":106,"id":3220}
{"File Name":"verify-matching-service-adapter\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":4647}
{"File Name":"CCS-Architecture-Decision-Records\/0006-all-new-services-should-use-the-same-front-end-toolkit.md","Context":"## Context\\nThe services need to look and feel the same. They need to comply with [adr 0002 - Use a consistent user experience](0002-use-a-consistent-user-experience.md).\\nNQC\u2019s SRS and DPS services are already built. They comply with GOV.UK styles to a certain extent. They should be partially exempted from this decision for the moment\\nGDS have released their new [Design System](https:\/\/design-system.service.gov.uk). It does not yet support non-GOV.UK designs but that is on the [roadmap](https:\/\/design-system.service.gov.uk\/roadmap\/).\\n[CCS Website](https:\/\/www.crowncommercial.gov.uk\/s\/) has complied with, and contributed to, GDS patterns, but has a divergent colour palette.\\n","Decision":"- All new services should share a common front end toolkit\\n- There may be different modules per language, sharing the same HTML, CSS, JavaScript assets\\n- SRS stylesheets should comply with this kit as closely as practicable. We should revisit this decision as new phases of work are considered for transparency and supplier registration including DPS\\n- Comply with GDS Design System except where CCS styling requires differences; document the differences\\n- Use a checkout of the Frontend toolkit as the base for front end resources\\n- Add a new repository for style and other resource overrides to meet CCS needs,\\n- Periodically pull new versions of Frontend toolkit and test\\n- Work with GDS to help them support non-GOV.UK styles\\n","tokens":167,"id":995}
{"File Name":"project-blueprint\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":4810}
{"File Name":"Sylius\/2021_06_15_api_platform_config_customization.md","Context":"## Context and Problem Statement\\nSylius is by design application that will be adjusted to customer needs.\\nTherefore each part of it has to be crafted with replaceability or customizability at its core.\\nNonetheless, the current state of API Platform integration requires the replacement of the whole config of the whole resource.\\nIn terms of the more complicated cases like Order or Customer, this practice may become error-prone and hard to maintain for both maintainers and Sylius users.\\n","Decision":"Chosen option: \"Config merging\", because it allows us easily overwrite any endpoint, without getting deep into api platform resources.\\n","tokens":98,"id":680}
{"File Name":"gsp\/ADR030-aws-service-operator.md","Context":"## Context\\n[Amazon announced](https:\/\/aws.amazon.com\/blogs\/opensource\/aws-service-operator-kubernetes-available\/) an AWS Service Operator for Kubernetes in October 2018.\\nThere is a need for one or more users to provision SQS queues for their application to interact with.\\nThe AWS Service Operator consists of a container that sits in the Kubernetes cluster and monitors for custom resource types which it maps to the appropriate CloudFormation resources and deploys.\\nIt supports the following resources:\\n* CloudFormation Template\\n* DynamoDB\\n* ECR Repository\\n* ElastiCache\\n* S3 Bucket\\n* SNS Subscription\\n* SNS Topic\\n* SQS Queue\\n","Decision":"We have included AWS Service Operator as part of GSP so that users can provision SQS queues.\\n","tokens":137,"id":3881}
{"File Name":"wordpress-template\/0008-use-dependabot-yml-to-configure-security-alerts.md","Context":"## Context\\nGitHub security alerts can provide us with useful information when there is a vulnerability in a package we use.\\nHowever, currently the vast majority of alerts we receive are for vulnerabilities in NPM dev dependencies, which are highly unlikely to lead to any real vulnerability in the production environment. See https:\/\/overreacted.io\/npm-audit-broken-by-design\/ for more details.\\nThis creates so many false positives that we're unable to spot the meaningful security alerts we need to act upon amongst the noise.\\n","Decision":"We will use a `dependabot.yml` file to [configure the security alerts](https:\/\/docs.github.com\/en\/code-security\/supply-chain-security\/keeping-your-dependencies-updated-automatically\/configuration-options-for-dependency-updates) so that alerts are raised for all composer packages, but only production NPM packages.\\n","tokens":104,"id":2264}
{"File Name":"cosmos-sdk\/adr-007-specialization-groups.md","Context":"## Context\\nThis idea was first conceived of in order to fulfill the use case of the\\ncreation of a decentralized Computer Emergency Response Team (dCERT), whose\\nmembers would be elected by a governing community and would fulfill the role of\\ncoordinating the community under emergency situations. This thinking\\ncan be further abstracted into the concept of \"blockchain specialization\\ngroups\".\\nThe creation of these groups are the beginning of specialization capabilities\\nwithin a wider blockchain community which could be used to enable a certain\\nlevel of delegated responsibilities. Examples of specialization which could be\\nbeneficial to a blockchain community include: code auditing, emergency response,\\ncode development etc. This type of community organization paves the way for\\nindividual stakeholders to delegate votes by issue type, if in the future\\ngovernance proposals include a field for issue type.\\n","Decision":"A specialization group can be broadly broken down into the following functions\\n(herein containing examples):\\n* Membership Admittance\\n* Membership Acceptance\\n* Membership Revocation\\n* (probably) Without Penalty\\n* member steps down (self-Revocation)\\n* replaced by new member from governance\\n* (probably) With Penalty\\n* due to breach of soft-agreement (determined through governance)\\n* due to breach of hard-agreement (determined by code)\\n* Execution of Duties\\n* Special transactions which only execute for members of a specialization\\ngroup (for example, dCERT members voting to turn off transaction routes in\\nan emergency scenario)\\n* Compensation\\n* Group compensation (further distribution decided by the specialization group)\\n* Individual compensation for all constituents of a group from the\\ngreater community\\nMembership admission to a specialization group could take place over a wide\\nvariety of mechanisms. The most obvious example is through a general vote among\\nthe entire community, however in certain systems a community may want to allow\\nthe members already in a specialization group to internally elect new members,\\nor maybe the community may assign a permission to a particular specialization\\ngroup to appoint members to other 3rd party groups. The sky is really the limit\\nas to how membership admittance can be structured. We attempt to capture\\nsome of these possibilities in a common interface dubbed the `Electionator`. For\\nits initial implementation as a part of this ADR we recommend that the general\\nelection abstraction (`Electionator`) is provided as well as a basic\\nimplementation of that abstraction which allows for a continuous election of\\nmembers of a specialization group.\\n``` golang\\n\/\/ The Electionator abstraction covers the concept space for\\n\/\/ a wide variety of election kinds.\\ntype Electionator interface {\\n\/\/ is the election object accepting votes.\\nActive() bool\\n\/\/ functionality to execute for when a vote is cast in this election, here\\n\/\/ the vote field is anticipated to be marshalled into a vote type used\\n\/\/ by an election.\\n\/\/\\n\/\/ NOTE There are no explicit ids here. Just votes which pertain specifically\\n\/\/ to one electionator. Anyone can create and send a vote to the electionator item\\n\/\/ which will presumably attempt to marshal those bytes into a particular struct\\n\/\/ and apply the vote information in some arbitrary way. There can be multiple\\n\/\/ Electionators within the Cosmos-Hub for multiple specialization groups, votes\\n\/\/ would need to be routed to the Electionator upstream of here.\\nVote(addr sdk.AccAddress, vote []byte)\\n\/\/ here lies all functionality to authenticate and execute changes for\\n\/\/ when a member accepts being elected\\nAcceptElection(sdk.AccAddress)\\n\/\/ Register a revoker object\\nRegisterRevoker(Revoker)\\n\/\/ No more revokers may be registered after this function is called\\nSealRevokers()\\n\/\/ register hooks to call when an election actions occur\\nRegisterHooks(ElectionatorHooks)\\n\/\/ query for the current winner(s) of this election based on arbitrary\\n\/\/ election ruleset\\nQueryElected() []sdk.AccAddress\\n\/\/ query metadata for an address in the election this\\n\/\/ could include for example position that an address\\n\/\/ is being elected for within a group\\n\/\/\\n\/\/ this metadata may be directly related to\\n\/\/ voting information and\/or privileges enabled\\n\/\/ to members within a group.\\nQueryMetadata(sdk.AccAddress) []byte\\n}\\n\/\/ ElectionatorHooks, once registered with an Electionator,\\n\/\/ trigger execution of relevant interface functions when\\n\/\/ Electionator events occur.\\ntype ElectionatorHooks interface {\\nAfterVoteCast(addr sdk.AccAddress, vote []byte)\\nAfterMemberAccepted(addr sdk.AccAddress)\\nAfterMemberRevoked(addr sdk.AccAddress, cause []byte)\\n}\\n\/\/ Revoker defines the function required for a membership revocation rule-set\\n\/\/ used by a specialization group. This could be used to create self revoking,\\n\/\/ and evidence based revoking, etc. Revokers types may be created and\\n\/\/ reused for different election types.\\n\/\/\\n\/\/ When revoking the \"cause\" bytes may be arbitrarily marshalled into evidence,\\n\/\/ memos, etc.\\ntype Revoker interface {\\nRevokeName() string      \/\/ identifier for this revoker type\\nRevokeMember(addr sdk.AccAddress, cause []byte) error\\n}\\n```\\nCertain level of commonality likely exists between the existing code within\\n`x\/governance` and required functionality of elections. This common\\nfunctionality should be abstracted during implementation. Similarly for each\\nvote implementation client CLI\/REST functionality should be abstracted\\nto be reused for multiple elections.\\nThe specialization group abstraction firstly extends the `Electionator`\\nbut also further defines traits of the group.\\n``` golang\\ntype SpecializationGroup interface {\\nElectionator\\nGetName() string\\nGetDescription() string\\n\/\/ general soft contract the group is expected\\n\/\/ to fulfill with the greater community\\nGetContract() string\\n\/\/ messages which can be executed by the members of the group\\nHandler(ctx sdk.Context, msg sdk.Msg) sdk.Result\\n\/\/ logic to be executed at endblock, this may for instance\\n\/\/ include payment of a stipend to the group members\\n\/\/ for participation in the security group.\\nEndBlocker(ctx sdk.Context)\\n}\\n```\\n","tokens":173,"id":805}
{"File Name":"super-eks\/0002-use-cdk-for-implementation.md","Context":"## Context\\nBuilding the installer requires choosing an appropriate infrastructure as code tool that allows configuring AWS EKS.\\n","Decision":"We can either use vanilla CloudFormation, Terraform or CDK.\\nWe choose CDK. CDK seems like having the most traction right now, it's easy to develop with.\\n","tokens":22,"id":2029}
{"File Name":"airline-reservation-system\/0005-TCH-RES-use-spring-repository.md","Context":"## Context and Problem Statement\\n**What technology is to be used for persistence?**\\nConsidered options:\\n1. Spring Template\\n2. JPA + Spring Repositores\\nDrivers:\\n1. Technology must be simple and well known\\n2. Cannot require a lot of code writing\\n","Decision":"Option no 2 - JPA+Spring Repositories.\\n","tokens":62,"id":609}
{"File Name":"pcmt\/adr-003.md","Context":"## Context\\nDeploying to the cloud is a fundamental part of increasing software delivery\\nfrequency.  However once the frequency is increased, it becomes increasingly\\nneeded to manage the provisioning and configuration of those cloud resources\\nlest a step is forgotten, a security vulnerability is found, or an instance is\\ncorrupted.  Following patterns found in software development, such as managing\\ninfrastructure as code (IaC) and tracking changes in source control, are\\nwidely regarded as good practice today.\\nMany cloud providers have bespoke tooling that helps manage the creation of\\nresources on their infrastructure.  While this tooling is hugely powerful, the\\ncontext of PCMT is that our cloud resources should be kept as simple as\\npossible, in part due to our constraint to allow for on-prem deployments.\\nFurther an organization that chooses to deploy PCMT as a SaaS may not use the\\nsame cloud provider as the one the PCMT project uses.\\n","Decision":"We will use Terraform to provision cloud resources:  compute instances,\\nstorage, DNS, etc.\\n","tokens":197,"id":2949}
{"File Name":"edgex-docs\/0019-EdgeX-CLI-V2.md","Context":"## Context\\nThis ADR presents a technical plan for creation of a 2.0 version of edgex-cli which supports the new V2 REST APIs developed as part of the Ireland release of EdgeX.\\n","Decision":"1. Use standardized command-line args\/flags\\n| Argument\/Flag      | Description |\\n| ----------- | ----------- |\\n| `-d`, `--debug`      | show additional output for debugging purposes (e.g. REST URL, request JSON, \u2026). This command-line arg will replace -v, --verbose and will no longer trigger output of the response JSON (see -j, --json).       |\\n| `-j`, `--json`   | output the raw JSON response returned by the EdgeX REST API and *nothing* else. This output mode is used for script-based usage of the client.    |\\n| `--version`   | output the version of the client and if available, the version of EdgeX installed on the system (using the version of the metadata data service)   |\\n2. Restructure the Go code hierarchy to follow the [most recent recommended guidelines](https:\/\/github.com\/golang-standards\/project-layout). For instance \/cmd should just contain the main application for the project, not an implementation for each command - that should be in \/internal\/cmd\\n3. Take full advantage of the features of the underlying command-line library, [Cobra](https:\/\/github.com\/spf13\/cobra), such as tab-completion of commands.\\n4. Allow overlap of command names across services by supporting an argument to specify the service to use: `-m\/--metadata`, `-c\/--command`, `-n\/--notification`, `-s\/--scheduler` or `--data` (which is the default). Examples:\\n- `edgex-cli ping --data`\\n- `edgex-cli ping -m`\\n- `edgex-cli version -c`\\n5. Implement all required V2 endpoints for core services\\n**Core Command**\\n- **`edgex-cli command`** `read | write | list`\\n**Core Data**\\n- **`edgex-cli event`** `add | count | list | rm | scrub**`\\n- **`edgex-cli reading`** `count | list`\\n**Metadata**\\n- **`edgex-cli device`**  `add | adminstate | list | operstate | rm | update`\\n- **`edgex-cli deviceprofile`**  `add | list | rm | update`\\n- **`edgex-cli deviceservice`** ` add | list | rm | update`\\n- **`edgex-cli provisionwatcher`**  `add | list | rm | update`\\n**Support Notifications**\\n- **`edgex-cli notification`** `add | list | rm`\\n- **`edgex-cli subscription`** `add | list | rm`\\n**Support Scheduler**\\n- **`edgex-cli interval`** `add | list | rm | update`\\n**Common endpoints in all services**\\n- **`edgex-cli version`**\\n- **`edgex-cli ping`**\\n- **`edgex-cli metrics`**\\n- **`edgex-cli status`**\\nThe commands will support arguments as appropriate. For instance:\\n- `event list` using `\/event\/all` to return all events\\n- `event list --device {name}` using `\/event\/device\/name\/{name}` to return the events sourced from the specified device.\\n6.  Currently, some commands default to always displaying GUIDs in objects when they're not really needed. Change this so that by default GUIDs aren't displayed, but add a flag which causes them to be displayed.\\n7. **scrub** may not work with Redis being secured by default. That might also apply to the top-level `db` command (used to wipe the entire db). If so, then the commands will be disabled in secure mode, but permitted in non-secure mode.\\n8. Have built-in defaults with port numbers for all core services and allow overrides, avoiding the need for static configuration file or configuration provider.\\n9. *(Stretch)* implement a `-o`\/`--output` argument which could be used to customize the pretty-printed objects (i.e. non-JSON).\\n10. *(Stretch)* Implement support for use of the client via the API Gateway, including being able to connect to a remote EdgeX instance. This might require updates in go-mod-core-contracts.\\n","tokens":44,"id":957}
{"File Name":"figgy\/0006-preservation-tombstones.md","Context":"## Context\\nAfter an item is deleted from the database all traces of it are gone, but if\\nit's been preserved then it's possible to restore it. However, finding the\\nparticular ID of the item to restore is difficult if all that's known are pieces\\nof its metadata (title, source metadata ID, etc.)\\nWe will remedy this by storing a \"tombstone\" of deleted items as a small record\\nof what was deleted.\\n","Decision":"1. When a preserved item is deleted it will create a \"tombstone\" containing the following\\nmetadata:\\n1. ID of deleted resource\\n1. Title of deleted resource\\n1. Original Filename of deleted resource (if a FileSet)\\n1. Embedded PreservationObject that existed at time of deletion\\n1. ID of Parent at time of deletion.\\n2. These tombstones will be displayed in the Figgy UI and used as a way to\\ndiscover material that can be recovered.\\n","tokens":96,"id":4827}
{"File Name":"midashboard-infrastructure\/0004-use-cloudwatch-logs-for-log-aggregation.md","Context":"## Context\\nContext here...\\n","Decision":"Decision here...\\n","tokens":7,"id":343}
{"File Name":"pcmt\/adr-004.md","Context":"## Context\\nWhile PCMT's basic unit of deployment is Docker and Docker-Compose\\n([ADR #2](adr-002.md)), and Terraform is able to provision a cloud environment\\n([ADR #3](adr-003.md)), we still need to address a gap where the computing\\nenvironment needs to be provisioned and client configuration needs to be loaded.\\n","Decision":"We will use ready-made Ansible roles to install the latest versions of\\nDocker and Docker-Compose, utilized through Terraform.\\nWe will build PCMT Ansible role's that will serve as a template and are\\navailable in the Ansible Galaxy repository.\\nConfiguration of a PCMT instance will be managed as part of an Ansible Playbook.\\n","tokens":75,"id":2954}
{"File Name":"cnp-design-documentation\/0008-jenkins-data-loss.md","Context":"## Context\\nAs per CNP-129: \"Ensure no data loss for Jenkins down time\", the requirements were to:\\n1.  Ensure no data loss for Jenkins down time\\n2.  Use a separate data drive for Jenkins\\n3.  Ensure that data drive has replication enabled\\n4.  Ensure that the data drive is backed up\\n5.  Ensure when the Jenkins box is rebuild, it mounts the (above) drive\\n6.  All data drives for IAAS should be encrypted\\n7.  manually Enable recovery VAULT for Jenkins\\n","Decision":"#### Data Loss\\nCurrently, the terraform script to create the infrastructure creates an additional volume as a jenkins \"data volume\".  This resolves point 2) and partially resolves point 1) as block devices currently support Locally Redundant Storage (LRS) with data being replicated to at least 3 places within the same data center.\\nOriginally, the intention was to use Azure Files (https:\/\/azure.microsoft.com\/en-gb\/services\/storage\/files\/) in order to have an NFS volume attached, which would ease migration to a Jenkins PaaS offering.  However, given the current state of that technology, Azure Files does not provide Zone-redundant storage (ZRS) and so does not meet the requirement of point 3).\\n#### Encryption\\nIn order to acheive the requirement for point 6), custom kernel modules are required to be installed on Linux systems to take advantage of the Azure Linux Agent (waagent) which communicates with the underlying hypervisor.  This is done during the Packer build of our base image, along with several other required packages.  As part of the bootstrap process for a Jenkins master, a script is installed and run to check for the presence of a data disk and configure it if required.  This script can be found here: https:\/\/raw.githubusercontent.com\/contino\/moj-module-jenkins\/master\/datadisk\/datadisk.sh?token=AJ8ylGQJSftk3zecme9EPRtUupRalF5Kks5aAdmiwA%3D%3D\\nUnfortunately, due to the way the decryption of the disk seems to take place (a virtual CDROM is mounted with the credentials required to decrypt the disk), the VM never comes back online once it has been rebooted.\\n**A summary of this bahaviour has been sent to Microsoft and we are currently awating feedback.**\\n#### Backup\\nCreating a Recovery Vault and backup policy achieve the requirement for point 4).  Terraform currently does not support this operation natively, so a Azure Resource Manager template is used to create the vault (based on the azure-quickstart-templates).  This works successfully, however, creation of a Recovery Vault policy fails to create (both on the CLI and via the console).  Looking at the current API specification for a vault policy, it seems like much of the old functionality has been regressed.\\n**A summary of this bahaviour has been sent to Microsoft and we are currently awating feedback.**\\n","tokens":115,"id":1074}
{"File Name":"simple-server\/017-ab-testing.md","Context":"## Context\\nOur primary goal at Simple is to reduce deaths from cardiovascular disease. To be able to do that, we need patients to return to the clinic for care. Currently, when a patient misses their follow-up appointment date by three days, we send them a polite text message reminding them to continue taking their medicine and to return to their clinic to get more. That is our last attempt via text to convince a patient to return. We would like to know if a different message or sending the message on a different date relative to their appointment date would result in a higher rate of patient return. We would also like patients who have recently stopped visiting their clinic to return to care, and we would like to know what type of message and frequency of message would be most effective for convincing these patients to return to care.\\n","Decision":"We will develop a framework for testing different messages, message delivery dates, and message delivery cadences.\\nThat framework will be able to run experiments for both patients who have upcoming appointments as well as patients who do not because they have not visited their clinic recently.\\nPatients who have appointments scheduled during the experiment date range will be referred to here as as \"current patients\".\\nPatients who last visited the clinic 35-365 days ago will be referred to here as \"stale patients\". We chose 35 days because most patients are expected to return to clinic monthly so a patient who hasn't been into clinic in over 35 days is probably late, and we chose 365 days as a cutoff because patients who haven't been to clinic in over a year are considered lost to follow up.\\nThose two types of experiments will require process differences. When the patient has an upcoming appointment, the reminders must be sent relative to the appointment date. Messages can be sent before, on, or after the appointment date.\\nMessaging for stale patients will run on a continual daily basis during the experiment. This is to stagger the messages over course of the experiment so as to not overwhelm clinics with many patients coming to a clinic on the same day.  We will add a specified number of random stale patients per day to the experiment and schedule them based on the messaging templates configured in the experiment.\\nIn both types of experiments, patients will be randomly placed into treatment groups. Treatment groups will define the message texts, number of messages, and when to send the messages. Patients in an experiment will not receive the standard missed visit reminder that is sent three days after a scheduled appointment. Control group patients will receive no messages at all.\\n","tokens":164,"id":1704}
{"File Name":"cosmos-sdk\/adr-036-arbitrary-signature.md","Context":"## Context\\nHaving the ability to sign messages off-chain has proven to be a fundamental aspect of nearly any blockchain. The notion of signing messages off-chain has many added benefits such as saving on computational costs and reducing transaction throughput and overhead. Within the context of the Cosmos, some of the major applications of signing such data includes, but is not limited to, providing a cryptographic secure and verifiable means of proving validator identity and possibly associating it with some other framework or organization. In addition, having the ability to sign Cosmos messages with a Ledger or similar HSM device.\\nFurther context and use cases can be found in the references links.\\n","Decision":"The aim is being able to sign arbitrary messages, even using Ledger or similar HSM devices.\\nAs a result signed messages should look roughly like Cosmos SDK messages but **must not** be a valid on-chain transaction. `chain-id`, `account_number` and `sequence` can all be assigned invalid values.\\nCosmos SDK 0.40 also introduces a concept of \u201cauth_info\u201d this can specify SIGN_MODES.\\nA spec should include an `auth_info` that supports SIGN_MODE_DIRECT and SIGN_MODE_LEGACY_AMINO.\\nCreate the `offchain` proto definitions, we extend the auth module with `offchain` package to offer functionalities to verify and sign offline messages.\\nAn offchain transaction follows these rules:\\n* the memo must be empty\\n* nonce, sequence number must be equal to 0\\n* chain-id must be equal to \u201c\u201d\\n* fee gas must be equal to 0\\n* fee amount must be an empty array\\nVerification of an offchain transaction follows the same rules as an onchain one, except for the spec differences highlighted above.\\nThe first message added to the `offchain` package is `MsgSignData`.\\n`MsgSignData` allows developers to sign arbitrary bytes valid offchain only. Where `Signer` is the account address of the signer. `Data` is arbitrary bytes which can represent `text`, `files`, `object`s. It's applications developers decision how `Data` should be deserialized, serialized and the object it can represent in their context.\\nIt's applications developers decision how `Data` should be treated, by treated we mean the serialization and deserialization process and the Object `Data` should represent.\\nProto definition:\\n```protobuf\\n\/\/ MsgSignData defines an arbitrary, general-purpose, off-chain message\\nmessage MsgSignData {\\n\/\/ Signer is the sdk.AccAddress of the message signer\\nbytes Signer = 1 [(gogoproto.jsontag) = \"signer\", (gogoproto.casttype) = \"github.com\/cosmos\/cosmos-sdk\/types.AccAddress\"];\\n\/\/ Data represents the raw bytes of the content that is signed (text, json, etc)\\nbytes Data = 2 [(gogoproto.jsontag) = \"data\"];\\n}\\n```\\nSigned MsgSignData json example:\\n```json\\n{\\n\"type\": \"cosmos-sdk\/StdTx\",\\n\"value\": {\\n\"msg\": [\\n{\\n\"type\": \"sign\/MsgSignData\",\\n\"value\": {\\n\"signer\": \"cosmos1hftz5ugqmpg9243xeegsqqav62f8hnywsjr4xr\",\\n\"data\": \"cmFuZG9t\"\\n}\\n}\\n],\\n\"fee\": {\\n\"amount\": [],\\n\"gas\": \"0\"\\n},\\n\"signatures\": [\\n{\\n\"pub_key\": {\\n\"type\": \"tendermint\/PubKeySecp256k1\",\\n\"value\": \"AqnDSiRoFmTPfq97xxEb2VkQ\/Hm28cPsqsZm9jEVsYK9\"\\n},\\n\"signature\": \"8y8i34qJakkjse9pOD2De+dnlc4KvFgh0wQpes4eydN66D9kv7cmCEouRrkka9tlW9cAkIL52ErB+6ye7X5aEg==\"\\n}\\n],\\n\"memo\": \"\"\\n}\\n}\\n```\\n","tokens":129,"id":814}
{"File Name":"functionaut\/0004-no-type-checking.md","Context":"## Context\\nMaking sure that a function operates with the right input and providing the user with a good debugging experience is not trivial.\\nConsider this contrived example:\\n```javascript\\nfunction inc(x) {\\nif (typeof x != 'number') throw new Error('x is not a number');\\nreturn x + 1;\\n}\\nfunction inc_array(xs) {\\nif (!Array.isArray(xs)) throw new Error('xs is not an array');\\nreturn xs.map(x => inc(x)); \/\/ <- May or may not throw\\n}\\n```\\nWe cannot guarantee that `inc_array` won't throw even if `xs` is an array. If we wanted to give early feedback, we would need to go through each `x` and do the same type checking that `inc` already does. So if the function is invoked correctly we would type check `xs` once and each `x` twice!\\n","Decision":"This project will adopt a *\"Garbage In \u2014 Garbage Out\"* philosophy and deliver a *\"no hand-holding\"* library.\\n","tokens":192,"id":2688}
{"File Name":"govuk-kubernetes-discovery\/0002-structure-for-kubernetes-apps.md","Context":"## Context\\nWe wanted to agree a structure for Kubernetes applications so that we could create\\nconsistent layouts when creating new applications.\\nThis structure should be iterated upon when we start to work more heavily with\\nmigrating apps over to Kubernetes.\\nWe need to also be aware that developers may want to create their own applications\\nand so must be easily understood.\\nWe want to split open and private repositories easily when we come to start publishing\\nour work.\\n","Decision":"We have agreed on the following tree:\\n```\\n\u251c\u2500\u2500 data\\n\u2502   \u2514\u2500\u2500 my-awesome-app\\n\u2502   |   \u251c\u2500\u2500 development_secrets.yaml\\n|   |   \u2514\u2500\u2500 development_configmaps.yaml\\n|   \u2514\u2500\u2500 global\\n|       \u2514\u2500\u2500 development_configmaps.yaml\\n\u2514\u2500\u2500 manifests\\n\u2514\u2500\u2500 my-awesome-app\\n\u251c\u2500\u2500 deployment.yaml\\n\u2514\u2500\u2500 service.yaml\\n```\\nThis allows us to split the manifests section of Kubernetes configuration from the\\ndata values that get created and mounted into any deployments.\\n","tokens":95,"id":2796}
{"File Name":"govuk-terraform-provisioning\/0003-statefile-management.md","Context":"## Context\\nCurrently a terraform run contains every resource we manage. As the scope\\nof this repo expands that will eventually encompass everything on GOV.UK.\\nWe would like to reduce this scope to make both reasoning about changes and\\nlessen possible impact.\\n","Decision":"In order to isolate changes, for safety and scope, we will separate terraform\\nstate files in to multiple files, based on a combination of project and environment.\\nOnly one instance of Terraform should be used at a time. This means you are limited to\\nhaving a single local statefile. The tooling has been written to assume this is true.\\n","tokens":55,"id":3434}
{"File Name":"elife-base-images\/0006-use-docker-cp-to-extract-files.md","Context":"## Context\\nTest suites and other tools may produce output files while running inside a container.\\nDue to environmental differences, container users and groups often do not match the host's users and groups, blocking one side from deleting the files of the other; often with corner cases such as files being deleted but not subdirectories.\\nAutomated infrastructure usually runs as the `elife` or `jenkins` user, not as `root`. The docker daemon runs as root and is capable of bridging differences.\\n","Decision":"Use `docker cp` to exchange files between containers, especially from inside a container to the outside.\\n","tokens":103,"id":1509}
{"File Name":"cloud-platform\/010-live-0-to-live-1-Cluster.md","Context":"## Context\\nMigrating from live-0 to live-1 cluster. The reason behind this is based on the need to move to a dedicated AWS account (moj-cp), which will be much easier to support, and the need to move away from the Ireland (EU) region to the London (UK) region as Cloud Platform requirement to host data in the UK, rather than in Europe.\\n","Decision":"After some long consideration of possible options, the decision has been made to migrate from the live-0 cluster to the new live-1 cluster.\\nSince we only want to be running a single cluster, we will need to shut down live-0 as soon as it's no longer needed. Also services migrate from live-0 to live-1 sooner will avoid the complexities of running two parallel clusters.\\n","tokens":83,"id":617}
{"File Name":"edgex-docs\/0004-Feature-Flags.md","Context":"## Context\\nOut of the proposal for releasing on time, the community suggested that we take a closer look at feature-flags.\\nFeature-flags are typically intended for users of an application to turn on or off new or unused features. This gives user more control to adopt a feature-set at their own pace \u2013 i.e disabling store and forward in App Functions SDK without breaking backward compatibility.\\nIt can also be used to indicate to developers the features that are more often used than others and can provided valuable feedback to enhance and continue a given feature. To gain that insight of the use of any given feature, we would require not only instrumentation of the code but a central location in the cloud (i.e a TIG stack) for the telemetry to be ingested and in turn reported in order to provide the feedback to the developers. This becomes infeasible primarily because the cloud infrastructure costs, privacy concerns, and other unforeseen legal reasons for sending \u201cUsage Metrics\u201d of an EdgeX installation back to a central entity such as the Linux Foundation, among many others. Without the valuable feedback loop, feature-flags don\u2019t provide much value on their own and they certainly don\u2019t assist in increasing velocity to help us deliver on time.\\nPutting aside one of the major value propositions listed above, feasibility of a feature flag \u201cmodule\u201d was still evaluated. The simplest approach would be to leverage configuration following a certain format such as FF_[NewFeatureName]=true\/false. This is similar to what is done today. Turning on\/off security is an example, turning on\/off the registry is another. Expanding this further with a module could offer standardization of controlling a given feature such as `featurepkg.Register(\u201cMyNewFeature\u201d)` or `featurepkg.IsOn(\u201cMyNewFeature\u201d)`. However, this really is just adding complexity on top of the underlying configuration that is already implemented. If we were to consider doing something like this, it lends it self to a central management of features within the EdgeX framework\u2014either its own service or possibly added as part of the SMA. This could help address concerns around feature dependencies and compatibility. Feature A on Service X requires Feature B and Feature C on Service Y. Continuing down this path starts to beget a fairly large impact to EdgeX for value that cannot be fully realized.\\n","Decision":"The community should NOT pursue a full-fledged feature flag implementation either homegrown or off-the-shelf.\\nHowever, it should be encouraged to develop features with a wholistic perspective and consider leveraging configuration options to turn them on\/off. In other words, once a feature compiles, can work under common scenarios, but perhaps isn\u2019t fully tested with edge cases, but doesn\u2019t impact any other functionality, should be encouraged.\\n","tokens":469,"id":960}
{"File Name":"amf\/0007-ref-with-facets-parsing-for-draft-2019-09.md","Context":"## Context\\nJson Schema Draft 2019-09 indicates that the $ref entry can now have other keywords alongside it. This is a departure from previous drafts as they didn't allow it.\\nThe validation result of $ref must be AND'ed to the validation of the other facets, similar to what an allOf, oneOf and others do.\\n","Decision":"Avoid creating a new field in the model for references as this would be confusing for clients as they would have to take into account that field on very particular ocasions.\\nInstead, we decided that if there is a $ref with keywords beside it, it should be parsed into an allOf in the same shape.\\n### Cases\\n#### Standalone ref\\nIf a json map has a $ref entry in it and it is a single entry map, then that $ref will be parsed as a link and returned as is.\\n```json\\n{\\n\"type\": \"object\",\\n\"properties\": {\\n\"name\": {\\n\"$ref\": \"#\/somewhere\"\\n}\\n}\\n}\\n```\\n#### Ref with adjacent keywords\\nOriginal\\n```json\\n{\\n\"type\": \"object\",\\n\"$ref\": \"#\/somewhere\",\\n\"properties\": {\\n\"name\": {\\n\"type\": \"string\"\\n}\\n}\\n}\\n```\\nTransformed\\n```json\\n{\\n\"type\": \"object\",\\n\"allOf\": [\\n{\"$ref\": \"#\/somewhere\"}\\n],\\n\"properties\": {\\n\"name\": {\\n\"type\": \"string\"\\n}\\n}\\n}\\n```\\n#### Ref with adjacent keywords and allOf\\nOriginal\\n```json\\n{\\n\"type\": \"object\",\\n\"$ref\": \"#\/somewhere\",\\n\"properties\": {\\n\"name\": {\\n\"type\": \"string\"\\n}\\n},\\n\"allOf\": [\\n{\"$ref\": \"#\/somewhereElse\"}\\n]\\n}\\n```\\nTransformed\\n```json\\n{\\n\"type\": \"object\",\\n\"allOf\": [\\n{\"$ref\": \"#\/somewhere\"},\\n{\"$ref\": \"#\/somewhereElse\"}\\n],\\n\"properties\": {\\n\"name\": {\\n\"type\": \"string\"\\n}\\n}\\n}\\n```\\n","tokens":72,"id":1390}
{"File Name":"james-project\/0020-cassandra-mailbox-object-consistency.md","Context":"## Context\\nMailboxes are denormalized in Cassandra in order to access them both by their immutable identifier and their mailbox\\npath (name):\\n- `mailbox` table stores mailboxes by their immutable identifier\\n- `mailboxPathV2` table stores mailboxes by their mailbox path\\nWe furthermore maintain two invariants on top of these tables:\\n- **mailboxPath** unicity. Each mailbox path can be used maximum once. This is ensured by writing the mailbox path first\\nusing Lightweight Transactions.\\n- **mailboxId** unicity. Each mailbox identifier is used by only a single path. We have no real way to ensure a given mailbox\\nis not referenced by two paths.\\nFailures during the denormalization process will lead to inconsistencies between the two tables.\\nThis can lead to the following user experience:\\n```\\nBOB creates mailbox A\\nDenormalization fails and an error is returned to A\\nBOB retries mailbox A creation\\nBOB is being told mailbox A already exist\\nBOB tries to access mailbox A\\nBOB is being told mailbox A does not exist\\n```\\n","Decision":"We should provide an offline (meaning absence of user traffic via for exemple SMTP, IMAP or JMAP) webadmin task to\\nsolve mailbox object inconsistencies.\\nThis task will read `mailbox` table and adapt path registrations in `mailboxPathV2`:\\n- Missing registrations will be added\\n- Orphan registrations will be removed\\n- Mismatch in content between the two tables will require merging the two mailboxes together.\\n* [Discussion on the mailing list](https:\/\/www.mail-archive.com\/server-dev@james.apache.org\/msg64432.html)\\n","tokens":230,"id":2877}
{"File Name":"arch\/0010-git-basics-and-style-guide.md","Context":"## Context\\nWe use git and GitHub in different ways, it\u2019s emergency to teach team members with the git basics and style guide we will use.\\n","Decision":"### git basics\\n1. setup ssh keys ([https:\/\/github.com\/settings\/keys][1]) or GUI;\\n2. clone repo into local system: `git clone git@github.com:huifenqi\/django-project-skeleton.git`;\\n3. files store in three stages:\\n![][image-1]\\n1. `Working Directory`: holds the actual files;\\n2. `Index`: staging area which store your changed files;\\n3. `HEAD`: points to the last commit you've made.\\n4. `Working Directory` -\\> `Index`: `git add <filename>` or `git add *`;\\n5. `Index` -\\> `HEAD`: `git commit -m \"Commit message\"`;\\n6. push updates to Github: `git push origin master`;\\n7. create branch: `git checkout -b feature\/x`\\n![][image-2]\\n8. push branch to Github and others can see: `git push origin <branch_name>`;\\n9. sync local with Github: `git pull`;\\n10. make a new tag: `git tag <tag_name>`;\\n11. show history: `git log`;\\n12. show changes: `git diff <previous_commit_hash>`;\\n13. others: `git status`, `git branch`, `git tag`, etc.\\n### git style guide\\n#### Branches\\n* Choose short and descriptive names: [https:\/\/github.com\/agis-\/git-style-guide#branches][2];\\n* Use dashes to separate words.\\n#### Commits\\n* Each commit should be a single logical change. Don't make several logical changes in one commit;\\n#### Messages\\n* when writing a commit message, think about what you would need to know if you run across the commit in a year from now.\\n### Refs\\n* [http:\/\/rogerdudler.github.io\/git-guide\/index.html][3]\\n* [https:\/\/confluence.atlassian.com\/bitbucketserver\/basic-git-commands-776639767.html][4]\\n* [https:\/\/github.com\/agis-\/git-style-guide][5]\\n","tokens":31,"id":2424}
{"File Name":"early-careers-framework\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":4279}
{"File Name":"tendermint\/adr-002-event-subscription.md","Context":"## Context\\nIn the light client (or any other client), the user may want to **subscribe to\\na subset of transactions** (rather than all of them) using `\/subscribe?event=X`. For\\nexample, I want to subscribe for all transactions associated with a particular\\naccount. Same for fetching. The user may want to **fetch transactions based on\\nsome filter** (rather than fetching all the blocks). For example, I want to get\\nall transactions for a particular account in the last two weeks (`tx's block time >= '2017-06-05'`).\\nNow you can't even subscribe to \"all txs\" in Tendermint.\\nThe goal is a simple and easy to use API for doing that.\\n![Tx Send Flow Diagram](img\/tags1.png)\\n","Decision":"ABCI app return tags with a `DeliverTx` response inside the `data` field (_for\\nnow, later we may create a separate field_). Tags is a list of key-value pairs,\\nprotobuf encoded.\\nExample data:\\n```json\\n{\\n\"abci.account.name\": \"Igor\",\\n\"abci.account.address\": \"0xdeadbeef\",\\n\"tx.gas\": 7\\n}\\n```\\n### Subscribing for transactions events\\nIf the user wants to receive only a subset of transactions, ABCI-app must\\nreturn a list of tags with a `DeliverTx` response. These tags will be parsed and\\nmatched with the current queries (subscribers). If the query matches the tags,\\nsubscriber will get the transaction event.\\n```\\n\/subscribe?query=\"tm.event = Tx AND tx.hash = AB0023433CF0334223212243BDD AND abci.account.invoice.number = 22\"\\n```\\nA new package must be developed to replace the current `events` package. It\\nwill allow clients to subscribe to a different types of events in the future:\\n```\\n\/subscribe?query=\"abci.account.invoice.number = 22\"\\n\/subscribe?query=\"abci.account.invoice.owner CONTAINS Igor\"\\n```\\n### Fetching transactions\\nThis is a bit tricky because a) we want to support a number of indexers, all of\\nwhich have a different API b) we don't know whenever tags will be sufficient\\nfor the most apps (I guess we'll see).\\n```\\n\/txs\/search?query=\"tx.hash = AB0023433CF0334223212243BDD AND abci.account.owner CONTAINS Igor\"\\n\/txs\/search?query=\"abci.account.owner = Igor\"\\n```\\nFor historic queries we will need a indexing storage (Postgres, SQLite, ...).\\n### Issues\\n- https:\/\/github.com\/tendermint\/tendermint\/issues\/376\\n- https:\/\/github.com\/tendermint\/tendermint\/issues\/287\\n- https:\/\/github.com\/tendermint\/tendermint\/issues\/525 (related)\\n","tokens":164,"id":1941}
{"File Name":"drt-v2\/0011-use-play.md","Context":"## Context\\nWe need a webserver. Obvious choices between spray (or akka-http) or Play. Given we're doing very little that uses the\\nfull power of play, I'd normally have gone with akka-http, but this reactjs tutorial started with Play, so we stuck with it\\nfor the spik\\n","Decision":"Play\\n","tokens":66,"id":1915}
{"File Name":"toc-poc\/1577573517559_language_choice.md","Context":"## Context\\nCurrently I'm capable of using two languages to develop what I'm intending to do - Ruby and Javascript. It didn't make sense to use both, since functionality is not that big. The main thing I wanted to check was related to presentation layer, so it kinda limited my options as well.\\n","Decision":"Use Javascript to develop proof of concept.\\n","tokens":63,"id":1884}
{"File Name":"james-project\/0012-jmap-partial-reads.md","Context":"## Context\\nJMAP core RFC8620 requires that the server responds only properties requested by the client.\\nJames currently computes all of the properties regardless of their cost, and if it had been asked by the client.\\nClearly we can save some latencies and resources by avoiding reading\/computing expensive properties that had not been explicitly requested by the client.\\n","Decision":"Introduce two new datastructures representing JMAP messages:\\n- One with only metadata\\n- One with metadata + headers\\nGiven the properties requested by the client, the most appropriate message datastructure will be computed, on top of\\nexisting message storage APIs that should remain unchanged.\\nSome performance tests will be run in order to evaluate the improvements.\\n","tokens":73,"id":2881}
{"File Name":"opg-modernising-lpa-docs\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"This will allow us to get a wider range of opinions on our decisions which we could not have got before.\\n### Interrogate your data decisions\\nN\/A\\n### Decision\\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n### Consequences\\nSee Michael Nygard's article, linked above.\\nADRs will be public for visibility and collaboration.\\nPull requests and Github issues can be used to drive conversations.\\n","tokens":16,"id":3729}
{"File Name":"gsp\/ADR017-vendor-provided-container-orchestration.md","Context":"## Context\\nFollowing the [rollout of AWS EKS in London](https:\/\/aws.amazon.com\/about-aws\/whats-new\/2019\/02\/amazon-eks-available-in-mumbai--london--and-paris-aws-regions\/) it is now an attractive alternative to the hand-rolled kubernetes installation that was created as a result of [ADR003](ADR003-container-orchestration.md). This will bring numerous benefits:\\n* reducing the amount of infrastructure we manage in-house, by offloading it to AWS\\n* better alignment with [Technology & Operations Strategic Principle #3 - \"Use fully managed cloud services by default\"](https:\/\/reliability-engineering.cloudapps.digital\/documentation\/strategy-and-principles\/re-principles.html#3-use-fully-managed-cloud-services-by-default)\\nAs of 1.12, EKS supports what we need (e.g. Istio, kiam etc.).\\n","Decision":"We will host the GDS Supported Platform on AWS EKS.\\n","tokens":194,"id":3911}
{"File Name":"beis-report-official-development-assistance\/0013-travis-only-tests-with-docker-containers.md","Context":"## Context\\nWe noticed that deployments to staging were silently failing since a gem update that succeeded in Travis did not succeed in our docker environment, where docker is used on our live environment.\\ndxw have a default stance to test with containers where we host with containers: https:\/\/github.com\/dxw\/tech-team-rfcs\/blob\/main\/rfc-013-use-docker-to-deploy-and-run-applications-in-containers.md\\n","Decision":"Travis uses Docker containers to test the application on every build and deploy.\\n","tokens":86,"id":2398}
{"File Name":"dalmatian-frontend\/0008-use-different-aws-account-for-interacting-with-infrastructure-variables.md","Context":"## Context\\nThe issue motivating this decision, and any context that influences or constrains the decision.\\nDalmatian stores variables in AWS Parameter store. Each infrastructure has an `account_id`\\nassociated with it which is used to manage which account variables are read, added and removed from - this is as you might expect.\\nDalmatian also stores another type of variable which the app has initially given\\nthe name of \"infrastructure variable\". These variables are NOT managed by the\\nindividual AWS accounts associated with each infrastructure. Rather they are all\\nmanaged within a central AWS account and given a meaningful name space to relate\\nit to a dependent service.\\nFor this apps purposes it would have been easier for all variables associated with\\na service to live in the same account. I believe there are restrictions to how\\nDalmatian itself (rather than the services there on) is deployed which has steered\\nDalmatian into being designed this way.\\n","Decision":"Handle the complexity of 2 different types of AWS Client within the service rather\\nthan seeking to change the way infrastructure variables are provisioned to align\\nwith the infrastructure they belong to.\\n","tokens":199,"id":2558}
{"File Name":"fundraising-application\/015_Semantic_Versioning.md","Context":"## Context and Problem Statement\\nWe need a way to deploy specific versions of our bounded contexts, having\\nreproducible releases of our software in testing and production .\\n## Decision Drivers\\n* We want to have frequent releases of the software, adding new features\\netc.\\n* We want to keep our dependencies up to date on a monthly basis, using\\n`composer update`\\n* We want to have reproducible versions of our software - `composer\\ninstall` must install the same code on the CI machine, each\\ndevelopers machine, the user acceptance environment and the production\\nenvironment.\\n* We want to have a trunk-based development process, where the current\\nmaster of each repository is always working and we can deploy at any time.\\n* We regularly update the `wmde\/fundraising-frontend-content` dependency.\\n","Decision":"* We want to have frequent releases of the software, adding new features\\netc.\\n* We want to keep our dependencies up to date on a monthly basis, using\\n`composer update`\\n* We want to have reproducible versions of our software - `composer\\ninstall` must install the same code on the CI machine, each\\ndevelopers machine, the user acceptance environment and the production\\nenvironment.\\n* We want to have a trunk-based development process, where the current\\nmaster of each repository is always working and we can deploy at any time.\\n* We regularly update the `wmde\/fundraising-frontend-content` dependency.\\nWe will use semantic versioning for 3 months (until 2020-08-14). After\\nthat period we'll evaluate the actual benefits and drawbacks of semantic\\nversioning. If the drawbacks outweigh the benefits and we can't fix them\\nthrough other means (automation, CI), we'll abandon semantic\\nversioning and return to trunk-based dependencies.\\n","tokens":173,"id":1531}
{"File Name":"read-more-api\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":726}
{"File Name":"moneycount-api\/001-Choose_Spring_Boot.md","Context":"## Context\\nI have to choose a framework to implement moneycount-api project. It could be a familiar framework, such as Spring Boot, Spark, Servlets + Jersey or even Python + Flask, or I could try another different framework to learn something new.\\n","Decision":"I decided to use Spring Boot 1.5.10, the last stable 1.X release at the time, because I want to have a first version of the software in a well known framework that allows me to implement it fast, with easy integration with other features that I can choose in future improvements.\\n","tokens":53,"id":3072}
{"File Name":"manuela\/0007-how-to-implement-multiple-demo-instances.md","Context":"## Context\\nWhen multiple demo instances exist in parallel, we need to avoid to step on each others toes. The scope is\\n- the manuela-gitops repository, where component deployment and configuration takes place as part of a demo run\\n- the manuela-dev repository, where coding changes take place as part of a demo run\\nOptions:\\n1. **BRANCHES** inside the existing sa-mw-dach repos, for each manuela demo env there would be a branch (e.g. name based on a convention).\\nPros:\\n- Easiest to set up since it's literally just cloning the central repos.\\n- only need to adjust branch names for new demo instance (however the number of required adjustments is likely the same)\\nCons:\\n- need to coordinate the creation\/assignment of branches\\n- all demoers need write access to the repos\\n- Danger of cluttering the repo with \"abandoned\" branches\\n2. **FORKS**: The \"owner\" of the new installation forks all repos in GitHub or creates new repositories from scratch. The changing remote URL needs to be adjusted when setting up the demo. Preferably, there would be somewhere one central configuration (GIT_BASE_URL) defining from which fork to pull from.\\nPros:\\n- demo instances can be set up without any coordination\\n- No conflicts when branches are used for e.g. production staging approvals\\nCons:\\n- need to adjust\\n- pipeline configs (pointing to forks of manuela-dev\/manuela-gitops)\\n- argoCD applications (poingint to forks of manuela-gitops)\\n3. **DEDICATED GIT**: We could deploy a dedicated GIT(LAB?) to a namespace, place the manuela-gitops +  manuela-dev repos there. As long as we can use only cluster-internal URLs these will not change across instances.\\nPros:\\n- Consistent URLs at least in a single-cluster scenario, no need to adapt anything\\nCons:\\n- Will require similar adaptations as **FORKS** in a multi-cluster scenario, since we will need to use external URLs for cross-cluster communication\\n","Decision":"Use **FORKS**: this requires least coordination, access rights or additional components.\\n","tokens":439,"id":2971}
{"File Name":"holdings-backend\/0002-mongodb.md","Context":"## Context\\nThe existing print holdings system keeps all data in MySQL. While this allows\\nflexible querying, it is also computationally expensive and is difficult to\\nscale.\\nKnown use cases largely involve computing things specific to a clusters'\\nholdings and HathiTrust items, so one way to parallelize queries is in a\\nmap-reduce fashion - compute something about each cluster, then process the\\ndata from each cluster into a final result.\\n","Decision":"We will use MongoDB for a persistent data store. Each print holdings cluster\\nwill be a document, and will contain holdings, HathiTrust items, and shared\\nprint commitments for that cluster as sub-documents.\\n","tokens":93,"id":4197}
{"File Name":"FlowKit\/0003-http-api.md","Context":"## Context\\nOriginally, the software tool that became FlowKit was designed to be used as an extensible library, which connected to one shared database. Users would extend their own copy of the library to add query types, or even modify existing ones.\\nThis introduced considerable difficulties, e.g.:\\n- No guarantee that an analysis written by one person could be run by another, or by the same person in future.\\n- All users required highly privileged access to the database\\n- No way to manage usage of shared resource\\n- No way to ensure that upstream changes were in use\\n- Difficult to effectively exploit the ability to reuse already computed results between analysts\\n- Significant complexity and blurred functional boundaries in the main library\\n- Very difficult to use the tool outside the Python ecosystem\\n- Substantial challenges in logging access and activity\\nThis motivates the revised design, where there is a _single_ copy of the library responsible for constructing and running queries, accessed through a language neutral HTTP API. This facilitates some significant improvements:\\n- Easy to produce clients for multiple language ecosystems\\n- Can make substantial changes to the enclosed code, database structure etc. with very little disruption\\n- Supports granular access control\\n- Enables more secure storage of raw data, by removing direct access to the data\\n- Allows for much more efficient sharing of resources\\n- Supports comprehensive logging\\n- Much clearer 'seams' between functional parts, and simpler codebase\\n- Simpler code, because scheduling of query runs is controlled by a single point.\\n- Considerable opportunities to be more efficient in scheduling runs and caching of queries.\\n","Decision":"FlowMachine will be wrapped by an HTTP API.\\n","tokens":328,"id":5056}
{"File Name":"simple-android\/007-screen-architecture-v2.md","Context":"## Context\\nOur [current screen architecture](001-screen-controllers.md) has a bunch of problems:\\n- We recreate application state from the UI state, which depends on some hidden behaviour:\\n- RxBinding should have initial value observables that emit the current event immediately.\\n- All widgets on the screen should save and restore state properly.\\n- The entire event stream is replayed because the system is setup in a way that they begin emission of events like ScreenCreated before the entire\\nevent handling loop is setup.\\n- The controller handles both, the business logic and the view logic.\\nAll of these come together to make it hard to test and maintain screens. In addition, screens which perform a lot of business logic in memory require\\na lot of working around the fact that the architecture depends on the state being saved either in persistence or by the platform mechanisms.\\n","Decision":"### Goals\\n- Separate presentation and business logic so that they can tested independently of each other.\\n- Make UI state explicit and save\/restore it manually instead of depending on hidden behaviour.\\n- Make it easy to migrate from the v1 architecture to the v2 for existing screens.\\n### Implementation\\nWe split the controller into two discrete pieces, each with their own responsibility.\\n- `UiStateProducer`: This will be responsible for performing the function of the controller related to the business logic.\\n- `UiChangeProducer`: This will be responsible for performing the function of the controller related to the presentation logic.\\nIn addition, we introduce a helper class, `ViewControllerBinding`, which is used to tie the state producer and consumer together with the event\\nstream.\\n#### Reference implementation\\nA reference implementation of the complete architecture can be found\\nat [this commit](https:\/\/github.com\/simpledotorg\/simple-android\/blob\/9e8412259e034e555fa40c2b07810a98d736df95\/app\/src\/main\/java\/org\/simple\/clinic\/shortcodesearchresult\/ShortCodeSearchResultScreen.kt)\\n.\\n#### Terminology\\n- `Event`: a generated event. This will typically be events generated by the user interface, but might also include events generated by the platform\\nlike sensors, camera, etc. These will generally be represent as Kotlin data classes that implement the `UiEvent` interface.\\n- `UiState`: a Kotlin data class that represents everything needed to render the content of a given screen.\\n- `Ui`: an interface which represents all the functionality that the actual screen needs to provide to the controller.\\n- `UiChange`: a Kotlin lambda that has the signature `(Ui) -> Unit`.\\n#### Screen Setup Process\\n- Define a `UiState` for whatever view\/screen is being built.\\n- Create a `UiStateProducer` which is an `ObservableTransformer<Event, UiState>`. This class will be responsible for business logic by transforming a\\nstream of events into a stream of UI states.\\n- Create a `UiChangeProducer` which is an `ObservableTransformer<UiState, UiChange>`. This is responsible for presentation logic by transforming a\\nstream of UI states into a stream of lambdas which will executed on the `Ui`.\\n- Create a controller which is an `ObservableTransformer<UiEvent, UiChange>` which will compose `UiStateProducer` and `UiChangeProducer` internally.\\n- Use the `ViewControllerBinding` to tie the event stream to the view.\\n##### Sample\\n```kotlin\\n@Inject\\nlateinit var uiStateProducer: ShortCodeSearchResultStateProducer\\n@Inject\\nlateinit var uiChangeProducer: ShortCodeSearchResultUiChangeProducer\\nlateinit var binding: ViewControllerBinding<UiEvent, ShortCodeSearchResultState, ShortCodeSearchResultUi>\\noverride fun onFinishInflate() {\\n\/\/ Inject the screen\\nbinding = ViewControllerBinding.bindToView(this, uiStateProducer, uiChangeProducer)\\nnewPatientButton.setOnClickListener { binding.onEvent(SearchPatient) }\\n}\\n```\\n### Goals Review\\n#### Separation of concerns and testability\\nSince the business logic and presentation logic are separated into two discrete components, testing them is simpler since they can be tested\\nindependently of each other. In addition, we can test the behaviour of the business logic using value testing (asserting the generated states) which\\nis more readable than verifying behaviours using mocks.\\n#### UI state saving\/restoration\\nSince the state of a screen is represented as a data class, saving and restoring the state is easy, and has just two steps:\\n1. Make the `UiState` class implement the `Parcelable` interface.\\n2. The `ViewControllerBinding` exposes two functions, `latestState()` and `restoreSavedState()`, which can be used in conjunction with the platform\\nlifecycle methods.\\n#### Migrating from the older architecture\\nOne of the major concerns about moving from the v1 architecture to v2 is breaking existing features while moving code around. Since both the v1 and v2\\narchitectures expose the same overall interface (`ObservableTransformer<Event, UiChange>`), we can replace the controller in tests for the older\\nscreens with a composition of the state producer and state consumer and verify that no behaviours were changed when migrating.\\n","tokens":180,"id":1132}
{"File Name":"scientific-thesis-template\/0003-use-minted-for-code-highlithing.md","Context":"## Context and Problem Statement\\nSource code needs to be highlighted\\n","Decision":"Chosen option: \"minted\", because\\n1. Listings is inferior because it doesn't utilize a full lexer\\n2. Minted offers integration with [pygments](http:\/\/pygments.org\/) and is actively maintained.\\n3. The person who took over minted development, [evaluated all alternatives](https:\/\/tex.stackexchange.com\/a\/103471\/9075) and came to the conclusion that minted is the most promising solution.\\nPositive Consequences:\\n* Source is highlighted properly\\nNegative consequences:\\n* Users need to install python and pygments\\n","tokens":13,"id":252}
{"File Name":"gsp\/ADR009-multitenant-ci-cd.md","Context":"## Context\\nTwo models have been proposed concerning CI and CD tool sets:\\n1. Multi-tenant: all tenants, including Reliability Engineering, share that same CI and CD instance\\n2. Per-tenant: each tenant has their own CI and CD cluster\\n","Decision":"- There will be a single CI and CD toolset used by all tenants of the new service\\n","tokens":54,"id":3882}
{"File Name":"architectural-decision-log\/0007-placement-support.md","Context":"## Context and Problem Statement\\nFor some time hamlet has had the idea of a `placement` - a mechanism to dynamically determine where resources should be hosted.\\nPlacements are currently part of the hierarchy of the state tree, and the resource groups within the occurrence structure were intended to allow groups of resources to be placed into different provider accounts.\\nHowever to date, a number of limitations and inefficiencies stemming from hamlet's original design has prevented full exploitation of the placement concept;\\n- the assembly of the solution before the engine is run\\n- the focus of a template run on a single segment\\n- the generation of account templates uses different concepts to those of product templates\\n- the maintenance of critical configuration information such as environment to account mappings external to the cmdb.\\nThe introduction of dynamic cmdb loading and the input pipeline processor with full cmdb qualification presents an opportunity to address these long standing issues and complete the implementation of placement support. The question is how.\\n## Decision Drivers <!-- optional -->\\n* reuse existing concepts and ways of doing things as much as possible\\n* given a cmdb, no other configuration information should be necessary to deploy a product\\n","Decision":"* reuse existing concepts and ways of doing things as much as possible\\n* given a cmdb, no other configuration information should be necessary to deploy a product\\nChosen option: \"Extend link semantics\", because it addresses the decision drivers best (see below) and aligns with the ADR-0002 decision previously made.\\n### Positive Consequences <!-- optional -->\\n* reduced configuration complexity - everything in one place\\n* better validation of configurations\\n* links have proven very flexible so expect the proposed changes to be re-purposed for other things in the future\\n* straightforward transition process once infrastructure in place\\n### Negative Consequences <!-- optional -->\\n* Expansion of semantics of existing concepts may be harder to document, understand and explain than dedicated new concepts\\n","tokens":236,"id":2978}
{"File Name":"island.is-glosur\/0010-cms.md","Context":"## Context and Problem Statement\\nisland.is will be maintaining and publishing content from many different government agencies and institutions. Their technical skill may vary a great deal, the content skill may also be lacking, therefore it is paramount for the system to be user friendly and intuitive.\\nAgencies and institutions should have enough autonomy with regards to editing content they are responsible for, to minimise the manual labour required by the island.is editors.\\nWhich CMS system would best suit the needs of island.is?\\n## Decision Drivers\\n- Content needs to be editable by non technical users\\n- Content needs to be accessible across multiple domains and platforms\\n- Setup should be simple for developers new to the project\\n- The system should manage flexible content structures to limit systems impact on design\\n- The system should be user friendly and easy to use for a non technical person\\n- The system needs to offer a suitable workflow option to ease content management once multiple agencies start to contribute\\n","Decision":"- Content needs to be editable by non technical users\\n- Content needs to be accessible across multiple domains and platforms\\n- Setup should be simple for developers new to the project\\n- The system should manage flexible content structures to limit systems impact on design\\n- The system should be user friendly and easy to use for a non technical person\\n- The system needs to offer a suitable workflow option to ease content management once multiple agencies start to contribute\\nDevs narrowed the choice down to two options Contentful and Contentstack.\\nBoth systems meet the required featureset.\\nA decision from management was made to use Contentful.\\nContentful is deemed to have a larger presence in the Icelandic dev community.\\nContentful is also believed to have a stronger funding base.\\nContentful is already implemented in some of our projects.\\n","tokens":191,"id":4770}
{"File Name":"kafka\/0006-require-php-7-3.md","Context":"## Context\\nThe issue motivating this decision, and any context that influences or constrains the decision.\\n","Decision":"The change that we're proposing or have agreed to implement.\\n","tokens":21,"id":639}
{"File Name":"terraform-aws-dynamodb\/0002-autoscaler-terraform-sub-module.md","Context":"## Context\\nDynamoDB can utilise the [AWS Application Auto\\nScaling](https:\/\/docs.aws.amazon.com\/autoscaling\/application\/APIReference\/Welcome.html)\\nto dynamically adjust provisioned throughput capacity on your behalf. Not all\\nuses of DynamoDB will use this autoscaling service.\\nThere is a plan to create a Terraform `Data Storage Module` which will include\\nthe DynamoDB module.\\n","Decision":"To support autoscaling a Terraform sub module will be utilised.\\nThis sub module will be included in the DynamoDB module using Terraform `module`\\nsyntax. An `enabled` toggle will be used to determine whether or not the\\nautoscaling resources will be created.\\n","tokens":83,"id":2542}
{"File Name":"my-notes\/react-hooks.md","Context":"### Context\\n- new way of writing React code\\n### Decision\\nSince it is still considered new and will be overwhelming for new developers, I don't think it's worth starting yet.\\n","Decision":"Since it is still considered new and will be overwhelming for new developers, I don't think it's worth starting yet.\\n","tokens":39,"id":5086}
{"File Name":"fxa\/0024-upgrade-templating-toolset-of-auth-server-emails.md","Context":"## Context and Problem Statement\\n[Handlebars](https:\/\/www.npmjs.com\/package\/mustache), an extension of [Mustache](https:\/\/www.npmjs.com\/package\/mustache) ([see the differences](https:\/\/github.com\/handlebars-lang\/handlebars.js#differences-between-handlebarsjs-and-mustache)) used in `fxa-auth-server` email templates, can accommodate very limited logic in its templates which has been a pain point for Firefox Accounts engineers and discussion around using a different templating system [began here](https:\/\/github.com\/mozilla\/fxa\/issues\/4627). While converting our emails to a more modern templating solution, there is an opportunity to evaluate what stack would be the most ideal for FxA emails beyond a proposed templating solution. This includes evaluating our CSS options, improving how we can preview emails in various states, and our localization tools, and the best approach for landing new templates in production.\\n","Decision":"### Templating and Styling\\n- Option A - Continue to use Mustache and ad-hoc inline styles\\n- Option B - Use React server-side to generate static HTML templates with TailwindCSS\\n- Option C - Use EJS and MJML, using CSS options offered by MJML\\n### Email previewing\\n- Option A - Continue to use the `write-emails` command\\n- Option B - Use Storybook\\n- Option C - Use Mailtrap\\n---\\nFurthermore, there are a few **other decisions** worth noting that won\u2019t necessarily have pros\/cons lists:\\n- How to handle generating plaintext versions\\n- Transitioning to Fluent over GetText for localization\\n- Plans around integration involving feature flagging and the QA process\\n### Templating and Styling\\nChosen option: \"Option C - Use EJS and MJML, using CSS options offered by MJML\", because:\\n- HTML email has a lot of quirks - MJML shifts the burden of maintaining solutions for these off of FxA engineers now and in the future\\n- While we use React and Tailwind in other parts of FxA, React is heavy for an email solution since no state is involved, component reuse across FxA would likely be very minimal, and it involves a more complex build setup than EJS\\n- MJML helps significantly with responsiveness in emails, reducing time spent developing templates and making future email redesigns easier\\n### Email Previewing\\nChosen option: \"Option B - Use Storybook\", because:\\n- It provides us the flexibility to preview all of the different email states together\\n- We have a Storybook deployment already in place so hooking it up for `fxa-auth-server` will ensure consistency\\n- Easy to test and perform QA without needing to touch the codebase\\n- Support for CSS (whether it's plain old CSS, TailwindCSS or something else)\\nNote: Along with using storybook to view the various states of the templates, we are also planning on continued support of [Maildev](https:\/\/www.npmjs.com\/package\/maildev) since it has applications beyond just previewing emails.\\n### Other\\n**Plaintext files**: We'll use [html-to-text](https:\/\/github.com\/html-to-text\/node-html-to-text) to automatically generate plaintext versions of templates rendered to HTML. There may be a scenario in which the automatically-generated plaintext version does not look exactly how we'd like, in which case we can look into exporting both an MJML _and_ plaintext version of the email from the template file.\\n**Localization**: We will upgrade the localization tool from GetText to [Fluent](https:\/\/github.com\/projectfluent\/fluent.js) since it's preferred by the l10n team and other FxA front-ends are using it. With our chosen templating option we can make use of Fluent's [`@fluent\/dom`](https:\/\/github.com\/projectfluent\/fluent.js\/tree\/master\/fluent-dom) package.\\n**Integration & QA**: During development templates will be marked as being part of a release group. This could be in the form of a mapped list of template names or some variable associated with the template file. Each group will have a corresponding environment variable flag. When this release group\u2019s flag is enabled any templates that fall under it will be used when generating an email template in that environment; until a template's corresponding release group is enabled it is not used and the old\/current template will continue to be served. Release groups will be initially enabled in staging until QA has had an opportunity to thoroughly test and evaluate each template in the group, after which they can be incrementally enabled in production. To give us the most flexibiity we can add a new auth-server configuration value that can control which users email are supported and which email templates are supported for mjml. The auth-server can then expose a feature flag method to check this value and then render the correct template.\\n### Positive Consequences\\n- Using MJML abstracts HTML email peculiarities away and handles responsiveness for us\\n- Allows us to move away from inline CSS during development, and improves style reusability and consistency in template files\\n- Using EJS allows us to write templates using JavaScript, removing the complexities of custom syntaxes like JSX and Mustache\\n### Negative Consequences\\n- The Storybook setup will be much more complex than with a React and Tailwind solution\\n- Introduces new dependencies, and MJML introduces a small learning curve\\n- Our email templates have been battle-tested over the years, and this change could introduce potential new bugs across various clients\\n","tokens":196,"id":361}
{"File Name":"ReportMI-service-manual\/0008-use-aws-for-hosting.md","Context":"## Context\\n[CCS-ADR-0001][ccs-adr-0001] says that all new services should use a common\\ncloud hosting provider.\\nThis will enable CCS to consider consolidating future support and operations\\nfor these services.\\nThe decision is to use Amazon Web Services (AWS) for the next 6 months.\\n### Impacts on Data Submission Service\\nIn [ADR-0005][adr-0005] we decided to use Docker for packaging applications to\\nallow us to use a Platform as a Service offering.\\nIn [ADR-0006][adr-0006] we decided to use Terraform to create and document our\\ncloud based Infrastructure.\\nTerraform supports building infrastructure in AWS, and AWS provides several\\nPlatform as a Service offerings for applications in Docker containers (Elastic\\nBeanstalk and Elastic Container Service).\\n","Decision":"Data Submission Service will be hosted in AWS.\\n[adr-0005]: 0005-use-docker-for-applications.md\\n[adr-0006]: 0006-use-terraform-to-create-and-document-infrastructure.md\\n","tokens":178,"id":2051}
{"File Name":"react-transcript-editor\/2018-01-24-frequency-of-local-storage-save.md","Context":"## Context and Problem Statement\\n<!-- [Describe the context and problem statement, e.g., in free form using two to three sentences. You may want to articulate the problem in form of a question.] -->\\nThe issue is how often to save to local storage when a user types, previous implementation saved every 5 characters. But that caused issues [#86](https:\/\/github.com\/bbc\/react-transcript-editor\/issues\/86)\\n## Decision Drivers <!-- optional -->\\n<!-- * [driver 1, e.g., a force, facing concern, \u2026] -->\\n* A simple and straight forward way to save to local storage\\n* saving on a good frequency\\n* without introducing performance issues especially on less performant devices\\n* if possible without introducing third party dependencies\\n","Decision":"<!-- * [driver 1, e.g., a force, facing concern, \u2026] -->\\n* A simple and straight forward way to save to local storage\\n* saving on a good frequency\\n* without introducing performance issues especially on less performant devices\\n* if possible without introducing third party dependencies\\nChosen option: **using a js timer**.\\nIt uses a timer that can be consolidated into one final one rather then having a lot of saves being delayed, we just have one final save once after user has stopped typing for more then 5 seconds.\\nThe timer is cleared before being called so that there is only the final one left. Leaving only one final save at the end. As a performance optimization.\\n```js\\nif (this.saveTimer!== undefined) {\\nclearTimeout(this.saveTimer);\\n}\\nthis.saveTimer = setTimeout(() => {\\nthis.localSave(this.props.mediaUrl);\\n}, 5000);\\n```\\n<!-- because [justification. e.g., only option, which meets k.o. criterion decision driver | which resolves force force | \u2026 | comes out best (see below)]. -->\\n<!--\\n### Positive Consequences\\n* [e.g., improvement of quality attribute satisfaction, follow-up decisions required, \u2026]\\n* \u2026\\n### Negative consequences\\n* [e.g., compromising quality attribute, follow-up decisions required, \u2026]\\n* \u2026\\n","tokens":157,"id":3181}
{"File Name":"cosmos-sdk\/adr-006-secret-store-replacement.md","Context":"## Context\\nCurrently, a Cosmos SDK application's CLI directory stores key material and metadata in a plain text database in the user\u2019s home directory.  Key material is encrypted by a passphrase, protected by bcrypt hashing algorithm. Metadata (e.g. addresses, public keys, key storage details) is available in plain text.\\nThis is not desirable for a number of reasons. Perhaps the biggest reason is insufficient security protection of key material and metadata. Leaking the plain text allows an attacker to surveil what keys a given computer controls via a number of techniques, like compromised dependencies without any privilege execution. This could be followed by a more targeted attack on a particular user\/computer.\\nAll modern desktop computers OS (Ubuntu, Debian, MacOS, Windows) provide a built-in secret store that is designed to allow applications to store information that is isolated from all other applications and requires passphrase entry to access the data.\\nWe are seeking solution that provides a common abstraction layer to the many different backends and reasonable fallback for minimal platforms that don\u2019t provide a native secret store.\\n","Decision":"We recommend replacing the current Keybase backend based on LevelDB with [Keyring](https:\/\/github.com\/99designs\/keyring) by 99 designs. This application is designed to provide a common abstraction and uniform interface between many secret stores and is used by AWS Vault application by 99-designs application.\\nThis appears to fulfill the requirement of protecting both key material and metadata from rogue software on a user\u2019s machine.\\n","tokens":215,"id":806}
{"File Name":"gsp\/ADR002-containers.md","Context":"## Context\\nAt the time of writing the infrastructure\/deployment landscape is:\\n* Many service teams are deploying applications to Virtual Machines (AWS EC2, VMWare, etc)\\n* Some service teams are deploying applications as containers (AWS ECS, GOV.UK PaaS, Docker)\\n* Few service teams are deploying applications as functions (AWS Lambda)\\nThere is a mix of target infrastructure\/providers in use, but there is a gradual migration towards hosting on AWS.\\n","Decision":"We will focus on providing the primitives to run stateless containerised workloads.\\n","tokens":97,"id":3892}
{"File Name":"boxwise-flask\/adr_python-dev-env.md","Context":"## Context\\nThe main programming language for the present repository is Python. The Python ecosystem features a rich set of tools and libraries that facilitate developing an industry-standard codebase. Development includes implementation of production and test code in an environment built on linting and formatting tools.\\n","Decision":"1. Test-driven development\\n1. Code maintainability\\n1. Code scalability\\n1. Code format consistency\\n1. Testing: `pytest`. Compared to unittest, test code is more concise yet readable, test fixture setup is straightforward, and test assertion output is very clear and helpful for debugging.\\n1. Code formatting: `black`. Developed by the Python Software Foundation themselves. Uncompromising, fast, deterministic.\\n1. Linting: `flake8`. Detection of common code smells. `pylint` tends to be pickier and might hinder rapid initial development.\\n1. Integration with `git`: `pre-commit`. Automatic style checks prior to committing. Orchestration of style check tools.\\n1. Isolated Python development environment: `venv`. Isolation of system and project Python packages in so called 'virtual environments'.\\n","tokens":54,"id":3055}
{"File Name":"dotcom-rendering\/024-script-loading.md","Context":"## Context\\nWhen splitting our Javascript to use dynamic imports, we also needed to review our use of module\/nomodule. You can see more context in the write-up below.\\n","Decision":"- Move from script elements to dynamically loaded scripts\\n- Use preload for high-priority scripts to ensure we get benefits of the preparser but let low priority scripts load with script injection\\n- \"Use the link tag and place it after the resource that will insert the preloaded resource\"\\n- Preload our main media image (can we do this?), then fonts, then the high priority Javascript (Our critical path)\\n- For **dynamic imports**, implement a mechanism for **high priority** components or scripts that preloads these\\n- For all other dynamic imports, rely on fetching after parsing\\n","tokens":37,"id":2668}
{"File Name":"ftd-scratch3-offline\/0020-use-json-for-scratch-to-local-server-communication.md","Context":"## Context\\nWe are using POST HTTP requests to communicate with the local server.\\nTo pass multiple parameters we would use requests with `multipart\/form-data` or `text\/plain` type.\\nCreating correctly encoded multipart requests is tricky.\\n","Decision":"We will use json for scratch to local server communication.\\nI.e. responses from server to scratch will be json objects similar to this:\\n````json\\n{\\n\"status\" : \"\", | \"SUCCESS\" or \"FAILED\"\\n\"errorMessage\" : \"\", | null when no error happened, otherwise an error message\\n\"result\" : \"\" | null when there is no output (e.g. in case of an error), otherwise the result\\n}\\n````\\n","tokens":48,"id":2639}
{"File Name":"csc-swr-architecture\/005-Monolith-First.html.md.erb","Context":"## Context\\nThe CSC Placement service is taking an [Evolutionary Architecture] approach.\\n","Decision":"Since we are in a highly uncertain phase of the Service, and we want to optimise around developer productivity and feature prototyping, the adoption of a [Monolith First] architecture is considered best.\\nAlthough Martin Fowler uses this as a precursor for a [Microservice Architecture], it is not definitely regarded as a transition to Microservices here, but rather a reasonable first step before deciding future architectural quanta.\\n","tokens":19,"id":917}
{"File Name":"ea-talk\/0001-use-markdown-for-adrs.md","Context":"## Context\\nThe decision records must be stored in a plain text format:\\n* This works well with version control systems.\\n* It allows the tool to modify the status of records and insert\\nhyperlinks when one decision supercedes another.\\n* Decisions can be read in the terminal, IDE, version control\\nbrowser, etc.\\nPeople will want to use some formatting: lists, code examples,\\nand so on.\\nPeople will want to view the decision records in a more readable\\nformat than plain text, and maybe print them out.\\n","Decision":"browser, etc.\\nPeople will want to use some formatting: lists, code examples,\\nand so on.\\nPeople will want to view the decision records in a more readable\\nformat than plain text, and maybe print them out.\\nRecord architecture decisions in [Markdown format](https:\/\/daringfireball.net\/projects\/markdown\/).\\nDecisions will be formatted nicely and hyperlinked by the\\nbrowsers of project hosting sites like GitHub and Bitbucket.\\nTools like [Pandoc](http:\/\/pandoc.org\/) can be used to convert\\nthe decision records into HTML or PDF.\\n","tokens":114,"id":1107}
{"File Name":"pace-developers\/0006-store-built-documentation-in-branch.md","Context":"## Context\\nGitHub Pages support two [publishing sources](https:\/\/help.github.com\/en\/github\/working-with-github-pages\/configuring-a-publishing-source-for-your-github-pages-site) for project documentation:\\n- the `docs` folder on the `master` branch\\n- the root folder of the `gh-pages` branch\\na third option is to store the built project documentation in\\n- documentation GitHub project.\\nIf the built documentation is stored on the `master` branch:\\n- the codebase includes built artifacts\\n- any release tag needs to be created on the commit after that from which the build was executed\\n- requires the CI server to make commits to the `master` branch\\n- it's straightforward to manage document builds from branches other than `master`\\nWhere the build artifacts on a separate branch:\\n- harder to compare source with the built documentation and manage the build artifacts from branches,\\n- cleanly separates build artefacts from source code\\nStoring built documentations in a separate GitHub repository:\\n- offers clear separation between source and built documentation\\n- carries an overhead or managing another repository\\n- documentation will served at a URL that doesn't match the source: e.g. `pace-neutrons.github.io\/horace-docs`\\n","Decision":"Built documentation will be stored on the `gh-pages` branch.\\n","tokens":259,"id":5175}
{"File Name":"adrflow\/2-Create_ADR_Object.md","Context":"## Context\\nDifferent commands in the ADR utility require manipulating the content of an ADR.\\nFor example, changing its status.\\nInstead of spreading the structure and management of ADR *content* into different commands, it would be better to centralize this related piece of knowledge into one file.\\nAlso, we may want to change how manipulate content, or export it. Centralizing it in one place will encapsulate that aspect of the tool into one place.\\n","Decision":"All ADR content manipulation is to be written and centralized in a single ADR object, under the `core` directory.\\n","tokens":96,"id":3633}
{"File Name":"jskatas.org\/001-server-side-rendering.md","Context":"## Context\\nThe web was born as HTML, lately we fallback too often to JS as our default.\\nThis makes pages slow, adds unnecessary load to the web and excludes users.\\nSSR is the least we can do to make sites easier to use, have less load on the\\nclients and deliver speed by default.\\n","Decision":"Use SSR.\\n","tokens":67,"id":4724}
{"File Name":"play-frontend-hmrc\/0006-allow-service-name-to-be-specified-using-messages-or-a-parameter.md","Context":"## Context and Problem Statement\\nNearly all services running on MDTP need to include a link to their landing page in the\\nGOV.UK header. Exceptionally, frontend microservices may either need to omit the service name\\nor be able to configure it based on the path. For example, in the case of shared frontend\\nmicroservices (contact-frontend, accessibility-statement-frontend) or microservices that host\\nmore than one public-facing service (view-external-guidance-frontend, gform-frontend).\\nWhen integrating with play-frontend-hmrc we want to\\nminimise boilerplate for service teams while providing flexibility to cope with edge\\ncases.\\nServices using the govukLayout component in play-frontend-govuk and not overriding the `headerBlock`\\nparameter, specify the service name in the\\n`service.name` message in `conf\/messages`, providing any Welsh translation in `conf\/messages.cy`.\\nBased on a Github search, most services overriding the headerBlock with, for example, `hmrcHeader`\\nor `govukHeader` are passing the service name from `messages('service.name')`. However, this\\nmechanism is awkward for services not requiring a service name \u2013 they have to override with a blank message \u2013\\nand services needing more than one are unable to use this functionality. This pattern\\nassumes a 1:1 correspondence between frontend microservices and public-facing\\ntax services.\\nShould we continue to support this pattern to reduce boilerplate for the majority of frontend\\nservices, insist on explicitly passing the service name or support some combination of the above?\\n## Decision Drivers\\n* Minimising of boilerplate and lines of code needed in frontend microservices\\n* Providing flexibility for frontend microservices that do not have a 1:1 correspondence with\\npublic-facing services.\\n* The need to keep things simple and unsurprising\\n","Decision":"* Minimising of boilerplate and lines of code needed in frontend microservices\\n* Providing flexibility for frontend microservices that do not have a 1:1 correspondence with\\npublic-facing services.\\n* The need to keep things simple and unsurprising\\nChosen option 2 because reducing boilerplate for the majority of services is important, we should support\\nmore unusual services and not dictate an architecture that assumes a 1:1 correspondence\\nbetween frontend microservices and public-facing services.\\n","tokens":390,"id":559}
{"File Name":"where-away\/0005-minimize-the-amount-of-code-that-must-be-tested-in-the-browser.md","Context":"## Context\\nThere are three main things the package must do:\\n1. give useful feedback if the input is in an invalid format\\n2. transform the input into an HTML file (including embedded CSS & JavaScript)\\n3. that HTML file must show the links and respond correctly to user input.\\nItems #1 and #2 are very easy to test, item #3 is not.\\n","Decision":"Minimize the amount of code that must be tested in the browser. Do as much as\\npossible in the build step and as little as possible in the browser.\\n","tokens":81,"id":2282}
{"File Name":"python-tuf\/0001-python-version-3-6-plus.md","Context":"## Context and Problem Statement\\nWe are planning a refactor of tuf where:\\n* We do not want to try and support end-of-life versions of the language.\\n* We want to use modern language features, such as typing.\\n* We want to ease maintainer burden, by reducing the major language versions supported.\\n## Decision Drivers\\n* Python 2.7 is end-of-life\\n* Python 3.5 is end-of-life\\n* Modern Python allows use of desirable features such as type hints\\n* Supporting end-of-life Python versions adds maintenance overhead\\n","Decision":"* Python 2.7 is end-of-life\\n* Python 3.5 is end-of-life\\n* Modern Python allows use of desirable features such as type hints\\n* Supporting end-of-life Python versions adds maintenance overhead\\nChosen option: \"Support only Python 3.6+\", because we want modern features and lower\\nmaintainer effort as we work to improve our codebase through the refactor effort.\\nNew modules should target Python 3.6+.\\nUsing modules to polyfill standard library features from Python 3.6+ feels\\nuntenable as more libraries are dropping support for EOL Python releases.\\n### Negative Consequences\\n* Leaves major adopter and contributor without an actively developed client for some of\\ntheir customers stuck on older Python versions.\\n","tokens":117,"id":4259}
{"File Name":"platform\/2020-07-02-Implement-sales-channel-context-token-requirement.md","Context":"## Context\\nSome routes for the sales-channel-api and the store-api depend on a sales-channel-context-token to identify the correct context.\\nTo ensure these routes cannot be called accidentally or intentionally without a token, a route parameter is in need to distinguish open routes and those that need a token.\\n","Decision":"Every route that depends on a sales-channel-token will only be callable with such a token provided.\\nTo decide whether a route depends on a token or not the following questions should help:\\n* Will the automatic generation of the token be a security Issue?\\n* Will the automatic generation of the token lead to an abandoned entity? (e.g. the cart)\\n* Can every possible caller create or know the needed token beforehand? (e.g. the asynchronous payment provider cannot)\\n","tokens":60,"id":4505}
{"File Name":"claim-additional-payments-for-teaching\/0010-dfe-can-download-claims-data-during-private-beta.md","Context":"## Context\\nDfE will need to verify claims to confirm eligibility before they can be\\napproved and paid. To help design and test the business process for checking and\\nverifying claims, the team will carry out a private beta where a limited set of\\nteachers will make real claims. The claims will be checked and processed\\nmanually during the private beta, enabling the team to learn the best way to\\nbuild the tooling into the service that will aid more automated checking and\\nprocessing of claims.\\n","Decision":"To give the team maximum flexibility to work with the claim data and design the\\nchecks and processes during private beta, the service will include a secure\\ndownload of all the claim data held within the system.\\nThis secure download will only be available during the private beta phase, and\\nwill be removed before the service goes live for public beta.\\n","tokens":106,"id":2097}
{"File Name":"report-a-defect\/0007-use-postgres-search.md","Context":"## Context\\nThe New Build Team need to be able to perform basic search over properties for finding properties to either report defects against or to manage existing defects.\\n","Decision":"To implement search we will use the built in Postgres Search rather than adding a new dependency on another service like ElasticSearch.\\n","tokens":32,"id":5217}
{"File Name":"teacher-training-api\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made for the manage-courses and\\nfind-courses projects. However, this project has multiple repositories, so we\\ncould keep a record here, or in it's own repo. I don't think creating another\\nrepo is good, if anything we've been considering moving repos together in the\\nfuture, so how about we create an ADR here.\\n","Decision":"We will keep the ADR in this repo. Additionally, we'll create ADRs in the other\\nrepos that are part of this project with pointers to look in this repo recorded\\nin the ADR.\\n","tokens":84,"id":2356}
{"File Name":"molgenis-r-armadillo\/0004-build-client-in-R.md","Context":"## Context\\n### Data management\\nData managers (in LifeCycle) use R to upload their data into Opal as this stage. Data managers often create scripts to generate the (harmonised) data they need to expose for a certain collection of variables.\\n### Usage Armadillo\\nWe are required to upload .RData files into the Armadillo service to be used in DataSHIELD. So .RData files need to be created before it can be pushed to the service.\\n### Possible solutions\\nPossible solutions regarding buildframeworks are R, Python or Java. Every framework has pros and cons.\\n**R**\\nTo connect to the current way of working with the data in LifeCycle, R is the most integrated platform to use. You can use the Armadillo client to integrate in the existing scripts of the data managers. You do not need a lot of training to incorparate the Armadillo client in the workflow. RData is the file format Armadillo uses and R is the platform to build .RData in which makes it easier to build the RData file in. When there is a need to do basic checks in the future, you can implement them.\\n**Python**\\nPython can be used together in one script with the molgenis-commander. Which is usefull when you use the MOLGENIS data provider. It allowes you to automate the whole process, from extracting data from MOLGENIS to converting into the right format for the Armadillo service (if Python supports generating .RData). From there you can upload it into the Armadillo. Another advantage is that Python is a language we are more experienced to program in.\\nA disadvantage is that at this point prospect customers are not used to work with python. You will need to teach them how to use Python and then how to use the client.\\n","Decision":"The Armadillo client will be written based on R to integrate easily in the current prospect customers infrastructure. We will look at the possibility of writing other clients in the near future when we need to add other customers as well.\\n","tokens":386,"id":548}
{"File Name":"oasis-core\/0002-go-modules-compatible-git-tags.md","Context":"## Context\\nProjects that depend on [Oasis Core's Go module], i.e.\\n`github.com\/oasisprotocol\/oasis-core\/go`, need a way to depend on its particular\\nversion.\\nGo Modules only allow [Semantic Versioning 2.0.0] for\\n[versioning of the modules][go-mod-ver] which makes it hard to work\\nwith [Oasis Core's CalVer (calendar versioning) scheme].\\nThe currently used scheme for Go Modules compatible Git tags is:\\n```\\ngo\/v0.YY.MINOR[.MICRO]\\n```\\nwhere:\\n- `YY` represents the short year (e.g. `19`, `20`, `21`, ...),\\n- `MINOR` represents the minor version starting with zero (e.g. `0`, `1`, `2`,\\n`3`, ...),\\n- `MICRO` represents the final number in the version (sometimes referred to as\\nthe \"patch\" segment) (e.g. `0`, `1`, `2`, `3`, ...).\\nIf the `MICRO` version is `0`, it is omitted.\\nIt turns out this only works for Oasis Core versions with the `MICRO` version\\nof `0` since the Go Modules compatible Git tag omits the `.MICRO` part and is\\nthus compatible with [Go Modules versioning requirements][go-mod-ver].\\n[Oasis Core's Go module]:\\nhttps:\/\/pkg.go.dev\/mod\/github.com\/oasisprotocol\/oasis-core\/go\\n[Semantic Versioning 2.0.0]:\\nhttps:\/\/semver.org\/spec\/v2.0.0.html\\n[go-mod-ver]:\\nhttps:\/\/golang.org\/ref\/mod#versions\\n[Oasis Core's CalVer (calendar versioning) scheme]: ..\/versioning.md\\n","Decision":"The proposed design is to tag Oasis Core releases with the following Go Modules\\ncompatible Git tags (in addition to the ordinary Git tags):\\n```\\ngo\/v0.YY0MINOR.MICRO\\n```\\nwhere:\\n- `YY` represents the short year (e.g. `19`, `20`, `21`, ...),\\n- `0MINOR` represents the zero-padded minor version starting with zero (e.g.\\n`00`, `01`, `02`, ..., `10`, `11`, ...),\\n- `MICRO` represents the final number in the version (sometimes referred to as\\nthe \"patch\" segment) (e.g. `0`, `1`, `2`, `3`, ...).\\nHere are some examples of how the ordinary and the corresponding Go Modules\\ncompatible Git tags would look like:\\n| Version       | Ordinary Git tag | Go Modules compatible Git tag  |\\n|:-------------:|:----------------:|:------------------------------:|\\n| 20.9          | `v20.9`          | `go\/v0.2009.0`                 |\\n| 20.9.1        | `v20.9.1`        | `go\/v0.2009.1`                 |\\n| 20.9.2        | `v20.9.2`        | `go\/v0.2009.2`                 |\\n| 20.10         | `v20.10`         | `go\/v0.2010.0`                 |\\n| 20.10.1       | `v20.10.1`       | `go\/v0.2010.1`                 |\\n| 20.10.2       | `v20.10.2`       | `go\/v0.2010.2`                 |\\n| ...           | ...              | ...                            |\\n| 21.0          | `v21.0`          | `go\/v0.2100.0`                 |\\n| 21.0.1        | `v21.0.1`        | `go\/v0.2100.1`                 |\\n| 21.0.2        | `v21.0.2`        | `go\/v0.2100.2`                 |\\n| 21.1          | `v21.1`          | `go\/v0.2101.0`                 |\\n| 21.1.1        | `v21.1.1`        | `go\/v0.2101.1`                 |\\n| 21.1.2        | `v21.1.2`        | `go\/v0.2101.2`                 |\\n| ...           | ...              | ...                            |\\nUsing such a scheme makes the version of the Oasis Core Go module fully\\ncompatible with the [Go Modules versioning requirements][go-mod-ver] and thus\\nenables users to use the familiar Go tools to check for new module versions,\\ni.e. `go list -m -u all`, or to obtain and require a module, i.e.\\n`go get github.com\/oasisprotocol\/oasis-core\/go@latest`.\\n","tokens":390,"id":4345}
{"File Name":"linshare-mobile-android-app\/0002-image-loading-with-glide.md","Context":"## Context\\nIn the android linshare application, we implement the list file from the space of user. Proposed user interface design includes the thumbnail of file preview when user wants to show\\nIt is necessary to have an library to process first part is image thumbnail and preview.\\nOver the best practice of image processing there are 2 libraries is very commons by android developer community: Glide and Picasso\\nTo compare between them, the commonly usage is the same, but Glide have much more strengthen rather than Picasso.\\nIt process the image source and have method to generate the thumbnail natively, it consume less the memory than Picasso and the library is have smaller packer with much more APIs to help process image, witch could be useful later when we implement more functionality in the application\\n","Decision":"We decided to use Glide instead of Picasso.\\n","tokens":155,"id":1642}
{"File Name":"dapr\/API-003-messaging-api-names.md","Context":"## Context\\nOur existing messaging interface names lack of clarity. This review was to make sure messaging interfaces were named appropriately to avoid possible confusions.\\n","Decision":"### Dapr\\n* All messaging APIs are grouped under a **messaging** namespace\/package.\\n* We define three distinct messaging interfaces:\\n- **direct**\\nOne-to-one messaging between two parties: a sender sending message to a recipient.\\n- **broadcast**\\nOne-to-many messaging: a sender sending message to a list of recipients.\\n- **pub-sub**\\nMessaging through pub-sub: a publisher publishing to a topic, to which subscribers subscribe.\\n* We distinguish message and direct invocation. For messaging, we guarantee at-least-once delivery. For direct invocation, we provide best-attempt delivery.\\n","tokens":30,"id":89}
{"File Name":"celestia-core\/adr-068-reverse-sync.md","Context":"## Context\\nThe advent of state sync and block pruning gave rise to the opportunity for full nodes to participate in consensus without needing complete block history. This also introduced a problem with respect to evidence handling. Nodes that didn't have all the blocks within the evidence age were incapable of validating evidence, thus halting if that evidence was committed on chain.\\n[RFC005](https:\/\/github.com\/tendermint\/spec\/blob\/master\/rfc\/005-reverse-sync.md) was published in response to this problem and modified the spec to add a minimum block history invariant. This predominantly sought to extend state sync so that it was capable of fetching and storing the `Header`, `Commit` and `ValidatorSet` (essentially a `LightBlock`) of the last `n` heights, where `n` was calculated based from the evidence age.\\nThis ADR sets out to describe the design of this state sync extension as well as modifications to the light client provider and the merging of tm store.\\n","Decision":"The state sync reactor will be extended by introducing 2 new P2P messages (and a new channel).\\n```protobuf\\nmessage LightBlockRequest {\\nuint64 height = 1;\\n}\\nmessage LightBlockResponse {\\ntendermint.types.LightBlock light_block = 1;\\n}\\n```\\nThis will be used by the \"reverse sync\" protocol that will fetch, verify and store prior light blocks such that the node can safely participate in consensus.\\nFurthermore this allows for a new light client provider which offers the ability for the `StateProvider` to use the underlying P2P stack instead of RPC.\\n","tokens":199,"id":4132}
{"File Name":"fundraising-application\/009_Translation_Messages_Deployment.md","Context":"## Context\\nThe \"Fundraising Frontend Content\" repository is a git repository where the Fundraising Department can make edits to the translated messages and texts of the Fundraising Application. Those changes get deployed automatically, independently from the code deployments.\\nThe [`wmde19` skin](008_Client_Side_Rewrite.md) uses client-side rendering and the [Vue i18n](https:\/\/kazupon.github.io\/vue-i18n\/) plugin for translating messages. There are several possibilities to get the translated strings into the client-side code:\\n1. Importing it directly in JavaScript, with an `import` statement. This requires a continuous delivery pipeline that creates a new client-side code bundle on every content change.\\n2. Loading it asynchronously when the client-side code loads. This has the benefit of working out of the box, but the drawback of an additional HTTP request.\\n3. Reading the file on the server side and putting its contents in a HTML [data attribute](https:\/\/developer.mozilla.org\/en-US\/docs\/Learn\/HTML\/Howto\/Use_data_attributes) where the bootstrap code will read it an inject it into the i18n plugin.\\n","Decision":"Since we don't have the engineering resources to create a continuous delivery pipeline, only options 2 and 3 remain. We choose the data attribute method for performance reasons: We want one less HTTP request and the size of the messages is acceptable: At the time of writing this ADR, it's 30K uncompressed, 7K compressed.\\nWe want to keep the message size down and implement the server-side code in a way that allows for splitting the messages into \"common\" and page-specific bundles.\\n","tokens":237,"id":1520}
{"File Name":"mnt-teet\/ADR-2-Frontend-app-structure.md","Context":"## Context\\nTEET is a large web application and will contain many namespaces and different types of namespaces:\\nviews, controllers, common UI components, etc.\\nThe way to structure namespaces affects how easy it is to find and refer to a given piece of code.\\nCommon structure makes it more predictable where developers can expect to find things.\\n","Decision":"We use feature based grouping with layer suffix.\\n`teet.<feature>.<feature>-<layer>`\\nExample: `teet.search.search-view` and `teet.search.search-controller`\\nWhen referring to namespaces, use the last part of the name fully, e.g. `search-view`\\n```clojure\\n(ns teet.search.search-view\\n(:require [teet.search.search-controller :as search-controller]))\\n```\\nFeatures may use implementation specific sub-namespaces as seen fit.\\nNon-feature functionality, like common UI utilities, are placed under the layer, e.g. `teet.ui.panels`.\\n","tokens":69,"id":4794}
{"File Name":"front-end-monorepo\/adr-18.md","Context":"## Context\\nOn PFE, there are a couple of different ways workflows are routed:\\n- Navigating to `\/classify`, the user sees either the default workflow for that project, a random active workflow, or the workflow stored in project preferences for logged-in volunteers.\\n- Navigating to `\/classify?workflow=[workflow_id]` shows the workflow determined by the query parameter when the \"Use workflow query\" feature is enabled for the project, or if the user has the correct role (owner, collaborator, tester, or admin).\\nThis is not optimal for several reasons. Users cannot bookmark specific workflows unless they use the query parameter; there can be uncertainty over which actual workflow is being shown (particularly as the workflow name isn't shown in the UI); different workflows can be shown at the same URL at different times; and ultimately, workflows are static resources that should be routed to - that's what URLs are for.\\nAdditionally, [we currently have a requirement for CSSI CitSci to be able to route to specific subjects](https:\/\/github.com\/zooniverse\/front-end-monorepo\/issues\/806#issuecomment-495685027). It's conceivable that a project will have a requirement for routing to a specific subject set linked to a workflow as well.\\n","Decision":"We adopt the following URL structure:\\n```\\n\/projects\/[owner]\/[project]\/classify\/workflow\/:workflow-id\/subject-set\/:subject-set-id\/subject\/:subject-id\\n```\\nThis would be facilitated by the [dynamic routing feature in Next.js 9](https:\/\/github.com\/zeit\/next.js\/#dynamic-routing) (see [#1071](https:\/\/github.com\/zooniverse\/front-end-monorepo\/pull\/1071)).\\nWorkflow links from the home page would route directly to their workflow-specific URLs.\\n### `\/classify` behaviour\\nWhen navigating to `\/classify`, the page should redirect the user to the correct workflow URL in the following priority:\\n1. Workflow ID set in user project preferences by the user\\n1. Workflow ID set in user project preferences by the project\\n1. Project default workflow\\n1. Random active selection\\nThe new URL that we redirect the user to should replace the entry for `\/classify` in the browser history.\\n### Error handling\\nWhen a user is unable to view a workflow at a given URL, we should provide an error page at that URL. Next.js allows you to provide a response code in `getInitialProps`, which we can use to tailor the message we show on a custom error component, so for non-existent workflows, `404`; incorrect permissions get a `401`\/`403` (depending on login status) etc.\\n","tokens":260,"id":507}
{"File Name":"ftd-scratch3-offline\/0007-use-arduino-cli-in-offline-mode.md","Context":"## Context\\nArduino-cli normally downloads all required libraries on-the-fly.\\nThis might not be desirable at some institutions because they restrict Internet access or because many people using arduino-cli at the same time might saturate the available bandwith.\\nUsing arduino-cli in offline mode also means that we know which version of libraries are used, which simplifies troubleshooting.\\nArduino-cli supports multiple platforms.\\nEach of these platforms has platform-specific tools that need to be downloaded.\\nIt is not possible to download the tools needed for a Windows host on a Linux host.\\nWe have to manually download the tools for the different platforms.\\n","Decision":"We will use arduino-cli in offline mode and manually download the tools for the different platforms.\\n","tokens":128,"id":2627}
{"File Name":"aws_infrastructure\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":1468}
{"File Name":"content-data-api\/adr-000-document-architectural-decisions.md","Context":"## Context\\nWe aim to:\\n- Make it easier to understand the codebase and its status\\n- Reduce the number of meetings to handover information across teams\\n- Facilitate team rotations across GOV.UK\\n","Decision":"Track architectural decision that impact the status of the CPM, [following a lightweight format: ADR][1]\\n","tokens":46,"id":1864}
{"File Name":"operational-data-hub\/0043-api-first.md","Context":"## Context\\nAn API-first approach means that for any given development project, your APIs are treated as \u201cfirst-class citizens.\u201d That everything about a project revolves around the idea that the end product will be consumed by mobile devices, and that APIs will be consumed by client applications. An API-first approach involves developing APIs that are consistent and reusable, which can be accomplished by using an API description language to establish a contract for how the API is supposed to behave.  Establishing a contract involves spending more time thinking about the design of an API. It also often involves additional planning and collaboration with the stakeholders providing feedback on the design of an API before any code is written.\\n","Decision":"Starting a new project starts with building and documenting the API.\\n","tokens":134,"id":2697}
{"File Name":"linshare-mobile-android-app\/0006-login-with-long-lived-token.md","Context":"## Context\\n- With mobile application, interaction with server base using REST API, in order for REST request to be accepted by LinShare server, they need to be authenticated.\\n- One of the most common questions we get here - when talking about token authentication for mobile devices, is about token expiration. But how long should I allow my access tokens to exist before expiring them? I don\u2019t want to force my users to re-authenticate every hour.\\n- In order to supply a convenient user experience, we need to store a way to authenticate requests to the LinShare server.\\n- Using directly authentication (and thus storing directly credentials) is discouraged, because of possible disclosure, and vulnerability to isolate a given device.\\n- LinShare allow the use of `Long lived Token`. They can be revoked without changing credentials. They allows finger per device permission setting, and per-device activity review. Credential don't need to be stored (only the token does) which means user credentials cannot be leaked.\\n","Decision":"- For all that reason we using approach to keep the token and renew when need, that is the way to use long lived token authentication\\n","tokens":203,"id":1641}
{"File Name":"operational-data-hub\/0005-oauth-scope-naming-conventions.md","Context":"## Context\\nWe feel the need to use a naming convention for OAuth scopes.\\n","Decision":"For this coding standard we will follow a [Restful API Guideline](https:\/\/opensource.zalando.com\/restful-api-guidelines\/)\\nfrom Zalando.\\nGuideline [#225](https:\/\/opensource.zalando.com\/restful-api-guidelines\/index.html#225) says that permission names in\\nAPIs must conform to the following naming pattern:\\n~~~text\\n<permission> ::= <standard-permission> |  -- should be sufficient for majority of use cases\\n<resource-permission> |  -- for special security access differentiation use cases\\n<pseudo-permission>      -- used to explicitly indicate that access is not restricted\\n<standard-permission> ::= <application-id>.<access-mode>\\n<resource-permission> ::= <application-id>.<resource-name>.<access-mode>\\n<pseudo-permission>   ::= uid\\n<application-id>      ::= [a-z][a-z0-9-]*  -- application identifier\\n<resource-name>       ::= [a-z][a-z0-9-]*  -- free resource identifier\\n<access-mode>         ::= read | write    -- might be extended in future\\n~~~\\nTo meet our own naming guidelines, we will use our Solution IDs as the `<application-id>`. The naming convention of\\nthese Solution IDs are described within our Cloud naming convention page on Confluence.\\n","tokens":17,"id":2726}
{"File Name":"polaris\/vision_node_python_refactor.md","Context":"## Context\\nThe focus of this ADR is the ROS vision node. Specifically, the programming language the node is based in. AUVic's software team is putting effort towards developing vision solutions for the sub, notably object detection and associated actions. There is a need to ensure a smooth transition and ensure that solutions are developed as soon as possible for testing, as it is a core functionality of the sub. Furthermore, there has been interest to assign new team members to efforts related to `OpenCV`.\\n","Decision":"A decision is proposed to fully refactor the vision node, including all its implementation in C++, to Python 3. The main motivation is the need to have a fully functioning vision node, with code that is easily unit testable and who\u2019s syntax is easier to understand than C++.\\nSpecific motivations are listed:\\n-\tAs it is shown that Python and C++ functionality is interchangeable [1], and Python syntax is easier to understand, a preference exists to focus on higher level applications.\\n- As opposed to employing the use of `CMakeLists.txt`, using Python only requires the Python interpreter itself [2]. As shown, nodes in Python with similar\\nfunctionality may be created with minimal dependencie on those files [3]\\n- A transition to Python as opposed to another scripting\/programming language such as **Go, Rust, Java, or C** [4] is preferred as Python is a syntactically more preferable language for team members, and does not entail compile-time considerations.\\n- A transition to Python eliminates the risk of dealing with memory management and desctructors. The need for those features have not\\nbeen determined at the time of this writing.\\n- A transition to Python will simplify the approach to writing automated unit tests, as syntax and lack of compile-time overhead\\nis minimized.\\n- The transition will provide us access to `numpy`, a computationally more efficient way of computing results. Furthermore, packages\\nlike `keras`, `theano` and `tensorflow` will lessen the barrier to entry for team members to develop deep learning solutions.\\n","tokens":103,"id":4719}
{"File Name":"verify-proxy-node\/20190320-additional-tracing.md","Context":"## Context\\neIDAS project has been challenged with some security concerns provided by\\nNational Cyber Security Centre (NCSC). One of these concerns was a possibility\\nfor a connector application or the connection between given connector and the\\nuser being compromised and allow for stolen request IDs impersonate or create\\nnew identities.\\nSadly, it's out of our control. We do however, have an obligation to monitor\\nactions that are going through the system, in order to prove our innocence.\\nWe have made a presumption, that Hardware Security Module (HSM) logs would be\\nable to tell us when it has been used to sign a thing and attach it to some of\\nthe logs that we're currently holding.\\nThis isn't however true for the version of HSM that is provided to us by AWS.\\nWe have managed to raise a feature request, which will not be coming for a\\nwhile.\\n","Decision":"Reliability Engineering has proposed to implement tracing through the system\\nwith the use of Service Mesh. This requires our services to pass around a\\nheader in the requests. Cyber Security team, will then create some monitoring\\naround these logs and alert when a path of the request is not performed in a\\ncorrect order indicating the request is being injected. These logs would then\\nbe compared with a size of a body sent through to the HSM and back, to\\nestablish when that exact request has been signed.\\nThis doesn't solve the problem entirely, but is good enough for now before we\\nreceive an appropriate solution from AWS.\\n","tokens":186,"id":4070}
{"File Name":"arch\/0021-split-redis-with-business-and-move-redis-to-aliyun-kvstore.md","Context":"## Context\\n1. redis server \u90e8\u7f72\u5728\u4e1a\u52a1\u673a\u4e0a\uff0c\u4e1a\u52a1\u4e0e\u6570\u636e\u5b58\u50a8\u8026\u5408\uff1b\\n2. \u6240\u6709\u4e1a\u52a1\u7684\u6570\u636e\u5b58\u5728\u4e00\u4e2a db 0 \u4e2d\uff0c\u4e1a\u52a1\u4e4b\u95f4\u5f88\u5bb9\u6613\u4ea7\u751f key \u51b2\u7a81\uff0c\u4e1a\u52a1\u6269\u5c55\u65f6\u9700\u987e\u8651\u5176\u4ed6\u4e1a\u52a1\u6570\u636e\uff1b\\n3. \u5355\u70b9\uff0c\u4e1a\u52a1\u4e2d\u5b58\u6709\u6d41\u7a0b\u6570\u636e\uff0c\u98ce\u9669\u6bd4\u8f83\u5927\uff1b\\n4. \u5982\u679c\u5207\u5230\u72ec\u7acb\u7684\u673a\u5668\uff0c\u8d44\u6e90\u5229\u7528\u7387\u4e0d\u9ad8\u3002\\n","Decision":"1. \u5c06\u81ea\u5efa redis server \u8fc1\u79fb\u81f3 aliyun redis \u4e2d\uff1b\\n2. \u7406\u6e05\u4e1a\u52a1\u5bf9 redis \u7684\u4f7f\u7528\u60c5\u51b5\uff0c\u6309\u4e1a\u52a1\u5212\u5206\u6307\u5b9a redis db index \u6216 \u72ec\u7acb redis \u5b9e\u4f8b\uff1b\\n3. aliyun redis \u505a\u4e86\u9ad8\u53ef\u7528\uff1b\\n4. aliyun \u76ee\u524d\u53ef\u9009\u7684\u8303\u56f4\u5f88\u591a\uff0c\u6700\u5c0f\u5b9e\u4f8b\u662f 256M\uff0c\u4ef7\u683c\u4e5f\u5408\u7406\u3002\\n\u4f7f\u7528 aliyun redis \u540e\uff0c\u989d\u5916\u5f97\u5230\u4e00\u4e2a\u6570\u636e\u7ba1\u7406\u4e0e\u76d1\u63a7\u529f\u80fd\u529f\u80fd\uff0c\u5982\u56fe\\n![][image-1]\\n","tokens":129,"id":2455}
{"File Name":"monolith\/0003-network-request-timeout.md","Context":"## Context\\nA slow network connection and overloaded server may negatively impact network response time.\\n","Decision":"Make the program simulate behavior of popular web browsers and CLI tools, where the default network response timeout is most often set to 120 seconds.\\nInstead of featuring retries for timed out network requests, the program should have an option to adjust the timeout length, along with making it indefinite when given \"0\" as its value.\\n","tokens":18,"id":1081}
{"File Name":"nearby-services-api\/0004-use-elastic-search.md","Context":"## Context\\nElasticsearch is configured as a cluster for reliability and failover, and\\nprovides a single point for data updates. MongoDB runs as a single instance and\\nis not clustered.\\n","Decision":"nearby-services-api will consume data from Elasticsearch rather than MongoDB.\\n","tokens":41,"id":483}
{"File Name":"push-sdk-android\/0003-fcm-over-gcm.md","Context":"## Context\\nWe had to decide on whether to implement Firebase Cloud Messaging (FCM) or\\nGoogle Cloud Messaging (GCM), as they both provide push notification support\\nfor Android.\\n","Decision":"We decided to implement FCM (despite there being a fair bit of GCM-related\\ninformation available) since, in Google's own words:\\n> Firebase Cloud Messaging (FCM) is the new version of GCM. It inherits the\\n> reliable and scalable GCM infrastructure, plus new features! See the FAQ to\\n> learn more. If you are integrating messaging in a new app, start with FCM.\\n> GCM users are strongly recommended to upgrade to FCM, in order to benefit\\n> from new FCM features today and in the future.\\nWe believe that GCM will have a long-tail, but that FCM is the recommended way\\nto move forward.\\n","tokens":39,"id":5041}
{"File Name":"snippets-service\/0002-export-asrsnippet-metadata-for-snippet-metrics-processing.md","Context":"## Context\\nData Engineers are building performance dashboards for Snippets using the\\nmetrics we collect from Firefox Telemetry. Telemetry pings include only basic\\ninformation about the Snippet, like Snippet ID.\\nFor better to understand and more complete dashboards, we want to enhance the\\nTelemetry received information with more Snippet metadata, like campaign\\ninformation, included URL, main message used and others.\\nTo achieve this we will export the metadata from the Snippets Service in a\\nmachine readable format and make the file available in a Cloud Storage Provider.\\nThen Data Engineers will import the metadata and combine them Telemetry data in\\nunified dashboards.\\nGitHub Issue: [#887](https:\/\/github.com\/mozmeao\/snippets-service\/issues\/887)\\n","Decision":"- Export in CSV format.\\n- Create a cron job to export and upload resulting file to S3.\\n- The job will run daily, on early morning UTC hours.\\n- The job will be monitored using Dead Man's Snitch and report to the usual\\nnotification channels that project developers follow.\\n","tokens":159,"id":3230}
{"File Name":"asb-client-spa\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as\\n[described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":391}
{"File Name":"trade-access-program\/0002-deployments.md","Context":"## Context\\nWe need a way of deploying our services to our various hosted environments (dev, staging, prod, etc.). We would\\nprefer these deployments to be automated with minimal to zero human interaction.\\n","Decision":"The DIT infrastructure already has a lot of tooling and infrastructure around deployments so we will utilise this. We\\nwill use Jenkins to automatically deploy from dedicated git branches - these will be:\\n- development\\n- staging\\n- uat\\nWe will have 5 Jenkins jobs in total - these will be:\\n- trade-access-program-backoffice\\n- trade-access-program-frontend\\n- trade-access-program-polling-dev\\n- trade-access-program-polling-staging\\n- trade-access-program-polling-uat\\nThe role of the \"polling\" jobs are to watch a related git branch for any commit changes. Once a change is detected then\\nit will trigger the `trade-access-program-backoffice` and `trade-access-program-frontend` jobs with a set of\\nenvironment parameters triggering the deployment to one of our three environments.\\nThis allows us to simply merge or push to one of the three dedicated git branches above and a full automated deployment\\nwill occur for that environment.\\n","tokens":42,"id":5013}
{"File Name":"tamr-client\/0009-separate-types-and-functions.md","Context":"## Context\\nCode must be organized to be compatible with:\\n- Static type-checking via [mypy](https:\/\/github.com\/python\/mypy)\\n- Runtime execution during normal usage and running tests via [pytest](https:\/\/docs.pytest.org\/en\/stable\/)\\n- Static doc generation via [sphinx-autodoc-typehints](https:\/\/github.com\/agronholm\/sphinx-autodoc-typehints)\\nAdditionally:\\n- Functions should be able to refer to any type\\n- Most types depend on other types non-recursively, but some types (e.g. `SubAttribute` and `AttributeType`) do depend on each other recursively \/ cyclically.\\n","Decision":"Put types (`@dataclass(frozen=True)`) into the `_types` module\\nand have all function modules depend on the `_types` module to define their inputs and outputs.\\n","tokens":139,"id":533}
{"File Name":"arch\/0025-apply-restful-design-in-our-service-interface.md","Context":"## Context\\n1. \u5f53\u524d URL \u5b9a\u4e49\u7c7b\u4f3c\u8fd9\u6837\uff0c`add_message`, `report_exception`, `check_pwd` \u7b49\uff0c\u51fd\u6570\u5f0f\u5b9a\u4e49\uff0c\u5bfc\u81f4\u63a5\u53e3\u6570\u91cf\u589e\u957f\u8fc7\u5feb\uff0c\u7ba1\u7406\u9ebb\u70e6\uff1b\\n2. \u6240\u6709\u7684\u8bf7\u6c42\u90fd\u8d70 POST \u65b9\u6cd5\uff1b\\n3. \u6240\u6709\u8bf7\u6c42\u8fd4\u56de\u72b6\u6001\u90fd\u662f 200\uff0c\u4f7f\u7528\u6666\u6da9\u96be\u61c2\u7684\u81ea\u5b9a\u4e49\u72b6\u6001\u7801\uff1b\\n4. \u51fd\u6570\u5f0f\u7f16\u7a0b\uff0c\u4ee3\u7801\u91cd\u7528\u5ea6\u4e0d\u9ad8\uff1b\\n5. \u81ea\u5b9a\u4e49\u63a5\u53e3\u6587\u6863\uff0c\u65e0\u6cd5\u53ca\u65f6\u66f4\u65b0\uff1b\\n6. \u8fd4\u56de\u7ed3\u6784\u5dee\u5f02\u5316\u5927\uff0c\u8fc7\u6ee4\uff0c\u6392\u5e8f\u548c\u5206\u9875\u5404\u5f0f\u5404\u6837\uff0c\u65e0\u7edf\u4e00\u7684\u89c4\u8303\uff1b\\n7. \u65e0 API \u63a7\u5236\u53f0\u53ef\u4f9b\u6d4b\u8bd5\uff1b\\n8. \u66f4\u591a\u3002\\n","Decision":"1. URL\u7684\u8bbe\u8ba1\u5e94\u4f7f\u7528\u8d44\u6e90\u96c6\u5408\u7684\u6982\u5ff5\uff1b\\n* \u6bcf\u79cd\u8d44\u6e90\u6709\u4e24\u7c7b\u7f51\u5740\uff08\u63a5\u53e3\uff09\uff1a\\n* \u8d44\u6e90\u96c6\u5408\uff08\u4f8b\u5982\uff0c\/orders\uff09\uff1b\\n* \u96c6\u5408\u4e2d\u7684\u5355\u4e2a\u8d44\u6e90\uff08\u4f8b\u5982\uff0c\/orders\/{orderId}\uff09\u3002\\n* \u4f7f\u7528\u590d\u6570\u5f62\u5f0f (\u4f7f\u7528 \u2018orders\u2019 \u800c\u4e0d\u662f \u2018order\u2019)\uff1b\\n* \u8d44\u6e90\u540d\u79f0\u548c ID \u7ec4\u5408\u53ef\u4ee5\u4f5c\u4e3a\u4e00\u4e2a\u7f51\u5740\u7684\u8282\u70b9\uff08\u4f8b\u5982\uff0c\/orders\/{orderId}\/items\/{itemId}\uff09\uff1b\\n* \u5c3d\u53ef\u80fd\u7684\u8ba9\u7f51\u5740\u8d8a\u77ed\u8d8a\u597d\uff0c\u5355\u4e2a\u7f51\u5740\u6700\u597d\u4e0d\u8d85\u8fc7\u4e09\u4e2a\u8282\u70b9\u3002\\n2. \u4f7f\u7528\u540d\u8bcd\u4f5c\u4e3a\u8d44\u6e90\u540d\u79f0 (\u4f8b\u5982\uff0c\u4e0d\u8981\u5728\u7f51\u5740\u4e2d\u4f7f\u7528\u52a8\u8bcd)\uff1b\\n3. \u4f7f\u7528\u6709\u610f\u4e49\u7684\u8d44\u6e90\u63cf\u8ff0\uff1b\\n* \u201c\u7981\u6b62\u5355\u7eaf\u7684\u4f7f\u7528 ID!\u201d \u54cd\u5e94\u4fe1\u606f\u4e2d\u4e0d\u5e94\u8be5\u5b58\u5728\u5355\u7eaf\u7684 ID\uff0c\u5e94\u4f7f\u7528\u94fe\u63a5\u6216\u662f\u5f15\u7528\u7684\u5bf9\u8c61\uff1b\\n* \u8bbe\u8ba1\u8d44\u6e90\u7684\u63cf\u8ff0\u4fe1\u606f\uff0c\u800c\u4e0d\u662f\u7b80\u7b80\u5355\u5355\u7684\u505a\u6570\u636e\u5e93\u8868\u7684\u6620\u5c04\uff1b\\n* \u5408\u5e76\u63cf\u8ff0\u4fe1\u606f\uff0c\u4e0d\u8981\u901a\u8fc7\u4e24\u4e2a ID \u76f4\u63a5\u8868\u793a\u4e24\u4e2a\u8868\u7684\u5173\u7cfb\uff1b\\n4. \u8d44\u6e90\u7684\u96c6\u5408\u5e94\u652f\u6301\u8fc7\u6ee4\uff0c\u6392\u5e8f\u548c\u5206\u9875\uff1b\\n5. \u652f\u6301\u901a\u8fc7\u94fe\u63a5\u6269\u5c55\u5173\u7cfb\uff0c\u5141\u8bb8\u5ba2\u6237\u7aef\u901a\u8fc7\u6dfb\u52a0\u94fe\u63a5\u6269\u5c55\u54cd\u5e94\u4e2d\u7684\u6570\u636e\uff1b\\n6. \u652f\u6301\u8d44\u6e90\u7684\u5b57\u6bb5\u88c1\u526a\uff0c\u8fd0\u884c\u5ba2\u6237\u7aef\u51cf\u5c11\u54cd\u5e94\u4e2d\u8fd4\u56de\u7684\u5b57\u6bb5\u6570\u91cf\uff1b\\n7. \u4f7f\u7528 HTTP \u65b9\u6cd5\u540d\u6765\u8868\u793a\u5bf9\u5e94\u7684\u884c\u4e3a\uff1a\\n* POST - \u521b\u5efa\u8d44\u6e90\uff0c\u975e\u5e42\u7b49\u6027\u64cd\u4f5c\uff1b\\n* PUT - \u66f4\u65b0\u8d44\u6e90\uff08\u66ff\u6362\uff09\uff1b\\n* PATCH - \u66f4\u65b0\u8d44\u6e90\uff08\u90e8\u5206\u66f4\u65b0\uff09\uff1b\\n* GET - \u83b7\u53d6\u5355\u4e2a\u8d44\u6e90\u6216\u8d44\u6e90\u96c6\u5408\uff1b\\n* DELETE - \u5220\u9664\u5355\u4e2a\u8d44\u6e90\u6216\u8d44\u6e90\u96c6\u5408\uff1b\\n8. \u5408\u7406\u4f7f\u7528 HTTP \u72b6\u6001\u7801\uff1b\\n* 200 - \u6210\u529f\uff1b\\n* 201 - \u521b\u5efa\u6210\u529f\uff0c\u6210\u529f\u521b\u5efa\u4e00\u4e2a\u65b0\u8d44\u6e90\u65f6\u8fd4\u56de\u3002 \u8fd4\u56de\u4fe1\u606f\u4e2d\u5c06\u5305\u542b\u4e00\u4e2a 'Location' \u62a5\u5934\uff0c\u4ed6\u901a\u8fc7\u4e00\u4e2a\u94fe\u63a5\u6307\u5411\u65b0\u521b\u5efa\u7684\u8d44\u6e90\u5730\u5740\uff1b\\n* 400 - \u9519\u8bef\u7684\u8bf7\u6c42\uff0c\u6570\u636e\u95ee\u9898\uff0c\u5982\u4e0d\u6b63\u786e\u7684 JSON \u7b49\uff1b\\n* 404 - \u672a\u627e\u5230\uff0c\u901a\u8fc7 GET \u8bf7\u6c42\u672a\u627e\u5230\u5bf9\u5e94\u7684\u8d44\u6e90\uff1b\\n* 409 - \u51b2\u7a81\uff0c\u5c06\u51fa\u73b0\u91cd\u590d\u7684\u6570\u636e\u6216\u662f\u65e0\u6548\u7684\u6570\u636e\u72b6\u6001\u3002\\n9. \u4f7f\u7528 ISO 8601 \u65f6\u95f4\u6233\u683c\u5f0f\u6765\u8868\u793a\u65e5\u671f\uff1b\\n10. \u786e\u4fdd\u4f60\u7684 GET\uff0cPUT\uff0cDELETE \u8bf7\u6c42\u662f[\u5e42\u7b49\u7684][1]\uff0c\u8fd9\u4e9b\u8bf7\u6c42\u591a\u6b21\u64cd\u4f5c\u4e0d\u5e94\u8be5\u6709\u526f\u4f5c\u7528\u3002\\n11. PUT\u3001POST\u3001PATCH \u8bf7\u6c42\u53c2\u6570\u901a\u8fc7 `application\/json` \u4f20\u9012\uff1b\\n12. \u6b63\u786e\u8fd4\u56de\u683c\u5f0f\uff1a\\n* \u5355\u4e2a\u8d44\u6e90\uff1a{field1: value1, \u2026}\\n* \u8d44\u6e90\u96c6\u5408\uff1a[{field1: value1, \u2026}]\\n* \u8d44\u6e90\u96c6\u5408\uff08\u5e26\u5206\u9875\uff09\uff1a\\n```json\\n{\\n\"count\": 0,\\n\"next\": null,\\n\"previous\": null,\\n\"results\": [{\"field1\": \"value1\", \u2026}]\\n}\\n```\\n13. \u9519\u8bef\u8fd4\u56de\u683c\u5f0f\uff1a\\n* \u975e\u7279\u5b9a\u5b57\u6bb5\u9519\u8bef\\n```json\\n{\\n\"non_field_errors\": [\\n\"\u8be5\u624b\u673a\u53f7\u7801\u672a\u6ce8\u518c\uff01\"\\n]\\n}\\n```\\n* \u7279\u5b9a\u5b57\u6bb5\u9519\u8bef\\n```json\\n{\\n\"phone_number\": [\\n\"\u8be5\u5b57\u6bb5\u4e0d\u80fd\u4e3a\u7a7a\u3002\"\\n],\\n\"address\": [\\n\"\u8be5\u5b57\u6bb5\u4e0d\u80fd\u4e3a\u7a7a\u3002\"\\n]\\n}\\n```\\n14.  \u4f7f\u7528  swagger  \u505a API \u5c55\u793a \u4e0e\u8c03\u8bd5\u3002\\n![][image-1]\\n","tokens":196,"id":2435}
{"File Name":"road-registry\/001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n### Changes to Nygard's approach\\nWe will keep ADRs in the project repository under `docs\/adr\/NNN-explanation-of-adr.md`.\\n","tokens":16,"id":4932}
{"File Name":"upd-articles-api\/ADR-003-16012019-configure-checkstyle-findbugs-support.md","Context":"**Context**\\nCode quality is very important and we need some way to ensure the code we are sending to production is clean code based on good practices and without bugs that can cause some unexpected behaviors. Sonar is a good option in our Integration server but how we can check our code in the local environment?\\n**Decision**\\nThere are two very famous plugins to help us. [Findbugs](http:\/\/findbugs.sourceforge.net\/) will scan the code and detect any possible bug that can cause an unexpected exception or some vulnerability in our code. Same way [Checkstyle](http:\/\/checkstyle.sourceforge.net\/) will help us to be aligned with the code standard depending on the platform, in this case Java.\\n**Status**\\nAccepted\\n**Consequences**\\nBoth plugins will be executed when the build command is executed. Currently Findbugs is activated to prevent build to be done if detects some code vulnerability or bug. It will generate a report in HTML code to analyze the point of failure. Checkstyle is configured to alert us about the code bad syntax, but will not stop the build.\\nThese two configurations can be modified on the `build.gradle` file.\\n","Decision":"There are two very famous plugins to help us. [Findbugs](http:\/\/findbugs.sourceforge.net\/) will scan the code and detect any possible bug that can cause an unexpected exception or some vulnerability in our code. Same way [Checkstyle](http:\/\/checkstyle.sourceforge.net\/) will help us to be aligned with the code standard depending on the platform, in this case Java.\\n**Status**\\nAccepted\\n**Consequences**\\nBoth plugins will be executed when the build command is executed. Currently Findbugs is activated to prevent build to be done if detects some code vulnerability or bug. It will generate a report in HTML code to analyze the point of failure. Checkstyle is configured to alert us about the code bad syntax, but will not stop the build.\\nThese two configurations can be modified on the `build.gradle` file.\\n","tokens":241,"id":2985}
{"File Name":"dp\/0004.md","Context":"## Context\\nWhen designing APIs to handle metadata about datasets, editions and versions\\nno particular limitations were placed on which order these documents should be\\nedited in.\\nIn the existing publishing workflow, a single item can be added to only one\\ncollection at a time. As the APIs have separated formerly joined concepts, it\\nis necessary to understand whether datasets can be edited or published in\\nisolation from versions.\\n","Decision":"Adding a version to a collection does not prevent the dataset being edited in a different collection\\n","tokens":85,"id":5114}
{"File Name":"Brighter\/0002-use-a-single-threaded-message-pump.md","Context":"## Context\\nAny service activator pattern will have a message pump, which reads from a queue.\\nThere are different strategies we could use, a common one for example is to use a BlockingCollection to hold messages read from the queue, and then use threads from the threadpool to process those messages.\\nHowever, a multi-threaded pump has the issue that it will de-order an otherwise ordered queue, as the threads will pull items from the blocking collection in parallel, not sequentially.\\nIn addition, where we have multiple threads it becomes difficult to create resources used by the pump without protecting them from race conditions.\\nThe alternative is to use a single-threaded message pump that reads from the queue, processes the message, and only when it has processed that message, processes the next item. This prevents de-ordering of the queue, because items are read in sequence.\\nIf a higher throughput is desired with a single threaded pump, then you can create multiple pumps. In essence, this is the competing consumers pattern, each performer is its own message pump.\\nThe message pump performs the usual sequence of actions:\\n- GetMessage. Read Message From Queue\\n- Translate Message. Translate Message from Wire Format to Type\\n- Dispatch Message. Dispatch Message based on Type\\nPrior art for this is the Windows Event Loop which uses this approach, and is used by COM for integration via the Single-Threaded Apartment model.\\n","Decision":"Use a single-threaded message pump to preserve ordering and ensure sequential access to shared resources. Allow multiple pump instances for throughput.\\n","tokens":285,"id":326}
{"File Name":"planet4-docs\/adr-0006-define-scope-for-deployment-environments.md","Context":"## Context and Problem Statement\\nCurrently it\u2019s not clear what\u2019s the difference between Develop and Staging environment. Both are part of a new version deployment cycle to production.\\nBefore Continuous Delivery \\(CD\\) these 3 environments were semantically used to represent the 3 different branches. Production &gt; master, Staging &gt; release\/vx.x, Develop &gt; develop. [Now](https:\/\/planet4.greenpeace.org\/story\/11560\/continuous-delivery\/) this distinction no longer makes sense, since we only have one branch \\(master\\). We use the [test instances](https:\/\/support.greenpeace.org\/planet4\/ci-cd\/test-environments) \\(in practice these are Develop environments\\) to deploy the feature branches, for testing and UAT.\\nThe NROs that have dev teams also use their Develop environments for testing new plugin versions or specific branches. In order to deploy to Production they need to commit to the develop branch first \\(or trigger the Develop pipeline\\), although this branch has different versions of dependencies from master. This can be confusing. There is also no branch that reflects the Staging environment, since the release branches in the deployment repos are ephemeral and deleted during the deployment cycle.\\n","Decision":"Chosen option: **Implemented option one. Re-defined the scope and excluded the Develop environment from the release deployment pipelines.**\\n","tokens":251,"id":4090}
{"File Name":"gti-genesearch\/adr-014.md","Context":"## Context\\nThe current model for joining between genes and variants is via the annotation field, matching on gene IDs.\\nThis is not sustainable in future for two main reasons:\\n1. EVA have expressed their opinion that range based querying is the only efficient way to do this with the current MongoDB schema\\n2. Restricted data for EBiSC\/HipSci is only available via a HTSlib based endpoint which responds solely to range queries\\nTo this end, we need to support range based joins as well as term based joins\\n","Decision":"The main changes can be made fairly easily in `JoinMergeSearch`, but require the following changes to `JoinStrategy`:\\n1. A `JoinType` enum to trigger RANGE vs TERM\\n2. Changing the key fields from single strings to arrays.\\nThe latter is a simple way to encode the fields required (seq region, min, max) though note that species\/assembly may also be required.\\nIn an ideal world, `JoinStrategy` would have a defined generic type with these fields explicitly declared, but this is the simplest approach for now.\\nThis change has triggered matching changes in all objects that use including `SubSearchParams`.\\nThe implementation provided currently has separate query and fetch methods for range-based joins, which are separate to the term-based methods.\\n`RangeBasedJoinGeneSearch` provides a range-based join from gene to variation.\\n","tokens":108,"id":3281}
{"File Name":"human-essentials\/0005-extract-all-partner-operations-into-a-separate-application-intended-for-partners.md","Context":"## Context\\nPartner organizations work directly with the families in need. Sometimes they will need to collect PII such as address, names and ages of family members, and other contact information. This data is not necessary for Diaperbase to operate, and there are security concerns about that PII being disclosed should there be a breach. There is also a separation of concerns between the two applications; Diaperbase is inventory management and Partnerbase is effectively a CRM. At this time, we belive that they are different enough in their purposes that they should be separate.\\n","Decision":"A new application, Partnerbase, will be created to handle the CRM aspects. It will communicate over a private API with Diaperbase to handle request fulfillment.\\n","tokens":114,"id":3342}
{"File Name":"arch\/0044-wildcard-subdomain-resolution.md","Context":"## Context\\n\u968f\u7740\u4f1a\u627e\u623f\u5e73\u53f0\u7b2c\u4e09\u65b9\u516c\u5bd3\u7684\u5165\u9a7b\uff0c\u6211\u4eec\u9700\u8981\u624b\u52a8\u6dfb\u52a0\u5927\u91cf\u7684\u516c\u5bd3\u4e8c\u7ea7\u57df\u540d\u4e8e DNS \u4e2d\uff0c\u5982\uff0cayk.huizhaofang.com, jingyu.huizhaofang.com \u7b49\u3002\\n1. \u6574\u4e2a\u6d41\u7a0b\u4f9d\u8d56\u57df\u540d\u7ba1\u7406\u8005\uff1b\\n2. DNS \u7ba1\u7406\u63a7\u5236\u53f0\u7ef4\u62a4\u8fd9\u4e9b\u8bb0\u5f55\u5f88\u9ebb\u70e6\uff0c\u5e76\u5c06\u539f\u6709\u4e13\u7528\u57df\u540d\u6df9\u6ca1\u5728\u5927\u91cf\u8bb0\u5f55\u4e2d\uff1b\\n3. \u7f51\u7ad9\u505a\u4e86\u524d\u540e\u7aef\u5206\u79bb\uff0c\u8d77\u59cb\u9875\u6c38\u8fdc\u8fd4\u56de\u72b6\u6001\u4e3a 200 \u7684\u9875\u9762\u3002\\n","Decision":"1. DNS \u8bb0\u5f55\u6dfb\u52a0\u6cdb\u89e3\u6790\uff1b\\n2. Nginx server name \u6dfb\u52a0\u6cdb\u89e3\u6790\uff1b\\n3. \u4e0d\u53d7\u7ea6\u675f\u7684\u6cdb\u89e3\u6790\uff0c\u4f1a\u4f7f\u6240\u6709\u5b50\u57df\u540d\u8fd4\u56de\u72b6\u6001\u4e3a 200 \u7684\u9875\u9762\uff0c\u5bfc\u81f4\u641c\u7d22\u5f15\u64ce\u964d\u6743\uff1b\\n4. \u4f7f\u7528 `include \/path\/to\/server_names;` \u53ef\u4ee5\u901a\u8fc7\u72ec\u7acb\u7684\u6587\u4ef6\u89e3\u51b3\u6b64\u95ee\u9898\uff0c\u5e76\u53ef\u5bf9\u65b0\u6dfb\u52a0\u7684\u5b50\u57df\u540d\u505a code review\uff1b\\n5. \u4e0a\u9762\u7684\u65b9\u6848\u9700\u8981\u6bcf\u6b21\u66f4\u6539\u540e\u505a nginx reload\uff1b\u6709\u4e2a\u60f3\u6cd5\u662f\u901a\u8fc7 lua \u4ece redis \u4e2d\u83b7\u53d6\u652f\u6301\u7684\u5b50\u57df\u540d\uff0c\u8fdb\u884c\u5224\u65ad\u5e76\u8fc7\u6ee4\uff1b\\n","tokens":166,"id":2417}
{"File Name":"hello\/006-access-control.md","Context":"## Context\\nWe need a way to protect our app. Only a small number of people\\nshould be able to access the application. This includes the\\ndevelopers and the intended end users (i.e. you).\\n","Decision":"To protect our application, we will require all requests to include\\nan `Authorization` header containing a JWT. Any request that is missing\\nthis header will be rejected. Futhermore, the JWT will include an\\nexpiry so we can control the time period in which users can access\\nthe application.\\nThe authentication process will be implemented as an additional AWS\\nLambda function. In [Chalice], this is referred to as a Custom Authorizer\\n","tokens":46,"id":4362}
{"File Name":"gti-genesearch\/adr-004.md","Context":"## Context\\nWe need to be able to specify a target and have that target be the focus\\ne.g.\\ngenes, with variations mapped to them\\nvariations, with genes mapped to them\\ntranscripts, with genes mapped to them\\ngenes, with transcripts mapped to them\\nsequences, with genes mapped to them\\ngenes, with sequences mapped to them\\nUltimately, we may also need 3-stage joins as well (expression->genes->variations). The current implementation is too restrictive.\\n","Decision":"Rather than overloading a search with complex behaviour, this boils down to needing to have one endpoint per data type\\ne.g.\\n\/api\/genes\/query?fields=[name,genome,variations:{id}]\\n\/api\/variations\/query?fields=[id,genes:{}]\\nThis might use the same search underneath e.g. a transcript-based ESSearch, which automatically does the retargetting and restructures fields and queries accordingly. This allows a merge search to deal with these in batches as required.\\nA join\/merge search would work as follows:\\n* decompose queries and fields to split them into \"from\" and \"to\"\\n* query the primary db including join field\\n* hash results (in batches)\\n* for the hits, query the secondary and then pull back and add to the results\\nNote that pass-through targets are no longer a consideration - a gene search will contain transcripts as before. Note that we may need to flatten out sub-fields in ESSearch though.\\nA merge strategy can be specified to deal with situations where the \"join object\" is already present in the \"from\" results.\\nFor instance, homologues are merged into the \"from\" homologues.\\nA new implementation `JoinMergeSearch` now replaces `JoinAwareSearch`.\\n","tokens":104,"id":3283}
{"File Name":"upd-articles-api\/ADR-002-16012019-add-docker-support.md","Context":"**Context**\\nThe application can run on any machine with JVM installed, but what if we don't have a correct Java version in our machine or server environment? Containers comes to the rescue.\\n**Decision**\\nDocker (https:\/\/www.docker.com\/) is an easy to configure container platform to manage our applications in isolated containers. That will help us to deploy our application in any environment with the Docker daemon installed.\\n**Status**\\nAccepted\\n**Consequences**\\nBy the moment the container can be used just for deploys, development support can be added sharing a Volume in the Dockerfile. Instructions to modify the Dockerfile can be found [here](https:\/\/docs.docker.com\/engine\/reference\/builder\/).\\n","Decision":"Docker (https:\/\/www.docker.com\/) is an easy to configure container platform to manage our applications in isolated containers. That will help us to deploy our application in any environment with the Docker daemon installed.\\n**Status**\\nAccepted\\n**Consequences**\\nBy the moment the container can be used just for deploys, development support can be added sharing a Volume in the Dockerfile. Instructions to modify the Dockerfile can be found [here](https:\/\/docs.docker.com\/engine\/reference\/builder\/).\\n","tokens":150,"id":2987}
{"File Name":"easi-app\/0001-separate-infra-repo.md","Context":"## Decision Drivers\\n* Visibility of changes to anyone working in the codebases.\\n* Management of CI\/CD configuration.\\n* Iteration speed.\\n* Coupling of infra and app changes.\\n","Decision":"* Visibility of changes to anyone working in the codebases.\\n* Management of CI\/CD configuration.\\n* Iteration speed.\\n* Coupling of infra and app changes.\\n* *Separate repo for Infrastructure.*\\nDecoupling the infrastructure and application deployments allows iteration to\\nhappen on both of those aspects of the project at separate speeds without\\nhindering one another. We have experience in making this repo separation work\\nand can easily configure CI and deployment tooling to allow for proper\\norchestration to bring these two pieces together.\\nPlease see the full good\/bad list below.\\n","tokens":42,"id":2167}
{"File Name":"platform\/2021-08-10-storefront-coding-standards.md","Context":"## Context\\n* The current coding standards are not put into an ADR yet.\\n* This ADR is to determine the current standards to have a start from where to enhance the storefront concept further more.\\n","Decision":"### Controller\\n* Each controller action has to be declared with a @Since tag\\n* Each controller action requires a @Route annotation\\n* The name of the route should be starting with \"frontend\"\\n* Each route should define the corresponding HTTP Method (GET, POST, DELETE, PATCH)\\n* Routes which renders pages for the storefront (GET calls) are calling a respective pageloader to get the data it needs.\\n* The function name should be concise\\n* Each function should define a return type hint\\n* A route should have a single purpose\\n* Use Symfony flash bags for error reporting\\n* Each storefront functionality has to be available inside the store-api too\\n* A storefront controller should never contain business logic\\n* The controller class requires the annotation: @RouteScope(scopes={\"storefront\"})\\n* Depending services has to be injected over the class constructor\\n* Depending services has to be defined in the DI-Container service definition\\n* Depending services has to be assigned to a private class property\\n* A storefront controller has to extend the \\Shopware\\Storefront\\Controller\\StorefrontController\\n* Using LoginRequired annotation to identify whether the Customer is logged in or not.\\n* Each storefront functionality needs to make use of a store-api route service. to make sure, this functionality is also available via API\\n### Operations inside Storefront controllers\\nA storefront controller should never use a repository directly, It should be injected inside a Route.\\nRoutes which should load a full storefront page, should use a PageLoader class to load all corresponding data that returns a Page-Object.\\nPages which contains data which are the same for all customers, should have the @HttpCache annotation\\n#### Write operations inside Storefront controllers\\nWrite operations should create their response with the createActionResponse function to allow different forwards and redirects.\\nEach write operation has to call a corresponding store-api route.\\n### Page-\/PageletLoader\\n* A PageLoader is a class which creates a page-object with the data for the called whole page.\\n* A PageletLoader is a class which creates a pagelet-object with the data for a part of a page.\\nThe pageLoaders are a specific class to load the data for a given page.\\nThe controller calls the pageloader, which collects the needed data for that page via the Store-api.\\nThe pageloader can call other pageletloaders to get the data for pagelets(subcontent for a page).\\nThe pageloader always returns a page-object.\\n","tokens":43,"id":4510}
{"File Name":"paas-team-manual\/ADR006-rds-broker.html.md","Context":"## Context\\nWe need to provide tenants with the ability to provision databases for use in\\ntheir applications. Our first iteration of this will be using RDS.\\nWe investigated some implementations of a service broker which supported RDS\\n- [cf platform eng](https:\/\/github.com\/cf-platform-eng\/rds-broker)\\n- [18F](https:\/\/github.com\/18F\/rds-service-broker)\\n","Decision":"We will use the [cf platform eng](https:\/\/github.com\/cf-platform-eng\/rds-broker)\\nrds broker. As this is not a supported product, we will fork this and maintain\\nthis and implement new features ourselves.\\n","tokens":85,"id":186}
{"File Name":"cloud-on-k8s\/0002-global-operator.md","Context":"## Context and Problem Statement\\nThis proposal explains the raison d'\u00eatre for the *global operator*. The *global operator* is an operator that contains controllers for resources that span more than one deployment.\\nAdditionally, this proposal outlines deploying an operator per namespace that is responsible for managing namespace-local resources, such as individual Elasticsearch clusters and Kibana nodes.\\n## Decision Drivers\\n### Prior state\\nAs of when this proposal was written, a single stack operator was running in the \"stack-operators-system\" namespace:\\nAll controllers watch all the namespaces for CRDs we define and reconcile them within the namespace they are defined in.\\n### Motivation\\nWe want to be able to address the following concerns:\\n- Security: RBAC does not allow us to limit the resources we may be watching based on their labels or annotations so our operator has to have a very wide set of permissions (practically close to full admin on all K8s cluster resources)\\n- All controllers watch a lot of K8s API resources in all namespaces. This has multiple downsides:\\n- Unnecessary load put on K8s apiserver as we'll be watching resources that are not related to our use case (e.g listening to all Services, ConfigMaps, Secrets etc). We can expect some improvements from controller-runtime in this area, but the time-frame is not clear.\\n- Single point of failure: if the operator crashes, not only the triggering resources will be affected, but an entire region.\\n- Slowly responding clusters \/ clusters with a large cluster state may negatively affect performance \/ resource consumption.\\n- Resource usage as number of deployed resources grow is difficult to ascertain a-priori.\\n- To upgrade the version of the operator, all operations need to go down temporarily (this may be mitigated at least somewhat by running multiple operators and introduce leader election between them).\\n- May or may not be fine: No ability to slowly roll changes out, needs to be all or nothing.\\n","Decision":"### Prior state\\nAs of when this proposal was written, a single stack operator was running in the \"stack-operators-system\" namespace:\\nAll controllers watch all the namespaces for CRDs we define and reconcile them within the namespace they are defined in.\\n### Motivation\\nWe want to be able to address the following concerns:\\n- Security: RBAC does not allow us to limit the resources we may be watching based on their labels or annotations so our operator has to have a very wide set of permissions (practically close to full admin on all K8s cluster resources)\\n- All controllers watch a lot of K8s API resources in all namespaces. This has multiple downsides:\\n- Unnecessary load put on K8s apiserver as we'll be watching resources that are not related to our use case (e.g listening to all Services, ConfigMaps, Secrets etc). We can expect some improvements from controller-runtime in this area, but the time-frame is not clear.\\n- Single point of failure: if the operator crashes, not only the triggering resources will be affected, but an entire region.\\n- Slowly responding clusters \/ clusters with a large cluster state may negatively affect performance \/ resource consumption.\\n- Resource usage as number of deployed resources grow is difficult to ascertain a-priori.\\n- To upgrade the version of the operator, all operations need to go down temporarily (this may be mitigated at least somewhat by running multiple operators and introduce leader election between them).\\n- May or may not be fine: No ability to slowly roll changes out, needs to be all or nothing.\\nSuperseded by [005](https:\/\/github.com\/elastic\/cloud-on-k8s\/blob\/main\/docs\/design\/0005-configurable-operator.md).\\n### Positive Consequences <!-- optional -->\\n+ Can limit what each operator can do based on namespace (trivial through RBAC).\\n+ Can attribute resource usage of the operator to namespaces \/ customers \/ projects.\\n+ Enables rolling out updates in a controlled manner.\\n### Negative Consequences <!-- optional -->\\n- Introduces more than one operator, complicating deployment and debugging.\\n- Controllers in the global operator, such as the CCR controller still need to be scaled on a number-of-clusters \/ associations basis (but it does not need to connect to individual ES clusters).\\n","tokens":405,"id":4700}
{"File Name":"Endjin.RecommendedPractices.NuGet\/0003-defer-links-to-solution-level-files.md","Context":"## Context\\nWe optionally generate certain solution-level files (e.g., `PackageIcon.png`, `stylecop.json`) and where appropriate or\\nnecessary, we also arrange for these files to show up in project. (This is necessary for `PackageIcon.png`\u2014the file\\nneeds to be in the project for the corresponding `<PackageIcon>` property to work.) However, this causes problems\\nwhen first adding `Endjin.RecommendedPractices` to a project.\\nThe problem is that we don't get the opportunity to generate these files until a build occurs. However, Visual Studio\\nwill attempt to display any files we add to the project immediately after the reference to `Endjin.RecommendedPractices`\\nhas been added, and before any build has occurred. The upshot is that for these kinds of files, Visual Studio shows\\nthem with a big red X, because the physical files they refer to don't exist. Worse, even after the build creates the\\nfiles, the crosses remain, because Visual Studio appears not to update such things when the filesystem changes.\\nThe effect of this was that after adding `Endjin.RecommendedPractices` to a project you would need to build, then\\nunload the solution, and then reload it, before everything looked OK.\\n","Decision":"We now make all additions of files to a project conditional on the file existing. E.g.:\\nWe are going to require all projects to contain this line at the top of the `csproj`:\\n```xml\\n<ItemGroup Condition=\"($(EndjinDisableCodeAnalysis) != 'true') and (Exists('$(SolutionDir)stylecop.json'))\">\\n<AdditionalFiles Include=\"$(SolutionDir)stylecop.json\" Link=\"stylecop.json\" \/>\\n<\/ItemGroup>\\n```\\nThis is how we make a file link to `stylecop.json` appear in a project. It used to be conditional only on the\\n`EndjinDisableCodeAnalysis` build variable. But now we have an additional `Exists` test.\\n","tokens":259,"id":4661}
{"File Name":"dos-server\/adr-7-handles.md","Context":"## Context\\nHandles are persistent links that currently point to Dome pages.\\nThe Handles server is embedded in Dome.\\nAn example in [Dome](https:\/\/dome.mit.edu\/handle\/1721.3\/82731).\\nAs handles point to a web page, and as DOS is not meant to expose a web interface for public consumption,\\nthe handles emitted by Dome can continue to point to Dome and the Handle server can remain embedded in Dome,\\nuntil such time when an application other than Dome is in production.\\n","Decision":"Handles will not be minted by DOS and the Handle server will remain in Dome.\\n","tokens":108,"id":3371}
{"File Name":"clean-architecture-example\/0003-use-spring-framework.md","Context":"## Context\\nWe need to build the how part of the application\\n","Decision":"We will use Springs ecosystem to implement how our application works.\\nMore explicitly we will use Spring Boot, Spring Data and Spring WebMvc.\\n","tokens":14,"id":3533}
{"File Name":"molgenis-frontend\/0002-use-typescript.md","Context":"## Context\\nWe need a way to describe the structure of Javascript objects for readers of the code to be able to understand the structure.\\nAnd we need a way to detect null references at build time Javascript.\\nOur Javascript passes though a build step.\\nOur Javascript is maintained by people who did not write the code.\\n","Decision":"We will use Typescript as a tool to describe object structure.\\n","tokens":66,"id":1515}
{"File Name":"connecting-to-services\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in\\nthis\\n[article](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions)\\n","tokens":16,"id":2214}
{"File Name":"drt-v2\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":1917}
{"File Name":"uniprot-rest-api\/0005-solrcloud.md","Context":"## Context\\nWe need a search engine to which we will send user queries, and from which we will receive their results.\\nMoreover, we need an engine that can scale with our data and be resilient to faults (network, filesystem, etc.).\\n","Decision":"The current UniProt website uses lucene as the search engine. This is very fast. However, the drawback is that it does not easily scale.\\nThis can be provided by Solrcloud. We have used this for the Proteins API with success, and therefore have 4 years of experience with it.\\n","tokens":52,"id":1476}
{"File Name":"fundraising-application\/006_Vue.js.md","Context":"## Context\\nMost of the JavaScript Code on the FundraisingFrontend already has the [[ https:\/\/facebook.github.io\/flux\/docs\/in-depth-overview.html#content | Flux architecture ]] with its one-way data flow (using [[ https:\/\/redux.js.org\/ | Redux ]]), but it does not use one of the popular reactive \"component\" frameworks like Vue or React. Instead, it uses self-written \"components\" (two-way binding with handling of DOM events) and \"view handlers\" (one-way binding DOM manipulators) that are tied to the markup of the page via jQuery, in an attempt to do progressive enhancement and decouple the markup from the functionality.\\nThe current JavaScript code has several drawbacks:\\n* **JavaScript resource size.** The components and stores are built as one big \"library\" file (called `wmde.js`, with a global object called `WMDE`), instead of having separate \"entry points\" for the different pages (donation, membership). The current \"entry points\", `donationForm.js` and `membershipForm`, add one more JavaScript resource that the browsers needs to download, adding HTTP overhead and latency.\\n* **Hard to understand.** While the [[ https:\/\/github.com\/wmde\/FundraisingFrontend\/blob\/master\/doc\/HOWTO_Create_a_form.md | architecture is documented ]], the setup lacks the in-depth explanations, code snippets and tutorials that common frameworks come with. Also, the code itself does not use modern ECMAScript features (classes, spread operator, arrow functions). Instead it uses  custom, factory-function based \"classes\", making the code harder to understand.\\n* **Hard to extend.** Where Vue and React have ecosystems attached to them, all code for the current JavaScript - asynchronous validation, connecting the store to the views, etc - is custom, adding to the accidental complexity and maintenance burden.\\n* **Hard to reuse.** While the self-written components are quite flexible and \"pluggable\", the entry point scripts are very long and hard to understand, since there is no hierarchy of elements. Instead, they are a big factory that initializes all the classes. That initialization code is duplicated across scripts.\\n","Decision":"Going forward, we will use Vue to render the frontend. We chose it for the following reasons:\\n* It's a mature, tested, well-documented widely used framework with an open source license and an active ecosystem\\n* There is already some knowledge about Vue in the developer and UX teams\\n* Vue is used in wikidata, making knowledge sharing easier.\\nWe will write all new features in Vue and ECMAScript 2015 (and later), refactoring and cleaning up the existing code base. The redux \"reducers\" and \"state aggregators\" will move to Vuex modules, while the Twig templates, \"view handlers\" and \"components\" of the old code base become Vue components.\\n","tokens":445,"id":1530}
{"File Name":"html-integrations\/002-Longevity--senderId--sessionId--Telemetry.md","Context":"## Context (Discussion)\\nReading the Telemetry documentation proposed by the Data Science team, we have\\nencountered that the sender and session ids are not concretely defined for each\\nproduct.\\nIn the case of MathType web, there could be various interpretations, e.g.\\n- senderId: changes on page load.\\n- sessionId: changes on opening MathType.\\n","Decision":"We asked the Data Science team and finally settled on the following interpretation:\\n- senderId: ideally lasts for ever. In practice, should at least last in a same web session, across page loads.\\n- sessionId: changes on page load.\\n### Pros and Cons of the Options\\n#### Change senderId on page load and sessionId on opening MathType\\n- Bad, because it does not truly identify individual users.\\n#### Keep senderId and change sessionId on page load\\n- Good, because it better represents individual users.\\n- Bad, because it's hard to keep permanent data on the client's browser.\\n","tokens":80,"id":338}
{"File Name":"modular-monolith-with-ddd\/0007-use-cqrs-architectural-style.md","Context":"## Context\\nOur application should handle 2 types of requests - reading and writing. <\/br>\\nFor now, it looks like:<\/br>\\n- for reading, we need data model in relational form to return data in tabular\/flattened way (tables, lists, dictionaries).\\n- for writing, we need to have a graph of objects to perform more sophisticated work like validations, business rules checks, calculations.\\n","Decision":"We applied the CQRS architectural style\/pattern for each business module. Each module will have a separate model for reading and writing. For now, it will be the simplest CQRS implementation when the read model is immediate consistent. This kind of separation is useful even in simple modules like User Access.\\n","tokens":87,"id":888}
{"File Name":"pride-london-app\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":2535}
{"File Name":"eq-questionnaire-runner\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":3678}
{"File Name":"oasis-core\/0009-ed25519-semantics.md","Context":"## Context\\n> In programming, it's often the buts in the specification that kill you.\\n>\\n> -- Boris Beizer\\nFor a large host of reasons, mostly historical, there are numerous definitions\\nof \"Ed25519 signature validation\" in the wild, which have the potential to\\nbe mutually incompatible.  This ADR serves to provide a rough high-level\\noverview of the issue, and to document the current definition of \"Ed25519\\nsignature verification\" as used by Oasis Core.\\n","Decision":"The Oasis Core consensus layer (and all of the Go components) currently uses\\nthe following Ed25519 verification semantics.\\n- Non-canonical s is rejected (MUST enforce `s < L`)\\n- Small order A\/R are rejected\\n- Non-canonical A\/R are accepted\\n- The cofactored verification equation MUST be used (`[8][S]B = [8]R + [8][k]A`)\\n- A\/R may have a non-zero torsion component.\\n### Reject Non-canonical s\\nEd25519 signatures are trivially malleable unless the scalar component is\\nconstrained to `0 <= s < L`, as is possible to create valid signatures\\nfrom an existing public key\/message\/signature tuple by adding L to s.\\nThis check is mandated in all recent formulations of Ed25519 including\\nbut not limited to RFC 8032 and FIPS 186-5, and most modern implementations\\nwill include this check.\\nNote: Only asserting that `s[31] & 224 == 0` as done in older implementations\\nis insufficient.\\n### Reject Small Order A\/R\\nRejecting small order A is required to make the signature scheme strongly\\nbinding (resilience to key\/message substitution attacks).\\nRejecting (or accepting) small order R is not believed to have a security\\nimpact.\\n### Accept Non-canonical A\/R\\nThe discrete logarithm of the Ed25519 points that have a valid non-canonical\\nencoding and are not small order is unknown, and accepting them is not\\nbelieved to have a security impact.\\nNote: RFC 8032 and FIPS 186-5 require rejecting non-canonically encoded\\npoints.\\n### Cofactored Verification Equation\\nThere are two forms of the Ed25519 verification equation commonly in use,\\n`[S]B = R + [k]A` (cofactor-less), and `[8][S]B = [8]R + [8][k]A`\\n(cofactored), which are mutually incompatible in that it is possible\\nto produce signatures that pass with one and fail with the other.\\nThe cofactored verification equation is explicitly required by FIPS 186-5,\\nand is the only equation that is compatible with batch signature verification.\\nAdditionally, the more modern lattice-reduction based technique for fast\\nsignature verification is incompatible with existing implementations unless\\ncofactored.\\n### Accept A\/R With Non-zero Torsion\\nNo other library enforces this, the check is extremely expensive, and\\nwith how Oasis Core currently uses Ed25519 signatures, this has no security\\nimpact.  In the event that Oasis Core does exotic things that, for example,\\nrequire that the public key is in the prime-order subgroup, this must be\\nchanged.\\n","tokens":105,"id":4355}
{"File Name":"elife-base-images\/0001-commit-tags.md","Context":"## Context\\nTraceability of a Docker image to a source code repository is valuable to debug any problem that comes up during testing and deployment.\\nDependency pinning on project images pinning a particular version of the base image they are using is also valuable for build reproducibility.\\n","Decision":"We will tag every new, tested version of an image using the commit SHA value that produced it.\\n","tokens":57,"id":1503}
{"File Name":"fare-platform\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":2219}
{"File Name":"heptaconnect-docs\/2021-06-17-flow-component-short-notation.md","Context":"## Context\\nWhen a portal has its code separated into different domains, many flow components will look like this:\\n```php\\n<?php\\ndefine(string_types=1);\\nnamespace FooBar\\Emitter;\\nuse FooBar\\Packer\\BottlePacker;\\nuse FooBar\\Service\\ApiClient;\\nuse Heptacom\\HeptaConnect\\Dataset\\Base\\Contract\\DatasetEntityContract;\\nuse Heptacom\\HeptaConnect\\Portal\\Base\\Emission\\Contract\\EmitContextInterface;\\nuse Heptacom\\HeptaConnect\\Portal\\Base\\Emission\\Contract\\EmitterContract;\\nclass BottleEmitter extends EmitterContract\\n{\\nprivate ApiClient $client;\\nprivate BottlePacker $packer;\\npublic function __construct(ApiClient $client, BottlePacker $packer)\\n{\\n$this->client = $client;\\n$this->packer = $packer;\\n}\\npublic function run(string $externalId, EmitContextInterface $context) : ?DatasetEntityContract\\n{\\nreturn $this->packer->pack($this->client->getBottleData($externalId));\\n}\\n}\\n```\\nThis sample emitter of about 30 lines of code only consists of, when trimmed down to the essentials, two lines of instructions.\\nAcquisition of dependencies:\\n```php\\npublic function __construct(ApiClient $client, BottlePacker $packer\/*, string $externalId*\/)\\n```\\nWiring everything into an emitter run method:\\n```php\\nreturn $this->packer->pack($this->client->getBottleData($externalId));\\n```\\nUsing this perspective we have 28 lines of code that are basically boilerplate.\\nBoilerplate code is code we want to eliminate.\\n","Decision":"* Allow declaration of flow components in a callback registration way.\\n","tokens":372,"id":3201}
{"File Name":"celestia-core\/adr-002-event-subscription.md","Context":"## Context\\nIn the light client (or any other client), the user may want to **subscribe to\\na subset of transactions** (rather than all of them) using `\/subscribe?event=X`. For\\nexample, I want to subscribe for all transactions associated with a particular\\naccount. Same for fetching. The user may want to **fetch transactions based on\\nsome filter** (rather than fetching all the blocks). For example, I want to get\\nall transactions for a particular account in the last two weeks (`tx's block time >= '2017-06-05'`).\\nNow you can't even subscribe to \"all txs\" in Tendermint.\\nThe goal is a simple and easy to use API for doing that.\\n![Tx Send Flow Diagram](img\/tags1.png)\\n","Decision":"ABCI app return tags with a `DeliverTx` response inside the `data` field (_for\\nnow, later we may create a separate field_). Tags is a list of key-value pairs,\\nprotobuf encoded.\\nExample data:\\n```json\\n{\\n\"abci.account.name\": \"Igor\",\\n\"abci.account.address\": \"0xdeadbeef\",\\n\"tx.gas\": 7\\n}\\n```\\n### Subscribing for transactions events\\nIf the user wants to receive only a subset of transactions, ABCI-app must\\nreturn a list of tags with a `DeliverTx` response. These tags will be parsed and\\nmatched with the current queries (subscribers). If the query matches the tags,\\nsubscriber will get the transaction event.\\n```\\n\/subscribe?query=\"tm.event = Tx AND tx.hash = AB0023433CF0334223212243BDD AND abci.account.invoice.number = 22\"\\n```\\nA new package must be developed to replace the current `events` package. It\\nwill allow clients to subscribe to a different types of events in the future:\\n```\\n\/subscribe?query=\"abci.account.invoice.number = 22\"\\n\/subscribe?query=\"abci.account.invoice.owner CONTAINS Igor\"\\n```\\n### Fetching transactions\\nThis is a bit tricky because a) we want to support a number of indexers, all of\\nwhich have a different API b) we don't know whenever tags will be sufficient\\nfor the most apps (I guess we'll see).\\n```\\n\/txs\/search?query=\"tx.hash = AB0023433CF0334223212243BDD AND abci.account.owner CONTAINS Igor\"\\n\/txs\/search?query=\"abci.account.owner = Igor\"\\n```\\nFor historic queries we will need a indexing storage (Postgres, SQLite, ...).\\n### Issues\\n- https:\/\/github.com\/tendermint\/tendermint\/issues\/376\\n- https:\/\/github.com\/tendermint\/tendermint\/issues\/287\\n- https:\/\/github.com\/tendermint\/tendermint\/issues\/525 (related)\\n","tokens":164,"id":4106}
{"File Name":"test_track\/adr-003.md","Context":"## Context\\nIn order to allow apps to ship independently, we need to ensure that\\nsplit names defined in different apps don't collide. The strategy until\\nnow has been for developers to run all the apps that might have\\nconflicting splits in local dev, but this doesn't scale well as they may\\nnot have the latest version of each app at all times and a\\nfully-upgraded local testtrack server to detect conflicts that might\\narise in production.\\nWe need to make sure that migrations don't get jammed up in production\\neven if a developer only has their own app downloaded locally.\\n","Decision":"New migration runners will be expected to prefix their split names with\\ntheir app names and a dot. Legacy runners will be grandfathered out of\\nthis constraint, but we will begin soft enforcing that any new split\\nname conform to using at most a single dot, and that the text before\\nthat dot be the app name of the split's owner_app.\\n","tokens":126,"id":4757}
{"File Name":"hee-web-blueprint\/0006-use-testcontainers-for-providing-ephemeral-test-environments.md","Context":"## Context\\nHaving determined to use Gherkin and Selenide to manage the generation of our automated tests, we need to determine the platform on which these tests will run. The platform should be open source, if at all possible and portable between hosting platforms.\\n","Decision":"We have determined to use the testcontainers.org project to manage the infrastructure for our automated tests. This platform provides a way of generically describing the containers that we use to run our tests and can be executed on platforms including Github Actions.\\n","tokens":54,"id":1198}
{"File Name":"sre-challenge\/0003-etcd-cluster-mode.md","Context":"## Context\\nEtcd has three ways to setup a cluster:\\n- Statically: setting all the IP's and listeners of the cluster members from\\ncommand line\\n- Dynamically: using an external etcd cluster (https:\/\/discovery.etcd.io\/new?size=X)\\n- Dynamically: using a DNS SRV record.\\nI have tested two first options, and found that the main problem is that the\\ncluster member is identified by its name AND it's peer URL, not only its name.\\nThat's make my job very difficult as I cannot just open a 0.0.0.0\/0 listener\\nfor listening and peering with other members.  It has to use always its IP.\\nI need to find a way to identify each etcd cluster member with the IP address\\nfor peering.\\n","Decision":"There is an option: setting a network in docker \/ aws vpc and force the IP\\naddress used for that container \/ instance.\\n","tokens":173,"id":3003}
{"File Name":"twig-infrastructure\/0006-use-ecs-fargate-host-type.md","Context":"## Context\\nAWS Bare Metal rig gives you the choice between EC2 hosting or FARGATE for compute.\\n","Decision":"For the Twig riglet, we will use FARGATE.  Primary driver for this decision is to have a reference the uses FARGATE instead of EC2, and we are in the process of updating the Twig riglet.\\n","tokens":23,"id":348}
{"File Name":"infra\/0004-prometheus-and-alertmanager-monitoring-and-alerting.md","Context":"## Context\\nWe want to have a flexible, easy to run, but not too expensive alerting and monitoring solution.  Since we are primarily kubernetes based, something built around that seems ideal.  We are in a little bit of a rush given that our current monitoring solution has yearly renewals, and that renewal would be coming up at the end of the month.\\nPossible options include, influx cloud (v1, v2), influx we host (rejected mostly because v2 is not yet ready for primetime, and team has no influx familiarity).  New Relic, DataDog, HoneyComb are all great products, but are a bit expensive for us (saas prices for our data load seem to be too much).  Tools like Nagios are hard to run in the cloud because they assume mostly assume 100% network reliability.  Given all that, and our team's familiarity with prometheus, it seems like the best choice.\\n","Decision":"Run prometheus and alertmanager, on our clusters.  Run one prom per k8s cluster to collect metrics. Run one prom\/grafana\/alertmanager deployment to collect and display the information all in one place.  Send alerts to slack and\/or pagerduty depending on severity.  Monitor this stack with dead man's snitch or other negative alerting services.\\n","tokens":194,"id":857}
{"File Name":"runner\/0354-runner-machine-info.md","Context":"## Context\\n- Provide a mechanism in the runner to include extra information in `Set up job` step's log.\\nEx: Include OS\/Software info from Hosted image.\\n","Decision":"The runner will look for a file `.setup_info` under the runner's root directory, The file can be a JSON with a simple schema.\\n```json\\n[\\n{\\n\"group\": \"OS Detail\",\\n\"detail\": \"........\"\\n},\\n{\\n\"group\": \"Software Detail\",\\n\"detail\": \"........\"\\n}\\n]\\n```\\nThe runner will use `::group` and `::endgroup` to fold all detail info into an expandable group.\\nBoth [virtual-environments](https:\/\/github.com\/actions\/virtual-environments) and self-hosted runners can use this mechanism to add extra logging info to the `Set up job` step's log.\\n","tokens":38,"id":3742}
{"File Name":"nucleus\/0002-push-data-changes-to-github.md","Context":"## Context\\nThe API for publishing release notes is not advanced and is just a giant blob of JSON containing every release in the database. A GitLab Job runs on a schedule and reads this blob, splits it into a file per release, and commits those changes to a GitHub repo. This job is slow and is something else to maintain and monitor separate from Nucleus. So the decision was between improving the API to only send the releases that had changed since the last sync, or to push changes to GitHub as soon as the're made. The latter has the advantages of happening very quickly after the change is saved, and having the context of the Nucleus user who made the change which can also be recorded in the Git commit.\\n","Decision":"We've decided to go with pushing changes directly to GitHub via the GitHub API and using an async worker system to do it. The async system chosen was [Spinach][].\\n","tokens":149,"id":93}
{"File Name":"gsp\/ADR021-alerting.md","Context":"## Context\\nThe teams need timely notifications based on key indicators in order that they can ensure reliability and respond to issues.\\nThe prometheus operator included in the GSP cluster can provide Alertmanager however we would like to manage alert routing across GDS and not duplicate routing rules or manage multiple sets of alert targets.\\n","Decision":"We will route alerts to a separately hosted shared [Alertmanager](https:\/\/prometheus.io\/docs\/alerting\/alertmanager\/) to handle platform alert routing\\n","tokens":63,"id":3894}
{"File Name":"js-sdk\/0014-managed-domain-verification-and-blocking-in-apps.md","Context":"## Context\\nThere is no validation for the managed domains specified in the gateways information on the explorer side. Some of these domains are not delegated properly to the gateway's name server so all subdomains we create using these domains are not populated and not resolvable.\\n","Decision":"Create a test subdomain of each managed domain and verify that the subdomain is resolvable and block managed domains that fail this check for a certain amount of time.\\n","tokens":54,"id":5199}
{"File Name":"modular-monolith-with-ddd\/0001-record-architecture-decisions.md","Context":"## Context\\nAs the project is an example of a more advanced monolith architecture, it is necessary to save all architectural decisions in one place.\\n","Decision":"For all architectural decisions Architecture Decision Log (ADL) is created. All decisions will be recorded as Architecture Decision Records (ADR).\\nEach ADR will be recorded using [Michael Nygard template](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions), which contains following sections: Status, Context, Decision and Consequences.\\n","tokens":30,"id":900}
{"File Name":"operational-data-hub\/0008-config-and-environment-variables.md","Context":"## Context\\nWe feel the need to create guidelines for the use of config variables and environment variables when using cloudbuild.\\n","Decision":"**In Short**\\nThe code that is going to be executed by the Cloud Function (aka the project that is deployed) should receive its configuration variables from a config file.\\nThe cloudbuild steps use environment variables which can be stored in the cloudbuild.yaml or an external file.\\n**Elaboration**\\nWhen developing a Cloud Function, you should store the variables in a config file. The variables are easy to read and use, and other developers (and you) don\u2019t need to build anything for the project to run (unless something other than variables need a build).\\nThis moves over to when a project is deployed: The variables used in the code of the Cloud Function are all stored within the project: A config file. The project should not receive variables from a cloudbuild, but from a file that is merged into the project by the build (or the other way around).\\nWhen building something, you might need variables that you are only going to use for building\/deployment. Or you might use an external project that needs to get some variables. These variables should be given as environment variables or CLI flags. These variables could be stored in a file that is placed inside a repository for easy access, but can be put into the build as environment files.\\n**Examples**\\nConfiguration stored in a repository and merged into the project environment (or the other way around), like this example:\\n`config.py`\\n```python\\nINDEX = 0\\nTOKEN = 'AAAAaaaaBBBBbbbb'\\n```\\n`__main__.py`\\n```python\\nfrom config import INDEX, TOKEN\\n```\\n<br>\\nConfiguration as an environment variable used to build in a specific step, like this example:\\n```yaml\\nsubstitutions:\\n_VENV: '\/venv'\\n- name: 'cloud'\\nentrypoint: 'bash'\\nargs:\\n- '-c'\\n- |\\nsource ${_VENV}\/bin\/activate\\n```\\n","tokens":27,"id":2745}
{"File Name":"twig-infrastructure\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":353}
{"File Name":"lcarsde\/use-of-colors.md","Context":"## Context and Problem Statement\\nWhat colors shall be used for buttons, statistics and inactive elements considering usability. This concerns first and\\nforemost the tool applications.\\n","Decision":"Buttons or any usable elements should use the colors #ff9900, #9999ff, #cc6666 and #cc6699 as background and black text.\\nDisplayed text should preferably be colored #ff9900 on black background. Other text colors may be used after consideration.\\nGraphs, diagrams and other status and statistical displays should use the colors #ffcc99, #9999cc, #9999ff and for warning purposes #ff9966, #ff9900 and #cc6666.\\nDesign elements should use the colors #cc99cc, #9999cc and #ff9966. These colors are less \"strong\" and don't take as much attention.\\n[1]: http:\/\/mrbsdomain.com\/gallery\/var\/albums\/repository\/lcars_colors.gif\\n","tokens":33,"id":141}
{"File Name":"plant-shop\/0004-use-the-plantuml-as-an-default-way-to-create-uml-diagrams.md","Context":"## Context\\nIt's occuring very often that drawn documentations is no longer up to date.\\n","Decision":"We are going to use [PlantUml](http:\/\/plantuml.com\/) to store our diagrams in repository.\\n","tokens":21,"id":3350}
{"File Name":"datalab\/0035-use-of-microbadger-for-docker-containers.md","Context":"## Context\\nWe have a growing number of Docker containers and it is useful to have at a glance\\ninformation available about them. [MicroBadger](https:\/\/microbadger.com\/) provides\\na way to inspect and visualise Docker containers.\\n","Decision":"We have decided to use MicroBadger for new containers and will update existing containers\\nas we make updates to them.\\n","tokens":51,"id":741}
{"File Name":"sfpowerscripts\/003-release-autorollback.md","Context":"## Context and Problem Statement\\nWhen one releases a set of packages into an environment, there could be situations where one of the package fails and resulting in an incorrect org in\\nterms of functionality.\\nFor eg: A release which consists of packages A,B and C failed during the installation of C, has new versions of A and B, introducing new functionality\\nto the org, where as its not accurate without package 'C'.\\nThough number of these instances are pretty low in higher environments, as a particular release would be tested a number of times in multiple environments in lower environments. There are still instances where packages fail to install mostly due to a missing manual step. This results in a potential downtime till the team addresses the failure by a roll-forward.\\nIt would be ideal in this scenario to have a rollback option, which basically realigns the org back to the versions of the packages that were available in the org before the release was intiated.\\n","Decision":"### Release command to support a rollback function\\nRelease command will support an optional rollback function enabled through `rollback:true` parameter in the release defintion. Once this functionality is activated, release command will keep track of existing packages in the org (in memory) before deploying packages as part of the current release. In case of any failures, release command will fetch the old artifacts from the artifact repository and proceed to installing these packages into the org.\\nUnlocked packages have its own lifecycle and Salesforce would maintain deprecation and removal of unused components arising from a rollback. However for source packages, it needs to have a destructive manifest to remove the items already deployed. This functionality only attemps to install an earlier set of packages, and doesnt attempt to destroy any deployed components, especially in the case of source packages. However we could let the users know what are the components left dangling by providing a table of metadata components that will not be removed.\\n","tokens":197,"id":4778}
{"File Name":"operational-data-hub\/0040-hpa.md","Context":"## Context\\nHigh privilege access (HPA) limits production access for developers to only the components and period this access is required to investigate issues of check system health. This implements the [principle of least privilege](0039-least-privilege-access.md) for support on production systems.\\n","Decision":"We will use a high privilege access procedure to secure access to production systems for support.\\n","tokens":60,"id":2721}
{"File Name":"nso.aurora\/SegmentationEngine.md","Context":"## Context\\nThe purpose of Segmentation engine are:\\n- grouping of customers' goals\\n- targeted groups\\n- history\\n- purchasing behaviour\\n- eating habit, location\\n- managing coupons and promotions\\nWhat is the difference between Recommendation and Segmentation engines?\\n","Decision":"It was decided to not have it.\\n","tokens":54,"id":304}
{"File Name":"verify-stub-idp\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":4284}
{"File Name":"buy-for-your-school\/0011-use-sidekiq-for-asynchronous-tasks.md","Context":"## Context\\nWe need a way for the service to automatically and regularly run a task.\\nWe already have Redis available as our caching layer and Sidekiq works great with it.\\nAn alternative could be to use Cron for scheduled tasks and a postgres backed asynchronous jobs, perhaps even run inline. We know how to get Sidekiq running with Docker and reusing Redis (rather than Postgres) for job data that is ephemeral feels a better fit given we already have Redis.\\n","Decision":"Use Sidekiq for processing asynchronous tasks.\\n","tokens":99,"id":1260}
{"File Name":"platform\/2021-09-14-technical-concept-custom-entities.md","Context":"## Context\\nIt should be possible for apps to define their entities. Furthermore, it should be possible, if desired, that these entities are available via Store API.\\nLater, it should also be possible for store operators to create such entities. The concept is to consider that Apps can not add PHP code into the system under current circumstances. Also, a store operator is, seen from our point of view, not able to write PHP code himself to guarantee logic for his custom entities.\\nTherefore, purely through the definition of a custom entity, certain business logic should be automatically guaranteed.\\n","Decision":"### Schema\\n* Definition\\n* An app can include a `config\/custom_entity.xml` file.\\n* Multiple custom entities can be defined in the XML file.\\n* Each custom entity, is registered with the prefix `custom_entity_` or the `ce_` shorthand.\\n* App developers can then define that they would like to have `custom_entity_swag_blog` as an entity\\n* To prevent naming collisions, app developers should always add their developer prefix to the entity name\\n* We then create the `custom_entity_swag_blog` table\\n* Tables \/ Properties \/ Columns:\\n* A proper MySQL table is created for each custom entity.\\n* For each custom entity field we create a real MySQL table column.\\n* We support the following field data types:\\n* All scalar fields (int, string, text, float, date, boolean)\\n* All JSON fields (JSON, list, price, etc.)\\n* All \"linking\" associations (many-to-one and many-to-many)\\n* A bi-directional association will be left out for now.\\n* one-to-one and one-to-many will be not supported for now.\\n* Install & Update\\n* When installing and updating an app, the core automatically performs a schema update.\\n* Consider running a `dal:validate` on the schema when installing and updating an app.\\n* New fields on a custom entity must always be nullable or have a default\\n* Changing a field\/property data type is not allowed\\n* If a field is no longer defined in the .xml file, it will be deleted from the database.\\n* Identification and representation\\n* Each custom entity gets a `IdField(id)`, which serves as primary key\\n* Each custom entity gets a field `TranslatedField(label)`, which is required and serves as display name for the admin\\n### Bootstrapping\\n* At kernel boot we load all custom entities from the database and register them in the registry and di-container.\\n* For each custom entity, an entity definition is registered\\n* A generic entity definition is used, which gets the property\/column schema injected\\n* It must be checked how performant this is in case of bad performance we must put a cache in front of it (serialized properties\/columns e.g.)\\n* If no database connection exists, a kernel boot should still be possible\\n* The loading of the custom entities for the kernel boot should be outsourced to a CustomEntityKernelLoader\\n### Api availability\\nFor routing, we have to trick a bit, because currently for each entity in the system the routes defined exactly. This is not possible because the API route loader is triggered before the custom entities are registered. Therefore...\\n* We always register `\/api\/custom-entity-{entity}` as an API route and point to a custom controller that derives from ApiController.\\n* A request `\/api\/custom-entity-swag-blog`, then runs into our controller, and we get for the parameter `entity` the value `swag-blog`. We then pass this value to the parent method and prefetch it\\n* If the entity was defined with the `ce_` shorthand the API endpoints also use that shorthand, which means the route would be `\/api\/ce-{entity}`.\\n### Store api integration\\n* On the schema of the entity, the developer can define if this is `store_api_aware`.\\n* Entities which are not marked as `store_api_aware` will be removed from the response\\n* We will provide no automatic generated endpoint for the entities.\\n* Store api logics will be realized with the app-scripting epic\\n","tokens":118,"id":4501}
{"File Name":"holochain-rust\/0014-p2p-ipc-abstraction.md","Context":"## Context\\nRust as a language, in my opinion, does not lend itself to rapid prototyping. Writing efficient Rust code, while absolutely possible, is a problem domain unto itself, and takes time. E.g. Is it worth returning borrowed references in this case to save memory? Am I able to craft the lifetimes appropriately given all the other api usages of my struct? What about synchronization? These questions are implementation details, not architectural. If we are still working out the details with the underlying architecture, we don't want to be spending brainpower on * how * we accomplish our experiments.\\nIn this prototyping \/ proof-of-concept phase, I don't want to take the time to write efficient rust code, but I don't want to leave us with a bunch of unmaintainable inefficient spaghetti if we *do* decide to go forward with the prototype.\\n","Decision":"Abstract the p2p library at the process level.\\n- The P2P process will host a [ZeroMQ](http:\/\/zeromq.org\/) ROUTER socket. This process can be in any language that supports zmq, and using any of the transports it supports. (Likely start with unix domain sockets for their high throughput).\\n- The holochain rust code will connect to the P2P process with a ROUTER socket using the [zmq](https:\/\/crates.io\/crates\/zmq) crate.\\n- The holochain rust code will access this ipc abstraction through the [network-abstraction](0007-abstraction-for-network-layer.md) framework allowing the option to implement, for example, the rust version of libp2p both internally, and as a separate process.\\n","tokens":180,"id":1501}
{"File Name":"tdr-dev-documentation\/0010-file-format-identification-software.md","Context":"## Context\\nWe need software to carry out the file format checks on the files uploaded to TDR. There are two choices, [Siegfried](https:\/\/github.com\/richardlehane\/siegfried) and [Droid](https:\/\/github.com\/digital-preservation\/droid)\\n","Decision":"We did use Siegfried in the Alpha prototype because it was much easier to run it in a lambda before AWS added EFS volumes to lambda but with EFS, running Droid is almost as easy as running Siegfried\\nDroid is more difficult to parse programmatically but this is a one off cost. Once it's written, we won't have to change it too much. Droid is significantly slower but as we are running the file format checks in parallel, this isn't too big an issue. Droid reports files with multiple matches correctly which is something that's important for TDR and as it is a TNA project, we can add new features easily.\\n","tokens":61,"id":1789}
{"File Name":"pride-london-app\/0002-introduce-redux.md","Context":"## Context\\nWe were interested in pushing the project until we felt we needed redux to see how necessary it really was.\\n","Decision":"We added redux to the project relatively early\\n","tokens":25,"id":2533}
{"File Name":"tamr-client\/0003-reproducibility.md","Context":"## Context\\nReproducing results from a program is challenging when operating systems, language versions, and dependency versions can vary.\\nFor this codebase, we will focus on consistent Python versions and dependency versions.\\n","Decision":"Manage multiple Python versions via [pyenv](https:\/\/github.com\/pyenv\/pyenv).\\nManage dependencies via [poetry](https:\/\/python-poetry.org\/).\\nDefine tests via [nox](https:\/\/nox.thea.codes\/en\/stable\/).\\nRun tests in automation\/CI via [Github Actions](https:\/\/github.com\/features\/actions).\\n","tokens":43,"id":535}
{"File Name":"bosh-bootloader\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":3874}
{"File Name":"bob\/0005-introduce-unprivileged-runtime.md","Context":"## Context\\nCurrently we are using [Docker](https:\/\/www.docker.com\/) as the runtime and orchestration platform for implementing the pipeline steps.\\nGiven its maturity, ubiquitous deployment and tooling around it, it gave us great ease to use it specially via its REST API.\\nBob is what it is mainly due to the enablement Docker had.\\nHowever, it raises the following issues:\\n- Docker mainly runs as a daemon and moreover **needs root access** for the daemon function\\n- Bob is generally intended as a [Cloud Native](https:\/\/en.wikipedia.org\/wiki\/Cloud_native_computing) tool which means all components should be containerized.\\n- Given that docker needs root permissions to run, the runner needs to be privileged to function, causing a security risk to the cluster\\n- The alternative being to mount the host's docker socket into the runner container which is an even bigger security risk\\n- Docker running as a daemon in a container is not a very reliable setup and its own monitoring is still a concern\\n","Decision":"Based on the above facts the following is decided:\\n- Use [Podman](https:\/\/podman.io\/) as the container runtime and orchestration engine for the following reasons:\\n- It is rootless and daemonless\\n- Developed by [RedHat](https:\/\/www.redhat.com\/) and the [OCI](https:\/\/opencontainers.org\/) community\\n- Fully FOSS\\n- Exposes a REST API which is docker complaint too\\n- Brings in the possibilities of more things like pods and more\\n- Swap out [clj-docker-client](https:\/\/github.com\/into-docker\/clj-docker-client) in favor of [contajners](https:\/\/github.com\/lispyclouds\/contajners) as the Clojure interface to the engine\\n- Have a self contained image having the runner, the JVM and Podman and run it **unprivileged**.\\n","tokens":210,"id":4289}
{"File Name":"cloud-sdk-js\/0020-generator-options.md","Context":"## Context 1\\nWhen generating only from one spec (either generator), which is probably the most common use case, when doing this manually, the resulting folder structure is `<outputDir>\/serviceDir`, which in turn is unexpected.\\nThe name of the generator command is: `generate-X-client` indicating that only one client will be generated.\\n","Decision":"1. Different behavior if only one specification is found vs. multiple.\\nFor one specification, put client directly into `<outputDir>`, for multiple make subdirectories.\\n2. `generate-openapi-client batch -i <inputFile> -o <outputDir>`\\nGenerate only the first matched input file found by default.\\nShow a log message in case there are multiple inputs found, that batch argument is required to nest the APIs.\\n3. `generate-openapi-client -i <inputFile> -o <outputDir> --flat`\\nFlatten the directory to be `<outputDir>`.\\nThrow an error if input is a directory (with multiple files).\\n4. Keep it as is. (decided)\\n5. Rename the client:\\n1. `openapi-generator` (decided)\\n2. `sap-cloud-sdk-openapi-generator`\\n| Current Name, Aliases                                    | Future Name, Aliases             | Current Behavior                                                                                            | Future Behavior                                                                                                                                                                                                |\\n| :------------------------------------------------------- | :------------------------------- | :---------------------------------------------------------------------------------------------------------- | :------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\\n| `--inputDir`, `-i` _(required)_                          | `--input`, `-i` _(required)_     | Input directory\/file                                                                                        | _same_                                                                                                                                                                                                         |\\n| `--outputDir`, `-o` _(required)_                         | `--outputDir`, `-o` _(required)_ | Output directory                                                                                            | _same_                                                                                                                                                                                                         |\\n| `--clearOutputDir`                                       | `--clearOutputDir`               | Delete all files in output directory                                                                        | _same_                                                                                                                                                                                                         |\\n| `--generateJs`                                           | `--transpile`, `-t`              | Transpiles, default true                                                                                    | Transpiles, default false. If set if tsconfig is enabled with default (unless configured). This should be explicitly stated in the documentation.                                                              |\\n| `--generatePackageJson`                                  | `--[no]-packageJson`             | Writes a default package.json, default true.                                                                | Writes a default package.json, default true (needs core dependency). Optionally in the future: Writes a custom package.json if passed. Keep boolean for now.                                                   |\\n| `--serviceMapping`                                       | `--optionsPerService`            | Considers a per service configuration file. Shows a warning if none is given (differs from OData behavior). | Only generates a per service configuration, if a file path is provided. No warning if it is not provided. If provided, `packageName` and `directoryName` will be generated, but are not required to be set. \\* |\\n| `--tsConfig`                                             | `--tsConfig`                     | tsconfig.json file to overwrite the default \"tsconfig.json\".                                                | Writes a custom tsconfig.json if passed. Document that this should be used in combination with `transpile`.                                                                                                    |\\n| `--versionInPackageJson`                                 | `--packageVersion`               | Version in package.json, default is generator version.                                                      | Version in package.json, default is `1.0.0`. Hide it.                                                                                                                                                          |\\n| **Options in OData, but not (yet in OpenAPI) generator** |\\n| `--forceOverwrite`                                       | `--overwrite`                    | Overwrite files even it they exist, default false.                                                          | _same_                                                                                                                                                                                                         |\\n| `--generateTypedocJson`                                  | `--typedocJson`                  | Writes a default typedoc.json, default true.                                                                | Remove\/deprecate, use `files` instead.                                                                                                                                                                         |\\n| **Currently hidden options**                             |\\n| `--additionalFiles`                                      | `--include`                      | Copy additional files, identified by glob. Hidden.                                                          | _same_ Expose it.                                                                                                                                                                                              |\\n| `--writeReadme`                                          | `--readme`                       | Writes a default README.md, default false.                                                                  | _same_                                                                                                                                                                                                         |\\n| **New options**                                          |\\n| -                                                        | `--skipValidation`               | Duplicate names are renamed by default.                                                                     | Duplicate names throw an error by default. Disable validation to rename duplicates.                                                                                                                            |\\n| -                                                        | `--verbose`                      | Logs everything that happens during generation by default.                                                  | Log only success \/ error per service by default. Enable verbosity through this flag.                                                                                                                           |\\n| -                                                        | `--version`, `-v`                | -                                                                                                           | Prints the version of the generator.                                                                                                                                                                           |\\n","tokens":70,"id":3622}
{"File Name":"claim-additional-payments-for-teaching\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in\\nthis article:\\n[http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions)\\n","tokens":16,"id":2102}
{"File Name":"dpul\/0003-synchronization-via-rabbitmq.md","Context":"## Context\\nWe want Pomegranate to be a separate application from Figgy, but need some way\\nfor Figgy to tell Pomegranate about new resources so that when something is\\nmarked Complete in Figgy or taken down that it's reflected in Pomegranate.\\n","Decision":"Figgy will send create\/update\/delete messages to a fanout RabbitMQ Exchange.\\nPomegranate will register a durable queue which listens to that exchange and\\nprocess messages using [Sneakers](https:\/\/github.com\/jondot\/sneakers).\\nThe message will contain the following information:\\n* Collection slugs the object is a member of\\n* Manifest URL of the object\\n* change event (create \/ update \/ delete)\\n","tokens":60,"id":119}
{"File Name":"entusiasme-kotlin\/000000_entusiasme_application.md","Context":"### **Discussion\/Context**\\nIn all discussions of technical development communities groups appears the difficult for women to choose a technology to start learn software development.\\n___________\\n### **Decision**\\n* Google's tecnology\\n* Great company supporting and improving\\n* Good documentation\\n* Small learning curve for Java developer\\n* Easy switch to other OOP language\\n* SO Interoperability\\n* Easy to learn\\n* Sugar Syntax\\n___________\\n### **Status**\\nAccepted\\n___________\\n### **Way\/State\/Version\/Model**\\nN\/A\\n___________\\n### **Consequences**\\nN\/A\\n___________\\n### **Updates**\\n| Information | From | To | Date |\\n|---|---|---|---|\\n| | | | |\\n","Decision":"* Google's tecnology\\n* Great company supporting and improving\\n* Good documentation\\n* Small learning curve for Java developer\\n* Easy switch to other OOP language\\n* SO Interoperability\\n* Easy to learn\\n* Sugar Syntax\\n___________\\n### **Status**\\nAccepted\\n___________\\n### **Way\/State\/Version\/Model**\\nN\/A\\n___________\\n### **Consequences**\\nN\/A\\n___________\\n### **Updates**\\n| Information | From | To | Date |\\n|---|---|---|---|\\n| | | | |\\n","tokens":172,"id":4796}
{"File Name":"gatemint-sdk\/adr-002-docs-structure.md","Context":"## Context\\nThere is a need for a scalable structure of the SDK documentation. Current documentation includes a lot of non-related SDK material, is difficult to maintain and hard to follow as a user.\\nIdeally, we would have:\\n- All docs related to dev frameworks or tools live in their respective github repos (sdk repo would contain sdk docs, hub repo would contain hub docs, lotion repo would contain lotion docs, etc.)\\n- All other docs (faqs, whitepaper, high-level material about Cosmos) would live on the website.\\n","Decision":"Re-structure the `\/docs` folder of the SDK github repo as follows:\\n```\\ndocs\/\\n\u251c\u2500\u2500 README\\n\u251c\u2500\u2500 intro\/\\n\u251c\u2500\u2500 concepts\/\\n\u2502   \u251c\u2500\u2500 baseapp\\n\u2502   \u251c\u2500\u2500 types\\n\u2502   \u251c\u2500\u2500 store\\n\u2502   \u251c\u2500\u2500 server\\n\u2502   \u251c\u2500\u2500 modules\/\\n\u2502   \u2502   \u251c\u2500\u2500 keeper\\n\u2502   \u2502   \u251c\u2500\u2500 handler\\n\u2502   \u2502   \u251c\u2500\u2500 cli\\n\u2502   \u251c\u2500\u2500 gas\\n\u2502   \u2514\u2500\u2500 commands\\n\u251c\u2500\u2500 clients\/\\n\u2502   \u251c\u2500\u2500 lite\/\\n\u2502   \u251c\u2500\u2500 service-providers\\n\u251c\u2500\u2500 modules\/\\n\u251c\u2500\u2500 spec\/\\n\u251c\u2500\u2500 translations\/\\n\u2514\u2500\u2500 architecture\/\\n```\\nThe files in each sub-folders do not matter and will likely change. What matters is the sectioning:\\n- `README`: Landing page of the docs.\\n- `intro`: Introductory material. Goal is to have a short explainer of the SDK and then channel people to the resource they need. The [sdk-tutorial](https:\/\/github.com\/cosmos\/sdk-application-tutorial\/) will be highlighted, as well as the `godocs`.\\n- `concepts`: Contains high-level explanations of the abstractions of the SDK. It does not contain specific code implementation and does not need to be updated often. **It is not an API specification of the interfaces**. API spec is the `godoc`.\\n- `clients`: Contains specs and info about the various SDK clients.\\n- `spec`: Contains specs of modules, and others.\\n- `modules`: Contains links to `godocs` and the spec of the modules.\\n- `architecture`: Contains architecture-related docs like the present one.\\n- `translations`: Contains different translations of the documentation.\\nWebsite docs sidebar will only include the following sections:\\n- `README`\\n- `intro`\\n- `concepts`\\n- `clients`\\n`architecture` need not be displayed on the website.\\n","tokens":113,"id":19}
{"File Name":"js-sdk\/0015-parameterize-zos-sal-in-identity.md","Context":"## Context\\nParameterize zos sal in identity so if we want to switch identity at certain point we can for example to deploy workloads with that identity\\n","Decision":"- Make zos sal paramertized with specific identity. If not passed it will use the default identity\\n","tokens":32,"id":5200}
{"File Name":"Brighter\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":327}
{"File Name":"super-eks\/0003-use-yarn-workspaces.md","Context":"## Context\\nWe need to organize our code. We also want to run integration tests, and code examples that use the code as a 3rd party library.\\n","Decision":"We want a monorepo style setup and settle for yarn workspaces. Lerna seems to complicated at the moment.\\n","tokens":34,"id":2030}
{"File Name":"operational-data-hub\/0045-code-config-separation.md","Context":"## Context\\nBoth Code and Configuration reside in source code control (Github in our case). This makes it very easy to mix-up code and configuration. However, these 2 should be clearly separated. Where possible code can be reused, but configuration is most of the times instance specific.\\n","Decision":"Code and Configuration is clearly separated. At deployment time the CI\/CD tools are responsible for bringin code and config together and deploy the code together with the correct configuration.\\n","tokens":58,"id":2741}
{"File Name":"automate\/adr-2018-09-19.md","Context":"## Context\\nWe want to ensure that the Data resilience (this is what we are calling the guarantee that data once sent to the ingest pipeline will be ingested into A2) in our system is well understood. We also want to make sure that this is captured here for future reference.\\n","Decision":"1) Data resilience \/ durability - our current approach on leaving retries to the edges (Chef Infra Client, Chef Infra Server, Chef InSpec) is still valid and we will not change that at this time. However, there is no retry logic built into Chef Infra Server or Chef InSpec. We will add that to those products' backlog and triage according to those teams' priorities.\\n2) For Notifications right now, this is built in a way where notifications might get triggered but the data has not yet been fully ingested. We will change that behavior so that notifications only get triggered upon successful ingestion of data\\n3) There is an event service in compliance right now that we all agreed can be extracted and enhanced to use across the system. However, the right time to look at this is when there is a new use-case for it. While the approach is agreed upon, we will await a use-case before proceeding\\n","tokens":59,"id":1211}
{"File Name":"FindMeFoodTrucks\/Choice of Compute for Serving tier.md","Context":"## :dart: Context\\nAzure offers a number of ways to host your application code. The following are the considerations for choosing a compute option for the serving tier:\\n* Should support exposing Web APIs\\n* Should be able to connect to backend Cosmos DB\\n* Should support .net core framework\\n* APIs hosted on this service will need to be secured\\n* Would prefer a managed service\\n* Does not have portability requirements\\n* Should support CD from Github\\nThe following options for compute are considered for this service:\\n* App Services\\n* Functions\\n* Container Instances\\n* Service Fabric\\n* AKS\\nChoosing the right compute model will help optimize the development experience and operations\\n","Decision":"The recommended approach is to use Azure App Services considering the following points:\\n* Supports CD from Github\\n* Supports development slots for updates to reduce downtime\\n* Can be integrated with App Gateway and VNET for higher security\\n* Natively supports AD integration\\n* Supports .net core framework and is well integrated with Visual Studio development experience\\n* Can leverage cosmos SDK (nuget) to connect back to the data store\\n* Is a fully managed PaaS service\\n* No overheads related to cluster management or infrastructure provisioning.\\nThe decision is based on the guidance provided by Microsoft here: https:\/\/docs.microsoft.com\/en-us\/azure\/architecture\/guide\/technology-choices\/compute-decision-tree\\n","tokens":139,"id":1093}
{"File Name":"tendermint\/adr-068-reverse-sync.md","Context":"## Context\\nThe advent of state sync and block pruning gave rise to the opportunity for full nodes to participate in consensus without needing complete block history. This also introduced a problem with respect to evidence handling. Nodes that didn't have all the blocks within the evidence age were incapable of validating evidence, thus halting if that evidence was committed on chain.\\n[ADR 068](https:\/\/github.com\/tendermint\/tendermint\/blob\/master\/docs\/architecture\/adr-068-reverse-sync.md) was published in response to this problem and modified the spec to add a minimum block history invariant. This predominantly sought to extend state sync so that it was capable of fetching and storing the `Header`, `Commit` and `ValidatorSet` (essentially a `LightBlock`) of the last `n` heights, where `n` was calculated based from the evidence age.\\nThis ADR sets out to describe the design of this state sync extension as well as modifications to the light client provider and the merging of tm store.\\n","Decision":"The state sync reactor will be extended by introducing 2 new P2P messages (and a new channel).\\n```protobuf\\nmessage LightBlockRequest {\\nuint64 height = 1;\\n}\\nmessage LightBlockResponse {\\ntendermint.types.LightBlock light_block = 1;\\n}\\n```\\nThis will be used by the \"reverse sync\" protocol that will fetch, verify and store prior light blocks such that the node can safely participate in consensus.\\nFurthermore this allows for a new light client provider which offers the ability for the `StateProvider` to use the underlying P2P stack instead of RPC.\\n","tokens":206,"id":1965}
{"File Name":"fundraising-application\/014_Object_Construction_Improvements.md","Context":"## Context and Problem Statement\\nCreating a central factory has served us well, but has drawbacks, as\\noutlined in [ADR 013](013_Main_Factory.md). This document is about\\nexploring ways to mitigate those drawbacks, while keeping the benefits of\\ntype-safety and an architecture based on the SOLID principles.\\n## Decision Drivers\\n* FunFunFactory is a long class with too many extension\\npoints for tests and exposing too many instances.\\n* Ideally, the class should not have any branching logic. Without\\nbranching logic, the only test we need for this class is that it\\nproduces the right instances. We do this with our edge-to-edge\\n(integration) tests.\\n* We will switch our web framework layer from the discontinued Silex\\nframework to Symfony. Symfony comes with a powerful [Dependency\\nInjection Container\\n(DIC)](https:\/\/symfony.com\/doc\/current\/components\/dependency_injection.html)\\nthat has the same purpose as our factory.\\n","Decision":"* FunFunFactory is a long class with too many extension\\npoints for tests and exposing too many instances.\\n* Ideally, the class should not have any branching logic. Without\\nbranching logic, the only test we need for this class is that it\\nproduces the right instances. We do this with our edge-to-edge\\n(integration) tests.\\n* We will switch our web framework layer from the discontinued Silex\\nframework to Symfony. Symfony comes with a powerful [Dependency\\nInjection Container\\n(DIC)](https:\/\/symfony.com\/doc\/current\/components\/dependency_injection.html)\\nthat has the same purpose as our factory.\\nThe presented options are don't exclude each other, they are  the start\\nand end state of a refactoring. In reality, we will start with injecting\\nthe FunFunFactory (without refactoring it) into the controllers. The next\\nstep is to swap the factory and inject the services instead, defining the\\nFunFunFactory as the service creating factory in the service configuration\\nfile. Over time, we will define more and more services in the service\\nconfiguration, extracting them from FunFunFactory, until we can delete it.\\n","tokens":205,"id":1534}
{"File Name":"CopperBend\/D002_unify_command_flow_for_player_and_creatures.md","Context":"## Context\\nThe 'core mechanic' of this game is the player being in control of allied plants.  There may be other cases of player control of other in-game entities as dev continues, but this is core.\\nGiving creatures and the player character pluggable command\/control is more dev overhead.  As a spare time project, any choice for extra complexity deserves extra scrutiny.  On the same wave, though, the outcomes which can justify the extra effort are a larger set:  Not just important functionality, but significant learning and straight pleasure in craftsmanship can be enough reason.\\nThis offers options for reuse.\\nIf our guy is terrified, the player's InputCommandSource can be temporarily replaced with a FleeingCommandSource (or some such).  When the player is in charge of other entities, they get an InputCommandSource themselves, and will naturally be controlled by the player when their moments to act arrive in the schedule.\\n","Decision":"I'll do this.  It feels correct, and it trends toward the sort of wins and frustrations I'm hoping for in this project.  That is, where my decisions, rather than the guts of frameworks, are what I'm wrestling most often.\\n","tokens":191,"id":95}
{"File Name":"SearchServices\/0004-community-mirror.md","Context":"## Context\\nIn [ADR 3: \"Combined Codebase\"](0003-combined-codebase.md) we decided to merge the Search Services and Insight Engine\\nrepositories.  Since we want to enable the community to submit pull requests to the Search Services project we need a\\nway to keep this code up to date on GitHub.\\n","Decision":"We will mirror `master` and all branches starting with `release\/` to a branch with the same name on GitHub.  We will\\nexclude the alfresco-insight-engine-parent directory. We will include these commands as part of our build to do this:\\n```\\n# This avoids making changes to the original branch.\\nget checkout -b tempBranch\\n# This strips all enterprise changes (in a reproducible way) and pushes any updates to the mirror.\\ngit filter-branch -f --prune-empty --index-filter 'git rm -r --cached --ignore-unmatch alfresco-insight-engine-parent'\\ngit push out HEAD:$branch\\n# This resets us back to where we were before the filtering.\\ngit checkout $branch\\n```\\n","tokens":69,"id":5130}
{"File Name":"time-diagnostics\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":1140}
{"File Name":"nebula\/0010-use-nginx-for-simple-https-proxies.md","Context":"Context\\n-------\\nWe currently use Apache on all our web hosts. While writing a profile\\nfor forwarding HTTPS to a local HTTP port, I found that the puppet\\nApache module didn't provide a simple approach to this. I would have had\\nto use `mod_proxy` and then add custom fragments.\\nOn the other hand, if I went with nginx instead of Apache, such a proxy\\ncould be configured with a single line:\\nproxy => \"http:\/\/localhost:${port}\",\\nThis carries with it a cost however. If we use nginx in this one case,\\nthen we can no longer claim to use a single web server, as we'll be\\nusing Apache on some servers and nginx on others.\\nDecision\\n--------\\nWe will use nginx for simple cases where all we need is to forward to a\\nlocal port.\\nConsequences\\n------------\\nForwarding https traffic to a local port is very easy. We now use both\\nnginx and Apache.\\n","Decision":"--------\\nWe will use nginx for simple cases where all we need is to forward to a\\nlocal port.\\nConsequences\\n------------\\nForwarding https traffic to a local port is very easy. We now use both\\nnginx and Apache.\\n","tokens":205,"id":4941}
{"File Name":"buildit-all\/0002-use-aws-bare-metal-rig-approach.md","Context":"## Context\\nWe need to create a riglet for our new twig project so that we practice what we preach.\\n","Decision":"We will use the AWS Bare Metal Riglet from bookit-infrastructure as a starting point for our riglet.  We will keep the previous twig-riglet and create a new twig-infrastructure project\/repo.\\nTechnologies:\\n* AWS: CloudFormation, ECR, ECS, Route53, VPC, ALB\\n* Deployment Mechanism: Docker images\\n* Build: CodePipeline, with Jenkins as an eventual target\\n","tokens":24,"id":4417}
{"File Name":"operational-data-hub\/0001-record-coding-guidelines.md","Context":"## Context\\nWe need to record the coding standard decisions made on the ODH platform.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":19,"id":2747}
{"File Name":"ftd-scratch3-offline\/0003-use-gradle-as-build-system.md","Context":"## Context\\nModern software needs to be easy to build.\\nFor this reason most software uses a build system to specify how the software is build and which dependencies it needs to work.\\nPopular build systems in the Java world are Ant, Maven and Gradle.\\nAnt is pretty flexible but lacks dependency management and is also very rarely used these days.\\nMaven is more rigid than Ant but supports dependency management and is still widely used.\\nGradle is the newest build system. It can be programmed in Groovy\/Kotlin and is the most flexible build system.\\nGradle is also the only way to build for the Android platform.\\nGradle enjoys widespread usage and the authors are most proficient in it.\\n","Decision":"We will use Gradle as build system.\\n","tokens":146,"id":2633}
{"File Name":"james\/0024-polyglot-strategy.md","Context":"## Context & Problem Statement\\nJames is written in Java for a very long time. In recent years, Java modernized a lot after a decade of slow progress.\\nHowever, in the meantime, most software relying on the JVM started supporting alternative JVM languages to keep being relevant.\\nIt includes Groovy, Clojure, Scala and more recently Kotlin, to name a few.\\nNot being open to those alternative languages can be a problem for James adoption.\\n","Decision":"Nowadays, libraries and framework targeting the JVM are expected to support usage of one or several of these alternative languages.\\nJames being not only a mail server but also a development framework needs to reach those expectations.\\nAt the same time, more and more developers and languages adopt Function Programming (FP) idioms to solve their problems.\\nWe decide for options 4, 5 and IV.\\nThat means we need to write some mailets in Scala and demonstrate how it's done and then used in a running server.\\nIt also means writing and\/or refactoring some server components in Scala, starting where it's the most relevant.\\n###\u00a0Positive Consequences\\n* Modernize parts of James code\\n* Leverage Scala richer FP ecosystem and language to overcome Java limitations on that topic\\n* Should attract people that would not like Java\\n###\u00a0Negative Consequences\\n* Adds even more knowledge requirements to contribute to James\\n* Scala build time is longer than Java build time\\n","tokens":93,"id":2124}
{"File Name":"verify-service-provider\/0007-we-will-document-a-strawman-api.md","Context":"## Context\\nThe client and the service provider will have to communicate using some API.\\nWe need to decide how the requests and responses will look like.\\n","Decision":"We will use swagger to document the API between the client and the service-provider. This will form part of the documentation of a strawman that we send to our users.\\n","tokens":32,"id":4439}
{"File Name":"jabref\/0006-only-translated-strings-in-language-file.md","Context":"## Context and Problem Statement\\nJabRef has translation files `JabRef_it.properties`, ... There are translated and untranslated strings. Which ones should be in the translation file?\\n## Decision Drivers\\n* Translators should find new strings to translate easily\\n* New strings to translate should be written into `JabRef_en.properties` to enable translation by the translators\\n* Crowdin should be kept as translation platform, because 1\\) it is much easier for the translators than the GitHub workflow and 2\\) it is free for OSS projects.\\n","Decision":"* Translators should find new strings to translate easily\\n* New strings to translate should be written into `JabRef_en.properties` to enable translation by the translators\\n* Crowdin should be kept as translation platform, because 1\\) it is much easier for the translators than the GitHub workflow and 2\\) it is free for OSS projects.\\nChosen option: \"Only translated strings in language file\", because comes out best \\(see below.\\n","tokens":113,"id":4752}
{"File Name":"buy-for-your-school\/0015-store-user-activity-in-app.md","Context":"## Context\\n- We need to keep a record of activities taken by users so that we can gather quantitative research data\\n- We need to avoid third-party tracking services, since these carry with them privacy concerns, lack of clarity around retention rules, additional user consent, and the need for additional approvals\\n","Decision":"Store all records of activities taken by users in-app.\\n","tokens":60,"id":1257}
{"File Name":"openlobby-server\/0013-black-code-formatter.md","Context":"## Context\\nWe would like to simplify code reviews and unify code style.\\n","Decision":"Use Black code formatter: https:\/\/github.com\/ambv\/black\\n","tokens":16,"id":471}
{"File Name":"community\/0002-continuously-delivery-travis-ci.md","Context":"## [Context](https:\/\/github.com\/libero\/community\/issues\/13)\\nLibero needs automated and human feedback over pull requests and release candidates.\\n","Decision":"We will provide Travis CI builds for all repositories, covering both testing and deployment to a demo environment.\\n","tokens":31,"id":3459}
{"File Name":"molgenis-r-armadillo\/0003-use-s3-api.md","Context":"## Context\\n* Minio already has a UI and an S3 API that allow administration of files and buckets.\\n* There are existing client libraries for the S3 API.\\n* It is nontrivial to proxy large file uploads through the armadillo server.\\n","Decision":"The Armadillo client will be written as a high-level library on top of an existing S3 API.\\n","tokens":56,"id":547}
{"File Name":"ios-architecture-decision-logs\/0015-use-UITestablePageProtocol-for-AccessibilityIdentifiers.md","Context":"## Context\\nOur team created new UITestablePage Interface for setting accessibility Identifiers. For consistency we should replace and use UITestablePage.\\n","Decision":"Every View Controller should conform UITestablePage.\\n","tokens":31,"id":4981}
{"File Name":"smarthub\/0004-use-openzeppelin.md","Context":"## Context\\nSmarthub SDK contracts are designed to be upgradable by abstracting proxy, logic and storage to separate contracts. This approach leads to maintaining 3 separate Solidity files per contract.\\n","Decision":"Use OpenZeppelin implementation based on generalized proxy, logic and storage to remove the need of keeping 3 separate custom implemented contracts.\\n","tokens":42,"id":3686}
{"File Name":"SAP-Cloud\/sidecar-container.md","Context":"## Context\\nSome projects need downstream systems, e.g. a database, for their Integration-tests.\\nTo enable tests with a downstream system in a containerized Jenkins environment it is required to spin up sidecar-containers on demand.\\nJenkins supports sidecar-containers with help of the [docker-workflow-plugin](https:\/\/github.com\/jenkinsci\/docker-workflow-plugin). There is also an official [example](https:\/\/jenkins.io\/doc\/book\/pipeline\/docker\/) how to use sidecar-containers in Jenkins.\\nUnfortunately with the current implementation of the plugin it is not possible to connect the sidecar-container to multiple docker networks.\\nAn [issue](https:\/\/issues.jenkins-ci.org\/browse\/JENKINS-56561) has already been created in Jenkins JIRA.\\n","Decision":"To provide the consumer of the pipeline the possibility to use sidecar-containers in their pipelines (in docker as well as kubernetes environments) we decided to use the already available functionality of the `dockerExecute` step provided by [jenkins-library](https:\/\/github.com\/SAP\/jenkins-library).\\n","tokens":161,"id":2925}
{"File Name":"govuk-aws\/0003-aws-networking-outline.md","Context":"## Context\\nAs part of the migration to AWS we need to define our IP addressing strategy and assign ranges to different environments. We will attempt to keep the new IP layout as close as possible to the current one in order to ease migration. This is not completely possible due to Amazon AZs necessarily being on different subnets within the same VPC range.\\nThis record does not currently cover the disaster recovery environments and treats CI as an embedded part of integration.\\n","Decision":"* 1 VPC per environment (currently integration, staging and production)\\n* 3 public subnets, spread across availability zones\\n* 3 private subnets, spread across availability zones\\n**These IP ranges have been superseded by [ADR #0033](0033-ip-ranges.md).**\\nThe VPCs will be assigned the following IP ranges:\\n|Environment|IP Range|\\n|-----------|--------|\\n|Integration|10.1.0.0\/16|\\n|Staging|10.2.0.0\/16|\\n|Production|10.3.0.0\/16|\\n|Test|10.200.0.0\/16|\\nEach AZ shall be a `\/24` within the above ranges.\\neg. Integration - AZ1: 10.1.1.0\/24, AZ2: 10.1.2.0\/24, AZ3: 10.1.3.0\/24\\nWe will be deliberately flattening the current VDC separation and placing all the hosts in a private subnet per availability zone and using security groups to maintain our current network isolation.\\n","tokens":93,"id":4035}
{"File Name":"HES_pipeline\/hardcoding_variables.md","Context":"## Context\\nSeveral parts of the pipeline execute operations on specific columns.\\n","Decision":"Where there was an easy way to do so, we gave users the options to supply variable names:\\n* checking whether all expected columns are present\\n* coercing data types.\\nIn other sections of the pipeline, variable names had to be hardcoded:\\n* cleaning variables (replacing missing values with NA)\\n* deriving variables\\n* deriving row quality and flagging duplicates (optional)\\n","tokens":15,"id":3256}
{"File Name":"adr-tools\/0006-packaging-and-distribution-in-other-version-control-repositories.md","Context":"## Context\\nUsers want to install adr-tools with their preferred package\\nmanager.  For example, Ubuntu users use `apt`, RedHat users use\\n`yum` and Mac OS X users use [Homebrew](http:\/\/brew.sh).\\nThe developers of `adr-tools` don't know how, nor have permissions,\\nto use all these packaging and distribution systems. Therefore packaging\\nand distribution must be done by \"downstream\" parties.\\nThe developers of the tool should not favour any one particular\\npackaging and distribution solution.\\n","Decision":"The `adr-tools` project will not contain any packaging or\\ndistribution scripts and config.\\nPackaging and distribution will be managed by other projects in\\nseparate version control repositories.\\n","tokens":109,"id":3563}
{"File Name":"beis-report-official-development-assistance\/0032-remove-unused-skylight-performance-monitoring-app.md","Context":"## Context\\nWe've had the Skylight profiler app installed since May 2020. However, it's not\\nbeen used for a long time and in fact has been misconfigured for as long as any\\none can remember. Should we invest further, at this point, in a service which is\\nnot being used, or should we remove it for now?\\n","Decision":"We opt to remove the Skylight service, for the following reasons:\\n- using the git history we can readily re-instate it\\n- it's not currently used and is an overhead to maintain\\n- it's currently misconfigured\\n- we are currently moving out of our development phase into an\\noperational\/support phase and are transferring ownership of all third party\\nservices to BEIS\\n","tokens":75,"id":2385}
{"File Name":"meadow\/0023-uuids.md","Context":"## Context\\nMeadow's predecessor, DONUT, uses straight UUIDs (not ULIDs) as primary identifiers.\\nSince DONUT's data will be migrated to Meadow, maintaining backward compatibility is\\nmore important than the improved aesthetics or lexical sorting of ULIDs.\\n","Decision":"Remove ULID identifiers in favor of UUIDs.\\n","tokens":56,"id":3826}
{"File Name":"dotcom-rendering\/012-amp.md","Context":"## Context\\nWe have an established aim of rendering content in Dotcom Rendering. This will dramatically improve the developer experience (measured in time to new features, time on maintenance).\\nAMP is a good candidate for early migration to the new platform:\\n- It is simpler than \u2018full\u2019 content types\\n- It is a discrete piece of functionality\\n- It receives a large volume of traffic, which will help validate the performance of our new platform\\n- It receives relatively little development outside of the Dotcom team, reducing risk during the migration period\\nThere are also some downsides. In particular:\\n- It is relatively \u2018static\u2019 (rarely developed on) so the developer experience benefits are likely less than for some non-AMP content, such as articles or liveblogs\\nIt is quite possible though that Google will weight AMP more heavily going forward, pushing us to develop the platform more than we have done so in the past.\\n","Decision":"To implement AMP using dotcom rendering.\\n","tokens":191,"id":2656}
{"File Name":"arch\/0027-message-queue.md","Context":"## Context\\n1. \u6211\u4eec\u7684\u5f88\u591a\u4e1a\u52a1\u90fd\u6709\u8fd9\u4e2a\u9700\u8981\uff0c\u5f53\u524d\u7684\u4e00\u4e9b\u5b9a\u65f6\u4efb\u52a1\u5df2\u5bfc\u81f4\u673a\u5668\u7684\u4ea7\u751f\u74f6\u9888\uff0c\u5f88\u591a\u4e1a\u52a1\u4e4b\u95f4\u7684\u8026\u5408\u6027\u4e5f\u5f88\u9ad8\uff1b\\n2. \u5f53\u524d\u53ea\u6709\u5c11\u91cf\u7684\u4e1a\u52a1\u5728\u4f7f\u7528\u6d88\u606f\u961f\u5217\u670d\u52a1\uff08activeMQ\uff09\uff1b\\n3. activeMQ \u5bf9\u975e java \u8bed\u8a00\u7684\u652f\u6301\u5e76\u4e0d\u53cb\u597d\uff1b\\n4. \u81ea\u5df1\u9700\u8981\u7ef4\u62a4\u961f\u5217\u670d\u52a1\uff0c\u505a\u76d1\u63a7\uff0c\u9ad8\u53ef\u7528\u7b49\u3002\\n","Decision":"\u6211\u4eec\u6025\u9700\u4e00\u4e2a\u591a\u8bed\u8a00\u652f\u6301\u4e14\u53ef\u9760\u7684\u670d\u52a1\u53ef\u4ee5\u89e3\u51b3\u5982\u4e0a\u95ee\u9898\uff0c\u5373\\n* \u5f02\u6b65\u89e3\u8026\\n* \u524a\u5cf0\u586b\u8c37\\nAliyun \u7684 MNS \u548c ONS \u529f\u80fd\u7684\u91cd\u5408\u5bfc\u81f4\u9009\u62e9\u8d77\u6765\u5f88\u56f0\u96be\uff0c\u6700\u7ec8\u9009\u62e9 MNS \u6e90\u4e8e\u4ee5\u4e0b\u51e0\u70b9\u3002\\n1. \u652f\u6301\u6d88\u606f\u4f18\u5148\u7ea7\uff1b\\n2. \u53ef\u9760\u6027\u66f4\u9ad8\uff1b\\n3. \u6587\u6863\u66f4\u5b8c\u5584\uff1b\\n4. \u670d\u52a1\u66f4\u6210\u719f\uff1b\\n5. \u4f7f\u7528\u8d77\u6765\u7b80\u5355\u65b9\u4fbf\uff0c\u5b9a\u65f6\u6d88\u606f\u4e0d\u652f\u6301\u8fd9\u70b9\u6bd4\u8f83\u9057\u61be\uff1b\\n6. ONS \u4f7f\u7528\u8d77\u6765\u6709\u4e9b\u590d\u6742\uff0c\u76d1\u63a7\u65b9\u9762\u7684\u529f\u80fd\u786e\u5b9e\u5f3a\u5927\u8bb8\u591a\uff0c\u4f46\u591a\u6570\u5728\u5f00\u53d1\u4e0e\u516c\u6d4b\u4e2d\uff1b\\n7. \u4e0d\u597d\u7684\u70b9\u662f\u5404\u81ea\u90fd\u52a0\u4e86\u4e00\u4e9b\u79c1\u6d3b\uff0cMNS \u52a0\u4e86\u81ea\u5bb6\u7684\u77ed\u4fe1\u670d\u52a1\uff0cONS \u52a0\u5165\u4e86 MQTT \u7269\u8054\u7f51\u5957\u4ef6\uff0c\u8ba9 Q \u770b\u8d77\u6765\u4e0d\u5355\u7eaf\u3002\\n","tokens":131,"id":2443}
{"File Name":"archcolider\/002 System approach.md","Context":"## Context\\nWe need to decide what the dominating system approach will be. There are several options like: monolith, microservices, micro-kernels, blackboard.\\nThere are also technical constraints and business drivers even if the sky is blue and we are free of choice, developer's team experience and capabilities should be considered.\\n### Alternatives\\nKey map:\\n- + Promotes\\n- ++ Strongly promotes\\n- O Neutral\\n- - Negative\\n- -- Strongly negative\\n| | Monolith | Microservices | Micro-kernel | Modularized Monolith |\\n|----|----|----|-----|-----|\\n| Ease of Deployment  | ++ | -  | -  | ++ |\\n| Availability        | -  | ++ | +  | - |\\n| Autonomy          | -- | ++ | ++ | + |\\n| Traceability        | ++ | -  | +  | O |\\n| Performance         | ++ | +  | +  | ++ |\\n| Modifiability        | O  | ++ | +  | + |\\n| Maintainability      | -  | ++ | +  | + |\\n| Integrity           | ++ | -- | O  | + |\\n| Security            | -  | ++ | +  | ++ |\\n| Scalability         | -- | ++ | -  | + |\\n","Decision":"Based on alternatives in the context of the business needs and a development team we think that modularized monolith is the best option for now. At the same time it opens the possibility to migrate to microservices when needed.\\n","tokens":284,"id":2674}
{"File Name":"dos-server\/adr-8-dome-ingest.md","Context":"## Context\\nSome of the target collections are already present in Dome, and it is simpler\\nto use Dome as the ingest source than upstream systems. Further, as Dome supports\\nOAI-PMH and OAI-ORE, these interfaces could be used to retrieve\\nrelevant information, such as the bitstreams of digital assets, directly from Dome.\\nTo accomplish this, a custom ingest script or a third-party tool could be used for this purpose.\\nSuch a tool is available for TIMDEX related ingests [link]() and could be modified for ingesting\\ndigital assets to DOS.\\n","Decision":"Ingest will be handled through the existing OAI-PMH\/ORE tool in conjunction with scripts written to prepare that output for the DOS API.\\n","tokens":123,"id":3380}
{"File Name":"titania-os\/0008-dual-root-with-data-volume.md","Context":"## Context\\nEmbedded system need a mechanism to keep the software up to date. We want updates to be atomary for the whole system as opposed to per-package approach to ease support. We want the user to have an option to re-boot in a stable version of the system in case the system update was unsuccessful either due to client side errors, miscomunication or malfunctioning release. Some configuration files must persist through updates. We want the root filesystem itself immutable so it can be checksummed and to cover some security concerns. We can achieve that by either point-mounting some files from a read-write partition or set up a read write overlay that will automatically hold all the modification without copying the underlying read-only system.\\n","Decision":"We will set up 2 root partitions for the system one of which will be dormant and other will be active. Updates will rewrite the dormant partition and flip which one is used on next reboot. We will lock root partitions read only and bind-mount read-write configuration files over them. We will set up a read-write \"data\" volume\/partition and expand it to the end of the disk for modifiable configuration and other data files.\\n","tokens":144,"id":2571}
{"File Name":"platform\/2021-05-14-when-to-use-plain-sql-or-dal.md","Context":"## Context\\nIt is often discussed whether to work with plain SQL or with the Data Abstraction Layer.\\n","Decision":"In the following application layers, the DAL should be used for the following reasons:\\n* In the Store API\\n* Data selected and returned via Store API must be extensible by third party developers.\\n* Requests against the Store API should always allow additional data to be loaded.\\n* Data retrieval and encoding must be secured by ACL.\\n* Storefront page loader and controller\\n* Data passed from the storefront to the Twig templates must be extensible by third party developers.\\n* Since our templates are customized by many developers, we cannot provide only a minimal offset of the actual data.\\n* On admin API level\\n* Data selected and returned via admin API must be extensible by third party developers.\\n* Requests that go against the Store API should always allow additional data to be loaded.\\n* Data retrieval and encoding must be secured by ACL.\\n* When writing data\\n* The DAL has a validation, event and indexing system which is used for the write process. Therefore, it is mandatory to ensure data integrity, that write processes take place exclusively via the DAL.\\n* The entity indexers are an exception here, see below.\\nIn the following application layers you should work with plain SQL because of the following reasons:\\n* In the entity indexers\\n* The entity indexers are located behind the entity repository layer, so it only makes sense that they do not work with the repositories but with the database connection directly.\\n* the entity indexers must be able to re-index all data after a versions update. To avoid as much hydration and event overhead as possible, they should work directly with the connection.\\n* The entity indexers are not an extension point of shopware. The queries that are executed there are only used for internal processing of data and should never be rewritten.\\n* In Core Components\\n* Core components like the theme compiler, request transformer, etc. are not places where a third party developer should be able to load additional data. The data loaded here is for pure processing only and should never be rewritten.\\n* Deep processes like theme compiling should not be affected by plugin entity schemas, because plugins are an optional part of the system and might be in an unstable state during an update process.\\n","tokens":22,"id":4511}
{"File Name":"connaisseur\/ADR-6_dynamic-config.md","Context":"## Context\\nThe configuration of validators are mounted into Connaisseur as a configmap, as it is common practice in the Kubernetes ecosystem. When this configmap is upgraded, say with a `helm upgrade`, the resource itself in Kubernetes is updated accordingly, but that doesn't mean it's automatically updated inside the pods which mounted it. That only occurs once the pods are restarted and until they are the pods still have an old version of the configuration lingering around. This is a fairly unintuitive behavior and the reason why Connaisseur doesn't mount the image policy into its pods. Instead, the pods have access to the kube API and get the image policy dynamically from there. The same could be done for the validator configuration, but there is also another solution.\\n## Problem 1 - Access to configuration\\nHow should Connaisseur get access to its configuration files?\\n### Solution 1.1 - Dynamic access\\nThis is the same solution as currently employed for the image policy configuration. The validators will get their own CustomResourceDefinition and Connaisseur gets access to this resource via RBAC so it can use the kube API to read the configuration.\\n**Pros:** Pods don't need to be restarted and the configuration can be changed \"on the fly\", without using Helm.\\n**Cons:** Not a very Kubernetes native approach and Connaisseur must always do some network requests to access its config.\\n### Solution 1.2 - Restart pods\\nThe other solution would be to use ConfigMaps for validators and image policy and then restart the pods, once there were changes in the configurations. This can be achieved by setting the hash of the config files as annotations into the deployment. If there are changes in the configuration, the hash will change and thus a new deployment will be rolled out as it has a new annotation. This corresponds to the [suggestion](https:\/\/helm.sh\/docs\/howto\/charts_tips_and_tricks\/#automatically-roll-deployments) made by Helm.\\n**Pros:** Kubernetes native and no more CustomResourceDefinitions!\\n**Cons:** No more \"on the fly\" changes.\\n### Decision Outcome (1)\\nSolution 1.2 was chosen, going with the more Kubernetes native way.\\n","Decision":"Solution 1.2 was chosen, going with the more Kubernetes native way.\\nSolution 2.2 was chosen as it is the more simpler of the two.\\n","tokens":458,"id":4398}
{"File Name":"external-service-operator\/0003-the-name-of-the-endpoints-service-and-ingress-gets-inherited-by-the-controlling-externalservice.md","Context":"## Context\\nTo easify finding the according Endpoints, Services and Ingress Ressources, they are named exactly the same as the Externalservice Ressource.\\nNethertheless, of course the Owner will be set correctly as well as every ressource gets the label:\\n```\\napp= <external-servicename>\\n```\\n","Decision":"The change that we're proposing or have agreed to implement.\\n","tokens":76,"id":1733}
{"File Name":"banner-server\/001-test-names.md","Context":"## Context\\nWhen deciding on a naming scheme for unit test methods, the following criteria were most important to us:\\n* **Readability** - The test method names should read like English sentences, with clear word boundaries\\n* **Flexibility** - We should be able to follow different patterns, like \"Given ... When .. Then\", but also like \"Does X\".\\n","Decision":"We use `snake_case` for method names in unit tests. We adapt the coding style settings to ignore the deviation from our usual `camelCase` convention.\\nIf it makes sense, we use sentences containing the words `given`, `when` and `then`. To give each section a clear boundary, when we use `given` or `when`, then we also use `then`.\\nGood:\\ntest_given_first_time_visitor_then_return_main_banner\\nBad:\\ntest_given_first_time_visitor_return_main_banner\\nWe keep in mind that the sentences always refer to the system-under-test (SUT) and don't unnecessarily repeat its class name.\\n","tokens":77,"id":852}
{"File Name":"mymove\/0067-ppm-db-design.md","Context":"## Decision Drivers\\n* Time and complexity\\n* Do we have bandwidth to make these changes?\\n* Do we have enough capacity handle the unknowns or deal with future issues this decision may cause?\\n* Do we need more information?\\n* Do we have enough future capacity to delay this?\\n* Flexibility\\n* How likely is this solution to support new shipment types or even existing shipments?\\n* Consistency\\n* How consistent are our design patterns?\\n* Are they intuitive?\\n","Decision":"* Time and complexity\\n* Do we have bandwidth to make these changes?\\n* Do we have enough capacity handle the unknowns or deal with future issues this decision may cause?\\n* Do we need more information?\\n* Do we have enough future capacity to delay this?\\n* Flexibility\\n* How likely is this solution to support new shipment types or even existing shipments?\\n* Consistency\\n* How consistent are our design patterns?\\n* Are they intuitive?\\n* Chosen Alternative: [Create a new table](#create-a-new-table)\\n* Positive Outcomes: We will have a pattern that is easier to understand and will potentially be helpful in the future when implementing new shipments. The tentative plan is to test this new pattern out with PPMs, and if things go smoothly, revisit this to be the default DB pattern for all new shipment types moving forward.\\n* Consequences: If this new pattern is not utilized elsewhere we may be adding more complexity by introducing another pattern that is only partially used.\\n","tokens":101,"id":3077}
{"File Name":"TANF-app\/007-object-storage.md","Context":"## Context\\nFor TDP, we need the ability to store raw, unparsed data files from TANF grantees in the states, territories, and tribes. Additionally, we need a storage system for static HTML\/CSS and files used to (re)create services infrastructure. Amazon S3 provides secure, durable, highly-scalable object storage. We will be able to store application content in S3 using Cloud.gov's secure and compliant  managed [service](https:\/\/cloud.gov\/docs\/services\/s3\/) that provides direct access to S3.\\n","Decision":"We will use cloud.gov managed S3 for object storage.\\n","tokens":112,"id":3787}
{"File Name":"datalab\/0019-react-js-for-front-end-ui.md","Context":"## Context\\nWe need to decide which front-end web framework to use for the Datalabs application.\\nThe choices we evaluated were [React.js](https:\/\/reactjs.org\/) and\\n[Angular](https:\/\/angular.io\/).\\n","Decision":"We have decided to use the React framework for the following reasons:\\n* Preferred the \"batteries not included\" approach of React vs the \"batteries\\nincluded\" approach of Angular.\\n* Better development tooling available for React.\\n* Larger community for React.\\n* The team had previous experience with React.\\n","tokens":48,"id":728}
{"File Name":"dotcom-rendering\/008-css-in-js.md","Context":"## Context\\nThere are a number of ways in which we could apply styles to components.\\n### CSS-in-JS\\nCSS-in-JS is a catch-all terms for defining CSS directly in JavaScript. It allows us to defining the styles in the same place as the markup (JSX) and component logic.\\nStyles may be defined as JavaScript objects, with some syntactical overhead (camel case rules instead of kebab case, pseudo-selectors and media queries are wrapped in quotes, ...). Alternatively, they may be defined as tagged template literals, which look much like CSS but require some additional tooling in developer IDEs (i.e. plugins for syntax highlighting).\\nCSS-in-JS is supported by a number of different libraries such as [Emotion](https:\/\/emotion.sh\/), [Styled Components](https:\/\/www.styled-components.com\/docs\/basics#getting-started) (both with TTL support), [Glamor](https:\/\/github.com\/threepointone\/glamor) and [Styletron](https:\/\/github.com\/styletron\/styletron) (no TTL support).\\n### [CSS Blocks](https:\/\/css-blocks.com\/)\\nCSS Blocks allows developers to define their CSS in `*.css` files and import them into JavaScript. It makes use of familiar CSS syntax, with clever use of attribute selector notation and the [`obj-str`](https:\/\/github.com\/lukeed\/obj-str) library to change styles based on props. The output is highly-efficient atomic CSS. The use of `obj-str` may make a steep learning curve for non-client-side-developers.\\n### CSS in `*.css` files imported with custom Webpack CSS loader\\nIt is possible to write a Webpack loader that will load the contents of `*.css` files and manipulate the content to make it usable with CSS-in-JS libraries. This would lock us in to Webpack and may become considerably complex over time.\\n### [Stylable](https:\/\/stylable.io\/)\\nSimilar to CSS Blocks, Styleable allows developers to write `*.css` files that look and feel like CSS, with added syntax that adheres \"to the spirit of CSS\". It provides access to state using custom pseudo-classes and pseudo-elements and uses a pre-processor to convert files down to vanilla CSS.\\n","Decision":"Use CSS-in-JS to represent styles.\\n","tokens":473,"id":2661}
{"File Name":"document-evidence-store-frontend\/0002-switch-from-client-side-api-requests-to-server-side.md","Context":"## Context\\nWhen designing this application, we decided to make all requests from the client side (using the `useEffect` hook) and we created a proxy middleware which authenticated the API requests and added the API tokens before forwarding the request to the Evidence API.\\nThis allowed us to move quickly, but it had two significant impacts:\\n1. a UX impact of showing a lot of _\"loading\"_ messages while client side requests were made\\n2. an architectural impact of not allowing us to make tamper proof API requests.\\nAn example of point 2:\\nOne endpoint of our API requires a filter parameter of a \"service\" (roughly linked to a google group). It's important that users are not able to filter by services they are not members of.\\nWe can extract the users' services from their google token, but with the proxy middleware we were not able to attach these filter parameters on the server side, and therefore could allow a user to request any service from their browser.\\n","Decision":"In order to prevent the above impacts, we can remove the proxy middleware and make as many API requests as possible from the server side, and take advantage of Next.js' `getServerSideProps` to pass the fetched data to the component.\\n","tokens":204,"id":2595}
{"File Name":"eq-author-app\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":2114}
{"File Name":"libelektra\/key_string_return_value.md","Context":"## Problem\\nWhen using keyString() on empty \/ binary values the return values are the literal strings (null) \/ (binary). This seems very awkward and unintuitive from a user's perspective.\\n","Decision":"- `key == NULL` return 0, error code via second channel\\n- `key->value == NULL` return 0, error code via second channel\\n- `key == <binary>` return 0, error code via second channel\\n- everything else as is\\n","tokens":41,"id":1295}
{"File Name":"embvm-core\/0022-unified-gpio-base-class.md","Context":"## Context\\nThe issue motivating this decision, and any context that influences or constrains the decision.\\n","Decision":"To simplify the implementation for drivers, we will create a \"unified\" GPIO base class, which provides the standard interfaces that can be used to set the mode (input, output, special function). This will provide a single class to implement for GPIO support, rather than one class for each mode.\\nIdeally, we will find a way to zero-overhead create wrapper classes that constrain the interface appropriately, so we can mark pins as `GPIOInput` or `GPIOOutput` and have the compiler report an error if we try to do an invalid operation for the given type.\\n","tokens":21,"id":3021}
{"File Name":"datalab\/0014-use-traefik-for-reverse-proxy.md","Context":"## Context\\nThe Redbird proxy does not support WebSockets which are required to support the\\ninteractive notebooks. We have also had problems with the reliability\\nof the proxy and have found it difficult to configure.\\n","Decision":"We have decided to replace the custom Redbird proxy with a [Traefik](https:\/\/traefik.io\/)\\nproxy as this looks easier to configure and claims Web Socket support.\\n","tokens":44,"id":770}
{"File Name":"pupperware-party\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":2851}
{"File Name":"dotcom-rendering\/000-technical-review-records.md","Context":"## Context\\nWe are successfully using lightweight architecture decision records to document decisions being made about architecture.\\nLast year legislation changes around data privacy (GDPR, CCPA, PECR) and their impact on our obligations means we need to increase our technical measures to ensure the safety of our reader's data through:\\n-   Reducing the number of third-party scripts\\n-   Architecting our systems with a [privacy by design](https:\/\/en.wikipedia.org\/wiki\/Privacy_by_design) approach\\n-   Documenting risks and mitigations in place or to be added.\\n-   Adding new controls to prevent issues, i.e being proactive rather than reactive.\\nAdditional to this privacy concern, we have noticed in the past the difficulty to correctly estimate and measure accurately the performance and by association the revenue impact of each third-party. While they usually come with a direct revenue, there is as well indirect revenue loss through performamce impact:\\n-   performance degradation impacts our ad display and our advertising revenue\\n-   performance degradation impacts our contributions conversion rate\\n-   performance degradation impacts our Google SEO which impact our reach and indirectly all of our revenue streams (contributions, subscriptions, advertising)\\nTo address those 2 concerns, we are making a change to the process for deciding if a third-party will be added. Before a decision is made by the business to add a third-party to our website or apps, we will perform an engineering technical review that will be used to inform the business decision.\\nThe suggested format of the review is the following:\\n-   Metadata about the review\\n-   Date of the review\\n-   Author of the review\\n-   Platforms concerned\\n-   Context\\n-   Data privacy engineering review\\n-   Security engineering review\\n-   Perfomance engineering review\\n-   Recommended mitigations\\n","Decision":"For all third-party technical reviews in this project, a record will be created with a filename of `XXX-technical-review.md` where XXX is the monotonically increasing number described in the linked document.\\nPlease note that as records will be public, specific attention should be paid about information being disclosed.\\n","tokens":371,"id":2650}
{"File Name":"alfresco-anaxes-shipyard\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":3607}
{"File Name":"zsh\/0002-use-drop-in-pattern-for-extensibility.md","Context":"## Context\\nFacilitate organizing shell customizations into smaller, more readable, more understandable, more maintainable blocks and to provide a clear and common patter for extensibility.\\n","Decision":"Create a $ZDOTDEEDIR directory where extension files can be stored. Add a rooutine to .zprofile or .zshrc which sources all files found in this directory in lexagraphical order by file name.\\n","tokens":37,"id":1102}
{"File Name":"wordpress-template\/0005-use-this-repo-as-source-of-setup-scripts.md","Context":"## Context\\nBefore we created this template repo, we had [WPC](https:\/\/github.com\/dxw\/wpc), a repo which served two main purposes:\\n* Generating the files necessary within a Whippet-based WordPress app for local development in Docker, using the \"scripts to rule them all\" model. This meant e.g. generating the `docker-compose.yml` file, and the relevant files for the script folder (e.g. `script\/setup`, `script\/server`, etc.)\\n* Acting as a source for the `thedxw\/wpc-wordpress` docker image we use for running WordPress in docker in local development environments\\nNow we have this template repo, which also contains all files necessary for running a docker-based development environment (but not the `wpc-wordpress` docker image itself). We shouldn't duplicate those files in two separate repos.\\nWe no longer have the need to generate those files within existing projects, as all existing Whippet apps have already been converted for docker-based development. New projects (or projects we  inherit, e.g. old Helpful repos) will use this template repo to provide that infrastructure.\\n","Decision":"Use this repository as the primary source for the files that were previously generated by WPC.\\nRemove those files from the [WPC repo](https:\/\/github.com\/dxw\/wpc), so that repo is only responsible for the `wpc-wordpress` docker image, and nothing else.\\n","tokens":234,"id":2266}
{"File Name":"afterwriting-labs\/003-release-process.md","Context":"## Context\\nTravisCI is currently used for building feature branches, pull requests and releases.\\nThe way releases, however, are performed is a bit convoluted and not flexible. By default minor version is updated\\nand it's not clear how to do a major or patch release. Due to the way TravisCI builds feature branches vs pull requests\\nit requires hacks in travis config.\\n","Decision":"Solution implemented in https:\/\/github.com\/ifrost\/starterkit have worked quite well so far. It's based on adding\\nlabels to commit messages when merging a pull request to master.\\nIt's way easier to do major\/minor\/patch or hotfix release and publish to npm separately.\\n","tokens":80,"id":5106}
{"File Name":"TDD-hexagonal-project\/2020_12_02_9_20_PROJECT.md","Context":"### Context\\nThis is the beginning of the project\\n### Decision\\nI decide to create a _Docs_ folder to store all the documentation.\\nIn this _Docs_ folder there will also have a _ADRS_ folder to store all the decision on this project.\\n### Consequences\\nThere will be a single point to store all the documentation\\n","Decision":"I decide to create a _Docs_ folder to store all the documentation.\\nIn this _Docs_ folder there will also have a _ADRS_ folder to store all the decision on this project.\\n### Consequences\\nThere will be a single point to store all the documentation\\n","tokens":72,"id":3287}
{"File Name":"docs\/0008-browser-compatibility.md","Context":"## Context and Problem Statement\\nWhich browser should we actively support with the MICO web interface?\\n","Decision":"We will only actively support Chrome (above version 70). We may test the web interface occasionally with Firefox (above version 63) to fix issues rendering the web interface unusable with firefox.\\n","tokens":20,"id":4682}
{"File Name":"deeplearning4j\/0005-optional_parameters_and_signatures.md","Context":"## Context\\nNot all inputs or args (= parameters) are always required.\\nOften there are sensible defaults available. We want to be able to make those defaults explicit where possible.\\nEven though some parameters may be optional, they might become required in the presence of other optional parameters.\\nWe need a way to explicitly define what combinations are possible.\\n","Decision":"We drop the `optional` property on parameters. Instead, parameters get an additional property `defaultValue`. It can be\\nset to either a fixed literal value (e.g. `7`, `\"something\"`, `null`), an Arg, or it may reference the specific methods\\n`shape()` and `dataType()` on inputs and outputs. Parameters with `defaultValue` specified are treated as optional.\\nTo be able to deal with languages that do not support default values for arguments, Signatures will be specified.\\nSignatures are specified using a `Signature(a,b,c){ \"signature specific documentation\" }` section for each signature.\\nWith the signature specific documentation being optional.\\nSignatures making use of outputs will only be generated for NDArray programming mode, not in SameDiff mode. This also\\nmeans that parameters with a `defaultValue` based on an output will be treated as required in SameDiff mode.\\nIf signatures are specified, only the specified signatures will be generated.\\nIf no signatures are explicitly specified, only the \"all-arg\" and \"no-optional-arg\" signatures will be generated. In\\nNDArray programming mode, the default signatures also include a variant that includes the output.\\n","tokens":71,"id":2940}
{"File Name":"infrastructure-adrs\/0004-remove-workflow-datastream.md","Context":"## Context and Problem Statement <!-- required -->\\nTo advance SDR evolution towards decoupling from Fedora, we should store workflow state outside of Fedora (in the workflow service's database).\\n","Decision":"Remove the datastream.\\nThis was done in dor-services v9.0.0 ([commit](https:\/\/github.com\/sul-dlss\/dor-services\/commit\/8745e7c2e86edbbaa7577af85779c4ea06258dd3)).\\n","tokens":39,"id":792}
{"File Name":"pace-developers\/0009-brille-integration.md","Context":"## Context\\n[brille](https:\/\/github.com\/brille\/brille) is a library for computing symmetry operations\\nand linear interpolation within an irreducible part of the first Brillouin zone.\\nWhilst its symmetry operations functionality can be used stand-alone,\\nthe interpolation functionality should be integrated with codes\\nwhich compute quantities in reciprocal space\\n(such as [euphonic](https:\/\/github.com\/pace-neutrons\/euphonic) and [spinW](https:\/\/github.com\/spinw\/spinw))\\nto make these programs more user friendly.\\nAt present there are separate projects, [brilleu](https:\/\/github.com\/brille\/brilleu\/)\\nand [brillem](https:\/\/github.com\/brille\/brillem\/) to achieve this integration.\\nIn both cases brille\/X interface constructs a brille object from X and handles calling the X method(s)\\nto determine the information required for brille's interpolation.\\nThis relationship could be flipped if each X constructs its own brille object and\\nthen uses it to perform interpolation.\\n","Decision":"A [meeting](https:\/\/stfc365.sharepoint.com\/:b:\/r\/sites\/PACEProject\/Shared%20Documents\/Meetings\/PACE-General\/20201001_brilleX_Xbrille.pdf?csf=1&web=1&e=9XBRUe)\\nwas held and the decision was made that the integration of the interpolation functionality of brille\\nshould not be done by the external projects `brilleu` and `brillem`.\\nInstead, the interface between the calculator X and brille should be embedded within X which will\\nconstruct is own brille object and then use this to perform the interpolation.\\n","tokens":230,"id":5183}
{"File Name":"sre-adrs\/0003-use-eks-cluster-as-default-container-platform.md","Context":"## Context\\nThere are quite a few options when choosing a platform and infrastructure patterns for managing applications. Across Mozilla teams, services & applications are already overwhelmingly containerized using Docker. We can deploy these containerized applications onto a number of options, including but not limited to multiple Kubernetes services or self-hosted routes.\\n","Decision":"* We will support Dockerized applications primarily, with some flexibility given to application components that greatly benefit from serverless architectures.\\n* Dockerized applications are deployed to Kubernetes, defaulting to a Web SRE shared applications clusters if not requiring their own Kubernetes cluster.\\n* We will use AWS' Elastic Kubernetes Service to manage our Kubernetes clusters, unless there is a documented reason for a particular project not to.\\n* We will deploy our EKS clusters using Terraform & our internal Terraform module, unless there is a documented reason for a particular project not to.\\n* Any divergences from the above decisions must be clearly documented in the project's or application's service documentation.\\n","tokens":63,"id":2258}
{"File Name":"meadow\/0015-phoenix-context-organization.md","Context":"# 15. Phoenix Context Organization\\nDate: 2019-09-11\\n## Context\\nOur Phoenix Contexts are becoming bloated. We need some organizational pattern to keep things modular.\\n","Decision":"We've decided to use the rules described here: [A Proposal for Some New Rules for Phoenix Contexts](http:\/\/devonestes.herokuapp.com\/a-proposal-for-context-rules)\\n1. Resources have Schema files, and those contain only schema definitions, type definitions, validations and changeset functions\\n2. Every Schema has its own Secondary Context\\n3. The only place you use your Repos is in a Secondary Context, and only for the associated resource\\n4. Primary Contexts define higher level ideas in your application, and most interactions between resources will take place there\\n","tokens":41,"id":3844}
{"File Name":"Conduit\/0004-postpone-xunit-msbuild.md","Context":"## Context\\nThe xunit msbuild target does not work under xbuild and also we would like to be able to select files by glob pattern. The xunit target requires you know the name up front. In a CI environment these may differ, for example you may be unpacking an artifact archive.\\n","Decision":"Try implementing our own one that allows globbing but uses the internal from the xunit version\\n","tokens":62,"id":4407}
{"File Name":"gp-redirect\/0003-use-prometheus-for-exposing-metrics.md","Context":"## Context\\nWe need to know what the application is doing in a more light weight way than\\nscraping logs. We need to be able to monitor KPIs of the application in order\\nto understand the health of the application. This will allow us to react and\\npotentially pro-actively initiate measures as to ensure the application's\\nhealth if sound. Ultimately providing a better service for our users.\\n","Decision":"We will use Prometheus to monitor and alert on the state of the application.\\n","tokens":86,"id":4400}
{"File Name":"arborescence\/0004-restrict-language-version-to-csharp-8.md","Context":"## Context and Problem Statement\\nWhich language version should we prefer? Should we restrict the version at all?\\n## Decision Drivers\\n* Nullable reference types\\n* Source compatible with Unity\u00a02018.4\\n* Binary compatible with Unity\u00a02018.4\\n* Source compatible with Unity\u00a02020.3\\n* Source compatible with online coding platforms like CodinGame or LeetCode\\n","Decision":"* Nullable reference types\\n* Source compatible with Unity\u00a02018.4\\n* Binary compatible with Unity\u00a02018.4\\n* Source compatible with Unity\u00a02020.3\\n* Source compatible with online coding platforms like CodinGame or LeetCode\\nChosen option: \u201cC# 8\u201d.\\nThis is specified in one of the Directory.Build.props files:\\n```xml\\n<Project>\\n<PropertyGroup>\\n<LangVersion>8<\/LangVersion>\\n<Nullable>enable<\/Nullable>\\n<\/PropertyGroup>\\n<\/Project>\\n```\\nThe main reason to ditch 7.3 is that it complicates nullable annotations.\\nThe latest available compiler version C#\u00a09 is not supported by some popular online coding platforms as of June 2021.\\n### Negative Consequences\\n- C# 8 is not source-compatible with Unity\u00a02018.4.\\nHowever it is binary-compatible via .NET Standard 2.0.\\nUnity 2020.3 partially supports C# 8.\\n","tokens":79,"id":1723}
{"File Name":"nearby-services-api\/0006-split-nearby-and-open-results-into-separate-endpoints.md","Context":"## Context\\nThe primary (only) consuming application for this API needs to show both open\\nand nearby services on separate pages (and more of them). Previously the\\napplication had shown a mix of open and nearby services within a\\nsingle page.\\nHaving the API so closely aligned to the needs of the consumer is not ideal.\\nThere is scope to increase the flexibility of the API along with increasing the\\nease with which it can be used both by the current and future consumers.\\n","Decision":"The decision is to add a new endpoint i.e. `\/open` alongside the current\\n`\/nearby` endpoint. The former endpoint will return only services that are open\\nwhere the latter will be refactored to return only services that are nearby\\nregardless of their opening state.\\n","tokens":100,"id":482}
{"File Name":"govuk-aws\/0036-performance-platform-and-backdrop-architecture.md","Context":"## Context\\nWe currently have dedicated performance-platform environments for both the staging and production GovUK environments. The performance-platform is made up of the following services:\\nFor GovUK Staging, the entire performance-platform resides in the GovUK PaaS:\\n- Stagecraft\\n- Spotlight\\n- PP-Admin\\n- Backdrop database\\n- Backdrop Worker, Read & Write API\\nFor GovUK Production:\\n- Stagecraft  (GovUK PaaS)\\n- Spotlight  (GovUK PaaS)\\n- PP-Admin  (GovUK PaaS)\\n- Backdrop database  (Carrenza performance-mongo cluster)\\n- Backdrop Worker, Read & Write API  (Carrenza api-1 & api-2 nodes)\\nThe info-frontend is the only service within the GovUK environments that makes connections to the read and write backdrop APIs. Specifically the endpoint www.gov.uk\/performance\/data directs traffic to backdrop. It accesses those APIs through the following environment corresponding names:\\n- www.performance.service.gov.uk\\n- www.staging.performance.service.gov.uk\\nThose names will resolve to fastly, which will serve what is required if cached, if it is not cached it will update its cache by hitting the appropriate backdrop endpoints, which for staging would be in the PaaS and for production reside on the api-1 and api-2 instances.\\n![Performance Platform](.\/0036-performance-platform-and-backdrop-architecture-img01.png?raw=true \"Performance Platform\")\\n","Decision":"We have outlined the current architecture of the performance environment here.\\nThis ADR outlines the current architecture of the performance platform for both the GovUK Staging and Production environments. It must be noted that the context of the ADR has been written during a migration period where GovUK services are been moved from Carrenza to AWS. It is mainly a record for people to see how the performance platform fits together with the GovUK environments.\\n","tokens":306,"id":4042}
{"File Name":"linshare-mobile-android-app\/0004-mvvm-for-android-development.md","Context":"## Context\\nWhen we start to develop the android application, we have to chose the right architecture\\nIf you don\u2019t choose the right architecture for your Android project, you will have a hard time maintaining it as your codebase grows and your team expands.\\nHere it comes with mostly behavioral design patents, MVC, MVP and MVVM, for linshare application and target for code quality, where MVVM is suitable for our code base could be testable, reduce some middle code to biding between layers\\n### Advantages of MVVM Architecture\\n- Code is even more easily testable than with plain MVVM.\\n- Code is further decoupled (the biggest advantage.)\\n- The package structure is even easier to navigate.\\n- The project is even easier to maintain.\\n- Team can add new features even more quickly.\\n### The Layers of MVVM with Clean Architecture\\nThe code is divided into three separate layers:\\n- Presentation Layer\\n- Domain Layer\\n- Data Layer\\n","Decision":"We agreed to implement the MVVM application design on android application.\\n","tokens":203,"id":1650}
{"File Name":"superwerker\/living-documentation.md","Context":"## Context\\nA dashboard with more information and deep-links to resources, e.g. setting up SSO with existing identity providers, GuardDuty\/Security Hub dashboards.\\n","Decision":"- Create a CloudWatch Dashboard called `superwerker` in the AWS management account. The CW dashboard a) ensures a deep link which can be used to link from the README.md and b) ensures the user is authorized to access the information.\\n- Display DNS delegation state and setup instructions\\n- Refresh dashboard with scheduler every minute since this removes the compexity to deal with event-based dashboard generation. Lambda invocations are completely covered by free-tier.\\n","tokens":36,"id":3399}
{"File Name":"ReportMI-service-manual\/0009-user-interface-look-and-feel.md","Context":"## Context\\nWe are building a new digital service for CCS.\\n[CCS-ADR-0002][ccs-adr-0002] says that all new services should have a common\\nuser experience, based on the GOV.UK Design System.\\nThese patterns should be amended to use the CCS colours and brand, but should\\nremain consistent with GOV.UK where possible.\\n","Decision":"We will follow this CCS Architectural Decision and use the GOV.UK Design System\\nand GOV.UK Frontend, amended to use CCS colours and brand.\\nThe service will be hosted on a subdomain of `crowncommercial.gov.uk`.\\n","tokens":82,"id":2047}
{"File Name":"oklog\/adr-001-multitenancy.md","Context":"## Context\\nSystem operators often run shared infrastructure for different workloads. For\\nexample, a single Kubernetes cluster may serve multiple departments in an\\norganization, none of whom should need to know about the others.\\nOK Log as it exists today (v0.3.2) assumes all users (both producers and\\nconsumers) are part of the same global namespace, and provides no way to\\nsegment ingestion or querying.\\nWe have at least one interesting use case, with one potential user, where\\nmulti-tenanancy would be a requirement.\\n","Decision":"Motivated by this new use case, we judge that adding multi-tenant features, in\\naddition to a handful of other longstanding feature requests, would push OK Log\\nin a useful direction.\\nThe initial set of issues include:\\n- [Add first-class concept of topics](https:\/\/github.com\/oklog\/oklog\/issues\/113)\\n- Separate indexer layer for faster queries\\n- [Move to length-delimited records](https:\/\/github.com\/oklog\/oklog\/issues\/112)\\n- [Extend record identifier](https:\/\/github.com\/oklog\/oklog\/issues\/114)\\n- [Long-term storage](https:\/\/github.com\/oklog\/oklog\/issues\/115)\\nAdditional issues may be filed, and these issues may be refactored or dropped\\noutright depending on the result of experimentation.\\n","tokens":115,"id":1836}
{"File Name":"wsgi-base\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":1889}
{"File Name":"clone_difitalcitizenship\/0010-we-select-an-azure-app-hosting-service.md","Context":"## Context\\nMost components of the Digital Citizenship platform are designed on the\\nserverless runtime model, specifically they are developed for the Azure\\nFunctions platform.\\nSome components don't fit into this model, specifically the integration with\\nthe SPID authentication service requires the deployment of an instance of the\\n[Shibboleth](https:\/\/wiki.shibboleth.net\/confluence\/display\/SHIB2) SAML\\nService Provider that sits in front of the application that needs to be secured\\nby SPID authentication.\\nThis kind of deployment requires a VM\/container IaaS service.\\n","Decision":"Microsoft Azure provides two application hosting services that suits our needs:\\n| Service                                                                                      | Pros | Cons |\\n| -------------------------------------------------------------------------------------------- | ---- | ---- |\\n| [Web App for Containers](https:\/\/azure.microsoft.com\/en-us\/services\/app-service\/containers\/) | Flexible and portable (it's basically a DCOS or Kubernetes platform) | High configuration\/management effort |\\n| [Web Apps](https:\/\/azure.microsoft.com\/en-us\/services\/app-service\/web\/)                      | Low configuration\/management effort | Deployment mechanism specific to the service, harder to port to other platforms |\\nGiven that:\\n1.  We already have a working Docker container that provides a SPID\/Shibboleth\\nsetup.\\n2.  We want to minimize the coupling to non open-source technologies\\nWe decide that we will use *Web App for Containers* to setup a *Kubernetes*\\ncluster.\\n","tokens":119,"id":1184}
{"File Name":"cnp-design-documentation\/0004-Setting_up_vault_as_a_cert_auth.md","Context":"## Context\\nWe wanted to explore Vault's capabilities to serve as a CA for services deployed on the strategic platform at HMCTS, that's to issue the certificates services may need for authentication purposes.\\n","Decision":"In order to discuss the CA capabilities Vault has we need first to talk about Secret Backends. Secret Backends are the components in Vault which store and generate secrets, they are part of the mount system in Vault. They behave very similarly to a virtual filesystem: any read\/write\/delete is sent to the secret backend, and the secret backend can choose to react to that operation however it sees fit.\\nFor CA purposes Vault provides the PKI Secret Backend which generates X.509 certificates dynamically based on configured roles. This means services can get certificates needed for both client and server authentication without going through the usual manual process of generating a private key and CSR, submitting to a CA, and waiting for a verification and signing process to complete.\\nVault's documentation [quick-start](https:\/\/www.vaultproject.io\/docs\/secrets\/pki\/index.html#quick-start) on this subject illustrate pretty well some of the operational concepts behind running Vault as a CA. The documentation also presents some of the [considerations](https:\/\/www.vaultproject.io\/docs\/secrets\/pki\/index.html#considerations) needed when using the PKI backend but we believe the [\"Be Careful with Root CAs\"](https:\/\/www.vaultproject.io\/docs\/secrets\/pki\/index.html#be-careful-with-root-cas) section require special attention and that's why we focused our spike on showing how Vault can be used to create a Root CA and an Intermediate CA for our organization.\\nWith the above in mind we start by creating the Root CA for our org. We begin by mounting the PKI backend for our hmcts Root CA:\\n```code\\n\/ # vault mount -path=hmcts -description=\"HMCTS Root CA\" pki\\nSuccessfully mounted 'pki' at 'hmcts'!\\n\/ #\\n```\\nNow we can create the CA certificate\\n```code\\n\/ # vault write hmcts\/root\/generate\/internal \\\\n> common_name=\"HMCTS Root CA\" \\\\n> key_bits=4096 \\\\n> exclude_cn_from_sans=true\\nKey             Value\\n---             -----\\ncertificate     -----BEGIN CERTIFICATE-----\\nMIIFADCCAuigAwIBAgIUd0VBVb7dPA3fO8X36dt9LVhBpVcwDQYJKoZIhvcNAQEL\\nBQAwGDEWMBQGA1UEAxMNSE1DVFMgUm9vdCBDQTAeFw0xNzA3MjcxNTU3MzRaFw0x\\nNzA4MjgxNTU4MDRaMBgxFjAUBgNVBAMTDUhNQ1RTIFJvb3QgQ0EwggIiMA0GCSqG\\n...\\n<output truncated>\\n...\\n-----END CERTIFICATE-----\\nexpiration      1503935884\\nissuing_ca      -----BEGIN CERTIFICATE-----\\nMIIFADCCAuigAwIBAgIUd0VBVb7dPA3fO8X36dt9LVhBpVcwDQYJKoZIhvcNAQEL\\nBQAwGDEWMBQGA1UEAxMNSE1DVFMgUm9vdCBDQTAeFw0xNzA3MjcxNTU3MzRaFw0x\\nNzA4MjgxNTU4MDRaMBgxFjAUBgNVBAMTDUhNQ1RTIFJvb3QgQ0EwggIiMA0GCSqG\\n...\\n<output truncated>\\n...\\n4QnBYzvMNlxCHyhlrmG7+TzRXRYEV2HSV9VNX0\/+DfGxo2Xk2UsXYDq3BvwUPkk7\\nW47N\/TSaO1kFNMFhE+QNeAzddp4SFrpoWrIZ1Z1mPo12Jl8aW89hzbj8CUHtv12t\\n6\/U0lfBERcz13VzPl5pAfEWQs3\/DJcfKJXVodkcMaD7BhDkk\\n-----END CERTIFICATE-----\\nserial_number   77:45:41:55:be:dd:3c:0d:df:3b:c5:f7:e9:db:7d:2d:58:41:a5:57\\n\/ #\\n```\\nUsing `curl` and `openssl` we can verify the certificate we just created:\\n```code\\n\/ # curl -s http:\/\/127.0.0.1:1234\/v1\/hmcts\/ca\/pem | openssl x509 -text\\nCertificate:\\nData:\\nVersion: 3 (0x2)\\nSerial Number:\\n77:45:41:55:be:dd:3c:0d:df:3b:c5:f7:e9:db:7d:2d:58:41:a5:57\\nSignature Algorithm: sha256WithRSAEncryption\\nIssuer: CN=HMCTS Root CA\\nValidity\\nNot Before: Jul 27 15:57:34 2017 GMT\\nNot After : Aug 28 15:58:04 2017 GMT\\nSubject: CN=HMCTS Root CA\\nSubject Public Key Info:\\nPublic Key Algorithm: rsaEncryption\\nPublic-Key: (4096 bit)\\nModulus:\\n00:a1:d8:f3:2a:43:09:6c:39:42:4e:f1:b8:c9:86:\\n82:e5:ed:4e:34:b2:92:f7:c6:d2:68:10:a2:46:ec:\\n...\\n<output truncated>\\n```\\nWe can now proceed to configure the URL's for accessing not only the CA but also the Certificate Revocation List (CRL):\\n```code\\n\/ # vault write hmcts\/config\/urls issuing_certificates=\"http:\/\/127.0.0.1:1234\/v1\/hmcts\"\\nSuccess! Data written to: hmcts\/config\/urls\\n\/ #\\n```\\nNote that we used the localhost ip address in the issuing_certificates URL only for demo purposes and this can be set to any address as appropriate. With the Root CA ready we can now create an Intermediate CA:\\n```code\\n\/ # vault write hmcts\/config\/urls issuing_certificates=\"http:\/\/127.0.0.1:1234\/v1\/hmcts\"\\nSuccess! Data written to: hmcts\/config\/urls\\n\/ # vault mount -path=hmcts_probate -description=\"Probate Intermediate CA\" pki\\nSuccessfully mounted 'pki' at 'hmcts_probate'!\\n\/ # vault mounts\\nPath            Type       Default TTL  Max TTL  Force No Cache  Replication Behavior  Description\\ncubbyhole\/      cubbyhole  n\/a          n\/a      false           local                 per-token private secret storage\\nhmcts\/          pki        system       system   false           replicated            HMCTS Root CA\\nhmcts_probate\/  pki        system       system   false           replicated            Probate Intermediate CA\\nsecret\/         generic    system       system   false           replicated            generic secret storage\\nsys\/            system     n\/a          n\/a      false           replicated            system endpoints used for control, policy and debugging\\n\/ #\\n```\\nNow we proceed to generate the Intermediate Certificate Signing Request (CSR):\\n```code\\n\/ # vault write hmcts_probate\/intermediate\/generate\/internal \\\\n> common_name=\"Probate Intermediate CA\" \\\\n> key_bits=4096 \\\\n> exclude_cn_from_sans=true\\nKey     Value\\n---     -----\\ncsr     -----BEGIN CERTIFICATE REQUEST-----\\nMIIEZzCCAk8CAQAwIjEgMB4GA1UEAxMXUHJvYmF0ZSBJbnRlcm1lZGlhdGUgQ0Ew\\nggIiMA0GCSqGSIb3DQEBAQUAA4ICDwAwggIKAoICAQDBAzV85KEbAH3UYgxrR6F5\\n...\\n<output truncated>\\n...\\nqoaquy\/+vS6o5Fg2e4XE0ZNMDpTj37visONk\\n-----END CERTIFICATE REQUEST-----\\n\/ #\\n```\\nNow we need to use the CSR to get a cert signed by the Root CA. For this we save\/copy the contents of the CSR onto a file (`probate.csr`) and send it to the Root CA backend:\\n```code\\n\/ # vault write hmcts\/root\/sign-intermediate \\\\n> csr=@probate.csr \\\\n> common_name=\"Probate Intermediate CA\" \\\\n> ttl=24h\\nKey             Value\\n---             -----\\ncertificate     -----BEGIN CERTIFICATE-----\\nMIIFjTCCA3WgAwIBAgIUUGSq+TcwSQnLtkFP5WjS\/yk1T5kwDQYJKoZIhvcNAQEL\\nBQAwGDEWMBQGA1UEAxMNSE1DVFMgUm9vdCBDQTAeFw0xNzA3MjcxNjMzMTNaFw0x\\n...\\n<output truncated>\\n...\\nIAO2K7gqK1gVV1DmXWSpmTcT4mUXFgGWtM3TP2ILV3lYRzRkkGK2PULJGEWS\/0pc\\nh1286BOsydSM2Ai6+bnEbV0s\/6X0YL4L3WlYxQgEkNuP\\n-----END CERTIFICATE-----\\nexpiration      1501259623\\nissuing_ca      -----BEGIN CERTIFICATE-----\\nMIIFADCCAuigAwIBAgIUd0VBVb7dPA3fO8X36dt9LVhBpVcwDQYJKoZIhvcNAQEL\\nBQAwGDEWMBQGA1UEAxMNSE1DVFMgUm9vdCBDQTAeFw0xNzA3MjcxNTU3MzRaFw0x\\n...\\n<output truncated>\\n...\\nW47N\/TSaO1kFNMFhE+QNeAzddp4SFrpoWrIZ1Z1mPo12Jl8aW89hzbj8CUHtv12t\\n6\/U0lfBERcz13VzPl5pAfEWQs3\/DJcfKJXVodkcMaD7BhDkk\\n-----END CERTIFICATE-----\\nserial_number   50:64:aa:f9:37:30:49:09:cb:b6:41:4f:e5:68:d2:ff:29:35:4f:99\\n\/ #\\n```\\nNow we need to import the Root CA signed cert we created in the previous step back to the Intermediate CA backend. For this we copy the contents to a file `probate.crt`:\\n```code\\n\/ # vault write hmcts_probate\/intermediate\/set-signed \\\\n> certificate=@probate.crt\\nSuccess! Data written to: hmcts_probate\/intermediate\/set-signed\\n\/ #\\n```\\nUsing `curl` and `openssl` we can verify the contents of this certificate:\\n```code\\n\/ # curl -s http:\/\/127.0.0.1:1234\/v1\/hmcts_probate\/ca\/pem | openssl x509 -text\\nCertificate:\\nData:\\nVersion: 3 (0x2)\\nSerial Number:\\n20:e8:ca:3d:49:33:e6:b0:65:32:4e:9c:9a:84:a3:20:90:0c:ae:61\\nSignature Algorithm: sha256WithRSAEncryption\\nIssuer: CN=HMCTS Root CA\\nValidity\\nNot Before: Jul 27 16:54:40 2017 GMT\\nNot After : Jul 28 16:55:10 2017 GMT\\nSubject: CN=Probate Intermediate CA\\nSubject Public Key Info:\\nPublic Key Algorithm: rsaEncryption\\nPublic-Key: (4096 bit)\\nModulus:\\n00:c1:03:35:7c:e4:a1:1b:00:7d:d4:62:0c:6b:47:\\na1:79:a0:c7:a8:7e:9e:1d:0b:b1:90:6e:6a:cd:96:\\n...\\n<output truncated>\\n```\\nJust like we did with the Root CA, we can now configure the CA and CRL URLs for our Intermediate CA:\\n```code\\n\/ # vault write hmcts_probate\/config\/urls \\\\n> issuing_certificates=\"http:\/\/127.0.0.1\/v1\/hmcts_probate\/ca\" \\\\n> crl_distribution_points=\"http:\/\/127.0.0.1\/v1\/hmcts_probate\/crl\"\\nSuccess! Data written to: hmcts_probate\/config\/urls\\n\/ #\\n```\\nBefore we can request certificates we should establish some restrictions around the certificates we generate like key types, ttl, etc ... For this we use the `role` functionality provided by vault:\\n```code\\n\/ # vault write hmcts_probate\/roles\/frontend \\\\n> key_bits=2048 \\\\n> max_ttl=10h \\\\n> allow_any_name=true\\nSuccess! Data written to: hmcts_probate\/roles\/frontend\\n\/ #\\n```\\nand now we should be able to issue certs for this role:\\n```code\\n\/ # vault write hmcts_probate\/issue\/frontend \\\\n> common_name=\"test.probate.hmcts.net\" \\\\n> ip_sans=\"172.16.0.1\" \\\\n> ttl=5h \\\\n> format=pem\\nKey                     Value\\n---                     -----\\nca_chain                [-----BEGIN CERTIFICATE-----\\nMIIFjTCCA3WgAwIBAgIUIOjKPUkz5rBlMk6cmoSjIJAMrmEwDQYJKoZIhvcNAQEL\\nBQAwGDEWMBQGA1UEAxMNSE1DVFMgUm9vdCBDQTAeFw0xNzA3MjcxNjU0NDBaFw0x\\nNzA3MjgxNjU1MTBaMCIxIDAeBgNVBAMTF1Byb2JhdGUgSW50ZXJtZWRpYXRlIENB\\n...\\n<output truncated>\\n```\\n","tokens":40,"id":1070}
{"File Name":"gp-finder\/0005-add-cache-control-headers.md","Context":"## Context\\nCache control headers can be used to prevent a client's browser from\\nre-requesting a page that has not changed, and may be leveraged by proxies to\\nreturn the same cached pages for multiple clients.\\n","Decision":"Cache-control headers will be added to all valid requests. 500 and 404 errors\\nwill not be cached.\\n","tokens":46,"id":3471}
{"File Name":"hoard\/0004-use-airflow-and-fargate.md","Context":"## Context\\nThe ingest process will be a scheduled task and we have an Airflow instance designed for just this sort of thing.\\n","Decision":"We will use Airflow to handle scheduling the ingest. The ingest process itself will be run inside a Fargate container.\\n","tokens":27,"id":4487}
{"File Name":"green_log\/0002-avoid-global-configuration.md","Context":"## Context\\nApplications require a way to configure their logging, e.g. what should be logged, where should logs be sent, and how should they be formatted.\\nSome Ruby logging libraries involve global configuration, and other state, e.g.\\n```ruby\\nrequire 'some_logger'\\nSomeLogger.default_level = :trace\\nSomeLogger.add_appender(file_name: 'development.log', formatter: :color)\\nSomeLogger[\"MyClass\"].info(\"Stuff happened\")\\n```\\nbut global state can have unintended consequences, and make testing difficult.\\n","Decision":"GreenLog will avoid global configuration. `GreenLog::Logger` instances should be created and configured explicitly, and injected as dependencies where needed.\\n","tokens":115,"id":1926}
{"File Name":"paas-team-manual\/ADR032-ssl-only-for-applications-and-cf-endpoints.html.md","Context":"## Context\\nNote: This has been superceeded. See [Status](#status) below.\\nIt is expected for the government websites to be secure and keep the user\\ninteractions private. Because that we want to enforce all communications to\\nany application and to the platform endpoints to use only and always HTTPS,\\nas [it is described in the Gov Service Manual](https:\/\/www.gov.uk\/service-manual\/technology\/using-https).\\nWhen a user inputs a website name without specifying the\\nprotocol in the URL, most browsers will try first the HTTP protocol by default.\\nEven if the server always redirect HTTP to HTTPS, an initial\\nunprotected request including user information will be transferred\\nin clear: full URL with domain, parameter, [cookies without secure flag](https:\/\/en.wikipedia.org\/wiki\/HTTP_cookie#Secure_and_HttpOnly)\\nor browser meta-information.\\n[HTTP Strict Transport Security](https:\/\/en.wikipedia.org\/wiki\/HTTP_Strict_Transport_Security)\\nmitigates this issue by instructing modern browsers that support it to\\nalways connect using HTTPS.\\nThis is also a [requirement in the service manual](https:\/\/www.gov.uk\/service-manual\/technology\/using-https).\\nThere is still a potential initial unprotected HTTP request that might happen\\nbefore retrieve the HSTS headers or after the specified HSTS `max-age`.\\nTo solve this issue, the root domain can be added to\\n[HSTS preload list](https:\/\/hstspreload.appspot.com\/) which will be used by most\\ncommon browsers.\\nCurrently the only way to avoid any clear text HTTP interaction is closing or\\ndropping any attempt to connect to the port 80 at TCP level.\\nAlthough not all application deployed on the PaaS will be \"services\"\\nas in the service manual meaning, we must not allow HTTP to make\\nit easier to service owners to comply with this requirements.\\n","Decision":"We will only open port 443 (HTTPS) and drop\/reject any TCP connection to TCP port 80 (HTTP).\\nWe will implement and maintain HSTS preload lists for our production domains.\\n","tokens":391,"id":192}
{"File Name":"dotcom-rendering\/021-react-portals.md","Context":"## Context\\nWe already have partial hydration implemented using react 'islands', a design pattern where we use multiple react apps on the same page, but now we have a requirement to share data between these islands.\\nReact portals would allow us to have one root react app with multiple portals existing within it without the need to hydrate the entire dom. See: https:\/\/reacttraining.com\/blog\/portals-with-context\/\\nThere are two ways to share data between portals. Using props, or react context.\\n### React context\\nUseful because it means all child components can access the context at any point in the tree but adds 'magic'.\\n### Props\\nMore explicit but causes additional prop drilling.\\n","Decision":"Implement react portals pattern and use props to pass down shared data.\\nDon't use react context, use pure props instead. This means more prop drilling but we prefer this cost over the complexity of context.\\n","tokens":143,"id":2653}
{"File Name":"time-tracker\/002-use-cljs-for-web-ui.md","Context":"## Context\\nThe web frontend of time-tracker is crucial, since it is the only way to access certain important features such as user management and invoicing. It will be built as a single-page application. Web frontend development is complex nowadays, and choosing the right stack can help mitigate this complexity and speed up the development process. We are evaluating Typescript and ClojureScript.\\nTypescript is widely used and has a lot of mindshare. As such, setting up a Typescript project and making use of the Typescript\/Javascript ecosystem is straightforward.\\nClojureScript offers a number of advantages over Typescript, such as programming in a functional-first style with immutable data structures, LISP syntax, seamless live code reloading, a fast REPL-driven workflow and generally less code and boilerplate. In addition to this, the [`re-frame`](http:\/\/day8.github.io\/re-frame\/) framework is a well thought-out approach to state management. It manages state and side effects very well and with much less boilerplate compared to popular JS\/TS equivalents such as Redux. All programmers at nilenso are familiar with Clojure (if not ClojureScript already), and learning ClojureScript and getting productive shouldn't be an issue for them.\\nOne potential concern with ClojureScript is easy access to the JavaScript\/NPM ecosystem of libraries and tooling features. This concern is addressed by `shadow-cljs`, a popular build tool for ClojureScript which (among other things) makes access to NPM packages as frictionless as possible.\\nSome features readily available in JS\/TS such as CSS modules are tricky to set up or unusable with ClojureScript.\\n","Decision":"1. We will use [ClojureScript](https:\/\/clojurescript.org\/) as the programming language for the web frontend.\\n2. We will use [re-frame](http:\/\/day8.github.io\/re-frame\/) as the main application framework.\\n3. We will use [shadow-cljs](http:\/\/shadow-cljs.org\/) as the build tool for ClojureScript. For other assets such as CSS, we will use other build tooling as necessary.\\n","tokens":336,"id":4921}
{"File Name":"life-dashboard\/20191026 arch.md","Context":"## Context\\nNeed to decide on a high level structure for this app to take. It's goals are:\\n- Me to learn\\n- Fun times \/ be aspirational (as a selling point if it ever gets published seriously)\\n- be a potential thing I'll publish\\n- make some elements of life easier \/ be an enabler to life automation.\\n","Decision":"\"Federated syncing\". It's aspirational, one hell of a challenge, awesome for the CV. The main con for this is complexity... but this _is_ a learning project so: bring it on! Next up is to sketch out the layout of the pieces.\\n","tokens":74,"id":2783}
{"File Name":"commercetools-adyen-integration\/0004-notification-error-handling.md","Context":"## Context\\nWhen error occurs during validating additional notification data and sending request to CTP platform, the Adyen\\nnotification wraps the thrown error by using customized error objects `CommercetoolsError` and `ValidationError`.\\n````\\nclass ValidationError extends Error {\\nconstructor({ stack, message }) {\\nsuper()\\nthis.stack = stack\\nthis.message = message\\nthis.retry = false\\n}\\n}\\n````\\n````\\nclass CommercetoolsError extends Error {\\nconstructor({ stack, message, statusCode }) {\\nsuper()\\nthis.stack = stack\\nthis.message = message\\nthis.retry = this._shouldRetry(statusCode)\\n}\\n_shouldRetry(statusCode) {\\nreturn statusCode < 200 || statusCode === 409 || statusCode >= 500\\n}\\n}\\n````\\nAs displayed above, these customized error objects only wrap partial information from original error. Since\\nthe whole error object is not logged, some important information may be missing for debug purpose.\\n","Decision":"For the error occurs when making request to CTP platform, we adapt VError package which helps to wrap the entire original\\nerror and propagate it to function caller.\\n```\\ntry {\\n...\\n\/\/Attempt to perform request to CTP platform\\n...\\n} catch (err) {\\nthrow new VError(err, 'customized error message')\\n}\\n```\\nThe wrapped error can be retrieved from VError wrapper by either following code snippet\\n```\\nconst cause = VError.cause(err)\\n```\\n```\\nconst cause = err.cause()\\n```\\nFor more detais, please refer to [VError documentation](https:\/\/www.npmjs.com\/package\/verror)\\nIn case error is not thrown from external call, but it happens when fail from data validation, we don't need to\\nadapt `VError`. We simply instantiate an Error object by parsing error message like example below\\n```\\nif (\\n!transactionStateFlow.hasOwnProperty(currentState) ||\\n!transactionStateFlow.hasOwnProperty(newState)\\n) {\\nconst errorMessage = `Wrong transaction state passed. CurrentState: ${currentState}, newState: ${newState}`\\nthrow new Error(errorMessage)\\n}\\n```\\n","tokens":208,"id":1715}
{"File Name":"dogma\/0006-stateless-aggregates-and-processes.md","Context":"## Context\\nWe need to decide how to represent aggregates and processes that do not have\\nany state.\\nThis may sound nonsensical at first, but there are legitimate implementations of\\nboth that do not require any state, or perhaps more correctly the only state is\\nwhether or not they exist at all.\\nThis is perhaps more likely to occur in a CQRS\/ES environment where the state\\nassociated with an aggregate is a \"write-model\". In this case, the only state\\nthat needs to be maintained is that which is required to make decisions about\\nwhich events to produce.\\n","Decision":"We've opted to have Dogma provide empty \"root\" implementations out of the box,\\nfor both aggregates and processes. The implementations should be unexported\\nstructs made available by exposed global variables in the `dogma` package.\\nHandler implementations can return these values from their `New()` methods to\\nindicate that they do not keep state.\\nAs these values are valid implementations of the `AggregateRoot` \/ `ProcessRoot`\\ninterfaces, engine implementations need not handle these impementations specially,\\nthough they can opt to do so by treating them as \"sentinel\" values.\\n","tokens":121,"id":1611}
{"File Name":"govuk-aws\/0030-change-in-architecture-to-asset-master.md","Context":"## Context\\nTraditionally we built an \"asset-master-1\" instance with a large disk, and exported\\nthat disk as an NFS share.\\nThe \"backend\", \"whitehall-backend\" and \"asset-slave\" instances all mounted this NFS\\nshare for the tasks they needed to do (uploading files for the backend machines, and\\nmaking backups on the asset-slave machines).\\nIn AWS, we have the option to use [Elastic File System (EFS)](https:\/\/aws.amazon.com\/efs).\\nThis is an autoscaling filesystem so we do not have to manage disk space, and is independent\\nto any instance. Each instance would have to mount this filesystem in the same way as NFS,\\nbut it is not managed by us.\\n","Decision":"We will create an EFS resource and expose the mount using our internal Route 53 DNS. We will allow\\nthe required machines to mount the resource using Security Groups.\\nThe backend and whitehall-backend instances will mount as usual, and the asset-master will also\\nmount the disk like an external share.\\nThe asset-master is required to mount because it moves files about the filesystem after running\\nvirus scans.\\nA decision has yet to be made on the role of the asset-slave, as we could potentially move\\nthese tasks onto the asset-master (pushing backups to an S3 bucket, for example).\\n","tokens":158,"id":4043}
{"File Name":"nebula\/0002-use-systemd-as-cgroup-driver-for-docker.md","Context":"Context\\n-------\\nWhen I first installed kubernetes with nothing but default settings, I\\ngot this warning:\\n```\\n[WARNING IsDockerSystemdCheck]: detected \"cgroupfs\" as the Docker cgroup driver. The recommended driver is \"systemd\". Please follow the guide at https:\/\/kubernetes.io\/docs\/setup\/cri\/\\n```\\nTurns out, they don't recommend using docker's default cgroup driver in\\ntandem with systemd long-term.\\nDecision\\n--------\\nSo I'm doing as the kubernetes website recommends, because they seem\\ntrustworthy.\\nConsequences\\n------------\\nThat warning message no longer appears.\\n","Decision":"--------\\nSo I'm doing as the kubernetes website recommends, because they seem\\ntrustworthy.\\nConsequences\\n------------\\nThat warning message no longer appears.\\n","tokens":141,"id":4949}
{"File Name":"dlp-curate\/0003-AWX-for-dlp-automation.md","Context":"## Context\\nThe Middleware team has strived to practice Infrastructure-As-Code, meaning DLP's AWS account should be reflected entirely within GitHub.\\nThis has mostly been accomplished but Ansible playbooks are currently run in Ansible Core on individual laptops of the Middleware team.\\nThe eventual goal should be GitOps, automated infrastructure changes triggered by a commit.\\nFor that purpose we will need a platform capable of running playbooks automatically.\\nThis platform would need to be able to run the playbooks on demand or scheduled.\\nRedHat provides two solutions: AWX and Ansible Tower.\\nAWX is the upline of Ansible Tower, meaning it is a more advanced version that lacks official support.\\n","Decision":"The Middleware team will use AWX to manage DLP's infrastructure automation needs.\\nA different tool may be used for application automation.\\n","tokens":145,"id":1729}
{"File Name":"klokwrk-project\/0012-response-format.md","Context":"## Context\\nEvery server-side application sends some kind of responses to its clients. Unfortunately, the exact structure of these responses is rarely defined.\\nThe most probable cause is that each **successful** response has a different content and structure as it communicates various domain data depending on the requested endpoint. Consequently, it is\\nimpossible to prescribe a predefined structure for such responses carrying only \"concrete\" data in its payload.\\nHowever, there is often a need to communicate some kind of metadata with every response in more elaborate systems. While metadata can be useful for successful responses, they are almost unavoidable\\nfor responses communicating failures. No matter if the failure cause originates from the client or the server.\\nThe standard way for sharing metadata is through means of a concrete protocol. For example, HTTP uses statuses and headers. Messaging systems also employ headers but of different format, etc.\\nHowever, if there is a need for more detailed metadata, headers are often combined with payloads.\\nSuch a situation brings fragmentation and inconsistency in metadata transfers, especially when multiple protocols and channels have to be supported. While standard protocol features must be obeyed,\\nfrom the application perspective, it might be helpful having all necessary data and metadata in one place.\\nTo enable generalized creation on the server-side and generalized processing on the client-side, metadata should follow some kind of prescribed shape and format.\\n### Architectural Context\\n* System (`klokwrk-project`)\\n","Decision":"**We will render responses following the prescribed format.**\\nFormat structure is independent of concrete protocol or channel and should be used for all generated responses.\\nConcrete details of the prescribed format are given in the \"[The format of rendered responses](..\/..\/article\/response-format\/responseFormat.md)\" article. Although the article presents prescribed\\nstructures in JSON, concrete format implementation is not essential. It can be anything else that is more suitable for some chosen protocol, as long as defined structures are followed.\\nImplementations of response renderers are specific for each supported channel. At the moment, we have an implementation for the Spring MVC channel that can be found in `cargotracking-lib-web` module\\nin `org.klokwrk.cargotracking.lib.web.spring.mvc` package.\\n","tokens":290,"id":4893}
{"File Name":"pattern-atlas-ui\/0001-define-pattern-structure.md","Context":"## Context and Problem Statement\\nWhen creating and updating pattern languages and patterns, the structure should be defined so that users can implement their own renderers for a specific pattern language.\\n## Decision Drivers\\n* Pattern languages can be very different, so we should restrict their properties as little as possible.\\n","Decision":"* Pattern languages can be very different, so we should restrict their properties as little as possible.\\nA pattern consists of\\n* name (string), e.g. \"Elastic Infrastructure\"\\n* type (string), e.g. <https:\/\/purl.org\/patternpedia\/cloudcomputingpatterns#CloudComputingPattern>\\n* uri (string, e.g. <https:\/\/purl.org\/patternpedia\/cloudcomputingpatterns\/elasticinfrastructure#ElasticInfrastructure>)\\n* a Map for the section properties of its pattern language (Map<section, string | string[]>),\\ne.g. for the section  https:\/\/purl.org\/patternpedia\/cloudcomputingpatterns#hasLogo we can obtain the corresponding value  \"https:\/\/www.cloudcomputingpatterns.org\/img\/book.png\" or an array of strings\\nTo render the pattern properties best, section should contain information about the value type, e.g. xsd:anyURI, xsd:string. This will allow us to display properties like \"https:\/\/www.cloudcomputingpatterns.org\/img\/book.png\" as links\/pictures and not only as text.\\n","tokens":60,"id":347}
{"File Name":"platform\/2021-10-13-refund-handling.md","Context":"## Context\\nShopware offers no way of unified refund handling. This results in every payment extension either implementing it themselves or not at all.\\n","Decision":"We want to implement the following structure to offer a unified refund handling for all extension types.\\n","tokens":29,"id":4527}
{"File Name":"manage-frontend\/05-form-submission-via-server-side.md","Context":"## Context\\nForms that do not require a login to access could be submitted directly to the service layer and bypass the server-side of this project. However, there are trade-offs with that decision.\\nBenefits of not bypassing the server side:\\n- Is consistent with manage-frontend\u2019s standard approach\\n- Lambda endpoints remain secret\\n- Can use API key to validate requests at API Gateway level\\n- Cheaper in case of DDoS attack\\n- Easier to debug manage-frontend's logs VS API gateway's log\\n- Allows for more flexibility in case requirements change\\nBenefits of bypassing the server-side:\\n- Simpler to maintain as there are fewer parts\\n- Less latency due to 1 less hop (relatively negligible)\\n","Decision":"We will submit all forms through the server-side of this project.\\n","tokens":152,"id":3712}
{"File Name":"verify-service-provider\/0019-we-will-validate-assertion-details.md","Context":"## Context\\nAssertions are generated by the MSA, and they look like:\\n```\\n<saml2:Assertion ID=\"_95dc6950-68ff-4ad7-b062-c7ff44998386\" IssueInstant=\"2017-08-01T15:21:20.087Z\" Version=\"2.0\">\\n<saml2:Issuer Format=\"urn:oasis:names:tc:SAML:2.0:nameid-format:entity\">http:\/\/www.test-rp-ms.gov.uk\/SAML2\/MD<\/saml2:Issuer>\\n<ds:Signature>...<\/ds:Signature>\\n<saml2:Subject>\\n<saml2:NameID Format=\"urn:oasis:names:tc:SAML:2.0:nameid-format:persistent\">expected-pid<\/saml2:NameID>\\n<saml2:SubjectConfirmation Method=\"urn:oasis:names:tc:SAML:2.0:cm:bearer\">\\n<saml2:SubjectConfirmationData InResponseTo=\"request-id-386f467d-85c1-4c71-b3fc-cdc3739682b1\" NotOnOrAfter=\"2017-08-01T16:21:20.087Z\" Recipient=\"http:\/\/rp-endpoint.com\"\/>\\n<\/saml2:SubjectConfirmation>\\n<\/saml2:Subject>\\n<saml2:Conditions>\\n<saml2:AudienceRestriction>\\n<saml2:Audience>rp-entity<\/saml2:Audience>\\n<\/saml2:AudienceRestriction>\\n<\/saml2:Conditions>\\n<saml2:AuthnStatement AuthnInstant=\"2017-08-01T15:21:20.092Z\">\\n<saml2:AuthnContext>\\n<saml2:AuthnContextClassRef>urn:uk:gov:cabinet-office:tc:saml:authn-context:level2<\/saml2:AuthnContextClassRef>\\n<\/saml2:AuthnContext>\\n<\/saml2:AuthnStatement>\\n<\/saml2:Assertion>\\n```\\nElements such as `Conditions` specify restrictions on where the assertion\\nshould be treated as valid. Relying parties SHOULD validate these things.\\n","Decision":"We will validate any details that the MSA currently provides. If we see\\nelements we're not expecting we'll throw.\\nWe'll validate:\\n* There must be exactly one `SubjectConfirmation`\\n* There must be exactly one `NameID` inside `Subject`\\n* The `SubjectConfirmationData` method MUST be `...bearer`\\n* If there's a `NotBefore` on the `SubjectConfirmationData` then it MUST be valid\\n* There MUST be a `NotOnOrAfter` on the `SubjectConfirmationData` and it MUST be valid\\n* `InResponseTo` MUST be present on the `SubjectConfirmationData` and it MUST be valid\\n* `Conditions` MUST be present\\n* If `Conditions` has `NotBefore` or `NotOnOrAfter` they MUST be valid\\n* `Conditions` MUST NOT contain any `OneTimeUse` or `ProxyRestriction` elements\\n* There MUST be exactly one `AudienceRestriction` and it must match our `entityID`\\nWe'll ignore:\\n* Recipient - this concept is already checked by AudienceRestriction, and it's hard for the\\nVSP to check whether it's correct because we don't know what URL we're running behind.\\n","tokens":484,"id":4438}
{"File Name":"ipsec-tester\/0002-use-libpcap-and-libnet.md","Context":"## Context\\nThis project is about learning and debugging IPsec.\\nTherefore it has to analyse every received IPsec datagram by itself and can't rely on the OS kernel.\\nLikewise it has to craft every IPsec datagram it will send by itself.\\n","Decision":"Use *libpcap* to receive datagrams and *libnet* to send datagrams by the IPsec translator.\\n","tokens":55,"id":1848}
{"File Name":"alfresco-identity-service\/0001-Internal-JWT-Token-Details.md","Context":"## Context\\nIn order to achieve Single sign on process among the Digital Business Platform we need to have a common authentication process between Alfresco Process Service and Alfresco Content Service.\\nAn internal [JWT](https:\/\/jwt.io\/introduction) bearer token generated by Keycloak will be consumed by both Alfresco Content Service and Alfresco Process Service.\\nWe need to define the format of the payload of that internal JWT token.\\n","Decision":"We will adopt standard Keycloak's OpenID Connect JWT format.\\nThe user claim payload may contain the standard information as per the link below.\\nhttps:\/\/openid.net\/specs\/openid-connect-core-1_0.html#StandardClaims\\nBeside the above fields, we will also add to the mapper custom fields about group information for future use cases.\\nThe example below shows an encoded JWT:\\n```\\neyJhbGciOiJSUzI1NiIsInR5cCIgOiAiSldUIiwia2lkIiA6ICJ0ZE5mMWhDMzlDLW16dXdTLV9hcjF4MGJXYnVTRWxNd25TRGRDZUJPQW1jIn0.eyJqdGkiOiJkZDVlMDYyZC04MzlmLTQwNmEtOTJlMS1hMDlhYzFjY2NiMTciLCJleHAiOjE1MjEwMjI0NzQsIm5iZiI6MCwiaWF0IjoxNTIxMDIyMTc0LCJpc3MiOiJodHRwOi8vbG9jYWxob3N0OjgxODAvYXV0aC9yZWFsbXMvYWxmcmVzY28iLCJhdWQiOiJhbGZyZXNjbyIsInN1YiI6Ijc4NGNhMDI2LWNmYWEtNDIyYy04OGU4LWI3NTY1NTE1ZmY3MSIsInR5cCI6IkJlYXJlciIsImF6cCI6ImFsZnJlc2NvIiwiYXV0aF90aW1lIjowLCJzZXNzaW9uX3N0YXRlIjoiZGI2ZTk0NDctMjJhZi00ZjJlLTllZTEtMTRhNWM1OTVhMzg4IiwiYWNyIjoiMSIsImFsbG93ZWQtb3JpZ2lucyI6WyIiXSwicmVhbG1fYWNjZXNzIjp7InJvbGVzIjpbInVtYV9hdXRob3JpemF0aW9uIiwidXNlciJdfSwicmVzb3VyY2VfYWNjZXNzIjp7ImFjY291bnQiOnsicm9sZXMiOlsibWFuYWdlLWFjY291bnQiLCJtYW5hZ2UtYWNjb3VudC1saW5rcyIsInZpZXctcHJvZmlsZSJdfX0sIm5hbWUiOiJocnVzZXIgciIsInByZWZlcnJlZF91c2VybmFtZSI6ImhydXNlciIsImdpdmVuX25hbWUiOiJocnVzZXIiLCJmYW1pbHlfbmFtZSI6InIiLCJlbWFpbCI6ImhyQHRlc3QuY29tIn0.JBm33o07oD13mVNGB5FNdmMnWpAhWSaReGbLP88oj6LoLHf6DiuGGtmwuYP2qoRi-M4yBYLEqXMdDZ97_or8RbJXTfCZVX4AuUHa0J_P8AhEwu9Q3rxl7jV8RvUIqMe7IFl2Y5FVF8Tb1sxBS_WY4JBU2AM1d9mzf0E-_m29OxqIfdh3tgvQ4cCBVUtlzm8ilvAVdXPUxNvtIlNDVwFSPEyDDxAFHJ-7GCed8uOLlZhxGgpeuIQ4jn2VplxCatwIcbnArwLJe8XKfb2P4oVWJCWpvj_rFwtyUftLv2sPpyKZEgUTStxsn9Wgu8Iki-_ne43HKZGqBqlrMYrGAVoj9Q\\n```\\nThe JWT above when decoded produces the following JSON:\\nHEADER:\\n```json\\n{\\n\"alg\": \"RS256\",\\n\"typ\": \"JWT\",\\n\"kid\": \"tdNf1hC39C-mzuwS-_ar1x0bWbuSElMwnSDdCeBOAmc\"\\n}\\n```\\nPAYLOAD:\\n```json\\n{\\n\"jti\": \"dd5e062d-839f-406a-92e1-a09ac1cccb17\",\\n\"exp\": 1521022474,\\n\"nbf\": 0,\\n\"iat\": 1521022174,\\n\"iss\": \"http:\/\/localhost:8180\/auth\/realms\/alfresco\",\\n\"aud\": \"alfresco\",\\n\"sub\": \"784ca026-cfaa-422c-88e8-b7565515ff71\",\\n\"typ\": \"Bearer\",\\n\"azp\": \"alfresco\",\\n\"auth_time\": 0,\\n\"session_state\": \"db6e9447-22af-4f2e-9ee1-14a5c595a388\",\\n\"acr\": \"1\",\\n\"allowed-origins\": [\\n\"\"\\n],\\n\"realm_access\": {\\n\"roles\": [\\n\"uma_authorization\",\\n\"user\"\\n]\\n},\\n\"resource_access\": {\\n\"account\": {\\n\"roles\": [\\n\"manage-account\",\\n\"manage-account-links\",\\n\"view-profile\"\\n]\\n}\\n},\\n\"name\": \"hruser r\",\\n\"preferred_username\": \"hruser\",\\n\"given_name\": \"hruser\",\\n\"family_name\": \"r\",\\n\"email\": \"hr@test.com\"\\n}\\n```\\n### Unique User ID\\nFor common identity across the Alfresco platform, the combination of the issuer (`iss`) and subject (`sub`) should generally be considered the unchanging unique identifier for a user (as stated by [section 2 of the OpenID Connect Core 1.0 specification](http:\/\/openid.net\/specs\/openid-connect-core-1_0.html#IDToken)).\\nNote that the issuer is a URL, which may change after an initial deployment in an organization, so our common identity work should include the ability for an organization to change their issuer URL.  Alternatively, we could at that time choose to loosen adherence to the specification and declare that only the subject is required for a unique user ID within the platform.\\nUntil components are updated to handle common identity they may chose to rely on the `preferred_username` or `email` claims.\\n","tokens":86,"id":4216}
{"File Name":"nr-arch\/2020-03-14-ARCH-record-architecture-decisions-detailed-template.md","Context":"## Context\\nThe issue motivatina# [short title of solved problem and solution]\\nWhat is the issue that we're seeing that is motivating this decision or change?\\n* Status: [proposed | rejected | accepted | deprecated | ... | superseded by [ADR-0005](0005-example.md)] <!-- optional -->\\n* Deciders: [list everyone involved in the decision] <!-- optional -->\\n* Date: [YYYY-MM-DD when the decision was last updated] <!-- optional -->\\nTechnical Story: [description | ticket\/issue URL] <!-- optional -->\\n## Context and Problem Statement\\n[Describe the context and problem statement, e.g., in free form using two to three sentences. You may want to articulate the problem in form of a question.]\\n## Decision Drivers <!-- optional -->\\n* [driver 1, e.g., a force, facing concern, ...]\\n* [driver 2, e.g., a force, facing concern, ...]\\n* ... <!-- numbers of drivers can vary -->\\n","Decision":"* [driver 1, e.g., a force, facing concern, ...]\\n* [driver 2, e.g., a force, facing concern, ...]\\n* ... <!-- numbers of drivers can vary -->\\nChosen option: \"[option 1]\", because [justification. e.g., only option, which meets k.o. criterion decision driver | which resolves force force | ... | comes out best (see below)].\\n### Positive Consequences <!-- optional -->\\n* [e.g., improvement of quality attribute satisfaction, follow-up decisions required, ...]\\n* ...\\n### Negative Consequences <!-- optional -->\\n* [e.g., compromising quality attribute, follow-up decisions required, ...]\\n* ...\\nWhat is the change that we're proposing and\/or doing?\\nThe change that we're proposing or have agreed to implement.\\n","tokens":218,"id":4800}
{"File Name":"gti-genesearch\/adr-003.md","Context":"## Context\\nQuery and QueryOutput example for simple query:\\n```\\nq={\"name\":\"BRCA2\"}\\nfields=[\"id\",\"name\",\"genome\"]\\n```\\nQuery and QueryOutput example for transcript join:\\n```\\nq={\"name\":\"BRCA2\"}\\nfields=[\"id\",\"name\",\"genome\",{\"transcripts\":[\"id\",\"seq_region_start\"]}]\\n```\\nQuery and QueryOutput example for sequence join:\\n```\\nq={\"name\":\"BRCA2\",\"sequences\":{\"type\":\"protein\"}}\\nfields=[{\"sequences\":[\"id\",\"desc\",\"seq\"]}]\\n```\\nOne consideration is whether searches should need to have the default search type specified in the queries and fields e.g.\\n```\\nq={\"genes\":{{\"name\":\"BRCA2\"}}\\nfields={\"genes\":[\"id\",\"name\",\"genome\"]}\\n```\\nThe best idea here is to use different endpoints for different searches e.g. `\/api\/genes` to get genes, `\/api\/variations` to get variations etc.\\n","Decision":"We have decided to remove target and targetQueries from the interface as they will rapidly become unsustainable as the range of joined queries increases. At present, the \"default\" search will not require a separate section.\\n","tokens":219,"id":3271}
{"File Name":"cena\/0006-manage-build-with-gradle.md","Context":"## Context\\n`menu-generation` application will use the Spring framework along with other third party libraries, thus requires a\\ndependency management tool.\\nEffective development lifecycle requires Continuous Integration, thus a build management tool is necessary.\\n[Gradle](https:\/\/gradle.org\/) is one of the two main build management tools for the Java ecosystem with [Maven](http:\/\/maven.apache.org\/).\\nGradle is considered as more extensive and quicker than Maven. It is also well integrated with many tools, including\\nthe Spring framework.\\n","Decision":"Gradle will be used to manage project dependencies and build tasks.\\n","tokens":111,"id":644}
{"File Name":"delayer-aws\/0001-select-proper-name-for-the-project.md","Context":"## Context and Problem Statement\\nEven this is not essentially a technical decision, I believe that this \"simple\"\\nname probably will drive some other important decisions, like the creation of\\nterminologies, naming packages and other stuff, helping defining\\nfunctionalities, etc. According to [Martin Fowler](https:\/\/martinfowler.com\/bliki\/TwoHardThings.html)\\nnaming things figures out as one of the hardest things in computer science.\\nApart of a joke, I believe that choose a good name is a good success factor,\\nbut this is not a technical decision, which can make things harder.\\n## Decision Drivers\\n*   Consider the serverless nature of the project\\n*   Consider the simplicity\\n*   Consider the vendor specific nature\\n*   Consider the fact that this could not be a scheduler\\n","Decision":"*   Consider the serverless nature of the project\\n*   Consider the simplicity\\n*   Consider the vendor specific nature\\n*   Consider the fact that this could not be a scheduler\\nChosen option: \"delayer-aws\".\\nAll the conception and idea behind of this project was made over\\nthe \"scheduler\" word, which sounds natural, since the main purpose of the project is to *schedule* a\\ntask to run in the future. However, the term \"scheduler\" reemsemble a lot of things that already are\\nin place, like *recurrence* e *orchestration*. The main objective of this project is to provide a way\\nto easily and cost-effectively execute tasks in future. Easily because it will be based in a Restful\\nAPI to deal with tasks (in face of config files or configuration screen), and cost-effective by using\\nserveless architectural approach. This is very different of current set of tooling, which could be\\ncalled \"schedulers\" too (and actually are!), but do a LOT of thing other then this, like but not\\nlimited to, recurrence and orchestration.\\nIn summary, I would like to think that people would find here more a \"system-to-system-todo-list\"\\nthan in a complete \"scheduler\". That's why I think that \"delayer\" reflects much more what it will do\\nthen \"scheduler\".\\nThe suffix \"-aws\" aims to:\\n-   confirm that it is not multicloud\\n-   confirm that there are plans to build this for other cloud providers\\n### Positive Consequences\\n*   clearly defines what this project do, and what it will do for the next iteractions\\n### Negative consequences\\n*   not using \"scheduler\" in the name can reduce the comprehensiveness about the project\\n","tokens":169,"id":2788}
{"File Name":"pfb-network-connectivity\/adr-0003-asynchronous-task-queue.md","Context":"## Context\\nThe key component of this project is a 'Bicycle Network Analysis' task which is run on an arbitrary, user-provided neighborhood boundary. This task performs the following actions:\\n- Import neighborhood boundary into a PostgreSQL database\\n- Download OSM extract for the provided neighborhood boundary + a buffer and import to PostgreSQL\\n- Download related census block and job data for the boundary and import to PostgreSQL\\n- Generate a network graph from the imported data\\n- Run a series of client-provided analyses on the graph and imported data\\n- Export relevant data to an external file store for archival\\n- Generate a tile set of the network graph for display on a web map\\nThe application will be configured with multiple organizations, and each organization can only run one analysis job at a time. A new analysis job triggered by a user of the organization will supersede any existing older analysis, which can be thrown away.\\nSince the analysis workflow is already a self-contained process, there are a few ways to trigger this job, and a few options for an asynchronous task queue. One option is to use Celery, a tool we are familiar with, to provide a known interface to trigger these analysis jobs. Another is to configure the analysis as an AWS ECS task, and have the application use the ECS API or Boto to start a new analysis.\\nCelery has multiple options for brokers:\\n| Broker | Advantages | Disadvantages |\\n| ------ | ---------- | ------------- |\\n| SQS | Cheap, easy to set up, now stable, provides configuration options to isolate environments | No result backend, [potential issues with result timeouts](http:\/\/docs.celeryproject.org\/en\/latest\/getting-started\/brokers\/sqs.html#caveats) |\\n| Redis | Trivial to configure, can additionally be used as a results backend without further architecting | Key eviction issues, additional cost to run dedicated instance |\\nRunning the analysis via AWS Lambda was briefly considered, but the project dependencies and resources required are not conducive to that environment.\\n","Decision":"The team will use Celery + SQS broker to manage the asynchronous analysis jobs. While Celery is not strictly necssary, it provides a potentially useful abstraction layer for triggering tasks, managing jobs and reporting errors. Celery also provides out of the box support for Django and allows us to write any peripheral task logic in Python. The SQS broker was chosen to keep the managed application architecture simple and reduce ongoing application stack costs. The team is familiar with an older version of the SQS broker used for the Cicero District Match project.\\n","tokens":415,"id":709}
{"File Name":"functionaut\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":2690}
{"File Name":"agentframework\/0003-performance-decisions.md","Context":"## Context\\n|  | Fastest | Faster  | Same    | Slower  | Slowest |\\n| ------- | ------- | ------- | ------- | ------- | ------- |\\n| Access prototype | closure | `Object.getPrototypeOf` | `Reflect.getPrototypeOf` | `__proto__` |  |\\n| Access property | `object[key]` | `Reflect.has` | `Reflect.get` |  |  |\\n| Create proxy |  |  | Create function, Create class |  |  |\\n","Decision":"Use closure to replace Object.getPrototypeOf\\nUse direct access to replace `Reflect.get`\\n","tokens":108,"id":2366}
{"File Name":"akka-monitor\/0002-use-akka.md","Context":"## Context\\nWe need to be able to monitor network services separately.\\n","Decision":"We will use _akka_ for managing the task to monitor a particular network service.\\n","tokens":15,"id":1624}
{"File Name":"community\/0006-project-naming.md","Context":"## [Context](https:\/\/docs.google.com\/document\/d\/1ayJlohp2-s49c1_oypDE1bAfgYa5shR0I1GM6KRzDNA\/edit)\\nLibero projects may span a single product or possibly more, but they live in a single namespace of the [Libero Github organization](https:\/\/github.com\/libero). Hence the projects need to:\\n- be attributable to the originating product\\n- avoid clashes in naming (e.g. many `article-store` projects)\\n","Decision":"- Name repositories that cover multiple products without prefixes (e.g. `infrastructure`, `community`)\\n- Name internal and\/or product-specific repositories with prefix for specific product (e.g. `reviewer-submission`, `my-product-storybook`)\\n- Name generic repositories with no prefix (e.g. `media-type`)\\n- Name forked repositories with the original name, no matter how bad (e.g. [`elifelibero-avada-child-theme`](https:\/\/github.com\/libero\/elifelibero-avada-child-theme), [`texture`](https:\/\/github.com\/libero\/texture))\\n- Name Docker images following Github repository names (e.g. [`reviewer-submission`](https:\/\/hub.docker.com\/r\/liberoadmin\/reviewer-submission))\\n- Name NPM packages using `@libero` followed by Github repository names (e.g. [`@libero\/event-bus`](https:\/\/www.npmjs.com\/package\/@libero\/event-bus)\\n","tokens":110,"id":3465}
{"File Name":"arch\/0033-capacity-evaluation-mysql.md","Context":"## Context\\nQPS: \u6bcf\u79d2\u949f\u67e5\u8be2\u91cf\u3002\u5982\u679c\u6bcf\u79d2\u949f\u80fd\u5904\u7406 100 \u6761\u67e5\u8be2 SQL \u8bed\u53e5\uff0c\u90a3\u4e48 QPS \u5c31\u7ea6\u7b49\u4e8e 100\\nTPS: \u6bcf\u79d2\u949f\u4e8b\u52a1\u5904\u7406\u7684\u6570\u91cf\\n","Decision":"### \u548c MySQL \u6027\u80fd\u76f8\u5173\u7684\u51e0\u4e2a\u6307\u6807\\n* CPU\\n* \u5bf9\u4e8e CPU \u5bc6\u96c6\u578b\u7684\u5e94\u7528\uff0c\u6211\u4eec\u9700\u8981\u52a0\u5feb SQL \u8bed\u53e5\u7684\u5904\u7406\u901f\u5ea6\u3002\u7531\u4e8eMySQL \u7684 SQL \u8bed\u53e5\u5904\u7406\u662f\u5355\u7ebf\u7a0b\u7684\uff0c\u56e0\u6b64\u6211\u4eec\u9700\u8981\u66f4\u597d\u7684 CPU\uff0c\u800c\u4e0d\u662f\u66f4\u591a\u7684cpu\\n* \u4e00\u4e2a CPU \u540c\u65f6\u53ea\u80fd\u5904\u7406\u4e00\u6761 SQL \u8bed\u53e5\u3002\u6240\u4ee5\u9ad8\u5e76\u53d1\u91cf\u7684\u60c5\u51b5\u4e0b\uff0c\u5c31\u9700\u8981\u66f4\u591a\u7684 CPU \u800c\u4e0d\u662f\u66f4\u5feb\u7684 CPU\u3002\\n* \u5185\u5b58\\n* \u628a\u6570\u636e\u7f13\u5b58\u5230\u5185\u5b58\u4e2d\u8bfb\u53d6\uff0c\u53ef\u4ee5\u5927\u5927\u63d0\u9ad8\u6027\u80fd\u3002\u5e38\u7528\u7684 MySQL \u5f15\u64ce\u4e2d\uff0cMyISAM \u628a\u7d22\u5f15\u7f13\u5b58\u5230\u5185\u5b58\uff0c\u6570\u636e\u4e0d\u7f13\u5b58\u3002\u800cInnoDB \u540c\u65f6\u7f13\u5b58\u6570\u636e\u548c\u7d22\u5f15\\n* \u78c1\u76d8\\n* MySQL \u5bf9\u78c1\u76d8 IO \u7684\u8981\u6c42\u6bd4\u8f83\u9ad8\uff0c\u63a8\u8350\u9ad8\u6027\u80fd\u78c1\u76d8\u8bbe\u5907\uff0c\u6bd4\u5982\uff0cAliyun \u5c31\u63a8\u8350\u4f7f\u7528\u5176 SSD\uff0c\u800c\u4e0d\u662f\u666e\u901a\u4e91\u76d8\u548c\u9ad8\u6548\u4e91\u76d8\uff0c\u5f53\u7136 SSD \u8fd9\u4e2a\u4ecb\u8d28\u7684\u53ef\u9760\u6027\uff0c\u6211\u4eec\u4e5f\u9700\u8981\u8003\u8651\\n* MySQL \u5b9e\u4f8b\u5e94\u8be5\u72ec\u4eab\uff0c\u5c24\u5176\u4e0d\u80fd\u548c\u5176\u4ed6\u78c1\u76d8 IO \u5bc6\u96c6\u578b\u4efb\u52a1\u653e\u5728\u4e00\u8d77\\n* \u7f51\u7edc\\n* \u4e00\u822c\u6570\u636e\u5e93\u670d\u52a1\u90fd\u90e8\u7f72\u5728\u5185\u7f51\uff0c\u5e26\u5bbd\u5f71\u54cd\u4e0d\u5927\uff0c\u907f\u514d\u4f7f\u7528\u5916\u7f51\u8fde\u63a5\u6570\u636e\u5e93\\n* \u5916\u7f51\u67e5\u8be2\uff0c\u5c24\u5176\u907f\u514d\u4f7f\u7528 `select *` \u8fdb\u884c\u67e5\u8be2\\n* \u4ece\u5e93\u5982\u679c\u8de8\u5730\u57df\uff0c\u8d70\u5916\u7f51\uff0c\u5219\u5c3d\u91cf\u51cf\u5c11\u4ece\u5e93\u6570\u91cf\u3002\\n### \u5f71\u54cd\u6570\u636e\u5e93\u6027\u80fd\u7684\u56e0\u7d20\u53ca\u5e94\u5bf9\u65b9\u5f0f\\n* \u9ad8\u5e76\u53d1\\n* \u6570\u636e\u5e93\u8fde\u63a5\u6570\u88ab\u5360\u6ee1\\n* \u5bf9\u4e8e\u6570\u636e\u5e93\u800c\u8a00\uff0c\u6240\u80fd\u5efa\u7acb\u7684\u8fde\u63a5\u6570\u662f\u6709\u9650\u7684\uff0cMySQL \u4e2d`max_connections` \u53c2\u6570\u9ed8\u8ba4\u503c\u662f 100\\n* \u5927\u8868\\n* \u8bb0\u5f55\u884c\u6570\u5de8\u5927\uff0c\u5355\u8868\u8d85\u8fc7\u5343\u4e07\u884c\\n* \u8868\u6570\u636e\u6587\u4ef6\u5de8\u5927\uff0c\u8868\u6570\u636e\u6587\u4ef6\u8d85\u8fc710G\\n* \u5f71\u54cd\\n* \u6162\u67e5\u8be2\uff1a\u5f88\u96be\u5728\u4e00\u5b9a\u7684\u65f6\u95f4\u5185\u8fc7\u6ee4\u51fa\u6240\u9700\u8981\u7684\u6570\u636e\\n* DDL \u64cd\u4f5c\u3001\u5efa\u7acb\u7d22\u5f15\u9700\u8981\u5f88\u957f\u7684\u65f6\u95f4\uff1aMySQL \u7248\u672c \\<5.5 \u5efa\u7acb\u7d22\u5f15\u4f1a\u9501\u8868\uff0c\\>=5.5 \u867d\u7136\u4e0d\u4f1a\u9501\u8868\u4f46\u4f1a\u5f15\u8d77\u4e3b\u4ece\u5ef6\u8fdf\\n* \u4fee\u6539\u8868\u7ed3\u6784\u9700\u8981\u5f88\u957f\u65f6\u95f4\u9501\u8868\uff1a\u4f1a\u9020\u6210\u957f\u65f6\u95f4\u7684\u4e3b\u4ece\u5ef6\u8fdf\uff1b\u5f71\u54cd\u6b63\u5e38\u7684\u6570\u636e\u64cd\u4f5c\\n* \u5904\u7406\\n* \u5206\u5e93\u5206\u8868\\n* \u5386\u53f2\u6570\u636e\u5f52\u6863\\n* \u5927\u4e8b\u52a1\\n* \u8fd0\u884c\u65f6\u95f4\u6bd4\u8f83\u957f\uff0c\u64cd\u4f5c\u6570\u636e\u6bd4\u8f83\u591a\u7684\u4e8b\u52a1\\n* \u5f71\u54cd\\n* \u9501\u5b9a\u592a\u591a\u6570\u636e\uff0c\u9020\u6210\u5927\u91cf\u7684\u963b\u585e\u548c\u9501\u8d85\u65f6\\n* \u56de\u6eda\u65f6\u9700\u8981\u7684\u65f6\u95f4\u6bd4\u8f83\u957f\\n* \u6267\u884c\u65f6\u95f4\u957f\uff0c\u5bb9\u6613\u9020\u6210\u4e3b\u4ece\u5ef6\u8fdf\\n* \u5904\u7406\\n* \u907f\u514d\u4e00\u6b21\u5904\u7406\u592a\u591a\u7684\u6570\u636e\\n* \u79fb\u51fa\u4e0d\u5fc5\u8981\u5728\u4e8b\u52a1\u4e2d\u7684select\u64cd\u4f5c\\n","tokens":58,"id":2431}
{"File Name":"verify-onboarding-prototypes\/0002-build-a-prototype.md","Context":"## Context\\nWe need to make sure that whatever we build meets the users' needs. To make sure of this we need to\\nconduct some user research, which should involve putting software in front of users and observing them using it.\\n","Decision":"We will build a simple \"prototype\" which we will use to test our assumptions about whether our proposed\\nsolution is the best way of meeting our users needs.\\nThe prototype will be architecturally similar to the product we envisage building, but won't be able to\\ndo the SAML interactions with Verify at this stage.\\nADRs for the first prototype will live in the [prototype-0](prototype-0) directory.\\n","tokens":48,"id":576}
{"File Name":"Wikibase\/0001-use-psr-16-cache-interface.md","Context":"## Context\\nWikibase uses cache in different parts of PHP code base. Wikibase itself does not provide any abstraction for cache implementation. Instead MediaWiki-specific classes, such as abstract `BagOStuff`, or `WANObjectCache`, are used in code.\\nThe [PSR-16] standard defines an abstract Simple Cache interface for use in PHP code.\\nMore formal and verbose Caching interface has been defined as [PSR-6].\\nWe find it overly verbose compared to simpler [PSR-16], which we favour here.\\nPossible use of [PSR-16] in MediaWiki is out of scope of this decision.\\nIt might be a possible next step.\\nIn this regard it is possibly worth mentioning that using [PSR-6] interface in MediaWiki has been proposed in 2016, but it has been declined (see: [T130528]).\\nIt should also be noted that there is already a PSR-6 adapter for the `BagOStuff`: https:\/\/packagist.org\/packages\/addshore\/psr-6-mediawiki-bagostuff-adapter.\\nThere are already adapters that can use a PSR-16 cache as a PSR-6 cache and vice versa: https:\/\/symfony.com\/doc\/current\/components\/cache\/psr6_psr16_adapters.html.\\nAlso, as `Psr\\Cache\\CacheItemPoolInterface` and `Psr\\SimpleCache\\CacheInterface` declare only one method of the same name, `clear()`, and as both declarations have the same signature, any cache system may implement both interfaces.\\n","Decision":"We will introduce PSR-16-compliant cache interface to Wikibase.\\nNew PHP code using cache will use this abstraction, instead of binding directly to MediaWiki cache classes, or any other specific third-party implementations.\\nWe will use [psr\/simple-cache] library to add `CacheInterface` to Wikibase.\\n","tokens":332,"id":1349}
{"File Name":"govuk-aws\/0033-ip-ranges.md","Context":"## Context\\nThis ADR in part supersedes [0003](0003-aws-networking-outline.md).\\nIn [ADR 0003](0003-aws-networking-outline.md), we specified\\nsome IP address ranges for staging and production - ones that matched\\nCarrenza as far as possible.\\nAs part of the gradual migration of Staging and Production to AWS, we\\nneed to use a VPN to talk to Carrenza, so the chosen IP addresses conflict.\\n","Decision":"The VPCs will be assigned the following IP ranges:\\n|Environment|IP Range|\\n|-----------|--------|\\n|Integration|10.1.0.0\/16|\\n|Staging|10.12.0.0\/16|\\n|Production|10.13.0.0\/16|\\n|Test|10.200.0.0\/16|\\nThat is:\\n- Integration and Test do not change.\\n- Staging moves from 10.2.0.0\/16 to 10.12.0.0\/16.\\n- Production moves from 10.3.0.0\/16 to 10.13.0.0\/16.\\n","tokens":102,"id":4027}
{"File Name":"TANF-app\/014-dependabot-dependency-management.md","Context":"## Context\\nCurrently our Snyk configuration is configured on the carltonsmith Snyk organization which will no longer work after @carltonsmith leaves the project.\\nAdditionally, these Snyk PRs require us to manage an unnecessary requirements.txt file in addition to our Pipfile and python dependency update PRs are not complete when they get opened since they don't update anything in our actual dependencies.\\nFurthermore, we currently use the Dependabot Preview app which is being deprecated in favor of a GitHub Native Dependabot which has more features and is configured via a YAML file committed to the repo.\\n","Decision":"Rather than setting up Snyk on a new organization and in order to get ahead on the impending Dependabot migration, we propose an update which provides the necessary YAML config to enable the new GitHub Native version of Dependabot.\\n","tokens":126,"id":3791}
{"File Name":"FlowKit\/0010-prefect-for-autoflow.md","Context":"## Context\\nThe first prototype of AutoFlow used [Apache Airflow](https:\/\/airflow.apache.org\/) (as used in FlowETL) to define and execute workflows. However, this proved to be problematic in some respects - Airflow has limited support for parametrising DAG runs and sharing data between tasks, and re-running a DAG for an execution date for which it has already run is complicated.\\n[Prefect Core](https:\/\/docs.prefect.io\/) is an alternative open-source workflow engine, which allows DAGs to be parametrised and run simultaneously for multiple sets of parameters, and allows data exchange between tasks. Prefect also allows the creation of dynamically-generated tasks mapped over the outputs from running another task, which makes it easier for AutoFlow to spawn multiple runs of a workflow when the sensor finds multiple days of data for which the workflow has not previously run.\\n","Decision":"AutoFlow will use Prefect to define and run workflows.\\n","tokens":179,"id":5053}
{"File Name":"news\/0003-integration-tests-and-ports.md","Context":"## Context\\nThere are two ways I can currently see of doing this, one is building stubs into bundles, the other is serialization of port functions\\nin the test:\\n```js\\nasync supplyPorts(ports = {}) {\\nthis._page = await this.page();\\nawait this._page.evaluate(fun => {\\ncore.queryWith(() => eval(fun)());\\ncore.onSaved(e => console.log(`[ACTION.SAVED]`));\\nwindow.console.log(`Reset hacker news port and added action listener`);\\ncore.news();\\n}, ports.top.toString());\\n}\\n```\\nNote the `ports.top.toString()`, this is serializing a function.\\nThis still leaves us disconnected, though -- we can't then use it as a mock because it does not share memory with the test.\\nThe function has been serialized and sent across the wire.\\n","Decision":"The change that we're proposing or have agreed to implement.\\n","tokens":182,"id":4320}
{"File Name":"Wikibase\/0007-use-vuex-smart-modules.md","Context":"## Context\\nThe Wikidata Bridge app uses VueJs version 2 and Vuex version 3 for state management.\\nBoth within the store and in vue components the calls to `dispatch()` and `commit()` are not type safe.\\nThat means that they accept any arguments and TypeScript will still compile without error.\\nSince `dispatch()` and `commit()` are typical seams that are usually mocked during unit testing,\\nit is up to integration, end-to-end and browser tests to detect these errors.\\nThis is particularly unfortunate as the store is one of the central locations where business logic happens.\\nWe considered two options:\\n1. writing our own set of wrappers for `dispatch()` and `commit()` to get type safety\\n1. using vuex-smart-modules\\nThe advantages of doing it ourselves included us not having another dependency.\\nThe risks include that this would be yet another homebrew layer of abstraction.\\nThe advantages of using vuex-smart-modules include:\\n- it gives us proper native type safety both in the store and in components\\n- we can call actions\/getters\/mutations with [method style access](https:\/\/github.com\/ktsn\/vuex-smart-module#method-style-access-for-actions-and-mutations)\\n- we can get rid of all the `BRIDGE_SET_TARGET_VALUE` constants without having to fall back to string literals\\n- we can actually use the IDE's `go to method definition` functionality\\n- we can rely on the action's return type instead of dispatch's `Promise<any>` being used everywhere\\n- we can drop the `vuex-class` dependency as we can use vuex-smart-modules for all store access in components\\n- it is developed by a VueJs core contributor\\nThe risks include:\\n- it is still a very new project with a 0.x.y version number\\n- it is another layer on top of vuex, which means the documentation may not be as good as it could be\\n- we are the first big project to use it\\n- mocking of dependencies and nested modules in testing still seems to be not handled as diligently as one would wish\\n","Decision":"We decided to rewrite our Vuex store using vuex-smart-modules version 0.3.4\\n","tokens":432,"id":1345}
{"File Name":"modular-monolith-with-ddd\/0013-protect-business-invariants-using-exceptions.md","Context":"## Context\\nAggregates should check business invariants. When the invariant is broken, we should stop processing and return an error immediately to the client.\\n","Decision":"Solution number 1 - Use exceptions. <\/br>\\nPerformance cost of throwing an exception is irrelevant, we don't want too many if\/else statements in entities, more familiar with exceptions approach.\\n","tokens":32,"id":895}
{"File Name":"teacher-training-api\/0006-controller-structure.md","Context":"## Context\\nWe cannot design endpoints in a flat structure due to existing requirements around the Find service. By flat structure, we're referring to just having endpoints like:\\n- `\/providers`\\n- `\/courses`\\nInstead, we have to design endpoints in a nested structure scoped to their parent resources, ie:\\n- `\/recruitment_cycles\/:recruitment_cycle_year\/providers\/:provider_code`\\n- `\/recruitment_cycles\/:recruitment_cycle_year\/providers\/:provider_code\/courses\/:course_code`.\\nIf we rely on having single controllers to deal with resources which may be nested under other resources, we'll end up mixing concerns\/responsibilities of different contraints into one class which will make it difficult to maintain over the long term and less flexible to change. It also violates the single responsibility principle as we end up bloating the class.\\n","Decision":"We have decided to go with option 2 based on the number of benefits we will gain given our current requirements.\\n","tokens":172,"id":2351}
{"File Name":"link_platform\/0015-use-storybook.md","Context":"## Context\\nWe are starting to develop multiple components for `link_admin`, and the idea of\\nusing some kind of component development environment to provide structure and\\nincreased productivity to this endeavor was broached by Mr. Jacob Pandl.\\n### Why Component Explorers?\\nAdding a component explorer to our project and including it in our workflow would\\nresult in a couple of key benefits. Component explorers allow engineers to build\\nmodular UIs in isolation of the app's business logic, increasing interchangability\\nand component reuse potential. They also allow for easier parallel production, as\\ndevelopers can work on different pieces of UI without state pollution. Using component\\nexplorers would also help with component durability, as they allow a developer to\\nmitigate inconsistency by being able to test many states of the application, in particular those that can be difficult to replicate through mocking. Finally,\\ncomponet explorers allow developers to create easily sharable artifacts which can be\\nshared with PR reviewers and other stakeholders.\\n### Why Storybook Instead of Styleguidist?\\nIt seems that while Storybook and Styleguidist both have similar toolsets, Storybook\\nwill better suit our needs. Storybook is a workshop application, meaning that it is\\ndesigned to allow a developer to create UI components in isolation, mock state, data\\nand adjust props. It was also one of the first tools for UI components, which means\\nit comes with a good deal of maturity and momentum. Styleguidist, meanwhile, seems\\nto be more of a documentation tool for UI, creating pages in markdown and importing\\nUI components.\\n","Decision":"For the purposes of developing UI components in isolation and speeding up\\ndevelopment, Storybook seems like the right choice. While Styleguidist would be a\\nuseful tool, Storybook is the right choice when it comes to development.\\n","tokens":337,"id":5029}
{"File Name":"toc-poc\/1577576739261_infrastructure_choice_for_deployment.md","Context":"## Context\\nThe final effect of the POC will be static HTML page, with css and javascripts included in the file. Therefore, it will be easy to host it online for demonstration purposes, with configuration for multiple environments.\\n","Decision":"Use S3 static webpage hosting as deployment infrastructure.\\n","tokens":47,"id":1885}
{"File Name":"dogma\/0009-immutable-keys.md","Context":"## Context\\nEngine implementations require a mechanism for associating ancillary data with\\nDogma applications and handlers.\\nFor example, such data might include application state in the form of aggregate\\nand process roots, or historical events in an event sourcing system.\\nCurrently, engine implementations rely on application and handler names as a key\\nfor associated data. This is especially problematic for handlers as the name\\ninitially chosen for a handler may become misleading over time as the handler's\\nimplementation changes.\\n","Decision":"We've decided to add an additional identifier to applications and handlers\\ncalled the \"key\".\\nThe key's express purpose is for identifying associated data, and therefore has\\nmore stringent requirements on its immutability than the name.\\nWe further recommend the use of an RFC 4122 UUID as the format of all keys.\\nUUIDs can be generated at the time the application or handler is first\\nimplemented. Many IDEs support generation of UUIDs.\\nApplications and handlers retain their names as a human-readable identifier.\\n","tokens":100,"id":1607}
{"File Name":"register-a-food-business-service\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":1812}
{"File Name":"front-end-monorepo\/adr-32.md","Context":"## Context\\nCurrently, the FEM Classifier's default \"Subject Viewer sizing\/fitting\" behaviour is _\"fit to width, no max height\"._\\nThis is analogous to PFE's special \"no-max-height\" behaviour, which had to be _explicitly set._\\nWhile the default \"fit to width, no max height\" behaviour is good in many cases, there can be issues for certain workflows, e.g. when the Subject is a tall image (such as a portrait photo) and the user's window is very wide, causing the bottom half of the image to be \"cut off\".\\nWe need to consider how to size\/fit viewers for a variety of Subjects and Subject Viewers.\\n","Decision":"The answer is to use **Layouts**.\\n- The plan for FEM is that every workflow should be able to _set their own Layout_ (e.g. portrait, landscape, fullscreen, etc) each with their own Subject Viewer sizing\/fitting behaviour.\\n- The choice of Layout (presumably per workflow) should be controllable by the project owner.\\n- Whether or not a specific subject viewer size configuration is supported will depend on the Layout.\\nSee also:\\n- The current [Layout code](..\/..\/packages\/lib-classifier\/src\/components\/Classifier\/components\/Layout), as of June 2021, currently only has DefaultLayout.\\n","tokens":149,"id":522}
{"File Name":"eq-questionnaire-runner\/0002-refactor-functional-tests-into-surveys-components-and-features.md","Context":"## Context\\nThe functional tests should be organised in such a way that encourages consistent testing of components and coverage of all features of survey runner.\\n","Decision":"- Organise test specifications (.spec.js files) under a surveys, components or features folder:\\n```\\ntests\/\\nfunctional\/\\nspec\/\\nsurveys\/\\ncensus-household.spec.js\\nmci.spec.js\\n...\\ncomponents\/\\nradio.spec.js\\ncheckbox.spec.js\\n...\\nfeatures\/\\nnavigation.spec.js\\nsave-and-resume.spec.js\\n...\\n```\\n- Create a consistent set of tests for each component e.g. TextField, Currency, Dates etc. Each component should test:\\n- Optional\/mandatory\\n- Saved data is persisted\\n- Summary screen content\\n- Summary screen updates after answer changes\\n- Component specific tests e.g. other field on radio\/checkbox answers\\n- Create a set of tests for each feature\\n- Final confirmation\/Summary\\n- Navigation\\n- Save and Resume\\n- Timeout\\n- Routing\\n- Language\\n- One spec file for each survey, component or feature\\n- The JSON schemas will remain in the data directory so they are accessible by both users of the service and the functional tests.\\n- The component level schemas will be named `component_<component_name>.json` and feature schemas will be named `feature_<feature_name>.json`. This will help group related schemas together.\\n","tokens":29,"id":3671}
{"File Name":"documentation\/0002-use-sha256-with-base64url-encoding.md","Context":"## Context and Problem Statement\\nWe have to transfer json data and verify the integrity of the data.\\nThe transfer involves an Authorization server which provides the json, a client which gets the data form that server and pass it to the WebSocket API.\\nThe WebSocket API must able to verify the integrity of the json data.\\n## Decision Drivers <!-- optional -->\\n* Use standard encodings\\n","Decision":"* Use standard encodings\\nChosen option: \"Send SHA256 hash of Base64Url encoded json\", because this method is platform independent and not much session state is required.\\n### Positive Consequences <!-- optional -->\\n* The JWT really function as a verification token for the other requests.\\n* Can be applied to all json data that must be verified.\\n### Negative Consequences <!-- optional -->\\n* The json must be transferred in Base64Url encoding\\n","tokens":77,"id":413}
{"File Name":"mbed-tools\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":4476}
{"File Name":"Nosedive\/0004-start-with-console-app.md","Context":"## Context\\nTe console appication it is the more siple app that come to my mind.\\n","Decision":"Creates a console app and send the data via parameters\\n","tokens":21,"id":106}
{"File Name":"NorthwindCore\/0003-arm-templates.md","Context":"## Context\\nApplication will be hosted in Azure & validated after each pull request to master\/main branch.\\n","Decision":"ARM template will be created to quickly create test environment to execute all unit tests.\\n","tokens":21,"id":1793}
{"File Name":"james\/0009-disable-elasticsearch-dynamic-mapping.md","Context":"## Context\\nWe rely on dynamic mappings to expose our mail headers as a JSON map. Dynamic mapping is enabled for adding not yet encountered headers in the mapping.\\nThis causes a serie of functional issues:\\n- Maximum field count can easily be exceeded\\n- Field type 'guess' can be wrong, leading to subsequent headers omissions [1]\\n- Document indexation needs to be paused at the index level during mapping changes to avoid concurrent changes, impacting negatively performance.\\n","Decision":"Rely on nested objects to represent mail headers within a mapping\\n","tokens":96,"id":2135}
{"File Name":"timdex\/0002-use-jwt-for-api-authorization.md","Context":"## Context\\nThe API portion of this application will require authentication.\\nJSON Web Token (JWT) is an open standard described by [RFC 7519]( https:\/\/tools.ietf.org\/html\/rfc7519).\\n[Additional Information](https:\/\/en.wikipedia.org\/wiki\/JSON_Web_Token).\\n","Decision":"We will use JWT for authentication.\\n","tokens":62,"id":2375}
{"File Name":"adrflow\/3-Allow_More_Permissive_File_Names.md","Context":"## Context\\nGiven that the ADR flow tool is intended to be a drop-in tool, and integrate well with other tools and repositories, we should aim also for ADR files created outside the tool, not with the same naming convention.\\nThe tool generally works with the file system - looking for the ADR dir, enumerating files, etc.\\nThe existing naming convention is rather strict, so the functions looking for files rely on the same regular expression that's used to create it.\\nWe would like to extend the file enumeration to more options for file names.\\n","Decision":"Change the regular expression that's used to enumerate files to include more options (hypens, spaces).\\nFile should still be with a `.md` extension, and reside in the ADR dir (the directory identified by `.adr` file).\\nThe file template itself still remains the same.\\n","tokens":114,"id":3634}
{"File Name":"simple_note\/0003-use-docker.md","Context":"## Context\\nThe app should be able to build in any basic environment, without the need to worry about environmental differences\\n","Decision":"The app will use Docker for production deployment.\\n","tokens":24,"id":3264}
{"File Name":"dotcom-rendering\/010-storybook.md","Context":"## Context\\nGUUI intends to be, at some level, a library of components.\\nDotcom rendering will require a mapping of CAPI element to React Component.\\nStorybook is a widely used library which allows a series of demos and examples to be easily constructed. It also has support for typescript.\\n","Decision":"It is possible to envision a split in components:\\n- those which form our design system\\n- those which render individual elements of content from CAPI\\nEach of these should have an independant storybook, allowing the design system ones to express the variety of ways each component can and should be used. And allowing, as they are developed, each CAPI element rendering component to demonstrate the variety of content they can encapsulate.\\n","tokens":64,"id":2657}
{"File Name":"pfb-network-connectivity\/adr-0004-asynchronous-task-queue-2.md","Context":"## Context\\nIn ADR0003, we described the 'Bicycle Network Analysis' task to be run via an asynchronous task queue. Since then, Amazon Web Services (AWS) released a new service simply named 'Batch'. This service provides a managed task queue, with Boto and HTTP API interfaces for creating queues and jobs, and triggering new jobs. Each job in AWS Batch is configured to run a Docker container provided to the job configuration. AWS Batch manages ordering and execution of tasks in the queue. In almost every way, AWS Batch is a superior choice to the strategy outlined in ADR 0003, for a few key reasons:\\n- AWS Batch manages the queue and task autoscaling without any management from the parent application. The service can be trivially configured to scale up or down on a few different resource considerations. If there are no jobs in the queue, the pool of workers will automatically scale to zero, saving on hosting costs.\\n- AWS Batch, in comparison with a manually managed stack of celery workers + broker + result backend, is easy to configure, as it only requires defining a \"worker\" stack via a JSON cofiguration.\\n- Switching from a Celery and ECS task based solution will be easy, as AWS Batch workers are configured with Docker containers in the same way as ECS tasks would be\\n- It will be easier to trigger jobs from Django using AWS Batch, since direct calls can be made via Boto, rather than having to write some management layer to trigger ECS tasks or work with the Celery API.\\n","Decision":"The team will build the Bicycle Network Analysis task queue on AWS Batch. The reduction in manual task queue management and ease of configuration should vastly outweigh having to learn how to develop applications using an unfamiliar service. While relatively new, AWS Batch has support in both Boto and via HTTP API and manual setup of a Batch stack was relatively straightforward.\\n","tokens":316,"id":707}
{"File Name":"dapr\/API-008-multi-state-store-api-design.md","Context":"## Context\\nThis decision record is to support multiple state stores support in Dapr. We agreed on the decision to introduce the breaking change in API\\nto support multi state store with no backward compatibility.\\nWith this change , the state API allows the app to target a specific state store by store-name, for example:\\nv1.0\/state\/storeA\/\\nv1.0\/state\/storeB\/\\nEarlier this breaking change, the API is v1.0\/state\/`<key>`\\nWe have reviewed multi storage API design for completeness and consistency.\\n","Decision":"*  New state store API is v1.0\/state\/`<store-name>`\/\\n*  If user is using actors and like to persist the state then user must provide actorStateStore: true in the configuration yaml.\\nIf the attribute is not specified or multiple actor state stores are configured, Dapr runtime will log warning.\\nThe actor API to save the state will fail in both these scenarios where actorStore is not specified or multiple actor stores\\nare specified.\\n*  It is noted that after this breaking change, actor state store has to be specified unlike earlier where first state store is picked up by default.\\n* It is noted that this breaking change will also require a CLI change to generate the state store YAML for redis with actorStateStore.\\n* To provide multiple stores, user has to provide separate YAML for each store and giving unique name for the store.\\n* It is noted that the param's keyPrefix represents state key prefix, it's value included ${appid} is the microservice appid, ${name} is the CRDs component's unique name, ${none} is non key prefix and the custom key prefix\\nFor example, below are the 2 sample yaml files in which redis store is used as actor state store while mongodb store is not used as actor state store.\\n```\\napiVersion: dapr.io\/v1alpha1\\nkind: Component\\nmetadata:\\nname: myStore1  # Required. This is the unique name of the store.\\nspec:\\ntype: state.redis\\nmetadata:\\n- name: keyPrefix\\nvalue: none # Optional. default appid. such as: appid, none, name and custom key prefix\\n- name: <KEY>\\nvalue: <VALUE>\\n- name: <KEY>\\nvalue: <VALUE>\\n- name: actorStateStore  # Optional. default: false\\nvalue : true\\n```\\n```\\napiVersion: dapr.io\/v1alpha1\\nkind: Component\\nmetadata:\\nname: myStore2 # Required. This is the unique name of the store.\\nspec:\\ntype: state.mongodb\\nmetadata:\\n- name: keyPrefix\\nvalue: none # Optional. default appid. such as: appid, none, name and custom key prefix\\n- name: <KEY>\\nvalue: <VALUE>\\n- name: <KEY>\\nvalue: <VALUE>\\n```\\nSo with the above example, the state APIs will be : v1.0\/state\/myStore1\/`<key>`\\nand v1.0\/state\/myStore2\/`<key>`\\n","tokens":115,"id":80}
{"File Name":"dependency-track-maven-plugin\/0003-use-two-dependency-injection-frameworks.md","Context":"## Context\\nDependency Injection (DI) is a well-known software design paradigm that makes developing de-coupled software easier\\nusing Inversion of Control principles and promotes designing for testability.\\nMaven provides a very basic DI framework, called [Plexus](https:\/\/codehaus-plexus.github.io\/), which allows for the\\ninjection of the Maven logger and the runtime configuration.  However that does not extend to the creation and injection\\nof collaborators at runtime.  This can lead to cluttered and hard to test code with having to pass around the logger and\\nconfig.\\nAs such it was deemed that, to facilitate testability and decoupling, another approach to bean creation and wiring was\\nrequired.\\n","Decision":"Use the Plexus DI framework to inject the Maven-specific logger and config and make these available to a different DI\\nframework, Apache [Sisu](https:\/\/www.eclipse.org\/sisu\/), to inject into collaborating components.\\n","tokens":150,"id":3365}
{"File Name":"monocle\/0002-choice-of-elasticsearch.md","Context":"## Context and Problem Statement\\nWe need to store changes data (Pull Requests and Reviewes) in a scalable way. The stored data must be easily accessible in an intelligible manner.\\n","Decision":"Chosen option: \"ElasticSearch\".\\nBecause it fits better our need regarding the style of data we expect to store and how we expect to query the data.\\n","tokens":38,"id":4643}
{"File Name":"wikiindex\/adr-008-search_and_indexing.md","Context":"## Context\\n* As per ADR-007\\n* Clucy takes over an hour to process the index\\n","Decision":"* We will use ElasticSearch as a frontend to Lucene.\\n","tokens":24,"id":1383}
{"File Name":"meadow\/0026-reentrant-processes.md","Context":"## Context\\nElixir provides a very high level of fault tolerance and quick restarts of failed processes, but our code has\\nto be written properly to take advantage of this restart capability. Work Queues are one solution, but not\\nalways feasible (because iterating over and queueing up thousands of tasks can be a long-running operation in\\nitself).\\n","Decision":"Avoid writing long-running, iterative operations \u2013\u00a0especially those that change data \u2013\u00a0as loops. Rather,\\nuse a single, atomic initialization task to record tasks to be done (and a way to track completion), and use a\\n[`GenServer`](https:\/\/hexdocs.pm\/elixir\/GenServer.html) or [`GenStage`](https:\/\/hexdocs.pm\/gen_stage\/GenStage.html)\\nto handle the task (and mark it done) in atomic, transactional pieces.\\n### Pseudocode Pattern\\nInit (one-time):\\n```\\nstart transaction\\ninitialize all task tickets\\ncommit transaction\\n```\\nProcess (periodic):\\n```\\nbatch = load n pending task tickets\\nmark batch tickets as processing\\niterate over batch tickets\\nstart transaction\\nprocess one ticket\\nmark ticket as complete\\ncommit transaction\\n```\\nSweep (periodic):\\n```\\nexpired = find processing task tickets older than a reasonable timeout\\nmark expired as pending in a single update\\n```\\n","tokens":74,"id":3838}
{"File Name":"arch\/0043-message-queue-with-filter.md","Context":"## Context\\n\u6700\u8fd1\u9700\u8981\u4f18\u5316\u8fd9\u6837\u7684\u573a\u666f\uff0c\u6211\u4eec\u7684\u5408\u540c\u72b6\u6001\u6709\u591a\u79cd\uff08\u5f85\u5b8c\u5584\uff0c\u5f85\u5ba1\u6838\uff0c\u5ba1\u6838\u901a\u8fc7\uff0c\u5ba1\u6838\u4e0d\u901a\u8fc7\u7b49\uff09\uff0c\u6bcf\u79cd\u72b6\u6001\u7684\u6539\u53d8\u6765\u81ea\u591a\u4e2a\u5730\u65b9\uff08\u4e2d\u4ecb\u540e\u53f0\uff0c\u8fd0\u8425\u540e\u53f0\uff0c\u98ce\u63a7\u7cfb\u7edf\u7b49\uff09\uff0c\u6bcf\u79cd\u72b6\u6001\u7684\u5904\u7406\u4e5f\u6765\u81ea\u591a\u4e2a\u5730\u65b9\uff08SAAS\u7cfb\u7edf\uff0c\u4e2d\u4ecb\u540e\u53f0\uff0c\u8fd0\u8425\u7cfb\u7edf\u7b49\uff09\uff0c\u4e14\u5904\u7406\u8005\u9700\u8981\u5904\u7406\u7684\u72b6\u6001\u4e0d\u76f8\u540c\uff1b\\n\u5f53\u524d\u5b9e\u73b0\u65b9\u6848\u662f\uff1a\\n1. \u5b9a\u65f6\u4efb\u52a1\uff0c\u5404\u4e2a\u7cfb\u7edf\u5b9a\u65f6\u5904\u7406\u72b6\u6001\u4e3a X \u7684\u5408\u540c(\u5904\u7406\u4e0d\u53ca\u65f6)\uff1b\\n2. \u4f7f\u7528\u56de\u8c03\uff0c\u72b6\u6001\u66f4\u65b0\u65f6\u56de\u8c03\u81f3\u5904\u7406\u8005\u5904(\u6269\u5c55\u4e0d\u65b9\u4fbf)\uff1b\\n","Decision":"1. \u6d88\u606f\u670d\u52a1(MNS) \u4e3b\u9898\u6a21\u578b\\n![][image-1]\\n* \u53ef\u8ba2\u9605\uff0c\u5e76\u4e14\u53ef\u4ee5\u901a\u8fc7 Tag \u8fdb\u884c\u6d88\u606f\u8fc7\u6ee4\uff1b\\n* \u53ea\u652f\u6301\u63a8\u9001\u6a21\u5f0f\uff0c\u6bcf\u4e2a\u6d88\u8d39\u8005\u9700\u8981\u521b\u5efa\u56de\u8c03\u63a5\u53e3\uff1b\\n* \u53ea\u652f\u6301\u5916\u7f51\u5730\u5740\u63a8\u9001\uff0c\u5bf9\u5185\u90e8\u670d\u52a1\u6765\u8bf4\uff0c\u7f51\u7edc\u8017\u65f6\u592a\u9ad8\u3002\\n2. \u6d88\u606f\u670d\u52a1(MNS) \u961f\u5217\u6a21\u578b\\n![][image-2]\\n* \u6839\u636e\u6d88\u8d39\u8005\u521b\u5efa\u961f\u5217\uff0c\u51e0\u4e2a\u6d88\u8d39\u8005\u5c31\u521b\u5efa\u51e0\u4e2a\u961f\u5217\uff1b\\n* \u751f\u4ea7\u8005\u6839\u636e\u6d88\u8d39\u8005\u6240\u9700\u5411\u5176\u961f\u5217\u53d1\u9001\u6d88\u606f\uff0c\u5373\u751f\u4ea7\u8005\u9700\u8981\u53d1\u9001\u76f8\u540c\u6d88\u606f\u81f3\u591a\u4e2a\u961f\u5217\uff1b\\n* \u6bcf\u589e\u52a0\u4e00\u4e2a\u6d88\u8d39\u8005\uff0c\u90fd\u9700\u66f4\u65b0\u6240\u6709\u5bf9\u5e94\u751f\u4ea7\u8005\u7684\u4ee3\u7801\uff0c\u7ef4\u62a4\u6027\u592a\u4f4e\u3002\\n3. \u6d88\u606f\u670d\u52a1(MNS) \u4e3b\u9898+\u961f\u5217\u6a21\u578b\\n![][image-3]\\n* \u6839\u636e\u6d88\u8d39\u8005\u521b\u5efa\u961f\u5217\uff0c\u51e0\u4e2a\u6d88\u8d39\u8005\u5c31\u521b\u5efa\u51e0\u4e2a\u961f\u5217\uff1b\\n* \u751f\u4ea7\u7740\u53ea\u9700\u5411\u4e3b\u9898\u961f\u5217\u53d1\u9001\u6d88\u606f\uff0c<del>\u6d88\u8d39\u8005\u961f\u5217\u8ba2\u9605\u6240\u6709\u4e3b\u9898\u961f\u5217\u7684\u6d88\u606f<\/del>\uff1b\\n* <del>\u6d88\u8d39\u8005\u9700\u8981\u63a5\u6536\u6240\u6709\u6d88\u606f\uff0c\u5e76\u5728\u4e1a\u52a1\u903b\u8f91\u4e2d\u8fc7\u6ee4\u51fa\u81ea\u5df1\u9700\u8981\u5904\u7406\u7684\u6d88\u606f<\/del>\uff1b\\n* \u6d88\u8d39\u8005\u961f\u5217\u53ef\u4ee5\u6309 tag \u8fdb\u884c\u8fc7\u6ee4\uff1b\\n* \u6d88\u8d39\u8005\u5b8c\u6574\u6d88\u8d39\u81ea\u5df1\u7684\u961f\u5217\u5373\u53ef\u3002\\n4. \u6d88\u606f\u961f\u5217(ONS) MQ\\n![][image-4]\\n* \u603b\u5171\u53ea\u9700\u4e00\u4e2a topic \u961f\u5217\uff0c\u5404\u4e2a\u6d88\u8d39\u8005\u901a\u8fc7 Tag \u8fdb\u884c\u8fc7\u6ee4\uff1b\\n* \u5b8c\u7f8e\u652f\u6301\u6211\u4eec\u7684\u4f7f\u7528\u573a\u666f\uff1b\\n* \u4e0d\u63d0\u4f9b Python SDK\u3002\\n","tokens":177,"id":2442}
{"File Name":"offender-management-architecture-decisions\/0008-use-rails.md","Context":"## Context\\nWe have already decided to use Ruby for our new applications (see [ADR 0007](0007-use-ruby-for-new-applications-for-manage-offenders-in-custody.md)).\\nThe team are already very familiar with Rails and it is widely used within MOJ.\\n","Decision":"We will use Rails as our web framework for our new applications.\\n","tokens":61,"id":266}
{"File Name":"cosmos-sdk\/adr-029-fee-grant-module.md","Context":"## Context\\nIn order to make blockchain transactions, the signing account must possess a sufficient balance of the right denomination\\nin order to pay fees. There are classes of transactions where needing to maintain a wallet with sufficient fees is a\\nbarrier to adoption.\\nFor instance, when proper permissions are setup, someone may temporarily delegate the ability to vote on proposals to\\na \"burner\" account that is stored on a mobile phone with only minimal security.\\nOther use cases include workers tracking items in a supply chain or farmers submitting field data for analytics\\nor compliance purposes.\\nFor all of these use cases, UX would be significantly enhanced by obviating the need for these accounts to always\\nmaintain the appropriate fee balance. This is especially true if we wanted to achieve enterprise adoption for something\\nlike supply chain tracking.\\nWhile one solution would be to have a service that fills up these accounts automatically with the appropriate fees, a better UX\\nwould be provided by allowing these accounts to pull from a common fee pool account with proper spending limits.\\nA single pool would reduce the churn of making lots of small \"fill up\" transactions and also more effectively leverages\\nthe resources of the organization setting up the pool.\\n","Decision":"As a solution we propose a module, `x\/feegrant` which allows one account, the \"granter\" to grant another account, the \"grantee\"\\nan allowance to spend the granter's account balance for fees within certain well-defined limits.\\nFee allowances are defined by the extensible `FeeAllowanceI` interface:\\n```go\\ntype FeeAllowanceI {\\n\/\/ Accept can use fee payment requested as well as timestamp of the current block\\n\/\/ to determine whether or not to process this. This is checked in\\n\/\/ Keeper.UseGrantedFees and the return values should match how it is handled there.\\n\/\/\\n\/\/ If it returns an error, the fee payment is rejected, otherwise it is accepted.\\n\/\/ The FeeAllowance implementation is expected to update it's internal state\\n\/\/ and will be saved again after an acceptance.\\n\/\/\\n\/\/ If remove is true (regardless of the error), the FeeAllowance will be deleted from storage\\n\/\/ (eg. when it is used up). (See call to RevokeFeeAllowance in Keeper.UseGrantedFees)\\nAccept(ctx sdk.Context, fee sdk.Coins, msgs []sdk.Msg) (remove bool, err error)\\n\/\/ ValidateBasic should evaluate this FeeAllowance for internal consistency.\\n\/\/ Don't allow negative amounts, or negative periods for example.\\nValidateBasic() error\\n}\\n```\\nTwo basic fee allowance types, `BasicAllowance` and `PeriodicAllowance` are defined to support known use cases:\\n```protobuf\\n\/\/ BasicAllowance implements FeeAllowanceI with a one-time grant of tokens\\n\/\/ that optionally expires. The delegatee can use up to SpendLimit to cover fees.\\nmessage BasicAllowance {\\n\/\/ spend_limit specifies the maximum amount of tokens that can be spent\\n\/\/ by this allowance and will be updated as tokens are spent. If it is\\n\/\/ empty, there is no spend limit and any amount of coins can be spent.\\nrepeated cosmos_sdk.v1.Coin spend_limit = 1;\\n\/\/ expiration specifies an optional time when this allowance expires\\ngoogle.protobuf.Timestamp expiration = 2;\\n}\\n\/\/ PeriodicAllowance extends FeeAllowanceI to allow for both a maximum cap,\\n\/\/ as well as a limit per time period.\\nmessage PeriodicAllowance {\\nBasicAllowance basic = 1;\\n\/\/ period specifies the time duration in which period_spend_limit coins can\\n\/\/ be spent before that allowance is reset\\ngoogle.protobuf.Duration period = 2;\\n\/\/ period_spend_limit specifies the maximum number of coins that can be spent\\n\/\/ in the period\\nrepeated cosmos_sdk.v1.Coin period_spend_limit = 3;\\n\/\/ period_can_spend is the number of coins left to be spent before the period_reset time\\nrepeated cosmos_sdk.v1.Coin period_can_spend = 4;\\n\/\/ period_reset is the time at which this period resets and a new one begins,\\n\/\/ it is calculated from the start time of the first transaction after the\\n\/\/ last period ended\\ngoogle.protobuf.Timestamp period_reset = 5;\\n}\\n```\\nAllowances can be granted and revoked using `MsgGrantAllowance` and `MsgRevokeAllowance`:\\n```protobuf\\n\/\/ MsgGrantAllowance adds permission for Grantee to spend up to Allowance\\n\/\/ of fees from the account of Granter.\\nmessage MsgGrantAllowance {\\nstring granter = 1;\\nstring grantee = 2;\\ngoogle.protobuf.Any allowance = 3;\\n}\\n\/\/ MsgRevokeAllowance removes any existing FeeAllowance from Granter to Grantee.\\nmessage MsgRevokeAllowance {\\nstring granter = 1;\\nstring grantee = 2;\\n}\\n```\\nIn order to use allowances in transactions, we add a new field `granter` to the transaction `Fee` type:\\n```protobuf\\npackage cosmos.tx.v1beta1;\\nmessage Fee {\\nrepeated cosmos.base.v1beta1.Coin amount = 1;\\nuint64 gas_limit = 2;\\nstring payer = 3;\\nstring granter = 4;\\n}\\n```\\n`granter` must either be left empty or must correspond to an account which has granted\\na fee allowance to fee payer (either the first signer or the value of the `payer` field).\\nA new `AnteDecorator` named `DeductGrantedFeeDecorator` will be created in order to process transactions with `fee_payer`\\nset and correctly deduct fees based on fee allowances.\\n","tokens":247,"id":837}
{"File Name":"helix-authentication-service\/0002-use-javascript-node.md","Context":"## Context\\nIn terms of the application implementation, the programming language and run time environment are important decisions. Significant factors include the general availability of developers, the level of support available in the community, and resources for learning, developing, and maintaining an application in that language.\\n","Decision":"JavaScript is, at least according to the Stack Overflow Developer Surveys, far and away a very popular choice of programming language. This is due in large part to the proliferation of web browsers, which natively run JavaScript, but also can be credited to Node.js, which makes writing backend systems almost as easy as writing the frontend client in the browser.\\nApplications running on Node are sufficiently fast, especially compared to Python or Ruby, and JavaScript is a small and predictable language when compared to PHP. There are multiple OIDC and SAML libraries for Node to choose from. In general the Node ecosystem is huge, so finding libraries with permissive licenses is very easy. Deploying to a variety of systems is well supported.\\nAs such, this application will be written in **JavaScript** and the run time environment will be **Node.js**.\\n","tokens":55,"id":2024}
{"File Name":"infra\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":859}
{"File Name":"govuk-terraform-provisioning\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":3436}
{"File Name":"edward\/0003-use-cobra-for-command-line-interface.md","Context":"## Context\\nEdward provides a rich command-line interface intended to use a command structure familiar to most developers.\\n","Decision":"[Cobra](https:\/\/github.com\/spf13\/cobra) will be used as a CLI framework.\\n","tokens":22,"id":1020}
{"File Name":"ios-architecture-decision-logs\/0019-naming-branch-name.md","Context":"## Context\\nWe run too many separate sprints and our naming gets mixed up. Also it is difficult to manage when too many branches are gathered under one folder.\\n","Decision":"Branch names should be named in such way:  `Channel Name\/Sprint Name\/Task Code + Name`\\nFor Example: `MPDP\/Sprint1\/MPDP-1050-ProductDetailPage`  or  `DISCO\/Sprint50\/IOSDISCO-1250-FixWidgetHeight`\\n","tokens":34,"id":4976}
{"File Name":"front-end-monorepo\/adr-22.md","Context":"## Context\\nWith new drawing tools being developed for the classifier, we need an API that's common to all drawing tools and marks, which can be easily extended by tool developers. This document lays out an overview of the drawing tool model and the public interfaces common to all tools and all marks.\\nTo support drawing, the subject viewer also needs to support:\\n- rendering a static list of marks from previous drawing task annotations.\\n- interacting with pointer events to create, edit and delete new marks for the current drawing task annotation.\\n","Decision":"### The subject viewer\\nThe subject viewer will render two components.\\n- _DrawingToolMarks_ takes an array of marks from drawing task annotations in the classification and renders it as a static, read-only list.\\n- _InteractionLayer_ wraps a _DrawingToolMarks_ component and adds pointer event support, so that the rendered array of marks can be edited and updated. THis component only acts on marks for the active drawing task.\\nMarks created by the _InterctionLayer_ are added to a new drawing task annotation, for the current classification, when we click Next or Done to complete the current task.\\nPointer event support is polyfilled in older browsers with [PEP](https:\/\/github.com\/jquery\/PEP).\\n### The drawing model\\nA drawing task has drawing tools. Each tool creates marks. On task completion, a drawing annotation is created, which is an array of all drawn marks. Each mark has a corresponding React component which renders the SVG for that particular shape.\\n","tokens":108,"id":516}
{"File Name":"react-transcript-editor\/2018-10-01-timed-editor-choice.md","Context":"## Context and Problem Statement\\n<!-- [Describe the context and problem statement, e.g., in free form using two to three sentences. You may want to articulate the problem in form of a question.] -->\\nTo build a timed text editor, for editing audio or video transcriptions it can be useful to use an open source text editor and add functionalities to it.\\n## Decision Drivers <!-- optional -->\\n<!-- * [driver 1, e.g., a force, facing concern, \u2026] -->\\n* A simple and straight forward way to keep words and time in sync, during playback and text editing.\\n","Decision":"<!-- * [driver 1, e.g., a force, facing concern, \u2026] -->\\n* A simple and straight forward way to keep words and time in sync, during playback and text editing.\\nChosen option: **Draft.js**, because there is previous work in this space that has explored and stressed tests this editor.\\nAltho Quilljs has a straight forward API, despite [a quircky data model internal  representation](https:\/\/quilljs.com\/docs\/delta). And has been used to quickly make [text based prototypes](https:\/\/github.com\/pietrop\/annotated_article_generator) (see [demo](http:\/\/pietropassarelli.com\/annotated_article_generator\/)) even with multi user collaboration support [through togetherjs](https:\/\/togetherjs.com\/).\\nIt's hard  to tell whether Quilljs is going to be around long term, while draft.js being core part of facebook is starting to see more of a community around it,  and a growing number of plugins.\\nThere also other more advanced features, like adding speakers label, timed text, and other non timed text in the transcription that has workarounds in Draftjs but is not immediatly obvious how it would be implemented in Quilljs.\\n<!-- because [justification. e.g., only option, which meets k.o. criterion decision driver | which resolves force force | \u2026 | comes out best (see below)]. -->\\n<!--\\n### Positive Consequences\\n* [e.g., improvement of quality attribute satisfaction, follow-up decisions required, \u2026]\\n* \u2026\\n### Negative consequences\\n* [e.g., compromising quality attribute, follow-up decisions required, \u2026]\\n* \u2026\\n","tokens":126,"id":3188}
{"File Name":"cdh-adrs\/0001-feed-parsers.md","Context":"## Context\\nWe need a clearly defined way for handling data feed files for processing, what are the expected inputs and outputs from the parsers and how will this feed into the whole parsing process for ETL ?\\n","Decision":"1. A EAM Parser Factory: this produces a content reader which will be used by all written parsers for reading the contents of a giving data source.\\n1. A Processing Adapter per EAM data feed type which has registered different parsers which handle the retrieval of different types of data out of giving data feed source (e.g CreditSuisse XML).\\n1. Custom Data Extractors (e.g IncomeCashFlowParsers, OrderBroker) which are responsible for extracting different data types from the ContentReader, these are then accumulated by the data feed ProcessingAdapter into a unified format which can be transformed into portions of the expected PriveXML format.\\n1. The custom data extractors will have rights to define specific errors for their data extraction process and how that will affect that specific extraction or for a giving set of files. We will have errors which may be critical and cause immediate failure or which can be considered non-critical and only stop giving feed extraction or ensure it is logged and continued from. The key is that such details should not be the responsibility of the core and as far as only specific errors which the core is concerned with towards stopping immediately for that giving source or a set of sources.\\n![Target Parser Flow](..\/assets\/images\/workflows\/image1.png)\\n","tokens":43,"id":6}
{"File Name":"gsp\/ADR016-code-verification.md","Context":"## Context\\nAll of our deployment processes start with code in git repositories hosted on\\nGithub and continuous delivery pipelines automate the build and deployment of\\napplications out to production. Git supports GPG signing of commits.\\nCode changes should not be able to make it out to a production environment\\nwithout being exposed to at least two authorised code owners.\\nOur aim is to improve on process-based approval methods by being able to\\ndigitally verify that code has been exposed to at least two authorised code\\nowners without adversely affecting developer workflow.\\nSome potential solutions involve:\\n1. Enforcing that two unique developers each an add empty signed commit at the\\ntip of the branch for PRs before being merged, and verifying the existence\\nof these empty signed commits as part of the delivery pipeline.\\n2. Enforcing that two unique developers each sign the commmit at the tip of the\\nbranch for PRs using\\n[git-signatures](https:\/\/github.com\/hashbang\/git-signatures) and verifying\\nthat at least two trusted developer keys are present as part of the delivery\\npipeline.\\n3. Verify that at least one (non-PR-authoring) trusted Github user has approved\\nthe commit from the delivery pipeline using the Github API (rather than the\\nweb UI, which is vulnerable to manipulation by a single Github \"owner\").\\n","Decision":"Solutions (1) and (2) both introduce significant changes to existing developer\\nworkflows. The benefit of either of these solutions is that when combined with\\nan external hardware security device (Yubikey) they can provide greater\\nprotection against developer machine compromise than Github's session duration\\nalone.  However we don't feel this benefit warrants the reduced usability for\\ndevelopers across GDS.\\nWe implement solution (3).\\n","tokens":275,"id":3900}
{"File Name":"adr-poc\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":4967}
{"File Name":"unfinished-design-system\/003-theming-framework.md","Context":"* [Context](#context)\\n* [Decision](#decision)\\n* [Status](#status)\\n* [Consequences](#consequences)\\n* [More reading](#more-reading)\\n* [Updates](#updates)\\n","Decision":"* [Status](#status)\\n* [Consequences](#consequences)\\n* [More reading](#more-reading)\\n* [Updates](#updates)\\nAmong most frameworks, [ThemeUI](https:\/\/theme-ui.com\/) stands out for the following reasons:\\n* It is incredibly light (only 19kb unpacked)\\n* It is highly customizable (we can use the `sx` props to use theme tokens without hooks)\\n* It is slightly opinionated (more than [styled-system](https:\/\/styled-system.com\/) but less than [rebass](https:\/\/rebassjs.org\/))\\n* It contains some common structures\\nWe've not decided to use Rebass, since most of the community is already changing to ThemeUI, and also it allows us to do a more robust structure in our themes.\\n","tokens":53,"id":4546}
{"File Name":"content-data-api\/adr-010-track-content-items-in-all-languages.md","Context":"## Context\\nIn [adr-008][1] we agreed on focusing only on English pages.\\nWe have noticed during the last few months that this restriction is causing more\\nissues than benefits, because:\\n- We have Content Items with the same `content_id` and different `locale`s.\\n- We needed to handle edge cases when retrieving the information from Publishing API in order to work around only retrieving English language content.\\n","Decision":"Track Content metrics of all Content Item regardless of their locale.\\n","tokens":88,"id":1862}
{"File Name":"james-project\/0042-applicative-read-repairs.md","Context":"## Context\\nCassandra eventual consistency is all about \"replication\", but \"denormalization\" consistency needs\\nto be handled at the applicative layer (due to the lack of transactions in a NoSQL database).\\nIn the past we did set up \"Solve inconsistency\" tasks that can be assimilated to Cassandra repairs. Such\\ntasks, after being scheduled, ensure that the according entity denormalization is correctly denormalized.\\nHowever, the inconsistencies persist between runs. We experienced inconsistencies in some production platform\\nfor both the mailbox entity, and the mailbox counter entity (whose table structure is exposed in\\n[these](0020-cassandra-mailbox-object-consistency.md), [ADRs](0023-cassandra-mailbox-counters-inconsistencies.md)).\\nMonitoring is required to detect when to run them and is time consuming for the platform administrator.\\nGiven a large dataset, it could even be impossible to run such tasks in a timely fashion.\\nAnother classic eventual consistency mechanism, that enables auto-healing is read-repair. Randomly piggy back upon reads\\nsynchronous or asynchronous consistency checks. If missed a repair is performed.\\nIn order to achieve denormalization auto-healing, we thus need to implement \"applicative read repairs\".\\n","Decision":"Provide a Proof of concept for \"Applicative read repairs\" for the mailbox and mailbox-counters entities.\\nThis enables read path simplification (and performance enhancements) for the mailbox object.\\nIMAP LIST should not read mailbox counters. This information is uneeded and we should avoid paying the\\nprice of read repairs for this operation.\\nProvide a comprehensive documentation page regarding \"Distributed James consistency model\".\\n","tokens":263,"id":2903}
{"File Name":"educational-platform\/0001-bounded-contexts-communication.md","Context":"## Context\\nSome common data should be used by several bounded contexts (modules, in the case of monolith application).\\n","Decision":"Communication between bounded contexts asynchronous. Bounded contexts don't share data, it's forbidden to create a transaction which spans more than one bounded context.\\n- https:\/\/www.infoq.com\/news\/2014\/11\/sharing-data-bounded-contexts\/\\n- http:\/\/www.kamilgrzybek.com\/design\/modular-monolith-primer\/\\n- https:\/\/github.com\/kgrzybek\/modular-monolith-with-ddd#37-modules-integration\\n","tokens":25,"id":2497}
{"File Name":"cljdoc\/0019-use-custom-search.md","Context":"## Context\\nWe need to be able to search (documented) projects, whether\\nthey come from Clojars or Maven Central.\\nSee https:\/\/github.com\/cljdoc\/cljdoc\/issues\/85\\n","Decision":"Implement our own search, using direct integration with Lucene. Download and\\nindex artifact list from Clojars and \"org.clojure\" artifacts from Maven Central.\\nWe will use Lucene as that is the absolutely prevailing solution for search in\\nJava. Direct Java interop is quite idiomatic in Clojure; it isn't too much work as we\\nonly need to implement the parts relevant for us and not a generic Lucene wrapper.\\nWe avoid the risk of depending on incomplete and potentially abandoned library\\n(as happend to Clojars with clucy). And to be able to use Lucene efficiently we\\nneed to understand it sufficiently anyway.\\n","tokens":44,"id":2609}
{"File Name":"ErisCasper-Data\/adr-002.md","Context":"## Context\\nWe will be generating a number of classes as per [ADR 1](adr-001.md).\\nGenerated classes should be generated during the build process and not committed to GitHub.\\nThis ensures we have reproducable builds and makes it clear that generated classes are not to be edited manually.\\nWe wish to publish artifats to the central maven repository.\\nThis does not necessarily limit us to working with Maven, but it encourages us to stick it.\\nUsing a compiled language to run the code generation would require two passes of compilation\\n(or another\/project module) - one to compile the codegen scripts, another to run it.\\n","Decision":"We will use ruby to write the code generation scripts.\\nWe will run them using the [jruby-maven-plugins](https:\/\/github.com\/torquebox\/jruby-maven-plugins) as part of the\\n`generate-sources` phase of the\\n[maven lifecycle](https:\/\/maven.apache.org\/guides\/introduction\/introduction-to-the-lifecycle.html)\\n","tokens":134,"id":2995}
{"File Name":"elife-xpub\/0003-workflow.md","Context":"## Context\\neLife follows a publication process that is a well defined workflow for a submission to follow to completion.\\nThe editorial team are familiar with this process and commonly use the language in their day to day jobs.\\n[This workflow has been modelled](https:\/\/drive.google.com\/drive\/u\/0\/folders\/1gRuWuoI9KcEwfgNrFYVNDUiQ5YzK88de) as part of the development work.\\nA simple library (npm package) [javascript-state-machine](https:\/\/github.com\/jakesgordon\/javascript-state-machine) was used to codify this workflow. The code generates an image of the workflow: (manuscript-workflow)[https:\/\/github.com\/diversemix\/manuscript-workflow]\\n[Libero](https:\/\/github.com\/libero\/) also have had a requirement for workflow and have [decided on Airflow](https:\/\/github.com\/libero\/walking-skeleton\/blob\/master\/adr\/0003-workflow-system.md).\\nThis solution was also considered for the submission workflow.\\nHowever, out of the box there is no solution for user interactions.\\nIt is assumed that all tasks in the flow are processes performed on the server and not by a user.\\nThere are plugins to overcome this but it does seem like its not the right tool for the job.\\nTwo Other tools were also considered:\\n* [bpmn-engine](https:\/\/github.com\/paed01\/bpmn-engine)\\n* [node-workflow](https:\/\/github.com\/joyent\/node-workflow)\\nBoth of these also did not support the idea of user interaction fully.\\nThe `bpmn-engine` did have the concept of a userTask however this was treated as a distict entity separate from the `task` that forms the workflows.\\n","Decision":"While it would be good to have a separate service that would have the responsibility to manage the submission's workflow in the system,\\nthere does not seem to be anything that would readily support this.\\nThe decision is to use the npm package `javascript-state-machine` as demo'ed above in the repo.\\nIt is envisaged that this will be used to extend the `Manuscript` object within `xpub-elife`\\n","tokens":376,"id":3775}
{"File Name":"BMMRO\/2019-11-20_webapp.md","Context":"## Context\\nWe need an application platform that meets the following requirements:\\n- Cross-platform: The app may be used to enter data on phone or tablet in real time or through a computer at a later stage.\\n- Works offline: Given the environmental conditions on a boat in the middle of the ocean, consistent internet connection is not a given. Thus, our tech stack must provide offline capabilities with minimal-to-no data loss.\\n### Option 1 - Cache First Web App (PWA)\\nBy creating a web app that meets certain criteria, the app will be downloaded to cache on first use. This means that the app will be accessible offline. This option would also allow us to create a single application which could be used on any device with a modern web browser.\\n### Option 2 - Native application\\nAnother option would be to develop a native application for each platform which may be used (iOS, macOS, Android, Windows). These apps could be developed more rapidly using something like React Native which would allow some components to be shared. This method would give us greater control and potentially give a better user experience.\\n","Decision":"The decision was made to create a web app with offline capability due to the flexibility and speed it offers. Creating native applications for each of the required platforms would not be feasible with the available time and resources.\\n### PWA Issues\\nSince selecting a web app as the platform for this project a number of issues have been discovered:\\n- Updating the app to the latest version is inconsistent between browsers and devices.\\n- Refreshing while offline may show a 'No internet connection' page until the app is closed and reopened.\\n- Offline behaviour on iOS in particular seems inconsistent with other platforms.\\n","tokens":227,"id":2490}
{"File Name":"mat-process-utils\/0004-use-prettier-to-format-code.md","Context":"## Context\\nWe want to ensure we're all using one code style, that is familiar across\\nprojects. [Prettier](https:\/\/prettier.io\/) is an opinionated code formatter with\\nsupport for most, if not all, of the languages in the JavaScript ecosystem. As\\nof writing, it is used by over\\n[1 million repositories](https:\/\/github.com\/prettier\/prettier\/network\/dependents?package_id=UGFja2FnZS00OTAwMTEyNTI%3D)\\non GitHub, and has become a standard.\\n","Decision":"We will enforce that everything supported by Prettier has its style enforced by\\nit.\\nWe will set up Git hooks to automatically run the formatter before committing.\\nWe will set continuous integration up to reject commits that are not correctly\\nformatted.\\n","tokens":121,"id":3760}
{"File Name":"claim-additional-payments-for-teaching\/0011-payroll-information-to-be-downloaded-directly-by-payroll-provider.md","Context":"## Context\\nEvery month the service pays out approved claims as part of a payroll process\\nperformed by a third party payroll provider. To perform this function the\\npayroll provider needs to be given the details of the claims and claimants. This\\ndata is highly personal in nature and needs to be shared safely and securely\\nwith the third-party provider.\\n","Decision":"The payroll provider will download the monthly payroll data file directly from\\nthe service\u2019s back-office, which uses DfE's single sign-on service\\n[DfE Sign In](https:\/\/services.signin.education.gov.uk\/) for authentication.\\nThe payroll provider will be set up with their own organisation in DfE Sign In\\nso that they can manage their own user access and be responsible for\\nmovers\/leavers. Their users will only support a specific \u201cPayroll operator\u201d\\nrole. This role will only allow the downloading of the monthly payroll file and\\nnothing else within the back-office.\\nThe monthly payroll file will only be downloadable by the payroll provider\u2019s\\nusers. Other users will not be able to access the file.\\nThe file will only be available as a one-time download; once downloaded, that\\nmonth\u2019s file will no longer be available.\\n","tokens":73,"id":2099}
{"File Name":"generator-latex-template\/0007-use-glossaries-package.md","Context":"## Context and Problem Statement\\nHow to manage abbreviations\\n","Decision":"Chosen option: \"glossaries-extra\", because seems to be best.\\n","tokens":12,"id":2335}
{"File Name":"granary\/0002-api-documentation.md","Context":"# Context\\nGranary will be a standalone deployable open source application. Users' primary\\nway of interacting with Granary will be through its REST API, rather than\\nthrough a UI, at least for the foreseeable future. This focus puts a larger\\nburden than usual on the quality of our API documentation.\\nWe'd like to evaluate different ways to keep API documentation up-to-date. For\\neach strategy, we want to know:\\n- What are the general pros and cons?\\n- How do we find out that our docs have drifted from the API?\\n- How can we document different versions of the API at the same time?\\n","Decision":"We should use `tapir` for automatically generating API documentation. The ADT\\nsupport and straightforward API (inputs use `.in`, outputs use `.out`, auth\\nextractors use `auth`) will flatten out the learning curve, and we'll have a\\nstable and correct reference point for API documentation that users setting up\\ntheir own deployments can refer to. We can call this out at the beginning of\\nthe README and hopefully save ourselves from having to answer an entire\\ncategory of questions.\\n# Consequences\\n- The first routes added to the API will be slightly more difficult, because\\nthey'll include writing API routes with a new library for the first time.\\n- The README should be updated to point to the location of the API\\ndocumentation.\\n","tokens":134,"id":3549}
{"File Name":"operational-data-hub\/0060-lock-pip-requirements.md","Context":"## Context\\nCode Injection is a specific type of injection attack where an executable program statement is constructed involving user input at an attack surface that becomes vulnerable when it can be manipulated in an unanticipated way to invoke functionality that can be used to cause harm.\\n","Decision":"To prevent dependency injection attacks we decided to have both a requirements.in file and a [pip-tools\/pip-compile](https:\/\/github.com\/jazzband\/pip-tools) generated requirements.txt\\n","tokens":51,"id":2698}
{"File Name":"react-native-app\/0004-use-amplitude-for-analytics.md","Context":"## Context\\nTo build the product we need to understand users\\n","Decision":"We will use [Amplitude's free plan](https:\/\/amplitude.com\/pricing?ref=nav), it has everything that we need at the beginning.\\n","tokens":13,"id":4212}
{"File Name":"deepspeech-node-wrapper\/2019-12-10-deepspeech-stt.md","Context":"## Context and Problem Statement\\nMaking a node wrapper for Mozilla Deepseepch, in `deepspeech-node-wrapper`.\\nUsing Mozilla node `deepspeech` module and their example [`DeepSpeech\/examples\/nodejs_wav`](https:\/\/github.com\/mozilla\/DeepSpeech-examples\/tree\/r0.6\/nodejs_wav) as a starting point.\\nThe question this ADR explore is, how to package the the STT models in this npm module?\\n## Decision Drivers <!-- optional -->\\n- Easy to reason around\\n- Avoid adding large binaries to git repository\\n- Avoid adding large binaries to NPM\\n- ease of use and setup\\n- considerate of slow internet connections\\n- \u2026 <!-- numbers of drivers can vary -->\\n","Decision":"- Easy to reason around\\n- Avoid adding large binaries to git repository\\n- Avoid adding large binaries to NPM\\n- ease of use and setup\\n- considerate of slow internet connections\\n- \u2026 <!-- numbers of drivers can vary -->\\n<!-- Chosen option: \"[option 1]\", because [justification. e.g., only option, which meets k.o. criterion decision driver | which resolves force force | \u2026 | comes out best (see below)]. -->\\n_TBC_\\nLeaning torwards option 3.\\n<!-- ### Positive Consequences\\n* [e.g., improvement of quality attribute satisfaction, follow-up decisions required, \u2026]\\n* \u2026\\n### Negative consequences\\n* [e.g., compromising quality attribute, follow-up decisions required, \u2026]\\n* \u2026\\n-->\\n","tokens":154,"id":3340}
{"File Name":"stamper\/0005-junit-as-test-framework.md","Context":"## Context\\nWe need to choose test framework.\\n","Decision":"We use JUnit because it is a standard framework for JVM, and we don't have any advanced requirements yet.\\n","tokens":11,"id":1241}
{"File Name":"arch\/0031-capacity-evaluation-memory.md","Context":"## Context\\n1. \u673a\u5668\u5185\u5b58\u592a\u5c0f\u4e86\uff0c\u9700\u8981\u5347\u7ea7\u4e00\u4e0b\uff1b\\n2. \u6211\u4eec\u4e4b\u524d\u516c\u53f8\u670d\u52a1\u9700\u8981\u7528\u591a\u5c11\u591a\u5c11\uff1b\\n3. Java \u7a0b\u5e8f\u6700\u5927\u6700\u5c0f\u5806\u7684\u6307\u5b9a\u65e0\u6807\u51c6\u3002\\n","Decision":"* \u5206\u6790\u4e1a\u52a1\u573a\u666f\uff0c\u660e\u786e\u6211\u4eec\u5230\u5e95\u9700\u8981\u591a\u5c11\u5185\u5b58\uff0c\u5408\u7406\u7684\u9884\u75591-2\u500d\u5185\u5b58\u90fd\u6ca1\u6709\u95ee\u9898\uff1b\\n* \u6211\u4eec\u7684\u4e1a\u52a1\u76ee\u524d\u5206\u522b\u7531\u4e24\u79cd\u8bed\u8a00\u5b9e\u73b0 Python \u53ca Java\uff0cJava \u7a0b\u5e8f\u5bf9\u5185\u5b58\u7684\u4f7f\u7528\u91cf\u975e\u5e38\u5927\u4e14\u76d1\u63a7\u4e0d\u76f4\u89c2\uff0c\u4e3a\u4e86\u5408\u7406\u5206\u914d\u5185\u5b58\uff0c\u6211\u4eec\u91c7\u53d6\u4e86\u4ee5\u4e0b\u5206\u6790\u65b9\u6cd5\u3002\\n* \u5206\u914d\u7684\u6700\u5927\u5185\u5b58\u7528\u5b8c\u540e\uff0cGC \u9891\u7387\u9ad8\u4e14\u7279\u5b9a\u65f6\u95f4\u5185\u65e0\u6cd5\u505a\u5230\u5783\u573e\u56de\u6536\uff0c\u5c06\u4f1a\u5bfc\u81f4 `java.lang.OutOfMemoryError`\uff1b\\n* OOF\uff0cGC \u9891\u7387\u53ca GC \u65f6\u95f4\u662f\u786e\u5b9a heap \u53c2\u6570\u7684\u4e00\u4e2a\u6307\u6807\uff1b\\n* \u7ebf\u4e0a\u4e0d\u53ef\u80fd\u7b49\u5230 OOF \u65f6\uff0c\u624d\u505a\u5347\u7ea7\uff0c\u8fd9\u6837\u76f4\u63a5\u5f71\u54cd\u4e86\u7cfb\u7edf\u7684\u53ef\u7528\u6027\uff0c\u6240\u4ee5\u9488\u5bf9 Java \u7a0b\u5e8f\uff0c\u4e00\u5b9a\u8981\u505a\u597d\u538b\u529b\u6d4b\u8bd5\u6216\u5bf9 JVM \u505a\u597d\u76d1\u63a7\uff08\u901a\u8fc7 Java VisualVM or JConsole\uff09\uff0c\u9274\u4e8e\u6211\u4eec\u5df2\u7528 Newrelic \u6808\uff0c\u5c06\u6709\u5176\u901a\u77e5\u6211\u4eec\u670d\u52a1\u7684\u72b6\u51b5\uff0cMemory Analyzer \u53ef\u4ee5\u505a\u66f4\u8be6\u7ec6\u7684\u53d8\u91cf\u7ea7\u522b\u5185\u5b58\u4f7f\u7528\u67e5\u770b\uff1b\\n* \u5185\u5b58\u7684\u4e0d\u5408\u7406\u4f7f\u7528\u6709\uff1a\u5c06\u5927\u6587\u4ef6\u5b8c\u6574\u52a0\u8f7d\u81f3\u5185\u5b58\uff0c\u5c06\u6574\u4e2a\u8868\u7684\u6570\u636e\u8bfb\u53d6\u81f3\u5185\u5b58\uff1b\\n* \u5185\u5b58\u6cc4\u6f0f\uff1a\u5fae\u89c2\u7684\u6709\u5bf9\u8c61\u7684\u5f15\u7528\u95ee\u9898\uff0c\u5b8f\u89c2\u7684\u6709\u8fde\u63a5\u672a\u91ca\u653e\u7b49\uff1b\\n* Xmx \u8bbe\u7f6e\u540e\uff0c\u5bf9\u7cfb\u7edf\u6765\u8bf4\u662f\u5df2\u4f7f\u7528\u7684\uff0c\u6240\u4ee5\u6211\u4eec\u9700\u8981\u540c\u65f6\u5173\u6ce8\u7cfb\u7edf\u7ea7\u522b\u548c JVM \u7ea7\u522b\u7684\u5185\u5b58\u4f7f\u7528\u60c5\u51b5\u3002\\n![][image-1]\\n","tokens":64,"id":2454}
{"File Name":"feathers\/002-personal-assistant.md","Context":"### Context\\nSooner or later, experienced developers find themselves carrying a toolkit from job to job, even job to home. This toolkit might contain: favorite shell configs, aliases, color schemes, keymappings, and so on. They may have also developed some sort of \"launcher\" that quickly opens up logs, dashboards, and frequently accessed pages and documents, in 2-3 keystrokes. Some go as far as integrating mouse \/ keyboard automations, even pixel-scanning to help those automations.\\nI've personally known several such developers (VM, AP, TB, EO) besides myself, and having listened to their process and demonstrations, I'm certain about one thing: Toolkits are not for sharing.\\nThe moment someone even thinks about having a toolkit, they've desired something truly for their self.\\n_TODO_\\n### Approaches\\n#### 1. TODO\\nTODO\\n#### 2. TODO\\nTODO\\n#### 3. TODO\\nTODO\\n### Decision\\nTODO\\n### Accepted Tradeoffs\\nTODO\\n### Retrospective\\nTODO\\n","Decision":"TODO\\n### Accepted Tradeoffs\\nTODO\\n### Retrospective\\nTODO\\n","tokens":217,"id":4922}
{"File Name":"csw-backend\/0006-use-a-relational-database.md","Context":"## Context\\nWe needed some persistent storage of audit results.\\nWe considered:\\n### Schemaless - DynamoDb\\nThis would be the most obvious choice for a lambda based\\nservice.\\nThe dynamo data model is tables of key value pairs.\\nThe main problem with Dynamo is the limit of I think 4K\\nper value.\\nOne of the things we wanted to do was briefly cache API\\nresponses which could easily breach that 4K limit.\\nWith Dynamo the access control is via IAM which would be\\nrelatively easy to manage and encryption at rest can\\nbe easily configured.\\n### Schemaless - MongoDb\\nMongo was a better fit for our service, saving JSON\\nrepresentations of API responses and resources.\\nThe problem with Mongo is it's not AWS native so we'd\\nhave to provision a server, manage the access controls,\\nmaintenance and availability.\\n### Relational - RDS MySQL \/ PostgreSQL\\nRDS gives us the benefit of running a managed service so\\nAWS are responsible for backups and patching minor\\nversion.\\nRelational databases give us the ability to do on-the-fly\\nanalysis more easily.\\nWe can store JSON as blob data although not ideal.\\nIf we want to re-use the database instance as a shared\\nresource across multiple services RDS is more capable.\\nIt's not unlikely that a future development may require\\ndjango.\\nPostgreSQL seems to be the general direction of travel\\nin GDS and is much more capable for a wider range of\\nuse-cases where we don't know what we're building in the\\nfuture.\\n","Decision":"Whilst none of the options were perfect we decided that\\na PostgreSQL RDS was the best option given the\\ninformation available to give us an infrastructure to\\nsupport multiple tools and services.\\n","tokens":328,"id":418}
{"File Name":"sesopenko_diamond_square\/ADR-1.md","Context":"Context goes here. Describe the forces at play, including technological political, social, and project local. These forces are likely in tension and should be called out as such. The language in this section is value-neutral. It is simply describing facts. Rationale should be self-evident from the context\\n","Decision":"This section describes our response to these forces. It is stated in full sentences, with active voice. \"We will ...\u201c\\n","tokens":61,"id":5045}
{"File Name":"klokwrk-project\/0006-contributing-back-to-open-source.md","Context":"## Context\\nQuite often, in commercial projects, reporting back to the community is entirely in the developer's hands. They are allowed to do this, but commonly there is no dedicated time in the plans.\\nOne of the ideas behind Project Klokwrk is to streamline development efforts as much as possible. As we are using many different open-source tools and libraries, it is in our best interest to try to\\nhelp with the issues we encountered.\\n### Architectural Context\\n* System (`klokwrk-project`)\\n","Decision":"**At least, we will report the issues in 3rd party open source software that are encountered during `klokwrk-project` development.** We'll strive to do this as consistently as possible. Besides being\\nconstructive community members, resolving encountered issues should benefit us with an easier and streamlined workflow.\\nWe will actively maintain [the list of reported issues and monitored issues](..\/..\/misc\/klokwrkRelatedIssuesInTheWild.md) that are relevant for `klokwrk-project.`\\nPutting open source contribution in ADR makes it an official guideline and moral obligation for all `klokwrk-project` members.\\n","tokens":108,"id":4887}
{"File Name":"WorkoutLog\/0003_edit_decision_from_fragment.md","Context":"## Context\\nThe ViewModel is responsible for the data, so i tried an attempt to notify the ViewModel from within the view that the\\nuser clicked the \"edit\" menu item. The ViewModel exposed a LiveData object which was observed by the view to trigger the\\nnavigation to a edit fragment.\\nAfter editing the workout and clicked saved, the edit fragment was opened again instantly. This was caused by the\\nViewModel which triggered the edit event again.\\n","Decision":"The view (DetailviewWorkoutFragment) no longer notifies the ViewModel about the edit action but directly calls the\\nnavigation component to open the EditWorkoutFragment, and therefore decides what should be done after the edit action\\nwas clicked (normally the view should not make this kind of decision).\\n","tokens":90,"id":1233}
{"File Name":"fxa\/0012-next-two-factor-authentication.md","Context":"## Context and Problem Statement\\nFirefox Account originally implemented multi-factor authentication (MFA) support in Q1 of 2018.\\nThis feature used TOTP based codes and was based on RFC 6238.\\nAdditionally, if users lost their MFA device, they could use one time backup authentication codes to regain access to the account.\\nHaving MFA support has helped secure our users' accounts and given them more security flexibility.\\nHowever, over time it has become more obvious that users that lose their MFA device (usually phone) are at risk of getting locked out of their account because they don't save their backup authentication codes.\\nThere are a non-trivial amount of users that don\u2019t save or download their backup authentication codes, which is currently the only way they can regain access.\\nIn 2019 Q4, FxA started requiring that users confirm backup authentication codes before enabling MFA.\\nWhile this did help reduce lockouts, we still want to reduce it further.\\nWe believe that adding a new MFA method to Firefox Accounts that has the similar security properties as TOTP would allow users to have another method to recover their account.\\n## Decision Drivers\\n- Improve user's account security\\n- Reduce the risk of account lockout because of lost device\/backup authentication code\\n- Could be completed in roughly a quarter\\n","Decision":"- Improve user's account security\\n- Reduce the risk of account lockout because of lost device\/backup authentication code\\n- Could be completed in roughly a quarter\\nChosen option: Option A, because this is more inline with Mozilla's security and privacy principles than option B.\\nThere is less security risk to users if this feature is added.\\n","tokens":274,"id":366}
{"File Name":"libelektra\/library_split.md","Context":"## Problem\\nOnly libelektra-core is supposed to access private data but this contradicts the goal to keep the library minimal.\\n`kdbprivate.h` was too generic, it contained many other parts next to the struct definitions of Key\/KeySet.\\n","Decision":"Also allow `libelektra-operations` library to access private Key\/KeySet.\\nPut struct definitions of Key\/KeySet in a separate header file, which gets\\nincluded by parts that need it\\n- none\\n","tokens":55,"id":1286}
{"File Name":"social-care-architecture\/0002-use-iso-8601-format-for-dates.md","Context":"## Context\\nThe system is composed of a number of API and related datastores. There are currently a number of differing date formats in use.\\n","Decision":"We will use the ISO 8601 format for dates: yyyy-mm-dd and times.\\n","tokens":30,"id":2173}
{"File Name":"editions\/05-\u2705-archiver-s3-event.md","Context":"## Context\\nThe archiver lambda needs to be able to respond to issue published events from tools.\\n","Decision":"For the archiver to react to S3 file created events from the fronts tool.\\n","tokens":21,"id":667}
{"File Name":"dotcom-rendering\/019-remove-monorepo.md","Context":"## Context\\nWe used to have a monorepo with multiple packages, including `guui` and `design`. However, once these packages were migrated to `src-foundation` we were left with a monorepo with only one package.\\n","Decision":"Remove the use of Yarn Workspaces which was being used for the monorepo. Restore a basic yarn package, merging dependencies.\\n","tokens":51,"id":2666}
{"File Name":"TDD-hexagonal-project\/2020_12_02_9_35_TECHNICAL.md","Context":"### Context\\nThis is the beginning on the project and for the moment all the commit are done on the master branch\\n### Decision\\nUse Gitflow to handle code source and project management.\\n### Consequences\\nInstall the gitflow plugin, explain how to contribute to the project with gitflow in the how to contribute documentation file.\\n","Decision":"Use Gitflow to handle code source and project management.\\n### Consequences\\nInstall the gitflow plugin, explain how to contribute to the project with gitflow in the how to contribute documentation file.\\n","tokens":68,"id":3288}
{"File Name":"converge\/001-data-diff.md","Context":"## Context\\nWe need to provide a nicely ergonomic API to programs using Converge,\\nand we wanted to adhere closely to the Clojure Way (TM).  We decided\\nto use the Atom API of swap!\/reset!\/deref.\\nWhen altering the value of the ConvergentRef via swap!\/reset!, we need\\nto create a covering set of Ops to describe the change being made.  We\\nconsidered the following alternatives:\\n1. Using `clojure.data\/diff` on old-value and new-value, with metadata\\nfor Id accounting\\n2. Using [`differ`](https:\/\/github.com\/Skinney\/differ) on old-value\\nand new-value, with metadata for Id accounting\\n3. Using [`editscript`](https:\/\/github.com\/juji-io\/editscript) on\\nold-value and new-value, with metadata for Id accounting\\n4. Implementing the Map\/Vector interfaces a la\\n[Schism](https:\/\/github.com\/aredington\/schism\/blob\/master\/src\/schism\/impl\/types\/nested_map.cljc)\\nand do our tracking and accounting within those implementation\\n","Decision":"On initial analysis, it appears that #4 above is flawed, as\\nimplementing our own tracking\/accounting nested map\/list types\\nwouldn't maintain the necessary context to translate to an opset.\\nSo that left us with a strategy based on diffing\/patching. Based on\\nanalysis in [this blog\\npost](https:\/\/juji.io\/blog\/comparing-clojure-diff-libraries\/), as well\\nas our own usage testing with `clojure.data\/diff`, `differ`, and\\n`editscript`, we have decided to use\\n[`editscript`](https:\/\/github.com\/juji-io\/editscript) for creating\\nopset patches.\\n","tokens":237,"id":978}
{"File Name":"beis-report-official-development-assistance\/0015-disable-auth0s-requirement-for-javascript-during-the-authentication-journey.md","Context":"## Context\\nThe welcome and the sign in journey currently bounce our user to pages controlled by Auth0. Auth0 give us 2 options called 'experiences' in the admin area under 'Universal Login':\\n1. Classic - requires Javascript to be enabled in the browser\\n2. New - no Javascript required\\nhttps:\/\/auth0.com\/docs\/universal-login\/new\\nThe [service standard does not mention failing or passing an assessment if the service doesn't work without JS](https:\/\/www.gov.uk\/service-manual\/service-assessments\/pre-july-2019-digital-service-standard) the guidance I can find is from the [service manual which advises that we should use progressive enhancement](https:\/\/www.gov.uk\/service-manual\/technology\/using-progressive-enhancement) to ensure that when a user doesn't have JS enabled, the service remains functional.\\n","Decision":"Use Auth0 in all environments without requiring Javascript.\\n","tokens":175,"id":2394}
{"File Name":"paas-team-manual\/ADR004-domain-naming-scheme.html.md","Context":"## Context\\nAs part of our deployment we have a pipeline, where changes that are made can move from a development environment through to production illustrated thusly:\\n![pipeline image](..\/images\/pipeline.jpg)\\nThere are a number of externally available endpoints that are accessed to manage and view information about the platform, as well as issue commands via the Cloud Foundry API. In addition to this, a URL also needs to be available to access Apps hosted on the platform. These need to be accessed via some sort of sensible URL.\\nThe reason for splitting system domains from app domains was to prevent applications from stealing traffic to CF components (for example, api.<domain>) or masquerading as official things of the platform (for example, signup.<domain>).\\n### Naming considerations\\nA number of aspects were considered as part of the naming process.\\n* Clear sense of purpose\\n* Clear distinction between Production and other Environments\\n* No overly technical names (for example, hosting\/paas\/scalable-elastic-public-government-hosting)\\n* Prevent possibility of domains suggesting 'live' service, for example if we allowed [app name].paas.gov.uk it could appear as thought they were live services.\\n","Decision":"For _non_ production environments we will be using the following domains:\\n* [environment name].cloudpipeline.digital\\n* [app name].[environment name].cloudpipelineapps.digital\\nFor our production environment we will be using the following domains:\\n* cloud.service.gov.uk\\n* [app name].cloudapps.digital\\nIt is important to note that live services will 'Bring Your Own' domain, apps available at cloudapps.digital are not live 'production' applications.\\n","tokens":247,"id":198}
{"File Name":"verify-service-provider\/0026-development-mode-for-the-verify-service-provider.md","Context":"## Context\\nInteracting with the Compliance Tool (CT) is not a straightforward process and the API could be better.\\nWhen a user is using the VSP for the first time it can be frustrating having to make sense of the CT API in order to make progress.\\nIn order to get the VSP running with the CT so that can start development they need to:\\n1. Create a set of private keys and certificates\\n2. Configure the VSP to interact with the CT and use the newly generated keys\\nand certificates\\n3. Create a script that can initiate a session with the CT\\n4. Create a client that can interact with the VSP\\n5. Test the interaction between the client, VSP, and CT\\n","Decision":"The VSP will provide a development mode initialized by a Dropwizard command.\\nWhen starting in this mode the VSP will:\\n- create in-memory keys and self-sign certificates\\n- use a random UUID for an entity ID\\n- configure itself to read the CT's metadata\\n- configure itself to run in non-matching mode\\n- initialize a new session with the compliance tool.\\nA user of the VSP will be able start development mode by running the following\\ncommand from the shell:\\n```\\n.\/bin\/verify-service-provider development\\n```\\nThe user should also be able to run this mode on Windows.\\nThe command will provide additional options to the user so that they can\\ncontrol:\\n- the host the VSP will bind to\\n- the port the VSP will run on\\n- the location where the CT will send SAML responses to\\n- the contents of the matching dataset that the CT will use in SAML responses\\nIn order to simplify the implementation of this mode the behaviour will only be\\navailable to users of the 'non-matching' journey.\\n","tokens":151,"id":4444}
{"File Name":"cpd-adr\/0002-use-cucumber-for-testing.md","Context":"## Context\\n* Dev team discussion over what level to write tests at and the trade-offs involved.\\n* Discussion over existing cucumber PR https:\/\/github.com\/DFE-Digital\/early-careers-framework\/pull\/151\/files\\n* We already have cypress.io tests in place and are currently working out the best approaches.\\n* There is an overhead writing matchers for cucumber.\\n* It is repeatedly useful having a description of the flows in the system:\\n* To share with accessibility (a11y) testers\\n* To share with security\/penetration testers\\n* To share (and possibly edit) with\/by non-coder team-members such as Business Analysts and our cross-team testing expert.\\n* Details of exactly what to put in gherkin syntax files and what to put in pure code tests will require some judgement.\\n* Consistency across the several projects involved is valued as some people are working across all of these projects.\\n","Decision":"* Use cucumber to describe all flows that are of significance to the business, notably the \"happy paths\".\\n* Use pull request reviews to refine exactly what to put in gherkin files versus pure-code tests.\\n","tokens":194,"id":2301}
{"File Name":"TechChallengeApp\/0006-environment-variables-overrides-config-file.md","Context":"## Context\\nIn some environments the port and other variables are only available on startup, and will have to be overriden. See https:\/\/github.com\/servian\/TechChallengeApp\/issues\/21\\n","Decision":"Add environment variables overrides back\\n","tokens":42,"id":4584}
{"File Name":"tendermint-rs\/adr-009-transport-agnostic-peer-abstraction.md","Context":"## Context\\nWith the opportunity to design and implement the peer-to-peer stack from\\nscratch in the context of the Tendermint implementation in Rust, a lot of the\\nlearnings of the shortcomings of the original Go implementation can be used to\\nprevent certain mistakes. Namely two:\\n* Leakage of physical concerns into the core domain\\n* Flexibility to adopt different wire protocols for transport of messages\\nFor that, the first set of newly introduced concepts will attempt to be generic\\nover which transport is used to connect and converse with other peers. Given\\nstrongly tailored abstract interfaces, concrete implementations will be easy to\\nspin up and plug into the machinery which lifts bytes from the wire into the\\ncore domain and transports messages into the rest of the system.\\n","Decision":"### Transport\\nWrapping the design is the `Transport`. Modelled with the properties of\\na physical network endpoint in mind, which can be bound and stopped. It should\\nstrongly correspond to the acquisition and lifecycle management of network\\nresources on the system.\\n``` rust\\npub trait Transport {\\ntype Connection: Connection;\\ntype Endpoint: Endpoint<Connection = <Self as Transport>::Connection>;\\ntype Incoming: Stream<Item = Result<<Self as Transport>::Connection>> + Send + Sync;\\nasync fn bind(self, bind_info: BindInfo) -> Result<(Self::Endpoint, Self::Incoming)>;\\n}\\n```\\nAfter the successful bind the caller holds an `Endpoint` as well as a stream of\\nincoming `Connection`s. Which is a standardised way to connect to new peers and\\nreact to newly connected ones respectively.\\n``` rust\\npub trait Endpoint: Send + Sync {\\ntype Connection;\\nasync fn connect(&self, info: ConnectInfo) -> Result<Self::Connection>;\\nfn listen_addrs(&self) -> Vec<SocketAddr>;\\n}\\n```\\nCenterpiece of the whole shebang is the `Connection`. It represents a connected\\npeer and provides the primitives to get data and send data from a peer. It is\\ndesigned with the outlook to support stream based transports down the road.\\nWhile being open to enable feature parity with current production installations\\nbased on tendermint-go's `MConn`.\\n``` rust\\npub trait StreamSend: Send + Sync {\\nasync fn send<B: AsRef<[u8]>>(msg: B) -> Result<()>;\\n}\\npub trait Connection: Send + Sync {\\ntype Error: std::error::Error + Send + Sync + 'static;\\ntype StreamRead: Stream<Item = Result<Vec<u8>>> + Send;\\ntype StreamSend: StreamSend;\\nfn advertised_addrs(&self) -> Vec<SocketAddr>;\\nasync fn close(&self) -> Result<()>;\\nfn local_addr(&self) -> SocketAddr;\\nasync fn open_bidirectional(\\n&self,\\nstream_id: StreamId,\\n) -> Result<(Self::StreamRead, Self::StreamSend), Self::Error>;\\nfn public_key(&self) -> PublicKey;\\nfn remote_addr(&self) -> SocketAddr;\\n}\\n```\\n### Peer\\nGiven a correct implementation of a `Transport` and its `Connection` newly\\nestablished ones will be wrapped with a `Peer`. Which is in charge of setting\\nup the correct streams on the `Connection` and multiplex messages - incoming\\nand outgoing alike - efficiently. It's also an attempt to enforce\\ncorrect-by-construction constraints on the state machine of the peer. To avoid\\nmisuse or unexpected transitions. The only way to construct is, is from an\\nexisting connection which gives the caller a connected peer. When invoking run\\non that one a fully function peer is \"returned\". Therefore the states look\\nlike: `Connected -> Running -> Stopped`.\\n``` rust\\nimpl<Conn> Peer<Connected<Conn>>\\nwhere\\nConn: Connection,\\n{\\npub async fn run(self, stream_ids: Vec<StreamId>) -> Result<Peer<Running<Conn>>> {\\n\/\/ ...\\n}\\nasync fn stop(self) -> Result<Peer<Stopped>> {\\n\/\/ ...\\n}\\n}\\nimpl<Conn> Peer<Running<Conn>>\\nwhere\\nConn: Connection,\\n{\\npub async fn send(&self, message: message::Send) -> Result<()> {\\n\/\/ ...\\n}\\npub async fn stop(self) -> Result<Peer<Stopped>> {\\n\/\/ ...\\n}\\n}\\n```\\nWhile sending messages is done through a method on a running peer, getting hold\\nof incoming messages can be achieved by draining the `Receiver` part of the\\nrunning state.\\n### Supervisor\\nThe `Supervisor` is the main entry point to the p2p package giving higher-level\\ncomponents access to a unified stream of peer events and messages as well as\\nthe ability to control peer lifecycle (connect, disconnect, etc.).\\n``` rust\\npub enum Command {\\nAccept,\\nConnect(SocketAddr),\\nDisconnect(node::Id),\\nMsg(node::Id, message::Send),\\n}\\npub enum Event {\\nConnected(node::Id, Direction),\\nDisconnected(node::Id, Report),\\nMessage(node::Id, message::Receive),\\nUpgraded(node::Id),\\nUpgradeFailed(node::Id, Report),\\n}\\nstruct CommandHandle;\\nimpl CommandHandle {\\nfn instruct(command: Command) -> Result<()> {\\n\/\/ ..\\n}\\n}\\nimpl Supervisor {\\npub fn new<T>(transport: T) -> Self\\nwhere\\nT: transport::Transport + Send + 'static,\\n{\\n\/\/ ..\\n}\\npub handle(&self) -> CommandHandle {\\n\/\/ ..\\n}\\npub subscribe(&self) -> Receiver<Event> {\\n\/\/ ..\\n}\\npub async fn run<T>(self) -> Result<()> {\\n\/\/ ...\\n}\\n}\\n```\\n","tokens":155,"id":4313}
{"File Name":"ftd-scratch3-offline\/0014-only-a-single-scratch-hat-is-supported.md","Context":"## Context\\nScratch can run many hats at the same time.\\nThis could be implemented on the arduino using cooperative multi-threading.\\nHowever this would be non trivial to get right.\\nThe ftduino library has not been built with multi-threading in mind as hasn't the Arduino standard library.\\nAlso program space is limited and more complex programs already take up most of the memory.\\nAdding a user mode scheduler and everything that is needed for cooperative multi-threading will likely exhaust the available memory.\\n","Decision":"We will only support a single scratch hat, i.e., multi-threading will not be supported.\\n","tokens":104,"id":2640}
{"File Name":"SearchServices\/0001-record-architecture-decisions.md","Context":"## Context\\nCapture and record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":15,"id":5132}
{"File Name":"pupperware-party\/0003-use-readme-driven-development.md","Context":"## Context\\nDocumentation for a project is critical - without it, you don't know if a project is \"complete\", or as a user, how to use it.  However, too much documentaion can delay the project, or lead to a good implementation of the wrong thing. This process, and the reasoning is much better explained at [this blog post](http:\/\/tom.preston-werner.com\/2010\/08\/23\/readme-driven-development.html)\\n","Decision":"This project will be developed using the readme driven decelopment process.\\n","tokens":97,"id":2848}
{"File Name":"sfpowerscripts\/002-parallel-development-process.md","Context":"## Context and Problem Statement\\nWhen creating a parallel development streams (e.g. release), there are a list of manual steps that must be performed:\\n- package versions may need to be updated so that packages between streams do not share the same version-space\\n- a new artifact feed or npm tags need to be created\\n- a set of artifacts needs to be created for the new development stream\\n","Decision":"All this issues arised from the fact that with the assumption of using multiple feeds. As we are moving to ask users to utilize a single feed\/artifact repository and how it is in most platforms like GitHub or GitLab, there is no specific need for a helper tool. Users should be versioning their artifacts using semantic version when dealing with multiple development streams\\n","tokens":80,"id":4776}
{"File Name":"rfcs\/0000-public-events.md","Context":"## Context\\n[context]: #context\\nIn addition to entries (= places) every user should be able to create and edit events.\\n> This section describes the forces at play, including technological, political, social, and project local. These forces are probably in tension, and should be called out as such. The language in this section is value-neutral. It is simply describing facts.\\n> MVP: Termine k\u00f6nnen genauso wie Initiativen und\\nUnternehmen eingetragen und verschlagwortet werden\\n> NTH:\\n> - Termine k\u00f6nnen wie in Trello als Kalenderansicht dargestellt\\nwerden.\\n> - Termine k\u00f6nnen \u00fcber Webdav ausgelesen und bei\\nentsprechender Berechtigung auch eingelesen werden.\\n(eingeloggte Nutzer, admin entsprechender Schlagworte)\\n","Decision":"[decision]: #decision\\n> This section describes our response to these forces. It is stated in full sentences, with active voice. \"We will ...\"\\n","tokens":178,"id":1877}
{"File Name":"amf\/0010-shape-annotation-types.md","Context":"## Context\\nWhen AMF builds a model from a content, it must justify from where each node and property came from, so the users can track and know for sure the origin of the values. This source map information is fundamental for editing features.\\nIn a parsed API, the elements declared in the source document have their corresponding annotations (think of `LexicalInformation` or `SourceNode`) in the final model, but some elements in the AMF model are not explicitly declared. Moreover, some of this not-declared elements are fields we can easily infer from the source document (like the name of an operation) while others have no real counterpart in the source document.\\n","Decision":"For these reasons, the decision has been made in favor of adding 3 annotation types to make every object or field traceable to where it came from, whether it was from the source document or it was generated by AMF:\\n- `Annotations.inferred()`\\n- A field is **inferred** when the entry has been inferred but the value is real (is present in the source document)\\n- Example: The name of an operation, which has no entry `name` but we can infer it from its declaration in the document\\n- `Annotations.virtual()`\\n- An object is **virtual** when it's generated based on one or more elements in the document, but doesn't have a specific declaration\\n- Does not stop model traversal because its children nodes may have sourcemaps\\n- Many objects in the AMF Model are virtual containers of information present in the document\\n- Examples:\\n- The `Request` object in the AMF model is virtual, because you can't assign a sourcemap to it, but the elements under it (like parameters) do have a specific definition and location in the source document\\n- `Annotations.synthesized()`\\n- A field is **synthesized** when the entry as well as the value are generated, it's not in the document\\n- This field and its value have nothing that matches it on the AST, and the same is true for its children\\n- Stops model traversal (won't look into value or its children)\\n- You can't have a virtual value in a synthesized field, because that means that the value is not entirely made up\\n- Examples:\\n- `PropertyShape.MinCount` when it's set to 1 based on a `required: true`\\n- Creating a default value when there is no value declared in the document\\n","tokens":137,"id":1394}
{"File Name":"gatemint-sdk\/adr-008-dCERT-group.md","Context":"## Context\\nIn order to reduce the number of parties involved with handling sensitive\\ninformation in an emergency scenario, we propose the creation of a\\nspecialization group named The Decentralized Computer Emergency Response Team\\n(dCERT).  Initially this group's role is intended to serve as coordinators\\nbetween various actors within a blockchain community such as validators,\\nbug-hunters, and developers.  During a time of crisis, the dCERT group would\\naggregate and relay input from a variety of stakeholders to the developers who\\nare actively devising a patch to the software, this way sensitive information\\ndoes not need to be publicly disclosed while some input from the community can\\nstill be gained.\\nAdditionally, a special privilege is proposed for the dCERT group: the capacity\\nto \"circuit-break\" (aka. temporarily disable)  a particular message path. Note\\nthat this privilege should be enabled\/disabled globally with a governance\\nparameter such that this privilege could start disabled and later be enabled\\nthrough a parameter change proposal, once a dCERT group has been established.\\nIn the future it is foreseeable that the community may wish to expand the roles\\nof dCERT with further responsibilities such as the capacity to \"pre-approve\" a\\nsecurity update on behalf of the community prior to a full community\\nwide vote whereby the sensitive information would be revealed prior to a\\nvulnerability being patched on the live network.\\n","Decision":"The dCERT group is proposed to include an implementation of a `SpecializationGroup`\\nas defined in [ADR 007](.\/adr-007-specialization-groups.md). This will include the\\nimplementation of:\\n- continuous voting\\n- slashing due to breach of soft contract\\n- revoking a member due to breach of soft contract\\n- emergency disband of the entire dCERT group (ex. for colluding maliciously)\\n- compensation stipend from the community pool or other means decided by\\ngovernance\\nThis system necessitates the following new parameters:\\n- blockly stipend allowance per dCERT member\\n- maximum number of dCERT members\\n- required staked slashable tokens for each dCERT member\\n- quorum for suspending a particular member\\n- proposal wager for disbanding the dCERT group\\n- stabilization period for dCERT member transition\\n- circuit break dCERT privileges enabled\\nThese parameters are expected to be implemented through the param keeper such\\nthat governance may change them at any given point.\\n### Continuous Voting Electionator\\nAn `Electionator` object is to be implemented as continuous voting and with the\\nfollowing specifications:\\n- All delegation addresses may submit votes at any point which updates their\\npreferred representation on the dCERT group.\\n- Preferred representation may be arbitrarily split between addresses (ex. 50%\\nto John, 25% to Sally, 25% to Carol)\\n- In order for a new member to be added to the dCERT group they must\\nsend a transaction accepting their admission at which point the validity of\\ntheir admission is to be confirmed.\\n- A sequence number is assigned when a member is added to dCERT group.\\nIf a member leaves the dCERT group and then enters back, a new sequence number\\nis assigned.\\n- Addresses which control the greatest amount of preferred-representation are\\neligible to join the dCERT group (up the _maximum number of dCERT members_).\\nIf the dCERT group is already full and new member is admitted, the existing\\ndCERT member with the lowest amount of votes is kicked from the dCERT group.\\n- In the split situation where the dCERT group is full but a vying candidate\\nhas the same amount of vote as an existing dCERT member, the existing\\nmember should maintain its position.\\n- In the split situation where somebody must be kicked out but the two\\naddresses with the smallest number of votes have the same number of votes,\\nthe address with the smallest sequence number maintains its position.\\n- A stabilization period can be optionally included to reduce the\\n\"flip-flopping\" of the dCERT membership tail members. If a stabilization\\nperiod is provided which is greater than 0, when members are kicked due to\\ninsufficient support, a queue entry is created which documents which member is\\nto replace which other member. While this entry is in the queue, no new entries\\nto kick that same dCERT member can be made. When the entry matures at the\\nduration of the  stabilization period, the new member is instantiated, and old\\nmember kicked.\\n### Staking\/Slashing\\nAll members of the dCERT group must stake tokens _specifically_ to maintain\\neligibility as a dCERT member. These tokens can be staked directly by the vying\\ndCERT member or out of the good will of a 3rd party (who shall gain no on-chain\\nbenefits for doing so). This staking mechanism should use the existing global\\nunbonding time of tokens staked for network validator security. A dCERT member\\ncan _only be_ a member if it has the required tokens staked under this\\nmechanism. If those tokens are unbonded then the dCERT member must be\\nautomatically kicked from the group.\\nSlashing of a particular dCERT member due to soft-contract breach should be\\nperformed by governance on a per member basis based on the magnitude of the\\nbreach.  The process flow is anticipated to be that a dCERT member is suspended\\nby the dCERT group prior to being slashed by governance.\\nMembership suspension by the dCERT group takes place through a voting procedure\\nby the dCERT group members. After this suspension has taken place, a governance\\nproposal to slash the dCERT member must be submitted, if the proposal is not\\napproved by the time the rescinding member has completed unbonding their\\ntokens, then the tokens are no longer staked and unable to be slashed.\\nAdditionally in the case of an emergency situation of a colluding and malicious\\ndCERT group, the community needs the capability to disband the entire dCERT\\ngroup and likely fully slash them. This could be achieved though a special new\\nproposal type (implemented as a general governance proposal) which would halt\\nthe functionality of the dCERT group until the proposal was concluded. This\\nspecial proposal type would likely need to also have a fairly large wager which\\ncould be slashed if the proposal creator was malicious. The reason a large\\nwager should be required is because as soon as the proposal is made, the\\ncapability of the dCERT group to halt message routes is put on temporarily\\nsuspended, meaning that a malicious actor who created such a proposal could\\nthen potentially exploit a bug during this period of time, with no dCERT group\\ncapable of shutting down the exploitable message routes.\\n### dCERT membership transactions\\nActive dCERT members\\n- change of the description of the dCERT group\\n- circuit break a message route\\n- vote to suspend a dCERT member.\\nHere circuit-breaking refers to the capability to disable a groups of messages,\\nThis could for instance mean: \"disable all staking-delegation messages\", or\\n\"disable all distribution messages\". This could be accomplished by verifying\\nthat the message route has not been \"circuit-broken\" at CheckTx time (in\\n`baseapp\/baseapp.go`).\\n\"unbreaking\" a circuit is anticipated only to occur during a hard fork upgrade\\nmeaning that no capability to unbreak a message route on a live chain is\\nrequired.\\nNote also, that if there was a problem with governance voting (for instance a\\ncapability to vote many times) then governance would be broken and should be\\nhalted with this mechanism, it would be then up to the validator set to\\ncoordinate and hard-fork upgrade to a patched version of the software where\\ngovernance is re-enabled (and fixed). If the dCERT group abuses this privilege\\nthey should all be severely slashed.\\n","tokens":292,"id":27}
{"File Name":"libelektra\/rest_api_documentation.md","Context":"## Problem\\nA standard way of describing REST APIs offered by tools and plugins for Elektra is required to ease development for and usage of these. Because many good standards for describing APIs are out there already, an existing one shall be used.\\n","Decision":"The decision is to use [API blueprints](https:\/\/apiblueprint.org\/) together with additional tools from its ecosystem.\\n","tokens":50,"id":1298}
{"File Name":"adr-playground\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":2027}
{"File Name":"deeplearning4j\/0001-kotlin_dsl_as_source_of_truth.md","Context":"## Context\\nThis code generation experiment is meant to be our starting point for both the API unification for ND4J and SameDiff,\\nand the multi-language support. For this reason we have to define ops, or their interface, in a language neutral way.\\nThe initial idea was to use a Language Workbench like MPS. This had to be discarded because of bugs and limitations\\nencountered while trying to define a language that would work for a few simple examples.\\nThe next idea was to use Ops defined in JSON files. This would have allowed us to define Ops as human readable data and\\nread and write those files from any programming language. However, the drawback with this approach is that writing json\\nmanually invites many problems if written manually (e.g. typos, bad structuring, having to look up the proper keys,...).\\nIn order to rectify that drawback, we would have to create custom tooling, that we would have to maintain and that\\ncontributors would have to use.\\nUsing a Java builder pattern based approach is very verbose.\\n","Decision":"We use a Kotlin-based DSL to define Ops.\\n","tokens":218,"id":2939}
{"File Name":"knot-documentation\/ARC-001-fog-architecture-stage-1.md","Context":"## Context\\nUntil the version KNoT-v02.00, we were using the Meshblu platform as our fog computing service. However, it has presented some issues related mainly to the performance, maintainability and scalability properties. Because of that, we have decided to create a new fog architecture and use open-source projects as its building blocks. The new fog service should contain device\/user management, data storage, functions, and intermittent connection support. It needs also to be lightweight and reusable.\\nIn this context, we choose to separate the development process in stages. The decisions related to the first stage are listed in the next section.\\n","Decision":"What do we want to achieve in this stage?\\n* Things should have digital representations in the fog.\\n* These things should be owned by an application.\\n* The knotd service should be able to operate on the fog (keeping the ability to remains operating directly on the cloud if needed).\\n* If the device can't be registered on the cloud the connector will keep trying to register it. This will probably change when we decide about the sync mechanism (what should be proposed in another decision proposal document).\\nThe following image illustrates the new fog architecture and its components responsibilities are described below.\\n<p align=\"center\">\\n<img src=\"images\/high_level_arch_stage_1.png\" width=\"700\" height=\"400\">\\n<p>\\n### Responsibilities:\\n* **knotd**: make possible non-IP devices to communicate with the platform by receiving messages through Unix socket and DBus technologies. In this scenario, the messages are coming in a KNoT binary format and the knotd translate it to JSON, which is a format supported by the fog\/cloud services. This service sends the translated messages to a message-oriented middleware that supports AMQP protocol.\\n* **Babeltower**: orchestrate the communication with the fog services, acting both as a proxy and a protocol translator since the internal operations can be made to services that support different application protocols. The devices that support IP connectivity can connect directly to it.\\nThe following image shows the relationship between the knotd and the babeltower according to the responsibilities described above.\\n<p align=\"center\">\\n<img src=\"images\/high_knot_babeltower_stage_1.png\" width=\"600\" height=\"300\">\\n<\/p>\\n* **App management**: manage the app\u2019s accounts by providing an interface to create them and generate tokens to operate on the things. Considering the confusion when defining the difference between the developer user and the final user, we have decided to consider this service as the developer\/admin user manager for now and renaming it to 'app' instead of 'user'.\\n* **Thing management**: manage things by providing an interface to create, remove, list and update them. This service will store the thing contextual information and any metadata that can be necessary further.\\n.\\n* **Connector**: create a bridge between the supported KNoT operations and the target cloud ones. This service defines an interface that must be implemented by a library that communicates with the integrated cloud and knows it needs.\\n* **WebUI**: configure the gateway components. This service provides a web interface to set the cloud configurations (including the necessary steps for that specific cloud such as associating with a gateway in the case of KNoT Cloud). Moreover, a local user is created so that the developer can log in to add or remove devices as well as to reboot, factory reset or change the gateway hostname.\\n","tokens":129,"id":3338}
{"File Name":"simple-android\/009-mdc-migration.md","Context":"## Context:\\nWe have been using `Theme.Design.*` theme from the start, while that worked perfectly it did not allow us to use new material components because those\\nare dependent on `Theme.MaterialComponents.*` app theme. Using new material components allows us to have much more control over component theming like\\ntext styles, icon & icon gravity, strokes, corners, etc.\\n- Right now we are using 3 different kind of buttons to satisfy various design requirements `Button`, `OmegaCenterIconButton` & `PrimarySolidButton`.\\n- Since all our Figma designs are made using material specs\/material components, the design specs for certain components didn\u2019t translate properly to\\nappcompat variants or there are no components available in appcompat at all. Moving to MDC will make the design to layout\/style much easier.\\nYou can find more information on setting up MDC Android [here](https:\/\/material.io\/develop\/android\/docs\/getting-started\/).\\n","Decision":"Moving to MDC will allow us to use `MaterialButton` & other material components if needed. You can find more material components supported by Android\\nand how to theme them [here](https:\/\/material.io\/components).\\nWith `MaterialButton` we can finally use icon, icon gravity, icon tint. This migration will also open better theming support for material components\\nsuch as corner radius, strokes, shapes etc., Since we no longer need 3 types of buttons we will remove\\n`PrimarySolidButton` & `OmegaCenterIconButton`, we will be using `MaterialButton` in place of `Button`.\\nThese are the changes that occur due to this migration.\\n### Button tag change:\\n- We will start using `com.google.android.material.button.MaterialButton` in views instead of `Button`.\\n- In order to use icon in a `MaterialButton` we will use the `icon` attribute instead of `drawableStart` or `drawableEnd` attr.\\n- We will use `iconGravity` (`textStart`, `viewStart`, `textEnd`, `viewEnd`) for setting the icon position, `iconTint` for setting the icon color.\\n### Style change:\\n- `theme_material.xml` will house the main app theme which now extends `Theme.MaterialComponents.Light.NoActionBar`, some of the new attrs we set in\\napp theme.\\n```\\n<style name=\"Clinic.V2.Theme\" parent=\"Theme.MaterialComponents.Light.NoActionBar\">\\n<item name=\"materialButtonStyle\">@style\/Clinic.V2.MaterialButton<\/item>\\n<item name=\"materialButtonOutlinedStyle\">@style\/Clinic.V2.OutlineButton<\/item>\\n<item name=\"borderlessButtonStyle\">@style\/Clinic.V2.TextButton<\/item>\\n<item name=\"toolbarStyle\">@style\/Clinic.V2.ToolbarStyle<\/item>\\n<\/style>\\n```\\nThese attrs can be directly referenced in the view styles instead of using the entire style tag in the view.\\n- We have 4 primary button styles that we use in app\\n- `Clinic.V2.MaterialButton`\\n- `Clinic.V2.TextButton`\\n- `Clinic.V2.OutlineButton`\\n- `Clinic.Button.Flat`\\n- You can then style `MaterialButton` using any of these styles or attr.\\n- Filled Button : `?attr\/materialButtonStyle` or `@style\/Clinic.V2.MaterialButton`\\n- Text Button: `attr\/borderlessButtonStyle` or `@style\/Clinic.V2.TextButton`\\n- Outline Button: `?attr\/materialButtonOutlinedStyle` or `Clinic.V2.OutlineButton`\\n- Un-Elevated\/Flat Button: `@style\/Clinic.Button.Flat`\\n- You can extend any of those primary button styles to override certain attrs like color, for example:\\n```\\n<style name=\"Clinic.V2.MaterialButton.Green3\">\\n<item name=\"backgroundTint\">@color\/green3<\/item>\\n<item name=\"android:textColor\">@color\/green1<\/item>\\n<item name=\"iconTint\">@color\/green1<\/item>\\n<\/style>\\n```\\n","tokens":199,"id":1131}
{"File Name":"copilot\/0002-use-event-streaming-model-for-diego-actuallrp-syncing.md","Context":"## Context\\nThe diego ActualLRP syncing model as currently implemented will fetch all LRPs\\nacross all diego cells at a specified time interval (at the time of writing 10\\nseconds). As the ActualLRP count grows on a cloudfoundry deployment this could\\nimpact the performance of the BBS (large response sets coming back).\\n","Decision":"We want to use the [Event package](https:\/\/github.com\/cloudfoundry\/bbs\/blob\/master\/doc\/events.md)\\nto get the event stream for each ActualLRP. We will also use a bulk sync every\\n60 seconds to catch any events that were missed.\\n","tokens":75,"id":53}
{"File Name":"cli\/0001-record-architecture-decisions.md","Context":"## Context\\nAs we are building out v7 of the CLI, we need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":27,"id":3650}
{"File Name":"SoundCloudVisualizer\/0009-use-postcss.md","Context":"## Context\\nAs mentioned in [ADR # 8. Use CSS modules to create scoped CSS for components](0008-use-css-modules-to-create-scoped-css-for-components.md) this application has separate CSS files for each component's CSS. We now require a need to define commonly used variables such as colours, spacing and so on in a centralised location so that these values are not duplicated across the codebase.\\nThere were two solutions considered:\\n1. [Sass](https:\/\/sass-lang.com\/)\\n- Superset of CSS that compiles to CSS.\\n- Supports variables, nesting, partials, operators, and so on.\\n- Has two major implementations in JavaScript:\\n- [`sass`](https:\/\/www.npmjs.com\/package\/sass) which is implemented in Dart.\\n- [`node-sass`](https:\/\/www.npmjs.com\/package\/node-sass) which provides bindings to the C library LibSass.\\n2. [PostCSS](https:\/\/postcss.org\/)\\n- Transforms CSS with JavaScript plugins.\\n- Plugins that add support for Sass features such as variables, nesting, imports, and so on are available.\\n- Allows for auto-vendor prefixing via [`autoprefixer`].\\n","Decision":"This system will use PostCSS to process CSS.\\n","tokens":256,"id":1584}
{"File Name":"python-tuf\/0009-what-is-a-reference-implementation.md","Context":"## Context and Problem Statement\\nThe original goal for the reference implementation refactor was to provide an\\nimplementation which is both an aid to understanding the specification and a\\ngood architecture for other implementations to mimic.\\nDuring refactoring efforts on the metadata API and ngclient, several friction\\npoints have arisen where a safe object-oriented API would result in a less\\ndirect mapping to the [Document formats] in the specification.\\nThe archetypal example friction point is that [Timestamp] lists snapshot _only_\\nin a `meta` dictionary of `METAPATH` -> attribute fields. The dictionary will\\nonly ever contain one value and creates an extra level of indirection for\\nimplementations which try to map to the file format.\\nWhen presented with such cases, we have considered multiple options:\\n* Strict mapping to the [Document formats]\\n* Simple and safe API in preference to mapping to the [Document formats]\\n* Strict mapping to the [Document formats] with additional convenience API\\nwhich is documented as the preferred interface for users\\nSo far implementation has tended towards the final option, but this is\\nunsatisfying because:\\n* the API contains traps for the unsuspecting users\\n* two code paths to achieve the same goal is likely to result in inconsistent\\nbehaviour and bugs\\nTherefore, we would like to define our primary purpose so that we can make\\nconsistent decisions.\\n[Document formats]: https:\/\/theupdateframework.github.io\/specification\/latest\/#document-formats\\n[Timestamp]: https:\/\/theupdateframework.github.io\/specification\/latest\/#file-formats-timestamp\\n## Decision Drivers\\n* The reference implementation is often the starting point for new\\nimplementations, porting architecture of the reference implementation to new\\nlanguages\/frameworks\\n* Reading reference implementation code is a common way to learn about TUF\\n* The TUF formats include non-intuitive JSON object formats when mapping to OOP\\nobjects\\n* Multiple code paths\/API for the same feature is a common source of bugs\\n","Decision":"* The reference implementation is often the starting point for new\\nimplementations, porting architecture of the reference implementation to new\\nlanguages\/frameworks\\n* Reading reference implementation code is a common way to learn about TUF\\n* The TUF formats include non-intuitive JSON object formats when mapping to OOP\\nobjects\\n* Multiple code paths\/API for the same feature is a common source of bugs\\nPrimary purpose of the reference implementation is as an exemplary reference:\\nproviding a safe, consistent API for users and a good architecture for other\\nimplementations to mimic.\\n","tokens":408,"id":4252}
{"File Name":"figgy\/0003-preservation.md","Context":"## Context\\nWe have agreed that we will preserve digital objects by saving resources in\\nGoogle Cloud Storage in a directory structure which preserves both the binaries\\nthe resource is made up of as well as the JSON serialization of the resource\\nitself.\\n","Decision":"1. Preserving\\n1. We will preserve materials in Google Cloud Coldline Storage with\\n`versioning` enabled. Versions will be kept indefinitely and without\\nlimit. All files will go in a single bucket.\\n- Staging bucket is configured with the following command:\\n```\\ngsutil mb -c regional -l us-west1 -p pulibrary-figgy-storage-1 gs:\/\/figgy-staging-preservation\\necho '{\"rule\": [{\"action\": {\"type\": \"Delete\"}, \"condition\": {\"age\": 2}}]}' > lifecycle.json\\ngsutil lifecycle set lifecycle.json gs:\/\/figgy-staging-preservation\\nrm lifecycle.json\\ngsutil bucketpolicyonly set on gs:\/\/figgy-staging-preservation\\ngsutil iam ch serviceAccount:figgy-staging@pulibrary-figgy-storage-1.iam.gserviceaccount.com:objectAdmin gs:\/\/figgy-staging-preservation\\n```\\n- Production bucket is configured with the following command:\\n```\\ngsutil mb -c coldline -l us-west1 -p pulibrary-figgy-storage-1 gs:\/\/figgy-preservation\\ngsutil bucketpolicyonly set on gs:\/\/figgy-preservation\\ngsutil iam ch serviceAccount:figgy-preservation-production@pulibrary-figgy-storage-1.iam.gserviceaccount.com:objectAdmin gs:\/\/figgy-preservation\\ngsutil versioning set on gs:\/\/figgy-preservation\\n```\\n1. When a resource is `complete` and marked with the `cloud` preservation\\npolicy it will save itself and all resources contained in `member_ids` in\\na directory structure in Google Cloud Storage that looks like the\\nfollowing:\\n```\\n- <resource-id>\\n- data\\n- <child-id>\\n- <child-id>.json\\n- <binary.tif>\\n- <resource-id>.json\\n```\\n1. Children are preserved on save if their parents are preserved.\\n1. Related objects such as collections, Ephemera Terms, etc. will not be\\npackaged inside the preserved object. If it's important they be preserved,\\nthose objects should be marked with the `cloud` preservation policy.\\n1. When a FileSet is added to a resource which is already complete and marked\\nwith the `cloud` preservation policy, it will upload the new binary\\ncontent to both the repository and to Google Cloud Storage.\\n1. If a child is marked to be preserved, but its parent is not, it will still\\nsave in a nested directory structure, but will not automatically create\\nbackups of its parents.\\n1. This behavior will be attached to the ChangeSetPersister.\\n1. Packaging Details\\n1. When preserved a `PreservationObject` will be created in Figgy with a\\n`preserved_object_id` property which points to the object it's preserving.\\n1. Each `PreservationObject` will contain `FileMetadata` for the binary\\nobject as well as a serialized JSON file of the resource it's preserving.\\nOn upload to preservation, those items' checksums will be calculated and\\nstored on the `PreservationObject`.\\n1. JSON metadata will have the use `pcdm:PreservedMetadata` and binary\\ncontent will have the use `pcdm:PreservationCopy`\\n1. We will only keep the most recent version of any file, overwriting any\\nfiles which match the same file name, but relying on versioning to go back\\nif necessary.\\n1. When a preserved resource is deleted, we will delete its directory from\\npreservation storage. If we need to get it again, we will look at Google\\nCloud Storage's stored versions.\\n1. If a child's hierarchy changes (it moves parents), we will move the\\ncontent in the preservation storage to match.\\n1. When a file's binary content is replaced on disk, we will upload a new\\ncopy of the file to preservation and calculate a new checksum.\\n2. Fixity Checks\\n1. Technical details of fixity checking will occur in a later ADR.\\n1. A random subset of the preserved copies will have their files pulled down\\nfrom preservation storage, their checksums calculated as they're streamed,\\nand then compared to the checksum of the object stored locally.\\n1. In the case of a failure it will be reported to Figgy and displayed in a\\ndashboard for further follow-up and repair.\\n","tokens":50,"id":4820}
{"File Name":"operational-data-hub\/0028-a-solution-is-implemented-by-one-or-more-gcp-projects.md","Context":"## Context\\nA [solution facilitates a coherent set of business functions](0026-solution-facilitates-a-coherent-set-of-business-functions.md). Those functions can originate from multiple domains. A [project always belongs to a single domain](0027-a-gcp-project-belongs-to-a-single-domain.md). Therefore, a solution can be implemented by multiple projects, either due to the fact that it requires functions from multiple domains, or because projects allow better modularization of the solution, or both.\\n![Structure of projects, domains and solutions](solution_project_domain.png \"Projects in different domains implementing a solution\")\\n","Decision":"We implement a solution by one or more projects.\\n","tokens":126,"id":2760}
{"File Name":"ng-kaart\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\nADR tools: https:\/\/github.com\/npryce\/adr-tools\\n","tokens":16,"id":2077}
{"File Name":"drt-v2\/0005-use-diode.md","Context":"## Context\\nhttps:\/\/github.com\/suzaku-io\/diode\\nDiode provides a\\n","Decision":"Decision here...\\n","tokens":19,"id":1908}
{"File Name":"docs\/0013-source-to-image-workflow.md","Context":"## Context and Problem Statement\\nWe want to have a Source-to-Image workflow to import services based on a GitHub repository. It should run inside our Kubernetes cluster, however currently Kubernetes doesn't have a resource build-in that is able to build container images. Therefore another technology is required.\\n## Decision Drivers\\n* MUST run on our Kubernetes cluster\\n* MUST run completely in userspace (no root access required)\\n* MUST be sufficient to provide a single URL to a GitHub repository (with included Dockerfile)\\n* SHOULD be independent of any cloud service provider\\n","Decision":"* MUST run on our Kubernetes cluster\\n* MUST run completely in userspace (no root access required)\\n* MUST be sufficient to provide a single URL to a GitHub repository (with included Dockerfile)\\n* SHOULD be independent of any cloud service provider\\nChosen option: *Knative Build*, because it meets all of our criterion decision drivers. It allows us to implement a Source-to-Image workflow on our Kubernetes cluster independently to any cloud service provider.\\n### Positive Consequences\\n* By using *Knative Build* we have the choice to use different kinds of `Builders`, follow-up decision is required: [Building OCI images](.\/0015-building-oci-images.md)\\n### Negative consequences\\n* *Nothing known*\\n","tokens":114,"id":4683}
{"File Name":"link_platform\/0012-use-jest-and-enzyme-for-unit-testing.md","Context":"## Context\\nJest is a general JavaScript testing framework. Enzyme is a testing utility that makes it easier to assert, manipulate, and traverse React components. [This](https:\/\/medium.com\/welldone-software\/an-overview-of-javascript-testing-in-2018-f68950900bc3) article goes into detail about various testing alternatives and [this](https:\/\/www.codementor.io\/vijayst\/unit-testing-react-components-jest-or-enzyme-du1087lh8) article details using Jest with Enzyme.\\n","Decision":"Based on the familiarity of our engineers with Jest and Enzyme and the large community of support, use Jest and Enzyme to unit test our React application.\\n","tokens":112,"id":5020}
{"File Name":"api-docs\/0005-using-markdown-in-jekyll.md","Context":"## Context\\nJekyll supports the markdown syntax by default. Files must be prefixed\\nwith a YAML Front Matter section for the engine to categorize, format\\nand display the blog post\/page. Project documentation files (e.g.: `README.md`)\\ntaken from a project release bundle must not have YAML Front Matter section\\nor any Jekyll or third-party specific formatting but pure markdown syntax.\\n","Decision":"Create YAML Front Matter index files in the `\/_posts` directory,\\nlike `2017-01-01-readme.md` and use the `include_relative`\\n[Jekyll command](https:\/\/jekyllrb.com\/docs\/includes\/) to include\\nthe unmodified `README.md`. Create a new layout format in Jekyll\\nnamed `markdown.html` and use the `markdown` value specifying\\nthe `layout:` in the YAML Front Matter index file, see example:\\n```\\n2017-01-01-readme.md\\n---\\nlayout: markdown\\ntitle: Sample title here\\nweight: 3\\ncategory: a-category-here\\ncategoryItemType: documentation\\ncategoryItemIsShown: 0\\n---\\n{% include_relative README.md  %}\\n```\\nThe `markdown.html` should have the same contents as the `post.html`\\nwith an extra `article`, `section` tags surrounding the `{content}` tag\\nto keep formatting consistent.\\n","tokens":81,"id":5151}
{"File Name":"james\/0011-remove-elasticsearch-document-source.md","Context":"## Context\\nThough very handy to have around, the source field does incur storage overhead within the index.\\n","Decision":"Disable `_source` for ElasticSearch indexed documents.\\n","tokens":22,"id":2117}
{"File Name":"aws-lambda-benchmark\/0002-use-aws-codepipeline-with-aws-codebuild-to-build-and-deploy-project.md","Context":"## Context\\nThe project needs a way to be built and deployed. It needs to be quick and easy to use so has to be able to pick up changes to the git repo. It should be easy for anyone recreate the workflow used here and flexible enough to handle building multiple languages. At the end of successful build the artifact should get deployed.\\n","Decision":"As this is project is benchmarking AWS Lambdas it makes sense to use the services AWS has for building (AWS CodeBuild) and deploying (AWS CodePipeline) code.\\n","tokens":70,"id":1568}
{"File Name":"docs\/0003-quality-attribute-security.md","Context":"## Context and Problem Statement\\nShould we consider and address security requirements?\\n","Decision":"For the initial start of the project we do not consider security as a must have.\\n","tokens":15,"id":4681}
{"File Name":"tdr-dev-documentation\/0013-file-check-queues-and-lambdas.md","Context":"## Context\\nWhen a user uploads a folder to a TDR consignment, TDR needs to run a set of\\nchecks on each of those files. For the MVP release, the checks are:\\n- [Antivirus scan]\\n- [SHA-256 checksum], which can be compared to the checksum calculated in the\\nuser's browser\\n- [File format ID], which is useful metadata for the preservation system that\\ningests files transferred by TDR. It can be used to highlight files that The\\nNational Archives does not accept, like password-protected files, or files a\\nuser might have uploaded by mistake, like executable files\\nFiles are uploaded to an S3 bucket, so we need a workflow that can run each scan\\non each uploaded file. The user may upload several thousand files at once, and\\nthe size could range from a few bytes to a few gigabytes.\\nWe will warn the user that the file checks may take some time, but we think the\\nuser experience will be better if the user can review the file check results as\\nsoon as possible after uploading the files, so ideally this should take minutes\\nrather than hours.\\nWe may add more file checks in future, such as additional checksum algorithms or\\nantivirus software. The architecture should be flexible enough to add or remove\\nchecks later.\\n[Antivirus scan]: https:\/\/github.com\/nationalarchives\/tdr-antivirus\/\\n[SHA-256 checksum]: https:\/\/github.com\/nationalarchives\/tdr-checksum\/\\n[File format ID]: https:\/\/github.com\/nationalarchives\/tdr-file-format\/\\n","Decision":"Use AWS [Lambda] to run the file checks, and use [SQS] to coordinate the steps.\\nSee the [architecture diagram] for how the file check workflow fits into the\\noverall architecture. The steps in the workflow are:\\n- A user uploads a file to S3 using the TDR frontend\\n- This triggers an upload event, which AWS sends to an SNS topic. The message\\ncontains the object's S3 key, which includes the TDR file ID UUID\\n- The message is passed to the first SQS queue\\n- A Lambda connected to the SQS queue takes the message, extracts the file ID,\\nand [downloads the file][tdr-file-download] from S3 to an [EFS] temporary file\\nstore\\n- When that Lambda is finished, it adds a message to three further SQS queues,\\none for each of the backend checks\\n- Each SQS message triggers the respective backend check Lambda (checksum, file\\nformat ID and antivirus)\\n- Each Lambda sends a message to the API update SQS queue containing the results\\nof the check\\n- A final Lambda picks up that message, and [sends the results to the consignment\\nAPI][tdr-api-update], where they are stored in the database\\nEach check is a single, short process, which makes it very suitable for a\\nserverless workflow. If a user uploads thousands of files, we can process them\\nin parallel and let SQS and Lambda work through the backlog without having to\\nscale any EC2 instances. We are also trying to use serverless services on TDR\\nwhere possible because it removes the need to patch servers.\\nIn all the testing so far, we haven't reached the maximum Lambda execution time\\nof 15 minutes. If we do hit those limits, we could consider moving the slow step\\nto an ECS task, which doesn't have an execution time limit but is slower to\\nstart up. We would need some way to trigger the ECS task from SQS, but it would\\notherwise fit into the workflow in the same way that Lambda does at the moment.\\nWe considered using [step functions] instead of SQS to coordinate the tasks. We\\ndecided against them because when we tried [using them in the Alpha\\nprototype][alpha-step-functions], we found that they failed when we hit the\\nLambda concurrent usage limit. This is currently 1000 functions, and we can hit\\nthis limit if a user uploads a consignment of thousands of small files. We don't\\nhave this problem with SQS, because the messages wait in the queues until AWS\\nlets us start another Lambda.\\nThe initial step of downloading the file from S3 to Lambda is necessary for the\\nantivirus and file format ID tasks, which need to access the file as if it was\\non disk. The file is downloaded to EFS rather than to Lambda's disk storage. EFS\\nis a network file store, so access is slower than a local file on Lambda, but\\nthe disk storage limit for Lambda is currently 512 MB, so EFS lets us run checks\\non much larger files. All of the file check Lambdas then read the file from the\\nsame EFS store, so the file only has to be downloaded once. This saves on\\ntransfer and storage costs, and reduces the complexity of what the individual\\nfile checks do.\\nWe have a separate Lambda for sending the results to the API. This introduces\\nanother SQS queue and Lambda, but it has several advantages over making each\\nfile check send results to the API separately:\\n- If the API update step fails, we only have to retry the API update. We don't\\nhave to rerun the whole, potentially slow, file check\\n- We can choose to [throttle][lambda-throttling] the API update Lambda if the\\nAPI cannot handle the load when receiving the results for thousands of files\\nwithout reducing the throughput for the file checks themselves\\n- It reduces the number of API clients, which makes it easier to roll out API\\nclient library updates\\nThe file checks do not apply any business logic. They just report the raw\\nresults of the check. For example, if the file format ID step finds a zip file,\\nit reports the result. It will be up to the API to record this as a failure or a\\nwarning.\\nThe antivirus check does perform an action in addition to reporting the results:\\nit copies files to the \"clean\" or \"quarantine\" S3 buckets, depending on whether\\nthe file passed or failed the virus scan. Files from the clean bucket are used\\nlater in the TDR workflow, such as when exporting data. Files in the quarantine\\nbucket can be inspected by TDR admins if we want to find out more about why the\\nfile failed the scan.\\nEach queue is configured to retry each message up to three times. If all the\\nattempts fail, the messages are sent to a dead letter queue. We will be able to\\ninspect messages in that queue and monitor the size of it to spot processing\\nproblems.\\n[Lambda]: https:\/\/aws.amazon.com\/lambda\/\\n[SQS]: https:\/\/aws.amazon.com\/sqs\/\\n[architecture diagram]: ..\/beta-architecture\/beta-architecture.md\\n[tdr-file-download]: https:\/\/github.com\/nationalarchives\/tdr-download-files\/\\n[tdr-api-update]: https:\/\/github.com\/nationalarchives\/tdr-api-update\/\\n[EFS]: https:\/\/aws.amazon.com\/efs\/\\n[step functions]: https:\/\/aws.amazon.com\/step-functions\/\\n[alpha-step-functions]: https:\/\/github.com\/nationalarchives\/prototype-state-machine\\n[lambda-throttling]: https:\/\/aws.amazon.com\/about-aws\/whats-new\/2017\/11\/set-concurrency-limits-on-individual-aws-lambda-functions\/\\n","tokens":333,"id":1768}
{"File Name":"prm-gp2gp-transfer-classifier\/0002-implement-data-pipeline-in-python.md","Context":"## Context\\nWe are looking to build a dashboard that will display GP2GP related metrics.\\nThis information could be obtained from one or more sources, including SPINE,\\nGP2GP MI and ODS portal. The initial features are expected to be powered by\\ndata derived from SPINE messages. However, the format and level of granularity\\nof this data set does not make it suitable for directly powering a dashboard.\\nTherefore a data pipeline is needed to interpret the metrics and outcomes\\nimplied by these low level system messages.\\nIt is assumed that:\\n- The pipeline will be deployed on public cloud, likely AWS.\\n- This codebase will receive contributions from polyglot software engineers.\\n- The pipeline will need to interact with web APIs\\n","Decision":"We have decided to implement the data pipeline in Python 3.\\n","tokens":156,"id":3352}
{"File Name":"architecture\/0019-GPIO.md","Context":"## Context\\nWe currently have a lot of integrations that use or are based on GPIO (including SPI and I2C busses).\\n- All of these integrations have a low usage count (source Home Assistant Analytics),\\nbut add the same amount of maintenance and review time to the core.\\n- Most of these integrations are unmaintained.\\n- The use of GPIO via containers (including our Home Assistant OS, Container\\n& Supervised installation methods) is often unstable or complicated.\\n- Support for GPIO on specific platforms\/SBCs: mileage varies (e.g., Raspberry Pi on 64 bits platforms).\\n- The use of these sensors, in general, is better supported and done via dedicated projects like ESPHome.\\n","Decision":"- We no longer accept integrations that integrate with devices over GPIO.\\n- We identify, deprecate and remove integrations that use GPIO.\\n- It will still be possible to have custom integrations that use GPIO.\\nGPIO, in this case, means interfacing directly with the individual GPIO pins\\n(or I\/O lines) on the board running Home Assistant. This includes bus\\ncommunications like I2C and SPI.\\nDevices connected on GPIO that are exposed on the system as regular serial\\ndevices are exempted from this ADR.\\n","tokens":153,"id":1426}
{"File Name":"Wikibase\/0005-frontend-changes-ParserCache-invalidation.md","Context":"## Context\\nWhen changes to the markup used on entity pages are made we often end up with outdated content in the ParserCache.\\nThis can result in user facing errors from either the backend code attempting to perform processing on ParserCache output or frontend Javascript attempting to access parts of the DOM that have changed.\\nExamples of this are:\\n* [T228978] - Backend attempting to fill placeholders that it doesn't expect to be there.\\n* [T205330] - Frontend looking for a newly introduced data-attribute that isn't present in historic parser cache entries.\\nIn the not distant past we have solved these problems by introducing a custom [RejectParserCacheValue Hook] in operations\/mediawiki-config e.g. (https:\/\/gerrit.wikimedia.org\/r\/c\/operations\/mediawiki-config\/+\/463221) to reject cache entries made before deployment time.\\nWe've also done it in stages leaving some invalid entries in the cache to avoid increased load from marking all entries as invalid at once.\\nWhile this has worked for us in the past we have typically applied it after discovering a problem. Even then the way we gradually reject historic invalid cached results in user facing errors for some time.\\n","Decision":"We should record things that impact the ParserOutput content (e.g. the version of the code used to generate the ParserOutput) in the parser options. These parser options should then be marked as used in generating the ParserCache key. The RejectParserCacheValue can be used if the two versions of the ParserOutput content cannot coexist and backwards compatibility is not possible.\\nIn this way we will split the ParserCache into old and new versions. An example of this was trialled when introducing new mobile Termbox. See: https:\/\/gerrit.wikimedia.org\/r\/c\/mediawiki\/extensions\/Wikibase\/+\/529055.\\nIn the event that we are tracking a new part of the code that doesn't currently have a corresponding option then it may be necessary to ensure it is introduced. The ParserOptions used to calculate the ParserCache key are determined from existing cache entries. Thus on introducing a new key (but not a new key value) a custom RejectParserCacheValue Hook may still be required. See: https:\/\/gerrit.wikimedia.org\/r\/c\/mediawiki\/extensions\/Wikibase\/+\/529059 for an example.\\nAn option already exists (but hasn't been used in the last 4 years) on [EntityContent] ([EntityHandler::PARSER_VERSION]) which may be useful for some changes.\\nHowever, currently this applies to all entities; in general we usually only make changes to one type of entity at a time so more options will be needed.\\n","tokens":249,"id":1348}
{"File Name":"trade-access-program\/0003-events-from-fixture-file.md","Context":"## Context\\nTrade show events are stored in many places across the TAP team.\\n- At the beginning of the year the Glasgow operations team validates the list of events in a spreadsheet stored on sharepoint.\\n- Events are uploaded to a hosted service called Aventri so that they are visible on great.gov.\\n- Any updates to event details are communicated via email.\\nAs of 2020-12-17 there are still discussions ongoing about how to manage trade show event data centrally within DIT\\nand move away from the spreadsheet being the golden source of truth. One idea is to use digital workspace for this.\\nHowever as of right now no decision on this has been made.\\nIn the meantime the our services still need a way of displaying trade show events to our users to select and view.\\n","Decision":"Without a firm decision on where events will ultimately be centrally stored we have decided to use a fixture file\\nto load events into our backoffice as a temporary solution.\\nThe fixture file exists at `backoffice\/web\/trade_events\/fixtures\/trade_events.json` and is automatically loaded on\\nstartup.\\n","tokens":166,"id":5011}
{"File Name":"compliantkubernetes\/0002-use-kubespray-for-cluster-lifecycle.md","Context":"## Context and Problem Statement\\nCompliant Kubernetes promises: \"Multi-cloud. Open source. Compliant\". So far, we delivered on our multi-cloud promise by using our in-house `ck8s-cluster` implementation. This strategy feels unsustainable for two reasons: First, we don't have the resources to catch up and keep up with open source projects in the cluster life-cycle space. Second, we don't want to differentiate on how to set up vanilla Kubernetes cluster, i.e., lower in the Kubernetes stack. Rather we want to differentiate on services on top of vanilla Kubernetes clusters.\\n## Decision Drivers\\n* We want to differentiate on top of vanilla Kubernetes cluster.\\n* We want to be able to run Compliant Kubernetes on top of as many cloud providers as possible.\\n* We promise building on top of best-of-breeds open source projets.\\n* We want to reduce burden with developing and maintaining our in-house tooling for cluster life-cycle management.\\n","Decision":"* We want to differentiate on top of vanilla Kubernetes cluster.\\n* We want to be able to run Compliant Kubernetes on top of as many cloud providers as possible.\\n* We promise building on top of best-of-breeds open source projets.\\n* We want to reduce burden with developing and maintaining our in-house tooling for cluster life-cycle management.\\nWe chose kubespray, because it is best aligned with our interests, both feature- and roadmap-wise. It has a large community and is expected to be well maintained in the future. It uses kubeadm for domain knowledge on how to set up Kubernetes clusters.\\n### Positive Consequences\\n* We learn how to use a widely-used tool for cluster lifecycle management.\\n* We support many cloud providers.\\n* We can differentiate on top of vanilla Kubernetes.\\n### Negative Consequences\\n* We need training on kubespray.\\n* We need to port our tooling and practices to kubespray.\\n* We need to port `compliantkubernetes-apps` to work on kubespray.\\n","tokens":196,"id":3108}
{"File Name":"heptaconnect-docs\/2021-04-13-portal-dependency-injection-implementation.md","Context":"## Context\\nThis is a follow-up to [the ADR about general service containers](.\/2020-12-10-portal-service-container.md).\\nDependency injection is a common pattern to create reusable components that can build upon each other.\\nPortals will have to communicate with their API of choice in different flow components.\\nPresumably a portal developer wants to build an API client that can be used within all flow components.\\nThis is where dependency injection comes in handy.\\nDepending on the implementation this can also be used to decorate services which will add more freedom for modifications.\\nWe were able to provide a service container for each portal node matching [PSR-11](https:\/\/www.php-fig.org\/psr\/psr-11\/) in the past, which allowed easy access to services but no injection into flow components.\\nThere is the [PSR-11](https:\/\/www.php-fig.org\/psr\/psr-11\/) standard to define a service container but not a service container builder and therefore there are no drop in implementations.\\n","Decision":"* Use Symfony dependency injection\\n* Replace custom service container with Symfony\\n* Enable auto-wiring, auto-configuration, auto-binding and automatic PSR-4 resource loading\\n* Automatically load flow components and drop their definition from portals\\n","tokens":219,"id":3204}
{"File Name":"wikibase-release-pipeline\/0005-release-notes-process.md","Context":"## Context\\nAs we adopt the new release strategy we also want to determine a process for writing and maintaining release notes.\\nOur release process will still be closely bound to the release branches WMF are using it makes sense to inspect their process for producing and maintaining release notes within the source control system.\\nFor mediawiki\/core the release notes are maintained and worked on within the source repository. Each release branch contains a RELEASE_NOTES-N.NN document describing changes that was made to the software up until the point the branch was cut from master. Any backports to these branches also comes with an update to the release notes document.\\nAs a new release branch is cut\/created a new [template] release document is added to the master branch and any previous release notes are merged into a [HISTORY] document within the repository containing all previous release notes.\\n","Decision":"Release notes within the Wikibase MediaWiki extension repository will adopt a similar process to the one being used by the MediaWiki\/core developers.\\n","tokens":174,"id":4468}
{"File Name":"island.is\/0013-feature-flags.md","Context":"## Context and Problem Statement\\nWe want to be able to roll out new features gradually, perform A\/B testing and target individual groups with a new feature. Also, we want to be able to flip a switch to turn features on or off for everyone.\\n## Decision Drivers\\n- Ease of setup\\n- Ease of maintenance\\n- Cost\\n- Developer experience\\n- Usability\/UX\\n- Operational concerns\\n- Handling of PII\\n","Decision":"- Ease of setup\\n- Ease of maintenance\\n- Cost\\n- Developer experience\\n- Usability\/UX\\n- Operational concerns\\n- Handling of PII\\nChosen option: \"ConfigCat\", because:\\n- We can probably get away with using it for very low cost\\n- We can start using it almost right away with little configuration\\nIf we decide later that we would like some of the features of LaunchDarkly, we want to be able to quickly swap. Thus, it is vital that we write some kind of service-agnostic wrapper.\\n### Positive Consequences\\n- We can start using feature flags across our stack.\\n### Negative Consequences\\n- Complexity of applications will increase\\n","tokens":89,"id":1116}
{"File Name":"dogma\/0010-handler-timeout-hints.md","Context":"## Context\\nWe need to decide on a mechanism for engine implementations to determine\\nsuitable timeout durations to apply when handling a message.\\nFor aggregate message handlers, which are not permitted to access external\\nresources, a fairly constant timeout duration should be discernable by the\\nengine developers.\\nFor all other handler types, which may make network requests or perform CPU\\nintensive work, there is no one timeout duration that makes sense in all\\ncircumstances.\\n","Decision":"We have decided to allow process, integration and projection message handlers\\nto provide a timeout \"hint\" on a per-message basis by way of a\\n`TimeoutHint(dogma.Message) time.Duration` method.\\nBy returning a zero-value duration, the handler indicates that it can provide no\\nuseful \"hint\" and that the engine should choose a timeout by other means.\\n","tokens":96,"id":1613}
{"File Name":"js-sdk\/0012-use-poetry-lock-file-to-install-instead-of-poetry-update.md","Context":"## Context\\nDependancies versions update can lead to failure. They needed to be tested well before using them. Poetry update bumps version to latest each time to execute and takes a long time\\n","Decision":"- Use poetry lock file to install dependancies using specific versions instead of executing poetry update each time\\n- Updating versions will be by the repository maintainers after making sure no compitablity issues\\n","tokens":39,"id":5189}
{"File Name":"embvm-core\/0004-track-documentation-alongside-source.md","Context":"## Context\\nProject documentation is:\\n* Usually tracked in a variety of locations (google drive, various hard drives, wiki, repository)\\n* Rarely maintained by an organization\\n* Not updated to match source changes\\n* Copied multiple times, often with differences between copies\/locations\\n* Usually not given a responsible party to own it\\nAs a result, documentation is scattered and rarely kept up to date. If it is used, documenation is often generated and referred to once.\\nQuality documentation, especially when kept up-to-date, enables us to support more clients and developers with less direct involvement.\\n","Decision":"All architectural, design, development, and user documentation will be maintained in the same repository as the source code.\\nThis means that documentation should be generated when possible, rather than manually constructed in a Microsoft Word document.\\n","tokens":125,"id":3013}
{"File Name":"core-bundle\/001-support-deserialization-plain-text.md","Context":"## Context\\nThe built-in deserializers included in JMS are sufficient for most cases, but sometimes the provided payloads are not compatible with them. Situations like this include strings in plain text, where no deserialization is expected. Feeding this type of data to JMS raises an exception that stops the execution of the application.\\nIn these cases, the serialization needs to be bypassed in a clean and transparent way that will allow the calling class to continue its execution.\\n","Decision":"We decided to implement a custom deserializer **that accepts plain text**.\\nThis deserializer is plugged **as a custom deserializer** in JMS' configuration and called whenever the format type \"plain_text\" is passed to the Serializer class.\\n","tokens":97,"id":1148}
{"File Name":"tech-team\/0002-use-containers-for-foreign-languages.md","Context":"## Context\\neLife has a small set of supported languages: PHP, Python, JavaScript; in-house developers are present for them. All other languages are defined as **foreign**.\\nThere are tools that are peculiar to our infrastructure, such as [goaws](https:\/\/github.com\/p4tin\/goaws) for AWS simulation, but are written in languages no one is an expert in (Go).\\nThere are also tools that were originally written in another language but are being adopted by us, like [INK](https:\/\/gitlab.coko.foundation\/INK\/ink-api) for document conversion, written in Ruby.\\nThese tools are usually distributed as source code. The operational overhead of writing formulas for the environment to build them is a form of waste.\\nSome tools written in Java instead have a very stable runtime platform (ElasticSearch, even Jenkins), as they are distributed as binaries.\\n","Decision":"We will use existing Docker containers to deploy tools that require building from source in a foreign language, in testing or production environments.\\n","tokens":183,"id":937}
{"File Name":"ng-kaart\/0002-locatie-adrs.md","Context":"## Context\\nGeoloket 2 en ng-kaart worden in tandem ontwikkeld.\\n","Decision":"ADRs voor ng-kaart worden genoteerd in het Geoloket 2 project. De context moeten duidelijk maken of de ADR eerder op Geoloket 2, ng-kaart of beide van toepassing is.\\n","tokens":23,"id":2075}
{"File Name":"operational-data-hub\/0046-javascript-framework.md","Context":"## Context\\nThe issue motivating this decision, and any context that influences or constrains the decision.\\nA Javascript framework is an abstraction in which software providing generic functionality can be selectively changed by additional user-written code. JavaScript framework is an application framework written in JavaScript where the programmers can manipulate the functions and use them for their convenience.\\nFrameworks are more adaptable for the designing of websites, and hence, most of the website developers prefer it. JavaScript frameworks are a type of tool that makes working with JavaScript easier and smoother. These frameworks also make it possible for the programmer to code the application as a device responsive. This responsiveness is yet another reason why the JavaScript frameworks are quite popular when it comes to the question of using a high-level machine language.\\n","Decision":"We have decided to use the Angular Javascript framework\\n","tokens":150,"id":2717}
{"File Name":"Horace\/0013-use-jenkins-for-secrets-management.md","Context":"## Context\\nThe build processes for all of the PACE projects require credentials to access GitHub and the SAN area.\\nThese need to be stored securely and be accessible to steps in the Jenkinsfile and any scripts launched from that.\\nFor security reasons this data cannot be stored in the build or pipeline scripts as these are stored in GitHub.\\n","Decision":"The data will be stored in ANVIL through the available Jenkins [Credentials](https:\/\/plugins.jenkins.io\/credentials\/) plugin.\\nA `file` object will be used for the SAN credentials as this maps directly onto the format required by be `gio mount` syntax on Linux build nodes.\\nA `string` object will be used for other credentials types, e.g. GitHub access tokens.\\nCredentials will be stored in the `PACE-neutrons`  store and use `snake_case` IDs that clearly identify their use. The first word will identify the associated system, e.g. `SAN_credentials_file`.\\n","tokens":69,"id":4239}
{"File Name":"beis-report-official-development-assistance\/0020-use-skylight-application-performance-monitoring.md","Context":"## Context\\nTo ensure the RODA application has the highest availability, we want\\nto have confidence that the application is resourced properly in times of\\nlow and high usage.\\n","Decision":"dxw have used the Skylight gem in many other projects and there is a Skylight\\naccount available for use. We have prior knowledge of using Skylight in\\nother projects.\\nSkylight provides a web interface and graphing so we can more easily identify\\npain points in our application which will cause it to become less performant\\nunder high load. It can identify slow-running parts of the service down to the\\ncontroller action level, and indicate where - for example - excessive database\\ncalls or slow queries are causing degradation in the user's experience.\\nCommon alternatives to Skylight are New Relic and DataDog. We do have some\\nexperience with DataDog at dxw, but we had more confidence within the team with\\nusing Skylight.\\n","tokens":39,"id":2391}
{"File Name":"adr-viewer\/0002-expose-command-line-interface.md","Context":"## Context\\nWe want to maximise the usability of adr-viewer whilst maintaining flexibility in future for other output formats, e.g. a live webserver.\\n","Decision":"The entry point for this project will be a command-line utility called `adr-viewer`. We will use the python [click](http:\/\/click.pocoo.org\/5\/) library to provide command-line options and documentation.\\n","tokens":33,"id":3814}
{"File Name":"infra\/0005-vpc-and-network-design.md","Context":"## Context\\nChanging networking can be hard.  It usually requires a full redeploy of all services and all infrastructure to make it 'real'.  Our current network has overlaps, which makes it more difficult to setup vpns, usually requring syncing of live IP addresses to their desired state.\\n","Decision":"Netops will reserve a \/16 block of ips for mozmeao. For each VPC make a block of \/20 ipv4 addresses.  Where VPC maps to a region within a cloud provider we use.  Divide that into \/24 subnets, where we'll have just one subnet per AZ.\\nOur \/16 is - 10.154.0.0\/16\\nFor example, in oregon, the network would look like:\\n| Label           | CIDR          | Range Start  | Range End      | Description                     |\\n|-----------------|---------------|--------------|----------------|---------------------------------|\\n| Oregon VPC      | 10.154.0.0\/20 | 10.154.0.1   | 10.154.15.254  | A large block for the whole VPC |\\n| Oregon Subnet A | 10.154.0.0\/24 | 10.154.0.1   | 10.154.0.254   | Subnet for oregon-a az          |\\n| Oregon Subnet B | 10.154.1.0\/24 | 10.154.1.1   | 10.154.1.254   | Subnet for oregon-b az          |\\n| Oregon Subnet C | 10.154.2.0\/24 | 10.154.2.1   | 10.154.3.254   | Subnet for oregon-c az          |\\nand Frankfurt would be:\\n| Label              | CIDR           | Range Start   | Range End       | Description                        |\\n|--------------------|----------------|---------------|-----------------|------------------------------------|\\n| Frankfurt VPC      | 10.154.16.0\/20 | 10.154.31.1   | 10.154.255.254  | A large block for the whole VPC    |\\n| Frankfurt Subnet A | 10.154.16.0\/24 | 10.154.16.1   | 10.154.16.254   | Subnet for frankfurt-a az          |\\n| Frankfurt Subnet B | 10.154.17.0\/24 | 10.154.17.1   | 10.154.17.254   | Subnet for frankfurt-b az          |\\n| Frankfurt Subnet C | 10.154.18.0\/24 | 10.154.18.1   | 10.154.18.254   | Subnet for frankfurt-c az          |\\nThe next few vpc blocks would be 10.154.32.0\/20, 19.154.48.0\/20, 19.154.128.0\/20\\nIn oregon we could continue with 10.154.3 and 10.154.4 until 15 for the subnets.  Essentially the same for frankfurt 10.154.19, 10.154.20.\\n","tokens":60,"id":854}
{"File Name":"hackathon2020-prominent-colors\/0001-use-node-vibrant.md","Context":"## Context\\n* Need to pick a library to use for finding the prominent colors in an image\\n* The scope of this project is subject to change in the future\\n* Unsure yet where this project will fit into Cond\u00e9 Nast platform architecture\\n* Needs to be deterministic for quality purposes\\n","Decision":"* Use node-vibrant because it's popular and therefore well maintained and tested\\n","tokens":59,"id":2691}
{"File Name":"manage-frontend\/04-node.md","Context":"## Context\\nIt's not intended for this app to have a significant server-side component, instead it will directly send calls from the front end to a service layer. This service layer will be discussed in its own project, but presently comprises solely of [members data API](https:\/\/github.com\/guardian\/members-data-api), the same back end that currently provides account management functionality.\\n","Decision":"The back end for this app will have two responsibilities. Firstly, it will act as a proxy for calls to the service layer. Secondly, it will provide server side rendering capabilities to improve user experience.\\n","tokens":78,"id":3708}
{"File Name":"datalab\/0030-kubernetes-direct-api-access.md","Context":"## Context\\nIn order to dynamically orchestrate the containers running in the Datalab environment we\\nneed to interact with the Kubernetes API. There are several choices for this:\\n* Use one of the officially [supported clients](https:\/\/kubernetes.io\/docs\/reference\/client-libraries\/#officially-supported-kubernetes-client-libraries).\\n* Use one of the Node.js community clients\\n* Directly interact with the Kubernetes REST API.\\n","Decision":"We have decided to directly interact with the Kubernetes REST API as this presented the\\neasiest option for development.\\nWe ruled out using a supported client as we didn't want to have to write a service in a\\nlanguage we were not familiar with.\\nWe trialled all of the Node.js community clients but didn't feel that they were complete\\nenough to meet our needs and were poorly documented.\\n","tokens":88,"id":750}
{"File Name":"scientific-thesis-template\/0004-use-lualatex-for-correct-ligatures.md","Context":"## Context and Problem Statement\\nFor high-quality documents, the [ligatures](https:\/\/en.wikipedia.org\/wiki\/Typographic_ligature) have to be right.\\nSee [english.stackexchange.com](https:\/\/english.stackexchange.com\/q\/50660\/66058) for a long discusisson.\\nSee a German rant on wrong ligatures: <https:\/\/web.archive.org\/web\/20150425155310\/http:\/\/www.mengensatz.de\/blog\/?p=79>.\\nFor instance, in the English word \"offline\", the letters \"f\" and \"l\" must not be joined.\\nIn the German word \"Auflage\", the letters \"f\" and \"l\" must not be joined.\\nSee also the last lines in <https:\/\/tex.stackexchange.com\/a\/64457\/9075>.\\n### More readings\\n* <https:\/\/www.typolexikon.de\/ligatur\/>\\n* Questions on ligatures: <https:\/\/tex.stackexchange.com\/questions\/tagged\/ligatures>\\n* Disable ligatures in headings: <https:\/\/tex.stackexchange.com\/q\/198743\/9075>\\n","Decision":"Chosen option: \"lualatex\", because comes out best (see below).\\n","tokens":237,"id":251}
{"File Name":"lrud\/ADR-002-rewrite-version-3.md","Context":"### Context\\nAfter much usage of LRUD V2, we (Lovely Horse \ud83d\udc34) have ended up with a laundry list of things we want LRUD to do that V2 doesn't support. We also have a desire for a more maintainable codebase that utilises more up-to-date terminology for the expected behaviour of LRUD.\\nThe list of desired functionality currently sits at;\\n- a real tree structure\\n- cleaner\/easier to understand \"grid\" functionality\\n- supporting a concept of \"column span\"\/\"column width\"\\n- real definition of which node is the \u201croot\u201d node\\n- all focusable nodes to maintain an \"index\" for easier understanding or sorting\\n- better handling of unregistering\\n### Decision\\nWe have decided to re-write LRUD from the ground up, maintaining many of the concepts from V2, while addressing the list of desired functionality.\\nThis will also give us an opportunity to re-write LRUD into Typescript, further increasing the maintainability of the codebase in future.\\n### Status\\nApproved\\n### Consequences\\n- user land usages of LRUD will need to update their code in order to make use of the new version. We are planning to keep breaking changes to a minimum, but some changes will be necessary.\\n- slightly increased library size, affecting response payload sizes. The increase in size is small enough (an increase of 2.6kb when minified) that we deem this acceptable. Furthermore, the changes mean that current \"workaround\" code in service land can be removed, reducing payload size in other areas.\\n- slightly increased runtime computation. Usage of a real tree in memory requires extra computation. Dedicated testing will take place to ensure LRUD is still performant enough on low powered devices, but initial testing of 92 test cases in 2.4s suggests this is well within limits.\\n### Further Reading\\n- [Paper Doc discussing what LRUD is and why we want to change some things](https:\/\/paper.dropbox.com\/doc\/SSR-Controller-Module-LRUD-V3--Aca6ZBsM4Uv8zEN44j5o4TsvAg-y0v9YqarEOXNP7R2151RK)\\n","Decision":"We have decided to re-write LRUD from the ground up, maintaining many of the concepts from V2, while addressing the list of desired functionality.\\nThis will also give us an opportunity to re-write LRUD into Typescript, further increasing the maintainability of the codebase in future.\\n### Status\\nApproved\\n### Consequences\\n- user land usages of LRUD will need to update their code in order to make use of the new version. We are planning to keep breaking changes to a minimum, but some changes will be necessary.\\n- slightly increased library size, affecting response payload sizes. The increase in size is small enough (an increase of 2.6kb when minified) that we deem this acceptable. Furthermore, the changes mean that current \"workaround\" code in service land can be removed, reducing payload size in other areas.\\n- slightly increased runtime computation. Usage of a real tree in memory requires extra computation. Dedicated testing will take place to ensure LRUD is still performant enough on low powered devices, but initial testing of 92 test cases in 2.4s suggests this is well within limits.\\n### Further Reading\\n- [Paper Doc discussing what LRUD is and why we want to change some things](https:\/\/paper.dropbox.com\/doc\/SSR-Controller-Module-LRUD-V3--Aca6ZBsM4Uv8zEN44j5o4TsvAg-y0v9YqarEOXNP7R2151RK)\\n","tokens":465,"id":1100}
{"File Name":"smjs\/2019032101-supporting-concurrent-secondary-adapters.md","Context":"## Context\\nWhen we define a secondary port, the purpose is to allow different kinds of adapters to hook to that port. However, in the simplest case, the implementation of an application will only allow a single adapter to be attached to a port at the same time. For example:\\n```java\\npublic interface Notifier\\n{\\npublic void notify(Message message);\\n}\\n```\\n```java\\npublic class Logger implements Notifier\\n{\\n@override\\npublic void notify(Message message)\\n{\\nlogger.log(message.getMessage());\\n}\\n}\\n```\\n```java\\npublic class Provider\\n{\\npublic void register()\\n{\\ncontainer.bind(Notifier, Logger);\\n}\\n}\\n```\\nso a single type of adapter can be attached to a specific port at the same time, and that's obvious because the application services that use that port, will each expect to receive a single object as instance of that port. But what if we need to attach multiple adapters to the same port at the same time? In the example above, what if we want multiple notifiers to send notifications when the abstract message is produced, for example sending an email in addition to logging the message?\\n","Decision":"The standard solution in this cases is using a message bus, so that multiple handlers can be registered to handle certain types of messages. What we can do, then, is creating a bus adapter that would work as a proxy that routes the messages to all registered handlers:\\n```java\\npublic class BusNotifier implements Notifier\\n{\\npublic void notify(Message message)\\n{\\nbus.send(message);\\n}\\n}\\n```\\nso we would still be attaching a single secondary adapter, but at the same time supporting multiple ones, like a logger, an email notifier, etc.\\nOf course the same application structure can be reused with no change whatsoever the moment we decide that we only need a single secondary adapter: we could then replace the bus adapter with the single adapter we need.\\n","tokens":247,"id":3974}
{"File Name":"hoard\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":4489}
{"File Name":"exercise3-group-6\/0002-create-repository-interface.md","Context":"## Context\\nThe system has various data types that users may want aggregated together so that they are easily accessible and sortable.\\ne.g. Types `Whale` and `Observation`. The system should have a consistent interface so that the user may access various\\ntypes of records.\\n","Decision":"We decided to implement a `Repository <T>` interface that can be realised by `Whale` or `Observation` objects. Users\\nwho need to access a large list\/repository of Whale's or Observation's will do so through the `Repository <T>` interface.\\n","tokens":58,"id":2777}
{"File Name":"gladiator_v2\/12-6-2017_combat_class.md","Context":"## Context:\\nNeed to create a basic structure for combat involving characters that know how to fight and a middle man referee that ensures that everyone gets a turn and works as a go between to ensure that the characters interact properly. This will create a basic means for the game to function.\\n","Decision":"There should be a combat module specific to the characters and an overarching combat entity that handles the mediation.\\n","tokens":58,"id":551}
{"File Name":"openchs-adr\/0007-separate-models-and-health-modules-from-the-android-application.md","Context":"## Context\\nIn the future, once all product features are built, there will be a lot of work on building and enhancing health modules. This might eventually also become its own project.\\nThere needs to be a contract that a health module exposes. We will have interfaces defined in the health module, and communication using the models (which will be common across openchs-android and openchs-health-modules). This will allow common domain logic to live in modules and health module specific business logic to live in the models.\\n","Decision":"The android client will be broken down into 3 components - openchs-android for the app, openchs-health-modules for the health modules and openchs-models for the contract objects that will be exchanged by openchs-android to openchs-health-modules.\\n","tokens":103,"id":2822}
{"File Name":"ios-architecture-decision-logs\/0014-use-CoreTracker-for-trackingEvents.md","Context":"## Context\\nOur team created new Core Tracker Interface for tracking events. For consistency we should replace and use CoreTrackable instead of Legacy Trackable Interface.\\n","Decision":"Every new events must use CoreTrackable. Also every new event should use new Tracker approach.\\n","tokens":32,"id":4971}
{"File Name":"portfolio\/0009-migrate-away-from-mdx.md","Context":"## Context\\nMDX\u2014while great\u2014has some notable tradeoffs that have made writing quickly more difficult for me.\\n- MDX files are not portable outside of React(ish) runtimes.\\n- MDX requires a \"runtime\" of accepted components. It does not gracefully degrade to raw HTML if those components are not provided.\\n- The type of content I produce on my blog does not require high interactivity or customization. MDX feels like overkill right now.\\n- MDX actually serves as a distraction while writing at the moment. I feel like constraints are more helpful (\"is this a `<Callout>` or a `blockquote`?\" decision fatigue).\\n- Other authoring tools (i.e. Notion, Bear, etc) usually allow exporting something as markdown, but if MDX is a first-class citizen on the blog this means there's an extra conversion step to get unified design.\\n","Decision":"Remove all MDX files in favor of simple Markdown\\n","tokens":187,"id":2082}
{"File Name":"poc-terraform-aws-web-cluster\/0002-infrastructure-layout.md","Context":"## Context\\nWhen creating a piece of infrastructure it is implied that this code would be able to be run in several environments. To illustrate this, there are three environments created, dev, uat, and prod. There will be one and only one\\nterraform module and each environment will feed it with different parameters to create distinct instances.\\n","Decision":"The terraform code is made multi environment and multi account through there are better tools to do that with, like terragrunt, which is outside the scope of the PoS.\\nThe credentials are assumed to be in the credentials file. the author is aware that the are more secure ways of doing this , but as the PoS needs to be easily reproducible the author has chosen to make it easy to use by dropping in the credential files in the home directory.\\nThe credentials are that of an administrator, there has been no attempt made to create assumed roles as would be done in production systems, as the goal in this case is speed.\\n","tokens":70,"id":1023}
{"File Name":"polaris\/developmentLanguage.md","Context":"## Context\\nReactNative supports both JavaScript and TypeScript.\\n","Decision":"Javascript was chosen at this time as it is a more widely used development language and, whilst Babel enables you to use TypeScript, it only enables transpilation of TypeScript and not type checking.\\n","tokens":12,"id":4720}
{"File Name":"ReportMI-service-manual\/0015-use-lambda-for-file-ingest-and-data-validation.md","Context":"## Context\\nIn [ADR-0002][adr-0002] we outlined the overall technical approach for the Data\\nSubmission Service, and highlighted the components which we expect will be\\ndeveloped.\\nThis ADR focusses on the technology choice for building a portion of the\\nfeatures. In particular:\\n1. File transformation service - a small service for extracting data from an\\nuploaded file, transforming it into a useful format and storing it via the API\\n(a process we're currently calling 'ingest').\\n1. Data validation service - a service for validating the data provided by\\nsuppliers and calculating the appropriate management fee.\\n### Features of these services\\nThese services have to handle large peaks in traffic, but will be rarely used\\nfor a large part of the month.\\nThe file transformation service will need to handle multiple files, of different\\nformats, at the same time, some with well over 100,000 rows of data.\\nThe data validation service will need to perform validation and calculations on\\neach row within the data.\\nTo provide a good user-experience, we want these services to operate quickly and\\nprovide feedback to users.\\nThese services will need to apply custom rules based on the framework the report\\napplies to. For example the fields of data provided for G-Cloud 9 are different\\nfrom General Legal Services, and the process for calculating the management fee\\nis different for Rail Legal Services and Courier Services.\\n### Serverless\\nServerless technologies are designed to run code without the need to manage\\ninfrastructure, which allows it to scale easily. Generally, you are only billed\\nfor the time it takes to run the code - usually charged in fractions of a\\nsecond.\\nDeveloping these services to be hosted as a serverless function would allow us,\\nin theory, to infinitely scale the file ingest and validation parts of the\\nservice and only pay for what is actually needed.\\nEach of the main cloud providers has their own implementation of a serverless\\nhosting option including Azure Functions, Google Cloud Functions and AWS Lambda.\\n","Decision":"We will use AWS Lambda for the ingest and data validation processes.\\nWe can deploy a different Lambda function for each file type we expect to\\ningest, and split the data validation scripts into easy-to-maintain functions.\\n","tokens":427,"id":2046}
{"File Name":"paas-team-manual\/ADR005-pingdom-healthchecks.html.md","Context":"## Context\\nWe wanted to open up access to tenant applications in our production environment.\\nAs part of an earlier story, Pingdom checks were set up for a healthcheck application in CI, Staging, and Production. At this stage applications were not accessible from non-office IP addresses.\\nThe problem we faced was a need to test the code introduced to make our production environment accessible to the world. Initially, we suggested applying the same change to our staging environment. However, this approach means all applications in staging will be accessible from anywhere.\\nIf we use Pingdom to assert an application is accessible from the outside world then we need to remove the explicit rules (security groups) allowing Pingdom traffic. This means our CI environment would not be accessible to Pingdom probes.\\n* [#116104189 - set up Pingdom](https:\/\/www.pivotaltracker.com\/story\/show\/116104189)\\n* [#115347323 - allow public access to tenant applications](https:\/\/www.pivotaltracker.com\/story\/show\/115347323)\\n","Decision":"It was decided we would make the staging environment accessible to the outside world as well as production, and define future work for removing the CI Pingdom check and security groups allowing Pingdom probes, and setting up tests from the pipeline which use the Pingdom API.\\nGiven that the advantages relate to the availability of our production environment, they outweigh not having an automated healthcheck on an application in our CI environment. However, we remain open to hearing solutions to providing healthchecks for CI in future.\\n","tokens":212,"id":212}
{"File Name":"SoundCloudVisualizer\/0006-use-webpack-to-build-source-code.md","Context":"## Context\\nWhen this project was first starting out [Gulp](https:\/\/gulpjs.com\/) was one of the tools of choice for running tasks such as bundling and minifying code.\\nSince then task runners seem to have fallen out of popularity and bundlers such as Webpack and Parcel are the preferred approach.\\n","Decision":"Webpack will be used to bundle source code.\\n","tokens":66,"id":1583}
{"File Name":"dddsample-pelargir\/0002-isolate-components-into-submodules.md","Context":"## Context\\nI want to be able to mix and match different implementations\\nof the ddd sample application, without needing to pull in\\na large collection of unnecessary dependencies.\\nI want all of the code to be together in one place; which\\nis to say, I want to treat the entire project as a mono-repo.\\nI can't be bothered to maven install\/maven deploy each\\nlittle piece to propagate the necessary changes between\\nisolated libraries.\\n","Decision":"Use a maven reactor project to track the dependencies between\\ndifferent libraries within the project\\n","tokens":97,"id":127}
{"File Name":"uniprot-website\/0003-design-system.md","Context":"## Context\\nIn order to provide a consistent UI, we need to use a design system. This design system also helps reduce the amount of code required, and ensures designers and developers speak the \"same language\".\\n","Decision":"We have created our own design system\/pattern librarly, [Franklin](https:\/\/ebi-uniprot.github.io\/franklin-sites). It is built on top [Foundation](https:\/\/foundation.zurb.com\/) (Atomic level components) and uses React.js. The library is published to `npm` as [`franklin-sites`](https:\/\/www.npmjs.com\/package\/franklin-sites) and can be used by any React.js website.\\n","tokens":44,"id":2012}
{"File Name":"alfresco-anaxes-shipyard\/0005-base-java-docker-image-composition.md","Context":"## Context\\nAs part of our efforts around containerized deployment of the Alfresco Digital Business Platform we should standardize on a base Docker Image for Java-based services that Alfresco engineering teams, other internal groups in the organization, and customers and partners can use.\\nWhile there are a few popular choices and de facto standards such as OpenJDK, we need to consider the support and legal implications of that choice, particularly since we could be considered as \u2018distributing\u2019 all layers in that image, including the OS and Java binaries.\\nWe could potentially workaround being the distributors of sensitive layers using techniques like init containers, composite containers, or foreign layers.\\n","Decision":"We will start from the official CentOS 7 Docker image and add the Oracle server JRE binaries for the first iteration of the Alfresco base Java image.\\nThis allows us to start from a popular OS and Java runtime which are already in the supported ACS and APS stacks and QuickStart AMIs, and closer to the most popular customer OS, RHEL, than others.  We also have precedence in distributing the Oracle JRE in our current installer.\\nThe CentOS image also contains very few known vulnerabilities which we can easily patch at the moment.\\nWhile Alpine is an attractive choice due to its simplicity, it may be more difficult to debug and support, and the majority of our components are not yet architected to take advantage of the smaller size and lower resource consumption.\\nWhile distribution workarounds mentioned above were considered, it was felt that they are not common practice and are likely to produce more problems than they solve.\\nThe summary [![report of the comparison](https:\/\/img.shields.io\/badge\/report%20of%20the%20comparison-PRIVATE-red.svg)](https:\/\/ts.alfresco.com\/share\/s\/bqDcnHWpSrSGybJhMxf93A) contains more details.\\nThe base image Dockerfile is [here](https:\/\/github.com\/Alfresco\/alfresco-docker-base-java\/blob\/master\/Dockerfile).\\n","tokens":133,"id":3603}
{"File Name":"hee-web-blueprint\/0011-use-federated-authentication-provider.md","Context":"## Context\\nWe need to describe an approach to authentication that supports the use of multiple authentication providers. Azure AD provides the ability to configure direct federation with a range of authentication providers and includes the ability to provide managed authorisation for users without the ability to authenticate with Azure AD\\n- https:\/\/docs.microsoft.com\/en-us\/azure\/active-directory\/b2b\/delegate-invitations\\n","Decision":"We will choose an authentication provider\/platform that supports the federation of authentication.\\n","tokens":76,"id":1207}
{"File Name":"ADR\/0013-use-typescript-on-the-project.md","Context":"## Context\\nWhile the whole JavaScript community welcomes TypeScript easily, our project's JS is not typed at all. To improve the maintainability of the project and use the latest TC39 features, a good idea would be to use TypeScript.\\nBenefits of using it for the project:\\n- Detect bugs before pushing PRs. TypeScript users say that globally, it allows detecting around 15% of bugs that you would detect by testing.\\n- Use latest features such as Optional Chaining, Tuples, and Records... really early.\\n- Types are increasing the quality of the project because we would be able to detect dangerous changes, related bugs... If we use it on the PHP side, why don't we use types while using JS?\\n- Vue 3 offers a new API: Composition API, this one is pretty easy to use with TypeScript as it's mainly functional programming instead of opinionated APIs of Vue, that would be a good move to preshot the Vue update in the BO.\\n","Decision":"Add the possibility to transpile ts files inside every js folder of the project with webpack.\\n[Here is a POC](https:\/\/github.com\/PrestaShop\/PrestaShop\/pull\/23221) - basically using TypeScript on a small part of the PrestaShop Grid system.\\n","tokens":202,"id":434}
{"File Name":"libelektra\/script_testing.md","Context":"## Problem\\nWriting portable shell code for testing command-line tools is difficult.\\n","Decision":"Develop [shell recorder](\/tests\/shell\/shell_recorder) and [tutorial wrapper](\/tests\/shell\/shell_recorder\/tutorial_wrapper).\\n","tokens":16,"id":1294}
{"File Name":"where-away\/0002-name-the-project-where-away.md","Context":"## Context\\nThe goal of the project is to make a lightweight, keyboard navigable set of\\nbookmarks. Basically the browser bookmark toolbar, but a webpage with\\nsingle-key navigation (like intelliJ or Excel when you press Alt and\\nnumbers\/letters show up on the actions so you can select them quickly).\\nSo, it's a link management tool.\\nAlso, I'd like to publish it as a package to npm so that I can use it easily at\\nwork. So, the name can't be taken in npm.\\nSome obvious ones that are taken:\\n- linker\\n- whereto\\nSo how about obscure nautical terms? \"Where Away?\" means \"Inquiry addressed to a\\nlook-out man, demanding precise direction of an object he has sighted and\\nreported.\" [nautical glossary](http:\/\/www.marinewaypoints.com\/learn\/glossary\/glossary.shtml)\\nI get a kick out of that, and `where-away` (and `whereaway`) are not taken on\\nnpm at the time of writing.\\n","Decision":"Call the package `where-away`.\\n","tokens":223,"id":2278}
{"File Name":"remultiform\/0006-use-jest.md","Context":"## Context\\nWe want a test framework that has good support for React and TypeScript.\\n[Jest](https:\/\/jestjs.io) is the standard, recommended test framework for React\\napps.\\n","Decision":"We will use Jest as our testing framework.\\n","tokens":40,"id":5252}
{"File Name":"JustRooms\/0003-allow-converters-in-application-layer.md","Context":"## Context\\nA clean architecture depends inwards, putting I\/O concerns at its boundary\/adapter layer. DynamoDB has an object model that allows us to mark up an entity with persistence information.\\nBy adding this persistence information to the entity, in the form of attributes, we couple the application to a persistence concern--how do we persist these entities to DynamoDB, which would be redundant if we changed how we stored the entity.\\nIn addition, we have to add converter classes to the Application layer, as the attribute markup needs to know how to persist a 'custom' field type, which contains information on DynamoDB stores values.\\nThe alternative is to use a DTO, marked up with DynamoDB attributes in the boundary\/adapter layer, and then use automapper in the ports\/interactors layer to map to and from DTOs.\\n","Decision":"Allow markup on the entity and put converters in the application layer. As we 'own' our own tables in Dynamo the risk of coupling to them is low, and the use of attributes is orthogonal to the code, allowing them to be easily removed if we were to change DB. This is tradeed-off against the cost of having to implement the DTO and autommapper.\\n","tokens":170,"id":2305}
{"File Name":"abracadabra\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":594}
{"File Name":"superwerker\/notifications.md","Context":"## Context\\nsuperwerker creates an OpsItem in Systems Manager OpsCenter for each email received by the [RootMail](rootmail.md) feature. Without notifications for new OpsItems, users need to check the OpsCenter for new items manually and might miss important information regarding their AWS accounts and resources.\\n","Decision":"- Use CloudWatch Events to trigger an AWS Lambda function whenever a new OpsItem is created.\\n- OpsCenter \/ OpsItem supports SNS notifications, but the desired SNS topic Arn needs to be provided explicitly whenever an OpsItem is created. Therefore, we decided against this native feature.\\n- Publish a message to an SNS topic for every new OpsItem.\\n- Use SNS, since subscriptions and email verification work out-of-the-box with CloudFormation tooling.\\n- Use native email subscriptions for SNS to notify a specified email address about new messages.\\n- We decided against using SES for email notifications since this would lead to several additional steps like verifying sender and recipient domains or email addresses.\\n- We decided against (re-)using the existing root email address of the management AWS account since we would need to keep this in sync with the SNS subscription (because the management account root email adress can be changed). And we wanted to keep the notification feature simple.\\n- If no email address is provided, no SNS topic is created.\\n","tokens":62,"id":3405}
{"File Name":"modular-monolith-with-ddd\/0003-use_dotnetcore_and_csharp.md","Context":"## Context\\nAs it is monolith, only one language (or platform) must be selected for implementation.\\n","Decision":"I decided to use:\\n- .NET Core platform - it is new generation multi-platform, fully supported by Microsoft and open-source community, optimized and designed to replace old .NET Framework\\n- C# language - most popuplar language in .NET ecosystem, I have 12 years commercial experience\\n- F# will not be used, I don't have commercial experience with it\\n","tokens":23,"id":886}
{"File Name":"agentframework\/0013-interceptor-only-works-with-class-got-agent-decorator.md","Context":"## Context\\nFor code like below. `Base` class don't have @agent decorator. So `Super` class need do deep search to lookup all interceptors in its prototype chain. this search impact performance a lot.\\n```typescript\\nclass Base {\\n@decorateMember({\\ninterceptor: {\\nintercept() {\\n\/\/....\\n}\\n}\\n})\\ntest1() {\\n\/\/....\\n}\\n}\\n@agent()\\nclass Super extends Base {\\n@decorateMember({\\ninterceptor: {\\nintercept() {\\n\/\/....\\n}\\n}\\n})\\ntest2() {\\n\/\/....\\n}\\n}\\n```\\n","Decision":"In current version \"2.x\". `test1` will not intercepted, only `test2` will be intercepted.\\n","tokens":147,"id":2362}
{"File Name":"FindMeFoodTrucks\/CosmosDB hosting.md","Context":"## :dart: Context\\nAzure Cosmos DB is available in two different capacity modes: provisioned throughput and server less.\\nChoosing the right model will help optimize the cost and performance\\n","Decision":"The recommended approach is to use Server less mode\\nThe decision is based on the guidance provided by Microsoft here: https:\/\/docs.microsoft.com\/en-us\/azure\/cosmos-db\/throughput-serverless\\nThe approach is based on the following assumptions\\n* The application had no organizational constraints to use preview technology.\\n* The application does not have requirements for being geo-redundant.\\n* Maximum throughput is less than 5,000 RU\/s\\n* Maximum storage is less than 50 GB\\n","tokens":37,"id":1088}
{"File Name":"SiebenApp\/0002-create-adaptive-goal-tree-enumeration.md","Context":"## Context\\nCurrently, we have a fixed mapping between the inner goal identificator (actually, just an index in `Goals.goals` dictionary) and the user-visible goal number that is used for selecion (also called \"enumeration\"). This decision was easy to implement, but it seems to have significant downsides.\\n1. Goal numbers can only grow and grow. This makes long-living trees look ugly. We may have only few open goals in a tree, but most of them could have 3-digit numbers (because of a big amount of closed goals which are also used to generate enumeration).\\n2. It makes difficult to create nested goaltrees (see issue #6). It seems reasonably for the nested goaltree to have its own enumeration starting from 1. But the current enumeration function is not flexible enough to support this naturally.\\n","Decision":"We decide to extract the enumeration function `id_mapping` out from the `Goals` class. It should be transformed into the wrapper which transforms actual goal identificators into user-visible numbers and, respectively, keeps backward mapping (which is used for goal selection).\\n","tokens":173,"id":1918}
{"File Name":"CrossyToad\/adr-0005-use-hpack.md","Context":"## Context\\nCabal files are annoying to specify, they tend to have a fair bit of redundancy and I found\\nthem annoying to use in the past.\\nPain points include:\\n- Having to specify `ghc-options` for each target\\n- Having to specify common dependencies for each target\\n- Having to manually specify _every exposed module_\\n[hpack](https:\/\/github.com\/sol\/hpack) solves these problems, so let's use it!\\n","Decision":"We're using [hpack](https:\/\/github.com\/sol\/hpack)!\\n","tokens":95,"id":2481}
{"File Name":"cena\/0003-adopt-hexagonal-architecture.md","Context":"## Context\\nAdopting the [Domain-driven design](0002-adopt-ddd-approach.md) approach requires isolating domain elements (i.e.\\naggregates and services) from the infrastructure (i.e. application clients and persistence).\\n","Decision":"`menu-generation` application will adopt [hexagonal architecture](https:\/\/en.wikipedia.org\/wiki\/Hexagonal_architecture_(software)),\\nas it aims to provide this separation.\\n","tokens":53,"id":643}
{"File Name":"editions\/08-\u2705-releases.md","Context":"## Context\\nWe need a consistent and battle tested process for releasing the app on both Android and iOS.\\n","Decision":"### iOS\\n#### Internal Beta\\nOur internal Beta is managed through testflight on the Guardian developer account. The group which this beta is sent to is labelled `GNM`. This includes the team and internal stakeholders within the organisation. We build this automatically through Fastlane and Github Actions once a day. Occasionally we will set off builds to test things on a number of devices.\\nIn github actions we have a [scheduled build](https:\/\/github.com\/guardian\/editions\/actions?query=workflow%3Ascheduled-ios-beta) and an [ad-hoc one](https:\/\/github.com\/guardian\/editions\/actions?query=workflow%3A%22Upload+ios-beta%22) triggered by a [script](https:\/\/github.com\/guardian\/editions\/blob\/main\/script\/upload-ios-build.sh)\\nAll builds generate a ['release' in github](https:\/\/github.com\/guardian\/editions\/releases) to help us keep track of build numbers against certain commits. This is handled by the [make-release script](https:\/\/github.com\/guardian\/editions\/blob\/main\/script\/make-release.js).\\n#### External Beta\\nBefore every release, we aim to do at least one external beta to gather feedback. We have a number groups within testflight that are prefixed with the name `External Testers...`. These different groups represent the different authentication methods we support. When we decide a build is good enough from an internal test, we add the build to the groups.\\n#### Release\\nAfter a successful beta period, we release the same build (identified by its build number) through the app store submission process.\\n#### Post Release\\nWe update the version number in XCode and raise that as a PR. The version number will depend on the goals for the next release. We follow a major and minor number approach with no patch i.e. 5.6\\n### Android\\n#### Internal Beta\\nIn a similar vein as above, the Android internal beta is managed through Google Play. The APK for this is created using Fastlane through TeamCity. The name of this process is `android-beta-deploy`. The list for this is managed within the Google Play console. This process runs once per day. Users will need to update their app through their Google Play store.\\n#### External Beta\\nWe take a slightly different approach to iOS. Due to not being able to determine within the app whether or not the app is in Beta or release, we have a different build configuration for releasing to production - which hides the 'report bug' button.\\nWe build the APK using `android-beta-deploy`. This will release a build to google play to the 'internal beta' group for internal testing by the team. It is then manually promoted within the Google Play console to our external beta testers.\\nAs with iOS, releases to the play store can be tracked in [github releases](https:\/\/github.com\/guardian\/editions\/releases) - each time the teamcity build is run a new github release is created including the play store version code of that release.\\n#### Release\\nAfter a successful external beta test, we **DO NOT** promote the external beta. This is because we have code in the app that attempts to determine whether or not the user is in beta. This does not work on Android as there isn't a distinction.\\nAs a result, we then use the TeamCity process `android-release-deploy` to then build the APK. As you will only want to release a version that has been beta tested, you can use the [releases](https:\/\/github.com\/guardian\/editions\/releases) list to find a release for the build you want to release, and copy the tag for that build. You can then search for this tag in the branch list within teamcity (the `android-release-deploy` config treats github tags as if they were branchs) and run a build on that tag.\\nBe warned, this process will automatically release the new version of the app. You will then need to go into the Google Play console to update the release notes.\\n","tokens":22,"id":668}
{"File Name":"re-build-systems\/0006-buildah-discovery.md","Context":"## Context\\nThere is a desire from users to be able to build docker images as an artifact from their jobs, this creates a security problem because of technical limitations within docker.  This ADR is the result of discovery work to discern whether there are alternative solutions to building docker images securely.  The discovery focussed on evaluating the software product called [Buildah].\\n## Technical Problem\\nAs the workers run within a docker container, they may need to build another docker image as an artifact.  To do so requires that the host servers docker socket be exposed to the worker container.  The docker daemon runs as root and has full control of the host server, meaning malicious code running within the workers docker container could trivially gain access to the host system by mounting (via -v) the host systems root.\\nAs Docker does not support RBAC to control and restrict access to its socket there is no way provided by docker to prevent this privilege escalation method occuring.\\nBuildah solves this problem by being able to build a docker container without the need to use docker and the vulnerable socket.\\n","Decision":"Buildah addresses the problems of docker container privilege escalation, is feature rich to support typical use cases (as it is feature compatible with docker), and provides us a means in which to provide docker image artifacts.\\n","tokens":222,"id":324}
{"File Name":"CMp-Architecture-Decision-Records\/0001-record-product-decisions.md","Context":"## Context\\nWe need to record the product decisions made on this project, including the context, rationale and date. This will help maintain common understanding across the wider project team, and enable us to review past decisions when the context or the basis for a decision change.\\nSee Michael Nygard's article, linked above. For a lightweight ADR toolset, see Nat Pryce's [adr-tools](https:\/\/github.com\/npryce\/adr-tools).\\n","Decision":"We will capture key Product Decisions and save them in an [accessible GitHub repository](https:\/\/github.com\/Crown-Commercial-Service\/CMp-Architecture-Decision-Records).\\nWe will not capture day-to-day decisions, but those material to the product direction, scope, or approach.\\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions)\\nto store these decisions.\\nIn particular, we recognise that decisions can be proposed and accepted, as well as deprecated or superseded, as we continue to iterate and learn.\\nThe Product Manager(s) own the status and content of PDRs.\\n","tokens":94,"id":2373}
{"File Name":"PerformanceTestDocs\/0005-homestead-dime-fails.md","Context":"## Context\\nHomestead instance fails before receiving request from sprout.\\n","Decision":"Homestead\u2019s HTTP interface is simple. If one homestead instance does not respond to sprout, sprout tries a different one\\n","tokens":15,"id":3954}
{"File Name":"stamper\/0006-thymeleaf.md","Context":"## Context\\nWe need to choose view representation\\n","Decision":"We use Thymeleaf because it provides:\\n- clean HTML view\\n- templates that can be run without server\\n","tokens":10,"id":1245}
{"File Name":"operational-data-hub\/0010-cloudbuild-yaml-style.md","Context":"## Context\\nWe feel the need to use a specified cloudbuild.yaml style.\\n","Decision":"We identify different rules that should be followed when creating or editing a cloudbuild.yaml file:\\n### 1. Start of file\\nSince cloudbuild.yaml is a yaml file, the file should always start with `---`.\\n### 2. An ID for every step\\nMake sure that every step has an ID. Rules for this ID:\\n* It is a short but clear title containing an explanation of what the step does, a maximum of 50 characters should suffice.\\n* The words in the ID should be divided by `-`.\\n* If you cannot explain the step in 50 characters, add a comment above the step explaining what the step does.\\n### 3. Avoid \"waitFor\" field\\nTry to avoid using the `waitFor` field as much as possible since not using it makes sure that the steps are done in the order they are put in the cloudbuild.\\n### 4. Empty lines between build steps\\nUse empty lines between build steps to increase readability.\\n### 5. Order of steps\\nThe following rules should be followed when deciding on the order of the steps in the cloudbuild:\\n* The first step of the cloudbuild should always be the deployment of the data catalog.\\n* The subsequent steps should be the cloning of the necessary Github repositories.\\n* Cloud function permissions should be set in the build step after the deployment of the cloud function.\\n* If a scheduler is deployed in a step and it schedules a specific function which is also deployed in the cloud build, the step containing the deployment of the scheduler should be deployed right after the step which deploys the permissions of the cloud function.\\n* If a chain test is added, make sure that it is the last step in de cloudbuild.\\n### 6. Avoid if-else statements based on branches\\nAvoid if-else statements based on branch names.\\n### 7. Build container\\nAlways use `'gcr.io\/google.com\/cloudsdktool\/cloud-sdk:latest'` als build container (in the `name` field of the step). Different entrypoints can be used with this container.\\n### 8. Bash pipe\\nWhen using bash as entrypoint in a cloud build step, one should always use a pipe.\\n### 9. Function call in step\\nIf a function call is done in a step, its command plus subcommand should be on one line while its options or flags should be defined on separate lines. One should also use the long options if they are available, e.g. `-q` becomes `--quiet`.\\n### 10. Do not use gcloud as entrypoint\\nDo not use `gcloud` as entrypoint but use bash instead when having to call gcloud functionalities.\\n### 11. Cloud scheduler deletion\\nWhen deploying a cloud scheduler, the old \"version\" of this scheduler should first be deleted before redeployment.\\n### 12. One line commands without bash entrypoint\\nWhen a step only has one line and does not have a bash entrypoint, there are two ways you can define the step. Below is an example of a step where a github repository is cloned.\\n* ```yaml\\n- name: 'gcr.io\/google.com\/cloudsdktool\/cloud-sdk:latest'\\nid: 'cloning-repository'\\nentrypoint: 'git'\\nargs:\\n- 'clone'\\n- '--branch=git-branch'\\n- 'github-repository-url'\\n```\\n* ```yaml\\n- name: 'gcr.io\/google.com\/cloudsdktool\/cloud-sdk:latest'\\nid: 'cloning-repository'\\nentrypoint: 'git'\\nargs: ['clone', '--branch=git-branch', 'github-repository-url']\\n```\\n","tokens":19,"id":2704}
{"File Name":"innovation-week\/adr-0003-mvvm-architecture.md","Context":"## Context\\nThe initial version of the codebase was built without a specific architecture in mind, and this has led to a few problems:\\n1. New UX flow proposals were incompatible with previous assumptions of how different views were bound together, and those were fragile and difficult to change.\\n1. Integrating large pieces of the android-components library caused many regressions, partly because state management is distributed throughout different parts of the UI.\\n1. Testing app state (especially the BrowserOverlay, or homescreen) was difficult because side effects of actions were spread out throughout the app, rather than being consolidated.\\n1. A formal architecture was needed to provide clarity for where to change\/add code as the number of people working with the codebase grows.\\nFormal architectures address these problems by explicitly decoupling the state and UI. The common ones are MVC\/MVP, MVVM, MVI.\\n","Decision":"We decided to use [Google's MVVM with Architecture Components](https:\/\/developer.android.com\/jetpack\/docs\/guide) because in addition to satisfying the other requirements of separating state and views, it also allowed for incremental change, which was important because switching to a new architecture would extend past a single sprint, and there was the possibility that we\u2019d need to respond to critical issues during the refactor. It\u2019s also a familiar and well-documented architecture, and integrates well with Android\u2019s `ViewModel` and `LiveData`.\\nWe also considered MVI, which would make testing easier because state is immutable, but MVI didn\u2019t allow for incremental change, and has a steeper learning curve. We also decided against MVP because it doesn\u2019t enforce very clear separations between model and presenter, which often leads to bloated presenters, and could complicate Android lifecycle management.\\n### Model\\n- Different Repositories hold state for components of the app old state\\n- Logic to handle actions that change the model\\n### ViewModel\\n- Observes changes in Repositories and updates views\\n- Calls actions exposed by Model from UI interactions\\n### Fragments\\n- Connect VM and Model during `Fragment#onViewCreated` by setting VM observers on the Model\\nStatus: Accepted\\n","tokens":183,"id":2017}
{"File Name":"CrossyToad\/adr-0006-sdl2-for-rendering.md","Context":"## Context\\nWe're building a game, which means we need a way to render things, do audio, create windows\\nand other game-like activities.\\nWe could use an engine like `Helm`, but since this project is about learning and exploring\\npossible architectures I don't want to take a pre-existing architecture.\\nWe could use [gloss](https:\/\/hackage.haskell.org\/package\/gloss). However I used it in\\na previous project and I don't like how it takes over the main loop, particularly as it\\nmade it tricky to integrate into a MTL stack.\\nInstead I'm going to use the haskell bindings for [sdl2](https:\/\/hackage.haskell.org\/package\/sdl2). It's fairly low level but it's a cross-platform way to get an OpenGL context, and the high-level bindings seem reasonable enough.\\n","Decision":"We're using [sdl2](https:\/\/hackage.haskell.org\/package\/sdl2)\\n","tokens":179,"id":2477}
{"File Name":"api-docs\/0003-blog-post-as-a-documentation-page.md","Context":"## Context\\nA document must be inserted into Jekyll.\\n","Decision":"For each project document, create a new blog post in Jekyll.\\n","tokens":13,"id":5154}
{"File Name":"dalmatian-frontend\/0007-use-coveralls-for-monitoring-test-coverage.md","Context":"## Context\\nWe want to keep our test coverage as high as possible without having to run manual checks as these take time.\\n","Decision":"Use the free tier of Coveralls to give us statistics and to give our pull requests feedback.\\n","tokens":26,"id":2556}
{"File Name":"opg-data\/0004-content-negotiation.md","Context":"## Context\\nIt is becoming more common to recognise that a specific format of a resource is the combination of content type AND version.\\n","Decision":"For the reasons explored in [2. API versioning strategy](0002-api-versioning-strategy.md),\\n* We will implement versioning via Content Negotiation using the Accept header, as per option 5, below. This seems the most future proof, most RESTful solution.\\nThis necessitates our own vendor content type, which we will call `opg-data`\\n`application\/vnd.opg-data.[version]+[representation]`\\nRepresentation will default to `json` and version will default to `latest`\\nSome Examples:\\n* `application\/vnd.opg-data.v1+json` (v1 presented as JSON)\\n* `application\/vnd.opg-data.v1+yml` (v1 presented as YAML)\\n* `application\/vnd.opg-data.v1` (v1 presented as JSON)\\n* `application\/vnd.opg-data` (latest version, presented as JSON)\\n* `application\/json` (latest version, as JSON)\\n","tokens":27,"id":2180}
{"File Name":"datalab\/0037-single-ingress-controller-per-cluster.md","Context":"## Context\\nCurrently we have an Nginx Ingress controller deployed in the same namespace\\nas the Datalabs application. This means that when multiple instances of\\nDatalabs are deployed to a single cluster (such as prod and test), ingress\\ncontrollers are deployed to each of these namespaces.\\nAs we are in the process of implementing multiple project functionality within\\nDatlabs, there is now a need for an ingress controller than can fulfill ingress\\nrules across all namespaces. This also will move control of ingress to be a\\nplatform service and not part of the deployment of the application itself,\\nwhich will aid to decouple Datalabs into being more of a standalone\\napplication.\\n","Decision":"We have decided to deploy a single Nginx Ingress Controller into the\\nkube-system namespace that will handle the ingress rules for the entire\\ncluster.\\n","tokens":141,"id":755}
{"File Name":"re-build-systems\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in\\n[documenting architecture decisions](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions)\\n","tokens":16,"id":325}
{"File Name":"clone_difitalcitizenship\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":1189}
{"File Name":"terraform-aws-msk-module\/0005-feature-flags.md","Context":"## Context\\nTo make this module user-friendly we need to support a number of identified use cases\\n1. MSK Cluster \/ Default Internal VPC\\n2. MSK Cluster \/ Configured Internal VPC\\n3. MSK Cluster \/ External VPC\\n4. MSK Cluster using Custom Configuration\\n5. MSK Cluster using Client Authentication\\nWe should make it simple for users to create an MSK cluster in each of these configurations.\\n","Decision":"To enable this the module will utilise feature toggles. The proposed feature toggles are\\n1. Create VPC\\n2. Use Custom Configuration\\n3. Use Client Authentication\\nThe configured internal VPC will utilise the Create VPC feature flag with additional configuration settings.\\n","tokens":91,"id":2229}
{"File Name":"js-sdk\/0013-update-cryptpad-flist.md","Context":"## Context\\nCryptpad solution image used in docker was deprecated and flist doesn't make use of volumes\\n","Decision":"- Update flist to make use of volumes\\n- Update base image in docker file to be able to maintain\\n","tokens":22,"id":5202}
{"File Name":"datalab\/0017-separate-storage-cluster-rather-than-hyper-converged.md","Context":"## Context\\nWe need to decide whether to run our storage cluster as a standalone cluster or\\nhyper-converged by running pods on the Kubernetes cluster.\\n","Decision":"We have decided to run a standalone storage cluster. The reason for using a separate\\ncluster is that by keeping the persistent data separate we keep flexibility over the\\nKubernetes cluster and can drop and recreate it without having to worry about the data.\\n","tokens":32,"id":758}
{"File Name":"rails-template\/0005-use-dotenv-for-managing-environment-variables.md","Context":"## Context\\nAccessing ENV directly without a wrapper is limited and can introduce problems.\\nWe want our tooling to help us guard against missing environment variables. When\\n`nil` is accidentally provided during the start up process it is preferable to\\nfail fast with an explicit message. Without this `nil` can be passed down\\nthrough the stack and cause strange behaviour where the code has been designed\\nwith it as a dependency. Instead of adding `nil` guards throughout the codebase\\nfor required environment variables (eg. `ENV.fetch('FOO', 'default')`, this\\nshould be managed centrally.\\nWe have previously used Figaro for this purpose but it was deprecated in 2016\\nhttps:\/\/github.com\/laserlemon\/figaro. We should only use supported gems to\\nensure we get support in the form of fixes and security patches.\\nWe also want to be able to stub our environment variables in our test suite. An\\neasy example of this is when we use environment variables as a feature flag\\nmechanism. We want to stub the value to test both scenarios without being\\ninfluenced by real values being loaded. Mutating the actual ENV value (eg.\\n`allow(ENV).to receive(:[]).with('BOX_ID').and_return(\"1234\")`) is possible but\\nmay have unexpected consequences where more than 1 part of the same process\\nunder test uses the same variable. Figaro used to be a handy abstraction layer\\nthat we could stub eg.\\n`allow(Figaro).to receive(:env).with(:foo).and_return('bar')`. We should then\\nconsider how we can stub environment variables.\\n","Decision":"Use DotEnv to load our environment variables.\\n","tokens":346,"id":4181}
{"File Name":"rfcs\/0002-replace-username-with-email.md","Context":"## Context\\n[context]: #context\\nCurrentrly existing users are identified by both a *user name* and an *e-mail address*.\\nOnly the user name is guaranteed to be unique, but not the e-mail address.\\nThe database already contains different users for the same e-mail address:\\n```sql\\nselect id, role, username, email, email_confirmed\\nfrom users\\nwhere email in (select email from users group by email having count(*) > 1)\\norder by email;\\n```\\nUsers are able to provide either the user name or the e-mail address and all\\ndatabase queries have to account for this duality.\\nFor new users the user name is already generated from the e-mail address\\nand cannot be chosen. The user name has become redundant and could be\\nreplaced by the e-mail address.\\n","Decision":"[decision]: #decision\\nUse the e-mail address for identifying users:\\n- Merge users with the same e-mail address into a single user\\n- Prevent duplicate e-mail addresses in the future\\n- Replace all references of *username* by *email*\\n- Remove *username* from the database and the code base\\n","tokens":179,"id":1882}
{"File Name":"opg-data\/0012-python-coding-standards.md","Context":"## Context\\n> Readbility counts -- [The Zen of Python](https:\/\/www.python.org\/dev\/peps\/pep-0020\/)\\n","Decision":"### Code Formatting\\n* Follow [PEP-8](https:\/\/www.python.org\/dev\/peps\/pep-0008\/)\\n* Use pre-commit hooks to enforce these standards where possible\\n### Types\\n* Type checking is a bit overkill for small single functions, so we will not enforce this.\\n### General Style Guide\\n* Make your comments verbose, not your code, eg:\\n* Use list comprehensions and map() where it is sensible to instead of a loop\\n* Use ternary operator\\n* Only use f-strings for string interpolation\\n* Catch  named errors, don't just use generic `except` blocks\\n* Be sensible with log levels, debug is useful but we don't need to know *everything*\\n### Documentation\\n* Follow [PEP-257](https:\/\/www.python.org\/dev\/peps\/pep-0257\/) for docstring formatting using [reStructured Text](https:\/\/www.writethedocs.org\/guide\/writing\/reStructuredText\/)\\n* Generate documentation with Sphinx\\n### AWS Lambda Specific Guidelines\\n* See [Amazon guidelines](https:\/\/docs.aws.amazon.com\/lambda\/latest\/dg\/best-practices.html)\\n### Standard Tools\\n* Code Formatting: [Black](https:\/\/black.readthedocs.io\/en\/stable\/)\\n* Linting\/Complexity\/Error Detection: [Flake8](https:\/\/flake8.pycqa.org\/en\/latest\/)\\n* Type Checking: [MyPy](http:\/\/mypy-lang.org\/)\\n* Unit Tests: [PyTest](https:\/\/docs.pytest.org\/en\/latest\/)\\n* Package Management: [Pip](https:\/\/pypi.org\/project\/pip\/)\\n* Emojis: [Emoji](https:\/\/pypi.org\/project\/emoji\/)\\n* Dates: TBD\\n","tokens":33,"id":2186}
{"File Name":"afterwriting-labs\/002-technical-debt.md","Context":"## Context\\nTechnical Debt is something that emerges as trade-off between short-term benefits and long-term value. It's OK to have\\nTechnical Debt as long as it measurable and manageable. At any point of time it should be possible to say how big the\\ndebt is, what parts of code are affected and how critical the debt is. It's required to help to plan and to estimate\\nnew features.\\n","Decision":"A little twist will be added to old good TODOs that all developers hate. Here it goes:\\n* When debt is identified a TODO note is added to file\/function\/code fragment depending on the scope;\\n* Debt Markers are added to the description (a debt marker is \"+\");\\n* Each time code is modified, the related debt should be:\\n* paid by fixing TODO,\\n* or increased by adding another Debt Marker;\\n* Debt cannot be greater than 5 (\"+++++\"). When debt reaches 5, no code can be modified before fixing the TODO;\\n* If debt for a code fragment is increased, the debt for the function and file also increases;\\nIt should assure that Technical Debt is tracked. In places of code that is not used frequently, debt will not increase too\\noften. There's no need to pay the debt for unused code.\\nIf code, file, function is used frequently, any related debt should be dealt with quickly. Debt markers will make sure\\neach time code in debt is used, the debt increases, forcing to pay it sooner.\\nIn cases of critical debt, a TODO note can be created with more than one initial Debt Marker.\\nEvery TODO must have at least one Debt Marker.\\nExample todo note:\\n\/**\\n* TODO: Improve performance from O(n^2) to O(n). (+++)\\n*\/\\nfunction foo() {\\n\/\/ ...\\n}\\nIf a TODO has multiple lines, Debt Markets must be added at the same line to make it easier to parse. Markers can be placed\\nat the beginning, e.g.:\\n\/\/ TODO: (++) Move method to presentation controller\\n\/\/ and add unit tests.\\nfoo: function() {\\n...\\n}\\n","tokens":85,"id":5108}
{"File Name":"event-routing-backends\/0002-binding-app-for-caliper.rst","Context":"Context\\n-------\\nWe need to decide some approach to encapsulate and structure the Caliper\\nevents. We have the following options for binding caliper events:\\n-  Make our own data structure to store a caliper event (e.g. a python dict)\\n-  Use IMS\u2019 `caliper-python <https:\/\/github.com\/IMSGlobal\/caliper-python>`__ library\\nDecision\\n--------\\nWe will be using python dictionaries for processing and encapsulation of\\ntransformed events to keep things simple.\\nConsequences\\n------------\\nPython dictionaries will be used for binding Caliper events. These\\nevents stored as python dictionaries can be converted into JSON later\\nand sent to interested consumers.\\nRejected Alternatives\\n---------------------\\n**Use IMS\u2019 caliper-python library**\\nThe library under discussion is not published on\\n`PyPI <https:\/\/pypi.org\/search\/?q=caliper>`__. We\u2019d have to fork the\\nrepo and add its dependency on our app. Therefore we won\u2019t be using this\\nlibrary.\\n","Decision":"--------\\nWe will be using python dictionaries for processing and encapsulation of\\ntransformed events to keep things simple.\\nConsequences\\n------------\\nPython dictionaries will be used for binding Caliper events. These\\nevents stored as python dictionaries can be converted into JSON later\\nand sent to interested consumers.\\nRejected Alternatives\\n---------------------\\n**Use IMS\u2019 caliper-python library**\\nThe library under discussion is not published on\\n`PyPI <https:\/\/pypi.org\/search\/?q=caliper>`__. We\u2019d have to fork the\\nrepo and add its dependency on our app. Therefore we won\u2019t be using this\\nlibrary.\\n","tokens":218,"id":4480}
{"File Name":"buy-for-your-school\/0012-create-infrastructure-on-gpaas-cloud-foundry-with-terraform.md","Context":"## Context\\nThe early beta was originally hosted on dxw's Heroku to get delivering quickly whilst access to GPaaS could be set up.\\nAccess to GPaaS with approved billing has now been confirmed so we are migrating the service to its longer term home.\\n","Decision":"- Move the service from Heroku to GPaaS for all environments except the ephemeral pull request review environments.\\n- Use Terraform to define the infrastructure as code.\\n","tokens":54,"id":1250}
{"File Name":"remultiform\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as\\n[described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":5261}
{"File Name":"mental-health-service-finder\/0004-use-local-data-for-redbridge-and-north-cumbria-ccgs.md","Context":"## Context\\nNorth Cumbria CCG and Redbridge CCG do not have an ODS code. As a consequence it is not possible to relate them to the IAPT services. When a GP is serving under either of these CCGs we can display the information for the CCG.\\n","Decision":"Rather than display no result we know the contact information for each of these CCGs. We have decided to store this information locally and display it for the user.\\nThis is only a temporary measure and the change will be reverted once the data has been loaded into the central system.\\n","tokens":61,"id":4579}
{"File Name":"paas-team-manual\/ADR029-aiven-project-structure.html.md","Context":"## Context\\nAiven provides hosted Elasticsearch for the Elasticsearch backing service.\\nThe PaaS has several environments which will need to use Aiven. These\\nenvironments should be isolated from each other so that changes made in testing\\nand development environments do do not affect production users.\\nAiven provide a \"Project\" abstraction where a user can be a member of several\\nprojects. API tokens are user specific. By creating one user per project it's\\npossible to scope API tokens to a project.\\n","Decision":"We'll use separate projects for separate environments, initially using the\\nfollowing Aiven projects:\\n* ci-testing (for the CI environment for the elasticsearch broker itself)\\n* paas-cf-dev\\n* paas-cf-staging\\n* paas-cf-prod\\nFor staging and prod we will use separate API tokens within the same project to\\nseparate credentials between the London and Ireland regions.\\nWe will have the following per-project users to hold API tokens:\\nthe-multi-cloud-paas-team+aiven-ci@digital.cabinet-office.gov.uk\\nthe-multi-cloud-paas-team+aiven-dev@digital.cabinet-office.gov.uk\\nthe-multi-cloud-paas-team+aiven-staging@digital.cabinet-office.gov.uk\\nthe-multi-cloud-paas-team+aiven-prod@digital.cabinet-office.gov.uk\\nThe credentials for the ci and dev users will be stored in the\\n`paas-credentials` passwordstore. staging and prod will be stored in\\n`paas-credentials-high`.\\nMembers of the PaaS team will each have their own user which will have access\\nall of the projects for management purposes.\\n","tokens":100,"id":191}
{"File Name":"boxtribute\/adr_python-dev-env.md","Context":"## Context\\nThe main programming language for the present repository is Python. The Python ecosystem features a rich set of tools and libraries that facilitate developing an industry-standard codebase. Development includes implementation of production and test code in an environment built on linting and formatting tools.\\n","Decision":"1. Test-driven development\\n1. Code maintainability\\n1. Code scalability\\n1. Code format consistency\\n1. Testing: `pytest`. Compared to unittest, test code is more concise yet readable, test fixture setup is straightforward, and test assertion output is very clear and helpful for debugging.\\n1. Code formatting: `black`. Developed by the Python Software Foundation themselves. Uncompromising, fast, deterministic.\\n1. Linting: `flake8`. Detection of common code smells. `pylint` tends to be pickier and might hinder rapid initial development.\\n1. Integration with `git`: `pre-commit`. Automatic style checks prior to committing. Orchestration of style check tools.\\n1. Isolated Python development environment: `venv`. Isolation of system and project Python packages in so called 'virtual environments'.\\n","tokens":54,"id":1458}
{"File Name":"simple-server\/009-single-user-model.md","Context":"## Context\\nWe currently have two separate models for Administrators (Admin) and Nurses (User)\\nto distinguish users of the dashboard and the app respectively. But there is an increasing\\noverlap between the two models.\\n- Some Administrators are using the app\\n- Access to dashboard needs to be audited similarly to syncing\\n","Decision":"Combine User and Admin into a single User model, and have different authentication mechanism for\\nboth these models.\\n- Authenticate syncing api using phone number authentication\\n- Authenticate dashboard access using email authentication\\n","tokens":67,"id":1699}
{"File Name":"deeplearning4j\/0004-Mapping_IR.md","Context":"## Context\\nGenerally, every neural network file format defines a sequence of operations\\nto execute mathematical operations that comprises a neural network.\\nEach element in the sequence is a node that contains information such as the\\ndesired operation, and a set of attributes that represent parameters\\nin to the mathematical function to execute.\\nIn order to write import\/export for different frameworks, we need to adapt\\nan attribute based format from various popular deep learning frameworks.\\nNd4j  has a different list based format for operation execution arguments.\\nIn the [previous ADR](.\/Import_IR.md), we added an IR which makes it easier to\\ninterop with other frameworks.\\nIn this ADR, this work is extended to add a file format for\\ndescribing lists of operations as MappingRules which allow transformations\\nfrom one framework to another.\\nThese transformations manipulate protobuf as input and output Nd4j's\\nnew OpDescriptor format as output.\\n","Decision":"We implement a mapping process framework that defines transforms on an input file format.\\nA MappingProcess defines a list of MappingRules which represent a sequence of transformations\\non each attribute of an op definition.\\nTo assist in mapping, a mapping context with needed information like rule arguments\\nfor transformation, current node, and whole graph are used as input.\\nThe input is a protobuf file for a specific framework and the output is an op descriptor\\ndescribed [here](.\/0003-Import_IR.md).\\nA MappingRule converts 1 or more attributes in to 1 more or arg definitions. A potential definition\\ncan be found in Appendix E.\\nAttributes are named values supporting a wide variety of types from floats\/doubles\\nto lists of the same primitive types. See Appendix C for a theoretical definition.\\nArg Definitions are the arguments for an OpDescriptor described in [the import IR ADR.](.\/0003-Import_IR.md)\\nSee Appendix D for a potential definition of arg definitions.\\nAll of this together describes how to implement a framework agnostic\\ninterface to convert between a target deep learning framework and the nd4j format.\\n","tokens":194,"id":2943}
{"File Name":"james-project\/0044-against-the-use-of-cassandra-lightweight-transactions.md","Context":"## Context\\nAs any kind of server James needs to provide some level of consistencies.\\nStrong consistency can be achieved with Cassandra by relying on LightWeight transactions. This enables\\noptimistic transactions on a single partition key.\\nUnder the hood, Cassandra relies on the PAXOS algorithm to achieve consensus across replica allowing us\\nto achieve linearizable consistency at the entry level. To do so, Cassandra tracks consensus in a system.paxos\\ntable. This `system.paxos` table needs to be checked upon reads as well in order to ensure the latest state of the ongoing\\nconsensus is known. This can be achieved by using the SERIAL consistency level.\\nExperiments on a distributed James cluster (4 James nodes, having 4 CPU and 8 GB of RAM each, and a 3 node Cassandra\\ncluster of 32 GB of RAM, 8 CPUs, and SSD disks) demonstrated that the system.paxos table was by far the most read\\nand compacted table (ratio 5).\\nThe table triggering the most reads to the `system.paxos` table was the `acl` table. Deactivating LWT on this table alone\\n(lightweight transactions & SERIAL consistency level) enabled an instant 80% throughput, latencies reductions\\nas well as softer degradations when load breaking point is exceeded.\\n","Decision":"Rely on `event sourcing` to maintain a projection of ACLs that do not rely on LWT or SERIAL consistency level.\\nEvent sourcing is thus responsible for handling concurrency and race conditions as well as governing denormalization\\nfor ACLs. It can be used as a source of truth to re-build ACL projections.\\nNote that the ACL projection tables can end up being out of synchronization from the aggregate but we still have a\\nnon-questionable source of truth handled via event sourcing.\\n","tokens":276,"id":2889}
{"File Name":"adr-playground\/0003-remove-unix-scripts.md","Context":"## Context\\nThe issue motivating this decision, and any context that influences or constrains the decision.\\n","Decision":"The change that we're proposing or have agreed to implement.\\n","tokens":21,"id":2026}
{"File Name":"datalab\/0025-mongodb-for-data-store.md","Context":"## Context\\nWe need to store persistent data in a form that is easy to query and need to select the\\nappropriate data store. We consider the choice to be between a relational database or a\\ndocument database.\\n","Decision":"We have decided to use [MongoDB](https:\/\/www.mongodb.com\/) as our database for datalabs\\ndata. We felt that the data model for the parts of the system known at this time, a\\ndocument structure provided more flexibility and easier integration with our Node.js\\napplication.\\nMongoDB also provides the ability to run in cluster providing the option for greater\\nresilience. For the time being we are opting to run a single node storing data to\\na mounted GlusterFS volume. This gives data resilience through node failure but obviously\\ndoes not give service resilience or time based backups.\\n","tokens":45,"id":730}
{"File Name":"Marain.Instance\/0002-services-use-arm-and-aad-indirectly-through-common-mechanisms.md","Context":"## Context\\nHistorically, our automated deployment scripts have tended to include a large amount of common boilerplate. This is partly because they are self-contained\u2014they can typically be run on their own to perform a deployment. And it is partly just because they originate from what the Visual Studio tooling creates.\\nThe problem with this is that it makes the project-specific details hard to spot. When looking at a sea of code that's almost identical to every other project, it's hard to see what it's doing that is in any way different from everything else.\\n","Decision":"With `Marain.Instance`, deployment scripts in individual services do not communicate directly with either ARM or Azure AD. (They should not even be aware of what mechanisms are being used to perform this work\u2014they should not need to know whether we are using the PowerShell Az module, the az CLI or even custom library code to talk to Azure, for example.)\\nAnything that needs to be done either in Azure or AAD must be done through operations provided by the shared `Marain.Instance` code. It passes in an object that provides various methods that provide the necessary services.\\n","tokens":112,"id":354}
{"File Name":"saas-platform-frontend\/0008-use-launchaco-com-to-generate-the-logo-for-free.md","Context":"## Context\\nThe landing page need a logo for the name. This could be design by hand but this requires skill and time or an online service could be used.\\n","Decision":"Generate the logo with [launchaco.com](https:\/\/www.launchaco.com\/logo\/editor).\\n","tokens":34,"id":3714}
{"File Name":"biosamples-v4\/0006-submitting-the-same-sample-twice.md","Context":"## Context\\nWhat should we do if a user submitted same sample twice ?\\n1. Update the first one by checking name and domain fields\\n2. Create another sample with an accession without considering the first one\\n3. Send an error message explaining that a sample exist with same name and domain\\n","Decision":"For now we decided to create a new sample with a new accession as this updating a sample could cause accidental data loss.\\n","tokens":61,"id":4725}
{"File Name":"remultiform\/0009-use-typedoc-to-generate-api-documentation.md","Context":"## Context\\nWe want to have API documentation for the code we publish. Rather than writing\\nseparate API docs that quickly get out of date, we would like to be able to\\ngenerate it from our code and comments in our code that live next to the thing\\nthey refer to. For JavaScript, the standard is [JSDoc](https:\/\/jsdoc.app\/), but\\nwith TypeScript, we're already defining the types in our code, and duplicating\\nthat in the associated comments is repeated effort and requires manual action to\\nkeep up-to-date.\\n[TypeDoc](https:\/\/typedoc.org\/) is a documentation generator based on JSDoc. It\\nuses a combination of comments and TypeScripts own types to generate API\\ndocumentation automatically.\\n","Decision":"We will use TypeDoc to generate documentation.\\nWe will document all exported code for the benefit of end users.\\nWe will commit the documentation we generate to the repository alongside changes\\nto behaviour.\\n","tokens":159,"id":5258}
{"File Name":"teacher-training-api\/0008-use-skylight-for-performance-monitoring.md","Context":"## Context\\nWe need to decide between using skylight versus sentry for performance monitoring.\\n","Decision":"[There was an attempt](https:\/\/github.com\/DFE-Digital\/teacher-training-api\/pull\/1860) to use sentry, but we managed to fill up our quota in less than a day, and while Sentry does offer tracing a subset of requests, this would only allow us to trace less than 1% of our requests.\\nAs such, we have decided to continue using skylight for performance monitoring.\\n","tokens":20,"id":2354}
{"File Name":"community\/dr-018-Knative_based_Kyma_eventing.md","Context":"## Context\\nBased on the decision of building Kyma on top of Knative, and along the effort to reuse as much of Knative functionality as possible, the eventing area shines as a good area for alignment. That is why a decision was made to have Kyma leverage as much of Knative Eventing as possible. This is a an architecture decision on how to achieve that target.\\nThe assumptions are as follows:\\n1. Base Kyma on Knative.\\n2. Favor a smooth, gradual migration of the Kyma components to Knative Eventing.\\n3. Consider areas of contribution to Knative Eventing.\\n4. Event Bus public integration: Publish RESTful API, EventActivation and Subscriptions CRDs are still needed.\\n5. Leveraging Knative Eventing is a requirement.\\n","Decision":"### Kyma Event Bus\\nThe decisions is to provide a solution that abstracts the Knative Eventing concepts for the rest of Kyma as a short- or medium-term solution. In that way, the transition to Knative Eventing is transparent to the rest of the Kyma components.\\nThe architecture is as follows:\\n![Architecture](.\/assets\/event-bus.png)\\n### NATS Streaming\\nKnative provides an abstraction of **The Bus** which is a pluggable component of Knative Eventing that have different implementations backed up by a certain messaging store, for example Kafka Bus, or GCP PubSub Bus.\\nThe decisions are to:\\n1. Implement a production-ready Knative Bus `NATS Streaming`.\\n2. Provide `NATS Streaming` operator that provides `NATS Streaming` clusters which satisfy all production needs.\\n3. Implement Knative Bus `Azure Service Bus Messaging` as a plan B.\\n4. Contribute both implementations to `Knative Eventing`.\\n","tokens":170,"id":3457}
{"File Name":"abracadabra\/0004-use-recast-for-ast-manipulation.md","Context":"## Context\\nWe used Babel to parse code into AST, transform this AST and re-generate code.\\nWe went for Babel because it's popular, very active and is capable of parsing a lot of different syntaxes. Notably, it parses JSX, TS and TSX out of the box. That's great!\\nWhat is not great is the code generation part. Babel formats the generated code. That means the code contained inside a transformed node gets reformated. This is not cool.\\n","Decision":"As we want to preserve the original style of the transformed code as much as possible, we went for [Recast][recast].\\nAs the library says:\\n> The magic of Recast is that it reprints only those parts of the syntax tree that you modify.\\nThus, we now use Recast to parse and generate the code. AST transformation is still performed by Babel. Recast uses Babel to parse the code into AST, so we keep Babel benefits such as parsing JSX, TS\u00a0and TSX out of the box.\\n","tokens":104,"id":587}
{"File Name":"winery\/0023-use-maven-as-build-tool.md","Context":"## Context and Problem Statement\\nWhich build tool should be used?\\n","Decision":"Chosen option: \"Maven\", because\\n* None of Gradle's customizability and the overhead in setup that comes with that is required.\\n* The structure the comes with Maven makes the build files easier to understand compared to ANT.\\n","tokens":14,"id":4306}
{"File Name":"auth-account-koa\/0001_use_adr.md","Context":"## Context\\nA way to document and track my design choices is needed, for myself in order to see how project architecture is changing.\\n","Decision":"I will use ADR to document any important architectural decisions I make.\\nADRs will be checked into the repository as numbered md files in the folder docs\/architecture\/adr\\nI will follow the template described [here](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions)\\nAll Records will contain, Title, Context, Decision, Status and Consequences\\nIf a decision is changed or overruled we don't delete the record, but change the status accordingly (to superseded or deprecated).\\nIf a decision is superseded or deprecated we should add a link to the new decision. In the format Superseded by [link]\\n","tokens":28,"id":706}
{"File Name":"polarbears\/ADR-1.md","Context":"## Context\\nThere are several databases within the system. One per module.\\n","Decision":"Under no circumstance should a module read\/write from more than one database.\\n","tokens":16,"id":4649}
{"File Name":"ibc-rs\/adr-007-error.md","Context":"## Context\\nThis document describes the reason behind the switch from using\\n`anomaly` for error handling to\\nthe [`flex-error`](https:\/\/docs.rs\/flex-error\/) crate that is developed in-house.\\n","Decision":"### Problem Statement\\nTo keep things brief, we will look at the issue of error handling from a specific example\\nin `relayer\/src\/error.rs`:\\n```rust\\npub type Error = anomaly::Error<Kind>;\\n#[derive(thiserror::Error)]\\npub enum Kind {\\n#[error(\"GRPC error\")]\\nGrpc,\\n...\\n}\\nimpl Kind {\\npub fn context(self, source: impl Into<Box<dyn std::error::Error>>) -> anomaly::Context<Self> {\\nContext::new(self, Some(source.into()))\\n}\\n}\\n```\\nThe design above is meant to separate between two concerns:\\n- The metadata about an error, as captured in `Kind`.\\n- The trace of how the error occurred, as captured in `anomaly::Context`.\\n- The type `Error` is defined to be `anomaly::Error<Kind>`, which is a newtype wrapper to `Box<anomaly::Context<Kind>>`.\\nThere are a few issues with the original design using `anomaly`:\\n- The error source type is erased and turned into a `Box<dyn std::error::Error>`, making it difficult to recover metadata\\ninformation about the original error.\\n- The `Kind::context` method allows any error type to be used as an error source, making it difficult to statically\\nanalyze which sub-error has what kind of error source.\\nWe can demonstrate the design issue with a specific use case:\\n```rust\\npub fn unbonding_period(&self) -> Result<Duration, Error> {\\nlet mut client = self\\n.block_on(QueryClient::connect(self.grpc_addr.clone()))\\n.map_err(|e| Kind::Grpc.context(e))?;\\nlet request = Request::new(QueryParamsRequest {});\\nlet response = self\\n.block_on(client.params(request))\\n.map_err(|e| Kind::Grpc.context(e))?;\\n...\\n}\\n```\\nWithout the help of an IDE, it would be challenging to figure out that\\nthe first use of `Kind::Grpc.context` has `tonic::Status` as the error source\\ntype, while the second use has the error source type\\n`tonic::TransportError`.\\nThe mixing up of `tonic::Status` and `tonic::TransportError` as error sources\\nare not too critical in this specific case. However this would not be the\\ncase if we want to use the error source information to determine whether\\nan error is _recoverable_ or not. For instance, let's say if we want to\\nimplement custom retry logic only when the error source is\\n`std::io::Error`, there is not easy way to distinguished if an error\\nvariant `Kind::Grpc` is caused by `std::io::Error`.\\n### Proposed Design\\nA better design is to define error construction functions with _explicit_\\nerror sources. The proposed design is as follows:\\n```rust\\npub struct Error(pub ErrorDetail, pub eyre::Report);\\npub enum ErrorDetail {\\nGrpcStatus {\\nstatus: tonic::Status\\n},\\nGrpcTransport,\\n...\\n}\\nimpl Error {\\npub fn grpc_status(status: tonic::Status) -> Error {\\nlet detail = ErrorDetail::GrpcStatus { status };\\nError(detail, Eyre::msg(detail))\\n}\\npub fn grpc_transport(source: tonic::TransportError) -> Error {\\nlet detail = ErrorDetail::GrpcTransport;\\nlet trace = Eyre::new(source).wrap_err(detail);\\nError(detail, trace)\\n}\\n}\\n```\\nThere are a few things addressed by the design above:\\n- We use the `eyre::Report` type as an _error tracer_ to trace\\nthe error sources, together with additional information such as backtrace.\\n- Depending on the error source type, we want to have different strategies\\nto trace the error.\\n- For example, we may not care about the metadata\\ninside `tonic::TransportError`, so we just discard the data\\nafter tracing it using `eyre`.\\n- We define _error constructor functions_ that handle the error source using\\ndifferent strategies. The function constructs the `ErrorDetail` and\\n`eyre::Report` values, and then wrap them as the `Error` tuple.\\nIn general, when the error sources are defined by external libraries,\\nwe have little control of how the types are defined, and need to have\\ndifferent ways to handle them.\\nBut when we have multiple error types that are defined in the same crate,\\nwe want to have special way to handle the propagation of error.\\nFor example, consider the `LinkError` type, which has the error\\nwe defined earlier as the error source:\\n```rust\\nuse crate::error::{Error as RelayerError, ErrorDetail as RelayerErrorDetail};\\npub struct LinkError(LinkErrorDetail, eyre::Report);\\npub enum LinkErrorDetail {\\nRelayer {\\nsource: RelayerErrorDetail\\n},\\n...\\n}\\nimpl LinkError {\\npub fn relayer_error((source_detail, trace): RelayerError) -> LinkError {\\nlet detail = LinkErrorDetail::Relayer(source_detail);\\nLinkError(detail, trace.wrap_err(detail))\\n}\\n}\\n```\\nWe propagate the error detail to LinkErrorDetail so that we can recover\\nadditional detail later on. Furthermore, we extract the `eyre::Report`\\nfrom the error source and use it to add additional information\\nwhen we construct `LinkError`.\\n### `flex-error`\\nThe proposed design has a lot of boilerplate required to properly define\\nthe error types. To reduce boilerplate, we have developed\\n[`flex-error`](https:\/\/docs.rs\/flex-error\/) with the `define_error!`\\nmacro which makes it straightforward to implement the error types\\nusing a DSL syntax. With that, the error types can instead be defined as:\\n```rust\\nuse flex_error::{define_error, TraceError};\\ndefine_error! {\\nError {\\nGrpcStatus\\n{ status: GrpcStatus }\\n| e | { format!(\"GRPC call return error status {0}\", e.status) },\\nGrpcTransport\\n[ TraceError<tonic::TransportError> ]\\n| _ | { \"error in underlying transport when making GRPC call\" },\\n...\\n}\\n}\\n```\\nAside from the syntactic sugar provided by the `define_error!` macro, `flex-error`\\nalso allows error tracer implementation to be switched based on the Cargo feature\\nflags set on the `flex-error` crate. For example, we can switch from the\\n[`eyre`](https:\/\/docs.rs\/eyre\/) tracer to the [`anyhow`](https:\/\/docs.rs\/anyhow\/)\\ntracer by disabling `\"flex-error\/eyre_tracer\"` and enabling `\"flex-error\/anyhow_tracer\"` features.\\nIf all error tracer features and the `\"flex-error\/std\"` feature are disabled,\\na simple `flex_error::StringTracer` is used for tracing errors. The `StringTracer`\\ndo not provide additional information such as back trace, but it is useful\\nfor supporting `no_std`, where standard constructs such as `std::error::Error` and\\nerror backtrace are not available.\\nThe full documentation for `flex-error` is available at [Docs.rs](https:\/\/docs.rs\/flex-error\/).\\n","tokens":44,"id":4655}
{"File Name":"bananatabs\/0003-testing-model-in-dom-integration-tests.md","Context":"## Context\\nWe want to avoid testing implementation details in our integration tests.\\nWe want to use `react-testing-library` which makes it easier to make assertions on the rendered DOM rather than assert implementation details. But mostly because it enable us to find and trigger click events on different UI elements.\\ne.g. toggling the visibility of different tabs and window groups.\\nWhen it comes to asserting the rendered DOM, in most cases we trust the view will render the model properly.  It could be more sensible to only verify the state\/model.\\nBut, we had a small problem described in ADR-0002.\\n","Decision":"We will assert both against the session in the provider **and** against the DOM elements to make sure the application view is updating properly.\\n","tokens":126,"id":2814}
{"File Name":"Nosedive\/0008-migrating-by-code.md","Context":"## Context\\nWe are testing how deploy in diferente databases per envirtoment, and don't find the way of do it using the pluggin\\n","Decision":"Develop the migration part in the code\\n","tokens":31,"id":112}
{"File Name":"link_platform\/0009-use-openreferal-api.md","Context":"## Context\\n[Open Referral](https:\/\/openreferral.org\/) is non-profit who develops a public API spec for health, human and social services data. Part of their mission is to standardize these civic services data.\\nZendesk has historically partnered with Open Referral in an effort to support the spread of this standardization; we modeled our previous LinkSF data to resemble that spec.\\nIn this new project, we want to further support these efforts by exposing an open API which adheres as closely as possible to the Open Referral standard. Documentation on this standard can be found [here](https:\/\/openreferral.readthedocs.io\/en\/latest\/).\\n","Decision":"Adopt and extend the OpenReferal API and data structure.\\nWe will extend the model to accommodate data that is specific to the link platform system.\\n","tokens":136,"id":5030}
{"File Name":"support-rota\/0003-use-standard-rb.md","Context":"## Context\\nWe need to make sure our code is written in a standard style for clarity, consistency across a project and to avoid back and forth between developers about code style.\\n","Decision":"We will use [Standard.rb](https:\/\/github.com\/testdouble\/standard) and run the standard.rb rake task to lint the code as part of the test suite.\\n","tokens":36,"id":3386}
{"File Name":"architecture-decision-log\/0003-domain-driven-design.md","Context":"* [Context](#context)\\n* [Decision](#decision)\\n* [Status](#status)\\n* [Consequences](#consequences)\\n* [More reading](#more-reading)\\n","Decision":"* [Status](#status)\\n* [Consequences](#consequences)\\n* [More reading](#more-reading)\\nTo prevent these issues, we decided to implement the Design-driven Development framework to architect our domain collaboratively with the product team and business owners.\\nWith DDD, we can ensure proper refactors in the future, having a clear overview of our entities' relationships and domains. To learn more about DDD, [check this summary](https:\/\/medium.com\/@ruxijitianu\/summary-of-the-domain-driven-design-concepts-9dd1a6f90091).\\nWe can't have a clear, detailed overview of our entire domain structure (that would be overwhelming), but we've organized our DDD in a way that we can have a high-level concept of the whole company and drill-down to see a more detailed view of each bounded context.\\n**The high-level concept of our Domain-Driven Design** is located in this repository, at [ADR#0015](0015-model-overview.md). You can take a look at that record to learn more about our architecture overview.\\n**Bounded context detailed domains** are located in the same folder (`records\/domains`), but there is a single file for each bounded context. Those architectures focus only on the given bounded context and any entity that relates to it.\\nEach microservice we have are related to a single bounded context. So, to create new microservices, you must add a new bounded context to our domain architecture.\\n","tokens":45,"id":4915}
{"File Name":"runner\/0297-base64-masking-trailing-characters.md","Context":"## Context\\nThe Runner registers a number of Value Encoders, which mask various encodings of a provided secret. Currently, we register a 3 base64 Encoders:\\n- The base64 encoded secret\\n- The secret with the first character removed then base64 encoded\\n- The secret with the first two characters removed then base64 encoded\\nThis gives us good coverage across the board for secrets and secrets with a prefix (i.e. `base64($user:$pass)`).\\nHowever, we don't have great coverage for cases where the secret has a string appended to it before it is base64 encoded (i.e.: `base64($pass\\n))`).\\nMost notably we've seen this as a result of user error where a user accidentally appends a newline or space character before encoding their secret in base64.\\n","Decision":"### Trim end characters\\nWe are going to modify all existing base64 encoders to trim information before registering as a secret.\\nWe will trim:\\n- `=` from the end of all base64 strings. This is a padding character that contains no information.\\n- Based on the number of `=`'s at the end of a base64 string, a malicious user could predict the length of the original secret modulo 3.\\n- If a user saw `***==`, they would know the secret could be 1,4,7,10... characters.\\n- If a string contains `=` we will also trim the last non-padding character from the base64 secret.\\n- This character can change if a string is appended to the secret before the encoding.\\n### Register a fourth encoder\\nWe will also add back in the original base64 encoded secret encoder for four total encoders:\\n- The base64 encoded secret\\n- The base64 encoded secret trimmed\\n- The secret with the first character removed then base64 encoded and trimmed\\n- The secret with the first two characters removed then base64 encoded and trimmed\\nThis allows us to fully cover the most common scenario where a user base64 encodes their secret and expects the entire thing to be masked.\\nThis will result in us only revealing length or bit information when a prefix or suffix is added to a secret before encoding.\\n","tokens":170,"id":3730}
{"File Name":"caia\/0010-aleph-circrequests-denied-entries-resubmission.md","Context":"## Context\\nIn the original \"caia\" implementation, denied items were resubmitted as part\\nof every submission to CaiaSoft, as long as the item was included in the list\\nfrom Aleph.\\nAs denied items were typically being denied multiple times, this was causing\\nredundant entries to appear in the CaiaSoft interface. So it was decided not\\nresubmit denied items.\\nAfter further consideration, there was a concern that never resubmitting\\ndenied items could result in patron requests getting lost. So it was decided\\nthat denied items should be resubmitted after a sufficient interval (such as\\n7 days) as long as the denied item is still in the list provided by Aleph.\\n","Decision":"Modify the application so that items denied by CaiaSoft are resubmitted if\\nthey are still being reported by Aleph after a configurable \"wait interval\".\\n","tokens":154,"id":3513}
{"File Name":"infrastructure-adrs\/0001-migrate-off-fedora-3.md","Context":"## Context and Problem Statement <!-- required -->\\nFedora 3 is unsupported and has been unsupported for four years; it is unlikely to be supported in the foreseeable future. It also requires a difficult-to-support version of the Java Virtual Machine. And yet, Fedora 3 is the cornerstone of our management \"repository,\" in which all SDR content is managed and from which said content flows to access and preservation environments. At the same time, there is a dwindling number of organizations in the cultural heritage community who are still using Fedora 3.\\n## Decision Drivers <!-- optional -->\\n* Fedora 3 is unsupported and unlikely to be supported\\n* Fedora 3 will be harder to install on newer operating systems\\n* The Fedora 3 data model is not inherently validatable\\n* The Fedora 3 community is disappearing, so we are increasingly going it alone\\n* Fedora 3 is a critical piece of SDR infrastructure and represents an enormous risk\\n* Samvera software that supports Fedora 3 is outdated and maintained\/supported only through our own efforts, preventing us from using mainstream Samvera software\\n* We have (unverified) concerns about the scalability of Fedora 3\\n","Decision":"* Fedora 3 is unsupported and unlikely to be supported\\n* Fedora 3 will be harder to install on newer operating systems\\n* The Fedora 3 data model is not inherently validatable\\n* The Fedora 3 community is disappearing, so we are increasingly going it alone\\n* Fedora 3 is a critical piece of SDR infrastructure and represents an enormous risk\\n* Samvera software that supports Fedora 3 is outdated and maintained\/supported only through our own efforts, preventing us from using mainstream Samvera software\\n* We have (unverified) concerns about the scalability of Fedora 3\\nNo decision made yet. See status field above.\\n","tokens":241,"id":796}
{"File Name":"james\/0038-distributed-eventbus.md","Context":"## Context\\nRead [Event Bus ADR](0037-eventbus.md) for context.\\nGiven several James servers, we need them to share a common EventBus.\\nThis:\\n- Ensures a better load balancing for `group mailbox listners`.\\n- Is required for correctness of notifications (like IMAP IDLE).\\n","Decision":"Provide a distributed implementation of the EventBus leveraging RabbitMQ.\\nEvents are emitted to a single Exchange.\\nEach group will have a corresponding queue, bound to the main exchange, with a default routing key. Each eventBus\\nwill consume this queue and execute the relevant listener, ensuring at least once execution at the cluster level.\\nRetries are managed via a dedicated exchange for each group: as we need to count retries, the message headers need to\\nbe altered and we cannot rely on rabbitMQ build in retries. Each time the execution fails locally, a new event is emitted\\nvia the dedicated exchange, and the original event is acknowledged.\\nEach eventBus will have a dedicated exclusive queue, bound to the main exchange with the `registrationKeys` used by local\\nnotification mailboxListeners (to only receive the corresponding subset of events). Errors are not retried for\\nnotifications, failures are not persisted within `DeadLetter`, achieving at most once event delivery.\\n","tokens":69,"id":2125}
{"File Name":"rails-template\/0010-use-eslint.md","Context":"## Context\\nWe want to enforce consistency in our code, and catch as many errors\\nautomatically as we are able to. Linting the code is good practice to achieve\\nthese aims. [ESLint](https:\/\/eslint.org\/) is the standard linter for modern\\nJavaScript, and has good support for TypeScript and React though plugins.\\n","Decision":"We will check code style using ESLint.\\nWe will let Prettier have precedence when ESLint and Prettier conflict in their\\nstyles.\\nWe will use the recommended configuration for plugins where possible.\\nWe will run ESLint as part of the test suite.\\n","tokens":71,"id":4173}
{"File Name":"libelektra\/elektra_web_recursive.md","Context":"## Problem\\nAfter deciding how to remotely manage instances and groups of instances\\n(clusters) with Elektra Web, there is still the issue of recursively nested\\nclusters (clusters of clusters).\\n","Decision":"Managing the hierarchy in a single clusterd instance.\\n- [Elektra Web Structure decision](elektra_web.md)\\n","tokens":40,"id":1282}
{"File Name":"docs\/0006-ui-framework.md","Context":"## Context and Problem Statement\\nWe want to use a well established UI framework in order to bootstrap and boost our development process for state-of-the-art web applications.\\n","Decision":"We chosen option 1, Angular, since existing knowledge and experience with this framework is available.\\n","tokens":33,"id":4673}
{"File Name":"dapr\/ENG-002-Dapr-Release.md","Context":"## Context\\nThis record descibes how to safely release new dapr binaries and the corresponding configurations without any blockers to users.\\n","Decision":"### Integration build release\\nIntegration build refers to the build from `master` branch once we merge PullRequest to master branch. This build will be used for development purposes and must not be released to users and impact their environments.\\n### Official build release\\n#### Pre-release build\\nPre-release build will be built from `release-<major>.<minor>` branch and versioned by git version tag suffix e.g. `-alpha.0`, `-alpha.1`, etc. This build is not released to users who use the latest stable version.\\n**Pre-release process**\\n1. Create branch `release-<major>.<minor>` from master and push the branch. e.g. `release-0.1`\\n2. Add pre-release version tag(with suffix -alpha.0 e.g. v0.1.0-alpha.0) and push the tag\\n```\\n$ git tag \"v0.1.0-alpha.0\" -m \"v0.1.0-alpha.0\"\\n$ git push --tags\\n```\\n3. CI creates new build and push the images with only version tag\\n4. Test and validate the functionalities with the specific version\\n5. If there are regressions and bugs, fix them in release-* branch and merge back to master\\n6. Create new pre-release version tag(with suffix -alpha.1, -alpha.2, etc)\\n7. Repeat from 4 to 6 until all bugs are fixed\\n#### Release the stable version to users\\nOnce all bugs are fixed, we will create the release note under [.\/docs\/release_notes](https:\/\/github.com\/dapr\/dapr\/tree\/master\/docs\/release_notes) and run CI release manually in order to deliver the stable version to users.\\n### Release Patch version\\nWe will work on the existing `release-<major>.<minor>` branch to release patch version. Once all bugs are fixed, we will add new patch version tag, such as `v0.1.1-alpha.0`, and then release the build manually.\\n","tokens":27,"id":81}
{"File Name":"dlp-lux\/0004-Use-X-Forward-For-header.md","Context":"## Context\\nThe lux application needs to provide filtering of content via client IP address. Currently the only IP addresses the application ever sees is the Application Load Balancer's.\\n","Decision":"Enable the injection of the X-Forward-For header at the F5. This was accomplished by changing an existing setting on a HTTP profile. Additionally, the apache conf will be modified on all\\ndlp instances to conside the X-Forward-For header, if present, the client IP address. Additionally apache access logs will be modified to show the header as well as the true client IP.\\nThis will make the access and rails application logs' contain more relevant information instead of displaying the same load balancer IP addresses repeatedly.\\n","tokens":35,"id":3216}
{"File Name":"interlok\/0010-apply-config-reconnect-loop.md","Context":"## Context and Problem Statement\\nAs recorded in [INTERLOK-2875](https:\/\/adaptris.atlassian.net\/browse\/INTERLOK-2975) when Interlok is attempting to reconnect to an endpoint it becomes very difficult to amend configuration through the UI.\\nThe reconnect loop is dictated by the reconnection settings with each configured connection.  When a connection is initialized if it cannot connect to it's endpoint the reconnect loop begins.  Connections are initialized on startup and during runtime should it lose it's connection, Interlok will shift the connection through the lifecycle phases until it is initialised again.\\nTherefore we have two triggers for this issue;\\n- A connection cannot be established on startup\\n- A connection breaks during runtime.\\nUntil a connection has either established it's connection or has reached the maximum number of configured reconnect attempts, the connection will remain in the initialization phase.\\nEach phase of an Interlok component is dictated by the StateManagedComponent interface that defines the 4 basic stages of __init__, __start__, __stop__ and __close__.  When we __request__ a change of a components state (from __init__ to __start__ for example) we typically synchronize the state change until it is complete.\\nThis means for example when Interlok launches the initialization phase kicks in which will attempt to initialize each connection.  Any connection that cannot be established as mentioned above will stay in the synchronized __init__ phase potentially forever.  Because the phase transition is synchronized we cannot therefore change this state until it's current transition has completed.\\nThere are currently two problems applying configuration from the UI while a connection is attempting reconnection.\\nWhen applying config the UI will request a restart of the running instance, which will attempt to run through the lifecycle of phases mentioned above.\\nBut of course if a connection is stuck in it's synchronized initialization phase trying to reconnect all other state change requests must wait therefore the process hangs.\\nThe second problem when applying config is simply that the UI process will wait until the Interlok process has completed it's full startup, which of course it will not be able to do if a connection cannot be established.  Essentially the process hangs for a period of time.\\n","Decision":"### First issue\\nChosen option: Not yet decided.\\n#### Do Nothing\\nIn this case we accept that most people are not modifying production ready instances and even if they are they are probably not using the UI, therefore they are not going to run into the issues raised here.\\n#### Remove the synchronization\\nThere are a couple of ways to do this.\\nThe first would be to literally remove the synch blocks from implementations like this one;\\n```java\\npublic final void init() throws CoreException {\\nsynchronized (lock) {\\nif (!prepared) {\\nprepare();\\n}\\ninitConnection();\\nLifecycleHelper.init(connectionErrorHandler());\\n}\\n}\\n```\\nThe second way to do it would be to wrap the actual work inside the synch block with a new thread.\\n##### Consequences\\nAllowing a process to request a state change while in the middle of a previous state change could cause undefined behaviour especially around edge cases.\\n#### Interrupt the blocking transition\\nEssentially we would stop the transition currently running, in this case the reconnect inside the initialization phase transition.\\nThis can be done a couple of ways;\\nFirst we could make sure the thread handling the initialization transition is a named and managed thread from our ThreadManagerFactory and then add a new static method to that factory that would send an interrupt to that named thread.\\nOr considering the reconnect code could be refactored into the parent connection impl, we could cover all connections that have reconnect ability, which is all of those that extend AllowsRetriesConnection.  Once we make sure the reconnect code only exists in this parent class we then have a new volatile variable that gets set when a __close__ request is made the reconnect loop could check the value and exit if necessary.\\n### Second issue\\nChosen option: Not yet decided.\\n#### Do Nothing\\nIn this case we deem the current hanging behaviour as expected and correct.\\n#### Non-blocking applying of config\\nIn this case a change would be made to the UI application of config where it would request the full restart but not necessarily wait for the running instance to complete it's full start-up.  There would then either be some kind of notification once Interlok has completed the start-up or users would just be directed to the dashboard.\\n","tokens":448,"id":2346}
{"File Name":"front-end-monorepo\/adr-04.md","Context":"## Context\\nFairly early on in the rebuild of the Classifier, we started using newer technologies such as [CSS Grid](https:\/\/developer.mozilla.org\/en-US\/docs\/Web\/CSS\/CSS_Grid_Layout), which are not supported in older browsers like Internet Explorer 11.\\nAnd, while Edge is now several major versions in, we still have a percentage of users on IE11. As such, we're now at a point where we have to determine whether to drop support for legacy browsers and risk inconvenience for that segment, or invest significant time in coding and testing fallbacks.\\n","Decision":"We will only officially support the following browsers:\\n### Desktop\\n- Safari\\n- Chrome\\n- Firefox\\n- Edge\\n### Mobile\\n- Safari\\n- Chrome\\n- Opera\\nOf these, we will support the current and last two major versions.\\n","tokens":118,"id":514}
{"File Name":"git-en-boite\/0002-copy-source-code-and-tests-into-docker-container.md","Context":"## Context\\nPreviously the docker-compose.yaml file mapped a volume on the host to the `\/app` directory on the container. This is great for a local development workflow, but it won't work in production.\\n","Decision":"The decision I've made is to configure the Dockerfile to copy the source code (and tests) to the container. This means the container will have everything it needs to run the web server, and also to run tests.\\n","tokens":43,"id":3501}
{"File Name":"kafkarator\/0006-future-adrs-in-pig-repository.md","Context":"## Context\\nADRs are important documents detailing how we do things in our domain.\\nOften an ADR will point to a new feature or even an entirely new service to create.\\nWhen an ADR initiates the creation of a new service, where does the ADR belong?\\nIn the new repo, or in the repo where the idea originated?\\nWould it simply be better to collect ADRs in a central location?\\n","Decision":"When writing new ADRs, they will be in the [PIG repo](https:\/\/github.com\/navikt\/pig).\\n","tokens":90,"id":2832}
{"File Name":"terraform-aws-elasticsearch-module\/0002-do-not-specify-maximum-version-constraints-for-required-providers.md","Context":"## Context\\nTerraform allows to pin the spcific versions of providers required for this module.\\n","Decision":"This module will not enforce maximum versions contraints for all required providers\\n","tokens":22,"id":1903}
{"File Name":"contact-frontend\/0004-start-all-versions-assets-frontend-jenkins.md","Context":"## Context and Problem Statement\\nAs part of PLATUI-1393, browser based acceptance tests were added that rely on `assets-frontend`. These acceptance tests\\nrun as part of the build in Jenkins, which means `ASSETS_FRONTEND` needs to be added as a parameter to the `build-jobs`\\njob builder `.withServiceManager` call. However, `assets-frontend` has specific logic that means all versions are started\\nvia Service Manager, that is multiple instances of `assets-frontend` will start, not just the latest release.\\nThere is logic in Service Manager to pass an `application.conf` file to start only required versions of `assets-frontend`,\\nhowever as we are not using Service Manager to start `contact-frontend` itself, this logic does not get called. Therefore,\\nall verisons of `assets-frontend` start during when running a build, either after merge or as part of the PR builder for\\n`contact-frontend`.\\n## Decision Drivers\\n* `assets-frontend` is a deprecated service - we want to ensure the functionality using it does not break, but we are\\nreluctant to invest large amounts of time in it\\n* Service Manager is a Python application used heavily across the Platform and not owned by PlatUI, and we would like to\\navoid making changes to it if possible to support our workflow.\\n* Similarly the job builders in the `build-jobs` repo are heavily shared, and we would like to avoid making changes to\\n`build-jobs` if possible\\n","Decision":"* `assets-frontend` is a deprecated service - we want to ensure the functionality using it does not break, but we are\\nreluctant to invest large amounts of time in it\\n* Service Manager is a Python application used heavily across the Platform and not owned by PlatUI, and we would like to\\navoid making changes to it if possible to support our workflow.\\n* Similarly the job builders in the `build-jobs` repo are heavily shared, and we would like to avoid making changes to\\n`build-jobs` if possible\\nChosen option: \"Accept that builds of `contact-frontend` will be slower due to starting all versions of\\n`assets-frontend`\", because:\\n* It is a quick solution that does not involve further investigation to support a test case that we hope to deprecate\\n* The performance impact will slow down development in terms of PR building but will not affect local development\\n* This decision can be easily revisited if performance becomes a problem, i.e. by making no changes there will be no\\nrolling back needed if we remove the test or decide to change other services\\n### Positive Consequences\\n* Allows us to add browser-based acceptance tests for forms with `contact-frontend` relying on `assets-frontend`\\n* Does not involve further development time\\n* Proven to work in the build environment\\n### Negative Consequences\\n* Will definitely cause a negative impact on the time taken to build a release, or run a PR builder\\n","tokens":321,"id":1753}
{"File Name":"boxwise-flask\/graphql-ariadne-apollo.md","Context":"## Context\\nBoxwise is in the middle of a planned migration from the old PHP-based Dropapp, to a new app based on a new stack (Python\/React). In the old app, the backend and the data model were closely intertwined, with SQL strings written directly within PHP. This made it challenging to evolve the data model for the app, which in turn imposed many product functionality constraints and slowed development time - especially since the Dropapp data model was a prototype rather than designed for scalability and product-market fit. As the team migrates to the new app and explores the possibility of entering new markets, it is time for us to reexamine if and how the team might benefit from a separation of concerns on the data layer.\\n","Decision":"1. **Scalability:** how well will it support expected future changes such as DB restructuring, database migrations, etc.? What timescale is the technology expected to be defunct on?\\n2. **Developer experience:** given the rotating environment of loosely affiliated developers of different backgrounds, what will support the rapid onboarding of developers with our data structure? Once onboarded, is the chosen technology pleasant to use? Is it useful from a career progression standpoint?\\n3. **Maintainability:** we expect to have rapid changes to the DB structure as we expand functionality. How easy is our solution to maintain and evolve? What about documentation?\\n4. **Support and production-readiness:** Is the library mature enough to use in a production environment? Is there an active community or support channel should we run into problems?\\nGraphQL with single endpoint for everything was selected, paired with Ariadne server-side and Apollo client side.\\nReasoning: while GraphQL may have a steeper learning curve for professional developers who are not familiar with the standard, in the long run this should be more scalable for iterations and easier to maintain than  multiple REST endpoints. In the future, should we end up ingesting external data APIs such as the UNHCR data, it will be easier to pull that all from the GraphQL endpoint as well. This should also be more favorable from a developer experience standpoint for both onboarding and maintaining the codebase, due to GraphQL's introspective capabilities, human-readable JSON query structure, and degree of client-side specificity in requesting field-level data.\\nApollo was selected on the client-side due to its maturity as a product, robust features including sophisticated caching, excellent documentation, and huge community. Ariadne was selected over Graphene on the server side due to it being designed to deliberately intended to mimic Apollo Server. With Ariadne being under active development by Mirumee software, and its excellent documentation that developers can cross-reference with Apollo server documentation, I believe this outweighs any cons that come from it being a less mature library than Graphene.\\nFinally, I believe any performance concerns that could result from queries being abstracted from SQL into resolvers will be compensated for by less load on the network due to overfetching, as long as no N+1 queries are created.\\n","tokens":149,"id":3054}
{"File Name":"integration-framework-bundle\/001-amqp.md","Context":"## Context\\nThere was a need to consume the messages **in a non-blocking way** on our ESB. The former driver -PHP's native driver- didn't support such functionality.\\nThe former driver also ignored signals, due to its inability to acknowledge them **while waiting for a message**.\\nThe possible solutions were:\\n- To find a way to implement a non-blocking `consume()` function with the native driver.\\n- To use an AMQP library that offers support for such functionalities, like php-amqp-lib.\\n### Native Driver\\n*Pros*:\\n- Faster than any other library, due to it's a native extension written in C.\\n- Already implemented in the application.\\n*Cons*:\\n- Does not offer support to consume messages in a non-blocking way.\\n- Cannot listen to posix signals while consuming.\\n### PHP-AMQP-LIB\\n*Pros*:\\n- It's RabbitMQ's recommended library.\\n- Offers the possibility to consume messages in a non-blocking way.\\n- Signals can be dispatched to the worker even if it's consuming messages.\\n- Offers other options like: heartbeats, channels, and multiple hosts.\\n*Cons*:\\n- Difficult to implement correctly, without bypassing our own interfaces.\\n- Different approach demands more time to learn and implement it.\\n","Decision":"We decided to use the PHP-AMQP-LIB.\\nAn initial investigation was done to force the previous driver to listen to signals while consuming, but proved to be impossible. The `consume` function of PHP's native driver blocks the execution and nothing else can be done **until a message arrives to the consumer**.\\nWe decided that it was worth to adapt the library into the framework, which greatly impacted the structure of the consumers and queue drivers. Because these classes were designed with a sync protocol in mind (STOMP, in this case) they didn't fit the asynchronous nature of AMQP, prompting for a significant redesign **of all the interfaces and abstractions involved** - which ultimately drove us to break backwards compatibility and think in the 2.0 version of the framework.\\n","tokens":276,"id":4754}
{"File Name":"terraform-aws-iam-module\/0003-dual-support-for-terraform-version-0-11-and-0-12.md","Context":"## Context\\nTerraform version 0.12 release was a major change with the API. Given the worked required to upgrade, it is envisaged that Terraform 0.11 will remain for quite some time.\\n","Decision":"This module will support both version 0.11 and 0.12 of Terraform. Version 0.11 support will be managed from the 0.11 branch and tagged with a version pattern 0.minor.patch. Version 0.12 support will be managed from the master branch and tagged with a version pattern 1.minor.patch.\\n","tokens":46,"id":1628}
{"File Name":"reactive-interaction-gateway\/0002-don-t-check-for-functionclauseerror-in-tests.md","Context":"## Context\\nTypically, a module API filters possible inputs implicitly by making use of pattern matching. For instance, a GenServer that only handles `:test` messages might have a method similar to `handle_call(:test, _, _)`.\\nWriting tests that assure that pattern matching works with the given signature (e.g., by checking that `handle_call(:illegal, 0, 0)` fails) has drawbacks:\\n* Asserting that the process `exit`s with a `FunctionClauseError` is not straight-forward.\\n* Arguably, one of the ideas of pattern matching in function signatures is to save on testing negative cases in the first place.\\n* Often, testing for missing function clauses make tests needlessly brittle.\\nStill, there is the case of regression testing, e.g., making sure that there will never exist a handler for other messages. We think that such a restriction is rarely required, though.\\n","Decision":"Except for regression tests, tests should not aim at triggering `FunctionClauseError`s.\\n","tokens":191,"id":5229}
{"File Name":"linshare-mobile-android-app\/0013-destination-picker-s-flow-for-upload.md","Context":"## Context\\nUse-case:\\n```\\nAlice want to share a file to a workgroup\\nAlice select file and select destination to upload to\\n```\\nWe have many UI flows of destination picker in Uploading case.\\n","Decision":"The accepted flows are implemented:\\n![destination_picker](.\/images\/destination_picker_in_uploading.png)\\n","tokens":48,"id":1648}
{"File Name":"cosmos-sdk\/adr-013-metrics.md","Context":"## Context\\nTelemetry is paramount into debugging and understanding what the application is doing and how it is\\nperforming. We aim to expose metrics from modules and other core parts of the Cosmos SDK.\\nIn addition, we should aim to support multiple configurable sinks that an operator may choose from.\\nBy default, when telemetry is enabled, the application should track and expose metrics that are\\nstored in-memory. The operator may choose to enable additional sinks, where we support only\\n[Prometheus](https:\/\/prometheus.io\/) for now, as it's battle-tested, simple to setup, open source,\\nand is rich with ecosystem tooling.\\nWe must also aim to integrate metrics into the Cosmos SDK in the most seamless way possible such that\\nmetrics may be added or removed at will and without much friction. To do this, we will use the\\n[go-metrics](https:\/\/github.com\/hashicorp\/go-metrics) library.\\nFinally, operators may enable telemetry along with specific configuration options. If enabled, metrics\\nwill be exposed via `\/metrics?format={text|prometheus}` via the API server.\\n","Decision":"We will add an additional configuration block to `app.toml` that defines telemetry settings:\\n```toml\\n###############################################################################\\n###                         Telemetry Configuration                         ###\\n###############################################################################\\n[telemetry]\\n# Prefixed with keys to separate services\\nservice-name = {{ .Telemetry.ServiceName }}\\n# Enabled enables the application telemetry functionality. When enabled,\\n# an in-memory sink is also enabled by default. Operators may also enabled\\n# other sinks such as Prometheus.\\nenabled = {{ .Telemetry.Enabled }}\\n# Enable prefixing gauge values with hostname\\nenable-hostname = {{ .Telemetry.EnableHostname }}\\n# Enable adding hostname to labels\\nenable-hostname-label = {{ .Telemetry.EnableHostnameLabel }}\\n# Enable adding service to labels\\nenable-service-label = {{ .Telemetry.EnableServiceLabel }}\\n# PrometheusRetentionTime, when positive, enables a Prometheus metrics sink.\\nprometheus-retention-time = {{ .Telemetry.PrometheusRetentionTime }}\\n```\\nThe given configuration allows for two sinks -- in-memory and Prometheus. We create a `Metrics`\\ntype that performs all the bootstrapping for the operator, so capturing metrics becomes seamless.\\n```go\\n\/\/ Metrics defines a wrapper around application telemetry functionality. It allows\\n\/\/ metrics to be gathered at any point in time. When creating a Metrics object,\\n\/\/ internally, a global metrics is registered with a set of sinks as configured\\n\/\/ by the operator. In addition to the sinks, when a process gets a SIGUSR1, a\\n\/\/ dump of formatted recent metrics will be sent to STDERR.\\ntype Metrics struct {\\nmemSink           *metrics.InmemSink\\nprometheusEnabled bool\\n}\\n\/\/ Gather collects all registered metrics and returns a GatherResponse where the\\n\/\/ metrics are encoded depending on the type. Metrics are either encoded via\\n\/\/ Prometheus or JSON if in-memory.\\nfunc (m *Metrics) Gather(format string) (GatherResponse, error) {\\nswitch format {\\ncase FormatPrometheus:\\nreturn m.gatherPrometheus()\\ncase FormatText:\\nreturn m.gatherGeneric()\\ncase FormatDefault:\\nreturn m.gatherGeneric()\\ndefault:\\nreturn GatherResponse{}, fmt.Errorf(\"unsupported metrics format: %s\", format)\\n}\\n}\\n```\\nIn addition, `Metrics` allows us to gather the current set of metrics at any given point in time. An\\noperator may also choose to send a signal, SIGUSR1, to dump and print formatted metrics to STDERR.\\nDuring an application's bootstrapping and construction phase, if `Telemetry.Enabled` is `true`, the\\nAPI server will create an instance of a reference to `Metrics` object and will register a metrics\\nhandler accordingly.\\n```go\\nfunc (s *Server) Start(cfg config.Config) error {\\n\/\/ ...\\nif cfg.Telemetry.Enabled {\\nm, err := telemetry.New(cfg.Telemetry)\\nif err != nil {\\nreturn err\\n}\\ns.metrics = m\\ns.registerMetrics()\\n}\\n\/\/ ...\\n}\\nfunc (s *Server) registerMetrics() {\\nmetricsHandler := func(w http.ResponseWriter, r *http.Request) {\\nformat := strings.TrimSpace(r.FormValue(\"format\"))\\ngr, err := s.metrics.Gather(format)\\nif err != nil {\\nrest.WriteErrorResponse(w, http.StatusBadRequest, fmt.Sprintf(\"failed to gather metrics: %s\", err))\\nreturn\\n}\\nw.Header().Set(\"Content-Type\", gr.ContentType)\\n_, _ = w.Write(gr.Metrics)\\n}\\ns.Router.HandleFunc(\"\/metrics\", metricsHandler).Methods(\"GET\")\\n}\\n```\\nApplication developers may track counters, gauges, summaries, and key\/value metrics. There is no\\nadditional lifting required by modules to leverage profiling metrics. To do so, it's as simple as:\\n```go\\nfunc (k BaseKeeper) MintCoins(ctx sdk.Context, moduleName string, amt sdk.Coins) error {\\ndefer metrics.MeasureSince(time.Now(), \"MintCoins\")\\n\/\/ ...\\n}\\n```\\n","tokens":230,"id":815}
{"File Name":"stac-ml-aoi-extension\/0002-use-case-definition.md","Context":"## Context\\nWe define the initial use case for `ml-aoi` spec that exposes assumptions and reasoning for specific layout choices:\\nproviding training data source for Raster Vision model training process.\\n`ml-aoi` STAC Items represent a reified relation between feature rasters and ground-truth label in a machine learning training dataset.\\nEach `ml-aoi` Item roughly correspond to a \"scene\" or a training example.\\n### Justification for new extension\\nCurrent known STAC extensions are not suitable for this purpose. The closest match is the STAC `label` extension.\\n`label` extension provides a way to define either vector or raster labels over area.\\nHowever, it does not provide a mechanism to link those labels with feature images;\\nlinks with `rel` type `source` point to imagery from which labels were derived.\\nSometimes this imagery will be used as feature input for model training, but not always.\\nThe concept of source label imagery and input feature imagery are semantically distinct.\\nFor instance it is possible to apply a single source of ground-truth building labels to train a model on either Landsat or Sentinel-2 scenes.\\n### Catalog Lifetime\\n`ml-aoi` Item links to both raster STAC item and label STAC item.\\nIn this relationship the source raster and label items are static and long lived, being used by several `ml-aoi` catalogs.\\nBy contrast `ml-aoi` catalog is somewhat ephemeral, it captures the training set in order to provide model reproducibility and provenance.\\nThere can be any number of `ml-aoi` catalogs linking to the same raster and label items, while varying selection, training\/testing\/validation split and class configuration.\\n","Decision":"We will adopt the use and development of `ml-aoi` extension in future machine-learning projects.\\n","tokens":365,"id":2284}
{"File Name":"adrflow\/6-Centralize_Definition_of_Filename.md","Context":"## Context\\nWith the evolution of the tool, several places in the code need to somehow relate to how the format\/template of the ADR file name.\\nThis immediately means that a change to the template has to take into account these different places.\\nAn example for a feature that suffers from this is customizing the ADR file name template, e.g. in [issue #17](https:\/\/github.com\/slior\/adrflow\/issues\/17).\\nCurrently identified places that refer to the file name:\\n1. The `new` command (`new.js`)\\n2. The `adr_util.js` file, in several functions.\\n3. The `adr_util_sync.js` file.\\n","Decision":"We will centralize the definition of the ADR file name template, so it can be easily changed and configured.\\nGiven that we want shared code to be more in `adr_util_sync` we will probably centralize the definition there.\\n","tokens":145,"id":3638}
{"File Name":"opg-use-an-lpa\/0019-will-use-symfony-translation-components.md","Context":"## Context\\nWe need to provide translation capabilities in a way that slots into our twig based rendering pipeline\\n","Decision":"We're going to use symfony\/translation to provide the translation capabilities\\n","tokens":21,"id":4845}
{"File Name":"holdings-backend\/0005-concurrency.md","Context":"## Context\\nLoading holdings data to MongoDB is largely CPU bound. For optimal performance\\nwhen loading data, we want to have multiple processes or threads loading data\\nsimultaneously.\\nBecause of the possibility for clusters to split or merge while loading data on\\nHathiTrust items or concordance data, loading data in parallel suffers from a\\nnumber of concurrency hazards. In particular, it is possible for one process to\\nattempt to update a document which has been deleted by another process or which\\nis about to be deleted and reclustered by another process.\\nMongoDB does not support proactive locking of individual documents, but it does\\nsupport transactions which can detect conflicts between updates to documents\\ninvolved in multiple simultaneous transactions.\\n","Decision":"* Wrap any operation that suffers from potential write conflicts in\\nretryable transactions.\\n* Ensure that update operations modify the expected number of documents; retry\\noperations that do not.\\n* Test handling of concurrency hazards.\\n","tokens":149,"id":4202}
{"File Name":"radiant-mlhub\/0002-resolving-api-keys.md","Context":"## Context\\nWe need a convenient system for managing API keys used by the Python client. This system should give the user multiple options for\\nproviding an API key to be used when making a request to the API. These options should include:\\n* Storing API keys on the users system\\n* Reading an API key from the environment\\n* Passing an API key directly to the API request methods\\nUsers may have multiple valid API keys associated with their account at any given time. The system for storing API keys on the user's\\nsystem must accommodate this and provide a clear, deterministic way of resolving an API key for a given project.\\nWe anticipate the need to store other data related to Radiant MLHub for uses unrelated to authentication. For instance, we may have a need to\\ntrack the progress of downloads so that they can be resumed if interrupted, or we may want to specify a base URL in a config file so that\\ndevelopers can test against the staging environment. The method that we choose for storing API keys on the user's system must not preclude\\nus from storing this additional information.\\n","Decision":"The Python client will resolve the API key to be used in a request in the following order:\\n1) Passing an `api_key` argument directly to the method\\n2) Setting an `MLHUB_API_KEY` environment variable\\n3) Passing a `profile` argument directly to the method. This will read the API key from the given profile (see below for details)\\n4) Setting an `MLHUB_PROFILE` environment variable. This will read the API key from the given profile (see below for details)\\n5) Using the API from the `default` profile\\nProfiles will be stored in a `.mlhub\/profiles` file in the user's home directory. This file will be an INI file containing at least a\\n`[default]` section with an `api_key` value. The file may contain other sections corresponding to named profiles. Any `profile` argument\\npassed to a method must correspond to one of these section names, or it will raise an exception.\\n","tokens":228,"id":702}
{"File Name":"kosmos\/0001-app-db.md","Context":"# Context\\nI'm currently working on migrating from redux store to re-frame. It's a good point to define data\\nstructure and API to access it.\\n# Decision\\nApplication DB be a hashmap with the following keys:\\n- `nodes` to store hashmap where the key is the node's id and value is a node.\\n- `root` to store a root node id\\n- `loc` to store focused node id\\nNode should be represented as a hashmap with `id`, `type`, `value`, `children`, and `parent` keys.\\nApplication should define the following event handlers to modify nodes:\\n- `[:nodes\/make node]` create a node at given loc.\\n- `[:nodes\/append-child child]` adds a child as a bottom child.\\n- `[:nodes\/insert-child child]` adds a child as a top child.\\n- `[:nodes\/insert-up child]` adds a sibling to up of loc.\\n- `[:nodes\/insert-down child]` adds a sibling to down of loc.\\n- `[:nodes\/remove]` removes node at loc; lot will be moved to the preceding node.\\n- `[:nodes\/replace node]` replaces node at loc.\\nFollowing event handler to modify loc:\\n- `[:loc\/root]` moves loc to root.\\n- `[:loc\/down]` moves loc down.\\n- `[:loc\/up]` moves loc up.\\n- `[:loc\/bottom]` moves loc to bottom.\\n- `[:loc\/top]` moves loc to top.\\n- `[:loc\/right]` moves loc right.\\n- `[:loc\/left]` moves loc left.\\nFollowing event handler to modify root:\\n- `[:root\/change root]` change root to a new value.\\n","Decision":"Application DB be a hashmap with the following keys:\\n- `nodes` to store hashmap where the key is the node's id and value is a node.\\n- `root` to store a root node id\\n- `loc` to store focused node id\\nNode should be represented as a hashmap with `id`, `type`, `value`, `children`, and `parent` keys.\\nApplication should define the following event handlers to modify nodes:\\n- `[:nodes\/make node]` create a node at given loc.\\n- `[:nodes\/append-child child]` adds a child as a bottom child.\\n- `[:nodes\/insert-child child]` adds a child as a top child.\\n- `[:nodes\/insert-up child]` adds a sibling to up of loc.\\n- `[:nodes\/insert-down child]` adds a sibling to down of loc.\\n- `[:nodes\/remove]` removes node at loc; lot will be moved to the preceding node.\\n- `[:nodes\/replace node]` replaces node at loc.\\nFollowing event handler to modify loc:\\n- `[:loc\/root]` moves loc to root.\\n- `[:loc\/down]` moves loc down.\\n- `[:loc\/up]` moves loc up.\\n- `[:loc\/bottom]` moves loc to bottom.\\n- `[:loc\/top]` moves loc to top.\\n- `[:loc\/right]` moves loc right.\\n- `[:loc\/left]` moves loc left.\\nFollowing event handler to modify root:\\n- `[:root\/change root]` change root to a new value.\\n","tokens":379,"id":126}
{"File Name":"js-sdk\/0020-dynamically-get-branch-for-threebot-deployer.md","Context":"## Context\\nIn threebot deployer we hardcode the branch we deploy from in the code, this will be annoying if we need to deploy from another branch.\\n","Decision":"Getting the branch dynamically from the active branch of js-sdk repository\\n","tokens":34,"id":5196}
{"File Name":"winery\/0025-use-same-logback-test-xml-for-each-sub-project.md","Context":"## Context and Problem Statement\\nWhen executing tests, logback is loaded as logging framework.\\nLogback in testing mode is configured using `logback-test.xml`.\\nIn case logback is not configured, it does not output anything.\\n## Decision Drivers\\n- Ease of use for developers\\n","Decision":"- Ease of use for developers\\nChosen option: \"Use same `logback-test.xml` for each sub project\", because\\n- During development, the \"local\" `logback-test.xml` file can be adjusted to the needs\\n- During continuous integration, the output should contain warnings and errors only and not any debug information\\n### Positive Consequences\\n- When modifying `logback-test.xml`, a developer just has to copy it over to the other sub projects without thinking which change to propagate to which sub project.\\n","tokens":61,"id":4309}
{"File Name":"Shelter\/0002-using-proptypes-over-typescript.md","Context":"## Context\\nThe use of TypeScript would in turn provide a more stable codebase.\\nFor this particular project since the vast majority of components will receive either strings, objects or arrays of strings as props and have no state due to the fact they just render externally provided content.\\nWe feel that in this use-case adding the complexity, additional dependencies, increased on-boarding and higher risk of incompatibilities the benefits do not out weigh the overhead, with PropTypes being sufficient given the static nature of the website.\\nTypeScript would increase the complexity of the codebase and the time needed to on-board a developer.\\n","Decision":"We will use PropTypes\\n","tokens":124,"id":4429}
{"File Name":"tendermint-rs\/adr-003-light-client-core-verification.md","Context":"## Context\\nThe high level context for the light client is described in\\n[ADR-002](adr-002-light-client-adr-index.md).\\nFor reference, a schematic of the light node is below:\\n![Light Node Diagram](assets\/light-node.png).\\nHere we focus on the core verification library, which is reflected in the\\ndiagram as the \"Light Client Verifier\" and \"Bisector\".\\nThe light node is subjectively initialized, and then attempts to sync to a most\\nrecent height by skipping straight to it. The verifier is the core logic\\nto check if a header can be trusted and if we can skip to it.\\nIf the validator set has changed too much, the header can't be trusted,\\nand we'll have to request intermediate headers as necessary to sync through the validator set changes.\\nNote the verifier should also work for IBC, though in IBC bisection can't be performed directly\\nsince blockchains can't make HTTP requests. There, bisection is expected to be done by an external relayer\\nprocess that can make requests to full nodes and submit the results to an IBC enabled chain.\\nThe library should lend itself smoothly to both use cases and should be difficult to use incorrectly.\\nThe core verification operates primarily on data types that are already implemented\\nin tendermint-rs (headers, public keys, signatures, votes, etc.). The crux of it\\nis verifying validator sets by computing their merkle root, and verifying\\ncommits by checking validator signatures. The particular data structures used by\\nTendermint have considerably more features\/functionality than needed for this,\\nhence the core verification library should abstract over it.\\nHere we describe the core verification library, including:\\n- traits\\n- public functions for IBC and light nodes\\n- implementations of traits for the existing Tendermint data-structures\\n","Decision":"The implementation of the core verification involves two components:\\n- a new `light` crate, containing traits and functions defining the core verification\\nlogic\\n- a `light-impl` module within the `tendermint` crate, containing implementations of\\nthe traits for the tendermint specific data structures\\nThe `light` crate should have minimal dependencies and not depend on\\n`tendermint`. This way it will be kept clean, easy to test, and easily used for\\nvariations on tendermint with different header and vote data types.\\nThe light crate exposes traits for the block header, validator set, and commit\\nwith the minimum information necessary to support general light client logic.\\nAccording to the specification, the key functionality the light client\\nverification requires is to determine the set of signers in a commit, and\\nto determine the voting power of those signers in another validator set.\\nHence we can abstract over the lower-level detail of how signatures are\\nvalidated.\\n### Header\\nA Header must contain a height, time, and the hashes of the current and next validator sets.\\nIt can be uniquely identified by its hash:\\n```rust\\npub trait Header {\\nfn height(&self) -> Height;\\nfn bft_time(&self) -> Time;\\nfn validators_hash(&self) -> Hash;\\nfn next_validators_hash(&self) -> Hash;\\nfn hash(&self) -> Hash;\\n}\\n```\\n### Commit\\nA commit in the blockchain contains the underlying signatures from validators, typically in\\nthe form of votes. From the perspective of the light client,\\nwe don't care about the signatures themselves.\\nWe're only interested in knowing the total voting power from a given validator\\nset that signed for the given block. Hence we can abstract over underlying votes and\\nsignatures and just expose a `voting_power_in`, as per the spec:\\n```rust\\npub trait Commit {\\ntype ValidatorSet: ValidatorSet;\\nfn header_hash(&self) -> Hash;\\nfn validate(&self, &Self::ValidatorSet) -> Result<(), Error>;\\nfn voting_power_in(&self, vals: &Self::ValidatorSet) -> Result<u64, Error>;\\n}\\n```\\nThe `header_hash` is the header being committed to,\\nand `validate` performs Commit-structure-specific validations,\\nfor instance checking the number of votes is correct, or that they are\\nall for the right block ID.\\nComputing `voting_power_in` require access to a validator set\\nto get the public keys to verify signatures. But this specific relationship\\nbetween validators and commit structure isn't the business of the `light` crate;\\nit's rather how a kind of Tendermint Commit structure implements the `light` traits.\\nBy making ValidatorSet an associated type of Commit, the light crate doesn't\\nhave to know much about validators or how they relate to commits, so the ValidatorSet\\ntrait can be much smaller, and we never even define the concept of an individual\\nvalidator.\\nNote this means `Commit` is expected to have access to the ids of the validators that\\nsigned, which can then be used to look them up in their ValidatorSet implementation.\\nThis is presently true, since Commit's contain the validator addresses\\nalong with the signatures. But if the addresses were removed, for instance to\\nsave space, the Commit trait would need access to the relevant validator set to\\nget the Ids, and this validator set may be different than the one being passed\\nin `voting_power_in`.\\nBy abstracting over the underlying vote type, this trait can support\\noptimizations like batch verification of signatures, and the use of\\naggregate signatures instead of individual votes.\\nSo long as it can be determined what voting power of a given validator set\\nsigned correctly for the commit.\\nThe method `voting_power_in` performs the underlying signature verifications.\\nIt should return an error if any of them fail or are for the wrong header hash.\\nNote the specification introduces a `signers(commit) -> validators` method that\\nreturns the list of validators that signed a commit. However, such a method\\nwould require access to the underlying validator set in order to verify the\\ncommits, and it is only ever used in computing `voting_power_in`. Hence, we\\ndispense with it here in favour of a `voting_power_in` that operates on a\\n`Commit` and `ValidatorSet`. However, this also means that ValidatorSet will\\nneed to expose facilities for determining whether a validator signed correctly in\\norder for implementations to make use of it to compute `voting_power_in`.\\nNote also that in Tendermint, commits are for a particular block ID, which\\nincludes both a header hash and a \"parts set hash\". The latter is completely\\nirrelevant to the light client, and can only be verified by downloading the full\\nblock. Hence it is effectively ignored here. It would be great if Tendermint\\ncould disentangle commits to the proposal block parts for gossip (ie. the parts\\nset hash) from commits to the header itself (ie. the header hash), but that's\\nleft for the future.\\nFor more background on implementation of Tendermint commits and votes, see:\\n- [ADR-025](https:\/\/github.com\/tendermint\/tendermint\/blob\/main\/docs\/architecture\/adr-025-commit.md)\\n- [Validator Signing Spec](https:\/\/github.com\/tendermint\/tendermint\/blob\/main\/docs\/spec\/consensus\/signing.md)\\n- [Tendermint consensus specification](https:\/\/arxiv.org\/abs\/1807.04938)\\n### Validator Set\\nA validator set has a unique hash which must match what's in the header.\\nIt also has a total power used for determining if the result of `voting_power_in` is greater\\nthan a fraction of the total power.\\nValidatorSet is implemented as an associated type of Commit, where it's\\nnecessary to compute `voting_power_in`, so the underlying implementation must\\nhave some way to determine the voting power of the validators that signed,\\nsince voting power is not found in the commit itself.\\nNote we don't need to define individual validators since all the details of how validators relates to commits\\nis encapsulated in the Commit.\\n```rust\\npub trait ValidatorSet {\\nfn hash(&self) -> Hash;\\nfn total_power(&self) -> u64;\\n}\\n```\\n### State\\nAccording to the spec, the light client is expected to have a store that it can\\npersist trusted headers and validators to. This is necessary to fetch the last trusted\\nvalidators to be used in verifying a new header, but it's also needed in case\\nany conflicting commits are discovered and they need to be published to the\\nblockchain. That said, it's not needed for the core verification, so we don't\\ninclude it here. Users of the `light` crate like a light node decide how to\\nmanage the state. We do include some convenience structs:\\n```rust\\npub struct SignedHeader<C, H>\\nwhere\\nC: Commit,\\nH: Header,\\n{\\ncommit: C,\\nheader: H,\\n}\\npub struct TrustedState<C, H>{\\nwhere\\nC: Commit,\\nH: Header,\\n{\\nlast_header: SignedHeader<C, H>,\\nvalidators: C::ValidatorSet,\\n}\\n```\\nHere the trusted state combines both the signed header and the validator set,\\nready to be persisted to some external store.\\n### TrustThreshold\\nThe amount of validator set change that occur when skipping to a higher height depends on the\\ntrust threshold, as per the spec. Here we define it as a trait that encapsulated the math of what percent\\nof validators need to sign:\\n```rust\\npub trait TrustThreshold: Copy + Clone {\\nfn is_enough_power(&self, signed_voting_power: u64, total_voting_power: u64) -> bool;\\n}\\n```\\nWe provide a conenvient implementation that takes a numerator and a denominator. The default is of course 1\/3.\\n### Requester\\nThe light node needs to make requests to full nodes during bisection for intermediate signed headers and validator sets:\\n```rust\\npub trait Requester<C, H>\\nwhere\\nC: Commit,\\nH: Header,\\n{\\nfn signed_header(&self, h: Height) -> Result<SignedHeader<C, H>, Error>;\\nfn validator_set(&self, h: Height) -> Result<C::ValidatorSet, Error>;\\n}\\n```\\nIn practice, this can be implemented as a Tendermint RPC client making requests\\nto the `\/commit` and `\/validators` endpoints of full nodes.\\nFor testing, the Requester can be implemented by JSON files.\\n### Verification\\nBoth IBC and full node syncing have to perform a common set of checks:\\n- validate the hashes\\n- if the header is sequential, validate the next validator set\\n- if the header is not sequential, check if the trust threshold is reached\\n- this uses `voting_power_in` with a validator set that may be different from\\nthe one that actually created the commit.\\n- check that +2\/3 of the validators signed\\n- this uses `voting_power_in` with the actual validator set\\nThese are implemented in a common function, `verify_single_inner`:\\n```rust\\nfn verify_single_inner<H, C, L>(\\ntrusted_state: &TrustedState<C, H>,\\nuntrusted_sh: &SignedHeader<C, H>,\\nuntrusted_vals: &C::ValidatorSet,\\nuntrusted_next_vals: &C::ValidatorSet,\\ntrust_threshold: L,\\n) -> Result<(), Error>\\n```\\nNote however that light client security model is highly sensitive to time, so the public functions\\nexposed for IBC and bisection, which will call `verify_single_inner`, must take a current time\\nand check we haven't expired.\\nFor IBC, since it can't make its own requests, the public function just takes the untrusted state\\nin full, and return it as a TrustedState if it verifies:\\n```rust\\npub fn verify_single<H, C, T>(\\ntrusted_state: TrustedState<C, H>,\\nuntrusted_sh: &SignedHeader<C, H>,\\nuntrusted_vals: &C::ValidatorSet,\\nuntrusted_next_vals: &C::ValidatorSet,\\ntrust_threshold: T,\\ntrusting_period: &Duration,\\nnow: &SystemTime,\\n) -> Result<TrustedState<C, H>, Error>\\n```\\nFor the light node, we pass in a Requester, and specify a height we want to sync to.\\nIt will fetch that header and try to verify it using the skipping method,\\nand will run a bisection algorithm to recursively request headers of lower height\\nas needed. It returns a list of headers it verified along the way:\\n```rust\\npub fn verify_bisection<C, H, L, R>(\\ntrusted_state: TrustedState<C, H>,\\nuntrusted_height: Height,\\ntrust_threshold: L,\\ntrusting_period: &Duration,\\nnow: &SystemTime,\\nreq: &R,\\n) -> Result<Vec<TrustedState<C, H>>, Error>\\n```\\n### Implementing Traits\\nThe core `light` traits can be implemented by the Tendermint data structures and\\ntheir variations. For instance, v0.33 of Tendermint Core introduced a breaking\\nchange to the Commit structure to make it much smaller. We can implement the\\n`light` traits for both versions of the Commit structure.\\nThe `light` abstractions also facilitate testing, as complete Tendermint data structures\\nare not required to test the light client logic, only\\nthe elements it cares about. This means we can implement mock commits and validators,\\nwhere validators are just numbered 0,1,2... and both commits and validators are\\nsimply lists of integers representing the validators that signed or are in the\\nvalidator set. This aligns closely with how these structures are represented in\\nthe [TLA+\\nspec](https:\/\/github.com\/interchainio\/verification\/blob\/develop\/spec\/light-client\/Blockchain.tla).\\nWhile this provides a lot of flexibility in mocking out\\nthe types, we must be careful to ensure they match the semantics of the actual\\nTendermint types, and that we still test the verification logic sufficiently for\\nthe actual types.\\n### Other Validation\\nSome fields in the header are left explicitly unvalidated as they have minimal bearing on the correctness of the light client.\\nThese include:\\n- LastCommitHash\\n- In the skipping case, it's not possible to verify the header refers to the correct previous block without reverting to the sequential case. So in the sequential case, we don't validate this either. If it's incorrect, in indicates the validators are misbehaving, though we can only detect it as a light client if there's a fork.\\n- BlockID\\n- As mentioned, this includes a merkle root of the entire block and is not verifiable without downloading the whole block, which would defeat the purpose of the light client!\\n- Time\\n- Verifying the time would require us to download the commit for the previous block, and to take the median of the timestamps from that commit. This would add significant overhead to the light client (an extra commit to validate for every block!). If the time is incorrect, in indicates that the validators are explicitly violating the protocol rules in a detectable way which full nodes should detect in the first place and shouldn't forward to light clients, so there would probably be bigger issues at foot.\\nThere are likely a few other instances of things the light client is not validating that it in theory could but likely indicate some larger problem afoot that the client can't do anything about anyways. Hence we really only focus on the correctness of commits and validator sets and detecting forks!\\n","tokens":386,"id":4317}
{"File Name":"front-end-monorepo\/adr-36.md","Context":"## Context\\nWhen we launched the NextJS apps, a single domain could only host one NextJS app. We gave each app its own subdomain: `fe-content-pages.zooniverse.org` and `fe-projects.zooniverse.org` then proxied URLs from `www.zooniverse.org` to those domains. Next data requests, on the `\/_next` URL, are proxied from `www.zooniverse.org\/_next` to `fe-project.zooniverse.org\/_next`. We deliberately broke the content pages app, in favour of supporting projects.\\n[Next 9.5](https:\/\/nextjs.org\/blog\/next-9-5), in July 2020, added support for multiple apps running on the same domain via the [`basePath`](https:\/\/nextjs.org\/blog\/next-9-5#customizable-base-path) config setting.\\n","Decision":"Set base paths of `\/about` and `\/projects` for the content pages app and project app respectively.\\nhttps:\/\/github.com\/zooniverse\/front-end-monorepo\/pull\/2519\\n","tokens":180,"id":506}
{"File Name":"life-dashboard\/20191017 Language.md","Context":"## Context\\nShould be a new language to me for learning.\\nShould be an appropriate one for native desktop apps (if it works cross platform that's a bonus!)\\nShould be one that improves future job options \/ pay ()\\n","Decision":"Python. The size of the community (it's _crazy_ popular! That pretty much wins it alone), it works with GIS (Chrissy), ml\/ai potential. May not be as (possibly) future focused with the whole function thing, but that'll likely be decades away and it's still doable with JS and python (I think).\\n","tokens":49,"id":2784}
{"File Name":"buy-for-your-school\/0021-use-msgraph-for-email-integration.md","Context":"## Context\\n* Procurement Operations team uses a shared mailbox hosted in DfE's Exchange Online to communicate with School Buying Professionals.\\n* A key requirement from case workers is to record all the email interactions regarding a case in the case management system , so there is a full view of a case and all associated interactions\\n","Decision":"* Use Microsoft Graph for to integrate with Microsoft Exchange Online.\\n* Use Microsoft Identity [OAuth 2.0 client credentials grant flow](https:\/\/github.com\/microsoftgraph\/microsoft-graph-docs\/blob\/main\/azure\/active-directory\/develop\/v2-oauth2-client-creds-grant-flow) to set up application's permissions for using MS Graph API with restricted permissions only to access shared mailbox.\\n* Microsoft Graph is recommended by Microsoft and [Outlook API has been deprecated](https:\/\/docs.microsoft.com\/en-us\/previous-versions\/office\/office-365-api\/api\/version-2.0\/use-outlook-rest-api)\\n","tokens":65,"id":1267}
{"File Name":"mario\/0004-use-aws-s3.md","Context":"## Context\\nOne of the tenants of Twelve Factor application design is that applications\\nshould be stateless, which includes not relying on local file storage to be\\npersistent. As such, this project needs a cloud based object store.\\nSee [3. Follow Twelve Factor methodology](0003-follow-twelve-factor-methodology.md)\\nAmazon Simple Storage Service (S3) is a secure, durable, and scalable object\\nstorage solution to use in the cloud with which we have established an existing\\npayment relationship.\\nAmazon provides official SDKs for various programming languages to interact\\nwith S3.\\n","Decision":"We will use Amazon S3 for our object store.\\n","tokens":121,"id":3417}
{"File Name":"amf\/0005-api-hierarchy-web-async.md","Context":"## Context\\nAsyncApis are not semantically the same that other kinds of apis, like Web Apis. Some adopters need to know if they are looking specifically an async api, regarding the structure is the same or no.\\nAlso, we need to prepare our model to support different kinds of Apis.\\n","Decision":"Create a hierarchy based on Apis. A Document unit will encodes an Api. This Api is abstract. There will be different types of Apis depending on the specifications.\\n","tokens":64,"id":1388}
{"File Name":"CrossyToad\/adr-0001-record-architecture-decisions.md","Context":"## Context\\nI want to be able to remember the reason I chose a particular architecture so I can change my mind!\\nI also hope the records will be useful to others trying to understand how someone might approach\\ncreating a game in Haskell.\\nNormally I would use this style for a team codebase, but I'm hoping the approach will have value\\neven though this is a solo project.\\nAlso I like architecture decision records and want more projects to use them, so let's lead by\\nexample!\\n","Decision":"I will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":105,"id":2479}
{"File Name":"cafebabel.com\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":3242}
{"File Name":"operational-data-hub\/0015-automatic-backup-of-each-data-component.md","Context":"## Context\\nTo protect against data loss, a backup of all storage components should be implemented. As all data components are specified in the [data catalog](0012-data-catalog-specifies-all-data-components.md), automated backup can be implemented from this specification.\\n","Decision":"We will implemented automated backup based on the [data catalog](0012-data-catalog-specifies-all-data-components.md) for each component that stores data.\\n","tokens":53,"id":2695}
{"File Name":"james\/0039-distributed-blob-garbage-collector.md","Context":"## Context\\nThe body, headers, attachments of the mails are stored as blobs in a blob store.\\nIn order to save space in those stores, those blobs are de-duplicated using a hash of their content.\\nTo attain that the current blob store will read the content of the blob before saving it, and generate its id based on\\na hash of this content. This way two blobs with the same content will share the same id and thus be saved only once.\\nThis makes the safe deletion of one of those blobs a non trivial problem as we can't delete one blob without ensuring\\nthat all references to it are themselves deleted. For example if two messages share the same blob, when we delete\\none message there is at the time being no way to tell if the blob is still referenced by another message.\\n","Decision":"To address this issue, we propose to implement a distributed blob garbage collector built upon the previously developed\\nDistributed Task Manager.\\nThe de-duplicating blob store will keep track of the references pointing toward a blob in a `References` table.\\nIt will also keep track of the deletion requests for a blob in a `Deletions` table.\\nWhen the garbage collector algorithm runs it will fetch from the `Deletions` table the blobs considered to be effectively deleted,\\nand will check in the `References` table if there are still some references to them. If there is no more reference to a blob,\\nit will be effectively deleted from the blob store.\\nTo avoid concurrency issues, where we could garbage collect a blob at the same time a new reference to it appear,\\na `reference generation` notion will be added. The de-duplicating id of the blobs which before where constructed\\nusing only the hash of their content,  will now include this `reference generation` too.\\nAt a given interval a new `reference generation` will be emitted, since then all new blobs will point to this new generation.\\nSo a `garbage collection iteration` will run only on the `reference generation` `n-2` to avoid concurrency issues.\\nThe switch of generation will be triggered by a task running on the distributed task manager. This task will\\nemit an event into the event sourcing system to increment the `reference generation`.\\n","tokens":167,"id":2157}
{"File Name":"where-away\/0012-provide-a-mechanism-for-adding-custom-html-for-the-header-footer-and-head.md","Context":"## Context\\n### Why custom HTML?\\nFor deploying a where-away site, I have a few considerations:\\n- For my personal links that I'll publish at a public domain, I'd like a service worker to cache the html\\n- For the demo site, I'd like a header and footer to give context and to link\\nto the Github repo\\n- down the road, I can imagine making the CSS themable by having colors in CSS\\ncustom properties. I would like a way to conveniently override those colors\\nwhen I use where-away.\\nBy accepting optional HTML strings to include in the head tag, at the start of\\nthe body tag, and at the end of the body tag, one can do all the above things\\nand more. It's cheap to implement and easy to explain.\\n### Alternative implementations\\n1. Update the bookmark xml format to accept those items\\n2. provide CLI arguments to pass in those strings\\n3. accepts paths as CLI arguments and read those files\\n### Considerations\\nI'd like this to be an opt-in feature -- it should be optional. Alternative #1\\nmakes the xml input format more complicated for everyone, even if you're not\\nusing this.\\nI'd like the implementation to be as cheap as possible for now, and #2 is\\ncheaper to build than #3 -- fewer edge cases.\\nDown the road, #3 seems easiest for the customer -- and on a related note, it\\nwould be nice to be able to provide a path to the input file instead of piping\\nit in through stdin. I could imagine a where-away config file with paths to\\nthese different html blocks, path to the input xml, etc.. Also, there could be\\ncli parameters if you don't want to author a config file. BUT that's too much\\nwork for now.\\n","Decision":"Accept CLI parameters (optional) for document head, body header, and body footer\\nhtml.\\n","tokens":380,"id":2281}
{"File Name":"adr-tools\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":3570}
{"File Name":"govuk-aws\/0031-security-group-naming.md","Context":"## Context\\nEach terraform `aws_security_group_rule` requires a name. We need these to be both unique and useful so we\\nneed to define a basic standard for them. We have a few basic patterns shown below:\\n```\\nRule 1:\\n`allow_${source-name}_from_${dest-name}_$service_(ingress|egress)`\\nExample:\\nallow_bastion-elb_from_bastion-asg_ssh_ingress\\nRule 2:\\nallow_${source-name}_(ingress|egress)_${dest-name}_$service\\nExample:\\nallow_bastion-elb_ingress_bastion-asg_ssh\\nRule 3:\\n${source-name}_(ingress|egress)_${dest-name}_$service\\nExample:\\nmonitoring2_ingress_webapps-elb_nrpe\\nRule 4:\\n${source-name}_$service_(ingress|egress)_${dest-name}\\nExample:\\nmonitoring2_nrpe_ingress_webapps-elb\\n```\\nThere are a few other guidelines that will be the same between rules. `$service` will be a named service,\\nnot just a port number. If the rule uses a protocol other than TCP we will add an additional `_$proto` to the end.\\n","Decision":"We will use \"rule 3\" and change all the existing rule names to comply with it.\\n","tokens":270,"id":4053}
{"File Name":"gsp\/ADR013-ci-cd-tools.md","Context":"## Context\\nWe need to choose which tool or tools to use for CI and CD.  Different tools suit different purposes, however some cross over exists which could allow the use of a single tool to do both CI and CD.\\n","Decision":"We will use [Concourse](https:\/\/concourse-ci.org\/) for both CI and CD.\\nReasons:\\n- It will allow the alpha work to progress without waiting for a decision based upon user research on which tool set is best suited for use for CI or CD\\n- The team has experience of using concourse for CI and CD with kubernetes\\n- A working example already exists that can be extended for use in the alpha\\n- Concourse supports simple RBAC which should allow for multi-tenancy capability in the future\\n- It will accelerate the development of the alpha, with the team only needing to learn a single tool rather than multiple tools\\n","tokens":47,"id":3898}
{"File Name":"ftd-scratch3-offline\/0016-don-t-support-scratch-sounds-and-sound-related-blocks.md","Context":"## Context\\nScratch supports sounds.\\nThe ftduino has no way to play sound.\\n","Decision":"Scratch sounds and sound related blocks are not supported.\\n","tokens":19,"id":2632}
{"File Name":"docs\/0019-architecture-for-background-jobs.md","Context":"## Context and Problem Statement\\nSeveral operations (crawl repo, deploy services, ...) are required to run in the background. Therefore, a background job architecture is required being able to launch arbitrary jobs in the background.\\n## Decision Drivers\\n* MUST be implemented in Java\\n* MUST offer a callback functionality\\n","Decision":"* MUST be implemented in Java\\n* MUST offer a callback functionality\\nChosen option: Java CompletableFuture, because of the possibility to chain multiple actions and the option to attach callbacks on completion of a background task.\\n### Positive Consequences\\n* Possibility to chain together multiple actions after a succesful execution of a background task.\\n* Use of FixedThreadPool, i.e., a pool of threads with a predefined number of threads which are reused. If all threads are active at a time and additional tasks are submitted, they will wait in a queue.\\n### Negative Consequences\\n* With the use of a FixedThreadPool, a background task may be delayed if all threads are active at the time of submission.This may produces time of waiting in the frontend of MICO.\\n","tokens":61,"id":4674}
{"File Name":"nebula\/0004-roll-our-own-kubernetes-profile.md","Context":"Context\\n-------\\nPuppetlabs maintains [a kubernetes module][1] in the forge, and Rancher\\nmaintains software for managing multiple clusters running anywhere.\\nThe puppetlabs module claims to support Debian, but, in practice, it\\nappears to only support Ubuntu. We tried forking it and making changes\\nto get it working, but it was a rabbithole of error after error. Also,\\ntheir solution to managing SSL keys was to generate them locally and\\nstore them all in hiera, which was cumbersome. They also didn't support\\nmanaging the CIDRs for the internal network.\\nRancher seemed very nice at first, but it was unstable in practice.\\nDespite running a highly available set of control nodes, bringing one\\ndown did in fact break the cluster. Also, if \/var\/lib\/docker filled up\\non any machine, the only solution was to destroy and recreate the entire\\ncluster.\\nMy assessment of the puppetlabs module is that it tries to do too much\\non its own. My assessment of rancher is that it's not yet stable enough\\nto rely on.\\n[1]: https:\/\/forge.puppet.com\/puppetlabs\/kubernetes\\nDecision\\n--------\\nWe will roll our own kubernetes profiles instead of relying on someone\\nelse's solution.\\nConsequences\\n------------\\nWhile our profiles are sufficient for creating an environment where\\nkubernetes can flourish, bootstrapping must be done outside of puppet,\\nby hand. It's not ideal, but it also shouldn't come up very often.\\nWe haven't yet tried upgrading docker or kubernetes on an existing\\ncluster, but it's easy to imagine that being tricky if we have to do it\\nby hand.\\nIt's worth checking back on rancher in a year or so to see if they've\\nimproved.\\n","Decision":"--------\\nWe will roll our own kubernetes profiles instead of relying on someone\\nelse's solution.\\nConsequences\\n------------\\nWhile our profiles are sufficient for creating an environment where\\nkubernetes can flourish, bootstrapping must be done outside of puppet,\\nby hand. It's not ideal, but it also shouldn't come up very often.\\nWe haven't yet tried upgrading docker or kubernetes on an existing\\ncluster, but it's easy to imagine that being tricky if we have to do it\\nby hand.\\nIt's worth checking back on rancher in a year or so to see if they've\\nimproved.\\n","tokens":385,"id":4942}
{"File Name":"simple-android\/005-single-activity-view-based-navigation.md","Context":"## Context\\nFragments were introduced in Android API 11, as a way to model behavior or a portion of the user interface. Fragments can be shared between multiple\\nActivities, and can be replaced by other Fragments without affecting the parent Activity.\\nHowever, Fragments have a lifecycle that, although part of the Activity lifecycle, is slightly different and the two are not sychronous. This has\\ncreated a class of bugs, of which many are common in Android apps; e.g.:\\n- Fragments are usually created after `Activity#onCreate()` has been called. But during state restoration, Fragments get created\\nin `Activity#onCreate()` when the Activity\u2019s properties may not have been initialised.\\n- When exchanging data between Activities, the `Activity#onActivityResult()` gets called before the Activity is resumed, and creating a Fragment when\\na result is received will crash the app.\\n- Fragments are not resumed when `Activity#onResume()` is called. The documentation recommends using `Activty#onResumeFragments()` instead.\\n- Fragment lifecycle events are not synchronised with Activity lifecycle, and trying to do this introduces an extra layer of complexity. Race\\nconditions are difficult to avoid with an asynchronous lifecycle\\ncausing [pain, suffering and misery](https:\/\/www.google.com\/search?q=illegal+state+exception+fragment+android).\\n","Decision":"We will use a single `Activity` to hold all our \u201cscreens\u201d, and entirely avoid using `Fragment` classes. To switch between \u201cscreens\u201d, we will\\nuse [Flow](https:\/\/github.com\/square\/flow) \u2014 a library that enables navigating between different UI states.\\nFlow makes it easy to construct screens using just `View` classes. Views are inflated synchronously, which drastically reduces complexity and\\neliminates race conditions. When we inflate a layout XML using `LayoutInflater#inflate()`, we are guaranteed that the View will be ready for making\\nany modifications. A single `Activity` also allows for easy animations, because inter-activity transitions are harder to get right.\\nSince we are using `View` classes as screens, we also lose the ability to use\\nthe [Back Stack](https:\/\/developer.android.com\/guide\/components\/activities\/tasks-and-back-stack). Flow also helps with this by providing a `View`\\n-based backstack.\\n","tokens":284,"id":1137}
{"File Name":"fundraising-application\/010_Components_Framework.md","Context":"## Context\\nThe old skin uses the Bootstrap 3, which uses the `float` and `clear` CSS properties for layout, instead of the more modern `flexbox`. Therefore, we looked at different frameworks which can help reduce our efforts in writing CSS classes and Vue components from scratch.\\n### [Vuetify](https:\/\/vuetifyjs.com\/en\/)\\nIt is a big collection of ready-made UI elements based on Google's Material Design. It looks like it could bloat our JavaScript and CSS and make the page look very generic. Also, binding to Material Design might be tricky due to overwriting lots of Material Design assumptions.\\n### [Buefy](https:\/\/buefy.org\/)\\nA \"lightweight\" component library. Looks nice and does a lot of work for us. It is based on [Bulma CSS](https:\/\/bulma.io\/) which seems to be established and is a good combination between usable and adaptable.\\n### [Element.io](https:\/\/element.eleme.io\/#\/en-US)\\nThe documentation does not look good enough. The Q&A page on their website is not in English.\\n### [Tailwind CSS](https:\/\/tailwindcss.com\/docs\/what-is-tailwind\/)\\nIt is not a component library, but a collection of useful CSS classes for rapid prototyping components (by adding lots and lots of class names to them). When the prototyping phase is over, we need to combine the CSS classes into components. We are not sure we want to go into the direction of \"CSS Framework external to components\".\\n","Decision":"We will use Buefy, a UI components framework for Vue.js based on Bulma which is a free, open source CSS framework based on Flexbox.\\n","tokens":329,"id":1516}
{"File Name":"modernisation-platform\/0004-use-bash-node-python-as-core-languages.md","Context":"## Context\\nAs we build the Modernisation Platform, we should define preferred languages for anything written and maintained by the Modernisation Platform team, including GitHub Actions and AWS Lambda functions.\\n","Decision":"We have decided to use `bash`, `nodejs`, and `python` as our core languages, as they are both: three of the most popular languages by repository in the Ministry of Justice, and the team already have knowledge of these languages.\\nWe ran a poll for what languages people already know within the team and the results were:\\n- 3 votes for `nodejs`\\n- 3 votes for `python`\\nWe have also includes `bash` in this list to be explicit in that we should and can use it when needed.\\nWe will leave it to each team member to decide what language is best for the problem they are trying to solve.\\n","tokens":37,"id":3139}
{"File Name":"terraform-aws-ecs\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":4567}
{"File Name":"cnp-design-documentation\/0009-developer-workflow-for-code-changes.md","Context":"## Context\\nAdoption of the CNP brings with it the ability to change and improve the developer workflow when making application code changes.  As per [5. Application Delivery Pipelines](0005-pipeline-design.md) the pipeline is opinionated and promotes the practice of Continuous Delivery.  This proposal goes into the detail of the expected workflow of a developer making an application change in the context of CNP using Continuous Delivery.\\n### What is Developer Workflow?\\nDeveloper workflow is the process developers use to create application changes and prepare them for release in a reliable manner.  This includes the branching strategies, code review, and testing, all with a heavy reliance on the pipeline.\\n","Decision":"The following patterns are proposed in order to facilitate confident code changes with fast feedback.\\n### Trunk based development using short lived feature branches\\nTrunk based development is an enabler for Continuous Delivery via Continuous Integration.  Changes are frequently made to the master branch (trunk) whilst ensuring the master branch is always in a releasable state.  The releasable state is supported by a wide range of automated testing and development strategies for managing change such as feature toggles or branch by abstraction.\\nA short lived feature branch is a branch created with a specific and small change as its purpose.  The branch should live no more than a couple of days, preferably under a single day, before it is merged into master and deleted.  The merging takes place via a Pull Request in the [Github Flow](https:\/\/guides.github.com\/introduction\/flow\/) (not to be confused with the more onerous [GitFlow](http:\/\/endoflineblog.com\/gitflow-considered-harmful)).  There should be no more than one short lived feature branch per developer and are not shared.  This is easily achievable when change is small, focussed and frequently merged into master.\\nThe master branch of each repository should be configured as a Protected Branch, thereby preventing force pushes, deletion and requiring status checks before merging.\\n![Diagram of a short lived feature branch with commits, a PR, comments and a merge to master](..\/..\/img\/trunk_pr.png)\\n<br \/>\\n_Image from [TrunkBasedDevelopment.com](https:\/\/trunkbaseddevelopment.com\/short-lived-feature-branches\/)_\\nIntegration is frequent, test are run automatically and master is kept green with failed builds fixed within ten minutes as per Martin Fowler's [Continuous Integration Certification Test](https:\/\/martinfowler.com\/bliki\/ContinuousIntegrationCertification.html).\\n### Use Pull Requests for code review\\nCode reviews are an enabler for improving code quality and knowledge sharing.  Combined with short lived feature branches, small pull requests (PR) enable fast feedback and an early opportunity to incorporate it.\\nThe in-built support within Github is excellent for facilitating reviews and each repository should configured to require at least one review before merge.\\nThe creation of a pull request will trigger a shortened pipeline.  The PR code will be applied to the master code locally on the Jenkins agent workspace, as such simulating a merge.  The shortened pipeline will then run the static checks (i.e. checks which do not require deployment to an environment).  This will include running unit tests and static analysis.  These checks, combined with a peer review, should provide enough confidence that a PR is ready for merge.\\n![Diagram of a pull request going through static checks before merge](..\/..\/img\/Pull-Request-Pipeline-Flow.png)\\nOnce a PR is merged, a full pipeline will be triggered and the integrated code will once again pass through the static checks before a deployment stage and on to the Production subscription.\\n### Use strategies to allow deployment to continue whilst controlling the release of changes\\nIn a Continuous Delivery world, using short lived feature branches, ideally all changes will be small enough to fit into this process.  For the times that this is not the case, it is important to separate the action of \"deploying code\" from \"releasing a change\".  This is to allow the code to continue to be tested and deployed whilst avoiding big bang deployments.\\nOne strategy is [Branch by Abstraction](https:\/\/martinfowler.com\/bliki\/BranchByAbstraction.html)\".  This is a technique for making a large-scale change to a software system in gradual way that allows you to release the system regularly while the change is still in-progress.\\nAnother strategy is [Feature Toggles](https:\/\/martinfowler.com\/articles\/feature-toggles.html).  At their most basic, they are flags that disable or enable code paths through an application.  There are multiple categories of feature toggles allowing for different levels of change, from experimentation to controlling a release.\\n### Blue\/Green deployments support \"zero downtime deploys\"\\nTo support frequent, small deployments we will use [Blue\/Green deployment mechanisms](https:\/\/martinfowler.com\/bliki\/BlueGreenDeployment.html) to ensure zero downtime during the process.  Production is split into two subsets of apps: one set serving live traffic and another parallel set which is not.  Deployments are sent to the non-live set and tests are run against them.  If the tests are successful a manual switch is hit and the traffic is routed to this subset making it live.\\n![Diagram of a blue\/green environment and traffic being routed from one to the other](..\/..\/img\/BlueGreen.png)\\nThis switch requires that new deployments of the applications are backwards compatible with the deployment currently receiving live traffic.  As a result, changes become small, gradual and less risky.\\n### Managing database changes whilst using Blue\/Green deployments\\nWith a Blue\/Green deployment any data store is shared between both subsets.  To support this, changes to the schema must be done in phases in tandem with application deployments. Recommended reading is the blog post [Database Migrations Done Right](http:\/\/www.brunton-spall.co.uk\/post\/2014\/05\/06\/database-migrations-done-right\/)\\nThis is explored in [10. Developer Workflow for DB Schema Changes](0010-developer-workflow-for-db-schema-changes.md).\\n","tokens":138,"id":1072}
{"File Name":"generator-latex-template\/0003-use-cleveref.md","Context":"## Context and Problem Statement\\nWhen referencing a figure (e.g., see Figure 5), should the text \"Figure\" also be a link?\\nWhen referencing figures at the beginning of a sentence, should be possible to use a different word?\\nFor instance, Springer demands \"Figure\" at the beginning of a sentence, but \"Fig.\" in the middle of the sentence?\\nShould it be possible to summarize multiple figures? For instance, range fig:one, fig:two, fig:three automagically gets 1--3.\\n## Decision Drivers\\n* The word before the number of the target should also be a hyperlink to offer a wider clicking space.\\nE.g., \"Figure 1\" should be the hyperlink text, not only the \"1\".\\n* Easy to use commands\\n* Package should be compatible to other packages\\n* Package should be available on overleaf\\n","Decision":"* The word before the number of the target should also be a hyperlink to offer a wider clicking space.\\nE.g., \"Figure 1\" should be the hyperlink text, not only the \"1\".\\n* Easy to use commands\\n* Package should be compatible to other packages\\n* Package should be available on overleaf\\nChosen option: \"[option 1]\", because [justification. e.g., only option, which meets k.o. criterion decision driver | which resolves force force | \u2026 | comes out best (see below)].\\n","tokens":184,"id":2332}
{"File Name":"macondo\/0004-allow-easily-publishing-of-commands-and-repositories.md","Context":"## Context\\nNow that the [repository story is a bit\\nclearer](0003-remote-repositories-management.md), `macondo build` command should\\nbe enhanced to also generating repository YAML files.\\n","Decision":"Add a new flag to the `macondo build` command that also generates a YAML file\\nwith all the commands generated.\\n","tokens":45,"id":2196}
{"File Name":"ELEN4010\/ADR Trunk-Based Development.md","Context":"## Context\\nTo perform Continual Integration and development, with weekly releases, it would be convenient and useful to have a testing branch as well. Accidental pull requests into the main branch may introduce features that have not been tested from the interfac\/front-end. It is difficult to automate these front-end interface tests, and there may be factors not present in a localhost\/express server that only become apparent in an online scanario.\\nThe use of **master** branch as the release branch is useful, as 'master' is usually the most protected on GitHub, with the most warnings about deleting, modifying, etc.\\nCode reviews ar essential from all developers, to become familiar with each other's code, and to learn about javascript, and web-development. THis way we all learn from each other, and also learn good review and communicaton practice.\\n","Decision":"**master** will be the release branch\\n**development** will be the main development\/test branch. This will also be made into the \"default\" branch for all pull requests, to avoid accidentaly PR into master\\n**feature** branches must be made off development, with unique names. All pull requests for completed features to be made into \"development\".\\n* All PRs must be reviewed by at least two developers to merge into \"development\"\\n* All PRs must be reviewed by at the three other developers to merge into \"master\"\\n* All PRs must pass all tests (Jest, Travis, and Coveralls) in order to be considered valid for a merge\\n* Stale reviews will be automatically dismissed if a new commit is pushed to the same branch\\n* Accepted PRs for completed features (User Stories) should be deleted after sucessfully merging\\n","tokens":172,"id":63}
{"File Name":"nhsuk-prototype-kit-version-one\/0005-use-handlebars-as-default-templating-solution.md","Context":"## Context\\nPrototypes built using the NHSUK prototype kit are a combination of markup and logic that\\nexhibit a standardised visual language. In order to provide developers easy access to the\\nvisual language and to build dynamic, data-driven prototypes, a templating system that\\nsupports partials and conditional rendering is required. As the intention is to encode\\nprototype logic in Javascript, the templating system is not required to provide logic\\nfunctions of its own.\\n","Decision":"We will provide the [Handlebars](http:\/\/handlebarsjs.com) templating system as the default\\nfor the NHSUK prototype kit. A library of Handlebars partials will be provided that encode\\nthe markup necessary for prototypes to use the standard visual language.\\n","tokens":94,"id":3684}
{"File Name":"digital-paper-edit-firebase\/2019-12-02-nosql-schema-security.md","Context":"## Context and Problem Statement\\n[Security rules](https:\/\/firebase.google.com\/docs\/firestore\/security\/get-started) in Firebase is glob-like.\\n```c++\\nservice cloud.firestore {\\nmatch \/databases\/{database}\/documents {\\nmatch \/<some_path>\/ {\\nallow read, write: if <some_condition>;\\n}\\n}\\n}\\n```\\nWe need to add user authentication ID to the conditional statement above to tighten security for who can access our data. This means that we might need to rethink how we want to organise the data, as it was initially thought to be flat:\\n```text\\n\/users\/{uId}\\n\/projects\/{pId}\\n\/transcripts\/{trId}\\n\/paperedits\/{peId}\\n\/uploads\/{upId}\\n```\\nIf we retained flatness, it will be easy to access the data - however we need to complicate our data structure by retaining the user ID in each of the records.\\n## Decision Drivers <!-- optional -->\\n- Security\\n- Ease in implementation (DB)\\n- Ease in implementation (Security Rules)\\n- Extensibility\\n","Decision":"- Security\\n- Ease in implementation (DB)\\n- Ease in implementation (Security Rules)\\n- Extensibility\\nChosen option: \"2 collections\", because when thinking about membership and extensibility, we'd want to be able to have multiple `users` in a single `project` and every asset (`transcripts`, `paperEdits`) should be associated with a `project`.\\nWe will authenticate users based on a field in projects called `users`. Each project will have several users like so:\\n- `projects\/{id}\/users` = []\\nA security rule like below will be able to test authentication and membership of the project on users.\\n```js\\nfunction isOnProject() {\\nreturn (\\nrequest.auth.uid in\\nget(\/databases\/$(database) \/ documents \/ projects \/ $(pid)).data.users\\n);\\n}\\n```\\n### Uploads (Storage and Firestore)\\nThe `uploads` subcollection has been added to the `users` for security and convenience: Storage rules are a lot easier to implement based on users, and we want to make things symmetric as possible.\\n| Firestore                   | Storage                     |\\n| --------------------------- | --------------------------- |\\n| `users\/{id}\/uploads\/{upId}` | `users\/{id}\/uploads\/{upId}` |\\nTo keep consistency, we also use the same `id` for `transcript` and `uploads` in Firestore\\n| Firestore                            | Firestore                     |\\n| ------------------------------------ | ----------------------------- |\\n| `projects\/{id}\/transcripts\/{itemId}` | `users\/{id}\/uploads\/{itemId}` |\\n","tokens":236,"id":5241}
{"File Name":"ols-client\/0003-looping-over-list.md","Context":"## Context\\nWe want to be able to loop simply over Ontologies \/ Terms results, without bothering if a\\nnew call is made to change page.\\nOLS API results are paginated, the page size is a parameter in Query.\\nThere is no simple way to loop over all elements, and returning all results is not a solution, considering amount of data\\nThe actual calls to API are hidden from final users.\\n```python\\nfrom ebi.ols.api.client import OlsClient\\nclient = OlsClient()\\nontology = client.ontology('fpo')\\nterms = ontology.terms()\\nindividuals = ontology.individuals()\\nproperties = ontology.properties()\\n# work with all 'list' item types\\nfor term in terms:\\n# do whatever\\nprint(term)\\n# Direct List'like access on all list types\\nterm = terms[1254]\\nindividual = individuals[123]\\n# ...\\n```\\n","Decision":"To Implement\\n","tokens":199,"id":605}
{"File Name":"Wikibase\/0014-monorepo.md","Context":"## Context\\nWikibase-related code is found in various places and under different degrees of isolation.\\nParts of the PHP code, especially early \/ fundamental parts like the data model and its serialization,\\nare independent libraries in separate Git repositories, distributed via Composer.\\nSome JavaScript code, such as the new Termbox,\\nis also developed independently, and typically included as a Git submodule.\\nOn the other hand, much of Wikibase is included directly in the Wikibase Git repository,\\noften as part of the monolithic \u201cLib\u201d component.\\nPart of the goal of the Decoupling Hike (June to August 2020) has been to develop a strategy for refactoring this \u201cLib\u201d component.\\nWe found that the separate libraries have obvious benefits thanks to being stand-alone, separate components,\\nbut also have downsides for development:\\nit is inconvenient to have to develop the libraries in separate repository,\\nand cumbersome to get the changes back into the main Wikibase Git repository.\\nAt best, a submodule pointer needs to be updated;\\nat worst, the library needs to publish a new release, which then needs to make its way into [mediawiki\/vendor][].\\nWe investigated a \u201cmonorepo\u201d approach in [T254920][],\\nand propose that it offers the best of both worlds.\\nMonorepos are also used by others, including the Symfony PHP framework\\n([blog post][Symfony blog post], [talk video][Symfony talk video]).\\n","Decision":"Wikibase.git will become a monorepo,\\ncontaining not just the overall MediaWiki extension code\\nbut also the code and history of libraries that can stand on their own.\\nChanges to those libraries become immediately effective within Wikibase,\\nbut are also published separately.\\nWhere possible, sections of Wikibase Lib will be extracted into separate libraries.\\nDependencies on MediaWiki are removed\\n(or replaced with suitable MediaWiki libraries, e.g. [wikimedia\/timestamp][] instead of `wfTimestamp()`).\\nTheir source code is moved from subdirectories of `lib\/includes\/` and `lib\/tests\/phpunit\/` into a subdirectory of `lib\/packages\/`\\n(adjusting the paths in extension `AutoloadNamespaces` and elsewhere),\\nnamed after the prospective Composer package name,\\nand a `composer.json` file is added there.\\nThe [git filter-repo][] tool can then be used to extract a subset of the Wikibase Git history with only the changes relevant to the new library;\\nthis new read-only repository can be used as the VCS source of the Composer package,\\nand is automatically updated through a GitHub action (see e.g. `.github\/workflows\/filterChanges.yml`).\\nThe Decoupling Hike team demonstrated these steps for the [wikibase\/changes][] library;\\nsee [T256058][] and related tasks for details.\\nFormerly stand-alone libraries will be merged into the Wikibase Git repository.\\nTheir history will be preserved, and they will also be extracted into separate Git repositories again,\\nusing the [git filter-repo][]-based process outlined above.\\nWe expect that it will be possible to produce identical Git hashes,\\nmaking this migration transparent to other users of the libraries\u2019 Git repositories \u2013\\nthe repositories will simply no longer be the main source of truth.\\nThe Decoupling Hike team has not done this for any existing library.\\n","tokens":306,"id":1356}
{"File Name":"buy-for-your-school\/0018-use-notify-api-for-sending-emails.md","Context":"## Context\\nIt is necessary for the service to confirm various actions with users via email messages.\\n","Decision":"Gov.UK Notify is the recommended solution, using the `notifications-ruby-client` library.\\n","tokens":20,"id":1262}
{"File Name":"rails-template\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as\\n[described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":4184}
{"File Name":"molgenis-r-armadillo\/0001-use-adr-to-describe-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions.\\n","tokens":16,"id":546}
{"File Name":"front-end-monorepo\/adr-09.md","Context":"## Context\\nThe big question: which Subject Viewer should we use to view a given Subject?\\nAt the moment, the Classifier tries to \"guess\" which Subject Viewer to use (see\\n`lib-classifier\/src\/store\/Subject.js`, `get viewer()`) by analysing the Subject\\nitself. e.g. if the Subject has multiple images, show a multi-image viewer. If\\nthe Subject has a video, show a video viewer.\\nWhile this method works in a majority of projects, certain projects with\\nextremely specific Subject structures breaks the generalised \"guessing\" logic,\\nor else warps the logic with so many \"what if\" clauses that it collapses into a\\nwrithing mass of nonsense.\\nFor example, a TESS Planet Hunters Subject looks like...\\n```\\nSubject 12345 = {\\n...\\nlocations: [\\n{ \"image\/png\": \"tess-data-12345.png\" },\\n{ \"application\/json\": \"tess-data-12345.json\" }\\n]\\n}\\ntess-data-12345.json = {\\nx: [1,2,3,4,5],\\ny: [6,7,8,9,0]\\n}\\n```\\nIf we were to add to the \"guess the Subject Viewer\" logic by stating that _\"any\\nSubject that has a JSON, and that JSON has x-y coordinates, should use the Light\\nCurve Viewer\",_ then we'd have trouble if, say, a future project needs similar\\nJSONs with x-y coordinates for a Map Viewer or a Line Graph Viewer, etc.\\nSide note:\\n- The Light Curve Viewer can still also serve as a generic Scatterplot Viewer\\ngiven proper tweaks - this should be marked for future dev so JSON data with\\ngeneric x-y data can be \"guessed\".\\n","Decision":"The \"Choose a Subject Viewer\" logic (again, see\\n`lib-classifier\/src\/store\/Subject.js`, `get viewer()`) will _first_ try to check\\nif there's a **Workflow Configuration** stating the preferred Subject Viewer.\\nIf this specific configuration does not exist, the logic will _continue_ to\\n\"guess\" the correct Subject Viewer to use, as per the current system.\\n```\\n\/\/Example of a workflow.configuration for TESS project:\\n{\\nsubject_viewer: \"lightcurveviewer\"\\n}\\n```\\nWe've also decided _not_ to specify the \"Custom\/Specific Subject Viewer\" logic\\nwithin the _Subject_ itself, since it'll make the Subjects more complicated and\\nmaintenance troublesome. (Compare changing the config field of one Workflow vs\\nupdating the metadata or JSON of a million Subjects.)\\ni.e. we should **not** do something like:\\n```\\ntess-data-12345.json = {\\ntype: 'lightcurve',\\nx: [1,2,3,4,5],\\ny: [6,7,8,9,0]\\n}\\n```\\n","tokens":385,"id":502}
{"File Name":"Data-Platform-Playbook\/011-using-datahub-as-a-data-catalogue.md","Context":"## Context\\nWe need users to be able to browse and search the datasets within the platform so that they can find what they need, and we can break down data siloes. There are various different ways to implement this.\\n## Decision Drivers\\n- Ease of set up and maintenance\\n- Features and user experience\\n- Ability to ingest metadata from the data lake\\n- Cost\\n","Decision":"- Ease of set up and maintenance\\n- Features and user experience\\n- Ability to ingest metadata from the data lake\\n- Cost\\nWe have decided to use an DataHub, an open source tool, as our data catalogue tool because:\\n- We have been able to successfully set it up in our AWS environment without much difficulty [please add here]\\n- It has the basic features we require to catalogue datasets (e.g. a range of metadata available which users can add to, search and browse functionality) as well as additional functionality that may enable us to catalogue data pipelines, dashboards, models etc in future.\\n- Users responded positively to it in user research.\\n- It is capable of ingesting metadata from Hive\/AWS Glue and thus our data lake.\\n- It is open-source and therefore cost is limited.\\n- It has an active community where we have an opportunity to influence its future development.\\n","tokens":77,"id":1008}
{"File Name":"govuk-aws\/0037-alb-health-checks.md","Context":"## Context\\nOn our platform, application instances receive traffic routed from a load balancer. Typically, an application instance runs Nginx\\nwith a server_name based Vhost per application. Each application Vhost proxies the requests to the local port\\nwhere the application listens.\\nLoad balancers use a health check to determine which instances to send traffic to. Health checks depend on the type of load\\nbalancer and protocol in use. Usually we are going to configure health checks using TCP+PORT (ELBs) or HTTP+PORT+Request_path+Response_code\\n(ALB). For more information on health checks, have a look at the following links:\\nhttps:\/\/docs.aws.amazon.com\/elasticloadbalancing\/latest\/classic\/elb-healthchecks.html\\nhttps:\/\/docs.aws.amazon.com\/elasticloadbalancing\/latest\/application\/target-group-health-checks.html\\nWhen we configure HTTP health checks, the load balancer sends a request without a Host header. In this case, Nginx processes that\\nrequest in the default Vhost. These requests have been configured in the default Vhost to serve an `200 OK` response code, which indicates to the\\nload balancer that the instance is healthy and ready to receive traffic. This presents a problem when we are starting or redeploying\\napplications in that instance, because the application port will be unavailable for a period of time, and Nginx will serve 5xx responses. The\\nload balancer still sees the health check responses as `200 OK`, so it continues sending traffic to that instance, even though the application\\nis not ready to receive traffic.\\nThe current ALB health check is rudimentary and does only test for the basic Nginx configuration being in place, rather than testing whether\\nall apps are properly deployed.  If an instance is terminated, this causes its replacement to be added to the target group pool prematurely which\\ncauses elevated 5xx rates.\\n","Decision":"When an instance is running more than one application, we are going to route traffic to the instance with an ALB. Each application has a dedicated\\ntarget group, with a specific health check path, following the format `\/_healthcheck_<app-name>`.\\nThis health check path is configured in the default Vhost to proxy the request to the upstream application, and rewrites the request to match the\\napplication health check path that is also used in Icinga checks. This is configured in Puppet, in `govuk::app::config`, as the `health_check_path`\\nparameter.\\nThe ALB includes routing rules based on Host header, and redirects the traffic to the application target group when the Host header matches\\nthe value `<<app-name>.*`\\n","tokens":394,"id":4036}
{"File Name":"docker-compose-rule\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":1539}
{"File Name":"Data-Platform-Playbook\/009-ingesting-data-from-apis.md","Context":"## Context\\nThe Data Platform currently has no capacity to ingest data from external APIs\\n","Decision":"![API ingestion](..\/images\/api-ingestion.png)\\nDeploy a Lambda that will call the the required API and save the returned data\\nobject into S3, once all the records have been downloaded the lambda will\\ntrigger an AWS glue workflow that will convert JSON objects into parquet and\\ncrawl the data enabling users to access that form a data catalogue in AWS Athena.\\n","tokens":17,"id":1015}
{"File Name":"embvm-core\/0009-event-driven-framework-design.md","Context":"## Context\\n* Embedded systems are highly event-driven, as they are responding to external stimuli and reacting in a planned way\\n* Event-driven APIs reduce coupling, as the various objects don't need to know anything about other objects that they work with\\n* We can reduce the number of threads used by relying on event-driven behavior\\n","Decision":"The framework will be defined with interfaces and processing models that support event-driven development (callbacks, events, register\/unregister for events).\\nDispatch queues will be provided to assist with the event driven model.\\nPlatform examples will default to dispatch-based processing models.\\n","tokens":66,"id":3007}
{"File Name":"titania-os\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":2576}
{"File Name":"govuk-aws\/0022-remove-the-elasticsearch-proxy.md","Context":"## Context\\nWe currently have a local nginx based proxy for Elasticsearch connections\\non a number of our machines. The original reason for this was:\\n```\\nLoad balance connections to Elasticsearch by creating a loopback-only\\nvhost in Nginx which will forward to a set of Elasticsearch servers.\\nThis is for the benefit of applications with client libraries that don't\\nsupport native cluster awareness or load balancing.\\n```\\nIn the AWS environments these services are behind an ELB so this layer may\\nno longer be required.\\n","Decision":"We've tested the instances in AWS without the local proxies and we have not discovered any\\ndegradation of service when using the ELBs. In light of this we will remove the configuration and\\nexclude this puppet code from running in AWS and accept the ELBs as its replacement.\\n","tokens":110,"id":4040}
{"File Name":"elife-base-images\/0003-extract-scripts.md","Context":"## Context\\nDockerfiles make no provision for extracting duplication of steps unless they are rendered from a template. Similar `RUN` steps can multiply across different files.\\n`RUN` steps also allow no logic or encapsulation, and promote long chains of commands due to the necessity of producing a single layer.\\n`RUN` steps are not testable in isolation or re-runnable inside an image for debugging.\\n","Decision":"Extract long `RUN` steps (or sequences of steps) into a bash script. If the script is only to be used by `root`, place it in `\/root\/scripts`.\\n","tokens":84,"id":1508}
{"File Name":"verify-matching-service-adapter\/0002-hash-pids-uniquely-for-each-loa.md","Context":"## Context\\nThe pid that the user is issued by their identity provider will not necessarily change when the level of assurance they achieve changes. For example a user may uplift from LoA1 to LoA2 in their IdP, but this will present to the relying party with the same identifier. This is an issue because a Cycle 0 match at LoA1 is not valid for use at LoA2, and presents a specific security risk whereby a user may obtain LoA1 as someone else, person A, and then uplift to LoA2 using their own identity (person B), but continue to assume the identity of person A at the higher LoA. So a change of LoA requires matching to be done afresh.\\n","Decision":"We will modify the pid for non-LoA2 assertions to be unique to that LoA so that it is not possible for relying parties to make an insecure match at Cycle 0 when the user moves between levels of assurance. This will be done by introducing the LoA into the pid hash.\\n","tokens":149,"id":4646}
{"File Name":"Maud\/0009-gpl.md","Context":"## Context\\nWe need to choose an appropriate license before making Maud public. The main\\ncontenders are the Apache license, which would allow others to use Maud in\\nproprietary software, and the GPL v3 license, which would not.\\n","Decision":"We will use the GPL license but may change to a more permissive license in a\\nfuture release, depending on the circumstances at the time.\\n","tokens":52,"id":244}
{"File Name":"linshare-mobile-flutter-app\/0008-upgrade-android-target-sdk-29.md","Context":"## Context\\n- Download feature does not work when targeting into SDK 29 and above\\n- From Android 10 and above, the [scoped storage](https:\/\/developer.android.com\/training\/data-storage#scoped-storage) has enabled that makes the access into external storage is limited. So, we need to adapt with this change.\\n","Decision":"- Target to Android 10 (SDK 29) first, then adapt to Android 11 (SDK 30) later.\\n- Temporarily [opt-out of scoped storage](https:\/\/developer.android.com\/training\/data-storage\/use-cases#opt-out-scoped-storage) in Android 10:\\n```xml\\n<manifest ... >\\n<!-- This attribute is \"false\" by default on apps targeting\\nAndroid 10 or higher. -->\\n<application android:requestLegacyExternalStorage=\"true\" ... >\\n...\\n<\/application>\\n<\/manifest>\\n```\\n```groovy\\ndefaultConfig {\\n...\\ntargetSdkVersion 29\\n...\\n}\\n```\\n","tokens":67,"id":3300}
{"File Name":"tendermint\/adr-056-light-client-amnesia-attacks.md","Context":"## Context\\nWhilst most created evidence of malicious behavior is self evident such that any individual can verify them independently there are types of evidence, known collectively as global evidence, that require further collaboration from the network in order to accumulate enough information to create evidence that is individually verifiable and can therefore be processed through consensus. [Fork Accountability](https:\/\/github.com\/tendermint\/tendermint\/blob\/master\/spec\/consensus\/light-client\/accountability.md) has been coined to describe the entire process of detection, proving and punishing of malicious behavior. This ADR addresses specifically what a light client amnesia attack is and how it can be proven and the current decision around handling light client amnesia attacks. For information on evidence handling by the light client, it is recommended to read [ADR 47](https:\/\/github.com\/tendermint\/tendermint\/blob\/master\/docs\/architecture\/adr-047-handling-evidence-from-light-client.md).\\n### Amnesia Attack\\nThe schematic below explains a scenario where an amnesia attack can occur such that two sets of honest nodes, C1 and C2, commit different blocks.\\n![](..\/imgs\/tm-amnesia-attack.png)\\n1. C1 and F send PREVOTE messages for block A.\\n2. C1 sends PRECOMMIT for round 1 for block A.\\n3. A new round is started, C2 and F send PREVOTE messages for a different block B.\\n4. C2 and F then send PRECOMMIT messages for block B.\\n5. F later on creates PRECOMMITS for block A and combines it with those from C1 to form a block\\nThis forged block can then be used to fool light clients trying to verify it. It must be stressed that there are a few more hurdles or dimensions to the attack to consider.For a more detailed walkthrough refer to Appendix A.\\n","Decision":"The decision surrounding amnesia attacks has both a short term and long term component. In the long term, a more sturdy protocol will need to be fleshed out and implemented. There is already draft documents outlining what such a protocol would look like and the resources it would require (see references). Prior revisions however outlined a protocol which had been implemented (See Appendix B). It was agreed that it still required greater consideration and review given it's importance. It was therefore discussed, with the limited time frame set before 0.34, whether the protocol should be completely removed or if there should remain some logic in handling the aforementioned scenarios.\\nThe latter of the two options meant storing a record of all votes in any height with which there was more than one round. This information would then be accessible for applications if they wanted to perform some off-chain verification and punishment.\\nIn summary, this seemed like too much to ask of the application to implement only on a temporary basis, whilst not having the domain specific knowledge and considering such a difficult and unlikely attack. Therefore the short term decision is to identify when the attack has occurred and implement the detector algorithm highlighted in [ADR 47](https:\/\/github.com\/tendermint\/tendermint\/blob\/master\/docs\/architecture\/adr-047-handling-evidence-from-light-client.md) but to not implement any accountability protocol that would identify malicious validators and allow applications to punish them. This will hopefully change in the long term with the focus on eventually reaching a concrete and secure protocol with identifying and dealing with these attacks.\\n","tokens":381,"id":1948}
{"File Name":"forkhandles\/0002-monorepo-and-bom.md","Context":"## Context\\nDo we have lots of little xxx4k libraries in their own repositories?  Or have a monorepo: one big project with each xxx4k library in a subdirectory.\\nThe former decouples release cadences.\\nThe latter makes it easier to maintain a single BOM for publishing to Maven Central, perform integration testing when libraries depend on one another, and use a consistent version number across all libraries.\\n","Decision":"We will have a monorepo.\\n","tokens":86,"id":4542}
{"File Name":"uqlibrary-reusable-components\/adr-001.md","Context":"## Context\\nThe UQ purple header is included in primo through include files like `assets.library.uq.edu.au\/primo-sand-box\/reusable-components\/`, via the primo BO.\\nUp until now we have been hard coding the primo-sand-box bit according to which environment we are in, and having to remember to manually change it depending on which environment we were uploading to.\\nManual processes are to be avoided.\\n","Decision":"Generate the branch to be used, by looking at the host name and the vid parameter on the url\\nhttps:\/\/github.com\/uqlibrary\/uqlibrary-reusable-components\/commit\/4f1c182\\n","tokens":87,"id":665}
{"File Name":"human-essentials\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":3346}
{"File Name":"gsp\/ADR029-continuous-delivery-tools.md","Context":"## Context\\nWe currently have a separate GSP cluster for CI tooling that runs Concourse and other build tools (Docker Registry, Image Scanning and Notary)\\nWe currently share configuration between the tools cluster and programme clusters.\\nWe have in-cluster tooling (based on [Flux](https:\/\/github.com\/weaveworks\/flux)) to perform a pull based continuous deployment loop (watch a branch, apply state to cluster).\\nWe forked flux to support git commit signature verification.\\nWe want to avoid sharing credentials capable of modifying the cluster state outside of the cluster itself.\\nWe have been experiencing the following issue with the current setup:\\n* Poor visibility to service teams of what Flux is doing\\n* Slow and hard to reason about issues with deployments\\n* Complex configuration that needs to span multiple cluster instances reduces our ability to automate provisioning\/deployments\\n* Having separate credentials for the tools cluster and programme clusters is confusing (they both look the same) and inconvenient (different kubeconfig, different IAM profile etc.)\\nIf we run the CI\/CD tools inside the programme cluster it could:\\n* Remove the need to have separate CD tooling (use Concourse for both CI and CD tasks)\\n* Remove the need to maintain our own fork of Flux (we can achieve the same thing with the Concourse resource we already have)\\n* Improve visibility of continuous deployment by using Concourse's user interface to expose status to developers\\n* Reduce the amount of configuration required (no need to share config between clusters\/accounts)\\n* Automate configuration of build tooling and provenance checking (signatures, Notary, etc)\\n* Reduce the amount of infrastructure we manage to one control plane, one account etc.\\n* Automate access control to concourse using the same RBAC rules as the cluster (One Concourse Team == One Namespace in the cluster)\\n","Decision":"We will run Concourse in each programmes's cluster to provide both CI and CD tooling.\\n","tokens":386,"id":3902}
{"File Name":"wikibase-release-pipeline\/0017-using-mediawiki-docker-image.md","Context":"## Context\\nDuring the first iteration of development on the wikibase release pipeline, one of the goals was to build and run it against the master branches of MediaWiki, Wikibase and other bundled extensions ([T270133](https:\/\/phabricator.wikimedia.org\/T270133)).\\nBecause of the lack of such docker images at the time the team decided to create our own, inspired by the work of the official docker images. The benefits of this decision was seen when previously untested parts of Wikibase (Multi Wiki testing client\/repo ) now had some coverage on the master branch. During the development of the pipeline several issues were detected by using these custom docker images, sometimes the pipeline would breakdown days before a bug report would appear on phabricator.\\nThis can be useful but also comes with some additional drawbacks that can affect the maintainability and the quality of the releases WMDE will produce.\\n- To offer the same quality and security as the official Mediawiki docker images we now also have to maintain our own rather than building upon what already exists.\\n- Any updates or security fixes to these images are probably also more likely to be identified and patched in the official MediaWiki docker images quicker than in any image maintained by WMDE.\\n- The MediaWiki docker images are battle proven with 10+ Million downloads, our custom images are not.\\nAs the priority of the release pipeline should be to provide stable and secure releases it could make sense to revert this initial decision of building our own image.\\nThe decision to adopt parts of the testing done in the release pipeline for Wikibase CI is still pending. Depending on the outcome of [T282476](https:\/\/phabricator.wikimedia.org\/T282476), custom images could then be required again and could serve as a base when used for testing in CI where the requirements for security or performance aren't as high ([T282479](https:\/\/phabricator.wikimedia.org\/T282479)).\\n","Decision":"- Use the official MediaWiki docker images as a base for the Wikibase base and bundle images.\\n","tokens":399,"id":4464}
{"File Name":"GDD-app\/0004-decouple-domain-models-from-representation.md","Context":"## Context\\nDomain models were used as-is for persistence and over-the-wire communication, making it difficult\\nto define a proper and strong domain model.\\n","Decision":"Separate DTOs should be defined for persistence and over-the-wire communication, which can have a\\nmore adequate representation for their use-case, without affecting our domain model.\\n","tokens":33,"id":4869}
{"File Name":"cosmos-sdk\/adr-027-deterministic-protobuf-serialization.md","Context":"### Context\\nFor signature verification in Cosmos SDK, the signer and verifier need to agree on\\nthe same serialization of a `SignDoc` as defined in\\n[ADR-020](.\/adr-020-protobuf-transaction-encoding.md) without transmitting the\\nserialization.\\nCurrently, for block signatures we are using a workaround: we create a new [TxRaw](https:\/\/github.com\/cosmos\/cosmos-sdk\/blob\/9e85e81e0e8140067dd893421290c191529c148c\/proto\/cosmos\/tx\/v1beta1\/tx.proto#L30)\\ninstance (as defined in [adr-020-protobuf-transaction-encoding](https:\/\/github.com\/cosmos\/cosmos-sdk\/blob\/main\/docs\/architecture\/adr-020-protobuf-transaction-encoding.md#transactions))\\nby converting all [Tx](https:\/\/github.com\/cosmos\/cosmos-sdk\/blob\/9e85e81e0e8140067dd893421290c191529c148c\/proto\/cosmos\/tx\/v1beta1\/tx.proto#L13)\\nfields to bytes on the client side. This adds an additional manual\\nstep when sending and signing transactions.\\n### Decision\\nThe following encoding scheme is to be used by other ADRs,\\nand in particular for `SignDoc` serialization.\\n","Decision":"The following encoding scheme is to be used by other ADRs,\\nand in particular for `SignDoc` serialization.\\n","tokens":284,"id":835}
{"File Name":"Wikibase\/0010-federated-properties-without-prefixed-ids.md","Context":"## Context\\nThe Federated Properties MVP introduces a [federatedPropertiesEnabled] setting which enables a Wikibase instance (local wiki) to read all Properties from a remote Wikibase (source wiki). A wiki with this setting enabled can only use federated Properties and disallows the creation or use of any local Properties. Switching from Federated Properties mode back to using local Properties or vice versa is not supported.\\nFederated Properties use API-based implementations of entity data access services. This ADR documents our decision regarding the dispatching mechanism for the respective services, i.e. how Wikibase determines whether to use a service for handling a local Property or a federated Property.\\n","Decision":"We decided that the second option outlined in the previous section is the better solution at this point in time, but we expect the decision to be revisited and likely superseded in the future. We are positive that most of the work we do now will be reusable if in the future we choose to go with the repository prefix dispatching approach. Any API-based service implementations can likely be reused without modification and only the surrounding wiring code will need adjustment. Choosing this simpler path leaves the decision of how to handle multiple Property sources with the responsible journey team.\\n","tokens":139,"id":1359}
{"File Name":"adr-tools\/0002-implement-as-shell-scripts.md","Context":"## Context\\nADRs are plain text files stored in a subdirectory of the project.\\nThe tool needs to create new files and apply small edits to\\nthe Status section of existing files.\\n","Decision":"The tool is implemented as shell scripts that use standard Unix\\ntools -- grep, sed, awk, etc.\\n","tokens":39,"id":3569}
{"File Name":"cloud-sdk-js\/0018-openapi-generator.md","Context":"## Context\\nWe are currently using the [Java-based OpenAPI generator CLI](https:\/\/www.npmjs.com\/package\/@openapitools\/openapi-generator-cli).\\nAs this generator is targeting many different programming languages in some points it does not fit our needs, e. g. handling of unique names or API files, which also differ between components.\\nIn our current approach we wrap the generated API with our own to fit those needs, but we realized, that we ended up writing many workarounds.\\nMany workarounds can currently only be covered by preprocessing\/manipulating the original service definition.\\nAt the moment, some API designs cannot be realized or are not worth investing into.\\nWe have observed that due to the size of the project, there are quite a lot of known bugs, that take a long time to solve or are not solved at all.\\nThe probably biggest pain point is that the used CLI needs a Java runtime.\\nTo run the CLI we have to download the given .jar file (~24MB) and make it part of our npm package, making it much bigger in size than usual.\\nThis is necessary as the file is not downloaded on installation but on command execution, causing other errors down the line (hurts SLC-29, race conditions in async batch execution).\\n","Decision":"Write our own generator, that is purely based on TypeScript and supports the API we need out of the box.\\n","tokens":269,"id":3626}
{"File Name":"talk\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":3368}
{"File Name":"smjs\/2018112801-encapsulate-sma-stack-usage.md","Context":"## Context\\nReal world assembly programming exposes the stack pointer and the base pointer, allowing assembly code to directly mess with them, for example changing the stack pointer regardless of how `push` and `pop` instructions have been used. Despite allowing wild optimizations, this is really error prone and, if used extensively, makes programs very obscure: for these reasons, we don't want to support such things in our SMA architecture.\\n","Decision":"To prevent code from messing up the stack, we'll avoid adding public stack pointer and base pointer registers. Rather, these two concepts will still be used in the implementation of the stack object, but clients won't be able to directly use them, since the only interfaces allowed will be the usual `push` and `pop`, which will take care of properly updating the stack pointer and the base pointer.\\nSince pointers are not directly usable from the code, we need to add a way to abstract the concept of stack frame: this is the role of the `pushFrame` and `popFrame` methods: the first one takes the return address, and takes care of pushing it on the stack at the right moment, while the second one will return the previously stored return address.\\nAnother problem with encapsulating frame handling, is that of passing procedure arguments to the stack. In regular assembly, one first pushes the arguments, then pushes the return address and the base pointer, and the jumps to the procedure. From inside the procedure, to get the passed arguments it's then necessary to first temporarily pop the base pointer and the return address, then pop the arguments, and then push back return address and base pointer.\\nSince we don't want to write all this boilerplate every time we call a procedure, we'll provide an alternative `call` instruction, supporting argument passing: this instruction will take the size of the pushed arguments (in bytes), and will automatically move the pushed arguments over the return address and the base pointer, after having pushed these, so that the procedure code will immediately find the arguments at the top of the stack, without the need to do any boilerplate.\\n","tokens":85,"id":3977}
{"File Name":"rfcs\/0000-backup-strategy.md","Context":"## Context\\n[context]: #context\\nAll contents of *kartevonmorgen.org* are stored in an SQLite database, i.e. a single file\\non the server. Currently this file is backed up manually, every few days or sometimes\\nweeks. The backup is stored both on the server and on private, external storage media.\\n","Decision":"[decision]: #decision\\n- A daily backup is generated automatically, compressed and stored on the server in a dedicated folder\\n- The backup folder is synchronized periodically (daily\/weekly?) via rsync or syncthing with external storage\\n- NTH: Old backups are selectively deleted, e.g. only te most recent 30 daily backups\\nare kept, for previous month only the last daily backup ist kept.\\n","tokens":71,"id":1876}
{"File Name":"terraform\/SSH-and-RDP.md","Context":"## Context\\nWe currently use RDP for Windows Server and SSH for Linux to connect to the servers to be migrated, when we have server administration tasks to perform. However, if EC2 instances are stateless, then we would not need to log into these servers, if they are sick, then then can be terminated.\\n","Decision":"RDP and SSH are allowed for all non-production EC2 istances, but are left blocked for production EC2 instances.\\n","tokens":66,"id":927}
{"File Name":"CICD-pipeline\/008-backwards-compatibility.md","Context":"## Context\\n1) We have commonly used existing Jenkins libraries\\n1) We want to be able to migrate a large number of projects with minimal effort\\n","Decision":"We provide a compatibility layer for the current library\\n","tokens":31,"id":2770}
{"File Name":"paas-team-manual\/ADR039-aiven-metrics-for-users.html.md","Context":"## Context\\nWe offer our users the following backing services through [Aiven](https:\/\/aiven.io):\\n- Elasticsearch\\n- InfluxDB (private beta)\\nWe want to make sure our users can view metrics for their Aiven backing services so that our users can:\\n- debug and respond to usage and service performance changes\\n- understand the operational characteristics of their applications and services\\n- make better capacity planning and budgeting decisions\\nAiven has service integrations which add extra functionality to an Aiven service. Service integrations are useful for:\\n- shipping logs to an Elasticsearch\/Rsyslog\\n- sending metrics to Datadog\\n- sending metrics to Aiven Postgres\/InfluxDB\\n- exposing metrics in Prometheus exposition format\\nWe currently run Prometheus for monitoring the platform, using the [Prometheus BOSH release](https:\/\/github.com\/bosh-prometheus\/prometheus-boshrelease) and have confidence and experience using it.\\nWe will need to think about Prometheus failover. If we load balance Prometheus without sticky sessions, the metrics Prometheus reports will be erratic, as different instances report different metrics.\\n","Decision":"We will use Prometheus to scrape Aiven-provided services.\\nWe will deploy new Prometheus in the Cloud Foundry BOSH deployment using the Prometheus BOSH release. This will reduce blast radius - tenant usage of metrics will not affect our ability to operate and monitor the platform using Prometheus.\\nWe will need to automate the following tasks:\\n1. Service discovery: make sure Prometheus has an updated list of Aiven services to scrape. We must colocate this automation with the Prometheus instance.\\n2. Service integration: make sure every eligible Aiven-provided service uses the Aiven service integration for Prometheus.\\n","tokens":229,"id":208}
{"File Name":"james-project\/0033-use-scala-in-event-sourcing-modules.md","Context":"## Context\\nAt the time being James use the scala programming language in some parts of its code base, particularily for implementing the Distributed Task Manager,\\nwhich uses the event sourcing modules.\\nThe module `event-store-memory` already uses Scala.\\n","Decision":"What is proposed here, is to convert in Scala the event sourcing modules.\\nThe modules concerned by this change are:\\n-  `event-sourcing-core`\\n-  `event-sourcing-pojo`\\n-  `event-store-api`\\n-  `event-store-cassandra`\\n","tokens":50,"id":2862}
{"File Name":"rfcs\/0000-make-events-searchable.md","Context":"## Context\\n[context]: #context\\nSince the the beginning of the year, https:\/\/pioneersofchange.org\/landkarte\/ is using the event fuction at Kvm. In Minsk Hackers developed the entry form for new events. As soon as this is online and in use by many, the map will be useless if you can not filter these events.\\nEvents do not only need to be searchable, but the results must also be filtered by bounding box and sorted by start\/end dates.\\n","Decision":"[decision]: #decision\\nWe will improve the backend-API so that events are searchable till the end of the year.\\nAt least for hashtags it has to work!\\n","tokens":106,"id":1878}
