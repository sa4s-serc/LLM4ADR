{"Context":"## Context\\nThe Identity, Metadata, and Storage currently have no way to be scaled. As it stands, they are only able to be ran as a single instance in a cloud specific way.\\nAn engineering goal for this product is to be cloud agnostic. This means that we need to be able to run on abstracted hardware provided by some Cloud or Host.\\n","Decision":"## Decision\nIn order to achieve One of the engineering goals for this project is to be cloud agnostic.\\n","Prediction":"## Decision\nWe will use [AWS Amplify](https:\/\/aws.amazon.com\/amp\/) as a library to abstract the cloud specific platform features. We will also use [AWS Amplify CLI](https:\/\/github.com\/aws-amplify\/awslabs-cli) to manage the platform.\\nWe will use the [AWS Amplify CLI](https:\/\/github.com\/aws-amplify\/awslabs-cli) to manage the platform.\\nWe will use the [AWS Amplify CLI](https:\/\/github.com\/aws-amplify\/awslabs-cli) to manage the platform.\\nWe will use the [AWS Amplify CLI](https:\/\/github.com\/aws-amplify\/awslabs-cli) to manage the platform.\\nWe will use the [AWS Amplify CLI](https:\/\/github.com\/aws-amplify\/awslabs-cli) to manage the platform.\\nWe will use the [AWS Amplify CLI](https:\/\/github.com\/aws-amplify\/awslabs-cli) to manage the platform.\\nWe will use the [AWS Amplify CLI](https:\/\/github.com\/aws-amplify\/awslabs-cli) to manage the platform.\\nWe will use the [AWS Amplify CLI](https:\/\/github.com\/aws-am"}
{"Context":"## Context\\nWe cannot design endpoints in a flat structure due to existing requirements around the Find service. By flat structure, we're referring to just having endpoints like:\\n- `\/providers`\\n- `\/courses`\\nInstead, we have to design endpoints in a nested structure scoped to their parent resources, ie:\\n- `\/recruitment_cycles\/:recruitment_cycle_year\/providers\/:provider_code`\\n- `\/recruitment_cycles\/:recruitment_cycle_year\/providers\/:provider_code\/courses\/:course_code`.\\nIf we rely on having single controllers to deal with resources which may be nested under other resources, we'll end up mixing concerns\/responsibilities of different contraints into one class which will make it difficult to maintain over the long term and less flexible to change. It also violates the single responsibility principle as we end up bloating the class.\\n","Decision":"## Decision\nWe have decided to go with option 2 based on the number of benefits we will gain given our current requirements.\\n","Prediction":"## Decision\nWe'll use a nested structure for endpoints to deal with a resource which may be nested under other resources.\\nThis structure will be scoped to the parent resources and will contain the following fields:\\n- `recruitment_cycle_year`: the ID of the current recruitment cycle year\\n- `provider_code`: the ID of the resource being managed\\n- `provider_code\/courses`: the list of courses and their ID\\n- `courses`: a collection of resources\\n- `courses\/courses`: the list of resources in the collection\\nWe'll use the following conventions for the nested structure:\\n- `resource` is a resource that has a relationship to the parent\\n- `parent_resource`: the parent resource which is the root of the structure\\n- `child_resources` are the resources that are related to the parent resource\\n- `child_resources\/courses` is the list of courses that are related to the parent resource\\n`parent_resources` and `child_resources` will be scoped to the parent resource.\\n`child_resources\/courses\/courses` is a list of courses for the child resources.\\n`child_resources\/courses\/courses\/:parent_resources\/courses` is a list of courses for the child resources which are related to the parent resource.\\n"}
{"Context":"## Context\\nReal world assembly programming exposes the stack pointer and the base pointer, allowing assembly code to directly mess with them, for example changing the stack pointer regardless of how `push` and `pop` instructions have been used. Despite allowing wild optimizations, this is really error prone and, if used extensively, makes programs very obscure: for these reasons, we don't want to support such things in our SMA architecture.\\n","Decision":"## Decision\nTo prevent code from messing up the stack, we'll avoid adding public stack pointer and base pointer registers. Rather, these two concepts will still be used in the implementation of the stack object, but clients won't be able to directly use them, since the only interfaces allowed will be the usual `push` and `pop`, which will take care of properly updating the stack pointer and the base pointer.\\nSince pointers are not directly usable from the code, we need to add a way to abstract the concept of stack frame: this is the role of the `pushFrame` and `popFrame` methods: the first one takes the return address, and takes care of pushing it on the stack at the right moment, while the second one will return the previously stored return address.\\nAnother problem with encapsulating frame handling, is that of passing procedure arguments to the stack. In regular assembly, one first pushes the arguments, then pushes the return address and the base pointer, and the jumps to the procedure. From inside the procedure, to get the passed arguments it's then necessary to first temporarily pop the base pointer and the return address, then pop the arguments, and then push back return address and base pointer.\\nSince we don't want to write all this boilerplate every time we call a procedure, we'll provide an alternative `call` instruction, supporting argument passing: this instruction will take the size of the pushed arguments (in bytes), and will automatically move the pushed arguments over the return address and the base pointer, after having pushed these, so that the procedure code will immediately find the arguments at the top of the stack, without the need to do any boilerplate.\\n","Prediction":"## Decision\nWe will not support the stack pointer and base pointer in SMA assembly code. Instead, we will use a stack-based address space for local state, and the base pointer will be used to point to the base of a function, which we will use for the return address.\\nThis will allow us to use \"normal\" stack-based address spaces for local state, and not have to worry about the stack pointer and base pointer being used as a way of pointing to the base of a function, which would be error prone.\\nThis will also allow us to use the base pointer to point to the base of a function, which is a bit more complex than just pointing to the base of a function, but is a bit more useful than using the base pointer as a pointer to the base of a function.\\nThis will also allow the base pointer to be used as a pointer to the base of a function, which is a bit more complex than just pointing to the base of a function, but is a bit more useful than using the base pointer as a pointer to the base of a function.\\nThis means that the base pointer can be used as a pointer to the base of a function, which is a bit more complex than just pointing to the base of a"}
{"Context":"## Context\\nContract-first approach enforces a definition of the API contract before implementation. This ensures that the API is\\nwell designed for specific use-cases, based on consumers point of view.\\nAdopting a contract-first approach forces to ensure that contract is not broken during implementation and evolution of\\nthe application.\\nWe need to ensure that the contract is well defined through a specification and implementation respects this specification.\\n","Decision":"## Decision\n[OpenAPI Specification](https:\/\/swagger.io\/specification\/) will be used to enforce contract-first approach.\\n","Prediction":"## Decision\nWe will use a contract-first approach to specify and implement the API.\\nWe will use a [specification-first approach](specification-first) to specify and implement the API.\\nWe will use a test-first approach to validate the contract specification.\\nWe will use a test-first approach to validate the contract specification.\\nWe will use a test-first approach to validate the contract specification.\\nWe will use a test-first approach to validate the contract specification.\\nWe will use a test-first approach to validate the contract specification.\\nWe will use a test-first approach to validate the contract specification.\\nWe will use a test-first approach to validate the contract specification.\\nWe will use a test-first approach to validate the contract specification.\\nWe will use a test-first approach to validate the contract specification.\\nWe will use a test-first approach to validate the contract specification.\\nWe will use a test-first approach to validate the contract specification.\\nWe will use a test-first approach to validate the contract specification.\\nWe will use a test-first approach to validate the contract specification.\\nWe will use a test-first approach to validate the contract specification.\\nWe will use a test-first approach to validate the contract specification.\\nWe will use a test-first"}
{"Context":"## Context and Problem Statement\\nKnow it before they do!\\nWe need a tool to discover, triage, and prioritize errors in real-time.\\n","Decision":"## Decision\nChosen option: `Sentry`, because it ranks higher in a community survey regarding our stack (Javascript). It's also much cheaper and offers the choice to be completely free if we self-host it.\\n","Prediction":"## Decision\nWe will use [Flaky](https:\/\/flaky.io\/) to discover and triage errors in our codebase.\\nFlaky provides a very simple and concise error reporting format that can be parsed by the tooling we already have in place, such as the `flaky` library.\\nIt also provides a simple schema that can be used to describe errors.\\nFlaky's goal is to be as simple as possible, so it can be used by both humans and machines.\\nFlaky's error reporting format is designed to be human-readable, so it can be more easily understood by humans than the verbose error messages that we currently use.\\nFlaky can also be used as a tool to help with code reviews.\\n### Positive Consequences <!-- optional -->\\n* We can quickly discover and identify errors in our codebase.\\n* We can use Flaky to triage errors and provide feedback to developers.\\n* We can use Flaky to help developers improve their code.\\n### Negative Consequences <!-- optional -->\\n* We can still use the verbose error messages that we currently use.\\n* We may need to change the error reporting format to a more human-readable format.\\n## Decision\nWe will use [Flaky"}
{"Context":"## Context\\nRead [Event Bus ADR](0037-eventbus.md) for context.\\nGiven several James servers, we need them to share a common EventBus.\\nThis:\\n- Ensures a better load balancing for `group mailbox listners`.\\n- Is required for correctness of notifications (like IMAP IDLE).\\n","Decision":"## Decision\nProvide a distributed implementation of the EventBus leveraging RabbitMQ.\\nEvents are emitted to a single Exchange.\\nEach group will have a corresponding queue, bound to the main exchange, with a default routing key. Each eventBus\\nwill consume this queue and execute the relevant listener, ensuring at least once execution at the cluster level.\\nRetries are managed via a dedicated exchange for each group: as we need to count retries, the message headers need to\\nbe altered and we cannot rely on rabbitMQ build in retries. Each time the execution fails locally, a new event is emitted\\nvia the dedicated exchange, and the original event is acknowledged.\\nEach eventBus will have a dedicated exclusive queue, bound to the main exchange with the `registrationKeys` used by local\\nnotification mailboxListeners (to only receive the corresponding subset of events). Errors are not retried for\\nnotifications, failures are not persisted within `DeadLetter`, achieving at most once event delivery.\\n","Prediction":"## Decision\nWe will use [Event Bus ADR](0037-eventbus.md) for the Event Bus.\\nWe will use [JGroups ADR](0038-jgroups.adr) for the JGroups.\\nWe will use [JGroups JMS ADR](0039-jgroups.jms.adr) for the JGroups JMS.\\nWe will use [JGroups JMS ADR](0039-jgroups.jms.adr) for the JGroups JMS.\\nWe will use [JGroups JMS ADR](0039-jgroups.jms.adr) for the JGroups JMS.\\nWe will use [JGroups JMS ADR](0039-jgroups.jms.adr) for the JGroups JMS.\\nWe will use [JGroups JMS ADR](0039-jgroups.jms.adr) for the JGroups JMS.\\nWe will use [JGroups JMS ADR](0039-jgroups.jms.adr) for the JGroups JMS.\\nWe will use [JGroups JMS ADR](0039-jgroups.jms.adr) for the JGroups JMS.\\nWe will use [JGroups JMS ADR](0039-jgroups.jms.adr) for the"}
{"Context":"## Context\\nThe verify-service-provider will be deployed to a number of relying parties.\\nWhich relying party is using which version could potentially become difficult\\nto keep track of.\\nTo make it as easy as possible for us to manage this we'd like the\\nverify-service-provider to report its version in some way.\\nBecause the verify-service-provider is not intended to be accessible to the\\ninternet we can't simply expose an endpoint that reports the version number.\\nAlso, since the SAML messages go via the browser we can't use a custom HTTP\\nheader.\\nThere's also a concern about the security implications of reporting a version\\nnumber in cleartext.\\nWe considered a couple of options:\\n- Requesting metadata from Verify with a custom user-agent string\\n- Sending the version in an unencrypted saml extension\\n- Sending the version in an encrypted saml extension\\n","Decision":"## Decision\nWe decided to send the version number in the SAML AuthnRequests as an encrypted\\nSAML extension. The XML will look roughly like this:\\n```\\n<saml:AuthnRequest>\\n<saml:Issuer>...<\/saml:Issuer>\\n<saml:Signature>...<\/saml:Signature>\\n<saml:Extensions>\\n<saml:EncryptedAttribute>...<\/saml:EncryptedAttribute>\\n<\/saml:Extensions>\\n<\/saml:AuthnRequest>\\n```\\nOnce decrypted, the Attribute in the Extensions will look like:\\n```\\n<saml:Attribute Name=\"Versions\">\\n<saml:AttributeValue xsi:type=\"metrics:VersionsType\">\\n<metrics:ApplicationVersion>3.4.1<\/metrics:ApplicationVersion>\\n<\/saml:AttributeValue>\\n<\/saml:Attribute>\\n```\\n","Prediction":"## Decision\nWe've decided to send the version in the saml extension.\\nThis allows the client to decide whether to trust the extension or not.\\nThis also allows us to remove the extension from the saml message in the\\nfuture.\\nThis also means that the client can change the version in a future\\nupdate without affecting the client.\\nThis also means that the client can remove the extension from the saml\\nmessage in the future without affecting the version.\\nThis also means that the client can change the version in a future\\nupdate without affecting the version.\\nThis also means that the client can remove the version from the\\nsaml message in the future without affecting the version.\\nThis also means that the client can remove the version in the\\nfuture.\\nThis also means that the client can remove the version in the\\nfuture.\\nThis also means that the client can remove the version in the\\nfuture.\\nThis also means that the client can remove the version in the\\nfuture.\\nThis also means that the client can remove the version in the\\nfuture.\\nThis also means that the client can remove the version in the\\nfuture.\\nThis also means that the client can remove the version in the\\nfuture.\\nThis"}
{"Context":"## Context\\nI was once a web developer when jQuery and CSS 2 were the new hotness. I did a lot of web 1.0, single-server work. Then I spent many years building iOS apps. These days I'm interested to be more of a generalist.\\nIn the fall of 2017, I learned some TypeScript and thought it was great. JavaScript is JavaScript, and I like strong types, and TypeScript makes that happen, so it was an instant favorite.\\nI decided to learn some more web technologies. For browser apps that means React and Redux, which is commonly used with Node, NPM, and Webpack. Learning the common stuff is a good way to get my bearings in the field.\\nWhen the idea for this project came to mind, it was a good chance to learn that web stack and catch up my knowledge.\\n","Decision":"## Decision\nI decided to build this site in TypeScript, with React and Redux, on Node, NPM, and Webpack.\\n","Prediction":"## Decision\nI will learn the web stack. I will learn React, Redux, and TypeScript.\\nI will learn Node.js and NPM. I will learn to use the command line.\\nI will learn to use cloud providers. I will use AWS, GCP, and Azure.\\nI will learn to build web apps. I will use React Router, Redux, and Styled Components.\\nI will learn to build serverless apps. I will use AWS Lambda and API Gateway.\\nI will learn to use serverless services. I will use AWS Lambda, API Gateway, and RDS.\\nI will learn to build microservices. I will use AWS Lambda, API Gateway, and RDS.\\nI will learn to use serverless computing. I will use AWS Lambda, API Gateway, and RDS.\\nI will learn to use serverless databases. I will use AWS DynamoDB.\\nI will learn to use serverless storage. I will use AWS S3 and Glacier.\\nI will learn to use serverless load balancing. I will use AWS Route53 and ELB.\\nI will learn to use serverless authentication. I will use AWS Cognito and IAM.\\nI will learn to use serverless functions. I will use"}
{"Context":"## Context\\nThe Modernisation Platform will be used by a large group of people across the Justice Digital and Technology estate, and each person will need their access to the Modernisation Platform managed.\\n","Decision":"## Decision\nRather than managing the administrative burden of the Joiners, Movers and Leavers (JML) process ourselves, we can use AWS SSO with SCIM automated provisioning to allow access management through an Identity Provider that is already managed within Ministry of Justice, such as G-Suite. AWS SSO provides some advantages, and some disadvantages, over [IAM Federated Access](0002-use-iam-federated-access.md), as listed below.\\nWe can, if we wish, still utilise a different service (e.g. GitHub) as a IdP through Auth0, whilst SCIM provisioning with a different IdP (e.g. G-Suite).\\nWe decided to use AWS SSO in favour of [IAM Federated Access](0002-use-iam-federated-access.md) to allow us to centrally manage identities across the Ministry of Justice at the organisational level rather than at a team level.\\nA further benefit of this is that AWS SSO can be used across all AWS accounts, not just ones provisioned within the Modernisation Platform, as long as the AWS account is part of the AWS organisation.\\n","Prediction":"## Decision\nAn access management system will be used to manage access to the Modernisation Platform. The system will use the following guidelines to manage the permissions on the platform:\\n* User must have a single consent to access the platform\\n* A single consent will be used for all users of the platform\\n* A single consent will be used for all non-technical users of the platform\\n* A single consent will be used for all people in the organisation that need to use the platform\\n* A single consent will be used for all people in the organisation that need to use the non-technical side of the platform\\n* A single consent will be used for all people in the organisation that need to use the admin side of the platform\\n* A single consent will be used for all people in the organisation that need to access the admin side of the platform\\n* A single consent will be used for all people in the organisation that need to access the non-technical side of the platform\\n* A single consent will be used for all people in the organisation that need to access the admin side of the platform\\n* A single consent will be used for all people in the organisation that need to access the admin side of the non-technical platform\\n* A single consent will be used"}
{"Context":"## Context and Problem Statement\\nAcceptance tests are embedded into game server application.\\nShould they be written one layer above (UI)?\\n## Decision Drivers\\n* Acceptance tests should be meaningful for end users\\n","Decision":"## Decision\n* Acceptance tests should be meaningful for end users\\nRedefine current acceptance tests and treat them like _component_ tests according to [this definition](https:\/\/www.simpleorientedarchitecture.com\/defining-test-boundaries\/).\\n* Good, because it promotes different tests for different audiences\\n* Good, because it sets a clear terminology\\n* Good, because end users don't care about layers below UI\\n","Prediction":"## Decision\n* Acceptance tests should be meaningful for end users\\nChosen option: \"Test Client\", because it is the only option which keeps the test client and the game server in sync. The game server is not aware of the client's state. This can be done by using a `@Before` annotation to inject the client's state into the test class. The test class can then use the client's state to reset the game server's state. This way, the game server only receives the state from the client, which is a much cleaner approach.\\n### Positive Consequences\\n* Test client is independent from game server and can be used without changes to the game server's code.\\n* Test client can be used in integration tests.\\n### Negative Consequences\\n* Test client is not aware of the game server's state.\\n* Test client can be used without changes to the game server's code.\\n* The game server can not be restarted.\\n* The game server is not aware of the client's state.\\n* The game server can not be restarted.\\n* The game server is not aware of the client's state.\\n* The game server can not be restarted.\\n* The game server can not be restarted.\\n"}
{"Context":"## Context and Problem Statement\\nFrequently occurring usage patterns of play-frontend-hmrc components require repetitious boilerplate\\ninvolving excessively nested case class instantiations. This has driven many teams to create wrappers to\\nsimplify the construction of components, which are then copied and pasted from service to service.\\nFor example, in order to add a legend to a GovukRadio the following boilerplate is required:\\n```scala\\nSome(Fieldset(\\nlegend = Some(Legend(\\ncontent = Text(\"message.key.for.legend\"),\\nclasses = \"govuk-fieldset__legend--l\",\\nisPageHeading = true\\n))\\n))\\n```\\nIn the above example, the only thing that changes between instantiations is the `message.key.for.legend`, everything\\nelse remains the same.\\nShould we solve this problem through the introduction of implicit conversions that will automatically wrap simple\\nobjects such as Strings with the additional boilerplate necessary for commonly occurring cases?\\n## Decision Drivers\\n* The need for API consistency in play-frontend-hmrc.\\n* The mixed sentiments towards implicit conversions in the Scala community and slight risk the feature may be removed\\nentirely in a future Scala version:\\n* See https:\/\/www.rallyhealth.com\/coding\/implicit-implications-part-2-implicit-conversions,\\nhttps:\/\/contributors.scala-lang.org\/t\/can-we-wean-scala-off-implicit-conversions\/4388\\n* The risk of unintended side effects if these conversions are added unilaterally\\nwithout service developers needing to specifically opt in to them.\\n* The repeated invocation of the `Messages` apply method to populate component content parameters, e.g.\\nHint, Key, Label, Legend.\\n* The difficulty of debugging code involving implicit conversions without turning on advanced IDE features that\\nshow implicit hints.\\n","Decision":"## Decision\n* The need for API consistency in play-frontend-hmrc.\\n* The mixed sentiments towards implicit conversions in the Scala community and slight risk the feature may be removed\\nentirely in a future Scala version:\\n* See https:\/\/www.rallyhealth.com\/coding\/implicit-implications-part-2-implicit-conversions,\\nhttps:\/\/contributors.scala-lang.org\/t\/can-we-wean-scala-off-implicit-conversions\/4388\\n* The risk of unintended side effects if these conversions are added unilaterally\\nwithout service developers needing to specifically opt in to them.\\n* The repeated invocation of the `Messages` apply method to populate component content parameters, e.g.\\nHint, Key, Label, Legend.\\n* The difficulty of debugging code involving implicit conversions without turning on advanced IDE features that\\nshow implicit hints.\\nChosen option: Option 1, because it solves the problem, is consistent with how we have been doing things, maintains\\nthe internal consistency of play-frontend-hmrc and has less risk of breaking services.\\n","Prediction":"## Decision\n* The need for API consistency in play-frontend-hmrc.\\n* The mixed sentiments towards implicit conversions in the Scala community and slight risk the feature may be removed\\nentirely in a future Scala version:\\n* See https:\/\/www.rallyhealth.com\/coding\/implicit-implications-part-2-implicit-conversions,\\nhttps:\/\/contributors.scala-lang.org\/t\/can-we-wean-scala-off-implicit-conversions\/4388\\n* The risk of unintended side effects if these conversions are added unilaterally\\nwithout service developers needing to specifically opt in to them.\\n* The repeated invocation of the `Messages` apply method to populate component content parameters, e.g.\\nHint, Key, Label, Legend.\\n* The difficulty of debugging code involving implicit conversions without turning on advanced IDE features that\\nshow implicit hints.\\nChosen option: \"Use implicit conversions with implicit parameters\", because it is consistent with the\\ncurrent practice of the rest of the components in the play-frontend-hmrc.\\n### Positive Consequences\\n* Consistent with the rest of the components, the boilerplate required to populate component content parameters\\nis no longer invoked.\\n* The `messages` apply method can be removed. The"}
{"Context":"## Context\\nThe current \"naive\" IBC Relayer strategy currently establishes a single predetermined IBC channel atop a single connection between two clients (each potentially of a different chain).  This strategy then detects packets to be relayed by watching for `send_packet` and `recv_packet` events matching that channel, and sends the necessary transactions to relay those packets.\\nWe wish to expand this \"naive\" strategy to a \"passive\" one which detects and relays both channel handshake messages and packets on a given connection, without the need to know each channel in advance of relaying it.\\nIn order to accomplish this, we propose adding more comprehensive events to expose channel metadata for each transaction sent from the `x\/ibc\/core\/04-channel\/keeper\/handshake.go` and `x\/ibc\/core\/04-channel\/keeper\/packet.go` modules.\\nHere is an example of what would be in `ChanOpenInit`:\\n```go\\nconst (\\nEventTypeChannelMeta = \"channel_meta\"\\nAttributeKeyAction = \"action\"\\nAttributeKeyHops = \"hops\"\\nAttributeKeyOrder = \"order\"\\nAttributeKeySrcPort = \"src_port\"\\nAttributeKeySrcChannel = \"src_channel\"\\nAttributeKeySrcVersion = \"src_version\"\\nAttributeKeyDstPort = \"dst_port\"\\nAttributeKeyDstChannel = \"dst_channel\"\\nAttributeKeyDstVersion = \"dst_version\"\\n)\\n\/\/ ...\\n\/\/ Emit Event with Channel metadata for the relayer to pick up and\\n\/\/ relay to the other chain\\n\/\/ This appears immediately before the successful return statement.\\nctx.EventManager().EmitEvents(sdk.Events{\\nsdk.NewEvent(\\ntypes.EventTypeChannelMeta,\\nsdk.NewAttribute(types.AttributeKeyAction, \"open_init\"),\\nsdk.NewAttribute(types.AttributeKeySrcConnection, connectionHops[0]),\\nsdk.NewAttribute(types.AttributeKeyHops, strings.Join(connectionHops, \",\")),\\nsdk.NewAttribute(types.AttributeKeyOrder, order.String()),\\nsdk.NewAttribute(types.AttributeKeySrcPort, portID),\\nsdk.NewAttribute(types.AttributeKeySrcChannel, channelID),\\nsdk.NewAttribute(types.AttributeKeySrcVersion, version),\\nsdk.NewAttribute(types.AttributeKeyDstPort, counterparty.GetPortID()),\\nsdk.NewAttribute(types.AttributeKeyDstChannel, counterparty.GetChannelID()),\\n\/\/ The destination version is not yet known, but a value is necessary to pad\\n\/\/ the event attribute offsets\\nsdk.NewAttribute(types.AttributeKeyDstVersion, \"\"),\\n),\\n})\\n```\\nThese metadata events capture all the \"header\" information needed to route IBC channel handshake transactions without requiring the client to query any data except that of the connection ID that it is willing to relay.  It is intended that `channel_meta.src_connection` is the only event key that needs to be indexed for a passive relayer to function.\\n### Handling Channel Open Attempts\\nIn the case of the passive relayer, when one chain sends a `ChanOpenInit`, the relayer should inform the other chain of this open attempt and allow that chain to decide how (and if) it continues the handshake.  Once both chains have actively approved the channel opening, then the rest of the handshake can happen as it does with the current \"naive\" relayer.\\nTo implement this behavior, we propose replacing the `cbs.OnChanOpenTry` callback with a new `cbs.OnAttemptChanOpenTry` callback which explicitly handles the `MsgChannelOpenTry`, usually by resulting in a call to `keeper.ChanOpenTry`.  The typical implementation, in `x\/ibc-transfer\/module.go` would be compatible with the current \"naive\" relayer, as follows:\\n```go\\nfunc (am AppModule) OnAttemptChanOpenTry(\\nctx sdk.Context,\\nchanKeeper channel.Keeper,\\nportCap *capability.Capability,\\nmsg channel.MsgChannelOpenTry,\\n) (*sdk.Result, error) {\\n\/\/ Require portID is the portID transfer module is bound to\\nboundPort := am.keeper.GetPort(ctx)\\nif boundPort != msg.PortID {\\nreturn nil, sdkerrors.Wrapf(porttypes.ErrInvalidPort, \"invalid port: %s, expected %s\", msg.PortID, boundPort)\\n}\\n\/\/ BEGIN NEW CODE\\n\/\/ Assert our protocol version, overriding the relayer's suggestion.\\nmsg.Version = types.Version\\n\/\/ Continue the ChanOpenTry.\\nres, chanCap, err := channel.HandleMsgChannelOpenTry(ctx, chanKeeper, portCap, msg)\\nif err != nil {\\nreturn nil, err\\n}\\n\/\/ END OF NEW CODE\\n\/\/ ... the rest of the callback is similar to the existing OnChanOpenTry\\n\/\/ but uses msg.* directly.\\n```\\nHere is how this callback would be used, in the implementation of `x\/ibc\/handler.go`:\\n```go\\n\/\/ ...\\ncase channel.MsgChannelOpenTry:\\n\/\/ Lookup module by port capability\\nmodule, portCap, err := k.PortKeeper.LookupModuleByPort(ctx, msg.PortID)\\nif err != nil {\\nreturn nil, sdkerrors.Wrap(err, \"could not retrieve module from port-id\")\\n}\\n\/\/ Retrieve callbacks from router\\ncbs, ok := k.Router.GetRoute(module)\\nif !ok {\\nreturn nil, sdkerrors.Wrapf(port.ErrInvalidRoute, \"route not found to module: %s\", module)\\n}\\n\/\/ Delegate to the module's OnAttemptChanOpenTry.\\nreturn cbs.OnAttemptChanOpenTry(ctx, k.ChannelKeeper, portCap, msg)\\n```\\nThe reason we do not have a more structured interaction between `x\/ibc\/handler.go` and the port's module (to explicitly negotiate versions, etc) is that we do not wish to constrain the app module to have to finish handling the `MsgChannelOpenTry` during this transaction or even this block.\\n","Decision":"## Decision\n- Expose events to allow \"passive\" connection relayers.\\n- Enable application-initiated channels via such passive relayers.\\n- Allow port modules to control how to handle open-try messages.\\n","Prediction":"## Decision\nWe will add a new event to the `x\/ibc\/core\/04-channel\/keeper\/handshake.go` and `x\/ibc\/core\/04-channel\/keeper\/packet.go` modules that will be used to monitor channel open attempts.  This event will only be emitted once per channel, and will be emitted at the end of the channel's `OpenInit` event.  The `chanKeeper` field in the event will be the channel that the relayer was opened on.\\nWe will also add an additional event that will be emitted at each attempt to open the channel, and will be emitted at the end of the channel's `OpenTry` event.  This event will only be emitted once per channel, and will be emitted at the end of the channel's `Open` event.  The `chanKeeper` field in the event will be the channel that the relayer was opened on.\\nWe will also add an additional event that will be emitted at each attempt to open the channel, and will be emitted at the end of the channel's `Open` event.  This event will only be emitted once per channel, and will be emitted at the end of the channel's `Open` event.  The `"}
{"Context":"## Context\\nOnly a subset of all scratch functions has been implemented.\\nIt might make sense to ignore unsupported functions.\\nThis would make sense when encountering sound\/image related functions.\\nSo the user could still use these functions when using the web version and not using the offline version.\\nOn the other hand the user will be surprised that some functions won't actually work.\\nFunctions silently being ignored would make troubleshooting harder.\\n","Decision":"## Decision\nUnknown scratch functions will cause the conversion to C to fail.\\n","Prediction":"## Decision\nWe will ignore unsupported functions.\\nThis doesn't mean that the user can't use these functions, they will just be silently ignored.\\nThis will make troubleshooting easier.\\nThis will make the user feel like they are using the web version.\\nThis will make the user feel like the state of the scratch functions is up to date.\\nThis will make the user feel more comfortable using the web version.\\nThis will make the user feel more confident about using the web version.\\nThis will make the user feel more confident about using the offline version.\\nThis will make the user feel more confident about using the offline version.\\nThis will make the user feel more confident about using the offline version.\\nThis will make the user feel more confident about using the offline version.\\nThis will make the user feel more confident about using the offline version.\\nThis will make the user feel more confident about using the offline version.\\nThis will make the user feel more confident about using the offline version.\\nThis will make the user feel more confident about using the offline version.\\nThis will make the user feel more confident about using the offline version.\\nThis will make the user feel more confident about using the offline version.\\nThis will make the user"}
{"Context":"## Context\\nThe choice of entities, i.e. what categories of information we want to anonymise and the specificity of these categories, will greatly impact the performance of our model. Additionally, time constraints mean that there is an upper limit to how many entities can be included. As such, we concluded that those entities which fulfill at least two of the following three categories should be included:\\n* It is either directly identifying or a close proxy (e.g. names, phone numbers, etc.)\\n* It is a so-called \u201cspecial categories\u201d of information (e.g. medical information)\\n* It is present in the data in non-trivial quantities\\n","Decision":"## Decision\nWe have chosen the following NER entities:\\n* ORG (Organisation)\\n* LOC (Location)\\n* PER (Person)\\n* FNR (Personal number)\\n* MONEY\\n* DATE_TIME (Dates, time of day, name of day, and name of month)\\n* MEDICAL_CONDITIONS\\nEntities that will be left purely to RegEx are:\\n* NAV_YTELSE and NAV_OFFICE\\n* AGE\\n* TLF (Telephone number)\\n* BACC (Bank account number)\\nWe believe this list strikes the right balance between performance (fewer entities are better) and coverage.\\n","Prediction":"## Decision\nWe will use the following entities for each category:\\n* Id: unique identifier for the entity\\n* Name: The name of the entity\\n* Phone: The phone number of the entity\\n* Email: The email address of the entity\\n* Location: The location of the entity\\n* Age: The age of the entity\\n* Sex: The sex of the entity\\n* Occupation: The occupation of the entity\\n* Income: The income of the entity\\n* Education: The education of the entity\\n* Income Type: The income type of the entity\\n* Data Type: The data type of the entity\\n* Data Source: The data source of the entity\\n* Data Type: The data type of the entity\\n* Metadata: The metadata of the entity\\n* Label: The label of the entity\\n* Annotation: The annotation of the entity\\n* Format: The format of the entity\\n* Field: The field of the entity\\n* Field Value: The field value of the entity\\n* Field Type: The field type of the entity\\n* Field Source: The field source of the entity\\n* Field Value Source: The field source of the entity\\n* Field Type Source: The field type of the entity\\n* Field Name Source"}
{"Context":"## Context\\nDynamoDB provides the ability to supply additional attributes, a local\\nsecondary index and a global secondary index. These additional attributes\\nconsumed by the DynamoDB AWS Provider as maps\/lists.\\nIn addtion to the consumption as maps\/lists there are additional requirements\\nthat the range\/hash keys be added to the additional attributes if declared. They\\nare not added if undeclared.\\n","Decision":"## Decision\nThe module will use a `null_resource` to manage the secondary indexes. Creating\\nthem from the existences of appropriate maps\/lists.\\nProperties related to these resources will consume a list of maps as input.\\nThese will them be mapped to the appropriate resource within in the module.\\nThe range\/hash key will be added automatically to the additional attributes by\\nthe module, reducing the load on the consumer with implementation detail.\\n","Prediction":"## Decision\nThe AWS Provider will create the additional attributes as a separate\\ntable and map the key to the DynamoDB range\/hash key.  The DynamoDB\\nAWS Provider will then provide the additional attributes to the DynamoDB\\nAWS Provider as a list of DynamoDB Range Hash Keys.\\nThis will allow the DynamoDB AWS Provider to provide the additional attributes\\nas a separate table and will not impact the DynamoDB AWS Provider's\\nability to provide the additional attributes as a map\/list.\\nThis decision is made based on the following:\\n1. The DynamoDB AWS Provider does not currently have the ability to add\\nthe additional attributes as a map\/list.\\n2. The DynamoDB AWS Provider will not impact the DynamoDB AWS Provider's\\nability to provide the additional attributes as a map\/list.\\n3. The DynamoDB AWS Provider will be more efficient than providing the\\nadditional attributes as a map\/list as they will be a separate table.\\n4. The DynamoDB AWS Provider can be more easily provisioned in a\\ncloud provider environment.\\n5. The DynamoDB AWS Provider is more efficient than providing the\\nadditional attributes as a map\/list as they will be a separate table.\\n6. The DynamoDB AWS Provider will be more efficient than"}
{"Context":"## Context\\nDo we need a Loyalty Program component?\\n","Decision":"## Decision\nIt doesn't look like there is a requirement for a need of a Loyalty Program component.  It's decided we will not have one for now.\\n","Prediction":"## Decision\nWe will use [Loyalty](https:\/\/github.com\/jeffreys\/loyalty) as our loyalty component.\\n### Components\\nWe will use the [Loyalty](https:\/\/github.com\/jeffreys\/loyalty) library for our loyalty program.\\n### Design\\nWe will use the [Loyalty](https:\/\/github.com\/jeffreys\/loyalty) library to create our loyalty program.\\n### Implementation\\nWe will create a new @angular\/core class to represent our loyalty program.\\n### Implementation Details\\nWe will create a new `Loyalty` class in our `app.module.ts` and add it to our `app.module`.\\n### Usage\\nWe will create a new `Loyalty` class in our `app.module.ts` and add it to our `app.module`.\\n### Example\\nFor an example of how to use this library, please see the [Loyalty](https:\/\/github.com\/jeffreys\/loyalty) library's README.\\n### Licensing\\nLicensing for this component is handled by the `Loyalty` library.\\nLicensing for this component is handled by the [Loyalty](https:\/\/github.com"}
{"Context":"## Context\\nWhen building monoliths or microservices that contain non-trivial business logic, we want to have a high-level component architecture pattern that natively leverages a strategic Domain Driven\\nDesign (DDD) concepts like ubiquitous language and bounded context. It should also support or allow, at least, the usage of application design architecture patterns like hexagonal architecture.\\nAt the lower technical level, component architecture pattern should provide native support for tactical DDD concepts like aggregates, entities, value objects, and events.\\nWhen building microservices, the chosen component architecture pattern should natively support asynchronous communication through events.\\nIt would be ideal to have all of these in a single coherent open-source framework and platform.\\nFor anemic CRUD style applications, more traditional approaches, like layered architecture, would be just fine.\\n### Architectural Context\\n* System (`klokwrk-project`)\\n","Decision":"## Decision\n**We will use the Axon framework and Axon Server as a mature open-source implementation of the CQRS\/ES component architecture pattern.**\\nEspecially as one of the major `klokwrk-project` goals is to demonstrate the creation of complex CQRS\/ES microservice applications. We can use more traditional approaches like classic layered\\narchitecture and anemic model for anemic CRUD style applications.\\n","Prediction":"## Decision\nThe architectural decision is to use a component architecture pattern that is built on top of the existing `klokwrk` framework.\\nThe `klokwrk` framework is already used by `klokwrk` and is already well-established and well-documented.\\nWe also want to showcase this approach in a real-world application, and it is a good fit for this project.\\n* `klokwrk` Framework\\nThe `klokwrk` framework is an open-source framework for building microservices and monolithic applications that is used by `klokwrk`.\\nIt is a strategy and design pattern that is based on the domain-driven design approach.\\n* Component Architecture Pattern\\nThe component architecture pattern is an architectural pattern that is based on the component architecture pattern. It is a pattern that is used to build component-based applications or microservices that are built on top of a component architecture pattern.\\n* `klokwrk` Architecture\\nThe `klokwrk` architecture is an architectural pattern that is based on the `klokwrk` framework. It is a pattern that is used to build monolith-based applications. It is a pattern that is used to build monolithic-based applications that have common"}
{"Context":"## Context\\nTrade show events are stored in many places across the TAP team.\\n- At the beginning of the year the Glasgow operations team validates the list of events in a spreadsheet stored on sharepoint.\\n- Events are uploaded to a hosted service called Aventri so that they are visible on great.gov.\\n- Any updates to event details are communicated via email.\\nAs of 2020-12-17 there are still discussions ongoing about how to manage trade show event data centrally within DIT\\nand move away from the spreadsheet being the golden source of truth. One idea is to use digital workspace for this.\\nHowever as of right now no decision on this has been made.\\nIn the meantime the our services still need a way of displaying trade show events to our users to select and view.\\n","Decision":"## Decision\nWithout a firm decision on where events will ultimately be centrally stored we have decided to use a fixture file\\nto load events into our backoffice as a temporary solution.\\nThe fixture file exists at `backoffice\/web\/trade_events\/fixtures\/trade_events.json` and is automatically loaded on\\nstartup.\\n","Prediction":"## Decision\nWe will use a hosted service called Aventri to store and manage trade show event data.\\nAventri will use the DIT service account to access the data in our hosted service.\\nWe will use Aventri's REST API to retrieve and update data. This will be a one-way API call.\\nWe will use the Aventri web interface to view and manage trade show events.\\nWe will use the Aventri web interface to create and manage trade show events.\\nWe will use the Aventri web interface to view trade show events in a list view.\\nWe will use the Aventri web interface to view trade show events in a calendar view.\\nWe will use the Aventri web interface to view trade show events in a map view.\\nWe will use the Aventri web interface to view trade show event details.\\nWe will use the Aventri web interface to create and manage trade show event details.\\nWe will use the Aventri web interface to view trade show event details in a list view.\\nWe will use the Aventri web interface to view trade show event details in a calendar view.\\nWe will use the Aventri web interface to view trade show event"}
{"Context":"## Context and Problem Statement\\nWe want a monorepo tool to help us to scale development up for multiple projects and teams. It should not be too much in the way, but help us manage code, dependencies and CI\/CD.\\n## Decision Drivers\\n* Low complexity and overhead in development.\\n* Fit for our stack.\\n* Optimize CI\/CD with dependency graphs and\/or caching.\\n* Flexible.\\n","Decision":"## Decision\n* Low complexity and overhead in development.\\n* Fit for our stack.\\n* Optimize CI\/CD with dependency graphs and\/or caching.\\n* Flexible.\\nChosen option: \"Nx\", because:\\n* It's specially designed around our stack (TypeScript, React, Node.JS, NPM, ESLint, Prettier, Cypress, Jest, NextJS).\\n* It's relatively easy to learn with focused documentation.\\n* It has schematics to generate apps, libraries and components that includes all of our tools.\\n* It is opinionated, which gives us a good base to start developing faster. Many things can still be configured or extended.\\n","Prediction":"## Decision\n* Low complexity and overhead in development.\\n* Fit for our stack.\\n* Optimize CI\/CD with dependency graphs and\/or caching.\\n* Flexible.\\nChosen option: \"Elasticsearch\", because it fits our stack and is easy to use.\\n### Positive Consequences\\n* It is easy to use and has good documentation.\\n* It is easy to setup and to scale.\\n* It is used in other projects.\\n### Negative Consequences\\n* It is hard to use for large monorepos.\\n* It has some limitations in CI\/CD.\\n* It is not free.\\n* It is not free for open source projects.\\n* It has a high learning curve.\\n* It is not free.\\n### Impact on Us\\n* We will have to use it as a monorepo tool.\\n* It is a bit more complex than our current tool.\\n* It has a lot of limitations in CI\/CD.\\n* It has a high learning curve.\\n* We will have to use it as a monorepo tool.\\n* It is a bit more complex than our current tool.\\n* It has a lot of limitations in CI\/CD.\\n* It"}
{"Context":"## Context\\nWe want to build a web-based application in Node. Express is a minimal and\\nflexible framework that sets up a lot basic configuration for us like routing\\nand middleware, and makes it easy to incoporate the GOV UK design system UI\\ncomponents.\\n","Decision":"## Decision\nWe will use Express as the basis for this project.\\n","Prediction":"## Decision\nWe will use Express as the server framework.\\nWe will use the Go design system to build the application UI.\\nWe will use the GOV UK design system UI components to build the application UI.\\nWe will use the Go design system to build the application UI.\\nWe will use the Go design system to build the application UI.\\nWe will use the Go design system to build the application UI.\\nWe will use the Go design system to build the application UI.\\nWe will use the Go design system to build the application UI.\\nWe will use the Go design system to build the application UI.\\nWe will use the Go design system to build the application UI.\\nWe will use the Go design system to build the application UI.\\nWe will use the Go design system to build the application UI.\\nWe will use the Go design system to build the application UI.\\nWe will use the Go design system to build the application UI.\\nWe will use the Go design system to build the application UI.\\nWe will use the Go design system to build the application UI.\\nWe will use the Go design system to build the application UI.\\nWe will use the Go design system to build the application UI.\\nWe"}
{"Context":"## Context\\nA large number of test files will be created  during the project. These should be stored in\\na consistent location across the C++ projects.\\nA number of options are available:\\n* filing alongside the source files\\n* in a `tests\/` folder within the source tree\\n* in a `.Tests` project created alongside the application source\\n","Decision":"## Decision\nWe will create a parallel directory structure of test projects for each C++ project.\\n``` file listing\\nservice\\\\nfunctionOne\\\\nCMakeLists.txt\\n...\\nfunctionTwo\\\\nCMakeLists.txt\\n...\\ntests\\\\nfunctionOne.Tests\\\\nCMakeLists.txt\\n...\\nfunctionTwo.Tests\\\\nCMakeLists.txt\\n...\\n```\\nThe test projects will include the associated function project as an explicit reference e.g.\\n```cmake\\nadd_executable(\"functionOne.test\" ${TEST_SRC_FILES} ${TEST_HDR_FILES} ${SRC_FILES} ${HDR_FILES})\\ntarget_include_directories(\"functionOne.test\" PRIVATE \"${Horace_ROOT}\/_LowLevelCode\/cpp\")\\ntarget_link_libraries(\"functionOne.test\" gtest_main)\\n```\\nThis enables the test artifacts and dependencies to be simply excluded from system components that  will be deployed. The separation will also provide a clear cognitive separation between `function` and `function.Test` and support further extension to `function.IntegrationTest`.\\n","Prediction":"## Decision\nThe tests will be stored in a `.Tests` project.\\nThis will be created alongside the source files and will be used to run tests.\\nIt will also be used to provide a consistent test environment across the project.\\nThis will be created using the `cppunit` test runner.\\nThe tests will be written in C++ and will be stored in a single directory.\\nThis directory will be used to store all tests that are run during the project.\\nIt will also be used to store the generated test suite.\\nThis directory will be used to store the generated test suite.\\nIt will also be used to store the generated test suite.\\nThis directory will be used to store the generated test suite.\\nThis directory will be used to store the generated test suite.\\nThis directory will be used to store the generated test suite.\\nThis directory will be used to store the generated test suite.\\nThis directory will be used to store the generated test suite.\\nThis directory will be used to store the generated test suite.\\nThis directory will be used to store the generated test suite.\\nThis directory will be used to store the generated test suite.\\nThis directory will be used to store the generated test suite.\\nThis directory"}
{"Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","Prediction":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\nWe will use [Confluence](https:\/\/www.confluence.atlassian.com\/display\/DOC\/Architecture+Decision+Record) as our source of truth.\\nWe will use the [RESTful API](https:\/\/www.confluence.atlassian.com\/display\/DOC\/REST+API) for storing our records.\\nWe will use the [JIRA](https:\/\/confluence.atlassian.com\/display\/DOC\/Architecture+Decision+Record) API to retrieve them.\\nWe will use the `RESTfulAPI` module for creating the RESTful API endpoints.\\nWe will use the `restfulapi` module for providing a RESTful API endpoint for storing our records.\\nWe will use the `restfulapi` module for providing a JSON API endpoint for storing our records.\\nWe will use the `restfulapi` module for providing a YAML API endpoint for storing our records.\\nWe will use the `restfulapi` module for providing a YAML file for storing our records.\\nWe will use the `restfulapi` module for providing a"}
{"Context":"## Context\\nWe would like to have an easy-to-use system for consistent styles that doesn't\\ntake much tinkering. We propose using the `bootstrap` gem.\\n","Decision":"## Decision\nPending\\n","Prediction":"## Decision\nWe will use bootstrap as the foundation of our styles.\\nWe will use Sass as our stylesheet language.\\nWe will use npm packages to help us with the styling.\\nWe will use Sass variables to help us with the styling.\\nWe will use Sass variables to help us with the styling.\\nWe will use Sass variables to help us with the styling.\\nWe will use Sass variables to help us with the styling.\\nWe will use Sass variables to help us with the styling.\\nWe will use Sass variables to help us with the styling.\\nWe will use Sass variables to help us with the styling.\\nWe will use Sass variables to help us with the styling.\\nWe will use Sass variables to help us with the styling.\\nWe will use Sass variables to help us with the styling.\\nWe will use Sass variables to help us with the styling.\\nWe will use Sass variables to help us with the styling.\\nWe will use Sass variables to help us with the styling.\\nWe will use Sass variables to help us with the styling.\\nWe will use Sass variables to help us with the styling.\\nWe will use Sass variables to help us with the styling.\\nWe will use Sass variables to help us with the"}
{"Context":"## Context\\nProtocol Buffers provide a basic [style guide](https:\/\/developers.google.com\/protocol-buffers\/docs\/style)\\nand [Buf](https:\/\/buf.build\/docs\/style-guide) builds upon that. To the\\nextent possible, we want to follow industry accepted guidelines and wisdom for\\nthe effective usage of protobuf, deviating from those only when there is clear\\nrationale for our use case.\\n### Adoption of `Any`\\nThe adoption of `google.protobuf.Any` as the recommended approach for encoding\\ninterface types (as opposed to `oneof`) makes package naming a central part\\nof the encoding as fully-qualified message names now appear in encoded\\nmessages.\\n### Current Directory Organization\\nThus far we have mostly followed [Buf's](https:\/\/buf.build) [DEFAULT](https:\/\/buf.build\/docs\/lint-checkers#default)\\nrecommendations, with the minor deviation of disabling [`PACKAGE_DIRECTORY_MATCH`](https:\/\/buf.build\/docs\/lint-checkers#file_layout)\\nwhich although being convenient for developing code comes with the warning\\nfrom Buf that:\\n> you will have a very bad time with many Protobuf plugins across various languages if you do not do this\\n### Adoption of gRPC Queries\\nIn [ADR 021](adr-021-protobuf-query-encoding.md), gRPC was adopted for Protobuf\\nnative queries. The full gRPC service path thus becomes a key part of ABCI query\\npath. In the future, gRPC queries may be allowed from within persistent scripts\\nby technologies such as CosmWasm and these query routes would be stored within\\nscript binaries.\\n","Decision":"## Decision\nThe goal of this ADR is to provide thoughtful naming conventions that:\\n* encourage a good user experience for when users interact directly with\\n.proto files and fully-qualified protobuf names\\n* balance conciseness against the possibility of either over-optimizing (making\\nnames too short and cryptic) or under-optimizing (just accepting bloated names\\nwith lots of redundant information)\\nThese guidelines are meant to act as a style guide for both the SDK and\\nthird-party modules.\\nAs a starting point, we should adopt all of the [DEFAULT](https:\/\/buf.build\/docs\/lint-checkers#default)\\ncheckers in [Buf's](https:\/\/buf.build) including [`PACKAGE_DIRECTORY_MATCH`](https:\/\/buf.build\/docs\/lint-checkers#file_layout),\\nexcept:\\n* [PACKAGE_VERSION_SUFFIX](https:\/\/buf.build\/docs\/lint-checkers#package_version_suffix)\\n* [SERVICE_SUFFIX](https:\/\/buf.build\/docs\/lint-checkers#service_suffix)\\nFurther guidelines to be described below.\\n### Principles\\n#### Concise and Descriptive Names\\nNames should be descriptive enough to convey their meaning and distinguish\\nthem from other names.\\nGiven that we are using fully-qualifed names within\\n`google.protobuf.Any` as well as within gRPC query routes, we should aim to\\nkeep names concise, without going overboard. The general rule of thumb should\\nbe if a shorter name would convey more or else the same thing, pick the shorter\\nname.\\nFor instance, `cosmos.bank.MsgSend` (19 bytes) conveys roughly the same information\\nas `cosmos_sdk.x.bank.v1.MsgSend` (28 bytes) but is more concise.\\nSuch conciseness makes names both more pleasant to work with and take up less\\nspace within transactions and on the wire.\\nWe should also resist the temptation to over-optimize, by making names\\ncryptically short with abbreviations. For instance, we shouldn't try to\\nreduce `cosmos.bank.MsgSend` to `csm.bk.MSnd` just to save a few bytes.\\nThe goal is to make names **_concise but not cryptic_**.\\n#### Names are for Clients First\\nPackage and type names should be chosen for the benefit of users, not\\nnecessarily because of legacy concerns related to the go code-base.\\n#### Plan for Longevity\\nIn the interests of long-term support, we should plan on the names we do\\nchoose to be in usage for a long time, so now is the opportunity to make\\nthe best choices for the future.\\n### Versioning\\n#### Don't Allow Breaking Changes in Stable Packages\\nAlways use a breaking change detector such as [Buf](https:\/\/buf.build) to prevent\\nbreaking changes in stable (non-alpha or beta) packages. Breaking changes can\\nbreak smart contracts\/persistent scripts and generally provide a bad UX for\\nclients. With protobuf, there should usually be ways to extend existing\\nfunctionality instead of just breaking it.\\n#### Omit v1 suffix\\nInstead of using [Buf's recommended version suffix](https:\/\/buf.build\/docs\/lint-checkers#package_version_suffix),\\nwe can omit `v1` for packages that don't actually have a second version. This\\nallows for more concise names for common use cases like `cosmos.bank.Send`.\\nPackages that do have a second or third version can indicate that with `.v2`\\nor `.v3`.\\n#### Use `alpha` or `beta` to Denote Non-stable Packages\\n[Buf's recommended version suffix](https:\/\/buf.build\/docs\/lint-checkers#package_version_suffix)\\n(ex. `v1alpha1`) _should_ be used for non-stable packages. These packages should\\nlikely be excluded from breaking change detection and _should_ generally\\nbe blocked from usage by smart contracts\/persistent scripts to prevent them\\nfrom breaking. The SDK _should_ mark any packages as alpha or beta where the\\nAPI is likely to change significantly in the near future.\\n### Package Naming\\n#### Adopt a short, unique top-level package name\\nTop-level packages should adopt a short name that is known to not collide with\\nother names in common usage within the Cosmos ecosystem. In the near future, a\\nregistry should be created to reserve and index top-level package names used\\nwithin the Cosmos ecosystem. Because the Cosmos SDK is intended to provide\\nthe top-level types for the Cosmos project, the top-level package name `cosmos`\\nis recommended for usage within the Cosmos SDK instead of the longer `cosmos_sdk`.\\n[ICS](https:\/\/github.com\/cosmos\/ics) specifications could consider a\\nshort top-level package like `ics23` based upon the standard number.\\n#### Limit sub-package depth\\nSub-package depth should be increased with caution. Generally a single\\nsub-package is needed for a module or a library. Even though `x` or `modules`\\nis used in source code to denote modules, this is often unnecessary for .proto\\nfiles as modules are the primary thing sub-packages are used for. Only items which\\nare known to be used infrequently should have deep sub-package depths.\\nFor the Cosmos SDK, it is recommended that that we simply write `cosmos.bank`,\\n`cosmos.gov`, etc. rather than `cosmos.x.bank`. In practice, most non-module\\ntypes can go straight in the `cosmos` package or we can introduce a\\n`cosmos.base` package if needed. Note that this naming _will not_ change\\ngo package names, i.e. the `cosmos.bank` protobuf package will still live in\\n`x\/bank`.\\n### Message Naming\\nMessage type names should be as concise possible without losing clarity. `sdk.Msg`\\ntypes which are used in transactions will retain the `Msg` prefix as that provides\\nhelpful context.\\n### Service and RPC Naming\\n[ADR 021](adr-021-protobuf-query-encoding.md) specifies that modules should\\nimplement a gRPC query service. We should consider the principle of conciseness\\nfor query service and RPC names as these may be called from persistent script\\nmodules such as CosmWasm. Also, users may use these query paths from tools like\\n[gRPCurl](https:\/\/github.com\/fullstorydev\/grpcurl). As an example, we can shorten\\n`\/cosmos_sdk.x.bank.v1.QueryService\/QueryBalance` to\\n`\/cosmos.bank.Query\/Balance` without losing much useful information.\\nRPC request and response types _should_ follow the `ServiceNameMethodNameRequest`\/\\n`ServiceNameMethodNameResponse` naming convention. i.e. for an RPC method named `Balance`\\non the `Query` service, the request and response types would be `QueryBalanceRequest`\\nand `QueryBalanceResponse`. This will be more self-explanatory than `BalanceRequest`\\nand `BalanceResponse`.\\n#### Use just `Query` for the query service\\nInstead of [Buf's default service suffix recommendation](https:\/\/github.com\/cosmos\/cosmos-sdk\/pull\/6033),\\nwe should simply use the shorter `Query` for query services.\\nFor other types of gRPC services, we should consider sticking with Buf's\\ndefault recommendation.\\n#### Omit `Get` and `Query` from query service RPC names\\n`Get` and `Query` should be omitted from `Query` service names because they are\\nredundant in the fully-qualified name. For instance, `\/cosmos.bank.Query\/QueryBalance`\\njust says `Query` twice without any new information.\\n","Prediction":"## Decision\nWe will adopt `google.protobuf.Any` as the recommended encoding for all Protobuf\\nmessage types.\\nTo achieve this we will:\\n* Define a new protobuf type `Message` that is the canonical encoding of a\\nmessage in protobuf\\n* Define a new protobuf type `MessageType` that is the canonical encoding of\\na message type in protobuf\\n* Define a new protobuf message type `Message` that is the canonical encoding of a\\nmessage type\\n* Define a new protobuf message type `MessageType` that is the canonical encoding of\\na message type\\n* Define a new protobuf message type `Message` that is the canonical encoding of\\na message type\\n* Define a new protobuf message type `Message` that is the canonical encoding of\\na message type\\n* Define a new protobuf message type `Message` that is the canonical encoding of\\na message type\\n* Define a new protobuf message type `Message` that is the canonical encoding of\\na message type\\n* Define a new protobuf message type `Message` that is the canonical encoding of\\na message type\\n* Define a new protobuf message type `Message` that is the canonical encoding of\\na message type\\n* Define a new protobuf message type `Message` that is"}
{"Context":"## Context\\nWe plan to use AWS as a development\/staging environment and as a production environment, on ECS.\\n","Decision":"## Decision\nUse CircleCI for CI, and deploy docker images to AWS ECR for use in deployments.\\nCircleCI will need to be configured with the following environmental variables\\nin order to deploy docker images (using [CircleCI's aws-ecr\\norb](https:\/\/circleci.com\/developer\/orbs\/orb\/circleci\/aws-ecr)):\\n- DEV_AWS_ACCESS_KEY_ID\\n- DEV_AWS_SECRET_ACCESS_KEY\\n- PROD_AWS_ACCESS_KEY_ID\\n- PROD_AWS_SECRET_ACCESS_KEY\\n- AWS_REGION\\n- AWS_ECR_ACCOUNT_URL\\nThis project's Dockerfile requires access to a privately hosted github project\\n(openfido-utils). Use [Buildkit](https:\/\/docs.docker.com\/develop\/develop-images\/build_enhancements\/) on CircleCI to grant SSH access to the docker build processes.\\n","Prediction":"## Decision\nWe will use AWS ECS for the deployment of our application. This will be used as a staging environment and as a production environment.\\nWe will use the following AWS services:\\n* S3: the AWS S3 bucket that the application will be deployed to.\\n* ECR: the AWS ECS cluster that will be used to deploy the application.\\n* ELB: the AWS ELB that will be used to load balance the application. The ELB will be used as a reverse proxy for the application.\\n* API Gateway: the AWS API Gateway that will be used to expose the API to the client.\\n* Route 53: the AWS Route 53 domain name service that will be used to route traffic to the application. This will be used as a load balancer to route traffic to the application.\\n* Route 53 failover: the AWS Route 53 failover that will be used to route traffic to the application. This will be used as a load balancer to route traffic to the application.\\n* CloudWatch: the AWS CloudWatch that will be used to monitor the application. This will be used as a monitoring service.\\n* Lambda: the AWS Lambda that will be used as an API gateway handler for the"}
{"Context":"## **Context**\\nAlongside the decision to adopt an event driven architecture, there is a need to define what an event will look like. There are several options for events:\\n- **Thin Events**\\nA thin event consists of the minimum amount of data that is required that will allow a subscriber to retrieve everything it needs. This normally consists of an ID with which to make an API call back to the source publisher to gather the data it needs.\\nThe benefits of thin events are:\\n- The payload is small in size\\n- Data is always up to date as it is retrieved at the point of consumption\\n- If calls to APIs fail due to unavailability of APIs, the message can be replayed\\n- Very little need for event versioning\\nThe downsides are:\\n- Consumers need to make API calls to gather the data they need\\n- **Fat Events**\\nA fat event contains all the data necessary for any subscriber to be able to perform its job.\\nThe benefits of fat events are:\\n- all the data needed for consumer processing is present in the event\\n- no need to make any API calls to retrieve data\\nThe downsides are:\\n- Event payload could grow to be quite big\\n- Data present in the payload may no longer be required by any consumer\\n- It is difficult to version events easily (and multiple versions of the same event may need to be sent for backwards compatibility)\\n**Hybrid Approach**\\nIdeally, we should use thin events wherever possible,as this reduces the complexity around sharing events, updating consumers with new versions of events, etc. However, there are some instances where a thin event might not be possible - notably when updating an activity audit log with details of what has changed. Therefore, the best solution would be to have a thin event that contains an optional message body to hold a specific payload.\\n","Decision":"## Decision\n**Hybrid Approach**\\nThe easiest solution is to use a hybrid approach, with consumers gathering the data they need using API calls. This has the benefit of reducing the need for event versioning, and event payloads that grow in size over time.\\nThe event payload will be:\\n```\\n{\\n\"id\": \"8e648f3d-9556-4896-8400-211cb1c5451b\",\\n\"eventType\": \"personCreated\",\\n\"sourceDomain\": \"person\",\\n\"sourceSystem\": \"personAPI\",\\n\"version\": \"v1\",\\n\"correlationId\": \"f4d541d0-7c07-4524-8296-2d0d50cb58f4\",\\n\"dateTime\": \"2021-05-17T11:59:57.25Z\",\\n\"user\": {\\n\"id\": \"ac703d87-c100-40ec-90a0-dabf183e7377\",\\n\"name\": \"Joe Bloggs\",\\n\"email\": \"joe.bloggs@hackney.gov.uk\"\\n},\\n\"entityId\": \"45c76564-2e38-48f3-bb31-6bab2fef8623\",\\n\"eventBody\": {\\n\"oldData\": {\\n\"optionalProperty1\": \"Property value\",\\n\"optionalProperty2\": \"Property value\",\\n\"optionalProperty3\": \"Property value\"\\n}\\n}\\n}\\n```\\n","Prediction":"## Decision\nWe will use thin events for all event messages. Thin events will be used to store the minimum amount of data required by a consumer to operate correctly, and will be versioned using the [Event Versioning Specification](https:\/\/www.w3.org\/TR\/event-versioning\/).\\nThin events will be used to store the minimum amount of data required by a consumer to operate correctly, and will be versioned using the Event Versioning Specification.\\nThin events will be used to store the minimum amount of data required by a consumer to operate correctly, and will be versioned using the Event Versioning Specification.\\nThin events will be used to store the minimum amount of data required by a consumer to operate correctly, and will be versioned using the Event Versioning Specification.\\nThin events will be used to store the minimum amount of data required by a consumer to operate correctly, and will be versioned using the Event Versioning Specification.\\nThin events will be used to store the minimum amount of data required by a consumer to operate correctly, and will be versioned using the Event Versioning Specification.\\nThin events will be used to store the minimum amount of data required by a consumer to operate correctly, and will be versioned using the Event Versioning Specification.\\"}
{"Context":"## Context\\nMany projections produce data that is only required for a limited period of\\ntime.\\nAn application developer needs to consider how long projection data will be\\nretained and ideally implement measures to remove any unnecessary data.\\nOften such data can be removed when some future event occurs. However, in some\\ncases there is no future event that appropriately indicates the data is no\\nlonger required.\\nWe have encountered a genuine example of this when implementing an SQL\\nprojection that inserts a row whenever a user performs a specific action. The\\ndata is queried in order to enforce a \"cool down\" that prevents the user from\\nrepeating that action again within a specific time frame.\\nThe insert is triggered by the occurrence of an event, but the data becomes\\nunnecessary whenever the \"cool down\" time has elapsed.\\nIn this particular use case the \"cool down\" was not part of the business logic,\\nbut rather an API level restriction. Hence, processes\/timeout messages were not\\nthe appropriate solution.\\n","Decision":"## Decision\nWe have decided to add a `Compact()` method to `ProjectionMessageHandler`.\\nThe implementation of `Compact()` can modify the projection's data by whatever\\nmeans is appropriate such that unnecessary data is removed but the projection\\nstill serves its purpose.\\n","Prediction":"## Decision\nWe have decided to move away from the use of \"cool down\" messages and instead\\ntake steps to ensure data is removed from the projection as soon as possible.\\nThis is achieved by adding a \"before\" event to the projection that will be triggered\\nby the application developer when the \"before\" event is raised. The \"before\"\\ntemplate will be written by the application developer and will provide the\\ndeveloper with the ability to specify how long the \"before\" event should be.\\nThis will allow application developers to specify how long the data should remain\\nin the projection. This will be stored in the database as an additional column\\nto the projection and will be used by the projection to enforce the \"cool down\"\\ntime.\\nFor example, a developer can specify that the \"before\" event should last for\\ntwo hours, and the \"before\" event will be removed from the projection 8 hours\\nafter the \"before\" event has been raised. If the \"before\" event is not raised\\nwithin the two hour time frame, then the \"before\" event will be removed from\\nthe projection at the end of the two hour time frame.\\nWhen the \"before\" event is raised, the application developer will"}
{"Context":"## Context\\nWe should always re-fetch the entity data after saving within admin pages.\\n","Decision":"## Decision\nReload the data after each saving progress to ensure the user will work only the latest data.\\nWhen you save data without reloading the entity, then you need to re-assign the values. But you can't be sure, that these values are the latest ones, because of possible data inconsistency during the saving process. That's why re-fetching data is always important for further CRUD operations.\\nFor example:\\n```html\\n<!-- we change the status by click to switch for example -->\\n<sw-switch-field\\nv-model=\"data.status\"\\n:label=\"$tc('sw-review.detail.labelStatus')\">\\n<\/sw-switch-field>\\n<!-- we will save data with onSave method -->\\n<sw-button-process @click=\"onSave\">\\n{{ $tc('global.default.save') }}\\n<\/sw-button-process>\\n```\\n```javascript\\n\/\/ This method for button save\\nonSave() {\\nthis.repository.save(this.data, Shopware.Context.api).then(() => {\\n\/\/ We should add the method to re-fetch the entity data after save success here\\nthis.loadEntityData();\\n});\\n},\\n\/\/ This method to re-fetch the data\\nloadEntityData() {\\nconst criteria = new Criteria();\\nconst context = { ...Shopware.Context.api, inheritance: true };\\nthis.repository.get(this.data.id, context, criteria).then((data) => {\\nthis.data = data;\\n});\\n},\\n```\\n","Prediction":"## Decision\nWe will re-fetch the data after the save operation has been successfully completed.\\nThis will enable us to handle the case where the user is not logged into the site.\\n### Positive Consequences\\n* Data can be re-loaded after the save operation has been successfully completed\\n* We can handle the case where the user is not logged into the site\\n### Negative Consequences\\n* The data may be lost if the user is not logged into the site\\n* We may not be able to re-fetch the data if the user is logged into the site\\n### Alternative solutions\\n* We can use the `sync` package to re-fetch the data after a successful save operation\\n* We can use the `sync` package to re-fetch the data after a successful save operation\\n* We can use the `sync` package to re-fetch the data after a successful save operation\\n* We can use the `sync` package to re-fetch the data after a successful save operation\\n* We can use the `sync` package to re-fetch the data after a successful save operation\\n* We can use the `sync` package to re-fetch the data after a successful save operation\\n* We can use the `sync` package to re-fetch the data after a successful"}
{"Context":"## Context\\nThere is an initiative about questioning the technical vocabulary we use, and avoiding some words that are widely used\\nand may offend people. This is a project that adheres to this initiative, therefore...\\n","Decision":"## Decision\nRemove all existing technical vocabulary that might be offensive, and prevent those terms to be added in the future.\\nFor instance, the use of \"master\/slave\" replaced by \"main\/replica\" (or similar), or \"whitelist\/blacklist\" by\\n\"safelist\/blocklist\" (or similar).\\n","Prediction":"## Decision\nWe will use the following technical vocabulary:\\n- [Mapping](https:\/\/en.wikipedia.org\/wiki\/Mapping_(computing)):\\n- [Descriptor](https:\/\/en.wikipedia.org\/wiki\/Descriptor_(computing)):\\n- [Property](https:\/\/en.wikipedia.org\/wiki\/Property_(computing)):\\n- [Value](https:\/\/en.wikipedia.org\/wiki\/Value_(computing)):\\n- [Attribute](https:\/\/en.wikipedia.org\/wiki\/Attribute_(computing)):\\n- [ValueSet](https:\/\/en.wikipedia.org\/wiki\/ValueSet_(computing)):\\n- [Value](https:\/\/en.wikipedia.org\/wiki\/Value)\\n- [ValueSet](https:\/\/en.wikipedia.org\/wiki\/ValueSet_(computing)):\\n- [DescriptorSet](https:\/\/en.wikipedia.org\/wiki\/DescriptorSet_(computing)):\\n- [ValueSet](https:\/\/en.wikipedia.org\/wiki\/ValueSet_(computing)):\\n- [Value](https:\/\/en.wikipedia.org\/wiki\/Value)\\n- [AttributeSet](https:\/\/en.wikipedia.org\/wiki\/AttributeSet_(computing)):\\n- [ValueSet](https:\/\/en.wikipedia.org\/wiki\/ValueSet_(computing)):\\n- [Value](https:\/\/"}
{"Context":"## Context\\nThere was a need to consume the messages **in a non-blocking way** on our ESB. The former driver -PHP's native driver- didn't support such functionality.\\nThe former driver also ignored signals, due to its inability to acknowledge them **while waiting for a message**.\\nThe possible solutions were:\\n- To find a way to implement a non-blocking `consume()` function with the native driver.\\n- To use an AMQP library that offers support for such functionalities, like php-amqp-lib.\\n### Native Driver\\n*Pros*:\\n- Faster than any other library, due to it's a native extension written in C.\\n- Already implemented in the application.\\n*Cons*:\\n- Does not offer support to consume messages in a non-blocking way.\\n- Cannot listen to posix signals while consuming.\\n### PHP-AMQP-LIB\\n*Pros*:\\n- It's RabbitMQ's recommended library.\\n- Offers the possibility to consume messages in a non-blocking way.\\n- Signals can be dispatched to the worker even if it's consuming messages.\\n- Offers other options like: heartbeats, channels, and multiple hosts.\\n*Cons*:\\n- Difficult to implement correctly, without bypassing our own interfaces.\\n- Different approach demands more time to learn and implement it.\\n","Decision":"## Decision\nWe decided to use the PHP-AMQP-LIB.\\nAn initial investigation was done to force the previous driver to listen to signals while consuming, but proved to be impossible. The `consume` function of PHP's native driver blocks the execution and nothing else can be done **until a message arrives to the consumer**.\\nWe decided that it was worth to adapt the library into the framework, which greatly impacted the structure of the consumers and queue drivers. Because these classes were designed with a sync protocol in mind (STOMP, in this case) they didn't fit the asynchronous nature of AMQP, prompting for a significant redesign **of all the interfaces and abstractions involved** - which ultimately drove us to break backwards compatibility and think in the 2.0 version of the framework.\\n","Prediction":"## Decision\nWe decided to use the PHP-AMQP-LIB library to consume messages in a non-blocking way.\\nWe decided to use the `consume()` function to consume messages, and not the `consumeAll()` method.\\nThis decision was made to avoid changing our application's design in the future, and to be able to use the library in the future.\\nWe also decided to use the `consume()` function to consume messages, and not the `consumeAll()` method.\\nThis decision was made to avoid changing our application's design in the future, and to be able to use the library in the future.\\nWe also decided to use the `consume()` function to consume messages, and not the `consumeAll()` method.\\nThis decision was made to avoid changing our application's design in the future, and to be able to use the library in the future.\\nWe also decided to use the `consume()` function to consume messages, and not the `consumeAll()` method.\\nThis decision was made to avoid changing our application's design in the future, and to be able to use the library in the future.\\nWe also decided to use the `consume()` function to consume messages, and not the `consumeAll()` method.\\n"}
{"Context":"## Context\\nThe issue motivating this decision, and any context that influences or constrains the decision.\\n","Decision":"## Decision\nThe change that we're proposing or have agreed to implement.\\n","Prediction":"## Decision\nThe change that we're proposing or have agreed to implement.\\n### Positive Consequences\\n* We are moving forward with a change that is likely to result in a positive change to the user experience.\\n### Negative Consequences\\n* We are changing something that has a significant negative impact that outweighs the positive impacts of the change.\\n* We're not confident that we have the right design and implementation approach to do this change right.\\n### Exceptions\\n* This decision does not apply to: \\n* Changes to the behavior of existing features that are part of the same product feature set that we're changing.\\n* Changes to the behavior of existing features that are not part of the same product feature set that we're changing.\\n* Changes to the behavior of existing features that have a significant negative impact that outweighs the positive impacts of the change.\\n* Changes to the behavior of existing features that are not part of the same product feature set that we're changing.\\n* Changes to the behavior of existing features that have a significant negative impact that outweighs the positive impacts of the change.\\n* Changes to the behavior of existing features that have a significant negative impact that outweighs the positive impacts of the change.\\n* Changes to the behavior"}
{"Context":"## Context\\nThe integration tests' coverage (e.g.\\n`integration\/v7\/isolated\/create_org_command_test.go`) overlaps with the command\\ntests' (`command\/v7\/create_org_command_test.go`), and this causes confusion\\nfor developers\u2014does a test belong in integration, command, or both? This ADR\\ngives guidelines in order to assist developers making that decision.\\n","Decision":"## Decision\nWe want to separate [black\\nbox](https:\/\/en.wikipedia.org\/wiki\/Black-box_testing) and [white\\nbox](https:\/\/en.wikipedia.org\/wiki\/White-box_testing) testing, where\\nblack box testing (functionality of an application) is done at the\\nintegration level, and white box testing (internal structures or\\nworkings of an application) is done at the unit (command) level.\\n#### Integration\\nIntegration tests should continue to test things from a user's perspective:\\n- Build contexts around use cases\\n- Test output based on story parameters and the style guides.\\nFor example, the following behavior (from the `unbind-security-group`\\nstory) should be tested at the integration level because it describes\\noutput\/functionality:\\n> When `cf unbind-security-group` attempts to unbind a running security group\\n> that is not bound to a space, it should return the following:\\n>\\n> ```\\n> Security group my-group not bound to space my-space for lifecycle phase 'running'.\\n> OK\\n>\\n> TIP: If Dynamic ASG's are enabled, changes will automatically apply for running and staging applications. Otherwise, changes will require an app restart (for running) or restage (for staging) to apply to existing applications.\\n> ```\\nTo do so, we would have to make a `BeforeEach()` that creates the scenario\\nlaid out in the story and write expectations on each line of output in\\nthe order they are listed. For example, from\\n`integration\/v7\/global\/unbind_security_group_command_test.go` (edited\\nfor readability):\\n```golang\\nBeforeEach(func() {\\nport := \"8443\"\\ndescription := \"some-description\"\\nsomeSecurityGroup := helpers.NewSecurityGroup(securityGroupName, \"tcp\", \"127.0.0.1\", &port, &description)\\nhelpers.CreateSecurityGroup(someSecurityGroup)\\nhelpers.CreateSpace(spaceName)\\n})\\nWhen(\"the space isn't bound to the security group in any lifecycle\", func() {\\nIt(\"successfully runs the command\", func() {\\nsession := helpers.CF(\"unbind-security-group\", securityGroupName, orgName, spaceName)\\nEventually(session).Should(Say(`Unbinding security group %s from org %s \/ space %s as %s\\.\\.\\.`, securityGroupName, orgName, spaceName, username))\\nEventually(session.Err).Should(Say(`Security group %s not bound to space %s for lifecycle phase 'running'\\.`, securityGroupName, spaceName))\\nEventually(session).Should(Say(\"OK\"))\\nEventually(session).Should(Say(`TIP: If Dynamic ASG's are enabled, changes will automatically apply for running and staging applications. Otherwise, changes will require an app restart \\(for running\\) or restage \\(for staging\\) to apply to existing applications\\.`))\\nEventually(session).Should(Exit(0))\\n})\\n})\\n```\\n#### Command (Unit)\\nIn unit tests we want to break away from this perspective. We\\nwant to organize our tests by the [line of\\nsight](https:\/\/engineering.pivotal.io\/post\/go-flow-tests-like-code\/)\\nmethod, where we cover code paths as we see them.\\n<!-- Brian is really uncomfortable linking to something he wrote -->\\nIn general, every `if err != nil {` should have a corresponding `When(\"actor returns an\\nerror\", func() {`, but in most cases we do not need to test different\\npossible errors that could be returned.\\nHere's an example from `bind-security-group` code and unit test:\\n```golang\\nsecurityGroup, warnings, err := cmd.Actor.GetSecurityGroup(cmd.RequiredArgs.SecurityGroupName)\\ncmd.UI.DisplayWarnings(warnings)\\nif err != nil {\\nreturn err\\n}\\n```\\n```golang\\nIt(\"Retrieves the security group information\", func() {\\nExpect(fakeActor.GetSecurityGroupCallCount).To(Equal(1))\\nsecurityGroupName := fakeActor.GetSecurityGroupArgsForCall(0)\\nExpect(securityGroupName).To(Equal(cmd.RequiredArgs.SecurityGroupName))\\n})\\nIt(\"prints the warnings\", func() {\\nExpect(testUI.Err).To(Say(getSecurityGroupWarning[0]))\\n})\\nWhen(\"an error is encountered getting the provided security group\", func() {\\nvar expectedErr error\\nBeforeEach(func() {\\nexpectedErr = errors.New(\"get security group error\")\\nfakeActor.GetSecurityGroupReturns(\\nresources.SecurityGroup{},\\nv7action.Warnings{\"get security group warning\"},\\nexpectedErr)\\n})\\nIt(\"returns the error and displays all warnings\", func() {\\nExpect(executeErr).To(MatchError(expectedErr))\\n})\\n})\\n```\\n","Prediction":"## Decision\nWe will use the following naming conventions:\\n* `integration\/v7\/` for integration tests\\n* `command\/v7\/` for command tests\\n* `integration\/v7\/` for integration tests that are part of integration tests\\n* `command\/v7\/` for integration tests that are part of command tests\\n* `integration\/v7\/` for integration tests that are part of integration tests\\n* `command\/v7\/` for integration tests that are part of command tests\\n* `integration\/v7\/` for integration tests that are part of integration tests\\n* `command\/v7\/` for integration tests that are part of command tests\\n* `integration\/v7\/` for integration tests that are part of integration tests\\n* `command\/v7\/` for integration tests that are part of integration tests\\n* `integration\/v7\/` for integration tests that are part of integration tests\\n* `command\/v7\/` for integration tests that are part of integration tests\\n* `integration\/v7\/` for integration tests that are part of integration tests\\n* `command\/v7\/` for integration tests that are part of integration tests\\n* `integration\/v7\/` for integration tests that are part of integration tests\\n*"}
{"Context":"## Context\\nIn [ADR 5](0005-use-cqrs-architectural-style.md) and [ADR 6](0006-use-ddd-tactical-design-patterns.md) we decided to\\nadopt CQRS and tactical DDD patterns respectively. In [ADR 7](0007-use-events-as-communication-mechanism-between-modules.md)\\nwe decided that we'll use events as a means of communication between modules. Maintenance of practicalities behind these\\npatterns can be quite overwhelming, especially that we are not very experienced.\\n","Decision":"## Decision\nWe will use AxonFramework to aid in\\n- Implementing EDA thanks to the provided EventBus support\\n- Adopting the DDD building blocks\\n- Implementing read model projections\\n- Adopting EventSourcing (optional)\\n","Prediction":"## Decision\nWe will use events as a means of communication between modules.\\nModules will be responsible for managing their own state.\\nWe will use [Event Sourcing](https:\/\/en.wikipedia.org\/wiki\/Event\\_sourcing) to store and query events.\\nWe will use [Dapper](https:\/\/dapper.io\/) for persistence.\\nWe will use [ORMs](https:\/\/en.wikipedia.org\/wiki\/ORM) to persist and query data.\\nWe will use [Service Locator](https:\/\/en.wikipedia.org\/wiki\/Service\\_locator) to provide a means of resolving\\nservice references.\\nWe will use [Service Mesh](https:\/\/en.wikipedia.org\/wiki\/Service\\_mesh) to provide a means of providing a\\nmeans of resolving service references.\\nWe will use [Service Discovery](https:\/\/en.wikipedia.org\/wiki\/Service\\_discovery) to provide a means of\\ndetermining which service is available to which service.\\nWe will use [Deployment](https:\/\/en.wikipedia.org\/wiki\/Deployment) to deploy and manage our services.\\nWe will use [Metrics](https:\/\/en.wikipedia.org\/wiki\/Metrics) to monitor the health and performance of our\\nservices.\\nWe will use [Service Mesh](https:\/\/en.wikipedia.org\/wiki"}
{"Context":"## Context\\nThere are two ways I can currently see of doing this, one is building stubs into bundles, the other is serialization of port functions\\nin the test:\\n```js\\nasync supplyPorts(ports = {}) {\\nthis._page = await this.page();\\nawait this._page.evaluate(fun => {\\ncore.queryWith(() => eval(fun)());\\ncore.onSaved(e => console.log(`[ACTION.SAVED]`));\\nwindow.console.log(`Reset hacker news port and added action listener`);\\ncore.news();\\n}, ports.top.toString());\\n}\\n```\\nNote the `ports.top.toString()`, this is serializing a function.\\nThis still leaves us disconnected, though -- we can't then use it as a mock because it does not share memory with the test.\\nThe function has been serialized and sent across the wire.\\n","Decision":"## Decision\nThe change that we're proposing or have agreed to implement.\\n","Prediction":"## Decision\nWe will make our own serialization library, and use it in the tests.\\nThe library is called `network-request` and is available at\\n`https:\/\/github.com\/network\/request`\\nWe will use this library in the tests to serialize and deserialize our port functions.\\n`network-request` includes a test harness, which will be used to deserialize the serialized response.\\n`network-request` is also used in the real code, but not in the tests.\\n`network-request` is a promise-based library, which means we can use it in the real code as well.\\n`network-request` also has a test harness, which will be used to test the test harness.\\n`network-request` will be used in the tests to make it easier to mock the real request\/response.\\n`network-request` is also used in the real code, but not in the tests.\\n`network-request` is also used in the real code, but not in the tests.\\n`network-request` is also used in the real code, but not in the tests.\\n`network-request` is also used in the real code, but not in the tests.\\n`network-request` is also used in the real code, but not in the"}
{"Context":"### Context\\n- new way of writing React code\\n### Decision\\nSince it is still considered new and will be overwhelming for new developers, I don't think it's worth starting yet.\\n","Decision":"## Decision\nSince it is still considered new and will be overwhelming for new developers, I don't think it's worth starting yet.\\n","Prediction":"## Decision\nSince it is still considered new and will be overwhelming for new developers, I don't think it's worth starting yet.\\nChosen option: use `react-scripts` and create a new project.\\nIt will be a simple boilerplate project that will have a `index.js` file, that will be a single file React application.\\nWhen it's time to start adding the components, it will be easier to reuse the code in the `index.js`.\\nThis will also allow us to reuse the components in the `App` component, which is the main component.\\n### Benefits\\n- easier to understand and reuse code\\n- can be used with any framework\\n- can be used with any framework that supports `create-react-app`\\n## Decision\nSince it is still considered new and will be overwhelming for new developers, I don't think it's worth starting yet.\\nChosen option: use `react-scripts` and create a new project.\\nIt will be a simple boilerplate project that will have a `index.js` file, that will be a single file React application.\\nWhen it's time to start adding the components, it will be easier to reuse the code in the `index.js`.\\nThis will also"}
{"Context":"## Context\\nRight now the file system is getting a little messy. In the spirit of \"code gardening\" and cleaning as you go, I think it's now time to do some dedicated shuffling of files.\\nThe `src` folder was nice for a while but I think it's a remnant from the days when this site was on Gatsby. I think it just adds an extra level of file system that might not be needed.\\nHistorically, I've seen that companies rarely make the mistake of having too flat of a file structure. It's usually that folders are too nested.\\nThe goal of this is to experiment with a very _flat_ file structure and see how well it scales to more complex use cases.\\n","Decision":"## Decision\nThe filesystem will be updated to look like the following.\\n```bash\\nroot\/\\n\u251c\u2500 pages\/ # NextJS page components\\n\u2502   \u251c\u2500 page.tsx\\n\u251c\u2500 lib\/ # helper functions, utilities\\n\u2502   \u251c\u2500 utility.ts\\n\u2502   \u251c\u2500 utility.test.ts\\n\u251c\u2500 styles\/ # CSS-in-JS helpers\\n\u251c\u2500 components\/ # reusable building blocks\\n\u2502   \u251c\u2500 Component.tsx\\n\u2502   \u251c\u2500 Component.test.tsx\\n\u251c\u2500 writing\/\\n\u2502   \u251c\u2500 post-a.mdx\\n\u2502   \u251c\u2500 post-b.mdx\\n\u251c\u2500 images\/\\n\u2502   \u251c\u2500 image-a.png\\n\u251c\u2500 tests\/ # page-level integration tests\\n\u251c\u2500 e2e\/ # end-to-end tests\\n\u251c\u2500 package.json, tsconfig.json, etc.\\n```\\n","Prediction":"## Decision\n* Create a single directory structure for all the files in the project\\n* Create one directory per application component\\n* Create one directory per page component\\n* Create one directory per component view\\n* Create one directory per `build` folder\\n* Create one directory per `assets` folder\\n* Create one directory per `test` folder\\n* Create one directory per `testfile` folder\\n* Use the `build` folder to build your app\\n* Use the `assets` folder to store assets\\n* Use the `test` folder to test your app\\n* Use the `testfile` folder to test your app\\n* Use the `testfile` folder to test your app\\n* Use the `assets` folder to store assets\\n* Use the `test` folder to test your app\\n* Use the `testfile` folder to test your app\\n* Use the `assets` folder to store assets\\n* Use the `test` folder to test your app\\n* Use the `testfile` folder to test your app\\n* Use the `assets` folder to store assets\\n* Use the `test` folder to test your app\\n* Use the `testfile` folder to test your app\\n* Use the `assets"}
{"Context":"## Context and Problem Statement\\nWe often get bombarded with questions like \"Why don't you use X?\" or \"Why don't you build on top of Y?\", sometimes preceded by \"product\/project X already has feature Y\". Needless to say, this can cause a [\"Simpsons Already Did It\"](https:\/\/en.wikipedia.org\/wiki\/Simpsons_Already_Did_It) feeling.\\nThis ADR clarifies one of the core values of the Compliant Kubernetes project, namely our belief in community-driven open source. The ADR is useful to clarify both to internal and external stakeholders the choices we make.\\n## Decision Drivers\\n* We do not want to depend on the interests of any single company, be it small or large.\\n* Our customers need to have a business continuity plan, see [ISO 27001, Annex A.17](https:\/\/www.isms.online\/iso-27001\/annex-a-17-information-security-aspects-of-business-continuity-management\/). Therefore, we want to make it easy to \"exit\" Compliant Kubernetes and take over platform management.\\n* We want to use the best tools out there.\\n","Decision":"## Decision\n* We do not want to depend on the interests of any single company, be it small or large.\\n* Our customers need to have a business continuity plan, see [ISO 27001, Annex A.17](https:\/\/www.isms.online\/iso-27001\/annex-a-17-information-security-aspects-of-business-continuity-management\/). Therefore, we want to make it easy to \"exit\" Compliant Kubernetes and take over platform management.\\n* We want to use the best tools out there.\\nChosen option: \"prefer community-driven open source solutions\".\\n### Positive Consequences\\n* We do not depend on the interests of any single company.\\n* Our customers do not depend on the interests of any single company.\\n* Business continuity is significantly simplified for our customers.\\n* We have better chances at influencing projects in a direction that is useful to us and our customers. The smaller the project, the easier to influence.\\n### Negative Consequences\\n* Sometimes we might need to give up \"that cool new feature\" until the community-driven open source solution catches up with their closed source or single-company open source alternative. Alternatively, we might need to put extra time and effort to develop \"that cool new feature\" ourselves.\\n* As they are not bound by vendor liability -- e.g., end-of-life promises -- community-driven projects present a greater risk of being abandoned. The smaller the project, the higher the risk.\\n","Prediction":"## Decision\n* We do not want to depend on the interests of any single company, be it small or large.\\n* Our customers need to have a business continuity plan, see [ISO 27001, Annex A.17](https:\/\/www.isms.online\/iso-27001\/annex-a-17-information-security-aspects-of-business-continuity-management\/). Therefore, we want to make it easy to \"exit\" Compliant Kubernetes and take over platform management.\\n* We want to use the best tools out there.\\nChosen option: \"We use kubectl (Kubernetes CLI)\", because \"kubectl is the most widely used tool for managing\" Kubernetes clusters, and because it is the tool we are most familiar with.\\n### Positive Consequences\\n* We are in control of our own platform, and we can choose the tools we use.\\n* We can easily manage and monitor our clusters, and we can easily switch tools.\\n* We can easily spin up new clusters with the proper configuration.\\n* We can easily spin up new deployments with the proper configuration.\\n* We can easily monitor our clusters and deployments.\\n* We can easily scale up our clusters and deployments.\\n* We can easily migrate our clusters and"}
{"Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"## Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","Prediction":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\nWe will use [Schrony](https:\/\/schrony.org\/) for a distributed architecture decision logging system.\\nWe will use [Jira](https:\/\/www.atlassian.com\/software\/jira) for issue tracking.\\nWe will use the [Docker image](https:\/\/registry.fabric.io\/g\/bradley\/bradley:latest) to host the architecture decision logging system.\\nWe will use the [AWS CloudFormation stack](https:\/\/github.com\/bradley\/bradley\/blob\/master\/templates\/cloudformation\/templates\/aws.s3.template) to deploy the architecture decision logging system to the AWS account that holds the Docker image of the project.\\nWe will use the [Ruby gem](https:\/\/github.com\/bradley\/bradley\/issues\/1623) to log architecture decisions made.\\nWe will use [the [bradley-architecture-decision-logging gem](https:\/\/github.com\/bradley\/bradley-architecture-decision-logging), which is a Ruby gem that can be used to"}
{"Context":"## Context\\nFiggy resources may have any of the following visibilities:\\n- Open (public)\\n- Princeton (netid)\\n- On Campus (ip)\\n- Reading Room\\n- Private\\nFor each of these we need a policy regarding whether it will be indexed in DPUL.\\nWe used to index only public \/ complete items. But to support the music reserves\\ncollection we need pages that would have a viewer for logged-in institutional\\nusers only.\\n","Decision":"## Decision\nResources with the following visibilities should index into DPUL:\\n- Open (public)\\n- Princeton (netid)\\n- On Campus (ip)\\nThis is implemented with a token authentication mechanism in `iiif_resource#def\\nurl`\\n","Prediction":"## Decision\nWe will index all resources that are not public or complete.\\nIf an item is not complete, we will not index it.\\nWe will index all resources that are not Princeton \/ On Campus \/ Reading Room.\\nIf an item is not Princeton \/ On Campus \/ Reading Room, we will not index it.\\nWe will index all resources that are Princeton \/ Reading Room \/ Princeton.\\nWe will index all resources that are Princeton \/ Reading Room \/ On Campus.\\nWe will index all resources that are Princeton \/ On Campus \/ Reading Room.\\nWe will index all resources that are Princeton \/ Reading Room \/ Reading Room.\\nWe will index all resources that are Reading Room \/ Reading Room \/ Reading Room.\\nWe will index all resources that are Reading Room \/ Reading Room \/ Reading Room.\\nWe will index all resources that are Reading Room \/ Reading Room \/ ReadingRoom.\\nWe will index all resources that are Reading Room \/ ReadingRoom \/ ReadingRoom.\\nWe will index all resources that are ReadingRoom \/ ReadingRoom \/ ReadingRoom.\\nWe will index all resources that are ReadingRoom \/ ReadingRoom \/ ReadingRoom.\\nWe will index all resources that are ReadingRoom \/ ReadingRoom \/ ReadingRoom.\\nWe will index all resources that are"}
{"Context":"## Context\\nWe are currently using the [Java-based OpenAPI generator CLI](https:\/\/www.npmjs.com\/package\/@openapitools\/openapi-generator-cli).\\nAs this generator is targeting many different programming languages in some points it does not fit our needs, e. g. handling of unique names or API files, which also differ between components.\\nIn our current approach we wrap the generated API with our own to fit those needs, but we realized, that we ended up writing many workarounds.\\nMany workarounds can currently only be covered by preprocessing\/manipulating the original service definition.\\nAt the moment, some API designs cannot be realized or are not worth investing into.\\nWe have observed that due to the size of the project, there are quite a lot of known bugs, that take a long time to solve or are not solved at all.\\nThe probably biggest pain point is that the used CLI needs a Java runtime.\\nTo run the CLI we have to download the given .jar file (~24MB) and make it part of our npm package, making it much bigger in size than usual.\\nThis is necessary as the file is not downloaded on installation but on command execution, causing other errors down the line (hurts SLC-29, race conditions in async batch execution).\\n","Decision":"## Decision\nWrite our own generator, that is purely based on TypeScript and supports the API we need out of the box.\\n","Prediction":"## Decision\nWe will use a [Java-based OpenAPI generator](https:\/\/www.npmjs.com\/package\/@openapitools\/openapi-generator-cli) to generate our API definition.\\nThis will allow us to reuse the generated API definition and we will only need to make a few minor changes to the generated code.\\nWe will use the [OpenAPI Generator](https:\/\/www.openapitools.org\/)\\nto generate our API definition.\\nThis will allow us to reuse the generated API definition and we will only need to make a few minor changes to the generated code.\\nWe will use the [OpenAPI Generator](https:\/\/www.openapitools.org\/)\\nto generate our API definition.\\nThis will allow us to reuse the generated API definition and we will only need to make a few minor changes to the generated code.\\nWe will use [Swagger](https:\/\/swagger.io\/)\\nto generate our API definition.\\nThis will allow us to reuse the generated API definition and we will only need to make a few minor changes to the generated code.\\nWe will use [Swagger](https:\/\/swagger.io\/)\\nto generate our API definition.\\nThis will allow us to reuse the generated API definition and we will only need"}
{"Context":"## Context\\nWe shouldn't overwhelm the Notifier.  This would happen then we have many orders, surveys and recommendations to send to customers.\\n","Decision":"## Decision\nThe decision is to introduce a queue for all these messages.\\n","Prediction":"## Decision\nWe will set up a queue to send to and receive from.  This will be a simple queue of 3 queues.  One for the customer, one for the order and one for the survey\/recommendation.  We will use RabbitMQ to handle the queue.  RabbitMQ will be the queue manager for the order and customer queues.  RabbitMQ will be the queue manager for the survey\/recommendation queues.  RabbitMQ will be the queue manager for the transactional queues.\\nQueue names will be generated by the system and will be unique to the queue.  Queue names will be in the format: `orderQueueName-transactionQueueName-recommendationQueueName`.  For example, `orderQueueName-transactionQueueName-recommendationQueueName-xxxxx`.  Queue names will be in the format: `orderQueueName-transactionQueueName-recommendationQueueName-xxxxx`.  Queue names will be in the format: `orderQueueName-transactionQueueName-recommendationQueueName-xxxxx`.\\nQueue names will be generated by the system and will be unique to the queue.  Queue names will be in the format: `orderQueueName-transactionQueueName-recommendationQueueName"}
{"Context":"## Context\\nWe need to store project documentation (Developer Guides, System Architecture, System Maintenance, Test Plans) somewhere that it is easy to read and update.\\nDocumentation will directly reference Horace and Herbert projects.\\nThe GitHub Wiki repository does not support pull requests, or the display of any content not on the the `master` branch.\\n","Decision":"## Decision\n~We will store documentation in the `Horace\/documentation` folder and link to this from the [GitHub Wiki](https:\/\/github.com\/pace-neutrons\/pace-developers\/wiki) which is attached to the PACE project on GitHub.~\\nWe will store documentation in the `Horace\/documentation` folder which will be built regularly and deployed to the GitHub pages under the following scheme:\\nThe documentation will be supported and built by the Jenkins pipeline and automatically deployed on:\\n- nightly builds (unstable)\\n- deploy pipeline (stable & horace-version)\\nThe Jenkins will also build the documentation as a zipped artefact on other pipelines (PR, Branch, etc.) to ensure that any modifications do not affect the documentation. Following the implementation of documentation tests, will also ensure that scripts contained in the documentation remain valid and functional.\\n","Prediction":"## Decision\nWe will use the GitHub Wiki repository as a source of truth for documentation.\\nWe will use a separate project repository for documentation, to be maintained separately from the Horace and Herbert projects.\\nWe will use a separate project for the documentation of the Horace and Herbert components.\\nWe will use a separate project for the documentation of the Horace and Herbert components.\\nWe will use a separate project for the documentation of the Horace and Herbert components.\\nWe will use a separate project for the documentation of the Horace and Herbert components.\\nWe will use a separate project for the documentation of the Horace and Herbert components.\\nWe will use a separate project for the documentation of the Horace and Herbert components.\\nWe will use a separate project for the documentation of the Horace and Herbert components.\\nWe will use a separate project for the documentation of the Horace and Herbert components.\\nWe will use a separate project for the documentation of the Horace and Herbert components.\\nWe will use a separate project for the documentation of the Horace and Herbert components.\\nWe will use a separate project for the documentation of the Horace and Herbert components.\\nWe will use a separate project for the documentation of the Horace and Herbert components"}
{"Context":"## Context\\nA common problem with website initialisation is that there can be elements of the\\npage that you want to be hidden until a user performs an action via JavaScript.\\nHowever if a user does not have JavaScript you want the item to always be\\nvisible.\\nWhen the page loads this can cause a distracting flash as it quickly hides or\\nreplaces content and can appear like the page has a loading issue to a user.\\nOn content publisher there have been a number of issues flagged in testing\\nrelating to these flashes, with different ad-hoc fixes applied. This ADR is\\nintended to define a route forward for them to be handled consistently.\\nThere were 3 options considered:\\n### 1. Do nothing\\nThis option involved considering the flash to be a natural consequence of\\nprogressive enhancement.\\n### 2. Hide content before JavaScript initialisation with a `js-enabled` class\\nWe have a class that is inserted in the body element of a HTML document while\\nthe page is loaded that sets whether the user has JavaScript enabled.\\nThis class can therefore be used to hide elements on the page in advance of\\nJavaScript initialisation to avoid the flicker.\\nThe downside that this approach has is that there is not a fallback for if\\nJavaScript is enabled on a device but fails for some reason.\\n### 3. Implement an initial hiding approach resilient to JavaScript failing\\nThe final approach considered was the introduction of a second class in addition\\nto `js-enabled` on the body element that would be `js-failed`. This would\\ninvolve enhancing the initial adding of a class from:\\n```\\ndocument.body.className = ((document.body.className) ? document.body.className + ' js-enabled' : 'js-enabled');\\n```\\nto something like:\\n```\\ndocument.body.className = ((document.body.className) ? document.body.className + ' js-enabled' : 'js-enabled');\\nsetTimeout(function() {\\nif (!window.jsInitialised) {\\ndocument.body.className += ' js-failed';\\n}\\n}, 2000);\\n```\\nWhich would allow a 2 second grace period before changing the body class so\\nfallback content is displayed.\\nCSS rules would need to be updated to show content when either `js-enabled` or\\n`js-failed` was present\\n","Decision":"## Decision\nA decision was made that in interim we should take option 2 and just use the\\n`js-enabled` class. This was deemed the most pragmatic approach as option 1 was\\ncausing problems for our testers and option 3 was considered something unusual\\nthat wanted to be investigated further.\\n","Prediction":"## Decision\nWe will implement the 3 options in the order of priority:\\n# 1. Do nothing\\nThis option involves considering the flash to be a natural consequence of\\nprogressive enhancement.\\n# 2. Hide content before JavaScript initialisation with a `js-enabled` class\\nWe have a class that is inserted in the body element of a HTML document while\\nthe page is loaded that sets whether the user has JavaScript enabled.\\nThis class can therefore be used to hide elements on the page in advance of\\nJavaScript initialisation to avoid the flicker.\\nThe downside that this approach has is that there is not a fallback for if\\nJavaScript is enabled on a device but fails for some reason.\\n# 3. Implement an initial hiding approach resilient to JavaScript failing\\nThe final approach considered was the introduction of a second class in addition\\nto `js-enabled` on the body element that would be `js-failed`. This would\\ninvolve enhancing the initial adding of a class from:\\n```\\ndocument.body.className = ((document.body.className) ? document.body.className + ' js-enabled' : 'js-enabled');\\n```\\nto something like:\\n```\\ndocument.body.className = ((document.body.className) ? document"}
{"Context":"## Context\\nWe need to be able to specify a lookup component within a runner schema. These are to be used to provide a dropdown list of suggestions and simple entry before a user enters the full answer text.\\n","Decision":"## Decision\nWe will define a new answer type 'Lookup' within the schema which identifies the answer should be rendered as a lookup component.\\n{\\nid: 'religion-question',\\ntitle: 'What is your religion?',\\ndescription: 'This question is voluntary',\\ntype: 'General',\\nanswers: [\\n{\\nid: 'religion-answer',\\nmandatory: false,\\nlabel: 'Religion',\\ntype: 'Lookup',\\nlookup_type: 'Religion'\\n}\\n]\\n}\\nThe answer behaves as a Textfield and specifies a `lookup_type` reference to a configuration value in runner used to determine a api url.\\nThe api url will then be passed to the frontend component to retrieve suggestions from when JavaScript is enabled.\\n### REST Service\\nThe api to query is a service that takes an input string and returns a single flat array of suggestion strings.\\nInitially there will be no authentication.\\nOne service is expected to host all single tier lookups.\\nThe region code is supplied as a query parameter and understood by the service in order to provide language variations when available.\\n","Prediction":"## Decision\nWe will use a `LookupComponent` for this. A lookup component is a component that reads data from a lookup field. It is usually used to provide a dropdown list of suggestions, but can be used to provide a single entry field if required.\\n### Usage\\nThe lookup component will be used in the `Runners` runner schema. In this runner schema, we will create a `RunnersLookupComponent` which will be used in the `Runners` runner schema. The `Runners` runner schema will use the `RunnersLookupComponent` to select the correct lookup component when an entry is made.\\n### Example\\nIn the `Runners` runner schema, we will create a runner which has a lookup component, `RunnersLookupComponent`:\\n```\\n<runners:RunnerRunnersLookupComponent  name=\"runnersLookupComponent\" id=\"runnersLookupComponent\" type=\"runners:RunnerRunnersLookupComponent\" label=\"RunnersLookupComponent\" field=\"runnersEntryField\" rowNumber=\"rowNumber\" columnNumber=\"columnNumber\" readField=\"readField\" writeField=\"writeField\" columnId=\"columnId\" columnName=\"columnName\" checkField=\"checkField\" checkType"}
{"Context":"## Context\\nOur problem was defining the limit of modules.\\n","Decision":"## Decision\nWe decided splitting frameworks with their user flows.\\n","Prediction":"## Decision\nWe use the [limit module](https:\/\/github.com\/golang\/limit) to define the limit of modules.\\nLimit defines the limit of modules as the maximum number of modules that can be loaded into the application at once.\\nExample:\\nmodule Limit = require(\"limit\")\\nmodule myApp = Limit(maxLimit)\\nmodule myModule = Limit(maxLimit)\\nmodule myModule2 = Limit(maxLimit)\\nmodule myModule3 = Limit(maxLimit)\\nmodule myModule4 = Limit(maxLimit)\\nmodule myModule5 = Limit(maxLimit)\\nmodule myModule6 = Limit(maxLimit)\\nmodule myModule7 = Limit(maxLimit)\\nmodule myModule8 = Limit(maxLimit)\\nmodule myModule9 = Limit(maxLimit)\\nmodule myModule10 = Limit(maxLimit)\\nmodule myModule11 = Limit(maxLimit)\\nmodule myModule12 = Limit(maxLimit)\\nmodule myModule13 = Limit(maxLimit)\\nmodule myModule14 = Limit(maxLimit)\\nmodule myModule15 = Limit(maxLimit)\\nmodule myModule16 = Limit(maxLimit)\\nmodule myModule17 = Limit(maxLimit)\\nmodule myModule18 = Limit(maxLimit)\\nmodule myModule19 = Limit"}
{"Context":"## Context\\nCurrently, branches are used to draft releases. Going forward, the branching model needs to satisfy the following criteria:\\n- Releases need to be tagged to ensure that, given a release name, the commit sha can be inferred.\\n- We need to support multiple networks (chains), but they are not necessarily running the same version of TruBudget.\\n- Each network should have its own deployment URL that does not change on non-breaking version updates.\\n- Conversely, in case of breaking changes, the deployment URL should be changed.\\n","Decision":"## Decision\n- `main` continues to be the main development branch.\\n- Releases are tagged using an [annotated Git tag](https:\/\/git-scm.com\/book\/en\/v2\/Git-Basics-Tagging) (optionally [signing the tag](https:\/\/git-scm.com\/book\/en\/v2\/Git-Tools-Signing-Your-Work)) like this:\\n```bash\\ngit tag -a \"v2.0.0-beta.1\" -m \"version 2.0.0-beta.1\"\\n```\\n- For each network, there is a Git branch prefixed by the major version the network is running on - **major version upgrades are assumed to break compatibility with the network**. For example, branch names may look like this:\\n```plain\\n1.x.x_my-example-network\\n1.x.x_my-other-example-network\\n2.x.x_network-running-a-newer-version\\n```\\nNote that a branch `1.x.x_ACMECorp-UmbrellaCorp` that is running tag `v1.0.0` may upgrade to a newer minor version like `v1.1.0`, but cannot upgrade to a `2.x` release. If the ACMECorp and the UmbrellaCorp decide to upgrade to the `2.x` series of TruBudget, they need to migrate to a new branch called `2.x.x_ACMECorp-UmbrellaCorp`. The definition of the necessary processes is not in scope of this ADR.\\nAdditionally, there is one release branch for each major version that tracks the most current release (tag) for that major version.\\nThe steps required for making a new release are:\\n1. Increase the version number (see #33).\\n1. Update the CHANGELOG.md file to reflect the release.\\n1. Tag the commit on the main branch (see above).\\n1. Checkout the release branch of the current major version (e.g. `release_1.x.x`) and `git merge main`.\\nAs soon as a network is ready to be upgraded to the new version, its branch may be updated:\\n1. Checkout the network branch, e.g. `1.x.x_my-example-network`.\\n1. Merge-in the release branch (or a specific tag), e.g. `git merge release_1.x.x`.\\n","Prediction":"## Decision\n* TruBranch will be deprecated and replaced with TruBranches in the near future.\\n* TruBranches will be used to track specific TruBudget releases. Releases will be tagged with the version number. This will allow TruBranches to be used for each new release, and to allow for breaking changes without breaking TruBranches.\\n* TruBranches will be deployed to a subdirectory of the TruBranches repository. This will allow TruBranches to be deployed to different environments, such as staging, test, and prod.\\n* TruBranches will use TruBranches\/TruBranches as their version number.\\n* TruBranches will be deployed to a separate Git repository. This will allow TruBranches to be deployed to multiple environments, and to allow for breaking changes without breaking TruBranches. This will also allow TruBranches to be used for staging, test, and prod.\\n* TruBranches will be deployed to multiple environments. This will allow TruBranches to be used for different environments, and to allow for breaking changes without breaking TruBranches.\\n* TruBranches will be deployed to multiple environments."}
{"Context":"## Context\\nThe bisection method of header verification used by the light client exposes\\nitself to a potential attack if any block within the light clients trusted period has\\na malicious group of validators with power that exceeds the light clients trust level\\n(default is 1\/3). To improve light client (and overall network) security, the light\\nclient has a detector component that compares the verified header provided by the\\nprimary against witness headers. This ADR outlines the process of mitigating attacks\\non the light client by using witness nodes to cross reference with.\\n","Decision":"## Decision\nThe light client will be divided into two components: a `Verifier` (either sequential or\\nskipping) and a `Detector` (see [Informal's Detector](https:\/\/github.com\/informalsystems\/tendermint-rs\/blob\/master\/docs\/spec\/lightclient\/detection\/detection.md))\\n. The detector will take the trace of headers from the primary and check it against all\\nwitnesses. For a witness with a diverging header, the detector will first verify the header\\nby bisecting through all the heights defined by the trace that the primary provided. If valid,\\nthe light client will trawl through both traces and find the point of bifurcation where it\\ncan proceed to extract any evidence (as is discussed in detail later).\\nUpon successfully detecting the evidence, the light client will send it to both primary and\\nwitness before halting. It will not send evidence to other peers nor continue to verify the\\nprimary's header against any other header.\\n","Prediction":"## Decision\nThe light client will use the [witness node](https:\/\/medium.com\/@mikebriant\/using-witness-to-verify-light-chains-3e8d9d0a6b7f) provided by the light client\\nto cross-reference against block headers. This will increase the robustness of the light client against malicious block headers and will\\nalso improve the usability of the light client.\\nTo ensure that the witness node is sufficiently large, the witness node will be\\ndistributed to the validators in the light client trust model. The witness node\\nwill be distributed to validators in the light client trust model in a way that\\nensures that no single validator has a majority of validators with a power greater than the trust level of the light client. The witness node will be\\ndistributed to validators in the following order:\\n- 1 validator with the highest power\\n- 1 validator with the next highest power\\n- 2 validators with the next highest power\\n- 3 validators with the next highest power\\n- 4 validators with the next highest power\\n- 5 validators with the next highest power\\n- 6 validators with the next highest power\\n- 7 validators with the next highest power\\n- "}
{"Context":"## Context\\nAn image is a powerful mechanism for deployment, as it is portable across environments.\\nSome images need to specify the version of their dependencies (such as other images) that should be run as sidecars.\\n","Decision":"## Decision\nUse image labels of the form `org.elifesciences.dependencies.api-dummy` to store the version of a dependency.\\n","Prediction":"## Decision\nWe will use [Dockerfiles](https:\/\/docs.docker.com\/engine\/recipes\/)\\nto create Docker images that will run sidecars.\\nWe will use Docker Compose to deploy the sidecars to the Kubernetes cluster.\\nWe will run the sidecar Docker image as part of the Docker build process.\\nWe will use the [Docker Compose CLI](https:\/\/docs.docker.com\/compose\/) to run the sidecar Docker image.\\nWe will run the sidecar Docker image as part of the Docker build process.\\nWe will use the [Docker Compose API](https:\/\/docs.docker.com\/compose\/api\/)\\nto expose the sidecar Docker image as a service on the Kubernetes cluster.\\nWe will use the [Docker Compose API](https:\/\/docs.docker.com\/compose\/api\/)\\nto expose the sidecar Docker image as a service on the Kubernetes cluster.\\nWe will use the [Docker compose service account](https:\/\/github.com\/docker\/compose\/blob\/master\/docs\/concepts\/service-accounts.md) to create the service account.\\nWe will use the [Docker compose service account](https:\/\/github.com\/docker\/compose\/blob\/master\/docs\/concepts\/service-accounts.md) to create the service account"}
{"Context":"## Context\\nQuite often, in commercial projects, reporting back to the community is entirely in the developer's hands. They are allowed to do this, but commonly there is no dedicated time in the plans.\\nOne of the ideas behind Project Klokwrk is to streamline development efforts as much as possible. As we are using many different open-source tools and libraries, it is in our best interest to try to\\nhelp with the issues we encountered.\\n### Architectural Context\\n* System (`klokwrk-project`)\\n","Decision":"## Decision\n**At least, we will report the issues in 3rd party open source software that are encountered during `klokwrk-project` development.** We'll strive to do this as consistently as possible. Besides being\\nconstructive community members, resolving encountered issues should benefit us with an easier and streamlined workflow.\\nWe will actively maintain [the list of reported issues and monitored issues](..\/..\/misc\/klokwrkRelatedIssuesInTheWild.md) that are relevant for `klokwrk-project.`\\nPutting open source contribution in ADR makes it an official guideline and moral obligation for all `klokwrk-project` members.\\n","Prediction":"## Decision\n* Use `klokwrk-reporting` for reporting back to the community\\n* Use `klokwrk-reporting` for reporting back to the project's own codebase.\\n* Use `klokwrk-reporting` for reporting back to the project's own codebase.\\n* Use `klokwrk-reporting` for reporting back to the project's own codebase.\\n* Use `klokwrk-reporting` for reporting back to the project's own codebase.\\n* Use `klokwrk-reporting` for reporting back to the project's own codebase.\\n* Use `klokwrk-reporting` for reporting back to the project's own codebase.\\n* Use `klokwrk-reporting` for reporting back to the project's own codebase.\\n* Use `klokwrk-reporting` for reporting back to the project's own codebase.\\n* Use `klokwrk-reporting` for reporting back to the project's own codebase.\\n* Use `klokwrk-reporting` for reporting back to the project's own codebase.\\n* Use `klokwrk-reporting` for reporting back to the project"}
{"Context":"## Context\\nOur problem was using chain delegate without doing any changes on every scope. It generates too many repeated codes.\\n","Decision":"## Decision\nDon't use chain delegate if you are not make any changes on every scope.\\n","Prediction":"## Decision\nWe decided to use chain delegate as a singleton to improve the code readability.\\nWe made a mixin for chain delegate which is a class with the same name but with an additional method, `chainDelegate`. This method is used to create the chain delegate.\\nIt's a class with a single instance which is passed to the chain delegate. The instance is the one that will be used to create the chain delegate.\\nThis is how we did it:\\n```js\\nconst chainDelegate = (className, chain) => ({\\nchainDelegate: chainDelegate, \/\/ The chain delegate\\nchainDelegate: chainDelegate() \/\/ The chain delegate which will be injected\\n})\\n```\\nThis method will be injected into the chain delegate and will be used to create it. This will be the only one that will be executed.\\n### Usage\\nAdd the mixin to the chain delegate class:\\n```js\\nconst chainDelegate = (className, chain) => ({\\nchainDelegate: chainDelegate, \/\/ The chain delegate\\nchainDelegate: chainDelegate() \/\/ The chain delegate which will be injected\\n})\\n```\\n### Example\\nIn our example, we want to create a delegate with the name `chainDelegate` and it will"}
{"Context":"## Context\\nGreenLog allows data to be attached to log entries, either as \"context\", or \"data\" associated with the logged event.\\nLog entries may be consumed by a variety of \"handlers\", which\\n","Decision":"## Decision\nTo make the job of \"handlers\" easier, GreenLog will restrict the type of value usable as log \"context\" or \"data\" to:\\n- `true`\/`false`\\n- `Numeric`\\n- `String`\\n- `Time`\\n- `Hash` (with `String` or `Symbol` keys)\\n","Prediction":"typically should not be expected to handle any data associated with the log entry.\\n## Decision\nWe will separate the logging and data handling of green logs into separate components.\\nWe will use the [GreenLog API](https:\/\/github.com\/awslabs\/greenlog\/wiki\/API) to abstract the details of the logging and data handling.\\nWe will use the [GreenLog library](https:\/\/github.com\/awslabs\/greenlog) to abstract the details of the logging and data handling.\\nWe will use the [GreenLog logger](https:\/\/github.com\/awslabs\/greenlog\/tree\/master\/libraries\/greenlog) to abstract the details of the logging and data handling.\\nWe will use the [GreenLog data logger](https:\/\/github.com\/awslabs\/greenlog\/tree\/master\/libraries\/greenlog\/tree\/master\/libraries\/greenlog\/data) to abstract the details of the logging and data handling.\\nWe will use the [GreenLog logging](https:\/\/github.com\/awslabs\/greenlog\/tree\/master\/libraries\/greenlog\/tree\/master\/libraries\/greenlog\/logging) to abstract the details of the logging and data handling.\\nWe will use the [GreenLog logging](https:\/\/github.com\/aw"}
{"Context":"## Context\\nPreviously the plugins are not completely independent from the core and other plugins. This has sometimes caused built plugin files to be incompatible with the core. Unless they were rebuilt again with the core.\\nThe reason for this was that dependencies between plugins and the Core were optimized by Webpack. This was because Webpack saw the combination of Core and plugins as one big program. So using tree-shaking, sometimes dependencies were removed or added depending on which plugins were installed.\\nAlso, a custom Webpack configuration in plugins resulted in it unavoidably being applied in core as well. This could sometimes result in the plugin only being compatible with the core if both were built together. If the plugin was then installed on other systems with only the built files, it could cause it not to work.\\n","Decision":"## Decision\nWebpack is known by many users and already in use. A switch to another builder needs to be deeply analyzed at first and then all plugin devs need to learn this bundler too, which can be frustrating, when you want to write a great plugin but has to learn a new bundler for no reason.\\nSo the isolated compiling and production bundling will be realized with webpack. Webpack also provides a good way how to solve the problem. With the webpack-multi-compiler we can build several independent configurations which do not affect each other. The watch mode also works with this setup so that no developer needs to relearn something.\\n","Prediction":"## Decision\nWe will use two different Webpack configuration files in plugins. One for the built files and one for the runtime dependencies. This will ensure that the plugins are built with the same dependencies and the same version of the core.\\nThe built files will be stored in the `build` folder in the `plugins` folder. The runtime dependencies will be stored in a separate Webpack configuration file in the `plugins\/runtime` folder. This will ensure that the runtime dependencies are always up to date.\\nThis will also ensure that the built plugins can be rebuilt with the same dependencies and version of the core.\\nPlugins will be built in the order they are listed in the `config.toml` file. This will ensure that built plugins are built before dependencies are added.\\nWe will also add the `build` and `runtime` configuration files to the `plugins` folder in the `web.config` file. This will ensure that the plugins are built in the correct order. This will also ensure that built plugins are always up to date.\\nThis will also ensure that the built plugins can be rebuilt with the same version of the core.\\nThis will also ensure that the built plugins are not overwritten by a newer version of the core.\\nThis will also ensure"}
{"Context":"## Context\\nSome scratch\/ftduino functions take complex arguments that need to be verified.\\nSome scratch\/ftduino functions are complex to implement.\\nThe code for these functions could be directly generated in the java files for the specific function.\\nThe code for these functions could also be written inside a helper function, such that only a single line - that calls the helper function - is generated for such a function.\\nWriting these functions in a helper function also makes changes and code reuse easier.\\n","Decision":"## Decision\nComplex scratch\/ftduino functions will be implemented in helper functions and those will be called by the generated code.\\n","Prediction":"## Decision\nThe code for the scratch\/ftduino functions will be directly generated in the java files for the specific function.\\nThis means that the code for the scratch\/ftduino functions will be generated in the java files for the specific function.\\nThere is no need to write a helper function for the scratch\/ftduino functions in Java.\\nThis also means that changes to the code for the scratch\/ftduino functions can be made in a single line.\\nThis also means that changes to the code for the scratch\/ftduino functions can be made in a single line.\\nThis also means that changes to the code for the scratch\/ftduino functions can be made in a single line.\\nThis also means that changes to the code for the scratch\/ftduino functions can be made in a single line.\\nThis also means that changes to the code for the scratch\/ftduino functions can be made in a single line.\\nThis also means that changes to the code for the scratch\/ftduino functions can be made in a single line.\\nThis also means that changes to the code for the scratch\/ftduino functions can be made in a single line.\\nThis also means that changes to the code for the scratch\/ftduino functions can be"}
{"Context":"## Context\\nCurrently TypeScript is not being used. For typing of component props PropTypes is being used.\\n","Decision":"## Decision\nMaking use of typing will make our code more readable and more reliable. Furthermore we don't need to use the PropTypes dependency anymore. TypeScript's interfaces and types are more reusable and adaptable than PropTypes.\\nTransition to TypeScript will be done gradually.\\n","Prediction":"## Decision\nWe will use PropTypes for Typescript props.\\n### Why\\n* We have many components with similar structure.\\n* It is a good practice to use Typescript.\\n### Why not typings\\n* It is not necessary for the component to know about Typescript.\\n* We can use Typescript for Typescript props.\\n* We can use Typescript for Typescript errors.\\n* We can use Typescript for Typescript warnings.\\n### Benefits\\n* We can use Typescript to help us write better code.\\n* We can use Typescript for Typescript errors.\\n* We can use Typescript for Typescript warnings.\\n### Limitations\\n* We can't use Typescript for Typescript errors.\\n* We can't use Typescript for Typescript warnings.\\n### Cost\\n* It's a free tool.\\n* It's supported by the community.\\n* It is recommended.\\n* It is a good practice to use Typescript.\\n### Consequences\\n* It's not a good practice to use Typescript for Typescript props.\\n* It's not a good practice to use Typescript for Typescript errors.\\n* It's not a good practice to use Typescript for Typescript warnings"}
{"Context":"## Context\\nContent published by Whitehall can be marked as 'political' and associated with a government (based on it's publication date). This is used to help indicate the content may no longer accurate as governments change over time.\\nMarking content as a political is a semi-automated process, which has the effect of adding two extra attributes to the payload we send to the Publishing API. Both attributes are required for them to be used by the frontends.\\n```\\n{\\n...,\\n\"details\": {\\n\"government\": {\\n\"title\": \"My Current Government\",\\n\"slug\": \"\/my-current-government\",\\n\"current\": true\\n},\\n\"political\": true\\n}\\n}\\n```\\nThere were 3 options considered:\\n### 1. Do not implement\\nThis would require changing the [content schema for news articles][news-article-schema] so that the `government` and `political` attributes are not required. We've checked to see the frontends do not require these attributes to be set.\\n### 2. Implement like Whitehall\\nThis would require us to replicate the [semi-automatic logic][whitehall-political-identifier] in Whitehall to identify political content, as well as extend our UI to allow users to mark content as political and change the publication date.\\n### 3. Implement from scratch\\nThis would involve designing our own UI to associate content with a government and\/or mark it as political. After some discussion with the people who worked on adding this feature to GOV.UK, we believe there is scope for improvement.\\n","Decision":"## Decision\nWe decided to go with option (1) and defer implementing support for political content. The implementation in Whitehall has several disadvantages, which we think merit further research with users:\\n* The `political` and `government` attributes are part of the content of a document, which means they contain redundant data.\\n* Storing the attributes in the content also means all documents must be republished when the current government changes.\\n* The effect of marking content as political is unclear, and the link between publication date and government is not visible.\\n* It's unclear if associating content with a government and marking it as political are separate concerns or the same thing.\\nSince manual intervention is required whether or not we implement the current approach, we think it's reasonable to defer supporting it until we have more knowledge to address the issues it presents.\\n","Prediction":"## Decision\nWe will use the [content schema for news articles][news-article-schema] to mark content as political and publish it. We will implement a new UI to allow users to mark content as political and change the publication date.\\nWe will also extend the existing [Whitehall publishing API] to include the new content attributes.\\nWe will also use the [content schema for news articles][news-article-schema] to mark content as a political update.\\nWe will also use the [content schema for news articles][news-article-schema] to mark content as a political update, but we will only mark it as such if it has a publication date that is greater than the last update date of the article.\\nWe will use the [content schema for news articles][news-article-schema] to mark content as a political update, but we will not mark it as such if it has a publication date that is less than the last update date of the article.\\nWe will use the [content schema for news articles][news-article-schema] to mark content as a political update, but we will not mark it as such if it has a publication date that is more than the last update date of the article.\\nWe will use the [content schema for news articles][news"}
{"Context":"## Context\\nWe need to provide translation capabilities in a way that slots into our twig based rendering pipeline\\n","Decision":"## Decision\nWe're going to use symfony\/translation to provide the translation capabilities\\n","Prediction":"## Decision\nWe will use [Tentacle](https:\/\/github.com\/wordpresstheme\/tentacle) for translation\\nusing [Tentacle TTS](https:\/\/github.com\/wordpresstheme\/tentacle-tts) or the [Tentacle TTS API](https:\/\/github.com\/wordpresstheme\/tentacle-tts-api) for more complex translations.\\nTentacle TTS is not yet available in all locales so we will need to support fallback translations\\nusing TTS via the [Tentacle TTS API](https:\/\/github.com\/wordpresstheme\/tentacle-tts-api) where applicable.\\nTentacle TTS API is available in the WooCommerce translation plugin and can be used directly\\nor via the [Tentacle TTS API](https:\/\/github.com\/wordpresstheme\/tentacle-tts-api) where available.\\nTentacle TTS API is not currently available in all locales so we'll need to fallback to\\nTTS via the [Tentacle TTS API](https:\/\/github.com\/wordpresstheme\/tentacle-tts-api) where applicable.\\nTentacle TTS API is not available in all locales so we'll need to"}
{"Context":"## Context\\nWe need take a decision for the minimal requirements of a IoT-Board to become a Home Assistant OS image.\\nAlso how we deal with old or replaced generation\/boards.\\n","Decision":"## Decision\nFollow are the requirements of a Board:\\n- The board needs to be available for private customers in most major economic regions\\n- The manufacturer produces and sells the boards for at least 3 years\\n- The manufacturer is know for his continuity\\n- The board needs to be supported by the upstream Linux kernel\\n- The board needs to be supported by the upstream U-Boot or Barebox boot loader\\nIt's allowed to use some patches as long they only minimal touch the Kernel and marked for upstream.\\nWe support the board as long as possible with updates. We might remove support if we see major downsides or we don't have hardware\\nfor the sporadic tests.  When a next generation is available, we provide updates for the old but don't recommend it anymore.\\nIf a board is no longer supported, no updates for the operating system or Home Assistant Core will be provided according ADR0015.\\nIt's recommended to update the hardware to next generation or a new board.\\n","Prediction":"## Decision\nWe will use the [OpenHAB project](https:\/\/github.com\/openhab\/openhab) as the base for Home Assistant OS images.\\nThis will allow us to use the [OpenHAB core](https:\/\/openhab.io\/), and to build on top of that.\\nWe will use [Smarthome](https:\/\/github.com\/smartthings\/smarthome) as the IoT Hub software.\\nThis will allow us to easily connect Home Assistant to the IoT-Board, and to easily configure the IoT-Board.\\nWe will use the [lack of a name for the OS](https:\/\/github.com\/openhab\/openhab\/issues\/521) as a motivation to name the OS.\\nThis will make it easier to understand what the OS is for, and to better help us to maintain the OS.\\nWe will use the [OpenHAB core](https:\/\/openhab.io) and [OpenHAB core MQTT](https:\/\/openhab.io\/docs\/add-ons\/mqtt) MQTT clients.\\nThese will allow our Home Assistant installation to be connected to the IoT-Board, and to be easily configured.\\nWe will use the [openHAB core MQTT](https:\/\/openhab.io\/docs\/add-ons\/mqtt"}
{"Context":"## Context\\nIn order for a project to be useful, it has to be shared with others. In order for it to be shared effectively, there needs to be a shared expectation of what can be done with it, how to make changes, whether or not changes can be made, and what if any warranty or suitablity for use claims there may be.\\nThe license dictates how other people can use the software, and will set the tone for the community - making it easier or harder for individuals and organziations to use the software and contribute back to it.\\nIn order to foster an active community, it is important to pick the correct license.\\n","Decision":"## Decision\nThe decision was made to use the Apache License version 2.0\\n","Prediction":"## Decision\nWe will use the MIT license.\\nThis means that the MIT license is used for all software in the project, not just open source software.\\nThis license is used because the MIT license is the most widely used and well-known free and open-source license in the world.\\nIt is also well-established, and has been adopted by many other projects and communities, including the Apache Software Foundation and the Mozilla Corporation.\\nThe MIT license is used because it is the most widely used free and open-source license in the world. This means that it has a large and active community, and is used by many other projects and communities.\\nThe MIT license is used because it is well-established and well-understood, and is well-known to be the most widely used free and open-source license in the world.\\nIt is also well-established and well-understood, and is well-known to be the most widely used free and open-source license in the world.\\nIt is also well-known to be the most widely used free and open-source license in the world, and well-established and well-understood by the rest of the world.\\nIt is also well-known to be the most widely used free and open-source license in the world, and well-understood and well"}
{"Context":"## Problem\\nOnly plugins like `dump` and `quickdump` are able to represent any KeySet\\n(as they are designed to do so). Limitations of other storage plugins are\\ne.g., that not every structure of configuration is allowed.\\nSome of these limitations were documented `infos\/status`, others were not.\\n","Decision":"## Decision\nAdd `infos\/features\/storage` to document limitations of storage plugins.\\nIdeally, storage plugins should throw an error in `kdbSet` for\\nunrepresentable KeySets.\\nElektra cannot guarantee that any configuration file format can\\nbe mounted anywhere.\\nDevelopers, maintainers and administrators are responsible for what\\nthey mount. They need to test the setup.\\n- [Base Name](base_name.md)\\n","Prediction":"## Decision\nWe will not support the `dump` and `quickdump` plugins in the plugin repository.\\nInstead, we will implement our own storage plugins for the plugins that\\neither implement any of the `dump` and `quickdump` plugins or are\\nequivalent to them (e.g., `plugin-2`, `plugin-3`, `plugin-4`, `plugin-5`,...).\\nWe will add the `plugin-2` and `plugin-3` plugins to the plugin repository as\\ne.g., `plugin-2_dump`, `plugin-3_dump`, `plugin-4_dump`, `plugin-5_dump`,...\\nThe `plugin-2` plugin will be the base storage plugin for plugins that\\neither implement the `dump` and `quickdump` plugins. The `plugin-2` plugin\\nwill inherit the `plugin-2_dump` and `plugin-2_quickdump` plugins.\\nIt will be the only storage plugin that can be installed as part of\\ne.g., the `plugin-2` plugin. It will be the only storage plugin that can be\\ne.g., the `plugin-2` plugin can be installed as a dependency.\\nIt will be the"}
{"Context":"## Context\\nA tool to make creating these ADRs helps reduce friction around creating documentation.\\n`adr-tools` is a simple cli tool that has been used in the past on previous projects to great effect (https:\/\/github.com\/npryce\/adr-tools)\\n","Decision":"## Decision\nInstall `adr-tools` locally and use it to help quickly generate consistent ADRs\\n","Prediction":"## Decision\nWe will use `adr-tools` to create documentation.\\nWe will use `adr-doc` to generate ADRs.\\nWe will use `adr-doc` to generate ADRs.\\nWe will use `adr-doc` to generate ADRs.\\nWe will use `adr-doc` to generate ADRs.\\nWe will use `adr-doc` to generate ADRs.\\nWe will use `adr-doc` to generate ADRs.\\nWe will use `adr-doc` to generate ADRs.\\nWe will use `adr-doc` to generate ADRs.\\nWe will use `adr-doc` to generate ADRs.\\nWe will use `adr-doc` to generate ADRs.\\nWe will use `adr-doc` to generate ADRs.\\nWe will use `adr-doc` to generate ADRs.\\nWe will use `adr-doc` to generate ADRs.\\nWe will use `adr-doc` to generate ADRs.\\nWe will use `adr-doc` to generate ADRs.\\nWe will use `adr-doc` to generate ADRs.\\nWe will use `adr-doc` to generate ADRs.\\nWe will use"}
{"Context":"## Context\\nAll applications inevitably have some form of \"design\" applied to them, whether it be an ugly bag of mostly functions, or a disciplined model developed by a group of software design experts. A good design can have a dramatic effect on the quality of an application, enabling a developer to quickly become familiar with the existing code, and knowing where new functionality would fit into the big picture.\\nOne simple and yet effective design is called the Clean Architecture, as described by Robert Martin in a blog post from 2012 (see references below). The application is divided into several layers, with dependencies between layers going in only one direction. This encourages loose coupling and enables thorough unit testing through isolation of units via mocking. The Clean Architecture builds on earlier models, including Hexagonal Architecture and the Onion Architecture, which similarly break up the application into layers, with dependencies flowing in one direction.\\nA [tutorial](https:\/\/resocoder.com\/2019\/08\/27\/flutter-tdd-clean-architecture-course-1-explanation-project-structure\/) by Matt Re\u0161et\u00e1 on Dart and Flutter uses a version of the Clean Architecture in which the domain layer defines the interface for one or more repositories, whose implementation lives in the data layer. This separation is often not covered in other depictions of Clean Architecture, but is helpful for understanding how best to keep dependencies unidirectional, as well as encouraging loose coupling and easier testing.\\n","Decision":"## Decision\nThe version of Clean Architecture described by Matt Re\u0161et\u00e1 will be used.\\nAdditionally, dependency injection (a.k.a. inversion of control) will be used to maintain a clean separation between layers and permit easy unit testing.\\n","Prediction":"## Decision\nWe will use the Clean Architecture to structure the project\\n- https:\/\/resocoder.com\/2019\/08\/27\/flutter-tdd-clean-architecture-course-1-explanation-project-structure\\n- https:\/\/hackernoon.com\/a-clean-architecture-for-our-architecture-design-2c3e5de7d6d7\\n- https:\/\/medium.com\/@ravikumar\/development-architecture-design-patterns-a-6a1c0b0c7f0c\\n- https:\/\/medium.com\/@craigcotton\/why-we-use-the-clean-architecture-6e3d3b3b9b3e\\n- https:\/\/www.investopedia.com\/terms\/c\/cleanarchitecture.asp\\n- https:\/\/www.investopedia.com\/terms\/k\/knowledge-organizer.asp\\n- https:\/\/www.c-sharpcorner.com\/memorization-knowledge-organizer-koa-1a1d8b9f4f5c\\n- https:\/\/www.c-sharpcorner.com\/development\/using-knowledge-organizer-to-organize-your-knowledge-2b8a6b9f4f5c\\n- https:\/\/www.c-sharpc"}
{"Context":"## Context\\nDifferent commands in the ADR utility require manipulating the content of an ADR.\\nFor example, changing its status.\\nInstead of spreading the structure and management of ADR *content* into different commands, it would be better to centralize this related piece of knowledge into one file.\\nAlso, we may want to change how manipulate content, or export it. Centralizing it in one place will encapsulate that aspect of the tool into one place.\\n","Decision":"## Decision\nAll ADR content manipulation is to be written and centralized in a single ADR object, under the `core` directory.\\n","Prediction":"## Decision\nWe will use the [ADR package](https:\/\/github.com\/adrtools\/adr) to manage the ADR content.\\nThis will allow us to create, modify, and read ADR content, and also allow us to export the ADR content to an external file.\\nWe will also use the [ADR package](https:\/\/github.com\/adrtools\/adr) to manage the ADR status.\\nThis will allow us to create, modify, and read ADR status, and also allow us to export the ADR status to an external file.\\nADR will be stored in a [repository](https:\/\/github.com\/adrtools\/adr) that will be managed by the ADR package.\\n[ADR repository](https:\/\/github.com\/adrtools\/adr) will be managed by ADR package.\\nADR will be used to manage the ADR content.\\nADR will be used to manage the ADR status.\\nWe will use the [ADR package](https:\/\/github.com\/adrtools\/adr) to manage the ADR content.\\nThis will allow us to create, modify, and read ADR content, and also allow us to export the ADR content to an external file.\\nWe will also use"}
{"Context":"## Context\\n[context]: #context\\n> This section describes the forces at play, including technological, political, social, and project local. These forces are probably in tension, and should be called out as such. The language in this section is value-neutral. It is simply describing facts.\\n","Decision":"## Decision\n[decision]: #decision\\n> This section describes our response to these forces. It is stated in full sentences, with active voice. \"We will ...\"\\n","Prediction":"## Decision\n* We will use the term \"positive deviance\" to describe the forces at play. Positive deviance is a force that is causing or has the potential to cause change.\\n* We will use the term \"negative deviance\" to describe forces that are in tension with the positive deviant forces.\\n* We will use the term \"positive deviance\" to describe forces that are in alignment with the positive deviant forces.\\n* We will use the term \"negative deviance\" to describe forces that are in tension with the negative deviant forces.\\n* For the sake of clarity, we will use the term \"deviant\" to refer to forces that are in tension with the positive deviant forces.\\n* We will use the term \"positive deviant\" to refer to forces that are in alignment with the positive deviant forces.\\n* We will use the term \"negative deviant\" to refer to forces that are in tension with the negative deviant forces.\\n* This section is written for the benefit of the reader, not for the benefit of the author. The language is value neutral and is intended to be read by a wide audience. The language may be edited and re-written by the reader in the future.\\n*"}
{"Context":"## Context\\n1. \u6211\u4eec\u7684\u4e00\u4e9b\u667a\u80fd\u8bbe\u5907\uff0c\u901a\u8fc7 TCP \u548c\u540e\u7aef\u8fdb\u884c\u901a\u4fe1\uff0c\u4e0a\u62a5\u6570\u636e\u53ca\u63a5\u6536\u6307\u4ee4\uff1b\\n2. \u667a\u80fd\u8bbe\u5907\u7531\u7b2c\u4e09\u65b9\u63d0\u4f9b\uff0c\u6211\u4eec\u5b9e\u73b0\u540e\u7aef\u670d\u52a1\uff0c\u5f53\u524d\u652f\u6301\u57df\u540d\u548c IP \u8bf7\u6c42\uff1b\\n3. \u968f\u7740\u672a\u6765\u8bbe\u5907\u6570\u91cf\u7684\u589e\u957f\uff0c\u6211\u4eec\u7684\u5355\u53f0\u670d\u52a1\u9884\u8ba1\u5c06\u65e0\u6cd5\u6ee1\u8db3\uff1b\\n4. \u539f\u65b9\u6848\u662f\u9488\u5bf9\u4e0d\u540c\u6279\u6b21\u7684\u667a\u80fd\u8bbe\u5907\uff0c\u6211\u4eec\u7ed1\u5b9a\u4e0d\u540c\u7684\u57df\u540d\uff0c\u5df2\u505a\u5ba2\u6237\u7684\u8d1f\u8f7d\uff1b\\n5. \u5b58\u5728\u8bbe\u5907\u4f7f\u7528\u7684\u4e0d\u53ef\u63a7\u5f15\u8d77\u670d\u52a1\u7aef\u7684\u4f7f\u7528\u7387\u4f1a\u5b58\u5728\u95ee\u9898\uff1b\\n6. \u670d\u52a1\u5668\u51fa\u95ee\u9898\u540e\u670d\u52a1\u7684\u6062\u590d\u65f6\u95f4\u53d7\u9650\u4e8e DNS \u66f4\u65b0\u7684\u65f6\u95f4\uff08\u5207\u6362\u65b0\u673a\u5668\uff09\u3002\\n","Decision":"## Decision\n![][image-1]\\n","Prediction":"## Decision\n1. \u670d\u52a1\u5668\u4f7f\u7528\u6bcf\u4e2a\u670d\u52a1\u5668\u7684\u4e3b\u673a\u540d\u8fdb\u884c\u57df\u540d\u7684\u67e5\u8be2\uff0c\u5982\u679c\u5b58\u5728\u7684\u670d\u52a1\u5668\u4e0d\u53ef\u63a7\uff0c\u5c06\u4ece\u4e3b\u673a\u540d\u4e2d\u5220\u9664\uff0c\u4f7f\u7528\u4e3b\u673a\u540d\u8fdb\u884c\u67e5\u8be2\u65f6\uff0c\u4f1a\u751f\u6210\u4e00\u4e2aIP\u5730\u5740\uff0c\u540e\u7eed\u5c06\u670d\u52a1\u5668\u4f7f\u7528\u6b64IP\u5730\u5740\u8fdb\u884c\u901a\u4fe1\uff1b\\n2. \u670d\u52a1\u7aef\u5c06\u81ea\u52a8\u4ece\u4e3b\u673a\u540d\u4e2d\u5220\u9664\u6240\u6709\u670d\u52a1\u5668\u7684\u57df\u540d\uff0c\u5c06\u6240\u6709\u7684\u57df\u540d\u4fdd\u5b58\u5728\u670d\u52a1\u5668\u7684\u4e3b\u673a\u540d\u4e2d\uff0c\u670d\u52a1\u5668\u7684\u4e3b\u673a\u540d\u4f1a\u4e0d\u4f1a\u88ab\u5220\u9664\uff0c\u540c\u65f6\u670d\u52a1\u5668\u7684\u4e3b\u673a\u540d\u4f1a\u5728\u670d\u52a1\u5668\u7684\u4e3b\u673a\u540d\u4e2d\u6dfb\u52a0\u4e00\u4e2a\u540e\u7eed\u6570\u5b57\uff0c\u540e\u7eed\u670d\u52a1\u5668\u7684\u4e3b\u673a\u540d\u5c06\u4f7f\u7528\u8fd9\u4e2a\u540e\u7eed\u6570\u5b57\uff1b\\n3. \u5728\u670d\u52a1\u5668\u7684\u4e3b\u673a\u540d\u4e2d\u6dfb\u52a0\u4e00\u4e2a\u540e\u7eed\u6570\u5b57\uff0c\u540e\u7eed\u670d\u52a1\u5668\u7684\u4e3b\u673a\u540d\u5c06\u4f7f\u7528\u8fd9\u4e2a\u540e\u7eed\u6570\u5b57\uff1b\\n4. \u5728\u670d\u52a1\u5668\u7684\u4e3b\u673a\u540d\u4e2d\u6dfb\u52a0\u4e00\u4e2a\u540e\u7eed\u6570\u5b57\uff0c\u540e\u7eed\u670d\u52a1\u5668\u7684\u4e3b\u673a\u540d\u5c06\u4f7f\u7528\u8fd9\u4e2a\u540e\u7eed\u6570\u5b57\uff1b\\n5. \u5728\u670d\u52a1\u5668\u7684\u4e3b\u673a\u540d\u4e2d\u6dfb\u52a0\u4e00\u4e2a\u540e\u7eed\u6570\u5b57\uff0c\u540e\u7eed\u670d\u52a1\u5668\u7684\u4e3b\u673a\u540d\u5c06\u4f7f\u7528\u8fd9\u4e2a\u540e"}
{"Context":"## Context\\nI have chosen not to ask for clarification about the specification like I would\\nwith a client -- instead, I'll make a sane decision and document it. The\\nspecification is not totally clear on when kickers should be shown. It says \"The\\nhand name should have a full description including kickers if necessary\". When a\\nkicker resolves a tie, it should be shown -- it's necessary. But when the final\\nrank is a tie despite all kickers, should they be shown?\\nYes: because they were considered when determining the rank.\\nNo: because they were found to not impact the final rank.\\nBased on decision 0012, I'm free to make either choice based purely on\\npreference.\\n","Decision":"## Decision\n\"No\", don't show the kickers for ties.\\n","Prediction":"## Decision\nI will not show kickers in the final rank even in the case where they do not\\nimpact the final rank.\\nThis is the default behavior. It is not a hard and fast rule, but an implementation\\nof the rule that I will accept as a reasonable default.\\nI will document this decision in the docs.\\nI will not change the default behavior unless it is clear that it is not a\\nreasonable default.\\nI will not change the default behavior without discussing it with the\\ncommitters.\\n### Decision 0012\\nIn the case where a tie occurs in the final rank and all kickers are\\nconsidered, the final rank should not show kickers. This is not a hard\\nand fast rule. It is a preference. I will document it in the docs.\\nI will not change the default behavior unless it is clear that it is not a\\nreasonable default.\\nI will not change the default behavior without discussing it with the\\ncommitters.\\n### Decision 0013\\nIn the case where all kickers are considered, the final rank should show\\nkickers. This is not a hard and fast rule. It is a preference. I will\\ndocument it in the docs.\\nI will"}
{"Context":"## Context\\nWe need to save information into a persistence layer in order to query and explore the data. The data we will be working with heavily relies on describing relationships which is well suited for a graph like structure.\\n","Decision":"## Decision\nWe will use Neo4J as the persistence layer. It is a fairly popular graph database, well documented and mature product.\\n","Prediction":"## Decision\nWe will use a graph database. This will allow us to query the data by its graph structure rather than by its string or numeric format. The graph database will also allow us to query by attribute, allowing us to filter based on that attribute.\\nWe will use the [GraphDB](https:\/\/www.graphdb.org\/) library for this. GraphDB is a noSQL database that is very simple to setup and use. It is also very fast and provides a good level of scalability and fault tolerance.\\n### Use cases\\n* Querying\\n* Data exploration\\n* Data modeling\\n* Data integration\\n* Data analysis\\n* Data security\\n* Data modeling\\n* Data analysis\\n* Data integration\\n* Data security\\n* Data sharing\\n* Data versioning\\n* Data synchronization\\n* Data schema migration\\n* Data migration\\n* Data migration\\n* Data migration\\n* Data migration\\n* Data migration\\n* Data migration\\n* Data migration\\n* Data migration\\n* Data migration\\n* Data migration\\n* Data migration\\n* Data migration\\n* Data migration\\n* Data migration\\n* Data migration\\n* Data migration\\n* Data migration\\n* Data migration\\n* Data migration\\n* Data migration\\n* Data migration\\n* Data migration\\n*"}
{"Context":"## Context\\nIn order to perform a search on GP or surgery name, a repository of GP Surgeries is needed.\\nA Docker image of GP data\\n([profiles-db](https:\/\/hub.docker.com\/r\/nhsuk\/profiles-db\/)) running a MongoDB\\ninstance has been created for use by other applications and we have chosen to adapt it\\nvia indexes to be able to efficiently be used by this application as well.\\nThe (MEAN stack)[https:\/\/en.wikipedia.org\/wiki\/MEAN_(software_bundle)] has quite a lot of backing as a\\nsoftware bundle within the community and we use everything except Angular from it's solution.\\n","Decision":"## Decision\nWe have decided to use the existing Docker image rather than spend effort\\nacquiring the data again. We added indexes to ensure our functionality requirements were met,\\nwithout putting to many constraints on the image that is being consumed by other applications.\\n","Prediction":"## Decision\nWe will use MongoDB and the [MongoDB C++ driver](https:\/\/www.mongodb.com\/c++\/) to provide the\\nback-end to GP. It will be used in a Docker image that can be deployed on any machine\\nrunning a supported version of MongoDB. The C++ driver will be used as it is likely to be\\neasier to use and maintain than the .NET driver.\\nWe will also use [MongoDB's Graph API](https:\/\/www.mongodb.com\/csharp\/api\/graph) to provide graph\\ndata and will implement our own Graph API client that does not use the C++ driver.\\nWe will also use the [MongoDB C++ driver's full-text search](https:\/\/www.mongodb.com\/csharp\/api\/document\/) to\\nprovide full-text search functionality. We will use the [C++ client's full-text search](https:\/\/www.mongodb.com\/csharp\/api\/document\/#search) to provide full-text search functionality and a custom implementation of the C++ client's search API.\\nWe will use the [MongoDB C++ driver's indexing](https:\/\/www.mongodb.com\/csharp\/api\/document\/#indexing) functionality to provide indexing of GP Surgeries. We will implement the indexing API as a separate package and will use it to"}
{"Context":"## Context\\nThere are several ways of migrating data, including:\\n* Using an ActiveRecord Migration\\n* Writing a Rake task and running it after deploying a code update\\n","Decision":"## Decision\n1. When migrations do not change the database structure or otherwise break the application, we should\\nwrite a Rake task to migrate data.\\n","Prediction":"## Decision\nThe decision is to run the migration in the CI pipeline. This will ensure that any changes to the database are automatically applied to the production database.\\nThis will also ensure that any changes to the database are immediately available to all developers. This is important as any changes to the database can impact the way that the application is deployed.\\n### Benefits\\n* The migration will be run automatically in the CI pipeline, ensuring that developers are not affected by any changes to the database\\n* Developers will not need to have access to the database to run the migration\\n* Developers will be able to run the migration without having to have access to the database\\n* Developers will be able to run the migration in a CI pipeline, ensuring that any changes to the database are immediately available to all developers\\n### Consequences\\n* The migration will need to be run in a CI pipeline, which means that the deployment process needs to be automated. This will be done using the Heroku Postgres plugin and the Heroku Postgres Docker image.\\n* It will be necessary to manually run the migration in a database, which will need to be created on the Heroku Postgres Docker image before the migration is run.\\n* The migration will need to be run in a database,"}
{"Context":"## Context\\nThe ability to update\/upgrade our servers for security or testing purposes is\\nkey. We will have at least two virtual machines sharing a mounted directory on\\neach that will contain files that all servers can view. The biggest benefit will\\nbe the Web Application Firewall that will be applied to all traffic that is\\ntargeted to the `library.princeton.edu`. Using deep packet inspection malicious\\npatterns will be diverted away.\\n","Decision":"## Decision\nWe will create\\n* a smb\/cifs and\/or nfs mount that will be present on all nodes behind the\\nloadbalancer\\n* we will create a service (Kemp terminology) called `library.princeton.edu`\\n* we will create nodes that will mount the aforementioned drives\\n* we will implement SolrCloud on a remote endpoint that all nodes can read and\\nwrite to\\n* we will implement remote database (mariadb) that all nodes can read\/write to\\n","Prediction":"## Decision\nWe will run a second instance of the `nginx` server on port 443 on all\\nservers. This will allow us to update all servers with the latest version of\\n`nginx` and will also allow us to create a load balanced cluster.\\nWe will also run a second instance of the `nginx` server on port 80 on\\nall servers to allow us to create load balanced clusters with the\\n`nginx` server on port 80.\\nWe will run a second instance of the `httpd` server on port 80 on\\nall servers to allow us to create load balanced clusters with the\\n`httpd` server on port 80.\\nWe will run a second instance of the `httpd` server on port 80 on\\nall servers to create load balanced clusters with the\\n`httpd` server on port 80.\\nWe will run a second instance of the `httpd` server on port 80 on\\nall servers to create load balanced clusters with the\\n`httpd` server on port 80.\\nWe will run a second instance of the `httpd` server on port 80 on\\nall servers to create load balanced clusters with the\\n`httpd` server on port "}
{"Context":"## Context\\n> This section contains all the context one needs to understand the current state, and why there is a problem. It should be as succinct as possible and introduce the high level idea behind the solution.\\n","Decision":"## Decision\n> This section explains all of the details of the proposed solution, including implementation details.\\n> It should also describe affects \/ corollary items that may need to be changed as a part of this.\\n> If the proposed change will be large, please also indicate a way to do the change to maximize ease of review.\\n> (e.g. the optimal split of things to do between separate PR's)\\n","Prediction":"## Decision\n* The current implementation of the API will be deprecated in favour of a new API. A new API is being developed, which will be used for the long term. This API is not being changed at this time.\\n* The decision to move to a new API is an architectural decision. It is not a decision that can be undone.\\n* The decision to move to a new API will be documented, and the reasons for the decision will be documented.\\n* The decision to move to a new API will be documented as part of a release process.\\n* The decision to move to a new API will be documented in the form of a ticket in the Jira\\ndocuments project.\\n* The decision to move to a new API will be documented in the form of a pull request to the\\ndocuments repository.\\n* The decision to move to a new API will be documented in the form of a discussion on the\\ndocuments mailing list.\\n* The decision to move to a new API will be documented in the form of a blog post.\\n* The decision to move to a new API will be documented in the form of a PR to the\\ndocuments repository.\\n* The decision to move to a"}
{"Context":"## Context and Problem Statement\\n* How do I start the explorer in my machine?\\n* Why do the scenes run in a WebWorker?\\n* Where are the builder resources stored?\\n* What are the main repositories?\\n* How can I add a feature to the SDK?\\nThose are common questions that are raised by both experienced contributor or newcomers to the project.\\nWe are searching for alignment for an approach towards a more sustainable future where people can search and find those answers without depending on specific individuals.\\n","Decision":"## Decision\nWe choose to _start a cultural shift towards documentation as a priority_ because:\\n* We want to accept more contributions to the project\\n* Our source code is public, but it has proven to be very complex to understand without help\\n* Onboarding new people to the project is very difficult and almost always requires a mentor\\n#### Actionables\\n* Start communicating to the teams this initiative\\n* Try to slowly start documenting new features or architectural changes\\n* Create a gitbook where we can add documentation in a bureaucracy-free fashion. We spoke about playbooks like: `How to clone and start the Decentraland client`\\n* Evaluate if we can start documenting what we currently have retroactively, no need to rush.\\n","Prediction":"## Decision\n* We will use WebWorker for the explorer and the builder in the future.\\n* We will use the [github.com\/kotlin\/rocket\/tree\/master\/rocket-webworker](https:\/\/github.com\/kotlin\/rocket\/tree\/master\/rocket-webworker) for the current explorer and builder.\\n* We will use the [github.com\/kotlin\/rocket\/tree\/master\/rocket-builder](https:\/\/github.com\/kotlin\/rocket\/tree\/master\/rocket-builder) for the current builder.\\n* We will use the [github.com\/kotlin\/rocket\/tree\/master\/rocket-projects](https:\/\/github.com\/kotlin\/rocket\/tree\/master\/rocket-projects) for the current rocket projects.\\n* We will use the [github.com\/kotlin\/rocket\/tree\/master\/rocket-extensions](https:\/\/github.com\/kotlin\/rocket\/tree\/master\/rocket-extensions) for the current rocket extensions.\\n* We will use the [github.com\/kotlin\/rocket\/tree\/master\/rocket-templates](https:\/\/github.com\/kotlin\/rocket\/tree\/master\/rocket-templates) for the current rocket templates.\\n* We will use [github.com\/kotlin\/rocket\/tree\/master\/rocket-builder](https:\/\/github.com\/kotlin\/rocket\/tree\/master\/rocket-builder) for the"}
{"Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"## Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this\\narticle: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","Prediction":"## Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/"}
{"Context":"## Context and Problem Statement\\nIf you are deployed somewhere that is an ephemeral filesystem then we can't rely on standard error handlers\/retriers failed messages in  (unless you use a docker volume mount or similar)l; so we need to have story or some codified best practise such that people can use it easily when they are deployed into docker containers and the like.\\nIf you are full ESB stylings, and you have dead-letter queues, then perhaps this doesn't matter, since everything would work off a queue, and be triggered by message from a queue.\\n## Decision Drivers\\n* Minimal configuration required to get the desired behaviour\\n* Upgrade to behaviour should not require changes to multiple interlok configurations.\\n* Filesystem agnostic\\n* Think about how this might manifest itself as a 'Work-Unit'\\n","Decision":"## Decision\n* Minimal configuration required to get the desired behaviour\\n* Upgrade to behaviour should not require changes to multiple interlok configurations.\\n* Filesystem agnostic\\n* Think about how this might manifest itself as a 'Work-Unit'\\nJetty Trigger; however, there is documentation that needs to happen around the behaviour of RetryMessageErrorHandler vis-a-vis JMS Async producers.\\n### Vanilla workflow based system.\\nWe take as our base implementation from [this blog post](https:\/\/interlok.adaptris.net\/blog\/2017\/10\/19\/interlok-s3-error-store.html). The configuration detailed therein could be made into a template, and subsequently imported into multiple interlok instances; but maintaining the configuration is harder than it needs to be vis-a-vis upgrading, managing multiple instances that want to use this concept.\\n- Good, because no changes\\n- Bad, hard to maintain\\n### Jetty Trigger\\nStill taking [this blog post](https:\/\/interlok.adaptris.net\/blog\/2017\/10\/19\/interlok-s3-error-store.html) as your feature set but with the design decision to separate out the raw payload away from the metadata. This is simply a pragmatic choice so that if we are dealing with arbitrarily large messages we don't need\\nto think about how to encode both the payload + metadata into a single blob.\\nThe target is to be able to something like this :\\n- `curl -XGET http:\/\/localhost\/api\/list-failed` and get a JSON array back that essentially that does an _ls of the target directory_\\n- `curl -XPOST http:\/\/localhost:8080\/api\/retry\/{msg-id}` and get that message automatically retried to the target workflow.\\nThis can be described with a pretty diagram generated [via plantuml](.\/assets\/0008-restful-sequence.puml)\\n![Sequence](http:\/\/www.plantuml.com\/plantuml\/proxy?cache=no&src=https:\/\/raw.githubusercontent.com\/adaptris\/interlok\/develop\/docs\/adr\/assets\/0008-restful-sequence.puml)\\nThis boils down to a new FailedMessageRetrier implementation (`RetryFromJetty`) and a supporting interface in `interlok-core`. Since FailedMessageRetriers know about all the workflows, this is the right object to use. We can re-use whatever components we need to.\\n```java\\npublic class RetryFromJetty implements FailedMessageRetrier {\\n\/\/ When configuring the JettyConsumer\\n\/\/ Same JettyConsumer for Reporting ?\\n\/\/ \/api\/retry -> \/api\/retry\/*\\n\/\/ \/api\/retry\/ -> \/api\/retry\/*\\n\/\/ \/api\/retry\/* -> \/api\/retry\/*\\nprivate String retryEndpointPrefix;\\nprivate String reportingEndpoint;\\nprivate JettyConnection connection;\\nprivate BlobListRenderer reportRenderer;\\nprivate RetryHandler handler;\\n@AdvancedConfig(rare=true)\\nprivate String retryHttpMethod=\"POST\"\\n@AdvancedConfig\\nprivate Boolean deleteAfterSubmit;\\n}\\n```\\n```java\\npublic interface RetryHandler {\\nList<RemoteFile> report()\\nAdaptrisMessage buildForRetrying();\\nvoid delete(String msgId);\\n}\\n```\\nThis ultimately manifests itself with XML two separate elements\\n```xml\\n<failed-message-retrier class=\"retry-from-jetty\">\\n<retry-endpoint-prefix>\/api\/retry\/<\/retry-endpoint-prefix>\\n<reporting-endpoint>\/api\/list-failed<\/reporting-endpoint>\\n<jetty-connection class=\"embedded-jetty-connection\"\/>\\n<report-renderer class=\"json-blob-list-renderer\"\/>\\n<handler class=\"from-fs\">\\n<base-url>file:\/\/localhost\/.\/fs\/errors<\/base-url>\\n<payload-name>payload.bin<\/payload-name>\\n<metadata-name>metadata.properties<\/metadata-name>\\n<\/handler>\\n<\/failed-message-retrier>\\n```\\nAnd a service that understands what it needs to write to the filesystem (rather than a StandaloneProducer + FsProducer).\\n```xml\\n<service class=\"put-message-on-filesystem\">\\n<bucket-name>bucket<\/bucket-name>\\n<base-url>ile:\/\/localhost\/.\/fs\/errors<\/base-url>\\n<payload-name>payload.bin<\/payload-name>\\n<metadata-name>metadata.properties<\/metadata-name>\\n<!-- Consider having 2 \"locational\" pieces of metadata that can used by\\nlater services.\\nfailed-message-payload=file:\/\/localhost\/.\/fs\/errors\/<msg-id>\/payload.bin\\nfailed-message-metadata=file:\/\/localhost\/.\/fs\/errors\/<msg-id>\/metadata.properties\\n-->\\n<\/service>\\n```\\n- Good, because pluggable\\n- Good, because it abstracts the behavioural complexity\\n- Bad, it's probably not as free form as some power-users want\\n#### Switching to cloud storage.\\nIf we consider the extended use-case which is to use S3 \/ Azure as your blob storage then this would have 2 custom implementations (for explicitness)\\n```xml\\n<handler class=\"get-from-s3\">\\n<aws-connection class=\"timebombed-credentials\"\/>\\n<bucket-name>bucket<\/bucket-name>\\n<s3-prefix>MyInterlokInstance\/failed\/<\/s3-prefix>\\n<payload-name>payload.bin<\/payload-name>\\n<metadata-name>metadata.properties<\/metadata-name>\\n<\/handler>\\n```\\nAnd something that \"does the same thing\" as part of a message-error-handler chain.\\n```xml\\n<service class=\"upload-message-to-s3\">\\n<aws-connection class=\"timebombed-credentials\"\/>\\n<bucket-name>bucket<\/bucket-name>\\n<s3-prefix>MyInterlokInstance\/failed\/<\/s3-prefix>\\n<payload-name>payload.bin<\/payload-name>\\n<metadata-name>metadata.properties<\/metadata-name>\\n<!-- Consider having 2 \"locational\" pieces of metadata that can used by\\nlater services.\\nfailed-message-payload=s3:\/\/bucket\/path\/to\/<msg-id>\/payload.bin\\nfailed-message-metadata=s3:\/\/bucket\/path\/to\/<msg-id>\/metadata.properties\\n-->\\n<\/service>\\n```\\n","Prediction":"## Decision\n*Minimal configuration required to get the desired behaviour\\n*Upgrade to behaviour should not require changes to multiple interlok configurations.\\n* Filesystem agnostic\\n* Think about how this might manifest itself as a 'Work-Unit'\\nChosen option: \"Use a custom error handler and a custom policy\", because it provides a simple solution to the problem and is likely to be the least change to the architecture.\\n### Positive Consequences\\n* No configuration changes required\\n* Easy to understand error handling\\n* No need to think about how the error handling might manifest itself\\n* It is likely to be the least change to the architecture\\n### Negative Consequences\\n* Will require users to understand the error handling\\n* If the error handling is not well documented, the user may not be able to understand what is going on\\n* It is likely to require more configuration\\n* Could be more difficult to understand and debug\\n* It may not be the least change to the architecture\\n### Impact on other Systems\\n* No impact on other systems that are using the file system\\n* If the error handling is not well documented, the user may not be able to understand what is going on\\n* It is likely to require more configuration\\n* Could be more"}
{"Context":"## Context\\nWe need to keep extendable architecture in order to prepare for future changes,\\nbecause we work in very flexible manner and requirements changes and evolve a lot.\\n","Decision":"## Decision\nIn backend directory:\\n- *libs* - technical parts of software - implementations of database connections etc. In the future will be moved to npm packages,\\nbut we leave it now for easier and faster development to avoid unnecessary complexity.\\nFolders in backend **src** directory are splitted by names of bounded contexts.\\nEach BC, which follow CQRS manner, has following subdirectories:\\n- *write-side* - keeps logical separated parts, which keep ubiquitous language + one domain inside each boundary\\nEach bounded-context will follow directories in manner of Domain-Driven Design Layered Architecture proposed by Eric Evans.\\n- *read-side* - projections from domain events of write-side\\n","Prediction":"## Decision\nWe will use Spring Boot as a framework. Spring Boot is a lightweight, open-source and cross-platform framework for building simple, but powerful, RESTful web services.\\nSpring Boot follows the Model View Controller (MVC) architectural pattern.\\nSpring Boot provides a rich set of features to help you build services quickly and easily.\\nSpring Boot provides a convenient way to integrate with third-party libraries and services.\\nSpring Boot provides a configuration management system to help you easily deploy your application.\\nSpring Boot provides a standard way to run your application with built-in support for\\ndifferent database engines, such as MySQL, PostgreSQL, Oracle, SQLServer, SQLite, and\\nMariaDB.\\nSpring Boot provides a standardized way to run your application with built-in support for\\ndifferent logging frameworks, such as log4j, logback, log4j, log4cxx, and\\nlog4net.\\nSpring Boot provides a standardized way to run your application with built-in support for\\ndifferent caching frameworks, such as memcached, redis, and memcached.\\nSpring Boot provides a standardized way to run your application with built-in support for\\ndifferent networking frameworks, such as HTTP, HTTPS, TCP, and UDP.\\nSpring Boot provides a standardized way to"}
{"Context":"## Context\\nOur implementation of AtlasDB clients and the TimeLock server were interacting in ways that were causing the TimeLock\\nserver to experience large thread buildups when running with HTTP\/2. This manifested in\\n[issue #1680](https:\/\/github.com\/palantir\/atlasdb\/issues\/1680). The issue was eventually root caused to long-running\\nlock requests in excess of the Jetty idle timeout; the server would close the relevant HTTP\/2 stream, but *not*\\nfree up the resources consumed by the request. Eventually, all server threads on the leader node would be busy handling\\nlock requests that had timed out. The leader would thus not respond to pings, resulting in other nodes proposing\\nleadership. This is problematic as leader elections cause all locks to be lost, and thus most\\ninflight transactions will fail. A concrete trace is as follows:\\n1. Client A acquires lock L\\n2. Client B blocks on acquiring lock L, with request B1\\n3. The idle timeout for Client B's connection runs out, and we close the HTTP\/2 stream\\n4. Client B retries, and blocks on acquiring lock L, with request B2\\n5. Client A releases L\\n6. Request B1 is granted, but client B is no longer listening on it\\n7. (2 minutes) The idle timeout for Client B will expire four times, and Client B retries with requests B3, B4, B5, B6\\n8. The lock granted to request B1 is reaped, and request B2 is granted, but client B is not listening for it\\nSince we retry every 30 seconds by default but only \"service\" one request every 2 minutes and 5 seconds, we accumulate\\na backlog of requests. Also, observe that setting the idle timeout to 2 minutes and 5 seconds does not solve the\\nproblem (though it does mitigate it), since multiple clients may be blocking on acquiring the same lock.\\n","Decision":"## Decision\n### Time Limiting Lock Requests\\nWe decided that solutions to this problem should prevent the above pattern by satisfying one or both of the following:\\n1. Prevent the idle timeout from reasonably occurring.\\n2. Ensure resources are freed if the idle timeout triggers.\\nWe introduced a time limit for which a lock request is allowed to block - we call this the *blocking timeout*.\\nThis is set to be lower than the Jetty idle timeout by a margin, thus achieving requirement 1. Even if we lose the\\nrace, we will still free the resources shortly after, thus achieving requirement 2.\\nIn the event that a lock request blocks for longer than the blocking timeout, we interrupt the requesting thread and\\nnotify the client by sending a `SerializableError` which wraps a `BlockingTimeoutException`. Upon receiving a\\n503, a client checks the nature of the error occurring on the server; in the event of `BlockingTimeoutException`,\\nwe retry on the same node, and reset the counter of the number of times we've failed to zero.\\nThe timeout mechanism is implemented using Guava's `TimeLimiter`, and server time is thus considered authoritative; we\\nbelieve this is reasonable as it is used for both our time limiting and for Jetty's handling of idle timeouts. Thus,\\nwe are relatively resilient to clock drift relative to the client or to other nodes in the TimeLock cluster.\\n### Alternatives Considered\\n#### 1. Significantly increase the Jetty idle timeout\\nWe could have configured the recommended idle timeout for TimeLock to be substantially longer than we expect any lock\\nrequest to reasonably block for, such as 1 day.\\nThis solution is advantageous in that it is simple. However, the current default of 30 seconds is already longer than\\nwe would expect any lock requests to block for. Furthermore, in the event of client or link failures, it would be\\npossible that resources would be unproductively allocated to the associated connections for longer periods of time.\\nWe would also introduce a dependency on the idle timeout on the HTTP client-side, which would also need to be\\nincreased to account for this (the current default is 60 seconds).\\n#### 2. Convert the lock service to a non-blocking API\\nWe could have changed the lock API such that lock requests return immediately regardless of whether the lock being\\nasked for is available or not. If any lock being asked for was not available yet, the server would return a token\\nindicating that the request was to be satisfied. The client can then, at a later time, poll the server with its token\\nto ask if its request had been satisfied; alternatively, we could investigate HTTP\/2 or WebSocket server push.\\nThis solution is likely to be the best long-term approach, though it does involve a significant change in the API\\nof the lock service which we would prefer not to make at this time.\\n#### 3. Implement connection keep-alives \/ heartbeats\\nWe close the connection if no bytes have been sent or received for the idle timeout. Thus, we can reset this timeout\\nby sending a *heartbeat message* from the client to the server or vice versa, at a frequency higher than the idle\\ntimeout. We would probably prefer this to live on the server, since the idle timeout is configured on the server-side.\\nThis solution seems reasonable, though it does not appear to readily be supported by Jetty.\\n#### 4. Send a last-gasp message to the lock service to free resources before the stream closes\\nAn idea we considered was to have Jetty free resources on the lock service before closing the HTTP\/2 stream.\\nThis solution appears to be the cleanest of the \"free resources\"-based solutions, including the one we chose to\\nimplement. Unfortunately, while this feature has been requested in Jetty, as at time of writing this has not\\nbeen implemented yet; see [Jetty issue #824](https:\/\/github.com\/eclipse\/jetty.project\/issues\/824).\\n#### 5. Have clients truncate individual requests to the idle timeout\\nAn alternative to having the server return `BlockingTimeoutException`s on long-running requests would be for clients\\nto trim down any requests to an appropriate length (or, in the case of `BLOCK_INDEFINITELY`, indefinitely send\\nrequests of a suitable length). For example, with the default idle timeout of 30 seconds, a client wishing to block\\nfor 45 seconds could send a lock request that blocks for 30 seconds, and upon failure submit another request that\\nblocks for just under 15 seconds (suitably accounting for network overheads).\\nThis solution is relatively similar to what was implemented, though it requires clients to know what the\\naforementioned \"appropriate length\" should be (it needs to be the idle timeout or less) which is inappropriate as\\nthat timeout is configured on the server side.\\n#### 6. Implement a \"magic\" HTTP status code or header to ask clients to retry\\nAn alternative to serializing exceptions into `SerializableError` s could be defining a specific HTTP status code\\nand\/or custom header to indicate that a blocking timeout has occurred and\/or clients should retry. This is used\\nin practice e.g. in nginx, where a 495 indicates an error with a client's SSL certificates.\\nThis solution would be simpler than serializing exceptions; our existing `AtlasDbErrorDecoder` already switched on the\\nstatus code returned in an HTTP response. However, we prefer not to introduce any custom status codes where feasible\\n(since clients are unlikely to understand these status codes). A similar argument, though perhaps weaker, applies\\nfor headers as well.\\n","Prediction":"## Decision\nWe will raise an issue with AtlasDB on GitHub to track the issue and work towards resolving it. We will also\\nmake sure that other AtlasDB clients are not affected by this issue.\\nWe will raise an issue with AtlasDB on GitHub to track the issue and work towards resolving it. We will also\\nmake sure that other AtlasDB clients are not affected by this issue.\\nWe will raise an issue with AtlasDB on GitHub to track the issue and work towards resolving it. We will also\\nmake sure that other AtlasDB clients are not affected by this issue.\\nWe will raise an issue with AtlasDB on GitHub to track the issue and work towards resolving it. We will also\\nmake sure that other AtlasDB clients are not affected by this issue.\\nWe will raise an issue with AtlasDB on GitHub to track the issue and work towards resolving it. We will also\\nmake sure that other AtlasDB clients are not affected by this issue.\\nWe will raise an issue with AtlasDB on GitHub to track the issue and work towards resolving it. We will also\\nmake sure that other AtlasDB clients are not affected by this issue.\\nWe will raise an issue with AtlasDB on GitHub to track the issue and work towards resolving it"}
{"Context":"## Context\\nStellar testnet network is overloaded recently and causes a lot of delay and errors\\n","Decision":"## Decision\nUsing Stellar Mainnet network (STD TFT). But prices will vary according to the explorer type, example: (devnet would cost 1% from mainnet, testnet would cost 10% from mainnet price)\\n","Prediction":"## Decision\nUse [Terraform](https:\/\/www.terraform.io\/) to deploy a testnet on AWS Fargate\\n[Image: image] which has a better performance than EKS and can be scaled up\/down\\nwithout any downtime\\n![Image 1](.\/images\/terraform-provider-aws\/images\/terraform-provider-aws-ec2-ami-1.png)\\n![Image 2](.\/images\/terraform-provider-aws\/images\/terraform-provider-aws-ec2-ami-2.png)\\n![Image 3](.\/images\/terraform-provider-aws\/images\/terraform-provider-aws-ec2-ami-3.png)\\n![Image 4](.\/images\/terraform-provider-aws\/images\/terraform-provider-aws-ec2-ami-4.png)\\n![Image 5](.\/images\/terraform-provider-aws\/images\/terraform-provider-aws-ec2-ami-5.png)\\n![Image 6](.\/images\/terraform-provider-aws\/images\/terraform-provider-aws-ec2-ami-6.png)\\n![Image 7](.\/images\/terraform-provider-aws\/images\/terraform-provider-aws-ec2-ami-7.png)\\n![Image 8](.\/images\/"}
{"Context":"## Context\\nAll data that is related to geographic structures is added to the ODH in a standardized way.\\n","Decision":"## Decision\nGeoJSON is a format for encoding a variety of geographic data structures (https:\/\/tools.ietf.org\/html\/rfc7946)\\nGeoJSON-events extends RFC 7649 GeoJSON with instants and intervals of time (https:\/\/github.com\/sgillies\/geojson-events)\\n","Prediction":"## Decision\nAll data that is related to geographic structures will be stored in the ODH.\\nThis will be achieved by using the following structure:\\n* `geography`: A structure that is used to store information about the geographical structure.\\n* `geography_id`: A PK that uniquely identifies a geography.\\n* `geography_type`: A PK that uniquely identifies the type of geography (e.g. `NorthAmerica` or `Europe`).\\n* `geography_name`: A PK that uniquely identifies a geography name.\\n* `geography_description`: A string that uniquely identifies a geography description.\\n* `geography_location`: A structure that is used to store information about the geographical structure.\\n* `geography_location_id`: A PK that uniquely identifies a geography location.\\n* `geography_location_type`: A PK that uniquely identifies the type of geography (e.g. `NorthAmerica` or `Europe`).\\n* `geography_location_name`: A PK that uniquely identifies a geography name.\\n* `geography_location_description`: A string that uniquely identifies a geography description.\\n* `geography_location_state`: A structure that is used to store information about the geographical structure.\\n* `geography_location_state_id`:"}
{"Context":"## Context\\nUsing a JSON file for the `allowed-users` listing makes the code within the\\napplication straight forward and ofc, the default data format within a JS app\\nshould be JSON (controversial). However, managing the file is (at least atm)\\nbeing done by a human. Editing a JSON file and ensuring it is valid JSON is\\nmore challenging than a CSV file. If the file is malformed the application is\\nunavailable, therefore, reducing this risk is paramount.\\n","Decision":"## Decision\nThe decision is to use a CSV format for the listing of allowed users.\\n","Prediction":"## Decision\nWe have decided to use a JSON file to store the `allowed-users` listing.\\nThis is to allow for the data to be easily managed and for the application to\\nbe deployed without the need for manual edits to the file. This also\\nsimplifies the code which is now being managed by a human.\\nThis file is stored in the `app\/protected` folder. We are using a JSON file\\nwhich is served over HTTPS for the following reasons:\\n- The `allowed-users` file is of a high quality and does not contain any\\nmalformed JSON\\n- It is easy to generate a new JSON file for the application by running\\n`yarn serve --config app\/protected\/allowed-users.json`\\n- It is easy to generate a new JSON file for the application by running\\n`yarn serve --config app\/protected\/allowed-users.json --save`\\n- It is easy to generate a new JSON file for the application by running\\n`yarn serve --config app\/protected\/allowed-users.json --export`\\n- It is easy to generate a new JSON file for the application by running\\n`yarn serve --config app\/protected\/allowed-users.json --export --save`\\"}
{"Context":"## Context\\nA ZooKeeper timeout last night brought several Solr servers down. Factors that might make servers unresponsive\\nare garbage collection, network traffic\/latency, and high amounts of disk I\/O.\\n","Decision":"## Decision\nIncrease the ZooKeeper timeout from 15 seconds to 60 seconds.\\n","Prediction":"## Decision\nWe will use ZooKeeper's `timeout` parameter to control ZooKeeper timeout duration.\\nWe will set this value to 30 minutes in our ZooKeeper configuration.\\nWe will monitor ZooKeeper sessions with `solrconfig status` to ensure a ZooKeeper connection is not lost.\\nWe will set `solrconfig timeout` to a value less than 30 minutes to allow for ZooKeeper timeouts for other reasons.\\nWe will set `solrconfig maxReceivedConnections` to a value less than 30 to allow for ZooKeeper timeouts for other reasons.\\nWe will set `solrconfig maxWait` to a value less than 30 to allow for ZooKeeper timeouts for other reasons.\\nWe will set `solrconfig maxWaitTime` to a value less than 30 to allow for ZooKeeper timeouts for other reasons.\\nWe will set `solrconfig maxWait` to a value less than 30 to allow for ZooKeeper timeouts for other reasons.\\nWe will set `solrconfig maxIdle` to a value less than 30 to allow for ZooKeeper timeouts for other reasons.\\nWe will set `solrconfig maxRequestsPerSecond` to a value less than 20 to allow for ZooKeeper requests"}
{"Context":"## Context\\nAPI should have the documentation, we should define the process of delivering API and documentation.\\n","Decision":"## Decision\nBy defining APIs outside the code, we want to facilitate early review feedback and also a development discipline that focus service interface design on:\\n- profound understanding of the domain and required functionality\\n- generalized business entities \/ resources, i.e. avoidance of use case specific APIs\\n- clear separation of WHAT vs. HOW concerns, i.e. abstraction from implementation aspects \u2014 APIs should be stable even if we replace complete service implementation including its underlying technology stack\\nUseful links:\\n- https:\/\/opensource.zalando.com\/restful-api-guidelines\/#api-first\\n","Prediction":"## Decision\nWe will use [Django Docutils](https:\/\/docs.djangoproject.com\/en\/1.9\/topics\/docutils\/) for generating documentation.\\nWe will use [Django Docutils+Django Docutils](https:\/\/github.com\/Docutils\/django-docutils+django-docutils) for generating API documentation.\\nWe will use [Django Docutils+Django Docutils+Django Docutils](https:\/\/github.com\/Docutils\/django-docutils+django-docutils) for generating API documentation.\\nWe will use [Django Docutils+Django Docutils+Django Docutils+Django Docutils](https:\/\/github.com\/Docutils\/django-docutils+django-docutils) for generating API documentation.\\nWe will use [Django Docutils+Django Docutils+Django Docutils+Django Docutils](https:\/\/github.com\/Docutils\/django-docutils+django-docutils) for generating API documentation.\\nWe will use [Django Docutils+Django Docutils+Django Docutils+Django Docutils](https:\/\/github.com\/Docutils\/django-docutils+django-docutils) for generating API documentation.\\nWe will use [Django Docutils+D"}
{"Context":"## Context and Problem Statement\\nAs part of PLATUI-1393, browser based acceptance tests were added that rely on `assets-frontend`. These acceptance tests\\nrun as part of the build in Jenkins, which means `ASSETS_FRONTEND` needs to be added as a parameter to the `build-jobs`\\njob builder `.withServiceManager` call. However, `assets-frontend` has specific logic that means all versions are started\\nvia Service Manager, that is multiple instances of `assets-frontend` will start, not just the latest release.\\nThere is logic in Service Manager to pass an `application.conf` file to start only required versions of `assets-frontend`,\\nhowever as we are not using Service Manager to start `contact-frontend` itself, this logic does not get called. Therefore,\\nall verisons of `assets-frontend` start during when running a build, either after merge or as part of the PR builder for\\n`contact-frontend`.\\n## Decision Drivers\\n* `assets-frontend` is a deprecated service - we want to ensure the functionality using it does not break, but we are\\nreluctant to invest large amounts of time in it\\n* Service Manager is a Python application used heavily across the Platform and not owned by PlatUI, and we would like to\\navoid making changes to it if possible to support our workflow.\\n* Similarly the job builders in the `build-jobs` repo are heavily shared, and we would like to avoid making changes to\\n`build-jobs` if possible\\n","Decision":"## Decision\n* `assets-frontend` is a deprecated service - we want to ensure the functionality using it does not break, but we are\\nreluctant to invest large amounts of time in it\\n* Service Manager is a Python application used heavily across the Platform and not owned by PlatUI, and we would like to\\navoid making changes to it if possible to support our workflow.\\n* Similarly the job builders in the `build-jobs` repo are heavily shared, and we would like to avoid making changes to\\n`build-jobs` if possible\\nChosen option: \"Accept that builds of `contact-frontend` will be slower due to starting all versions of\\n`assets-frontend`\", because:\\n* It is a quick solution that does not involve further investigation to support a test case that we hope to deprecate\\n* The performance impact will slow down development in terms of PR building but will not affect local development\\n* This decision can be easily revisited if performance becomes a problem, i.e. by making no changes there will be no\\nrolling back needed if we remove the test or decide to change other services\\n### Positive Consequences\\n* Allows us to add browser-based acceptance tests for forms with `contact-frontend` relying on `assets-frontend`\\n* Does not involve further development time\\n* Proven to work in the build environment\\n### Negative Consequences\\n* Will definitely cause a negative impact on the time taken to build a release, or run a PR builder\\n","Prediction":"## Decision\n* `assets-frontend` is a deprecated service - we want to ensure the functionality using it does not break, but we are\\nreluctant to invest large amounts of time in it\\n* Service Manager is a Python application used heavily across the Platform and not owned by PlatUI, and we would like to\\navoid making changes to it if possible to support our workflow.\\n* Similarly the job builders in the `build-jobs` repo are heavily shared, and we would like to avoid making changes to\\n`build-jobs` if possible\\nChosen option: \"Use the existing `build-jobs` job builder to start the `assets-frontend` service, and use\\n`publish` to publish the `assets-frontend` service to the `platui-frontend` registry. This will mean\\nthat we will not need to add the `assets-frontend` service to the `build-jobs` job builder, and we will be able to\\ncontinue to use the existing `build-jobs` job builder to start the `contact-frontend` service.\\n### Positive Consequences\\n* We can continue to use the existing `build-jobs` job builder to start the `contact-frontend` service\\n* We"}
{"Context":"## Context\\n- The problem instructions state \"C++ or node.js are preferred\".\\n- I am most familiar with node.js out of those two.\\n- I am completing this problem in my spare time and have a limited amount of\\ntime to spend on it: it's convenient to work faster if possible.\\n","Decision":"## Decision\nUse Node.js to complete this problem.\\n","Prediction":"## Decision\n- Node.js is the preferred language for this project.\\n- C++ is the preferred language for the design and implementation of the\\ndatabase\\n- C++ is the preferred language for the database itself\\n- C++ is the preferred language for the database schema\\n- C++ is the preferred language for testing and deployment\\n- C++ is the preferred language for maintaining the database\\n- C++ is the preferred language for interacting with the database\\n- C++ is the preferred language for interacting with the web API\\n- C++ is the preferred language for interacting with the mobile API\\n- C++ is the preferred language for interacting with the command-line tool\\n- C++ is the preferred language for interacting with the command-line tool's output\\n- C++ is the preferred language for interacting with the command-line tool's error messages\\n- C++ is the preferred language for interacting with the command-line tool's documentation\\n- C++ is the preferred language for interacting with the command-line tool's source code\\n- C++ is the preferred language for interacting with the command-line tool's test suite\\n- C++ is the preferred language for interacting with the command-line tool's build system\\n- C++ is the preferred language for interacting with the command-line"}
{"Context":"## Context\\nCurrently, each application interacting with the [Oasis Network] defines its own\\nmethod of generating an account's private\/public key pair.\\n[Account]'s public key is in turn used to derive the account's address of the\\nform `oasis1 ... 40 characters ...` which is used to for a variety of operations\\n(i.e. token transfers, delegations\/undelegations, ...) on the network.\\nThe blockchain ecosystem has developed many standards for generating keys which\\nimprove key storage and interoperability between different applications.\\nAdopting these standards will allow the Oasis ecosystem to:\\n- Make key derivation the same across different applications (i.e. wallets).\\n- Allow users to hold keys in hardware wallets.\\n- Allow users to hold keys in cold storage more reliably (i.e. using the\\nfamiliar 24 word mnemonics).\\n- Define how users can generate multiple keys from a single seed (i.e.\\nthe 24 or 12 word mnemonic).\\n","Decision":"## Decision\n### Mnemonic Codes for Master Key Derivation\\nWe use Bitcoin's [BIP-0039]: _Mnemonic code for generating deterministic keys_\\nto derivate a binary seed from a mnemonic code.\\nThe binary seed is in turn used to derive the _master key_, the root key from\\nwhich a hierarchy of deterministic keys is derived, as described in\\n[Hierarchical Key Derivation Scheme][hd-scheme].\\nWe strongly recommend using 24 word mnemonics which correspond to 256 bits of\\nentropy.\\n### Hierarchical Key Derivation Scheme\\nWe use Sathoshi Labs' [SLIP-0010]: _Universal private key derivation from master\\nprivate key_, which is a superset of\\nBitcoin's [BIP-0032]: _Hierarchical Deterministic Wallets_ derivation algorithm,\\nextended to work on other curves.\\nAccount keys use the [edwards25519 curve] from the Ed25519 signature scheme\\nspecified in [RFC 8032].\\n### Key Derivation Paths\\nWe adapt [BIP-0044]: _Multi-Account Hierarchy for Deterministic Wallets_ for\\ngenerating deterministic keys where `coin_type` equals 474, as assigned to the\\nOasis Network by [SLIP-0044].\\nThe following [BIP-0032] path should be used to generate keys:\\n```\\nm\/44'\/474'\/x'\\n```\\nwhere `x` represents the key number.\\nNote that all path levels are _hardened_, e.g. `44'` is `44 | 0x8000000` or\\n`44 + 2^31`.\\nThe key corresponding to key number 0 (i.e. `m\/44'\/474'\/0'`) is called the\\n_primary key_.\\nThe account corresponding to the _primary key_ is called the _primary account_.\\nApplications (i.e. wallets) should use this account as a user's default Oasis\\naccount.\\n","Prediction":"## Decision\nWe will use the [Golang's crypto\/hmac package](https:\/\/golang.org\/pkg\/crypto\/hmac\/) to\\ngenerate the account's public\/private key pairs.\\nTo enable the Oasis ecosystem to generate keys using the Golang crypto library, we will\\ndefine the following `GoKey` struct:\\n```\\ntype GoKey struct {\\nkey uint32\\npubKey []byte\\n}\\n```\\nWe will also define the following `GoKeyPair` struct:\\n```\\ntype GoKeyPair struct {\\nkey []byte\\nkeyPair []GoKey\\n}\\n```\\nWe will also define the following `GoKeyPair` struct:\\n```\\ntype Account struct {\\nkey []byte\\nkeyPair []GoKey\\n}\\n```\\nWe will also define the following `GoKeyPair` struct:\\n```\\ntype Account struct {\\nkey []byte\\nkeyPair []GoKey\\n}\\n```\\nWe will also define the following `GoKeyPair` struct:\\n```\\ntype Account struct {\\nkey []byte\\nkeyPair []GoKey\\n}\\n```\\nWe will also define the following `GoKeyPair` struct:\\n```\\n"}
{"Context":"## Context\\nWe need to decide whether to configure our web application code base ourselves or make\\nuse of the [Create React App](https:\/\/github.com\/facebookincubator\/create-react-app)\\nto provide project configuration.\\n","Decision":"## Decision\nWe have decided to use the Create React App project to provide the base for our React\\nproject as this brings best practice configuration and build and is actively being\\ndeveloped.\\n","Prediction":"## Decision\nWe will use Create React App to configure our web application code base.\\nWe will use [Webpack](https:\/\/webpack.js.org\/) to bundle our JavaScript files.\\nWe will use [Rollup](https:\/\/rollupjs.com\/) to bundle our CSS files.\\nWe will use [Open-Source Licenses](https:\/\/open-source-licenses.org\/) to allow\\nour code base to be used by other developers.\\nWe will use [Webpack Bundler](https:\/\/webpack.js.org\/bundlers\/) to bundle our JavaScript and CSS\\nfiles.\\nWe will use [Webpack Config](https:\/\/webpack.js.org\/en\/guide\/configuration\/) to configure\\nour application configuration.\\nWe will use [Webpack-dev-server](https:\/\/webpack.js.org\/en\/guide\/configuration\/#webpack-dev-server) to serve\\nour application in a local development environment.\\nWe will use [Webpack-dev-server (hot reload)](https:\/\/webpack.js.org\/en\/guide\/configuration\/#webpack-dev-server-hot-reload) to serve\\nour application in a production environment.\\nWe will use [Rollup.io](https:\/\/rollupjs.com\/) to serve static files from our application\\nfiles.\\nWe will use [Rollup.io (server-side rendering)]("}
{"Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"## Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","Prediction":"## Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\nWe will use [Decision Manager](https:\/\/www.atlassian.com\/software\/jira\/overview) to manage the ADRs.\\nDecision Manager will be used to manage the ADRs.\\nDecision Manager will be integrated with [Jira](https:\/\/www.atlassian.com\/software\/jira\/overview) to allow for easy record keeping of the ADRs.\\nWe will use a [style guide](https:\/\/www.atlassian.com\/software\/jira\/overview\/styleguide) to ensure that the ADRs are written in the same style.\\nWe will use a [code formatter](https:\/\/www.jetbrains.com\/idea\/idea\/help\/formatter.html) to ensure that the ADRs are formatted well.\\nWe will use a [code formatter](https:\/\/www.jetbrains.com\/idea\/idea\/help\/formatter.html) to ensure that the ADRs are formatted well.\\nWe will use a [code formatter](https:\/\/www.jetbrains.com\/idea\/idea\/help\/formatter.html) to ensure that the ADRs are formatted well.\\n"}
{"Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","Prediction":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\nWe will use [Model-View-Controller Architecture](http:\/\/www.thoughtworks.com\/radar\/entry\/Model_View_Controller_Architecture) to model our architecture decisions.\\nWe will use [TDD](http:\/\/en.wikipedia.org\/wiki\/Test-Driven_Development) to test the architectural decisions.\\nWe will use [Cucumber](http:\/\/cucumber.io\/), a library that makes it easier to write acceptance tests.\\nWe will use [BDD](http:\/\/www.bdd-factory.org\/) to document the architecture decisions.\\nWe will use [Swagger](http:\/\/swagger.io\/), an open source framework that makes it easier to document APIs.\\nWe will use [Swagger UI](https:\/\/swagger.io\/) to document the architecture decisions.\\nWe will use [Swagger API](https:\/\/swagger.io\/docs\/) to describe the architecture.\\nWe will use `swagger-codegen` to generate code that can be used to generate documentation from the architecture.\\nWe will use `swagger-ui` to generate an HTML output from the architecture.\\"}
{"Context":"## Context\\nFirst party action `actions\/cache` needs a input which is an explicit `key` used for restoring and saving the cache. For packages caching, the most common `key` might be the hash result of contents from all `package-lock.json` under `node_modules` folder.\\nThere are serval different ways to get the hash `key` input for `actions\/cache` action.\\n1. Customer calculate the `key` themselves from a different action, customer won't like this since it needs extra step for using cache feature\\n```yaml\\nsteps:\\n- run: |\\nhash=some_linux_hash_method(file1, file2, file3)\\necho ::set-output name=hash::$hash\\nid: createHash\\n- uses: actions\/cache@v1\\nwith:\\nkey: ${{ steps.createHash.outputs.hash }}\\n```\\n2. Make the `key` input of `actions\/cache` follow certain convention to calculate hash, this limited the `key` input to a certain format customer may not want.\\n```yaml\\nsteps:\\n- uses: actions\/cache@v1\\nwith:\\nkey: ${{ runner.os }}|${{ github.workspace }}|**\/package-lock.json\\n```\\n","Decision":"## Decision\n### Add hashFiles() function to expression engine for calculate files' hash\\n`hashFiles()` will only allow on runner side since it needs to read files on disk, using `hashFiles()` on any server side evaluated expression will cause runtime errors.\\n`hashFiles()` will only support hashing files under the `$GITHUB_WORKSPACE` since the expression evaluated on the runner, if customer use job container or container action, the runner won't have access to file system inside the container.\\n`hashFiles()` will only take 1 parameters:\\n- `hashFiles('**\/package-lock.json')`  \/\/ Search files under `$GITHUB_WORKSPACE` and calculate a hash for them\\n**Question: Do we need to support more than one match patterns?**\\nEx: `hashFiles('**\/package-lock.json', '!toolkit\/core\/package-lock.json', '!toolkit\/io\/package-lock.json')`\\nAnswer: Only support single match pattern for GA, we can always add later.\\nThis will help customer has better experience with the `actions\/cache` action's input.\\n```yaml\\nsteps:\\n- uses: actions\/cache@v1\\nwith:\\nkey: ${{hashFiles('**\/package-lock.json')}}-${{github.ref}}-${{runner.os}}\\n```\\nFor search pattern, we will use basic globbing (`*`, `?`, and `[]`) and globstar (`**`).\\nAdditional pattern details:\\n- Root relative paths with `github.workspace` (the main repo)\\n- Make `*` match files that start with `.`\\n- Case insensitive on Windows\\n- Accept `\\` or `\/` path separators on Windows\\nHashing logic:\\n1. Get all files under `$GITHUB_WORKSPACE`.\\n2. Use search pattern filter all files to get files that matches the search pattern. (search pattern only apply to file path not folder path)\\n3. Sort all matched files by full file path in alphabet order.\\n4. Use SHA256 algorithm to hash each matched file and store hash result.\\n5. Use SHA256 to hash all stored files' hash results to get the final 64 chars hash result.\\n**Question: Should we include the folder structure info into the hash?**\\nAnswer: No\\n","Prediction":"## Decision\nWe will use `steps.createHash` to create the hash `key` input for `actions\/cache` action.\\nSteps `createHash` will be used to calculate hash, it will be using the value of `steps.hash` and `steps.output.hash` from `actions\/cache` action. The `steps.hash` will be the same as `steps.output.hash` but with the `steps.output.hash` value.\\n```\\nsteps:\\n- run: |\\nhash=some_linux_hash_method(file1, file2, file3)\\necho ::set-output name=hash::$hash\\nid: createHash\\n- uses: actions\/cache@v1\\nwith:\\nkey: ${{ steps.createHash.outputs.hash }}\\n```\\n2. Make the `key` input of `actions\/cache` follow certain convention to calculate hash, this limited the `key` input to a certain format customer may not want.\\n```yaml\\nsteps:\\n- run: |\\nhash=some_linux_hash_method(file1, file2, file3)\\necho ::set-output name=hash::$hash\\nid: createHash\\n- uses: actions\/cache@v1\\nwith:\\nkey: ${{ steps.createHash"}
{"Context":"## Context\\nInteracting with git repos involves many fine-grained operations, each of which is independent from the others - there's no shared state between them, other than what lives in the repo itself on disk. So they can be handled by independent functions rather than being modelled as methods on a class.\\nWe anticipate that as the application grows, the number of these operations is likely to grow too. To maintain the open\/closed, principle, we need a way to add new operations whithout modifying too much existing code.\\nIt's also useful to be able to test each operation independently, since some of them will need quite involved test cases, and a single test of tests for one class would get pretty lengthy.\\nFinally, would be helpful to be able to compose fine-grained operations into higher-order ones, such as combining the operations of creating a branch, making a commit, and pushing to origin into a `PushCommit` operation.\\n","Decision":"## Decision\nWe've implemented a [command-bus](..\/..\/packages\/command-bus) package which provides a typescript-friendly way to build a single `dispatch` function using a that takes different types of message object and dispatches the message to the appropriate handler function.\\n","Prediction":"## Decision\nWe will use a new module to encapsulate git operations. It will be called `Operation` and will be implemented as a class.\\nEach operation will have a name, a description, and a type. A type will be a set of operations, and a set of operations can be combined by composing them with an operation of the same name, as described in the \"Git Operations\" section of this document.\\nAn operation will be a function that takes a Git repository, and returns a Git operation. A Git operation will be a function that takes a Git repository, and returns a Git operation.\\nOperations will be exported by their names, and types will be exported by their names, as well as by their types. For example, a `PushCommit` operation can be implemented as a function that takes a Git repository, and a Git operation, and returns a Git operation.\\nOperations will also be exported by their names, as well as by their types, in order to make them easy to use in tests, and to facilitate testing.\\nEach operation will have a name, a description, and a type. A type will be a set of operations, and a set of operations can be combined by composing them with an operation of the same name"}
{"Context":"## Context\\n- Using the OrcidAPI we get back complex and deeply nested JSON objects.\\n- Modeling each API endpoint by hand is time consuming.\\n","Decision":"## Decision\nWe will use models for these objects generated by the swagger-codegen code\\ngeneration tool: https:\/\/github.com\/swagger-api\/swagger-codegen.\\nWe will run the generator once, and add the resulting code to the lib directory. We don't intend to\\nmaintain any changes to the generator that were required to produce these\\nmodels.\\n","Prediction":"## Decision\n- Use the [Orcid API](https:\/\/api.orcid.net\/) to get the URI for an API endpoint.\\n- Use the [Orcid API](https:\/\/api.orcid.net\/) to get the URI for a resource.\\n- Use the [Orcid API](https:\/\/api.orcid.net\/) to get the URI for an account.\\n- Use the [Orcid API](https:\/\/api.orcid.net\/) to get the URI for a user.\\n- Use the [Orcid API](https:\/\/api.orcid.net\/) to get the URI for a resource\\n- Use the [Orcid API](https:\/\/api.orcid.net\/) to get the URI for a resource\\n- Use the [Orcid API](https:\/\/api.orcid.net\/) to get the URI for a resource\\n- Use the [Orcid API](https:\/\/api.orcid.net\/) to get the URI for a resource\\n- Use the [Orcid API](https:\/\/api.orcid.net\/) to get the URI for a resource\\n- Use the [Orcid API](https:\/\/api.orcid.net\/) to get the URI for a resource\\n- Use the [Orcid API"}
{"Context":"### Context\\nThe touch table app should support multi touch while also being robust enough to handle various UI demands. Although we initially considered a 3D model of the universe in the center of the table, that approach was dropped as it didn't help accomplish our learning goals. Given that we use a Windows device for the touch table, we must choose a technology that can run on such a machine.\\n### Decision\\nAlthough considering Unity early in the planning phases, we dropped that consideration due to the learning curve of C++. Unity has also dropped support of Javascript, a language most of our team is familiar with. The transition from Javascript to C# seemed less jarring, and we have more resources and shared knowledge in C#. Initial work on the table was also done with the help of Florian Block, who designed the [DeepTree](https:\/\/lifeonearth.seas.harvard.edu\/learning-activities\/deeptree\/), another app built in WPF.\\n### Status\\nAccepted\\n### Consequences\\nFortunately, documentation for C# is quite strong, and the language allows enough flexibility to fit the needs of the table. The .NET community is also very active, and it is easy to find answers to obstacles we would encounter during development. However, WPF seems to be losing popularity, and many of the articles concerning WPF were written a decade ago, although some Stack Overflow posts are only a year or two old.\\n_In Retrospect_: Although I'm concerned about the longevity of WPF, the subsystem fits our needs for this project. It's also helpful to have the [Panoptes .NET client](https:\/\/github.com\/zooniverse\/panoptes-net-client) available for future C# projects. However, it is discerning to see many frameworks and packages used by WPF becoming deprecated (Silverlight, Microsoft Surface SDK).\\nIn the future, I think it would be worthwhile to explore new technologies; however, WPF is a suitable choice for most of our needs.\\n","Decision":"## Decision\nAlthough considering Unity early in the planning phases, we dropped that consideration due to the learning curve of C++. Unity has also dropped support of Javascript, a language most of our team is familiar with. The transition from Javascript to C# seemed less jarring, and we have more resources and shared knowledge in C#. Initial work on the table was also done with the help of Florian Block, who designed the [DeepTree](https:\/\/lifeonearth.seas.harvard.edu\/learning-activities\/deeptree\/), another app built in WPF.\\n### Status\\nAccepted\\n### Consequences\\nFortunately, documentation for C# is quite strong, and the language allows enough flexibility to fit the needs of the table. The .NET community is also very active, and it is easy to find answers to obstacles we would encounter during development. However, WPF seems to be losing popularity, and many of the articles concerning WPF were written a decade ago, although some Stack Overflow posts are only a year or two old.\\n_In Retrospect_: Although I'm concerned about the longevity of WPF, the subsystem fits our needs for this project. It's also helpful to have the [Panoptes .NET client](https:\/\/github.com\/zooniverse\/panoptes-net-client) available for future C# projects. However, it is discerning to see many frameworks and packages used by WPF becoming deprecated (Silverlight, Microsoft Surface SDK).\\nIn the future, I think it would be worthwhile to explore new technologies; however, WPF is a suitable choice for most of our needs.\\n","Prediction":"## Decision\nAlthough considering Unity early in the planning phases, we dropped that consideration due to the learning curve of C++. Unity has also dropped support of Javascript, a language most of our team is familiar with. The transition from Javascript to C# seemed less jarring, and we have more resources and shared knowledge in C#. Initial work on the table was also done with the help of Florian Block, who designed the [DeepTree](https:\/\/lifeonearth.seas.harvard.edu\/learning-activities\/deeptree\/), another app built in WPF.\\n### Status\\nAccepted\\n### Consequences\\nFortunately, documentation for C# is quite strong, and the language allows enough flexibility to fit the needs of the table. The .NET community is also very active, and it is easy to find answers to obstacles we would encounter during development. However, WPF seems to be losing popularity, and many of the articles concerning WPF were written a decade ago, although some Stack Overflow posts are only a year or two old.\\n_In Retrospect_: Although I'm concerned about the longevity of WPF, the subsystem fits our needs for this project. It's also helpful to have the [Panoptes .NET client](https:\/\/github.com\/zoon"}
{"Context":"## Context\\nThe aim is to cover doubled or none navigation bar issues with deeplink routings.\\n","Decision":"## Decision\nShould use `BaseViewController` or `TYRootViewController` and manage it there, and override if need your preferred `NavigationBarType`\\n","Prediction":"## Decision\nThe following routes will be used:\\n* `home` - `\/index.html`\\n* `\/about` - `\/about.html`\\n* `\/contact` - `\/contact.html`\\n* `\/products` - `\/products.html`\\n* `\/products\/` - `\/products.html`\\n* `\/products\/2` - `\/products.html`\\n* `\/products\/3` - `\/products.html`\\n* `\/products\/4` - `\/products.html`\\n* `\/products\/5` - `\/products.html`\\n* `\/products\/6` - `\/products.html`\\n* `\/products\/7` - `\/products.html`\\n* `\/products\/8` - `\/products.html`\\n* `\/products\/9` - `\/products.html`\\n* `\/products\/10` - `\/products.html`\\n* `\/products\/11` - `\/products.html`\\n* `\/products\/12` - `\/products.html`\\n* `\/products\/13` - `\/products.html`\\n* `\/products\/14` - `\/products.html`\\n* `\/products\/15` - `\/products.html`\\n* `\/products\/16` - `\/products.html`\\n* `\/products\/17` - `\/products.html`\\n* `\/"}
{"Context":"## Context\\nThe original intent was to run both the base and gamma models and compare them with a chi-squared test.\\n","Decision":"## Decision\nCurrently it seems more effective to allow the user to run base and gamma models separately and compare them\\nusing their own favorite method.\\n","Prediction":"## Decision\nWe will use the `R` package `pca` to run the model on the data, and then run the `R` package `pca` to compare the results.\\nThis will ensure that the results are comparable to the `pca` package.\\nThis will also allow us to run the `pca` model on the data after the model has been trained.\\nThis will also allow us to compare the results to the `pca` package.\\nThis will also allow us to compare the results to the `pca` package.\\nThis will also allow us to compare the results to the `pca` package.\\n### 1. Run the base model and compare the results\\nWe will run the base model on the data, and then compare the results to the `pca` model.\\nThis will ensure that the results are comparable to the `pca` model.\\n### 2. Run the base model and gamma model separately\\nWe will run the base model on the data, and then run the `pca` model on the data.\\nThis will ensure that the results are comparable to the `pca` model.\\nThis will also allow us to compare the results to the `"}
{"Context":"## Context\\nAs DOS will be an important component of our collections platform,\\nthe software stack (web server, application container, programming language, database)\\nDOS uses to realize core functionality and to fulfill non-functional requirements\\nof scalability, performance, reliability, security, and backward compatibility needs to be robust. Java can help meet various implicit non-functional requirements out of the box. Similarly, the use of proven and\\nmature ecosystem libraries can help meet the functional requirements easily. The Spring\\nframework also makes it relatively easy to create RESTful web services.\\n","Decision":"## Decision\nThe core service will rely on Java and Spring framework as the basic stack for implementing core services.\\n","Prediction":"## Decision\nWe will use Spring Boot for building and running our application. Spring Boot is a\\nrelatively mature and popular open-source Java web framework.\\nSpring Boot has a strong focus on providing a full-stack framework for building\\nRESTful web services. It provides a convenient dependency injection framework, a\\nbuild system, and a configuration format that makes it easy to build a\\nmicroservices architecture.\\nSpring Boot also provides a number of useful tools such as a build system, a\\nconfiguration format, and a configuration management tool. Spring Boot also has\\na mature and well-documented ecosystem of libraries that can be used to build a\\nmicroservices architecture and to build applications that are highly available.\\nSpring Boot is also well-supported by many popular web frameworks, including\\nSpring MVC, Spring Web Flow, Spring WebFlux, Spring Data, Spring Boot Data\\nJPA, and Spring Data REST. Spring Boot is also well-supported by Spring Cloud.\\nSpring Boot is well-supported by many popular databases including\\nMySQL, PostgreSQL, Oracle, MariaDB, and SQL Server.\\nSpring Boot is also well-supported by popular application servers including\\nNginx, Apache, Jetty, and Jetty2.\\nSpring Boot is also well-supported by popular operating"}
{"Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","Prediction":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\nWe will use the Architecture Decision Journal, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\nWe will use Architecture Decision Tools, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\nWe will use Architecture Decision Making, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\nWe will use Architecture Decision Support, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\nWe will use Architecture Decision Making for Architecture Decisions, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\nWe will use Architecture Decision Support for Architecture Decision Support, as [des"}
{"Context":"## Context\\nCurrently the voting period for all governance proposals is the same.  However, this is suboptimal as all governance proposals do not require the same time period.  For more non-contentious proposals, they can be dealt with more efficiently with a faster period, while more contentious or complex proposals may need a longer period for extended discussion\/consideration.\\n","Decision":"## Decision\nWe would like to design a mechanism for making the voting period of a governance proposal variable based on the demand of voters.  We would like it to be based on the view of the governance participants, rather than just the proposer of a governance proposal (thus, allowing the proposer to select the voting period length is not sufficient).\\nHowever, we would like to avoid the creation of an entire second voting process to determine the length of the voting period, as it just pushed the problem to determining the length of that first voting period.\\nThus, we propose the following mechanism:\\n### Params\\n* The current gov param `VotingPeriod` is to be replaced by a `MinVotingPeriod` param.  This is the default voting period that all governance proposal voting periods start with.\\n* There is a new gov param called `MaxVotingPeriodExtension`.\\n### Mechanism\\nThere is a new `Msg` type called `MsgExtendVotingPeriod`, which can be sent by any staked account during a proposal's voting period.  It allows the sender to unilaterally extend the length of the voting period by `MaxVotingPeriodExtension * sender's share of voting power`.  Every address can only call `MsgExtendVotingPeriod` once per proposal.\\nSo for example, if the `MaxVotingPeriodExtension` is set to 100 Days, then anyone with 1% of voting power can extend the voting power by 1 day.  If 33% of voting power has sent the message, the voting period will be extended by 33 days.  Thus, if absolutely everyone chooses to extend the voting period, the absolute maximum voting period will be `MinVotingPeriod + MaxVotingPeriodExtension`.\\nThis system acts as a sort of distributed coordination, where individual stakers choosing to extend or not, allows the system the gauge the conentiousness\/complexity of the proposal.  It is extremely unlikely that many stakers will choose to extend at the exact same time, it allows stakers to view how long others have already extended thus far, to decide whether or not to extend further.\\n### Dealing with Unbonding\/Redelegation\\nThere is one thing that needs to be addressed.  How to deal with redelegation\/unbonding during the voting period.  If a staker of 5% calls `MsgExtendVotingPeriod` and then unbonds, does the voting period then decrease by 5 days again?  This is not good as it can give people a false sense of how long they have to make their decision.  For this reason, we want to design it such that the voting period length can only be extended, not shortened.  To do this, the current extension amount is based on the highest percent that voted extension at any time.  This is best explained by example:\\n1. Let's say 2 stakers of voting power 4% and 3% respectively vote to extend.  The voting period will be extended by 7 days.\\n2. Now the staker of 3% decides to unbond before the end of the voting period.  The voting period extension remains 7 days.\\n3. Now, let's say another staker of 2% voting power decides to extend voting period.  There is now 6% of active voting power choosing the extend.  The voting power remains 7 days.\\n4. If a fourth staker of 10% chooses to extend now, there is a total of 16% of active voting power wishing to extend.  The voting period will be extended to 16 days.\\n### Delegators\\nJust like votes in the actual voting period, delegators automatically inherit the extension of their validators.  If their validator chooses to extend, their voting power will be used in the validator's extension.  However, the delegator is unable to override their validator and \"unextend\" as that would contradict the \"voting power length can only be ratcheted up\" principle described in the previous section.  However, a delegator may choose the extend using their personal voting power, if their validator has not done so.\\n","Prediction":"## Decision\nThe voting period for proposals will be extended to 30 days for all proposals.\\nAll proposals will be counted as 1 vote.\\nAll proposals will be counted as 1 vote.\\nAll proposals will be counted as 1 vote.\\nAll proposals will be counted as 1 vote.\\nAll proposals will be counted as 1 vote.\\nAll proposals will be counted as 1 vote.\\nAll proposals will be counted as 1 vote.\\nAll proposals will be counted as 1 vote.\\nAll proposals will be counted as 1 vote.\\nAll proposals will be counted as 1 vote.\\nAll proposals will be counted as 1 vote.\\nAll proposals will be counted as 1 vote.\\nAll proposals will be counted as 1 vote.\\nAll proposals will be counted as 1 vote.\\nAll proposals will be counted as 1 vote.\\nAll proposals will be counted as 1 vote.\\nAll proposals will be counted as 1 vote.\\nAll proposals will be counted as 1 vote.\\nAll proposals will be counted as 1 vote.\\nAll proposals will be counted as 1 vote.\\nAll proposals will be counted as 1 vote.\\nAll proposals will be counted"}
{"Context":"## Context\\nAs we embraced using Docker repositories to store our images, and keeping in mind we support multiple repositories along with versioning of images and different architectures,\\nWe needed a way to construct an accepted and constant way of naming our Docker images.\\n","Decision":"## Decision\n* An image will conform to the following format: \\<namespace>\/\\<repository>:\\<tag>\\n* A valid tag conforms to the following format: \\<version>-\\<architecture>, or just \\<version>, then arch is assumed Linux\\n","Prediction":"## Decision\nWe decided to go with the official [Docker Registry](https:\/\/registry.docker.com\/) to host our Docker images.\\nWe also decided to use the [Docker Compose](https:\/\/docs.docker.com\/compose\/)\\nto help us build Docker images.\\nWe also decided to use the [Docker Compose Registry](https:\/\/registry.docker.com\/) to host our Docker images.\\nWe also decided to use the [Docker Registry (Docker Hub)](https:\/\/hub.docker.com\/) to host our Docker images.\\nWe also decided to use the [Docker Registry (Docker Hub)](https:\/\/registry.docker.com\/) to host our Docker images.\\nWe also decided to use the [Docker Registry (Docker Hub)](https:\/\/hub.docker.com\/) to host our Docker images.\\nWe also decided to use the [Docker Registry (Docker Hub)](https:\/\/hub.docker.com\/) to host our Docker images.\\nWe also decided to use the [Docker Registry (Docker Hub)](https:\/\/hub.docker.com\/) to host our Docker images.\\nWe also decided to use the [Docker Registry (Docker Hub)](https:\/\/hub.docker.com\/) to host our Docker images"}
{"Context":"## Context and Problem Statement\\nOur functional test suite currently has a 7% success rate when run against our stage environment after a deployment and a 44% success rate when run in CI for pull requests. These low rates are more from flaky tests and a finicky testing stack than legitimate bugs in the code being tested.\\nIn stage this rate is too low to be able to confidently move to a continuous delivery pipeline. In CI it slows down development and decreases morale.\\nBecause of our low success rate for pull requests each PR needs two runs of a relatively expensive task on average. In the last 90 days we used ~1.2M CircleCI credits for PRs. Ideally we could cut that in half.\\nWe should evaluate other testing stack options to improve reliability.\\n","Decision":"## Decision\nPlaywright performs better than Intern in all goals. We should prefer it for new tests and begin migrating old tests when they need maintenance and on a case-by-case basis to improve our CI pass rate.\\nTo reduce our CircleCI bill and speed up CI runs for pull requests we should segment our functional tests into two categories: P1 and P2 (for lack of a better name). P1 tests for critical functionality run on every pull request. P2 tests should run periodically (daily) and send results to Slack. The difference between a P1 and P2 test suites is that a failed P1 means some \"happy path\" is broken, an S1 or S2 level bug, while P2 tests would represent S3 or S4 bugs.\\nP1 tests are the first priority for converting to Playwright.\\n","Prediction":"## Decision\nWe will investigate the alternatives to CircleCI including: RSpec, Capybara, and Selenium.\\nWe will evaluate each stack for ease of use, cost, and availability of solutions for CI and CD integration.\\nWe will evaluate the stack for ease of use and cost in staging and production environments.\\nWe will evaluate the stack for ease of development and testing.\\nWe will evaluate the stack for ease of integration with CI and CD.\\nWe will evaluate the stack for ease of integration with other platform features.\\nWe will evaluate the stack for ease of testing and deployment.\\nWe will evaluate the stack for ease of deployment and deployment speed.\\nWe will evaluate the stack for integration with CircleCI and CD.\\nWe will evaluate the stack for integration with other platform features.\\nWe will evaluate the stack for ease of deployment and deployment speed.\\nWe will evaluate the stack for integration with CircleCI and CD.\\nWe will evaluate the stack for ease of deployment and deployment speed.\\nWe will evaluate the stack for integration with CircleCI and CD.\\nWe will evaluate the stack for ease of deployment and deployment speed.\\nWe will evaluate the stack for integration with CircleCI and CD.\\nWe will evaluate the stack for ease of deployment and"}
{"Context":"## Context\\nWe use RDS for tenant databases. Tenants can bind more than one application to a database instance created via the services console. Database migrations were broken when a binding was removed and re-added, or when another bound application other than the one that created the database tables attempted to modify them.\\nPreviously the RDS broker PostgreSQL engine copied the MySQL engine and granted all rights on the database to the newly created user. In PostgreSQL this will give the user rights to create tables, but because it has a more finely-grained permission model than MySQL this does not give the user rights on existing tables, or tables newly created by other users.\\nOnly the owner of a table can alter\/drop it, and you cannot grant this permission to other users. Users who are the owners of tables cannot be removed until the table ownership is changed.\\nWe attempted to work around the PostgreSQL permissions system in the following ways:\\n* Using [`ALTER DEFAULT PRIVILEGES`](https:\/\/www.postgresql.org\/docs\/9.5\/static\/sql-alterdefaultprivileges.html) on every newly created user to `GRANT ALL ON PUBLIC` - this means that every user can `SELECT`, `INSERT`, and `DELETE`, but because only the table owner can `ALTER` or `DROP` this will not allow other bound users to run migrations. This is also limited to the `PUBLIC` (default) schema, so would fail to work for any applications that have custom schemas.\\n* Making the group the owner of the `PUBLIC` schema. This allowed members of the group to `DROP` tables within the schema, but still did not allow them to `ALTER` these tables.\\n* Creating a \"parent\" group that is given ownership of all tables, sequences, and functions. New users are granted rights on this group and thus can carry out all owner actions on group-owned items. A [DDL event trigger](https:\/\/www.postgresql.org\/docs\/9.5\/static\/event-trigger-definition.html) using the built-in `REASSIGN OWNED BY` command handed over ownership of all resources owned by the current user to the parent `$DBNAME_owner` group. This worked perfectly on all the PostgreSQL databases we tested with, however RDS does not allow use of event triggers so it was unworkable in production.\\n","Decision":"## Decision\nWe decided the simplest solution was to issue the same credentials to all applications bound to the database. This means that the same user will be used by each application, and they will not suffer ownership problems when issuing `DROP` or `ALTER` commands.\\nRather than deterministically deriving the credentials we decided to store them in the backing database itself meaning they can be changed at any time in case of a breach. To protect against database breaches that may leak stored credentials we also decided to encrypt the credentials using a symmetric key held by the broker.\\nWe accepted that in the future we may wish to extend this by allowing the broker to issue read-only credentials, or credentials that are suitable for `SELECT`, `INSERT`, and `DELETE` operations, but not schema modifications.\\n","Prediction":"## Decision\nWe will use a \"parent\" group that is given ownership of all tables, sequences, and functions. New users are granted rights on this group and thus can carry out all owner actions on group-owned items. A [DDL event trigger](https:\/\/www.postgresql.org\/docs\/9.5\/static\/event-trigger-definition.html) using the built-in `REASSIGN OWNED BY` command handed over ownership of all resources owned by the current user to the parent `$DBNAME_owner` group. This worked perfectly on all the PostgreSQL databases we tested with, however RDS does not allow use of event triggers so it was unworkable in production.\\nWe will add a new role to the RDS PostgreSQL engine that is given ownership of the `PUBLIC` schema, and then grant this role permissions to the `owner` of the database, hence allowing the database to be owned by another user. This role will also have the rights to create and drop tables and sequences, and to grant and revoke permissions to the database's users.\\nWe will create a new database in RDS that is owned by the new role. We will then create a new database user in PostgreSQL that is given the role and the new database is then added to the `tenant` in"}
{"Context":"## Context\\nWe have to decide on a programming language to write our API code.\\nCurrently the team members are mostly familiar with _Java_ as a programming language. Therefor we might want to stick to _Java_ because then we do not have to learn a new language and we can focus on the new tools and the problem domain that we address.\\nHowever the support for _akka_ is much better in _Scala_. And we might also be eager to learn a new language.\\n","Decision":"## Decision\nWe will use _Scala_ to program the monitoring part of our application.\\n","Prediction":"## Decision\nWe will use _Scala_ as our programming language for the API.\\nThis has the advantage that support for _akka_ is much better in _Scala_ and that we do not have to learn a new language.\\nThis has the disadvantage that we have to learn a new language and that we have to learn a new tooling.\\nHowever the support for _akka_ is much better in _Scala_. And we might also be eager to learn a new language.\\nWe will use the _akka_ library in _Scala_ to communicate with the API. We will use _Akka HTTP_ to send and receive HTTP requests to the API. We will use _Akka Streams_ to send and receive messages in the API. We will use _Akka Streams_ to send and receive messages in the API. We will use _Akka DSL_ to send and receive messages in the API. We will use _Akka DSL_ to send and receive messages in the API.\\nWe will use the _akka_ library in _Scala_ to communicate with the API. We will use _Akka HTTP_ to send and receive HTTP requests to the API. We will use _Akka Streams_ to send and receive"}
{"Context":"## Context\\nThis decision record is to support multiple state stores support in Dapr. We agreed on the decision to introduce the breaking change in API\\nto support multi state store with no backward compatibility.\\nWith this change , the state API allows the app to target a specific state store by store-name, for example:\\nv1.0\/state\/storeA\/\\nv1.0\/state\/storeB\/\\nEarlier this breaking change, the API is v1.0\/state\/`<key>`\\nWe have reviewed multi storage API design for completeness and consistency.\\n","Decision":"## Decision\n*  New state store API is v1.0\/state\/`<store-name>`\/\\n*  If user is using actors and like to persist the state then user must provide actorStateStore: true in the configuration yaml.\\nIf the attribute is not specified or multiple actor state stores are configured, Dapr runtime will log warning.\\nThe actor API to save the state will fail in both these scenarios where actorStore is not specified or multiple actor stores\\nare specified.\\n*  It is noted that after this breaking change, actor state store has to be specified unlike earlier where first state store is picked up by default.\\n* It is noted that this breaking change will also require a CLI change to generate the state store YAML for redis with actorStateStore.\\n* To provide multiple stores, user has to provide separate YAML for each store and giving unique name for the store.\\n* It is noted that the param's keyPrefix represents state key prefix, it's value included ${appid} is the microservice appid, ${name} is the CRDs component's unique name, ${none} is non key prefix and the custom key prefix\\nFor example, below are the 2 sample yaml files in which redis store is used as actor state store while mongodb store is not used as actor state store.\\n```\\napiVersion: dapr.io\/v1alpha1\\nkind: Component\\nmetadata:\\nname: myStore1  # Required. This is the unique name of the store.\\nspec:\\ntype: state.redis\\nmetadata:\\n- name: keyPrefix\\nvalue: none # Optional. default appid. such as: appid, none, name and custom key prefix\\n- name: <KEY>\\nvalue: <VALUE>\\n- name: <KEY>\\nvalue: <VALUE>\\n- name: actorStateStore  # Optional. default: false\\nvalue : true\\n```\\n```\\napiVersion: dapr.io\/v1alpha1\\nkind: Component\\nmetadata:\\nname: myStore2 # Required. This is the unique name of the store.\\nspec:\\ntype: state.mongodb\\nmetadata:\\n- name: keyPrefix\\nvalue: none # Optional. default appid. such as: appid, none, name and custom key prefix\\n- name: <KEY>\\nvalue: <VALUE>\\n- name: <KEY>\\nvalue: <VALUE>\\n```\\nSo with the above example, the state APIs will be : v1.0\/state\/myStore1\/`<key>`\\nand v1.0\/state\/myStore2\/`<key>`\\n","Prediction":"## Decision\nWe will introduce the breaking change in API to support multi storage API design for completeness and consistency.\\nWe will introduce the breaking change in API to support multi state store with no backward compatibility.\\nWe will add the documentation for the breaking change in the API.\\nWe will add the documentation for the multi state store API in the API.\\nWe will add the documentation for the breaking change in the API.\\nWe will add the documentation for the multi state store API in the API.\\nWe will add the documentation for the breaking change in the API.\\nWe will add the documentation for the multi state store API in the API.\\nWe will add the documentation for the breaking change in the API.\\nWe will add the documentation for the multi state store API in the API.\\nWe will add the documentation for the breaking change in the API.\\nWe will add the documentation for the multi state store API in the API.\\nWe will add the documentation for the breaking change in the API.\\nWe will add the documentation for the multi state store API in the API.\\nWe will add the documentation for the breaking change in the API.\\nWe will add the documentation for the multi state store API in the API.\\nWe will add the"}
{"Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"## Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n### Changes to Nygard's approach\\nWe will keep ADRs in the project repository under `docs\/adr\/NNN-explanation-of-adr.md`.\\n","Prediction":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\nWe will use the [Monarch library](https:\/\/github.com\/monarchproject\/architecture-decision-records) for this.\\nMonarch is a library for managing records of architecture decisions made on an application, and is the recommended library for this task by [The Architecture Review Board](http:\/\/www.ahr-consulting.com\/adrb\/).\\nMonarch's `Documenting Architecture Decisions` chapter is a good starting point for learning more about this topic, and the `Monarch library` README is a good place to start reading about using it.\\nMonarch is also included in the `monarch` package, which is a library for managing the architecture decisions made on a project.\\nMonarch has an `Architectural Decision Records` directory, which has a number of ready-made templates for ADRs and can be used as a starting point.\\nMonarch's ADR templates are documented in the `monarch-docs` directory, which is a good place to start reading about using them.\\nMonarch's"}
{"Context":"# Context\\nAs noted in [ADR-4]({% post_url adr\/2017-01-02-ADR_4_decide_analysis_architecture %}) CodeCharta will use a pipes and filters architecture.\\nBut it has not been defined how the user will interact with the analysis.\\n# Status\\naccepted\\n# Decision\\nCodeCharta Analysis will be a set of command-line tools that use [PicoCli](https:\/\/picocli.info\/). PicoCli is small, powerful, regularly updated and works great in combination with Kotlin.\\n# Consequences\\n-   A cli is not as intuitive as a well-structured graphical user interface would have been.\\n","Decision":"## Decision\nCodeCharta Analysis will be a set of command-line tools that use [PicoCli](https:\/\/picocli.info\/). PicoCli is small, powerful, regularly updated and works great in combination with Kotlin.\\n# Consequences\\n-   A cli is not as intuitive as a well-structured graphical user interface would have been.\\n","Prediction":"## Decision\nCodeCharta Analysis will be a set of command-line tools that use [PicoCli](https:\/\/picocli.info\/). PicoCli is small, powerful, regularly updated and works great in combination with Kotlin.\\n# Consequences\\n-   A cli is not as intuitive as a well-structured graphical user interface would have been.\\nChosen option: \"CodeCharta Analysis will be a set of command-line tools that use a command-line interface\", because it is the simplest solution.\\n### Positive Consequences\\n-   A CLI is more intuitive than a GUI for the user.\\n-   It allows the user to run the tools in a standalone way.\\n-   It has a well-documented API.\\n-   It is less likely to be modified by the user.\\n-   It is easier to maintain.\\n-   It reduces the complexity of the analysis.\\n-   It is easy to use in conjunction with other tools, for example, [Kotlin DSL](https:\/\/kotlinlang.org\/docs\/reference\/usage.html#command-line-integration).\\n### Negative Consequences\\n-   It is not as simple as a graphical user interface would have been.\\n-   The user must install a CLI"}
{"Context":"## Context\\nA key part of release pipeline is to produce a bundled wikibase docker image prepared with the extensions and components known as the \"wikibase suite\". This bundle image will consist of the build artifacts and as described in [ADR0001](0001-docker-image-repository.md) these images should be published to dockerhub.\\nIn previous wikibase docker artifacts WMDE has offered a \"base\" and a \"bundled\" version of Wikibase where the base version only contain mediawiki and Wikibase. The new pipeline should still produce and publish these artifacts.\\nAs we publish our releases we also need to make a decision if we want this bundle version to be available in a tarball for the user to install manually. In a daily today this topic was discussed and the team decided against it.\\n","Decision":"## Decision\nThe wikibase release pipeline will not produce a bundled tarball to be published.\\n","Prediction":"## Decision\nWe will not bundle the wikibase artifacts and will instead publish the base and bundled version of the wikibase artifacts in a tarball.\\nThis decision does not affect the existing pipeline as we will still produce a tarball for the user to install manually.\\nThe decision is based on the fact that we want to provide the user with a tarball that they can install on their own when they want to use the latest version of Wikibase.\\nWe will continue to monitor the usage of our wikibase artifacts and will make decisions based on the usage of the artifacts.\\nWe will continue to monitor the usage of the base version of the wikibase artifacts.\\nWe will continue to monitor the usage of the bundled version of the wikibase artifacts.\\nWe will continue to monitor the usage of the tarball for the user to install manually.\\nWe will continue to monitor the usage of the tarball for the user to install manually.\\nWe will continue to monitor the usage of the tarball for the user to install manually.\\nWe will continue to monitor the usage of the tarball for the user to install manually.\\nWe will continue to monitor the usage of the tarball for the user to install manually.\\nWe will continue to monitor the usage"}
{"Context":"## Context\\nIn [ADR0005] we are proposing a process for maintaining release notes within the Wikibase repository. The Wikibase repository and some additional WMDE owned repositories are bound to the release branches as a [compatibility policy](https:\/\/www.mediawiki.org\/wiki\/Compatibility#mediawiki_extensions).\\nCurrently only mediawiki related code (core, skins, extensions) is covered by this compatibility policy and could easily apply the same process for maintaining and publishing release notes as to that what has been proposed to be used by Wikibase ([ADR0005], [ADR0007]).\\n### WMDE owned repositories currently bound to release branches\\n- [WikibaseManifest](https:\/\/www.mediawiki.org\/wiki\/Extension:WikibaseManifest)\\n- [EntitySchema](https:\/\/www.mediawiki.org\/wiki\/Extension:EntitySchema)\\n- [WikibaseQualityConstraints](https:\/\/www.mediawiki.org\/wiki\/Extension:WikibaseQualityConstraints)\\n### WMDE owned\/maintaned repositories not bound to release branches:\\n- [Wikidata Query GUI](https:\/\/gerrit.wikimedia.org\/r\/admin\/repos\/wikidata\/query\/gui)\\nThe Wikidata Query GUI is the exception of the WMDE owned repositories that we intend to include. In order to adopt the same process as proposed to be used by Wikibase we would either need to cut our own release branches or adopt some other process.\\n### Current usage of release notes by WMF\\nA quick review showed that the following [5 most downloaded WMF extensions](https:\/\/grafana.wikimedia.org\/d\/000000161\/extension-distributor-downloads?orgId=1&from=now-5y&to=now&var-release=REL1_35&var-groupby=1d) do not maintain or publish release notes.\\n| Name                | Release notes | Historic release notes |\\n| ------------------- | ------------- | ---------------------- |\\n| VisualEditor        | no            | no                     |\\n| MobileFrontend      | no            | no                     |\\n| LDAPAuthentication2 | no            | no                     |\\n| PluggableAuth       | no            | no                     |\\n| TemplateStyles      | no            | no                     |\\nThis is of course not ideal but could be interpreted as a hint that writing and maintaining release notes for all extensions is a cumbersome task that should only be done for the repositories we deem to be our main products and bring the biggest impact.\\n","Decision":"## Decision\nFor the first release to use the our new strategy the Wikibase extension will be the only repository to include release notes. This decision will be re-evaluated after the first successful release to minimize the initial burden on the team.\\n","Prediction":"## Decision\nWe will maintain release notes for our main extensions within the Wikibase repository.\\nWe will not maintain release notes in the WikibaseManifest repository for extensions we do not maintain or publish release notes in.\\nWe will not maintain release notes in the WikibaseQualityConstraints repository for extensions we do not maintain or publish release notes in.\\nWe will maintain release notes for our core extensions in the WikibaseManifest repository.\\nWe will maintain release notes for our core extensions in the WikibaseQualityConstraints repository.\\nWe will maintain release notes for our core extensions in the WikibaseManifest repository.\\nWe will maintain release notes for our core extensions in the WikibaseManifest repository.\\nWe will maintain release notes for our core extensions in the WikibaseManifest repository.\\nWe will maintain release notes for our core extensions in the WikibaseManifest repository.\\nWe will maintain release notes for our core extensions in the WikibaseManifest repository.\\nWe will maintain release notes for our core extensions in the WikibaseManifest repository.\\nWe will maintain release notes for our core extensions in the WikibaseManifest repository.\\nWe will maintain release notes for our core extensions in the WikibaseManifest repository.\\nWe will maintain release notes for our core extensions in the WikibaseManifest repository"}
{"Context":"## Context and Problem Statement\\nWe need to decide which language to use for implementing a MICO composition service.\\nThe framework will be [Apache Kafka](https:\/\/kafka.apache.org) [See ADR0021](0021-kafka-as-messaging-middleware.md)\\n## Decision Drivers\\n* Should be known by everyone (on the developement team)\\n* Must support Apache Kafka\\n","Decision":"## Decision\n* Should be known by everyone (on the developement team)\\n* Must support Apache Kafka\\nWe want to use Java. Since existing knowledge and experience is given for everyone.\\n","Prediction":"## Decision\n* Should be known by everyone (on the developement team)\\n* Must support Apache Kafka\\nChosen option: \"Java\", because it is the most widely used language in the industry and it is in the Apache ecosystem.\\n### Positive Consequences\\n* It is widely used in the industry\\n* It is integrated in the AWS ecosystem\\n* We have already experience with it in the past\\n* It is supported by the most widely used IDEs and tooling (e.g. IntelliJ, Eclipse, NetBeans, Gradle, Maven, Ant, Maven, Gradle, etc.)\\n* It supports the most widely used containers (e.g. Docker, Kubernetes, etc.)\\n* It is the most widely used runtime (e.g. Kotlin, Groovy, etc.)\\n* It is the most widely used build tool (e.g. Gradle, Maven, etc.)\\n* It is the most widely used Kafka provider (e.g. ZooKeeper, Apache Kafka, etc.)\\n* It is the most widely used Kafka consumer (e.g. Kafka Connect, Kafka, etc.)\\n* It is the most widely used Kafka producer (e.g. Kafka Connect, Kafka, etc.)\\n*"}
{"Context":"## Context\\nMany modern systems require up-front data before a device can be discovered and added to a network. To facilitate this, a Provisioning system is to be added as part of the Discovery services, with the aim to provide the information required for devices to join the network.\\nTwo use cases are currently forseen.\\n1. Users must provide device information prior to discovery. This information is managed by the binding, however a consistent way is needed to provide this data from the user, to the binding. An example of this is systems may require a cryptographic key to be provided before a device is able to be discovered.\\n2. Centrally administered systems may want to provision information about a device from a central repository. Thus a user could be shipped the device, plug it in, and it would automatically be discovered and added with little user interaction.\\nIt is proposed to add a `ProvisioningService` and `ProvisioningRegistry` to act as an interface between the provisioning information (from the user, or other system) and the binding. The binding will be able to query the `ProvisioningRegistry` to retrieve information about any provisioned devices when discovery is enabled, and will be able to update the `ProvisioningRegistry` if a device is successfully discovered. Additionally, the `ProvisioningRegistry` can be queryable to allow the status of provisioned devices to be retrieved.\\nThe provisioning information provided through the `ProvisioningRegistry` is a set of properties that are interpretted by the binding. These are linked by a `ProvisioningUID` which is used to identify the device. This is akin to a `ThingTypeUID`, however used to identify the device before it is discovered.\\n","Decision":"## Decision\nTBD\\n","Prediction":"## Decision\nThe decision is to add a `ProvisioningService` and `ProvisioningRegistry` to the Discovery services. The `ProvisioningServices` will be responsible for creating and managing the provisioning information, and the `ProvisioningRegistry` will be responsible for retrieving and updating that information. The `ProvisioningServices` will be responsible for provisioning devices for discovery, and the `ProvisioningRegistry` will be responsible for provisioning devices for the binding. The `ProvisioningServices` will be responsible for provisioning devices for discovery, and the `ProvisioningRegistry` will be responsible for provisioning devices for the binding.\\nThe `ProvisioningRegistry` will be responsible for providing a consistent way to retrieve information about devices for the `ProvisioningService`. This will be achieved by having the `ProvisioningRegistry` return a `ProvisioningUID` that is used in the `ProvisioningService` to identify the device. The `ProvisioningService` will be responsible for retrieving the `ProvisioningUID` from the `ProvisioningRegistry` and returning it to the `ProvisioningService`.\\nThe `ProvisioningService` will be responsible for retrieving the `ProvisioningUID` from"}
{"Context":"## Context\\nDuring recent months we've been deploying new code to production on Tuesdays and\\nThursdays. This has been in order to allow BEIS to communicate changes in RODA\\nto Delivery Partners and avoid causing confusion with a service which is changing\\n\"under their feet\". This Tuesday\/Thurs release routine has been a convention\\nrather than a rule, as sometimes there have been urgent fixes which we've\\ndeployed as soon as possible, to everyone's satisfaction.\\nWe are now moving out of a \"development\" phase, where we have full-time\\ndevelopers with deep context of the system, into a \"support\/operations\" phase\\nwhere updates and fixes will be applied on an adhoc basis.\\n","Decision":"## Decision\nWe will remove the convention of deploying only on Tuesdays and Thursdays in\\nrecognition that it will now often be most efficient to deploy new code as soon\\nas it's been tested and approved for release.\\nExisting conventions around preparing a clear ChangeLog (see\\n[ADR 0007](https:\/\/github.com\/UKGovernmentBEIS\/beis-report-official-development-assistance\/blob\/main\/doc\/architecture\/decisions\/0007-use-a-changelog-for-tracking-changes-in-a-release.md#L26))\\nand communicating releases via Slack will remain unchanged.\\nDeployments will now be made regularly at the discretion of the team.\\n","Prediction":"## Decision\nWe will continue to deploy new code to production on Tuesdays and Thursdays.\\nWe will stop deploying code to production on a weekly basis, and will instead\\ntest new features on a limited number of BEIS users on a regular basis.\\nWe will not deploy code to production on a Friday.\\nWe will notify the Delivery Partners of any changes to the deployment routine.\\nWe will notify the Delivery Partners of any changes to the Support\/Operations\\nphase.\\nWe will notify the Delivery Partners of any changes to the Release Notes.\\nWe will provide a live demo of new features to the Delivery Partners and the\\nProject Team.\\nWe will provide a live demo of new features to the Delivery Partners and the\\nProject Team.\\nWe will provide a live demo of new features to the Delivery Partners and the\\nProject Team.\\nWe will provide a live demo of new features to the Delivery Partners and the\\nProject Team.\\nWe will provide a live demo of new features to the Delivery Partners and the\\nProject Team.\\nWe will provide a live demo of new features to the Delivery Partners and the\\nProject Team.\\nWe will provide a live demo of new features to the Delivery Partners and the\\nProject Team.\\nWe will"}
{"Context":"## Context\\nWe will only [serve HTTPS traffic, keeping TCP port 80 (HTTP) closed and use HSTS preload lists](..\/ADR032-ssl-only-for-applications-and-cf-endpoints).\\nTo add our domains to [HSTS preload lists](https:\/\/hstspreload.appspot.com\/), there are these requirements:\\n1. Serve a valid certificate.\\n2. Redirect from HTTP to HTTPS on the same host.\\n3. Serve all subdomains over HTTPS (actually checks for `www.domain.com`)\\n4. Serve an HSTS header on the base domain for HTTPS requests:\\nWe need an endpoint to provide these requirements.\\nOur Cloud Foundry app endpoint already [serves the\\nright HSTS Security header with HAProxy](..\/ADR008-haproxy-for-request-rewriting)\\nand could be configured to serve the additional `preload` and `includeSubDomains` flags,\\nbut we cannot use it because we keep port 80 (HTTP) closed for this endpoint.\\nWe can implement a second ELB to listening on HTTP and HTTPS and use\\nHAProxy to do the HTTP to HTTPS redirect and serve the right header.\\nBut this increases our dependency on the HAProxy service.\\nWe must serve from the root domain (or apex domain), but it is not allowed to\\nserve [CNAME records in the root\/apex domain](http:\/\/serverfault.com\/questions\/613829\/why-cant-a-cname-record-be-used-at-the-apex-aka-root-of-a-domain). We must configure A records in this domain. This can be\\nan issue when serving the service using ELB or CloudFront.\\n","Decision":"## Decision\n* We will implement a basic [AWS API Gateway](https:\/\/aws.amazon.com\/api-gateway\/)\\nwith a default [MOCK response](https:\/\/aws.amazon.com\/about-aws\/whats-new\/2015\/09\/introducing-mock-integration-generate-api-responses-from-api-gateway-directly\/)\\nthat returns the right HTTP header `Strict-Transport-Security`. The actual\\ncontent of the response is irrelevant, it can be a 302.\\nA [Custom Domain Name](http:\/\/docs.aws.amazon.com\/apigateway\/latest\/developerguide\/how-to-custom-domains.html),\\nwhich creates a [AWS Cloud Front distribution](http:\/\/docs.aws.amazon.com\/AmazonCloudFront\/latest\/DeveloperGuide\/distribution-overview.html),\\nwill provide public access to this API.\\n* We will use [AWS Route 53 `ALIAS` resource record](http:\/\/docs.aws.amazon.com\/Route53\/latest\/APIReference\/CreateAliasRRSAPI.html)\\nto [serve the IPs of the AWS Cloud Front distribution as A records](http:\/\/docs.aws.amazon.com\/Route53\/latest\/DeveloperGuide\/routing-to-cloudfront-distribution.html).\\n","Prediction":"## Decision\nWe will serve from the root domain, but we will serve [HTTPS](https:\/\/www.ssllabs.com\/ssltest\/analyze.html?d=example.com) using the HAProxy ELB.\\nWe will [redirect to HTTPS if the user requests a non-HTTPS URL](https:\/\/www.ssllabs.com\/ssltest\/analyze.html?d=example.com) using the HAProxy ELB.\\nWe will [serve the right HSTS header on the base domain](https:\/\/hstspreload.appspot.com\/).\\nWe will [serve an HSTS header on every request](https:\/\/hstspreload.appspot.com\/)\\nand [serve the right HSTS preflight request header](https:\/\/hstspreload.appspot.com\/)\\non every request, but we will only serve the [HTTP 1.1](https:\/\/en.wikipedia.org\/wiki\/HTTP\/1.1) standard\\nresponse.\\nWe will [serve the [HSTS preload list](https:\/\/hstspreload.appspot.com\/)](https:\/\/hstspreload.appspot.com\/) and [include the [HSTS preload list](https:\/\/hstspreload.appspot.com\/) and [include the [HSTS preload list](https:\/\/hstspreload.appspot.com\/)"}
{"Context":"## Context\\nThe issue motivating this decision, and any context that influences or constrains the decision.\\n","Decision":"## Decision\nThe change that we're proposing or have agreed to implement.\\n","Prediction":"## Decision\nThe change that we're proposing or have agreed to implement.\\n* We will use the [Docker Compose](https:\/\/docs.docker.com\/compose\/) stack to manage all of our services.\\n* We will use the [Docker Registry](https:\/\/docs.docker.com\/images\/) image to store our application images.\\n* We will use the [Docker Swarm](https:\/\/docs.docker.com\/swarm\/) service to manage the Docker containers.\\n* We will use the [Docker-Compose](https:\/\/docs.docker.com\/compose\/) service to start and stop the Docker containers.\\n* We will use the [Docker Networking](https:\/\/docs.docker.com\/network\/overview\/) service to connect the Docker containers to the application.\\n* We will use the [Docker-Compose](https:\/\/docs.docker.com\/compose\/) service to run the containerized application.\\n* We will use the [Docker Compose](https:\/\/docs.docker.com\/compose\/) service to deploy the application.\\n* We will use the [Docker Registry](https:\/\/docs.docker.com\/images\/) image to store the application images.\\n* We will use the [Docker-Compose](https:\/\/docs.docker.com\/compose\/) service to start and"}
{"Context":"## Context\\nWe are rewriting our client-side from scratch. That gives us the opportunity to restructure and rethink our css classes.\\nWe discussed two CSS schemas `naming classes as components` vs `naming classes according to visual properties`.\\nNaming classes as components - This will make it more clear where the css classes are used, but will duplicate what we already reflect in our markup. A variation of this would be to use the [Block-Element-Modifier (BEM)](http:\/\/getbem.com\/introduction\/) pattern.\\nNaming classes as styling - This will make the CSS classes reusable and fits well with the naming schema of our CSS framework, [Bulma](https:\/\/bulma.io\/). Examples of this schema are the [Atomic CSS](https:\/\/github.com\/nemophrost\/atomic-css) schema and [Tailwind CSS](https:\/\/tailwindcss.com\/docs\/what-is-tailwind\/). There were several small concerns with this style:\\n* The style makes our markup less performant: because each element has several classes, the resulting size will be bigger. Since the markup will be compressed anyway and the compression algorithm is efficient for reducing repeating patterns, this is not an issue.\\n* We might end up with \"lying\" class names, e.g. `.is-red { color: blue}`. This will be mitigated by the style names having a slightly higher and more purpose-oriented schema, e.g. `.primary-color`. We strive to achieve a sweet spot between the extremes of BEM and Atomic CSS.\\n* The added classes make the HTML source less readable. \"Readability\", especially when compared to BEM, is kind of subjective.\\n","Decision":"## Decision\nWe will name CSS classes according to the styling they produce instead of the component they are used in. Adding component class names to Vue components feels like a violation of the [DRY principle](https:\/\/en.wikipedia.org\/wiki\/Don%27t_repeat_yourself).\\nWe will share as much of those styling classes between components as possible, instead of each component defining their own styles.\\n","Prediction":"## Decision\nWe decided to go with the naming classes as components approach.\\nWe decided to not use the style names to reflect visual properties of the element.\\nWe decided to use the style names to reflect the structure of the classes.\\nWe decided to use the style names to reflect the structure of the classes.\\nWe decided to use the style names to reflect the structure of the classes.\\nWe decided to use the style names to reflect the structure of the classes.\\nWe decided to use the style names to reflect the structure of the classes.\\nWe decided to use the style names to reflect the structure of the classes.\\nWe decided to use the style names to reflect the structure of the classes.\\nWe decided to use the style names to reflect the structure of the classes.\\nWe decided to use the style names to reflect the structure of the classes.\\nWe decided to use the style names to reflect the structure of the classes.\\nWe decided to use the style names to reflect the structure of the classes.\\nWe decided to use the style names to reflect the structure of the classes.\\nWe decided to use the style names to reflect the structure of the classes.\\nWe decided to use the style names to reflect the structure of the classes.\\"}
{"Context":"## Context\\nFailed to install weblibs package on fresh installation\\n","Decision":"## Decision\nAdd pre-install method to packages. This method will have the code that will be executed once before installation and package can't go without and that will seperate using install with kwargs for configuring the package and pre-install to dependancies like git clone.\\n","Prediction":"## Decision\nUse `pip` to install weblibs package\\nInstall `pypa` package to manage dependencies\\nInstall `pypa` to manage dependencies\\nInstall `pypa` to manage dependencies\\nInstall `pypa` to manage dependencies\\nInstall `pypa` to manage dependencies\\nInstall `pypa` to manage dependencies\\nInstall `pypa` to manage dependencies\\nInstall `pypa` to manage dependencies\\nInstall `pypa` to manage dependencies\\nInstall `pypa` to manage dependencies\\nInstall `pypa` to manage dependencies\\nInstall `pypa` to manage dependencies\\nInstall `pypa` to manage dependencies\\nInstall `pypa` to manage dependencies\\nInstall `pypa` to manage dependencies\\nInstall `pypa` to manage dependencies\\nInstall `pypa` to manage dependencies\\nInstall `pypa` to manage dependencies\\nInstall `pypa` to manage dependencies\\nInstall `pypa` to manage dependencies\\nInstall `pypa` to manage dependencies\\nInstall `pypa` to manage dependencies\\nInstall `pypa` to manage dependencies\\nInstall `pypa` to manage dependencies\\nInstall"}
{"Context":"## Context\\nThe mechanism for directing people to the new GP Profile pages is to use an\\nexisting system where a banner is displayed on the current page. From this\\nbanner the user is able to confirm they wish to see the new page by clicking on\\na button. The button is a link, able to be configured for a single static URL.\\nThe user needs to experience a seamless transition to the new page after they\\nhave clicked on the button. In order to do this the ID of the previous page\\nis required. Given the constraints of the existing system the `referer` header\\nis the only way that can be determined.\\n","Decision":"## Decision\nWe will use the `referer` header to identify the ID of the GP profile page the\\nuser was previously viewing in order to redirect them to the new page.\\n","Prediction":"## Decision\nIn order to achieve the required behaviour the following changes are required:\\n1. The `referer` header is passed in the `x-ua-compatible` request header\\n2. The `referer` header is passed in the `x-redirect` request header\\n3. The `referer` header is passed in the `x-redirect-all` request header\\n4. The `referer` header is passed in the `x-forwarded-for` request header\\n5. The `referer` header is set to the `origin` value of the current\\nrequest\\n6. The `referer` header is set to the `origin` value of the current\\nbrowser\\n7. The `referer` header is set to the `origin` value of the current\\nbrowser's `user-agent`\\n8. The `referer` header is set to the `origin` value of the current\\nbrowser's `request-id`\\n9. The `referer` header is set to the `origin` value of the current\\nbrowser's `referrer`\\n10. The `referer` header is set to the `origin` value of the current\\nbrowser's `referer-id`\\n11. The `referer` header is set to the `origin` value"}
{"Context":"## Context\\nWe need a way to deploy our application to AWS.\\n","Decision":"## Decision\nWe will use [Terraform] to deploy our application. More specifically,\\nwe will use [Chalice] to create the build artifact for the source\\ncode (.zip) and then use [Terraform] to manage the infrastructure\\nresources.\\n\ud83d\udc49 **Note:** Ideally, we would publish the build artifacts to S3 before\\ndeployment. That way the deployment process could happen independently\\nof the build process.\\n\ud83d\udc49 **Note:** To begin with, we'll just convert the JSON data created\\nby [Chalice] to [HCL] and add any other resources needed. Going forward,\\nwe will use modules to create reusable components.\\n","Prediction":"## Decision\nWe will use CloudFormation to deploy our application.\\nCloudFormation is a service that makes it easy to create and manage infrastructure as code.\\nIt is a service from AWS that allows us to create and manage our infrastructure in a\\ncomposable way.\\nWe will use CloudFormation to deploy our application to AWS.\\nWe will use Terraform to provision our infrastructure.\\nWe will use CloudFormation templates to define our infrastructure.\\nWe will use Terraform to provision our infrastructure.\\nWe will use Terraform to provision our infrastructure.\\nWe will use Terraform to provision our infrastructure.\\nWe will use Terraform to provision our infrastructure.\\nWe will use Terraform to provision our infrastructure.\\nWe will use Terraform to provision our infrastructure.\\nWe will use Terraform to provision our infrastructure.\\nWe will use Terraform to provision our infrastructure.\\nWe will use Terraform to provision our infrastructure.\\nWe will use Terraform to provision our infrastructure.\\nWe will use Terraform to provision our infrastructure.\\nWe will use Terraform to provision our infrastructure.\\nWe will use Terraform to provision our infrastructure.\\nWe will use Terraform to provision our infrastructure.\\nWe will use Terraform to provision our infrastructure.\\n"}
{"Context":"## Context\\nDiscussion about whether to make this application a multi-tenant (single instance with many Essentials Banks) or single-tenant (each Essentials Bank gets their own instance).\\n","Decision":"## Decision\nWe've decided to go with a multi-tenancy. Rails has good support for this, with some initial configuration. This will help keep costs down and allow us to provide it as a cheap\/free service. This will require ongoing support by us, to maintain the production instance, but since this is intended to be an ongoing project that was implied anyways. This will also ensure that all Essentials Banks have access to the same version of the software, universally.\\n","Prediction":"## Decision\nSingle-tenant (each Essentials Bank gets their own instance).\\n- No scaling\\n- No scaling\\n- No scaling\\n- No scaling\\n- No scaling\\n- No scaling\\n- No scaling\\n- No scaling\\n- No scaling\\n- No scaling\\n- No scaling\\n- No scaling\\n- No scaling\\n- No scaling\\n- No scaling\\n- No scaling\\n- No scaling\\n- No scaling\\n- No scaling\\n- No scaling\\n- No scaling\\n- No scaling\\n- No scaling\\n- No scaling\\n- No scaling\\n- No scaling\\n- No scaling\\n- No scaling\\n- No scaling\\n- No scaling\\n- No scaling\\n- No scaling\\n- No scaling\\n- No scaling\\n- No scaling\\n- No scaling\\n- No scaling\\n- No scaling\\n- No scaling\\n- No scaling\\n- No scaling\\n- No scaling\\n- No scaling\\n- No scaling\\n- No scaling\\n- No scaling\\n- No scaling\\n- No scaling\\n- No scaling\\n- No scaling\\n- No scaling\\n- No scaling\\n- No scaling\\n- No scaling\\n- No scaling\\n- No scaling\\n- No scaling\\n- No scaling\\n- No scaling\\n- No scaling\\n"}
{"Context":"## Context\\nAnalyzing untrusted Clojure code means loading it which should only be done in some kind of\\nsandboxed environment. A Docker image has been created to help with this but this still\\nrequires us to run and monitor job execution. Bad actors could still trigger many builds\\nto run Bitcoin miners and other compute-stealing stuff.\\nAlternatives to running Docker ourselves are AWS Lambda (probably similar compute-stealing\\nissues) and \"hacking\" a continous integration service to do the job for us. More detail can\\nbe found in the [notes on Isolation](https:\/\/github.com\/martinklepsch\/cljdoc\/blob\/72da65055ab94942f33fb63b29b732e81b559508\/doc\/isolation.md)\\n","Decision":"## Decision\nFor a first version of cljdoc we will use CircleCI to run analysis for us. The result of\\nthis analysis will be made available as a build artifact which can then be laoded in\\na trusted environment to import data into Grimoire and build HTML (or other) documentation\\nfrontends.\\n","Prediction":"## Decision\nUse Jenkins to run all builds. Jenkins is a continuous integration server that\\ntests the code as it is being built and reports any errors. It will also be used to\\ndeploy code to AWS Lambda.\\nWe will also use a continuous integration server to test the build results. The\\n[cljdoc](https:\/\/github.com\/martinklepsch\/cljdoc) Github repository will be used as the\\ntest repository.\\nA Jenkinsfile will be used to test the code. The Jenkinsfile will be stored in\\n`~\/cljdoc\/cljdoc\/jenkinsfile`.\\nA Jenkinsfile will be created for each project to test. The `cljdoc` project will be\\nused as an example.\\nA Jenkinsfile will be used to run the tests. The `cljdoc` project will be used as an\\nexample.\\nA Jenkinsfile will be used to deploy the code to AWS Lambda. The `cljdoc` project\\nwill be used as an example.\\nA Jenkinsfile will be used to deploy the code to a Docker image. The `cljdoc`\\nproject will be used as an example.\\nA Jenkinsfile will be used to run the tests. The `"}
{"Context":"## Context\\nAs part of our deployment we have a pipeline, where changes that are made can move from a development environment through to production illustrated thusly:\\n![pipeline image](..\/images\/pipeline.jpg)\\nThere are a number of externally available endpoints that are accessed to manage and view information about the platform, as well as issue commands via the Cloud Foundry API. In addition to this, a URL also needs to be available to access Apps hosted on the platform. These need to be accessed via some sort of sensible URL.\\nThe reason for splitting system domains from app domains was to prevent applications from stealing traffic to CF components (for example, api.<domain>) or masquerading as official things of the platform (for example, signup.<domain>).\\n### Naming considerations\\nA number of aspects were considered as part of the naming process.\\n* Clear sense of purpose\\n* Clear distinction between Production and other Environments\\n* No overly technical names (for example, hosting\/paas\/scalable-elastic-public-government-hosting)\\n* Prevent possibility of domains suggesting 'live' service, for example if we allowed [app name].paas.gov.uk it could appear as thought they were live services.\\n","Decision":"## Decision\nFor _non_ production environments we will be using the following domains:\\n* [environment name].cloudpipeline.digital\\n* [app name].[environment name].cloudpipelineapps.digital\\nFor our production environment we will be using the following domains:\\n* cloud.service.gov.uk\\n* [app name].cloudapps.digital\\nIt is important to note that live services will 'Bring Your Own' domain, apps available at cloudapps.digital are not live 'production' applications.\\n","Prediction":"## Decision\nWe will use the following naming conventions:\\n* [app name].paas.gov.uk\\n* [app name].paas.local\\n* [app name].paas.cloud\\n* [app name].paas.cloud.local\\n* [app name].paas.cloud.paas\\n* [app name].paas.paas.cloud.paas.cloud\\n* [app name].paas.cloud.paas.cloud.paas.cloud.paas.cloud.paas.cloud.paas.cloud.paas.cloud.paas.cloud.paas.cloud.paas.cloud.paas.cloud.paas.cloud.paas.cloud.paas.cloud.paas.cloud.paas.cloud.paas.cloud.paas.cloud.paas.cloud.paas.cloud.paas.cloud.paas.cloud.paas.cloud.paas.cloud.paas.cloud.paas.cloud.paas.cloud.paas.cloud.paas.cloud.paas.cloud.paas.cloud.paas.cloud.paas.cloud.paas.cloud.paas.cloud.paas.cloud.paas.cloud.paas.cloud.paas.cloud.paas.cloud.paas.cloud.paas.cloud.paas.cloud.paas.cloud.paas.cloud.paas.cloud.paas.cloud.paas.cloud.paas.cloud.paas.cloud.paas.cloud.paas.cloud.paas.cloud.paas.cloud.paas.cloud.paas.cloud.paas.cloud.paas.cloud.paas"}
{"Context":"## Context\\nShop owners should be able to define custom sorting options for product listings and search result pages out of the administration.\\nIt should be possible to define a system default sorting option for product listings.\\n`Top Results` will be the default on search pages and suggest route, which sorts products by `_score`.\\nCurrently, to define a custom sorting option, you need to define it as a service and tag it as `shopware.sales_channel.product_listing.sorting`.\\nThis is somewhat tedious and makes it impossible to define individual sortings via the administration.\\n","Decision":"## Decision\nFrom now on, it is possible to define custom sortings via the administration.\\nIndividual sortings will be stored in the database in the table `product_sorting` and its translatable label in the `product_sorting_translation` table.\\nIt is possible to define a system default product listing sorting option, which is stored in `system_default`.`core.listing.defaultSorting`.\\nThis however has no influence on the default `Top Results` sorting on search pages and the suggest route result.\\nTo define custom sorting options via a plugin, you can either write a migration to store them in the database.\\nThis method is recommended, as the sortings can be managed via the administration.\\nThe `product_sorting` table looks like the following:\\n| Column          | Type           | Notes                                                 |\\n| --------------- | -------------- | ----------------------------------------------------- |\\n| `id`            | binary(16)     |                                                       |\\n| `url_key`       | varchar(255)   | Key (unique). Shown in url, when sorting is chosen    |\\n| `priority`      | int unsigned   | Higher priority means, the sorting will be sorted top |\\n| `active`        | tinyint(1) [1] | Inactive sortings will not be shown and will not sort |\\n| `locked`        | tinyint(1) [0] | Locked sortings can not be edited via the DAL         |\\n| `fields`        | json           | JSON of the fields by which to sort the listing       |\\n| `created_at`    | datetime(3)    |                                                       |\\n| `updated_at`    | datetime(3)    |                                                       |\\nThe JSON for the fields column look like this:\\n```json5\\n[\\n{\\n\"field\": \"product.name\",        \/\/ property to sort by (mandatory)\\n\"order\": \"desc\",                \/\/ \"asc\" or \"desc\" (mandatory)\\n\"priority\": 0,                  \/\/ in which order the sorting is to applied (higher priority comes first) (mandatory)\\n\"naturalSorting\": 0\\n},\\n{\\n\"field\": \"product.cheapestPrice\",\\n\"order\": \"asc\",\\n\"priority\": 100,\\n\"naturalSorting\": 0\\n},\\n\/\/ ...\\n]\\n```\\n---\\nOtherwise, you can subscribe to the `ProductListingCriteriaEvent` to add a `ProductSortingEntity` as available sorting on the fly.\\n```php\\n<?php\\nnamespace Shopware\\Core\\Content\\Product\\SalesChannel\\Sorting\\Example;\\nuse Shopware\\Core\\Content\\Product\\Events\\ProductListingCriteriaEvent;\\nuse Shopware\\Core\\Content\\Product\\SalesChannel\\Sorting\\ProductSortingCollection;\\nuse Shopware\\Core\\Content\\Product\\SalesChannel\\Sorting\\ProductSortingEntity;\\nuse Symfony\\Component\\EventDispatcher\\EventSubscriberInterface;\\nclass ExampleListingSubscriber implements EventSubscriberInterface {\\npublic static function getSubscribedEvents(): array\\n{\\nreturn [\\nProductListingCriteriaEvent::class => ['addMyCustomSortingToStorefront', 500],\\n];\\n}\\npublic function addMyCustomSortingToStorefront(ProductListingCriteriaEvent $event): void\\n{\\n\/** @var ProductSortingCollection $availableSortings *\/\\n$availableSortings = $event->getCriteria()->getExtension('sortings') ?? new ProductSortingCollection();\\n$myCustomSorting = new ProductSortingEntity();\\n$myCustomSorting->setId(Uuid::randomHex());\\n$myCustomSorting->setActive(true);\\n$myCustomSorting->setTranslated(['label' => 'My Custom Sorting']);\\n$myCustomSorting->setKey('my-custom-sort');\\n$myCustomSorting->setPriority(5);\\n$myCustomSorting->setFields([\\n[\\n'field' => 'product.name',\\n'order' => 'desc',\\n'priority' => 1,\\n'naturalSorting' => 0,\\n],\\n]);\\n$availableSortings->add($myCustomSorting);\\n$event->getCriteria()->addExtension('sortings', $availableSortings);\\n}\\n}\\n```\\n","Prediction":"## Decision\nShop owners will be able to define custom sorting options for product listings and search result pages out of the administration.\\nIt should be possible to define a system default sorting option for product listings.\\n`Top Results` will be the default on search pages and suggest route, which sorts products by `_score`.\\nCurrently, to define a custom sorting option, you need to define it as a service and tag it as `shopware.sales_channel.product_listing.sorting`.\\nThis is somewhat tedious and makes it impossible to define individual sortings via the administration.\\nShop owners will be able to define custom sorting options for product listings and search result pages out of the administration.\\nIt should be possible to define a system default sorting option for product listings.\\n`Top Results` will be the default on search pages and suggest route, which sorts products by `_score`.\\nCurrently, to define a custom sorting option, you need to define it as a service and tag it as `shopware.sales_channel.product_listing.sorting`.\\nThis is somewhat tedious and makes it impossible to define individual sortings via the administration.\\nShop owners will be able to define custom sorting options for product listings and search result pages out of the administration.\\nIt should"}
{"Context":"## Context and Problem Statement\\nAs any modern system, Volley Management faces a problem of concurrent changes to data and we need to support such scenario.\\nWe explicitly do not consider an option to go without concurrency checks - time will tell if it is a good decision).\\n## Decision Drivers <!-- optional -->\\n* Performance - decision should support high throughput scenarios\\n* Maintainability - amount of code needed to write should be minimized\\n","Decision":"## Decision\n* Performance - decision should support high throughput scenarios\\n* Maintainability - amount of code needed to write should be minimized\\nChosen option: \"Optimistic concurrency\", because it is a better from the perfromance perspective and it will support our choice to [use Azure Table Storage](0006-use-azure-table-storage-for-persistence.md).\\n### Positive Consequences <!-- optional -->\\n* Concurrency check will fail very rarely so it will have almost no overhead\\n* It is aligned with Azure Table storage Optimistic concurrency\\n### Negative Consequences <!-- optional -->\\n* Whole application will have a responsibility to manage versions properly\\n* If we change a datastore we might need to have a larger change\\n","Prediction":"## Decision\n* Performance - decision should support high throughput scenarios\\n* Maintainability - amount of code needed to write should be minimized\\nChosen option: \"Concurrency Manager\", because it is the simplest option and it's supported by Volley.\\n### Positive Consequences <!-- optional -->\\n* No need to create an additional thread for each possible concurrency scenario\\n* Code is much simpler and easier to understand\\n* No need to deal with synchronization issues\\n* No need to manage concurrent requests with multiple threads\\n* No need to deal with exception handling\\n* No need to deal with multithreading issues\\n* No need to deal with synchronization issues\\n* No need to deal with multithreading issues\\n* No need to deal with exception handling\\n* No need to deal with multithreading issues\\n* No need to deal with exception handling\\n* No need to deal with multithreading issues\\n* No need to deal with exception handling\\n* No need to deal with multithreading issues\\n* No need to deal with exception handling\\n* No need to deal with multithreading issues\\n* No need to deal with exception handling\\n* No need to deal with multithreading issues\\n* No need to deal with exception handling\\n* No need to deal"}
{"Context":"## Context\\nPython has many ways of importing:\\n```python\\n# option 1: import module\\n# option 1.a\\nimport foo.bar.bazaar as baz\\nbaz.do_the_thing()\\n# option 1.b\\nfrom foo.bar import bazaar as baz\\nbaz.do_the_thing()\\n# option 2: import value\\nfrom foo.bar.bazaar import do_the_thing\\ndo_the_thing()\\n```\\nNot to mention that each of these styles may be done with relative imports (replacing `foo.bar` with `.bar` if the `bar` package is a sibling).\\nConfusingly, Option 1.a and Option 1.b are _conceptually_ the same, but mechanically there are [subtle differences](https:\/\/stackoverflow.com\/questions\/24807434\/imports-in-init-py-and-import-as-statement\/24968941#24968941).\\n","Decision":"## Decision\nImports within `tamr_client`:\\n- Must import statements for modules, classes, and exceptions\\n- Must `from foo import bar` instead of `import foo.bar as bar`\\n- Must not import functions directly. Instead import the containing module and use `module.function(...)`\\n- Must not use relative imports. Use absolute imports instead.\\n","Prediction":"## Decision\nOption 1.a and Option 1.b are the same, and will be treated as such in the current code.\\nOption 1.b will be deprecated in the future, and will be removed in the future.\\nOption 1.a will be removed in the future, and will be removed in the future.\\nOption 1.b will be removed in the future, and will be removed in the future.\\nOption 1.a will be removed in the future, and will be removed in the future.\\nOption 1.b will be removed in the future, and will be removed in the future.\\nOption 1.a will be removed in the future, and will be removed in the future.\\nOption 1.b will be removed in the future, and will be removed in the future.\\nOption 1.a will be removed in the future, and will be removed in the future.\\nOption 1.b will be removed in the future, and will be removed in the future.\\nOption 1.a will be removed in the future, and will be removed in the future.\\nOption 1.b will be removed in the future, and will be removed in the future.\\nOption 1.a will be removed in"}
{"Context":"## Context\\n`adr-viewer` presents all records with the same `lightgreen` header, even though records may be in different states.\\n","Decision":"## Decision\nRecords marked as 'Superseded' will distinguish themselves from 'Accepted'\\n","Prediction":"## Decision\n* We will change the presentation to display the light green state first, followed by the dark green state\\n* This will be done by changing the `lightgreen` state to `lightgreen: 1` and the `darkgreen` state to `darkgreen: 2`\\n* A new field `lightgreen` will be added to the `to` and `from` objects, and `lightgreen` will be the default value for the `state` field\\n* A new field `darkgreen` will be added to the `to` and `from` objects, and `darkgreen` will be the default value for the `state` field\\n* A new field `state` will be the default value for the `to` and `from` objects\\n* A new field `to` and `from` objects will be automatically populated with the new fields\\n* A new field `state` will be the default value for the `to` and `from` objects\\n* A new field `state` will be the default value for the `to` and `from` objects\\n* A new field `state` will be the default value for the `to` and `from` objects\\n* A new field"}
{"Context":"## Context\\nPrimer React components was originally released without TypeScript type definitions, making it difficult for engineers to consume the library in TypeScript applications. In [July 2019](https:\/\/github.com\/primer\/react\/commit\/2983c935ea9ad600c04078adb25e40c3624c11fa#diff-7aa4473ede4abd9ec099e87fec67fd57afafaf39e05d493ab4533acc38547eb8), we created an [ambient declaration](https:\/\/www.geeksforgeeks.org\/typescript-ambients-declaration\/) file (`index.d.ts`) file to provide type definitions for TypeScript applications without having to rewrite Primer React components in TypeScript.\\n`index.d.ts` has been an effective stopgap, enabling teams to build complex applications with Primer React components and TypeScript. However, because `index.d.ts` is disconnected from the implementation code, we've struggled to keep the type definitions up-to-date and accurate, as evidenced by [many](https:\/\/github.com\/primer\/react\/issues\/906) [TypeScript](https:\/\/github.com\/primer\/react\/issues\/540) [bug](https:\/\/github.com\/primer\/react\/issues\/520) [reports](https:\/\/github.com\/primer\/react\/issues\/534). As the library continues to grow in size and complexity, manually maintaining type definitions will become unsustainable.\\n","Decision":"## Decision\nWe will rewrite Primer React components in TypeScript.\\n","Prediction":"## Decision\nWe will switch to using the [ambient declaration](https:\/\/www.geeksforgeeks.org\/typescript-ambients-declaration\/) file to provide type definitions for TypeScript applications without having to rewrite Primer React components in TypeScript. This will enable us to keep the TypeScript type definitions up-to-date and accurate, while also allowing us to refactor and improve the library without breaking existing applications. We will continue to use the [index.d.ts](https:\/\/github.com\/primer\/react\/blob\/3c6e9a6e5c7b3de6e7e6b3d4c5d7f3f7b4e5b5d3e\/README.md#index-d.ts) file to provide type definitions for TypeScript applications.\\n### Why use the ambient declaration file?\\nAmbient declarations are a more effective solution than manually maintaining type definitions to keep the TypeScript type definitions up-to-date and accurate. This is because: (1) the TypeScript type definitions are maintained in the same file as the TypeScript code, rather than being located in separate files as in the past; (2) the TypeScript type definitions are maintained in a way that's easier to understand and maintain than manually maintaining type definitions; (3) the TypeScript type definitions"}
{"Context":"## Context\\nArchitectural Decisions are not made in isolation and need to be discussed and agreed amongst the Service Team.\\n","Decision":"## Decision\nUtilise [GitHub Workflow](https:\/\/guides.github.com\/introduction\/flow\/) to manage the acceptance, or not, of ADRs by the Service Team. The Pull Request review process, prior to merging into `main`, provides suitable mechanisms for tracking comments, amendments, approvals and so on.\\n","Prediction":"## Decision\n**Service Design Decisions** are made in isolation and need to be discussed and agreed amongst the Service Team.\\n**Service Architecture Decisions** are made in isolation and need to be discussed and agreed amongst the Service Team.\\n**Service Design Decisions** are made in isolation and need to be discussed and agreed amongst the Service Team.\\n**Service Architecture Decisions** are made in isolation and need to be discussed and agreed amongst the Service Team.\\n**Service Design Decisions** are made in isolation and need to be discussed and agreed amongst the Service Team.\\n**Service Architecture Decisions** are made in isolation and need to be discussed and agreed amongst the Service Team.\\n**Service Design Decisions** are made in isolation and need to be discussed and agreed amongst the Service Team.\\n**Service Architecture Decisions** are made in isolation and need to be discussed and agreed amongst the Service Team.\\n**Service Design Decisions** are made in isolation and need to be discussed and agreed amongst the Service Team.\\n**Service Architecture Decisions** are made in isolation and need to be discussed and agreed amongst the Service Team.\\n**Service Design Decisions** are made in isolation and need to be discussed and agreed amongst the Service Team.\\n"}
{"Context":"## Context and Problem Statement\\nTo compose service with the mico application we need a messaging middleware that is highly scalable and supports the [Enterprise Integration Patterns](https:\/\/www.enterpriseintegrationpatterns.com).\\n## Decision Drivers\\n* Should be a well known and proven solution\\n* Support a pipes and filters style architecture\\n","Decision":"## Decision\n* Should be a well known and proven solution\\n* Support a pipes and filters style architecture\\nWe want to use Kafka as our messaging middleware.\\n### Important things to know about Kafka\\nKafka topics have a different semantic than traditional message queues. Kafka also does NOT log wether a message was consumed. It only stores the last comitted offset for a consumer. Because of that and the different topic semantic some patterns need to be adapted to work with Kafka.\\nFor more detailed information about the inner workings of Kafka consult the following links:\\n*  [Client subscriptions](https:\/\/kafka.apache.org\/intro#intro_consumers)\\n*  [Client positions](https:\/\/kafka.apache.org\/documentation\/#design_consumerposition)\\n*  [Guarantees for message delivery](https:\/\/kafka.apache.org\/intro#intro_guarantees)\\n*  [Message delivery modes (how to achive exactly once)](https:\/\/kafka.apache.org\/documentation\/#semantics)\\n*  [Use topics storage for key based data](https:\/\/kafka.apache.org\/documentation\/#compaction)\\n","Prediction":"## Decision\n* Should be a well known and proven solution\\n* Support a pipes and filters style architecture\\nChosen option: \"Mulesoft\", because it has a good reputation and it is widely used.\\n### Positive Consequences\\n* The solution is already known and it is widely used.\\n* The solution is already in the mico application and it is already used.\\n### Negative Consequences\\n* The solution is not open source and it is not well known.\\n* It is not in the mico application and it is not well known.\\n### Other Considerations\\n* The solution is not in the mico application and it is not well known.\\n* It is not in the mico application and it is not well known.\\n* It is not in the mico application and it is not well known.\\n* It is not in the mico application and it is not well known.\\n### Alternative 1\\n* The solution is not in the mico application and it is not well known.\\n* The solution is not in the mico application and it is not well known.\\n* The solution is not in the mico application and it is not well known.\\n### Alternative 2\\n* The"}
{"Context":"# Context\\nWe run a Cloud Foundry platform in London (AWS eu-west-2 region) to allow GOV.UK PaaS tenants to host their applications and services in the UK. This is driven by the hosting needs of some of the existing GOV.UK PaaS tenants.\\nIn addition we need to manage the use of Amazon Web Service resources to reduce our running costs. Having staging environments in both Ireland and the UK increases the total infrastructural costs, which is not justified by the benefit of the additional tested cases.\\nThe London region is newer than the Ireland and currently offers a subset of services compared to the Ireland region. Hence, having the staging environment in London should allow us to capture the cases that may cause failure due to the unavailability of services.\\nTherefore, the risks of running a single staging environment are offset by the cost savings and simplification of deployment pipelines.\\n# Decision\\nWe will remove the Ireland (eu-west-1) staging environment resulting in a single staging environment in AWS London (eu-west-2).\\n# Status\\nAccepted\\n# Consequences\\nWe will have one staging environment for testing both London and Ireland production environment. This ADR does not affect the setup of the dev or production environment.\\n","Decision":"## Decision\nWe will remove the Ireland (eu-west-1) staging environment resulting in a single staging environment in AWS London (eu-west-2).\\n# Status\\nAccepted\\n# Consequences\\nWe will have one staging environment for testing both London and Ireland production environment. This ADR does not affect the setup of the dev or production environment.\\n","Prediction":"## Decision\nWe will remove the Ireland (eu-west-1) staging environment resulting in a single staging environment in AWS London (eu-west-2).\\n# Status\\nAccepted\\n# Consequences\\nWe will have one staging environment for testing both London and Ireland production environment. This ADR does not affect the setup of the dev or production environment.\\n[![Status of the ADR](..\/assets\/adr-status-2018-03.png)](..\/assets\/adr-status-2018-03.png) [Image by](https:\/\/github.com\/mojojo\/adr\/blob\/master\/assets\/adr-status-2018-03.png) <div class=\"col-md-4 col-md-offset-3\">**Status**<\/div>\\n**Accepted**\\n**2018-03-26** \\nIt has been decided that we will remove the Ireland (eu-west-1) staging environment resulting in a single staging environment in AWS London (eu-west-2).\\n# Status\\nAccepted\\n# Consequences\\nWe will have one staging environment for testing both London and Ireland production environment. this ADR does not affect the setup of the dev or production environment.\\n[![Status of the ADR](..\/assets\/adr-status-"}
{"Context":"## Context\\nGoal: The platform must be scalable according to the load\\nWe need to autoscale nodes in case we need to cope with a high load.\\nI have researching, and I have found that we have to scale the nodes\\nas well as the pods.\\nFor some loads, scaling pods can be enough, but for other workloads it\\ncan be better to scale the cluster nodes.\\nIssues then:\\n1. Autoscaling pods\\n2. Autoscaling nodes\\n","Decision":"## Decision\nFor the first issue, I'm going to use:\\n* [Horizontal Pod Autoscaler](https:\/\/kubernetes.io\/docs\/tasks\/run-application\/horizontal-pod-autoscale\/)\\nThis object is a controller and an API resource that is included in kubectl.  It\\nallows to set a period of time to check the load of a pod (cpu or custom metrics)\\nand increase the number of pods running.\\nFor the second issue, I'm going to use:\\n* [Cluster Autoscaler Addon](https:\/\/github.com\/kubernetes\/kops\/tree\/master\/addons\/cluster-autoscaler)\\nWith this addon, you can set a policy in AWS IAM and attach it to the previously\\ndefined autoscaling group for nodes.\\nI haven't found another way to set the AutoScale of the nodes instanceGroup\\n","Prediction":"## Decision\nWe will use Autoscaling and Kubernetes Operators to scale the cluster\\nnodes and pods.\\nWe will use the [Kubernetes Operator](https:\/\/kubernetes.io\/docs\/concepts\/workloads\/autoscaling\/) to scale the nodes.\\nWe will use the [Kubernetes Operator](https:\/\/kubernetes.io\/docs\/concepts\/workloads\/autoscaling\/) to scale the pods.\\nWe will use the [Kubernetes Operator](https:\/\/kubernetes.io\/docs\/concepts\/workloads\/autoscaling\/) to scale the pods.\\nWe will use the [Kubernetes Operator](https:\/\/kubernetes.io\/docs\/concepts\/workloads\/autoscaling\/) to scale the pods.\\nWe will use the [Kubernetes Operator](https:\/\/kubernetes.io\/docs\/concepts\/workloads\/autoscaling\/) to scale the pods.\\nWe will use the [Kubernetes Operator](https:\/\/kubernetes.io\/docs\/concepts\/workloads\/autoscaling\/) to scale the pods.\\nWe will use the [Kubernetes Operator](https:\/\/kubernetes.io\/docs\/concepts\/workloads\/autoscaling\/) to scale the pods.\\nWe will use the [Kubernetes Operator](https:\/\/kubernetes.io\/docs\/concepts\/workloads\/autoscaling\/) to scale the pods.\\nWe will use"}
{"Context":"## Context\\nCurrently all runtimes can only be governed by a single entity -- the runtime\\nowner. In this regard governance means being able to update certain fields in\\nthe runtime descriptor stored by the consensus layer registry service. On one\\nhand the runtime descriptor contains security-critical parameters and on the\\nother there needs to be a mechanism through which the runtimes can be upgraded\\n(especially so for TEE-based runtimes where a specific runtime binary is\\nenforced via remote attestation mechanisms).\\nThis proposal extends runtime governance options and enables a path towards\\nruntimes that can define their own governance mechanisms. This proposal assumes\\nthat [ADR 0003] has been adopted and runtimes can have their own accounts in the\\nstaking module.\\n","Decision":"## Decision\nThis proposal takes a simplistic but powerful approach which allows each runtime\\nto choose its governance model upon its first registration. It does so through\\na newly introduced field in the runtime descriptor which indicates how the\\nruntime descriptor can be updated in the future.\\n### Runtime Descriptor\\nThe runtime descriptor version is bumped to `2`. Version `1` descriptors are\\naccepted at genesis and are converted to the new format by assuming the entity\\ngovernance model as that is the only option in v1. All new runtime registrations\\nmust use the v2 descriptor.\\n#### Governance Model\\nThis proposal updates the runtime descriptor by adding fields as follows:\\n```golang\\ntype Runtime struct {\\n\/\/ GovernanceModel specifies the runtime governance model.\\nGovernanceModel RuntimeGovernanceModel `json:\"governance_model\"`\\n\/\/ ... existing fields omitted ...\\n}\\n\/\/ RuntimeGovernanceModel specifies the runtime governance model.\\ntype RuntimeGovernanceModel uint8\\nconst (\\nGovernanceEntity    RuntimeGovernanceModel = 1\\nGovernanceRuntime   RuntimeGovernanceModel = 2\\nGovernanceConsensus RuntimeGovernanceModel = 3\\n)\\n\/\/ ... some text serialization methods omitted ...\\n```\\nThe `governance_model` field can specifiy one of the following governance\\nmodels:\\n- **Entity governance (`GovernanceEntity`).** This causes the runtime to behave\\nexactly as before, the runtime owner (indicated by `entity_id` in the runtime\\ndescriptor) is the only one who can update the runtime descriptor via\\n`registry.RegisterRuntime` method calls.\\nThe runtime owner is also the one that needs to provide the required stake\\nin escrow in order to avoid the runtime from being suspended. As before note\\nthat anyone can delegate the required stake to the runtime owner in order to\\nenable runtime operation (but the owner can always prevent the runtime from\\noperating by performing actions which would cause the stake claims to no\\nlonger be satisfied).\\n- **Runtime-defined governance (`GovernanceRuntime`).** In this case the runtime\\nitself is the only one who can update the runtime descriptor by emitting a\\nruntime message. The runtime owner (indicated by `entity_id`) is not able to\\nperform any updates after the initial registration and such attempts must\\nreturn `ErrForbidden`.\\nThe runtime itself is the one that needs to provide the required stake in\\nescrow in order to avoid the runtime from being suspended. This assumes that\\nruntimes can have accounts in the staking module as specified by [ADR 0003].\\nNote that anyone can delegate the required stake to a runtime in order to\\nenable its operation.\\n- **Consensus layer governance (`GovernanceConsensus`).** In this case only the\\nconsensus layer itself can update the runtime descriptor either through a\\nnetwork upgrade or via a consensus layer governance mechanism not specified by\\nthis proposal.\\nRuntimes using this governance model are never suspended and do not need to\\nprovide stake in escrow.\\nRuntimes using this governance model cannot be registered\/updated via regular\\nregistry method calls or runtime messages (doing so must return\\n`ErrForbidden`). Instead such a runtime can only be registered at genesis,\\nthrough a network upgrade or via a consensus layer governance mechanism not\\nspecified by this proposal.\\n#### Entity Whitelist Admission Policy\\nThe entity whitelist admission policy configuration structure is changed to\\nallow specifying the maximum number of nodes that each entity can register under\\nthe given runtime for each role.\\n```golang\\ntype EntityWhitelistConfig struct {\\n\/\/ MaxNodes is the maximum number of nodes that an entity can register under\\n\/\/ the given runtime for a specific role. If the map is empty or absent, the\\n\/\/ number of nodes is unlimited. If the map is present and non-empty, the\\n\/\/ the number of nodes is restricted to the specified maximum (where zero\\n\/\/ means no nodes allowed), any missing roles imply zero nodes.\\nMaxNodes map[node.RolesMask]uint16 `json:\"max_nodes,omitempty\"`\\n}\\ntype EntityWhitelistRuntimeAdmissionPolicy struct {\\nEntities map[signature.PublicKey]EntityWhitelistConfig `json:\"entities\"`\\n}\\n```\\nThe new `max_nodes` field specifies the maximum number of nodes an entity can\\nregister for the given runtime for each role. If the map is empty or absent, the\\nnumber of nodes is unlimited. If the map is present and non-empty, the number of\\nnodes is restricted to the specified number (where zero means no nodes are\\nallowed). Any missing roles imply zero nodes.\\nEach key (roles mask) in the `max_nodes` map must specify a single role,\\notherwise the runtime descriptor is rejected with `ErrInvalidArgument`.\\nWhen transforming runtime descriptors from version 1, an entry in the `entities`\\nfield maps to an `EntityWhitelistConfig` structure with `max_nodes` absent,\\ndenoting that an unlimited number of nodes is allowed (as before).\\n#### Minimum Required Committee Election Pool Size\\nThe executor and storage runtime parameters are updated to add a new field\\ndefining the minimum required committee election pool size. The committee\\nscheduler is updated to refuse election for a given runtime committee in case\\nthe number of candidate nodes is less than the configured minimum pool size.\\n```golang\\ntype ExecutorParameters struct {\\n\/\/ MinPoolSize is the minimum required candidate compute node pool size.\\nMinPoolSize uint64 `json:\"min_pool_size\"`\\n\/\/ ... existing fields omitted ...\\n}\\ntype StorageParameters struct {\\n\/\/ MinPoolSize is the minimum required candidate storage node pool size.\\nMinPoolSize uint64 `json:\"min_pool_size\"`\\n\/\/ ... existing fields omitted ...\\n}\\n```\\nThe value of `min_pool_size` must be non-zero and must be equal to or greater\\nthan the corresponding sum of `group_size` and `group_backup_size`. Otherwise\\nthe runtime descriptor is rejected with `ErrInvalidArgument`.\\nWhen transforming runtime descriptors from version 1, `min_pool_size` for the\\nexecutor committee is computed as `group_size + group_backup_size` while the\\n`min_pool_size` for the storage committee is equal to `group_size`.\\n### State\\nThis proposal introduces\/updates the following consensus state in the registry\\nmodule:\\n#### Stored Runtime Descriptors\\nSince the runtime descriptors can now be updated by actors other than the\\ninitial registering entity, it does not make sense to store signed runtime\\ndescriptors. The value of storage key prefixed with `0x13` which previously\\ncontained signed runtime descriptors is modified to store plain runtime\\ndescriptors.\\n### Genesis Document\\nThis proposal updates the registry part of the genesis document as follows:\\n- The type of the `runtimes` field is changed to a list of runtime descriptors\\n(was a list of _signed_ runtime descriptors before).\\n- The type of the `suspended_runtimes` field is changed to a list of runtime\\ndescriptors (was a list of _signed_ runtime descriptors before).\\nRuntime descriptors must be transformed to support the new fields.\\n### Transaction Methods\\nThis proposal updates the following transaction methods in the registry module:\\n#### Register Runtime\\nRuntime registration enables a new runtime to be created or an existing runtime\\nto be updated (in case the governance model allows it).\\n**Method name:**\\n```\\nregistry.RegisterRuntime\\n```\\nThe body of a register runtime transaction must be a `Runtime` descriptor.\\nThe signer of the transaction must be the owning entity key.\\nRegistering a runtime may require sufficient stake in either the owning entity's\\n(when entity governance is used) or the runtime's (when runtime governance is\\nused) escrow account.\\nChanging the governance model from `GovernanceEntity` to `GovernanceRuntime` is\\nallowed. Any other governance model changes are not allowed and must fail with\\n`ErrForbidden`. Support for other changes is deferred to a consensus layer\\ngovernance mechanism not specified by this proposal.\\nUsing the `GovernanceRuntime` governance model for a runtime of any kind other\\nthan `KindCompute` must return `ErrInvalidArgument`.\\n### Messages\\nThis proposal introduces the following runtime messages:\\n#### Update Runtime Descriptor\\nThe update runtime descriptor message enables a runtime to update its own\\ndescriptor when the current governance model allows it.\\n**Field name:**\\n```\\nupdate_runtime\\n```\\n**Body:**\\n```golang\\ntype UpdateRuntimeMessage struct {\\nregistry.Runtime\\n}\\n```\\nThe body of the update runtime descriptor message is a new runtime descriptor\\nthat must be for the runtime emitting this message. Otherwise the message is\\nconsidered malformed.\\nThe actions performed when processing the message are the same as those\\nperformed when processing the `registry.RegisterRuntime` method call, just made\\non the runtime's (instead of an entity's) behalf.\\n### Consensus Parameters\\n#### Registry\\nThis proposal introduces the following new consensus parameters in the registry\\nmodule:\\n- `enable_runtime_governance_models` (set of `RuntimeGovernanceModel`) specifies\\nthe set of runtime governance models that are allowed to be used when\\ncreating\/updating registrations (either via method calls or via runtime\\nmessages). In case a runtime is using a governance model not specified in this\\nlist, an update to such a runtime must fail with `ErrForbidden`.\\n### Rust Runtime Support Library\\nThe Rust runtime support library (`oasis-core-runtime`) must be updated to\\nsupport the updated and newly needed message structures (the runtime descriptor\\nand the update runtime message).\\n","Prediction":"## Decision\nThe proposal assumes that the runtime is already using a runtime account\\n(``runtimeAccount`) and that it has been created in the registry service\\n(``runtimeAccount.create`) and that it has been registered in the runtime\\nmanager (``runtimeManager`).\\nWe propose that runtimes that want to define their own governance mechanisms can do so\\nby creating a new runtime account (``runtimeAccount`) and registering it in the\\nruntime manager (``runtimeManager`). The runtime manager will then register the\\nnew runtime account in the runtime service registry service (``runtimeService\\`).\\n### Runtime Account Creation\\nWhen a runtime account is created it will be associated with a runtime\\nmanager. This runtime manager will then register the runtime account in the\\nruntime service registry service (``runtimeService`).\\n### Runtime Account Registration\\nThe runtime service registry service will be responsible for registering the\\nnew runtime account in the runtime service registry service (``runtimeService\\).\\n### Runtime Service Registry Service\\nThe runtime service registry service will be responsible for registering the\\nnew runtime account in the runtime service registry service (``runtimeService\\).\\n### Runtime Service Registry Service\\nThe runtime service registry service will be responsible for registering the\\nnew runtime account in"}
{"Context":"## Context\\nQuery and QueryOutput example for simple query:\\n```\\nq={\"name\":\"BRCA2\"}\\nfields=[\"id\",\"name\",\"genome\"]\\n```\\nQuery and QueryOutput example for transcript join:\\n```\\nq={\"name\":\"BRCA2\"}\\nfields=[\"id\",\"name\",\"genome\",{\"transcripts\":[\"id\",\"seq_region_start\"]}]\\n```\\nQuery and QueryOutput example for sequence join:\\n```\\nq={\"name\":\"BRCA2\",\"sequences\":{\"type\":\"protein\"}}\\nfields=[{\"sequences\":[\"id\",\"desc\",\"seq\"]}]\\n```\\nOne consideration is whether searches should need to have the default search type specified in the queries and fields e.g.\\n```\\nq={\"genes\":{{\"name\":\"BRCA2\"}}\\nfields={\"genes\":[\"id\",\"name\",\"genome\"]}\\n```\\nThe best idea here is to use different endpoints for different searches e.g. `\/api\/genes` to get genes, `\/api\/variations` to get variations etc.\\n","Decision":"## Decision\nWe have decided to remove target and targetQueries from the interface as they will rapidly become unsustainable as the range of joined queries increases. At present, the \"default\" search will not require a separate section.\\n","Prediction":"## Decision\nUse different endpoints for different searches e.g. `\/api\/genes` to get genes, `\/api\/variations` to get variations etc.\\nIn the future we will probably want to use more generic endpoints e.g. `\/api\/sequences` to get sequences.\\nThis will make it easier to handle different types of searches e.g. if we want to add support for finding all mutations in a family.\\nIf possible we will also want to make it possible to filter results e.g. by gene type.\\nFor example, we may want to do:\\n```html\\nvar query = {\"genes\":{{\"name\":\"BRCA2\"}}}\\nfields=[\"id\",\"name\",\"genome\"]\\n```\\nvar results = query.get(\"sequences\", fields=[\"seq_region_start\", \"seq_region_end\"], query.get(\"sequences\", fields=[\"seq_region_start\", \"seq_region_end\"]))\\n```\\nFor this to work we will need to add a way to specify filters e.g. by gene type or mutation type.\\nThis is a good opportunity to add a way to specify filters e.g. by gene type or mutation type.\\nFor example, we may want to do:\\n```html\\nvar query = {\"genes"}
{"Context":"## Context\\nThe API for publishing release notes is not advanced and is just a giant blob of JSON containing every release in the database. A GitLab Job runs on a schedule and reads this blob, splits it into a file per release, and commits those changes to a GitHub repo. This job is slow and is something else to maintain and monitor separate from Nucleus. So the decision was between improving the API to only send the releases that had changed since the last sync, or to push changes to GitHub as soon as the're made. The latter has the advantages of happening very quickly after the change is saved, and having the context of the Nucleus user who made the change which can also be recorded in the Git commit.\\n","Decision":"## Decision\nWe've decided to go with pushing changes directly to GitHub via the GitHub API and using an async worker system to do it. The async system chosen was [Spinach][].\\n","Prediction":"## Decision\nPushing to GitHub instead of sending the release notes to Nucleus.\\nThis will allow us to do the following:\\n* be able to push changes to GitHub as soon as they're made\\n* be able to get a list of all the releases that have been made since the last sync\\n* have the context of the user who made the change recorded in the release notes\\n* have the context of the version of Nucleus used in the release notes\\n* have the version of the Nucleus API in the release notes\\n* be able to have a history of the versions of Nucleus that have been used\\n* be able to have a history of the versions of the Nucleus API that have been used\\n* be able to have a history of the version of the Nucleus API that has been used\\n* be able to have a history of the version of the Nucleus API that has not been used\\n* be able to have a history of the version of the Nucleus API that hasn't been used\\n* be able to have a history of the versions of the Nucleus API that hasn't been used\\n* be able to have a history of the versions of the Nucleus API"}
{"Context":"## :dart: Context\\nAzure offers a number of ways to host your application code. The following are the considerations for choosing a compute option for the ingestion sub-system:\\n* Should support scheduling\\n* Should support running in background\\n* Should be able to connect to backend Cosmos DB\\n* Should support .net core framework\\n* Service will only run once a day\\n* Would prefer a managed service\\n* Does not have portability requirements\\nThe following options for compute are considered for this service:\\n* App Services\\n* Functions\\n* Container Instances\\n* Service Fabric\\n* AKS\\nChoosing the right compute model will help optimize the development experience and operations\\n","Decision":"## Decision\nThe recommended approach is to use Azure Functions considering the following points:\\n* Supports consumption plan (Pay-per-use) which is ideal for sparse usage\\n* Supports .net core framework and is well integrated with Visual Studio development experience\\n* Can leverage cosmos SDK (nuget) to connect back to the data store\\n* Has built-in scheduling capabilities\\n* Is a fully managed PaaS service\\n* No overheads related to cluster management or infrastructure provisioning.\\nThe decision is based on the guidance provided by Microsoft here: https:\/\/docs.microsoft.com\/en-us\/azure\/architecture\/guide\/technology-choices\/compute-decision-tree\\n","Prediction":"## Decision\nWe will use App Services for the ingestion platform. This will help us reduce cost and complexity while providing a more stable platform for the ingestion platform.\\nWe will use Functions for the ingestion platform. This will help us reduce cost and complexity while providing a more stable platform for the ingestion platform.\\nWe will use AKS for the ingestion platform. This will help us reduce cost and complexity while providing a more stable platform for the ingestion platform.\\nWe will use Functions for the ingestion platform. This will help us reduce cost and complexity while providing a more stable platform for the ingestion platform.\\nWe will use AKS for the ingestion platform. This will help us reduce cost and complexity while providing a more stable platform for the ingestion platform.\\nWe will use Functions for the ingestion platform. This will help us reduce cost and complexity while providing a more stable platform for the ingestion platform.\\nWe will use AKS for the ingestion platform. This will help us reduce cost and complexity while providing a more stable platform for the ingestion platform.\\nWe will use Functions for the ingestion platform. This will help us reduce cost and complexity while providing a more stable platform for the ingestion platform.\\nWe will use Functions for the ingestion platform. This will help us reduce cost and complexity"}
{"Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","Prediction":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\nWe will use [DORA](https:\/\/www.ibm.com\/developerworks\/library\/r-rmd-dora\/).\\nWe will use [CockroachDB](https:\/\/www.cockroachlabs.com\/).\\nWe will use [Postgres](https:\/\/www.postgresql.org\/).\\nWe will use the [PostgreSQL Schema Explorer](https:\/\/www.postgresql.org\/docs\/12\/sqlref\/schema-explorer.html).\\nWe will use the [PostgreSQL Documentation](https:\/\/www.postgresql.org\/docs\/12\/).\\nWe will use the [PostgreSQL Labs](https:\/\/www.postgresql.org\/).\\nWe will use the [PostgreSQL Testing](https:\/\/www.postgresql.org\/docs\/12\/psql-testing.html).\\nWe will use [PostgreSQL Concurrency](https:\/\/www.postgresql.org\/docs\/12\/psql-concurrency.html).\\nWe will use the [PostgreSQL Database Backup](https:\/\/www.postgresql.org\/docs\/12\/pspguide-db-backup.html).\\nWe will use the [PostgreSQL"}
{"Context":"## Context\\nWhen we began the Advance Passenger Info (API) work, we had been going to stand it up as a webservice, so that\\nit could provide values to both PHP drt and drtv2. So it was a separate project. With the decision to focus on v2, we don't need that separation\\n","Decision":"## Decision\nBring the API service codebase into the main project\\n","Prediction":"any more. We can now use the same API server for both v1 and v2.\\n## Decision\nWe will use the same API server for both v1 and v2. This means we can use the same API keys and credentials. This\\nalso means that we can provide a single login mechanism that can be used for both v1 and v2.\\nWe will also have a single API endpoint for both v1 and v2, and will use an authentication token as the API key for v1 and v2.\\nThis will also mean that we can have a single API key for both v1 and v2, and not have to worry about any\\nadditional authentication for v1 in v2.\\nWe will also have a single URL for both v1 and v2, and will use a query string parameter for v1 and v2.\\nThis will mean that the API endpoint for v1 will be the same as the API endpoint for v2. We will also have a single URL\\nfor both v1 and v2, and will use a query string parameter for v1 and v2.\\nWe will also have a single file for both v1 and v2, and will use a query string parameter for v1 and"}
{"Context":"## Context\\n1. \u6570\u636e\u76ee\u524d\u4ea4\u7531\u670d\u52a1\u540e\u53f0\u6216\u524d\u7aef\u8fdb\u884c\u654f\u611f\u4fe1\u606f\u5904\u7406\uff0c\u539f\u59cb\u6570\u636e\u5b58\u5728\u591a\u79cd\u98ce\u9669\uff08\u5916\u90e8\u8131\u88e4\uff0c\u5185\u90e8\u4eba\u5458\u975e\u5fc5\u8981\u6027\u7684\u67e5\u770b\u7b49\uff09\uff1b\\n2. \u5404\u4e2a Team \u6570\u636e\u52a0\u5bc6\u7b56\u7565\u4e0d\u4e00\u81f4(\u6709 MD5, AES etc)\uff1b\\n3. \u6570\u636e\u63a9\u7801\u65b9\u5f0f\u4e5f\u4e0d\u7edf\u4e00\u3002\\n","Decision":"## Decision\n\u4f7f\u7528\u56db\u4e2a\u7b56\u7565\u7684\u7ed3\u5408\uff1a\\n1. \u63a9\u7801 - \u4e1a\u52a1\u7cfb\u7edf\u9700\u8981\u67e5\u770b\u90e8\u5206\u5185\u5bb9\uff0c\u5df2\u6838\u5bf9\u4fe1\u606f\uff0c\u65e5\u5fd7\u4e2d\u4e3a\u4e86\u4fbf\u4e8e\u5b9a\u4f4d\uff1b\\n2. \u66ff\u6362 - \u5b57\u6bb5\u5b9e\u9645\u4e0d\u88ab\u4f7f\u7528\uff0c\u4f46\u4fdd\u7559\u5b57\u6bb5\uff0c\u5e76\u5c06\u6570\u636e\u66ff\u6362\u4e3a\u7a7a\u5185\u5bb9\uff1b\\n3. \u53ef\u9006\u52a0\u5bc6 - AES(ECB\/PKCS5Padding - 128)\\n4. \u4e0d\u53ef\u9006\u52a0\u5bc6 - SHA1\\n\u7b56\u7565\u7684\u4f7f\u7528\u9700\u8003\u8651\uff0c\u4f7f\u7528\u573a\u666f\uff08\u751f\u4ea7\u3001\u6d4b\u8bd5\u7b49\u73af\u5883\uff09\uff0c\u6210\u672c\uff08\u5bf9\u4eba\u5458\u3001\u670d\u52a1\u5668\u7684\u9700\u6c42\uff09\uff0c\u662f\u5426\u6613\u7528\uff08\u5f71\u54cd\u5f00\u53d1\u6548\u7387\uff09\uff0c\u7ef4\u5ea6\uff08\u76ee\u524d\u5c31\u5206\u4e24\u79cd\uff1a\u673a\u5bc6\u548c\u516c\u5f00\uff09\\n\u63a9\u7801\u89c4\u5219\uff1a\\n* \u59d3\u540d - `*\u658c` - \u4fdd\u7559\u6700\u540e\u4e00\u4e2a\u5b57\uff0c\u5176\u4f59\u90e8\u5206\u7edf\u4e00\u4e3a\u4e00\u4e2a\u661f\u53f7\\n* \u7535\u8bdd - `131****0039` - \u4e2d\u95f44\\~7\u4f4d\u6bcf\u4e2a\u5b57\u7b26\u66ff\u6362\u4e3a\u4e00\u4e2a\u661f\u53f7\\n* \u8eab\u4efd\u8bc1\u53f7 - `**************1234` - \u4fdd\u7559\u540e\u56db\u4f4d\uff0c\u5176\u4f59\u90e8\u5206\u6bcf\u4e2a\u5b57\u7b26\u66ff\u6362\u4e3a\u4e00\u4e2a\u661f\u53f7\\n* \u94f6\u884c\u5361\u53f7 - `3111111******1234` - \u4fdd\u7559\u524d\u516d\u3001\u540e\u56db\u4f4d\uff0c\u5176\u4f59\u90e8\u5206\u6bcf\u4e2a\u5b57\u7b26\u66ff\u6362\u4e3a\u4e00\u4e2a\u661f\u53f7\\n* \u5730\u5740 - \\`\\`\\n* \u90ae\u7bb1 - \\`\\`\\n* \u5bc6\u7801 - \\`\\`\\n* \u4ea4\u6613\u91d1\u989d - \\`\\`\\n* \u5145\u503c\u7801 - \\`\\`\\n\u4e24\u4e2a\u539f\u5219\uff1a\\n1. remain meaningful for application logic(\u5c3d\u53ef\u80fd\u7684\u4e3a\u8131\u654f\u540e\u7684\u5e94\u7528\uff0c\u4fdd\u7559\u8131\u654f\u524d\u7684\u6709\u610f\u4e49\u4fe1\u606f)\\n2. sufficiently treated to avoid reverse engineer(\u6700\u5927\u7a0b\u5ea6\u4e0a\u9632\u6b62\u9ed1\u5ba2\u8fdb\u884c\u7834\u89e3)\\n","Prediction":"## Decision\n1. \u901a\u8fc7\u6570\u636e\u5bc6\u94a5\u52a0\u5bc6\uff1b\\n2. \u901a\u8fc7 Data Masking \u6570\u636e\u63a9\u7801\uff1b\\n3. \u901a\u8fc7 Private Key Encryption \u6570\u636e\u4fdd\u62a4\\n4. \u4f7f\u7528 AES 256bit \u673a\u5236\u8fdb\u884c\u6570\u636e\u4fdd\u62a4\\n5. \u4f7f\u7528 OpenPGP \u673a\u5236\u8fdb\u884c\u6570\u636e\u63a9\u7801\\n6. \u4f7f\u7528 Git Repo \u4fdd\u62a4\\n7. \u4f7f\u7528 Docker \u5e38\u89c4\u5316\u4fdd\u62a4\\n8. \u4f7f\u7528 Docker Compose \u4fdd\u62a4\\n9. \u4f7f\u7528 Docker Swarm \u4fdd\u62a4\\n10. \u4f7f\u7528 Docker Swarm LB \u4fdd\u62a4\\n11. \u4f7f\u7528 Docker Swarm API \u4fdd\u62a4\\n12. \u4f7f\u7528 Docker Swarm Operator \u4fdd\u62a4\\n13. \u4f7f\u7528 Docker Stack \u4fdd\u62a4\\n14. \u4f7f\u7528 Docker Compose Stack \u4fdd\u62a4\\n15. \u4f7f\u7528 Docker Registry \u4fdd\u62a4\\n16. \u4f7f\u7528 Docker Registry Registry \u4fdd\u62a4\\n17. \u4f7f\u7528 Docker Compose Registry Registry \u4fdd\u62a4\\n18. \u4f7f\u7528 Docker Compose Registry Registry \u4fdd\u62a4\\n19. \u4f7f\u7528 Docker Compose Registry Registry \u4fdd\u62a4\\n20. \u4f7f\u7528 Docker Compose Registry Registry \u4fdd\u62a4\\n21. \u4f7f\u7528 Docker Compose Registry Registry \u4fdd"}
{"Context":"## Context\\nThis idea was first conceived of in order to fulfill the use case of the\\ncreation of a decentralized Computer Emergency Response Team (dCERT), whose\\nmembers would be elected by a governing community and would fulfill the role of\\ncoordinating the community under emergency situations. This thinking\\ncan be further abstracted into the concept of \"blockchain specialization\\ngroups\".\\nThe creation of these groups are the beginning of specialization capabilities\\nwithin a wider blockchain community which could be used to enable a certain\\nlevel of delegated responsibilities. Examples of specialization which could be\\nbeneficial to a blockchain community include: code auditing, emergency response,\\ncode development etc. This type of community organization paves the way for\\nindividual stakeholders to delegate votes by issue type, if in the future\\ngovernance proposals include a field for issue type.\\n","Decision":"## Decision\nA specialization group can be broadly broken down into the following functions\\n(herein containing examples):\\n* Membership Admittance\\n* Membership Acceptance\\n* Membership Revocation\\n* (probably) Without Penalty\\n* member steps down (self-Revocation)\\n* replaced by new member from governance\\n* (probably) With Penalty\\n* due to breach of soft-agreement (determined through governance)\\n* due to breach of hard-agreement (determined by code)\\n* Execution of Duties\\n* Special transactions which only execute for members of a specialization\\ngroup (for example, dCERT members voting to turn off transaction routes in\\nan emergency scenario)\\n* Compensation\\n* Group compensation (further distribution decided by the specialization group)\\n* Individual compensation for all constituents of a group from the\\ngreater community\\nMembership admission to a specialization group could take place over a wide\\nvariety of mechanisms. The most obvious example is through a general vote among\\nthe entire community, however in certain systems a community may want to allow\\nthe members already in a specialization group to internally elect new members,\\nor maybe the community may assign a permission to a particular specialization\\ngroup to appoint members to other 3rd party groups. The sky is really the limit\\nas to how membership admittance can be structured. We attempt to capture\\nsome of these possibilities in a common interface dubbed the `Electionator`. For\\nits initial implementation as a part of this ADR we recommend that the general\\nelection abstraction (`Electionator`) is provided as well as a basic\\nimplementation of that abstraction which allows for a continuous election of\\nmembers of a specialization group.\\n``` golang\\n\/\/ The Electionator abstraction covers the concept space for\\n\/\/ a wide variety of election kinds.\\ntype Electionator interface {\\n\/\/ is the election object accepting votes.\\nActive() bool\\n\/\/ functionality to execute for when a vote is cast in this election, here\\n\/\/ the vote field is anticipated to be marshalled into a vote type used\\n\/\/ by an election.\\n\/\/\\n\/\/ NOTE There are no explicit ids here. Just votes which pertain specifically\\n\/\/ to one electionator. Anyone can create and send a vote to the electionator item\\n\/\/ which will presumably attempt to marshal those bytes into a particular struct\\n\/\/ and apply the vote information in some arbitrary way. There can be multiple\\n\/\/ Electionators within the Cosmos-Hub for multiple specialization groups, votes\\n\/\/ would need to be routed to the Electionator upstream of here.\\nVote(addr sdk.AccAddress, vote []byte)\\n\/\/ here lies all functionality to authenticate and execute changes for\\n\/\/ when a member accepts being elected\\nAcceptElection(sdk.AccAddress)\\n\/\/ Register a revoker object\\nRegisterRevoker(Revoker)\\n\/\/ No more revokers may be registered after this function is called\\nSealRevokers()\\n\/\/ register hooks to call when an election actions occur\\nRegisterHooks(ElectionatorHooks)\\n\/\/ query for the current winner(s) of this election based on arbitrary\\n\/\/ election ruleset\\nQueryElected() []sdk.AccAddress\\n\/\/ query metadata for an address in the election this\\n\/\/ could include for example position that an address\\n\/\/ is being elected for within a group\\n\/\/\\n\/\/ this metadata may be directly related to\\n\/\/ voting information and\/or privileges enabled\\n\/\/ to members within a group.\\nQueryMetadata(sdk.AccAddress) []byte\\n}\\n\/\/ ElectionatorHooks, once registered with an Electionator,\\n\/\/ trigger execution of relevant interface functions when\\n\/\/ Electionator events occur.\\ntype ElectionatorHooks interface {\\nAfterVoteCast(addr sdk.AccAddress, vote []byte)\\nAfterMemberAccepted(addr sdk.AccAddress)\\nAfterMemberRevoked(addr sdk.AccAddress, cause []byte)\\n}\\n\/\/ Revoker defines the function required for a membership revocation rule-set\\n\/\/ used by a specialization group. This could be used to create self revoking,\\n\/\/ and evidence based revoking, etc. Revokers types may be created and\\n\/\/ reused for different election types.\\n\/\/\\n\/\/ When revoking the \"cause\" bytes may be arbitrarily marshalled into evidence,\\n\/\/ memos, etc.\\ntype Revoker interface {\\nRevokeName() string      \/\/ identifier for this revoker type\\nRevokeMember(addr sdk.AccAddress, cause []byte) error\\n}\\n```\\nCertain level of commonality likely exists between the existing code within\\n`x\/governance` and required functionality of elections. This common\\nfunctionality should be abstracted during implementation. Similarly for each\\nvote implementation client CLI\/REST functionality should be abstracted\\nto be reused for multiple elections.\\nThe specialization group abstraction firstly extends the `Electionator`\\nbut also further defines traits of the group.\\n``` golang\\ntype SpecializationGroup interface {\\nElectionator\\nGetName() string\\nGetDescription() string\\n\/\/ general soft contract the group is expected\\n\/\/ to fulfill with the greater community\\nGetContract() string\\n\/\/ messages which can be executed by the members of the group\\nHandler(ctx sdk.Context, msg sdk.Msg) sdk.Result\\n\/\/ logic to be executed at endblock, this may for instance\\n\/\/ include payment of a stipend to the group members\\n\/\/ for participation in the security group.\\nEndBlocker(ctx sdk.Context)\\n}\\n```\\n","Prediction":"## Decision\nTo enable a decentralized Computer Emergency Response Team (dCERT), the\\nfollowing blockchain specialization groups will be created:\\n- [dCERT](https:\/\/github.com\/ethereum\/eth-dcert) - This group will be\\ninvolved in the creation of the dCERT and dCERT application. The group\\nwill also be responsible for the governance of the dCERT application.\\n- [dCERT-Software](https:\/\/github.com\/ethereum\/eth-dcert-softwar) - This group will be\\ninvolved in the creation of the dCERT and dCERT application. The group\\nwill also be responsible for the governance of the dCERT application.\\n- [dCERT-Code](https:\/\/github.com\/ethereum\/eth-dcert-code) - This group will be\\ninvolved in the creation of the dCERT and dCERT application. The group\\nwill also be responsible for the governance of the dCERT application.\\n- [dCERT-Code-Review](https:\/\/github.com\/ethereum\/eth-dcert-code-review) - This group\\nwill be responsible for the creation of the dCERT and dCERT application. The group\\nwill also be responsible for the governance of the dCERT application.\\n- [dCERT-E"}
{"Context":"## Context\\nThe app needs a database to store user data including authentication and notes.\\n","Decision":"## Decision\nThe app will use mongoDB for its database system, as its JSON-like structure is well-suited to the storage of simple text.\\n","Prediction":"## Decision\nWe will use MongoDB.\\nIt is the most widely used NoSQL database and has a great community.\\nIt has a great performance and low cost.\\nIt is easy to use and has great documentation.\\nIt is available on all major cloud providers.\\nIt has a great support community.\\nIt supports replication and sharding.\\nIt has excellent security features and is available in the cloud.\\nIt has a mature release cycle.\\nIt is already in production at [MyHeritage](https:\/\/github.com\/myheritage\/mongo-db).\\nIt is fully managed by [MongoDB Cloud](https:\/\/www.mongodb.com\/cloud).\\nIt is fully managed by [MongoDB Managed Cloud Service](https:\/\/www.mongodb.com\/mccloud).\\nIt is available in the cloud in all regions.\\nIt is available on all major cloud providers.\\nIt is available in the cloud in all regions.\\nIt is available on all major cloud providers.\\nIt is available in the cloud in all regions.\\nIt is available in the cloud in all regions.\\nIt is available in the cloud in all regions.\\nIt is available in the cloud in all regions.\\nIt is available in the cloud in all regions.\\nIt is available"}
{"Context":"## Context\\nOur service plans have evolved incrementally over the last few years and are in\\nneed of some attention. Names are inconsistent, potentially confusing and\\nin many cases contain irrelevant redundant information that is of no practical\\nuse to the platform operators or to tenants consuming the service.\\nAdding additional versions of services has the potential to compound the\\nproblem by multiplying plans of different characteristics with different\\nversions.\\n","Decision":"## Decision\nWe have decided to use the following naming convention for naming plans going forward:\\n```\\nSIZE[-HA][-LABEL,-LABEL,...]-VERSION\\n```\\nWhere:\\n* `SIZE` is a string describing the scale one of the plan, it should be one of: `xlarge` `large` `medium` `small` `tiny`.\\n* `HA` is the string `ha` to indicate highly available if relevent.\\n* `LABEL` is a string describing some specific variant of the service if relvent.\\n* `VERSION` is the version number of the service plan.\\n### For example:\\nA large multi-az postgres plan for version 9.6 would be `large-ha-9.6`.\\nA small not multi-az, unclustered, redis 3.2 plan would be `redis-unclustered-3.2`.\\n### Example migrations of some existing plans:\\n```\\nL-HA-enc-dedicated-9.5 => large-ha-9.5\\nM-HA-enc-dedicated-9.5 => medium-ha-9.5\\ntiny-clustered => tiny-clustered-3.2\\ntiny-unclustered => tiny-unclustered-3.2\\n```\\n### Additionally:\\n* We will avoid use of the word \"free\" in names.\\n* We will avoid using redundent words (like 'dedicated') in names to reduce noise.\\n* We will avoid use of uppercase characters in names.\\n* We will avoid abbriviations where possible (\"medium\" instead of \"M\", \"large\" instead of \"L\").\\n* We will avoid offering unencrypted plans where an encrypted version is available (legacy unencrypted plans will be explicitly labelled with `-unencrypted` and hidden from the marketplace).\\n","Prediction":"## Decision\nWe will refactor the service plan system to have a single plan per service.\\nThis will allow us to standardize the names and the versions of the services.\\nThis will allow us to provide a consistent user experience across\\ndifferent tenants.\\nThis will also allow us to group services together into logical\\nsub-groups.\\nThis will also allow us to improve the scalability of the service plan system\\nby avoiding the need to create multiple instances of the same service.\\nWe will create a new service plan API to allow tenants to create new\\nservices.\\nWe will also create a new service plan UI to allow tenants to create\\nnew services and manage existing services.\\nWe will also create a new service plan versioning API to allow tenants to\\ncreate new versions of services.\\nWe will also create a new service plan versioning UI to allow tenants to\\nmanage the versioning of services.\\nWe will also create a new service plan versioning documentation to\\nallow tenants to better understand the versioning of services.\\nWe will create a new service plan API to allow tenants to create\\nservices that are not currently in use.\\nWe will create a new service plan UI to allow tenants to create\\nservices that are not currently in use.\\n"}
{"Context":"## Context\\n[#1776](https:\/\/github.com\/tendermint\/tendermint\/issues\/1776) was\\nopened in relation to implementation of a Plasma child chain using Tendermint\\nCore as consensus\/replication engine.\\nDue to the requirements of [Minimal Viable Plasma (MVP)](https:\/\/ethresear.ch\/t\/minimal-viable-plasma\/426) and [Plasma Cash](https:\/\/ethresear.ch\/t\/plasma-cash-plasma-with-much-less-per-user-data-checking\/1298), it is necessary for ABCI apps to have a mechanism to handle the following cases (more may emerge in the near future):\\n1. `deposit` transactions on the Root Chain, which must consist of a block\\nwith a single transaction, where there are no inputs and only one output\\nmade in favour of the depositor. In this case, a `block` consists of\\na transaction with the following shape:\\n```\\n[0, 0, 0, 0, #input1 - zeroed out\\n0, 0, 0, 0, #input2 - zeroed out\\n<depositor_address>, <amount>, #output1 - in favour of depositor\\n0, 0, #output2 - zeroed out\\n<fee>,\\n]\\n```\\n`exit` transactions may also be treated in a similar manner, wherein the\\ninput is the UTXO being exited on the Root Chain, and the output belongs to\\na reserved \"burn\" address, e.g., `0x0`. In such cases, it is favourable for\\nthe containing block to only hold a single transaction that may receive\\nspecial treatment.\\n2. Other \"internal\" transactions on the child chain, which may be initiated\\nunilaterally. The most basic example of is a coinbase transaction\\nimplementing validator node incentives, but may also be app-specific. In\\nthese cases, it may be favourable for such transactions to\\nbe ordered in a specific manner, e.g., coinbase transactions will always be\\nat index 0. In general, such strategies increase the determinism and\\npredictability of blockchain applications.\\nWhile it is possible to deal with the cases enumerated above using the\\nexisting ABCI, currently available result in suboptimal workarounds. Two are\\nexplained in greater detail below.\\n### Solution 1: App state-based Plasma chain\\nIn this work around, the app maintains a `PlasmaStore` with a corresponding\\n`Keeper`. The PlasmaStore is responsible for maintaing a second, separate\\nblockchain that complies with the MVP specification, including `deposit`\\nblocks and other \"internal\" transactions. These \"virtual\" blocks are then broadcasted\\nto the Root Chain.\\nThis naive approach is, however, fundamentally flawed, as it by definition\\ndiverges from the canonical chain maintained by Tendermint. This is further\\nexacerbated if the business logic for generating such transactions is\\npotentially non-deterministic, as this should not even be done in\\n`Begin\/EndBlock`, which may, as a result, break consensus guarantees.\\nAdditinoally, this has serious implications for \"watchers\" - independent third parties,\\nor even an auxilliary blockchain, responsible for ensuring that blocks recorded\\non the Root Chain are consistent with the Plasma chain's. Since, in this case,\\nthe Plasma chain is inconsistent with the canonical one maintained by Tendermint\\nCore, it seems that there exists no compact means of verifying the legitimacy of\\nthe Plasma chain without replaying every state transition from genesis (!).\\n### Solution 2: Broadcast to Tendermint Core from ABCI app\\nThis approach is inspired by `tendermint`, in which Ethereum transactions are\\nrelayed to Tendermint Core. It requires the app to maintain a client connection\\nto the consensus engine.\\nWhenever an \"internal\" transaction needs to be created, the proposer of the\\ncurrent block broadcasts the transaction or transactions to Tendermint as\\nneeded in order to ensure that the Tendermint chain and Plasma chain are\\ncompletely consistent.\\nThis allows \"internal\" transactions to pass through the full consensus\\nprocess, and can be validated in methods like `CheckTx`, i.e., signed by the\\nproposer, is the semantically correct, etc. Note that this involves informing\\nthe ABCI app of the block proposer, which was temporarily hacked in as a means\\nof conducting this experiment, although this should not be necessary when the\\ncurrent proposer is passed to `BeginBlock`.\\nIt is much easier to relay these transactions directly to the Root\\nChain smart contract and\/or maintain a \"compressed\" auxiliary chain comprised\\nof Plasma-friendly blocks that 100% reflect the canonical (Tendermint)\\nblockchain. Unfortunately, this approach not idiomatic (i.e., utilises the\\nTendermint consensus engine in unintended ways). Additionally, it does not\\nallow the application developer to:\\n- Control the _ordering_ of transactions in the proposed block (e.g., index 0,\\nor 0 to `n` for coinbase transactions)\\n- Control the _number_ of transactions in the block (e.g., when a `deposit`\\nblock is required)\\nSince determinism is of utmost importance in blockchain engineering, this approach,\\nwhile more viable, should also not be considered as fit for production.\\n","Decision":"## Decision\n### `ProposeTx`\\nIn order to address the difficulties described above, the ABCI interface must\\nexpose an additional method, tentatively named `ProposeTx`.\\nIt should have the following signature:\\n```\\nProposeTx(RequestProposeTx) ResponseProposeTx\\n```\\nWhere `RequestProposeTx` and `ResponseProposeTx` are `message`s with the\\nfollowing shapes:\\n```\\nmessage RequestProposeTx {\\nint64 next_block_height = 1; \/\/ height of the block the proposed tx would be part of\\nValidator proposer = 2; \/\/ the proposer details\\n}\\nmessage ResponseProposeTx {\\nint64 num_tx = 1; \/\/ the number of tx to include in proposed block\\nrepeated bytes txs = 2; \/\/ ordered transaction data to include in block\\nbool exclusive = 3; \/\/ whether the block should include other transactions (from `mempool`)\\n}\\n```\\n`ProposeTx` would be called by before `mempool.Reap` at this\\n[line](https:\/\/github.com\/tendermint\/tendermint\/blob\/9cd9f3338bc80a12590631632c23c8dbe3ff5c34\/consensus\/state.go#L935).\\nDepending on whether `exclusive` is `true` or `false`, the proposed\\ntransactions are then pushed on top of the transactions received from\\n`mempool.Reap`.\\n### `DeliverTx`\\nSince the list of `tx` received from `ProposeTx` are _not_ passed through `CheckTx`,\\nit is probably a good idea to provide a means of differentiatiating \"internal\" transactions\\nfrom user-generated ones, in case the app developer needs\/wants to take extra measures to\\nensure validity of the proposed transactions.\\nTherefore, the `RequestDeliverTx` message should be changed to provide an additional flag, like so:\\n```\\nmessage RequestDeliverTx {\\nbytes tx = 1;\\nbool internal = 2;\\n}\\n```\\nAlternatively, an additional method `DeliverProposeTx` may be added as an accompanient to\\n`ProposeTx`. However, it is not clear at this stage if this additional overhead is necessary\\nto preserve consensus guarantees given that a simple flag may suffice for now.\\n","Prediction":"## Decision\n### Solution 2: Broadcast to Tendermint Core from ABCI app\\nThis approach is inspired by `tendermint`, in which Ethereum transactions are\\nrelayed to Tendermint Core. It requires the app to maintain a client connection\\nto the consensus engine.\\nWhenever an \"internal\" transaction needs to be created, the proposer of the\\ncurrent block broadcasts the transaction or transactions to Tendermint as\\nneeded in order to ensure that the Tendermint chain and Plasma chain are\\ncompletely consistent.\\nThis allows \"internal\" transactions to pass through the full consensus\\nprocess, and can be validated in methods like `CheckTx`, i.e., signed by the\\nproposer, is the semantically correct, etc. Note that this involves informing\\nthe ABCI app of the block proposer, which was temporarily hacked in as a means\\nof conducting this experiment, although this should not be necessary when the\\ncurrent proposer is passed to `BeginBlock`.\\nIt is much easier to relay these transactions directly to the Root\\nChain smart contract and\/or maintain a \"compressed\" auxiliary chain comprised\\nof Plasma-friendly blocks that 100% reflect the canonical (Tendermint)\\nblockchain. Unfortunately, this approach not"}
{"Context":"## Context\\nWe need to be able to test that the production cluster(s) and any test clusters we create behave the way we expect them to.\\nSo, we need automated tests that exercise a cluster and confirm that the effect we get is the one we expected.\\n","Decision":"## Decision\nWe chose the ruby testing framework [rspec] for this.\\nAlthough there are some go-based testing frameworks for kubernetes, there are some problems with them, and some benefits to rspec:\\n* The kubernetes testing frameworks seem quite immature, with limited documentation, tooling and other resources such as examples\\n* There is limited go expertise in the team\\n* RSpec is a very mature framework, with a lot of tooling, documentation and support\\n* There is a lot of ruby\/rspec experience in the wider organisation\\n* Ruby is our scripting language of choice, so rspec fits with that\\n","Prediction":"## Decision\nWe will use [Rspec-rails](https:\/\/github.com\/rspec\/rspec-rails) for our tests.\\nWe will use [Capybara](https:\/\/github.com\/rspec\/capybara) to run the tests\\nWe will use [Capybara-Webdriver](https:\/\/github.com\/rspec\/capybara-webdriver) to run the tests on a browser\\nWe will use [Capybara-Webdriver-Test](https:\/\/github.com\/rspec\/capybara-webdriver-tdf) to run the tests on a browser\\nWe will use [Capybara-Webdriver-Test-DSL](https:\/\/github.com\/rspec\/capybara-webdriver-tdf\/capybara-webdriver-tdf) to provide a DSL for describing the tests\\nWe will use [Capybara-Webdriver-Test-DSL-TestSuite](https:\/\/github.com\/rspec\/capybara-webdriver-tdf\/capybara-webdriver-tdf\/capybara-webdriver-tdf\/capybara-webdriver-tdf\/capybara-webdriver-tdf) to provide a suite of tests\\nWe will use [Capybara-Webdriver-Test-DSL-TestSuite-"}
{"Context":"Context\\n-------\\n``Event-routing-backends`` transforms and emits edx events that may contain PII which is not meant to be shared with learning record consumers. New xAPI and Caliper transformers are expected to be added in ``Event-routing-backends`` and therefore, a mechanism needs to be put in place to reduce chances of PII leakage via these transformers.\\nDecision\\n--------\\n#. An accessor `method`_ will be developed to get values from open edx events for a specified key. Information inside the open edx event will only be accessed using this accessor method in base transformers and in event transformers.\\n#. If a key, required by the specification (xAPI or Caliper), is not found in open edx event, this method will throw an exception.\\n#. If a key, deemed optional by the specification (xAPI or Caliper), is not found in open edx event, this method will log this instance and return None.\\n#. The efficacy of fuzzy comparison between keys in transformed events and potential PII keys will be evaluated in future and incorporated if found useful.\\nBenefits\\n---------\\n#. Only the instances where this accessor is called, will need to be reviewed during code review, in order to check for potential PII leakage.\\n.. _method: https:\/\/github.com\/openedx\/event-routing-backends\/blob\/f430d4cf58bdab01e42fcc944241898606873d82\/event_routing_backends\/processors\/mixins\/base_transformer.py#L139\\n","Decision":"## Decision\n--------\\n#. An accessor `method`_ will be developed to get values from open edx events for a specified key. Information inside the open edx event will only be accessed using this accessor method in base transformers and in event transformers.\\n#. If a key, required by the specification (xAPI or Caliper), is not found in open edx event, this method will throw an exception.\\n#. If a key, deemed optional by the specification (xAPI or Caliper), is not found in open edx event, this method will log this instance and return None.\\n#. The efficacy of fuzzy comparison between keys in transformed events and potential PII keys will be evaluated in future and incorporated if found useful.\\nBenefits\\n---------\\n#. Only the instances where this accessor is called, will need to be reviewed during code review, in order to check for potential PII leakage.\\n.. _method: https:\/\/github.com\/openedx\/event-routing-backends\/blob\/f430d4cf58bdab01e42fcc944241898606873d82\/event_routing_backends\/processors\/mixins\/base_transformer.py#L139\\n","Prediction":"## Decision\n--------\\n#. An accessor `method`_ will be developed to get values from open edx events for a specified key. Information inside the open edx event will only be accessed using this accessor method in base transformers and in event transformers.\\n#. If a key, required by the specification (xAPI or Caliper), is not found in open edx event, this method will throw an exception.\\n#. If a key, deemed optional by the specification (xAPI or Caliper), will not found in open edx event, this method will log this instance and return None.\\n#. The efficacy of fuzzy comparison between keys in transformed events and potential PII keys will be evaluated in future and incorporated if found useful.\\nBenefits\\n---------\\n#. Only the instances where this accessor is called, will need to be reviewed during code review, in order to check for potential PII leakage.\\n.. _method: https:\/\/github.com\/openedx\/event-routing-backends\/blob\/f430d4cf58bdab01e42fcc944241898606873d82\/event_routing_backends\/processors\/mixins\/base_transformer.py#L139\\n## Decision\n--------\\n#. An accessor `method`_ will be developed to get values from open edx events"}
{"Context":"## Context\\nTo uniquely identify an entity in Ensembl (e.g. gene, transcript, region\/slice\/location) we need three pieces of information:\\n-   The **dataset** to which the object belongs, e.g. homo_sapiens_GCA_000001405_14 [^1]\\n-   The **object type**, e.g. gene, transcript, region\\n-   The **object ID**, e.g. ENSG00000139618, AT3G52430, 13:32315474-32400266 [^2]\\nEnsembl entities (called objects) are used extensively in the web client and passing around three separate pieces of information for each one can be cumbersome, error prone, and adds additional work when serialisation is required (e.g. for use in URLs or hash keys). Therefore creating a single string which can be decomposed to the 3 data tuple is useful.\\n[^1]: For our purposes dataset can be considered synonymous to `genome_id`. Also note issues in the current `genome_id` structure\\n[^2]: Region strings are not IDs (they do not point to an entity) but we can consider them as such for convenience\\n","Decision":"## Decision\nTo allow more convenient manipulation of Ensembl objects we will use a string to uniquely identify each \"object\". These \u201cobject IDs\u201d will be generated and parsed by the back-end APIs, and should not be generated or parsed by the client (i.e. the client should not assume anything about the ID format). When the client needs information about an object it will request it through the relevant API calls.\\n### Object ID format\\n`<dataset>:<object type>:<object id>`\\nExamples:\\n```\\nhomo_sapiens_GCA_000001405_14:gene:ENSG00000139618\\narabidopsis_thaliana_GCA_000001735_1:gene:AT3G52430\\narabidopsis_thaliana_GCA_000001735_1:transcript:AT3G52430\\nhomo_sapiens_GCA_000001405_14:location:13:32315474-32400266\\n```\\nSemantically the dataset identifier used (we have chosen `genome_id` here) might differ and so the consuming API should be capable of knowing how to decode said dataset identifier (e.g. it could be a track identiifer).\\n**Currently `location` is known as `region` in the 2020 codebase**\\n","Prediction":"## Decision\nWe will use the **genus** as the unique identifier for the object. We will use the **species** to uniquely identify the species of the object. We will use the **genome** to uniquely identify the genome of the object.\\n#### Design\\nWe will create the three pieces of information for the object as separate strings, which can be combined to one single string for the unique ID of the object and which can be used as a key in the object's `genome_id` field (in the `EnsemblGCA` table).\\n#### Implementation\\nWe will use the **genus** and **species** strings as the unique identifiers for the objects. We will use the **genome** string as the unique identifier for the genome.\\n#### Issues\\n#### Not unique unique identifier for the object\\n#### Not unique unique identifier for the genome\\n#### How to handle this?\\n#### Incremental update\\n#### How to handle this?\\n#### Incremental update\\n#### How to handle this?\\n#### Incremental update\\n#### How to handle this?\\n#### Incremental update\\n#### How to handle this?\\n#### Incremental update\\n#### How to handle this?\\n#### Incremental update\\n#### How to handle this?\\n####"}
{"Context":"## Context and Problem Statement\\nThere is a need to send real time SignalR-based notifications about current missile position to the map.\\n## Decision Drivers\\n* Need easy way to post missile status updates\\n* Need easy way to get those updates using SignalR on client side\\n","Decision":"## Decision\n* Need easy way to post missile status updates\\n* Need easy way to get those updates using SignalR on client side\\nDecided to simply push SignalR messages directly from MGCC module.\\n### Positive Consequences\\n* Initially I planned to post current missile status to Message Bus, then process it to CosmosDB, then have Azure Function to send SignalR messages. But that seemed too complicated. With direct SignalR messages from MGCC module the implementation will be much easier and faster.\\n### Negative Consequences\\n* n\/a\\n","Prediction":"## Decision\n* Need easy way to post missile status updates\\n* Need easy way to get those updates using SignalR on client side\\nChosen option: \"Send 3rd party server\", because it provides a better solution to our problem than the other options.\\n### Positive Consequences\\n* SignalR-based notifications can be updated in real time\\n* No need for a server-side implementation of a web server\\n### Negative Consequences\\n* Need to implement the SignalR server-side implementation on the client side\\n* Need to implement a web server on the server side\\n* Need to implement a web server on the client side\\n* Need to implement a web server on the server side\\n* Need to implement a web server on the client side\\n* Need to implement a web server on the server side\\n* Need to implement a web server on the server side\\n* Need to implement a web server on the server side\\n* Need to implement a web server on the server side\\n* Need to implement a web server on the server side\\n* Need to implement a web server on the server side\\n* Need to implement a web server on the server side\\n* Need to implement a web server on the server side\\n* Need to implement a web server"}
{"Context":"## Context\\nTo improve SDBM data interoperability, in particular for the [Mapping Manuscript Migrations][mmm] Linked Data project, the project wanted to add authority URIs to SDBM Place records.\\n[mmm]: http:\/\/mappingmanuscriptmigrations.org \"Mapping Manuscript Migrations project site\"\\n","Decision":"## Decision\nThe project chose to add name records from the [Getty Thesaurus of Geographic Names (TGN)][TGN] and [GeoNames][GEO] databases. Search functionality was enabled for GeoNames, which supports HTTPS, but not TGN which does not. TGN is used because it includes historic place and region names. Form integration is managed through an Angular controller.\\n[TGN]: http:\/\/www.getty.edu\/research\/tools\/vocabularies\/tgn\/ \"Getty Thesaurus of Geographic Names Online\"\\n[GEO]: https:\/\/www.geonames.org \"GeoNames\"\\n","Prediction":"## Decision\nFor all SDBM Place records that are part of a Linked Data Linked Data project, the project will add the following authority URIs to the `metadata` property of the Place record:\\n* `http:\/\/www.w3.org\/2005\/11\/sdbm-1.1#Place`\\n* `http:\/\/www.w3.org\/2005\/11\/sdbm-1.1#PlaceAddress`\\n* `http:\/\/www.w3.org\/2005\/11\/sdbm-1.1#PlaceLocation`\\n* `http:\/\/www.w3.org\/2005\/11\/sdbm-1.1#PlaceType`\\n* `http:\/\/www.w3.org\/2005\/11\/sdbm-1.1#PlaceTypeCode`\\n* `http:\/\/www.w3.org\/2005\/11\/sdbm-1.1#PlaceTypeCodeString`\\n* `http:\/\/www.w3.org\/2005\/11\/sdbm-1.1#PlaceTypeString`\\n* `http:\/\/www.w3.org\/2005\/11\/sdbm-1.1#PlaceLocationString`\\n* `http:\/\/www.w3.org\/200"}
{"Context":"## Context\\nIn the alpha version we had *hcd* that runs a single Holochain app\/agent as a background demon.\\nGoing forward we need a more sophisticated conductor of several Holochain apps for several reasons:\\n* UX - One end-user friendly tool with GUI to administer all installed and running Holochain apps\\n* coalescing of the network layer \/ having one network manager, to handle ports etc.\\n* Mid-term roadmap entails having a generalized Holochain UI \/ browser that would act as an app conductor as well\\nSince with ADR 8 we want to go mobile first, we need to have an easy way to run Holochain apps on mobile phones.\\nQt sports QML for rapid UI development and compiles the same code natively to Windows\/Linux\/MacOS\/Android\/iOS\/Blackberry.\\nFor system and network capabilities, C++ would be at our disposal.\\n","Decision":"## Decision\nUse Qt framework for building a cross platform Holochain app conductor as a replacement for *hcadmin* for Holochain app management & deployment, and possiblly integrating *hcd* into it if we can manage the Rust integration.\\n","Prediction":"## Decision\nWe will use Qt QML to create a Holochain UI for the Holochain console.\\nWe will use Qt QML for connecting to Holochain apps.\\nWe will use Qt QML for connecting to the network layer.\\nWe will use Qt QML for the Holochain UI itself.\\nWe will use Qt QML for the Holochain console itself.\\nWe will use Qt QML for the Holochain apps themselves.\\nWe will use Qt QML for the Holochain app's own UI.\\nWe will use Qt QML for Holochain's own QML files.\\nWe will use Qt QML for the Holochain app's own C++ code.\\nWe will use Qt QML for the Holochain app's own C++ code.\\nWe will use Qt QML for the Holochain app's own QML files.\\nWe will use Qt QML for Holochain's own QML files.\\nWe will use Qt QML for Holochain's own C++ code.\\nWe will use Qt QML for the Holochain app's own QML files.\\nWe will use Qt QML for Holochain's own C++"}
{"Context":"## Context\\nThis service will need to store sensitive diversity data for each candidate - gender, ethnicity, etc - which, although of little interest to attackers, would be potentially impactful to the candidate if someone was to gain unauthorised access to it. Therefore we need to take extra care to minimise the risk of this happening.\\n","Decision":"## Decision\nFor all of the solutions except 4 (not storing the data at all), the Apply app will still need to decrypt the data to present it in the API and UI as plain text to the Student Records Systems or the user. So however we store the data, we would have added complexity in the code and the operations management, yet still be vulnerable to all the considered risks (except risk 2 - SQL injection - we have framework-level protection against this, and our most recent pen test found no SQL-injection vulnerabilities).\\nWe have discounted solution 4 as the user should not have to deal with the complexity involved - we should be aiming to offer the best possible user experience, and this is not it.\\nTherefore we have decided to pursue option 5 - concentrating on separation of the data at the point of the access.\\n","Prediction":"## Decision\nWe will use a separate database for the diversity data, with the following features:\\n- The database will be kept in a separate AWS account\\n- The database will be encrypted with AWS Key Management Service (KMS)\\n- The database will be kept in a separate AWS region\\n- The database will be kept in a separate AWS account\\n- The database will be kept in a separate AWS region\\n- The database will be kept in a separate AWS account\\n- The database will be kept in a separate AWS account\\n- The database will be kept in a separate AWS account\\n- The database will be kept in a separate AWS account\\n- The database will be kept in a separate AWS account\\n- The database will be kept in a separate AWS account\\n- The database will be kept in a separate AWS account\\n- The database will be kept in a separate AWS account\\n- The database will be kept in a separate AWS account\\n- The database will be kept in a separate AWS account\\n- The database will be kept in a separate AWS account\\n- The database will be kept in a separate AWS account\\n- The database will be kept in a separate AWS account\\n- The database will be kept in a separate AWS account\\n- The database will"}
{"Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","Prediction":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\nWe will use [UDR](http:\/\/udr-project.org\/).\\nWe will use [Confluence](https:\/\/confluence.atlassian.com\/display\/UDEC\/UDR) to store our UDRs.\\nWe will use [GitLab](https:\/\/gitlab.com\/udr-project\/udr) to store our UDRs.\\nWe will use [Silo](https:\/\/silo-project.org\/udr\/) to store our UDRs.\\nWe will use [Pivotal Tracker](https:\/\/pivotal.io\/products\/tracker) to store our UDRs.\\nWe will use [Google Cloud Storage](https:\/\/cloud.google.com\/storage) to store our UDRs.\\nWe will use [Pivotal Tracker](https:\/\/pivotal.io\/products\/tracker) to store our UDRs.\\nWe will use [Google Cloud Storage](https:\/\/cloud.google.com\/storage) to store our UDRs.\\nWe will use [Google Cloud Storage](https:\/\/cloud.google.com\/storage) to"}
{"Context":"## Context\\nWe need to choose a programming language to implement the `menu-generation` application.\\n`menu-generation` application will be developed initially during the author's free time, thus this time is limited.\\nDevelopment may involve other developers in the future, and the chosen language should not restrict participation.\\n","Decision":"## Decision\nJava is a broadly used programming language, and the most well mastered one by the author. Thus Java will be the\\nprogramming language used to implement the `menu-generation` application.\\n","Prediction":"## Decision\nWe will develop the application using the Go language.\\nGo is an open source programming language developed by Google.\\nGo is a statically typed language, which means that you can be sure that the code you write will compile and run without errors.\\nGo is an interpreted language, which will allow us to write code that is faster and more efficient than the compiled code that is written in C or C++.\\nGo is an object-oriented language, which means that you can use it to write applications that have a large number of users and\/or a large number of data sources.\\nGo is a functional programming language, which means that you can use it to write applications that have a large number of users and\/or a large number of data sources.\\nGo is a type-safe programming language, which means that you can write code that is safe and guarantees predictable semantics.\\nGo is a statically typed language, which means that you can be sure that the code you write will compile and run without errors.\\nGo is an interpreted language, which will allow us to write code that is faster and more efficient than the compiled code that is written in C or C++.\\nGo is an object-oriented language, which means that you can use it to write applications that"}
{"Context":"## Context\\nOrigin SDK contracts are designed to be upgradable by abstracting proxy, logic and storage to separate contracts. This approach leads to maintaining 3 separate Solidity files per contract.\\n","Decision":"## Decision\nUse OpenZeppelin implementation based on generalized proxy, logic and storage to remove the need of keeping 3 separate custom implemented contracts.\\n","Prediction":"## Decision\nWe will use the `pragma solidity ^0.4.*` instead of the `pragma solidity ^0.6.*` to allow for future upgrades. We will also include a `pragma solidity 0.4.*` for backwards compatibility.\\nWe will provide a `pragma solidity 0.5.*` first, and then a `pragma solidity 0.6.*` to support the transition to the new API.\\nWe will provide a `pragma solidity 0.6.1` to support the transition to the new API.\\nWe will provide a `pragma solidity 0.6.0` to support the transition to the new ABI.\\nWe will provide a `pragma solidity 0.5.0` to support the transition to the new ABI.\\nWe will provide a `pragma solidity 0.4.0` to support the transition to the new ABI.\\nWe will provide a `pragma solidity 0.3.0` to support the transition to the new ABI.\\nWe will provide a `pragma solidity 0.2.0` to support the transition to the new ABI.\\nWe will provide a `pragma solidity 0.1.0` to support the transition to the new ABI"}
{"Context":"## Context\\nGP Data changes on a daily basis.\\nNightly ETLs are run to obtain Syndication and POMI data.\\nCurrently [profiles-db](https:\/\/github.com\/nhsuk\/profiles-db) requires manually updating from the ETL outputs, and re-deploying to refresh the data.\\nNo automated validation of data is performed during the update.\\n","Decision":"## Decision\nTo avoid a new release of [profiles-db](https:\/\/github.com\/nhsuk\/profiles-db) each time data changes, data will be inserted into a running mongodb instance on a daily schedule.\\nThe output from the nightly [gp-data-etl](https:\/\/github.com\/nhsuk\/gp-data-etl) and [pomi-data-etl](https:\/\/github.com\/nhsuk\/pomi-data-etl) will be validated and combined.\\nThe contents of the existing database will only be replaced if the new data is of a comparable size.\\n","Prediction":"## Decision\nWe will use the [profile-db](https:\/\/github.com\/nhsuk\/profiles-db) repository to store the Syndication and POMI data.\\nWe will use a [POMI-DB](https:\/\/github.com\/pomi\/pomi-db) repository to store the POMI data.\\nWe will use a [Syndication-DB](https:\/\/github.com\/nhsuk\/syndication-db) repository to store the Syndication data.\\nWe will use a [POMI-DB-Validation](https:\/\/github.com\/nhsuk\/pomi-pomi-db-Validation) repository to store the POMI validation data.\\nWe will use a [POMI-DB-Validation-Repository](https:\/\/github.com\/nhsuk\/pomi-pomi-db-Validation-Repository) repository to store the POMI validation data.\\nWe will use a [POMI-DB-Validation-Repository-File](https:\/\/github.com\/nhsuk\/pomi-pomi-db-Validation-Repository-File) to store the POMI validation data.\\nWe will use a [POMI-DB-Validation-Repository-File-Repository](https:\/\/github.com\/nhsuk\/pomi-p"}
{"Context":"## Context\\nWe need a mechanism for highlighting security vulnerabilities in our code before it reaches production environments\\n","Decision":"## Decision\nUse the [Brakeman](https:\/\/brakemanscanner.org\/) static security analysis tool to find vulnerabilities in development and test\\n","Prediction":"## Decision\nWe will use **[Vulnerability Lifecycle](https:\/\/www.tenable.com\/vulnerability-management\/security-essentials\/)](https:\/\/www.tenable.com\/vulnerability-management\/security-essentials\/)\\nto highlight security vulnerabilities in our code before it reaches production environments\\nWe will use **[Jenkins](https:\/\/jenkins.io)** to run our vulnerability scanning jobs\\nWe will use **[Jenkins CI](https:\/\/jenkins-ci.org)** to automatically run our vulnerability scanning jobs\\nWe will use **[Pipeline](https:\/\/help.github.com\/en\/project-administration\/pipelines)''s build step to automatically run the vulnerability scanning jobs\\nWe will use **[Pipeline](https:\/\/help.github.com\/en\/project-administration\/pipelines)''s code review step to automatically run the vulnerability scanning jobs\\nWe will use **[Github Actions](https:\/\/help.github.com\/en\/project-administration\/github-actions)''s continuous integration step to automatically run the vulnerability scanning jobs\\nWe will use **[GitHub Actions](https:\/\/help.github.com\/en\/project-administration\/github-actions)''s build and deploy step to automatically run the vulnerability scanning jobs\\nWe will use **[GitHub Actions](https:\/\/help.github.com\/en\/project-administration\/github-actions)''"}
{"Context":"## Context\\nWe want to enforce consistency in our code, and catch as many errors\\nautomatically as we are able to. Linting the code is good practice to achieve\\nthese aims. [ESLint](https:\/\/eslint.org\/) is the standard linter for modern\\nJavaScript, and has good support for TypeScript and React though plugins.\\n","Decision":"## Decision\nWe will check code style using ESLint.\\nWe will let Prettier have precedence when ESLint and Prettier conflict in their\\nstyles.\\nWe will use the recommended configuration for plugins where possible.\\nWe will run ESLint as part of the test suite.\\n","Prediction":"## Decision\nWe will use Linter for code analysis for the main codebase.\\nWe will use [ESLint](https:\/\/eslint.org\/) for linting configuration.\\nWe will use [Prettier](https:\/\/prettier.io\/), a code formatter, for formatting configuration.\\nWe will use [Travis CI](https:\/\/travis-ci.org\/) for continuous integration.\\nWe will use [Rollup](https:\/\/rollupjs.org\/) as a bundler for production builds.\\nWe will use [Webpack](https:\/\/webpack.js.org\/) as a bundler for production builds.\\nWe will use `babel-plugin-transform-runtime` for transpilation of ES6 modules.\\nWe will use `babel-plugin-transform-runtime` and `babel-loader` for transpilation of\\nES6 modules with transpilation enabled.\\nWe will use `babel-loader` and `babel-preset-env` for transpilation of ES6 modules with\\ntranspilation enabled.\\nWe will use `babel-preset-react` for transpilation of React applications.\\nWe will use `babel-preset-react` and `babel-preset-env` for transpilation of React applications.\\nWe will use `webpack-dev-server` to"}
{"Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","Prediction":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\nWe will use [erlang-records](https:\/\/github.com\/erlang\/erlang-records) as a tool to generate ADRs.\\nWe will use [Elixir-records](https:\/\/github.com\/elixir-lang\/elixir-records) as a tool to generate ADRs.\\nWe will use [Erlang-records](https:\/\/github.com\/elixir-lang\/elixir-records) as a tool to generate ADRs.\\nWe will use [Erlang-records](https:\/\/github.com\/elixir-lang\/elixir-records) as a tool to generate ADRs.\\nWe will use [Erlang-records](https:\/\/github.com\/elixir-lang\/elixir-records) as a tool to generate ADRs.\\nWe will use [Erlang-records](https:\/\/github.com\/elixir-lang\/elixir-records) as a tool to generate ADRs.\\nWe will use [Erlang-records](https:\/\/github"}
{"Context":"## Context\\nThe uplift front-end work should be based on the [U-M Library Design System](https:\/\/design-system.lib.umich.edu\/):\\n* The Digital Collections Platform long-term goal is that \"DLXS collections\" should blend with the Library (unless contrained by stakeholder requirements, e.g. the Bentley Library).\\n* Building templates against the Design System today should mean more of these templates can be re-purposed for however the DCP evolves.\\n","Decision":"## Decision\nUse the Design System components and CSS to build out the uplift front-end.\\nSet a milestone to evaluate which (if any) uplift patterns can be extracted up to the Design System.\\n","Prediction":"## Decision\nWe will use the [U-M Library Design System](https:\/\/design-system.lib.umich.edu\/) templates to build the front-end of the uplift work.\\nThis decision was made to help the Library make the best decisions for the digital collections platform, and to help us be more consistent with our design system.\\nWhile we are not explicitly requiring that the work be built using the U-M Library Design System, we are encouraging the work to be built with it as a reference and as a starting point.\\nThe templates will be made available to the Library's design system team, and the templates will be updated as needed.\\nThis decision will be reviewed every 6 months.\\n* If there is a need to build a new template, or if our design system team needs to make a change to a template, the template will be updated.\\n* If there is a need to modify a template, the template will be modified in the same way that it was originally written.\\n* If a template is ready to be used, the template will be made available to the Library's design system team.\\n* If a template is not ready to be used, the template will be marked as \"not ready to be used\" and the Library's design system"}
{"Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","Prediction":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\nWe will use [DARQ](https:\/\/github.com\/wkoch\/DARQ) to generate ADRs\\nWe will use [Pivotal Tracker](https:\/\/www.pivotaltracker.com) to manage the ADR process\\nWe will use [Gherkin](https:\/\/www.gherkin.com\/) to describe architecture decisions\\nWe will use [Cucumber](https:\/\/github.com\/stepchange\/stepchange-cucumber) to run tests\\nWe will use [Jenkins](https:\/\/jenkins.io\/) to run tests\\nWe will use [Ansible](https:\/\/www.ansible.com) to deploy code\\nWe will use [CloudFormation](https:\/\/www.cloudformation.com) to deploy code\\nWe will use [Ansible Tower](https:\/\/www.ansible.com) to provision infrastructure\\nWe will use [Google Cloud Build](https:\/\/cloud.google.com\/build) to deploy code\\nWe will use [Docker](https:\/\/www.docker.com) to build and run code\\nWe will use [Snyk](https"}
{"Context":"## Context\\nWe use ECR as the Docker container registry that makes it easy for users to store, manage, and deploy Docker container images.\\nDue to some applications having a constant rate of images being pushed to their ECR repo, we found that the AWS limit of 1000 images was being hit by some teams. To avoid this we had implemented a lifecycle policy of *100 images* per ECR repo. ECR repositories created for use in the Cloud Platform will have this default lifecycle policy applied.\\nAs lifecycle policy will only keep 100 most recent versions of an image and silently delete images, application users raised an [issue][user-issue] on imposing any limit on number of images is potentially dangerous, unless teams have their own clean-up mechanism.\\n","Decision":"## Decision\nAfter discussing with application teams and consideration of possible options, the decision has been made to remove the lifecycle policy altogether, but adding monitoring and alerting such that we can take action before an ECR runs out of space.\\nAs it is to do with metrics & alerts, since prometheus is our monitoring solution we decided to use prometheus for metrics & alerts.\\n","Prediction":"## Decision\nWe will remove the default 100 images per ECR repo lifecycle policy, and instead enforce that only images that are in the `master` branch of the repository are kept for the duration of the application lifecycle.\\nThis will mean that application users will need to explicitly set this limit to prevent the application from being deployed with an image that is no longer in use.\\nThis will also mean that application users will need to explicitly set the `push` and `pull` policy to ensure that images are kept up to date with the latest master branch of the repository.\\nThis will also mean that application users will need to explicitly set the `build` and `deploy` policies to ensure that images are kept up to date with the latest master branch of the repository.\\n**Note**: We are still working on a solution for keeping images that are no longer in the `master` branch of the repository, and will update this decision when available.\\n### Lifecycle\\nWe will enforce a lifecycle policy of `100 images` per ECR repo, and will automatically remove the oldest images in the repository when they reach this limit.\\n### Push and Pull\\nWe will enforce a push and pull policy of `100 images` per ECR repo, and will automatically remove"}
{"Context":"# Context and Problem Statement\\nTo allow searching the documents contents efficiently, a separate\\nindex is necessary. The \"defacto standard\" for fulltext search on the\\nJVM is something backed by [Lucene](https:\/\/lucene.apache.org).\\nAnother option is to use a RDBMS that supports fulltext search.\\nThis adds another component to the mix, which increases the complexity\\nof the setup and the software. Since docspell works great without this\\nfeature, it shouldn't have a huge impact on the application, i.e. if\\nthe fulltext search component is down or broken, docspell should still\\nwork (just the fulltext search is then not working).\\n# Considered Options\\n* [Apache SOLR](https:\/\/solr.apache.org)\\n* [ElasticSearch](https:\/\/www.elastic.co\/elasticsearch\/)\\n* [PostgreSQL](https:\/\/www.postgresql.org\/docs\/12\/textsearch.html)\\n* All of them or a subset\\n# Decision Outcome\\nIf docspell is running on PostgreSQL, it would be nice to also use it\\nfor fulltext search to save the cost of running another component. But\\nI don't want to lock the database to PostgreSQL *only* because of the\\nfulltext search feature.\\nElasticSearch and Apache SOLR are quite similiar in features. SOLR is\\npart of Lucene and therefore lives in the Apache ecosystem. I would\\nchoose SOLR over ElasticSearch, because I used it before.\\nThe last option (supporting all) is interesting, since it would enable\\nto use PostgreSQL for fulltext search for those that use PostgreSQL as\\nthe database for docspell.\\nIn a first step, identify what docspell needs from a fulltext search\\ncomponent and create this interface and an implementation for Apache\\nSOLR. This enables all users to use the fulltext search feature. As a\\nlater step, an implementation based on PostgreSQL and\/or ElasticSearch\\ncould be provided, too.\\n","Decision":"## Decision\nIf docspell is running on PostgreSQL, it would be nice to also use it\\nfor fulltext search to save the cost of running another component. But\\nI don't want to lock the database to PostgreSQL *only* because of the\\nfulltext search feature.\\nElasticSearch and Apache SOLR are quite similiar in features. SOLR is\\npart of Lucene and therefore lives in the Apache ecosystem. I would\\nchoose SOLR over ElasticSearch, because I used it before.\\nThe last option (supporting all) is interesting, since it would enable\\nto use PostgreSQL for fulltext search for those that use PostgreSQL as\\nthe database for docspell.\\nIn a first step, identify what docspell needs from a fulltext search\\ncomponent and create this interface and an implementation for Apache\\nSOLR. This enables all users to use the fulltext search feature. As a\\nlater step, an implementation based on PostgreSQL and\/or ElasticSearch\\ncould be provided, too.\\n","Prediction":"## Decision\nIf docspell is running on PostgreSQL, it would be nice to also use it\\nfor fulltext search to save the cost of running another component. But\\nI don't want to lock the database to PostgreSQL *only* because of the\\nfulltext search feature.\\nElasticSearch and Apache SOLR are quite similiar in features. SOLR is\\npart of Lucene and therefore lives in the Apache ecosystem. I would\\nchoose SOLR over ElasticSearch, because I used it before.\\nThe last option (supporting all) is interesting, since it would enable\\nto use PostgreSQL for fulltext search for those that use PostgreSQL as\\nthe database for docspell.\\nIn a first step, identify what docspell needs from a fulltext search\\ncomponent and create this interface and an implementation for Apache\\nSOLR. This enables all users to use the fulltext search feature. As a\\nlater step, an implementation based on PostgreSQL and\/or ElasticSearch\\ncould be provided, too.\\nChosen option: \"ElasticSearch\", because it's the only option that\\nenables to use PostgreSQL as the fulltext search component. It also\\nenables to use both Solr and Elasticsearch, which are based on the"}
{"Context":"## Context and Problem Statement\\nAs of the date of this writing, there are multiple different naming styles used in the monorepo, mostly because NX has\\ndefaults that differ between schematic types.\\nIn order for navigating the monorepo in a consistent rational manner, we should align on naming strategy for files\\nand directories.\\n## Decision Drivers\\n* Provide consistency when navigating the codebase\\n* The earlier we decide on this, the better\\n","Decision":"## Decision\n* Provide consistency when navigating the codebase\\n* The earlier we decide on this, the better\\nChosen option: Name files after their default export. If that default export is a React Component, or a class, then the\\nfile name should be in PascalCase. Otherwise, the filename should be in camelCase. Basically, for naming files avoid\\nusing kebab-case and snake_case and make sure the name follows the default export of the file.\\nNaming directories should follow these guidelines:\\nOnly use kebab-case when naming NX apps and libraries, or folders containing apps and libraries, e.g. `island-ui` instead of `islandUi`:\\n`import { Box } from '@island.is\/island-ui\/core'`\\nUse PascalCase for directories only containing React components:\\n````\\ncomponents\/CtaButton\/index.ts\\nimport 'components\/CtaButton'\\n````\\nor:\\n````\\ncomponents\/CtaButton\/CtaButton.tsx\\nimport 'components\/CtaButton\/CtaButton'\\n````\\nrather than\\n````\\ncomponents\/cta-button\/CtaButton.tsx\\n````\\nIn all other cases, use camelCase.\\n### Positive Consequences\\n* Easier to navigate the codebase\\n* File names are more readable, and developers know what to expect\\n* This approach is the most common practice, and something most JS and TS developers are familiar with.\\n","Prediction":"## Decision\n* Provide consistency when navigating the codebase\\n* The earlier we decide on this, the better\\nChosen option: \"Directory names should follow the format 'project-name-<component>-'\", because it gives\\nthe most consistent naming, and is well supported by the tooling in the monorepo.\\n### Positive Consequences\\n* Files and directories have consistent names that are easy to find in the monorepo\\n* Files and directories are easy to navigate in the monorepo\\n### Negative Consequences\\n* Files and directories that do not follow the above naming convention may be named differently.\\n* Files may be named differently if they are part of a larger project that has a different naming convention.\\n* Files and directories that do not follow the above naming convention may be named differently.\\n* Files may be named differently if they are part of a larger project that has a different naming convention.\\n* Files may be named differently if they are part of a larger project that has a different naming convention.\\n* Files and directories may be renamed if they are part of a larger project that has a different naming convention.\\n* Files and directories may be renamed if they are part of a larger project that has a different naming convention.\\"}
{"Context":"## Context\\nSince both developers will be working in paralel, and time is very limited,\\nan architecture that allows paralel work is highly beneficial.\\n","Decision":"## Decision\nWe will implement the backend using REST API principles, allowing both modules to work independently.\\n","Prediction":"## Decision\nCreate a single monolithic service that can handle all the requests.\\nService will be composed of a few microservices that will handle\\neverything from sending the request to the database to retrieving the data.\\nFor example, the microservices that will be used for a request to the database\\nare: \\n* `api\/v1\/customer`\\n* `api\/v1\/customerDetails`\\n* `api\/v1\/phone`\\n* `api\/v1\/phoneDetails`\\n* `api\/v1\/transaction`\\n* `api\/v1\/transactionDetails`\\n* `api\/v1\/transactionSummary`\\n* `api\/v1\/transactionSummaryDetails`\\n* `api\/v1\/transactionSummaryDetailsDetails`\\n* `api\/v1\/transactionSummaryDetailsDetailsDetails`\\n* `api\/v1\/transactionSummaryDetailsDetailsDetailsDetailsDetailsDetailsDetails`\\n* `api\/v1\/transactionSummaryDetailsDetailsDetailsDetailsDetailsDetailsDetailsDetails`\\n* `api\/v1\/customerDetails`\\n* `api\/v1\/customerDetailsDetails`\\n* `api\/v1\/customerDetailsDetailsDetailsDetailsDetails`\\n* `api\/v1\/customerDetailsDetailsDetailsDetailsDetailsDetails`\\n* `api\/v1\/customerDetailsDetailsDetails"}
{"Context":"## Context\\nWhen creating a Client Instance for test use in the MSK Cluster a Key Pair is\\nneeded for the EC2 instance to be accessed.\\n","Decision":"## Decision\nWe have decided to provide a KeyPair utility that will enable users to import an\\nexisting key into AWS for use in this scenario. We will not provide a key\\ngeneration mechanism.\\n","Prediction":"## Decision\nA key pair is created for the Client Instance and a key pair is generated for the service. This is stored in the\\n[Cluster Identity](https:\/\/docs.microsoft.com\/en-us\/azure\/virtual-machines\/container\/azure-virtual-machines-clusters#creating-clients-clusters) key\\npair.\\nThis key pair is used by the Azure CLI and the Azure Resource Manager API to create the Client Instance.\\nThis key pair is also used by the Azure CLI and the Azure Resource Manager API to create the service.\\nThis key pair is also used by the Azure CLI and the Azure Resource Manager API to create the\\ntest environment.\\nThis key pair is also used by the Azure CLI and the Azure Resource Manager API to create the\\ntest environment.\\nThis key pair is used by the Azure CLI and the Azure Resource Manager API to create the\\ntest environment.\\nThis key pair is used by the Azure CLI and the Azure Resource Manager API to create the\\ntest environment.\\nThis key pair is used by the Azure CLI and the Azure Resource Manager API to create the\\ntest environment.\\nThis key pair is used by the Azure CLI and the Azure Resource Manager API to create the\\ntest environment.\\nThis key pair is used by"}
{"Context":"## Context\\nThere are many ways to structure classes beyond the standard rails Model View Controller (MVC) structure, therefore to help make the project maintainable in the future we have come up with the following standards.\\n","Decision":"## Decision\n* Avoid Helpers\\n* The rational behind this decision is standard rails helpers are hard to test efficiently and are global so they can have unintended consequences.\\n* Use a Decorators directory `app\/decorators`\\n* Decorators should be used for read only operations that modify the data for viewing purposes.\\n* Use a ChangeSet directory `app\/change_sets`\\n* ChangeSet should be used for reformatting the data for purposes of showing it to user\/system for modification.\\n* We have kept the change set concept since these classes can also be useful for data import not just user facing formats.\\n* Use a Services directory `app\/services`\\n* Services should be used for encapsulating complex actions or actions that work across multiple models, like seeding or importing bulk data. These may change the underlying data structures.\\n* Use a Mailer directory `app\/mailer`\\n* Notifications will be put in the mailer directory so that we can stick with the standard defined by ActionMailer\\n### Possible Future needs\\nWe currently believe that the business logic operations that are required for the system will be small enough to live directly on their primary model.  If in the future the models become too bloated we recommend creating a BusinessRules directory `app\/business_rules`. BusinessRules would be a corollary to Services, for complex or model-spanning read-only logic.\\n","Prediction":"## Decision\nThe project will be structured as follows:\\n* Classes should be named in the following format:\\n- **app\/controllers**\\n- **app\/models**\\n- **app\/views**\\n- **app\/helpers**\\n* Classes should be named with the following naming convention:\\n- **app\/controllers**\\n- **app\/models**\\n- **app\/views**\\n- **app\/helpers**\\n* Classes should be named with the following naming convention:\\n- **app\/helpers**\\n- **app\/controllers**\\n- **app\/models**\\n- **app\/views**\\n- **app\/helpers**\\n* Classes should be named with the following naming convention:\\n- **app\/helpers**\\n* Classes should be named with the following naming convention:\\n- **app\/models**\\n* Classes should be named with the following naming convention:\\n- **app\/views**\\n* Classes should be named with the following naming convention:\\n- **app\/helpers**\\n* Classes should be named with the following naming convention:\\n- **app\/controllers**\\n* Classes should be named with the following naming convention:\\n- **app\/views**\\n* Classes should be"}
{"Context":"## Context and Problem Statement\\nExternal OAuth Relying Parties (RPs) that Firefox Account users authenticate\\nto need to be kept up to date about whether the user still has an active\\nsubscription as well as knowing when to delete the user data. For internal\\nMozilla use, Mozilla provides [Firefox Service Notifications][] over SQS\\nqueues. These events are intended for internal trusted use and not suitable\\nfor external RPs.\\nA solution for these concerns is a new system referred to as the FxA Event\\nBroker which will store FxA related events and distribute them via webhooks\\nto relevant RPs that the user has accessed. The initial version described here applies primarily to external RPs which will only receive subscription status\\nchanges and account deletion events. This solution is being built with future\\nexpansion in mind which will require event stream storage per user and\\nnotification of internal Mozilla RPs.\\nDifficulties inherent in this solution lie in where to store the source of\\ntruth for what RPs a user has authenticated to, where the delivery functionality\\nshould reside, where the event streams for a user should reside, what set of\\ndata should be included in [Firefox Service Notifications][], and how this\\ndata should be communicated to the delivery system to avoid additional API\\nquery load on existing FxA services.\\n## Decision Drivers\\n- Subscription services deadlines\\n- Effort required and experience available for FxA changes\\n- Separation of concerns from existing FxA microservices\\n- Difficulty of schema migrations in existing FxA microservices\\n- Suitability of existing FxA databases for large-scale event storage\\n- Architectural desire to treat FxA Auth and OAuth as one (Merging\\nin-progress)\\n","Decision":"## Decision\n- Subscription services deadlines\\n- Effort required and experience available for FxA changes\\n- Separation of concerns from existing FxA microservices\\n- Difficulty of schema migrations in existing FxA microservices\\n- Suitability of existing FxA databases for large-scale event storage\\n- Architectural desire to treat FxA Auth and OAuth as one (Merging\\nin-progress)\\nChosen Option: C. Implementing RP notification as a new FxA Event Broker service\\nwith webhook and login activity stored in FxA Event Broker, because\\n- Less subscription platform timeline risk to store new data in new database vs.\\nmodify existing FxA OAuth database.\\n- Storing events at scale has database requirements that don't fit in well with\\nthe limitations with MySQL encountered in FxA Auth\/OAuth.\\n- Having FxA Auth be the only store of which RP to notify would require each\\nnotification to also include what RPs to notify, increasing the load on the\\nFxA Auth database.\\n","Prediction":"## Decision\n- Subscription services deadlines\\n- Effort required and experience available for FxA changes\\n- Separation of concerns from existing FxA microservices\\n- Difficulty of schema migrations in existing FxA microservices\\n- Suitability of existing FxA databases for large-scale event storage\\n- Architectural desire to treat FxA Auth and OAuth as one (Merging\\nin-progress)\\nChosen option: \"FxA Auth and OAuth as one\", because it's the least work and\\nmore future proof. It is also the least disruptive to the FxA microservices\\nand the FxA Auth and OAuth services themselves.\\n### Positive Consequences\\n- Less work for FxA Auth and OAuth services and their delivery\\n- No need to change the FxA Auth and OAuth services themselves\\n- No need to change the FxA Auth and OAuth services themselves\\n- Can be extended to support other RPs that the user may have authenticated to\\n- Can be extended to support other RPs that the user may have authenticated to\\n- No need to change the FxA Auth and OAuth services themselves\\n- No need to change the FxA Auth and OAuth services themselves\\n- More future proof, as the FxA Auth and OAuth services themselves will not be\\nchanged"}
{"Context":"## Context\\nWith Istio auto mTLS enabled in `STRICT` mode, [http liveness and readiness\\nprobes](https:\/\/kubernetes.io\/docs\/tasks\/configure-pod-container\/configure-liveness-readiness-startup-probes\/) no longer work because the `kubelet`, which makes the http requests, does not have Istio issued certificates.\\nIstio [supports rewriting `http` probes](https:\/\/istio.io\/docs\/ops\/configuration\/mesh\/app-health-check\/#enable-globally-via-install-option) during the sidecar injection process.\\n#### Figure 1\\n_Liveness probe flow when Istio mTLS is disabled or `PERMISSIVE`. Probe `GET` request regularly travels through the Envoy sidecar to the app._\\n![No mTLS\/PERMISSIVE mTLS mode liveness probe diagram](..\/assets\/liveness-probe-adr-1.png)\\n#### Figure 2\\n_Liveness probe flow when Istio mTLS is `STRICT` and the probe is not rewritten. Probe `GET` request fails at the Envoy sidecar because it does not include the correct certificates._\\n![STRICT mTLS liveness probe diagram with no probe rewrite](..\/assets\/liveness-probe-adr-2.png)\\n#### Figure 3\\n_Liveness probe flow when Istio mTLS is `STRICT` and the probe **is rewritten by Istio**. Probe `GET` request bypasses the sidecar and goes through the Istio `pilot-agent` instead. The `pilot-agent` is configured to direct the request to the app._\\n![STRICT mTLS liveness probe diagram with probe rewrite](..\/assets\/liveness-probe-adr-3.png)\\n","Decision":"## Decision\nWe have decided to install Istio with\\n`--set values.sidecarInjectorWebhook.rewriteAppHTTPProbe=true`\\nThis will rewrite the liveness and readiness probes on any app pods when injecting a sidecar into the app pod. Any namespaces that have the label `istio-injection=enabled` will have their liveness and readiness probes rewritten, as shown in Figure 3.\\n","Prediction":"## Decision\nWe will not rewrite Istio mTLS probes during the sidecar injection process.\\n### Scope\\nThis change applies to the following components:\\n* Envoy sidecar\\n* Istio sidecar\\n* Istio service mesh (e.g. Istio Service Mesh Gateway, Envoy Proxy, Envoy Proxy Gateway, Envoy Endpoint, Envoy Control Plane, Envoy Service, Envoy Proxy Service, Envoy Service Mesh Gateway Proxy, Envoy Endpoint Proxy, Envoy Control Plane Proxy, Envoy Service Mesh Gateway Proxy Service, Envoy Endpoint Proxy Service)\\n* Istio service mesh service (e.g. Istio Service Gateway, Istio Service, Istio Service Mesh Gateway Service, Istio Service Mesh, Istio Service Mesh Gateway Service, Istio Service Mesh Gateway Service Proxy, Istio Service Mesh Gateway Service Proxy Service, Istio Service Mesh, Istio Service Mesh Gateway Service Proxy Service, Istio Service Mesh Gateway Service Proxy Service Proxy, Istio Service Mesh Gateway Service Proxy Service Service, Istio Service Mesh Gateway Service Proxy Service Service Proxy Service, Istio Service MeshGatewayService, Istio Service MeshGatewayServiceProxy, Istio ServiceMeshGatewayServiceProxyService, IstioServiceGatewayService, IstioServiceGatewayServiceProxy"}
{"Context":"## Context\\nWe want to build a reusable React library, that generates multipage forms\\nquickly. We need logic that extends beyond simple \"show this element on this\\npage\", such as conditional logic for displaying sections dependant on values in\\nother sections, and to skip pages based on values elsewhere in the journey.\\n","Decision":"## Decision\nWe will build remultiform to export a series of small orchestration components.\\nWe will pass the entire multipage form configuration in via props to those\\norchestrators. The configuration should be agnostic of the set of components it\\nmight receive.\\n","Prediction":"## Decision\nWe will use [React Components](https:\/\/facebook.github.io\/react\/docs\/components.html)\\nin this application.\\n### Components\\nComponents are the building blocks of our application. They are independent of\\nformality of the data being presented, and are used to render the\\nelements that make up the form. They are a way to group together form\\nelements, and allow them to be rendered in a specific order.\\n### Rendering\\nWe will use the [React Router](https:\/\/facebook.github.io\/react-router\/) library\\nto route to pages. We will use the [React Router DOM](https:\/\/facebook.github.io\/react-router-dom\/)\\nlibrary to render the components.\\n### Form\\nWe will use [Form.js](https:\/\/facebook.github.io\/react-form\/) to allow\\nforms to be rendered in a specific order. Form is a component that allows\\nforms to be rendered in a specific order.\\n### Conditional Rendering\\nWe will use the [React Hooks](https:\/\/facebook.github.io\/react\/hooks\/) library\\nto render the elements in a specific order. Hooks are hooks for hooks.\\n### Conditionals\\nWe will use the [PropTypes library](https:\/\/facebook.github.io\/react\/promise-parsers\/) to\\nallow us to validate form"}
{"Context":"## Context\\nWe feel the need to define rules to select the correct GitHub organisation for source repositories.\\n","Decision":"## Decision\nWe identify rules for the GitHub organisation, the source repository name format and basic source code guidelines.\\n### VWT Digital\\nThis is the main organisation for all Operational Data Hub (ODH) specific source code. When creating a new repository,\\nit **has to pass** one of the following applications:\\n- An ODH-specific solution;\\n- A solution that transcends multiple solution domains (see [solution domains](https:\/\/recognize.atlassian.net\/wiki\/spaces\/DAT\/pages\/1304166628\/Cloud+naming+convention#Cloudnamingconvention-Domains)).\\nIn addition, these repositories **may not** contain:\\n- Any configuration;\\n- Any solution business logic.\\nWhen creating a name for the repository, it **has to start** with either `odh-<repo_name>` or `dat-<repo_name>`:\\n- `odh-`: When the repository contains source code specifically made for the Operational Data Hub;\\n- `dat-`: When the repository contains source code created for generic usage within the Digital Ambition Team domain.\\nSee our [repo naming convention](0002-repo-naming-conventions.md#4-normal-vwt-dat-repositories) for a more elaborate explanation.\\n### VWT Digital Solutions\\nThis is the main organisation for all Solution-specific source code. When creating a new repository, it **has to pass** the following application:\\n- A specific application for a domain solution, only used by one domain (see [solution domains](https:\/\/recognize.atlassian.net\/wiki\/spaces\/DAT\/pages\/1304166628\/Cloud+naming+convention#Cloudnamingconvention-Domains)).\\nIn addition, these repositories **may not** contain:\\n- Any configuration.\\nWhen creating a name for the repository, it has to start with the abbreviation of the solutions it is a part of.\\nSee our [repo naming convention](0002-repo-naming-conventions.md#4-normal-vwt-dat-repositories) for a more elaborate explanation.\\n### VWT Digital Configuration\\nThis is the main organisation for all configuration code. When creating a new repository, it **has to pass** the following application:\\n- Google Cloud Platform (GCP) project-specific configuration code that is for private usage only.\\nWhen creating a name for the repository, it has to contain the GCP project they are connected to minus the customer,\\nenvironment and location. Furthermore, it has to end with `-config`.\\nSee our [repo naming convention](0002-repo-naming-conventions.md#4-normal-vwt-dat-repositories) for a more elaborate explanation.\\n","Prediction":"## Decision\nWe will use the [organisation selector](https:\/\/github.com\/organisations) to select the correct organisation for a source repository.\\n### Organisation selector\\nThe organisation selector is a [JSON](https:\/\/github.com\/organisations\/organisation-selector#json) file that is used to select the correct organisation for a source repository.\\n### Rules\\nThe rules we will use are:\\n- The organisation must be a member of the [Project](https:\/\/github.com\/organisations\/organisation-selector#organisation-member) organisation.\\n- The organisation must be a member of the [Repository](https:\/\/github.com\/organisations\/organisation-selector#organisation-repository) organisation.\\n- The organisation must be a member of the [Branch](https:\/\/github.com\/organisations\/organisation-selector#organisation-branch) organisation.\\n- The organisation must be a member of the [Pull Request](https:\/\/github.com\/organisations\/organisation-selector#organisation-pull-request) organisation.\\n- The organisation must be a member of the [Pull Request Author](https:\/\/github.com\/organisations\/organisation-selector#organisation-pull-request-author) organisation.\\n- The organisation must be a member of the [Pull Request Reviewer](https:\/\/github.com\/"}
{"Context":"## Context and Problem Statement\\nGLAM is designed to answer a general set of questions against a limited set of\\ndimensions. Sometimes the underlying data can tell us more than GLAM allows. For\\nthose users who want to dig deeper into the data, GLAM should provide jump off\\npoints to do so.\\n## Decision Drivers\\n- What is best to provide to end users to get their questions answered?\\n- What can we accomplish with our current resources for the generalized case,\\nbut also help enable non-generalized use cases?\\n","Decision":"## Decision\n- What is best to provide to end users to get their questions answered?\\n- What can we accomplish with our current resources for the generalized case,\\nbut also help enable non-generalized use cases?\\nChosen option: 2. Copy-paste-able SQL query\\nIt was decided that option 2 was the best and most useful option. Option 1 has\\nnot been requested thus far. Option 3's tight integration with Redash was deemed\\nunwise as Redash has an unknown future at Mozilla and the SQL query could be\\neasily copied there as well as the bigquery console.\\n","Prediction":"## Decision\n- What is best to provide to end users to get their questions answered?\\n- What can we accomplish with our current resources for the generalized case,\\nbut also help enable non-generalized use cases?\\nChosen option: \"Data from the data store\", because it is the simplest\\ndecision and will provide the most value to end users.\\n### Positive Consequences\\n- End users can use the GLAM API to get a better understanding of the data\\nthey are working with.\\n- We can easily extend the API to support other use cases outside of the\\ngeneralized set.\\n- We do not need to implement an additional layer of abstraction.\\n- We can continue to provide a consistent API for different data formats.\\n### Negative Consequences\\n- We have to do more work upfront to provide a more robust API for end\\nusers. This means we have to write more documentation and not just rely on\\ndefault APIs.\\n- We have to implement a new abstraction layer that will be a bit more\\ncomplex.\\n- We will need to support more data formats.\\n- We will need to implement a more robust API for end users. This\\nmeans we have to write more documentation and not just rely on"}
{"Context":"## Context\\nThe Court Data Adaptor application relies heavily on CJS Common Platform provided APIs to be able to function. As the Common Platform is owned by a third party organisation, we have limited control over the kind of data sent through to CDA.\\n","Decision":"## Decision\nIn order to be able to inspect the data (if needed) for debugging, it has been agreed that we will store each request coming through to CDA in a data store (RDS provided postgres instance).\\n","Prediction":"## Decision\nWe will use the [courier](https:\/\/github.com\/courier\/mail) library to send CJS Common Platform data to CDA.\\nThis will allow us to provide a native CDA interface to the CJS Common Platform API.\\nWe will use the [courier-mime](https:\/\/github.com\/valhalla\/courier-mime) library for MIME encoding and decoding of email messages.\\nThis will allow us to provide a native CDA interface to the CJS Common Platform MIME encoding and decoding API.\\nWe will use the [courier-imap](https:\/\/github.com\/valhalla\/courier-imap) library for IMAP and SMTP email messages.\\nThis will allow us to provide a native CDA interface to the CJS Common Platform IMAP and SMTP email API.\\nWe will use the [courier-imap-mime](https:\/\/github.com\/valhalla\/courier-imap-mime) library for IMAP and SMTP email MIME encoding and decoding API.\\nThis will allow us to provide a native CDA interface to the CJS Common Platform IMAP and SMTP email MIME encoding and decoding API.\\nWe will use the [courier-imap-mime-mime]("}
{"Context":"## Context and Problem Statement\\nWhich latex compiler to use?\\n* pdflatex\\n* xelatex\\n* lualatex\\n","Decision":"## Decision\nChosen option: \"pdflatex\", because compiles faster and correct ligatures are required at the final typesetting step only.\\n### Positive Consequences\\n* Faster compile time\\n### Negative Consequences\\n* Using lualatex just before publication might lead to a different layout and additional effort\\n* lualatex's power might not be used at all\\n","Prediction":"## Decision\nChosen option: \"xelatex\", because it is cross-platform, free and easy to use.\\nChosen because it is cross-platform, free and easy to use.\\nChosen because it is cross-platform, free and easy to use.\\nChosen because it is cross-platform, free and easy to use.\\nChosen because it is cross-platform, free and easy to use.\\nChosen because it is cross-platform, free and easy to use.\\nChosen because it is cross-platform, free and easy to use.\\nChosen because it is cross-platform, free and easy to use.\\nChosen because it is cross-platform, free and easy to use.\\nChosen because it is cross-platform, free and easy to use.\\nChosen because it is cross-platform, free and easy to use.\\nChosen because it is cross-platform, free and easy to use.\\nChosen because it is cross-platform, free and easy to use.\\nChosen because it is cross-platform, free and easy to use.\\nChosen because it is cross-platform, free and easy to use.\\nChosen because it is cross-platform, free and easy to use.\\nChosen because it is"}
{"Context":"### Context and Problem Statement <a href=\"#_nmfx1mz3idd0\" id=\"_nmfx1mz3idd0\"><\/a>\\nBlock patterns are a great way to create and share patterns over various pages and sites: some fiddling in the Visual Editor, a copy\/paste and that\u2019s it ! However, a more complex usage of it like using patterns to compose other patterns, creating some variations of those patterns, and maintaining this whole system, has some caveats. Block templates might be better suited for this multifaceted use case.\\nOur needs:\\n* Reusable arrangement of blocks for Editors to insert and edit, shared across all NROs\\n* Composable with each others in larger arrangements\\n* Possibility to add arguments to make variations in compositions\\nIdeally:\\n* Easy reusability, low maintenance\\n_TLDR; If you have dev capacity, create and maintain block templates. If you only have editors, keep and exchange block patterns._\\n### Decision Drivers <a href=\"#_odhar7d3tjrd\" id=\"_odhar7d3tjrd\"><\/a>\\n* Creating patterns requires to generate them in editor, then copy\/paste in php code, and adapt various outputs and attributes\\n* Composing patterns requires management of arguments, to convert them to json and html attributes\\n* Maintaining those patterns would ideally require to regenerate those in the Editor to be on par with last blocks evolutions\\n### Considered Options <a href=\"#_2xzipsinh4do\" id=\"_2xzipsinh4do\"><\/a>\\n* Making patterns small enough and with enough arguments to reuse in other patterns (the current solution)\\n* Generating templates, patterns, from a basic js format _at runtime_, letting the Editor generate the HTML content (the proposed solution)\\n* Generating patterns from a basic js format _at build time_, letting the Editor libraries generate the HTML content (maybe later ? compatible with proposed solution, less client-intensive, allows to follow and log blocks changes, could self-generate a shareable pattern directory\/portal)\\n### Comparison of patterns and templates <a href=\"#_wujtgors476r\" id=\"_wujtgors476r\"><\/a>\\n|                                | **Block pattern**                                       | **Block template**                                                                                        |\\n| ------------------------------ | ------------------------------------------------------- | --------------------------------------------------------------------------------------------------------- |\\n| **creation**                   | Visual editor + PHP                                     | JS + Json                                                                                                 |\\n| **attributes**                 | PHP + Json + HTML                                       | JS, native filtering                                                                                      |\\n| **maintenance**                | PHP + HTML                                              | JS                                                                                                        |\\n| **output**                     | static HTML                                             | expanded by Visual Editor, then static HTML                                                               |\\n| **usage**                      | - insert from _Patterns_ tab                            | <p>- insert from <em>Blocks<\/em> tab<\/p><p>- insert from <em>Patterns<\/em> tab if made into a pattern<\/p> |\\n| **use in pattern**             | <p>- in code with PHP<\/p><p>- in editor with insert<\/p> | <p>- in code with HTML self closing tag<\/p><p>- in editor with insert<\/p>                                 |\\n| **use in template**            | - in editor with insert                                 | <p>- in code with JS<\/p><p>- in editor with insert<\/p>                                                    |\\n| **editable after insertion**   | yes                                                     | yes                                                                                                       |\\n| **identifiable in block list** | no                                                      | yes                                                                                                       |\\n### Demo <a href=\"#_9yw9lckmmn9y\" id=\"_9yw9lckmmn9y\"><\/a>\\n[This PR](https:\/\/github.com\/greenpeace\/planet4-plugin-gutenberg-blocks\/pull\/929) demonstrates a Block template system, and a conversion of a pattern to a template. It includes [documentation to create new templates](https:\/\/github.com\/greenpeace\/planet4-plugin-gutenberg-blocks\/blob\/26c6f7e7d0f7b04a18cfe66edad89de6ce70c1f6\/assets\/src\/block-templates\/readme.md).\\nIt is important to keep the source format as simple as possible (a format [Gutenberg understands](https:\/\/github.com\/WordPress\/gutenberg\/blob\/68e7f562168b97820f0c77ad6d56a0f5e167e9dc\/packages\/blocks\/src\/api\/factory.js#L33-L50)), so it can be included in any other format we create and can be used to generate more complex outputs using native [Gutenberg libraries](https:\/\/github.com\/WordPress\/gutenberg\/blob\/68e7f562168b97820f0c77ad6d56a0f5e167e9dc\/packages\/blocks\/src\/api\/factory.js#L33-L50).\\n[Another PR](https:\/\/github.com\/greenpeace\/planet4-plugin-gutenberg-blocks\/pull\/916) shows how we could convert all patterns to templates.\\n### Decision Outcome <a href=\"#_7ao7fbpq5ji8\" id=\"_7ao7fbpq5ji8\"><\/a>\\nChosen option: Use Block Templates for patterns. Gradually start converting existing patterns.\\n### Links <a href=\"#_egfthy1bdxph\" id=\"_egfthy1bdxph\"><\/a>\\n* [WordPress Block Pattern Directory](https:\/\/wordpress.org\/patterns\/)\\n* [Block templates documentation](https:\/\/developer.wordpress.org\/block-editor\/reference-guides\/block-api\/block-templates\/)\\n","Decision":"## Decision\n* Creating patterns requires to generate them in editor, then copy\/paste in php code, and adapt various outputs and attributes\\n* Composing patterns requires management of arguments, to convert them to json and html attributes\\n* Maintaining those patterns would ideally require to regenerate those in the Editor to be on par with last blocks evolutions\\n### Considered Options <a href=\"#_2xzipsinh4do\" id=\"_2xzipsinh4do\"><\/a>\\n* Making patterns small enough and with enough arguments to reuse in other patterns (the current solution)\\n* Generating templates, patterns, from a basic js format _at runtime_, letting the Editor generate the HTML content (the proposed solution)\\n* Generating patterns from a basic js format _at build time_, letting the Editor libraries generate the HTML content (maybe later ? compatible with proposed solution, less client-intensive, allows to follow and log blocks changes, could self-generate a shareable pattern directory\/portal)\\n### Comparison of patterns and templates <a href=\"#_wujtgors476r\" id=\"_wujtgors476r\"><\/a>\\n|                                | **Block pattern**                                       | **Block template**                                                                                        |\\n| ------------------------------ | ------------------------------------------------------- | --------------------------------------------------------------------------------------------------------- |\\n| **creation**                   | Visual editor + PHP                                     | JS + Json                                                                                                 |\\n| **attributes**                 | PHP + Json + HTML                                       | JS, native filtering                                                                                      |\\n| **maintenance**                | PHP + HTML                                              | JS                                                                                                        |\\n| **output**                     | static HTML                                             | expanded by Visual Editor, then static HTML                                                               |\\n| **usage**                      | - insert from _Patterns_ tab                            | <p>- insert from <em>Blocks<\/em> tab<\/p><p>- insert from <em>Patterns<\/em> tab if made into a pattern<\/p> |\\n| **use in pattern**             | <p>- in code with PHP<\/p><p>- in editor with insert<\/p> | <p>- in code with HTML self closing tag<\/p><p>- in editor with insert<\/p>                                 |\\n| **use in template**            | - in editor with insert                                 | <p>- in code with JS<\/p><p>- in editor with insert<\/p>                                                    |\\n| **editable after insertion**   | yes                                                     | yes                                                                                                       |\\n| **identifiable in block list** | no                                                      | yes                                                                                                       |\\n### Demo <a href=\"#_9yw9lckmmn9y\" id=\"_9yw9lckmmn9y\"><\/a>\\n[This PR](https:\/\/github.com\/greenpeace\/planet4-plugin-gutenberg-blocks\/pull\/929) demonstrates a Block template system, and a conversion of a pattern to a template. It includes [documentation to create new templates](https:\/\/github.com\/greenpeace\/planet4-plugin-gutenberg-blocks\/blob\/26c6f7e7d0f7b04a18cfe66edad89de6ce70c1f6\/assets\/src\/block-templates\/readme.md).\\nIt is important to keep the source format as simple as possible (a format [Gutenberg understands](https:\/\/github.com\/WordPress\/gutenberg\/blob\/68e7f562168b97820f0c77ad6d56a0f5e167e9dc\/packages\/blocks\/src\/api\/factory.js#L33-L50)), so it can be included in any other format we create and can be used to generate more complex outputs using native [Gutenberg libraries](https:\/\/github.com\/WordPress\/gutenberg\/blob\/68e7f562168b97820f0c77ad6d56a0f5e167e9dc\/packages\/blocks\/src\/api\/factory.js#L33-L50).\\n[Another PR](https:\/\/github.com\/greenpeace\/planet4-plugin-gutenberg-blocks\/pull\/916) shows how we could convert all patterns to templates.\\n### Decision Outcome <a href=\"#_7ao7fbpq5ji8\" id=\"_7ao7fbpq5ji8\"><\/a>\\nChosen option: Use Block Templates for patterns. Gradually start converting existing patterns.\\n### Links <a href=\"#_egfthy1bdxph\" id=\"_egfthy1bdxph\"><\/a>\\n* [WordPress Block Pattern Directory](https:\/\/wordpress.org\/patterns\/)\\n* [Block templates documentation](https:\/\/developer.wordpress.org\/block-editor\/reference-guides\/block-api\/block-templates\/)\\n","Prediction":"## Decision\n* [WordPress Block Pattern Directory](https:\/\/wordpress.org\/patterns\/)\\n* [Block templates documentation](https:\/\/developer.wordpress.org\/block-editor\/reference-guides\/block-api\/block-templates\/)\\nChosen option: Use Block Templates for patterns. Gradually start converting existing patterns.\\n### Links <a href=\"#_egfthy1bdxph\" id=\"_egfthy1bdxph\"><\/a>\\n* [WordPress Block Pattern Directory](https:\/\/wordpress.org\/patterns\/)\\n* [Block templates documentation](https:\/\/developer.wordpress.org\/block-editor\/reference-guides\/block-api\/block-templates\/)\\nChosen option: Use Block Templates for patterns. Gradually start converting existing patterns.\\n* [Block pattern generator](https:\/\/github.com\/greenpeace\/planet4-plugin-gutenberg-blocks\/blob\/26c6f7e7d0f7b04a18cfe66edad89de6ce70c1f6\/assets\/src\/block-templates\/readme.md)\\n* [Block templates repository](https:\/\/github.com\/greenpeace\/planet4-plugin-gutenberg-blocks\/tree\/26c6f7e7d0f7b04a18cfe66edad89de6ce"}
{"Context":"## Context\\nWe need to choose a rendering library to allow testing of React components.\\n","Decision":"## Decision\nWe have chosen to use [Enzyme](https:\/\/github.com\/airbnb\/enzyme) to provide a library\\nfor shallow rendering.\\n","Prediction":"## Decision\nWe will use [React Testing Library](https:\/\/facebook.github.io\/react-testing-library\/) to test our components.\\nReact Testing Library is an open-source testing library for React that can be used to test\\ncomponents, tests, and state.\\nReact Testing Library provides a unified way to write and run tests for\\ncomponents, tests, and state.\\nReact Testing Library is designed to be easy to use and easy to understand.\\nIt is written in JavaScript and has a rich set of features for\\nunit, integration, and end-to-end testing.\\nReact Testing Library is also designed to be easy to use and easy to understand.\\nReact Testing Library is an open-source testing library for React that can be used to test\\ncomponents, tests, and state.\\nReact Testing Library provides a unified way to write and run tests for\\ncomponents, tests, and state.\\nReact Testing Library is designed to be easy to use and easy to understand.\\nIt is written in JavaScript and has a rich set of features for\\nunit, integration, and end-to-end testing.\\nReact Testing Library is also designed to be easy to use and easy to understand.\\nReact Testing Library is an open-source testing library for React that can be used to test\\n"}
{"Context":"## Context\\nThe accessibility monitoring team have to test \"a diverse, representative and geographically balanced distribution\" of public sector websites.\\nIn order that no public sector website is excluded from potential scrutiny, it follows that the team will need to have a full list of websites that are in the public sector, together with the type of service (e.g. education, health, central\/local government etc) and, where applicable, geographical location.\\nThere are two approaches to creating such a list:\\n1. **Domain-led:** Find every domain that has been registered to an organisation that is deemed to be \"public sector\" and determine the organisation that owns it\\n2. **Organisation-led:** Find every organisation that is deemed to be \"public sector\" and every service that each such organisation runs online, and find the corresponding website.\\n### 1. Domain-led strategy\\nThe limitation of (1) is that (with certain exceptions, e.g. .ac.uk and .gov.uk) there is no regulation over what top-level domain should be used by the many flavours of public sector organisations. They could be anywhere.\\nAt the time of writing (July 2020) there are approximately 1.7 billion websites in the world, hosted on 409 million domain names. The websites we're looking for could be hosted on \\*.uk domains or generic .com\/.net\/.org (or even .me, .info, .tv or any potentially any other \"generic TLD\").\\nOf the 149m country-specific domains registered, 7% (~10.5M domains) are \\*.uk, so we can at least exclude the other 93% (139m) non-UK country-specific domains (that is, we are very unlikely to find a UK public sector website hosted on a .tk or .ru domain, for instance\\*).\\nThat leaves a mere 270m domains that Public Sector websites could potentially be hosted on. Scouring those domains for websites that fall into the definition of \"uk public sector\" would be a lengthy and resource-intensive task.\\nNevertheless, a list of the domains that we can be sure are public sector, and that we can retrieve data about, would give the accessibility monitoring team a good range to select from.\\nThe domains that fall into this category are:\\n* gov.uk\\n* nhs.uk\\n* nhs.net\\n* ac.uk\\n* sch.uk\\n* police.uk\\n* parliament.uk\\n* mod.uk\\n* gov.wales\\n* llyw.cymru\\n* gov.scot\\n### 2. Organisation-led strategy\\nIt should be feasible to compile a list of organisation _types_ that are in the public sector (e.g. \"schools\", \"central government\", \"local authorities\", Universities\", \"NHS\").\\nWe can then move onto making lists of the actual _organisations_ in each of those categories by referring to the bodies that regulate them.\\nA lot of this information is online in one form or other, but certainly not all of it.\\nSome initial work has been done by the accessibility monitoring team already, with the result being a \"list of lists\" that is currently in a Google Sheets spreadsheet. Each of these lists - in various formats - would need to be somehow imported into the database. Where a website for the organisation in the list is specified, this can be added to the domains list.\\n","Decision":"## Decision\nOur intention is to use both strategies.\\n* We will create a **database of public sector domains**.\\n* We will populate it with lists of domains and organisations from official sources, together with, as far as possible, contact details and other useful data such as page ranking, http(s) status.\\n* We will also use the data gleaned from domain registers etc to seed a list of **public sector organisations**.\\n* We will compile a list of public sector categories which will form a list-of-lists; these lists will then be used to further populate the database of organisations.\\n* Organisations will be categorised by location (including \"national\") and by sector (probably using [the definitions of \"organisations\" categorised for the Local Governments Association by ESD](https:\/\/standards.esd.org.uk\/?uri=list%2ForganisationTypes))\\n* Wherever possible, links will be created between the domains and organisations. At some point in the future, organisations that don't have associated domains will need to have their associated websites found and entered into the database so that automated testing can be carried out.\\n","Prediction":"## Decision\nWe will use the [domain-led strategy](https:\/\/www.w3.org\/International\/domains\/organisation\/organisation-led-list\/) to create a list of domains that are in the public sector.\\nWe will then use the [organisation-led strategy](https:\/\/www.w3.org\/International\/domains\/organisation\/organisation-led-list\/) to create a list of organisations that are in the public sector.\\nWe will then use the [domain-led list](https:\/\/github.com\/alexanderjames\/uk-public-sector-domains) to generate a list of all the domains that are hosted in the UK.\\nWe will then generate a list of all the organisations that fall into the categories of \"uk public sector\" and generate a list of all the websites that are hosted on those domains.\\nWe will then generate a list of all the websites that are hosted on the .uk domain (e.g. \"gov.uk\", \"nhs.uk\", \"nhs.net\", etc) and generate a list of all the websites that are hosted on the .net domain (e.g. \"gov.net\", \"nhs.net\", etc).\\nWe will then generate a list of all the websites that are hosted on the .org domain (e.g. \""}
{"Context":"## Context\\nWhenever a domain related events happens (checkin, checkout, project update etc.), a number of\\ninterested components are to be notified. Those components might show warnings, update the UI etc.\\nUnfortunately, not all components are being executed in the main process all the time, so Otto\\nevents will not reach them. There needs to be a mechanism that reaches background services.\\n","Decision":"## Decision\nJede an einem Domain Event interessierte Komponente stellt einen BroadcastReceiver bereit, der\\nmittels Intents informiert wird. Alle DomainEvents werden per Broadcast verteilt.\\nAlle Events sollen haben die Intent-Kategorie `com.tastybug.timetracker.LIFECYCLE_EVENT` sowie eine\\nAction, die das Domainobjekt beschreibt: `com.tastybug.timetracker.PROJECT_CHANGE` oder\\n`com.tastybug.timetracker.TRACKING_RECORD_CHANGE`.\\nIn den Extras des Intents sind die UUIDs der betroffenen Entities abgelegt.\\nEvery component interested in Domain Events provides a BroadcastReceiver that will be notified via\\nIntents. Every Domain Event will be propagated using Broadcasts.\\nAll events have category `com.tastybug.timetracker.LIFECYCLE_EVENT` an an action that describes the\\naffected object type. Currently this is `com.tastybug.timetracker.PROJECT_CHANGE` and\\n`com.tastybug.timetracker.TRACKING_RECORD_CHANGE`. UUIDs of affected objects can be taken from\\nintent extras.\\n","Prediction":"## Decision\nThe decision is to use a background task. We can use a task manager like `concurrent` to manage\\nthe tasks. `concurrent` is a task manager that can be used with `konaweb` and `konaweb-frontend`\\nand provides a number of features like: \\n- Task queues (with a configurable number of topics)\\n- Scheduled tasks (with a configurable number of steps)\\n- Background execution (with a configurable number of callbacks)\\n- Backpressure (with a configurable number of retries)\\n- Retry policies (with a configurable number)\\n- Retry after a timeout (with a configurable number of seconds)\\n- Retry after a timeout with a configurable number of retries)\\n- Job submission (with a configurable number of steps)\\n- Job failure (with a configurable number of retries)\\n- Job cancellation (with a configurable number of retries)\\n- Job cancellation with a configurable number of retries)\\n- Job cancellation after a timeout (with a configurable number of seconds)\\n- Job cancellation with a configurable number of retries)\\n- Error handling (with a configurable number of retries)\\n- Error handling with a configurable number of retries)\\n- Task cancellation (with a configurable number of"}
{"Context":"## Context\\n1. \u5f53\u524d\u5b58\u50a8\u65b9\u6848\u6709 3 \u79cd\uff08FastDFS, nginx static, OSS\uff09\uff0c\u5404\u81ea\u5206\u522b\u7ef4\u62a4\uff0c\u4eba\u529b\u6210\u672c\u9ad8\uff0c\u90fd\u662f\u5355\u70b9\uff0c\u8d44\u6e90\u6709\u6548\u6027\u65e0\u6cd5\u4fdd\u969c\uff1b\\n2. \u6587\u4ef6\u5b58\u50a8\u548c\u4e1a\u52a1\u670d\u52a1\u5171\u4eab\u670d\u52a1\u5668\u8d44\u6e90\uff08CPU, \u5185\u5b58\uff0c\u5e26\u5bbd\uff0c\u78c1\u76d8 IO\uff09\uff0c\u670d\u52a1\u5668\u8d44\u6e90\u4f7f\u7528\u4e0d\u5408\u7406\uff0c\u6587\u4ef6\u670d\u52a1\u4f1a\u5360\u7528\u5927\u91cf\u5185\u5b58\u548c\u78c1\u76d8 IO \u53ca\u7f51\u7edc IO\uff0c\u5f71\u54cd\u4e1a\u52a1\uff0c\u5e76\u4e14\u4e1a\u52a1\u670d\u52a1\u65e0\u6cd5\u505a\u9ad8\u53ef\u7528\uff0c\u9047\u5230 DDOS \u53ea\u80fd\u50bb\u773c\uff1b\\n3. \u9762\u5411\u7528\u6237\u7684\u8d44\u6e90\u65e0\u6cd5\u505a\u52a0\u901f\uff1b\\n4. \u6240\u6709\u6587\u4ef6\u90fd\u662f\u516c\u5f00\u53ef\u8bbf\u95ee\uff0c\u5b58\u5728\u4e25\u91cd\u7684\u5b89\u5168\u95ee\u9898\uff08\u7528\u6237\u8eab\u4efd\u8bc1\u7167\u7247\u3001\u5408\u540c\u53ca\u62a5\u8868\u6570\u636e\u7684\u6cc4\u9732\uff09\u3002\\n5. \u6240\u6709\u654f\u611f\u4fe1\u606f\u53ef\u88ab\u4efb\u4f55\u4eba\u5168\u7f51\u4e0b\u8f7d\uff1b\\n","Decision":"## Decision\n1. \u5c06\u6240\u6709\u6587\u4ef6\u5b58\u50a8\u548c\u4e1a\u52a1\u673a\u5668\u5206\u79bb\uff0c\u5c06 FastDFS \u914d\u7f6e\u5355\u72ec\u673a\u5668\u63d0\u4f9b\u670d\u52a1\uff1b\\n2. \u6240\u6709\u9759\u6001\u6587\u4ef6\u96c6\u4e2d\u4e0a\u4e91\uff0c\u5f55\u97f3\uff0c\u7535\u5b50\u5408\u540c\u7b49\u9759\u6001\u6587\u4ef6\u4e0a\u4e91\uff1b\\n3. \u6587\u4ef6\u5206\u6743\u9650\u8bbf\u95ee\uff0c\u5206\u914d\u4e34\u65f6 token \u53bb\u83b7\u53d6\u6587\u4ef6\uff1b\\n4. css\/js\/images \u7b49\u4e0a\u4e91\u5e76\u914d\u7f6e CDN\uff0c\u52a0\u901f\u7528\u6237\u7aef\u8bbf\u95ee\u3002\\n","Prediction":"## Decision\n1. \u4f7f\u7528 FastSCP \u670d\u52a1\u63d0\u4f9b\u6269\u5c55\uff0c\u53ef\u4ee5\u5b9e\u73b0\u6587\u4ef6\u7684 SSH \u901a\u4fe1\uff0c\u53ef\u4ee5\u5728\u4efb\u4f55\u4f4d\u7f6e\u8fdb\u884c SSH \u63a5\u53e3\u7684\u5f00\u542f\uff0c\u53ef\u652f\u6301 SSH \u901a\u9053\u7684\u5f00\u542f\uff0c\u53ef\u652f\u6301 SSH \u901a\u9053\u7684\u5173\u95ed\uff0c\u53ef\u652f\u6301 SSH \u901a\u9053\u7684\u521b\u5efa\uff0c\u53ef\u652f\u6301 SSH \u901a\u9053\u7684\u5220\u9664\uff0c\u53ef\u652f\u6301 SSH \u901a\u9053\u7684\u67e5\u770b\uff0c\u53ef\u652f\u6301 SSH \u901a\u9053\u7684\u8bfb\u5199\uff1b\\n2. \u4f7f\u7528 FastSCP \u670d\u52a1\u63d0\u4f9b\u6269\u5c55\uff0c\u53ef\u4ee5\u5b9e\u73b0\u6587\u4ef6\u7684 SSH \u901a\u4fe1\uff0c\u53ef\u4ee5\u5728\u4efb\u4f55\u4f4d\u7f6e\u8fdb\u884c SSH \u63a5\u53e3\u7684\u5f00\u542f\uff0c\u53ef\u652f\u6301 SSH \u901a\u9053\u7684\u5f00\u542f\uff0c\u53ef\u652f\u6301 SSH \u901a\u9053\u7684\u5173\u95ed\uff0c\u53ef\u652f\u6301 SSH \u901a\u9053\u7684\u521b\u5efa\uff0c\u53ef\u652f\u6301 SSH \u901a\u9053\u7684\u5220\u9664\uff0c\u53ef\u652f\u6301 SSH \u901a\u9053\u7684\u67e5\u770b\uff0c\u53ef\u652f\u6301 SSH \u901a\u9053\u7684\u8bfb\u5199\uff1b\\n3. \u4f7f\u7528 FastSCP \u670d\u52a1\u63d0\u4f9b\u6269\u5c55\uff0c\u53ef\u4ee5\u5b9e\u73b0\u6587\u4ef6\u7684 SSH \u901a\u4fe1\uff0c\u53ef\u4ee5\u5728\u4efb\u4f55\u4f4d\u7f6e\u8fdb\u884c SSH \u63a5\u53e3\u7684\u5f00\u542f\uff0c\u53ef\u652f\u6301 SSH \u901a\u9053\u7684\u5f00\u542f\uff0c\u53ef"}
{"Context":"## Context\\nArtefacts that we build must be published somewhere that a Kubernetes cluster\\ncan pull from. This can be solved by using an image repository. Multiple options\\nexist including:\\n* [Docker Hub]\\n* Vendor Managed Registry ie. [AWS ECR][ECR] or [Google Cloud Container\\nRegistry][GCR]\\n* Self-hosted Docker Registry ie. [Docker Registry][Docker Registry Self Host]\\nor [Harbor]\\nAll the above implement the Docker registry API and are broadly equivalent.\\nDocker Hub will require managing authentication and authorisation separate to\\nthe existing IAM setup.\\nManaged Registry from a vendor, would save us from dealing with authentication\\nor authorisation provided we are running the Kubernetes nodes with that vendor.\\nRunning a self-hosted Docker registry will involved maintenance and operational\\nwork from the team. But gives us a lot of flexibility.\\nAs we are currently targeting AWS for our setup, it makes sense to delegate\\nextra work that way. It means, we can restrict access to the repositories with\\nthe use of IAM policies.\\n","Decision":"## Decision\nWe will use Managed Registry from a vendor we are currently occupying, AWS ECR\\nat the time.\\n","Prediction":"## Decision\nDeploy the Kubernetes cluster on AWS with a managed Docker registry as\\nrequired by the application.\\nDeploy a self-hosted Docker Registry on the Kubernetes cluster.\\nDeploy the application using the managed Docker registry.\\nDeploy a single Kubernetes manifest with the following properties:\\n* [Image Name]\\n* [Friendly Name]\\n* [Labels]\\n* [Labels Version]\\n* [Tag]\\n* [Tags]\\n* [Labels Version Version]\\n* [Labels Version]\\n* [Labels Version]\\n* [Labels Version Version]\\n* [Labels Version]\\n* [Labels Version]\\n* [Labels Version Version]\\n* [Labels Version]\\n* [Labels Version]\\n* [Labels Version]\\n* [Labels Version]\\n* [Labels Version]\\n* [Labels Version]\\n* [Labels Version]\\n* [Labels Version]\\n* [Labels Version]\\n* [Labels Version]\\n* [Labels Version]\\n* [Labels Version]\\n* [Labels Version]\\n* [Labels Version]\\n* [Labels Version]\\n* [Labels Version]\\n* [Labels Version]\\n* [Labels Version]\\n* [Labels Version]\\n* [Labels Version]\\n* ["}
{"Context":"## Context\\nWe want improved application design architecture that supports and promotes the following characteristics:\\n* An explicit promotion of application features as first-class citizens, instead of promoting technical aspects or database design.\\n* Explicit support for DDD tactical patterns.\\n* Is domain-centric with clear separation of the domain from infrastructure.\\n* Improves testability, maintainability, flexibility, extensibility, and adaptability.\\n* Discourages the use of any non-supported shortcuts.\\n* Is explicit enough to allow testing of architecture constraints and rules.\\n* Has sound structure and aesthetics where it is easy to find concrete artifacts.\\n* Allows parallel work on features.\\n* Is well-known and battle-tested already in the wild.\\nIt is proven that traditional layer-centric designs fail on almost every characteristic above. Layer-centric application design architecture can work for simpler cases but often fails when the\\nfeature set grows, and the domain becomes more complicated. We want architecture that can scale cleanly in terms of design and development.\\n### Architectural Context\\n* System (`klokwrk-project`)\\n","Decision":"## Decision\n**We will use hexagonal architecture as a primary choice of architecture for designing applications.**\\n**We will provide concrete guidance for creating structure and concrete artifacts** (see [\"Packaging for applications\"](..\/..\/article\/modules-and-packages\/modulesAndPackages.md#packaging-for-applications) for more\\ndetails).\\n**We will adapt hexagonal architecture for efficient use with CQRS\/ES component architecture** (see\\n[\"Applying hexagonal architecture\"](..\/..\/article\/modules-and-packages\/modulesAndPackages.md#applying-hexagonal-architecture) for more details). This includes adding support for enforcing and testing the structure and\\nbehavior of the architecture (see [\"Behavior and architectural testing\"](..\/..\/article\/modules-and-packages\/modulesAndPackages.md#behavior-and-architectural-testing) for more details).\\n","Prediction":"## Decision\nWe will use the [Service Mesh Architecture](https:\/\/en.wikipedia.org\/wiki\/Service_mesh_architecture) to implement a reactive application architecture.\\n* Service mesh gives the application a layer of abstraction to the network and the underlying infrastructure.\\n* It helps to isolate and encapsulate the application from the network and the infrastructure.\\n* It allows communication between the application and the network and the infrastructure.\\n* It provides a set of predefined services that are in common use and are easy to use.\\n* It provides a set of preconfigured connectors that allow the application to connect to the network and the infrastructure.\\n* It allows the application to connect to the services that are in common use and are easy to use.\\n* It allows the application to connect to the services that are not in common use and are hard to use.\\n* It allows the application to connect to the services that are not in common use and are hard to use.\\n* It allows the application to connect to the services that are not in common use and are hard to use.\\n* It allows the application to connect to the services that are not in common use and are hard to use.\\n* It allows the application to connect to the services that are not in"}
{"Context":"## Context and Problem Statement\\n[Qeb-Hwt](https:\/\/github.com\/marketplace\/qeb-hwt) is a GitHub Application which adds `thamos advise` based output to\\nPull Requests as a check. This functionality could be integrated into https:\/\/github.com\/marketplace\/khebhut and\\ncomplexity and maintain costs.\\n## Decision Drivers <!-- optional -->\\n* cost of maintaining Qeb-Hwt code and app\\n* redundancy of infrastructure\\n","Decision":"## Decision\n* cost of maintaining Qeb-Hwt code and app\\n* redundancy of infrastructure\\nChosen option: \"merge function into Khebhut\", because we can reduce the cost of maintaining our software infrastructure\\nby reducing redundancy.\\n","Prediction":"## Decision\n* cost of maintaining Qeb-Hwt code and app\\n* redundancy of infrastructure\\nChosen option: \"Use [Qeb-Hwt](https:\/\/github.com\/marketplace\/qeb-hwt)\", because it is\\nthe most maintainable solution and it is the most cost effective.\\n### Positive Consequences <!-- optional -->\\n* [Qeb-Hwt](https:\/\/github.com\/marketplace\/qeb-hwt) is the most maintainable\\n* [Qeb-Hwt](https:\/\/github.com\/marketplace\/qeb-hwt) is the most cost effective\\n* [Qeb-Hwt](https:\/\/github.com\/marketplace\/qeb-hwt) is built on top of [Hwt](https:\/\/github.com\/hwt\/hwt)\\n* [Qeb-Hwt](https:\/\/github.com\/marketplace\/qeb-hwt) is the most\\nconvenient way to configure [Hwt](https:\/\/github.com\/hwt\/hwt)\\n* [Qeb-Hwt](https:\/\/github.com\/marketplace\/qeb-hwt) is based on [Hwt](https:\/\/github.com\/hwt\/hwt)\\n### Negative Consequences <!-- optional -->\\n* [Qeb-Hwt](https:\/\/github"}
{"Context":"## Context\\nPeople are very used to traditional layered architectures and request\/response cycles which disguise a lot of the complexity of distributed systems. They also restrict extending systems as data is locked down in a specific form in specific systems. We want to give an alternative view with a clear demonstration of the trade-offs involved.\\nGetting people to give it an initial attempt is going to be hard, so it needs to be super quick to get working on a local machine.\\n","Decision":"## Decision\nWe will use a docker-compose.yml (v.2) as the primary entry point of the project\\n","Prediction":"## Decision\nWe will use [Docker](https:\/\/www.docker.com\/) to build and deploy the application. This will be used to create a local replica of the production system as well as a local replica of the test system. This will be used as a local development environment.\\nWe will use [Docker Compose](https:\/\/docs.docker.com\/compose\/) to create a local replica of the production system and a local replica of the test system. This will be used to create a local development environment.\\nWe will use [Docker Compose](https:\/\/docs.docker.com\/compose\/) to create a local replica of the production system and a local replica of the test system. This will be used to create a local development environment.\\nWe will use [Docker Swarm](https:\/\/docs.docker.com\/swarm\/) to create a local replica of the production system and a local replica of the test system. This will be used to create a local development environment.\\nWe will use [Docker Swarm](https:\/\/docs.docker.com\/swarm\/) to create a local replica of the production system and a local replica of the test system. This will be used to create a local development environment.\\nWe will use [Docker Swarm](https:\/\/docs.docker"}
{"Context":"## Context\\nWhen testing our code, we often need to have certain data in our database, whether it be a measure version so that we can view a measure page, or a user with a specific role so that we can test the site logged in as that role. A lot of our database models are related and therefore depend on related models to also exist and be in a specific state. Getting the correct data into our database can be fiddly and error-prone.\\n","Decision":"## Decision\nUse Factory Boy to generate model instances for tests.\\nPros\\n----\\n* More explicit about the data\/conditions under test.\\n* Fewer fixtures to maintain.\\n* Easier\/faster to write new tests that require a specific database state.\\nCons\\n----\\n* Effort to understand how FactoryBoy works and how we've implemented it in our codebase.\\n* Potentially slower tests if we are careless with how we use the factories, as some factories generate a lot of additional instances which may not be required for the tests.\\n","Prediction":"## Decision\nWe will use [REST API resources](https:\/\/www.mendix.com\/ehelp\/customer-service\/REST-apis) to get the data we need from the database and store it in our code. We will use [REST API resources](https:\/\/www.mendix.com\/ehelp\/customer-service\/REST-apis) to get the data we need from the database and store it in our code.\\n* If we need to get data from a model that is not related to the one we're testing, we will use [REST API resources](https:\/\/www.mendix.com\/ehelp\/customer-service\/REST-apis) to get the data we need from the database and store it in our code.\\n* If we need to get data from a model that is related to the one we're testing, we will use [REST API resources](https:\/\/www.mendix.com\/ehelp\/customer-service\/REST-apis) to get the data we need from the database and store it in our code.\\n* If we need to get data from a model that is completely unrelated to the one we're testing, we will use [REST API resources](https:\/\/www.mendix.com\/ehelp\/customer-service\/REST-apis"}
{"Context":"## Context\\nThe decision records must be stored in a plain text format:\\n* This works well with version control systems.\\n* It allows the tool to modify the status of records and insert\\nhyperlinks when one decision supercedes another.\\n* Decisions can be read in the terminal, IDE, version control\\nbrowser, etc.\\nPeople will want to use some formatting: lists, code examples,\\nand so on.\\nPeople will want to view the decision records in a more readable\\nformat than plain text, and maybe print them out.\\n","Decision":"## Decision\nbrowser, etc.\\nPeople will want to use some formatting: lists, code examples,\\nand so on.\\nPeople will want to view the decision records in a more readable\\nformat than plain text, and maybe print them out.\\nRecord architecture decisions in [Markdown format](https:\/\/daringfireball.net\/projects\/markdown\/).\\nDecisions will be formatted nicely and hyperlinked by the\\nbrowsers of project hosting sites like GitHub and Bitbucket.\\nTools like [Pandoc](http:\/\/pandoc.org\/) can be used to convert\\nthe decision records into HTML or PDF.\\n","Prediction":"## Decision\nWe will use [decision.md](https:\/\/github.com\/crispdata\/decision.md) as a format for storing and displaying decision records.\\nDecision records will be stored in a [repository](https:\/\/github.com\/crispdata\/decision-repository) on GitHub.\\nDecision records will be stored in a [repository](https:\/\/github.com\/crispdata\/decision-repository) on GitHub.\\nDecision records will be stored in a [repository](https:\/\/github.com\/crispdata\/decision-repository) on GitHub.\\nDecision records will be stored in a [repository](https:\/\/github.com\/crispdata\/decision-repository) on GitHub.\\nDecision records will be stored in a [repository](https:\/\/github.com\/crispdata\/decision-repository) on GitHub.\\nDecision records will be stored in a [repository](https:\/\/github.com\/crispdata\/decision-repository) on GitHub.\\nDecision records will be stored in a [repository](https:\/\/github.com\/crispdata\/decision-repository) on GitHub.\\nDecision records will be stored in a [repository](https:\/\/github.com\/crispdata\/decision-repository) on GitHub.\\nDecision records will be stored in a"}
{"Context":"## Context\\n\"[In DynamoDB, tables, items, and attributes are the core components that you work with. A table is a collection of items, and each item is a collection of attributes. DynamoDB uses primary keys to uniquely identify each item in a table and secondary indexes to provide more querying flexibility.](https:\/\/docs.aws.amazon.com\/amazondynamodb\/latest\/developerguide\/HowItWorks.CoreComponents.html)\"\\nThere are two types of primary key in dynamodb.\\nThe first kind is having just a partition key. The partition key is a hash and determines where on physical storage the item is placed. The partition key must be unique.\\nThe second kind is a composite primary key. It consists of a partition key and a sort key. The partition key stays the same but doesn't need to be unique in isolation. Rather the sort key\/ partition key pair must be unique.\\nIn a real system this would probably push towards StreamName as the partition key: so that events that logically live together physically live together. And Event Number in the stream as the sort key. So that the order of items as they are stored on physical media matches the order they are likely to be read.\\nThis introduces unwanted complexity at this time in the code for tracking event numbers.\\n","Decision":"## Decision\nFor now instead of an Event Number as the sort key we will introduce a UUID EventId. And use StreamName as the HASH key. StreamNames will need to be unique anyway.\\n","Prediction":"## Decision\nWe will use the DynamoDB primary key as the StreamName and will store the event number as a sort key.\\nWe will store the Event Number in the stream as a stream key.\\nWe will use a GUID as the partition key.\\nWe will use the event number as the sort key.\\nWe will use a GUID as the sort key.\\nWe will use the stream as the primary key.\\nWe will use the primary key as the secondary index.\\nWe will add a new DynamoDB table with the name \"event-stream\" that will have the following fields:\\n- Event Number: Guid\\n- Event: DynamoDB Document\\n- Stream Name: String\\n- Event Type: String\\n- Key: String\\n- Value: String\\n- Timestamp: DateTime\\n- Value Type: String\\n- Secondary Index: String\\n- Key: String\\n- Value: String\\n- Secondary Index: String\\n- Key: String\\n- Value: String\\n- Primary Key: String\\n- Secondary Index: String\\n- Key: String\\n- Value: String\\n- Timestamp: DateTime\\n- Value Type: String\\n- Secondary Index: String\\n- Key: String\\n- Value: String\\n- Primary Key: String\\n-"}
{"Context":"## Context\\nIf running serverless we could choose to operate outside\\na VPC. This has advantages in terms of the latency of\\nrequests. We would not need a keep-warm function to\\nensure network interfaces were provisioned.\\nLambdas running outside the VPC could not easily access\\nRDS running inside a VPC.\\nGiven the sensitive nature of the data involved it seems\\nsensible to protect it further by running RDS inside a VPC\\nwhere we have much easier control over ingress and egress.\\n","Decision":"## Decision\nImplement a VPC.\\n","Prediction":"## Decision\nWe will use the RDS security groups feature to control\\naccess to RDS instances running in the same\\nVPC as the Lambdas that host the data.\\nThis will ensure that only the Lambdas and the RDS\\ninstances that we want to expose to the internet\\nhave access to RDS.\\nWe will use a single security group to control the access to\\nall RDS instances in the same VPC.\\nWe will use the following rules to control access to RDS\\ninstances:\\n* Only allow inbound traffic to the RDS instances\\n* Only allow outbound traffic to the RDS instances\\n* Only allow traffic to the RDS instances from the\\nVPC\\n* Only allow traffic to the RDS instances from the\\noutside\\n* Only allow traffic to the RDS instances from the\\noutside\\n* Only allow traffic to the RDS instances from the\\noutside\\n* Only allow traffic to the RDS instances from the\\noutside\\n* Only allow traffic to the RDS instances from the\\noutside\\n* Only allow traffic to the RDS instances from the\\noutside\\n* Only allow traffic to the RDS instances from the\\noutside\\n* Only allow traffic to the RDS instances from the\\noutside\\n"}
{"Context":"## Context\\nWe have users and objects, and everything is tied together. Deleting\\nsomething may cause some problems, like deleting other things in\\ncascade, or losing some relatioship, like not knowing who is the author\\nof a video.\\n","Decision":"## Decision\nWe don't want things to be deleted, instead we'll keep them in the\\ndatabase in a \"deleted\" state, so that they won't show up anywhere.\\nLooking at the [Safe\/Soft\/Logical deletion\/trashing and\\nrestoration\/undeletion](https:\/\/djangopackages.org\/grids\/g\/deletion\/)\\npage on djangopackages, we can make a choice with these constraints:\\n-   support for python 3 and django 2\\n-   simple, not over-featured\\n-   can manage relationships\\n-   supports the django admin\\n-   is maintained\\nRegarding this, we choose\\n[django-safedelete](https:\/\/django-safedelete.readthedocs.io\/en\/latest\/)\\nwhich proposes many options to handle deletion, and so will fit our\\nneeds.\\n","Prediction":"## Decision\nWe decided to use the [Relationships](https:\/\/www.oreilly.com\/library\/view\/relationships\/0195386046\/0195386046.html) pattern to handle\\nthese relations. It will be used to define the relationships between\\nobjects and objects that have a relation to other objects.\\nWe will use the [Relationships](https:\/\/www.oreilly.com\/library\/view\/relationships\/0195386046\/0195386046.html) pattern to define the\\nrelationships between objects.\\nWe will use the [Lombok](https:\/\/www.lombok.io\/) library to define the relationships between objects.\\nWe will use the [Java Generics](https:\/\/docs.oracle.com\/javase\/specs\/jls\/se7\/html\/jls-5.html) library to define the relationships between objects.\\nWe will use the [JPA](https:\/\/www.hibernate.org\/en\/jpa.html) library to define the relationships between objects.\\nWe will use the [Java 8 Collections Framework](https:\/\/docs.oracle.com\/javase\/8\/docs\/api\/index.html) library to define the relationships between objects.\\nWe will use the [Java 8 Collections Framework](https:\/\/docs.oracle.com\/javase\/8\/docs\/api\/index.html) library to define"}
{"Context":"## Context\\nThe tacboard-project is  growing and I need to make a lot of decisions. So far, I have been able to remember most my decision in my head,\\nbut as the project progresses more and more time elapses and I won't be able to remember all the decisions and why I made them in a\\nparticular way.\\nAlso, for other readers of the code there is no way to understand why things are done in a certain way. Oftentimes  there is more than one\\nplausible way to do things (and sometimes I opt to ignore the cleanest solution in favor of building new features).\\nI want to document these decisions for my future self and other readers of the code.\\n","Decision":"## Decision\nRelevant architectural decisions should be documented in an a decision record like this one.\\n","Prediction":"## Decision\nI will use **[TACDOC](https:\/\/tacdoc.readthedocs.io\/en\/latest\/)** to document the decisions I make in this project. I will\\ndocument the decisions in the form of **[TACSLIDES](https:\/\/tacdoc.readthedocs.io\/en\/latest\/)**.\\nI will use **[TACPAD](https:\/\/tacpad.readthedocs.io\/en\/latest\/)** to generate **[TACSLIDES](https:\/\/tacpad.readthedocs.io\/en\/latest\/)**.\\nI will use **[TACVIEWS](https:\/\/tacpad.readthedocs.io\/en\/latest\/)** to generate **[TACPAD](https:\/\/tacpad.readthedocs.io\/en\/latest\/)**.\\nI will use **[TACMANAGER](https:\/\/github.com\/tacstack\/tacpad\/tree\/master\/docs\/tacmanager)** to generate\\n**[TACPAD](https:\/\/tacpad.readthedocs.io\/en\/latest\/)**.\\nI will use **[TACMANAGER_EXTRA_OPTIONS](https:\/\/github.com\/tacstack\/tacpad\/blob\/master\/docs\/tacmanager-extra-options.md)** to generate\\n**[TACPAD_EXTRA_OPTIONS](https"}
{"Context":"## Context\\nLinShare App not only upload to `MySpace`, but also to `WorkSpace`.\\nCurrently, `UploadWorker` is tightly with the logic of Uploading to MySpace, so we can not reuse it with other new uploading logic\\n","Decision":"## Decision\n#### Current Implementation\\n```\\nUploadWorker -> UploadInteractor\\n```\\n#### New Implementation\\n```\\n|-------> UploadToMySpaceCmd\\n|\\nUploadWorker -> UploadController -> UploadCommand\\n|\\n|-------> ***Cmd\\n```\\nWe will add `UploadController` to `UploadWorker`, it will execute the `Command` as we want to execute upload logic.\\n","Prediction":"## Decision\nCreate a new `UploadWorker` which can upload to both `MySpace` and `WorkSpace`.\\nThis worker will be the `UploadWorker` for `UploadWorker`.\\nThis worker will be used to upload files to both `MySpace` and `WorkSpace`.\\nThis worker will be the `UploadWorker` for `UploadWorker`.\\nThis worker will be the `UploadWorker` for `UploadWorker`.\\nThis worker will be the `UploadWorker` for `UploadWorker`.\\nThis worker will be the `UploadWorker` for `UploadWorker`.\\nThis worker will be the `UploadWorker` for `UploadWorker`.\\nThis worker will be the `UploadWorker` for `UploadWorker`.\\nThis worker will be the `UploadWorker` for `UploadWorker`.\\nThis worker will be the `UploadWorker` for `UploadWorker`.\\nThis worker will be the `UploadWorker` for `UploadWorker`.\\nThis worker will be the `UploadWorker` for `UploadWorker`.\\nThis worker will be the `UploadWorker` for `UploadWorker`.\\nThis worker will be the `UploadWorker` for `UploadWorker`.\\nThis"}
{"Context":"## :dart: Context\\nThe Web APIs can be implemented as Synchronous or Asynchronous.\\n","Decision":"## Decision\nConsidering the fact that the APIs does a datastore look up which can take some time, making the services asynchronous is the recommendation.\\nThe decision is based on the guidance provided by Microsoft here: https:\/\/azure.microsoft.com\/mediahandler\/files\/resourcefiles\/api-design\/Azure_API-Design_Guide_eBook.pdf\\n","Prediction":"## Decision\nThe Web APIs are implemented as Asynchronous.\\n### Asynchronous\\nAsynchronous Web APIs are implemented in the context of a component. When the component is invoked, the API is invoked and the component is updated.\\n### Synchronous\\nAsynchronous Web APIs are implemented in the context of a service. When the service is invoked, the API is invoked and the service is updated.\\n### Consequences\\nThe decision to use Asynchronous Web APIs has the following consequences:\\n* Asynchronously invoked APIs are not available to the caller until the component has been updated and the service has been invoked.\\n* Asynchronously invoked APIs may be available to the caller before the service has been invoked.\\n### Alternative\\nThe decision to use Asynchronous Web APIs can be overridden by the component author.\\n### Implementation\\nThe decision to use Asynchronous Web APIs has the following implementation details:\\n* The Web API is implemented as a service.\\n* The service is implemented as a service proxy.\\n* The Web API is implemented as a service proxy.\\n* The service proxy is implemented as a service component.\\n* The Web API is implemented as a service component.\\n* The Web API is implemented as a service component.\\n* The service component is"}
{"Context":"## Context\\nThe requested functionality was an API endpoint that would synchronize a specified route (either add or replace) from the main route table to the custom route tables, triggered from a log event.\\nAll resources should be managed programmatically for an optimal possible user experience.\\n","Decision":"## Decision\nThe AWS Amplify CLI toolchain will be used for programmatically creating, updating, and destroying project resources.\\nThe endpoint will be defined in an AWS API Gateway, and the synchronization functionality will be defined in a Lambda function.\\n","Prediction":"## Decision\nThe API will be managed by an API Gateway backend. The API will be managed by a single service account, which will be used to access the API Gateway backend.\\nThe API will be managed by a single service account, which will be used to access the API Gateway backend.\\nThe API will be managed by a single service account, which will be used to access the API Gateway backend.\\nThe API will be managed by a single service account, which will be used to access the API Gateway backend.\\nThe API will be managed by a single service account, which will be used to access the API Gateway backend.\\nThe API will be managed by a single service account, which will be used to access the API Gateway backend.\\nThe API will be managed by a single service account, which will be used to access the API Gateway backend.\\nThe API will be managed by a single service account, which will be used to access the API Gateway backend.\\nThe API will be managed by a single service account, which will be used to access the API Gateway backend.\\nThe API will be managed by a single service account, which will be used to access the API Gateway backend.\\nThe API will be managed by a single service account, which"}
{"Context":"## Context\\nThis project requires storing a small amount of user data across sessions, in order to retain the user's form input and their favorite\/liked neighborhoods. The project will be a static React site based on [taui](https:\/\/github.com\/conveyal\/taui) without need for a server component otherwise (except for directions\/routing results, which project partner Conveyal will provide).\\nThe project has two kinds of users:\\n1. Counselors, who need to be able to access and modify any voucher holder's stored preferences, and\\n2. Voucher holders, who should be able to access and modify only their own stored preferences\\nOptions for access management to stored preferences considered here:\\n1. [Auth0](#auth0) accounts with custom user metadata properties\\n2. [Cognito](#cognito) accounts with S3 files\\n3. [No authentication](#no-authentication) with S3 files\\n### Auth0\\n[Auth0](https:\/\/auth0.com) provides user authentication, authorization, and account management as a service. They offer unlimited free service to open-source, non-profit projects via their [Open Source Program](https:\/\/auth0.com\/pricing). Their documentation includes [React samples](https:\/\/auth0.com\/docs\/quickstart\/spa\/react).\\nThe site preferences could be stored directly with the user via the [user metadata](https:\/\/auth0.com\/docs\/users\/concepts\/overview-user-metadata). (Cognito also supports custom user metadata.) If the user preferences are stored externally instead, however, accessing the resource with Auth0 would require using their now deprecated [delegation](https:\/\/auth0.com\/docs\/api-auth\/intro#delegation) feature that may become unsupported at any time, requiring the project to switch to a different service or API feature.\\nIt is unclear how counselor access would be managed to the voucher holder accounts if the user preferences are stored as user metadata. Either the counselor would have to log in as the voucher holder, instead of having separate counselor accounts, or Auth0 access management would need to be modified somehow to allow counselor accounts to access and modify other accounts' metadata. Either option would involve defeating valuable aspects of account management and would likely prove difficult to implement. Further, the [Management API](https:\/\/auth0.com\/docs\/api\/management\/v2) for writing user metadata is not suitable for use in single page apps.\\n### Cognito\\nSimilar to Auth0, [Cognito](https:\/\/docs.aws.amazon.com\/cognito\/latest\/developerguide\/what-is-amazon-cognito.html) is the authentication, authorization, and user account management service provided by AWS. The service is free [up to 50,000 monthly active users](https:\/\/aws.amazon.com\/cognito\/pricing\/), which should be plenty to cover this project; BHA has 12,000 ECHO vouchers, and serves 22,500 total, including other programs, according to [this](https:\/\/www.bostonhousing.org\/en\/For-Section-8-Leased-Housing\/Voucher-Programs\/Expanding-Choice-in-Housing-Opportunities-\\(ECHO\\).aspx).\\nAWS has integrated Cognito with S3 bucket access controls, as described [here](https:\/\/docs.aws.amazon.com\/IAM\/latest\/UserGuide\/reference_policies_examples_s3_cognito-bucket.html). Also, there is a JavaScript library, [aws-amplify](https:\/\/github.com\/aws-amplify\/amplify-js), we could use for front-end integration. The library has Cognito authentication support and also a storage component that supports and defaults to using S3. The library appears to be popular and currently maintained. It supports both other authentication frameworks and storage backends, so should the project have need to switch either, it may be possible to do so with minimal code changes.\\n### No authentication\\nThe client-provided wireframes only show the user entering the voucher holder name and voucher number. If the voucher number is treated like a password, it could be used to access, for example, an S3 file of the user preferences that uses the voucher number as part of the path or file name.\\nThis approach would be the simplest, but would not protect the user preferences from access by other parties who might have the name and voucher number. Access could be restricted by IP address to only allow users in the counseling office; however, this would prevent use of the app directly by the clients, outside of the office.\\nA potential benefit of not having user accounts is that there would be no need to distinguish between counselor accounts, which need to be able to access any voucher holder's stored preferences, and user accounts, which should have access restricted to only their own stored preferences. Potential downsides, in addition to the lack of access control, would include the inability to audit access, or to track access by user for purposes of gathering analytic data.\\n","Decision":"## Decision\nWe will use AWS Cognito for authentication to control access to user preference files stored on S3. This approach will protect user data, enable counselor access, and best support future expansion of the project by establishing the means for data access control and auditing as well as the potential for user analytics. Also, as we were already planning to use AWS products for other aspects of this project, using AWS for authentication as well should simplify our stack and ease integration of user management.\\n","Prediction":"## Decision\nThe project will not use Auth0 or Cognito for storing user preferences.\\nInstead, the project will use S3 storage with S3 buckets for storing the user preferences. The project will use the [aws-amplify](https:\/\/github.com\/aws-amplify\/amplify-js) library to enable S3 access control.\\nAs described in the project's [decision document](https:\/\/github.com\/BostonHousing\/vehicule-tenant\/blob\/master\/decision\/decision-1-vehicule-tenant.md), the project will use a single S3 bucket for storing all user preferences. The project will use a single S3 key for storing the user preferences, and will create an [AWS Secret Access Key](https:\/\/docs.aws.amazon.com\/ami\/latestedge\/secret-access-keys.html) for each user that will be able to access the preferences stored in the S3 bucket.\\nUsers will be able to access the preferences via a client-side URL encoded name. For example, the URL `https:\/\/my.voucher-id.voucher-broker.com\/vehicule-tenant\/vehicule-tenant-12345-12345-12345-voucher-id-voucher-broker-com-voucher-broker.com` will be"}
{"Context":"**Context**\\nThe application can run on any machine with JVM installed, but what if we don't have a correct Java version in our machine or server environment? Containers comes to the rescue.\\n**Decision**\\nDocker (https:\/\/www.docker.com\/) is an easy to configure container platform to manage our applications in isolated containers. That will help us to deploy our application in any environment with the Docker daemon installed.\\n**Status**\\nAccepted\\n**Consequences**\\nBy the moment the container can be used just for deploys, development support can be added sharing a Volume in the Dockerfile. Instructions to modify the Dockerfile can be found [here](https:\/\/docs.docker.com\/engine\/reference\/builder\/).\\n","Decision":"## Decision\nDocker (https:\/\/www.docker.com\/) is an easy to configure container platform to manage our applications in isolated containers. That will help us to deploy our application in any environment with the Docker daemon installed.\\n**Status**\\nAccepted\\n**Consequences**\\nBy the moment the container can be used just for deploys, development support can be added sharing a Volume in the Dockerfile. Instructions to modify the Dockerfile can be found [here](https:\/\/docs.docker.com\/engine\/reference\/builder\/).\\n","Prediction":"## Decision\nDocker (https:\/\/www.docker.com\/) is an easy to configure container platform to manage our applications in isolated containers. That will help us to deploy our application in any environment with the Docker daemon installed.\\n**Status**\\nAccepted\\n**Consequences**\\nBy the moment the container can be used just for deploys, development support can be added sharing a Volume in the Dockerfile. Instructions to modify the Dockerfile can be found [here](https:\/\/docs.docker.com\/engine\/reference\/builder\/).\\n* [Dockerfile](Dockerfile)\\n* [Docker Compose](https:\/\/docs.docker.com\/compose\/)\\n* [Docker Compose.yml](Docker Compose.yml)\\n* [Dockerfile.lock](dockerfile.lock)\\n* [Docker Compose.yml.lock](docker-compose.yml.lock)\\n* [Docker-compose.yml](Dockerfile.lock)\\n* [Dockerfile.lock](docker-compose.yml.lock)\\n* [Dockerfile](docker-compose.yml)\\n* [Dockerfile.lock](docker-compose.yml.lock)\\n* [Dockerfile](docker-compose.yml)\\n* [Dockerfile.lock](docker-compose.yml.lock)\\n* [Docker"}
{"Context":"## Context\\nWhen designing APIs to handle metadata about datasets, editions and versions\\nno particular limitations were placed on which order these documents should be\\nedited in.\\nIn the existing publishing workflow, a single item can be added to only one\\ncollection at a time. As the APIs have separated formerly joined concepts, it\\nis necessary to understand whether datasets can be edited or published in\\nisolation from versions.\\n","Decision":"## Decision\nAdding a version to a collection does not prevent the dataset being edited in a different collection\\n","Prediction":"## Decision\nA single item can be added to only one collection at a time. This means that\\ndatasets and versions can be edited in isolation.\\n### Use of \"collection\" and \"collection-item\" in the API\\nThe API provides a way to create a collection and add items to it. This\\ncollection can be edited or published. These operations are not\\nseparated by a \"collection-item\", because the API does not care about\\nwhich item is being edited or published.\\n### Use of \"collection-item\" in the UI\\nThe API provides a way to create a collection and add items to it. This\\ncollection can be edited or published. These operations are not\\nseparated by a \"collection-item\", because the API does not care about\\nwhich item is being edited or published.\\n#### Collection\\nAn item in the collection can be edited or published. It is possible to\\nedit or publish a single item.\\n#### Collection-item\\nAn item in the collection can be edited or published. It is possible to\\nedit or publish a single item.\\n#### Collection-item in the API\\nThe API provides a way to create a collection and add items to it. This\\ncollection can be edited or published. These operations are not\\n"}
{"Context":"## Context\\nA list of destinations should be reorderable, not fixed\\n","Decision":"## Decision\nA trip is made up of a list of destinations. This list should be able to be reordered, on the main site or the mobile version of the site. Draggable would be the best, but a button for moving an extry up and down will also work.\\n","Prediction":"## Decision\nThe list of destinations will be re-ordered\\nA list of destinations will be fixed\\nA list of destinations will be re-ordered\\nEach list of destinations will be re-ordered\\nEach list of destinations will be re-ordered\\nEach list of destinations will be re-ordered\\nEach list of destinations will be re-ordered\\nEach list of destinations will be re-ordered\\nEach list of destinations will be re-ordered\\nEach list of destinations will be re-ordered\\nEach list of destinations will be re-ordered\\nEach list of destinations will be re-ordered\\nEach list of destinations will be re-ordered\\nEach list of destinations will be re-ordered\\nEach list of destinations will be re-ordered\\nEach list of destinations will be re-ordered\\nEach list of destinations will be re-ordered\\nEach list of destinations will be re-ordered\\nEach list of destinations will be re-ordered\\nEach list of destinations will be re-ordered\\nEach list of destinations will be re-ordered\\nEach list of destinations will be re-ordered\\nEach list of destinations will be re-ordered\\nEach list of destinations will be re-ordered\\nEach list of destinations will be re-ordered\\nEach list of destinations will"}
{"Context":"## Context\\nMetadata for works and collections should be publicly viewable. Only the binary content, i.e. the files, within the\\nwork needs to be restricted based on visibility or other permissions.\\n","Decision":"## Decision\nGrant discovery access to the public on all works and collections by creating the appropriate ACL for each work and\\ncollection. Discovery access stipulates that all metadata is viewable, but that binary content is not downloadable.\\n","Prediction":"## Decision\n* Works and collections will be published to the public web\\n* Work metadata will be published to the public API\\n* Collections will be published to the public web\\n* Collections will be restricted to public endpoints\\n* Work metadata will be restricted to public APIs\\n* Collections will be restricted to public endpoints\\n* Work metadata will be restricted to public endpoints\\n* Collections will be restricted to public endpoints\\n* Collections will be restricted to public endpoints\\n* Collections will be restricted to public endpoints\\n* Collections will be restricted to public endpoints\\n* Collections will be restricted to public endpoints\\n* Collections will be restricted to public endpoints\\n* Collections will be restricted to public endpoints\\n* Collections will be restricted to public endpoints\\n* Collections will be restricted to public endpoints\\n* Collections will be restricted to public endpoints\\n* Collections will be restricted to public endpoints\\n* Collections will be restricted to public endpoints\\n* Collections will be restricted to public endpoints\\n* Collections will be restricted to public endpoints\\n* Collections will be restricted to public endpoints\\n* Collections will be restricted to public endpoints\\n* Collections will be restricted to public endpoints\\n* Collections will be restricted to public endpoints\\n* Collections will be restricted to public endpoints\\n* Collections will be restricted to public endpoints\\n* Collections"}
{"Context":"## Context\\nAn application has to written in some programming languge.\\n","Decision":"## Decision\nThe following languages were considered for this application:\\n* Java\\n* Python\\n* Ruby\\nThe initial inclination was to use either Java or Ruby, based solely on\\ndeveloper experience.\\nAfter discussions with Ben Wallberg, it was decided to use Python because:\\n* The application is fairly simple, an unlikely to reach a complexity where\\nJava's more explicit typing and compile-time verification is needed.\\n* We have a number of developers familiar with Python, providing a broader\\nbase from which to draw support.\\n* While we are content with using Ruby (specifically Ruby on Rails) for website\\nprojects, this is a command-line application. We have fewer developers\\nfamiliar with Ruby.\\n","Prediction":"## Decision\nWe will use Python.\\n### Consequences\\nThis will have a significant impact on the application.\\n### Dependencies\\nWe will use [pip](https:\/\/pip.pypa.io\/en\/stable\/).\\n### Installation\\nWe will use the [virtualenv](https:\/\/virtualenv.pypa.io\/en\/stable\/).\\n### Conventions\\nWe will use the [virtualenvwrapper](https:\/\/virtualenvwrapper.pypa.io\/en\/stable\/).\\n### Environment variables\\nWe will use the [environment variable](https:\/\/docs.python.org\/3.8\/library\/ev.html) to set environment variables.\\n### Concurrency\\nWe will use the [multiprocessing](https:\/\/docs.python.org\/3.8\/library\/multiprocessing.html) module.\\n### Concurrency control\\nWe will use the [lock module](https:\/\/docs.python.org\/3.8\/library\/lock.html).\\n### Process management\\nWe will use the [multiprocessing](https:\/\/docs.python.org\/3.8\/library\/multiprocessing.html) module.\\n### Docker images\\nWe will use the [docker](https:\/\/docs.docker.com\/).\\n### Environment variables\\nWe will use the [environment variable](https:\/\/docs.python.org"}
{"Context":"## Context and problem statement\\nArchivematica users rely on spreadsheets created in a specific way to perform\\ntasks within or after Archivematica. Documentations and examples can be found\\n[here](https:\/\/www.archivematica.org\/en\/docs\/archivematica-1.9\/user-manual\/transfer\/import-metadata\/)\\nbut there is no clear validation as can be performed by a machine. As one\\nexample, the metadata.csv and rights.csv files are \"special\" and are utilized by\\nArchivematica to add metadata or rights metadata into the AIP's METS file.\\nAnother example is the Avalon Media System having a specific Manifest.csv file\\nthat is used to recreate hierarchical information and additional metadata, which\\nis used after a DIP is created from a stored AIP. It would be beneficial if this\\nmanifest could be validated prior to going through the preservation process.\\nBoth of these examples would benefit from a validation service that a user (or\\nautomated system) could access prior to ingest into Archivematica.\\n","Decision":"## Decision\n* More closely integrate two open source projects: Archivematica and Avalon\\n* [Feature request](https:\/\/github.com\/archivematica\/Issues\/issues\/563) for\\nvalidation of Archivematica-specific CSVs within Archivematica\\n* Creation of something for a specific purpose that can later be extended to\\nsuit many use cases\\nChosen option: \"1. API endpoint for pre-ingest CSV validation\", because it is\\nflexible, it lays the groundwork for future work around CSV validation as a step\\nto be taken by Archivematica. It doesn't have the complications of the long-term\\nmaintenance and testing of a GUI component. It can be more easily automated. The\\nsolution allows for custom or institutionally-specific CSV to be used\/added.\\nTechnically, this would live in the Archivematica codebase and be a new endpoint\\nin the [Archivematica API](https:\/\/wiki.archivematica.org\/Archivematica_API)\\nProposed endpoint below:\\n```yaml\\nURL: \/api\/transfer\/validate_csv\\nVerb: POST\\nValidates local CSV with validator service Python script\\nParameters: CSV\\ninput: Path to the CSV\\nvalidator: Name of service CSV should be checked against, i.e. \"avalon\"\\nor \"rights\"\\nResponse: JSON\\nmessage: Approval or non-approval, depending on service output\\n```\\n### Positive consequences\\n* Fulfills requirement of Avalon\/Archivematica integration work.\\n* Relatively small feature with big potential benefits.\\n* Lays foundation for more development in the future.\\n### Negative consequences\\n* Code must be maintained and tested over time.\\n* User would need API expertise to benefit from this feature.\\n* Implementation -- timeouts on very big CSVs?\\n","Prediction":"## Decision\nThe decision is that Archivematica will not provide a validation service.\\nArchivematica will rely on the use of a third-party service for this task. We will\\nmaintain Archivematica's reliance on Archivematica's own validation service.\\nArchivematica will provide a means for Archivematica users to provide feedback on\\nthis service being used by Archivematica.\\nArchivematica will provide a means for Archivematica users to provide feedback on\\nthis service being used by Archivematica.\\nArchivematica will provide a means for Archivematica users to provide feedback on\\nthis service being used by Archivematica.\\nArchivematica will provide a means for Archivematica users to provide feedback on\\nthis service being used by Archivematica.\\nArchivematica will provide a means for Archivematica users to provide feedback on\\nthis service being used by Archivematica.\\nArchivematica will provide a means for Archivematica users to provide feedback on\\nthis service being used by Archivematica.\\nArchivematica will provide a means for Archivematica users to provide feedback"}
{"Context":"## Context\\nGovUK Notify is used as the email provider for Keycloak: [0012 Keycloak Email Provider](0012-keycloak-email-provider.md)\\n","Decision":"## Decision\nNo Full Local Development Stack for GovUK Notify.\\nGovUK Notify does not interact with any other TDR systems, therefore setting up a full local development stack is not necessary.\\n","Prediction":"## Decision\nWe will use the [Keycloak Email Provider](https:\/\/github.com\/keycloak\/keycloak-email-provider) to provide the email provider for Keycloak for the GovUK Notify API integration. This will allow us to integrate with GovUK Notify using the Keycloak Email Provider.\\nWe have chosen this solution because it provides a simple way to provide email services to GovUK Notify and it is well supported by the Keycloak team. There are also some additional benefits to using this solution:\\n* The Keycloak Email Provider is free to use for external services and it is well supported by the Keycloak team\\n* It is possible to manage the email addresses in GovUK Notify via the Keycloak Email Provider\\n* It is possible to use external email services via the Keycloak Email Provider\\n* The Keycloak Email Provider is well supported by the Keycloak team, which means we can get support from the Keycloak team if required\\n* It is possible to use external email services via the Keycloak Email Provider\\n* External email addresses can be managed in the Keycloak Email Provider\\n* It is possible to use external email services via the Keycloak Email Provider\\n* It is possible to use external email services via the Keycloak Email Provider\\nTo use the Keycloak"}
{"Context":"## Context\\nWeb Components encapsulate their shadow dom away from the rest of the DOM, meaning that a general stylesheet cannot style the internals of a Web Component.\\nAs such, we need to find a strategy to maintain component level styles but enable shared styles so that we limit code duplication.\\nWe had previously decided to follow ITCSS for its way to enable scalability as well as limit specificity war, and would like to keep following this approach.\\n","Decision":"## Decision\nConsidering the outcome of the exploration, and seeing that we want to maintain a sense of scalability and separation of concerns as advocated by the ITCSS methodology, we have decided to maintain component specific stylesheets at component directory level, and use Sass `import` within these sheets to add specific classes that many component might use (basically any other layer from ITCSS beyond components).\\nShared styles will be hosted under `src\/shared-styles` folder and organized following the ITCSS nomenclature.\\nAt this moment the `main.scss` file is maintained as it contains information around the available files, and may prove worthy in the future. The value of this needs to be reassessed.\\n","Prediction":"## Decision\nWe will use [this](https:\/\/github.com\/itcss\/itcss#style-shadows) library to provide a shadow dom for Web Components. This library has been used by a number of other projects to provide a way to style components.\\nWe will have a component style for each component that we want to style. We will use the following naming convention:\\n* Component: <component-name> (e.g. <my-component>)\\n* Style: css-file-name. (e.g. my-component.css)\\n* Shadow: shadow-file-name. (e.g. my-component-shadowed.css)\\nWe will then apply the style to the shadow using the shadow style:\\n* <component-name>: shadow(#my-component), override shadow style\\n* <component-name>: shadow(#my-component), override shadow style with color: #fff;\\n* <component-name>: shadow(#my-component), override shadow style with color: #000;\\n* <component-name>: shadow(#my-component), override shadow style with color: #000;\\n* <component-name>: shadow(#my-component), override shadow style with color: #000;\\n* <component-name>: shadow(#my-component), override shadow style with color: #000"}
{"Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","Prediction":"## Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n\\nWe will use [Architecture Guild](http:\/\/architectureguild.com\/) to host our ADRs.\\nWe will use the [RACI](http:\/\/en.wikipedia.org\/wiki\/RACI_definition) matrix to assign responsibility.\\nWe will use the [Jadu](https:\/\/github.com\/jadu\/jadu) library to generate an ADR if it doesn't exist yet.\\nWe will use the [Drools](https:\/\/github.com\/robbiehodge\/drools) library to generate ADRs when they are needed.\\nWe will use the [Jira](https:\/\/www.atlassian.com\/software\/jira) issue tracking system to track ADRs.\\nWe will use the [Rally](https:\/\/rallyapp.com) project management tool to track ADRs.\\nWe will use the [Jenkins](https:\/\/jenkins.io\/) continuous integration server to build and deploy our ADRs.\\nWe will use the [Google Cloud Build](https:\/\/cloud.google.com\/build) continuous deployment"}
{"Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"## Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","Prediction":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\nWe will use [JIRA](https:\/\/issues.apache.org\/jira\/browse\/REL-7239) for version control and tracking\\nWe will use [Pivotal Tracker](https:\/\/tracker.pivotal.io) for issue tracking\\nWe will use [Bitbucket](https:\/\/www.atlassian.com\/software\/bitbucket) for version control and source control\\nWe will use [CircleCI](https:\/\/circleci.com) for CI\/CD\\nWe will use [Buildkite](https:\/\/buildkite.com) for continuous integration and deployment\\nWe will use [Google Cloud Build](https:\/\/cloud.google.com\/build) for CI\/CD\\nWe will use [Pivotal Tracker](https:\/\/tracker.pivotal.io) for issue tracking\\nWe will use [CircleCI](https:\/\/circleci.com) for CI\/CD\\nWe will use [Google Cloud Source](https:\/\/cloud.google.com\/source) for source control\\nWe will use [GitHub](https:\/\/github.com) for version control and source control\\nWe will"}
{"Context":"## Context\\nJust trying to implement bookmarks and I have come across something interesting.\\nWe have a ui element that looks like this:\\n```html\\n<a href=\"javascript:application.bookmarks.add('gbj1jf')\" class=\"bookmark\">bookmark<\/a>\\n```\\nWhich means \"on click, please bookmark the news item with id 'gbj1jf'\".\\nThe bookmarking use case looks like this:\\n```js\\nbookmark = new Bookmark('id-a', 'http:\/\/abc', 'src-rnz');\\nawait application.bookmarks.add(bookmark);\\n```\\nIt is expecting more information for bookmarks so that it can be used later without any further lookups.\\nWe don't wish to have to look it up, especially since it may no longer exist in our sources -- and we don't want to store every single\\narticle just for this purpose.\\nWe don't really want to bake all the information into the link, either, that seems wrong for some reason.\\nAnd we don't really want to add arguments to `application.bookmarks.add`, that also feels wrong.\\n","Decision":"## Decision\nIntroduce some idea of state in to `Application` so that it can handle notifications like this.\\nIt seems like simplifying UI elements is a good idea.\\nAnother option is giving this responsibility to the view element. But then how do we let it know?\\n","Prediction":"## Decision\nWe will use the `bookmark` element instead of the `bookmark` method, since it is more familiar and less confusing.\\nWe will add a `onClick` event to `Bookmark` that calls a function that will be called in the template when a bookmark is added.\\nWe will add a `onRemove` event to `Bookmark`, that will be called when the bookmark is removed.\\nWe will use the `onClick` event to pass a function to `Bookmark` that is called when the bookmark is clicked.\\nWe will use the `onRemove` event to pass a function to `Bookmark` that is called when the bookmark is removed.\\nWe will add a `onDelete` event to `Bookmark` that will be called when the bookmark is deleted.\\nWe will add a `onRemove` event to `Bookmark`, that will be called when the bookmark is removed.\\nWe will use the `onClick` event to pass a function to `Bookmark` that is called when a bookmark is clicked.\\nWe will add a `onRemove` event to `Bookmark`, that will be called when a bookmark is removed.\\nWe will use the `onClick` event to pass a function to `Bookmark` that is"}
{"Context":"## Context\\n<!--- What is the issue that we're seeing that is motivating this decision or change? -->\\nOne of the benefits of writing our infrastructure in a fully fledged programming language is the ability to write reusable components which can be tested. This library defines those components, both in the form of constructs and patterns (see [001-constructs-and-patterns](.\/001-constructs-and-patterns.md) for more details).\\nThere are two main strategies that can be used to unit test these components.\\nSnapshot tests synthesise a given stack and compare the output to a previous version that has been verified by the user and checked into the repository. If there are any changes, the test fails and the user is displayed the diff to either fix or update the stored snapshot.\\n[+] Quick and easy to write\\n[+] Pick up any changes to the output cloudformation (particularly useful for unintended side effects)\\n[-] A change to the component may cause many unrelated tests to also fail\\n[-] When testing different permutations of a component, a number of snapshots will be created, each of which contains a certain amount of information which is irrelevant to the particular test. This adds some extra effort to understand what is relevant to the test which you're looking at\\n[-] Snapshots are easy to update which, especially when multiple are affected by a change, makes it easy to accidentally update them incorrectly. Further to this, as the snapshot (which is essentially the assertion) is stored in a different file, it's not immediately obvious if the assertions are valid\\nDirect Assertions use the different assertions provided by the test framework to test specific functionality. For example, asserting that a property exists or that an array contains a particular value.\\n[+] Each test only contains the relevant assertions for the test, making it easier to understand the consequence of settings certain props\\n[+] Changes will only fail tests that cover that particular area\\n[-] More complex and time consuming to write\\n","Decision":"## Decision\n<!-- What is the change that we're proposing and\/or doing? -->\\nUse direct assertions for constructs and snapshots for patterns (and stacks)\\n_This decision is a recommendation for the general approach. There may be some cases where using a different approach is more applicable for a given test._\\n","Prediction":"## Decision\nUse Direct Assertions to test the functions of the library.\\nUse Snapshot tests to test the output of the library\\nUse [Snapshot Tests](https:\/\/www.mattjames.net\/2017\/04\/09\/snapshot-tests\/) to test the output of the library\\nUse [Direct Assertions](https:\/\/www.mattjames.net\/2017\/04\/09\/snapshot-assertions\/) to test the output of the library\\nUse [Direct Assertions with Snapshots](https:\/\/www.mattjames.net\/2017\/04\/09\/snapshot-assertions-with-snapshots\/) to test the output of the library\\nUse [Snapshots](https:\/\/www.mattjames.net\/2017\/04\/09\/snapshots\/) to test the output of the library\\nUse Direct Assertions to test the output of the library\\nUse [Snapshots](https:\/\/www.mattjames.net\/2017\/04\/09\/snapshots\/) to test the output of the library\\nUse Snapshots to test the output of the library\\nUse [Snapshots with Direct Assertions](https:\/\/www.mattjames.net\/2017\/04\/09\/snapshots-with-direct-assertions\/) to test the output of"}
{"Context":"## Context\\nKubernetes and Spring Cloud Netflix have both solutions for the microservices\\narchitecture, sometimes they can be combined, but some times they conflict.\\nThis has been my case, as I was using some services with netflix and they didn't\\nintegrate with kubernetes.  In fact, all the examples, docs and recomendations I\\nhave found just recommends to eliminate the netflix dependencies.\\nBelow there is a table showing the microservices concerns and how Spring Cloud\\nand Kubernetes address them:\\n![](\/doc\/assets\/microservices_concerns.png)\\nFeatures in my code affected with this issue:\\n* Service Discovery: netflix eureka vs kubernetes naming\\n* Config Service: spring config vs kubernetes ConfigMaps and Secrets\\n* API Gateway: zuul vs ingress\\n### References\\n* [Deploying Microservices: Spring Cloud vs. Kubernetes](https:\/\/dzone.com\/articles\/deploying-microservices-spring-cloud-vs-kubernetes)\\n","Decision":"## Decision\nMy original code was valid for autonomous microservices (deployed in docker), but\\nin kubernetes are not working.  I'm going to refactor them in order to make them\\nwork with the target of the test.\\n","Prediction":"## Decision\nI will use Spring Cloud to deploy my microservices.\\nI will use Kubernetes to manage them.\\nI will use Zuul to hide the external APIs from the public facing applications and to\\nprovide a service discovery mechanism.\\nI will use the Zuul API Gateway to expose the APIs to the public facing applications.\\nI will use the Zuul Ingress Controller to expose the APIs to the public facing applications.\\nI will use the Zuul Service Discovery to expose the services to the public facing applications.\\nI will use the Zuul Config Service to expose the ConfigMaps and Secrets to the public facing applications.\\nI will use the Zuul Service Discovery to expose the services to the public facing applications.\\nI will use the Zuul API Gateway to expose the services to the public facing applications.\\nI will use the Zuul Ingress Controller to expose the services to the public facing applications.\\nI will use the Zuul Service Discovery to expose the services to the public facing applications.\\nI will use the Zuul Config Service to expose the services to the public facing applications.\\nI will use the Zuul Service Discovery to expose the services to the public facing applications.\\nI will use the Zuul API Gateway to expose the services"}
{"Context":"## Context\\nCurrently, creating Dask clusters is not ideal:\\n* The service has create-pod permission which is unwise\\n* Different projects can currently use each other's clusters, which is poor resource management and could allow algorithm inspection from the Desk scheduler plan\\n* There is sometimes a requirement to give Dask clusters storage access, to share data or Conda environments.  This is not self-service, and potentially exposes project data since the Dask clusters are not secured by project.\\n* Dask Gateway was investigated, but does not fit in our authentication model (since it does not use OIDC), and it still provides no straight-forward route to storage access.\\n","Decision":"## Decision\nAfter prototyping a working solution, we have decided to handle Dask cluster creation within the infrastructure-api service.\\nThis will allow us to securely mount volumes and utilise user-defined Conda environments.\\n","Prediction":"## Decision\n- Dask clusters will not be provisioned into a Dask Gateway\\n- Dask clusters will not be provisioned into a Conda environment\\n- Dask clusters will not be provisioned into a Kubernetes cluster\\n- Dask clusters will not be provisioned into a Mesos cluster\\n- Dask clusters will not be provisioned into a Mesos cluster\\n- Dask clusters will not be provisioned into a Kubernetes cluster\\n- Dask clusters will not be provisioned into a Kubernetes cluster\\n- Dask clusters will not be provisioned into a Kubernetes cluster\\n- Dask clusters will not be provisioned into a Kubernetes cluster\\n- Dask clusters will not be provisioned into a Kubernetes cluster\\n- Dask clusters will not be provisioned into a Kubernetes cluster\\n- Dask clusters will not be provisioned into a Kubernetes cluster\\n- Dask clusters will not be provisioned into a Kubernetes cluster\\n- Dask clusters will not be provisioned into a Kubernetes cluster\\n- Dask clusters will not be provisioned into a Kubernetes cluster\\n- Dask clusters will not be provisioned into a Kubernetes cluster\\n- Dask clusters will not be provisioned into a Kubernetes cluster\\n- Dask clusters will not be provisioned into a"}
{"Context":"## Context\\nThis ADR seeks to define the EdgeX direction on using encryption\\nto secure \"in-cluster\" EdgeX communications, that is,\\ninternal microservice-to-microservice communication.\\nThis ADR will seek to clarify the EdgeX direction\\nin several aspects with regard to:\\n- EdgeX services communicating within a single host\\n- EdgeX services communicating across multiple hosts\\n- Using encryption for confidentiality or integrity in communication\\n- Using encryption for authentication between microservices\\nThis ADR will be used to triage EdgeX feature requests in this space.\\n","Decision":"## Decision\nAt this time, EdgeX is primarily a single-node IoT application framework.\\nShould this position change, this ADR should be revisited.\\nBased on the single-node assumption:\\n- TLS will not be used for confidentiality and integrity of internal on-host microservice communication.\\n- TLS will be avoided as an authentication mechanism of peer microservices.\\n- Integrity and confidentiality of microservice communcations crossing host boundaries is required to secure EdgeX, but are an EdgeX customer responsibility.\\n- EdgeX customers are welcome to add extra security to their own EdgeX deployments.\\n","Prediction":"## Decision\nWe will follow the following approach for EdgeX in this space:\\n- Use the `evp` community project to implement edge protocols in\\ncorporate microservices. The `evp` project is a mature, well\\ndocumented and maintained project. The `evp` project will be\\nsupported by the `evp` core team and the core team of the\\nEdgeX project.\\n- The `evp` project will be used as the basis for all\\ntriage of the EdgeX project.\\n- The `evp` project will be used to implement the\\n`evp` protocol.\\n- The `evp` project will be used to implement the\\n`evp` API.\\n- The `evp` project will be used to implement the\\n`evp` API.\\n- The `evp` project will be used to implement the\\n`evp` API.\\n- The `evp` project will be used to implement the\\n`evp` API.\\n- The `evp` project will be used to implement the\\n`evp` API.\\n- The `evp` project will be used to implement the\\n`evp` API"}
{"Context":"## Context\\nJava projects often contain a large amount of boilerplate code, e.g., defining data\/value classes, builders, etc. All\\nsuch code follows a certain pattern and needs testing -- and writing both of these can be error prone. A library\\nthat enables cutting down boilerplate code, and which generates tested code would be beneficial to the project.\\n","Decision":"## Decision\nWe will use the [Lombok](https:\/\/projectlombok.org\/) library to reduce the amount of boilerplate code we need to write.\\n","Prediction":"## Decision\nWe will use [Jasmine](https:\/\/jasmine.github.io\/) to help test code. Jasmine is a popular tool for testing\\nand is widely used in the [Angular ecosystem](https:\/\/angular.io\/docs\/ts\/latest\/cookbook\/testing.html#jasmine).\\nWe will use [NUnit](https:\/\/nunit.io\/), which is a unit test framework used in the [Angular ecosystem](https:\/\/angular.io\/docs\/ts\/latest\/cookbook\/testing.html#nunit).\\nWe will use [Cypress](https:\/\/cypress.io\/) to test components. Cypress is a Cypress testing framework\\nthat enables us to write Cypress tests that are easy to read and have a small footprint.\\nWe will use the [test-coverage-report](https:\/\/github.com\/jasemine\/test-coverage-report) tool to generate\\ncoverage reports.\\nWe will use [Prettier](https:\/\/prettier.io\/) to generate code formatting reports.\\nWe will use [ESLint](https:\/\/eslint.org\/) to help us write better code.\\nWe will use [JSHint](https:\/\/www.jshint.com\/) to help catch common bugs.\\nWe will use [Stylus](https:\/\/stylus-lang.org\/) to help"}
{"Context":"## Context\\nWhen identifying duplicates in the AE dataset a one of the columns to test\\nfor equality between rows is ARRIVALDATE. If this is identical, a difference\\nin ARRIVALTIME of <1hr between rows is then used as an indicator of a\\nduplicate row. This means when there are two rows with arrival times between\\n2330 and 0030, these will not be identified as duplicates as these rows have\\nnot passed the test for equality on ARRIVALDATE.\\n","Decision":"## Decision\nWe anticipate a very low number of rows matching this issue and have chosen to\\nnot further complicate the duplication identification process.\\n","Prediction":"## Decision\nWe have implemented a new column called ARRIVALTIME in the AE dataset which will\\nbe used to test for a difference in arrival times between rows.\\nThis will be tested by comparing the ARRIVALDATE in the AE dataset to the\\nARRIVALTIME in the AE1 dataset. If there is a difference in arrival time between\\nthe two rows, this will be considered a difference in arrival time and a\\nduplicate will be identified.\\nIf there is no difference in arrival time between the two rows, no\\nduplicates will be identified.\\nIf there is a difference in ARRIVALDATE of <1hr between the two rows, the\\ntwo rows will be considered different.\\nIf there is a difference in ARRIVALTIME of <2hrs between the two rows, the\\ntwo rows will be considered different.\\nIf there is a difference in ARRIVALDATE of <3hrs between the two rows, the\\ntwo rows will be considered different.\\nIf there is a difference in ARRIVALTIME of <4hrs between the two rows, the\\ntwo rows will be considered different.\\nIf there is a difference in ARRIVALDATE of <5hrs between the two rows, the\\ntwo rows will be considered different.\\nIf there"}
{"Context":"### Context\\nThe app needs tests. This is a must. Although test driven development is best, the app added tests after development began to test logic and encourage future changes do not break functionality. It would be best to test all aspects of the MVVM architecture, but view models are where the majority of the app logic lives.\\n### Decision\\nSeveral testing frameworks exist for .NET apps, with the main three being xUnit, NUnit and MSTest. The app uses xUnit to test view models, but any of the other two frameworks would've also been suitable. I chose xUnit because the testing syntax looked cleaner, the framework was a bit newer, and I found some nice tutorials that recommended xUnit above the other two. Perhaps most importantly, I wanted to get started with testing instead of spending too much time debating the merits of which one to chose.\\nPerhaps just as important as which testing framework to use is which mocking package to use when faking API or other function calls in my tests. For that, Moq seems to be the clear winner. It's easy to find documentation online about using Moq with each testing framework.\\n### Status\\nAccepted\\n### Consequences\\nGetting started with testing is an undertaking, and it is very easy to let tests fall by the wayside when the touch table has a deadline and new features need to be added quickly. However, many xUnit tutorials seem incomplete and only test the view models. I'm curious if this is a sufficient amount of test coverage.\\n_In Retrospect:_ Unfortunately, I added tests later in the development process, and that caused some unnecessary delay while I was restructuring the architecture. This could have been avoided if I was a bit more familiar with how [Moq](https:\/\/www.nuget.org\/packages\/moq\/) worked from the outset of building the app. I also had to explore using [Unity](https:\/\/www.nuget.org\/packages\/Unity\/) to make sure dependency injection was in place for my unit tests. For more information in using Unity, refer to the `ContainerHelper` class in the `Lib` folder of the app.\\n","Decision":"## Decision\nSeveral testing frameworks exist for .NET apps, with the main three being xUnit, NUnit and MSTest. The app uses xUnit to test view models, but any of the other two frameworks would've also been suitable. I chose xUnit because the testing syntax looked cleaner, the framework was a bit newer, and I found some nice tutorials that recommended xUnit above the other two. Perhaps most importantly, I wanted to get started with testing instead of spending too much time debating the merits of which one to chose.\\nPerhaps just as important as which testing framework to use is which mocking package to use when faking API or other function calls in my tests. For that, Moq seems to be the clear winner. It's easy to find documentation online about using Moq with each testing framework.\\n### Status\\nAccepted\\n### Consequences\\nGetting started with testing is an undertaking, and it is very easy to let tests fall by the wayside when the touch table has a deadline and new features need to be added quickly. However, many xUnit tutorials seem incomplete and only test the view models. I'm curious if this is a sufficient amount of test coverage.\\n_In Retrospect:_ Unfortunately, I added tests later in the development process, and that caused some unnecessary delay while I was restructuring the architecture. This could have been avoided if I was a bit more familiar with how [Moq](https:\/\/www.nuget.org\/packages\/moq\/) worked from the outset of building the app. I also had to explore using [Unity](https:\/\/www.nuget.org\/packages\/Unity\/) to make sure dependency injection was in place for my unit tests. For more information in using Unity, refer to the `ContainerHelper` class in the `Lib` folder of the app.\\n","Prediction":"## Decision\nSeveral testing frameworks exist for .NET apps, with the main three being xUnit, NUnit and MSTest. The app uses xUnit to test view models, but any of the other two frameworks would've also been suitable. I chose xUnit because the testing syntax looked cleaner, the framework was a bit newer, and I found some nice tutorials that recommended xUnit above the other two. Perhaps most importantly, I wanted to get started with testing instead of spending too much time debating the merits of which one to chose.\\nPerhaps just as important as which testing framework to use is which mocking package to use when faking API or other function calls in my tests. For that, Moq seems to be the clear winner. It's easy to find documentation online about using Moq with each testing framework.\\n### Status\\nAccepted\\n### Consequences\\nGetting started with testing is an undertaking, and it is very easy to let tests fall by the wayside when the touch table has a deadline and new features need to be added quickly. However, many xUnit tutorials seem incomplete and only test the view models. I'm curious if this is a sufficient amount of test coverage.\\n_In Retrospect:_ Unfortunately, I added tests later in the development process,"}
{"Context":"## Context\\nWe want to derive data about Clojure projects that can be used to render API documentation\\nas well as plain text documentation (e.g. tutorials).\\nCodox is a popular tool to create this kind of documentation as HTML files.\\n","Decision":"## Decision\nSince Codox renders to HTML instead of some more well defined data format it will be hard to\\nturn Codox' output into other formats. Due to this problem building on top of Codox is not\\na viable path forward.\\n","Prediction":"## Decision\nWe will use Codox to generate API documentation and plain text documentation. This will be done\\nthrough the ClojureDoc API.\\nWe will use the [clojure-doc](https:\/\/clojure-doc.org\/) API to generate API documentation.\\nWe will use the [clojure-doc-html](https:\/\/clojure-doc.org\/clojure-doc-html\/) API to generate plain text\\ndocumentation.\\nThese tools will be provided by Clojuriansphere.\\nWe will use the [clojure-doc-html](https:\/\/clojurist.org\/clojure-doc\/html\/) API for generating HTML files.\\nWe will use the [clojure-doc](https:\/\/clojurist.org\/clojure-doc\/) API for generating plain text files.\\nWe will use the [clojure-doc](https:\/\/clojurist.org\/clojure-doc\/) API for generating API documentation.\\nWe will use the [clojure-doc](https:\/\/clojurist.org\/clojure-doc\/) API for generating plain text documentation.\\nWe will use the [clojure-doc](https:\/\/clojurist.org\/clojure-doc\/) API for generating API documentation.\\nWe will use the [clojure-doc](https:\/\/clojurist.org\/clojure-doc\/) API"}
{"Context":"## Context\\nA claimant\u2019s eligibility is, in part, determined by their qualifications. We\\nwant to be able to validate that a claimant\u2019s qualifications match those of the\\neligibility criteria.\\n","Decision":"## Decision\nTo aid DfE in the process of validating a claimant\u2019s qualifications, we will\\ncollect the claimant\u2019s \u2018Teacher Reference Number\u2019 or TRN.\\nWith the TRN, DfE can use the Database of Qualified Teachers\\n([DQT](https:\/\/teacherservices.education.gov.uk\/SelfService\/Login)) to validate\\na claimant\u2019s qualifications.\\n","Prediction":"## Decision\nWe will use the [qualifications\\_matching\\_algorithm](https:\/\/en.wikipedia.org\/wiki\/Qualifications_matching_algorithm) to check the qualifications of a claimant against the eligibility criteria.\\nWe will also use the [qualifications\\_matching\\_algorithm\\_in\\_practice](https:\/\/github.com\/QualificationsMatchingAlgorithm\/QualificationsMatchingAlgorithm\/blob\/master\/README.md) to check the qualifications of a claimant against the eligibility criteria.\\nWe will use the `qualifications\\_matching\\_algorithm` library to check the\\nqualifications of a claimant against the eligibility criteria.\\nWe will use the `qualifications\\_matching\\_algorithm` library to check the\\nqualifications of a claimant against the eligibility criteria.\\nWe will use the `qualifications\\_matching\\_algorithm` library to check the\\nqualifications of a claimant against the eligibility criteria.\\nWe will use the `qualifications\\_matching\\_algorithm` library to check the\\nqualifications of a claimant against the eligibility criteria.\\nWe will use the `qualifications\\_matching\\_algorithm` library to check the\\nqualifications of a claimant against the eligibility criteria.\\nWe will use the `qualifications\\_matching\\_algorithm` library to check the\\nqualifications"}
{"Context":"## Context\\nIt is important to ensure for each team member the same environment configuration in which the application will operate.\\n","Decision":"## Decision\nDocker\\n","Prediction":"## Decision\nWe will use the [AWS CloudFormation stack](https:\/\/docs.aws.amazon.com\/lambda\/latest\/dg\/stack-creation.html) to create and manage our application environment.\\nWe will use the [AWS CodeBuild pipeline](https:\/\/docs.aws.amazon.com\/codebuild\/latest\/userguide\/codebuild-pipeline.html) to build and test our application and deploy it on the Elastica Cloud.\\nWe will use the [AWS CodePipeline](https:\/\/docs.aws.amazon.com\/codepipeline\/latest\/userguide\/codepipeline.html) to automatically create a new environment and deploy our application to it.\\nWe will use the [AWS CodeBuild Pipeline to build and test a Lambda function](https:\/\/docs.aws.amazon.com\/codebuild\/latest\/userguide\/codebuild-pipeline.html#codebuild-pipeline-for-lambda-functions) using the `lambda:latest` image.\\nWe will use the `AWS CodePipeline` to automatically create an environment and deploy our application to it.\\nWe will use the `AWS CodePipeline` to automatically create a new environment and deploy our application to it.\\nWe will use the `AWS CodePipeline` to automatically create a new environment and deploy our application to it.\\nWe will use the `AWS CodePipeline` to automatically create a new"}
{"Context":"## Context and Problem Statement\\nplay-frontend-govuk relies on a webjar for [alphagov\/govuk-frontend](https:\/\/www.github.com\/alphagov\/govuk-frontend)\\npublished to www.webjars.org. This has a number of drawbacks:\\n* publishing is a manual process\\n* it can take many hours to complete\\n* webjars.org has been down in the past and HMRC has no support arrangements with webjars.org\\nThe main impact of the above is an excessive lead time for making improvements in the\\nunderlying govuk-frontend library available in production via play-frontend-govuk.\\nPreviously we considered self-publishing this WebJar, as we did for the hmrc-frontend WebJar (see related [ADR]((..\/adr\/0009-self-publish-webjar.md)\\n)). However, this is complicated by the fact that we do not own govuk-frontend and\\nself-publishing would involve additional engineering and ongoing maintenance.\\nSince v0.42.0 of [play-frontend-hmrc](https:\/\/github.com\/hmrc\/play-frontend-hmrc\/releases\/tag\/v0.42.0), we have recommended teams\\nuse the minified CSS and JS bundles provided as part of the now self-published `hmrc-frontend` webjar via `hmrcHead` and `hmrcScripts`.\\nThese bundles include the CSS and JS from both govuk-frontend and hmrc-frontend. For teams using this approach, the only\\nassets still being retrieved from the govuk-frontend webjar are the handful of icons referenced\\nin `govukTemplate` and `govukHeader`. Architecturally, this split of assets between the two libraries is awkward and a potential source\\nof confusion for future users and maintainers, addressing it would reduce complexity and ease maintenance.\\nBearing the above in mind, should we remove the hard dependency on the govuk-frontend webjar by:\\n* inlining the images into this repository, for teams directly using `govukTemplate` `govukLayout` or\\n`govukHeader`\\n* provide a mechanism to override the `assetsPath` parameter in\\n`govukTemplate` and `govukHeader` so that `play-frontend-hmrc` can supply its own images and\\n* remove the govuk-frontend webjar dependency now or at some point in the future?\\n## Decision Drivers\\n* The need to make improvements and upgrades to govuk-frontend\\navailable in play-frontend-govuk quickly\\n* The increasing user base of play-frontend-govuk, and accelerating demand for new features and\\nimprovements.\\n* The desire to reduce boilerplate in consuming services.\\n* The high number of services still referencing `lib\/govuk-frontend\/govuk\/all.js`\\n* The fact that the images have not changed for a long time (since September 2018)\\n* The need to minimise the impact of breaking changes on service teams.\\n* The hardship, frustration and toil the current manual publishing process is causing the team.\\n","Decision":"## Decision\n* The need to make improvements and upgrades to govuk-frontend\\navailable in play-frontend-govuk quickly\\n* The increasing user base of play-frontend-govuk, and accelerating demand for new features and\\nimprovements.\\n* The desire to reduce boilerplate in consuming services.\\n* The high number of services still referencing `lib\/govuk-frontend\/govuk\/all.js`\\n* The fact that the images have not changed for a long time (since September 2018)\\n* The need to minimise the impact of breaking changes on service teams.\\n* The hardship, frustration and toil the current manual publishing process is causing the team.\\nChosen option: Option 2, because doing so will (a) put is in a better position to eliminate the use of the webjar eventually,\\n(b) allow us to make changes to play-frontend-hmrc to eliminate the `\/govuk-frontend` route and (c) not introduce any\\nbreaking changes.\\n","Prediction":"## Decision\n* The need to make improvements and upgrades to govuk-frontend available in play-frontend-govuk quickly\\n* The increasing user base of play-frontend-govuk, and accelerating demand for new features and\\nimprovements.\\n* The desire to reduce boilerplate in consuming services.\\n* The high number of services still referencing `lib\/govuk-frontend\/govuk\/all.js`\\n* The Image link in `govukTemplate` is no longer valid, and no longer points to the govuk-frontend webjar\\n* The `govukHead` and `govukTemplate` assets have been removed from this repository\\n* The `govukScripts` asset has been removed from this repository\\n* `govukHead` and `govukTemplate` assets have been removed from this repository\\n* The `govukScripts` asset has been removed from this repository\\n* The `govukHead` and `govukTemplate` assets have been removed from this repository\\n* The `govukScripts` asset has been removed from this repository\\n* The `govukHead` and `govukTemplate` assets have been removed from this repository\\n* The `govukHead` and `govukTemplate` assets"}
{"Context":"## Context\\nAdding field info and carrying out array flattening requires access to DataTypeInfo for primary and linked searches.\\nThe current implementation uses instances of JsonDataTypeProvider which read multiple types from a single file. This would mean lots of duplication between different searches.\\nEach search needs to expose the correct data type, as it will provide a single data type now, albeit with multiple targets.\\n","Decision":"## Decision\nThe situation can be greatly simplified, with the following changes:\\n* Search provides a single DataTypeInfo which is passed on construction\\n* DataTypeInfo has methods for building from JSON or a JSON resource\\n* DataTypeInfoRegistry and implementing classes become unnecessary\\n* SearchRegistry provides access to list of Searches, and hence DataTypes, for use in result decoration\/array flattening\\n","Prediction":"## Decision\nWe will introduce a new JsonDataTypeProvider which will allow the primary and linked searches to expose a single type.\\nThis will be stored in a single file in the repo that can be used by multiple search types. The type will be a simple string mapping to a specific type.\\nThis will allow the primary search to be configured to use the same type as the linked search and the linked search to use the same type as the primary search.\\nThis will allow the primary and linked searches to be configured to use the same type as the primary and linked searches.\\nThis will allow the primary and linked searches to be configured to use the same type as the primary and linked searches.\\nThe type will be a simple string mapping to a specific type.\\nThis will allow the primary and linked searches to be configured to use the same type as the primary and linked searches.\\nThis will allow the primary and linked searches to be configured to use the same type as the primary and linked searches.\\nThis will allow the primary and linked searches to be configured to use the same type as the primary and linked searches.\\nThis will allow the primary and linked searches to be configured to use the same type as the primary and linked searches.\\nThis will allow the primary and"}
{"Context":"## Context\\nSince we decided to design the simulator as a separated component we didn't thought how it would communicate with the WebApp. So, the simulator is a simple java program that can't do anything to communicate with Nozama since it is a web application and has a different ecosystem (spring framework).\\n","Decision":"## Decision\nAs of now we decided to create a simple webserver to expose simulator to our main WebApp. As it main functionality is to just pass some data when required and send notifications to Nozama's backend when some task is done.\\n","Prediction":"## Decision\nWe decided to move the simulator to a different package called `com.nazarin.simulator` and add a web component to the `com.nazarin.simulator` which will be the web application. This web component will be used by the simulator to communicate with the WebApp.\\nThis way the simulator can be used as a standalone application with no dependency on the WebApp and the WebApp can be used as a dependency to the simulator.\\nThe web component will have a RESTful API which will return the simulator state in JSON and the simulator will have a RESTful API which will return the WebApp state in JSON.\\nThe web component will have a `@RestController` which will return the simulator state in JSON.\\nThe web component will have a `@RestController` which will return the WebApp state in JSON.\\nThe web component will have a `@RestController` which will return the simulator state in JSON.\\nThe web component will have a `@RestController` which will return the WebApp state in JSON.\\nThe web component will have a `@RestController` which will return the simulator state in JSON.\\nThe web component will have a `@RestController` which will return the WebApp state in JSON.\\nThe web"}
{"Context":"## Context\\nWe're building a new publishing architecture and MVP workflow in private beta. We have taken the decision to build this in a new application and therefore we need to decide how we use the Publishing API.\\nOne of the goals of the migration effort from 2016 was to for all publishing applications to be clients of the [Publishing API] and this API would means that these applications would not need local datastores. This is written up in the [GOV.UK Platform Architecture Goals 2017-18+][goals]\\nSince writing these goals we have built the [Specialist Publisher] application using this pattern. The difficulties of building Rails application this way have led us to reconsider this approach.\\nWe are about to build another [Publishing API] client and we should now use the learnings we have to ensure that not using a local datastore is the best decision.\\n### Option 1: No local datastore\\nIn this option we continue with the platform architecture goals.\\n#### Pros\\n+ Single source of truth - all data is in the Publishing API\\n+ Multiple apps can edit the same content\\n+ Single implementation of scheduling\\n+ Single abstraction layer\\n#### Cons\\n- Effectively building an Active Resource layer (object-relational mapper for REST)\\n- Harder to resolve performance problems\\n- Current workflow is opinionated and may not meet needs of content\\n- Testing against an API is hard, it requires a lot of stubbing or running a version of the API as part of the test suite\\n### Option 2: Use a local datastore\\n#### Pros\\n+ It\u2019s standard Rails so quick to build and easy to understand\\n+ Easy to test, testing with a local datastore is very straightforward\\n+ Easy to iterate workflow, changing the API workflow is much harder\\n+ Only final state of content can be put into Publishing API - some formats will need interim data which isn\u2019t needed for publishing\\n+ No new features needed for meta data - notes, and other publisher info\\n#### Cons\\n- Syncing between local and Publishing API\\n- Local data is hard to query by other applications\\n- Combination of local and Publishing API workflows\\n- Hard to feedback to use on asynchronous API updates\\n- Content-Schema may not match Rails validations causing unhelpful error messages\\nIn addition to the above there are a few pragmatic problems which have influenced our decision:\\n- We want to store content prior to it being available as a draft on Publishing API. Doing this with no local datastore would have the complexity of a holding place for this data.\\n- We want to model the full edit history of documents, which Publishing API doesn't support.\\n- We want to support migrating of Whitehall documents with their histories - Moving this data to a new publishing application is arguably simpler than having to resume the Publishing API migration work and completing full history migration.\\n- We want to support features the Publishing API doesn't have such as reviews, scheduling and storing additional user level metadata.\\nAll of these can be done with a commitment to developing the Publishing API but it would much slower, would mean resuming the migration programme and have potential impacts on every client of the Publishing API. It also isn't clear that the Publishing API should have all of this functionality.\\nTo achieve the aims of rapidly building a new publishing application to be ready for beta we're able to do this faster and more effectively by modelling these concepts in a local datastore.\\n","Decision":"## Decision\nWe will use a local datastore which syncs to the Publishing API.\\n","Prediction":"## Decision\nWe will use a local datastore for the Publishing API.\\nWe will have a local datastore for the Publishing API and will use it for the following:\\n+ Publishing API state - the state of the publishing application and its databases\\n+ Publishing API state is stored in the Publishing API\\n+ Publishing API state is available to all applications and can be used to synchronise state between applications\\n+ Publishing API state is available to other publishing applications\\n+ Publishing API state can be used by other publishing applications to synchronise state between publishing applications\\n+ Publishing API state has a unique identifier which is used to identify the state of the publishing application\\n+ Publishing API state is available to all publishing applications\\n+ Publishing API state has a unique identifier which is used to identify the state of the publishing application\\n+ Publishing API state is available to all publishing applications\\n+ Publishing API state is available to all publishing applications\\n+ Publishing API state is available to all publishing applications\\n+ Publishing API state is available to all publishing applications\\n+ Publishing API state is available to all publishing applications\\n+ Publishing API state is available to all publishing applications\\n+ Publishing API state is available to all publishing applications\\n+ Publishing API state is available to all publishing applications\\n+ Publishing API state is available to"}
{"Context":"## Context\\nThe issue motivating this decision, and any context that influences or constrains the decision.\\n","Decision":"## Decision\nThe change that we're proposing or have agreed to implement.\\n","Prediction":"## Decision\nThe change that we're proposing or have agreed to implement.\\n### Positive Consequences\\n* The change that we're proposing or have agreed to implement.\\n### Negative Consequences\\n* We're not sure, we'll know when it's ready.\\n### Alternatives considered\r\n* We're not sure, we'll know when it's ready.\\n* We're not sure, we'll know when it's ready.\\n* We're not sure, we'll know when it's ready.\\n* We're not sure, we'll know when it's ready.\\n### Decision\n* We're not sure, we'll know when it's ready.\\n* We're not sure, we'll know when it's ready.\\n* We're not sure, we'll know when it's ready.\\n* We're not sure, we'll know when it's ready.\\n### Positive Consequences\\n* We're not sure, we'll know when it's ready.\\n* We're not sure, we'll know when it's ready.\\n* We're not sure, we'll know when it's ready.\\n### Negative Consequences\\n* We're not sure, we'll know when it's ready.\\n* We're"}
{"Context":"## Context\\nAs the project is an example of a clean code and best practices of software development, it is necessary to save all architectural decisions in one place.\\n","Decision":"## Decision\nFor all architectural decisions Architecture Decision Log (ADL) is created. All decisions will be recorded as Architecture Decision Records (ADR).\\nEach ADR will be recorded using [Michael Nygard template](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions), which contains following sections: Status, Context, Decision and Consequences.\\n","Prediction":"## Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\nThis is the first document in the [project's Architecture Decision Record](https:\/\/github.com\/3rb\/3rb\/blob\/master\/3rb\/decisions.md) which will be used as a reference for future decisions.\\n### Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\nThis is the first document in the [project's Architecture Decision Record](https:\/\/github.com\/3rb\/3rb\/blob\/master\/3rb\/decisions.md) which will be used as a reference for future decisions.\\n### Repository to hold Architecture Decisions\\nThe Architecture Decision Record for this project is in the [3rb repository](https:\/\/github.com\/3rb\/3rb) under the [3rb\/architecture-decisions](https:\/\/github.com\/3rb\/3rb\/tree\/master\/3rb\/architecture-decisions) directory.\\n### Architecture Decisions\\nFor each Architecture Decision, there"}
{"Context":"**Context**: we write examples for all important functions, typically the ones\\nthat result in one or more RPCs. From time to time we need to deprecate a\\nfunction, maybe because the function has a serious defect that cannot be\\nresolved without changing its API, or because the function represents an\\nexperiment that \"failed\", or because we found a better way to do things.\\n**Decision**: when we deprecate a function we will remove the examples to that\\nfunction. If the function is being replaced, we will reference the replacement\\nin the \"Examples\" paragraph. If the function is being removed, we will simply\\nremove the examples before the function itself. Unit tests and integration tests\\nare preserved. While a function might be deprecated, it is expected to work.\\n**Consequences**: customers will be directed to the improved replacement,\\ncustomers will not be misguided by examples that point to a deprecated approach.\\nDevelopers of `google-cloud-cpp` will have fewer examples to maintain. Customers\\nthat cannot migrate to the new approach will not find examples of the approach\\nthey are currently using, this is mitigated by (a) they already are using the\\nold approach, and (b) they can find the examples in the documentation\\naccompanying old releases.\\n","Decision":"## Decision\nfunction. If the function is being replaced, we will reference the replacement\\nin the \"Examples\" paragraph. If the function is being removed, we will simply\\nremove the examples before the function itself. Unit tests and integration tests\\nare preserved. While a function might be deprecated, it is expected to work.\\n**Consequences**: customers will be directed to the improved replacement,\\ncustomers will not be misguided by examples that point to a deprecated approach.\\nDevelopers of `google-cloud-cpp` will have fewer examples to maintain. Customers\\nthat cannot migrate to the new approach will not find examples of the approach\\nthey are currently using, this is mitigated by (a) they already are using the\\nold approach, and (b) they can find the examples in the documentation\\naccompanying old releases.\\n","Prediction":"## Decision\n**Examples** are deprecated, and replaced by the new function and example\\ncode. This is documented in the `examples` section of the documentation in\\nthe `google-cloud-cpp` repository. The `examples` section is also\\navailable from the documentation's `examples` page.\\nFor example code that was part of the `google-cloud-cpp` library, the\\ncurrent version is available from the `google-cloud-cpp` repository, and\\nthe old version is available from the `google-cloud-cpp` documentation.\\nExamples are not deprecated, and are not removed, because they are\\nstill useful for customers, and because they are not being removed.\\nExamples that are being deprecated, and are being removed, are\\ndescribed in the `examples-deprecation.md` file. The current\\nversion of `examples-deprecation.md` is available from the\\ndocumentation's `examples-deprecation.md` file.\\nExamples are not deprecated, and are not removed, because they are\\nstill useful for customers, and because they are not being removed.\\nExamples that are being deprecated, and are being removed, are\\ndescribed in the `examples-deprecation.md` file. The current\\nversion of `examples-deprecation.md`"}
{"Context":"## Context\\n[ADR002](ADR002-containers.md) decided that we should provide the primitives for building, deploying and running container-based workloads.\\nIn order to meet the needs existing service teams such a primitive must be able to support:\\n* **High Availability** and **Load Balancing**: running multiple instances of applications across multiple physical nodes\\n* **Horizontal Scalability**: ability to increase\/decrease the number of instances of an application to respond to demand\\n* **Declarative Configuration**: ability to describe infrastructure\/deployments as code\\n* **Internal Networking**: ability to restrict traffic to a non internet-facing network\\n* **Service Discovery**: ability to describe relationships between running containers as microservices\\nThere are also some expectations that people may have from experience with other container-based platforms, for instance:\\n* **Self Healing**: features that aid with the restarting and replacing of failed application instances\\n* **Logging**: solutions to how to access, and ship log output from running applications\\nThere are many popular solutions that can meet the needs of this orchestration problem:\\n* **Vendor specific**: Cloud Providers have various solutions to manage various container workloads. AWS ECS\/Fargate is currently in use by a few service teams at the time of writing as could meet the needs.\\n* **CloudFoundry**: It provides a full-stack PaaS-type solution for building and running container workloads. GOV.UK PaaS is based on CloudFoundry.\\n* **Cloud Native** \/ **Kubernetes**: Provides a minimal but extensible API for managing container workloads and large community of compatible projects for common solutions.\\n* **OpenShift**: Builds upon Kubernetes to provide a full-stack PaaS-type solution for building and running container workloads.\\n* **DCOS\/Marathon**: An extra abstraction layer (DCOS) above a container orchestrator layer (Marathon) that could potentially support other types of workloads ie VMs.\\n* **Docker Swarm**: An orchestrator tightly integrated with the Docker container runtime.\\nIn [ADR001](ADR001-support-model.md) we decided against running a \"batteries included\" PaaS-type platform, and to sacrifice the centralised control such a system would bring in exchange for giving more control\/flexibility to service teams.\\nWe did not feel like the additional abstraction layer in DCOS\/Marathon provided any value we needed.\\nWe were unsure that a system based on Swarm would provide the flexibility we want to respond to the evolving needs of service teams.\\nKubernetes has been emerging as an industry standard, with most major cloud providers offering various services around the technology. It offers the basic building blocks for managing container workloads but has a large strong community of compatible projects ([CNCF](https:\/\/landscape.cncf.io\/)) to extend functionality.\\nThe vendor specific solutions such as ECS\/Fargate are very compelling, potentially reducing the maintenance burden of running infrastructure. However as a public body we should try to avoid unnecessary vendor lock-in.\\n","Decision":"## Decision\nWe will build upon Kubernetes as our base container orchestrator and IaaS abstraction layer.\\n","Prediction":"## Decision\nWe will use Kubernetes to provide a platform for container-based workloads.\\nWe will adopt the [Kubernetes](https:\/\/kubernetes.io\/), as the platform for managing containerised workloads.\\nWe will use [Docker Swarm](https:\/\/docs.docker.com\/swarm\/) as the platform for managing containerised workloads.\\nWe will use the [Kubernetes Helm](https:\/\/kubernetes.io\/docs\/concepts\/understanding-helm\/) package management system to manage the Kubernetes namespace.\\nWe will use the [Kubernetes Operator](https:\/\/kubernetes.io\/docs\/concepts\/administration\/operating-systems\/) package to manage the Kubernetes namespace.\\nWe will use the [Kubernetes Service Mesh](https:\/\/kubernetes.io\/docs\/concepts\/services\/managed-service-mesh\/) package to manage the Kubernetes namespace.\\nWe will use the [Kubernetes Service Discovery](https:\/\/kubernetes.io\/docs\/concepts\/administration\/service-discovery\/) package to manage the Kubernetes namespace.\\nWe will use the [Kubernetes Deployment](https:\/\/kubernetes.io\/docs\/concepts\/administration\/deployment\/) package to manage the Kubernetes namespace.\\nWe will use the [Kubernetes Load Balancer](https:\/\/kubernetes.io\/docs\/concepts\/administration\/load-balancing"}
{"Context":"## Context\\nWe have a requirement to store the JSON messages that arrive in the request bodies to the EventController endpoints and this is done to an S3 bucket where they can be retained securely for 28 days. This could have been done by adding a service call in the EventController itself.\\nHowever, the EventController is a spring-managed endpoint and receives the payload already transformed into the required ```Hearing``` instance. If there was a problem parsing the payload, it would never arrive in the endpoint and the opportunity to store would have been lost.\\nAlternatively, a Filter could be implemented which intercepts the request before it arrives at the endpoint. This requires more code and a little more development effort.\\n","Decision":"## Decision\nDecision taken to implement Filter so that no request body is lost.\\n","Prediction":"## Decision\nWe have decided to use a Filter to store the JSON messages. This will be much more efficient and flexible than storing the payload in the endpoint directly. The Filter will be responsible for transforming the payload into the required ```Hearing``` instance. If there was a problem parsing the payload, it will throw an exception and not be able to reach the endpoint.\\nA Filter will be implemented in the EventController and the handler will be implemented in the EventControllerFilter. The handler will be responsible for transforming the payload. The handler will be called from the EventController itself and will be responsible for handling the payload transformation.\\nA Filter will be implemented in the EventController and the handler will be implemented in the EventControllerFilter. The handler will be responsible for transforming the payload. The handler will be called from the EventController itself and will be responsible for handling the payload transformation.\\nA Filter will be implemented in the EventController and the handler will be implemented in the EventControllerFilter. The handler will be responsible for transforming the payload. The handler will be called from the EventController itself and will be responsible for handling the payload transformation.\\nA Filter will be implemented in the EventController and the handler will be implemented in the EventControllerFilter. The handler will be"}
{"Context":"## Context\\nWe want to serve [HSTS\\nheaders](https:\/\/en.wikipedia.org\/wiki\/HTTP_Strict_Transport_Security) for all\\nHTTPS requests to the apps domains, but it will safeguard existing users from\\nbeing MITMed over insecure connections and it will improve the user experience\\nwhen they select a hostname that doesn't have a protocol.\\n(Note that without pre-loading in browsers this won't help first time users,\\nbut that is out of context)\\nWe want to leave open the option of able to override these headers from\\nthe tenant application if they wish.\\nThis feature requires conditionally process and modify the request headers.\\nThere are several possible implementations:\\n1. Implement the logic in the `gorouter` itself: `gorouter` shall process\\nand add the header if required, by:\\n* Supporting the specific HSTS headers, and allowing configure some\\nsort of behaviour and default value.\\n* Allow inject any additional header if they are missing.\\nBut [current `gorouter` implementation](https:\/\/github.com\/cloudfoundry\/gorouter\/commit\/0d475e57b1742c42ba6d98d1ed853edc9f709893)\\ndoes not support any of these features, which require being added.\\n2. Add some intermediate proxy (for example nginx, haproxy) in front of\\nthe go-routers and after the ELB.\\n3. Implement it in a external CDN in front of PaaS origin (PaaS LB entry point):\\nAll the commercial CDN have the capacity to add additionally headers\\nconditionally.\\n4. AWS ELB: They do not support this logic and will not in the short term.\\nIn consequence they cannot be used to solve this problem.\\n","Decision":"## Decision\nWe do not want to add any additional logic in the CDN, as they will\\nbe an optional part of the platform and we will try to keep as simple\\nas possible.\\nWe consider that the optional solution would be implement this logic in\\nthe `gorouter`, but that requires some development effort and a PR being merged\\nupstream.\\nBecause that we will implement, in the short term, the second option: a proxy\\nin front of the `gorouter`.\\n* We will implement [HAproxy](http:\/\/www.haproxy.org\/) in front of the go router.\\n* Ha-proxy is the default LB solution for the official CF distribution.\\n* It is really powerful and has good support.\\n* Enough features to cover our needs.\\n* It will be setup colocated with the `gorouter`, proxying directly to\\nlocalhost.\\n* We will do SSL termination in HAProxy, and plain text to `gorouter`. This\\nis OK as the two services are colocated in the same VM.\\n* We will reuse the code from [official haproxy job from cf-release](https:\/\/github.com\/cloudfoundry\/cf-release\/tree\/master\/jobs\/haproxy),\\nalthough we will have to fork it to add additional settings in the\\nhaproxy configuration.\\nFuture work:\\n* We will implement and propose a PR to add logic in go-router to allow\\ndefine additional headers.\\n","Prediction":"## Decision\nWe will use a [AWS ELB](https:\/\/aws.amazon.com\/elasticloadbalancing\/) to serve the\\nHSTS header to the apps.\\nWe will use the [AWS ELB](https:\/\/aws.amazon.com\/elasticloadbalancing\/) to serve the\\nHSTS header to the apps.\\nWe will use the [AWS ELB](https:\/\/aws.amazon.com\/elasticloadbalancing\/) to serve the\\nHSTS header to the apps.\\nWe will use the [AWS ELB](https:\/\/aws.amazon.com\/elasticloadbalancing\/) to serve the\\nHSTS header to the apps.\\nWe will use the [AWS ELB](https:\/\/aws.amazon.com\/elasticloadbalancing\/) to serve the\\nHSTS header to the apps.\\nWe will use the [AWS ELB](https:\/\/aws.amazon.com\/elasticloadbalancing\/) to serve the\\nHSTS header to the apps.\\nWe will use the [AWS ELB](https:\/\/aws.amazon.com\/elasticloadbalancing\/) to serve the\\nHSTS header to the apps.\\nWe will use the [AWS ELB](https:\/\/aws.amazon.com\/elasticloadbalancing\/) to serve the\\nHSTS header to the"}
{"Context":"## Context\\nWhen the team for the re-write of the Fundraising formed in 2016, we discovered that team members had different approaches to do validation:\\n* Use an established library, like [Symfony Validation](https:\/\/symfony.com\/doc\/current\/validation.html).\\n* Write our own validation logic.\\nThe arguments in favor of writing our own logic were:\\n* We don't want to bind our domain layer to a concrete validation library implementation.\\n* The individual validations - checking for required fields in most cases - are so simple that using an external library would make the validation more complicated.\\n* We don't know the \"maintenance cycles\" of the library, either we need to constantly update or the library is not maintained properly.\\n* Every developer would have to learn the API of the external library.\\nAt the start of the project we did not know where we should put the validation logic:\\n* At the framework\/presentation layer, forcing us to create valid, fully formed domain objects as input for use cases.\\n* At the use case layer, making validation part of the use case.\\n","Decision":"## Decision\nFor each use case we write a validator class that checks the `Request` value object of that use case. The validator class must ensure that the use case can create valid domain objects from the request object. The validator class uses simple `if` checks and no external framework.\\nWe return result data structures from validation classes. The result data structures that have some way of communicating to the framework layer what input caused the validation error. If necessary, one input can have more than one validation error.\\nValidation error names are language-independent unique strings in `snake_case`. When we need to translate those error codes, we put the translations in the file [`validations.js`](https:\/\/github.com\/wmde\/fundraising-frontend-content\/blob\/test\/i18n\/de_DE\/messages\/validations.json) in the [content repository](https:\/\/github.com\/wmde\/fundraising-frontend-content). We don't  map every error to a translation, we can write frontend layer code that summarizes the errors or maps them in a different way.\\n","Prediction":"## Decision\nWe decided to create our own validation logic at the use case layer.\\nWe decided to do this because we wanted to have full control over the validation logic, and we didn't want to rely on a specific library.\\nWe decided to use [Symfony's [Validator](https:\/\/symfony.com\/doc\/current\/validation\/index.html)]() because it was the most widely used validation library in the Symfony community, and we had a lot of experience with it.\\nWe decided to use [Symfony's [Validator's](https:\/\/symfony.com\/doc\/current\/validation\/index.html)]() because it was the most widely used library in the Symfony community, and we had a lot of experience with it.\\nWe decided to use [Symfony's [Validator's][3rd-party-libraries]]() because it was the most widely used library in the Symfony community, and we had a lot of experience with it.\\nWe decided to use [Symfony's [Validator's][3rd-party-libraries][https:\/\/symfony.com\/dependency_injection\/dependency_injection_with_validator\/]]() because it was the most widely used library in the Symfony community, and we had a lot of experience with it.\\nWe decided to use [Symfony's [Validator's][3rd"}
{"Context":"## Context and Problem Statement\\nWe want to have a well defined structure for what to document where.\\n","Decision":"## Decision\nChosen option: Leave it as is.\\n### Clarification for what to document where:\\n#### Docs Repository\\nRationale: If I want wo use mico this documentation should contain all I need to know.\\nExamples:\\n* Design Decisions\\n* Architecture\\n* Component descriptions (mico-core, mico-admin, mico-grapheditor)\\n* API Interfaces (REST Api, etc.)\\n* How To's (How To install mico, How To use mico, etc.)\\n#### Mico Repository\\nRationale: If I want to change something in the code this documentation should contain additional information about code specifics.\\n* Javadoc, Typedoc, etc.\\n* Descriptions for important classes and how to use them\\n* How to write a new REST Endpoint, Unit Test, Plugin, etc.\\n* Links to dependencies (with documentation), relevant documentation in docs repo\\nIf possible use links to the relevant documentation instead of describing the whole thing.\\n","Prediction":"## Decision\nWe will use [Cascading Style Sheets](https:\/\/en.wikipedia.org\/wiki\/Cascading_Style_Sheets)\\nfor the style of our code.\\nWe will use [Markdown](https:\/\/daringfireball.net\/projects\/markdown\/syntax) for formatting our code.\\nWe will use [Markdown::Table](https:\/\/github.com\/greenduck\/markdown-table) for formatting our tables.\\nWe will use [Markdown::List](https:\/\/github.com\/greenduck\/markdown-list) for formatting our lists.\\nWe will use [Markdown::Markdown::Table](https:\/\/github.com\/greenduck\/markdown-table) for formatting our tables.\\nWe will use [Markdown::Markdown::List](https:\/\/github.com\/greenduck\/markdown-list) for formatting our lists.\\nWe will use [Markdown::Markdown::Table](https:\/\/github.com\/greenduck\/markdown-table) for formatting our lists.\\nWe will use [Markdown::List](https:\/\/github.com\/greenduck\/markdown-list) for formatting our lists.\\nWe will use [Markdown::List](https:\/\/github.com\/greenduck\/markdown-list) for formatting our lists.\\nWe will use [Markdown::List]("}
{"Context":"## Context\\nCosmos SDK modules currently use the `KVStore` interface and `Codec` to access their respective state. While\\nthis provides a large degree of freedom to module developers, it is hard to modularize and the UX is\\nmediocre.\\nFirst, each time a module tries to access the state, it has to marshal the value and set or get the\\nvalue and finally unmarshal. Usually this is done by declaring `Keeper.GetXXX` and `Keeper.SetXXX` functions,\\nwhich are repetitive and hard to maintain.\\nSecond, this makes it harder to align with the object capability theorem: the right to access the\\nstate is defined as a `StoreKey`, which gives full access on the entire Merkle tree, so a module cannot\\nsend the access right to a specific key-value pair (or a set of key-value pairs) to another module safely.\\nFinally, because the getter\/setter functions are defined as methods of a module's `Keeper`, the reviewers\\nhave to consider the whole Merkle tree space when they reviewing a function accessing any part of the state.\\nThere is no static way to know which part of the state that the function is accessing (and which is not).\\n","Decision":"## Decision\nWe will define a type named `Value`:\\n```go\\ntype Value struct {\\nm   Mapping\\nkey []byte\\n}\\n```\\nThe `Value` works as a reference for a key-value pair in the state, where `Value.m` defines the key-value\\nspace it will access and `Value.key` defines the exact key for the reference.\\nWe will define a type named `Mapping`:\\n```go\\ntype Mapping struct {\\nstoreKey sdk.StoreKey\\ncdc      *codec.LegacyAmino\\nprefix   []byte\\n}\\n```\\nThe `Mapping` works as a reference for a key-value space in the state, where `Mapping.storeKey` defines\\nthe IAVL (sub-)tree and `Mapping.prefix` defines the optional subspace prefix.\\nWe will define the following core methods for the `Value` type:\\n```go\\n\/\/ Get and unmarshal stored data, noop if not exists, panic if cannot unmarshal\\nfunc (Value) Get(ctx Context, ptr interface{}) {}\\n\/\/ Get and unmarshal stored data, return error if not exists or cannot unmarshal\\nfunc (Value) GetSafe(ctx Context, ptr interface{}) {}\\n\/\/ Get stored data as raw byte slice\\nfunc (Value) GetRaw(ctx Context) []byte {}\\n\/\/ Marshal and set a raw value\\nfunc (Value) Set(ctx Context, o interface{}) {}\\n\/\/ Check if a raw value exists\\nfunc (Value) Exists(ctx Context) bool {}\\n\/\/ Delete a raw value value\\nfunc (Value) Delete(ctx Context) {}\\n```\\nWe will define the following core methods for the `Mapping` type:\\n```go\\n\/\/ Constructs key-value pair reference corresponding to the key argument in the Mapping space\\nfunc (Mapping) Value(key []byte) Value {}\\n\/\/ Get and unmarshal stored data, noop if not exists, panic if cannot unmarshal\\nfunc (Mapping) Get(ctx Context, key []byte, ptr interface{}) {}\\n\/\/ Get and unmarshal stored data, return error if not exists or cannot unmarshal\\nfunc (Mapping) GetSafe(ctx Context, key []byte, ptr interface{})\\n\/\/ Get stored data as raw byte slice\\nfunc (Mapping) GetRaw(ctx Context, key []byte) []byte {}\\n\/\/ Marshal and set a raw value\\nfunc (Mapping) Set(ctx Context, key []byte, o interface{}) {}\\n\/\/ Check if a raw value exists\\nfunc (Mapping) Has(ctx Context, key []byte) bool {}\\n\/\/ Delete a raw value value\\nfunc (Mapping) Delete(ctx Context, key []byte) {}\\n```\\nEach method of the `Mapping` type that is passed the arguments `ctx`, `key`, and `args...` will proxy\\nthe call to `Mapping.Value(key)` with arguments `ctx` and `args...`.\\nIn addition, we will define and provide a common set of types derived from the `Value` type:\\n```go\\ntype Boolean struct { Value }\\ntype Enum struct { Value }\\ntype Integer struct { Value; enc IntEncoding }\\ntype String struct { Value }\\n\/\/ ...\\n```\\nWhere the encoding schemes can be different, `o` arguments in core methods are typed, and `ptr` arguments\\nin core methods are replaced by explicit return types.\\nFinally, we will define a family of types derived from the `Mapping` type:\\n```go\\ntype Indexer struct {\\nm   Mapping\\nenc IntEncoding\\n}\\n```\\nWhere the `key` argument in core method is typed.\\nSome of the properties of the accessor types are:\\n* State access happens only when a function which takes a `Context` as an argument is invoked\\n* Accessor type structs give rights to access the state only that the struct is referring, no other\\n* Marshalling\/Unmarshalling happens implicitly within the core methods\\n","Prediction":"## Decision\nWe will introduce a new interface to access the state of a module.\\nThis interface will be called `StoreKey` and will be used to extract the access rights of a module's\\n`Keeper`.\\nA module will be able to supply its own `StoreKey` implementation to its `Keeper`.\\nThis implementation will be used to extract the access rights of a module's `Keeper`.\\nThis interface will be used to extract the access rights of a module's `Keeper`.\\nA module will be able to supply its own `StoreKey` implementation to its `Keeper`.\\nThis implementation will be used to extract the access rights of a module's `Keeper`.\\nThis interface will be used to extract the access rights of a module's `Keeper`.\\nA module will be able to supply its own `StoreKey` implementation to its `Keeper`.\\nThis implementation will be used to extract the access rights of a module's `Keeper`.\\nThis interface will be used to extract the access rights of a module's `Keeper`.\\nA module will be able to supply its own `StoreKey` implementation to its `Keeper`.\\nThis implementation will be used to extract the access rights of a"}
{"Context":"## Context\\nMany advanced features require additional information to be added to `Type`s - this could include secondary information as part of a `Command` or `State` update, or security context, `Command` source identifier, priority, etc. The type system structure inheritted from Eclipse SmartHome is difficult to extend due to the class hierarchy. A wrapper could be used around the `Command` or `State`, but this would require considerable rework of core functions.\\nThere is no base class for a `Type` - the ESH structure defines `Type`, `Command` and `State` as interfaces and many concrete `Type`s are derived from an `enum` (eg `OnOffType`, `UpDownType` etc) while others may be derived from Java classes (eg `DecimalType`). The absence of a clear class hierarchy within the framework makes it difficult or impossible for the framework to control and extend the `Type` system - one of the fundamental building blocks of the framework.\\nIt is proposed to rewrite the type system to introduce a clear class hierarchy. `Type`, `Command` and `State` remain as interfaces however an `AbstractBaseType` class is introduced from which all concrete `Type`s are derived. `ComplexType` and `PrimitiveType` are not generally used and are to be removed as they add unnecessary complexity. The type system should be implemented to be compatible at the basic level with the Eclipse SmartHome definitions such that backward compatability with existing bindings is not compromised.\\nThe ESH type system derives `DecimalType` and `QuantityType` from `Number` which makes it impossible to extend the `AbstractBaseType`. To resolve this we provide the core `Number` functionality with a new `NumberType` class. This provides the same interface, but can now extend `AbstractBaseType`. This potentially means that any code within the framework that checks `instanceof Number` will now fail and such checks must instead use `instanceof NumberType`.\\n","Decision":"## Decision\nThis change provides a better separation between Java and OpenSmartHouse types, and most importantly allows further enhancements in functionality so is agreed to be implemented.\\n","Prediction":"## Decision\nWe will rewrite the type system to introduce a new class hierarchy and introduce a new `NumberType` class.\\nWe will introduce a new `NumberType` class that is derived from the core `Number` type. `NumberType` will have the same interface as `Number` but will extend the `NumberType` interface. This will allow the ESH framework to extend the type system from the core `Number` type.\\nWe will introduce a new `NumberType` class that is derived from the core `DecimalType` type. This will provide a new concrete `Number` type with all the functionality of the core `Number` type (eg `instanceof DecimalTypeNumberNumberType` and `instanceof NumberTypeNumberType`) however will not include the `instanceof Number` functionality. The `NumberType` and `NumberTypeNumber` classes will be abstract and the concrete `Number` class will be derived from the abstract class. The concrete `Number` class will be extended by the `NumberType` class where required.\\nWe will introduce a new `NumberType` class that is derived from the core `NumberTypeNumber` type. This will provide a new concrete `NumberTypeNumber` type with all the functionality of"}
{"Context":"#### Context\\nSS\ud83d\udc19 fundamentally relies on a third-party provider service to authenticate requests (e.g. OAuth2\\nvia Google). The provider may experience transient faults for any number of reasons - perhaps the\\nprovider is experiencing a transient outage, or our traffic has been temporarily rate limited. We\\nwanted to build SS\ud83d\udc19 to anticipate and handle the presence of faults during communication with\\nauthentication providers, no matter the underlying cause.\\n#### Decision\\nWe implemented a generic `circuit.Breaker` type, which implements the \"Circuit Breaker\" design\\npattern made popular by Michael T. Nygard's book, [\"Release It!\"](\\nhttps:\/\/pragprog.com\/book\/mnee\/release-it). All requests to the provider service are issued through\\na stateful `circuit.Breaker` instance, which tracks the frequency of request failures. When the\\n`Breaker` sees that enough requests are failing, it temporarily suspends all outgoing traffic to the\\nprovider (i.e. \"enters `Open` state). After some time, the `Breaker` transitions to a `HalfOpen`\\nstate, in which a limited number of outbound requests are allowed. If failures persist, then the\\n`Breaker` will once again suspend outbound traffic, re-enter `Open` state, and typically will wait\\nfor a longer interval of time before trying again. If instead the `Breaker` observes that requests\\nare consistently succeeding, then it will resume all outbound traffic (i.e. enter `Closed` state).\\nThe SS\ud83d\udc19 [`docs`](\/docs\/) directory contains a [Circuit Breaker](\\n\/docs\/architecture\/circuit-breaker.md) document with more details. The implementation can be found\\nin [`breaker.go`](\/internal\/auth\/circuit\/breaker.go)\\n#### Consequences\\nSS\ud83d\udc19 now utilizes a robust and generic strategy for handling faults originating from the upstream\\nauthentication provider service.\\n","Decision":"## Decision\nWe implemented a generic `circuit.Breaker` type, which implements the \"Circuit Breaker\" design\\npattern made popular by Michael T. Nygard's book, [\"Release It!\"](\\nhttps:\/\/pragprog.com\/book\/mnee\/release-it). All requests to the provider service are issued through\\na stateful `circuit.Breaker` instance, which tracks the frequency of request failures. When the\\n`Breaker` sees that enough requests are failing, it temporarily suspends all outgoing traffic to the\\nprovider (i.e. \"enters `Open` state). After some time, the `Breaker` transitions to a `HalfOpen`\\nstate, in which a limited number of outbound requests are allowed. If failures persist, then the\\n`Breaker` will once again suspend outbound traffic, re-enter `Open` state, and typically will wait\\nfor a longer interval of time before trying again. If instead the `Breaker` observes that requests\\nare consistently succeeding, then it will resume all outbound traffic (i.e. enter `Closed` state).\\nThe SS\ud83d\udc19 [`docs`](\/docs\/) directory contains a [Circuit Breaker](\\n\/docs\/architecture\/circuit-breaker.md) document with more details. The implementation can be found\\nin [`breaker.go`](\/internal\/auth\/circuit\/breaker.go)\\n#### Consequences\\nSS\ud83d\udc19 now utilizes a robust and generic strategy for handling faults originating from the upstream\\nauthentication provider service.\\n","Prediction":"## Decision\nWe implemented a generic `circuit.Breaker` type, which implements the \"Circuit Breaker\" design\\npattern made popular by Michael T. Nygard's book, [\"Release It!\"](\\nhttps:\/\/pragprog.com\/book\/mnee\/release-it). All requests to the provider service are issued through\\na stateful `circuit.Breaker` instance, which tracks the frequency of request failures. When the\\n`Breaker` sees that enough requests are failing, it temporarily suspends all outgoing traffic to the\\nprovider (i.e. \"enters `Open` state\"). After some time, the `Breaker` transitions to a `HalfOpen`\\nstate, in which a limited number of outbound requests are allowed. If failures persist, then the\\n`Breaker` will once again suspend outbound traffic, re-enter `Open` state, and typically will wait\\nfor a longer interval of time before trying again. If instead the `Breaker` observes that requests\\nare consistently succeeding, then it will resume all outbound traffic (i.e. enter `Closed` state).\\nThe SS\ud83d\udc19 [`docs`](\/docs\/) directory contains a [Circuit Breaker](\\n\/docs\/architecture\/circuit-breaker"}
{"Context":"## Context and Problem Statement\\nWe're building websites and web applications that share a common design system with reusable components. How do we write CSS styles in a way that is performant and safe?\\n## Decision Drivers\\n- Should be performant, with code splitting, caching and minimal runtime overhead.\\n- Needs to have easy access to our design system constants. These should optimally be shared with JS logic.\\n- Should be type-safe to catch issues when refactoring.\\n- Reusable components should be closed, not accepting arbitrary styles\/classes.\\n- We want a pattern for responsive props with atomic layout components.\\n","Decision":"## Decision\n- Should be performant, with code splitting, caching and minimal runtime overhead.\\n- Needs to have easy access to our design system constants. These should optimally be shared with JS logic.\\n- Should be type-safe to catch issues when refactoring.\\n- Reusable components should be closed, not accepting arbitrary styles\/classes.\\n- We want a pattern for responsive props with atomic layout components.\\nChosen option: Treat, because it combines the best of both worlds from CSS-in-JS and CSS modules.\\nWe'll create shared components that have responsive props, but are otherwise closed for modifications. Theme variables are defined in a shared library with TypeScript.\\nExample:\\n```typescript jsx\\n\/\/ Good:\\n<Box padding\"small\" \/>\\n<Box padding={{xs: 'small', md: 'medium'}} \/>\\n<Input large \/>\\n<Text preset=\"heading3\" as=\"p\" \/>\\n\/\/ Bad:\\n<Box className={customLayout} \/>\\n<Input style={{ height: 50, padding: 16 }} \/>\\n<Text className={styles.heading} \/>\\n```\\n### Positive Consequences\\n- Treat is statically extracted at build time, so it has minimal runtime.\\n- Styles load in parallel with JS, also when code splitting.\\n- Styles are written in TypeScript which gives us type safety when referring to shared variables, styles and helpers.\\n- Styles are in special files, separate from markup and components giving us clear separation with good visibility into the rendered markup.\\n- We can pull in responsive layout component patterns from [Braid], which gives us a good base to lay out components and pages.\\n### Negative Consequences\\n- We are choosing a pretty new framework, so it may 1) have bugs or issues, 2) be an obstacle for new developers or 3) be discontinued.\\n- When we're generating responsive styles at build time we need to be mindful at how many variations we allow (eg media queries, columns, whitespace), since they can easily bloat our CSS with unused styles.\\n","Prediction":"## Decision\n- Should be performant, with code splitting, caching and minimal runtime overhead.\\n- Needs to have easy access to our design system constants. These should optimally be shared with JS logic.\\n- Should be type-safe to catch issues when refactoring.\\n- Reusable components should be closed, not accepting arbitrary styles\/classes.\\n- We want a pattern for responsive props with atomic layout components.\\nWe will use `@apply` to apply styles to components. This will inline style in the component's component tree.\\nWe will use `@apply-when` to apply styles to components at render time. This will inline style in the component's component tree.\\nWe will use `@apply-when` to apply styles to components at render time. This will inline style in the component's component tree.\\nWe will use `@apply` to apply styles to components at render time. This will inline style in the component's component tree.\\nWe will use `@apply` to apply styles to components at render time. This will inline style in the component's component tree.\\nWe will use `@apply` to apply styles to components at render time. This will inline style in the component's component tree.\\nWe will"}
{"Context":"## Context\\nThe application accepts user data both in the forms shown on web pages and in\\ncsv files that are uploaded.\\nOne of the most common applications our users utilise for preparing their data\\nis Microsoft Excel. Excel follows a non-standardised way of identifying utf-8\\nencoded text. It includes a 'byte order mark' (bom) at the start of the file when\\nsaving as utf-8, it also looks for the bom when opening a file in order to\\nidentify the encoding.\\n","Decision":"## Decision\nAs per the GDS guidance on encoding text, the application expect users to\\nprovide unicode text encoded in utf-8.\\nhttps:\/\/www.gov.uk\/government\/publications\/open-standards-for-government\/cross-platform-character-encoding-profile\\nTo best help our users, the application will handle the bom in Excel files\\nand add the bom when exporting its own csv files encoded in utf-8.\\n","Prediction":"## Decision\nWe will use the following character encoding conventions for storing the user\\ndata:\\n* A standard utf-8 file encoding will be used when saving the user data\\n* If a file is uploaded in csv format, the user data will be stored in the\\n`csv` format. The file will be saved in the user's preferred encoding\\n* If a file is uploaded in utf-8 format, the user data will be stored in\\nthe `utf-8` encoding. The file will be saved in the user's preferred\\nencoding\\n* If a file is uploaded in the `utf-8` encoding, the user data will be\\nconverted to the `utf-8` encoding when reading the file from disk. If\\nthe file is saved in the `utf-8` encoding, the user data will be stored\\nin the `utf-8` encoding\\n* If a file is uploaded in the `utf-16` encoding, the user data will be\\nconverted to the `utf-16` encoding when reading the file from disk. If\\nthe file is saved in the `utf-16` encoding, the user data will be stored\\nin the `utf-16` encoding\\n* If a file is uploaded in"}
{"Context":"## Context\\nTo comply with [Payment Card Industry (PCI) Requirements](https:\/\/www.pcisecuritystandards.org\/pci_security\/maintaining_payment_security) we will remove the use of shared credentials anywhere in the platform.\\nWe will use Google Single Sign On (SSO) to avoid credential sharing.\\nTo enable SSO we will expose some of the APIs on the BOSH instance without using a SOCKS5 proxy. This is due to the SOCKS5 proxy not being compatible with our VPN, which in turn creates a support risk.\\nWe discussed proposed methods with IA and Cyber after reviewing the [RFC created as part of #169915408](https:\/\/docs.google.com\/document\/d\/1XZsrNp88tOSyC_bjy1mg3Yyv2TkpKgYSjoYResGAbps\/edit#heading=h.xscqoqxlc072)\\n","Decision":"## Decision\nWe will remove the reliance on SOCKS5 or SSH tunnels for the User Account and Authentication Service (UAA) API endpoint so that we can remove the use of shared credentials.\\nWith these in place we are unable to complete an SSO journey from one of our existing IDPs as the BOSH instance is not browser accessible.\\nThe current method will be replaced with SSO in order to obtain a UAA token. This token is then used to interact with the rest of the APIs on the BOSH instance.\\n","Prediction":"## Decision\nWe will use Google Single Sign On for authentication.\\nWe will expose the BOSH APIs on the BOSH instance without using a SOCKS5 proxy.\\nWe will use Google Cloud Identity to provide SSO to the BOSH instance.\\nWe will configure the BOSH instance to use the Google Cloud Identity SSO provider.\\nWe will configure the BOSH instance to use the Google Cloud Identity SSO provider to provide SSO to the BOSH API.\\nWe will configure the BOSH instance to use the Google Cloud Identity SSO provider to provide SSO to the BOSH API to avoid credential sharing.\\nWe will configure the BOSH instance to use the Google Cloud Identity SSO provider to avoid credential sharing.\\nWe will configure the BOSH instance to use the Google Cloud Identity SSO provider to avoid credential sharing.\\nWe will configure the BOSH instance to use the Google Cloud Identity SSO provider to avoid credential sharing.\\nWe will configure the BOSH instance to use the Google Cloud Identity SSO provider to avoid credential sharing.\\nWe will configure the BOSH instance to use the Google Cloud Identity SSO provider to avoid credential sharing.\\nWe will configure the BOSH instance to use the Google Cloud Identity SSO provider to avoid"}
{"Context":"## Context\\nPermissions on works and collections can come from two sources: 1) the person who authored the resource, such as the\\ndepositor or the proxy depositor; and 2) access controls (ACLs) that grant permissions based on user or group identity.\\nWhen determining who has access to a given resource, both these sources may need to be consulted.\\n","Decision":"## Decision\nAccess controls and depositor or proxy depositor rights are independent from one another.\\nAccess controls should not include permissions granted by the Actor-to-resource arrangement, such as edit rights of the\\ndepositor. They are a separate form of permission structure and therefore independent of one another. Likewise,\\npermissions that come from a depositor should have no bearing on what access controls may be applied to a resource.\\n","Prediction":"## Decision\nThe repository will use the user's primary group as its primary group for ACLs.  This will allow\\ncollaborators to work with the repository without having to concern themselves with the details of who has\\naccess to which resources.\\nCollaborators will be able to create their own ACLs and grant them to other collaborators.  They will also be\\nequipped with the capability to revoke ACLs that they grant to collaborators.\\nIn order to grant permission to a resource, the repository will use the group's primary group.  This\\nallows collaborators to work with the repository without having to concern themselves with the details of who\\nhas access to which resources.\\nCollaborators will be able to create their own ACLs and grant them to other collaborators.  They will also be\\nequipped with the capability to revoke ACLs that they grant to collaborators.\\nPermission for a given resource will be defined using the `resource` resource type.  The `resource` resource\\ntype will be used to define permissions for a given resource.  For example, this resource type might be\\ndesigned to grant permissions to a given `collection` or `work`.\\nPermission for a given resource will be defined using the"}
{"Context":"## Context\\nWe want to unit test the ViewModel classes but the ViewModel class itself creates a specific UseCase and is using hardwired Rx Schedulers.\\nThis makes it impossible (or unnecessary hard) to mock the dependencies used by the ViewModel.\\n","Decision":"## Decision\nThe creation of a UseCase instance is moved to a factory class. The factory itself is passed to every ViewModel class (using an interface for inversion of control).\\nFor unit testing we simply can mock the factory.\\nAs the factory only holds the creation of a specific UseCase, it should be enough to only create a single factory for all UseCases.\\nIn the future it could be necessary to create additional factories if a single one is polluted to much.\\nThe Schedulers are also passed as parameter to be able to mock them too.\\n","Prediction":"## Decision\nWe will use the RxJava2 library for the ViewModel classes to test them.\\nThis will allow us to unit test the ViewModel classes without creating any hardwired dependencies.\\nThis means that the ViewModel classes can be mocked by mocking the RxJava2 library itself.\\nThis will also allow us to unit test the ViewModel classes without creating a hardwired dependency on RxView.\\nThis will also allow us to unit test the ViewModel classes without creating hardwired dependencies on RxScheduler.\\nThis decision was made because the ViewModel classes are used by the ViewModel itself and therefore we cannot unit test them.\\nThis also means that we cannot unit test the ViewModel classes without creating any hardwired dependencies on RxView.\\nThis also means that we cannot unit test the ViewModel classes without creating hardwired dependencies on RxScheduler.\\nThis also means that we cannot unit test the ViewModel classes without creating hardwired dependencies on RxView.\\nThis also means that we cannot unit test the ViewModel classes without creating hardwired dependencies on RxView.\\nThis also means that we cannot unit test the ViewModel classes without creating hardwired dependencies on RxView.\\n### Positive Consequences\\n* Unit testing the ViewModel class will be easier.\\n* Mocking"}
{"Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","Prediction":"## Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/"}
{"Context":"## Context\\nWe implemented the [002 Proposal](https:\/\/github.com\/alphagov\/govuk-design-system-architecture\/blob\/master\/proposals\/002-publish-one-npm-package-instead-of-multiple-npm-packages.md) in https:\/\/github.com\/alphagov\/govuk-frontend\/pull\/680\\nWe have then restructured the code base in https:\/\/github.com\/alphagov\/govuk-frontend\/pull\/693\\nThis is to:\\n- solve issues around components importing files that export CSS\\n- take advantage of the single package - the old structure was based around each folder being it's own package.\\n- make the entry points to files clearer.\\n- better reflect conventions by giving each layer it's own folder, this moves them out of a confusingly named 'global' parent folder.\\n","Decision":"## Decision\n### Structure\\nGOV.UK Frontend is broken into a number of layers in order to help provide a\\nlogical structure, loosely following the conventions of [ITCSS].\\n1. [Settings](#settings)\\n2. [Tools](#tools)\\n3. [Helpers](#helpers)\\n4. [Core](#core)\\n5. [Objects](#objects)\\n6. [Components](#components)\\n7. [Utilities](#utilities)\\n8. [Overrides](#overrides)\\nEach folder has its own entry point (`_all.scss`).\\n### Settings\\nGlobal variables for the project, for example colour palettes and spacing and\\ntypography scales.\\nThis layer should not output any CSS.\\n### Tools\\nMixins or functions that implement either logic (e.g. conditionally outputting\\nCSS) or calculations, and that need to be made available globally.\\nThis layer should not output any CSS.\\n### Helpers\\nMixins that abstract common styling (for example, focus states or visually\\nhidden content)\\nThis layer should not output any CSS.\\n### Core\\nBasic content styles for typography, links etc. The classes defined in this\\nlayer would generally correspond to an HTML element - for example paragraphs,\\nheadings, lists or links.\\n### Objects\\nObjects can range from something as simple as a wrapper element, to layout\\nsystems.\\nThey may be used in any number of unrelated contexts to the one you can\\ncurrently see it in. This means that making modifications to these types of\\nclass could potentially have knock-on effects in other places.\\n### Components\\nDiscrete pieces of UI. In most cases these will map to the components in the\\nDesign System.\\n### Utilities\\nUtility classes \u2013 for example clearing floats, visually hiding content.\\n### Overrides\\nClasses that map to a specific property \u2013 for example margin or font weight -\\nand override all other layers. Rules in this layer will generally be marked as\\n!important.\\nClass names within this layer use an additional prefix `-!-`, for example\\n`.govuk-!-w-regular`.\\n[ITCSS]: (http:\/\/www.creativebloq.com\/web-design\/manage-large-css-projects-itcss-101517528)\\n","Prediction":"## Decision\nWe have decided to move away from having a global package and instead have 3 packages:\\n- [Alphagov App](https:\/\/github.com\/alphagov\/govuk-frontend\/commits\/master\/packages\/alphagov-app)\\n- [Alphagov Server](https:\/\/github.com\/alphagov\/govuk-frontend\/commits\/master\/packages\/alphagov-server)\\n- [Alphagov Web](https:\/\/github.com\/alphagov\/govuk-frontend\/commits\/master\/packages\/alphagov-web)\\nThese packages will be used to provide common functionality for all frontend applications.\\n- [Alphagov App](https:\/\/github.com\/alphagov\/govuk-frontend\/commits\/master\/packages\/alphagov-app)\\n- [Alphagov Server](https:\/\/github.com\/alphagov\/govuk-frontend\/commits\/master\/packages\/alphagov-server)\\n- [Alphagov Web](https:\/\/github.com\/alphagov\/govuk-frontend\/commits\/master\/packages\/alphagov-web)\\n- [Alphagov App](https:\/\/github.com\/alphagov\/govuk-frontend"}
{"Context":"## Context\\n[brille](https:\/\/github.com\/brille\/brille) is a library for computing symmetry operations\\nand linear interpolation within an irreducible part of the first Brillouin zone.\\nWhilst its symmetry operations functionality can be used stand-alone,\\nthe interpolation functionality should be integrated with codes\\nwhich compute quantities in reciprocal space\\n(such as [euphonic](https:\/\/github.com\/pace-neutrons\/euphonic) and [spinW](https:\/\/github.com\/spinw\/spinw))\\nto make these programs more user friendly.\\nAt present there are separate projects, [brilleu](https:\/\/github.com\/brille\/brilleu\/)\\nand [brillem](https:\/\/github.com\/brille\/brillem\/) to achieve this integration.\\nIn both cases brille\/X interface constructs a brille object from X and handles calling the X method(s)\\nto determine the information required for brille's interpolation.\\nThis relationship could be flipped if each X constructs its own brille object and\\nthen uses it to perform interpolation.\\n","Decision":"## Decision\nA [meeting](https:\/\/stfc365.sharepoint.com\/:b:\/r\/sites\/PACEProject\/Shared%20Documents\/Meetings\/PACE-General\/20201001_brilleX_Xbrille.pdf?csf=1&web=1&e=9XBRUe)\\nwas held and the decision was made that the integration of the interpolation functionality of brille\\nshould not be done by the external projects `brilleu` and `brillem`.\\nInstead, the interface between the calculator X and brille should be embedded within X which will\\nconstruct is own brille object and then use this to perform the interpolation.\\n","Prediction":"## Decision\nWe will use the [brilleu](https:\/\/github.com\/brille\/brilleu\/)\\nto provide the brille integration functionality.\\nThis will be provided as a separate library (built using [brille](https:\/\/github.com\/brille\/brille)).\\nThis will be made available as part of the brille project.\\nThis will provide a brille implementation that is fully compatible with the existing brille\\nimplementation, and will allow the brille project to be removed.\\n### Use of brille in brillem\\nThe brillem project will now provide brille as a separate library (built using brille).\\nThis will be made available as part of the brille project.\\nThis will provide a brille implementation that is fully compatible with the existing brille\\nimplementation, and will allow the brille project to be removed.\\n### Use of brille in brillem\\nThe brillem project will now provide brille as a separate library (built using brille).\\nThis will be made available as part of the brille project.\\nThis will provide a brille implementation that is fully compatible with the existing brille\\nimplementation, and will allow the brille project to"}
{"Context":"## Context\\n`PactSwift` takes advantage of Mock Server FFI binaries built from shared Rust code. These are generally large binary files when it comes to iOS and macOS platforms and we are limited with hosting them in the GitHub repo. The FFI also follows it's own source and changes are available independently to changes to `PactSwift`'s functionality. Separating the responsibilities would be welcomed.\\nFurthermore, the pain of managing multiple binaries with the same name but each with its specific architecture slice could be reduced by generating an `XCFramework` using an automated script and kept from the framework user. These can blow up to more than 100Mb each (the fat binary with all slices for iOS platform blew up to more than 300MB). Using `XCFramework` we can shed off a lot of the statically linked code. Mock Server FFI (`MockServer.swift`) is the only part of `PactSwift` package that depends on binaries being built for specific architectures and run platforms. With removal of binaries from the main `PactSwift` project, we should be able to avoid managing them, mixing them up (as they are all named the same), discarding them at `git add` and `commit` steps and rebuilding them at next `PactSwift` build\/test cycle.\\n","Decision":"## Decision\n- Mock Server FFI interface and implementation to be split into it's own Swift Package called `PactSwiftMockServer` and distributed as a binary (`XCFramework`) when on Apple platforms and as a source package when used on Linux platforms.\\n- Utilities used by both the main `PactSwift` and `PactSwiftMockServer` packages are split into one package called `PactSwiftToolbox`.\\n- Where it makes sense the dependencies' versions should be exact. If exact version is not set for a valid reason then `.upToMinor()` must be used to avoid breaking changes when releasing packages in isolation.\\n- Scripts to automate the release processes will be provided within the projects' scripts folders.\\n","Prediction":"## Decision\nRemove binaries from `PactSwift` package and remove `PactSwift` from `PactSwift` package.\\nRemove binaries from `PactSwift` package and remove `PactSwift` from `PactSwift` package.\\nRemove binaries from `PactSwift` package and remove `PactSwift` from `PactSwift` package.\\nRemove binaries from `PactSwift` package and remove `PactSwift` from `PactSwift` package.\\nRemove binaries from `PactSwift` package and remove `PactSwift` from `PactSwift` package.\\nRemove binaries from `PactSwift` package and remove `PactSwift` from `PactSwift` package.\\nRemove binaries from `PactSwift` package and remove `PactSwift` from `PactSwift` package.\\nRemove binaries from `PactSwift` package and remove `PactSwift` from `PactSwift` package.\\nRemove binaries from `PactSwift` package and remove `PactSwift` from `PactSwift` package.\\nRemove binaries from `PactSwift` package and remove `PactSwift` from `PactSwift` package.\\nRemove binaries from"}
{"Context":"## Context\\njavadocs are not very pleasant according to project marketing.\\nWe like an easy way to publish some techical articles related to our project\\n","Decision":"## Decision\n1. We are using mkdocs with mkdocs gradle plugin to generate docs pages.\\n2. We are going to host javadocs\\n3. Our architectural decisions also would be shared on pages\\n4. Pages would be hosted as github pages [plant-shop docs](http:\/\/wojciech.zarski.net\/plant-shop)\\n","Prediction":"## Decision\nWe will use [AtomPub](https:\/\/atompub.io\/) for publishing our tech articles.\\n### Benefits\\n* [AtomPub](https:\/\/atompub.io\/) is the standard for publishing\\n* [AtomPub](https:\/\/atompub.io\/) is well supported by all major platform\\n* [AtomPub](https:\/\/atompub.io\/) is the standard for publishing\\n* [AtomPub](https:\/\/atompub.io\/) is the standard for publishing\\n* [AtomPub](https:\/\/atompub.io\/) is the standard for publishing\\n* [AtomPub](https:\/\/atompub.io\/) is the standard for publishing\\n* [AtomPub](https:\/\/atompub.io\/) is the standard for publishing\\n* [AtomPub](https:\/\/atompub.io\/) is the standard for publishing\\n* [AtomPub](https:\/\/atompub.io\/) is the standard for publishing\\n* [AtomPub](https:\/\/atompub.io\/) is the standard for publishing\\n* [AtomPub](https:\/\/atompub.io\/) is the standard for publishing\\n* [AtomPub](https:\/\/atompub.io\/) is the standard for publishing\\n* [AtomPub](https:\/\/atompub.io\/) is the standard for publishing\\n* [AtomPub](https:\/\/"}
{"Context":"## Context\\nWe need to define the API specifications of the services we're going to implement.\\n","Decision":"## Decision\nWe use the [OpenAPI 2.0](https:\/\/swagger.io\/specification\/) specification (aka Swagger spec) as standard for our REST API definitions.\\n","Prediction":"## Decision\nWe will use [Swagger](https:\/\/swagger.io\/).\\nSwagger is an open source tool that helps us generate documentation for our API.\\nSwagger defines an API specification language that is used by the Swagger UI.\\nSwagger also provides a tool for generating documentation for our API.\\nSwagger is also used by the [Swagger UI](https:\/\/swagger.io\/)\\nSwagger UI provides a visual interface to generate documentation for our API.\\nSwagger is a mature tool that has been around for a long time.\\nSwagger is also widely used by other companies that provide API services.\\nSwagger is also used by the [Swagger UI](https:\/\/swagger.io\/)\\nSwagger is also used by the [Swagger UI](https:\/\/swagger.io\/)\\nSwagger is also used by the [Swagger UI](https:\/\/swagger.io\/)\\nSwagger is also used by the [Swagger UI](https:\/\/swagger.io\/)\\nSwagger is also used by the [Swagger UI](https:\/\/swagger.io\/)\\nSwagger is also used by the [Swagger UI](https:\/\/swagger.io\/)\\nSwagger is also used by the [Swagger UI](https:\/\/swagger.io\/)\\nSwagger is also used by the [Swagger UI](https:\/\/swagger.io\/)\\n"}
{"Context":"## Context\\nSome operations work in-place on the inputs that they are given in ND4J, but in SameDiff the same operations will\\ngenerate an array from a given shape. Examples for this include `BernoulliDistribution`, and other random ops, that\\neffectively initialize the array that they are given.\\nFrom a consistency point of view, it would be nice if both API's would support both ways of using those ops.\\n","Decision":"## Decision\nWe introduce an option to mark inputs as `inPlace = true` to make it clear that this input is going to be changed\\nin-place. In addition we introduce an option `supportsInPlaceInit = true` to mark an input as initialize-able. If the\\n`supportsInPlaceInit` option is enabled, two signatures for the Op will be created, one that takes an input, and one\\nthat takes the appropriate shape and data type information in its stead.\\n","Prediction":"## Decision\nSameDiff will use `ND4J`'s built-in `Random` to initialize the array that it is given. This will be done\\nthrough the same API that the `ND4J` API uses: `Random` is a convenience method that is implemented by the `Random\\ntype` class. In the same way that the `Random` method in the `ND4J` API returns an array of elements, the\\ngenerated array in SameDiff will be an array of `ND4J`'s elements.\\nSameDiff will also support the `ND4J` API's `Random` method to initialize an array of `ND4J`'s elements:\\n```\\ndiff = ND4J.Random(size)\\n```\\nND4J will generate a random number between 0 and size (the size of the array that it is given). In the\\neither API, the generated random number will be a `Double`. In the `ND4J` API, the generated random numbers are\\ngenerally between 0 and 1.\\nFor the same reason that the `ND4J` API uses a `Double` as the type of the random numbers, Same"}
{"Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","Prediction":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\nWe will keep ADRs in a [repository](https:\/\/github.com\/pega\/asterisk-adr).\\nWe will use [Pivotal Tracker](https:\/\/www.pivotaltracker.com\/).\\nWe will use [Google Sheets](https:\/\/sheets.google.com\/)\\nWe will use [Pivotal Tracker](https:\/\/www.pivotaltracker.com\/).\\nWe will use [Pivotal Tracker](https:\/\/www.pivotaltracker.com\/).\\nWe will use [Pivotal Tracker](https:\/\/www.pivotaltracker.com\/).\\nWe will use [Pivotal Tracker](https:\/\/www.pivotaltracker.com\/).\\nWe will use [Pivotal Tracker](https:\/\/www.pivotaltracker.com\/).\\nWe will use [Pivotal Tracker](https:\/\/www.pivotaltracker.com\/).\\nWe will use [Pivotal Tracker](https:\/\/www.pivotaltracker.com\/).\\nWe will use [Pivotal Tracker](https:\/\/www.piv"}
{"Context":"## Context\\nWe've had the Skylight profiler app installed since May 2020. However, it's not\\nbeen used for a long time and in fact has been misconfigured for as long as any\\none can remember. Should we invest further, at this point, in a service which is\\nnot being used, or should we remove it for now?\\n","Decision":"## Decision\nWe opt to remove the Skylight service, for the following reasons:\\n- using the git history we can readily re-instate it\\n- it's not currently used and is an overhead to maintain\\n- it's currently misconfigured\\n- we are currently moving out of our development phase into an\\noperational\/support phase and are transferring ownership of all third party\\nservices to BEIS\\n","Prediction":"## Decision\nRemove the Skylight profiler app.\\nThis will allow us to focus on other workloads and features, such as the new\\nAnalytics platform.\\nIf we decide to deploy the Skylight profiler app again in the future, we will\\nturn it off by default.\\nWe will leave the Skylight profiler app running in the background, but we will\\nalso remove it from the list of services in the `app_configs` section of the\\napplication.yml file.\\nWe will also remove the Skylight profiler app from the `services.yml` file.\\nWe will then create a new service in the `app_configs` section, which we will call\\n`skylight-profiler`, with the following configuration:\\n* `name`: The name of the new service. This should be the same name as the\\n`skylight-profiler` service.\\n* `description`: The description of the new service. This should be the same\\nas the description of the Skylight profiler app.\\n* `ports`: The ports associated with the new service. This should be the same\\nas the ports associated with the Skylight profiler app.\\n* `environment_variables`: The environment variables associated with the new\\nservice. This should be"}
{"Context":"## Context\\nOur AWS instance of Elasticsearch will be awkward to maintain unless we build the right tools into the CLI. Some of these commands will not be used often, but are anticipatory of being able to intervene and correct issues in production. As such, these are the CLI commands we intend to make available to developers \/ maintainers of the Discovery Index. Future needs may dictate additional commands or adjustments to these.\\n","Decision":"## Decision\n#### COMMANDS:\\nindexes  List Elasticsearch indexes and aliases\\ningest   Parse and ingest the input file\\n[note: was parse. Ingest is more reflective of what this does as parsing is just one aspect]\\nping     Request and display general info about the Elasticsearch server\\nhelp, h  Shows a list of commands or help for one command\\nIndex actions:\\ncreate   Currently available, removed in this proposal.\\ndelete   Delete an Elasticsearch index\\ndemote   Remove the given index from the production alias\\npromote  Add the given index to production alias\\nstats    Stats for provided Index (total records, maybe more?)\\nIn order to ensure we always have a production index available, we want to use a known, semi-hardcoded alias value. The fixed value will be \"production\".\\nAs we'll have new sources soon, it will be best to have each source maintain their own indexes for ease of use. Elasticsearch will allow multiple indexes to be associated with the alias \"production\" to allow us to search as many sources as we need. For aleph, the prefix will be `aleph_`. Future sources will declare a prefix that is appropriate as we add them following the `source_` convention.\\nWe currently default to an index name of `timdex`. We should no longer do that and instead default to different values depending on the command. Some commands will not have a default at all and are detailed below.\\nWe currently allow for an independent `create` command. That will be removed and the `ingest` command will handle creating indexes if necessary.\\nFor interactive work, we allow specific index values to be passed in as it may be useful for either intervening in a problem in production, or general local development. A default index value for `ingest` will be a combination of the source, such as `aleph_` and a partial timestamp such as `2018_11_26_1001`.\\nOnce we are confident the new index is ready, we'd then `promote` that index while `demoting` any existing indexes for the source we are working with. `promote` and `demote` are used to signify adding \/ removing the index to \/ from the alias \"production\". `promote` and `demote` will not default to any index value to ensure intent. When in fully automated mode, we'll need to ensure the index value set during the `ingest` process is used to `promote` and the source is used to `demote` as part of the single atomic action. However, that will not be done via the `promote` \/ `demote` CLI interface and will instead be kicked off as part of the `ingest` process when run in `auto` mode. The `index` argument will be required to inform these command what index to operate on. Additionally, we will ensure we do not allow demoting and index if it will leave the `production` alias with no indexes for the specific source. In other words, we will programmatically ensure that at least one `aleph_` source is always accessible via the `production` alias. Once we add additional sources, they will follow the same requirement of always having one index with the alias of `production`.\\nWe will keep as many old indexes as we deem useful and then go back and `delete` indexes we no longer need. The `delete` process will ensure that the index is not assigned to the alias `production` before proceeding. Additionally, `delete` will not default to any index and one must be supplied to continue.\\nThose set of commands will allow us to fully manually run the pipeline in production. The following is a proposed automatic flow:\\n```\\nmario ingest aleph --promote auto --url esurl --v4 file\\n```\\nThe source argument (`aleph` above) will allow us to construct the new index automatically when used in conjunction with a current timestamp.\\nWe'll also use the source to check the alias `production` for any indexes that currently have that prefix. `--promote auto` would dd the new index to production alias as well as remove the old index from the production alias. If `--promote auto` is not set, the new index will be created but the old index would remain in place for production use until further interactive steps were taken.\\n","Prediction":"## Decision\nWe will make the search commands available as CLI commands. We will make them available as JSON, but will not make them available as YAML. This will allow us to more easily test them in other languages.\\nWe will create a `search` directory in the repository and deploy the code there. We will make the directory available as a git submodule to the Elasticsearch repo.\\nWe will make the files in the `search` directory available as a git submodule to the Elasticsearch repo.\\nWe will make the files in the `search` directory available as a git submodule to the Elasticsearch repo.\\nWe will make the files in the `search` directory available as a git submodule to the Elasticsearch repo.\\nWe will make the files in the `search` directory available as a git submodule to the Elasticsearch repo.\\nWe will make the files in the `search` directory available as a git submodule to the Elasticsearch repo.\\nWe will make the files in the `search` directory available as a git submodule to the Elasticsearch repo.\\nWe will make the files in the `search` directory available as a git submodule to the Elasticsearch repo.\\nWe will make the files in the `search` directory available as a git submodule to the Elasticsearch repo.\\nWe will"}
{"Context":"## Context\\nhttp:\/\/www.pac4j.org\\nWhile exporing alternatives to our custom filter and token validator, we came across pac4j which can deal with Azure AD out of the box.\\nThe driver for this alternative is to have a library where we can point it to the discovery url (https:\/\/login.microsoftonline.com\/organizations\/v2.0\/.well-known\/openid-configuration) and have it \"just work.\"  Microsoft is a bit off standard where they add {tenantId} to the Issuer URL because this is a multi-tenant endpoint.  While it's not hard, openid connect libraries need to be adjusted to handle this.  Pac4j does this for free.\\n","Decision":"## Decision\n* Use Pac4J to validate MS id_tokens instead of our current custom filter code\\n","Prediction":"## Decision\nhttps:\/\/github.com\/pac4j\/pac4j\/issues\/196\\nhttps:\/\/github.com\/pac4j\/pac4j\/issues\/196\\nhttps:\/\/github.com\/pac4j\/pac4j\/issues\/196\\nhttps:\/\/github.com\/pac4j\/pac4j\/issues\/196\\nhttps:\/\/github.com\/pac4j\/pac4j\/issues\/196\\nhttps:\/\/github.com\/pac4j\/pac4j\/issues\/196\\nhttps:\/\/github.com\/pac4j\/pac4j\/issues\/196\\nhttps:\/\/github.com\/pac4j\/pac4j\/issues\/196\\nhttps:\/\/github.com\/pac4j\/pac4j\/issues\/196\\nhttps:\/\/github.com\/pac4j\/pac4j\/issues\/196\\nhttps:\/\/github.com\/pac4j\/pac4j\/issues\/196\\nhttps:\/\/github.com\/pac4j\/pac4j\/issues\/196\\nhttps:\/\/github.com\/pac4j\/pac4j\/issues\/196\\nhttps:\/\/github.com\/pac4j\/pac4j\/issues\/196\\nhttps:\/\/github.com\/pac4j\/pac4j\/issues\/196\\nhttps:\/\/github.com\/pac4j\/pac4j\/issues"}
{"Context":"## Context\\nWe are transitioning from Rails migrations and a legacy TestTrack schema\\nformat to testtrack CLI. In order to have confidence that migrations\\nwill apply cleanly in production, we need to validate as much as we can,\\nand we have a path to full information on extant splits.\\n","Decision":"## Decision\nWe will not allow retirements of splits missing from the schema. We will\\ninstead import all legacy splits and validate against the schema.\\n","Prediction":"## Decision\nWe will run all of our unit tests with a different spec spec and will use\\nthe [Spec::Rspec](https:\/\/github.com\/benjaminmoore\/spec-rspec) framework\\nto validate the results.\\nWe will also run all of our tests with a different test spec spec and will\\nuse the [Spec::Rspec::Helpers](https:\/\/github.com\/benjaminmoore\/spec-rspec\/wiki\/Spec-Helper) library\\nto validate the results.\\nWe will run all of our integration tests with a different spec spec and will use\\nthe [Spec::Rspec::IntegrationTests](https:\/\/github.com\/benjaminmoore\/spec-rspec\/wiki\/Spec-IntegrationTests) library\\nto validate the results.\\nWe will run all of our end-to-end tests with a different spec spec and will use the\\n[Spec::Rspec::Tester](https:\/\/github.com\/benjaminmoore\/spec-rspec\/wiki\/Spec-RSpec-Tester) library\\nto validate the results.\\nWe will run all of our end-to-end tests with a different spec spec and will use the\\n[Spec::Rspec::Runner](https:\/\/github.com\/benjaminmoore\/spec-rspec\/wiki\/Spec-RSpec"}
{"Context":"## Context\\n> In programming, it's often the buts in the specification that kill you.\\n>\\n> -- Boris Beizer\\nFor a large host of reasons, mostly historical, there are numerous definitions\\nof \"Ed25519 signature validation\" in the wild, which have the potential to\\nbe mutually incompatible.  This ADR serves to provide a rough high-level\\noverview of the issue, and to document the current definition of \"Ed25519\\nsignature verification\" as used by Oasis Core.\\n","Decision":"## Decision\nThe Oasis Core consensus layer (and all of the Go components) currently uses\\nthe following Ed25519 verification semantics.\\n- Non-canonical s is rejected (MUST enforce `s < L`)\\n- Small order A\/R are rejected\\n- Non-canonical A\/R are accepted\\n- The cofactored verification equation MUST be used (`[8][S]B = [8]R + [8][k]A`)\\n- A\/R may have a non-zero torsion component.\\n### Reject Non-canonical s\\nEd25519 signatures are trivially malleable unless the scalar component is\\nconstrained to `0 <= s < L`, as is possible to create valid signatures\\nfrom an existing public key\/message\/signature tuple by adding L to s.\\nThis check is mandated in all recent formulations of Ed25519 including\\nbut not limited to RFC 8032 and FIPS 186-5, and most modern implementations\\nwill include this check.\\nNote: Only asserting that `s[31] & 224 == 0` as done in older implementations\\nis insufficient.\\n### Reject Small Order A\/R\\nRejecting small order A is required to make the signature scheme strongly\\nbinding (resilience to key\/message substitution attacks).\\nRejecting (or accepting) small order R is not believed to have a security\\nimpact.\\n### Accept Non-canonical A\/R\\nThe discrete logarithm of the Ed25519 points that have a valid non-canonical\\nencoding and are not small order is unknown, and accepting them is not\\nbelieved to have a security impact.\\nNote: RFC 8032 and FIPS 186-5 require rejecting non-canonically encoded\\npoints.\\n### Cofactored Verification Equation\\nThere are two forms of the Ed25519 verification equation commonly in use,\\n`[S]B = R + [k]A` (cofactor-less), and `[8][S]B = [8]R + [8][k]A`\\n(cofactored), which are mutually incompatible in that it is possible\\nto produce signatures that pass with one and fail with the other.\\nThe cofactored verification equation is explicitly required by FIPS 186-5,\\nand is the only equation that is compatible with batch signature verification.\\nAdditionally, the more modern lattice-reduction based technique for fast\\nsignature verification is incompatible with existing implementations unless\\ncofactored.\\n### Accept A\/R With Non-zero Torsion\\nNo other library enforces this, the check is extremely expensive, and\\nwith how Oasis Core currently uses Ed25519 signatures, this has no security\\nimpact.  In the event that Oasis Core does exotic things that, for example,\\nrequire that the public key is in the prime-order subgroup, this must be\\nchanged.\\n","Prediction":"## Decision\nThe Oasis Core team will use the following definition of \"Ed25519 signature\\nverification\"\\n\"Signatures are not required on messages that have been signed with\\nEd25519, but the sender and receiver must both sign the message\\ncorrectly. The receiver MUST NOT perform Ed25519 signature\\nverification on the message. A signature verification failure is\\nconsidered a failure of Ed25519 signature verification.\\nSignatures are only required on messages that have been signed with\\nEd25519, and Ed25519 signature verification is not required on\\nmessages that have been signed with Ed25519.\\nSignatures are required on messages that have been signed with Ed25519\\nand have not been signed with Ed25519 yet.\\nSignatures are required on messages that have been signed with Ed25519\\nand have not been signed with Ed25519 yet.\\nSignatures are required on messages that have been signed with Ed25519\\nand have not been signed with Ed25519 yet.\\nSignatures are required on messages that have been signed with Ed25519\\nand have not been signed with Ed25519 yet.\\nSignatures are required on messages that have been signed with Ed25519\\nand have not been signed with Ed"}
{"Context":"## Context and Problem Statement\\nFor high-quality documents, the [ligatures](https:\/\/en.wikipedia.org\/wiki\/Typographic_ligature) have to be right.\\nSee [english.stackexchange.com](https:\/\/english.stackexchange.com\/q\/50660\/66058) for a long discusisson.\\nSee a German rant on wrong ligatures: <https:\/\/web.archive.org\/web\/20150425155310\/http:\/\/www.mengensatz.de\/blog\/?p=79>.\\nFor instance, in the English word \"offline\", the letters \"f\" and \"l\" must not be joined.\\nIn the German word \"Auflage\", the letters \"f\" and \"l\" must not be joined.\\nSee also the last lines in <https:\/\/tex.stackexchange.com\/a\/64457\/9075>.\\n### More readings\\n* <https:\/\/www.typolexikon.de\/ligatur\/>\\n* Questions on ligatures: <https:\/\/tex.stackexchange.com\/questions\/tagged\/ligatures>\\n* Disable ligatures in headings: <https:\/\/tex.stackexchange.com\/q\/198743\/9075>\\n","Decision":"## Decision\nChosen option: \"lualatex\", because comes out best (see below).\\n","Prediction":"## Decision\nChosen option: \"Use the same ligatures with the same weights in all headings\", because the choice of the font is only used for the headings, it is not used for the body text, and the font is only used for the body text in the headings.\\nThis is the only reasonable solution, because the font is the only thing that changes, the rest is just decoration.\\nFor example, in the English word \"offline\", the letters \"f\" and \"l\" must not be joined.\\nIn the German word \"Auflage\", the letters \"f\" and \"l\" must not be joined.\\nSee also the last lines in <https:\/\/tex.stackexchange.com\/a\/64457\/9075>.\\n### More readings\\n* <https:\/\/www.typolexikon.de\/ligatur\/>\\n* Questions on ligatures: <https:\/\/tex.stackexchange.com\/questions\/tagged\/ligatures>\\n* Disable ligatures in headings: <https:\/\/tex.stackexchange.com\/q\/198743\/9075>\\n### Positive Consequences\\n* The correct ligatures are used.\\n* The correct ligatures are used for the body text.\\n* The correct ligatures are used for the headings.\\n"}
{"Context":"## Context\\n- Originally we wanted to use just PointerEvents API for capturing events because the API covers mouse and pointer inputs.\\n- Unfortunately Safari and iOS have very limited support. Safari does currently have an experimental API for it but the movementX and movementY properties are always 0.\\n- MovementX\/Y is a clean browser only method for determining distance without having to track previous coordinates.\\n- Mobile is a secondary concern for this app, but pointer events on desktop Safari is also unsupported :(\\n","Decision":"## Decision\nAll pointer event interactions have a mouse event fallback.\\n","Prediction":"## Decision\n- Use PointerEvents API for all device events\\n- Use PointerEvents API for all iOS events\\n- Use PointerEvents API for all Safari events\\n- Use PointerEvents API for all Xcode projects (or at least Xcode 6.1) because it's the most recent version of Xcode available for OSX\\n- Use PointerEvents API for all Xcode projects (or at least Xcode 6.1) because it's the most recent version of Xcode available for OSX\\n- Use PointerEvents API for all Swift projects because Swift is the most recent version of Xcode available for OSX\\n- Use PointerEvents API for all Swift projects because Swift is the most recent version of Xcode available for OSX\\n- Use PointerEvents API for all Swift projects because Swift is the most recent version of Xcode available for OSX\\n- Use PointerEvents API for all Swift projects because Swift is the most recent version of Xcode available for OSX\\n- Use PointerEvents API for all Swift projects because Swift is the most recent version of Xcode available for OSX\\n- Use PointerEvents API for all Swift projects because Swift is the most recent version of Xcode available for OSX\\n- Use PointerEvents API for all Swift projects because Swift is the most recent"}
{"Context":"## Problem\\nValidation plugins operate as independent blackboxes.\\nFor every backend each mounted validation plugin iterates\\nover the whole keyset, checks every key for its trigger metakey,\\nand validates the key.\\nCurrently all needed validation plugins need to be specified at\\nmount-time - if additional validation is required, the backend\\nhas to be remounted with the required plugins and plugin\\nconfiguration.\\nIf validation of a key fails, each plugin decides on its own\\nhow to handle the issue and proceed in ways that might be\\ndifferent from what is expected or desired.\\n","Decision":"## Decision\nUse a wrapper plugin to iterate over the keyset and delegate the validation\\nof each key to the corresponding validation plugin.\\n","Prediction":"## Decision\nWe will add a plugin \"backend\" that can be mounted and\\nde-activated. This plugin will take care of resolving all\\nvalidation issues and can be used to override the validation\\nplugins of a given backend.\\nThe backend \"backend\" will be created at mount-time by\\ncalling a plugin factory.\\nThis plugin factory will be used to create a plugin\\nthat will be used by all backend modules.\\nThis plugin factory will be used to create a plugin\\nthat will be used by the given backend module.\\nThis plugin factory will be used to load the required\\nplugins and will be used to create the required plugin\\nconfiguration.\\nThis plugin factory will be used to create the\\nrequired plugin and will be used to create the\\nrequired configuration.\\nThis plugin factory will be used to create the\\nrequired configuration that is required by the given\\nbackend module.\\nThis plugin factory will be used to create the\\nrequired configuration that is required by the given\\nbackend module.\\nThis plugin factory will be used to create the\\nrequired configuration that is required by the given\\nbackend module.\\nThis plugin factory will be used to create the\\nrequired configuration that is required by the given\\nbackend module.\\nThis plugin factory will be"}
{"Context":"## Context and problem statement\\nWhat syntax should Rufus use for list types, literals, and cons expressions?\\n","Decision":"## Decision\n* The syntax should be consistent, especially between literal and cons\\nrepresentations, and avoid unnecessary boilerplate where doing so helps\\nreadability.\\n* The syntax should work well in match expressions as well as patterns in\\nfunction heads.\\n* The syntax should sit cleanly alongside tuples and maps (and eventually sets).\\n* The syntax should scale to custom types.\\n* Parsing and typechecking the syntax should be straightforward to reduce\\ncomplexity in the compiler and ensure that other tools, such as `rf fmt`, can\\neasily work with the syntax.\\nChosen option: option 2, because it will simplify compiler implementation, and\\nbecause making the list type explicit should make code easier to read because\\nthere will be less guessing about what the type is.\\n","Prediction":"## Decision\nUse Python 3.8 syntax for list types, literals, and cons expressions.\\nOnly list types, literals, and cons expressions will be accepted.\\nList types will be accepted as:\\n- list of tuples\\n- list of lists\\n- list of lists of tuples\\n- list of lists of lists\\n- list of lists of tuples of lists\\n- list of tuples of lists of lists\\n- list of lists of tuples of lists of lists of lists\\n- list of tuples of lists of lists of lists of lists\\n- list of lists of lists of lists of lists of lists\\n- list of lists of lists of lists of lists of lists\\n- list of lists of lists of lists of lists of lists\\n- list of lists of lists of lists of lists of lists\\n- list of lists of lists of lists of lists of lists\\n- list of lists of lists of lists of lists of lists\\n- list of lists of lists of lists of lists of lists\\n- list of lists of lists of lists of lists of lists\\n- list of lists of lists of lists of lists of lists\\n- list of lists of lists of lists of lists of lists\\n- list of lists of lists of lists of lists of lists\\n- list"}
{"Context":"## Context and Problem Statement\\nThe desktop client will be executed outside of an internet navigator like Chrome or Firefox, and the `explorer-website` will not be used.\\nCurrently, to log in into the world, you must enter through `explorer-website` (or derived), and the authentication happens with exchanges of messages between `kernel` and `website`.\\nOn Desktop, `website` will not be used, and those exchanges will be with `kernel` and `renderer`. So `explorer-desktop` must implement a way to communicate those to be able to log in.\\nThe problem is how to execute `kernel` without a navigator and communicate it with the `renderer`.\\n","Decision":"## Decision\nLaunching `kernel` with `Chromium headless` is the most compatible with what currently `explorer` has. But executing `kernel` in `NodeJS` is inevitable because it will execute `kernel` unit tests and other cases. Taking advantage of this approach is the best option not to rewrite code and isolate it as much as possible.\\nRemoving `explorer-website` from the Desktop Client Project is required. So the communication with `kernel` and `renderer` to authenticate will be implemented without other options.\\n# Solution\\nTo implement what it's described above the team decided the following flows:\\n","Prediction":"## Decision\nWe will use a `bootstrap` to run `kernel` with the `bootstrap.js` file.\\nThe `bootstrap` will be injected in the `kernel` in the `constructor`.\\n`bootstrap.js` will be in the `resources\/js` folder.\\n`bootstrap.js` will have the following structure:\\n```javascript\\nimport { bootstrap } from 'bootstrap';\\n\\ntype MyComponent = (props) => ( <MyComponent {...props} \/> );\\nimport { createApp } from 'react';\\nconst MyApp = createApp(MyComponent);\\nexport default MyApp;\\n```\\n`bootstrap.js` will be in the `resources\/js` folder.\\n`bootstrap.js` will have the following structure:\\n```javascript\\nimport { bootstrap } from 'bootstrap';\\n\\ntype MyComponent = (props) => ( <MyComponent {...props} \/> );\\nimport { createApp } from 'react';\\nconst MyApp = createApp(MyComponent);\\nexport default MyApp;\\n```\\n`app.js` will be in the `resources\/js\/app.js` folder.\\n`app.js` will have the following structure:\\n```javascript\\nimport {"}
{"Context":"## Context\\n[CosmosDB](https:\/\/docs.microsoft.com\/en-us\/azure\/cosmos-db\/) provide two document-oriented APIs:\\n* [DocumentDB API](http:\/\/azure.github.io\/azure-documentdb-node\/DocumentClient.html)\\n* [MongoDB API](http:\/\/mongodb.github.io\/node-mongodb-native\/2.0\/)\\nFor the purpose of the Digital Citizenship project we will need the following capabilities:\\n* Single document CRUD operations\\n* Query capabilities on secondary indexes (with filtering, result limiting and ordering)\\n* Conflict detection during concurrency writes\\nWe also want to minimize the coupling with proprietary technologies and protocols.\\nComparison of the CosmosDB APIs:\\n|                    | DocumentDB API | MongoDB API |\\n| ------------------ | -------------- | ----------- |\\n| CRUD operations    | Yes            | Yes         |\\n| Query capabilities | Yes            | Yes         |\\n| Conflict detection | Yes            | Yes (1)     |\\n| Lock-in            | High           | Low         |\\n(1) Conflict detection is not supported by the MongoDB API but can be implemented at the application level via [MVCC](https:\/\/en.wikipedia.org\/wiki\/Multiversion_concurrency_control) with the addition of a `version` field to the documents.\\n","Decision":"## Decision\nWe decide to implement all database operations via the MongoDB API.\\n","Prediction":"## Decision\nWe will use the MongoDB API for the document-oriented storage.\\nWe will use the CosmosDB API for the application layer.\\nWe will use the [MVCC](https:\/\/en.wikipedia.org\/wiki\/Multiversion_concurrency_control) mechanism for the document-level locking.\\nWe will use [the [Node.js document-based storage library](https:\/\/github.com\/alexandria\/node-mongodb-native)](https:\/\/github.com\/alexandria\/node-mongodb-native) to provide the CRUD operations and for the secondary indexes.\\nWe will use [the [MongoDB driver](https:\/\/www.mongodb.com\/mongodb-driver)](https:\/\/www.mongodb.com\/mongodb-driver) to provide the query capabilities.\\nWe will use the [MongoDB driver](https:\/\/www.mongodb.com\/mongodb-driver) to provide the conflict detection.\\nWe will use the [MongoDB driver](https:\/\/www.mongodb.com\/mongodb-driver) for the single document storage.\\nWe will use the [MongoDB driver](https:\/\/www.mongodb.com\/mongodb-driver) for the secondary indexes.\\nWe will use the [MongoDB driver](https:\/\/www.mongodb.com\/mongodb-driver) for the document-level locking.\\nWe will use the [MongoDB driver](https"}
{"Context":"## Context\\nCurrently there are only several services planned, but most of them require time for initialization (warming cache for instance) and they can't start processing requests from other services immediately. The situation might arise when restarting a service, introducing a new instance, or some other situations.\\nIf a service is not ready for processing a request, the timeout might happen and the supervisor might decide that there should be a new instance that leads to a cascading effect and higher bills in the end.\\n","Decision":"## Decision\nHealth check endpoints provide information about readiness of processing requests and other services should respect this information.\\n","Prediction":"## Decision\nThe strategy is to create a new service for each new request (or a new instance) based on the same technology. This allows to have a separate service for each request, and each request is handled by a new instance.\\nAfter the service is ready for processing the request, it will be stopped and the new request will be processed. The new request will be handled by the same instance, and the old one will be stopped.\\nThe service will be restarted if the service is not ready for processing the request or if the request was canceled.\\nIf the service is not ready for processing the request, the supervisor will wait for a while, and if the service is still not ready, the supervisor will stop the service and start the new one.\\nThe service will be restarted if the service is not ready for processing the request or if the request was canceled.\\nIf the service is not ready for processing the request, the supervisor will wait for a while, and if the service is still not ready, the supervisor will start the new one.\\nThe service will be restarted if the service is not ready for processing the request or if the request was canceled.\\nIf the service is not ready for processing the request, the supervisor will wait for a while, and if"}
{"Context":"## Context\\nWe want a test framework that has good support for React and TypeScript.\\n[Jest](https:\/\/jestjs.io) is the standard, recommended test framework for React\\napps.\\n","Decision":"## Decision\nWe will use Jest as our testing framework.\\n","Prediction":"## Decision\nWe will use Jest for testing.\\nWe will use TypeScript for writing tests.\\nWe will use [eslint](https:\/\/eslint.org\/) to enforce style.\\nWe will use [tslint](https:\/\/tslint.io\/), [eslint-plugin-import](https:\/\/github.com\/eslint\/eslint-plugin-import), and\\n[eslint-plugin-import-loader](https:\/\/github.com\/eslint\/eslint-plugin-import-loader) to enforce\\nstyle and syntax.\\nWe will use [ts-node](https:\/\/github.com\/ts-node\/ts-node) to run tests on the server.\\nWe will use [ts-node-coverage](https:\/\/github.com\/ts-node-coverage\/ts-node-coverage) to run\\ncoverage tests on the client.\\nWe will use [ts-node-coverage-sass](https:\/\/github.com\/ts-node-coverage-sass\/ts-node-coverage-sass) to run\\ncoverage tests on the client with Sass.\\nWe will use [ts-node-coverage-sass-loader](https:\/\/github.com\/ts-node-coverage-sass-loader) to run\\ncoverage tests on the client with Sass and the following plugins:\\n* [ts-node-coverage-sass-loader](https:\/\/github.com\/ts-node-coverage-sass-loader) - This is the"}
{"Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"## Decision\nWe will use Architecture Decision Records, as\\n[described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","Prediction":"## Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\nThis document will be used to track architectural decisions made on this project. A decision record is a short text file that contains the details of a decision made in a project, and references the previous decision that was made, and the benefits and drawbacks of the proposed solution. A decision record is not a task list, and is not intended to be used for tracking work items. A decision record should be kept in a file called `arch-decisions` in the root directory of the repository.\\nWe will use the following naming convention for the file:\\n[Decision Record Name](arch-decisions\/\\n[Documenting Project](arch-decisions\/\\n[Documenting Architecture Decision](arch-decisions\/\\n[A Decision Record](arch-decisions\/\\n[A Decision Record (1.0)](arch-decisions\/\\n[A Decision Record (2.0)](arch-decisions\/\\n[A Decision Record (3.0)](arch-decisions\/\\n[A Decision Record (4.0)](arch-decisions\/\\"}
{"Context":"## Context\\nTenants provision a wide variety of services and AWS\/Aiven resources via GOV.UK PaaS. We need to calculate bills for these services\/resources. GOV.UK PaaS billing receives notifications of when services or resources are created, renamed, or deleted from upstream in the form of events from Cloud Foundry.\\nThe original GOV.UK PaaS billing system translated the Cloud Foundry events into records of services\/resources by calendar month before calculating the final monthly bill for each tenant. This process, called billing consolidation, was done at the start of every month and there was no persistent record of the results of each stage of processing, including what services or resources tenants had provisioned. After each stage of processing database tables were populated but the contents of these tables were impermanent, being refreshed the next time billing consolidation was run.\\nIn the GOV.UK PaaS billing rewrite, this has been changed. We want to calculate bills for variable time periods and also to forecast bills for the future (for the web-based billing calculator). However, the method to calculate the actual bill always needs to be the same.\\n","Decision":"## Decision\nThe code to calculate the bill has been decoupled from the code used to calculate the bill. This is so we can use exactly the same code for calculation of all bills, whether these bills are for tenants or for prospective tenants (using the billing calculator).\\nThe approach taken is:\\n1. Populate database temporary table with what is being billed (which resources over which time interval, including the future). This can be in a stored function or embedded SQL. This is the code entry point into billing.\\n2. Call a stored function to calculate the bill ([`calculate_bill`](https:\/\/github.com\/alphagov\/paas-billing\/blob\/main\/billing-db\/sprocs\/calculate_bill.sql)) only using the contents of the temporary table populated in step 1 above. No parameters are passed into ([`calculate_bill`](https:\/\/github.com\/alphagov\/paas-billing\/blob\/main\/billing-db\/sprocs\/calculate_bill.sql)).\\n","Prediction":"## Decision\nWe will use the billing system as it is and will not implement a new billing system. This decision was made because this approach is the most reliable and easy to maintain. We have a clear understanding of the GOV.UK PaaS billing system and it is clear what needs to be done. In the future, if we want to change the billing system we can do so in a straightforward manner.\\nWe will introduce a new billing system in the future, which will be a replacement for the GOV.UK PaaS billing system. We will develop this new billing system in a separate project, which will exist for the duration of the GOV.UK PaaS billing project.\\nThis decision was made because it is the most straightforward approach and we do not expect to have to change the GOV.UK PaaS billing system in the future. It also means that the GOV.UK PaaS billing system can be removed in the future without any impact to the GOV.UK PaaS billing service.\\nWe will use the billing system as it is and will not implement a new billing system. This decision was made because this approach is the most reliable and easy to maintain. We have a clear understanding of the GOV.UK PaaS billing"}
{"Context":"## Context and Problem Statement\\nPurpose of this decision record is to outline an implementation for license management for Elasticsearch clusters managed by the Elastic k8s operator.\\n## Decision Drivers <!-- optional -->\\n* gold\/platinum level licenses can only be applied to clusters using internal TLS\\n* user applying the license needs to have `manage` privileges if security features are enabled (which is always the case)\\n* enterprise licenses should be shared between multiple clusters\\n* in some use cases we might want to isolate enterprise licenses from clusters by using a different namespace\\n","Decision":"## Decision\n* gold\/platinum level licenses can only be applied to clusters using internal TLS\\n* user applying the license needs to have `manage` privileges if security features are enabled (which is always the case)\\n* enterprise licenses should be shared between multiple clusters\\n* in some use cases we might want to isolate enterprise licenses from clusters by using a different namespace\\nOption 1 and 2 are both valid as two separate implementation phases. Without a running license controller option 1 still ensures licenses can be applied to Elasticsearch.\\n### Positive Consequences <!-- optional -->\\n* both options are orthogonal to each other\\n* when used in combination user just needs to create Enterprise licenses in the namespace of the license controller\\n* one license controller can manage licenses for all Elasticsearch clusters\\n### Negative Consequences <!-- optional -->\\n* we are limited to Enterprise licenses for option 2 at the moment (could be revisited)\\n","Prediction":"## Decision\n* gold\/platinum level licenses can only be applied to clusters using internal TLS\\n* user applying the license needs to have `manage` privileges if security features are enabled (which is always the case)\\n* enterprise licenses should be shared between multiple clusters\\n* in some use cases we might want to isolate enterprise licenses from clusters by using a different namespace\\nChosen option: \"License Manager for Elasticsearch\", because it's the most simple and most obvious solution.\\n### Positive Consequences <!-- optional -->\\n* easy to implement\\n* can be applied to the cluster without needing to install any additional software\\n* can be used to apply a license to multiple clusters\\n* can be used to apply an enterprise license to multiple clusters\\n* can be used to apply a user license to multiple clusters\\n* can be used to enable\/disable the license manager for clusters\\n* can be used to apply a user license to multiple clusters\\n* can be used to enable\/disable the license manager for Elasticsearch\\n* can be used to enable\/disable the license manager for Elasticsearch\\n* can be used to enable\/disable the license manager for Elasticsearch\\n* can be used to enable\/disable the license manager for Elasticsearch\\n* can be used to enable\/disable the license manager for Elasticsearch\\n* can"}
{"Context":"## Context\\nWe need a reasonable tradeoff between ease-of-use and maintainability.\\nSpecifically, we need composable, combinable units that can be improved independently.\\n### Approach 1: Classes + Methods\\nOne approach is to embrace Object-Oriented Programming (OOP) with fluent interfaces (i.e. method chaining):\\n```python\\nproject\\n.create(...)\\n.update(...)\\n.delete(...)\\n```\\nCharacteristics:\\n- Ease-of-use is maximized, but this requires each method to `return self`.\\n- Also, this approach implies that if a function can be called with X different object types,\\neach of those object types should have a corresponding method that applies that functionality and then `return self`.\\nHow to enforce these characteristics?\\nAny solution will be a tax on maintainability, as code that adheres to these characteristics will include many non-semantic lines simply going through the motions of `return self` and copying function usage into dedicated methods for each class.\\n### Approach 2: Types + Functions\\nAnother approach is to embrace a functional programming style: simple types and functions (no methods).\\nUsage is not as terse as for OOP:\\n```python\\np = tc.project.create(...)\\nu = tc.project.update(p, ...)\\nd = tc.project.delete(p, ...)\\n```\\nCharacteristics:\\n- Ease-of-use is not optimized, but still reasonable.\\n- With tab-completion, ease-of-use is comparable to OOP.\\n- Each type can be made immutable\\n- Each function can be made pure\\n- Functionality can be shared by calling the same function in user-land, not copying function calls in contributor-land.\\n","Decision":"## Decision\nUse `@dataclass(frozen=True)` to model types and plain Python modules and functions to capture business logic.\\n","Prediction":"How to enforce these characteristics?\\nThis approach requires the developer to be aware of all possible type and function names, and to always be aware of the context in which they are used. It also requires that the developer be aware of the consequences of making a function or type `pure`, and of the need to always use the same function name in user-land.\\n## Decision\n### Approach 1: Classes + Methods\\nOne approach is to embrace Object-Oriented Programming (OOP) with fluent interfaces (i.e. method chaining):\\n```python\\nproject\\n.create(...)\\n.update(...)\\n.delete(...)\\n```\\nCharacteristics:\\n- Ease-of-use is maximized, but this requires each method to `return self`.\\n- Also, this approach implies that if a function can be called with X different object types,\\neach of those object types should have a corresponding method that applies that functionality and then `return self`.\\nHow to enforce these characteristics?\\nAny solution will be a tax on maintainability, as code that adheres to these characteristics will include many non-semantic lines simply going through the motions of `return self` and copying function usage into dedicated methods for each class.\\n### Approach 2:"}
{"Context":"## Context\\nFor the Proof of Skill assignment several auxillary technology have been used on top of the required terraform. As terraform\\nas a infrastructure as source is a given in the assignment, others will be explained here.\\n","Decision":"## Decision\n### Repeatable project creation\\nThe use of ansible as a project templater. Ansible is selected to have a reentry method to create projects from a template. In this case this is more used for documentation than template but it is used\\nto illustrate its use. The author is aware that ask nicely is using Puppet, but the author is more familliar with ansible and as this is an anxillary\\ntool this one is selected over puppet.\\n","Prediction":"## Decision\n- [terraform-provider-aws](https:\/\/github.com\/terraform-providers\/terraform-provider-aws)\\n- [terraform-provider-aws-aws-s3](https:\/\/github.com\/terraform-providers\/terraform-provider-aws-aws-s3)\\n- [terraform-provider-aws-aws-s3-aws-iamss](https:\/\/github.com\/terraform-providers\/terraform-provider-aws-aws-s3-aws-iamss)\\n- [terraform-provider-aws-iamss-aws-iamss-bundle](https:\/\/github.com\/terraform-providers\/terraform-provider-aws-iamss-aws-iamss-bundle)\\n- [terraform-provider-aws-iamss-aws-iamss-bundle-aws-iamss-aws-iamss](https:\/\/github.com\/terraform-providers\/terraform-provider-aws-iamss-aws-iamss-aws-iamss-bundle-aws-iamss-aws-iamss-aws-iamss-bundle)\\n- [terraform-provider-aws-iamss-aws-iamss-aws-iamss-aws-iamss](https:\/\/github.com\/terraform-providers\/terraform-provider-aws-iamss-aws-iam"}
{"Context":"## Context\\nAs part of our pipelines we will be building artefacts that will be used to test\\nand deploy our applications. We will be deploying applications to Kubernetes. We\\nwill need to build a container image of some kind.\\nThere are some competing container image formats, namely:\\n* [OCI]\\n* [ACI]\\nThe OCI image format is [based on the Docker v2][oci-standard] image format.\\nThe Kubernetes project appears to [prefer Docker\/OCI][k8s-preferance] images\\nover ACI.\\n[rkt is moving to OCI][rkt-oci] and away from ACI. OCI will become the preferred\\nimage format.\\nDocker has wide industry adoption and appears to have wide understanding within\\nGDS.\\nDocker is the default container runtime for Kubernetes.\\n","Decision":"## Decision\nWe will build and store OCI images built using Docker.\\n","Prediction":"## Decision\nWe will use OCI as the container format for the Kubernetes cluster.\\nWe will build container images using rkt to build container images from an\\nOCI image.\\nWe will build container images using rkt to build container images from a\\nKubernetes manifest.\\nWe will build container images using rkt to build container images from a\\nKubernetes manifest.\\nWe will build container images using rkt to build container images from a\\nKubernetes manifest.\\nWe will build container images using rkt to build container images from a\\nKubernetes manifest.\\nWe will build container images using rkt to build container images from a\\nKubernetes manifest.\\nWe will build container images using rkt to build container images from a\\nKubernetes manifest.\\nWe will build container images using rkt to build container images from a\\nKubernetes manifest.\\nWe will build container images using rkt to build container images from a\\nKubernetes manifest.\\nWe will build container images using rkt to build container images from a\\nKubernetes manifest.\\nWe will build container images using rkt to build container images from a\\nKubernetes manifest.\\nWe will build container images using rkt to build container images from a\\nKubernetes manifest.\\n"}
{"Context":"## Context\\nVersioning APIs can be controversial.  Ideally, APIs don't need to change, or be additive so as not to break backwards compatibility.  Occasionally, some changes require bigger changes.\\nWe will do our best to be open w\/ what we accept and strict w\/ what we return\\nThere are lots of ways an API can change:\\n* Breaking changes across the entire API (all endpoints change from REST to GraphQL for example)\\n* Breaking changes at a resource level (\/v1\/booking) needs to change\\n* Breaking changes in behavior\\nThis decision currently applies to the 1st bullet.\\n","Decision":"## Decision\nAll API endpoints will start with the version of the API (e.g. \/v1\/resource).  We don't anticipate this to change.\\n","Prediction":"## Decision\n* Breaking changes across the entire API (all endpoints change from REST to GraphQL for example)\\n* Breaking changes at a resource level (\/v1\/booking) needs to change\\n* Breaking changes in behavior\\nThis decision currently applies to the 1st bullet.\\nWe will only accept breaking changes where the change is significant enough to break backwards compatibility (see https:\/\/twitter.com\/alexanderj\/status\/1043716902688481024\\nIf you make a breaking change to an API, you MUST document it in the CHANGELOG\\nFor example, if you add a new version of a field to a resource, you MUST document it in the CHANGELOG\\nIf you make a breaking change in behavior, you MUST document it in the CHANGELOG\\nIf you make a breaking change in the API (e.g. a new endpoint), you MUST document it in the CHANGELOG\\nIf you make a breaking change to the API (e.g. a new resource type), you MUST document it in the CHANGELOG\\nIf you make a breaking change to the API (e.g. a new endpoint), you MUST document it in the CHANGELOG\\nIf you make a breaking change to the API (e.g. a new resource type), you MUST document it"}
{"Context":"## Context\\nWe need the geometries (boundaries) of a municipality to derive the municipality of road segments that do not have an associated left or right street.\\nWe can't rely on the geometries used in the municipality registry because they are (1) not exposed and (2) they do not match the boundary along which road segments have been split up.\\nWhen the boundaries in the municipality registry change, one can't expect all segments to change as well.\\n","Decision":"## Decision\nWe keep track of municipalities as defined in the legacy road registry database because those are the ones along which road segments have been split up.\\nWe use `ImportedMunicipality` as the main event to track this. It contains all the data we need for now. This event is assembled while extracting events and imported during deployment.\\n","Prediction":"## Decision\nWe will use the [geospatial boundary](https:\/\/github.com\/geospatial\/geospatial\/blob\/master\/doc\/geometry-geospatial.md) specification for the municipality boundaries for the remainder of this project.\\nGeospatial boundary definitions are available in the [geospatial boundary library](https:\/\/github.com\/geospatial\/geospatial\/blob\/master\/doc\/geometry-geospatial.md#geospatial-geometry) maintained by the [Geospatial Boundary Library](https:\/\/github.com\/geospatial\/geospatial).\\nGeospatial boundary definitions will be stored in the [geospatial boundary library](https:\/\/github.com\/geospatial\/geospatial.git) and will be versioned as part of the repository.\\nGeospatial boundary definitions will be stored in the [geospatial boundary library](https:\/\/github.com\/geospatial\/geospatial.git) in the `geospatial\/geometry` directory.\\n### Use of [geospatial boundary library](https:\/\/github.com\/geospatial\/geospatial.git)\\nGeospatial boundary definitions will be stored in the `geospatial\/geometry` directory of the repository.\\nGeospatial boundary definitions will be stored as plain text files in the `ge"}
{"Context":"## Context\\nOur initial way of implementing standard build practices was to copy a bunch of files into place in every project. There were various problems:\\n- there was no single master location from which to copy the files\\n- each project tended to make its own tweaks and it was never clear which were project-specific, and which were useful enhancements\\n- there was no procedure defined for pushing updates to the standards out to all projects\\n- there was no established mechanism around how to handle .NET Core 3.1 or various .NET Standard versions\\n- there was an inconsistent mixture of mechanisms\u2014some use of imports, some use of Directory.props\\nWe could have tweaked the existing mechanism. For example we could have defined a repository somewhere whose job was to hold the latest master copies of the files. We could have established conventions that could have clarified which parts of the config were the unaltered standard mechanism, and which parts are project-specific modifications.\\nHowever, our `Endjin.RecommendedPractices.AzureDevopsPipelines.GitHub` works a lot more smoothly, partly as a result of not going down this path. Instead, it uses a mechanism designed specifically to support reuse (Azure DevOps pipeline templates), and which ensures there is exactly one master copy that everything shares, which has built-in extensibility points to enable project-specific adjustments.\\nBecause projects don't copy and then modify the relevant files\u2014instead they just refer to them and then plug into the extensibility points supplied\u2014it is absolutely clear when something is a project-specific tweak. And this also provides a straightforward mechanism for updates: when something breaks the build (e.g., as happened when .NET Core SDK 3.1.200 came out) we can push a single update to fix everything.\\nWhat if `Endjin.RecommendedPractices` could enjoy all these same benefits as `Endjin.RecommendedPractices.AzureDevopsPipelines.GitHub`? The key characteristics are:\\n- one definitive copy of the relevant files, distributed through a mechanism designed to enable sharing of such files\\n- clearly-defined extensibility points to provide individual projects with the flexibility they require, without needing them to have their own copies of everything\\n","Decision":"## Decision\nMove our standard build files into a NuGet package: [Endjin.RecommendedPractices.NuGet](https:\/\/www.nuget.org\/packages\/Endjin.RecommendedPractices.NuGet). Define extensibility mechanisms as required.\\n","Prediction":"## Decision\nWe are going to use Azure DevOps pipeline templates to implement `Endjin.RecommendedPractices.AzureDevopsPipelines.GitHub`.\\nWe will use Azure DevOps pipeline templates to create a new project, `Endjin.RecommendedPractices.AzureDevopsPipelines`, and to push this project to the `RecommendedPractices` branch in the GitHub repository.\\nWe will use Azure DevOps pipeline templates to create a new pipeline, `Endjin.RecommendedPractices.AzureDevopsPipelines`, and to deploy it to the `RecommendedPractices` branch in the GitHub repository.\\nWe will use Azure DevOps pipeline templates to create a new pipeline, `Endjin.RecommendedPractices.AzureDevopsPipelines`, and to push this pipeline to the `Endjin.RecommendedPractices` branch in the GitHub repository.\\nWe will use Azure DevOps pipeline templates to create a new pipeline, `Endjin.RecommendedPractices.AzureDevopsPipelines`, and to deploy it to the `Endjin.RecommendedPractices` branch in the GitHub repository.\\nWe will use Azure DevOps pipeline templates to create a new pipeline, `Endjin.RecommendedPractices.AzureDevopsPipelines`, and to deploy it to the `End"}
{"Context":"## Context\\nWe are importing applications from apply into register by using the `ApplyApplication` model as an intermediate datastore. Since these applications might contain data that is not understood by register, we need a way to handle such attributes, so that they can be prompted back to the user managing the trainee in register.\\nWe spiked a couple of approaches; option 1, [to add accessor methods to ApplyApplication](https:\/\/github.com\/DFE-Digital\/register-trainee-teachers\/pull\/1149), and option 2, [storing invalid data on the ApplyApplication model](https:\/\/github.com\/DFE-Digital\/register-trainee-teachers\/pull\/1147).\\nConsensus was for option 2, but there were mixed opinions about where it should live. Some felt it should be on the relevant model, eg, invalid trainee data should be on the `Trainee` model, and invalid degree data on the `Degree` model.\\nOn the other hand, if we did move it to `Trainee`, it could get confusing, since it can be argued that `trainee.invalid_data` should account for all fields on the trainee and we already have validations to check the state of the trainee data entered in our system before submission.\\n","Decision":"## Decision\nWe decided to keep it on the `ApplyApplication` model, and wipe it out on successful submission of a TRN.\\n","Prediction":"## Decision\nWe will move the accessor methods for `invalid_data` from the `ApplyApplication` model to be an accessor method on the `Trainee` model. This will allow us to move the validation logic from the `ApplyApplication` model to the `Trainee` model, and provide a common interface for the trainee model.\\nWe will also add a new method to the `Trainee` model to allow the trainee to store its data in the datastore, and a new method to allow the trainee to retrieve its data from the datastore. This will allow us to refactor the `apply_application` method to remove the need for a datastore, and to allow the trainee to be managed in register without needing an intermediate datastore.\\nWe will also add a `new_object` method to the `Trainee` model to allow the trainee to store its data in the datastore, and a new method to allow the trainee to retrieve its data from the datastore. This will allow us to refactor the `apply_application` method to remove the need for a datastore, and to allow the trainee to be managed in register without needing an intermediate datastore.\\nWe will also add a `new_object` method to the `Trainee` model to allow"}
{"Context":"## Context\\nWe need to choose which tool or tools to use for CI and CD.  Different tools suit different purposes, however some cross over exists which could allow the use of a single tool to do both CI and CD.\\n","Decision":"## Decision\nWe will use [Concourse](https:\/\/concourse-ci.org\/) for both CI and CD.\\nReasons:\\n- It will allow the alpha work to progress without waiting for a decision based upon user research on which tool set is best suited for use for CI or CD\\n- The team has experience of using concourse for CI and CD with kubernetes\\n- A working example already exists that can be extended for use in the alpha\\n- Concourse supports simple RBAC which should allow for multi-tenancy capability in the future\\n- It will accelerate the development of the alpha, with the team only needing to learn a single tool rather than multiple tools\\n","Prediction":"## Decision\nWe will use Travis CI for CI and CircleCI for CD.\\nTravis CI is a free hosted continuous integration service that is well known in the open source community and has a large and active community.\\nCircleCI is a paid hosted continuous integration service that has a large and active community and is well known in our community.\\nTravis CI will be used for CI and CircleCI will be used for CD.\\nCircleCI will be used to manage the deployment pipeline.\\nTravis CI will be used to manage the continuous integration build.\\nTravis CI will be used to manage the deployment pipeline.\\nTravis CI will be used to manage the continuous integration build.\\nTravis CI will be used to manage the continuous integration build.\\nTravis CI will be used to manage the continuous integration build.\\nTravis CI will be used to manage the continuous integration build.\\nTravis CI will be used to manage the continuous integration build.\\nTravis CI will be used to manage the continuous integration build.\\nTravis CI will be used to manage the continuous integration build.\\nTravis CI will be used to manage the continuous integration build.\\nTravis CI will be used to manage the continuous integration build.\\nTravis CI"}
{"Context":"## Context and Problem Statement\\nWe want to present LaTeX examples.\\nThey should be shown both as LaTeX source and as rendered output.\\n","Decision":"## Decision\n* Write once - have two outputs (source and rendered)\\n* Good interaction with all pacakges\\n* Should support floats\\n* Should support compilation without special configuration (such as `--shell-escape`)\\nChosen option: \"custom solution\", because resolves all forces.\\n### Positive Consequences\\n* Works with floats and umlauts\\n### Negative Consequences\\n* A little bit more than minimal code is presented to the user\\n","Prediction":"## Decision\nChosen option: \"Use a lightweight editor to generate LaTeX source\", because it is the easiest to implement and the least resource intensive.\\nThis will be implemented in the form of a separate plugin for the TeXnicCenter editor.\\nThis plugin will generate LaTeX source from the `latex` command line argument.\\nIt will also generate a PDF of the generated LaTeX source.\\nThis PDF will be rendered inside a new window.\\nThis new window will be rendered in the same window as the generated LaTeX.\\nThis will allow the user to copy the generated LaTeX.\\nThis will also allow the user to open the generated LaTeX in a new browser tab.\\nThis will ensure that the generated LaTeX is rendered correctly.\\nThis will also ensure that the generated LaTeX is rendered correctly.\\nThis will also add a button to the generated LaTeX to open it in a new window.\\nThis will also add a button to the generated LaTeX to open it in a new window.\\nThis will also add a button to the generated LaTeX to open it in a new window.\\nThis will also add a button to the generated LaTeX to open it in a new window.\\nThis will also add a button to the generated LaTeX to open it in a new window.\\"}
{"Context":"## Context\\n[ICS 26 - Routing Module](https:\/\/github.com\/cosmos\/ibc\/tree\/master\/spec\/core\/ics-026-routing-module) defines a function [`handlePacketRecv`](https:\/\/github.com\/cosmos\/ibc\/tree\/master\/spec\/core\/ics-026-routing-module#packet-relay).\\nIn ICS 26, the routing module is defined as a layer above each application module\\nwhich verifies and routes messages to the destination modules. It is possible to\\nimplement it as a separate module, however, we already have functionality to route\\nmessages upon the destination identifiers in the baseapp. This ADR suggests\\nto utilize existing `baseapp.router` to route packets to application modules.\\nGenerally, routing module callbacks have two separate steps in them,\\nverification and execution. This corresponds to the `AnteHandler`-`Handler`\\nmodel inside the SDK. We can do the verification inside the `AnteHandler`\\nin order to increase developer ergonomics by reducing boilerplate\\nverification code.\\nFor atomic multi-message transaction, we want to keep the IBC related\\nstate modification to be preserved even the application side state change\\nreverts. One of the example might be IBC token sending message following with\\nstake delegation which uses the tokens received by the previous packet message.\\nIf the token receiving fails for any reason, we might not want to keep\\nexecuting the transaction, but we also don't want to abort the transaction\\nor the sequence and commitment will be reverted and the channel will be stuck.\\nThis ADR suggests new `CodeType`, `CodeTxBreak`, to fix this problem.\\n","Decision":"## Decision\n`PortKeeper` will have the capability key that is able to access only the\\nchannels bound to the port. Entities that hold a `PortKeeper` will be\\nable to call the methods on it which are corresponding with the methods with\\nthe same names on the `ChannelKeeper`, but only with the\\nallowed port. `ChannelKeeper.Port(string, ChannelChecker)` will be defined to\\neasily construct a capability-safe `PortKeeper`. This will be addressed in\\nanother ADR and we will use insecure `ChannelKeeper` for now.\\n`baseapp.runMsgs` will break the loop over the messages if one of the handlers\\nreturns `!Result.IsOK()`. However, the outer logic will write the cached\\nstore if `Result.IsOK() || Result.Code.IsBreak()`. `Result.Code.IsBreak()` if\\n`Result.Code == CodeTxBreak`.\\n```go\\nfunc (app *BaseApp) runTx(tx Tx) (result Result) {\\nmsgs := tx.GetMsgs()\\n\/\/ AnteHandler\\nif app.anteHandler != nil {\\nanteCtx, msCache := app.cacheTxContext(ctx)\\nnewCtx, err := app.anteHandler(anteCtx, tx)\\nif !newCtx.IsZero() {\\nctx = newCtx.WithMultiStore(ms)\\n}\\nif err != nil {\\n\/\/ error handling logic\\nreturn res\\n}\\nmsCache.Write()\\n}\\n\/\/ Main Handler\\nrunMsgCtx, msCache := app.cacheTxContext(ctx)\\nresult = app.runMsgs(runMsgCtx, msgs)\\n\/\/ BEGIN modification made in this ADR\\nif result.IsOK() || result.IsBreak() {\\n\/\/ END\\nmsCache.Write()\\n}\\nreturn result\\n}\\n```\\nThe Cosmos SDK will define an `AnteDecorator` for IBC packet receiving. The\\n`AnteDecorator` will iterate over the messages included in the transaction, type\\n`switch` to check whether the message contains an incoming IBC packet, and if so\\nverify the Merkle proof.\\n```go\\ntype ProofVerificationDecorator struct {\\nclientKeeper ClientKeeper\\nchannelKeeper ChannelKeeper\\n}\\nfunc (pvr ProofVerificationDecorator) AnteHandle(ctx Context, tx Tx, simulate bool, next AnteHandler) (Context, error) {\\nfor _, msg := range tx.GetMsgs() {\\nvar err error\\nswitch msg := msg.(type) {\\ncase client.MsgUpdateClient:\\nerr = pvr.clientKeeper.UpdateClient(msg.ClientID, msg.Header)\\ncase channel.MsgPacket:\\nerr = pvr.channelKeeper.RecvPacket(msg.Packet, msg.Proofs, msg.ProofHeight)\\ncase channel.MsgAcknowledgement:\\nerr = pvr.channelKeeper.AcknowledgementPacket(msg.Acknowledgement, msg.Proof, msg.ProofHeight)\\ncase channel.MsgTimeoutPacket:\\nerr = pvr.channelKeeper.TimeoutPacket(msg.Packet, msg.Proof, msg.ProofHeight, msg.NextSequenceRecv)\\ncase channel.MsgChannelOpenInit;\\nerr = pvr.channelKeeper.CheckOpen(msg.PortID, msg.ChannelID, msg.Channel)\\ndefault:\\ncontinue\\n}\\nif err != nil {\\nreturn ctx, err\\n}\\n}\\nreturn next(ctx, tx, simulate)\\n}\\n```\\nWhere `MsgUpdateClient`, `MsgPacket`, `MsgAcknowledgement`, `MsgTimeoutPacket`\\nare `sdk.Msg` types correspond to `handleUpdateClient`, `handleRecvPacket`,\\n`handleAcknowledgementPacket`, `handleTimeoutPacket` of the routing module,\\nrespectively.\\nThe side effects of `RecvPacket`, `VerifyAcknowledgement`,\\n`VerifyTimeout` will be extracted out into separated functions,\\n`WriteAcknowledgement`, `DeleteCommitment`, `DeleteCommitmentTimeout`, respectively,\\nwhich will be called by the application handlers after the execution.\\n`WriteAcknowledgement` writes the acknowledgement to the state that can be\\nverified by the counter-party chain and increments the sequence to prevent\\ndouble execution. `DeleteCommitment` will delete the commitment stored,\\n`DeleteCommitmentTimeout` will delete the commitment and close channel in case\\nof ordered channel.\\n```go\\nfunc (keeper ChannelKeeper) WriteAcknowledgement(ctx Context, packet Packet, ack []byte) {\\nkeeper.SetPacketAcknowledgement(ctx, packet.GetDestPort(), packet.GetDestChannel(), packet.GetSequence(), ack)\\nkeeper.SetNextSequenceRecv(ctx, packet.GetDestPort(), packet.GetDestChannel(), packet.GetSequence())\\n}\\nfunc (keeper ChannelKeeper) DeleteCommitment(ctx Context, packet Packet) {\\nkeeper.deletePacketCommitment(ctx, packet.GetSourcePort(), packet.GetSourceChannel(), packet.GetSequence())\\n}\\nfunc (keeper ChannelKeeper) DeleteCommitmentTimeout(ctx Context, packet Packet) {\\nk.deletePacketCommitment(ctx, packet.GetSourcePort(), packet.GetSourceChannel(), packet.GetSequence())\\nif channel.Ordering == types.ORDERED [\\nchannel.State = types.CLOSED\\nk.SetChannel(ctx, packet.GetSourcePort(), packet.GetSourceChannel(), channel)\\n}\\n}\\n```\\nEach application handler should call respective finalization methods on the `PortKeeper`\\nin order to increase sequence (in case of packet) or remove the commitment\\n(in case of acknowledgement and timeout).\\nCalling those functions implies that the application logic has successfully executed.\\nHowever, the handlers can return `Result` with `CodeTxBreak` after calling those methods\\nwhich will persist the state changes that has been already done but prevent any further\\nmessages to be executed in case of semantically invalid packet. This will keep the sequence\\nincreased in the previous IBC packets(thus preventing double execution) without\\nproceeding to the following messages.\\nIn any case the application modules should never return state reverting result,\\nwhich will make the channel unable to proceed.\\n`ChannelKeeper.CheckOpen` method will be introduced. This will replace `onChanOpen*` defined\\nunder the routing module specification. Instead of define each channel handshake callback\\nfunctions, application modules can provide `ChannelChecker` function with the `AppModule`\\nwhich will be injected to `ChannelKeeper.Port()` at the top level application.\\n`CheckOpen` will find the correct `ChennelChecker` using the\\n`PortID` and call it, which will return an error if it is unacceptable by the application.\\nThe `ProofVerificationDecorator` will be inserted to the top level application.\\nIt is not safe to make each module responsible to call proof verification\\nlogic, whereas application can misbehave(in terms of IBC protocol) by\\nmistake.\\nThe `ProofVerificationDecorator` should come right after the default sybil attack\\nresistant layer from the current `auth.NewAnteHandler`:\\n```go\\n\/\/ add IBC ProofVerificationDecorator to the Chain of\\nfunc NewAnteHandler(\\nak keeper.AccountKeeper, supplyKeeper types.SupplyKeeper, ibcKeeper ibc.Keeper,\\nsigGasConsumer SignatureVerificationGasConsumer) sdk.AnteHandler {\\nreturn sdk.ChainAnteDecorators(\\nNewSetUpContextDecorator(), \/\/ outermost AnteDecorator. SetUpContext must be called first\\n...\\nNewIncrementSequenceDecorator(ak),\\nibcante.ProofVerificationDecorator(ibcKeeper.ClientKeeper, ibcKeeper.ChannelKeeper), \/\/ innermost AnteDecorator\\n)\\n}\\n```\\nThe implementation of this ADR will also create a `Data` field of the `Packet` of type `[]byte`, which can be deserialised by the receiving module into its own private type. It is up to the application modules to do this according to their own interpretation, not by the IBC keeper.  This is crucial for dynamic IBC.\\nExample application-side usage:\\n```go\\ntype AppModule struct {}\\n\/\/ CheckChannel will be provided to the ChannelKeeper as ChannelKeeper.Port(module.CheckChannel)\\nfunc (module AppModule) CheckChannel(portID, channelID string, channel Channel) error {\\nif channel.Ordering != UNORDERED {\\nreturn ErrUncompatibleOrdering()\\n}\\nif channel.CounterpartyPort != \"bank\" {\\nreturn ErrUncompatiblePort()\\n}\\nif channel.Version != \"\" {\\nreturn ErrUncompatibleVersion()\\n}\\nreturn nil\\n}\\nfunc NewHandler(k Keeper) Handler {\\nreturn func(ctx Context, msg Msg) Result {\\nswitch msg := msg.(type) {\\ncase MsgTransfer:\\nreturn handleMsgTransfer(ctx, k, msg)\\ncase ibc.MsgPacket:\\nvar data PacketDataTransfer\\nif err := types.ModuleCodec.UnmarshalBinaryBare(msg.GetData(), &data); err != nil {\\nreturn err\\n}\\nreturn handlePacketDataTransfer(ctx, k, msg, data)\\ncase ibc.MsgTimeoutPacket:\\nvar data PacketDataTransfer\\nif err := types.ModuleCodec.UnmarshalBinaryBare(msg.GetData(), &data); err != nil {\\nreturn err\\n}\\nreturn handleTimeoutPacketDataTransfer(ctx, k, packet)\\n\/\/ interface { PortID() string; ChannelID() string; Channel() ibc.Channel }\\n\/\/ MsgChanInit, MsgChanTry implements ibc.MsgChannelOpen\\ncase ibc.MsgChannelOpen:\\nreturn handleMsgChannelOpen(ctx, k, msg)\\n}\\n}\\n}\\nfunc handleMsgTransfer(ctx Context, k Keeper, msg MsgTransfer) Result {\\nerr := k.SendTransfer(ctx,msg.PortID, msg.ChannelID, msg.Amount, msg.Sender, msg.Receiver)\\nif err != nil {\\nreturn sdk.ResultFromError(err)\\n}\\nreturn sdk.Result{}\\n}\\nfunc handlePacketDataTransfer(ctx Context, k Keeper, packet Packet, data PacketDataTransfer) Result {\\nerr := k.ReceiveTransfer(ctx, packet.GetSourcePort(), packet.GetSourceChannel(), packet.GetDestinationPort(), packet.GetDestinationChannel(), data)\\nif err != nil {\\n\/\/ TODO: Source chain sent invalid packet, shutdown channel\\n}\\nk.ChannelKeeper.WriteAcknowledgement([]byte{0x00}) \/\/ WriteAcknowledgement increases the sequence, preventing double spending\\nreturn sdk.Result{}\\n}\\nfunc handleCustomTimeoutPacket(ctx Context, k Keeper, packet CustomPacket) Result {\\nerr := k.RecoverTransfer(ctx, packet.GetSourcePort(), packet.GetSourceChannel(), packet.GetDestinationPort(), packet.GetDestinationChannel(), data)\\nif err != nil {\\n\/\/ This chain sent invalid packet or cannot recover the funds\\npanic(err)\\n}\\nk.ChannelKeeper.DeleteCommitmentTimeout(ctx, packet)\\n\/\/ packet timeout should not fail\\nreturn sdk.Result{}\\n}\\nfunc handleMsgChannelOpen(sdk.Context, k Keeper, msg MsgOpenChannel) Result {\\nk.AllocateEscrowAddress(ctx, msg.ChannelID())\\nreturn sdk.Result{}\\n}\\n```\\n","Prediction":"## Decision\nWe will add the `CodeType` to the `AnteHandler` and use it to verify and\\nexecute the IBC related state changes. We will make the `AnteHandler`\\nimplementations to use the `CodeType` in a similar manner to the `Ante`\\nHandler in the SDK. We will also add the `AnteHandler` to the IBC\\napplication module to handle the message verification and\\ntransaction execution. The `AnteHandler` will verify the IBC related state\\nchanges and execute the IBC related state changes in the right order.\\nIn the `AnteHandler` implementation, we will keep the IBC related\\nstate in the `CodeTxBreak` to be preserved.\\nWe will add a new `CodeType` to the `IbcHandler` to fix the atomic multi-message\\ntransaction issue. We will have a separate `IbcHandler` for each application module.\\nFor example, the IBC token sending message will have a `IbcHandler` for the\\n`Ibc` module. The `IbcHandler` for the `Ibc` module will verify the IBC\\nrelated state changes and execute the IBC related state changes in"}
{"Context":"## Context\\nIstanbul version <1.x.x has been deprecated and is no longer receiving updates.\\nnyc is one of the suggested replacements. Additional information is available\\non [npm](https:\/\/www.npmjs.com\/package\/istanbul). This leaves the package open\\nto security flaws that will not be patched. Features available in the latest\\nversions of node will not be supported.\\n","Decision":"## Decision\nThe decision is to migrate from istanbul to nyc.\\n","Prediction":"## Decision\nWe will remove support for the version of node that is currently in use.\\nWe will remove the package name from the package.json file.\\nWe will update the package.json file to reference the `yarn` package instead.\\nThis will also remove the dependency on the deprecated package.\\nThis will also remove the dependency on the deprecated package.\\nThis will also remove the dependency on the deprecated package.\\nThis will also remove the dependency on the deprecated package.\\nThis will also remove the dependency on the deprecated package.\\nThis will also remove the dependency on the deprecated package.\\nThis will also remove the dependency on the deprecated package.\\nThis will also remove the dependency on the deprecated package.\\nThis will also remove the dependency on the deprecated package.\\nThis will also remove the dependency on the deprecated package.\\nThis will also remove the dependency on the deprecated package.\\nThis will also remove the dependency on the deprecated package.\\nThis will also remove the dependency on the deprecated package.\\nThis will also remove the dependency on the deprecated package.\\nThis will also remove the dependency on the deprecated package.\\nThis will also remove the dependency on the deprecated package.\\nThis will also remove the dependency on the deprecated package.\\nThis will"}
{"Context":"## Context\\nWe need API for the Zoo to expose issues, analytics data, etc.\\n","Decision":"## Decision\nWe will create API by GraphQL specification. Pagination will be done according\\nto Relay server specification. We considered to build REST API, but we decided\\nthat for fresh new APIs is the GraphQL right choice.\\n","Prediction":"## Decision\nWe decided to use [Zoo](https:\/\/github.com\/zooprojects\/zooprojects) for the Zoo API. Zoo is a popular and well-known Python package for API development, and Zoo provides a simple and well-organized DSL for defining API endpoints.\\nZoo has been used in many projects over the years, including [Terraform](https:\/\/github.com\/terraform-providers\/) and [Grafana Labs](https:\/\/grafana.com\/). The Zoo project provides an [example](https:\/\/github.com\/zooprojects\/example) of a Zoo project that we will use as an example.\\n### Zoo examples\\n### [Zoo example](https:\/\/github.com\/zooprojects\/example) of a Zoo project that we will use as an example\\n### Zoo endpoints\\n### [Zoo endpoints](https:\/\/github.com\/zooprojects\/zoo-endpoints) of a Zoo project that we will use as an example\\n### Zoo API versioning\\n### [Zoo API versioning](https:\/\/github.com\/zooprojects\/zoo-versioning) of a Zoo project that we will use as an example\\n### Zoo service endpoints\\n### [Zoo service endpoints](https:\/\/github.com\/zooprojects"}
{"Context":"## Context\\n* Embedded systems are highly event-driven, as they are responding to external stimuli and reacting in a planned way\\n* Event-driven APIs reduce coupling, as the various objects don't need to know anything about other objects that they work with\\n* We can reduce the number of threads used by relying on event-driven behavior\\n","Decision":"## Decision\nThe framework will be defined with interfaces and processing models that support event-driven development (callbacks, events, register\/unregister for events).\\nDispatch queues will be provided to assist with the event driven model.\\nPlatform examples will default to dispatch-based processing models.\\n","Prediction":"## Decision\n* Write our code in a separate thread, for example, as a background thread\\n* Use a message queue as a communication channel between the event-driven and non-event-driven code\\n* Use a message queue for the event-driven code, and a queue for the non-event-driven code\\n* We can use a message queue for the event-driven code, and a queue for the non-event-driven code\\n* We can use a message queue for the event-driven code, and a queue for the non-event-driven code\\n* We can use a message queue for the event-driven code, and a queue for the non-event-driven code\\n* We can use a message queue for the event-driven code, and a queue for the non-event-driven code\\n* We can use a message queue for the event-driven code, and a queue for the non-event-driven code\\n* We can use a message queue for the event-driven code, and a queue for the non-event-driven code\\n* We can use a message queue for the event-driven code, and a queue for the non-event-driven code\\n* We can use a message queue for the event-driven code, and a queue for the non-event-driven code\\n* We can use a message queue for the event-driven code"}
{"Context":"## Context\\nThe Capacity Status API will be built using the Django web framework. The framework offers a number of modules and features that can be utilised when building an API. This ADR discusses these concepts and the decisions made in determining which Django features to use, and how we can use them. Essentially, this comes down to three distinct parts:\\n- Rest Interface (views and controllers)\\n- Serialization (the broker between the request and the domain model; and the domain model and the response)\\n- Domain Model (where and how the information is kept)\\nThe Capacity Status API is required to:\\n- Take in capacity status information to update a specific service, containing the following information:\\n- Capacity Status (the capacity status to put the service into in readable format: Red, Amber, Green)\\n- Reset Status In Minutes (when setting the capacity status to either a Red or Amber state, this is the number of minutes the service will persist in this state. After this time, the service will be automatically set back to the Green capacity status)\\n- Notes (a free text field that the requester can set)\\n- Update the capacity status information in Core DoS with the following information:\\n- Capacity Status (in enumerated format: 1, 2, 3)\\n- Reset Date Time (a timestamp specifying when the service should revert back to a Green capacity status)\\n- Notes\\n- Modified By (the user who has updated the capacity information)\\n- Modified Date (a timestamp specifying when the capacity information was last updated)\\n- Respond with the capacity information for a requested service, containing the following information:\\n- Service UID (the UID identifying the service)\\n- Service Name (the human readable name of the service)\\n- Capacity Status (Red, Amber, Green)\\n- Reset Date Time\\n- Notes\\n- Modified By\\n- Modified Date\\n","Decision":"## Decision\n### Rest Interface\\nFor the interface, we decided upon using the Django Rest Framework extension (DRF). This extension provides a vast number of useful modules and components which form part of a modern RESTful API, and by leveraging these components we keep our code clean, understandable, and easy to maintain. In addition, it is very easy to plugin API documentation into this framework, the majority of the documentation being automatically generated from code, therefore keeping documentation maintenance down to a minimum.\\nThe extension comes with already defined 'views' which automatically configure (and give) the API any set of the standard REST endpoints that we would require. The Capacity Status API will require a GET and PUT endpoint, which can be automatically configured using the DRF RetrieveUpdateAPIView view. The default endpoint functionality provided by the DRF can also be easily modified to suit the specific business needs of the API.\\n### Serialization\\nDjango provides a component known as a 'Serializer' which deals with everything concerning the transfer of data to and from the client (request\/response) to and from the domain model. The serializer deals with:\\n- validation of the data\\n- conversion of the data (from JSON to domain model or visa-versa)\\nThe Capacity Status API needs to deal with three streams of information:\\n- Capacity information coming in\\n- Capacity information held in the data model\\n- Capacity going out\\nEach stream of information contains slightly different views on the capacity information, we have therefore decided to adopt a three-serializer approach in dealing with this use case. In doing this, we have a separate, well defined serializer for each information stream. Each serializer will be configured with its own set of validation rules specifically catered to handle its specified data stream. Each serializer will also have its own defined mechanism to deal with passing the data along to the next information stream. This gives us clear separation of concerns for what each serializer is doing. We therefore have:\\n- Payload Serializer - responsible for consuming the JSON payload from the request, validating the data, and converting the data into the format that the Data Model Serializer expects.\\n- Data Model Serializer - responsible for consuming the data from the Payload Serializer, validating the data, and updating the Domain Model. Also responsible for retrieving capacity information from the Domain Model.\\n- Response Serializer - responsible for consuming the data from the Data Model Serializer, validating the data, and converting the data into JSON for the response.\\n### Domain Model\\nThe Capacity Status API comprises of two domain models:\\n- Capacity Status model\\n- User Management model\\nThe Capacity Status domain model represents the capacity status information for every service in Core DoS, extending to which users have which permissions to be able to view or manipulate the capacity status information. There are a collection of related tables which make up this model, and these are stored and maintained within the Core DoS database.\\nThe part of the model relating to capacity status information (not user permissions) is coupled to the Data Model Serializer in the API. This allows the API to be able to retrieve and update capacity information using the Django framework, thus keeping the API code clean and removing the need for bespoke ORM code. Although this introduces tight coupling of this part of the Core DoS database to the API, we consider this very low risk since the probably of changes in this area of Core DoS is considered low.\\nThe part of the model relating to user permissions to be able to update capacity status information also resides in the Core DoS database, and retrieval of this data will be performed by a dedicated 'DoS' module that the Capacity Status API can call. The mechanism for interfacing with the DoS DB is discussed in ADR-006. The driving purpose of the 'DoS' module is to reduce coupling concerns between Core DoS and the API itself, thus maintaining the integrity of the APIs core code base.\\nFinally, the User Management model is created and maintained by the API and is stored in its own database. This further decouples the concept of the API user (or more specifically the API Key) from what is known as the user in Core DoS. The approach taken is for Django to manage the data model, and this will be performed by Django's built in database migration tools. Using this approach we are completely decoupled from what type of database we are using as Django will automatically apply the correct drivers to whatever database we choose. This also means that database migration scripts do not need to be created. This is all handled by Django.\\n","Prediction":"## Decision\nThe Capacity Status API will be implemented using the Django web framework and will utilise the following Django features:\\n- Rest Interface (views and controllers)\\n- Serialization (the broker between the request and the domain model; and the domain model and the response)\\n- Domain Model (where and how the information is kept)\\n- Querysets\\n- The Django Queryset Framework\\n- The Django ORM\\n- The Django ORM Querysets\\n- The Django ORM Queryset Framework\\n- The Django ORM Queryset Framework\\n- The Django ORM Queryset Framework\\n- The Django ORM Queryset Framework\\n- The Django ORM Queryset Framework\\n- The Django ORM Queryset Framework\\n- The Django ORM Queryset Framework\\n- The Django ORM Queryset Framework\\n- The Django ORM Queryset Framework\\n- The Django ORM Queryset Framework\\n- The Django ORM Queryset Framework\\n- The Django ORM Queryset Framework\\n- The Django ORM Queryset Framework\\n- The Django ORM Queryset Framework\\n- The Django ORM Queryset Framework\\n- The Django ORM Queryset Framework\\n- The Django ORM Queryset Framework\\n- The Django ORM Queryset Framework\\n- The Django ORM Queryset Framework\\n- The Django ORM Queryset Framework\\n- The Django ORM Query"}
{"Context":"## Context\\nThough very handy to have around, the source field does incur storage overhead within the index.\\n","Decision":"## Decision\nDisable `_source` for ElasticSearch indexed documents.\\n","Prediction":"## Decision\nThe decision is to remove the source field from the index.\\nThis will allow the user to add the source field to the index manually, and will allow the application to be more efficient when it comes to searching and filtering.\\nThis decision is based on the following:\\n- The source field is not needed to be indexed\\n- There are no use cases to index the source field\\n- The source field is not used in any other fields\\n- The source field is not used in any other queries\\n- The source field is not used in any other indexing\\n- The source field is not used in any other search\\n- The source field is not used in any other document search\\n- The source field is not used in any other indexing queries\\n- The source field is not used on the query results\\n- The source field is not used on the search results\\n- The source field is not used on the query results\\n- The source field is not used on the search results\\n- The source field is not used on the search results\\n- The source field is not used on the search results\\n- The source field is not used on the search results\\n- The source field is not used on the search results\\n- The source field is"}
{"Context":"## Context\\nWe need machine-readable logging so that we can do better analytics and our logging is consistent with the Python services, which use Structlog.\\nWe are aiming to meet the ONS's logging standard, which is documented here: https:\/\/github.com\/ONSdigital\/software-engineer-community\/blob\/master\/standards\/logging.md\\n","Decision":"## Decision\nWe investigated the available open source projects which would give us an enhanced API to make it easier to log key-value pairs in the desired JSON format. Projects with few contributors and without recent releases were not considered to be viable candidates. It seems that Godaddy have a nice enhancement to the vanilla SLF4J API, which is quite convenient for the developer to use.\\nThe proposal to use the Godaddy logger was put on Slack in the developers channel and a vote was held. 6 developers voted \"YES\" and there weren't any \"NO\" votes or votes for other solutions.\\nGodaddy logging API allows you to do logging like this: `log.with(\"key\", \"value\").error(\"Here be error messagez\", exception);`\\nMore on the Godaddy logger: https:\/\/github.com\/godaddy\/godaddy-logger\/\\n","Prediction":"## Decision\nWe will use Structlog for logging.\\nWe will use a custom logging format for our Python services, to be written in the future.\\nWe will use the standard Python logging framework, with the following configuration:\\n* Log level: `ERROR`\\n* Format: `%(asctime)s %(name)s %(levelname)s %(message)s`\\n* File: `log_file_name` (default: `log_file_name.log`\\n* File extension: `.log`\\n* Target: `%(process)s`\\n* Target: `%(thread)s`\\n* Target: `%(threadpool)s`\\n* Target: `%(threadpool)s`\\n* Target: `%(os)s`\\n* Target: `%(os)s`\\n* Target: `%(os)s`\\n* Target: `%(os)s`\\n* Target: `%(os)s`\\n* Target: `%(os)s`\\n* Target: `%(os)s`\\n* Target: `%(os)s`\\n* Target: `%(os)s`\\n* Target: `%(os)s`\\n* Target: `%(os)s`\\n* Target: `%(os)s`\\n* Target: `%(os)s`\\n* Target"}
{"Context":"## Context\\nComplicated with uses of different css rules\\n","Decision":"## Decision\nOnly uses styled-components for design\\n","Prediction":"## Decision\nUse one css rule for the whole project\\n* `main.css`: https:\/\/gist.github.com\/bobosco\/45d9b6d0c1f1e0f6f3c0c34a1e2d5b3d\\n* `sidebar.css`: https:\/\/gist.github.com\/bobosco\/0cde5a7c8e3c1a7d6e1e8d3a0b5b8b8\\n* `index.css`: https:\/\/gist.github.com\/bobosco\/5c3f2ba2a4bf4d8f2f7c28a3a7b0b6a\\n* `index.css.scss`: https:\/\/gist.github.com\/bobosco\/2c2c2c2e1d7c3a0b5b8b8e1c1a0dcf2\\n* `page.css`: https:\/\/gist.github.com\/bobosco\/4b8b8d3f11e1b0f6f3c0c34a1e2d5b3d\\n* `page.css.scss`: https:\/\/gist.github.com\/bobosco"}
{"Context":"## Context\\nA critical part of building these services is being able to track who evidence belongs to, and therefore we must be able to identify residents in a platform-wide manner.\\nAt the time of this decision, the HackIT team is in the process of building the Platform APIs necessary to provide access to this sort of core data, but they are not available yet.\\n","Decision":"## Decision\nTo build a resident data source in this application, but make the extra effort to abstract it appropriately so that when an external resident data source and accompanying API is available.\\n","Prediction":"## Decision\nWe will use the [OpenID Connect](https:\/\/openid.net\/specs\/openid-connect-core-0_4.html) protocol to provide access to core data for these services.\\nThis will be done in the following ways:\\n- We will use the [OpenID Connect Provider](https:\/\/openid.net\/docs\/providers\/) to provide a way to obtain OpenID Connect tokens for our services. This will happen on the [OpenID Connect Provider](https:\/\/openid.net\/docs\/providers\/) page.\\n- We will use the [OpenID Connect Client](https:\/\/openid.net\/docs\/clients\/) to provide a way for an application to obtain an OpenID Connect token for a user. This will happen on the [OpenID Connect Client](https:\/\/openid.net\/docs\/clients\/) page.\\n- We will use the [OpenID Connect Service Provider](https:\/\/openid.net\/docs\/services\/) to provide a way for an application to obtain an OpenID Connect token for a service. This will happen on the [OpenID Connect Service Provider](https:\/\/openid.net\/docs\/services\/) page.\\n- We will use the [OpenID Connect Assertion Provider](https:\/\/openid.net\/docs\/claims\/) to provide a way for an application to obtain an OpenID Connect assertion for"}
{"Context":"**Context**: We know there will be users of these C++ libraries who want to use\\nC++ exceptions as well as those who are not able to. Our C++ libraries must\\nwork for all of our users, regardless of their ability to use exceptions.\\n**Decision**: None of our APIs will throw exceptions to indicate errors.\\nInstead, our APIs will typically report errors to callers by returning a\\n`Status` or `StatusOr<T>` object, unless the library we're using has another\\nnon-throwing way to report errors (e.g., [badbit][badbit-link] in the standard\\nI\/O library).\\n**Consequences**: This decision will result in a single set of APIs and a\\nconsistent vocabulary for all users, whether or not they choose to compile with\\nexceptions. This decision does not prevent callers from using exceptions in\\ntheir own code.\\nA downside of this decision is that our APIs will not be natural or idiomatic\\nfor the [50+%][survey-link] of users who might prefer exceptions for error\\nreporting.\\nChanging existing APIs from throwing exceptions to returning `StatusOr<T>` is a\\nbreaking change. As of this writing (Jan 2019), this project has a [Google\\nCloud Storage][gcs-link] component that is at the Alpha quality level, and a\\n[Google Cloud Bigtable][bigtable-link] that is already at the Beta quality\\nlevel. We plan to immediately change the API for Google Cloud Storage. We have\\nno immediate plans to change the API for Cloud Bigtable. We will communicate a\\ntimeline to change this API in a separate document.\\n[badbit-link]: https:\/\/en.cppreference.com\/w\/cpp\/io\/ios_base\/iostate\\n[gcs-link]: https:\/\/github.com\/googleapis\/google-cloud-cpp\/tree\/master\/google\/cloud\/storage\\n[bigtable-link]: https:\/\/github.com\/googleapis\/google-cloud-cpp\/tree\/master\/google\/cloud\/bigtable\\n[survey-link]: https:\/\/isocpp.org\/blog\/2018\/03\/results-summary-cpp-foundation-developer-survey-lite-2018-02\\n","Decision":"## Decision\nInstead, our APIs will typically report errors to callers by returning a\\n`Status` or `StatusOr<T>` object, unless the library we're using has another\\nnon-throwing way to report errors (e.g., [badbit][badbit-link] in the standard\\nI\/O library).\\n**Consequences**: This decision will result in a single set of APIs and a\\nconsistent vocabulary for all users, whether or not they choose to compile with\\nexceptions. This decision does not prevent callers from using exceptions in\\ntheir own code.\\nA downside of this decision is that our APIs will not be natural or idiomatic\\nfor the [50+%][survey-link] of users who might prefer exceptions for error\\nreporting.\\nChanging existing APIs from throwing exceptions to returning `StatusOr<T>` is a\\nbreaking change. As of this writing (Jan 2019), this project has a [Google\\nCloud Storage][gcs-link] component that is at the Alpha quality level, and a\\n[Google Cloud Bigtable][bigtable-link] that is already at the Beta quality\\nlevel. We plan to immediately change the API for Google Cloud Storage. We have\\nno immediate plans to change the API for Cloud Bigtable. We will communicate a\\ntimeline to change this API in a separate document.\\n[badbit-link]: https:\/\/en.cppreference.com\/w\/cpp\/io\/ios_base\/iostate\\n[gcs-link]: https:\/\/github.com\/googleapis\/google-cloud-cpp\/tree\/master\/google\/cloud\/storage\\n[bigtable-link]: https:\/\/github.com\/googleapis\/google-cloud-cpp\/tree\/master\/google\/cloud\/bigtable\\n[survey-link]: https:\/\/isocpp.org\/blog\/2018\/03\/results-summary-cpp-foundation-developer-survey-lite-2018-02\\n","Prediction":"## Decision\nAll of our APIs will throw exceptions to indicate errors. Instead, our APIs\\nwill typically report errors to callers by returning a `StatusOr<T>` object, unless\\nthe library we're using has another non-throwing way to report errors (e.g., [badbit][badbit-link] in the standard I\/O library).\\n**Consequences**: This decision will result in a single set of APIs and a\\nconsistent vocabulary for all users, whether or not they choose to compile with\\nexceptions. This decision does not prevent callers from using exceptions in\\ntheir own code.\\nA downside of this decision is that our APIs will not be natural or idiomatic\\nfor the [50+%][survey-link] of users who might prefer exceptions for error\\nreporting.\\nChanging existing APIs from throwing exceptions to returning `StatusOr<T>` is a\\nbreaking change. As of this writing (Jan 2019), this project has a [Google\\nCloud Storage][gcs-link] component that is at the Alpha quality level, and a\\n[Google Cloud Bigtable][bigtable-link] that is already at the Beta quality\\nlevel. We plan to immediately change the API for Google Cloud Storage. We have\\nno immediate plans to change"}
{"Context":"## Context\\nWe have to decide for a framework for web controller \/ adapter.\\n","Decision":"## Decision\nUtilize [Spring Reactive Web](https:\/\/docs.spring.io\/spring-framework\/docs\/5.0.0.M1\/spring-framework-reference\/html\/web-reactive.html).\\n","Prediction":"## Decision\nWe have decided to use [GWT](https:\/\/gwt.dev\/).\\nGWT is a [JavaScript library](https:\/\/gwt.dev\/docs\/) that provides a Java-like syntax and a rich set of features.\\nIt is a lightweight [JavaScript library](https:\/\/gwt.dev\/docs\/gwt-features.html#lightweight)\\nand is highly optimized for performance.\\nGWT is the [preferred framework](https:\/\/gwt.dev\/docs\/gwt-vs-netty.html#gwt-vs-netty)\\nfor the [Google Web Toolkit](https:\/\/gwt.dev\/docs\/gwt-api.html#google-web-toolkit).\\nGWT is also the recommended framework for the [Google App Engine](https:\/\/cloud.google.com\/appengine\/docs\/java\/gwt\/)\\nplatform.\\nGWT is also one of the [preferred frameworks](https:\/\/gwt.dev\/docs\/gwt-vs-netty.html#gwt-vs-netty) for the [Android](https:\/\/developer.android.com\/guide\/developing\/app-index.html)\\nplatform.\\nGWT is also one of the [preferred frameworks](https:\/\/gwt.dev\/docs\/gwt-vs-netty.html#gwt-vs-netty) for the [iOS](https:\/\/developer.apple"}
{"Context":"## Context\\nYetibot currently resides across two primary repos:\\n- github.com\/yetibot\/yetibot\\n- github.com\/yetibot\/yetibot-core\\nThese code bases continue to grow in size, and consist of a diverse range of\\nfeatures, many of which many users won't care to use.\\n","Decision":"## Decision\nSwitching to a plugin system allows us to split up the code base into much more\\nfine grained, logical units. For example, we may split the `github` command into\\nits own plugin.\\nThe first plugin is [yetibot-kroki](https:\/\/github.com\/yetibot\/yetibot-kroki).\\nWe will continue to extract plugins from both of the above code bases.\\n","Prediction":"## Decision\nWe will separate the code base into two separate repos: one for the core bot and one for the\\nyetibot library. This will allow us to track and manage changes to the code base\\nseparately, and allow us to easily track and manage the performance of each\\nbot.\\nWe will use the following naming convention for the code base: `yetibot-core`\\n`yetibot` will be a reference to the yetibot library.\\n`yetibot-library` will be a reference to the core bot.\\n`yetibot-library` will be a reference to the yetibot library.\\n`yetibot-core` will be a reference to the yetibot library.\\n`yetibot` will be a reference to the yetibot library.\\n`yetibot-library` will be a reference to the core bot.\\n`yetibot` will be a reference to the yetibot library.\\n`yetibot-core` will be a reference to the yetibot library.\\n`yetibot` will be a reference to the yetibot library.\\n`yetibot-core` will be a reference to the yetibot library.\\n`yet"}
{"Context":"## Context\\nThis application will have severals functionnalities on severals domains. They must be independent to increase maintainability.\\n","Decision":"## Decision\nI decided to choose the modular monolith for some reasons:\\n- The time to develop is shorter than microservices\\n- It can evolve to microservices in the future\\n- The [Modular monolith DDD](https:\/\/github.com\/kgrzybek\/modular-monolith-with-ddd) project is a great inspiration for this approach\\n","Prediction":"## Decision\nWe will use microservices architecture, with the following features:\\n* Each domain will have a dedicated application server with its own database, and it will be the only application to access the database\\n* All services will be decoupled, and they will be able to communicate with each other\\n* Each service will have a web interface\\n* Each service will have a private API to communicate with each other\\n* Each service will have a private database to store its state\\n* Each service will have a private memcached to store its data\\n* Each service will have a private memcached to store its secrets\\n* Each service will have a private redis to store its data\\n* Each service will have a private redis to store its secrets\\n* Each service will have a private redis to store its state\\n* Each service will have a private redis to store its routes\\n* Each service will have a private redis to store its secrets\\n* Each service will have a private redis to store its state\\n* Each service will have a private redis to store its routes\\n* Each service will have a private redis to store its secrets\\n* Each service will have a private redis to store its state\\n* Each service will have a private redis to store its routes\\n"}
{"Context":"## Context\\nDepartment for Education have a Cloud Infrastructure Program based on Azure that\\nthey would like digital services to use. Access to Azure is heavily restricted\\nfor production, and slightly restricted for lower environments.\\nWe need to be able to work quickly, particularly in the early stages of this\\nproject.\\nWe need to be able to deploy prototypes and experimental features and versions\\nof the service for user research.\\n","Decision":"## Decision\nWe will use Heroku to deploy the application.\\nWe will use Heroku's pipeline feature to run CI and deploy the application.\\n","Prediction":"## Decision\nWe will use Azure for the Cloud Infrastructure Program.\\nWe will use Azure DevOps for the Continuous Integration and Continuous Delivery\\nof the Cloud Infrastructure Program.\\nWe will use Azure Pipelines for the Continuous Delivery of the Cloud Infrastructure\\nProgram.\\nWe will use Azure Functions for the event-driven service that will be used by\\nthe Cloud Infrastructure Program.\\nWe will use Azure Functions for the event-driven service that will be used by\\nthe Cloud Infrastructure Program.\\nWe will use Azure Functions for the event-driven service that will be used by\\nthe Cloud Infrastructure Program.\\nWe will use Azure Functions for the event-driven service that will be used by\\nthe Cloud Infrastructure Program.\\nWe will use Azure Functions for the event-driven service that will be used by\\nthe Cloud Infrastructure Program.\\nWe will use Azure Functions for the event-driven service that will be used by\\nthe Cloud Infrastructure Program.\\nWe will use Azure Functions for the event-driven service that will be used by\\nthe Cloud Infrastructure Program.\\nWe will use Azure Functions for the event-driven service that will be used by\\nthe Cloud Infrastructure Program.\\nWe will use Azure Functions for the event-driven service that will be used by\\nthe Cloud Infrastructure Program.\\nWe will use Azure Functions"}
{"Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","Prediction":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\nSee [this example project for more information](https:\/\/github.com\/psu\/decision-tree-architecture-examples).\\nSee also [this article on how to write ADRs](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions) for more information.\\nSee also [this blog post on how to use ADRs](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions) for more information.\\nSee also [this article on how to use ADRs](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions) for more information.\\nSee also [this blog post on how to use ADRs](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions) for more information.\\nSee also [this blog post on how to use ADRs](http:\/\/thinkrelevance.com\/blog\/2011\/11\/"}
{"Context":"## Context\\nWe need to make sure our code is written in a standard style for clarity,\\nconsistency across a project and to avoid back and forth between developers\\nabout code style.\\n","Decision":"## Decision\nWe will use [Standard.rb](https:\/\/github.com\/testdouble\/standard) and run the\\nstandard.rb rake task to lint the code as part of the test suite.\\n","Prediction":"## Decision\nWe will follow [Microsoft Style Guide](https:\/\/docs.microsoft.com\/en-us\/cpp\/ast\/coding-style-guide) for\\ncode style.\\nWe will follow [Microsoft Code Style Guide for Libraries](https:\/\/github.com\/Microsoft\/TypeScript\/tree\/develop\/microsoft-csdp) for\\ncode style of our library.\\nWe will use [Microsoft Code Style Guide](https:\/\/docs.microsoft.com\/en-us\/cpp\/ast\/coding-style-guide) for\\ncode style of our project.\\nWe will use [Microsoft Code Style Guide for Libraries](https:\/\/github.com\/Microsoft\/TypeScript\/tree\/develop\/microsoft-csdp) for\\ncode style of our library.\\nWe will use [Microsoft Code Style Guide](https:\/\/docs.microsoft.com\/en-us\/cpp\/ast\/coding-style-guide) for\\ncode style of our project.\\nWe will use [Microsoft Code Style Guide](https:\/\/github.com\/Microsoft\/TypeScript\/tree\/develop\/microsoft-csdp) for\\ncode style of our library.\\nWe will use [Microsoft Code Style Guide](https:\/\/docs.microsoft.com\/en-us\/cpp\/ast\/coding-style-guide) for\\ncode style of our project.\\nWe will use [Microsoft Code Style Guide](https:\/\/github.com"}
{"Context":"## Context\\nWhen application developers wants to interact with Kafkarator, they need an API. We have previously been vague about how that API should look, should it be one CRD, multiple CRDs, piggyback on existing NAIS CRDs etc.\\nWe need to make a decicion, so that we can proceed with detailing how the API looks, and what can be expected from it. It is also needed so that we can actually start implementing Kafkarator in earnest.\\nFrom various discussions, we have a few findings that guide our decision:\\n- Piggybacking on the existing NAIS CRDs are not encouraged and should be avoided\\n- When doing NAIS deploy, it is possible for developers to supply multiple resources to be applied to the cluster\\n- We have two separate concerns that needs two separate configurations\\n","Decision":"## Decision\n- We will define two new CRD objects (see github issues #3 and #12)\\n- App developers will create these in the cluster when deploying their application\\n- Kafkarator will watch these two CRDs and take needed actions\\n","Prediction":"## Decision\nWe will use the existing Kafkarator API and will provide a single API endpoint for the application to interact with.\\nThis API endpoint will be exposed via the Kafkarator API. The details of this endpoint are still to be determined, but will be described in a separate issue.\\nThis API will be used to interact with Kafkarator in a way that is analogous to how the existing API is used in the current NAIS cluster.\\nThe API endpoint will be exposed via the Kafkarator API. The details of this API endpoint are still to be determined, but will be described in a separate issue.\\nThis API will be used to interact with Kafkarator in a way that is analogous to how the existing API is used in the current NAIS cluster.\\nThe API endpoint will be exposed via the Kafkarator API. The details of this API endpoint are still to be determined, but will be described in a separate issue.\\nThis API will be used to interact with Kafkarator in a way that is analogous to how the existing API is used in the current NAIS cluster.\\nThe API endpoint will be exposed via the Kafkarator API. The details of this endpoint are still to"}
{"Context":"## Context\\nIn order to make blockchain transactions, the signing account must possess a sufficient balance of the right denomination\\nin order to pay fees. There are classes of transactions where needing to maintain a wallet with sufficient fees is a\\nbarrier to adoption.\\nFor instance, when proper permissions are setup, someone may temporarily delegate the ability to vote on proposals to\\na \"burner\" account that is stored on a mobile phone with only minimal security.\\nOther use cases include workers tracking items in a supply chain or farmers submitting field data for analytics\\nor compliance purposes.\\nFor all of these use cases, UX would be significantly enhanced by obviating the need for these accounts to always\\nmaintain the appropriate fee balance. This is especially true if we wanted to achieve enterprise adoption for something\\nlike supply chain tracking.\\nWhile one solution would be to have a service that fills up these accounts automatically with the appropriate fees, a better UX\\nwould be provided by allowing these accounts to pull from a common fee pool account with proper spending limits.\\nA single pool would reduce the churn of making lots of small \"fill up\" transactions and also more effectively leverages\\nthe resources of the organization setting up the pool.\\n","Decision":"## Decision\nAs a solution we propose a module, `x\/feegrant` which allows one account, the \"granter\" to grant another account, the \"grantee\"\\nan allowance to spend the granter's account balance for fees within certain well-defined limits.\\nFee allowances are defined by the extensible `FeeAllowanceI` interface:\\n```go\\ntype FeeAllowanceI {\\n\/\/ Accept can use fee payment requested as well as timestamp of the current block\\n\/\/ to determine whether or not to process this. This is checked in\\n\/\/ Keeper.UseGrantedFees and the return values should match how it is handled there.\\n\/\/\\n\/\/ If it returns an error, the fee payment is rejected, otherwise it is accepted.\\n\/\/ The FeeAllowance implementation is expected to update it's internal state\\n\/\/ and will be saved again after an acceptance.\\n\/\/\\n\/\/ If remove is true (regardless of the error), the FeeAllowance will be deleted from storage\\n\/\/ (eg. when it is used up). (See call to RevokeFeeAllowance in Keeper.UseGrantedFees)\\nAccept(ctx sdk.Context, fee sdk.Coins, msgs []sdk.Msg) (remove bool, err error)\\n\/\/ ValidateBasic should evaluate this FeeAllowance for internal consistency.\\n\/\/ Don't allow negative amounts, or negative periods for example.\\nValidateBasic() error\\n}\\n```\\nTwo basic fee allowance types, `BasicAllowance` and `PeriodicAllowance` are defined to support known use cases:\\n```protobuf\\n\/\/ BasicAllowance implements FeeAllowanceI with a one-time grant of tokens\\n\/\/ that optionally expires. The delegatee can use up to SpendLimit to cover fees.\\nmessage BasicAllowance {\\n\/\/ spend_limit specifies the maximum amount of tokens that can be spent\\n\/\/ by this allowance and will be updated as tokens are spent. If it is\\n\/\/ empty, there is no spend limit and any amount of coins can be spent.\\nrepeated cosmos_sdk.v1.Coin spend_limit = 1;\\n\/\/ expiration specifies an optional time when this allowance expires\\ngoogle.protobuf.Timestamp expiration = 2;\\n}\\n\/\/ PeriodicAllowance extends FeeAllowanceI to allow for both a maximum cap,\\n\/\/ as well as a limit per time period.\\nmessage PeriodicAllowance {\\nBasicAllowance basic = 1;\\n\/\/ period specifies the time duration in which period_spend_limit coins can\\n\/\/ be spent before that allowance is reset\\ngoogle.protobuf.Duration period = 2;\\n\/\/ period_spend_limit specifies the maximum number of coins that can be spent\\n\/\/ in the period\\nrepeated cosmos_sdk.v1.Coin period_spend_limit = 3;\\n\/\/ period_can_spend is the number of coins left to be spent before the period_reset time\\nrepeated cosmos_sdk.v1.Coin period_can_spend = 4;\\n\/\/ period_reset is the time at which this period resets and a new one begins,\\n\/\/ it is calculated from the start time of the first transaction after the\\n\/\/ last period ended\\ngoogle.protobuf.Timestamp period_reset = 5;\\n}\\n```\\nAllowances can be granted and revoked using `MsgGrantAllowance` and `MsgRevokeAllowance`:\\n```protobuf\\n\/\/ MsgGrantAllowance adds permission for Grantee to spend up to Allowance\\n\/\/ of fees from the account of Granter.\\nmessage MsgGrantAllowance {\\nstring granter = 1;\\nstring grantee = 2;\\ngoogle.protobuf.Any allowance = 3;\\n}\\n\/\/ MsgRevokeAllowance removes any existing FeeAllowance from Granter to Grantee.\\nmessage MsgRevokeAllowance {\\nstring granter = 1;\\nstring grantee = 2;\\n}\\n```\\nIn order to use allowances in transactions, we add a new field `granter` to the transaction `Fee` type:\\n```protobuf\\npackage cosmos.tx.v1beta1;\\nmessage Fee {\\nrepeated cosmos.base.v1beta1.Coin amount = 1;\\nuint64 gas_limit = 2;\\nstring payer = 3;\\nstring granter = 4;\\n}\\n```\\n`granter` must either be left empty or must correspond to an account which has granted\\na fee allowance to fee payer (either the first signer or the value of the `payer` field).\\nA new `AnteDecorator` named `DeductGrantedFeeDecorator` will be created in order to process transactions with `fee_payer`\\nset and correctly deduct fees based on fee allowances.\\n","Prediction":"## Decision\nWe will move to a fee account pool for all blockchain transactions. This will allow users to pull from a common fee\\naccount with spending limits.\\nThis account will be managed by the blockchain service and will be managed by a single user.\\nThis user will be responsible for keeping the fee account in good standing, and will be responsible for\\nensuring that the right balance of funds is always maintained. The account will be managed using a\\nsingle sign-on mechanism to the blockchain service.\\nThis fee account will be managed in a manner similar to a credit card account. It will have a\\ndefault balance, a spending limit, and a withdrawal limit. These\\nparameters will be defined by the organization setting up the fee account.\\nEach time a transaction is submitted, the blockchain service will verify that the fee account balance is\\nsufficient to pay the fees and will issue a transaction to the account owner.\\nIf the fee account balance is insufficient, the blockchain service will attempt to recover the transaction fees\\nand\/or withdraw the funds from the fee account. If the transaction is successful, the funds will be\\ndeposited into the fee account. If the transaction is not successful, the fee account will be returned to the\\nbalance.\\nIf"}
{"Context":"## Context\\n* As per ADR-007\\n* Clucy takes over an hour to process the index\\n","Decision":"## Decision\n* We will use ElasticSearch as a frontend to Lucene.\\n","Prediction":"## Decision\n* Clucy will be used as the indexer\\n* Clucy will be responsible for indexing the data\\n* Clucy will be responsible for creating and maintaining the index\\n* Clucy will be responsible for uploading the index to S3\\n* Clucy will be responsible for maintaining the index\\n* Clucy will be responsible for managing the lifecycle of the index\\n* Clucy will be responsible for managing the lifecycle of the database\\n* Clucy will be responsible for managing the lifecycle of the application\\n* Clucy will be responsible for managing the lifecycle of the application\\n* Clucy will be responsible for managing the lifecycle of the application\\n* Clucy will be responsible for managing the lifecycle of the application\\n* Clucy will be responsible for managing the lifecycle of the application\\n* Clucy will be responsible for managing the lifecycle of the application\\n* Clucy will be responsible for managing the lifecycle of the application\\n* Clucy will be responsible for managing the lifecycle of the application\\n* Clucy will be responsible for managing the lifecycle of the application\\n* Clucy will be responsible for managing the lifecycle of the application\\n* Clucy will be responsible for managing"}
{"Context":"## Context\\nTo speed up the prototype process it would be beneficial for the prototype kit to have generic styles available for consumers of the kit to use. This would also encourage more unified styling of prototypes and services to keep a feel of consistency across products.\\n","Decision":"## Decision\nWe will use the frontend library, available on the NHSUK GitHub, as a dependency. Using NPM (Node Package Manager) to pull in the contents of the repo and Gulp to move the files to the correct location and compile the assets. The version of the frontend library can be updated by changing the version number at the end of the following dependency in `package.json`:\\n```\\n\"frontend-library\": \"git:\/\/github.com\/nhsuk\/frontend-library.git#0.6.0\"\\n```\\n","Prediction":"## Decision\nWe will use the [ui-kit](https:\/\/github.com\/ivy\/react-ui\/tree\/master\/packages\/ui-kit) for the template engine and the [react-native-styled-components](https:\/\/github.com\/facebook\/react-native-styled-components) for the component library.\\nWe will use the [react-native-styled-components](https:\/\/github.com\/facebook\/react-native-styled-components) component library for the components we need to use.\\nWe will use [react-native-styled-components](https:\/\/github.com\/facebook\/react-native-styled-components) for the styles we need to apply to our components.\\nWe will use [react-native-styled-components](https:\/\/github.com\/facebook\/react-native-styled-components) for the components we need to use.\\nWe will use [react-native-styled-components](https:\/\/github.com\/facebook\/react-native-styled-components) for the components we need to use.\\nWe will use [react-native-styled-components](https:\/\/github.com\/facebook\/react-native-styled-components) for the components we need to use.\\nWe will use [react-native-styled-components](https:\/\/github.com\/facebook\/react-native-styled-components) for the components we need to use.\\nWe will use [react-native-styled-components](https:\/\/github.com\/facebook"}
{"Context":"## Context and Problem Statement\\nWe want to have a Source-to-Image workflow to import services based on a GitHub repository. It should run inside our Kubernetes cluster, however currently Kubernetes doesn't have a resource build-in that is able to build container images. Therefore another technology is required.\\n## Decision Drivers\\n* MUST run on our Kubernetes cluster\\n* MUST run completely in userspace (no root access required)\\n* MUST be sufficient to provide a single URL to a GitHub repository (with included Dockerfile)\\n* SHOULD be independent of any cloud service provider\\n","Decision":"## Decision\n* MUST run on our Kubernetes cluster\\n* MUST run completely in userspace (no root access required)\\n* MUST be sufficient to provide a single URL to a GitHub repository (with included Dockerfile)\\n* SHOULD be independent of any cloud service provider\\nChosen option: *Knative Build*, because it meets all of our criterion decision drivers. It allows us to implement a Source-to-Image workflow on our Kubernetes cluster independently to any cloud service provider.\\n### Positive Consequences\\n* By using *Knative Build* we have the choice to use different kinds of `Builders`, follow-up decision is required: [Building OCI images](.\/0015-building-oci-images.md)\\n### Negative consequences\\n* *Nothing known*\\n","Prediction":"## Decision\n* MUST run on our Kubernetes cluster\\n* MUST run completely in userspace (no root access required)\\n* MUST be sufficient to provide a single URL to a GitHub repository (with included Dockerfile)\\n* SHOULD be independent of any cloud service provider\\nChosen option: \"Curl\", because it is easy to configure and it supports a single URL to a GitHub repository.\\n### Positive Consequences\\n* We don't need to provision any infrastructure or services\\n* We can provide a single URL to a GitHub repository that can be used by any Kubernetes provider and any other tool\\n* It will be possible to run the Source-to-Image workflow in userspace\\n### Negative Consequences\\n* It will be necessary to provide a public API to the Source-to-Image workflow\\n* It will be necessary to provide a public interface for the Source-to-Image workflow\\n* It will be necessary to provide a public interface for the Kubernetes provider\\n* It will be necessary to provide a public interface for the other tools and services that will be able to use the Source-to-Image workflow\\n* It will be necessary to provide a public interface for the Source-to-Image workflow\\n* It will be necessary to provide a public interface for the Kubernetes provider"}
{"Context":"## Context\\nUnlike in the case of Mediawiki extensions, where the extension code is packaged as a tarball to be installed in end user's environment, there is no immediate idea of how to package the Query Service (in particular as it actually involves a number of software components). Components included [Query Service backend] (including the WMF Blazegraph fork) and [Query Service UI]. Should these be bundled as separate tarballs? Or as a single one?\\n[Query Service backend] releases is currently [built and published](https:\/\/gerrit.wikimedia.org\/r\/plugins\/gitiles\/wikidata\/query\/rdf\/+\/refs\/heads\/master\/dist\/) as a [service zip archive] which contains the necessary components including the built-in UI of blazegraph.\\nThe [Query Service UI] has at one point been part of this build process but the two components have recently been [separated](https:\/\/phabricator.wikimedia.org\/T241291).\\n","Decision":"## Decision\nAs the [Query Service backend] does not depend on the [Query Service UI] and they are currently two separate components there is no need to alter the structure of the [service zip archive] as no obvious benefit of doing so can be identified.\\n","Prediction":"## Decision\nThe [Query Service UI] will be packaged as a single tarball, with the [Query Service backend](https:\/\/gerrit.wikimedia.org\/r\/plugins\/gitiles\/wikidata\/query\/rdf\/) and [Query Service UI] as the dependencies. This tarball will be used to install the service.\\nThe [Query Service backend](https:\/\/gerrit.wikimedia.org\/r\/plugins\/gitiles\/wikidata\/query\/rdf\/) will be packaged as a single tarball, using the [Query Service backend](https:\/\/gerrit.wikimedia.org\/r\/plugins\/gitiles\/wikidata\/query\/rdf\/) as a dependency. This tarball will be used to install the service.\\nThe [Query Service UI] will be packaged as a single tarball, using the [Query Service backend](https:\/\/gerrit.wikimedia.org\/r\/plugins\/gitiles\/wikidata\/query\/rdf\/) as a dependency. This tarball will be used to install the service.\\nThe [Query Service UI] will be packaged as a single tarball, using the [Query Service backend](https:\/\/gerrit.wikimedia.org\/r\/plugins\/gitiles\/wikidata\/query\/rdf\/) as a dependency. This tarball will be used to install the service.\\n"}
{"Context":"Context\\n=======\\nWe will only [serve HTTPS traffic, keeping TCP port 80 (HTTP) closed and use HSTS preload lists](ADR443-ssl-only-for-applications-and-cf-endpoints.md).\\nTo add our domains to [HSTS preload lists](https:\/\/hstspreload.appspot.com\/), there are these requirements:\\n1. Serve a valid certificate.\\n2. Redirect from HTTP to HTTPS on the same host.\\n3. Serve all subdomains over HTTPS (actually checks for `www.domain.com`)\\n4. Serve an HSTS header on the base domain for HTTPS requests:\\nWe need an endpoint to provide these requirements.\\nOur Cloud Foundry app endpoint already [serves the\\nright HSTS Security header with HAProxy](ADR008-haproxy-for-request-rewriting.md)\\nand could be configured to serve the additional `preload` and `includeSubDomains` flags,\\nbut we cannot use it because we keep port 80 (HTTP) closed for this endpoint.\\nWe can implement a second ELB to listening on HTTP and HTTPS and use\\nHAProxy to do the HTTP to HTTPS redirect and serve the right header.\\nBut this increases our dependency on the HAProxy service.\\nWe must serve from the root domain (or apex domain), but it is not allowed to\\nserve [CNAME records in the root\/apex domain](http:\/\/serverfault.com\/questions\/613829\/why-cant-a-cname-record-be-used-at-the-apex-aka-root-of-a-domain). We must configure A records in this domain. This can be\\nan issue when serving the service using ELB or CloudFront.\\nDecision\\n========\\n* We will implement a basic [AWS API Gateway](https:\/\/aws.amazon.com\/api-gateway\/)\\nwith a default [MOCK response](https:\/\/aws.amazon.com\/about-aws\/whats-new\/2015\/09\/introducing-mock-integration-generate-api-responses-from-api-gateway-directly\/)\\nthat returns the right HTTP header `Strict-Transport-Security`. The actual\\ncontent of the response is irrelevant, it can be a 302.\\nA [Custom Domain Name](http:\/\/docs.aws.amazon.com\/apigateway\/latest\/developerguide\/how-to-custom-domains.html),\\nwhich creates a [AWS Cloud Front distribution](http:\/\/docs.aws.amazon.com\/AmazonCloudFront\/latest\/DeveloperGuide\/distribution-overview.html),\\nwill provide public access to this API.\\n* We will use [AWS Route 53 `ALIAS` resource record](http:\/\/docs.aws.amazon.com\/Route53\/latest\/APIReference\/CreateAliasRRSAPI.html)\\nto [serve the IPs of the AWS Cloud Front distribution as A records](http:\/\/docs.aws.amazon.com\/Route53\/latest\/DeveloperGuide\/routing-to-cloudfront-distribution.html).\\nStatus\\n======\\nAccepted\\nConsequences\\n============\\nTo setup AWS API Gateway Domain Names, it is required access to the SSL certificates. There is the option of uploading the certificates in a different step and create the AWS Cloud Front distribution manually.\\n","Decision":"## Decision\n========\\n* We will implement a basic [AWS API Gateway](https:\/\/aws.amazon.com\/api-gateway\/)\\nwith a default [MOCK response](https:\/\/aws.amazon.com\/about-aws\/whats-new\/2015\/09\/introducing-mock-integration-generate-api-responses-from-api-gateway-directly\/)\\nthat returns the right HTTP header `Strict-Transport-Security`. The actual\\ncontent of the response is irrelevant, it can be a 302.\\nA [Custom Domain Name](http:\/\/docs.aws.amazon.com\/apigateway\/latest\/developerguide\/how-to-custom-domains.html),\\nwhich creates a [AWS Cloud Front distribution](http:\/\/docs.aws.amazon.com\/AmazonCloudFront\/latest\/DeveloperGuide\/distribution-overview.html),\\nwill provide public access to this API.\\n* We will use [AWS Route 53 `ALIAS` resource record](http:\/\/docs.aws.amazon.com\/Route53\/latest\/APIReference\/CreateAliasRRSAPI.html)\\nto [serve the IPs of the AWS Cloud Front distribution as A records](http:\/\/docs.aws.amazon.com\/Route53\/latest\/DeveloperGuide\/routing-to-cloudfront-distribution.html).\\nStatus\\n======\\nAccepted\\nConsequences\\n============\\nTo setup AWS API Gateway Domain Names, it is required access to the SSL certificates. There is the option of uploading the certificates in a different step and create the AWS Cloud Front distribution manually.\\n","Prediction":"## Decision\n========\\n* We will implement a basic [AWS API Gateway](https:\/\/aws.amazon.com\/api-gateway\/)\\nwith a default [MOCK response](https:\/\/aws.amazon.com\/about-aws\/whats-new\/2015\/09\/introducing-mock-integration-generate-api-responses-from-api-gateway-directly\/)\\nthat returns the right HTTP header `Strict-Transport-Security`. The actual\\ncontent of the response is irrelevant, it can be a 302.\\nA [Custom Domain Name](http:\/\/docs.aws.amazon.com\/apigateway\/latest\/developerguide\/how-to-custom-domains.html),\\nwhich creates a [AWS Cloud Front distribution](http:\/\/docs.aws.amazon.com\/AmazonCloudFront\/latest\/DeveloperGuide\/distribution-overview.html),\\nwill provide public access to this API.\\n* We will use [AWS Route 53 `ALIAS` resource record](http:\/\/docs.aws.amazon.com\/Route53\/latest\/APIReference\/CreateAliasRRSAPI.html)\\nto [serve the IPs of the AWS Cloud Front distribution as A records](http:\/\/docs.aws.amazon.com\/Route53\/latest\/DeveloperGuide\/routing-to-cloudfront-distribution.html).\\nStatus\\n======\\nAccepted\\nConsequences"}
{"Context":"## Context\\nWe need something better than ProtoBuf\\n","Decision":"## Decision\nWe checked the Cap'n'Proto documentation and examples on RPC.\\nSeems to be a perfect match for our needs, so we started integrating it to our SDK for communication between client\/server, replacing the previous ProtoBuf.\\n","Prediction":"## Decision\nWe will use [Golang](https:\/\/golang.org\/) instead of ProtoBuf\\nGolang is a modern, fast, and safe language with high-level support for concurrency and networking, and it has a large ecosystem of libraries to make it easier to work with\\nGolang is used by the majority of the Google technology stack, including Google Cloud, Google Kubernetes Engine, Google Dataflow, and Google Cloud Pub\/Sub\\nGolang is used by the majority of the Go ecosystem\\nGolang has a large ecosystem of libraries to make it easier to work with\\nGolang is used by the majority of the Google technology stack, including Google Cloud, Google Kubernetes Engine, Google Dataflow, and Google Cloud Pub\/Sub\\nGolang is used by the majority of the Go ecosystem\\nGolang is used by the majority of the Go ecosystem\\nGolang is used by the majority of the Go ecosystem\\nGolang is used by the majority of the Go ecosystem\\nGolang is used by the majority of the Go ecosystem\\nGolang is used by the majority of the Go ecosystem\\nGolang is used by the majority of the Go ecosystem\\nGolang is used by the majority of the Go ecosystem\\nGolang is used by the majority of the Go ecosystem\\nG"}
{"Context":"## Context and Problem Statement\\nUsing acs-info inside our standard and component modules is creating pain. Does it make more sense to expect products to pass the necessary variables in, allowing them to use acs-info if they want?\\n## Decision Drivers <!-- optional -->\\n* Terraform's lack of optimization with modules and data sources\\n* Usage of modules in non-OIT accounts\\n* We want our modules to be simple to use\\n* We want development teams to understand the infrastructure of their products.\\n","Decision":"## Decision\n* Terraform's lack of optimization with modules and data sources\\n* Usage of modules in non-OIT accounts\\n* We want our modules to be simple to use\\n* We want development teams to understand the infrastructure of their products.\\nChosen option: \"Pull acs-info out of modules\", because it was the best option based on an analysis of pros and cons (see below).\\n<!-- ### Positive Consequences optional -->\\n<!-- * TBD -->\\n<!-- ### Negative Consequences optional -->\\n<!-- * TBD -->\\n","Prediction":"## Decision\n* Terraform's lack of optimization with modules and data sources\\n* Usage of modules in non-OIT accounts\\n* We want our modules to be simple to use\\n* We want development teams to understand the infrastructure of their products.\\nChosen option: \"Modules are not supported by acs-info\", because it's the simplest option for now. If we do decide to support modules in the future, we'll need to consider how we'd support different data sources.\\n### Positive Consequences <!-- optional -->\\n* Modules are supported by acs-info\\n* We can continue to use modules in our standard and component modules\\n* We can continue to support modules in the future\\n* We can support modules in non-OIT products if we need to.\\n### Negative Consequences <!-- optional -->\\n* We need to support modules in the future\\n* We need to support modules in non-OIT products if we need to.\\n* We need to consider how to support modules in the future\\n* We need to consider how to support modules in non-OIT products if we need to.\\n* We need to implement modules in the future\\n* We need to implement modules in the standard and component modules\\n* We need to implement modules"}
{"Context":"## Context\\nAs it's hard to understand code, it is crucial that anybody can easily\\nunderstand the code you're working on. This applies to all levels of code, not\\nonly the code itself, but groups of code, complete applications and groups of\\napplications.\\n","Decision":"## Decision\nI will always focus on simplicity, readaility and the ease of understanding\\nany code or structure.\\n","Prediction":"## Decision\nUse [Linting](https:\/\/eslint.org\/)\\nto check for common coding errors and warnings.\\nCheck for common errors and warnings before submitting code to the project.\\nCheck for common errors and warnings during development.\\nUse [Code Quality](https:\/\/www.codacy.com\/) to check code quality.\\nUse [Code Style](https:\/\/www.codestyle.io\/) to check code style.\\nUse [Code Formatting](https:\/\/eslint.org\/docs\/handbook\/code-formatting) to\\nformat your code correctly.\\nUse [Code Style Guide](https:\/\/github.com\/bradleyb\/awesome-code\/blob\/master\/docs\/awesome-code.md) to\\nprovide guidelines for formatting code.\\nUse [Code Style Rules](https:\/\/github.com\/bradleyb\/awesome-code\/blob\/master\/docs\/awesome-code.md#code-style-rules) to\\nprovide guidelines for formatting code.\\nUse [Code Style Conventions](https:\/\/github.com\/bradleyb\/awesome-code\/blob\/master\/docs\/awesome-code.md#code-style-conventions) to\\nprovide guidelines for formatting code.\\nUse [Code Style Rules](https:\/\/github.com\/bradleyb\/awesome-code\/blob\/master\/docs\/awesome-code.md#code-style-rules"}
{"Context":"## Context\\nWe want to use feature flags in the service to allow us to easily enable and disable functionality. Feature flags should be easy to change, audited and able to take effect immediately.\\n","Decision":"## Decision\nWe will use AWS Parameter Store to store our feature flags in a standardised format: `\/{environment}\/flag\/{flagName}`. This will make flags easy to identify, change and debug. Operators will have access to change the value of these flags.\\nResources inside our service can access feature flags either by having them passed in as environment variables, or by directly querying Parameter Store on-demand.\\nParameter Store values must be strings, so we will consistently use the values `0` (off) and `1` (on).\\n","Prediction":"## Decision\nWe will use feature flags to allow us to easily enable and disable functionality.\\nWe will use a separate service for feature flags to allow us to easily test and deploy changes to the service.\\nWe will use feature flags to allow us to easily audit changes to the service.\\nWe will use feature flags to allow us to easily take effect immediately.\\nWe will use feature flags to allow us to easily take effect immediately.\\nWe will use feature flags to allow us to easily take effect immediately.\\nWe will use feature flags to allow us to easily take effect immediately.\\nWe will use feature flags to allow us to easily take effect immediately.\\nWe will use feature flags to allow us to easily take effect immediately.\\nWe will use feature flags to allow us to easily take effect immediately.\\nWe will use feature flags to allow us to easily take effect immediately.\\nWe will use feature flags to allow us to easily take effect immediately.\\nWe will use feature flags to allow us to easily take effect immediately.\\nWe will use feature flags to allow us to easily take effect immediately.\\nWe will use feature flags to allow us to easily take effect immediately.\\nWe will use feature flags to allow us to easily take effect immediately.\\nWe will"}
{"Context":"## Context\\nThe REST API for Azure Search requires the API Version identifier to be passed as the query parameter `api-version` for all calls<sup>[1]<\/sup>.\\nAt the time of writing the General Available (GA) version is '2017-11-11' and the Preview version is '2017-11-11-Preview'. Attempting to retrieve an index definition for an index which includes Preview features with a query using the GA version in the query parameter `api-version` results in a HTTP error.\\nAs a result of this we need to support the parametrisation and validation of the search API version. Validation could be against values held in environment variables or against config values within the code itself.\\n","Decision":"## Decision\n* In order to maintain backwards compatibility the API Version identifier is an optional property in the payload. The default value is the current GA version.\\n* The validation will be based on config values **_**within**_** the code to check the api version parameter against a range of allowed values. The reason we have adopted this approach over using environment variables is that the code has been written to support specific versions of the API and therefore the responsibility for establishing this relationship rests with the code not the environment.\\n","Prediction":"## Decision\nWe will use the `api-version` query parameter for the search API version.\\nWe will support the parametrisation of the search API version using environment variables or config values.\\nWe will use the `api-version` query parameter to validate the search API version parametrisation against the environment variables and config values held in Azure Search.\\nWe will use the `api-version` query parameter to allow for parametrisation of the search API version against a config value.\\nWe will use the `api-version` query parameter to allow for parametrisation of the search API version against an environment variable.\\nWe will use the `api-version` query parameter to allow for parametrisation of the search API version against a config value.\\nWe will use the `api-version` query parameter to allow for parametrisation of the search API version against an environment variable.\\nWe will use the `api-version` query parameter to allow for parametrisation of the search API version against an environment variable.\\nWe will use the `api-version` query parameter to allow for parametrisation of the search API version against an environment variable.\\nWe will use the `api-version` query parameter to allow for param"}
{"Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","Prediction":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\nWe will use the [Architecture Decision Suite](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions-architecture-decision-suite) for our architecture decision records.\\nWe will use the [Architecture Decision Framework](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions-architecture-decision-framework) for our architecture decision records.\\nWe will use the [Architecture Decision Toolkit](http:\/\/thinkrelevance.com\/blog\/2012\/01\/16\/documenting-architecture-decisions-architecture-decision-toolkit) for our architecture decision records.\\nWe will use the [Architecture Decision Toolkit](http:\/\/thinkrelevance.com\/blog\/2012\/01\/16\/documenting-architecture-decisions-architecture-decision-toolkit) for our architecture decision records.\\nWe will use the [Architecture Decision Framework](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions-architecture-decision-framework) for our architecture decision"}
{"Context":"## Context\\nPython 3 added support for static type checking (see: https:\/\/docs.python.org\/3\/library\/typing.html). Type hinting is not an all or nothing thing, and can be applied in a progressive manner. It's also worth noting that type checks are not applied at runtime.\\n","Decision":"## Decision\nWe will use type hinting. Our focus should be on type hints for function arguments and return values, and not aim for full coverage.\\n","Prediction":"## Decision\nPython 3 will add support for static type checking to all files in the project. The static type checks will be applied at build time.\\n### How it will work\\nStatic type checking will be applied to all files in the project. The static type checks will be applied at build time.\\n### How it will work\\nStatic type checking will be applied to all files in the project. The static type checks will be applied at build time.\\n### How it will work\\nStatic type checking will be applied to all files in the project. The static type checks will be applied at build time.\\n### How it will work\\nStatic type checking will be applied to all files in the project. The static type checks will be applied at build time.\\n### How it will work\\nStatic type checking will be applied to all files in the project. The static type checks will be applied at build time.\\n### How it will work\\nStatic type checking will be applied to all files in the project. The static type checks will be applied at build time.\\n### How it will work\\nStatic type checking will be applied to all files in the project. The static type checks will be applied at build time.\\n### How it will work\\nStatic"}
{"Context":"## Context\\n### Legacy Sweep\\nTo achieve its transactional guarantees, AtlasDB maintains historical versions of cells that have been written to. Eventually,\\nall existing and future transactions will have a start timestamp large enough that some of these historical versions will never be\\nvisible again, resulting in unnecessary cruft. This is not only an issue due to taking up storage space in the underlying KVS, but\\nalso because certain access patterns require scanning over the obsolete historic versions, leading to significant performance\\ndegradation over time. The process of removing old historic versions of cells from tables in AtlasDB is called **Sweep**.\\nWe will refer to the current implementation of sweep that AtlasDB is relying on as **Legacy Sweep**. Legacy sweep is an iterative\\nprocedure that, given a table reference and a start row, sequentially scans through all the historic versions of each cell searching\\nfor candidates written in transactions with a start timestamp and a commit timestamp both lower than the **Sweep Timestamp**. The\\nsweep timestamp is a timestamp expected to be lower than the start timestamp of all open transactions, rendering all but the last\\nhistoric version for each cell prior to the sweep timestamp effectively invisible and thus obsolete. These versions can therefore be\\nsafely deleted.\\nLegacy sweep continues these scans until enough candidates have been found, processing at least one full row of cells, and\\nthen deletes each of the obsolete historic versions. There are two main modes for running legacy sweep:\\n1. Background Sweep, which is a background task that repeatedly chooses a table to sweep and then proceeds to run iterations\\nof sweep until all the rows have been processed.\\n2. Manual Sweep, which can be triggered through a REST endpoint or a CLI to perform an iteration or a full sweep for a given table\\nand start row.\\nNote that there are corner cases where the sweep timestamp may be mistaken: a read-only transaction that has been running\\nfor longer than one hour (which is also subject to clock drift), or when a write transaction loses its lock while committing. In\\ntables that allow read-only transactions we must therefore defensively write a **garbage deletion sentinel**, which is an empty\\nvalue at a timestamp of -1, i.e., before any valid start timestamp. If a read-only transaction encounters a sentinel, that\\nsignalizes there could have been a historic version it should be able to read that may have been deleted, and the transaction must\\ntherefore abort. If a table allows read-only transactions, and therefore requires sentinels, is defined by the table's\\n**sweep strategy**: `CONSERVATIVE`, which allows read-only transactions and requires sentinels, and `THOROUGH`, which does not.\\nOver time we have identified a number of issues with the architecture and implementation of legacy sweep as outlined below.\\n#### Legacy Sweep is Slow\\nEven if a table has no historic versions of cells that can be swept, we still have to scan through all historic versions of\\nall cells in the table, which can take weeks in extreme cases. The performance only gets worse as more data is written to the\\nKVS, increasing the number of entries legacy sweep must iterate through. This is a particularly large problem for users with tables\\nwhose access patterns mandate that they must be swept regularly for performance or stability reasons. If background sweep is busy\\nfor weeks sweeping some other large table, we must resort to manual sweeps to avoid performance and stability degradation. This is\\nobviously error-prone and subject to random failures, since if the manual sweep is interrupted for any reason, it will not be\\nautomatically retried.\\nMoreover, legacy sweep depends on complicated heuristics to decide which table to sweep next. Given that some tables must be swept\\nfrequently, and the slowness of legacy sweep, significant developer time must be spent tweaking the heuristic to produce the desired\\neffect.\\n#### Legacy Sweep can Exert Significant Pressure on the Underlying KVS\\nScanning through a table to find historic versions of cells can in some cases cause significant pressure on the underlying KVS,\\nin particular for Cassandra. Even though we are only interested in the start and commit timestamp of the transaction in which the\\nwrites were performed, it is our understanding that Cassandra internally still loads the contents of the cell in memory regardless.\\n#### Legacy Sweep can Get Stuck\\nFor tables that regularly have new rows added in increasing lexicographical order (for example, tables keyed on steadily  increasing\\n`FIXED` or `VAR_LONG` s), legacy sweep can end up sweeping the table indefinitely as each iteration will discover a new row, therefore\\nnever declaring the table as fully swept. As a consequence, no other tables are swept at all until the issue is noticed and manually\\nresolved.\\n","Decision":"## Decision\nThe problems of legacy sweep are architectural, and cannot be solved by simply improving the implementation.\\nWe have therefore decided to change the architecture of sweep so that it does not require scanning of tables to find candidates\\nto delete, and instead maintains a **Sweep Queue** which contains the information on all the writes into AtlasDB. This, in\\nconjunction with **ranged tombstones** (ranged deletions), which delete all versions of a cell between two timestamps, allows us to sweep all tables\\nin parallel without having to read data from the tables being swept. *Note that, at the time of writing this ADR, only the Cassandra\\nKVS implementation of ranged tombstones actually avoids reading from the table*.\\n### Targeted Sweep using a Targeted Sweep Queue\\nThe **targeted sweep queue** is a persisted queue containing the relevant metadata for each write that was about to be committed into AtlasDB.\\nThis metadata is encapsulated in a `WriteInfo` object and contains the following:\\n- `TableReference` of the table written to.\\n- `Cell` written to, i.e., the row and column names.\\n- A **start timestamp** of the transaction that performed the write.\\n- A flag specifying if the write was a **tombstone**, i.e., a delete, or a regular write.\\nOn a high level, sweeping using the targeted sweep queue, i.e., **targeted sweep** is as follows:\\n1. Whenever a transaction is about to commit, before any of its writes are persisted to the KVS, for each of the writes put a\\ncorresponding `WriteInfo` into the queue.\\n2. Targeted sweep reads an entry from the front of the queue. Depending on the sweep strategy for the table specified by the entry, we\\nacquire an appropriate *sweep timestamp*, and compare it to the *start timestamp* of the transaction.\\n- If *sweep timestamp <= start timestamp*, we must pause and try later.\\n3. Check the *commit timestamp* of the transaction.\\n- If the transaction is not committed, abort it.\\n- If the transaction is aborted, delete the write from the KVS and pop the queue and read the next entry.\\n- If the transaction is committed at a timestamp greater or equal to the *sweep timestamp*, pause and try later.\\n4. Otherwise, insert a ranged tombstone as follows:\\n- If the strategy is conservative: write a garbage deletion sentinel, then put a ranged tombstone deleting all versions of the\\ncell between 0 and write timestamp - 1, not deleting the sentinel or the write.\\n- If the strategy is thorough\\n- If the write was a tombstone, then put a ranged tombstone deleting all versions of that cell between -1 and write timestamp,\\ndeleting both a potentially existing sentinel and the write.\\n- Otherwise, put a ranged tombstone deleting all versions of that cell between -1 and write timestamp - 1, deleting a\\npotentially existing sentinel, but not the write.\\n5. Pop the queue and read the next entry.\\nFor a detailed implementation of targeted sweep and the targeted sweep queue, refer to the implementation section below.\\n","Prediction":"## Decision\nWe will replace the current implementation of legacy sweep with a new, more performant, implementation.\\nWe will provide a CLI option to trigger a background sweep that will perform a sweep of the table and start the sweep task in the background.\\nWe will provide a CLI option to trigger a manual sweep that will perform a sweep of the table and start the sweep task in the background.\\nWe will provide a CLI option to trigger a background sweep that will perform a sweep of the table, start the sweep task in the background, and\\nstart the sweep task in the background.\\nWe will provide a CLI option to trigger a manual sweep that will perform a sweep of the table, start the sweep task in the background, and\\nstart the sweep task in the background.\\nWe will provide a CLI option to trigger a manual sweep that will perform a sweep of the table and start the sweep task in the background.\\nWe will provide a CLI option to trigger a manual sweep that will perform a sweep of the table and start the sweep task in the background.\\nWe will provide a CLI option to trigger a manual sweep that will perform a sweep of the table and start the sweep task in the background.\\nWe will provide a CLI option to trigger a manual"}
{"Context":"## Context and Problem Statement\\nWith the Fundraising App split into two repositories (see [ADR\\n017](.\/017_Split_Repositories.md)), we need to deploy two specific\\nbranches of the repositories to test and production. We need a mechanism\\nor process to define which two branches should be deployed together.\\n## Decision Drivers\\n* **Speed**: Currently, the deployment playbook builds the assets of the\\nfrontend branch from scratch. If we could use pre-built assets, the deployment\\nwould be faster.\\n* **Developer experience**: The app won't work as expected or represent\\nthe required test state if we deploy the wrong branches together. Our\\nsolution should prevent such an \"out of sync\" scenario as good as\\npossible while at the same time not requiring too many manual steps from\\nthe developers.\\n* **Traceability**: We should be able to check which branches were used\\nfor a deployment.\\n","Decision":"## Decision\n* **Speed**: Currently, the deployment playbook builds the assets of the\\nfrontend branch from scratch. If we could use pre-built assets, the deployment\\nwould be faster.\\n* **Developer experience**: The app won't work as expected or represent\\nthe required test state if we deploy the wrong branches together. Our\\nsolution should prevent such an \"out of sync\" scenario as good as\\npossible while at the same time not requiring too many manual steps from\\nthe developers.\\n* **Traceability**: We should be able to check which branches were used\\nfor a deployment.\\nChosen option: \"Decide at deploy time\", because it's the least amount of\\ninitial effort and the least amount of ongoing effort for each client-side change.\\nWe accept the additional risks of making mistakes at deploy time. If we\\nmake too many mistakes, we'll develop checks to mitigate them.\\n","Prediction":"## Decision\n* **Speed**: Currently, the deployment playbook builds the assets of the\\nfrontend branch from scratch. If we could use pre-built assets, the deployment\\nwould be faster.\\n* **Developer experience**: The app won't work as expected or represent\\nthe required test state if we deploy the wrong branches together. Our\\nsolution should prevent such an \"out of sync\" scenario as good as\\npossible while at the same time not requiring too many manual steps from\\nthe developers.\\n* **Traceability**: We should be able to check which branches were used\\nfor a deployment.\\nChosen option: \"Use the pre-built assets provided by the [Maven](https:\/\/maven.apache.org\/)\", because we can't use them in the\\ndeployments and because they are not part of the [Maven](https:\/\/maven.apache.org\/) project. We also have to build\\nthe artifacts from the pre-built version. The deployment playbook will build the assets of the\\nfrontend branch from scratch.\\n### Positive Consequences\\n* **Speed:** The deployment will be faster than manually deploying\\nthe frontend and the backend. We can deploy the frontend and the backend\\ntogether in parallel, and we don't have to manually deploy the"}
{"Context":"## Context\\nEmail, as an asynchronous communication medium, inherently has two boolean\\nscenarios that need to be true for an email to be received. An email has to be\\n_sent_ to a mail server and the mail server then needs to _deliver_ the email to the\\nrecipient's mail server. Resolving whether an email is sent is a rather simple\\nquestion that can be resolved synchronously - \"did the mail server accept the\\nemail?\" - however, determining whether an email is received is more complex\\nas delivery is not synchronous and may be subject to automated retrying should\\nany problems occur. Typically, in the medium of email, clients only consider\\nthe first scenario in reporting an email's status, this is the point an email\\nbecomes sent. The latter scenario, delivery, isn't typically reflected in\\nemail clients, it is normal to assume the email was received\\nsuccessfully unless later you receive an email indicating a bounce has\\noccurred.\\nThe [initial Email Alert API Notify integration][adr-1] was designed to\\nconsider both the sending and delivery of email. It had systems to monitor\\nwhether Notify had managed to deliver an email to a recipient and an automatic\\nretry mechanism for when Notify had problems delivering an email.\\nIn September 2020 we decided to re-evaluate this concept conflation as we were\\nconcerned that Notify functionality was duplicated in Email Alert API and\\nthat this had an adverse effect on the complexity of the system.\\n[adr-1]: adr-001-notify-integration.md\\n","Decision":"## Decision\nWe decided that Email Alert API has a responsibility to send email, but does\\nnot have a responsibility to deliver it. That is the responsibility of Notify.\\nWe defined that, in the context of Email Alert API, the process of sending\\nemail was the ability to successfully send a request to Notify to perform\\nthis action - we consider this the equivalent of a mail client successfully\\nsending an email to an [SMTP][] server. We then consider the\\n[Notify callback][notify-callback] as the mechanism to learn if an email was\\ndelivered or not.\\nTo reflect this we have [switched][switched-to-sent-success] the\\nmeaning of an [email's status][email-status]. A status of \"sent\" now means that\\nthe email was accepted by Notify and not that the email was delivered. A\\nstatus of \"failed\" now means we weren't able to send the email to Notify,\\ninstead of the previous meaning where it meant that it may not have been sent\\nto Notify or Notify failed to deliver the email.\\n[SMTP]: https:\/\/en.wikipedia.org\/wiki\/Simple_Mail_Transfer_Protocol\\n[notify-callback]: https:\/\/docs.notifications.service.gov.uk\/ruby.html#delivery-receipts\\n[switched-to-sent-success]: https:\/\/github.com\/alphagov\/email-alert-api\/commit\/c457f62c3b6f1eaadf47e6596223cc0fdcffa853\\n[email-status]: https:\/\/github.com\/alphagov\/email-alert-api\/blob\/0b87e62288ddb1653451f84e1f36e17ce4e8e9dc\/app\/models\/email.rb#L7\\n","Prediction":"## Decision\nWe decided that the initial Email Alert API Notify integration should be replaced\\nwith a `deliver` and `sent` endpoint to the Email Alert API. This will allow\\nthe Email Alert API to deliver email to a mail server and the mail server can\\nthen check the `delivery_status` field of the email to determine whether it\\nwas successfully delivered to the recipient's mail server. If the email is\\ndelivered it will also be sent to the Notify system and the Notify system\\nwill then retry sending the email if it has failed. If the email is not sent\\nit will be considered as being received successfully.\\nThis will allow the Email Alert API to continue to support both the\\ninitial Email Alert API Notify integration and the refactor of the current\\nEmail Alert API Notify integration to be phased out.\\nWe will also have to consider how to deal with email that is received by the\\nEmail Alert API but is not delivered to a recipient's mail server. This\\nshould be handled by the Email Alert API and the Email Alert API will\\nconsider a retry mechanism if the email is not delivered by the Notify system.\\nHowever, we have not yet decided what to do for email that is delivered but is\\nnot considered received by a mail server"}
{"Context":"## Context\\nOpenCHS client will be used in places of low or no connectivity. This means the application should be usable at any point in time without internet. However, data needs to be pushed to a central server for reporting, as well as for backup. This means no functionality other than sync to server should require connectivity to the server.\\n","Decision":"## Decision\nOpenCHS client should be usable offline.\\n","Prediction":"## Decision\nWe will use the [OpenHRS API](https:\/\/github.com\/openshift\/openhrs\/tree\/master\/openhrs) to push data to the cloud.\\nWe will also use the [OpenCHS API](https:\/\/github.com\/openshift\/openhrs\/tree\/master\/openhrs) to pull data from the cloud.\\nWe will use the [OpenHRS API](https:\/\/github.com\/openshift\/openhrs\/tree\/master\/openhrs) to push data to a database, and use the [OpenCHS API](https:\/\/github.com\/openshift\/openhrs\/tree\/master\/openhrs) to pull data from the database.\\nWe will use the [OpenCHS API](https:\/\/github.com\/openshift\/openhrs\/tree\/master\/openhrs) to push data to a database, and use the [OpenCHS API](https:\/\/github.com\/openshift\/openhrs\/tree\/master\/openhrs) to pull data from the database.\\nWe will use the [OpenCHS API](https:\/\/github.com\/openshift\/openhrs\/tree\/master\/openhrs) to push data to a database, and use the [OpenCHS API](https:\/\/github.com\/openshift\/openhrs"}
{"Context":"## Context\\nI have to decide how to persist data related to supported currencies in the project. Choices are a relational database or text files such as txt, xml or json. Even NoSQL databases could be used.\\n","Decision":"## Decision\nI decided to use json files to persist supported currencies in the project because this kind of data doesn't change all the time. JSON files are easy to store, to write, and to read in Java Objects. Each time a new currency is added it's just a matter of update the file.\\n","Prediction":"## Decision\nThe database will be a Relational Database stored in a simple file\\n[Relational Database](#relational-database). Relational Database is a database that stores data in the form of tables and the relationships between them. It is the most commonly used database type in the industry.\\nRelational Database stores data in the following form:\\n```\\nid | type | value | 1 | 2 | 3 | 4 | 5 | ... | 9 | 10 | ... | \\n1 | int | 0 | 0 | 1 | 2 | 3 | 4 | 5 | ... | 9 | 10 | ... | \\n2 | int | 0 | 0 | 1 | 2 | 3 | 4 | 5 | ... | 9 | 10 | ... | \\n3 | int | 0 | 0 | 1 | 2 | 3 | 4 | 5 | ... | 9 | 10 | ... | \\n4 | int | 0 | 0 | 1 | 2 | 3 | 4 | 5 | ... | 9 | 10 | ... | \\n"}
{"Context":"## Context\\nOriginally, the software tool that became FlowKit was designed to be used as an extensible library, which connected to one shared database. Users would extend their own copy of the library to add query types, or even modify existing ones.\\nThis introduced considerable difficulties, e.g.:\\n- No guarantee that an analysis written by one person could be run by another, or by the same person in future.\\n- All users required highly privileged access to the database\\n- No way to manage usage of shared resource\\n- No way to ensure that upstream changes were in use\\n- Difficult to effectively exploit the ability to reuse already computed results between analysts\\n- Significant complexity and blurred functional boundaries in the main library\\n- Very difficult to use the tool outside the Python ecosystem\\n- Substantial challenges in logging access and activity\\nThis motivates the revised design, where there is a _single_ copy of the library responsible for constructing and running queries, accessed through a language neutral HTTP API. This facilitates some significant improvements:\\n- Easy to produce clients for multiple language ecosystems\\n- Can make substantial changes to the enclosed code, database structure etc. with very little disruption\\n- Supports granular access control\\n- Enables more secure storage of raw data, by removing direct access to the data\\n- Allows for much more efficient sharing of resources\\n- Supports comprehensive logging\\n- Much clearer 'seams' between functional parts, and simpler codebase\\n- Simpler code, because scheduling of query runs is controlled by a single point.\\n- Considerable opportunities to be more efficient in scheduling runs and caching of queries.\\n","Decision":"## Decision\nFlowMachine will be wrapped by an HTTP API.\\n","Prediction":"## Decision\nWe will remove the shared database, and replace it with a single copy of the library, which will be used by all users. This will be achieved by moving the shared database to a separate layer, which will be accessed through an HTTP API. The shared database will be replaced with a more granular copy of the original, which will be used by only the _exclusive_ subset of users that require it.\\nThis will enable the following:\\n- Users will only be able to run analysis on data that is _exclusively_ stored in the _shared_ copy of the library. This will be enforced by requiring that all queries are run on the shared copy, and will be run by all users.\\n- Users will be able to run analysis on _raw_ data, which will be stored in the _granular_ copy of the original. This will be enforced by requiring that all queries be run on the _granular_ copy, and will be run by all users.\\n- Users will be able to run analysis on data that is _partially_ cached in the _shared_ copy of the library. This will be enforced by requiring that all queries are run on the shared copy, and will be run by only a subset of"}
{"Context":"## Context\\nWe need to record the architectural decisions made for our team.\\n","Decision":"## Decision\nWe will use Architecture Decision Records, as discussed in our original proposal: https:\/\/docs.google.com\/document\/d\/1pZlYCyXcZbmQq1O-g4BNJD1uZTjluYKkk7BSu2BwOGU\/edit#\\n","Prediction":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\nWe will use [Architecture Decision Records](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions) as a reference for our Architecture Decision Library. Architecture Decision Records are the primary format for storing ADRs.\\nWe will use [Atomic Architecture](https:\/\/atomic-architecture.org) as our framework for documenting architecture decisions.\\nWe will use [ESLint](https:\/\/eslint.org) as our code formatting tool.\\nWe will use [Jira](https:\/\/www.atlassian.com\/software\/jira) as our issue tracking tool.\\nWe will use [Google Sheets](https:\/\/sheets.google.com) as our database for storing architecture decisions.\\nWe will use [Google Drive](https:\/\/drive.google.com) as our database for storing architecture decision documents.\\nWe will use [Google Sheets](https:\/\/sheets.google.com) as our database for storing architecture decisions.\\nWe will use [Google Sheets](https:\/\/sheets.google.com) as our database for storing architecture decisions.\\nWe will use [Google Sheets"}
{"Context":"## Context\\nWe need to supply the user with prompts that they have been logged out due to inactivity. This is not currently possible\\ndue to our session lifetime being dictated by the expiry of our session cookie (currently 20 minutes). Once a cookie has\\nexpired we do not recognise a user in order to tell them that they have been logged out.\\n","Decision":"## Decision\n* We will lengthen the cookie expiry to be as long as the business requires to display a _\"logged out due to\\ninactivity\"_ message. Somewhere in the region of 1 day - 1 week will likely be acceptable.\\n* We will store the last access time of the user within the cookie and compare that upon new requests with our\\ndesired session time (20 minutes). This middleware could then force a user to re-login with an appropriate message if\\nthe time has expired.\\n","Prediction":"## Decision\nAdd a session cookie expiry timer of 20 minutes to the application. If the expiry timer has not expired the user will be\\nlogged back in and the session cookie will be cleared.\\nThis will prevent the user from being logged out of the application if they are still logged in, but will also prevent\\nthem from being logged out if they are currently offline.\\nWe will allow users to remove the session cookie manually, by sending a GET request to the `\/api\/session\/` endpoint.\\nThis will remove the cookie, and the session will be re-created on the next request to the application.\\nThis will allow us to track session usage and ensure that the session cookie is only ever removed when it\\nhas expired.\\n### Positive Consequences\\n* Users will be able to log back in to the application\\n* We will not need to store a session cookie for every user\\n* Session cookie lifetime can be longer\\n### Negative Consequences\\n* We will need to increase the size of our application by 2KB\\n* We will need to increase the number of requests we make to the application\\n* We will need to increase our database size\\n* We will need to increase our memory usage\\n### Impact on the application\\nWe will need"}
{"Context":"## Context\\n### Log collection and forwarding\\nApplication containers should output their logs as [`stdout`\/`stderr`](https:\/\/12factor.net\/logs) streams, which can then be forwarded by an aggregator to a single log store. These logs should be searchable, filterable and appropriately tagged with application\/component etc.\\nThere are two broad approaches to log collection and forwarding in Kubernetes\\n- Run a [sidecar container](https:\/\/kubernetes.io\/docs\/concepts\/cluster-administration\/logging\/#sidecar-container-with-logging-agent) with each `Pod` or container to pick up the application logs from a file or socket and forward on (this is the only available option with [EKS Fargate, although partially managed by AWS](https:\/\/docs.aws.amazon.com\/eks\/latest\/userguide\/fargate-logging.html))\\n- Run a log collection and forwarder agent on [each node as a `DaemonSet`](https:\/\/kubernetes.io\/docs\/concepts\/cluster-administration\/logging\/#using-a-node-logging-agent)\\nLogging should be provided as a service and enabled by default for all containers, so the sidecar pattern does not meet our needs.\\nFor application logs to be searchable and navigable we require the logging agent to annotate and index logs with kubernetes cluster metadata (namespace, pod name, labels etc).\\nThe most commonly used logging agents for Kubernetes are: [fluentd](https:\/\/www.fluentd.org) and [fluentbit](https:\/\/fluentbit.io). Fluentbit is a successor to fluentd, and aims to be more lightweight and offer higher performance, but has a smaller plugin ecosystem than fluentd.\\n### Log storage and UI\\nLogs must also be stored somewhere, with a browser-based UI for searching, filtering and browsing. There are many options in the space, but the most common option is [elasticsearch and kibana](https:\/\/www.elastic.co\/what-is\/elk-stack).\\nWe may ultimately push logs to a GDS-wide central log store, but should deploy something ourselves in the short term so that we can provide a usable logging service to users sooner rather than later, and get familiar with the details of logging within Kubernetes ourselves.\\nFor now, the obvious choice is Elasticsearch and Kibana, due to their extensive support for this use case and in this ecosystem.\\n","Decision":"## Decision\nUse a **fluentbit** Daemonset with AWS Elasticsearch Service.\\n","Prediction":"## Decision\nWe will use [elasticsearch](https:\/\/www.elastic.co\/) and [kibana](https:\/\/www.elastic.co\/kibana\/) to collect application logs for use by the service as a service.\\nWe will use [fluentbit](https:\/\/fluentbit.io) to collect and forward application logs to a central logging service\\nWe will use [fluentd](https:\/\/www.fluentd.org) to collect and forward application logs to a central logging service\\nWe will use [elasticsearch](https:\/\/www.elastic.co\/elasticsearch\/) to collect and forward application logs to a central logging service\\nWe will use [kibana](https:\/\/www.elastic.co\/kibana\/) to collect and forward application logs to a central logging service\\nWe will use [fluentbit](https:\/\/fluentbit.io) to collect and forward application logs to a central logging service\\nWe will use [elasticsearch](https:\/\/www.elastic.co\/elasticsearch\/) to collect and forward application logs to a central logging service\\nWe will use [kibana](https:\/\/www.elastic.co\/kibana\/) to collect and forward application logs to a central logging service\\nWe will use [fluentd](https"}
{"Context":"## Context\\nI used the following points to make my decision:\\n- Ubuntu 20.04 uses a unique auto-installation method, which is generally nice, but completely non-transferable\\n- `snapd` remains controversial, but using Ubuntu systems forces that path more nowadays\\n- Debian should be relatively \"install and forget\"\\n- Centos is more opinionated about some things like how network configuration is done\\n- Centos doesn't do automatic security upgrades, at least not in the same way RHEL advertises\\n- I've successfully used Debian before for this role, before trying out Ubuntu again\\n- Remastering a Debian iso with the preseed configuration can result in a one-button install process\\n","Decision":"## Decision\nUse Debian buster as the hypervisor OS, with an automated installation via Debian preseeding.\\n","Prediction":"## Decision\nSnapd is used for the base package management, and the snapd.service is used to start the snapd daemon\\n* Snapd.service is used to start the snapd daemon\\n* Snapd is used to start the snapd daemon\\n* Snapd is used to start the snapd daemon\\n* Snapd is used to start the snapd daemon\\n* Snapd is used to start the snapd daemon\\n* Snapd is used to start the snapd daemon\\n* Snapd is used to start the snapd daemon\\n* Snapd is used to start the snapd daemon\\n* Snapd is used to start the snapd daemon\\n* Snapd is used to start the snapd daemon\\n* Snapd is used to start the snapd daemon\\n* Snapd is used to start the snapd daemon\\n* Snapd is used to start the snapd daemon\\n* Snapd is used to start the snapd daemon\\n* Snapd is used to start the snapd daemon\\n* Snapd is used to start the snapd daemon\\n* Snapd is used to start the snapd daemon\\n* Snapd is used to start the snapd daemon\\n* Snapd is used to start the snapd daemon\\n"}
{"Context":"## Context\\nWe need to choose a frontend framework.\\n","Decision":"## Decision\nWe don't have any specific requirements yet, so use Bootstrap because it is the most popular framework, and has a lot of documentation.\\n","Prediction":"## Decision\nWe will use React.\\nReact is the most popular frontend framework on the market and it is very mature. It is also the most popular framework used by Facebook and Google.\\nReact is also very popular with the React Native team. React Native is a library that allows us to use React on Android and iOS.\\nReact is also very popular with the React team. React is the most popular React library.\\nReact is also the most popular React library in the React ecosystem and many popular projects use it.\\nReact is also very popular with the Facebook team. React is the most popular React library for Facebook.\\nReact is also very popular with the Google team. React is the most popular React library for Google.\\nReact is also very popular with the React team. React is the most popular React library for the React team.\\nReact is also very popular with the React team. React is the most popular React library for the React team.\\nReact is also very popular with the React team. React is the most popular React library for the React team.\\nReact is also very popular with the React team. React is the most popular React library for the React team.\\nReact is also very popular with the React team. React is the most popular React"}
{"Context":"## Context\\nWe want to make the MOLGENIS \"Armadillo\" service data provider agnostic. There are a couple of reasons why we are doing this\\n- the service is usable for other parties as well\\n- the service can still integrate with MOLGENIS.\\n- the release cycle of the service is data provider independent\\n- the service can be developed by other parties as well\\n","Decision":"## Decision\nWe implement an endpoint to upload and load RData files in the MOLGENIS \"Armadillo\" service to manage data for the use in DataSHIELD.\\n","Prediction":"## Decision\nWe will use the [Service Registry](https:\/\/registry.molgenis.com\/) to host the Armadillo service. This is a data provider independent service that can be used by other parties.\\nWe will use the [Service Registry](https:\/\/registry.molgenis.com\/) to host the Armadillo service. This is a data provider independent service that can be used by other parties.\\nWe will use the [Service Registry](https:\/\/registry.molgenis.com\/) to host the Armadillo service. This is a data provider independent service that can be used by other parties.\\nWe will use the [Service Registry](https:\/\/registry.molgenis.com\/) to host the Armadillo service. This is a data provider independent service that can be used by other parties.\\nWe will use the [Service Registry](https:\/\/registry.molgenis.com\/) to host the Armadillo service. This is a data provider independent service that can be used by other parties.\\nWe will use the [Service Registry](https:\/\/registry.molgenis.com\/) to host the Armadillo service. This is a data provider independent service that can be used by other parties.\\nWe will use the [Service Registry"}
{"Context":"## Context\\nServerless has the problem of narrowing available functionality from CloudFormation, which itself narrows functionality from AWS. For instance, CloudFormation can create stacks no larger than 200 resources, and Serverless wants to put everything in one CloudFormation stack. In short, Serverless is good for a basic API, but it's not very appropriate for other cloud resources.\\nIn my research I kept running across Terraform. It is a less magical, more granular tool. It's open source and has very broad support for provisioning AWS resources and other services. It was called out in the book Infrastructure as Code several years ago, and it has had all that time to mature.\\n","Decision":"## Decision\nI will use Terraform for infrastructure definition and deployment, not Serverless.\\n","Prediction":"## Decision\nI will use Terraform to provision Serverless resources. It is a better tool for this purpose.\\nI will use Terraform to provision AWS resources. It is a better tool for this purpose.\\nI will use Terraform to provision other services. It is a better tool for this purpose.\\nI will use Terraform to provision existing resources. It is a better tool for this purpose.\\nI will use Terraform to provision existing resources. It is a better tool for this purpose.\\nI will use Terraform to provision existing resources. It is a better tool for this purpose.\\nI will use Terraform to provision existing resources. It is a better tool for this purpose.\\nI will use Terraform to provision existing resources. It is a better tool for this purpose.\\nI will use Terraform to provision existing resources. It is a better tool for this purpose.\\nI will use Terraform to provision existing resources. It is a better tool for this purpose.\\nI will use Terraform to provision existing resources. It is a better tool for this purpose.\\nI will use Terraform to provision existing resources. It is a better tool for this purpose.\\nI will use Terraform to provision existing resources. It is"}
{"Context":"## Context\\nAfter an item is deleted from the database all traces of it are gone, but if\\nit's been preserved then it's possible to restore it. However, finding the\\nparticular ID of the item to restore is difficult if all that's known are pieces\\nof its metadata (title, source metadata ID, etc.)\\nWe will remedy this by storing a \"tombstone\" of deleted items as a small record\\nof what was deleted.\\n","Decision":"## Decision\n1. When a preserved item is deleted it will create a \"tombstone\" containing the following\\nmetadata:\\n1. ID of deleted resource\\n1. Title of deleted resource\\n1. Original Filename of deleted resource (if a FileSet)\\n1. Embedded PreservationObject that existed at time of deletion\\n1. ID of Parent at time of deletion.\\n2. These tombstones will be displayed in the Figgy UI and used as a way to\\ndiscover material that can be recovered.\\n","Prediction":"## Decision\nWe will use the [Tombstone API](https:\/\/github.com\/GoogleCloudPlatform\/tds-legacy\/blob\/master\/docs\/tombstone.md) to store deleted items.\\nThis will allow us to recover items with a small amount of metadata, such as the\\noriginal source.\\nWe will also be able to use the API to recover items that were never saved,\\nsuch as when a user deletes a note with a bad title.\\nWe will use the [Tombstone API](https:\/\/github.com\/GoogleCloudPlatform\/tds-legacy\/blob\/master\/docs\/tombstone.md) to store\\ndeleted items.\\nThis will allow us to recover items with a small amount of metadata, such as the\\noriginal source.\\nWe will also be able to use the API to recover items that were never saved,\\nsuch as when a user deletes a note with a bad title.\\nWe will use the [Tombstone API](https:\/\/github.com\/GoogleCloudPlatform\/tds-legacy\/blob\/master\/docs\/tombstone.md) to store\\ndeleted items.\\nWe will use the [Tombstone API](https:\/\/github.com\/GoogleCloudPlatform\/tds-legacy\/blob\/master\/docs\/tombstone.md) to"}
{"Context":"## Context and Problem Statement\\nA hypothetical future maker space has many networked CNC devices; Mills, Lathes, 3D Printers, Laser Cutters, and so on. The hypothetical maker space uses various future versions of Teg with different plugins to control each of these machines. The maker space has an additional combinator Teg host that combines the other Teg instances and allows makers to securely control and configure all of the machines through a single WebRTC connection to a single GraphQL schema.\\nWhat would the configuration API of Teg look like to accomodate all the different machines\/teg version\/teg plugin variations in that hypothetical future makerspace with one GraphQL Schema accessed by users from a statically hosted printspool.io GUI?\\n## Decision Drivers\\n* Current `extendedConfig` GUI puts Plugin-specific settings into a JSON text field under 'Advanced'. The UX of which is that Plugin settings do not feel as tightly integrated as other settings. It is felt that plugin-specific settings should not feel like second class citizens and that having them feel that way limits their usefulness.\\n* Teg is being designed with intent to support combinators in a future version. Any current configuration architecture decision that does not take combinators into account today may cause breaking changes and cause unneccessary rewrites when combinators are introduced in the future.\\n","Decision":"## Decision\n* Current `extendedConfig` GUI puts Plugin-specific settings into a JSON text field under 'Advanced'. The UX of which is that Plugin settings do not feel as tightly integrated as other settings. It is felt that plugin-specific settings should not feel like second class citizens and that having them feel that way limits their usefulness.\\n* Teg is being designed with intent to support combinators in a future version. Any current configuration architecture decision that does not take combinators into account today may cause breaking changes and cause unneccessary rewrites when combinators are introduced in the future.\\nChosen options: [Queries: JSON Flat Objects] and [Mutations: JSON Flat Objects] because they are the only Query and Mutation option that meets the criteria of maintaining security via internal type safety, preventing schema collisions in a combinator and presents both plugin-specific and core settings as first class citizens.\\nChosen GUI option: [GUI: Dynamic Forms] because it is the only GUI option that allows plugin-specific settings to be placed in the ideal arbitrary position within the GUI for an optimal UX.\\n","Prediction":"## Decision\n* Current `extendedConfig` GUI puts Plugin-specific settings into a JSON text field under 'Advanced'. The UX of which is that Plugin settings do not feel as tightly integrated as other settings. It is felt that plugin-specific settings should not feel like second class citizens and that having them feel that way limits their usefulness.\\n* Teg is being designed with intent to support combinators in a future version. Any current configuration architecture decision that does not take combinators into account today may cause breaking changes and cause unneccessary rewrites when combinators are introduced in the future.\\nChosen option: \"Use `extendedConfig` and a static host\".\\n### Positive Consequences\\n* Integrates well with the current Teg themeing\\n* Allows for plugin-specific settings to be exposed through a static host\\n* Doesn't require breaking changes in the current Teg plugin architecture\\n### Negative Consequences\\n* Plugin-specific settings are not exposed through a static host\\n* The ability to dynamically load plugins is not supported\\n* Plugin-specific settings are not exposed through a static host\\n### Positive Consequences\\n* Integrates well with the current Teg themeing\\n* Allows for plugin-specific settings to be exposed through a static host\\n* Doesn't"}
{"Context":"## Context\\nWhen the delivery people deliver meals to the smart fridges or kiosks, they update the system with the delivery information (the location and the meals).\\nTo determine the inventory at a certain location, the Location Inventory component has to reconcile between the two sources of truths: 1) the information fed\\nto the system by the delivery people 2) the inventory reported by the external Fridge\/Toast APIs.\\nUpon receiving the delivery confirmation, the Location Inventory component can 1) query the Fridge\/Toast APIs and update its data upon synchronous\\nreconciliation 2) asynchronously query the Fridge\/Toast APIs and reconcile.\\n","Decision":"## Decision\nWe will use asynchronous communication between the Location Inventory and the External Fridge\/Toast APIs for reconciliation.\\nWaiting for the synchronous reconciliation at a location will increase the time delivery people spend at a location and\\neven may not be possible if the External Fridge\/Toast Inventory APIs are down for some reason.\\n","Prediction":"## Decision\nThe Location Inventory component will query the Fridge\/Toast APIs using the [Fridge\/Toast SDK](https:\/\/github.com\/teapplabs\/kiosk-frodo-sdk)\\nand will not use the system's own Fridge\/Toast implementation.\\nThis means that when the delivery people update the delivery information, they will have to do so in two stages:\\n1) the delivery people will update the delivery information in the system, by feeding it into the delivery system's own\\nFridge\/Toast implementation.\\n2) the delivery people will update the delivery information in the external Fridge\/Toast implementation.\\nThis means that when the delivery people update the delivery information, they will have to do so in two stages:\\n1) the delivery people will update the delivery information in the system, by feeding it into the delivery system's own\\nFridge\/Toast implementation.\\n2) the delivery people will update the delivery information in the external Fridge\/Toast implementation.\\nThis means that when the delivery people update the delivery information, they will have to do so in two stages:\\n1) the delivery people will update the delivery information in the system, by feeding it into the delivery system's own\\nFridge\/Toast implementation.\\"}
{"Context":"## Context\\n> This section contains all the context one needs to understand the current state, and why there is a problem. It should be as succinct as possible and introduce the high level idea behind the solution.\\n","Decision":"## Decision\n> This section explains all of the details of the proposed solution, including implementation details.\\n> It should also describe affects \/ corollary items that may need to be changed as a part of this.\\n> If the proposed change will be large, please also indicate a way to do the change to maximize ease of review.\\n> (e.g. the optimal split of things to do between separate PR's)\\n","Prediction":"## Decision\nThe solution will be a `RESTful` API, which will be exposed to the `client` via a REST client. The client will be able to consume the API in a number of ways, including (but not limited to):\\n- `GET` (to get a single record) - `POST` (to update a single record) - `PUT` (to update multiple records) - `DELETE` (to delete a single record) - `PATCH` (to update multiple records) - `OPTIONS` (to get a single record) - `HEAD` (to get a list of records) - `DELETE` (to delete a single record) - `DELETE` (to delete a list of records) - `PATCH` (to update multiple records) - `GET` (to get a single record) - `GET` (to update multiple records) - `PUT` (to update multiple records) - `GET` (to get a single record) - `GET` (to update multiple records) - `GET` (to get a single record) - `GET` (to update multiple records) - `GET` (to get a single record) - `GET` (to update multiple records) -"}
{"Context":"## Context\\nThe Office of Head Start is expanding data sharing across internal systems. To support this, TTA Hub would like\\nto make documented APIs available to external systems in reliable, consistent, and secure ways.\\n### Options Considered\\nTo achieve this, two paths were considered.\\n#### Expose existing internal APIs\\nTTA Hub has a robust set of APIs already implemented to support the React frontend.\\nPros:\\n* already implemented\\n* well documented\\nCons:\\n* API is highly coupled to TTA Hub UI needs, will become more difficult to change rapidly\\n* authentication is based on sessions stored in browser cookies\\n* authorization fully limited to existing TTA Hub roles\\n#### Create a new API layer for external partners\\nCreating a new API layer would de-couple the data needs from the TTA Hub UI needs.\\nPros:\\n* existing API can continue to be optimized for UI considerations\\n* new API can be more rigidly backwards-compatible\\n* authentication schemes can be easily customized for each use\\n* authorization can be scope-based and integrated into the full HSES OAuth implementation\\nCons:\\n* two API endpoints will need to be maintained through data model changes\\n","Decision":"## Decision\nA new API layer has been chosen as the better set of trade offs.\\nTo distinguish between the API flavors, the external API will be namespaced under the `\/api\/v1` route path.\\n### Authentication\\nThe initial API client also utilizes HSES as a single-sign-on solution, so API authentication will be done\\nby passing an HSES token via the `Authentication` HTTP header. TTA Hub will then validate that token with HSES\\nto verify the user.\\n### JSON Format\\nTo promote consistency, the external facing API will conform to [the JSON:API schema](https:\/\/jsonapi.org\/)\\nas much as possible. The first endpoints will include objects in `attributes` that may eventually migrate\\nto `relationships`. This trade-off has been made due to the exponential growth of required API endpoints when each\\nobject is represented in the `relationships` section.\\n","Prediction":"## Decision\nExternal partners will be able to use the TTA Hub API to access data in the TTA Hub system.\\nExternal partners will need to authenticate against TTA Hub.\\nExternal partners will need to create new API calls to access data in the TTA Hub system.\\nExternal partners will need to use the TTA Hub API to create new data elements in the TTA Hub system.\\nExternal partners will need to use the TTA Hub API to update existing data elements in the TTA Hub system.\\nExternal partners will need to update existing data elements in the TTA Hub system.\\nExternal partners will need to update existing data elements in the TTA Hub system.\\nExternal partners will need to update existing data elements in the TTA Hub system.\\nExternal partners will need to update existing data elements in the TTA Hub system.\\nExternal partners will need to update existing data elements in the TTA Hub system.\\nExternal partners will need to update existing data elements in the TTA Hub system.\\nExternal partners will need to update existing data elements in the TTA Hub system.\\nExternal partners will need to update existing data elements in the TTA Hub system.\\nExternal partners will need to update existing data elements in the TTA Hub"}
{"Context":"Context\\n=======\\nWe wanted to open up access to tenant applications in our production environment.\\nAs part of an earlier story, Pingdom checks were set up for a healthcheck application in CI, Staging, and Production. At this stage applications were not accessible from non-office IP addresses.\\nThe problem we faced was a need to test the code introduced to make our production environment accessible to the world. Initially, we suggested applying the same change to our staging environment. However, this approach means all applications in staging will be accessible from anywhere.\\nIf we use Pingdom to assert an application is accessible from the outside world then we need to remove the explicit rules (security groups) allowing Pingdom traffic. This means our CI environment would not be accessible to Pingdom probes.\\n* [#116104189 - set up Pingdom](https:\/\/www.pivotaltracker.com\/story\/show\/116104189)\\n* [#115347323 - allow public access to tenant applications](https:\/\/www.pivotaltracker.com\/story\/show\/115347323)\\nDecision\\n========\\nIt was decided we would make the staging environment accessible to the outside world as well as production, and define future work for removing the CI Pingdom check and security groups allowing Pingdom probes, and setting up tests from the pipeline which use the Pingdom API.\\nGiven that the advantages relate to the availability of our production environment, they outweigh not having an automated healthcheck on an application in our CI environment. However, we remain open to hearing solutions to providing healthchecks for CI in future.\\nStatus\\n======\\nProposed\\nConsequences\\n============\\nA story is now required to remove the Pingdom health check for our CI environment, and the security groups allowing Pingdom probes.\\n### Positive\\n* We are now able to test accessibility using the staging environment\\n* We are now able to use Pingdom to assert not just application health, but routing as well.\\n* We have maintained consistency between staging and production\\n### Negative\\n* Any applications in our staging environment need to be considered for whether they are suitable to be public.\\n* We would no longer have healthchecks via Pingdom for our CI environment.\\n","Decision":"## Decision\n========\\nIt was decided we would make the staging environment accessible to the outside world as well as production, and define future work for removing the CI Pingdom check and security groups allowing Pingdom probes, and setting up tests from the pipeline which use the Pingdom API.\\nGiven that the advantages relate to the availability of our production environment, they outweigh not having an automated healthcheck on an application in our CI environment. However, we remain open to hearing solutions to providing healthchecks for CI in future.\\nStatus\\n======\\nProposed\\nConsequences\\n============\\nA story is now required to remove the Pingdom health check for our CI environment, and the security groups allowing Pingdom probes.\\n### Positive\\n* We are now able to test accessibility using the staging environment\\n* We are now able to use Pingdom to assert not just application health, but routing as well.\\n* We have maintained consistency between staging and production\\n### Negative\\n* Any applications in our staging environment need to be considered for whether they are suitable to be public.\\n* We would no longer have healthchecks via Pingdom for our CI environment.\\n","Prediction":"## Decision\n========\\nIt was decided we would make the staging environment accessible to the outside world as well as production, and define future work for removing the CI Pingdom check and security groups allowing Pingdom probes, and setting up tests from the pipeline which use the Pingdom API.\\nGiven that the advantages relate to the availability of our production environment, they outweigh not having an automated healthcheck on an application in our CI environment. However, we remain open to hearing solutions to providing healthchecks for CI in future.\\nStatus\\n======\\nProposed\\nConsequences\\n============\\nA story is now required to remove the Pingdom health check for our CI environment, and the security groups allowing Pingdom probes.\\n### Positive\\n* We are now able to test accessibility using the staging environment\\n* We are now able to use Pingdom to assert not just application health, but routing as well.\\n* We have maintained consistency between staging and production\\n### Negative\\n* Any applications in our staging environment need to be considered for whether they are suitable to be public.\\n* We would no longer have healthchecks via Pingdom for our CI environment.\\n* We're still open to hearing solutions to applying healthchecks to CI in future.\\n## Decision"}
{"Context":"## Context and Problem Statement\\nThe Open Data Hub itself consists of many components all having their own release cycles. ODH users might decide to\\nupdate individual components such as container images used by JupyterHub. Nevertheless it is required to create\\nreleases of ODH as a whole.\\n","Decision":"## Decision\nChosen option: we do a monolithic, coordinated release, because it will enable us to have a release at the\\nproject\/product level while maintianing freedom of others to update.\\n### Positive Consequences <!-- optional -->\\n* Operators of ODH have a clear base line of versions, these versions have been tested with each other and have\\nundergone ODH integration testing.\\n* Operators of ODH can update individual components, they could maintain a repository analog to odh-manifests declaring\\nthe exact versions (container image tags, git repository tags) of components they want to deploy.\\n* Operators can mix in their builds of container images following the method mentioned above.\\n### Negative Consequences <!-- optional -->\\n* An ODH release (a tag in the odh-manifests repository) might not contain the latest versions of components, for example\\nsecurity updates might have forced the build of a S2I image used with JupyterHub component of ODH.\\n<!-- markdownlint-disable-file MD013 -->\\n","Prediction":"## Decision\nWe will create a single release of ODH as a single binary. We will use the `ambassadors` tool to create an ODH\\nrelease.\\n### Positive Consequences\\n* The ODH service is more resilient to changes in the ODH components.\\n* Each individual component is more resilient to changes in the ODH components.\\n### Negative Consequences\\n* The ODH service depends on the individual components to be in a consistent state. In particular, the\\ncontainer images used by JupyterHub, the JupyterLab server and the JupyterLab visualization server\\nare all dependent on the ODH component. As such, if these services are updated, the ODH service will need to be\\nupdated as well. This could cause the ODH service to be unavailable for short periods of time.\\n### Impact on Users\\n* Users of the ODH service will not be affected.\\n* Users of the ODH service will not be affected.\\n## Decision\nWe will create a single release of ODH as a single binary. We will use the `ambassadors` tool to create an ODH\\nrelease.\\n### Positive Consequences\\n* The ODH service is more resilient to changes in the ODH components.\\"}
{"Context":"## Context\\nGiven the limitations of Lambdas we decided to rely on containers to handle the bulk of the processing. Fargate provides a cheap, accessible container runtime.\\n","Decision":"## Decision\nWe will use AWS Lambda to trigger a Fargate task for the processing pipeline.\\n","Prediction":"## Decision\nWe will use Fargate for the majority of our lambda processing. We will use Lambda@Edge for the few edge cases where Lambdas are not suitable.\\nWe will use AWS Fargate cluster for our dev\/test environment and the rest of our infrastructure will be managed by AWS CodeBuild and CodePipeline.\\nWe will use AWS CodePipeline and CodeBuild to build and deploy our lambda functions in a continuous integration pipeline. The build and deployment will be managed by AWS CodePipeline and CodeBuild.\\nWe will use AWS CodePipeline and CodeBuild to deploy a lambda function to the AWS IoT Edge service. The deployment will be managed by AWS CodePipeline and CodeBuild.\\nWe will use AWS CodePipeline and CodeBuild to deploy an AWS IoT Edge service to a private subnet. The deployment will be managed by AWS CodePipeline and CodeBuild.\\nWe will use AWS CodePipeline and CodeBuild to deploy an AWS IoT Edge service to an external endpoint. The deployment will be managed by AWS CodePipeline and CodeBuild.\\nWe will use AWS CodePipeline and CodeBuild to deploy an AWS IoT Edge service to an AWS ElastiCache backend. The deployment will be managed by AWS CodePipeline and CodeBuild.\\nWe will use AWS CodePipeline and Code"}
{"Context":"## Context\\nWith the absence of a more complete set of organization-wide standards, I want to document some decisions on how the REST endpoints within this service should behave.\\n","Decision":"## Decision\n* Requests MUST respond with the following status codes:\\n* `200` if the format of the request was valid, and there was meaningful data found\\n* `400` if the format of the request was not valid\\n* `404` if the format of the request was valid, but no meaningful data could be found\\n* JSON responses MUST be in the format of an object, not an array. If a search endpoint needs to return an array, it should be a top-level element of the response object. Rationale: Objects can be extended, arrays cannot.\\n","Prediction":"## Decision\nI will use the [RESTful API](https:\/\/en.wikipedia.org\/wiki\/Representational_state_transfer) as the foundation for all endpoints within this service.\\nI will use JSON Web Tokens (JWT) to authenticate requests.\\nI will use the [JWT Authentication Library](https:\/\/jwt.io\/) to generate and validate JWTs.\\nI will use the [JWT Signature Syntax](https:\/\/jwt.io\/docs\/signature-syntaxis) to validate JWTs.\\nI will use the [JWT Payload Format](https:\/\/jwt.io\/docs\/standard-format) to validate JWTs.\\nI will use the [JWT Policy Language](https:\/\/jwt.io\/docs\/policy-language) to validate JWTs.\\nI will use the [JWT Access Token](https:\/\/jwt.io\/docs\/token-format) to validate JWTs.\\nI will use the [JWT Signature Algorithm](https:\/\/jwt.io\/docs\/signature-algorithm) to validate JWTs.\\nI will use the [JWT Vary: none] option to avoid client-side caching of JWTs.\\nI will use the [JWT Authorization Server](https:\/\/jwt.io\/) to generate and validate JWTs.\\nI will use the [JWT Authorization Server Certificate](https:\/\/jwt.io\/docs\/issuer-cert"}
{"Context":"# Context\\nOur tile servers are under constant load. When errors are thrown, we tend to log useful information,\\nbut the background noise volume is high enough that it's difficult to look at logs and see what was\\ngoing on with a particular request. Separately, it would be nice to have some metrics for functions\\nand api calls used for a request -> response transformation. The constant load makes it almost\\nimpossible to gather useful information outside of a highly controlled and constrained environment.\\nThis difficulty masks issues that only become visible in production.\\nThere are a few approaches to tracing and dumping metrics that we could use, but we don't know\\nanything about any of them.  Some of the libraries available in the scala ecosystem include\\n[`natchez`](https:\/\/github.com\/tpolecat\/natchez), [`http4s-tracer`](https:\/\/github.com\/profunktor\/http4s-tracer),\\nand [`scala-opentracing`](https:\/\/github.com\/Colisweb\/scala-opentracing). Eventually, we will want to be able to dump traces to an external source, like [Jaeger](https:\/\/www.jaegertracing.io\/),\\n[AWS X-Ray](https:\/\/aws.amazon.com\/xray\/), or [Honeycomb](https:\/\/www.honeycomb.io\/).\\nWe'd like to assess how difficult it would be to:\\n- get started with a logging implementation\\n- eventually hook into one of these external services\\nto trace, you'll need a `TracingContextBuilder[F]` for your effect type. The\\nonly off-the-shelf non-trivial one is for DataDog. I don't know if there's a\\nway to get a local copy of the DataDog service running (I don't think there\\nis), so I pretended to have a fancier implementation that was really just\\nwrapping a `new TracingContextBuilder[IO]` around `LoggingTracingContext[IO]`,\\nsince the `LoggingTracingContext` doesn't provide the implicit that I need. It\\nwas fairly easy to set up the dependency, with the exception of adding\\na repository to my resolvers (so we'd need to add that to Nexus in a few\\nplaces as well). We could start with the `LoggingTracingContext` very easily.\\nThe out-of-the-box logging tracer [doesn't print or store tags\\nanywhere](https:\/\/github.com\/Colisweb\/scala-opentracing\/blob\/e9563d6da8d921e1d4c25178ffb5131e047bd9b0\/src\/main\/scala\/com\/colisweb\/tracing\/LoggingTracingContext.scala#L18),\\nwhich is unfortunate, but I don't think it would be hard\\nto roll our own, since it's an 86-line Scala file. If\\nwe want integration with a non-DataDog sink, I _think_\\nit's the case that we can provide a `Tracer` from `io.opentracing` to the\\n[`OpenTracingContext`](https:\/\/github.com\/Colisweb\/scala-opentracing\/blob\/e9563d6da8d921e1d4c25178ffb5131e047bd9b0\/src\/main\/scala\/com\/colisweb\/tracing\/OpenTracingContext.scala)\\nclass and wrap it the same way I wrapped the `LoggingTracingContext`.\\nBecause it looked the most promising, we worked on two extensions to the basic implementation.\\nThe first involved building a custom trace ID based on the incoming request and logging that instead\\nof the randomly generated trace ID that `scala-opentracing`'s `LoggingTracerContext` will\\ncreate on its own. That commit was added to the initial implementation [here](https:\/\/github.com\/jisantuc\/tracing-demos\/tree\/cd8b5b7aa33d35d9f57837451a93b4bdceb96417),\\nand worked fine.\\nThe second extension involved setting up Jaeger as a destination instead of just logging. It's similar,\\nI think, to how any OpenTracing compliant destination will work, though the particular methods for\\nconfiguration will probably vary by target. That commit is available [here](https:\/\/github.com\/jisantuc\/tracing-demos\/tree\/e8986e32da61b0a4d01c688fa2f5c2ca5a432f6f). There's one scala warning if you compile from that point\\nabout how `idGenerator` is unused. I left it in to highlight that the `OpenTracingContext` doesn't let\\nyou choose how to generate trace IDs. However, the Jaeger UI will let you search by tags. Changes to use\\nJaeger and include `X-Amzn-Trace-Id` in the trace tags are [here](https:\/\/github.com\/jisantuc\/tracing-demos\/tree\/fa931325c55286290cebb64b6753186363f2b999).\\nThat search interface looks like this:\\n![Jaeger search](.\/images\/adr-0028-search-by-tag.png)\\nThe initial implementation I wound up with is on\\n[`feature\/js\/scala-opentracing`](https:\/\/github.com\/jisantuc\/tracing-demos\/tree\/f02408d4942c14eec84dd676199056e8e1b99add).\\n### `http4s-tracer`\\n`http4s-tracer` is oriented around atagless final\\nimplementation of tracing. It requires a tracing algebra to\\nbe present in a lot of different places. In examples at least,\\nit requires you to include the tracing activity everywhere.  What\\nthat means in practice is that you don't have as much tracing logic [_around_\\nthings](https:\/\/github.com\/jisantuc\/tracing-demos\/blob\/feature\/js\/scala-opentracing\/app-backend\/api\/src\/main\/scala\/com\/jisantuc\/tracingdemos\/InterpreterService.scala#L27-L29)\\nas you do [within\\nthings](https:\/\/http4s-tracer.profunktor.dev\/guide.html#traced-programs),\\nso your tracing has to be less of an afterthought. It's probable that that\\nencourages better design and thinking about monitoring and instrumentation\\nwhen the code is written. It makes showing a toy example with with only a\\nsingle place to do computation sort of challenging.\\nSomething that appears to be missing is the notion of a \"span\", which was\\ncentral in `natchez` and `scala-opentracing` and also in the Open Tracing spec.\\nThere also doesn't seem to be any notion of tagging the traces with key-value\\npairs, which is also central in the Open Tracing spec. While it would probably\\nmeet our current needs to have a log-only trace ID, we've already done a less\\nfancy version of that with the incoming trace IDs from AWS. Given that \"we\\nwill want to be able to dump traces\", I think the absence of out-of-the-box\\nsupport is concerning, especially given the comparatively high degree of\\ndifficulty for implementing new tracing behaviors.\\nI didn't implement tracing in the demos repository for `http4s-tracer`.\\n# Decision\\nWe should use `scala-opentracing` for our initial implementation of tracing. There\\nare several reasons why this is the case. The general risk in each of these libraries\\nis that they're maintained seemingly by a single person. However, in `scala-opentracing`'s\\ncase, we at least know that the person maintaining it is responsible for the health of\\na production system. It's not super challenging to get started with `scala-opentracing`.\\nI was able to get the logging tracer up and running in under two hours. Chris was able\\nto make substantive changes to it in order to extract a header from the API under 90\\nminutes after he started looking at this ADR. I was later able to configure dumping\\ntraces to Jaeger instead of just logging without considerable difficulty.\\nThose are all very different from the experiences with the other two libraries. `natchez`\\nis advertised as not quite ready for production, and even \"hook it into `http4s`\" is a\\nhard problem right now. There's some [ongoing work](https:\/\/github.com\/tpolecat\/natchez\/issues\/5)\\non that front that looks close to done, but it's also been ongoing for two months, and there's\\nno timeline. `http4s-tracer` uses a separate vocabulary from what's used in the\\nOpen Tracing spec and doesn't have an API for writing traces to external services.\\n# Consequences\\nWe'll need to add `scala-opentracing` to backsplash. This is fairly easy. Getting\\nthe tracer configured will also be fairly easy as long as we're just doing a logging\\ntracer to start. There's some infra work necessary once we want to add third party\\nservice.\\nWe'll need to choose a repository for our traces. We'd like that to be X-Ray, since\\nit means we don't have to configure and deploy another service, but that may not be\\nan option. We can also decide that we should just log for a while, since even that\\nwould be useful. For reference, changes necessary to add Jaeger as a repository\\nfor local development are in\\n[this pr](https:\/\/github.com\/jisantuc\/tracing-demos\/pull\/1\/files).\\nWe'll need to figure out what we want to trace. We could, for example, make all of\\nthe color correction and mosaic implicits functions take implicit trace contexts.\\nI believe it's the case that if we had, e.g., Jaeger as the repository, we could\\nset up the sampler to only send traces on one percent of requests, which would mean\\nwe could deploy some really aggressive tracing and pay whatever price we pay for\\ntracing on one tile in every three or so map loads. If that's true, we should trace\\naggressively. If it's not, we should trace conservatively.\\n","Decision":"## Decision\nWe should use `scala-opentracing` for our initial implementation of tracing. There\\nare several reasons why this is the case. The general risk in each of these libraries\\nis that they're maintained seemingly by a single person. However, in `scala-opentracing`'s\\ncase, we at least know that the person maintaining it is responsible for the health of\\na production system. It's not super challenging to get started with `scala-opentracing`.\\nI was able to get the logging tracer up and running in under two hours. Chris was able\\nto make substantive changes to it in order to extract a header from the API under 90\\nminutes after he started looking at this ADR. I was later able to configure dumping\\ntraces to Jaeger instead of just logging without considerable difficulty.\\nThose are all very different from the experiences with the other two libraries. `natchez`\\nis advertised as not quite ready for production, and even \"hook it into `http4s`\" is a\\nhard problem right now. There's some [ongoing work](https:\/\/github.com\/tpolecat\/natchez\/issues\/5)\\non that front that looks close to done, but it's also been ongoing for two months, and there's\\nno timeline. `http4s-tracer` uses a separate vocabulary from what's used in the\\nOpen Tracing spec and doesn't have an API for writing traces to external services.\\n# Consequences\\nWe'll need to add `scala-opentracing` to backsplash. This is fairly easy. Getting\\nthe tracer configured will also be fairly easy as long as we're just doing a logging\\ntracer to start. There's some infra work necessary once we want to add third party\\nservice.\\nWe'll need to choose a repository for our traces. We'd like that to be X-Ray, since\\nit means we don't have to configure and deploy another service, but that may not be\\nan option. We can also decide that we should just log for a while, since even that\\nwould be useful. For reference, changes necessary to add Jaeger as a repository\\nfor local development are in\\n[this pr](https:\/\/github.com\/jisantuc\/tracing-demos\/pull\/1\/files).\\nWe'll need to figure out what we want to trace. We could, for example, make all of\\nthe color correction and mosaic implicits functions take implicit trace contexts.\\nI believe it's the case that if we had, e.g., Jaeger as the repository, we could\\nset up the sampler to only send traces on one percent of requests, which would mean\\nwe could deploy some really aggressive tracing and pay whatever price we pay for\\ntracing on one tile in every three or so map loads. If that's true, we should trace\\naggressively. If it's not, we should trace conservatively.\\n","Prediction":"## Decision\nWe should use `scala-opentracing` for our initial implementation of tracing. There\\nare several reasons why this is the case. The general risk in each of these libraries\\nis that they're maintained seemingly by a single person. However, in `scala-opentracing`'s\\ncase, we at least know that the person maintaining it is responsible for the health of\\na production system. It's not super challenging to get started with `scala-opentracing`.\\nI was able to get the logging tracer up and running in under two hours. Chris was able\\nto make substantive changes to it in order to extract a header from the API under 90\\nminutes after he started looking at this ADR. I was later able to configure dumping\\ntraces to Jaeger instead of just logging without considerable difficulty.\\nThose are all very different from the experiences with the other two libraries. `natchez`\\nis advertised as not quite ready for production, and even \"hook it into `http4s`\" is a\\nhard problem right now. There's some [ongoing work](https:\/\/github.com\/tpolecat\/natchez\/issues\/5)\\non that front that looks close to done, but it"}
{"Context":"## Context\\nA common pitfall in choice of data storage for currency for money is to use floating point which then results in rounding errors due to the inexact representation of base-10 (decimal) fractional numbers in base-2 (binary) floating point storage that can result in a loss of faith in a calculations system.\\nOptions available:\\n1. [BigDecimal](https:\/\/ruby-doc.org\/stdlib-3.0.0\/libdoc\/bigdecimal\/rdoc\/BigDecimal.html) as pounds.pence\\n2. [BigDecimal](https:\/\/ruby-doc.org\/stdlib-3.0.0\/libdoc\/bigdecimal\/rdoc\/BigDecimal.html) as pence\\n3. The [RubyMoney\/Money gem](https:\/\/github.com\/RubyMoney\/money)\\n4. [float](https:\/\/ruby-doc.org\/core-3.0.0\/Float.html) (default for numbers in ruby)\\nI'm glad to say rounding errors here are [unlikely to be fatal as has been seen before](https:\/\/www.gao.gov\/products\/imtec-92-26) but we'd still like to avoid them.\\n### Research\\n* <https:\/\/www.honeybadger.io\/blog\/ruby-currency\/>\\n","Decision":"## Decision\n### 1. BigDecimal as pounds.pence - yes\\n* Provides accurate rounding compared to float, producing the results that accountants would expect.\\n* Extra dependency, albeit commonly used.\\n* No mismatch between normal written representation of the value and the stored value (compared to storing as pence).\\nBigDecimal provides [`BigDecimal.round`](https:\/\/ruby-doc.org\/stdlib-3.0.0\/libdoc\/bigdecimal\/rdoc\/BigDecimal.html#method-i-round) that we can use to round up to pennies as needed.\\n### 2. BigDecimal as pence - no\\nSome developers like to store currency in pennies.\\nI've not seen a compelling reason to do this. I personally think it adds extra risk of confusion compared to using the decimal type in a way that matches the currency (i.e. pounds as the integer part of a decimal and pennies as the fractional part).\\n* <https:\/\/stackoverflow.com\/questions\/18934774\/what-are-the-downsides-to-storing-money-values-as-cents-minor-units>\\n### 3. RubyMoney - no (for now)\\nThe [RubyMoney\/Money gem](https:\/\/github.com\/RubyMoney\/money):\\nThis type provides extra information about the currency in use (e.g. GBP, USD) which is of no use to us as this is an entirely GBP system.\\nUsing a money type would explicitly shows that the numbers stored are financial which would be nice but isn't critical to understanding.\\nIt also provides formatting and truncation methods. We don't yet know if these will prove useful.\\nIt is judged that the tradeoff for introducing an additional relatively complex dependency for the possible benefits is not currently worthwhile.\\n### 4. Floats - no\\nDo not use for money. Ever.\\nStoring currency in floats causes rounding errors so that's out.\\n","Prediction":"## Decision\nUse BigDecimal for currency in the app.\\n### Positive Consequences\\n* No rounding errors, even for numbers in base-10\\n* Can be used in calculations for money\\n### Negative Consequences\\n* No floating-point numbers\\n* Some may find the implementation a bit confusing at first\\n* Not as easy to use as the other options (BigDecimal, BigDecimal, BigDecimal, BigDecimal)\\n* Ruby will have to add support for BigDecimal (which may not be a problem, but is a pain)\\n* Float is still a bit more complex than the other options.\\n* Float is still a bit more complex than the other options for money (float)\\n* Float is still a bit more complicated than the other options for money (float)\\n## Decision\nUse BigDecimal for currency in the app.\\n### Positive Consequences\\n* No rounding errors, even for numbers in base-10\\n* Can be used in calculations for money\\n### Negative Consequences\\n* No floating-point numbers\\n* Some may find the implementation a bit confusing at first\\n* Not as easy to use as the other options (BigDecimal, BigDecimal, BigDecimal, BigDecimal)\\n* Ruby will have to add support for BigDecimal (which may not be a problem, but is"}
{"Context":"## Contextadjecent\\nWe need to adopt application architecture style adjecent to our architectural drivers\\n","Decision":"## Decision\nWe will use ports&adapters architecture style\\n","Prediction":"## Decision\nWe will use [Dagger](https:\/\/github.com\/airbnb\/dagger) to build our application\\narchitecture\\nWe will use [Dagger 2](https:\/\/github.com\/airbnb\/dagger2) to build our application\\narchitecture\\nWe will use [Dagger 2.1](https:\/\/github.com\/airbnb\/dagger2.1) to build our application\\narchitecture\\nWe will use [Dagger 2.2](https:\/\/github.com\/airbnb\/dagger2.2) to build our application\\narchitecture\\nWe will use [Dagger 2.3](https:\/\/github.com\/airbnb\/dagger2.3) to build our application\\narchitecture\\nWe will use [Dagger 2.4](https:\/\/github.com\/airbnb\/dagger2.4) to build our application\\narchitecture\\nWe will use [Dagger 2.5](https:\/\/github.com\/airbnb\/dagger2.5) to build our application\\narchitecture\\nWe will use [Dagger 2.6](https:\/\/github.com\/airbnb\/dagger2.6) to build our application\\narchitecture\\nWe will use [Dagger 2.7](https:\/\/github.com\/"}
{"Context":"## Context\\nIdempotency guarantees for an action are normally provided for a **limited period of time**. That creates an opportunity for a service to periodically do a **cleanup** of stored actions and as a result avoid ever growing storage requirements and performance implications.\\nWe\u2019ve considered few approaches to this problem:\\n- partition\\n- periodical purge\\nPerformance wise introducing partitions in idempotent_action table seemed like the best idea. Performance benefits outweighed any complications of facilitating and dropping partitions (i.e. there is no way to automatically create partition in Postgres). The issue comes with an absence of global index across partitions in some databases (i.e. Postgres) that creates a limitation that any idempotent action key has to **uniquely map into a partition**. Since the most natural choice of a partition key is a date, it would mean each `action_id` would need to come along with a date or have a date bit embedded, i.e. **UUIDv2**. So far we\u2019ve tried to avoid imposing any limitations on a key format, hence we put the idea of partitioning on hold.\\nPeriodical purge seemed like the only other feasible approach for a database that is lacking time-to-live (TTL) indexes.\\nIt comes with its own challenges like multi-instance set up and DB performance.\\nClients have to be extremely careful with batch size of purging to avoid any negative effect on database performance.\\nTo avoid any need for co-ordination of purge job execution in a multi instance setup or a dependency on Zookeeper\/etcd for a leader election we opted for a lightweight database scheduler `db-scheduler` that guarantees single instance execution.\\n","Decision":"## Decision\nAllow clients to configure retention policy for idempotent actions.\\nClients can specify:\\n- retention period (ISO-8601 format)\\n- cron schedule for purge job (spring format)\\n- batch-size for removal operation\\n```yaml\\nidempotence4j:\\nretention:\\nenabled: true\\nperiod: P0Y0M20D\\npurge:\\nschedule: \"*\/15 * * * * ?\"\\nbatchSize: 150\\n```\\nPurge job will only be executed on a single node in a multi instance set up.\\n","Prediction":"## Decision\nWe will use `db-scheduler` for periodic purge.\\nWe will use `db-scheduler` to trigger periodic purging when an action is created or updated in a database.\\nWe will use `db-scheduler` to trigger periodic purging when an action is deleted from a database.\\nWe will use `db-scheduler` to trigger periodic purging when an action is updated to a database.\\nWe will use `db-scheduler` to trigger periodic purging when an action is deleted from a database.\\nWe will use `db-scheduler` to trigger periodic purging when an action is updated to a database.\\nWe will use `db-scheduler` to trigger periodic purging when an action is deleted from a database.\\nWe will use `db-scheduler` to trigger periodic purging when an action is deleted from a database.\\nWe will use `db-scheduler` to trigger periodic purging when an action is deleted from a database.\\nWe will use `db-scheduler` to trigger periodic purging when an action is deleted from a database.\\nWe will use `db-scheduler` to trigger periodic purging when an action is deleted from a database.\\nWe will use `db-scheduler`"}
{"Context":"## Context\\n### Background\\nCljdoc imports documentation from clojars artifacts and SCM (Source Code Management) repositories.\\nThe SCM is a git repository and usually one hosted on GitHub.\\nDocumentation is imported to cljdoc from 2 distinct sources:\\n1. API docstrings from sources in a clojars artifact\\n2. Stand-alone articles such as `README.md` and documentation under `doc\/**` from an SCM repository.\\nIt is not uncommon for documentation to contain links to other files under in the SCM repository.\\nIn these cases, cljdoc automatically rewrites these links to target the correct resource.\\nIf the SCM document is an article that has been imported to cljdoc, the link will target the imported article hosted by cljdoc,\\notherwise the link will target the hosted SCM, which in most cases, is GitHub.\\n### Issues with Testing Locally\\nBefore a project is released to clojars and automatically imported to cljdoc, an author will often want to preview their\\ndocumentation by running cljdoc locally.\\nBecause cljdoc works from the published artifact, when testing locally, authors will first publish their project to their local maven repository.\\nThis will remain true for whatever solution we choose here.\\nA current limitation of running cljdoc locally, as directed by current guidance, is that direct links to SCM files cannot be previewed.\\nConcrete examples of non-functioning links are images, source code and articles.\\nThese broken links make it difficult to get a real sense of how docs will appear and behave on cljdoc.\\n","Decision":"## Decision\nAfter evaluating several alternatives (see below), I realize that our guidance on running cljdoc locally is more\\nthe issue than anything else.\\nI propose that we complement our guidance to include a workflow of importing from hosted SCM (i.e. GitHub) rather than just the local file system.\\nThe tester would commit their changes and push to a branch on their hosted SCM.\\nThey would then ingest docs from this branch.\\nAt doc browse time, because ingested docs are referencing a hosted SCM, the links to the SCM would automatically work.\\nThe advantages are no extra local setup and an SCM preview that truly reflects production.\\nA minor con is that the author has to push to a hosted SCM.\\n**#2 [rejected, not viable] rewrite SCM links to target the file system**\\nTargeting `file:\/\/` has the advantage of having nothing extra to run for the tester but is\\nsimply not technically feasible; the browser prohibits it for security reasons.\\n**#3 [rejected] rewrite SCM links to target a local static web server**\\nThe tester would launch a static web server from their project root.\\nA locally running cljdoc would render SCM links to the static web server.\\nWhile this would be easy to setup for the tester, the fetches would be to an unversioned\\nfile system rather than SCM. Also the fetches would always be raw, if the browser\/server\\ncombo cannot figure out the MIME type, it would prompt for save\/download. And links to source\\nwould not support navigating to a specific line number.\\nAlthough this was my original plan, because it does not offer a full preview,\\nI have since rejected it.\\n**#3 [rejected]  rewrite SCM links to target a local git instaweb server**\\nThe tester would run `git instaweb` from his\/her project root to startup a local SCM web server.\\nThe advantages to this approach include easy setup for tester, fetches from SCM are versioned,\\nsupport for raw fetches and support for formatted fetches with ability to navigate to line number.\\nThe cons are that git instaweb URLs and UI are not GitHub-like.\\nWe are also relying on the instaweb implementation remaining static, which seems to be a safe bet, but who knows?\\nNote that I have also experimented with [git instaweb via docker](https:\/\/hub.docker.com\/r\/leeread\/gitweb-quick).\\nAlthough this was my secondary choice, I have since decided to that option #1 is simpler.\\n**#4 [rejected] rewrite SCM links to target a local GitHub URL compatible server**\\nThere are GitHub clones like [gitea](https:\/\/gitea.io\/en-us\/) and [gogs](https:\/\/gogs.io\/).\\nPerhaps one of these could be setup to deliver a local project.\\nI had a brief look, but did not qucikly see something that would work, even via docker.\\nThis does not mean this couldn't work, just that I gave up in favor of something simpler.\\nThe pros are GitHub-like URLs and UI.\\nThe cons are a more difficult setup by the user, more moving parts and the maintenance burden\\nof tracking an external component.\\n**#5 [rejected] rewrite SCM links to target a local custom GitHub compatible server**\\nI suppose this would not be terribly hard but it is also not something I want us to have to maintain.\\n**#6 [rejected] add preview support to cljdoc**\\nI'm not sure how this would work, but we could add preview support to the cljdoc production server.\\nThis would be a larger project and have to address many more concerns, so I am rejecting for now.\\n","Prediction":"## Decision\nFor now, the cljdoc project will not be updated to support running locally.\\nWe will monitor the use of this feature and update this decision if needed.\\nThe decision will be made public in the decision document.\\nIf a decision is made to support running locally, the cljdoc project will be updated to support this.\\nIf a decision is made to support running locally, the cljdoc project will be updated to support this.\\nIf a decision is made to support running locally, the cljdoc project will be updated to support this.\\nIf a decision is made to support running locally, the cljdoc project will be updated to support this.\\nIf a decision is made to support running locally, the cljdoc project will be updated to support this.\\nIf a decision is made to support running locally, the cljdoc project will be updated to support this.\\nIf a decision is made to support running locally, the cljdoc project will be updated to support this.\\nIf a decision is made to support running locally, the cljdoc project will be updated to support this.\\nIf a decision is made to support running locally, the cljdoc project will be updated to support this.\\n"}
{"Context":"## Context\\nWhen receiving a User Account Creation response from the MSA, the response contains the attributes required for account creation.\\nA User Account Creation SAML object contains *_VERIFIED attributes which are siblings of the attribute they reference.\\n","Decision":"## Decision\nThe Verify Service Provider will require the service to have requested both an attribute and its\\ncorresponding verified attribute or it will error. This is already enforced by the integration environment form.\\nThe JSON the VSP returns will group the attribute value and verified status together.\\n","Prediction":"## Decision\nThe SAML response will contain the _VERIFIED attributes for the User Account Creation SAML object.\\nThis will allow the SAML response to be validated by the MSA.\\nThis will also allow the MSA to validate the user account creation response.\\n### Positive Consequences\\n* The SAML response will contain the _VERIFIED attributes for the User Account Creation SAML object.\\n* This will allow the SAML response to be validated by the MSA.\\n* This will also allow the MSA to validate the user account creation response.\\n### Negative Consequences\\n* This will require the creation of a new SAML response for every User Account Creation SAML object.\\n* This will require the creation of a new SAML response for every User Account Creation SAML object.\\n### Impact on existing integrations\\n* Existing integrations that need to perform account creation will need to be updated to handle the new SAML response.\\n* Existing integrations that use the _VERIFIED attribute to validate the User Account Creation response will need to be changed to use the _VERIFIED attribute in the SAML response.\\n* Existing integrations that use the _VERIFIED attribute to validate the User Account Creation response will need to be"}
{"Context":"## Context\\nSince Cloud Security Watch specifically aims to monitor for\\nmisconfigurations in AWS accounts it does not make sense to\\nmake the tool cloud agnositic.\\n","Decision":"## Decision\nWhilst we support the notion of writing cloud agnostic\\ncode in general. In this instance it is not appropriate\\nor desirable.\\n","Prediction":"## Decision\nUse the Cloud Security Report to monitor for misconfigurations.\\nThis tool will run CloudWatch Events and CloudWatch Logs against\\ncloudsecwatchd and report any misconfigurations to the user.\\nThis tool will be deployed as a CloudWatch Alarm to notify\\nthe user in the form of an email when a misconfiguration is\\nidentified.\\nThis tool will be deployed as a CloudWatch Cloudwatch\\nAlarm to notify the user in the form of an email when\\na misconfiguration is detected.\\nThis tool will be deployed as a CloudWatch Cloudwatch\\nAlarm to notify the user in the form of an email when\\na misconfiguration is detected.\\nThis tool will be deployed as a CloudWatch Cloudwatch\\nAlarm to notify the user in the form of an email when\\na misconfiguration is detected.\\nThis tool will be deployed as a CloudWatch Cloudwatch\\nAlarm to notify the user in the form of an email when\\na misconfiguration is detected.\\nThis tool will be deployed as a CloudWatch Cloudwatch\\nAlarm to notify the user in the form of an email when\\na misconfiguration is detected.\\nThis tool will be deployed as a CloudWatch Cloudwatch\\nAlarm to notify the user in the form of"}
{"Context":"## Context\\nBy switching the task manager to a distributed implementation, we need to be able to run a `Task` on any node of the cluster.\\n","Decision":"## Decision\nFor the time being we will keep the sequential execution property of the task manager.\\nThis is an intermediate milestone toward the final implementation which will drop this property.\\n* Use a RabbitMQ queue as a workqueue where only the `Created` events are pushed into.\\nInstead of using the brittle exclusive queue mechanism described in [3. Distributed WorkQueue](0003-distributed-workqueue.md), we will\\nnow use the natively supported [Single Active Consumer](https:\/\/www.rabbitmq.com\/consumers.html#single-active-consumer) mechanism.\\n","Prediction":"## Decision\nWe will use the [Kubernetes Operator](https:\/\/operator.kubernetes.io\/) to run the `Task` on each node of the cluster.\\nThis means that the `Task` will be run on any node that is part of the cluster and that has the `kube-system` namespace, as well as the `kubernetes` operator.\\nThis is the same approach that we use for running `Pods` and `Endpoints`.\\nThis will also allow us to use the Kubernetes Operator to run the `Task` in a more distributed manner.\\nThis will be done by using the [Kubernetes Ingress Controller](https:\/\/kubernetes.io\/docs\/concepts\/ingress\/ingress-controller\/) to provide a way to expose the `Task` to the public internet.\\nThis will allow us to run the `Task` in a load balanced manner, meaning that the `Task` can be run on any node in the cluster.\\nThis will also allow us to use the Kubernetes Operator to manage the `Task` and its lifecycle.\\nThis will also allow us to use the Kubernetes Operator to scale the number of instances of the `Task`.\\nThis will also allow us to use the Kubernetes Operator to scale the number of workers that"}
{"Context":"## Problem\\nCurrently it is inefficient to detect the length of an array and\\nit is impossible to know if a key (without subkeys) should be\\nan array or not.\\n","Decision":"## Decision\nStore length in metadata `array` of key, or keep metadata `array` empty if empty array.\\nOnly children that have `#` syntax are allowed in a valid array.\\nThe index start with `#0`.\\nBoth `keyAddName(\"#12\")` or `keyAddBaseName(\"#_12\")` is allowed to add the 13th index.\\nFor example (`ni syntax`, sections used for metadata):\\n```\\nmyarray\/#0 = value0\\nmyarray\/#1 = value1\\nmyarray\/#2 = value2\\nmyarray\/#3 = value3\\nmyarray\/#4 = value4\\nmyarray\/#5 = value5\\n[myarray]\\narray = #5\\n```\\nIt is not allowed to have anything else than `#5` in the metadata `array`,\\ne.g. even `#4` would be a malformed array.\\nWith the metadata `array` in `spec:\/` the `spec` plugin will add the\\n`array` marker with the correct length.\\nThis needs to be happening early, so that plugins can rely on having\\ncorrect arrays.\\nFor example:\\n```\\nspec:\/myarray      # <- has array marker\\nuser:\/myarray      # <- either has correct array marker or no array marker\\nuser:\/myarray\/#0\\nuser:\/myarray\/#1\\n```\\nHere, the `spec` plugin would add `array=#1` to `user:\/myarray` if it was not there before.\\nTo look up an array, first do `ksLookupByName (ks, \"\/myarray\", 0)` on the parent.\\nWith the last index you get from its metadata `array`, iterate over the children.\\nA cascading lookup on every individual child is also needed to make sure that overrides on individual\\nelements are respected.\\nFor example:\\n```\\nspec:\/myarray    # <- contains the specification for the array\\nspec:\/myarray\/#  # <- contains the specification for the array elements\\ndir:\/myarray\/#0  # <- not an array, just an override for user:\/myarray\/#\\nuser:\/myarray    # <- with metadata array=#0, this would be the array we get\\nuser:\/myarray\/#0\\nsystem:\/myarray  # <- not found in cascading lookup, as user:\/myarray exists\\n```\\nThe `spec` plugin should check if it is a valid array, i.e.:\\n- that the parent key always contains the metadata `array`,\\n- that the correct length is in `array`,\\n- that the array only contains `#` children, and\\n- that the children are numbered from `#0` to `#n`, without holes.\\n- [Global Plugins](global_plugins.md)\\n- [Global Validation](global_validation.md)\\n- [Base Names](base_name.md)\\n- [Metadata in Spec Namespace](spec_metadata.md)\\n- [Spec Expressiveness](spec_expressiveness.md)\\n","Prediction":"## Decision\nAdd a type `Array` to the language.\\nAdd a type `Array` to the language. It is defined as follows:\\n```go\\ntype Array []byte\\nfunc (a []byte) len() int\\nfunc (a []byte) key() string\\nfunc (a []byte) isArray() bool\\nfunc (a []byte) isNonArray() bool\\n```\\nIt is a type that can be used to indicate the type of an array.\\n```go\\ntype Array []byte\\nfunc (a []byte) len() int\\nfunc (a []byte) key() string\\nfunc (a []byte) isArray() bool\\nfunc (a []byte) isNonArray() bool\\n```\\nIt is a type that can be used to indicate the type of an array.\\n```go\\ntype Array []byte\\nfunc (a []byte) len() int\\nfunc (a []byte) key() string\\nfunc (a []byte) isArray() bool\\nfunc (a []byte) isNonArray() bool\\n```\\nIt is a type that can be used to indicate the type of an array.\\n```go\\ntype Array []byte\\nfunc (a []"}
{"Context":"## Context\\nWhen deciding on a naming scheme for unit test methods, the following criteria were most important to us:\\n* **Readability** - The test method names should read like English sentences, with clear word boundaries\\n* **Flexibility** - We should be able to follow different patterns, like \"Given ... When .. Then\", but also like \"Does X\".\\n","Decision":"## Decision\nWe use `snake_case` for method names in unit tests. We adapt the coding style settings to ignore the deviation from our usual `camelCase` convention.\\nIf it makes sense, we use sentences containing the words `given`, `when` and `then`. To give each section a clear boundary, when we use `given` or `when`, then we also use `then`.\\nGood:\\ntest_given_first_time_visitor_then_return_main_banner\\nBad:\\ntest_given_first_time_visitor_return_main_banner\\nWe keep in mind that the sentences always refer to the system-under-test (SUT) and don't unnecessarily repeat its class name.\\n","Prediction":"## Decision\nWe will use the following naming conventions:\\n* `TestMethod` - A test method name (usually a single word)\\n* `Given` - A test method name, which is used to assert a set of conditions and is followed by the name of the variable to be tested, e.g. `AssertThat` - A test method name, which is used to assert a set of conditions and is followed by the name of the variable to be tested, e.g. `AssertThat` - A test method name, which is used to assert a set of conditions and is followed by the name of the variable to be tested, e.g. `AssertThat` - A test method name, which is used to assert a set of conditions and is followed by the name of the variable to be tested, e.g. `AssertThat` - A test method name, which is used to assert a set of conditions and is followed by the name of the variable to be tested, e.g. `AssertThat` - A test method name, which is used to assert a set of conditions and is followed by the name of the variable to be tested, e.g. `AssertThat` - A test method name, which is used to assert a set"}
{"Context":"## Context\\nOPG Metrics will live in the opg-management account, which is home to other products and services.\\nWe want to keep OPG Metrics infrasture separate from those other products so that it can be more portable across accounts and avoids impacting the performance or configuration of those other products.\\n","Decision":"## Decision\nOPG Metrics will have it's own VPC.\\n","Prediction":"## Decision\nWe will create a new account, OPG Metrics, for OPG Metrics metrics.\\nThis will be used by all OPG Metrics services.\\nWe will use the same Google Cloud storage bucket and bucket policy as the other Google Cloud services.\\nWe will use the same IAM roles and policies as the other Google Cloud services.\\nWe will use the same billing account as the other Google Cloud services.\\nWe will use the same versioning strategy as the other Google Cloud services.\\nWe will use the same monitoring strategy as the other Google Cloud services.\\nWe will use the same audit log strategy as the other Google Cloud services.\\nWe will use the same versioning strategy as the other Google Cloud services.\\nWe will use the same versioning strategy as the other Google Cloud services.\\nWe will use the same versioning strategy as the other Google Cloud services.\\nWe will use the same versioning strategy as the other Google Cloud services.\\nWe will use the same versioning strategy as the other Google Cloud services.\\nWe will use the same versioning strategy as the other Google Cloud services.\\nWe will use the same versioning strategy as the other Google Cloud services.\\nWe will use the same versioning strategy as the other Google"}
{"Context":"## Context and Problem Statement\\nWe want to use a well established UI framework in order to bootstrap and boost our development process for state-of-the-art web applications.\\n","Decision":"## Decision\nWe chosen option 1, Angular, since existing knowledge and experience with this framework is available.\\n","Prediction":"## Decision\nWe will use React, since it is a well established, maintainable and tested framework that is widely used in the industry.\\nWe will use [React Router](https:\/\/facebook.github.io\/react-router\/) to manage routes and state.\\nWe will use [Redux](https:\/\/redux.js.org\/)\\nWe will use [Next.js](https:\/\/nextjs.org\/) for server-rendered rendering.\\nWe will use [React Router Navigation](https:\/\/facebook.github.io\/react-router\/docs\/navigation) to manage the router.\\nWe will use [Redux Router](https:\/\/redux.js.org\/)\\nWe will use [Apollo](https:\/\/facebook.github.io\/apollo) to manage the state of the application.\\nWe will use [PostCSS](https:\/\/postcss.org\/)\\nWe will use [ESLint](https:\/\/eslint.org\/)\\nWe will use [NPM](https:\/\/www.npmjs.com\/)\\n### Positive Consequences\\n* We will be using a well established framework that has been widely adopted in the industry.\\n* We will be using a state management library that has been widely adopted in the industry.\\n* We will be using a popular routing library that has been widely adopted in the industry.\\n*"}
{"Context":"Context\\n-------\\nWe currently use Apache on all our web hosts. While writing a profile\\nfor forwarding HTTPS to a local HTTP port, I found that the puppet\\nApache module didn't provide a simple approach to this. I would have had\\nto use `mod_proxy` and then add custom fragments.\\nOn the other hand, if I went with nginx instead of Apache, such a proxy\\ncould be configured with a single line:\\nproxy => \"http:\/\/localhost:${port}\",\\nThis carries with it a cost however. If we use nginx in this one case,\\nthen we can no longer claim to use a single web server, as we'll be\\nusing Apache on some servers and nginx on others.\\nDecision\\n--------\\nWe will use nginx for simple cases where all we need is to forward to a\\nlocal port.\\nConsequences\\n------------\\nForwarding https traffic to a local port is very easy. We now use both\\nnginx and Apache.\\n","Decision":"## Decision\n--------\\nWe will use nginx for simple cases where all we need is to forward to a\\nlocal port.\\nConsequences\\n------------\\nForwarding https traffic to a local port is very easy. We now use both\\nnginx and Apache.\\n","Prediction":"## Decision\n--------\\nWe will use nginx for simple cases where all we need is to forward to a\\nlocal port.\\nConsequences\\n------------\\nForwarding https traffic to a local port is very easy. We now use both\\nnginx and Apache.\\n"}
{"Context":"## Context\\nRecord the architectural decisions made on this project.\\n","Decision":"## Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","Prediction":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\nWe will use [design by contract](http:\/\/en.wikipedia.org\/wiki\/Design_by_contract) to enforce the decision.\\nWe will use [design by convention](http:\/\/en.wikipedia.org\/wiki\/Design_by_convention) to enforce the decision.\\nWe will use [design by convention](http:\/\/en.wikipedia.org\/wiki\/Design_by_convention) to enforce the decision.\\nWe will use [design by convention](http:\/\/en.wikipedia.org\/wiki\/Design_by_convention) to enforce the decision.\\nWe will use [design by convention](http:\/\/en.wikipedia.org\/wiki\/Design_by_convention) to enforce the decision.\\nWe will use [design by convention](http:\/\/en.wikipedia.org\/wiki\/Design_by_convention) to enforce the decision.\\nWe will use [design by convention](http:\/\/en.wikipedia.org\/wiki\/Design_by_convention) to enforce the decision.\\nWe will use [design by convention](http:\/\/en.wikipedia.org\/wiki\/Design_by_convention) to enforce the decision.\\nWe will use [design by convention"}
{"Context":"## Context and Problem Statement\\nIn the problem detection and solving approach by Saatkamp et al., detected problems in a topology are solved by specific algorithms.\\nThese algorithms must know some semantics in order to perform correctly.\\nTherefore, collection of predefined and known elements which the algorithms can work on is required.\\n","Decision":"## Decision\nChosen option: \"Predefined elements in constants\".\\nHowever, in near future, we could make this configurable by using the new configuration which is currently implemented by some students.\\n","Prediction":"## Decision\nChosen option: \"Include [a predefined set of algorithms] in the topology\", because it is the most generic approach.\\nThis approach does not require any knowledge about the algorithms used by the problem solver to be included in the topology.\\nIt is also less verbose and has the added benefit of allowing for future changes to the topology (e.g. adding new algorithms).\\nIt is also less verbose and has the added benefit of allowing for future changes to the topology (e.g. adding new algorithms).\\nIt is also less verbose and has the added benefit of allowing for future changes to the topology (e.g. adding new algorithms).\\nIt is also less verbose and has the added benefit of allowing for future changes to the topology (e.g. adding new algorithms).\\nIt is also less verbose and has the added benefit of allowing for future changes to the topology (e.g. adding new algorithms).\\nIt is also less verbose and has the added benefit of allowing for future changes to the topology (e.g. adding new algorithms).\\nIt is also less verbose and has the added benefit of allowing for future changes to the topology (e.g. adding new algorithms).\\nIt is also less verbose and has the added benefit of allowing for future changes"}
{"Context":"## Context\\nNorth Cumbria CCG and Redbridge CCG do not have an ODS code. As a consequence it is not possible to relate them to the IAPT services. When a GP is serving under either of these CCGs we can display the information for the CCG.\\n","Decision":"## Decision\nRather than display no result we know the contact information for each of these CCGs. We have decided to store this information locally and display it for the user.\\nThis is only a temporary measure and the change will be reverted once the data has been loaded into the central system.\\n","Prediction":"## Decision\nWe have decided to use the IAPT code for both CCGs. This will be displayed in the IAPT section of the website.\\nWe will also display the IAPT code for the CCGs in the NHS Choices search. This is not yet implemented but will be done before the end of the project.\\nWe will also display the IAPT code in the NHS Choices search for the CCGs, but only for the next 12 months.\\nTo make this work we will need to set up a new API key for the NHS Choices search. This is a new API key and will not be used in production. It will only be used for the next 12 months.\\nWe will also need to update the NHS Choices search to display the new IAPT code. This is a new API key and will not be used in production. It will only be used for the next 12 months.\\nWe will also need to update the NHS Choices search to display the new IAPT code and the NHS Choices search will display the new IAPT code and the NHS Choices search will display the new IAPT code.\\nWe will also need to update the NHS Choices search to display the new IAPT code and the NHS Choices search will display"}
{"Context":"## Context\\nWe want to enforce consistency in our code, and catch as many errors\\nautomatically as we are able to. Linting the code is good practice to achieve\\nthese aims. [ESLint](https:\/\/eslint.org\/) is the standard linter for modern\\nJavaScript, and has good support for TypeScript and React though plugins.\\n","Decision":"## Decision\nWe will check code style using ESLint.\\nWe will let Prettier have precedence when ESLint and Prettier conflict in their\\nstyles.\\nWe will use the recommended configuration for plugins where possible.\\nWe will run ESLint as part of the test suite.\\n","Prediction":"## Decision\nWe will use ESLint for linting.\\nWe will use a plugin to enable code formatting.\\nWe will use a plugin to enforce style.\\nWe will run linting and formatting as part of the build process.\\nWe will run linting and formatting as part of the code review process.\\nWe will run linting and formatting as part of the code review process.\\nWe will run linting and formatting as part of the code review process.\\nWe will run linting and formatting as part of the code review process.\\nWe will run linting and formatting as part of the code review process.\\nWe will run linting and formatting as part of the code review process.\\nWe will run linting and formatting as part of the code review process.\\nWe will run linting and formatting as part of the code review process.\\nWe will run linting and formatting as part of the code review process.\\nWe will run linting and formatting as part of the code review process.\\nWe will run linting and formatting as part of the code review process.\\nWe will run linting and formatting as part of the code review process.\\nWe will run linting and formatting as part of the code review process.\\"}
{"Context":"## Context and Problem Statement\\nDecision was required on what sub-section to be analysed under Receipts. The requirements for Receipts section in the current scope was discussed.\\n","Decision":"## Decision\n- Not in the project scope for either of the engagements\\n- Good to have and can be incorporated at a later date\\nReceipts section parked for analysis scope, can be picked up later depending to requirements.\\n","Prediction":"## Decision\nThe current scope of the project will be the Receipts sub-section. This will be used to analyse all the different types of Receipts.\\nTo analyse Receipts, two different sub-sections will be created:\\n* Receipts - Collection Receipts\\n* Receipts - Payment Receipts\\n* Receipts - Deposit Receipts\\n* Receipts - Invoice Receipts\\n* Receipts - Statement Receipts\\n* Receipts - Authorization Receipts\\n* Receipts - Other Receipts\\n* Receipts - Delivery Receipts\\n* Receipts - Other Receipts\\n* Receipts - Miscellaneous Receipts\\n* Receipts - Miscellaneous Receipts\\n* Receipts - Miscellaneous Receipts\\n* Receipts - Miscellaneous Receipts\\n* Receipts - Miscellaneous Receipts\\n* Receipts - Miscellaneous Receipts\\n* Receipts - Miscellaneous Receipts\\n* Receipts - Miscellaneous Receipts\\n* Receipts - Miscellaneous Receipts\\n* Receipts - Miscellaneous Receipt\\n* Receipts - Miscellaneous Receipt\\n* Receipts - Miscellaneous Receipt\\n* Receipts - Miscellaneous Receipt\\n* Receipts - Miscellaneous Receipt\\n* Receipts - Miscellaneous Receipt\\n* Receipts - Miscellaneous Receipt\\n* Receipts - Miscellaneous Receipt\\n* Receipts - Miscellaneous Receipt\\n* Receipt"}
{"Context":"## Context\\nWe want a generic runtime that let's us deploy the services a uniform way in various environments like AWS, Azure, Kubernetes or locally.\\nDocker is the de-facto standard today and is the system used by us for all projects.\\n","Decision":"## Decision\nUse docker to deploy all services.\\n","Prediction":"## Decision\nWe will use Docker to run the application.\\nWe will use Docker Compose to deploy the application into production.\\nWe will use Docker Swarm to manage the application across multiple nodes.\\nWe will use Kubernetes to manage the application across multiple nodes.\\nWe will use Helm to manage the application in a single namespace.\\nWe will use Helm Charts to provide the application with a consistent user interface.\\nWe will use Helm Charts to provide the application with a consistent versioning scheme.\\nWe will use `Docker Compose` for building and maintaining the application. This will be used by the `Dockerfile` and `docker-compose.yml` files.\\nWe will use `Docker Compose` for managing the application. This will be used by the `Dockerfile` and `docker-compose.yml` files.\\nWe will use `Docker Compose` for managing the Docker images. This will be used by the `Dockerfile` and `docker-compose.yml` files.\\nWe will use `Docker Compose` for managing the containers. This will be used by the `Dockerfile` and `docker-compose.yml` files.\\nWe will use `Docker Compose` for managing the Kubernetes resources. This"}
{"Context":"## Context\\nThe ExternalService Operator uses a Custom Resource Definition [..\/..\/deploy\/crds\/eso_v1alpha1_externalservice_cr.yaml]() in Order to describe external services which made available for the Kubernetes Cluster as they would be running inside the cluster.\\nThis makes it possible to expose services to a central in the cluster running ingress container.\\nFor both mechanisms, but especially the ingress routing one, it is not needed to have more than one EndpointSubset or having more than one port.\\nAs you might see, the Ingress Ressource Rules are only capable of having one Service and one Port, so this are the Kubernetes Resources managed by the Operator\\n```\\napiVersion: v1\\nitems:\\n- apiVersion: extensions\/v1beta1\\nkind: Ingress\\nmetadata:\\nannotations:\\nkubectl.kubernetes.io\/last-applied-configuration: |\\n{\"apiVersion\":\"eso.crowdfox.com\/v1alpha1\",\"kind\":\"ExternalService\",\"metadata\":{\"annotations\":{\"nginx.ingress.kubernetes.io\/rewrite-target\":\"\/\"},\"name\":\"example-externalservice\",\"namespace\":\"default\"},\"spec\":{\"hosts\":[{\"host\":\"subdomain.example.com\",\"path\":\"\/\"}],\"ips\":[\"192.168.0.10\",\"192.168.0.11\",\"192.168.0.12\"],\"port\":80}}\\nnginx.ingress.kubernetes.io\/rewrite-target: \/\\ncreationTimestamp: 2019-07-16T13:32:13Z\\ngeneration: 4\\nlabels:\\napp: example-externalservice\\nserviceType: external\\nname: example-externalservice\\nnamespace: default\\nresourceVersion: \"39534\"\\nselfLink: \/apis\/extensions\/v1beta1\/namespaces\/default\/ingresses\/example-externalservice\\nuid: 1f255906-a7ce-11e9-8a65-0800278d3f0c\\nspec:\\nrules:\\n- host: test.crowdfox.test\\nhttp:\\npaths:\\n- backend:\\nserviceName: example-externalservice\\nservicePort: 9090\\npath: \/\\nstatus:\\nloadBalancer: {}\\n- apiVersion: v1\\nkind: Service\\nmetadata:\\ncreationTimestamp: 2019-07-16T13:32:13Z\\nlabels:\\napp: example-externalservice\\nserviceType: external\\nname: example-externalservice\\nnamespace: default\\nresourceVersion: \"39533\"\\nselfLink: \/api\/v1\/namespaces\/default\/services\/example-externalservice\\nuid: 1f14497e-a7ce-11e9-8a65-0800278d3f0c\\nspec:\\nclusterIP: None\\nports:\\n- port: 9090\\nprotocol: TCP\\ntargetPort: 9090\\nsessionAffinity: None\\ntype: ClusterIP\\nstatus:\\nloadBalancer: {}\\n- apiVersion: v1\\nkind: Endpoints\\nmetadata:\\ncreationTimestamp: 2019-07-16T13:32:13Z\\nlabels:\\napp: example-externalservice\\nserviceType: external\\nname: example-externalservice\\nnamespace: default\\nownerReferences:\\n- apiVersion: eso.crowdfox.com\/v1alpha1\\nblockOwnerDeletion: true\\ncontroller: true\\nkind: ExternalService\\nname: example-externalservice\\nuid: 1eeffc8f-a7ce-11e9-8a65-0800278d3f0c\\nresourceVersion: \"42925\"\\nselfLink: \/api\/v1\/namespaces\/default\/endpoints\/example-externalservice\\nuid: 1f041bf1-a7ce-11e9-8a65-0800278d3f0c\\nsubsets:\\n- notReadyAddresses:\\n- ip: 192.168.0.10\\n- ip: 192.168.0.12\\nports:\\n- port: 9090\\nprotocol: TCP\\nkind: List\\nmetadata:\\nresourceVersion: \"\"\\nselfLink: \"\"\\n```\\n","Decision":"## Decision\nThe implementation relies on this decision, that there is always exactly one EndpointSubset and in that EndpointSubset exactly one Port defined.\\n","Prediction":"## Decision\nThe Operator will use the [Custom Resource Definition](..\/deploy\/crds\/eso_v1alpha1_externalservice_cr.yaml) to describe the external services to the Kubernetes Cluster.\\nThis will also make it possible to expose services to a central in the cluster running ingress container.\\nFor both mechanisms, but especially the ingress routing one, it is not needed to have more than one EndpointSubset or having more than one port.\\nAs you might see, the Ingress Ressource Rules are only capable of having one Service and one Port, so this are the Kubernetes Resources managed by the Operator\\n```\\napiVersion: v1\\nitems:\\n- apiVersion: extensions\/v1beta1\\nkind: Ingress\\nmetadata:\\nannotations:\\nkubectl.kubernetes.io\/last-applied-configuration: |\\n{\"apiVersion\":\"extensions\/v1beta1\",\"kind\":\"ExternalService\",\"metadata\":{\"annotations\":{\"nginx.ingress.kubernetes.io\/rewrite-target\":\"\/\"},\"name\":\"example-externalservice\",\"namespace\":\"default\"},\"spec\":{\"hosts\":[{\"host\":\"subdomain.example.com\",\"path\":\"\/\"}],\"ips\":[\"192.168.0.10\",\"192.168.0.11\",\"192.168.0.12\"],\""}
{"Context":"## Context\\nThe concrete use cases which motivated this module include:\\n* the desire to delegate the ability to vote on proposals to other accounts besides the account which one has\\ndelegated stake\\n* \"sub-keys\" functionality, as originally proposed in [\\#4480](https:\/\/github.com\/cosmos\/cosmos-sdk\/issues\/4480) which\\nis a term used to describe the functionality provided by this module together with\\nthe `fee_grant` module from [ADR 029](.\/adr-029-fee-grant-module.md) and the [group module](https:\/\/github.com\/cosmos\/cosmos-sdk\/tree\/main\/x\/group).\\nThe \"sub-keys\" functionality roughly refers to the ability for one account to grant some subset of its capabilities to\\nother accounts with possibly less robust, but easier to use security measures. For instance, a master account representing\\nan organization could grant the ability to spend small amounts of the organization's funds to individual employee accounts.\\nOr an individual (or group) with a multisig wallet could grant the ability to vote on proposals to any one of the member\\nkeys.\\nThe current implementation is based on work done by the [Gaian's team at Hackatom Berlin 2019](https:\/\/github.com\/cosmos-gaians\/cosmos-sdk\/tree\/hackatom\/x\/delegation).\\n","Decision":"## Decision\nWe will create a module named `authz` which provides functionality for\\ngranting arbitrary privileges from one account (the _granter_) to another account (the _grantee_). Authorizations\\nmust be granted for a particular `Msg` service methods one by one using an implementation\\nof `Authorization` interface.\\n### Types\\nAuthorizations determine exactly what privileges are granted. They are extensible\\nand can be defined for any `Msg` service method even outside of the module where\\nthe `Msg` method is defined. `Authorization`s reference `Msg`s using their TypeURL.\\n#### Authorization\\n```go\\ntype Authorization interface {\\nproto.Message\\n\/\/ MsgTypeURL returns the fully-qualified Msg TypeURL (as described in ADR 020),\\n\/\/ which will process and accept or reject a request.\\nMsgTypeURL() string\\n\/\/ Accept determines whether this grant permits the provided sdk.Msg to be performed, and if\\n\/\/ so provides an upgraded authorization instance.\\nAccept(ctx sdk.Context, msg sdk.Msg) (AcceptResponse, error)\\n\/\/ ValidateBasic does a simple validation check that\\n\/\/ doesn't require access to any other information.\\nValidateBasic() error\\n}\\n\/\/ AcceptResponse instruments the controller of an authz message if the request is accepted\\n\/\/ and if it should be updated or deleted.\\ntype AcceptResponse struct {\\n\/\/ If Accept=true, the controller can accept and authorization and handle the update.\\nAccept bool\\n\/\/ If Delete=true, the controller must delete the authorization object and release\\n\/\/ storage resources.\\nDelete bool\\n\/\/ Controller, who is calling Authorization.Accept must check if `Updated != nil`. If yes,\\n\/\/ it must use the updated version and handle the update on the storage level.\\nUpdated Authorization\\n}\\n```\\nFor example a `SendAuthorization` like this is defined for `MsgSend` that takes\\na `SpendLimit` and updates it down to zero:\\n```go\\ntype SendAuthorization struct {\\n\/\/ SpendLimit specifies the maximum amount of tokens that can be spent\\n\/\/ by this authorization and will be updated as tokens are spent. This field is required. (Generic authorization\\n\/\/ can be used with bank msg type url to create limit less bank authorization).\\nSpendLimit sdk.Coins\\n}\\nfunc (a SendAuthorization) MsgTypeURL() string {\\nreturn sdk.MsgTypeURL(&MsgSend{})\\n}\\nfunc (a SendAuthorization) Accept(ctx sdk.Context, msg sdk.Msg) (authz.AcceptResponse, error) {\\nmSend, ok := msg.(*MsgSend)\\nif !ok {\\nreturn authz.AcceptResponse{}, sdkerrors.ErrInvalidType.Wrap(\"type mismatch\")\\n}\\nlimitLeft, isNegative := a.SpendLimit.SafeSub(mSend.Amount)\\nif isNegative {\\nreturn authz.AcceptResponse{}, sdkerrors.ErrInsufficientFunds.Wrapf(\"requested amount is more than spend limit\")\\n}\\nif limitLeft.IsZero() {\\nreturn authz.AcceptResponse{Accept: true, Delete: true}, nil\\n}\\nreturn authz.AcceptResponse{Accept: true, Delete: false, Updated: &SendAuthorization{SpendLimit: limitLeft}}, nil\\n}\\n```\\nA different type of capability for `MsgSend` could be implemented\\nusing the `Authorization` interface with no need to change the underlying\\n`bank` module.\\n##### Small notes on `AcceptResponse`\\n* The `AcceptResponse.Accept` field will be set to `true` if the authorization is accepted.\\nHowever, if it is rejected, the function `Accept` will raise an error (without setting `AcceptResponse.Accept` to `false`).\\n* The `AcceptResponse.Updated` field will be set to a non-nil value only if there is a real change to the authorization.\\nIf authorization remains the same (as is, for instance, always the case for a [`GenericAuthorization`](#genericauthorization)),\\nthe field will be `nil`.\\n### `Msg` Service\\n```protobuf\\nservice Msg {\\n\/\/ Grant grants the provided authorization to the grantee on the granter's\\n\/\/ account with the provided expiration time.\\nrpc Grant(MsgGrant) returns (MsgGrantResponse);\\n\/\/ Exec attempts to execute the provided messages using\\n\/\/ authorizations granted to the grantee. Each message should have only\\n\/\/ one signer corresponding to the granter of the authorization.\\nrpc Exec(MsgExec) returns (MsgExecResponse);\\n\/\/ Revoke revokes any authorization corresponding to the provided method name on the\\n\/\/ granter's account that has been granted to the grantee.\\nrpc Revoke(MsgRevoke) returns (MsgRevokeResponse);\\n}\\n\/\/ Grant gives permissions to execute\\n\/\/ the provided method with expiration time.\\nmessage Grant {\\ngoogle.protobuf.Any       authorization = 1 [(cosmos_proto.accepts_interface) = \"cosmos.authz.v1beta1.Authorization\"];\\ngoogle.protobuf.Timestamp expiration    = 2 [(gogoproto.stdtime) = true, (gogoproto.nullable) = false];\\n}\\nmessage MsgGrant {\\nstring granter = 1;\\nstring grantee = 2;\\nGrant grant = 3 [(gogoproto.nullable) = false];\\n}\\nmessage MsgExecResponse {\\ncosmos.base.abci.v1beta1.Result result = 1;\\n}\\nmessage MsgExec {\\nstring   grantee                  = 1;\\n\/\/ Authorization Msg requests to execute. Each msg must implement Authorization interface\\nrepeated google.protobuf.Any msgs = 2 [(cosmos_proto.accepts_interface) = \"cosmos.base.v1beta1.Msg\"];;\\n}\\n```\\n### Router Middleware\\nThe `authz` `Keeper` will expose a `DispatchActions` method which allows other modules to send `Msg`s\\nto the router based on `Authorization` grants:\\n```go\\ntype Keeper interface {\\n\/\/ DispatchActions routes the provided msgs to their respective handlers if the grantee was granted an authorization\\n\/\/ to send those messages by the first (and only) signer of each msg.\\nDispatchActions(ctx sdk.Context, grantee sdk.AccAddress, msgs []sdk.Msg) sdk.Result`\\n}\\n```\\n### CLI\\n#### `tx exec` Method\\nWhen a CLI user wants to run a transaction on behalf of another account using `MsgExec`, they\\ncan use the `exec` method. For instance `gaiacli tx gov vote 1 yes --from <grantee> --generate-only | gaiacli tx authz exec --send-as <granter> --from <grantee>`\\nwould send a transaction like this:\\n```go\\nMsgExec {\\nGrantee: mykey,\\nMsgs: []sdk.Msg{\\nMsgVote {\\nProposalID: 1,\\nVoter: cosmos3thsdgh983egh823\\nOption: Yes\\n}\\n}\\n}\\n```\\n#### `tx grant <grantee> <authorization> --from <granter>`\\nThis CLI command will send a `MsgGrant` transaction. `authorization` should be encoded as\\nJSON on the CLI.\\n#### `tx revoke <grantee> <method-name> --from <granter>`\\nThis CLI command will send a `MsgRevoke` transaction.\\n### Built-in Authorizations\\n#### `SendAuthorization`\\n```protobuf\\n\/\/ SendAuthorization allows the grantee to spend up to spend_limit coins from\\n\/\/ the granter's account.\\nmessage SendAuthorization {\\nrepeated cosmos.base.v1beta1.Coin spend_limit = 1;\\n}\\n```\\n#### `GenericAuthorization`\\n```protobuf\\n\/\/ GenericAuthorization gives the grantee unrestricted permissions to execute\\n\/\/ the provided method on behalf of the granter's account.\\nmessage GenericAuthorization {\\noption (cosmos_proto.implements_interface) = \"Authorization\";\\n\/\/ Msg, identified by it's type URL, to grant unrestricted permissions to execute\\nstring msg = 1;\\n}\\n```\\n","Prediction":"## Decision\nWe will use the `sub-keys` functionality for delegating the capability to vote on proposals. This functionality\\nwill be implemented as a new feature in the `proposal` module. This feature will be a simple implementation of the\\n`sub-keys` functionality provided by the `proposal` module, with the benefit of being able to be implemented\\nin a way that is more familiar to the Cosmos SDK user base.\\n### Decision Process\\nWe will implement the `sub-keys` functionality as a new feature in the `proposal` module. This feature will be\\nimplemented as a new feature in the `proposal` module, with the benefit of being able to be implemented\\nin a way that is more familiar to the Cosmos SDK user base.\\n### Decision\\nWe will implement the `sub-keys` functionality as a new feature in the `proposal` module. This feature will be\\nimplemented as a new feature in the `proposal` module, with the benefit of being able to be implemented\\nin a way that is more familiar to the Cosmos SDK user base.\\n### Decision\\nWe will implement the `sub-keys` functionality as a new feature in the `proposal` module. This feature will be\\nimplemented as a new feature in the `proposal`"}
{"Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","Prediction":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\nWe will use [Decision Manager](http:\/\/decisionmanager.org) to keep track of our decisions.\\nWe will use [Architecture Decision Suite](https:\/\/github.com\/airbrake\/airbrake) to manage our architecture decisions.\\nWe will use [Architecture Code Complete](http:\/\/www.amazon.com\/Architecture-Code-Complete-Stephen-Krensky\/dp\/0132074260) to keep our architectural decisions.\\nWe will use [Architecture Style Guide](http:\/\/www.amazon.com\/Architecture-Style-Guide-Code-Complete\/dp\/0132074252) to make sure that our architecture decisions are consistent.\\nWe will use [Decision Log](http:\/\/www.opengrad.org\/decision-log) to record our architectural decisions.\\nWe will use [Architectural Styles Explained](http:\/\/www.amazon.com\/Architectural-Styles-Explained-Adopting\/dp\/0136099955) to help us decide on our architecture style.\\nWe will use [Architecture at Work](http:\/\/www.amazon.com"}
{"Context":"## Context\\nThis application will have 2 front end and possibly a mobile app.\\nThese differents access will use common and specifics API. Somes datas will be use differently.\\n","Decision":"## Decision\nEvery front end will have a specific backend to manage the differents needs.\\n","Prediction":"## Decision\nWe will use the same API for all access. We will use the same data model.\\nWe will use the same data store.\\nWe will use the same version control.\\nWe will use the same monitoring tool.\\nWe will use the same CI\/CD tool.\\nWe will use the same documentation.\\nWe will use the same documentation for all services.\\nWe will use the same documentation for all APIs.\\nWe will use the same documentation for all logs.\\nWe will use the same documentation for all metrics.\\nWe will use the same documentation for all dashboards.\\nWe will use the same documentation for all CI\/CD workflows.\\nWe will use the same documentation for all logs.\\nWe will use the same documentation for all metrics.\\nWe will use the same documentation for all dashboards.\\nWe will use the same documentation for all CI\/CD workflows.\\nWe will use the same documentation for all logs.\\nWe will use the same documentation for all metrics.\\nWe will use the same documentation for all dashboards.\\nWe will use the same documentation for all CI\/CD workflows.\\nWe will use the same documentation for all logs.\\nWe will use the same documentation for all metrics.\\n"}
{"Context":"## Context\\nWe faced a problem that missing test cases on some presenters because of not injectable variables like ab tests, config or global values on presenters. So we want to cover all of these missings.\\n","Decision":"## Decision\nWe decided to inject this variables to related presenters from their constructors.\\n","Prediction":"## Decision\nWe decided to create a new test suite for all injectable values and variables.\\nThis suite will be run on both the test runner and on the presenter.\\nThis test suite will be used to validate the injectable values and variables.\\nIt will also be used to validate the injectable variables.\\nIt will be used to validate the injectable classes.\\nIt will be used to validate the injectable parameters.\\nIt will be used to validate the injectable properties.\\nThis suite will cover all injectable variables and classes.\\nThis suite will cover all injectable properties.\\nThis suite will cover all injectable classes.\\nThis suite will cover all injectable parameters.\\nThis suite will cover all injectable classes.\\nThis suite will cover all injectable parameters.\\nThis suite will cover all injectable parameters.\\nThis suite will cover all injectable parameters.\\nThis suite will cover all injectable parameters.\\nThis suite will cover all injectable parameters.\\nThis suite will cover all injectable parameters.\\nThis suite will cover all injectable parameters.\\nThis suite will cover all injectable parameters.\\nThis suite will cover all injectable parameters.\\nThis suite will cover all injectable parameters.\\nThis suite will"}
{"Context":"## Context\\nThe key component of this project is a 'Bicycle Network Analysis' task which is run on an arbitrary, user-provided neighborhood boundary. This task performs the following actions:\\n- Import neighborhood boundary into a PostgreSQL database\\n- Download OSM extract for the provided neighborhood boundary + a buffer and import to PostgreSQL\\n- Download related census block and job data for the boundary and import to PostgreSQL\\n- Generate a network graph from the imported data\\n- Run a series of client-provided analyses on the graph and imported data\\n- Export relevant data to an external file store for archival\\n- Generate a tile set of the network graph for display on a web map\\nThe application will be configured with multiple organizations, and each organization can only run one analysis job at a time. A new analysis job triggered by a user of the organization will supersede any existing older analysis, which can be thrown away.\\nSince the analysis workflow is already a self-contained process, there are a few ways to trigger this job, and a few options for an asynchronous task queue. One option is to use Celery, a tool we are familiar with, to provide a known interface to trigger these analysis jobs. Another is to configure the analysis as an AWS ECS task, and have the application use the ECS API or Boto to start a new analysis.\\nCelery has multiple options for brokers:\\n| Broker | Advantages | Disadvantages |\\n| ------ | ---------- | ------------- |\\n| SQS | Cheap, easy to set up, now stable, provides configuration options to isolate environments | No result backend, [potential issues with result timeouts](http:\/\/docs.celeryproject.org\/en\/latest\/getting-started\/brokers\/sqs.html#caveats) |\\n| Redis | Trivial to configure, can additionally be used as a results backend without further architecting | Key eviction issues, additional cost to run dedicated instance |\\nRunning the analysis via AWS Lambda was briefly considered, but the project dependencies and resources required are not conducive to that environment.\\n","Decision":"## Decision\nThe team will use Celery + SQS broker to manage the asynchronous analysis jobs. While Celery is not strictly necssary, it provides a potentially useful abstraction layer for triggering tasks, managing jobs and reporting errors. Celery also provides out of the box support for Django and allows us to write any peripheral task logic in Python. The SQS broker was chosen to keep the managed application architecture simple and reduce ongoing application stack costs. The team is familiar with an older version of the SQS broker used for the Cicero District Match project.\\n","Prediction":"## Decision\nThe analysis will be run by AWS Lambda, using SQS as the message broker, and Redis as the results storage. The application will be configured to use SQS as the message broker to reduce the cost of running a dedicated SQS instance. Furthermore, SQS provides the capability to store an intermediate result in a key, which can be used to perform a lookup in Redis, eliminating the need for the intermediate result to be persisted to disk.\\nA key for the intermediate result is provided by the SQS message payload, which is a JSON object with a key of 'result'. The SQS message payload can be used as the input to the analysis job. This allows for the analysis to be triggered by a user of the organization, without requiring any configuration of the analysis job itself. The SQS message payload can be configured to use SQS as the message broker, or SQS as a result storage backend.\\nThis decision was made in the context of a multi-organization, multi-location project. The analysis will be run on a single AWS account, and will be hosted on a single EC2 instance. The analysis will be triggered by a user of the organization, and will be triggered by a user of a different organization. The analysis will be triggered"}
{"Context":"## Context\\nWe must allow developers to use new JavaScript syntax and features without excluding older execution environments. Code must be automatically checked against common development mistakes and optimised for download and execution.\\n","Decision":"## Decision\nWe acknowledge that there is a plethora of Node.js tooling options available, however we have decided to use the [Google Closure Compiler]. It is developed and maintained by Google and is used for high traffic, complex and global applications such as Gmail and Google Maps. By adopting this tool we leverage decade of research and engineering in that field.\\n","Prediction":"## Decision\nWe will use [ESLint](https:\/\/eslint.org\/) to enforce common development mistakes and optimise code for download and execution.\\nWe will use [Prettier](https:\/\/prettier.io\/) to enforce common development mistakes and optimise code for download and execution.\\nWe will use [CommonJS module bundlers](https:\/\/commonjs.org\/) to compile module bundlers to a single file.\\nWe will use [CommonJS modules](https:\/\/commonjs.org\/) to compile module bundlers to a single file.\\nWe will use [ESLint configuration files](https:\/\/github.com\/eslint\/eslint\/wiki\/Configuration) to enforce common development mistakes and optimise code for download and execution.\\nWe will use [ESLint configuration files](https:\/\/github.com\/eslint\/eslint\/wiki\/Configuration) to enforce common development mistakes and optimise code for download and execution.\\nWe will use [ESLint configuration files](https:\/\/github.com\/eslint\/eslint\/wiki\/Configuration) to enforce common development mistakes and optimise code for download and execution.\\nWe will use the [ESLint configuration files](https:\/\/github.com\/eslint\/eslint\/wiki\/Configuration) to enforce common development mistakes and optimise code for download and execution.\\"}
{"Context":"### Context\\nIn order to send an offline message, the Responder needs to obtain cryptographic\\nkeys from the Initiator so they can engage in a non-interactive DAKE. These\\ncryptographic keys are distributed as prekey ensembles, which are made public in\\na untrusted Prekey Server.\\nA participant is identified by some identity in the underlying network (like\\nalice@xmpp.org for the XMPP network, or +99 1234-5678 for the SMS network).\\nWe assume this is everything each participant is required to know about who they\\nwant to send a message to.\\n### Decision\\nHaving in mind a client-server architecture, a prekey server instance MUST offer\\nthe following services to its users:\\n- Receive Client Profiles, Prekey Profiles and a set of prekey messages, and\\nstore them. Inform that this operation have failed or has been successful.\\n- Deliver prekey ensembles previously stored.\\n- Inform the publisher about how many prekey messages are stored for them.\\n- Inform the retriever when there are no prekey ensembles (or any of its values)\\nfrom an specific party.\\nTo publish Client profiles, Prekey Profiles and a set of prekey messages, a\\nparticipant deniably authenticates itself to the Prekey Server through a\\ninteractive DAKE, and sends those values to be stored and published by the\\nPrekey Server. The Prekey Server will associate the prekeys received to the\\nidentity used by the sender to identify itself to the Prekey Server.\\nTo obtain prekey ensembles, a participant asks the Prekey Server for prekey\\nensembles from a particular identity, and the server delivers one prekey ensemble\\nfor each instance tag and long-term public key it knows about from that\\nparticular identity.\\n### Notation\\nWe use the following notation to represent a the three values stored on a\\nPrekey Server for an identity:\\n```\\n(Identity || Client Profile)\\n```\\n```\\n(Identity || Prekey Profile)\\n```\\n```\\n(Identity || prekey message)\\n```\\n### How to publish Client Profiles, Prekey Profiles and Prekey Messages\\n1. The Initiator starts a interactive DAKE with the Prekey Server.\\n2. The Initiator sends Client Profiles, Prekey Profiles and a set of prekey\\nmessages to be published by the server.\\n3. The server stores all valid (non-expired) Client Profiles and Prekey\\nProfiles, and all prekey messages, and associates them with the publisher's\\nidentity.\\n### Prekey Ensemble Retrieval\\n#### SCENARIO 1\\nThe server must deliver only one Prekey Ensemble when multiple prekey messages\\nare available for the same identity and the same instance tag.\\nGiven the server has the following values stored:\\n(Identity 0x01 || Client Profile (instance tag 0x01))\\n(Identity 0x01 || Prekey Profile (instance tag 0x01))\\n(Identity 0x01 || Prekey message 1 (instance tag 0x01))\\n(Identity 0x01 || Prekey message 2 (instance tag 0x01))\\nWhen a participant asks the Prekey Server for Indentity 0x01,\\nThen the Prekey server should send any one of the following prekey\\nmessages, along with the Client Profile and Prekey Profile:\\n(Identity 0x01 || Client Profile (instance tag 0x01) || Prekey Profile\\n(instance tag 0x01) || Prekey message 1 (instance tag 0x01))\\n#### SCENARIO 2\\nThe server must deliver additional prekey ensembles when multiple instance tags\\nare found for the same identity.\\nGiven the server has the following prekey messages stored:\\n(Identity 0x01 || Client Profile (instance tag 0x01))\\n(Identity 0x01 || Client Profile (instance tag 0x02))\\n(Identity 0x01 || Prekey Profile (instance tag 0x01))\\n(Identity 0x01 || Prekey Profile (instance tag 0x02))\\n(Identity 0x01 || Prekey message 1 (instance tag 0x01))\\n(Identity 0x01 || Prekey message 2 (instance tag 0x02))\\nWhen a participant asks a Prekey Server for Identity 0x01,\\nThen the Prekey Server should send two prekey ensembles:\\n(Identity 0x01 || Client Profile (instance tag 0x01) || Prekey Profile\\n(instance tag 0x01) || Prekey message 1 (instance tag 0x01))\\n(Identity 0x01 || Client Profile (instance tag 0x02) || Prekey Profile\\n(instance tag 0x02) || Prekey message 1 (instance tag 0x02))\\n#### Receiving Prekey Ensembles\\nClients should not trust that the Prekey Server will always return valid Prekey\\nEnsembles, and must validate them by themselves. If a client can find any usable\\nPrekey Ensembles (or any of its values) from the Prekey Server's response, it\\nmay perform additional requests.\\n#### Receiving multiple Prekey Ensembles\\nClients should group all received Prekey Ensembles by instance tag, and, from\\nthese groups by long-term public key. They should choose from each group only\\nthe one with the latest expiry time. This must be done to avoid sending multiple\\noffline messages to the same instance tag.\\nIf there's still multiple prekey ensembles after filtering out duplicate\\ninstance tags, the client needs to decide which client the offline message\\nshould be sent to, or even send to all of them. Clients may need to inform\\nthe user before sending the offline encrypted messages to multiple instance\\ntags, or ask the user about which from the many possible actions should be\\ntaken.\\nIf the client decides to send offline encrypted messages to multiple instances\\ntags, it also needs to decide to either keep multiple conversations established\\nwith the publisher (one for each received Prekey Ensemble) or always terminate\\nthe conversation after the offline message is sent.\\nNotice that an attacker that impersonates the publisher's identity to the\\nPrekey Server (someone can, for example, steal the XMPP password) can publish a\\nnew Client Profile (with a new long-term public key), Prekey Profile and prekey\\nmessages, and guarantee they will receive copies of every encrypted offline\\nmessage sent by the retriever. Notice, although, that an attacker can do this\\nuntil the profiles expire.\\n### Consequences\\nThe Prekey Server can be subject to DoS attacks. As it is untrusted, it can,\\nfurthermore, send expired Prekey Ensembles, or send incomplete values or not\\nsend anything at all to a retriever. Clients are expected to keep this in mind\\nwhile working with the Prekey Server.\\n","Decision":"## Decision\nHaving in mind a client-server architecture, a prekey server instance MUST offer\\nthe following services to its users:\\n- Receive Client Profiles, Prekey Profiles and a set of prekey messages, and\\nstore them. Inform that this operation have failed or has been successful.\\n- Deliver prekey ensembles previously stored.\\n- Inform the publisher about how many prekey messages are stored for them.\\n- Inform the retriever when there are no prekey ensembles (or any of its values)\\nfrom an specific party.\\nTo publish Client profiles, Prekey Profiles and a set of prekey messages, a\\nparticipant deniably authenticates itself to the Prekey Server through a\\ninteractive DAKE, and sends those values to be stored and published by the\\nPrekey Server. The Prekey Server will associate the prekeys received to the\\nidentity used by the sender to identify itself to the Prekey Server.\\nTo obtain prekey ensembles, a participant asks the Prekey Server for prekey\\nensembles from a particular identity, and the server delivers one prekey ensemble\\nfor each instance tag and long-term public key it knows about from that\\nparticular identity.\\n### Notation\\nWe use the following notation to represent a the three values stored on a\\nPrekey Server for an identity:\\n```\\n(Identity || Client Profile)\\n```\\n```\\n(Identity || Prekey Profile)\\n```\\n```\\n(Identity || prekey message)\\n```\\n### How to publish Client Profiles, Prekey Profiles and Prekey Messages\\n1. The Initiator starts a interactive DAKE with the Prekey Server.\\n2. The Initiator sends Client Profiles, Prekey Profiles and a set of prekey\\nmessages to be published by the server.\\n3. The server stores all valid (non-expired) Client Profiles and Prekey\\nProfiles, and all prekey messages, and associates them with the publisher's\\nidentity.\\n### Prekey Ensemble Retrieval\\n#### SCENARIO 1\\nThe server must deliver only one Prekey Ensemble when multiple prekey messages\\nare available for the same identity and the same instance tag.\\nGiven the server has the following values stored:\\n(Identity 0x01 || Client Profile (instance tag 0x01))\\n(Identity 0x01 || Prekey Profile (instance tag 0x01))\\n(Identity 0x01 || Prekey message 1 (instance tag 0x01))\\n(Identity 0x01 || Prekey message 2 (instance tag 0x01))\\nWhen a participant asks the Prekey Server for Indentity 0x01,\\nThen the Prekey server should send any one of the following prekey\\nmessages, along with the Client Profile and Prekey Profile:\\n(Identity 0x01 || Client Profile (instance tag 0x01) || Prekey Profile\\n(instance tag 0x01) || Prekey message 1 (instance tag 0x01))\\n#### SCENARIO 2\\nThe server must deliver additional prekey ensembles when multiple instance tags\\nare found for the same identity.\\nGiven the server has the following prekey messages stored:\\n(Identity 0x01 || Client Profile (instance tag 0x01))\\n(Identity 0x01 || Client Profile (instance tag 0x02))\\n(Identity 0x01 || Prekey Profile (instance tag 0x01))\\n(Identity 0x01 || Prekey Profile (instance tag 0x02))\\n(Identity 0x01 || Prekey message 1 (instance tag 0x01))\\n(Identity 0x01 || Prekey message 2 (instance tag 0x02))\\nWhen a participant asks a Prekey Server for Identity 0x01,\\nThen the Prekey Server should send two prekey ensembles:\\n(Identity 0x01 || Client Profile (instance tag 0x01) || Prekey Profile\\n(instance tag 0x01) || Prekey message 1 (instance tag 0x01))\\n(Identity 0x01 || Client Profile (instance tag 0x02) || Prekey Profile\\n(instance tag 0x02) || Prekey message 1 (instance tag 0x02))\\n#### Receiving Prekey Ensembles\\nClients should not trust that the Prekey Server will always return valid Prekey\\nEnsembles, and must validate them by themselves. If a client can find any usable\\nPrekey Ensembles (or any of its values) from the Prekey Server's response, it\\nmay perform additional requests.\\n#### Receiving multiple Prekey Ensembles\\nClients should group all received Prekey Ensembles by instance tag, and, from\\nthese groups by long-term public key. They should choose from each group only\\nthe one with the latest expiry time. This must be done to avoid sending multiple\\noffline messages to the same instance tag.\\nIf there's still multiple prekey ensembles after filtering out duplicate\\ninstance tags, the client needs to decide which client the offline message\\nshould be sent to, or even send to all of them. Clients may need to inform\\nthe user before sending the offline encrypted messages to multiple instance\\ntags, or ask the user about which from the many possible actions should be\\ntaken.\\nIf the client decides to send offline encrypted messages to multiple instances\\ntags, it also needs to decide to either keep multiple conversations established\\nwith the publisher (one for each received Prekey Ensemble) or always terminate\\nthe conversation after the offline message is sent.\\nNotice that an attacker that impersonates the publisher's identity to the\\nPrekey Server (someone can, for example, steal the XMPP password) can publish a\\nnew Client Profile (with a new long-term public key), Prekey Profile and prekey\\nmessages, and guarantee they will receive copies of every encrypted offline\\nmessage sent by the retriever. Notice, although, that an attacker can do this\\nuntil the profiles expire.\\n### Consequences\\nThe Prekey Server can be subject to DoS attacks. As it is untrusted, it can,\\nfurthermore, send expired Prekey Ensembles, or send incomplete values or not\\nsend anything at all to a retriever. Clients are expected to keep this in mind\\nwhile working with the Prekey Server.\\n","Prediction":"## Decision\nHaving in mind a client-server architecture, a prekey server instance MUST offer\\nthe following services to its users:\\n- Receive Client Profiles, Prekey Profiles and a set of prekey messages, and\\nstore them. Inform that this operation have failed or has been successful.\\n- Deliver prekey ensembles previously stored.\\n- Inform the publisher about how many prekey messages are stored for them.\\n- Inform the retriever when there are no prekey ensembles (or any of its values)\\nfrom an specific party.\\nTo publish Client profiles, Prekey Profiles and a set of prekey messages, a\\nparticipant deniably authenticates itself to the Prekey Server through a\\ninteractive DAKE, and sends those values to be stored and published by the\\nPrekey Server. The Prekey Server will associate the prekeys received to the\\nidentity used by the sender to identify itself to the Prekey Server.\\nTo obtain prekey ensembles, a participant asks the Prekey Server for prekey\\nensembles from a particular identity, and the server delivers one prekey ensemble\\nfor each instance tag and long-term public key it knows about from that\\nparticular identity.\\n### Notation\\nWe use the"}
{"Context":"## Context\\nWe had to enhance editor capabilities and implement new editor adapters with the latest features. Not having integration tests to cover these changes became more and more risky.\\nAfter giving it another try, we were able to get Jest & Mocha installed together, without compilation errors because of type conflicts. Therefore, it was possible to create integration tests that would have access to VS Code API.\\n","Decision":"## Decision\nWe will now cover adapters with integration tests.\\nBecause _integration tests_ has different meanings for different people, we have decided to call them **contract tests** instead. Our intention is to test that all adapters of an interface do follow the same contract.\\n","Prediction":"## Decision\nWe decided to use Jest & Mocha for our editor tests.\\nWe will use [Jest](https:\/\/jestjs.com\/) as our test runner and [Mocha](https:\/\/mocha.github.io\/) as our test framework.\\nJest is a test framework that helps us write tests for our codebase.\\nIt provides a consistent testing framework and is easy to setup.\\nMocha is a test framework that helps us run our tests.\\nIt provides a consistent test framework and is easy to setup.\\n[Note: Mocha is an alternative to Jest](https:\/\/www.chris-eggertsen.com\/blog\/2015-09-16-mocha-vs-jest-which-one-is-better).\\nMocha is a test runner that helps us run our tests.\\nIt provides a consistent test runner and is easy to setup.\\n[Note: Mocha is an alternative to Jest](https:\/\/github.com\/jamesreid\/mocha).\\nJest & Mocha are used in [this editor test](https:\/\/github.com\/adamantios\/editor-test) to create tests for the editor.\\nThis test is used to create and run tests for the editor.\\nIt is used to create a test for ["}
{"Context":"## Context\\nDatasets are made up of observations and dimensions. Each of the options for a\\ndimension have wider definitions and metadata which apply to that option in all\\nscenarios, across multiple datasets. These code definitions live in code-lists.\\nThe list of codes in the code-list may not exactly match the list of codes in\\nthe dimension options, due to sparsity or how datasets are broken down.\\nIn order to build a list of datasets which use each code, a relationship needs\\nto be built between the code-list\/code graph and the instance\/dimension-option\\ngraph. There was a question as to whether this should link:\\n- **A) From: code-list, To: instance** -  As instances may contain sparse\\nsubsets of a code-list, this option would still require extra work to know\\nwhich codes specifically existed per instance.\\n- **B) From: code, To: instance** - This effectively duplicates the existing relationships to an instance, as it is the same as the dimension options.\\n- **C) From: code, To: dimension option** - The extra step to traverse back up\\nto the instance decreases the performance.\\n- **D) Don't build a relationship at all, just query for it on request** -\\nThis is very un-performant, and something that can easily be optimized by\\nbuilding the relationships.\\n","Decision":"## Decision\nA relationship should be created between a code node and each instance node which\\nfeatures the code as a dimension option. (Option B)\\n","Prediction":"## Decision\nWe will not build relationships between the code-list and the dimension\\noptions. Instead, we will query the data on request to build the relationships.\\nThis will reduce the amount of roundtrips to the server, and therefore\\nimproves performance.\\nWe will query the dataset for the code-list on request, and send the\\n`CodeList` class as a parameter to the dimension options. We will also\\nsend the `Dimension` class with the dimensions to query.\\nWe will use the `CodeList` class as the primary key for the dimension\\noptions, and the `Dimension` class as the foreign key to the dimension\\noptions. This will allow us to query for a `CodeList` on\\nrequest, but only for the dimensions it applies to.\\nWe will also send the `Dimension` class as a parameter to the dimension\\noptions, and the `CodeList` class as a foreign key to the dimension\\noptions. This will allow us to query for the dimension on\\nrequest, but only for the dimensions it applies to.\\nWe will also use the `CodeList` as the primary key for the dimension\\noptions, and the `Dimension` as the foreign key to the dimension\\noptions. This will"}
{"Context":"## Context\\nWe do not currently send any notifications to patients in Bangladesh. As part of our ongoing efforts to increase patient return rates, we would like to start sending patients text messages, much like we do in India. SMS messages are so widely abused by spammers in India and Bangladesh that we expect most patients to ignore SMS messages. In India, we've addressed that concern by integrating with WhatsApp. In Bangladesh, Imo is more popular than WhatsApp, so we would like to be able to send notifications to Bangladeshi patients via Imo.\\n","Decision":"## Decision\nImo has recently created a partner API at our request. We will use that API in Bangladesh to send appointment reminder notifications to patients. We will first attempt to send the notification via Imo, then fall back to SMS via Twilio. This mirrors the approach we use in India of first sending via WhatsApp and falling back to SMS.\\nWe will initially start sending notifications via an A\/B test to determine the most effective message types. Once we've established the most effective messaging strategy, we will use that messaging for all appointments moving forward.\\n","Prediction":"## Decision\nWe will send notifications to Bangladeshi patients via Imo.\\nWe will use the [Imo Notification Library](https:\/\/github.com\/Imo\/Imo-Notification-Library) to send SMS notifications to Bangladeshi patients.\\nWe will integrate the Imo Notification Library with our existing [Gmail](https:\/\/github.com\/abhishek-15\/abhishek-15) implementation.\\nWe will use the [Gmail SMS API](https:\/\/developers.google.com\/sms\/sending) to send SMS notifications to Bangladeshi patients.\\nWe will use the [Gmail SMS Sender](https:\/\/github.com\/abhishek-15\/abhishek-15-gmail-sender) to send SMS notifications to Bangladeshi patients.\\nWe will use the [Gmail Stripe Billing](https:\/\/stripe.com\/docs\/adding-payment-gateways\/adding-gmail-stripe) to handle the billing for SMS notifications.\\nWe will use the [Stripe Billing Defences](https:\/\/stripe.com\/docs\/adding-payment-gateways\/adding-gmail-stripe) to handle billing for Imo notifications.\\nWe will use the [Stripe Billing Policies](https:\/\/stripe.com\/docs\/adding-payment"}
{"Context":"## Context\\nThere was a generally dissatisfaction with our PR flow process from multiple stakeholders, including devs, QA and design. These largely centered around size, speed of integration and quality of PRs.\\nFor more details, please see pain points in the meeting notes held end of February here: https:\/\/notes.status.im\/C5pj8g7gQOu9Wo8PtDZsMw?edit# as well as the preceeding Discuss thread: https:\/\/discuss.status.im\/t\/better-pull-requests-process\/1044\\nAlso see conversations in Core Dev Call #12 and #13: https:\/\/github.com\/status-im\/pm\/\\n","Decision":"## Decision\nThese specific policy changes were agreed upon (a la 'rough consensus' model). These are heuristics, and should be more of a guide than strict rules. Exception to these should be minimal though.\\n**1. Reject PRs that don't have tests**\\nAs a rule of thumb, coverage should increase.\\n**2. Don't use or promote nightly as something for end users.**\\n**3. Reject PRs that don't write what tests the author have done and what platforms you used to test, including screenshots, videos or logs**\\nThis means QA and dev have to spend less time catching obvious problems.\\n**4. Run tests when a PR is at \"Review\" stage on https:\/\/github.com\/status-im\/status-react\/projects\/7 (or add new 'e2e stage' between review and `TO TEST`)**\\n**5. Dev asks for 2-3 reviewers, including a designer if the change affects UI, and these reviewers review within a day.**\\nThis ensures we don't have *diffusion of responsibility* and that PRs are reviewed quickly.\\n","Prediction":"## Decision\nWe will use the following process to create PRs:\\n1. You will create a PR for a feature request or bug fix.\\n2. The PR will be assigned to you and to the team.\\n3. The PR will be reviewed by the team.\\n4. The PR will be merged if it passes review.\\n5. The PR will be merged and deployed if it passes review.\\n6. The PR will be merged and deployed to the staging environment if it passes review.\\n7. The PR will be deployed to the production environment if it passes review.\\n8. The PR will be reverted if it passes review.\\n9. The PR will be reverted if it fails review.\\n10. The PR will be reverted if it fails review.\\n11. The PR will be reverted if it fails review.\\n12. The PR will be reverted if it fails review.\\n13. The PR will be reverted if it fails review.\\n14. The PR will be reverted if it fails review.\\n15. The PR will be reverted if it fails review.\\n16. The PR will be reverted if it fails review.\\n17. The PR will be reverted if it fails review.\\n18. The PR"}
{"Context":"## Context\\nWe want to ensure we're all using one code style, that is familiar across\\nprojects. [Prettier](https:\/\/prettier.io\/) is an opinionated code formatter with\\nsupport for most, if not all, of the languages in the JavaScript ecosystem. As\\nof writing, it is used by over\\n[1 million repositories](https:\/\/github.com\/prettier\/prettier\/network\/dependents?package_id=UGFja2FnZS00OTAwMTEyNTI%3D)\\non GitHub, including React itself, and has become a standard.\\n","Decision":"## Decision\nWe will enforce that everything supported by Prettier has its style enforced by\\nit.\\nWe will set up Git hooks to automatically run the formatter before committing.\\nWe will set continuous integration up to reject commits that are not correctly\\nformatted.\\n","Prediction":"## Decision\nWe will use [Prettier](https:\/\/prettier.io\/) for code formatting.\\nWe will use [eslint](https:\/\/eslint.org\/) to enforce code formatting.\\nWe will use [eslint-plugin-react](https:\/\/www.npmjs.com\/package\/eslint-plugin-react) to enforce code formatting.\\nWe will use [eslint-plugin-import](https:\/\/www.npmjs.com\/package\/eslint-plugin-import) to enforce code formatting.\\nWe will use [eslint-plugin-import-config](https:\/\/www.npmjs.com\/package\/eslint-plugin-import-config) to enforce code formatting.\\nWe will use [eslint-plugin-import-config-set](https:\/\/www.npmjs.com\/package\/eslint-plugin-import-config-set) to enforce code formatting.\\nWe will use [eslint-config-prettier](https:\/\/www.npmjs.com\/package\/eslint-config-prettier) to enforce code formatting.\\nWe will use [eslint-config-prettier-config](https:\/\/www.npmjs.com\/package\/eslint-config-prettier-config) to enforce code formatting.\\nWe will use [eslint-config-prettier-config-set](https:\/\/www.npmjs.com\/package\/eslint-config-prettier-config-set) to enforce code formatting.\\nWe will use"}
{"Context":"## Context and Problem Statement\\nWe are using BBC `VideoContext` to display a preview of the paper-cuts in a program script.\\nMain export is an EDL or XML to import into audio or video editing software to continue from rough cut to fine cut.\\nHowever it be useful to be able to export the \"rough cut preview\" as a video to remove extra steps if users need to share it as is.\\n## Decision Drivers <!-- optional -->\\n* Easy to reason around\\n* Light on the client, for low performance PC running IE (for web version)\\n","Decision":"## Decision\n* Easy to reason around\\n* Light on the client, for low performance PC running IE (for web version)\\n<!--\\nChosen option: \"[option 1]\", because [justification. e.g., only option, which meets k.o. criterion decision driver | which resolves force force | \u2026 | comes out best (see below)]. -->\\n_TBC, most likely 1 - `ffmpeg-remix`_\\n<!-- ### Positive Consequences\\n* [e.g., improvement of quality attribute satisfaction, follow-up decisions required, \u2026]\\n* \u2026\\n### Negative consequences\\n* [e.g., compromising quality attribute, follow-up decisions required, \u2026]\\n* \u2026  -->\\n","Prediction":"## Decision\n* Easy to reason around\\n* Light on the client, for low performance PC running IE (for web version)\\nChosen option: \"Export video to a temporary directory\", because we can use the `video.js` library to create a `video` and `img` element in the same file.\\nThis means that the `video` and `img` elements are the same size in the same directory.\\nThis also means that the `video.js` library can be used to enable the `video` and `img` elements to be played back together, as a single file.\\n### Positive Consequences <!-- optional -->\\n* No need to export to an intermediate directory\\n* No need to export the intermediate directory to a temp directory\\n* No need to use the `video.js` library\\n### Negative Consequences <!-- optional -->\\n* The intermediate directory is still created on export, and can be deleted by the user\\n* The intermediate directory is still created on export, and can be deleted by the user\\n* The intermediate directory is still created on export, and can be deleted by the user\\n* The intermediate directory is still created on export, and can be deleted by the user\\n* The intermediate directory is still created"}
{"Context":"## Context\\nCurrently the Wikibase docker images are built on travis and deployed to Dockerhub.\\nThere are a number of existing docker repositories we could consider to host our images. We need not pick only one repository but for the purposes of documentaion and clarity we probably want a single canonical one then other repositories could then be \"syndicated\" from the canonical one.\\nSome key properties of existing registries were considered in the table below:\\n| Repository | Free to WMDE | Self-Service (1) | Tooling provided for a built-in pipeline | Visibility of built images (2) | Possibility of replication to repository | Restrictions on use of non-Wikimedia images |\\n| --- | --- | --- | --- | --- | --- | --- |\\n| Dockerhub | Currently (3) | Yes | Static builds from dockerfile | High | Yes | no |\\n| Github Container registry | Currently | Yes | Github Actions | Medium | Yes | no |\\n| Google Container Registry | No (pay for storage and egress) | Yes | Google Cloud | Medium | Yes | no |\\n| AWS Container registry | No (pay for storage and egress) | Yes | Amazon Cloud | Medium | Yes | no |\\n| Azure Container Registry | No (some complex structure) | Yes | Azure Container Registry Tasks | Medium | Yes | no |\\n| WMF Docker Registry | Yes | No (negotiation needed) | [https:\/\/wikitech.wikimedia.org\/wiki\/PipelineLib](https:\/\/wikitech.wikimedia.org\/wiki\/PipelineLib) | Low | No (probably not) | Yes (only audited versions of specific images are allowed) (4) |\\n1. We can create new images and names without filing a ticket for speaking to people\\n2. Approximate fraction of the market of docker pulls that happens here\\n3. Dockerhub introduces limitations in use - but WMDE will likely be entitled to a free unlimited \"open source\" plan\\n4. Potentially WMDE could get their space of the registry with more loose restrictions (discussion ongoing)\\n","Decision":"## Decision\nWe will publish Docker release images to Dockerhub. Dockerhub is the goto location for publishing Docker images. We do not consider publishing Wikibase Docker images to WMF Docker Registry yet, as its purpose is to hold images to be used in Wikimedia production infrastructure. This might change once Wikibase (i.e. MediaWiki and extensions) will be deployed using container images - not the case as of December 2020.\\n","Prediction":"## Decision\nWe will use the [Dockerhub registry](https:\/\/hub.docker.com\/) as the canonical registry for our container images.\\nWe will then use the [Docker build pipeline](https:\/\/docs.docker.com\/engine\/reference\/builder\/) to build our container images\\nWe will then use the [Docker build pipeline](https:\/\/docs.docker.com\/engine\/reference\/builder\/) to create a public repository on Dockerhub\\nWe will then use the [Docker hub registry](https:\/\/hub.docker.com) as the canonical registry for our container images\\nWe will then use the [Docker build pipeline](https:\/\/docs.docker.com\/engine\/reference\/builder\/) to create a public repository on Dockerhub\\nWe will then use the [Docker build pipeline](https:\/\/docs.docker.com\/engine\/reference\/builder\/) to create a public repository on Dockerhub\\nWe will then use the [Docker build pipeline](https:\/\/docs.docker.com\/engine\/reference\/builder\/) to create a public repository on Dockerhub\\nWe will then use the [Docker build pipeline](https:\/\/docs.docker.com\/engine\/reference\/builder\/) to create a public repository on Dockerhub\\nWe will then use the [Docker build pipeline](https:\/\/docs.docker.com\/engine\/reference\/builder\/) to create a"}
{"Context":"## Context and Problem Statement\\nAny reasonably sized infrastructure deployment requires the creation,\\nmanagement and utilisation of secrets and configuration. Any approach\\ntaken to achieve these goals must be sensitive to maintaining not only\\nthe secure storage and usage of these secrets and parameters, but should\\nreduce friction in the design, deployment, and maintenance of the\\ninfrastructure.\\n## Decision Drivers\\n* Secure storage of the secrets and parameters.\\n* Ease of integration with infrastructure tooling (e.g. Terraform, AWS).\\n* Ease of maintenance of secrets and parameters.\\n* Low ongoing costs.\\n","Decision":"## Decision\n* Secure storage of the secrets and parameters.\\n* Ease of integration with infrastructure tooling (e.g. Terraform, AWS).\\n* Ease of maintenance of secrets and parameters.\\n* Low ongoing costs.\\nChosen option: a combination of both AWS Secrets Store and AWS Systems\\nManager for storing both secrets and configuration parameters\\nrespectively.\\nFor secrets and parameters created outside of the scope of the\\nTerraform, AWS Secrets Store and AWS Systems Manager resources will be\\ncreated in Terraform with placeholder values provided for the initial\\nresource creation. Actual secrets and parameters will then be manually\\ncurated by developers and support staff. This approach allows such\\nsecrets and configuration to be referenced in the Terraform by other\\nresources, without exposing them.\\nFor secrets and parameters generated by Terraform, these will be fed\\ndirectly into other Terraform resources that require them.\\n","Prediction":"## Decision\n* Secure storage of the secrets and parameters.\\n* Ease of integration with infrastructure tooling (e.g. Terraform, AWS).\\n* Ease of maintenance of secrets and parameters.\\n* Low ongoing costs.\\nThis decision is based on the following:\\n* The decision is based on the following:\\n- [AWS Secrets Manager](https:\/\/aws.amazon.com\/secrets\/)\\n- [AWS Secrets Manager documentation](https:\/\/docs.aws.amazon.com\/secrets\/latest\/userguide\/secret-manager.html)\\n- [AWS Secrets Manager API reference](https:\/\/docs.aws.amazon.com\/secrets\/latest\/reference\/services\/secrets-manager.html)\\n- [AWS Secrets Manager integration with Terraform](https:\/\/github.com\/aws\/aws-s3\/tree\/main\/terraform-aws-secrets-manager\/secret-manager)\\n- [AWS Secrets Manager integration with Helm](https:\/\/kubernetes.io\/docs\/concepts\/basics\/configuration\/secret-management\\#helm)\\n- [AWS Secrets Manager integration with Helm](https:\/\/kubernetes.io\/docs\/concepts\/basics\/configuration\/secret-management\\#helm)\\n- [AWS Secrets Manager integration with Helm](https:\/\/kubernetes.io\/docs\/concepts\/basics\/configuration\/secret-management\\#helm)\\n- [AWS Secrets Manager integration with Helm]("}
{"Context":"## Context\\nLanes, Widths, Surfaces are dynamic attributes which might change both in number and attributes over the life time of a road segment.\\nEach lane, width, surface is identified using an attribute identifier.\\nHanding out a new attribute identifier upon each change could cause us to exhaust the attribute identifier space rapidly.\\n","Decision":"## Decision\nWe keep track of all the attribute identifiers we have handed out for a road segment.\\nWe do this for Lanes, Widths and Surfaces separately.\\nUpon each change in the number of dynamic attributes we try to exhaust the previously used identifiers first, that is we reuse them.\\nIf additional attribute identifiers are needed we allocate them, continuing with the next free attribute identifier across road segments.\\nAs soon as an identifier is allocated, it becomes dedicated to the road segment.\\nIt remains so even after a road segment was removed.\\n","Prediction":"## Decision\nWe will use a field called \"lane_id\", which will be a unique identifier for a lane.\\nWe will use a field called \"lane_width\", which will be the width of a lane.\\nWe will use a field called \"surface_id\", which will be the ID of a surface.\\nWe will use a field called \"surface_type\", which will be a 1 or 2 digit number identifying the type of surface.\\nWe will use a field called \"surface_array\", which will be an array of 1 or 2 digit numbers identifying the array of surface IDs.\\nWe will use a field called \"surface_array_type\", which will be a 1 or 2 digit number identifying the array of surface IDs.\\nWe will use a field called \"surface_array_array\", which will be a 1 or 2 digit number identifying the array of surface IDs.\\nEach lane, width, surface, and surface array will be unique within a given segment.\\nEach segment will have a number of lanes, widths, and surfaces.\\nEach segment will have a number of lanes, widths, and surfaces.\\nEach segment will have a number of lanes, widths, and surfaces.\\nEach segment will have a number of lanes,"}
{"Context":"### Context\\nAfter much usage of LRUD V2, we (Lovely Horse \ud83d\udc34) have ended up with a laundry list of things we want LRUD to do that V2 doesn't support. We also have a desire for a more maintainable codebase that utilises more up-to-date terminology for the expected behaviour of LRUD.\\nThe list of desired functionality currently sits at;\\n- a real tree structure\\n- cleaner\/easier to understand \"grid\" functionality\\n- supporting a concept of \"column span\"\/\"column width\"\\n- real definition of which node is the \u201croot\u201d node\\n- all focusable nodes to maintain an \"index\" for easier understanding or sorting\\n- better handling of unregistering\\n### Decision\\nWe have decided to re-write LRUD from the ground up, maintaining many of the concepts from V2, while addressing the list of desired functionality.\\nThis will also give us an opportunity to re-write LRUD into Typescript, further increasing the maintainability of the codebase in future.\\n### Status\\nApproved\\n### Consequences\\n- user land usages of LRUD will need to update their code in order to make use of the new version. We are planning to keep breaking changes to a minimum, but some changes will be necessary.\\n- slightly increased library size, affecting response payload sizes. The increase in size is small enough (an increase of 2.6kb when minified) that we deem this acceptable. Furthermore, the changes mean that current \"workaround\" code in service land can be removed, reducing payload size in other areas.\\n- slightly increased runtime computation. Usage of a real tree in memory requires extra computation. Dedicated testing will take place to ensure LRUD is still performant enough on low powered devices, but initial testing of 92 test cases in 2.4s suggests this is well within limits.\\n### Further Reading\\n- [Paper Doc discussing what LRUD is and why we want to change some things](https:\/\/paper.dropbox.com\/doc\/SSR-Controller-Module-LRUD-V3--Aca6ZBsM4Uv8zEN44j5o4TsvAg-y0v9YqarEOXNP7R2151RK)\\n","Decision":"## Decision\nWe have decided to re-write LRUD from the ground up, maintaining many of the concepts from V2, while addressing the list of desired functionality.\\nThis will also give us an opportunity to re-write LRUD into Typescript, further increasing the maintainability of the codebase in future.\\n### Status\\nApproved\\n### Consequences\\n- user land usages of LRUD will need to update their code in order to make use of the new version. We are planning to keep breaking changes to a minimum, but some changes will be necessary.\\n- slightly increased library size, affecting response payload sizes. The increase in size is small enough (an increase of 2.6kb when minified) that we deem this acceptable. Furthermore, the changes mean that current \"workaround\" code in service land can be removed, reducing payload size in other areas.\\n- slightly increased runtime computation. Usage of a real tree in memory requires extra computation. Dedicated testing will take place to ensure LRUD is still performant enough on low powered devices, but initial testing of 92 test cases in 2.4s suggests this is well within limits.\\n### Further Reading\\n- [Paper Doc discussing what LRUD is and why we want to change some things](https:\/\/paper.dropbox.com\/doc\/SSR-Controller-Module-LRUD-V3--Aca6ZBsM4Uv8zEN44j5o4TsvAg-y0v9YqarEOXNP7R2151RK)\\n","Prediction":"## Decision\nWe have decided to re-write LRUD from the ground up, maintaining many of the concepts from V2, while addressing the list of desired functionality.\\nThis will also give us an opportunity to re-write LRUD into Typescript, further increasing the maintainability of the codebase in future.\\n### Status\\nApproved\\n### Consequences\\n- user land usages of LRUD will need to update their code in order to make use of the new version. We are planning to keep breaking changes to a minimum, but some changes will be necessary.\\n- slightly increased library size, affecting response payload sizes. The increase in size is small enough (an increase of 2.6kb when minified) that we deem this acceptable. Furthermore, the changes mean that current \"workaround\" code in service land can be removed, reducing payload size in other areas.\\n- slightly increased runtime computation. Usage of a real tree in memory requires extra computation. Dedicated testing will take place to ensure LRUD is still performant enough on low powered devices, but initial testing of 92 test cases in 2.4s suggests this is well within limits.\\n### Further Reading\\n- [Paper Doc discussing what LRUD is and why we want to change"}
{"Context":"## Context and Problem Statement\\nDecentraland Wearables are a key aspect of the experience. The ability to customize your experience with different wearables and the ability for content creators to come and contribute to the look and feel of Decentraland avatars is key for decentralization.\\nWe currently have a need to add an anti-spam mechanism to the flow; given that the previous designs were not intended to run on L2 and the Ethereum Gas Fees was a high enough barrier to entry.\\n- Problems with the 3D mesh, bones, textures or coloring\\n- Copyright issues\\n- Incompatibility with other wearables\\n### Details on the Committee mechanics\\n- Committee members can be added or removed by the L1 DAO.\\n- After a collection is submitted to be reviewed, anyone of the committee members can reject it, or approve it. Before approval, the collection will not be available to be bought on the marketplace.\\n- After rejection, the creator might continue to modify the collection and ask for approval again.\\n- There are no extra fees to re-submit.\\n- Automatically Submit collection to be approved upon deployment of the smart contract.\\n- There is no special timing or automated approval.\\n- Requirements on Committee members (automated requirements on committee members to approve\/reject)\\n- What happens if there is not enough volume? We'll wait and see if this is really a problem.\\n#### Questions from the meeting (and their answer):\\n- Do we need a proof of work of the committee in order to keep the \"committee member\" status?\\n- No, we'll play it by ear (if someone doesn't do the work we can vote them out)\\n- Do the committee members have an incentive to work?\\n- We could think about grants, but out of scope for this discussion\\n- Having a single member of the committee allowed to approve or deny collections, they could be corrupt.\\n- We can think about a multisig-like approaches. **Out of scope**\\n- If they are corrupt, it would be a governance problem, not software.\\n- Approved\/rejected is only a boolean in the collection: we can revisit decisions later, approvals or rejections may not be final.\\n- There are mechanisms in place already to freeze models in case we want to prevent future changes to collections.\\n- Members of the committee freeze the collections by submitting a valid content hash\\n- If we reject a collection we should be able to send feedback.\\n- This should be made on the forum. We should implement automated pings and alerts so the committee has to post.\\nRoles, Contracts, Capabilities (without considering L1\/L2 bridges):\\n<!--\\n```dot\\ndigraph G {\\nrankdir=TB\\ngraph [fontname = \"arial\", fontsize=\"10\", color=\"grey\", fontcolor=\"grey\"]\\nnode [fontname = \"arial\",fontsize=\"10\", shape=\"box\", style=\"rounded\"]\\nedge [fontname = \"arial\",color=\"blue\", fontcolor=\"black\",fontsize=\"10\"]\\ndao [label=\"DAO\"];\\ngroup [label=\"Approver's Committee\"];\\ncom_member [label=\"Committee Members\"];\\nmanager [label=\"Collections Owner\/Manager\"];\\nfactory [label=\"Collection Factory\"];\\nnew_collection [label=\"Collection Contract\"];\\ncreator [label=\"Content Creator\"];\\ngroup -&gt; com_member [label=\" has N\" color=\"green\"]\\ndao -&gt; group [label=\" Set Members (Add, Remove)\"]\\nmanager -&gt; dao [label=\"  owned by\" color=\"green\"]\\ncom_member -&gt; manager [label=\" can approve\/reject\\n collections\"]\\ncreator -&gt; factory [label=\" can deploy new contracts\"]\\nfactory -&gt; new_collection [label= \"  is deployer of\"]\\nnew_collection -&gt; manager [label=\" owned by\", color=\"green\"]\\nmanager -&gt; new_collection [label=\" can reject       \"]\\n}\\n```\\n-->\\n![resources\/ADR-17\/fig-2020-11-05-adr-wearable-collection-approval-1.svg](resources\/ADR-17\/fig-2020-11-05-adr-wearable-collection-approval-1.svg)\\n_This graph is illustrative and can vary_\\n","Decision":"## Decision\n- Approval by committee members: **approved** \u2705\\n- Paying MANA to get reviewed: **approved** \u2705\\n- Every rarity has its own price (DAO can vote price): **approved** \u2705\\n- Staking MANA to issue items: **rejected\/deferred** \ud83d\uded1\\n- We should have a mechanism to stop the current collection deployment to switch to new ones \u2705\\n- Economic Incentives: **rejected\/deferred** \ud83d\uded1\\n","Prediction":"## Decision\n- We will implement a proof of work for the committee\\n- We will use a multisig approach to grant grant permissions to the committee members\\n- We will create a collection contract that will hold the collection metadata and the collection metadata will be stored in the smart contract\\n- We will create a collection contract that will hold the collection contract metadata and the collection contract metadata will be stored in the smart contract\\n- We will create a collection contract that will hold the collection contract metadata and the collection contract metadata will be stored in the smart contract\\n- We will create a collection contract that will hold the collection contract metadata and the collection contract metadata will be stored in the smart contract\\n- We will create a collection contract that will hold the collection contract metadata and the collection contract metadata will be stored in the smart contract\\n- We will create a collection contract that will hold the collection contract metadata and the collection contract metadata will be stored in the smart contract\\n- We will create a collection contract that will hold the collection contract metadata and the collection contract metadata will be stored in the smart contract\\n- We will create a collection contract that will hold the collection contract metadata and the collection contract metadata will be stored in the smart contract\\n- We will create a collection contract that will hold"}
{"Context":"## Context and Problem Statement\\nApplications deployed within Operate First are run on top of OpenShift. Many applications require users to log in in order to be able to enforce RBAC, quotas and sandboxing. It is also essential for auditing. User management of the applications should be unified so the same user can access all the applications with a single set of credentials. The username should be unique, but also utilized across applications. The accepted solution should provide SSO, so the user can cary over the identity across different applications. The identity and authentication should be based on the OpenShift authentication.\\n## Decision Drivers\\n- For the same user a single user identity is provided to all applications.\\n- All users are able to authenticate using the same credentials as they use for the underlying OpenShift platform.\\n- User identity provided by the SSO allows for similar RBAC settings across different applications.\\n","Decision":"## Decision\n- For the same user a single user identity is provided to all applications.\\n- All users are able to authenticate using the same credentials as they use for the underlying OpenShift platform.\\n- User identity provided by the SSO allows for similar RBAC settings across different applications.\\nChosen option: _\"2. Standalone Dex server in a separate namespace\"_, because\\n### Positive Consequences\\n- Each Operate First application can consume user identity for all SSO authenticated users.\\n- Applications don't have to care about user management.\\n- SSO allows us to share the RBAC rules between applications in similar contexts since the user identity structure (LDAP groups, etc.) is the same.\\n### Negative Consequences\\n- Introduces a single point of failure for all user authentications.\\n","Prediction":"## Decision\n- For the same user a single user identity is provided to all applications.\\n- All users are able to authenticate using the same credentials as they use for the underlying OpenShift platform.\\n- User identity provided by the SSO allows for similar RBAC settings across different applications.\\nChosen option: \"OpenShift Identity Provider\", because the OpenShift SSO is already in place and allows for a simple and secure integration of the SAML token.\\n### Positive Consequences\\n- SAML token will be used for the SSO integration.\\n- OpenShift SSO is already in place and allows for a simple and secure integration of the SAML token.\\n### Negative Consequences\\n- OpenShift Identity Provider is not yet in place and therefore the integration of the SAML token has to be done manually.\\n- OpenShift Identity Provider is not yet in place and therefore the integration of the SAML token has to be done manually.\\n- OpenShift Identity Provider is not yet in place and therefore the integration of the SAML token has to be done manually.\\n- OpenShift Identity Provider is not yet in place and therefore the integration of the SAML token has to be done manually.\\n- OpenShift Identity Provider is not yet in"}
{"Context":"## Problem\\nCurrently, you can easily come into wrong assumptions\\nthat something would work in the specification.\\nWe need to find minimal requirements to implement a sane spec plugin.\\n","Decision":"## Decision\n- no defaults for `sw\/_\/key` specifications\\n(default will not work for `ksLookup(\/sw\/sthg\/key)`)\\n- plugins are not allowed to create keys (may change in future; depends on plugin positions)\\nThe spec plugin should yield errors when it detects such situations.\\n","Prediction":"## Decision\nThe spec plugin will use the [spec-mechanisms](https:\/\/github.com\/alexanderkrieger\/spec-mechanisms) library to provide a specification.\\nThis library provides a [data model](https:\/\/github.com\/alexanderkrieger\/spec-mechanisms\/blob\/master\/data-model.md) for the specification.\\nThis data model has a `Spec` struct, which is a type for a specification.\\nIt also has a `SpecSpec` struct, which is a type for a specification.\\nThese structs are the same type, but are different in implementation.\\nIt is possible to use the same `Spec` struct for multiple specs.\\nThis is useful for testing and mocking.\\nFor example, you can use the `Spec` struct as a mock for the `SpecSpec` struct.\\nThis means that the spec plugin will not have to make any assumptions about\\nthe implementation.\\n### Implementation\\nThe spec plugin will use the spec-mechanisms library to provide a specification.\\nThis library provides a data model for the specification.\\nIt also has a mock implementation for the spec-mechanisms library.\\nThis means that the spec plugin will not have to make any assumptions about\\nthe"}
{"Context":"## Context\\nTEET is a large web application and will contain many namespaces and different types of namespaces:\\nviews, controllers, common UI components, etc.\\nThe way to structure namespaces affects how easy it is to find and refer to a given piece of code.\\nCommon structure makes it more predictable where developers can expect to find things.\\n","Decision":"## Decision\nWe use feature based grouping with layer suffix.\\n`teet.<feature>.<feature>-<layer>`\\nExample: `teet.search.search-view` and `teet.search.search-controller`\\nWhen referring to namespaces, use the last part of the name fully, e.g. `search-view`\\n```clojure\\n(ns teet.search.search-view\\n(:require [teet.search.search-controller :as search-controller]))\\n```\\nFeatures may use implementation specific sub-namespaces as seen fit.\\nNon-feature functionality, like common UI utilities, are placed under the layer, e.g. `teet.ui.panels`.\\n","Prediction":"## Decision\nUse the [namespace pattern](https:\/\/en.wikipedia.org\/wiki\/Name_pattern) for namespaces.\\nThis pattern describes how namespaces should be structured for code reuse. It is also\\nimportant to note that this is not an all-or-nothing decision. It is possible to have a\\nnamespace with a different name or structure to the rest of the namespace.\\nThis can be useful for code that is shared between namespaces, or for code that is\\ntypically used in multiple namespaces.\\nFor example, if a class is used in multiple namespaces, but has a similar name\\nto a class that is used in a different namespace, the class can be placed\\nin the same namespace.\\nThis pattern is not meant to be a hard and fast rule. It is important to note\\nthat there are other patterns that are more suited to some situations than others.\\nFor example, the [abstract pattern](https:\/\/en.wikipedia.org\/wiki\/Abstract_pattern)\\nis a good pattern for classes that are used in multiple namespaces, but is\\nnot a good pattern for classes that are used in a single namespace.\\nThere are other patterns that are more appropriate for code that is used in\\nmultiple namespaces, but are still useful for classes that are used in"}
{"Context":"## Context\\nThe self-service application needs to provide a facility where we can view and\\naudit the events that occur within it. For example, if we become aware of\\na security incident where a user's account has been compromised we need to be\\nable triage possible damage by investigating how that account has used the\\nservice.\\n","Decision":"## Decision\nThe app will be built using the principle of Event Sourcing:\\n>Event Sourcing ensures that all changes to application state are stored as a sequence of events. Not just can we query these events, we can also use the event log to reconstruct past states, and as a foundation to automatically adjust the state to cope with retroactive changes.\\nWhen a change is made an event will be written into an events\\ntable that fully represents that change. With the events table it will be\\npossible to view the changes that have occurred in the system.\\nThe primary aim for this implementation will be to be able to view historical\\nchanges to the system, but it should also be possible to use the events table to\\nrecreate the previous states of the system.\\nThis behaviour is implemented 'natively' in ActiveRecord and is heavily inspired\\nby Kickstarter's [Event Sourcing made Simple](https:\/\/kickstarter.engineering\/event-sourcing-made-simple-4a2625113224)\\nand [its demo](https:\/\/github.com\/kickstarter\/event-sourcing-rails-todo-app-demo\/)\\nAll distinct events will be defined as Ruby classes that inherit from an [`Event`](app\/models\/event.rb) class.\\nThey will be stored in the `events` table and Rail's Single Table Inheritance will used to identify the class of each event.\\nAggregate objects will inherit from an abstract class [`Aggregate`](app\/models\/aggregate.rb).\\nWhen an event is saved a callback will be used to update an aggregate object\\n(equivalent to a Calculator object or function).\\nAggregates should not be changed unless told to by an event.\\nEvents will associate themselves to an aggregate using a polymorphic association.\\nWhen the state of an object needs to be changed the Aggregate shall be used.\\n","Prediction":"## Decision\nOur application will use [Fiddler](https:\/\/www.fiddler2.com\/) to record\\nthe requests and responses between the client and server.\\nThis facility will allow us to:\\n* View the requests made during a user session\\n* View the responses received by the user\\n* View the errors returned by the server\\n* View the SSL certificates used by the service\\n* View the TLS versions used by the service\\n* View the server's version\\nWe will use Fiddler to record the requests and responses for the purposes of\\nour application's development and testing.\\nWe will use [Mocha](https:\/\/mochajs.org) to run the tests against the service\\nin our application's environment.\\nWe will use [Selenium](https:\/\/seleniumhq.org) to automate the tests and the\\nlogging of the tests.\\nWe will use [Postman](https:\/\/www.postman.com) to test the service using a REST API.\\nWe will use [Rspec](https:\/\/github.com\/benjaminhenderson\/rspec) to test the service\\nin our application's environment.\\nWe will use [Capybara](https:\/\/github.com\/rspec\/capybara) to test the"}
{"Context":"## Context\\nWe need to make sure our code is written in a standard style for clarity, consistency across a project and to avoid back and forth between developers about code style.\\n","Decision":"## Decision\nWe will use [Standard.rb](https:\/\/github.com\/testdouble\/standard) and run the standard.rb rake task to lint the code as part of the test suite.\\n","Prediction":"## Decision\nWe will use the [styleguide](https:\/\/github.com\/alexanderl\/awesome-styleguide) on the [awesome-awesome-styleguide](https:\/\/github.com\/alexanderl\/awesome-awesome-styleguide) repo as a starting point for our style guide.\\nWe will also use [the [standard library](https:\/\/github.com\/alexanderl\/awesome-standard-library) for some of our more common styles and conventions](https:\/\/github.com\/alexanderl\/awesome-standard-library).\\n### Conventions\\n* `\/\/` and `\/\/ \/\/` are used to mark comments as whitespace.\\n* `\/\/` and `\/\/ \/\/` are used to mark code as whitespace.\\n* `\/\/` is used to mark code as whitespace.\\n* `\/\/` is used to mark code as whitespace.\\n* `\/\/` is used to mark code as whitespace.\\n* `\/\/` is used to mark code as whitespace.\\n* `\/\/` is used to mark code as whitespace.\\n* `\/\/` is used to mark code as whitespace.\\n* `\/\/` is used to mark code as whitespace.\\n* `\/\/` is used to mark code as whitespace.\\n* `\/\/` is used to mark"}
{"Context":"## Context\\nDataLabs makes extensive use of reverse proxying, to give users access to resources (such as Minio or JupyterLabs).  These resources need individually from an external URL to an internal service URL.  There are four design options for reverse proxying (<http:\/\/sawers.com\/blog\/reverse-proxying-with-nginx\/>):\\n1. Subdomain - this allows the external path and internal path to be the same, probably with a default root base path (\/).  Different services are identified by the external URL's hostname.  This has some disadvantages - multiple hostnames require a wildcard certificate, or multiple certificates if a wildcard certificate can not be acquired; and it makes the development environment more difficult, because you can not just use localhost.\\n2. Port - this also allows the external path and internal path to be the same, probably with a default root base path (\/).  Different services are identified by the external URL's port.  This has the disadvantage that some organisational firewalls restrict http traffic to unusual hosts.\\n3. Symmetric Path - this allows the external path and internal path to be the same, but with that path configured.  Different services are identified by the path.  This is the best option, but the internal service must allow the path to be configurable.\\n4. Asymmetric Path - here the external and internal paths are different.  Different services are identifiable by the external path.  This requires a search-and-replace of the path on the rendered HTML and JavaScript, so unless these are simple, then this is too fragile.\\nHistorically DataLabs has used Subdomain proxying.\\n","Decision":"## Decision\nWhere possible, Symmetric Path or Asymmetric Path proxying should be used.  If this is not possible, a ConfigMap option should determine whether the remaining proxying strategy should be Subdomain or Port proxying.\\n","Prediction":"## Decision\nWe will use Symmetric Path proxying, with a wildcard certificate.\\nThis design is likely to be more resilient to external service configuration changes, because the internal service will be aware of the external service's hostname, and it will be possible to configure the internal service to accept a wildcard certificate.  This means that if the external service is moved to a different host, then the wildcard certificate will still work.\\nWe will configure the wildcard certificate with a \"port\" to the external service (e.g. \"https:\/\/datalabs.io\/\").  This allows the external service to be accessible from a different port, and a different host.  The port can be configured in the external service's configuration, or in the wildcard certificate.\\nIf the external service is moved to a different host, then the wildcard certificate will still work.\\nWe will configure the internal service to use the external service's hostname, and port.  This means that the internal service can be moved to a different host, and the wildcard certificate will still work.\\nWe will configure the internal service to use the external service's hostname, and path.  This means that the internal service can be moved to a different path, and the wildcard certificate will still work.\\nWe will"}
{"Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"## Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","Prediction":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\nWe will use [this template](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions#header-architectural-decisions) to generate a new ADR every time there is a significant change to the architecture.\\nWe will use [this template](https:\/\/gist.github.com\/anonymous\/5f2b3e6e2e6d8b7a3e0a45e1d9d3a6a) to generate a new ADR every time there is a major change to the architecture.\\nWe will use [this template](https:\/\/gist.github.com\/anonymous\/5f2b3e6e2e6d8b7a3e0a45e1d9d3a6a) to generate a new ADR every time there is a minor change to the architecture.\\nWe will use [this template](https:\/\/gist.github.com\/anonymous\/5f2b3e6e2e6d8b7"}
{"Context":"## Context\\nTest suites and other tools may produce output files while running inside a container.\\nDue to environmental differences, container users and groups often do not match the host's users and groups, blocking one side from deleting the files of the other; often with corner cases such as files being deleted but not subdirectories.\\nAutomated infrastructure usually runs as the `elife` or `jenkins` user, not as `root`. The docker daemon runs as root and is capable of bridging differences.\\n","Decision":"## Decision\nUse `docker cp` to exchange files between containers, especially from inside a container to the outside.\\n","Prediction":"## Decision\nUse a non-root user to run the test suite, and use a non-root user to run the container.\\nThis decision was made in part because it is not clear whether the container will be running as root or not.\\nFor example, if the container is running as root, then it is possible that it will have access to the files of the host, which could lead to a security vulnerability.\\nThis decision was also made in part because it is not clear whether users will be able to access test suites and other tools from containers. If the user is not the container user, then the container will not be able to access the files of the host.\\nAs a result, the container will need to run as the non-root user, which will be the container user. If the container is running on the same machine as the host, then the container user will be the host user.\\nIf the container is running on a different machine, the container user will be the container user. When a container is launched, the container user will be the root user, and the container will be run as the container user.\\nThe container user will be able to access the files of the host, but will not be able to delete them. A user will"}
{"Context":"## Context\\nOur certificate rotation was a largely manual process, involving an operator triggering a series of Concourse pipeline jobs in a particular sequence. We did not have a routine for doing rotations, and would typically only do them as part of a CF upgrade.\\nThe only means we had for knowing if a cert rotation was necessary was the `check-certificates` job, in the `create-cloudfoundry` Concourse pipeline, which would fail if any certificate had less than 30 days until it expired.\\nIn Q2 2019 (August\/September) we moved all of our platform secrets from AWS S3 to [Credhub](https:\/\/docs.cloudfoundry.org\/credhub\/). This covered third-party service credentials, platform passwords, and certificates. Since Credhub supports [certificate rotation](https:\/\/github.com\/pivotal-cf\/credhub-release\/blob\/master\/docs\/ca-rotation.md), we chose to implement automatic certificate rotation. This ADR contains details of how we did it.\\n","Decision":"## Decision\nCredhub has the notion of a transitional certificate. As written in [their documentation](https:\/\/github.com\/pivotal-cf\/credhub-release\/blob\/master\/docs\/ca-rotation.md), a transitional certificate is\\n> a new version that will not be used for signing yet, but can be added to your servers trusted certificate lists.\\nOur certificate rotation process is built around the setting and migration of the `transitional` flag, such that over a number of deployments an active certificate is retired and a new certificate is deployed, without downtime.\\nIn order to make certificate rotation automatic, and require no operator interaction, it is implemented as a job at the tail end of the `create-cloudfoundry` pipeline; after acceptance tests and before releases tagging.\\nThe new `rotate-certs` job has three tasks:\\n- `remove-transitional-flag-for-ca`\\n- `move-transitional-flag-for-ca`\\n- `set-transitional-flag-for-ca`\\nThese three tasks are in reverse order of the process for rotating a certificate. If the tasks were ordered normally, the first task would set up the state for the second, and the second would set up the state for the third, and Bosh would be unable to deploy the certificates without downtime. However, here the tasks are explained in the proper order to make it easier to understand how a certificate is rotated. To understand how it happens in the pipeline, assume a Bosh deploy happens between each step.\\n`set-transitional-flag-for-ca` is the first step in the process. It iterates through all CA certificates in Credhub, looking for any expiring under 30 days. Any that are, are regenerated as transitional certificates. This results in Credhub holding two certificates for the same credential name: the expiring certificate, and the new certificate with the `transitional` flag.\\n`move-transitional-flag-for-ca` is the second step in the process, and has two jobs:\\n1. It finds all CA certificates in Credhub which have 2 values, where the oldest certificate does not have the `transitional` flag and the newer one does. For each of those, it swaps the `transitional` flag to the older certificate. Finally, it looks for any leaf certificates signed by the CA certificate and regenerates them using the new CA certificate.\\n2. It looks for any leaf certificates that are expiring in less than 30 days and regenerates them. This is a one step process and they are deployed on the next Bosh deploy.\\n`remove-transitional-flag-for-ca` is the third and final step in the process. It iterates through all of the CA certificates in Credhub, looking for any with 2 values, where the older certificate is marked as `transitional` and the newer certificate is not. It then removes the `transitional` flag from the older certificate, which has the effect of dropping the certificate.\\nThe existing `check-certificates` job has also been modified to check for certificates that are expiring in less than 15 days. If a certificate fails this check, that should suggest to us that something has gone wrong in our certificate rotation process.\\n","Prediction":"## Decision\nWe will use Credhub to manage certificate rotation. Credhub will be used in conjunction with the `certbot` tool in the `pivotal-website` Concourse pipeline.\\nWe will use the `certbot` tool to generate certificates for deployment to the platform. We will use the `certbot` tool to renew existing certificates. We will use the `certbot` tool to generate new certificates in the future. We will use the `certbot` tool to manage certificate chains.\\nWe will use the `certbot` tool to generate new certificates for deployment to the platform. We will use the `certbot` tool to renew existing certificates. We will use the `certbot` tool to manage certificate chains.\\nWe will use the `certbot` tool to generate new certificates for deployment to the platform. We will use the `certbot` tool to renew existing certificates. We will use the `certbot` tool to manage certificate chains.\\nWe will use the `certbot` tool to generate new certificates for deployment to the platform. We will use the `certbot` tool to renew existing certificates. We will use the `certbot` tool to manage certificate chains.\\nWe will use the `"}
{"Context":"## Context\\nThe frontend project currently parses capi block elements into a list of BodyBlock objects as part of the\\nfrontend model. This is currently used (in frontend) by the liveblog, and we also forward this structure to\\ndotcom rendering.\\n","Decision":"## Decision\nWe have created a duplicate set of models in frontend to be used only by the dotcom rendering datamodel.\\nDoing this will allow us to iterate on the dotcom rendering models more quickly and make enhancements such\\nas better image support, without the potential to impact liveblogs.\\n","Prediction":"## Decision\nWe will use the [Liveblog](https:\/\/github.com\/markpierce\/livblog) and [Backbone](https:\/\/github.com\/backbone\/backbone) to\\nbuild a single Frontend Model.\\nThis model will be used to represent the frontend block\\nstructure. This model will be used in both frontend and\\nbackbone rendering.\\n### Backend Model\\nThe backend model will be used by both frontend and backbone rendering.\\nThis model will be used to represent the frontend block\\nstructure. This model will be used in both frontend and\\ndotcom rendering.\\n### Rendering\\nRendering will use the frontend model to render the block\\nstructure. This model will be used in both frontend and\\ndotcom rendering.\\n### Validation\\nAll validation will be done in the backend model. This model will\\nbe used in both frontend and backbone rendering.\\n### Backward Compatibility\\nThe backend model will be used by both frontend and backend\\nrendering. This model will be used in both frontend and\\ndotcom rendering.\\n### Alternatives\\nOption 1: Use [Backbone](https:\/\/github.com\/backbone\/backbone) to build the frontend model\\nOption 2: Use [Backbone.js](https:\/\/"}
{"Context":"## Context\\nBuilding a complex web application brings challenges around how to manage state. The\\nRedux website provides an excellent [motivation page](http:\/\/redux.js.org\/docs\/introduction\/Motivation.html) that discusses the challenges in detail.\\n","Decision":"## Decision\nWe have decided to adopt the [Redux](http:\/\/redux.js.org\/) architecture to provide a clean separation between\\nour views, actions and state store.\\n","Prediction":"## Decision\nUse Redux to manage state.\\nThe State Library of Queensland has chosen to use Redux for managing state.\\nA good starting point is the [Redux docs](http:\/\/redux.js.org\/).\\nThere is a lot more detail in the docs than we need here, but the key points are:\\n* Use a single store to store all your states and actions\\n* Use a single store to store the state of your components\\n* Use a single store to pass data between components\\n* Use a single store to store global state\\n* Use a single store to store store configuration\\n* Use a single store to store session state\\n* Use a single store to store authentication\\n* Use a single store to store session storage\\n* Use a single store to store session storage store\\n* Use a single store to store session storage store\\n* Use a single store to store global state store\\n* Use a single store to store global state store\\n* Use a single store to store global state store\\n* Use a single store to store global state store\\n* Use a single store to store global state store\\n* Use a single store to store global state store\\n* Use a single store to store global state store\\n* Use a single store to store"}
{"Context":"## Context\\nBuilding forms in Rails that are compliant with the GOVUK Design System involve\\nmanually declaring the correct HTML structure, class names and ARIA attributes,\\nwhich is time-consuming and hard to get right.\\nAdditionally, our validation errors currently use Rails' default pattern, rather\\nthan the one recommended for use as part of the design system, which is designed\\nwith accessibility in mind.\\n","Decision":"## Decision\nWe will use DfE's `govuk_design_system_formbuilder` to simplify the creation of\\nGOV.UK Design System-compliant forms.\\nAs we are currently using Simple Form rather than Rails' default form builder\\nfor our other forms, the two form builders can co-exist for the time being,\\nwhilst we transition the forms over.\\n","Prediction":"## Decision\nWe will use the GOVUK Design System's [validations](https:\/\/github.com\/govukdesign\/validations) to\\nvalidate our forms.\\nWe will use the [validations-rails gem](https:\/\/github.com\/validations-rails\/) to automatically\\nvalidate all forms.\\nWe will use the [validations-rails-rails](https:\/\/github.com\/validations-rails\/rails) gem to\\nautomatically validate all forms.\\nWe will use the [validations-rails-forms](https:\/\/github.com\/validations-rails\/forms) gem to\\nvalidate all forms.\\nWe will use the [validations-rails-forms-forms](https:\/\/github.com\/validations-rails\/forms-forms) gem to\\nvalidate forms with a focus on accessibility.\\nWe will use the [validations-rails-forms-forms-forms-rails](https:\/\/github.com\/validations-rails\/forms-forms-forms-forms-rails) gem to\\nvalidate forms with a focus on accessibility.\\nWe will use the [validations-rails-forms-forms-forms-forms-forms-rails-forms](https:\/\/github.com\/validations-rails\/forms"}
{"Context":"## Context\\nI want to use docker at AWS but I don't want to provision a docker\\ninfrastructure myself.  So I'm going to use AWS ECS managed service and create\\na cluster.\\n","Decision":"## Decision\nDone.\\n","Prediction":"## Decision\nI will use ECS managed service to manage the deployment of my app.\\nI will use ECS ECS Manager to manage the cluster\\nI will use ECS ECS Manager to provision the ECS cluster\\nI will use ECS ECS Manager to manage the ECS service\\nI will use ECS ECS Manager to scale up and scale down the ECS\\nservice.\\nI will use ECS ECS Manager to monitor the ECS service\\nI will use ECS ECS Manager to restart the ECS service\\nI will use ECS ECS Manager to stop the ECS service\\nI will use ECS ECS Manager to restart the ECS service\\nI will use ECS ECS Manager to failover the ECS service\\nI will use ECS ECS Manager to failover the ECS service\\nI will use ECS ECS Manager to failover the ECS service\\nI will use ECS ECS Manager to failover the ECS service\\nI will use ECS ECS Manager to failover the ECS service\\nI will use ECS ECS Manager to failover the ECS service\\nI will use ECS ECS Manager to failover the ECS service\\nI will use ECS ECS Manager to failover the ECS service\\nI will use ECS ECS Manager to failover the ECS service\\nI will use ECS ECS Manager to failover the ECS service\\nI will use ECS ECS Manager to"}
{"Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","Prediction":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\nWe will use [Maven Architecture Tool Suite](https:\/\/mvnrepository.com\/artifact\/org.apache.maven\/assembly\\%20artifactId\\%20maven-assembly-plugin) to generate the ADRs.\\nWe will use [JIRA](https:\/\/issues.apache.org\/jira\/secure\/Project\\%20Architect\/ADR\\n) to manage the ADRs.\\nWe will use [Ansible](https:\/\/github.com\/ansible\/ansible) to manage the ADRs.\\nWe will use [Pivotal Tracker](https:\/\/pivotaltracker.com) to manage the ADRs.\\nWe will use [Jenkins](https:\/\/jenkins-ci.org) to build and deploy the ADRs.\\nWe will use [Jenkins Pipeline](https:\/\/jenkins-ci.org) to build and deploy the ADRs.\\nWe will use [Jenkins Continuous Delivery Pipeline](https:\/\/jenkins-ci.org) to build and deploy the ADRs.\\nWe will use [Jenkins Continuous Integration Pipeline"}
{"Context":"## Context\\nDummy services such as api-dummy are stateless, do not require many components such as databases and caches, and do not produce logs.\\nThe ease with of pushing, pulling and running a single image inside larger orchestration is its selling point.\\nReal projects have dependencies that should be centralized to specialized containers such as `nginx` or `postgres`. These dependencies are not maintained in-house. These dependencies also have to be bootstrapped in a certain order.\\nReal projects also have various volumes that need to be mounted for them to function, such as configuration files.\\n`docker-compose` has some support for IDEs, and is portable between laptops and headless environments such as `ci` and `prod`.\\n","Decision":"## Decision\nBy default, use a single docker image to ship dummy projects. Provide additional options for setup if needed to satisfy other requirements.\\nUse one or more `docker-compose.yml` files to build and deploy real projects.\\n","Prediction":"## Decision\nUse `docker-compose` to define and run Docker services.\\n### Use a single image for all services\\nThis will allow for easier configuration and deployment. It also allows for easier maintenance and debugging.\\n### Use `docker-compose up` to run services\\nThis will allow for easier configuration and deployment. It also allows for easier monitoring.\\n### Use `docker-compose logs` to monitor services\\nThis will allow for easier debugging.\\n### Use `docker-compose up -d` to restart services\\nThis will allow for easier configuration and deployment. It also allows for easier monitoring.\\n### Use `docker-compose up -f` to force stop and start services\\nThis will allow for easier configuration and deployment. It also allows for easier monitoring.\\n### Use `docker-compose up -d` to force start services\\nThis will allow for easier configuration and deployment. It also allows for easier monitoring.\\n### Use `docker-compose up -f` to restart services\\nThis will allow for easier configuration and deployment. It also allows for easier monitoring.\\n### Use `docker-compose up -f` to force stop and start services\\nThis will allow for easier configuration and deployment. It also allows for easier monitoring.\\n### Use `docker-compose up -d"}
{"Context":"## Context\\nAs more applications have come to interact with the Raster Foundry API, the need to control user permissions on a different axis has become more apparent.\\nCurrently, the Raster Foundry API handles permissions that refer to specific object instances. Each access control rule assigned to a user points to a specific object by id and indicates the allowed action.\\nThis system does not support access control across object types, such as a rule that encodes \"this user cannot create projects\" or \"this user cannot delete annotations\".\\nThis ADR seeks to develop a solution to this problem.\\n","Decision":"## Decision\nThe lack of flexibility is enough to discard _The Basic Solution_ and the need for conflict resolution is probably enough to discard _The Wild West_ solution. _The Civilized Solution_ is less clear than _The More Granular Variation_ and they both seems to require the same amount of effort to implement.\\nWe should go forward using _The More Granular Variation_.\\n","Prediction":"## Decision\n* The Raster Foundry API will support access control across object types.\\n* Access control will be implemented as a per-object permission system.\\n* Each object will have a per-user permission set.\\n* User permissions will map to object permissions.\\n* User permissions will map to permissions for objects.\\n* User permissions will be stored in the database.\\n* User permissions will be stored in the API.\\n* User permissions will be stored in the API's configuration file.\\n* User permissions will be stored in the API's console.\\n* User permissions will be stored in the API's console's configuration file.\\n* User permissions will be stored in the API's configuration file's own configuration file.\\n* User permissions will be stored in the API's configuration file's own configuration file's own configuration file.\\n* User permissions will be stored in the API's configuration file's own configuration file's own configuration file.\\n* User permissions will be stored in the API's configuration file's own configuration file's own configuration file.\\n* User permissions will be stored in the API's configuration file's own configuration file's own configuration file.\\n* User permissions will be stored in the API's configuration file's own configuration file's own configuration"}
{"Context":"## Context\\nInconsistent code formatting slows down development and the review process.\\nCode should be linted for things like:\\n- unused imports and variables\\n- consistent import order\\nCode formatting should be done automatically or programmatically, taking the burden off of reviewers.\\n","Decision":"## Decision\nFor linting, use [flake8](https:\/\/flake8.pycqa.org\/en\/latest\/) and [flake8-import-order](https:\/\/github.com\/PyCQA\/flake8-import-order).\\nFor formatting, use [black](https:\/\/github.com\/psf\/black).\\n","Prediction":"## Decision\nWe will use [eslint](https:\/\/eslint.org\/)\\nto format our code.\\nWe will run eslint on every file in our project.\\nWe will run eslint on every pull request.\\nWe will run eslint on every pull request that fails linting.\\nWe will run eslint on every merge that fails linting.\\nWe will run eslint on every merge that fails with uncaught errors.\\nWe will run eslint on every failing test.\\nWe will run eslint on every failing test that fails with uncaught errors.\\nWe will run eslint on every failing test that fails with uncaught errors.\\nWe will run eslint on every failing test that fails with uncaught errors.\\nWe will run eslint on every failing test that fails with uncaught errors.\\nWe will run eslint on every failing test that fails with uncaught errors.\\nWe will run eslint on every failing test that fails with uncaught errors.\\nWe will run eslint on every failing test that fails with uncaught errors.\\nWe will run eslint on every failing test that fails with uncaught errors.\\nWe will run eslint on every failing test that fails with uncaught errors.\\nWe will run eslint on every failing test that fails with uncaught errors.\\n"}
{"Context":"## Context\\nOkta is an Identity as a Service provider (similar to Auth0) that can provide authentication and authorization support to an application.  They have a forever free developer account that allows up to 7000 active users\/mo (well below our anticipated usage).\\nOkta provides the ability to authenticate against a variety of providers (okta, social (facebook, github, microsoft azure, etc), SAML, AD\/LDAP).  We would use the Microsoft social OpenConnect ID provider.  It will map the Microsoft user into an Okta user, essentially acting as a proxy or facade to Microsoft while providing a more uniform and standards adhering API.\\nThey have integrations into Spring Boot 1.x (via okta spring security starter) and Spring Security 5.x (via Spring Boot 2.x) supports Okta out of the box.\\nThey have a React client SDK instead of using our handrolled client code for Microsoft Azure AD.\\nThe real value add comes when we need to add roles\/groups (admin level users).  You get admin screens\/dashboards \"for free\" to add\/remove users etc.\\nLater on, we could add additional social logins (or support AD directly if needed).\\nSpike can be found at: https:\/\/github.com\/buildit\/bookit-api\/tree\/okta-spring-boot-1.x\\n","Decision":"## Decision\n* Delay for now - because we think we still need a local user table, we can add this in later\\n* Use Okta as IdP for Bookit\\n* Use Okta's React client for client side code\\n* Use Okta's Spring Boot 1.x starter for\\n* Use Okta Access Tokens as Bearer tokens\\n","Prediction":"## Decision\nUse Okta for all user authentication and authorization.\\nWe will leverage the Okta Spring Boot starter.\\nWe will use the Okta Spring Security starter to add support for SAML.\\nWe will use the Okta Spring Boot starter to add support for SAML SSO.\\nWe will use the Okta Spring Boot starter to add support for SAML Single Sign-On.\\nWe will use the Okta Spring Boot starter to add support for SAML single sign-on via Azure AD.\\nWe will use the Okta Spring Boot starter to add support for SAML Single Sign-On via SAML SSO.\\nWe will use the Okta Spring Boot starter to add support for SAML SSO via Okta SAML Single Sign-On.\\nWe will use the Okta Spring Boot starter to add support for SAML single sign-on via Okta SAML SSO.\\nWe will use the Okta Spring Boot starter to add support for SAML SSO via Okta SAML SSO.\\nWe will use the Okta Spring Boot starter to add support for SAML SSO via Okta SAML Single Sign-On.\\nWe will use the Okta Spring Boot starter to add support for SAML SSO via Okta"}
{"Context":"## Context\\nIn some of our projects we configure ragtime migrations by explicitly defining\\neach individual migration like so:\\n```edn\\n{:duct.migrator\/ragtime\\n{:database   #ig\/ref :duct.database\/sql\\n:logger     #ig\/ref :duct\/logger\\n:strategy   :raise-error\\n:migrations [#ig\/ref :foo.migration\/create-foo-table]}\\n[:duct.migrator.ragtime\/sql :foo.migration\/create-foo-table]\\n{:up   [#duct\/resource \"foo\/migrations\/create-foo-table.up.sql\"]\\n:down [#duct\/resource \"foo\/migrations\/create-foo-table.down.sql\"]}}\\n```\\nIs other projects, however, we use the sugar to include all migrations found in\\na path:\\n```edn\\n{:duct.migrator\/ragtime\\n{:database   #ig\/ref :duct.database\/sql\\n:logger     #ig\/ref :duct\/logger\\n:strategy   :raise-error\\n:migrations [#ig\/ref :foo.migrations\/dev\\n#ig\/ref :foo.migrations\/prod]}\\n[:duct.migrator.ragtime\/resources :foo.migrations\/dev]\\n{:path \"dev\/migrations\"}\\n[:duct.migrator.ragtime\/resources :foo.migrations\/prod]\\n{:path \"prod\/migrations\"}}\\n```\\nThis is useful, yes. But it has some serious downsides:\\n1. It is harder to catch a conflict when 2 developers push new\\nmigrations. Especially if the conflict is in the ordering of the migrations\\nonly. While if we would be using explicit `:migrations []` listing then there\\nwill be obvious code conflicts.\\n2. There is a problem of partial changes being applied and the other few changes\\nfailing when a SQL sentence fails within a single migration file. On the\\nother hand, doing explicit listing treats the whole migration file like a\\nsingle transaction. So either all the SQL sentences in the migration file are\\napplied, or all are rolled back.\\nThere is an orthogonal issue that is related to this. The development migrations\\nalways go at the beginning of the migrations list (or at the end, depending on\\nthe order you specify in the `:migrations` key). But once you have at least one\\nproduction migration applied and you need to add a new for development migration\\n(or vice-versa, depending on the order specified in `:migrations`), it fails. It\\nexpects to add the new development migration after the last development one (and\\nbefore any of the production ones) and it cannot. The reason why is that the\\nfirst production migration is already at that position.\\nThus we need to make sure development migrations and production migrations are\\nnot intertwined. One way to achieve this is by using separate ragtime migration\\ntables for each class of migrations. ragtime library let us specify the name of\\nthe table where a given list of migrations will be recorded. By using different\\ntable names for development and production we can keep the migrations separate\\nand guarantee the order of application in all cases.\\nIn this case, as we will have more than one `:duct.migrator\/ragtime`\\nconfiguration, we will need to use composite Integrant keys for the development\\nand production configurations.\\nWe need to change `:duct.migrator\/ragtime` in `config.edn` to:\\n```edn\\n{[:duct.migrator\/ragtime :foo\/prod]\\n{:database   #ig\/ref :duct.database\/sql\\n:logger     #ig\/ref :duct\/logger\\n:strategy   :raise-error\\n:migrations-table \"ragtime_migrations\"\\n:migrations [#ig\/ref :foo.migration\/create-foo-table]}\\n[:duct.migrator.ragtime\/sql :foo.migration\/create-foo-table]\\n{:up   [#duct\/resource \"foo\/migrations\/create-foo-table.up.sql\"]\\n:down [#duct\/resource \"foo\/migrations\/create-foo-table.down.sql\"]}}\\n```\\nand change `:duct.migrator\/ragtime` in `dev.edn` to:\\n```edn\\n{[:duct.migrator\/ragtime :foo\/dev]\\n{:database #ig\/ref :duct.database\/sql\\n:logger #ig\/ref :duct\/logger\\n:migrations-table \"ragtime_migrations_dev\"\\n:fake-dependency-to-force-initialization-order #ig\/ref [:duct.migrator\/ragtime :foo\/prod]}\\n:migrations [#ig\/ref :foo.dev-migration\/create-dev-table]}\\n[:duct.migrator.ragtime\/sql :foo.dev-migration\/create-dev-table]\\n{:up   [#duct\/resource \"foo\/dev_migrations\/create-dev-table.up.sql\"]\\n:down [#duct\/resource \"foo\/dev_migrations\/create-dev-table.down.sql\"]}\\n```\\nWe also need to add a fake dependency in `[:duct.migrator\/ragtime :foo\/dev]`\\n(that `:duct.migration\/ragtime` library completely ignores) to force the order\\nof application of the migrations. With the configuration shown above, production\\nmigrations will always be applied before development ones.\\n","Decision":"## Decision\nWe will change the template to configure ragtime to use explicit listings of\\nmigrations. The template will also configure two `:duct.migrator\/ragtime`\\nIntegrant keys, one for development and one for production. And set the order of\\napplication of the migrations to production before development.\\n","Prediction":"## Decision\nWe will use separate ragtime migration tables for each class of migrations\\n(e.g. `:migrations-table :foo\/migrations-table`).\\nThis will allow us to keep the migrations separate and ensure that they are\\napplied in the order they are specified in the `:migrations` key.\\nThis will also allow us to catch a conflict between development migrations and\\nproduction migrations (and rollback them).\\nThis is a breaking change. It will be backwards compatible.\\n### Example\\nWe will use the example migration table `:migrations-table` to demonstrate\\nthe new configuration.\\n```edn\\n{:migrations-table {:foo\/migrations-table {:foo\/migrations-table {:foo\/migrations-table {:foo\/migrations-table {:foo\/migrations-table {:foo\/migrations-table {:foo\/migrations-table {:foo\/migrations-table {:foo\/migrations-table {:foo\/migrations-table :nil}}}}}}}}}\\n{:migrations-table {:foo\/migrations-table {:foo\/migrations-table {:foo\/migrations-table {:foo\/migrations-table {:foo\/migrations-table {:foo\/migrations-table :nil}}}}}\\n[:migrations-table {:foo\/migrations-table {:foo\/migrations-table {:foo\/migrations-table {:foo\/migrations-table {:"}
{"Context":"## Context\\nGiven that prototypes built using the NHSUK prototype kit will use the same styling and branding as live NHSUK services, if a member of the public came across the prototype they may confuse it with a live service. It would be beneficial to secure these prototypes behind some level of authentication.\\n","Decision":"## Decision\nWe will use the [basic-auth](https:\/\/www.npmjs.com\/package\/basic-auth) package to add a layer of authentication when the prototype application is run on a production environment. This will allow the creator of the prototype to set a username and password to access the prototype when published.\\n","Prediction":"## Decision\nTo prevent this we will use the NHSUK authentication framework for authentication.\\nThis will allow the NHSUK authentication framework to be used to read and modify the prototype. This will allow the NHSUK application to be used to read and modify the prototype. This will also allow a member of the public to login to the NHSUK application and read and modify the prototype.\\nAdditionally, the NHSUK authentication framework will be used to set the user's avatar and to set the avatar's password if the user is not logged in. This will allow the NHSUK application to be used to read and modify the prototype.\\nAdditionally, the NHSUK authentication framework will be used to change the avatar and password if the user is logged in.\\nThis change will be reviewed by the NHSUK design team before being committed to the live NHSUK application.\\nThis change will be reviewed by the NHSUK design team before being committed to the live NHSUK application.\\nThis change will be reviewed by the NHSUK design team before being committed to the live NHSUK application.\\nThis change will be reviewed by the NHSUK design team before being committed to the live NHSUK application.\\nThis change will be reviewed by the NHSUK design team before being committed to the live NHS"}
{"Context":"## Context\\nIt's not intended for this app to have a significant server-side component, instead it will directly send calls from the front end to a service layer. This service layer will be discussed in its own project, but presently comprises solely of [members data API](https:\/\/github.com\/guardian\/members-data-api), the same back end that currently provides account management functionality.\\n","Decision":"## Decision\nThe back end for this app will have two responsibilities. Firstly, it will act as a proxy for calls to the service layer. Secondly, it will provide server side rendering capabilities to improve user experience.\\n","Prediction":"## Decision\nThe back end will be removed and replaced with a single domain service layer. This service layer will provide a single endpoint to the front end, as well as the ability to consume the data API directly from the front end.\\nThis approach will allow the front end to have a single point of integration into the server-side app, and this will be discussed in a later section.\\nThis approach will also be discussed in the next section, where the data API will be removed from the back end.\\n### Data API\\nThe back end will provide a single endpoint to the front end. This endpoint will return the data in JSON format.\\nThis approach will allow the front end to have a single point of integration into the server-side app, and this will be discussed in a later section.\\nThis approach will also be discussed in the next section, where the data API will be removed from the back end.\\n### Data API to be removed\\nThe data API will be removed as it has been superseded by the single domain service layer. The data API will be removed by migrating the service layer to a new package.\\n### Data API to be deprecated\\nThis data API will be deprecated as it is no longer used by the server-side app. The data"}
{"Context":"## Context\\n[Lab][hapi-lab] is misreporting the code coverage stats because I have not set it up to [work with Typescript][lab-ts]. The other issue with Lab is that its community is quite small meaning less plug-and-play with other tools. I thought about using [Ava][ava-typescript] but [this review][dodds-jest] of Ava's performance issues doesn't sound great.\\n","Decision":"## Decision\nJest seems like the way to go:\\n- Typescript support\\n- Large community\\n- Familiar\\n","Prediction":"## Decision\nI'm going to use [TypeScript][lab-ts] for the types and [Ava][ava-typescript] for the code coverage.\\nI'm going to use [TypeGraphQL][lab-graphql] to query the coverage data.\\nI'm going to use [TypeDoc][lab-doc] to generate documentation for the coverage tests.\\nI'm going to use [Docker][lab-docker] to deploy and run the coverage tests.\\nI'm going to use [Ansible][lab-ansible] to deploy and run the coverage tests.\\nI'm going to use [Docker Compose][lab-containerd] to deploy and run the coverage tests.\\nI'm going to use [Elixir][lab-elixir] to run the coverage tests.\\nI'm going to use the [Elixir Test Etc][lab-elixir-test] test suite.\\nI'm going to use [Elixir Test Etc][lab-elixir-test]:latest to run the coverage tests.\\nI'm going to use [Elixir Test Etc][lab-elixir-test]:stable to run the coverage tests.\\nI'm going to use [Elixir Test Etc][lab-elixir-test]:"}
{"Context":"## Context\\nThe New Build Team need to be able to perform basic search over properties for finding properties to either report defects against or to manage existing defects.\\n","Decision":"## Decision\nTo implement search we will use the built in Postgres Search rather than adding a new dependency on another service like ElasticSearch.\\n","Prediction":"## Decision\nThe New Build Team will use the [Hudson](https:\/\/hudson-ci.org\/) platform to perform basic search over properties for finding properties to either report defects against or to manage existing defects.\\nHudson will be used as it has the following benefits:\\n* It is a well known open source project\\n* It has a wide range of plugins to support its use in the New Build Team\\n* It has a good level of documentation\\n* It has a well defined process for adding plugins\\n* It has a good level of integration with other tools such as Jenkins\\n* It is hosted on GitHub which is well known and well established\\n* It is well supported by the community\\n* It has a well known release process\\n* It has a well documented versioning scheme\\n* It can be configured to run on a schedule as part of a CI\/CD pipeline\\n* It has a well known, well documented, and well supported release cycle\\n* It has a well documented release process\\n* It has a well supported versioning scheme\\n* It has a well documented, well supported, and well supported release cycle\\n* It has a well documented, well supported, and well supported versioning scheme\\n* It has a well documented, well"}
{"Context":"## Context\\nCurrently in the Cosmos SDK, events are defined in the handlers for each message, meaning each module doesn't have a canonical set of types for each event. Above all else this makes these events difficult to consume as it requires a great deal of raw string matching and parsing. This proposal focuses on updating the events to use **typed events** defined in each module such that emitting and subscribing to events will be much easier. This workflow comes from the experience of the Akash Network team.\\n[Our platform](http:\/\/github.com\/ovrclk\/akash) requires a number of programmatic on chain interactions both on the provider (datacenter - to bid on new orders and listen for leases created) and user (application developer - to send the app manifest to the provider) side. In addition the Akash team is now maintaining the IBC [`relayer`](https:\/\/github.com\/ovrclk\/relayer), another very event driven process. In working on these core pieces of infrastructure, and integrating lessons learned from Kubernetes development, our team has developed a standard method for defining and consuming typed events in Cosmos SDK modules. We have found that it is extremely useful in building this type of event driven application.\\nAs the Cosmos SDK gets used more extensively for apps like `peggy`, other peg zones, IBC, DeFi, etc... there will be an exploding demand for event driven applications to support new features desired by users. We propose upstreaming our findings into the Cosmos SDK to enable all Cosmos SDK applications to quickly and easily build event driven apps to aid their core application. Wallets, exchanges, explorers, and defi protocols all stand to benefit from this work.\\nIf this proposal is accepted, users will be able to build event driven Cosmos SDK apps in go by just writing `EventHandler`s for their specific event types and passing them to `EventEmitters` that are defined in the Cosmos SDK.\\nThe end of this proposal contains a detailed example of how to consume events after this refactor.\\nThis proposal is specifically about how to consume these events as a client of the blockchain, not for intermodule communication.\\n","Decision":"## Decision\n**Step-1**:  Implement additional functionality in the `types` package: `EmitTypedEvent` and `ParseTypedEvent` functions\\n```go\\n\/\/ types\/events.go\\n\/\/ EmitTypedEvent takes typed event and emits converting it into sdk.Event\\nfunc (em *EventManager) EmitTypedEvent(event proto.Message) error {\\nevtType := proto.MessageName(event)\\nevtJSON, err := codec.ProtoMarshalJSON(event)\\nif err != nil {\\nreturn err\\n}\\nvar attrMap map[string]json.RawMessage\\nerr = json.Unmarshal(evtJSON, &attrMap)\\nif err != nil {\\nreturn err\\n}\\nvar attrs []abci.EventAttribute\\nfor k, v := range attrMap {\\nattrs = append(attrs, abci.EventAttribute{\\nKey:   []byte(k),\\nValue: v,\\n})\\n}\\nem.EmitEvent(Event{\\nType:       evtType,\\nAttributes: attrs,\\n})\\nreturn nil\\n}\\n\/\/ ParseTypedEvent converts abci.Event back to typed event\\nfunc ParseTypedEvent(event abci.Event) (proto.Message, error) {\\nconcreteGoType := proto.MessageType(event.Type)\\nif concreteGoType == nil {\\nreturn nil, fmt.Errorf(\"failed to retrieve the message of type %q\", event.Type)\\n}\\nvar value reflect.Value\\nif concreteGoType.Kind() == reflect.Ptr {\\nvalue = reflect.New(concreteGoType.Elem())\\n} else {\\nvalue = reflect.Zero(concreteGoType)\\n}\\nprotoMsg, ok := value.Interface().(proto.Message)\\nif !ok {\\nreturn nil, fmt.Errorf(\"%q does not implement proto.Message\", event.Type)\\n}\\nattrMap := make(map[string]json.RawMessage)\\nfor _, attr := range event.Attributes {\\nattrMap[string(attr.Key)] = attr.Value\\n}\\nattrBytes, err := json.Marshal(attrMap)\\nif err != nil {\\nreturn nil, err\\n}\\nerr = jsonpb.Unmarshal(strings.NewReader(string(attrBytes)), protoMsg)\\nif err != nil {\\nreturn nil, err\\n}\\nreturn protoMsg, nil\\n}\\n```\\nHere, the `EmitTypedEvent` is a method on `EventManager` which takes typed event as input and apply json serialization on it. Then it maps the JSON key\/value pairs to `event.Attributes` and emits it in form of `sdk.Event`. `Event.Type` will be the type URL of the proto message.\\nWhen we subscribe to emitted events on the CometBFT websocket, they are emitted in the form of an `abci.Event`. `ParseTypedEvent` parses the event back to it's original proto message.\\n**Step-2**: Add proto definitions for typed events for msgs in each module:\\nFor example, let's take `MsgSubmitProposal` of `gov` module and implement this event's type.\\n```protobuf\\n\/\/ proto\/cosmos\/gov\/v1beta1\/gov.proto\\n\/\/ Add typed event definition\\npackage cosmos.gov.v1beta1;\\nmessage EventSubmitProposal {\\nstring from_address   = 1;\\nuint64 proposal_id    = 2;\\nTextProposal proposal = 3;\\n}\\n```\\n**Step-3**: Refactor event emission to use the typed event created and emit using `sdk.EmitTypedEvent`:\\n```go\\n\/\/ x\/gov\/handler.go\\nfunc handleMsgSubmitProposal(ctx sdk.Context, keeper keeper.Keeper, msg types.MsgSubmitProposalI) (*sdk.Result, error) {\\n...\\ntypes.Context.EventManager().EmitTypedEvent(\\n&EventSubmitProposal{\\nFromAddress: fromAddress,\\nProposalId: id,\\nProposal: proposal,\\n},\\n)\\n...\\n}\\n```\\n### How to subscribe to these typed events in `Client`\\n> NOTE: Full code example below\\nUsers will be able to subscribe using `client.Context.Client.Subscribe` and consume events which are emitted using `EventHandler`s.\\nAkash Network has built a simple [`pubsub`](https:\/\/github.com\/ovrclk\/akash\/blob\/90d258caeb933b611d575355b8df281208a214f8\/pubsub\/bus.go#L20). This can be used to subscribe to `abci.Events` and [publish](https:\/\/github.com\/ovrclk\/akash\/blob\/90d258caeb933b611d575355b8df281208a214f8\/events\/publish.go#L21) them as typed events.\\nPlease see the below code sample for more detail on this flow looks for clients.\\n","Prediction":"## Decision\nWe will add typed events to the Cosmos SDK as defined by the [Typed Events Specification](https:\/\/github.com\/ethereum\/typescript\/blob\/master\/spec\/typescript\/cosmos.md). This will be done by updating the `EventEmitter` class to use typed events. In addition, we will add a new `EventEmitter` type that will be used for on-chain communication between modules.\\n### Module-level events\\nThe `EventEmitter` class will be used to emit events to all modules, and to receive events from them. A module will emit an event to the `EventEmitter` if it has an `EventEmitter` registered with it. When the `EventEmitter` is registered with a module, it will be passed a `Context` object that contains the `EventEmitter` and some other information about the user and the application that the event is for. This Context object will be passed to each event emitted by the module.\\nEach module will emit events with a type that is the name of the event type that the event is for. For example, in the case of a `NewOrder` event, the event type for the new order will be `NewOrder`. Each module can emit multiple events with the same type, but only one of"}
{"Context":"## Context\\nCurrently, users who log onto va.gov during repeat visits have potentially inconsistent behavior from the site, especially if they switch the credential they login with.\\nThis is most stark with aspects of the site that leverage persistent data, such as forms that the user has partially filled out, and which must be matched up to the user.\\nThe Accounts table is used today to aid in this, it creates a table entry that links an idme_uuid, icn, sec_id, to different data in our schema, and which we can retrieve\\neven after a user session has been ended and a new one started.\\nThere are some issues with the current implementation, however, which make the intended experience inconsistent. The Accounts table relies on a matching idme_uuid, first and foremost.\\nThen will attempt to match with an ICN or a sec_id, depending on the retrieval method. If a user logs in with different credentials, such as ID.me from va.gov, and then later\\nDS Logon from MyHealtheVet (then navigates to VA.gov), they may see different forms and other persistent data depending on the credential.\\nMy Proposal involes refactoring the work done for the Accounts table into something more along the lines of how we actually deal with our users. This involves the concept\\nof a 'credential_user', and a 'real_user', and tables to represent each of these: `user_verifications` and `user_accounts`.\\nA 'credential_user' is our perspective of a user simply as information we get from a specific credential provider. For example, if a user logs in with MHV, we would\\nexpect to have an MHV uuid representing their account uuid. When a user authenticates, we would store the 'credential_user' by finding or creating an entry in the `user_verifications`\\ntable. This table would store the uuid from whatever credential provider they logged in with, whether or not the user was verified (LOA3\/IAL2) and a link to an entry in the `user_accounts`\\ntable.\\nA 'real_user' is our perspective of an actual human behind the entity logging in, regardless of how they logged in. These are reflected in a `user_accounts` table, and would\\nonly have an optional icn value (if icn is set, we know they are a real, verified human. Because we can make an MPI call with only an icn, this value represents all of the 'real'\\ndata we have on the 'real_user'. If an icn is not set, we know there is a human behind the entity logging in, but we cannot verify it to any human in our records).\\nEvery 'credential_user' will have a link to a single 'real_user', and every `user_verifications` entry will have a link to a `user_accounts` record. If a user was not verified\\nand then becomes verified, the `user_accounts` entry may either simply be updated with the new icn, or, if an existing `user_accounts` record exists with that icn, it will be\\nreplaced by the existing `user_accounts` entry, and the unverified `user_accounts` entry will be moved into a `deprecated_user_accounts` table.\\nEvery persistent table on the backend will point to a `user_accounts` uuid, as we want to correlate these to a 'real_user', not to the specific credential. By organizing\\nthings this way, we will have consistent behavior for the end user, regardless of whatever credential combination they often use. This\\n","Decision":"## Decision\n1 or more sentences summarizing the decision made.\\n","Prediction":"includes both the `user_verifications` table, and the `user_accounts` table.\\n## Decision\nWe will refactor the Accounts table into a 'credential_user' table, and a 'real_user' table. This will involve changing the current Accounts table\\nto reflect the following changes:\\n- Create a new table entry for each 'real_user' we have\\n- Link each 'real_user' to a single 'credential_user' table entry\\n- Delete all existing entries in the Accounts table, and create a new Accounts table with a single entry for each 'real_user' we have\\n- Merge the 'real_user' table into the `user_verifications` table\\n- Merge the `user_verifications` table into the `user_accounts` table\\n- Merge the `user_verifications` table into the `user_accounts` table\\n- Remove the `user_verifications` table and the associated `user_accounts` table entries from the `user_verifications` table\\n- Remove the `user_accounts` table entry and the associated `user_verifications` table entries from the `user_verifications` table\\n- Remove the `user_verifications` table entry and the associated `user_accounts` table entries from the `user_accounts` table\\n- Remove"}
{"Context":"## Context\\nBlocking interfaces are those that only return when an operation has been completed.\\nNon-blocking interfaces initiate a request and return when the request is submitted. Completion is annouced via a callback or by checking the status through another interface.\\nNon-blocking interfaces provide a more flexible implementation route.\\nBlocking implementations are typically avoided on embedded systems. Typically, a thread should sleep while waiting for an action to complete, rather than hogging processor resources.\\n","Decision":"## Decision\nInterfaces provided by the framework will be non-blocking. Users can write their own blocking wrappers if blocking code is needed.\\n","Prediction":"## Decision\nWe will use non-blocking interfaces where possible.\\nWe will use blocking interfaces when non-blocking interfaces are required to complete a task.\\nWe will use non-blocking interfaces where we are aware of the limitations of the system.\\nWe will use blocking interfaces where we are not.\\nWe will use non-blocking interfaces for all necessary communication with the operating system.\\nWe will use blocking interfaces for all non-blocking interfaces.\\nWe will use non-blocking interfaces for all data access.\\nWe will use blocking interfaces for all data access that is not strictly necessary for the operation of the application.\\nWe will use non-blocking interfaces for all non-blocking interfaces that we are not aware of.\\nWe will use non-blocking interfaces for all non-blocking interfaces that we are not aware of.\\nWe will use non-blocking interfaces for all non-blocking interfaces that we are not aware of.\\nWe will use blocking interfaces for all blocking interfaces that we are not aware of.\\nWe will use blocking interfaces for all blocking interfaces that we are not aware of.\\nWe will use blocking interfaces for all blocking interfaces that we are not aware of.\\nWe will use blocking interfaces for all blocking interfaces that we are not aware of.\\nWe will use blocking interfaces for all blocking interfaces that"}
{"Context":"## Context\\nWe need a reliable infrastructure definition that will allow developers to replicate the environment on their local\\nmachine.\\n","Decision":"## Decision\nWe use Docker (1.13.0+) to encapsulate all architecture components. We use Docker Compose (1.13.0+) to orchestrate\\nthe containers in a local environment.\\n","Prediction":"## Decision\nWe will use [Docker](https:\/\/www.docker.com\/)\\nWe will use [Docker Compose](https:\/\/docs.docker.com\/compose\/)\\nWe will use [Docker Compose Service](https:\/\/docs.docker.com\/compose\/service\/) to deploy the application\\nwe will use [Docker Swarm](https:\/\/docs.docker.com\/compose\/swarm\/) to manage the containers\\nWe will use [Docker Swarm Persistent Volume](https:\/\/docs.docker.com\/compose\/swarm\/persistent-volume\/) to provide persistent storage to the container\\nWe will use [Docker Volume](https:\/\/docs.docker.com\/compose\/volumes\/) to provide a persistent volume for the container\\nWe will use [Docker Network](https:\/\/docs.docker.com\/compose\/network\/) to provide network connectivity between containers\\nWe will use [Docker Swarm Persistent Volume](https:\/\/docs.docker.com\/compose\/volumes\/) for persistent storage to the container\\nWe will use [Docker Swarm Persistent Volume](https:\/\/docs.docker.com\/compose\/volumes\/) for persistent storage to the container\\nWe will use [Docker Swarm Persistent Volume](https:\/\/docs.docker.com\/compose\/volumes\/) for persistent storage to the container\\nWe will use [Docker Swarm Persistent Volume"}
{"Context":"## Context\\nThe Shadow DOM API allows to encapsulate the DOM and the styles of a component. This means that we don't have to worry about the correct scoping of our CSS, nor about our internal DOM being disturbed by anything outside our component. Stencil provides the ability to enable the Shadow DOM for our components.\\nDuring the implementation of the Shadow DOM we encountered the following problems:\\n- It was necessary to rework the styling of all components where the Shadow DOM was activated\\n- We use the Material Design Components Web, which expect a defined structure of the DOM. However, the structure is hidden by the Shadow DOM, which means that some of the functions and styling of the MDC components were no longer effective\\n- Using the Shadow DOM often resulted in complex workarounds to mimic the MDC functionality and therefore to re-implement most of the styles from MDC\\n- The selectors `::shadow`, `\/deep\/` and `>>>` for the Shadow DOM are no longer supported by the browser and the alternative selectors `::part` and `::theme` are only a draft of the CSS Shadow Parts\\n","Decision":"## Decision\nThe problems mentioned above led us to decide against using the Shadow DOM for the time being. The Shadow DOM offers practical advantages, but in its current form it is not worth the effort and complexity.\\nFor some components the Shadow DOM has already been activated without significant complications. We leave these components untouched and the Shadow DOM remains activated.\\n","Prediction":"## Decision\nWe will use the Shadow DOM for styling our components.\\nThis will mainly be used by the Material Design Components Web.\\nWe will implement the Shadow DOM using the Shadow DOM API and we will use the Shadow DOM configuration file to specify the Shadow DOM.\\nWe will use the Shadow DOM API to expose the Shadow DOM to our components.\\nWe will use the Shadow DOM API to enable the Shadow DOM for our components.\\nWe will use the Shadow DOM API to disable the Shadow DOM for our components.\\nWe will use the Shadow DOM API to enable the Shadow DOM for all components.\\nWe will use the Shadow DOM API to disable the Shadow DOM for all components.\\nWe will only register the Shadow DOM in the HTML of our components.\\nWe will register the Shadow DOM in the CSS of our component stylesheets.\\nWe will register the Shadow DOM in the stylesheets of our component HTML.\\nWe will register the Shadow DOM in the React component tree.\\nWe will register the Shadow DOM in the stylesheets of our component templates.\\nWe will register the Shadow DOM in the stylesheets of our component components.\\nWe will register the Shadow DOM in the stylesheets of our component components.\\nWe will register the Shadow DOM in the"}
{"Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","Prediction":"## Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\nWe will use [Confluence](http:\/\/www.confluence.org)\\nWe will use [JIRA](https:\/\/confluence.atlassian.com\/display\/JIRA\/Architectural+Decision+Records)\\nWe will use [Pivotal Tracker](https:\/\/www.pivotaltracker.com\/)\\nWe will use [Trello](https:\/\/trello.com\/)\\nWe will use [AWS CloudFormation](https:\/\/aws.amazon.com\/blogs\/compute\/aws-architecture-decision-records-architectural-decision-records\/)\\nWe will use [AWS CloudFormation Runbooks](https:\/\/aws.amazon.com\/blogs\/compute\/using-architecture-decision-records-architectural-decision-records-in-aws-architecture-decision-records\/)\\nWe will use [AWS CodePipeline](https:\/\/aws.amazon.com\/blogs\/compute\/using-architecture-decision-records-architectural-decision-records-in-aws-architecture-decision-records\/)\\nWe will use [AWS CodeBuild](https:\/\/"}
{"Context":"## Context\\nFacing the concern that by serving views from the backend API, I have less opportunity for reusing APIs, creating new apps and staying on trend, I propose to decouple the frontend and backend architecture of the site.\\nThe opportunities that this creates would be to learn new technologies such as\\n- GraphQL: new approach to APIs to solve the problem of creating different endpoints for different frontend requirements.\\n- Server-side rendering: important for Search engine optimization and performance.\\n","Decision":"## Decision\n- Re-write the backend in Fastify for better Typescript support.\\n- Create a server-side rendered frontend.\\n","Prediction":"## Decision\nI will use GraphQL as the frontend API to the site. This will allow me to decouple the frontend from the backend and keep the API accessible from the frontend.\\nIn the future, I will also consider using SASS and SCSS to create a more consistent look and feel.\\nI will use the following technologies as part of my frontend architecture:\\n* Vue.js: for the frontend frontend\\n* React: for the frontend\\n* TypeScript: for the frontend\\n* React-Redux: for the frontend\\n* React-Router: for the frontend\\n* React-Router-DevTools: for the frontend\\n* React-Router-Local: for the frontend\\n* React-Local: for the frontend\\n* React-Bootstrap: for the frontend\\n* React-Bootstrap-DevTools: for the frontend\\n* React-Bootstrap-Local: for the backend\\n* React-Bootstrap-Local-DevTools: for the backend\\n* React-Bootstrap-Local: for the frontend\\n* React: for the frontend\\n* React-Bootstrap: for the backend\\n* React-Bootstrap-DevTools: for the backend\\n* React-Bootstrap-Local: for the frontend\\n* React-Bootstrap-Local-DevTools: for the"}
{"Context":"## Context\\nThe container currently runs an nginx web server to host the output file for consuming applications.\\nIf the container is not running, the file is not available.\\n","Decision":"## Decision\nThe nginx service will be removed and the `gp-data.json` file will be written to the team's preferred cloud hosting platform.\\n","Prediction":"## Decision\nThe container will run the nginx web server to host the file for consuming applications.\\nThis will allow the container to run without the need to restart the nginx web server.\\nThis should also allow the container to run in a non-default port, if the application needs to be run on a different port.\\nThis change will also allow the container to run in a non-default path, if the application needs to be run in a different location.\\nThis change will also allow the container to run in a non-default log file, if the application needs to log to a different location or file.\\nThis change will also allow the container to run in a non-default log file, if the application needs to run in a non-default log file.\\nThis change will allow the container to run in a non-default log file, if the application needs to run in a non-default log file.\\nThis change will allow the container to run in a non-default log file, if the application needs to run in a non-default log file.\\nThis change will allow the container to run in a non-default log file, if the application needs to run in a non-default log file.\\nThis change will allow the container to run in a non-default log file, if"}
{"Context":"## Context\\nBefore [PR#4845](https:\/\/github.com\/tendermint\/tendermint\/pull\/4845),\\n`Header#LastResultsHash` was a root of the Merkle tree built from `DeliverTx`\\nresults. Only `Code`, `Data` fields were included because `Info` and `Log`\\nfields are non-deterministic.\\nAt some point, we've added events to `ResponseBeginBlock`, `ResponseEndBlock`,\\nand `ResponseDeliverTx` to give applications a way to attach some additional\\ninformation to blocks \/ transactions.\\nMany applications seem to have started using them since.\\nHowever, before [PR#4845](https:\/\/github.com\/tendermint\/tendermint\/pull\/4845)\\nthere was no way to prove that certain events were a part of the result\\n(_unless the application developer includes them into the state tree_).\\nHence, [PR#4845](https:\/\/github.com\/tendermint\/tendermint\/pull\/4845) was\\nopened. In it, `GasWanted` along with `GasUsed` are included when hashing\\n`DeliverTx` results. Also, events from `BeginBlock`, `EndBlock` and `DeliverTx`\\nresults are hashed into the `LastResultsHash` as follows:\\n- Since we do not expect `BeginBlock` and `EndBlock` to contain many events,\\nthese will be Protobuf encoded and included in the Merkle tree as leaves.\\n- `LastResultsHash` therefore is the root hash of a Merkle tree w\/ 3 leafs:\\nproto-encoded `ResponseBeginBlock#Events`, root hash of a Merkle tree build\\nfrom `ResponseDeliverTx` responses (Log, Info and Codespace fields are\\nignored), and proto-encoded `ResponseEndBlock#Events`.\\n- Order of events is unchanged - same as received from the ABCI application.\\n[Spec PR](https:\/\/github.com\/tendermint\/spec\/pull\/97\/files)\\nWhile it's certainly good to be able to prove something, introducing new events\\nor removing such becomes difficult because it breaks the `LastResultsHash`. It\\nmeans that every time you add, remove or update an event, you'll need a\\nhard-fork. And that is undoubtedly bad for applications, which are evolving and\\ndon't have a stable events set.\\n","Decision":"## Decision\nAs a middle ground approach, the proposal is to add the\\n`Block#LastResultsEvents` consensus parameter that is a list of all events that\\nare to be hashed in the header.\\n```\\n@ proto\/tendermint\/abci\/types.proto:295 @ message BlockParams {\\nint64 max_bytes = 1;\\n\/\/ Note: must be greater or equal to -1\\nint64 max_gas = 2;\\n\/\/ List of events, which will be hashed into the LastResultsHash\\nrepeated string last_results_events = 3;\\n}\\n```\\nInitially the list is empty. The ABCI application can change it via `InitChain`\\nor `EndBlock`.\\nExample:\\n```go\\nfunc (app *MyApp) DeliverTx(req types.RequestDeliverTx) types.ResponseDeliverTx {\\n\/\/...\\nevents := []abci.Event{\\n{\\nType: \"transfer\",\\nAttributes: []abci.EventAttribute{\\n{Key: []byte(\"sender\"), Value: []byte(\"Bob\"), Index: true},\\n},\\n},\\n}\\nreturn types.ResponseDeliverTx{Code: code.CodeTypeOK, Events: events}\\n}\\n```\\nFor \"transfer\" event to be hashed, the `LastResultsEvents` must contain a\\nstring \"transfer\".\\n","Prediction":"## Decision\nWe will not introduce new events or remove the ones we already have.\\nWe will instead introduce a new set of events that will be part of the Merkle tree\\nand will be used to prove that a particular Merkle tree is the one that was\\ncreated by the ABCI application.\\nWe will also introduce a new `MerkleTree` class to the ABCI project.\\nThis class will be used to prove the tree, and will be used to create new\\nmerkle trees.\\n* `MerkleTree` will be a non-`Array` structure. It will contain the\\n`MerkleTree` node, the `MerkleTree` leaf, and the Merkle root.\\n* `MerkleTree` nodes will be stored as `Array`s of the\\n`MerkleTree` type.\\n* `MerkleTree` leafs will be stored as `Array`s of the\\n`MerkleTree` type.\\n* `MerkleTree` root will be stored as a pointer to the\\n`MerkleTree` node.\\n* `MerkleTree` will contain a `MerkleTree` node, a\\n`M"}
{"Context":"## Context\\nStrong references between SQL tables using foreign keys are useful for maintaining data integrity. They introduce safeguards against orphaned records\\nby,\\n- making it impossible to create data records that do not fulfill relation.\\n- deleting referenced records in a cascading fashion when the root dependent record is deleted.\\nIn the context of Simple, a good example would be the `Patient` and `BloodPressureMeasurement` tables. In real world, it\u2019s impossible to record a\\nblood pressure for a non-existent patient. Likewise, maintaining a strong reference between these two tables in SQL would have ensured that blood\\npressures do not get created either unless the associated patient already exists in the database.\\nAdditionally, if the patient ever gets deleted, SQLite would have handled deletion of all their blood pressures automatically.\\nUnfortunately, strong references work only when the data storage is centralized. Because Simple\u2019s data is distributed across many devices, it\u2019s\\nimpossible to guarantee the existence of all data records at the same time due to its offline by default nature. For instance, it\u2019s easy to imagine a\\nscenario where the app receives some blood pressures from the server, but fails to receive their associated patients because of bad network\\nconnectivity. This is expected behavior and we do not want the app to fail because of a foreign key integrity failure.\\n","Decision":"## Decision\nSQL tables in Simple will not keep strong references using foreign keys unless they can be synced **together** in the same network call with the\\nserver.\\n","Prediction":"## Decision\nWe will enforce strict foreign key constraints between the `Patient` and `BloodPressureMeasurement` tables. This will ensure that\\nrecords in the `Patient` table have no relation with any records in the `BloodPressureMeasurement` table.\\nWe will also enforce strict foreign key constraints between the `Patient` and `BloodPressureMeasurement` tables when the following conditions are met:\\n* The `Patient` table has a foreign key to the `BloodPressureMeasurement` table.\\n* The `Patient` table has a foreign key to the `BloodPressureMeasurement` table\u2019s parent table.\\n* The `Patient` table has a foreign key to the `BloodPressureMeasurement` table\u2019s child table.\\n* The `Patient` table\u2019s foreign key to the `Patient` table\u2019s parent table\u2019s foreign key.\\n* The `Patient` table\u2019s foreign key to the `Patient` table\u2019s child table\u2019s foreign key.\\n* The `Patient` table\u2019s foreign key to the `Patient` table\u2019s child table\u2019s foreign key.\\n* The `Patient` table\u2019s primary key is not the foreign key to the `Patient` table\u2019s foreign key.\\n* The `Patient` table\u2019s primary key is not the foreign key to the `"}
{"Context":"## Context\\nThe issue motivating this decision, and any context that influences or constrains the decision.\\n","Decision":"## Decision\nThe change that we're proposing or have agreed to implement.\\n","Prediction":"## Decision\nThe change that we're proposing or have agreed to implement.\\nThis includes any code review that's required as part of the change, as well as any change that's required to do something that we didn't explicitly say we'd do.\\nThis includes any change that causes or could cause unexpected behaviour.\\nThis includes any change that might break existing behaviour.\\nThis includes any change that might require changes to third-party APIs or services.\\nThis includes any change that might require changes to internal systems, products, or services.\\nThis includes any change that might require changes to external systems, products, or services.\\nThis includes any change that might require changes to internal infrastructure, products, or services.\\nThis includes any change that might require changes to external infrastructure, products, or services.\\nThis includes any change that might require changes to internal infrastructure, products, or services.\\nThis includes any change that might require changes to external infrastructure, products, or services.\\nThis includes any change that might require changes to internal infrastructure, products, or services.\\nThis includes any change that might require changes to external infrastructure, products, or services.\\nThis includes any change that might require changes to internal infrastructure, products, or services.\\nThis includes"}
{"Context":"## Context\\nThe old skin uses the Bootstrap 3, which uses the `float` and `clear` CSS properties for layout, instead of the more modern `flexbox`. Therefore, we looked at different frameworks which can help reduce our efforts in writing CSS classes and Vue components from scratch.\\n### [Vuetify](https:\/\/vuetifyjs.com\/en\/)\\nIt is a big collection of ready-made UI elements based on Google's Material Design. It looks like it could bloat our JavaScript and CSS and make the page look very generic. Also, binding to Material Design might be tricky due to overwriting lots of Material Design assumptions.\\n### [Buefy](https:\/\/buefy.org\/)\\nA \"lightweight\" component library. Looks nice and does a lot of work for us. It is based on [Bulma CSS](https:\/\/bulma.io\/) which seems to be established and is a good combination between usable and adaptable.\\n### [Element.io](https:\/\/element.eleme.io\/#\/en-US)\\nThe documentation does not look good enough. The Q&A page on their website is not in English.\\n### [Tailwind CSS](https:\/\/tailwindcss.com\/docs\/what-is-tailwind\/)\\nIt is not a component library, but a collection of useful CSS classes for rapid prototyping components (by adding lots and lots of class names to them). When the prototyping phase is over, we need to combine the CSS classes into components. We are not sure we want to go into the direction of \"CSS Framework external to components\".\\n","Decision":"## Decision\nWe will use Buefy, a UI components framework for Vue.js based on Bulma which is a free, open source CSS framework based on Flexbox.\\n","Prediction":"## Decision\nWe will use Tailwind CSS.\\nWe will use Tailwind because:\\n* It is a component library which is actively developed.\\n* It is very popular and widely used.\\n* It is well documented.\\n* It is cross-browser compatible.\\n* It is a CSS framework.\\n* It is easy to setup and use.\\n* It is easy to maintain.\\n* It is easy to upgrade.\\n* It is free.\\n* It is easy to use and understand.\\n* It is easy to style.\\n* It is easy to customize.\\n* It is a component library which does not need to be used inside components.\\n* It is a CSS framework which can be used inside components.\\n* It is a CSS framework which can be used in templates.\\n* It is a CSS framework which can be used in semantic markup.\\n* It is a CSS framework which can be used in semantic markup.\\n* It is a CSS framework which does not require any external libraries.\\n* It is a CSS framework which can be used in a semantic manner.\\n* It is a CSS framework which does not require a lot of boilerplate code.\\n* It is a CSS framework which does not require"}
{"Context":"## Context\\nThe Zooniverse maintains numerous Rails APIs, mostly built between three and five years ago. These vary wildly in the amount of attention they've received and all use gems that are deprecated, unmaintained, or otherwise generally not a good idea in TYOOL 2019. As such, research was necessary into what was more modern, standardized API functionality.\\nAny solution would need to meet the following requirements:\\n* CRUD and serialization\\n* Pagination\\n* Filtration\\n* Authentication and Authorization\\n","Decision":"## Decision\nAt the end of the day it was difficult to not choose the JSON:API spec for a greenfield project. It was a balance of standardization vs. gadgetiness, basic non-DRY controllers on one end, GraphQL on the other. It is a spec that our front end devs are sort-of familiar with on account of our existing services' sort-of implementations of it. It gave me the opportunity to go down the rabbit hole a bit to learn what's out there, and some of those discoveries were surprising (ActiveModelSerializers is fully deprecated, who'd have guessed?)\\nWith that in mind, I test drove a few of the suboptions. JSONAPI::Resources was more a framework, and while I'm sure that this relatively simple use case could be handled by it, I found myself getting hung up on straightforward questions because I wasn't sure I was using all of the magic correctly. This definitely seemed like overkill for what I wanted to accomplish. Furthermore, all of that magic and extra (required) documentation reading would make it that much more difficult to onboard someone new to work on this app.\\nIn the end, I decided to go with jsonapi.rb. It leans on existing, popular tech to do the hard stuff (fast_jsonapi and ransack for serialization and filter\/sort, respectively). And pretty much everything else it does is stuff that I'd have to implement manually. This way, though, I can see exactly what it's doing, how it's being done, and override it if necessary. There's no sorcery or empty controllers, just simple classes\/methods that are clear and extendable.\\nI even fixed an example in the README and my PR was merged a couple hours later, so it's certainly active. That said, it would be a relatively straightforward process to disentangle this gem entirely from the rest of the app by just grabbing a few of the necessary classes out of its \/lib and throwing them into the app's.\\n","Prediction":"## Decision\n* Use the [Swagger specification](https:\/\/swagger.io\/spec\/) to generate documentation for the API endpoints.\\n* Use [Swagger UI](https:\/\/swagger.io\/) to generate documentation for the UI.\\n* Use the [JWT authentication mechanism](https:\/\/jwt.io\/), as it is supported by the API and the UI.\\n* Use the [JWT secret generation mechanism](https:\/\/jwt.io\/) to generate a secret key to use for authentication.\\n* Use the [JWT signature mechanism](https:\/\/jwt.io\/) for signing the JWT token.\\n* Use the [JWT signature validation mechanism](https:\/\/jwt.io\/) for validating the JWT token.\\n* Use the [JWT authorization mechanism](https:\/\/jwt.io\/) for granting or denying authorization for the API and the UI.\\n* Use the [JWT authorization scopes](https:\/\/jwt.io\/) to grant or deny access to the API and the UI.\\n* Use the [JWT authorization policies](https:\/\/jwt.io\/) to define access control policies.\\n* Use the [JWT policy inheritance mechanism](https:\/\/jwt.io\/) to define who can access the API and the UI.\\n* Use the [JWT policy inheritance rules](https:\/\/jwt.io\/) to define what can be inherited.\\n"}
{"Context":"## Context and Problem Statement\\nHow to guarantee reproducibility of Jupyter Notebooks?\\nIn order to allow any user to re run the notebook with similar behaviour, it's important that each notebook is shipped with dependencies requirements\\nthat include direct and transitive dependencies. This would also enforce and support security, reproducibility, traecability.\\nNotebooks should be treated as component\/service that use their own dependencies, therefore when storing notebooks,\\nthey should be stored with dependencies so that an image can be built to run them or they can be shared and reused by others.\\n## Decision Drivers <!-- optional -->\\n* user prospective\\n* reproducibility\\n* traecability\\n","Decision":"## Decision\n* user prospective\\n* reproducibility\\n* traecability\\nThe option selected is 3. because:\\n* enforce reproducibility\\n* enforce traceability between notebook\\n### Positive Consequences <!-- optional -->\\n* Satisfy reproducibility, traecability, shareability.\\n* Notebooks are coupled with dependencies in their metadata.\\n* If more notebooks are present, a common Pipfile can be created with a button that can automatically extract from all notebook dependencies and new common Pipfile.lock will be created. This would allow creation of an image that can run the notebooks.\\n","Prediction":"## Decision\n* user prospective\\n* reproducibility\\n* traecability\\nChosen option: \"Use JupyterHub\", because it's the most mature and feature-rich solution and it's also the most widely used.\\n### Positive Consequences <!-- optional -->\\n* easy to use and maintain\\n* most widely used\\n* stable\\n* good community support\\n* open-source\\n* easy to integrate with existing tools\\n* [reference implementation](https:\/\/github.com\/jupyter\/jupyterhub#reference\\n### Negative Consequences <!-- optional -->\\n* requires a JupyterHub server to be installed\\n* requires a network connection to the JupyterHub server\\n* requires a Python installation\\n* requires Docker installed\\n* requires [Jupyter notebook](https:\/\/jupyter.org\/)\\n* requires [JupyterLab](https:\/\/jupyterlab.com\/)\\n* requires [JupyterLab notebooks](https:\/\/jupyterlab.com\/)\\n* requires a JupyterHub account to be used\\n* requires a JupyterHub account to be used\\n* requires a JupyterHub account to be used\\n* requires a JupyterHub account to be used\\n* requires a JupyterHub account to be used\\n* requires a Jupyter"}
{"Context":"## Context\\nWe need to choose test framework.\\n","Decision":"## Decision\nWe use JUnit because it is a standard framework for JVM, and we don't have any advanced requirements yet.\\n","Prediction":"## Decision\nWe will use [Jasmine](https:\/\/jasmine.github.io\/) for testing in this project.\\nWe will use [Mocha](https:\/\/mochajs.org\/) for testing in this project.\\nWe will use [Karma](https:\/\/karma-runner.io\/) for testing in this project.\\nWe will use [Protractor](https:\/\/protractor.io\/) for testing in this project.\\nWe will use [QUnit](https:\/\/qunit.io\/) for mocking.\\nWe will use [Selenium](https:\/\/seleniumhq.org\/) for testing in this project.\\nWe will use [Thrust.js](https:\/\/thrustjs.org\/) for testing in this project.\\nWe will use [YUI Test Framework](https:\/\/yui-test-framework.org\/) for testing in this project.\\n### Decision\nWe will use [Jasmine](https:\/\/jasmine.github.io\/) for testing in this project.\\nWe will use [Mocha](https:\/\/mochajs.org\/) for testing in this project.\\nWe will use [Karma](https:\/\/karma-runner.io\/) for testing in this project.\\nWe will use [Protractor](https:\/\/protractor.io\/) for testing in this project.\\n"}
{"Context":"## Context\\nSome ops have an ordinal parameter which switches between a few possible modes. Giving those modes a proper name\\nmakes usage and documentation easier.\\n","Decision":"## Decision\nWe allow `Arg` sections to have an `ENUM` data type and add a `possibleValues` property to define the possible values\\nfor this arg. The ordinal number of the enum is the same as its position within the `possibleValues` list starting from\\n`0`.\\nA runtime check on op construction, will ensure that each enum arg has one or more possible values, and that default\\nvalues match one of the possible values (if applicable).\\nOn code generation, an appropriate representation of this enum will be generated in the target language. The name of\\nthe generated enum will be derived from the name of the arg.\\n### Example\\n```kotlin\\nArg(ENUM, \"padMode\"){\\npossibleValues = listOf(\"CONSTANT\", \"REFLECT\", \"SYMMETRIC\")\\ndescription = \"padding mode\"\\n}\\n```\\n","Prediction":"## Decision\nThe ordinal parameter will be called `mode`, and will be prefixed with an `o`.\\nThe `o` prefix will be removed from the parameter name in the future.\\nThe `mode` will be the name of the operation mode (e.g. `get`, `update`, `delete`, etc.).\\n`get` will be the default mode. It will be used when the ordinal parameter is not specified.\\n`update` will be used when the ordinal parameter is specified and the mode is `get`.\\n`delete` will be used when the ordinal parameter is not specified.\\n`create` will be used when the ordinal parameter is not specified.\\n`update` will be used for operations that modify the underlying object, and `get` will be used for operations that do not modify the underlying object.\\n`delete` will be used for operations that modify the underlying object, and `get` will be used for operations that do not modify the underlying object.\\n`create` will be used for operations that modify the underlying object, and `get` will be used for operations that do not modify the underlying object.\\n`get` will be used for operations that modify the underlying object, and `update`"}
{"Context":"## Context\\nProvide reasons and context of this ADR.\\n","Decision":"## Decision\nProvide the decision taken by the team.\\n","Prediction":"## Decision\nThe decision is to use the [TypeDoc](https:\/\/typekit.com\/blog\/) ADR framework.\\nTypeDoc is a free and open-source tool for building ADRs that is recommended and widely used by the\\nopen source community.\\nTypeDoc is being used for this ADR because:\\n* It is free and open-source\\n* It is supported by the community\\n* It is recommended by the [Typekit](https:\/\/typetalk.typekit.io\/) team\\n* It is well documented\\n* It is well maintained\\n* It is being used by the [Typekit](https:\/\/typekit.com\/) team\\n* It is fast and easy to use\\n* It has a large community of developers\\nTypeDoc is being used for this ADR because:\\n* It is free and open-source\\n* It is supported by the community\\n* It is recommended by the [Typekit](https:\/\/typetalk.typekit.io\/) team\\n* It is well documented\\n* It is well maintained\\n* It is being used by the [Typekit](https:\/\/typekit.com\/) team\\n* It is fast and easy to use\\n* It has a large community of developers\\nTypeDoc is being used for this"}
{"Context":"## Context\\nValidators use remote signers to help secure their keys. This system is Tendermint's recommended way to secure validators, but the path to integration with Tendermint's private validator client is plagued with custom protocols.\\nTendermint uses its own custom secure connection protocol (`SecretConnection`) and a raw tcp\/unix socket connection protocol. The secure connection protocol until recently was exposed to man in the middle attacks and can take longer to integrate if not using Golang. The raw tcp connection protocol is less custom, but has been causing minute issues with users.\\nMigrating Tendermint's private validator client to a widely adopted protocol, gRPC, will ease the current maintenance and integration burden experienced with the current protocol.\\n","Decision":"## Decision\nAfter discussing with multiple stake holders, [gRPC](https:\/\/grpc.io\/) was decided on to replace the current private validator protocol. gRPC is a widely adopted protocol in the micro-service and cloud infrastructure world. gRPC uses [protocol-buffers](https:\/\/developers.google.com\/protocol-buffers) to describe its services, providing a language agnostic implementation. Tendermint uses protobuf for on disk and over the wire encoding already making the integration with gRPC simpler.\\n","Prediction":"## Decision\nThe Tendermint private validator client will use gRPC instead of the current custom protocol.\\nThis will allow the Tendermint protocol to be fully integrated into the Tendermint ecosystem. It will also make it easier to move the private validator client to a new protocol in the future.\\nIt will also allow the Tendermint private validator client to be used by other clients, such as the Tendermint client library for Go, which is currently only using the raw tcp\/unix socket protocol.\\nThis also allows the Tendermint private validator client to be used by other clients, such as the Tendermint client library for Go, which is currently only using the raw tcp\/unix socket protocol.\\nThis also allows the Tendermint private validator client to be used by other clients, such as the Tendermint client library for Go, which is currently only using the raw tcp\/unix socket protocol.\\nThis also allows the Tendermint private validator client to be used by other clients, such as the Tendermint client library for Go, which is currently only using the raw tcp\/unix socket protocol.\\nThis also allows the Tendermint private validator client to be used by other clients, such as the Tendermint client library for Go, which is currently only using the raw tcp\/unix"}
{"Context":"## Context\\nThe IBC module was originally developed in the Cosmos SDK and released during the Stargate release series (v0.42).\\nIt was subsequently migrated to its own repository, ibc-go.\\nThe first official release on ibc-go was v1.0.0.\\nv1.0.0 was decided to be used instead of v0.1.0 primarily for the following reasons:\\n- Maintaining compatibility with the IBC specification v1 requires stronger support\/guarantees.\\n- Using the major, minor, and patch numbers allows for easier communication of what breaking changes are included in a release.\\n- The IBC module is being used by numerous high value projects which require stability.\\n### Problems\\n#### Go module version must be incremented\\nWhen a Go module is released under v1.0.0, all following releases must follow Go semantic versioning.\\nThus when the go API is broken, the Go module major version **must** be incremented.\\nFor example, changing the go package version from `v2` to `v3` bumps the import from `github.com\/cosmos\/ibc-go\/v2` to `github.com\/cosmos\/ibc-go\/v3`.\\nIf the Go module version is not incremented then attempting to go get a module @v3.0.0 without the suffix results in:\\n`invalid version: module contains a go.mod file, so major version must be compatible: should be v0 or v1, not v3`\\nVersion validation was added in Go 1.13. This means that in order to release a v3.0.0 git tag without a \/v3 suffix on the module definition, the tag must explicitly **not** contain a go.mod file.\\nNot including a go.mod in our release is not a viable option.\\n#### Attempting to import multiple go module versions for ibc-go\\nAttempting to import two versions of ibc-go, such as `github.com\/cosmos\/ibc-go\/v2` and `github.com\/cosmos\/ibc-go\/v3`, will result in multiple issues.\\nThe Cosmos SDK does global registration of error and governance proposal types.\\nThe errors and proposals used in ibc-go would need to now register their naming based on the go module version.\\nThe more concerning problem is that protobuf definitions will also reach a namespace collision.\\nibc-go and the Cosmos SDK in general rely heavily on using extended functions for go structs generated from protobuf definitions.\\nThis requires the go structs to be defined in the same package as the extended functions.\\nThus, bumping the import versioning causes the protobuf definitions to be generated in two places (in v2 and v3).\\nWhen registering these types at compile time, the go compiler will panic.\\nThe generated types need to be registered against the proto codec, but there exist two definitions for the same name.\\nThe protobuf conflict policy can be overridden via the environment variable `GOLANG_PROTOBUF_REGISTRATION_CONFLICT`, but it is possible this could lead to various runtime errors or unexpected behaviour (see [here](https:\/\/github.com\/protocolbuffers\/protobuf-go\/blob\/master\/reflect\/protoregistry\/registry.go#L46)).\\nMore information [here](https:\/\/developers.google.com\/protocol-buffers\/docs\/reference\/go\/faq#namespace-conflict) on namespace conflicts for protobuf versioning.\\n### Potential solutions\\n#### Changing the protobuf definition version\\nThe protobuf definitions all have a type URL containing the protobuf version for this type.\\nChanging the protobuf version would solve the namespace collision which arise from importing multiple versions of ibc-go, but it leads to new issues.\\nIn the Cosmos SDK, `Any`s are unpacked and decoded using the type URL.\\nChanging the type URL thus is creating a distinctly different type.\\nThe same registration on the proto codec cannot be used to unpack the new type.\\nFor example:\\nAll Cosmos SDK messages are packed into `Any`s. If we incremented the protobuf version for our IBC messages, clients which submitted the v1 of our Cosmos SDK messages would now be rejected since the old type is not registered on the codec.\\nThe clients must know to submit the v2 of these messages. This pushes the burden of versioning onto relayers and wallets.\\nA more serious problem is that the `ClientState` and `ConsensusState` are packed as `Any`s. Changing the protobuf versioning of these types would break compatibility with IBC specification v1.\\n#### Moving protobuf definitions to their own go module\\nThe protobuf definitions could be moved to their own go module which uses 0.x versioning and will never go to 1.0.\\nThis prevents the Go module version from being incremented with breaking changes.\\nIt also requires all extended functions to live in the same Go module, disrupting the existing code structure.\\nThe version that implements this change will still be incompatible with previous versions, but future versions could be imported together without namespace collisions.\\nFor example, let's say this solution is implemented in v3. Then\\n`github.com\/cosmos\/ibc-go\/v2` cannot be imported with any other ibc-go version\\n`github.com\/cosmos\/ibc-go\/v3` cannot be imported with any previous ibc-go versions\\n`github.com\/cosmos\/ibc-go\/v4` may be imported with ibc-go versions v3+\\n`github.com\/cosmos\/ibc-go\/v5` may be imported with ibc-go versions v3+\\n","Decision":"## Decision\nSupporting importing multiple versions of ibc-go requires a non-trivial amount of complexity.\\nIt is unclear when a user of the ibc-go code would need multiple versions of ibc-go.\\nUntil there is an overwhelming reason to support importing multiple versions of ibc-go:\\n**Major releases cannot be imported simultaneously**.\\nReleases should focus on keeping backwards compatibility for go code clients, within reason.\\nOld functionality should be marked as deprecated and there should exist upgrade paths between major versions.\\nDeprecated functionality may be removed when no clients rely on that functionality.\\nHow this is determined is to be decided.\\n**Error and proposal type registration will not be changed between go module version increments**.\\nThis explicitly stops external clients from trying to import two major versions (potentially risking a bug due to the instability of proto name collisions override).\\n","Prediction":"`github.com\/cosmos\/ibc-go\/v6` may be imported with ibc-go versions v3+\\n`github.com\/cosmos\/ibc-go\/v7` may be imported with ibc-go versions v3+\\n`github.com\/cosmos\/ibc-go\/v8` may be imported with ibc-go versions v3+\\n`github.com\/cosmos\/ibc-go\/v9` may be imported with ibc-go versions v3+\\n`github.com\/cosmos\/ibc-go\/v10` may be imported with ibc-go versions v3+\\n`github.com\/cosmos\/ibc-go\/v11` may be imported with ibc-go versions v3+\\n`github.com\/cosmos\/ibc-go\/v12` may be imported with ibc-go versions v3+\\n## Decision\nThe Cosmos SDK will be restructured to use a versioning scheme based on the protobuf version.\\nThis will allow for the protobuf version to be added to the Go module version without breaking compatibility with the IBC specification.\\nThis will also allow for the protobuf version to be used to unpack the protobuf type.\\nThis will also allow for the protobuf type to be used in extended functions.\\"}
{"Context":"## Context\\nWe need the user to be able to define Jenkins jobs in code and be able to import them into Jenkins.\\nWe have identified a number of ways to do this:\\n1. Define jobs with Groovy and inject the script, as we do for the Jenkins configuration\\n2. Automatically create the jobs by scanning a Github account\\n3. [Jenkins Job Builder]\\n4. [Job DSL plugin]\\n### Option 1\\nThis is the easiest for us as we don't need to implement anything new. We can use the mechanism of injecting Groovy script which is already available. It is also relatively easy for the user to use. The code for the jobs and configuration can't exceed 16 KB, which is a limitation, but we believe that is enough for compact Jenkins installations (a Jenkins with hundreds of jobs is an anti-pattern). [The limit] is because we use [user data] to implement this option.\\n### Option 2\\nJenkins provides a way to scan a Github organisation or accounts for repositories containing pipeline configurations in a Jenkinsfile. This should be quite easy for the user but the implementation can be quite complex. This option would require the user to pass extra parameters to the Jenkins Terraform module: at least one regular expression to filter the repositories to match, and a Github personal token. The token is needed because scanning Github as an unauthenticated user is extremely slow but only takes a few minutes for a user with authentication. As the module needs a token as an input, there is extra complexity around managing that secret. This would be relatively straightforward to do using the UI but providing this solution as code would be quite involved.\\n### Option 3\\n(Jenkins Job Builder) is probably the most commonly used at GDS (it's used by Notify, Digital Marketplace) - people generally like it but some issues were pointed out like a difficulty in upgrading to a newer version or in escaping quotes correctly. [GOV.UK] and Pay use a more ad-hoc, homebrewed approach. Both groups rely on Puppet or Chef to inject their jobs into Jenkins. We allow users to install their configuration management tool via cloud-init, so the user is still free to override any mechanism we provide.\\n### Option 4\\nThis hasn't been explored in great detail as we felt we'd already found a good solution. However, if we revisit the decision made in this PR, this tool should be evaluated more thoroughly.\\n","Decision":"## Decision\nWe decided to implement solution 1 to keep things simple and because of time constraints. Options two and three can still be used but will not be supported. In the future, we may consider to change to another solution if we feel there is the user need.\\n","Prediction":"## Decision\nWe will use the [Job DSL plugin](https:\/\/plugins.jenkins.io\/job-dsl\/) to define Jenkins jobs.\\nWe will run a [Jenkins Job Builder](https:\/\/plugins.jenkins.io\/job-builder\/) to define a new Jenkins job.\\nThis will allow users to define new jobs in a more familiar way, without needing to understand the technical details of Jenkins and its job DSL. This will also help us to test the job definition without the need for users to configure and maintain their own Jenkins installation.\\n### Implementation\\nWe will use the [Job DSL plugin](https:\/\/plugins.jenkins.io\/job-dsl\/) to define Jenkins jobs.\\nWe will use the [Jenkins Job Builder](https:\/\/plugins.jenkins.io\/job-builder\/) to define a new Jenkins job.\\nThis will allow users to define new jobs in a more familiar way, without needing to understand the technical details of Jenkins and its job DSL. This will also help us to test the job definition without the need for users to configure and maintain their own Jenkins installation.\\n### Implementation\\nWe will use the [Job DSL plugin](https:\/\/plugins.jenkins.io\/job-dsl\/) to define Jenkins jobs.\\nWe will use the [Jenkins Job Builder](https:\/\/"}
{"Context":"## Context\\nOur first attempt at [A\/B testing notifications](.\/017-ab-testing.md) surfaced some concerns:\\n- The patient selection mechanism selected patients across multiple treatment groups, and patients were excluded\\nincorrectly due to past experiments.\\n- There were inconsistencies in how notifications were dispatched, which resulted in some patients only\\nreceiving a subset of the message cascade.\\n- There are issues with how appointments are being recorded in the mobile app which affects how visits are counted.\\nThese inconsistencies make reporting complicated and reduce the quality of experiment data.\\nThese issues impact the validity of the experiment and need to be addressed before we rerun the first experiment and run subsequent experiments.\\nRelevant documents:\\n- [Relevant PRD](https:\/\/docs.google.com\/document\/d\/1WushMGEvKzRarGbYerWqUISevjKONPHXwpBgR8y7dOE\/edit#)\\n- [Experiment flow](https:\/\/docs.google.com\/document\/d\/1IMXu_ca9xKU8Xox_3v403ZdvNGQzczLWljy7LQ6RQ6A\/edit#)\\n","Decision":"## Decision\n### Patient Selection a.k.a Enrollment\\n- We will enforce treatment group membership constraints. Patients should not have multiple treatment group memberships in active experiments.\\n- Patients who have multiple scheduled appointments will not be included in the experiment.\\n- All experiments (including completed ones) will have start and end dates - we accidentally excluded a large number of patients from the experiments by not enforcing this constraint.\\n- Select patients close to their expected visit date. Selecting patients too far in advance increases the likelihood of the patient's state changing before their expected visit.\\n### Patient eviction\\nPatients may need to be evicted from the experiment as the experiment progresses. Evicting a patient from the experiment means that they will not be sent any pending messages, and will be excluded from the results. We will evict patients:\\n1. who had a new appointment created or their original appointment updated without a corresponding visit (i.e, no bp, blood sugar, or prescription drug was recorded).\\nThis is to avoid tracking their new expected visit date and moving their notifications to the correct date, which will introduce noise in the results.\\nA nurse can update a patient\u2019s appointment from the app and change its status or scheduled date.\\nIn this case the patient\u2019s expected return date becomes unclear because the appointment they were enrolled in the experiment for is no longer relevant.\\nThis can also happen if an accidental appointment with a different scheduled date was created.\\n2. where we failed to send a notification. This could happen if we have a invalid number or due to network failures.\\n### Tracking visits\\n- The start date for monitoring return visits for each patient should be the date of enrollment in an experiment (and treatment group).\\n- The end date for monitoring return visits for each patient should be 14 days from their appointment\u2019s scheduled date.\\nWe will not infer visits from the creation of appointments. We will rely on Blood Pressures, Blood Sugars and Prescription Drugs only.\\nNote that this is only in experimentation, and we will not make any changes to the existing dashboards\/reports that infer visits using appointments.\\n**Rationale**: A portion of our appointment records are created due to bugs and problematic UI flows.\\nThis makes it impossible to distinguish real visits from accidentally created appointments.\\n**Consequences**: Excluding appointment creation from our criteria will mean that our experiment patients\u2019 total visits will be ~10% lower than the numbers we report on the dashboard.\\nThis will not affect the validity of our results since this ~10% reduction will be distributed uniformly across the treatment groups.\\nSince this may lead to us undercounting visits, we will need to report relative improvements in return rates rather than absolute percentages.\\nFor example, if cascade group patients have a 20% return rate and control group patients have a 15% return rate, we should look at the difference here, which is 5% and not the absolute percentages.\\n### Reporting\\nWe will store the results of the experiment in a denormalized format that is close to our final reporting needs.\\n- The reporting schema will be tied to a treatment group membership.\\n- We will populate it everyday by scanning the previous day\u2019s activity.\\n- Any new metrics that we want to track will need to be added to this table.\\nWe will use the same database table to track membership, and the experiment related activity on each membership.\\nAlthough it might appeal to separate the notion of \"membership\" from \"reporting\" on them, we currently do not see a need to separate the tables.\\nUsing the same table and domain model simplifies the design significantly.\\nAdditionally, a \"membership_report\" table would essentially be a superset of the \"membership\" table, and reporting over (joining) two large data tables is not performant.\\nIf there is divergence in the reporting needs of various experiments, we will revisit this decision.\\n### Experiment timeline\\nWe currently capture an experiment's timeline by storing a `state` field. The `state` can be\\n`upcoming`, `running` and `completed`. There are more cadences to the experiment though, which aren't captured\\nwell by this attribute alone. For example\\n- Notifications are sent out until 3 days after enrollment ends.\\n- Patients need to be monitored till 14 days after enrollment ends.\\nThe cadences when put on a timeline look like:\\n![ab-experiment-timeline](https:\/\/github.com\/simpledotorg\/simple-server\/raw\/5a4008a79e1cffd635b2ce2348ec1b9dea5318e9\/doc\/arch\/resources\/ab_experiment_timeline.png)\\nAdditionally, the `state` attribute needs to be tracked every day and modified when the state changes.\\nThe `start_time`, `end_time`, and buffers already have all the information required to compute the cadences.\\nWe will remove the `state` field and introduce methods that work with `start_time` and `end_time` to compute each cadence.\\n### Object design, and separation of concerns\\nThe [experiment flow](https:\/\/docs.google.com\/document\/d\/1IMXu_ca9xKU8Xox_3v403ZdvNGQzczLWljy7LQ6RQ6A\/edit#)\\nprocedurally describes all the steps involved in running an experiment. Currently these responsibilities are shared by\\n`Experimentation::Experiment` and `Experimentation::Runner`. We will refactor them into the following classes:\\n| Class |  Responsibility |\\n| :--- | :----------- |\\n| `Experimentation::Experiment` | Setup experiments and assign treatment groups integrally. |\\n| `Experimentation::TreatmentGroup` | Describes a treatment group and the behaviour for the patients in it. |\\n| `Experimentation::TreatmentGroupMembership` | Stores the patients membership in a treatment group and collects related data. |\\n| `Experimentation::ReminderTemplate` | Specifies what messages need to be sent to which treatment group and when. |\\n| `Experimentation::NotificationsExperiment` | Specifies the default eligibility criteria and conducts a notifications experiment. |\\n| `Experimentation::CurrentPatientExperiment` | Specifies the eligibility criteria for selection and conducts a current patient experiment. |\\n| `Experimentation::StalePatientExperiment` | Specifies the eligibility criteria for selection and conducts a stale patient experiment. |\\n### FAQs\\n- Why split `Experiment` and `NotificationsExperiment`?\\n`Experiment` has significant responsibilities in maintaining integrity across experiments and providing an API for the timeline.\\nAnd `NotificationsExperiment` handles all \"notification\" related aspects of an experiment including day to day operations.\\n- Why create subclasses for individual experiments? The individual experiments will be subclassed from `NotificationsExperiment`\\nsince they have enough shared behaviour but have different selection and notifying behaviours.\\n","Prediction":"## Decision\nWe will run a new experiment which will address the issues outlined above.\\nWe will run the experiment in parallel to the first experiment and test the new interface in a controlled environment\\nto ensure it is working correctly.\\nWe will run the tests before releasing the new interface to ensure the new interface is working correctly.\\nWe will run the new interface in parallel to the first experiment and run a validation test to ensure the new interface is working correctly.\\nWe will remove the old interface from the second experiment.\\nWe will run the second experiment in parallel to the first experiment and run a validation test to ensure the new interface is working correctly.\\nWe will remove the old interface from the second experiment.\\nWe will run the second experiment in parallel to the first experiment and run a validation test to ensure the new interface is working correctly.\\nWe will remove the old interface from the second experiment.\\nWe will run the second experiment in parallel to the first experiment and run a validation test to ensure the new interface is working correctly.\\nWe will remove the old interface from the second experiment.\\nWe will run the second experiment in parallel to the first experiment and run a validation test to ensure the new interface is working correctly.\\nWe will remove the old interface"}
{"Context":"## Context\\nDrains are an important aspect of any model, they are an essential boundary\\ncondition for cases such as biomass drains. Curently rate laws are only specified using\\nthe Generalised MWC format with the catalytic aspect using the modular rate law.\\nIntroducing drains into Maud requires implementing this into the ODE system which\\nrelies on enzymes as the defining feature, which are attributes of reactions in Maud.\\nTo specify drains we create a new class independent of reactions, despite the fact that\\nthey occur in the system of ODEs. This benefits post-processing as some techniques rely\\non varying enzyme concentrations, which we've established that drains do not have.\\nDrains are considered as priors to ensure that prior predictive checks are informative\\nof the experimental condition.\\n","Decision":"## Decision\nDrains will not be considered as a reaction class.\\n","Prediction":"## Decision\n- We introduce a new class independent of reactions, the `Drain` class, which is\\nimplemented in the `Drain` module.\\n- We introduce a new class `Drain` that is implemented in the `Drain` module.\\n- We introduce a new class `Drain` that is implemented in the `Drain` module.\\n- We introduce a new method `drain()` that is implemented in the `Drain` module.\\n- We introduce a new method `drain()` that is implemented in the `Drain` module.\\n- We introduce a new method `drain()` that is implemented in the `Drain` module.\\n- We introduce a new method `drain()` that is implemented in the `Drain` module.\\n- We introduce a new method `drain()` that is implemented in the `Drain` module.\\n- We introduce a new method `drain()` that is implemented in the `Drain` module.\\n- We introduce a new method `drain()` that is implemented in the `Drain` module.\\n- We introduce a new method `drain()` that is implemented in the `Drain` module.\\n- We introduce"}
{"Context":"## Context\\nAfter sampling from the posterior distribution users may want the option\\nof validating their results against experimental data or to predict\\ncellular behaviour using a trained kinetic model.\\n","Decision":"## Decision\nPosterior predictive samples will be implemented using the posterior\\ndraws for kinetic and thermodynamic parameters. The boundary conditions\\nfrom the predicted experiments will be sampled from their marginal distributions.\\nThe number of samples will be limited to the number of samples from the posterior\\ndistribution.\\nIn order to define what is part of the training set and prediction set a new\\nfile will be introduced called experiment_metadata. A toml file where the training\\nprediction split will be defined under the headers [\"training\"], and [\"prediction\"]\\nrespectively.\\nA new stan file will be made where there is no model block (minimal if required).\\nIn the generated_quantities (gqs), we will iterate through the draws from a previously\\ngenerated set of csvs. the drains, conc_enzymes, and conc_unbalanced_metabolites\\nwill be sampled from their priors using the functions normal_rng() and lognormal_rng()\\nfor their respective generative distributions.\\nlog_probabilities should be calculated if measurements are also included for the\\nprediction experiments.\\n","Prediction":"## Decision\nWe will implement a function to allow users to validate their results against\\nexperimental data or to predict cell behaviour using a trained kinetic model.\\nThis function will be implemented as a separate API and will be accessible via\\na REST API. This REST API will be implemented using the `Postman` API toolkit.\\nThis REST API will be implemented using the `Postman` API toolkit.\\nThis function will provide a consistent way of validating results.\\nThis function will be written as a separate API and will be accessible via\\na REST API. This REST API will be implemented using the `Postman` API toolkit.\\nThis REST API will be implemented using the `Postman` API toolkit.\\nThis function will provide a consistent way of validating results.\\nThis function will be written as a separate API and will be accessible via\\na REST API. This REST API will be implemented using the `Postman` API toolkit.\\nThis function will provide a consistent way of validating results.\\nThis function will be written as a separate API and will be accessible via\\na REST API. This REST API will be implemented using the `Postman` API toolkit.\\nThis function will provide a consistent way of validating results.\\nThis function will be written"}
{"Context":"## Context\\nThe HTML Diff Service is born out of need to increase performance of HTML\\ndiffing by not requiring a command line execution in Contently's web\\napplication. Additionally, we wanted to remove the Java dependency from our\\nRails application container to simplify and reduce the threat surface.\\n","Decision":"## Decision\nSpring Boot is used to allow for the service to be standalone with support for\\nembedded servers. The decision allows portability to deploy the service without\\na J2EE server.\\n","Prediction":"## Decision\nWe will use the [Jenkins](https:\/\/jenkins.io\/)\\npipeline to run the HTML Diff job. This will provide a consistent and\\nrobust build and deployment process for the HTML Diff service.\\nAdditionally, the Jenkins build will allow us to automate the deployment\\nprocess to Contently's instance of the HTML Diff service.\\nThis decision will allow us to quickly and reliably deploy the HTML Diff\\nservice to Contently's environment. This will also allow us to have the\\nability to quickly and reliably test the HTML Diff service without\\nhaving to deploy the entire Contently application to Contently's\\nenvironment.\\n### Deployment\\nWe will use the [Jenkins](https:\/\/jenkins.io\/) pipeline to run the HTML\\ndiffing job. This will allow us to run the HTML Diff job in Contently's\\ndeployed application container. Additionally, Jenkins will allow us to\\nautomate the deployment of the HTML Diff service to Contently's\\ninstance of the HTML Diff service.\\n### Deployment log\\nWe will create a Jenkins pipeline to run the HTML Difference job. This\\nwill be run as part of the Contently application deployment process.\\nThe pipeline will be set up to run the HTML Difference job"}
{"Context":"## Context\\nThere are a few different options for application logging.\\n1. Logging to file(s) used to be the norm. Logging to a file has several issues, however. You must be on the host to view the log file, or setup a system to ship the log file off the host. You must rotate log files or your host will eventually run out of disk space.\\n2. Logging to stdout and letting a different system handle the logging is a more modern approach to logging. In our case cloud.gov takes care of gathering logs sent to stdout\/stderr. Logs can be viewed with a cloud.gov cli tool.\\n","Decision":"## Decision\nWe will log to stdout\/stderr. On development machines logs will be presented as human readable strings. In deployed environments (dev, staging and prod) logs will be formatted in JSON.\\n","Prediction":"## Decision\nUse cloud.gov to log to stdout\/stderr.\\nUse cloud.gov to collect logs from all other systems.\\nUse logrotate to rotate log files.\\nUse logrotate to send logs to cloud.gov.\\nUse logrotate to send logs to the local host.\\nUse logrotate to send logs to a file on the local host.\\nUse logrotate to send logs to a log file on the local host.\\nUse logrotate to send logs to a log file on the local host.\\nUse logrotate to send logs to a log file on the local host.\\nUse logrotate to send logs to a log file on the local host.\\nUse logrotate to send logs to a log file on the local host.\\nUse logrotate to send logs to a log file on the local host.\\nUse logrotate to send logs to a log file on the local host.\\nUse logrotate to send logs to a log file on the local host.\\nUse logrotate to send logs to a log file on the local host.\\nUse logrotate to send logs to a log file on the local host.\\nUse logrotate to send logs to a log file on the local host.\\nUse logrotate to send logs to"}
{"Context":"## Context\\nThe application will support drawing, as its predecessor. The question is, can we achieve an approach that doesn't rely on (or isn't intertwined with) the mapping framework.\\nThe goal is also to pave the way to enable users to draw with some snapping help (on roads, or other geometries)\\n### Potential paths prospected\\n- No framework, implement a minimalist drawing library with only Javascript\\n- Use a library, such as PaperJS, to handle the drawing. Translate the output into geographic geometries.\\n- Use OpenLayers as a drawing tool (as in the viewer `mf-geoadmin3`)\\n","Decision":"## Decision\nAfter looking into all paths above, decision has been made to go with the OpenLayers approach. Here's reasons why :\\nPure Javascript approach could fit neatly in the current technology stack, but will require a lot of investment to achieve any kind of snapping\\nPaperJS has a nice toolbox for drawing, but will also require some work in order to support snapping. So the balance benefice (good drawing tools) vs. work needed isn't positive.\\nThis leaves us with the OpenLayers approach, that has already proven itself on the viewer `mf-geoadmin3`. We know that snapping is a possibility, as there's some snapping (only on the current drawing though).\\n","Prediction":"## Decision\nWe have decided to implement a minimal drawing library with only Javascript, using PaperJS as the library.\\nThe library will be used in the `mf-geoadmin3` viewer to handle the drawing.\\nThe library will be used in the `mf-geoadmin3` viewer to handle the drawing.\\nThe library will be used in the `mf-geoadmin3` viewer to handle the drawing.\\nThe library will be used in the `mf-geoadmin3` viewer to handle the drawing.\\nThe library will be used in the `mf-geoadmin3` viewer to handle the drawing.\\nThe library will be used in the `mf-geoadmin3` viewer to handle the drawing.\\nThe library will be used in the `mf-geoadmin3` viewer to handle the drawing.\\nThe library will be used in the `mf-geoadmin3` viewer to handle the drawing.\\nThe library will be used in the `mf-geoadmin3` viewer to handle the drawing.\\nThe library will be used in the `mf-geoadmin3` viewer to handle the drawing.\\nThe library will be used in the `mf-geoadmin3` viewer to handle the drawing.\\nThe library"}
{"Context":"## Context\\nThe Registraties project will need to contain a page that will show data per address (or verblijfsobject) from existing APIs, like [Basisregistraties adressen en gebouwen (BAG)](https:\/\/api.data.amsterdam.nl\/bag\/), [Basisregistratie kadaster (BRK)](https:\/\/api.data.amsterdam.nl\/brk\/) and [Handelsregister (HR)](https:\/\/api.data.amsterdam.nl\/handelsregister). All required data is readily available.\\n","Decision":"## Decision\nThe project's MVP, which will contain a simple search field with auto-suggest functionality, get its data from existing API endpoints and therefore doesn't need its own back-end setup with API. In the future it might, but for the initial phase, the front-end code is capable of retrieving, formatting and showing the data from [api.data.amsterdam.nl](https:\/\/api.data.amsterdam.nl).\\nThe application will have container components that have sagas injected into them. Each saga is responsible for retrieving data from a single endpoint. If the situation calls for it, a saga can retrieve data from more than one endpoint.\\n","Prediction":"## Decision\nThe API endpoint will be used to get the data.\\n### Example endpoint\\nThe endpoint will be `api.data.amsterdam.nl\/basisregistraties\/1\/address`\\n### Example request body\\nThe request body will be the `id` of the address to get the data for.\\n### Example responses\\nThe response will have the `id` of the address to get the data for.\\n### Example error response\\nThe response will have the `id` of the address to get the data for.\\n### Example caching\\nThe requests will be cached for a while (10 minutes by default)\\n### Example caching methods\\nThe request body will be cached for a while (10 minutes by default)\\n### Example caching methods\\nThe request body will be cached for a while (10 minutes by default)\\n### Example caching methods\\nThe request body will be cached for a while (10 minutes by default)\\n### Example caching methods\\nThe request body will be cached for a while (10 minutes by default)\\n### Example caching methods\\nThe request body will be cached for a while (10 minutes by default)\\n### Example caching methods\\nThe request body will be cached for a while (10 minutes by default)\\n### Example caching"}
{"Context":"## Context and Problem Statement\\nAs of today, we already decoupled the `renderer` (decentraland\/unity-renderer) from the `explorer` repository. And we are in the process of decoupling `website` (decentraland\/explorer-website). The ECS and building tools are also in the path of being migrated to their own repositories (decentraland\/js-sdk-toolchain).\\nThe context of the problem is around the tactical\/execution aspect. Since the changes affect directly the publishing cycles and processes of every package. We must optimize not blocking development and also not breaking things in a way that prevent us from releasing hotfixes.\\n","Decision":"## Decision\nWe choose to create a separated repository to not block the releases and development of explorer during this migration stage.\\nAlso having separated repositories, will foster external contributions, ensuring clear scopes and responsibilities for each repository.\\n","Prediction":"## Decision\nWe will use the `explorer` repository as a repository for the decoupled packages. We will use the `website` repository for the other decoupled packages. We will also create a new `explorer` repository for the ECS and building tools that will be decoupled from the `website` repository. This way, we will have a new `explorer` repository that will be created only when a new version is needed. The `explorer` repository will be used for the decoupled packages, and the `website` repository will be used for the other packages.\\nIn the `explorer` repository, we will also create a new `explorer` directory for the ECS and building tools that will be decoupled from the `website` repository. This directory will be used for the ECS and building tools that will be decoupled from the `website` repository.\\nIn the `website` repository, we will create a new `website` directory for the decoupled packages. This directory will be used for the decoupled packages. The directory will be used for the decoupled packages. It will be a symlink to the `explorer` directory.\\nIn the `explorer` repository,"}
{"Context":"## Context\\nWe used Babel to parse code into AST, transform this AST and re-generate code.\\nWe went for Babel because it's popular, very active and is capable of parsing a lot of different syntaxes. Notably, it parses JSX, TS and TSX out of the box. That's great!\\nWhat is not great is the code generation part. Babel formats the generated code. That means the code contained inside a transformed node gets reformated. This is not cool.\\n","Decision":"## Decision\nAs we want to preserve the original style of the transformed code as much as possible, we went for [Recast][recast].\\nAs the library says:\\n> The magic of Recast is that it reprints only those parts of the syntax tree that you modify.\\nThus, we now use Recast to parse and generate the code. AST transformation is still performed by Babel. Recast uses Babel to parse the code into AST, so we keep Babel benefits such as parsing JSX, TS\u00a0and TSX out of the box.\\n","Prediction":"## Decision\nWe will use ESLint to format the generated code. ESLint is a popular ESLint plugin for JavaScript and it's already part of the TypeScript ecosystem. ESLint is also very popular and has a very active community.\\nESLint is also the default lint tool for TypeScript.\\nESLint will be used to format the generated code.\\nESLint will be used to validate generated code.\\nESLint will be used to generate TypeScript code that is used by the TypeScript compiler.\\nESLint will be used to generate code that is used by the TypeScript compiler.\\nESLint will be used to generate code that is used by the TypeScript compiler.\\nESLint will be used to generate code that is used by the TypeScript compiler.\\nESLint will be used to generate code that is used by the TypeScript compiler.\\nESLint will be used to generate code that is used by the TypeScript compiler.\\nESLint will be used to generate code that is used by the TypeScript compiler.\\nESLint will be used to generate code that is used by the TypeScript compiler.\\nESLint will be used to generate code that is used by the TypeScript compiler.\\nESLint will be used to generate code that is used by the TypeScript compiler.\\nESLint"}
{"Context":"## Problem\\nMost of the credentials\/information of the environment (like a service address) are only available as soon the application is deployed.\\n","Decision":"## Decision\n* Chosen Alternative: creating scripts\\n","Prediction":"## Decision\nThe credentials will be stored in a file called `credentials.yaml` and will be available to the application in the `envs` directory. \\nThe `credentials.yaml` file will be located in the `service` directory.\\nThe `envs` directory will be located in the `src` directory.\\nThe `credentials.yaml` file will be stored in the `envs` directory.\\nThis will allow the application to be deployed without having to worry about credentials being in the `envs` directory.\\nThe `credentials.yaml` file will be stored in the `src` directory.\\nThis will allow the application to be deployed without having to worry about credentials being in the `src` directory.\\nWhen a new service is deployed, the credentials will be copied to the `service` directory. This will allow the application to be deployed without having to worry about credentials being in the `service` directory.\\nWhen a new service is deployed, the credentials will be written in the `credentials.yaml` file. This will allow the application to be deployed without having to worry about credentials being in the `credentials.yaml` file.\\nWhen a new service is deployed, the credentials will be checked against the `credentials.yaml` file. This"}
{"Context":"## Context\\nThere are significant differences between AMP and regular article rendering.\\nThese include:\\n* The markup is different\\n* The HTML structure\/limitations are different\\n* AMP does not require (or support) client-side custom Javascript, which means\\nwebpack is likely a poor fit\\n* From a project perspective, it seems that the two project change at different\\nrates (historically at least)\\n* The caching rules are different\\n* Ads are very different\\n..and more.\\nCoupling things that are different (in terms of code, building, deployment, or\\notherwise) can lead to brittle and complex solutions; it is generally better to\\nkeep things separate and share only what is absolutely required.\\nFrom this context, the question arises: should we attempt to build AMP in the\\nsame (node) package as articles, or separately?\\n","Decision":"## Decision\n*For our initial milestone* we will build AMP as simply a new page in the\\nexisting frontend package. The advantage of this approach is to bootstrap\\nquickly while leaving room to separate things later if necessary.\\nWithin this, we will adhere to a couple of rules:\\n* no `if (amp)` - branching is not permitted other than the absolute top-level\\nThere are many disadvantages to this kind of branching and it also would make it\\ndifficult to separate the projects later on.\\n* no divergence of data model\\nInstead, we will document pressures here and use any to bring forward\\ndiscussions around moving things into a separate project. So note, the\\nmotivation here is not that branching of data model is bad at all, but that it\\nrequires thought and possibly more significant changes.\\n","Prediction":"## Decision\nWe will attempt to build AMP in the same (node) package as articles.\\nThis is because we believe that building AMP in the same package as\\narticles will make it easier to test and maintain, and we don't believe\\nthat the benefits of building AMP will be worth the potential drawbacks.\\nWe will apply this decision to the `content` and `render` components.\\nWe will use the [AMP-CLI](https:\/\/github.com\/ampjs\/amp-cli) to build AMP.\\nWe will use [AMP-CLI docs](https:\/\/github.com\/ampjs\/amp-cli-docs) to build AMP.\\nWe will use [AMP-CLI docs](https:\/\/github.com\/ampjs\/amp-cli-docs) to build AMP.\\nWe will use [AMP-CLI docs](https:\/\/github.com\/ampjs\/amp-cli-docs) to build AMP.\\nWe will use [AMP-CLI docs](https:\/\/github.com\/ampjs\/amp-cli-docs) to build AMP.\\nWe will use [AMP-CLI docs](https:\/\/github.com\/ampjs\/amp-cli-docs) to build AMP.\\nWe will use [AMP-CLI docs](https:\/\/github.com\/amp"}
{"Context":"## Context and Problem Statement\\nCurrently, JabRef uses some localized preferences. We want to remove the Localization-dependency from the JabRefPreferences and move the Localization to where the String is used.\\nThe problem is how to store the default values.\\n","Decision":"## Decision\nChosen option: \"_Store the unlocalized String._ Consumers then check the String they got as a preference against the defaults. If it matches, localize it. Otherwise, use it.\", because Achieves goals without requiring too much refactoring and without (known) downsides.\\n","Prediction":"## Decision\nChosen option: \"Store the default values in the preferences file\", because it preserves the \"classical\" JabRef approach:\\n* Localizations are stored in a local file\\n* The file is parsed once at startup and stored in the preferences location\\n* The preferences file is read once at startup and the default values are set\\n* If the user changes the default values, the preferences file is parsed and the defaults are set again\\n* The preferences file is read once at startup and the default values are set\\n* If the file is changed, the preferences file is parsed and the defaults are set again\\n* The preferences file is read once at startup and the default values are set\\n* If the preferences file is changed, the preferences file is parsed and the defaults are set again\\n* A user can change the default values.\\n* A user can change the default values.\\n* A user can change the default values.\\n* A user can change the default values.\\n* A user can change the default values.\\n* A user can change the default values.\\n* A user can change the default values.\\n* A user can change the default values.\\n* A user can change the default values.\\n* A user"}
{"Context":"## Context\\nJest tests are fast because they can be run in parallel. If we use the same database for every test, it can cause race conditions as multiple operations are performed on models and collections. There are two ways to decouple tests:\\n- [define databases in tests][defined-test-database]\\n- [randomly create databases for each test][random-test-database]\\n","Decision":"## Decision\nIn the context of database tests, and facing the concern of race conditions then create random databases for each test. There are different approaches for setting up the [test Mongodb server][test-mongodb-server] with Jest. I prefer using the `beforeAll` and `afterAll` hooks because this is more flexible and is less coupled to Jest's idiosyncracies.\\n","Prediction":"## Decision\nWe will use a random database for each test. This will allow us to verify that each test passes and that there are no race conditions between tests.\\nWe will not use a database for every model or collection. This will allow us to run tests that depend on a database and reduce the risk of race conditions.\\nWe will use an [OpenId Connect provider](https:\/\/openid.net\/specs\/openid-connect-authentication\/3.0.html) to generate random credentials. This will allow us to verify that each test passes and that there are no race conditions between tests.\\nWe will use a [randomly generate a JWT](https:\/\/jwt.io\/) to verify that each test passes and that there are no race conditions between tests.\\nWe will use a [randomly generate a JWT](https:\/\/jwt.io\/) to verify that each test passes and that there are no race conditions between tests.\\nWe will use a [randomly generate a JWT](https:\/\/jwt.io\/) to verify that each test passes and that there are no race conditions between tests.\\nWe will use a [randomly generate a JWT](https:\/\/jwt.io\/) to verify that each test passes and that there are no race conditions between tests.\\nWe will use a"}
{"Context":"## Context\\nUsing Proxy will not as fast as native code. so we use CodeGen to generate a transparent layer on top of user's class to provide interceptor function.\\n","Decision":"## Decision\nUse CodeGen\\n","Prediction":"## Decision\nWe will use Proxy to generate transparent layer on top of user's class.\\nWe will use Proxy to provide interceptors function.\\nWe will use Proxy to provide dynamic proxy creation.\\nWe will use Proxy to provide dynamic proxy creation for the following:\\n- Class to Class calls\\n- Class to Method calls\\n- Class to Field access\\n- Class to Method calls with and without parameters\\n- Class to Field access with and without parameters\\n- Class to Method with and without parameters\\n- Class to Field with and without parameters with and without parameters\\n- Class to Method with and without parameters with and without parameters with and without parameters with and without parameters with and without parameters with and without parameters with and without parameters with and without parameters with and without parameters\\n- Class to Class calls with and without parameters\\n- Class to Method calls with and without parameters\\n- Class to Field calls with and without parameters\\n- Class to Method calls with and without parameters with and without parameters with and without parameters with and without parameters\\n- Class to Field calls with and without parameters with and without parameters\\n- Class to Method with and without parameters with and without parameters with and without parameters\\n- Class to Field calls with and without parameters with and without parameters\\n- Class to"}
{"Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"## Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","Prediction":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\nWe will use [design by contract](http:\/\/www.pragprog.com\/book\/bookitem?id=160675) to enforce the use of those records.\\nWe will use [design review](https:\/\/en.wikipedia.org\/wiki\/Design_review#Design_review_methods) to help facilitate the review of architecture decisions.\\nWe will use [design by contract](https:\/\/en.wikipedia.org\/wiki\/Design_by_contract) to enforce the use of those records.\\nWe will use [design review](https:\/\/en.wikipedia.org\/wiki\/Design_review#Design_review_methods) to help facilitate the review of architecture decisions.\\nWe will use [design by contract](https:\/\/en.wikipedia.org\/wiki\/Design_by_contract) to enforce the use of those records.\\nWe will use [design review](https:\/\/en.wikipedia.org\/wiki\/Design_review#Design_review_methods) to help facilitate the review of architecture decisions.\\nWe will use [design review](https:\/\/en.wikipedia.org\/wiki\/Design_review#Design_review_methods) to help facilitate the review of architecture decisions.\\nWe will"}
{"Context":"## Context\\nRedux is a state container for JavaScript applications. It helps to manage state across an application.\\n[This article](https:\/\/hackernoon.com\/the-react-state-museum-a278c726315) provides a nice non-comprehensive listing of various alternative state management libraries.\\n","Decision":"## Decision\nWe will use Redux in `link_platform` for state management.\\n","Prediction":"## Decision\nWe will use Redux for state management.\\nWe will use the [redux-form](https:\/\/redux-form.com\/) library for form management.\\nWe will use [redux-thunk](https:\/\/redux-thunk.com\/) for middleware.\\nWe will use [redux-saga](https:\/\/redux-saga.github.io\/) for asynchronous testing.\\nWe will use [redux-saga-testing](https:\/\/redux-saga-testing.com\/) for testing Redux based applications.\\nWe will use [redux-monitoring](https:\/\/redux-monitoring.com\/) for logging and monitoring Redux based applications.\\nWe will use [redux-persist](https:\/\/redux-persist.js.org\/) for storing state.\\nWe will use [redux-saga-logger](https:\/\/redux-saga.org\/docs\/en\/stable\/using-react-state-machine\/) for logging and monitoring Redux based applications.\\nWe will use [redux-logger](https:\/\/redux-saga.org\/docs\/en\/stable\/using-react-state-machine\/) for logging and monitoring Redux based applications.\\nWe will use [redux-logger-middleware](https:\/\/redux-saga.org\/docs\/en\/stable\/using-react-state-machine\/middleware\/) for logging and monitoring Redux based applications.\\nWe will use [redux-store](https:\/\/redux"}
{"Context":"## Context\\nAPI web requests, file system access, database requests, message queue dispatches, I\/O interrupts, interprocess communication and similar interactions are part of any portal.\\nThe usage of these resources should be known to a certain degree for monitoring of usage compared against a usage limitation, order of calls across portal nodes and their time of action.\\n","Decision":"## Decision\nThe information have to be connected to a portal node and its structural resource subdivision.\\nThere is no context given as it is just about when and what kind of categories are affected.\\nFor example a processing involving the transport of a file does not contain the complete file but can be tagged with e.g. the mime type or encoding, \u2026.\\nThere need to be common decorators for common implementations to simplify automatic recording like PSR HTTP client.\\nAdding common decorators in a global registry simplifies the usage of portal node developer that can depend on common decorator implementations.\\nThe telemetry entry only represents a single direction in a synchronous interaction and should represent two directions as two entries in an asynchronous interaction.\\nIncoming web requests and cronjob runs are automatically recorded.\\nTelemetry has to be optional by core configuration as the storage is impacted heavily and it can affect the performance.\\nStoring telemetry data is not allowed to break the portal node flow by e.g. throwing an exception.\\nRecording describes an incrementable reference object that stores the timestamps of each increment and is taggable.\\nThe exact way for analysis of the recorded data is up to discussion and postponed to a different point in time.\\n","Prediction":"## Decision\nUse the [request monitoring](https:\/\/github.com\/zenoss\/request-monitoring) package to monitor API requests, file system access, database requests, message queue dispatches, I\/O interrupts, interprocess communication and similar interactions.\\nThe monitoring of these interactions should be done in the context of a given logic, which is a `Module` in Zenoss.\\nThe `Module` should be a `Handler` that returns the requested information to a `Monitor`.\\nA `Monitor` is an `Event` that is emitted after a given action has been performed.\\nThe `Module` should be a `Handler` that is registered to the `ModuleManager`.\\nThe `ModuleManager` should be a `Container` that is used to bundle all of the `Module` instances.\\nThe `Container` should be responsible for registering handlers to the `ModuleManager`.\\nThe `ModuleManager` should be a `Cluster` that is responsible for managing the registration of handlers to the `ModuleManager`.\\nA `Cluster` is a `Node` that is responsible for managing the registration and execution of handlers within a given number of `Nodes`.\\nA `Node` is a `Node` that is a `NodeManager` that"}
{"Context":"## Context\\nA service that's onboarding at the moment depends on MySQL, memcached and\\nelasticsearch that we don't yet offer, but are likely to in future, and has\\nchosen to host them on AWS using RDS MySQL, Elasticache memcached and hosted\\nElasticache.\\nWe wanted to ensure that only the app instances belonging to that service could\\nconnect to those services in order to avoid other services being able to access or modify their data.\\n- MySQL requires authentication and allows TLS\\n- Elasticache memcached does not provide authentication or TLS\\n- AWS Hosted elasticsearch [appears to provide the ability to sign\\nelasticsearch requests using an AWS access\\nkey](https:\/\/aws.amazon.com\/blogs\/security\/how-to-control-access-to-your-amazon-elasticsearch-service-domain\/).\\nRequiring a custom elasticsearch client.\\nThe options for authentication are:\\nVia Internet, using network access controls\\n-------------------------------------------\\nThis is described for\\n[elasticache](http:\/\/docs.aws.amazon.com\/AmazonElastiCache\/latest\/UserGuide\/Access.Outside.html)\\nHow we would set it up:\\n- Tenant binds services to public IP addresses, restricts access to PaaS egress IPs.\\n- PaaS team changes default [application security\\ngroup](https:\/\/docs.cloudfoundry.org\/adminguide\/app-sec-groups.html) set to\\nforbid access to those IP addresses.\\n- PaaS team creates new application security group allowing access to those IP addresses.\\n- PaaS team binds application security group to space(s) belonging to tenants\\nRisks\/costs:\\n- Access via Internet IP is via NAT instance, introducing a likely single point\\nof failure and additional running costs to tenants.\\n- Traffic is in plaintext over the Internet for some services.\\n- PaaS team need to do work every time a tenant adds\/changes service IP addresses.\\n- If they forget to request this, other PaaS tenants have access to their\\nservice IPs.\\n- PaaS team need to do work (assigning application security groups) in every\\noccasion a space needs to gain or lose\\naccess to the list of service IPs.\\nUsing private address space (VPC peering)\\n-----------------------------------------\\nHow we would set it up:\\n- Tenant raises support request to begin setup process\\n- PaaS team responds with a unique IP allocation eg. 172.16.0.0\/24 for tenant to use\\n- Tenant creates VPC using that address space\\n- Tenant creates AWS security group(s) restricting access from PaaS VPC to expected services\\n- Tenant provides PaaS team with their AWS account id and the VPC id.\\n- PaaS team sends VPC peering request\\n- PaaS team creates new application security group allowing access to the VPC IP allocation\\n- PaaS team binds application security group to space(s) belonging to tenants\\n- Tenant accepts VPC peering request\\nRisks\/costs:\\n- Introduces a new network security boundary between VPCs; a risk of\\naccidentally introducing security group rules that allow more access from the\\npeered VPC than intended.\\n- PaaS team need to do work (assigning application security groups) on every\\noccasion a space needs to gain or lose\\naccess to the peered VPC.\\nAlthough this specific example uses VPC peering because the tenant in question\\nuses AWS, we could use the same principle (us assigning IP address space and\\nchanging application security groups) to a VPN or some other network overlay\\ntechnology to allow us to connect to things other than VPCs.\\n","Decision":"## Decision\nWe will offer VPC peering to tenants in specific cases where it is appropriate.\\n","Prediction":"## Decision\nWe will create a custom elasticsearch client to allow us to connect to hosted elasticsearch\\nusing the AWS access key as the authentication mechanism.\\nWe will use a single `elasticsearch` service to provide all of the services we want to connect to.\\nWe will use an `elasticsearch` security group to allow us to connect to hosted elasticsearch\\nusing the AWS access key as the authentication mechanism.\\nWe will use the `elasticsearch` VPC peering to allow us to connect to hosted elasticsearch using the VPC IP\\nallocation as the authentication mechanism.\\nWe will use the `elasticsearch` VPC peering to allow us to connect to hosted elasticsearch using the VPC IP\\nallocation and the VPC security group as the authentication mechanism.\\nWe will use the `elasticsearch` VPC peering to allow us to connect to hosted elasticsearch using the VPC IP\\nallocation and the VPC security group as the authentication mechanism.\\nWe will use the `elasticsearch` VPC peering to allow us to connect to hosted elasticsearch using the VPC IP\\nallocation and the VPC security group as the authentication mechanism.\\nWe will create a custom elasticsearch client to allow us to connect to hosted el"}
{"Context":"## Context\\nSometimes we make a change to the catalog config set that will break search results if it's deployed without a new index already in place. In these cases the config set in use cannot be updated with this change and we have to deploy a different config set for the reindexing collection, which is then swapped in once it's fully populated.\\n","Decision":"## Decision\nFor most config changes we will continue updating and reloading the config set in production.\\nIn the cases described above we will copy a new config set from the most recent production config set, incrementing the version suffix. Once the index is created and swapped in, the previously-used config set should be deleted.\\n","Prediction":"## Decision\nWe will use a `config-set` to hold the new config set and then a `config-set` to hold the old config set. We will ensure that the `config-set` names are unique.\\nWe will use the `config-set` names to be the same as the `config` name, so that we can use the same config set name for both. We will use the `config` name to be the same as the `config` name of the `config-set` that will be replaced.\\nWe will ensure that the `config` name can be updated to a new config set name without breaking the search result. We will ensure that the `config-set` name can be updated without breaking the search result.\\nWe will use the `config` and `config-set` names to be the same as the `config` and `config-set` names of the `config` and `config-set`.\\nWe will use the `config` name to be the same as the `config` name of the `config`.\\nWe will use the `config-set` name to be the same as the `config` name of the `config-set`.\\nWe will use the `config` name to be the same"}
{"Context":"## Context\\nThere were a [number of issues with \"NTA\"](https:\/\/github.com\/alphagov\/govuk-frontend\/issues\/1012), the previous version of the font:\\n1. The file size for all versions of the font was larger than it needed to be.\\n2. A separate font was required for tabular numbers.\\n3. The glyphs sat too high in the fonts bounding box (baseline) which led to the need for adjustments in CSS.\\n4. Hinting requirement for oldIE (EOT) increased the file size even further.\\n","Decision":"## Decision\n- Replace the \"NTA\" font with a new version, called \"GDS Transport\", which fixes the above issues.\\n- Specifically, stop serving Transport font to IE8 due to a [large file required for hinting](https:\/\/designnotes.blog.gov.uk\/2019\/07\/29\/weve-updated-the-gov-uk-colours-and-font\/). The font now [falls back to Arial](https:\/\/github.com\/alphagov\/govuk-frontend\/blob\/master\/CHANGELOG.md#gds-transport-now-falls-back-to-arial-in-internet-explorer-8-ie8) for IE8.\\n","Prediction":"## Decision\nThe decision was made to remove the [NTA](https:\/\/github.com\/alphagov\/govuk-frontend\/issues\/1012) font, and replace it with the [default font](https:\/\/fonts.google.com\/specimen\/Roboto).\\nThis decision was made due to the following:\\n1. The [NTA](https:\/\/github.com\/alphagov\/govuk-frontend\/issues\/1012) font is not in the [default font](https:\/\/fonts.google.com\/specimen\/Roboto) repository.\\n2. The [NTA](https:\/\/github.com\/alphagov\/govuk-frontend\/issues\/1012) font is the only one that meets the [hinting requirement](https:\/\/github.com\/alphagov\/govuk-frontend\/issues\/1012#issuecomment-339548309) for oldIE (EOT).\\n3. The [NTA](https:\/\/github.com\/alphagov\/govuk-frontend\/issues\/1012) font is the only one that meets the [baseline](https:\/\/fonts.google.com\/specimen\/Roboto) requirement for oldIE (EOT).\\n4. The [NTA](https:\/\/github.com\/alphagov\/govuk-"}
{"Context":"## Context and Problem Statement\\nTo facilitate testing, a clean, object oriented architecture as well as the plugin systems for various components a configurable Inversion of Control (IOC) container is required.\\nThis container is responsible for plugin discovery, as well as injecting services required by the API to serve it's \"external\" customers.\\n## Decision Drivers\\n* Support for a plugin system that can discover additional components not originally compiled into the deployed WAR\\n* Support for minimal configuration, allowing easy modification and discovery by convention\\n","Decision":"## Decision\n* Support for a plugin system that can discover additional components not originally compiled into the deployed WAR\\n* Support for minimal configuration, allowing easy modification and discovery by convention\\nThe chosen IoC container is Spring, because it supports plugin discovery at minimal configuration and has easy support for servlet-based injection with `spring-mvc` and `spring-web`.\\n### Negative consequences\\nThe Plugins loaded cannot be adjusted at runtime.\\nAt time of writing, no such capability is required or planned.\\n","Prediction":"## Decision\n* Support for a plugin system that can discover additional components not originally compiled into the deployed WAR\\n* Support for minimal configuration, allowing easy modification and discovery by convention\\nChosen option: `Spring IoCContainer`, because it provides a simple and clear solution to the problem, without the need of additional plugins. It also provides a very simple and clear API to manage the plugin system. Additionally, it's easy to understand and to use, and offers the possibility to configure it to meet the needs of the plugin developer.\\n### Positive Consequences\\n* Plugin discovery can be done in a simple manner, without requiring specific knowledge of IoC which is required for other plugins\\n* Plugin discovery is very easy, as it does not require additional plugins to be installed on the server\\n* Plugin configuration can be done in a simple manner, without additional plugins being installed on the server\\n* Plugin configuration can be done without a web interface\\n* Plugin configuration can be done in a very simple manner, without additional plugins being installed on the server\\n* Plugin configuration can be done without a web interface\\n### Negative Consequences\\n* Plugin discovery is not currently possible, which is a limitation that will be addressed in the future\\n* Plugin configuration is not possible, which is"}
{"Context":"## Context\\nFor the project a database structure need to be chosen and adopted.\\n","Decision":"## Decision\nThree table database will be used at this point:\\n![database structure](.\/assets\/db_structure.png)\\n```\\nTable book {\\nisbn varchar(50) [pk]\\ntitle varchar(300)\\nyear char(4)\\ncategory varchar(300)\\nsub_category varchar(300)\\n}\\nTable author {\\nid bigserial [pk]\\nname varchar(300)\\n}\\nTable book_author {\\nbook_isbn varchar(50) [ref: > book.isbn]\\nauthor_id bigint [ref: > author.id]\\n}\\n```\\n","Prediction":"## Decision\nWe will use PostgreSQL, the most popular and well known open-source database.\\nWe will use the [PostgreSQL 13](https:\/\/www.postgresql.org\/13\/) as the default version.\\nWe will use the [PostgreSQL 13.1](https:\/\/www.postgresql.org\/13.1\/) as a development version.\\nWe will use the [PostgreSQL 13.3](https:\/\/www.postgresql.org\/13.3\/) as a production version.\\nWe will use the [PostgreSQL 13.10](https:\/\/www.postgresql.org\/13.10\/) as an older version of PostgreSQL.\\nWe will use the [PostgreSQL 12.13](https:\/\/www.postgresql.org\/12.13\/) as the minimum PostgreSQL version.\\nWe will use the [PostgreSQL 11.19](https:\/\/www.postgresql.org\/11.19\/) as the minimum PostgreSQL version for the application version.\\nWe will use the [PostgreSQL 11.18](https:\/\/www.postgresql.org\/11.18\/) as the minimum PostgreSQL version for the database.\\nWe will use the [PostgreSQL 11.18.1](https:\/\/www.postgresql.org\/11.18.1"}
{"Context":"## Context\\nLike so many other accessibility tools, such as the [Accessibility Developer Tools by Google](https:\/\/github.com\/GoogleChrome\/accessibility-developer-tools), [aXe by Deque](https:\/\/github.com\/dequelabs\/axe-core), and [HTML_CodeSniffer by Squiz](https:\/\/github.com\/squizlabs\/HTML_CodeSniffer) to name a few, our proprietary accessibility conformance testing engine at Siteimprove runs within the context of a browser. The reason why this seems to be the de facto way of implementing an accessibility tool is obvious: The browser is the tool used to consume your website, so why not test directly within that very tool? Through the APIs exposed by the browser, we get access to all the information needed in order to assess the accessibility of a website; the structure we can access and inspect through the DOM, information about styling can be gained through the CSSOM, and soon we also get our hands on a standardised accessibility tree through the [AOM](https:\/\/wicg.github.io\/aom\/).\\nHowever, not all is good in the land of browsers. Rendering a website is an inherently non-deterministic process and the timing of network requests, script execution, the content of request headers, and much more, all play a role in what the final result will look like. In most cases, this will directly affect the assessment of a tool that runs within the browser and will become very apparent at scale. At Siteimprove, we feel the effect of this on a daily basis; a customer asking us why we came up with a certain result and us having little to no clue because we cannot replicate the exact circumstances that led to that result. This is a frustrating experience for both our customers and ourselves as it makes it difficult to reason about our tool.\\nWe want to fix this and we want to fix it for good. To do so, we must ensure that we have the ability to exactly replicate the results of a given accessibility assessment. Ideally, as many unknown browser variables as possible should be taken out of the equation and the browser only be used for what is absolutely necessary.\\n","Decision":"## Decision\nWe will abandon any sort of dynamic analysis within the context of a browser. The input to Alfa will be static data and any assessment must be made based on that data alone. A browser may or may not be involved in the construction of the data, but the browser will not be required for any further assessment thereof.\\nIf additional data is needed by a given accessibility rule, we will adjust the data format to meet the needs of the rule. We will also carefully consider the extent of the data format as to not bloat it with information that could otherwise be inferred from existing data. Ideally, the size of the data when serialised and stored on disk will not be much larger than the size of the original source code on which the data is based.\\n","Prediction":"## Decision\nOur accessibility conformance testing engine will run within the context of a browser and will not attempt to render a website. This will allow us to run accessibility tests in a way that is as close to the real-world as possible. We will continue to run tests within the browser and we will not attempt to render the results of these tests to the browser.\\n### Why this is good for accessibility testing\\nThe reasons for this decision are twofold:\\n* Having a browser be the tool used to assess accessibility is a proven and widely accepted best practice.\\n* The accessibility tree is a standardised and well-understood concept that is widely used to describe the accessibility of a website.\\n### Why this is bad for accessibility testing\\nThe reasons for this decision are twofold:\\n* The accessibility tree is not a standardised and well-understood concept. The accessibility tree is a relatively new concept and we are not yet familiar with it. It is not a standardised and widely accepted best practice, and it is not a standardised and well-understood concept.\\n* Having a browser be the tool used to assess accessibility is a proven and widely accepted best practice. As we have already stated, browsers are the tool that is used to consume a website"}
{"Context":"## :dart: Context\\nAzure offers a number of ways to host your application code. The following are the considerations for choosing a compute option for the serving tier:\\n* Should support exposing Web APIs\\n* Should be able to connect to backend Cosmos DB\\n* Should support .net core framework\\n* APIs hosted on this service will need to be secured\\n* Would prefer a managed service\\n* Does not have portability requirements\\n* Should support CD from Github\\nThe following options for compute are considered for this service:\\n* App Services\\n* Functions\\n* Container Instances\\n* Service Fabric\\n* AKS\\nChoosing the right compute model will help optimize the development experience and operations\\n","Decision":"## Decision\nThe recommended approach is to use Azure App Services considering the following points:\\n* Supports CD from Github\\n* Supports development slots for updates to reduce downtime\\n* Can be integrated with App Gateway and VNET for higher security\\n* Natively supports AD integration\\n* Supports .net core framework and is well integrated with Visual Studio development experience\\n* Can leverage cosmos SDK (nuget) to connect back to the data store\\n* Is a fully managed PaaS service\\n* No overheads related to cluster management or infrastructure provisioning.\\nThe decision is based on the guidance provided by Microsoft here: https:\/\/docs.microsoft.com\/en-us\/azure\/architecture\/guide\/technology-choices\/compute-decision-tree\\n","Prediction":"## Decision\nWe will use App Services for the serving tier. App Services provides a simple, manageability, and cost model that is well suited for the current workload. Azure provides a number of features that can be leveraged to support the development of the application code.\\n* App Service provides a simple, manageability, and cost model that is well suited for the current workload\\n* Azure provides a number of features that can be leveraged to support the development of the application code\\n* Provides a security model that allows us to create a separate service account to deploy our app\\n* Provides a cost model that can be easily managed\\n* Supports deployment of multiple versions of the app to a single service\\n* Allows us to manage the deployment of multiple instances of the app\\n* Allows us to deploy the app to a separate region\\n* Allows us to use the service for an isolated environment\\n* Allows us to split the application into multiple instances\\n* Allows us to scale the app as needed\\n* Allows us to add additional storage in the cloud\\n* Allows us to add additional security in the cloud\\n* Allows us to use the service for an isolated environment\\n* Allows us to split the application into multiple instances\\n* Allows us to scale the app as needed\\n"}
{"Context":"## Context\\nMany ASP.NET applications use [Entity Framework (EF)](https:\/\/docs.microsoft.com\/en-us\/ef\/), an Object Relational Mapper (ORM) that helps access data stored in database.\\nEF allows data in a database to be accessed by extending a DbContext class and adding properties to this extending class of type DbSet. DbContext and DbSet provide methods for performing basic CRUD operations against entities in a database that are defined in model classes. These model classes contain annotations that define the table name, columns and relationships with other entities. When a query is performed, EF handles creating instances of model classes and filling them with the received data.\\nSome properties are lazily loaded, with the queries related to fetching the required data only being run when thoses properties are accessed. This approach is commonly used when accessing a property representing a relationship with another entity.\\nA DbContext by default tracks changes to entities returned as the result of queries, with changes being saved when a call is made to a DbContext's SaveChanges or SaveChangesAsync methods.\\nThe DbContext and DbSet classes provide methods that can be used to fetch data, with the ability to apply limitations on what data is returned. EF will generate the required query, execute it, parse the response data and return the appropriate entity model instances.\\nEF supports migrations written as classes with Up and Down methods, to support upgrading and rolling back, respectively. These methods are implemented by adding calls to a provided MigrationBuilder instance.\\nDapper is a library that is commonly referred to as a \"micro-ORM\". It provides methods to support executing SQL queries and parsing the results to create instances of particular model classes. Unlike EF, Dapper does not support the tracking of changes and queries must be written using SQL.\\nDapper was developed for the StackOverflow website to address performance issues, as outlined in [this blog post](https:\/\/samsaffron.com\/archive\/2011\/03\/30\/How+I+learned+to+stop+worrying+and+write+my+own+ORM).\\n","Decision":"## Decision\nWe will use Dapper with the [repository pattern](http:\/\/blog.mantziaris.eu\/blog\/2016\/10\/24\/the-repository-and-unit-of-work-pattern\/) to access data stored in the database.\\n","Prediction":"## Decision\nWe will use Dapper for accessing Entity Framework properties from ASP.NET Core applications.\\nThis will allow the application to continue supporting ASP.NET Core 3.1 applications, while providing the performance benefits of using Dapper.\\nWe will use the [Dapper NuGet package](https:\/\/www.nuget.org\/packages\/Dapper\/) to provide a migration mechanism that will automatically generate the appropriate SQL queries to execute when the Entity Framework properties are accessed.\\nWe will use the [Dapper documentation](https:\/\/docs.dapper.io\/en\/latest\/) to provide an introduction to Dapper, as well as links to the various resources available.\\nWe will also use the [Dapper documentation](https:\/\/docs.dapper.io\/en\/latest\/) to provide an introduction to Dapper, as well as links to the various resources available.\\nWe will also use the [Dapper documentation](https:\/\/docs.dapper.io\/en\/latest\/) to provide an introduction to Dapper, as well as links to the various resources available.\\nWe will also use the [Dapper documentation](https:\/\/docs.dapper.io\/en\/latest\/) to provide an introduction to Dapper, as well as links to the various resources available.\\nWe will also use the [Dapper documentation](https"}
{"Context":"## Context\\n[context]: #context\\nSince the the beginning of the year, https:\/\/pioneersofchange.org\/landkarte\/ is using the event fuction at Kvm. In Minsk Hackers developed the entry form for new events. As soon as this is online and in use by many, the map will be useless if you can not filter these events.\\nEvents do not only need to be searchable, but the results must also be filtered by bounding box and sorted by start\/end dates.\\n","Decision":"## Decision\n[decision]: #decision\\nWe will improve the backend-API so that events are searchable till the end of the year.\\nAt least for hashtags it has to work!\\n","Prediction":"## Decision\nWe will use Kvm for filtering and sorting events. This will allow to filter events by the name, the location and the other parameters. This will allow to create a list of events that fits your needs.\\n### Filters and sorting\\nEvents are grouped in categories and the filter will be applied to all the events in that category. The category will be used to group the events.\\n### Sorting\\nThe order of the events will be decided by the category and the start and end date of the event. The category will be used to sort the events in ascending order.\\n### Filtering by bounding box\\nThe filter will be applied to all the events in the selected category. The bounding box will be used to filter the events. The bounding box will be used to create a list of events that fits your needs.\\n### Filtering by start date and end date\\nThe filter will be applied to all the events in the selected category. The start and end date will be used to filter the events. The start date will be used to filter the events in the selected category and the end date will be used to filter the events in the category. The start date and end date will be used in ascending order.\\n### Filtering by start date\\nThe filter will"}
{"Context":"## Context\\nThe [Next.js docs on `getServerSideProps`](https:\/\/nextjs.org\/docs\/basic-features\/data-fetching#getserversideprops-server-side-rendering) specify that objects returned from this function must be JSON serializable\u2014this means only primitives, no complex objects or class instances.\\nOur gateways return domain models (instances of classes)\u2014these cannot be directly serialized to JSON to be passed to the client side components.\\nWe have a number of options:\\n1. provide methds on the domain classes to serialize and deserialize instances, then manually use these in our `getServerSideProps` functions and component definitions\\n2. return JSON response objects from our gateways and map them to domain objects in the components\\n3. use one of a number of third party libraries to do the serialisation for us\\n","Decision":"## Decision\nOption 1 is reasonable, but would mean writing and maintaining more code that necessarily required.\\nOption 2 goes against our [principles](https:\/\/github.com\/madetech\/clean-architecture) and would litter our presentation components with boundary logic.\\nTherefore we have decided to use option 3: [`superjson`](https:\/\/github.com\/blitz-js\/superjson) was built for this explicit purpose and has a [Babel plugin](https:\/\/github.com\/blitz-js\/babel-plugin-superjson-next) which handles the serialisation and deserialisation for us implicitly, meaning our functions are clean from this logic.\\n","Prediction":"## Decision\nWe will return JSON response objects from our gateways and map them to domain objects in the components.\\nThis avoids the need to manually use our gateways to convert objects returned from the server side to domain objects in the client side.\\nThese objects are then passed to the component's `render` method.\\nThis allows us to pass domain objects as props to our components, without having to explicitly pass them to the server side.\\nThis also allows us to pass domain objects as props to our components, without having to explicitly pass them to the server side.\\nWe will store the domain objects in the client's state.\\nThis has the advantage that we can use the client's state to render the components without having to pass domain objects as props to the client side.\\nThis has the disadvantage of requiring the client to have a state managed by our server, which may not be the most suitable solution for some use cases.\\nWe will use the [nextjs\/gateway](https:\/\/nextjs.org\/docs\/guides\/gateway) library to return JSON response objects from our gateways.\\nThis allows us to pass domain objects as props to our components, without having to explicitly pass them to the server side.\\nThis also allows us to pass"}
{"Context":"## Context\\nI (Wolfram) am writing most of the katas and slowly I see a pattern arising in how I write them and also of which depth\\nI want them to be. Lately some things made me think about in-depth versions of katas.\\nOne was writing the ES10 `Object.fromEntries()` kata. Writing this kata took me more than a week, every evening\\nfiguring out certain details of the spec and how to transport those into a kata. This made me realize that\\nthis kata has become too complex to get just a feeling for how `Object.fromEntries()` works, and it also made\\nme realize that I want this depth because I don't want to write any kata that states wrong things.\\nBut this kata has simply become too big and too detailed.\\nDiscussing this at #jscc19 and also at the #jslang meetup, I realized that people would appreciate a simple\\nintroductory kata, which allows one to learn the feature (superficically) and have an in-depth or expert kata,\\nwhich covers all specifics that might be described in the spec.\\n","Decision":"## Decision\nKatas might come in two flavours, the simple kata and the in-depth kata.\\nIn case of the above described `Object.fromEntries()` kata these two katas will be:\\n- stored in `es10\/language\/object-api\/fromEntries.js` and `es10\/language\/object-api\/fromEntries-in-depth.js`\\n- they are two separate katas, with separate metadata, etc.\\nThe simple kata serves the purpose of understanding and being able to use the functionality\/API.\\nThe in-depth kata covers things like how does this functionality behave with edge cases, type coercion, using\\nthe function directly from it's prototype and alike things.\\n","Prediction":"## Decision\nI will write a kata that is simple but covers the ES10 `Object.fromEntries()` feature.\\nIt will be a kata that is in-depth but not too deep. It will be a kata that gives a feeling for the feature.\\nAnd it will be written in a way that is easy to read and also simple to understand.\\nFor example, it will be written in a way that uses the `Object.fromEntries` method, but it will be written in\\na way that makes it easy to understand and follow the code.\\n### Use of `Object.fromEntries` method\\n`Object.fromEntries` is a method in the `Object` class that is used to extract a single entry from an array or object\\nof objects. If the object or array is already in an array of entries, it will return the same object. If it is not, it will\\nreturn a new one. For example:\\n```js\\nconst entries = [1, 2, 3, 4, 5, 6];\\nObject.fromEntries(entries, entry => entry[0]);\\n```\\n### Example\\nThis kata will be written in a way that it uses the `Object.fromEntries` method, but"}
{"Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"## Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","Prediction":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\nWe will use [Zero Architecture](http:\/\/zeroarchitecture.org) to keep the architecture in sync with the code.\\nWe will use [Architecture Code Complete](http:\/\/www.amazon.com\/Architecture-Code-Complete-Practical-Professional\/dp\/0596005198) to help us keep the architecture in sync with the code.\\nWe will use [Architecture Code Secrets](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions-architecture-code-secrets) to help us keep the architecture in sync with the code.\\nWe will use [Architecture Studio](https:\/\/github.com\/Thinkrelevance\/ArchitectureStudio) to help us keep the architecture in sync with the code.\\nWe will use [SOLID](http:\/\/www.solidproject.org) to help us keep the architecture in sync with the code.\\nWe will use [Design Patterns](http:\/\/www.amazon.com\/Design-Patterns-Practical-Common-Solutions\/dp\/0201719642) to help us keep the"}
{"Context":"## Context\\nThere are two primary patterns of use when managing multiple environments (staging, prod, etc) in Terraform. The first is to use multiple directories--one for each environment. This has the advantage of being explicit, with an associated cost of repeated TF configuration. The second alternative uses TF workspaces to switch between environments. This option appears to be [recommended](https:\/\/www.terraform.io\/docs\/enterprise\/guides\/recommended-practices\/part1.html#one-workspace-per-environment-per-terraform-configuration) by Terraform. The latter which we will use here allows a DRY approach across the environment.\\n","Decision":"## Decision\nUse workspaces to manage multiple environments.\\n","Prediction":"## Decision\nWe will use the [TF workspaces](https:\/\/www.terraform.io\/docs\/enterprise\/guides\/recommended-practices\/part1.html#one-workspace-per-environment-per-terraform-configuration) approach to manage multiple environments.\\nWe will use the following structure for each environment:\\n- [tf-workspaces\/terraform.tfstate](tf-workspaces\/terraform.tfstate)\\n- [tf-workspaces\/terraform.tfstate.json](tf-workspaces\/terraform.tfstate.json)\\n- [tf-workspaces\/terraform.tfstate.yml](tf-workspaces\/terraform.tfstate.yml)\\n- [tf-workspaces\/terraform.tfstate.yaml](tf-workspaces\/terraform.tfstate.yaml)\\n- [tf-workspaces\/terraform.tfstate.cloned](tf-workspaces\/terraform.tfstate.cloned)\\n- [tf-workspaces\/terraform.tfstate.git](tf-workspaces\/terraform.tfstate.git)\\n- [tf-workspaces\/terraform.tfstate.gitlab](tf-workspaces\/terraform.tfstate.gitlab)\\n- [tf-workspaces\/terraform.tfstate.gitlab.scm](tf-workspaces\/terraform.tfstate.gitlab.scm)\\n- [tf-workspaces\/terraform.tfstate.gitlab.sh](tf-workspaces\/terraform"}
{"Context":"## Context\\nThis is planned to be a multi-tenant system. There will be multiple CASA orgs in the system, so every case, case_contact, volunteer, supervisor, casa_admin etc must have a casa_org_id, because no one is allowed to belong to multiple CASAs. Volunteer, supervisor, and casa_admin are all roles for a \"User\" db object. In addition to those existing roles, we want to create a new kind of user: all_casa_admin. We need to handle the case of super users who have access to multiple casa_orgs, so they would be difficult to handle in the existing User table--with null handling around their casa_org_id field. We have used the built-in Devise ability to have multiple user tables, as recommended to us by our Rails expert Betsy. This is to prevent needing null handling around casa_id for User records since all_casa_admin users will not have casa_id populated.\\nAdditionally, all_casa_admin users are currently intended to be allowed to create casa_admin users, but NOT to be able to see or edit any CASA data like volunteer assignments, cases, case_updates etc.\\n","Decision":"## Decision\nWe are using two tables for users: \"user\" table for volunteers,supervisors, and casa_admin (all of which must have a casa_id). \"all_casa_admin\" for all_casa_admins, which will have no casa_id.\\n","Prediction":"## Decision\nWe will create a new User model that will hold all CASA users and their CASA orgs. We will add a new table to the User model to hold all CASA orgs in the system. We will use the built-in Devise authentication in the new user model. We will have CASA users have a casa_id field in the User model which will be unique to that CASA user. We will add a new field to the User model called casa_org_id which will be the CASA org id, which will be unique to each CASA user. This will be the only CASA org id that will be persisted in the database. We will also create a new CASA user model that will hold all CASA users and their CASA orgs.\\n### CASA User Model\\nWe will use the existing Rails User model, which already has a CASA_id field. We will use the new CASA User model as the \"User\" model. We will have the CASA_id field be the primary key of the CASA User model, and not the CASA org id. We will have the CASA_org_id field be the foreign key to the CASA org id in the CASA User model. We will set"}
{"Context":"## Context\\nNumber of document types which does not use Elasticsearch's fulltext\\ncapabilities is growing. Recently released Elasticsearch 6 is bringing one type\\nper index which means management of many indices.\\n","Decision":"## Decision\nWe will add relational database as primary database. Elasticsearch will be used\\nfor denormalized reports and related data intended for fulltext search.\\n","Prediction":"## Decision\nWe will not use Elasticsearch fulltext indexing in our project. We will use Lucene\\nwhich is more flexible and easier to use for our use case.\\nWe will consider Elasticsearch fulltext indexing in the future, but only when the\\nneed arises.\\nWe will use Lucene instead of Elasticsearch fulltext indexing when:\\n* Lucene is faster for querying\\n* Lucene is more flexible for indexing\\n* Lucene has better search engine performance\\n* Lucene is more flexible for searching\\n* Lucene has better search engine performance\\n* Lucene is more flexible for searching\\n* Lucene has better search engine performance\\n* Lucene is more flexible for searching\\n* Lucene has better search engine performance\\n* Lucene is more flexible for searching\\n* Lucene has better search engine performance\\n* Lucene is more flexible for searching\\n* Lucene has better search engine performance\\n* Lucene is more flexible for searching\\n* Lucene has better search engine performance\\n* Lucene is more flexible for searching\\n* Lucene has better search engine performance\\n* Lucene is more flexible for searching\\n* Lucene has better search engine performance\\n* Lucene has better search engine performance\\n* Lucene has better search engine performance"}
{"Context":"### Problem to solve\\nProvide unified way to manage translations for translatable entities in API.\\n### Possible solutions\\nThe translations from collection could be embedded as objects within a `ProductOption`\/`ProductOptionValue` resource\\n**or** provided by [IRIs](https:\/\/en.wikipedia.org\/wiki\/Internationalized_Resource_Identifier)\\n### Decision and reasoning\\nTranslations should always be embedded as the collection of objects. They're irrelevant outside of the main object and\\ndo not provide any value alone.\\n","Decision":"## Decision\nTranslations should always be embedded as the collection of objects. They're irrelevant outside of the main object and\\ndo not provide any value alone.\\n","Prediction":"## Decision\nTranslations should always be embedded as the collection of objects. They're irrelevant outside of the main object and\\ndo not provide any value alone.\\n### Positive Consequences\\n* Encapsulation of translations and their values\\n* Translation is not stored in the main object\\n* Translation is not visible outside of the main object\\n* Translation is not directly accessible by the application\\n* No need for translation-specific logic in the API\\n* No need to use a temporary key to store translations as part of an entity\\n* Translation is not required to be stored in the database\\n* Translation is not required to be part of a resource path\\n* Translation is not required to be part of a resource name\\n* Translation is not required to be part of a resource type\\n* Translation is not required to be part of a resource value\\n## Decision\nTranslations should always be embedded as the collection of objects. They're irrelevant outside of the main object and\\ndo not provide any value alone.\\n### Positive Consequences\\n* Encapsulation of translations and their values\\n* Translation is not stored in the main object\\n* Translation is not visible outside of the main object\\n* Translation is not directly accessible by the application\\n* No need for translation-specific logic in the API"}
{"Context":"## Context\\nWe have to decide for a programming language \/ application framework.\\n","Decision":"## Decision\nWe stick to [Spring Boot](https:\/\/spring.io\/projects\/spring-boot), with [Java 11](https:\/\/docs.aws.amazon.com\/corretto\/latest\/corretto-11-ug\/downloads-list.html) and [Gradle](https:\/\/gradle.org\/).\\n","Prediction":"## Decision\nWe will use Python as a programming language for the project.\\nPython is a widely used, high-quality, and actively maintained programming language.\\nIt is also the main language of the official Python library, and is used by a large proportion of\\nthe project's community.\\nPython has a large ecosystem of supported libraries, and is used by a large number of\\nopen source projects and independent software vendors.\\nPython is also the primary language for the official Python library and is used by a\\nlarge proportion of the project's community.\\nPython is also the official language of the official Python library and is used by a\\nlarge number of open source projects and independent software vendors.\\nPython is also the primary language of the official Python library and is used by a\\nlarge number of open source projects and independent software vendors.\\nPython is also the official language of the official Python library and is used by a\\nlarge number of open source projects and independent software vendors.\\nPython is also the primary language of the official Python library and is used by a\\nlarge number of open source projects and independent software vendors.\\nPython is also the primary language of the official Python library and is used by a\\nlarge number of open source projects and independent software vendors.\\"}
{"Context":"## Context and Problem Statement\\nAs recorded in [INTERLOK-2875](https:\/\/adaptris.atlassian.net\/browse\/INTERLOK-2975) when Interlok is attempting to reconnect to an endpoint it becomes very difficult to amend configuration through the UI.\\nThe reconnect loop is dictated by the reconnection settings with each configured connection.  When a connection is initialized if it cannot connect to it's endpoint the reconnect loop begins.  Connections are initialized on startup and during runtime should it lose it's connection, Interlok will shift the connection through the lifecycle phases until it is initialised again.\\nTherefore we have two triggers for this issue;\\n- A connection cannot be established on startup\\n- A connection breaks during runtime.\\nUntil a connection has either established it's connection or has reached the maximum number of configured reconnect attempts, the connection will remain in the initialization phase.\\nEach phase of an Interlok component is dictated by the StateManagedComponent interface that defines the 4 basic stages of __init__, __start__, __stop__ and __close__.  When we __request__ a change of a components state (from __init__ to __start__ for example) we typically synchronize the state change until it is complete.\\nThis means for example when Interlok launches the initialization phase kicks in which will attempt to initialize each connection.  Any connection that cannot be established as mentioned above will stay in the synchronized __init__ phase potentially forever.  Because the phase transition is synchronized we cannot therefore change this state until it's current transition has completed.\\nThere are currently two problems applying configuration from the UI while a connection is attempting reconnection.\\nWhen applying config the UI will request a restart of the running instance, which will attempt to run through the lifecycle of phases mentioned above.\\nBut of course if a connection is stuck in it's synchronized initialization phase trying to reconnect all other state change requests must wait therefore the process hangs.\\nThe second problem when applying config is simply that the UI process will wait until the Interlok process has completed it's full startup, which of course it will not be able to do if a connection cannot be established.  Essentially the process hangs for a period of time.\\n","Decision":"## Decision\n### First issue\\nChosen option: Not yet decided.\\n#### Do Nothing\\nIn this case we accept that most people are not modifying production ready instances and even if they are they are probably not using the UI, therefore they are not going to run into the issues raised here.\\n#### Remove the synchronization\\nThere are a couple of ways to do this.\\nThe first would be to literally remove the synch blocks from implementations like this one;\\n```java\\npublic final void init() throws CoreException {\\nsynchronized (lock) {\\nif (!prepared) {\\nprepare();\\n}\\ninitConnection();\\nLifecycleHelper.init(connectionErrorHandler());\\n}\\n}\\n```\\nThe second way to do it would be to wrap the actual work inside the synch block with a new thread.\\n##### Consequences\\nAllowing a process to request a state change while in the middle of a previous state change could cause undefined behaviour especially around edge cases.\\n#### Interrupt the blocking transition\\nEssentially we would stop the transition currently running, in this case the reconnect inside the initialization phase transition.\\nThis can be done a couple of ways;\\nFirst we could make sure the thread handling the initialization transition is a named and managed thread from our ThreadManagerFactory and then add a new static method to that factory that would send an interrupt to that named thread.\\nOr considering the reconnect code could be refactored into the parent connection impl, we could cover all connections that have reconnect ability, which is all of those that extend AllowsRetriesConnection.  Once we make sure the reconnect code only exists in this parent class we then have a new volatile variable that gets set when a __close__ request is made the reconnect loop could check the value and exit if necessary.\\n### Second issue\\nChosen option: Not yet decided.\\n#### Do Nothing\\nIn this case we deem the current hanging behaviour as expected and correct.\\n#### Non-blocking applying of config\\nIn this case a change would be made to the UI application of config where it would request the full restart but not necessarily wait for the running instance to complete it's full start-up.  There would then either be some kind of notification once Interlok has completed the start-up or users would just be directed to the dashboard.\\n","Prediction":"## Decision\nWe will introduce a new state management system that will allow the UI to request a restart of the running instance as well as be able to check the status of the running instance and the state of the connection.\\nWe will introduce a new `StateManager` class that will encapsulate the lifecycle of the state management system.  The `StateManager` class will be responsible for managing the lifecycle of the state management system.\\nWe will introduce a new `StatefulSet` class that will be used to store the stateful state of each state management component.  This allows the UI to request a restart of the running instance and the state of the connection.\\nWe will introduce a new `StatefulState` class that will be used to store the state of the stateful state of each state management component.\\nThis class will be responsible for managing the state of the stateful state and should be used only for storing the state of the stateful state.\\nWe will introduce a new `StatefulStateManager` class that will be responsible for managing the lifecycle of the stateful state management system.\\nThis class will be responsible for managing the lifecycle of the state management system.\\nWe will introduce a new `StatefulStateManager` that will be"}
{"Context":"## Context\\n- I wasn't getting anything built with all of the libvirt shenanigans\\n- I have a NAS now, so things I'm worried about running on Kubernetes can go there\\n","Decision":"## Decision\nInstall Kubernetes straight onto a Debian install on `swan`. Use MetalLB or similar later to manage the routing.\\n","Prediction":"## Decision\n- Use libvirt to manage the guest OS\\n- Use Kubernetes to manage the containers\\n- Use the new network API to create a bridge between the two\\n- Use the new API to create a load balancer\\n- Use the new API to create a load balancer for the bridge\\n- Use the new API to create a load balancer for the load balancer\\n- Use the new API to create a load balancer for the bridge\\n- Use the new API to create a load balancer for the load balancer\\n- Use the new API to create a load balancer for the load balancer\\n- Use the new API to create a load balancer for the load balancer\\n- Use the new API to create a load balancer for the load balancer\\n- Use the new API to create a load balancer for the load balancer\\n- Use the new API to create a load balancer for the load balancer\\n- Use the new API to create a load balancer for the load balancer\\n- Use the new API to create a load balancer for the load balancer\\n- Use the new API to create a load balancer for the load balancer\\n- Use the new API to create a load"}
{"Context":"## Context and Problem Statement\\nChanges of a release have to be communicated. How and which stile to use?\\n","Decision":"## Decision\nChosen option: \"Keep-a-changelog format with freedom in the bullet points\", because\\n* [Keep-a-changelog](https:\/\/keepachangelog.com\/) structures the changelog\\n* Each entry can be structured to be understandable\\n* Forcing to prefix each line with `We fixed`, `We changed`, ... seems to be read strange.\\nWe nevertheless try to follow that style.\\nFurther discussion can be found at [\\#2277](https:\/\/github.com\/JabRef\/jabref\/issues\/2277).\\n","Prediction":"## Decision\nChosen option: \"Confluence\", because it's the only tool that has a good integration with Jira and it is the most used tool by the developers.\\nChosen because it has the best feature set, and it is free. And since we are using Confluence, we don't need to use Jira.\\nChosen because it has a good feature set and it is free.\\nChosen because it has a good feature set and it is free.\\nChosen because it has a good feature set and it is free.\\nChosen because it has a good feature set and it is free.\\nChosen because it has a good feature set and it is free.\\nChosen because it has a good feature set and it is free.\\nChosen because it has a good feature set and it is free.\\nChosen because it has a good feature set and it is free.\\nChosen because it has a good feature set and it is free.\\nChosen because it has a good feature set and it is free.\\nChosen because it has a good feature set and it is free.\\nChosen because it has a good feature set and it is free.\\nChosen because it has a"}
{"Context":"## Context\\nGitHub Actions are the new shiny and have been in Beta for sometime. They are\\ngoing to be generally available on\\n[2019-11-13](https:\/\/github.blog\/2019-08-08-github-actions-now-supports-ci-cd\/).\\nGitHub Actions will have a long term future. It is likely GitHub Actions\\nwill become the default CI mechanism (and possibly more) for projects hosted on\\nGitHub. Using them in this repo, which has a basic use case will provide some\\nexposure to the service.\\n","Decision":"## Decision\nThe decision is to replace Travis CI with GitHub Actions.\\n","Prediction":"## Decision\nWe will use GitHub Actions for CI\/CD.\\nWe will use GitHub Actions for continuous integration.\\nWe will use GitHub Actions for continuous delivery.\\nWe will use GitHub Actions as part of the continuous deployment process.\\nWe will use GitHub Actions for the build pipeline.\\nWe will use GitHub Actions for the release process.\\nWe will use GitHub Actions for the deployment process.\\nWe will use GitHub Actions for the code review process.\\nWe will use GitHub Actions for the pull request process.\\nWe will use GitHub Actions for the issue tracker.\\nWe will use GitHub Actions for the issue queue.\\nWe will use GitHub Actions for the issue board.\\nWe will use GitHub Actions for the release management process.\\nWe will use GitHub Actions for the release management system.\\nWe will use GitHub Actions for the release workflow.\\nWe will use GitHub Actions for the release pipeline.\\nWe will use GitHub Actions for the release management process.\\nWe will use GitHub Actions for the release management system.\\nWe will use GitHub Actions for the release workflow.\\nWe will use GitHub Actions for the release pipeline.\\nWe will use GitHub Actions for the release management process.\\nWe will use GitHub Actions for the release management system.\\nWe will"}
{"Context":"## Context\\nThere are currently two different training schemes in scope for the track-and-pay project. There are rumours of a possible third in the future.\\nThere are some similarities and many differences in the inputs, outputs and maths for payments for these training schemes. For example:\\n1. Both have fixed payments at 40%\\n2. Both allow pulling some of the fixed payment into earlier \"setup\" payments (for cashflow).\\n3. The output payments are very different in detail.\\n4. They have different banding systems.\\n### People\\nTim Abell and Pavel Lisovin (track and pay developers) discussed the issue amongst ourselves and came to this as a decision for the time being.\\n","Decision":"## Decision\n1. Build two payment engines that do not share code.\\n2. Have similar input\/output interfaces (ruby hash structures) that can later be easily converted to JSON.\\n3. Use similar patterns for both engines of Gherkin BDD driven unit tests plus normal rspec unit tests.\\n","Prediction":"## Decision\nThe decision is to have the two schemes run in parallel, with the track and pay scheme being the main scheme and the other being a \"stand alone\" scheme. This is to allow the track and pay scheme to be developed in parallel with the other schemes.\\nThe track and pay scheme will be the one that pulls the fixed payments into the \"setup\" payments. This will be done by using the \"push\" method, where we have a fixed amount of funds available in the \"push\" account and can add more to the \"push\" account as it is needed.\\nThe \"stand alone\" scheme will be the one that pulls the \"output\" payments. This will be done by using the \"pull\" method, where the \"pull\" account is credited with the amount of money it is holding. We can then use this account to pull the \"output\" payments from. This will be the only \"pull\" account that is not linked to the \"push\" account.\\nThe \"push\" and \"pull\" accounts will be linked in the \"accounting\" view, so we can see the total amount of money that is available in the \"push\" account. We will then be able to work out the total amount of money that"}
{"Context":"## Context\\nUsing http protocol and APIs, authentication and authorisation is mostly done by passing some token with every request. Fixed secrets are easy to understand and configure, but somewhat weak in protection. Instead, oauth2 is a stronger mechanism to create tokens, though more complicated. In oauth2 authentication and authorisation are decoupled and the security critical authentication is delegated to the authorization server. Still, oauth2 allows distributed token validation next to validating the token with the server.\\n","Decision":"## Decision\nWe will use oauth2 for authentication and authorization.\\n","Prediction":"## Decision\nWe will use oauth2 to create and manage tokens and secrets.\\nWe will use the [jwt.io](https:\/\/jwt.io\/) library for generating and validating tokens.\\nWe will use the [google-oauth2-client-library-java](https:\/\/github.com\/google\/oauth2-client-library-java) library for creating and managing secrets.\\nWe will use the [google-oauth2-server](https:\/\/github.com\/google\/oauth2-server) library to host the Google APIs.\\nWe will use the [google-oauth2-server-side](https:\/\/github.com\/google\/oauth2-server-side) library to create and manage the Google APIs.\\nWe will use the [google-oauth2-server-side-side](https:\/\/github.com\/google\/oauth2-server-side-side-side) library to create and manage the secrets.\\nWe will use the [google-oauth2-server-side-side-side-side-side](https:\/\/github.com\/google\/oauth2-server-side-side-side-side-side-side-side-side-side-side-side-side-side-side-side-side-side-side-side-side-side-side-side-side-side-side-side-side-side-side-side-side-side-side-side-side-side-side-side-side-side-side-side-side-side-side-side-side-side-side-side-side-side-side-side-side-side-side-side-side-side-side-side"}
{"Context":"## Context\\nIt is necessary for the service to confirm various actions with users via email messages.\\n","Decision":"## Decision\nGov.UK Notify is the recommended solution, using the `notifications-ruby-client` library.\\n","Prediction":"## Decision\nThe service will use a Redis queue to store emails and will use a Redis subscription to confirm actions.\\nRedis will be used as a queue because Redis is a distributed key-value store which is the recommended database for the service.\\nRedis will also allow for the creation of a persistent queue which can be used for future work.\\nRedis will also allow for the creation of a persistent queue which can be used for future work.\\nRedis will also allow for the creation of a persistent queue which can be used for future work.\\nRedis will also allow for the creation of a persistent queue which can be used for future work.\\nRedis will also allow for the creation of a persistent queue which can be used for future work.\\nRedis will also allow for the creation of a persistent queue which can be used for future work.\\nRedis will also allow for the creation of a persistent queue which can be used for future work.\\nRedis will also allow for the creation of a persistent queue which can be used for future work.\\nRedis will also allow for the creation of a persistent queue which can be used for future work.\\nRedis will also allow for the creation of a persistent queue which can be used for future work.\\nRedis will also allow for the creation"}
{"Context":"## Context\\nIn computing, an **idempotent operation** is one that has no additional effect if it is called more than once with the same input parameters.\\nA vast majority of critical flows of a web services we're running is required to provide this guarantee for their clients.\\nThus, it makes sense to extract this functionality into a library with support for common use-cases.\\nThere a two common approaches in the industry when it comes to implementing support for idempotency:\\n1) distributed **in-memory** data stores (i.e. Redis)\\n2) **database** - master node to avoid any issues caused by possible replication lag\\n### Client\\nTypically, the only additional requirement for the clients to enable idempotency is to add a unique identifier for the request\\nthat is only expected to have a side effect once. Sometimes APIs make this a mandatory property of the request.\\nFrom a security considerations and to lower a chance of collisions it's recommended to use UUIDv4 as a unique request identifier.\\n### Service\\nCritical requirements for the processing service:\\n1) **persisted result** for a successful and failed with non-retriable error request\\n2) **non-blocking** - any concurrent request with a same identifier should be rejected with a transient error\\nResult persistence aims to avoid any heavy computations on a re-try or duplicate requests if original request completed with a **success** or **non-retriable** error.\\nClients should have a freedom of choice on what exactly is being persisted and which format to use.\\nQuick failure in case of the aggressive retry policy on the client side ensures that no server resource is blocked and wasted for such concurrent requests with a same identifier.\\n3) request processing procedure and persistence of its result has to be transactional\\nWe require request execution logic and idempotence status update to happen in a same transaction.\\nThat requirement comes at a cost of the client flexibility. Allowing clients to control transactional context\\nintroduces edge-cases such as a failure to persist idempotency status whereas action has been completed and side effect taken place.\\n","Decision":"## Decision\nProvide code idempotence service based on a database approach, specific database integration is pluggable.\\nBoth **in-memory** and **database** approaches have their own pros and cons, we're not going to go in depth comparision here\\nmostly due to the lack of adoption of Redis in our current infrastructure. That said we should definitely consider providing such support in the future.\\nWe require request execution logic and idempotence status update to happen in a **same transaction**.\\nThat requirement comes at a cost of the client flexibility, but allowing clients to have full control over transaction context\\nintroduces edge-cases such as a **failure** to update imdepotency action status after **side effect** already took place.\\nThat would require clients to verify if side effect has taken place on each execution - which we consider suboptimal for most common cases.\\nIf that approach proves to be limiting for certain flows we can extend core service interface to allow clients to choose different execution strategies and have better control over transactional context.\\n","Prediction":"## Decision\nWe will support idempotency in all our services, including the ones that are not critical to their operation.\\nThis will allow clients to take actions based on the idempotency status and avoid unnecessary side effects.\\nWe will use the **transactional** pattern for idempotency handling in all our services.\\nTransactional idempotency is a critical requirement for our services and we will enforce it on all our services.\\nAs a general recommendation, we will not recommend any other pattern for idempotency handling in our services.\\nFor example, we will not recommend or support any **non-transactional** pattern for idempotency handling.\\nWe will use **idempotency** as a general term for the **idempotent** operation and the **transactional** pattern for idempotency handling.\\nFor example, we will not recommend or support any **non-transactional** pattern for idempotency handling in our services.\\nFor the moment, we will recommend only **transactional** idempotency pattern for idempotency handling and we will enforce it on all our services.\\nIn the future, we will consider the **non-transactional** idemp"}
{"Context":"## Context\\nFor the project a front end technology need to be chosen and adopted coupled with a unit test framework.\\n","Decision":"## Decision\n[React](https:\/\/reactjs.org\/) as a front end development library, and [Jest](https:\/\/jestjs.io\/) as a unit testing framework.\\n","Prediction":"## Decision\nWe will use React.\\nWe will use Jest as a testing framework.\\nWe will use Cypress as a front end testing framework.\\nWe will use Cypress as a testing framework.\\nWe will use Jest as a testing framework.\\nWe will use Cypress as a testing framework.\\nWe will use Cypress as a testing framework.\\nWe will use Cypress as a testing framework.\\nWe will use Cypress as a testing framework.\\nWe will use Cypress as a testing framework.\\nWe will use Cypress as a testing framework.\\nWe will use Cypress as a testing framework.\\nWe will use Cypress as a testing framework.\\nWe will use Cypress as a testing framework.\\nWe will use Cypress as a testing framework.\\nWe will use Cypress as a testing framework.\\nWe will use Cypress as a testing framework.\\nWe will use Cypress as a testing framework.\\nWe will use Cypress as a testing framework.\\nWe will use Cypress as a testing framework.\\nWe will use Cypress as a testing framework.\\nWe will use Cypress as a testing framework.\\nWe will use Cypress as a testing framework.\\nWe will use Cypress as a testing framework.\\nWe will use Cypress as a testing framework.\\nWe will use Cypress as"}
{"Context":"## Context\\nThis set of Docker images are opinionated and meant to run within a Docker orchestrator, kubernetes for instance.\\nSince most (if not all) the orchestrators have resource management built-in there are certain PHP settings which can be tweaked to make use of them, if PHP has memory limits itself, it'll die as a fatal error, in which case the orchestrator would be unaware that it's actually a out of memory situation.\\n","Decision":"## Decision\nSet php ini configuration to have `memory_limit = -1`, this will affect both fpm and cli processes since it's added in the `default.ini` file of this repository.\\n","Prediction":"## Decision\nTo make use of the kubernetes resource manager we will need to modify the Dockerfile to include the following lines:\\n```\\nRUN php -r \"memory_limit = 512M;\" \/usr\/bin\/php-cgi -f \/var\/www\/html\/fake.php\\n```\\nThis will make use of the PHP 512MB memory limit and the php-cgi command, which is the PHP executable which runs the script, to run the php script, and then the php-cgi command will be used to run the php script in a separate process.\\nThis will also make use of the `memory_limit` option, which is the default memory limit used by the PHP interpreter, which will be the maximum amount of memory which the PHP interpreter can operate in, and not the physical amount of memory in the server\\nThis will also allow for the use of the `memory_limit` option to be overridden if the `memory_limit` option is not set to 512MB, which means the PHP interpreter will be able to use a lower memory limit, if needed.\\nAdditionally, this will make use of the `memory_limit` option to be set to the maximum amount of memory which the PHP interpreter can operate in, and not the physical amount of"}
{"Context":"## Context\\nThis is a second iteration of content tracking via message bus design. See [previous version](0007-message-driven-content-tracker.md).\\nNew content tracker implementation will be based on new Search Services architecture (SS v2.0 or next gen). Main context behind this decision is almost the same as for v1 - get more throughput by leveraging new Transform Service.\\n","Decision":"## Decision\nThe decision is based on [version 1](0007-message-driven-content-tracker.md). The main differences are:\\n* Shared File Store may not be the right option as it is only available for Enterprise. Alternatively the URL to content can point to other locations. (TBC)\\n* The change in behaviour requires a major release of Search Services, most likely version 2.0.\\n* The changes in Content Repository will be available from version 6.3.\\n* The synchronous transformation APIs will remain functional until 7.0.\\nDetails of the architecture to be clarified (WIP).\\n","Prediction":"## Decision\nWe will use new Search Services architecture and implement new content tracker in it.\\nWe will use new Search Services to provide new features such as content tracking, search and indexing in the future.\\nWe will use new Search Services to provide new features such as content tracking, search and indexing in the future.\\nWe will use new Search Services to provide new features such as content tracking, search and indexing in the future.\\nWe will use new Search Services to provide new features such as content tracking, search and indexing in the future.\\nWe will use new Search Services to provide new features such as content tracking, search and indexing in the future.\\nWe will use new Search Services to provide new features such as content tracking, search and indexing in the future.\\nWe will use new Search Services to provide new features such as content tracking, search and indexing in the future.\\nWe will use new Search Services to provide new features such as content tracking, search and indexing in the future.\\nWe will use new Search Services to provide new features such as content tracking, search and indexing in the future.\\nWe will use new Search Services to provide new features such as content tracking, search and indexing in the future.\\nWe will use new Search Services to"}
{"Context":"## Context\\nContext here...\\n","Decision":"## Decision\n### Github workflow\\nThere are various workflows and each one has its strengths and weaknesses. Whether a workflow fits your case, depends on the team, the project and your development procedures.\\nTBD\\n","Prediction":"## Decision\nDecision here...\\n### Positive Consequences\\n* We can use the existing codebase\\nt\\n### Negative Consequences\\n* It's not easy to understand the code\\n* We have to change the whole way to use the new class\\n* We have to change the whole way to use the new class\\n* We have to change the whole way to use the new class\\n* We have to change the whole way to use the new class\\n* We have to change the whole way to use the new class\\n* We have to change the whole way to use the new class\\n* We have to change the whole way to use the new class\\n* We have to change the whole way to use the new class\\n* We have to change the whole way to use the new class\\n* We have to change the whole way to use the new class\\n* We have to change the whole way to use the new class\\n* We have to change the whole way to use the new class\\n* We have to change the whole way to use the new class\\n* We have to change the whole way to use the new class\\n* We have to change the whole way to use the new class\\n* We have to change the whole way"}
{"Context":"## Context and Problem Statement\\nTo build custom dashboards we need to define queries that are too complex for the existing filter box form.\\nWe would like to use flexible search expressions based on a proper query language.\\n","Decision":"## Decision\nChosen option: \"Monocle Query Language\", because it comes out best (see below).\\n### Positive Consequences\\n- We improve the user experience by replacing the clunky filter box with a simpler search bar.\\n- We create a re-usable component.\\n### Negative Consequences\\n- We need to maintain a language toolchain.\\n","Prediction":"## Decision\nWe will use [Django REST Framework](https:\/\/www.djangoproject.com\/en\/versions\/1.11\/) and the [searchable query language](https:\/\/searchablequerylanguage.org\/) to define a flexible query language for the filter box form.\\nWe will use [Django REST Framework's custom fields](https:\/\/www.djangoproject.com\/en\/versions\/1.11\/custom-fields\/) to define the fields of our search form.\\nWe will use [Django REST Framework's `search_fields`](https:\/\/www.djangoproject.com\/en\/versions\/1.11\/search_fields\/) to define the fields of our search form.\\nWe will use the `searchable_query` and `searchable_fields` forms to define the search form. The form will be rendered using [Django's template rendering](https:\/\/docs.djangoproject.com\/en\/1.11\/ref\/templates\/).\\n### Use of search_fields in form\\nThe search form will use the `search_fields` to define the fields of the search form.\\n### Use of searchable_query in form\\nThe search form will use the `searchable_query` to define the search form. The form will be rendered using [Django's template rendering](https:\/\/docs.djangoproject.com\/en\/1.11"}
{"Context":"## Context\\nWe need to decide whether to run our storage cluster as a standalone cluster or\\nhyper-converged by running pods on the Kubernetes cluster.\\n","Decision":"## Decision\nWe have decided to run a standalone storage cluster. The reason for using a separate\\ncluster is that by keeping the persistent data separate we keep flexibility over the\\nKubernetes cluster and can drop and recreate it without having to worry about the data.\\n","Prediction":"## Decision\nWe will run the storage cluster as a standalone cluster and will run the Pods on the Kubernetes cluster.\\nThis will allow us to scale the storage cluster to a larger number of nodes.\\nThis also allows us to add additional storage to the cluster without having to scale the\\nKubernetes cluster.\\nWe will run the storage cluster as a standalone cluster in the same way as the\\nKubernetes cluster. This will allow us to run different storage\\nproducts on the storage cluster without having to manage the Kubernetes\\ncluster.\\nWe will run the storage cluster as a hyper-converged cluster by running Pods on the Kubernetes\\ncluster. This will allow us to scale the storage cluster to a larger number of\\nnodes. It will also allow us to add additional storage to the cluster without having to\\nmanage the Kubernetes cluster.\\nWe will run the storage cluster as a single cluster. This will allow us to scale the\\nstorage cluster to a larger number of nodes.\\nWe will run the storage cluster as a single cluster and run the Pods on the Kubernetes\\ncluster. This will allow us to scale the storage cluster to a larger number of\\nnodes. It will also allow us to add additional storage to the cluster without having to\\nmanage the Kubernetes cluster.\\"}
{"Context":"## Context\\nBecause this project has `path-alias` in tsconfig.json which is a must for bootstrapping. `TS-NODE` not support `path-alias`.\\n","Decision":"## Decision\nUse `tsmon`\\n","Prediction":"## Decision\nUse `tsconfig` to define path alias. Its accepted values are:\\n- `app` (default)\\n- `src\/main.ts` (alias)\\n- `src\/main.tsx` (alias with JSX)\\n- `app.tsx` (alias with JSX, tsconfig.json version 1.0)\\n- `node_modules` (alias)\\n- `package.json` (alias)\\n- `tsconfig.json` (alias with version 1.0)\\n- `package-lock.json` (alias)\\n- `tsconfig.app.json` (alias with version 1.0)\\n- `package.json` (alias)\\n- `tsconfig.json` (alias with version 1.0)\\n- `tsconfig.app.json` (alias with version 1.0)\\n- `node_modules` (alias)\\n- `package.json` (alias)\\n- `tsconfig.json` (alias with version 1.0)\\n- `tsconfig.app.json` (alias with version 1.0)\\n- `node_modules` (alias)\\n- `package.json` (alias)\\n- `tsconfig.json` (alias with version 1."}
{"Context":"## Context\\nWe need a way to control the lifetime of an aggregate from the domain layer.\\nIf you imagine an aggregate running atop a \"CRUD-based\" Dogma engine, it's easy\\nto see that creation and destruction of aggregate data are necessary operations.\\nThis is less obvious in an event-sourced scenario where the notion of \"deletion\"\\nis not present.\\nWe need to define these operations in such as a way that the domain implementor\\ncan use the operations in a meaningful way within their domain, but engine\\nimplementors are free to determine their own persistence semantics.\\n","Decision":"## Decision\nWe've opted to name the methods used to create and destroy aggregate instances\\n`AggregateScope.Create()` and `Destroy()`, respectively. Note that\\n`AggregateScope` has since been renamed to `AggregateCommandScope`.\\n`Create()` is a fairly self explanatory name. This is an idempotent operation.\\nThe method returns `true` if the call actually resulted in the creation of the\\ninstance; or `false` if the instance already exists.\\n`Destroy()` was chosen in preference to words such as \"delete\", as depending on\\nthe engine implementation, no deletion necessarily occurs. It was chosen in\\npreference to \"done\", as it more clearly indicates that the aggregate instance\\nstate will be \"reset\". This could be implemented in an ES-based engine by\\nrecording internal events to represent the create\/destroy operations, and only\\nloading those events that occurred since the most recent creation.\\n### Mandatory Events\\nIt was also decided to require an event message be recorded in the same scope\\nas any successful call to `Create()` or `Destroy()`. This ensures that creation\\nand deletion is always represented by a domain event.\\nWe decided against having `Create()` or `Destroy()` take an event as a\\nparameter for two reasons:\\n1. This would necessitate a further decision about `Create()` as to whether the\\nevent it is passed should be recorded in all cases or only if the instance\\ndoes not already exist. Neither of which is appropriate in all cases.\\n2. If we decide to relax this requirement in the future, those methods would\\nhave to lose those event arguments, breaking backwards compatibilty.\\n","Prediction":"## Decision\nAggregate data will be persisted in the database as a \"collection\" of objects.\\nEach object will have a unique identifier, and a set of fields that may be 1 or more\\nvalues in the collection.\\nWhen an aggregate is created, the domain implementor will create a list of\\nobjects with the same set of fields. If the collection does not already exist,\\nthis will be done automatically. If the collection already exists, it will be\\nupdated with the new objects.\\nWhen an aggregate is destroyed, the domain implementor will destroy all\\nobjects that share the same set of fields. If the collection does not already\\nexist, it will be created with the new objects.\\nEach entity will have a unique identifier, and a set of fields that may be 1 or more\\nvalues in the collection.\\nWhen an entity is created, the domain implementor will create a list of\\nobjects with the same set of fields. If the collection does not already\\nexist, it will be added to the entity. If the collection already exists, it will be\\nupdated with the new objects.\\nEach entity will have a unique identifier, and a set of fields that may be 1 or more\\nvalues in the collection.\\"}
{"Context":"## Context\\nBecause this project is going to be delivered as a Beta project by the end of the year, it will be likely that its needs will grow as feedback from real users begins to filter in, as new ideas are conceived, new platforms are required, and as sources of funding are renewed. In order to accommodate these concerns, CHIP needs a simple, flexible, yet powerful API that is well situated for both changes in feature requirements and changes in technology.\\n","Decision":"## Decision\nGraphQL is a specification for an alternative to traditional REST APIs that provides a declarative development experience where you tell your program _what_ you want to happen and the GraphQL library will handle the _how_. It was initially developed by Facebook in 2012, announced by them in 2015, and has since grown exponentially to where 2018 will likely see more new APIs being written in GraphQL than in REST.\\nThe Absinthe library is a fully-featured, spec-compliant GraphQL implementation for Elixir and Phoenix applications and will be used for this project.\\n","Prediction":"## Decision\nWe will use [Swagger](https:\/\/swagger.io\/) to document our API.\\nSwagger is a tool that allows us to easily generate API documentation in a variety of formats, including HTML, PDF, and Markdown. It can also be used to generate documentation for REST APIs that are hosted on GitHub Pages.\\nSwagger can be used with a variety of programming languages and IDEs, and it is well supported by the Swagger community.\\nSwagger documentation is generated by the Swagger UI, which is a web-based tool that allows us to generate documentation in a variety of formats.\\nSwagger documentation can also be generated using the Swagger Editor tool, which is available as a NuGet package.\\nSwagger documentation can be generated using the Swagger Editor tool, which is available as a NuGet package.\\nAs Swagger documentation is generated by the Swagger UI, Swagger is available as a NuGet package.\\nIt is also possible to generate Swagger documentation using the Swagger Editor tool, which is available as a NuGet package.\\nIt is also possible to generate Swagger documentation using the Swagger Editor tool, which is available as a NuGet package.\\nWe will use Swagger to generate documentation for all API endpoints, and will also generate documentation for the [API Explorer](https:\/\/swagger.io"}
{"Context":"## Context and Problem Statement\\nFor L1, Decentraland has a DAO where people, part of the community, can vote and make decisions from basic signaling to executing an on-chain action.\\nDecentraland needs to scale and give a better experience to its users by using an L2 (second layer).\\nUsers will interact with the same set of smart contracts like the Marketplace and Bid in L1, and new\\nones like the upcoming store for collections' primary sales. Every smart contract which uses MANA but, not only, has\\nprotocol parameters that must be managed by some kind of governance.\\nSo far, DAO voting power is based on the user's wrapped MANA and LAND balance in **L1**.\\nThis document presents alternatives on how to manage governance in L2.\\n","Decision":"## Decision\n### Alternative 1\\n#### Pros\\n- N\/A\\n### Cons\\n- A lot of MANA locked on each layer.\\n- Voting Power divided between layers that can not be summed for on-chain voting.\\n### Implications\\n- Users will need to transfer MANA between layers and wrap it before a proposal started.\\n- Users won't be able to use all the voting power together.\\n- LAND and Estate smart contracts along with their bridges should be deployed in L2 to be used as voting power.\\n- Votes will be cheaper in L2 than L1.\\n### Alternative 2 \u2705 \u2705\\n### Pros\\n- Users will have all their voting power in L1.\\n### Cons\\n- Risky by transmitting all the messages by the same bridge. A smart contract upgrade and a marketplace fee change will go to the same smart contract.\\n### Implications\\n- Votes must still happen in L1.\\n### Alternative 3 \u2705 (Could be an iteration from alternative 2)\\n### Pros\\n- Users will have all their voting power in L1.\\n- We can have different implementations on each bridge.\\n### Cons\\n- N\/A\\n### Implications\\n- Votes must still happen in L1.\\n- Clear execution paths.\\n","Prediction":"## Decision\nUse **L2** as the governance mechanism. This means that we will use **L2** for all governance related\\nactivities, including voting and resolving issues.\\nWe will use **L2** to manage **L1**'s** governance** as well. The decision to use **L2** as the governance\\nmechanism will be driven by the community's needs and the available resources.\\n### How it works\\nWhen the community votes, it is decided on a block height, and the highest voted candidate is\\nselected as the winner. The winner is selected by a combination of consensus and voting, and the result is\\nannounced to the community.\\n### How to vote\\nTo vote, you must have the following permissions on your **L1** account:\\n* **api:** vote (get, set, delete, and others)\\n* **wallet:** vote (get, set, delete, and others)\\n* **web3:** vote (get, set, delete, and others)\\nYou must also have the following permissions on your **L2** account:\\n* **api:** vote (get, set, delete, and others)\\n* **wallet:** vote (get, set, delete"}
{"Context":"# Context and problem statement\\nThe Nimbus experiment DTO (\u201cdata transfer object,\u201d i.e. the canonical serialization) contains an\\n\u201capplication\u201d and \u201cchannel\u201d field but the contents are undefined. Jetstream, X-Man and the Rust SDK\\nhave made different assumptions about what this field should contain based on their different needs.\\nHow should we present this information to each consumer?\\n","Decision":"## Decision\n- Avoid standing up new information services: ideally each consumer will be able to derive all of\\nthe context they need from the payload\\n- Align values and labels with other systems where possible\\n- Handle quirks of existing and known-future (i.e. desktop Glean) consumers\\n- Allow Experimenter to use the same mechanisms for channel selection across platforms someday: it\\nwill simplify the recipe-writing logic if we can rely on the metadata fields in the DTO to\\ndescribe channel targeting on all platforms\\n# Decision outcome\\nAligning with the proposed probe-scraper API (formerly \"Option 1b\") is selected because it aligns\\nwell with other systems, gives consumers complete information, and has a coherent cross-platform\\ninterpretation of each field. The higher short-run costs are accepted in the name of consistency\\nbetween systems.\\nJeff Klukas has\\n[a proposal](https:\/\/docs.google.com\/document\/d\/1_zx0cxBnnFOhct1cCb_QnXntV-HN9sjqfGfiBwEBL-8\/edit?ts=5fd8dab1#heading=h.151eux68loqq)\\nto introduce new values to the Glean probe-scraper API that represent the concepts that we care\\nabout.\\nTo align, we would remove \u201capplication\u201d from the DTO and introduce two new keys:\\n- **appName** is the same as \u201cfamily\u201d in 1a, e.g. \u201cfenix\u201d, \u201cfirefox_desktop\u201d\\n- **appId** is the same as \u201capplication\u201d in 1a, e.g. \u201corg.mozilla.firefox.\u201d\\n- **channel** is like \u201crelease,\u201d \u201cbeta,\u201d etc (even on mobile)\\nAdditional **cons** compared to 1a are a breaking change for the Rust SDK. Additional **pros**\\ninclude aligning with expected data pipeline tooling. This is churnier in the short term but\\nhopefully less ambiguous in the long run.\\n# Other considered options\\n### Option 1a - Identify application family and application bundle ID in the DTO\\n**application** in the DTO will represent the app bundle ID (or \u201cfirefox-desktop\u201d on desktop), which\\ncan be used by both the SDK for targeting and Jetstream for deciding the right glean datasets.\\n**family** (replacing \u201cchannel\u201d) will represent a notion of application family, like \u201cFirefox,\u201d\\n\u201cFenix,\u201d \u201cFirefox iOS,\u201d \u201cFocus Android,\u201d etc.\\n**channel** represents a release channel identifier like \u201crelease,\u201d \u201cbeta,\u201d \u201caurora,\u201d etc (even on\\nmobile).\\nPros\\n- Consumers have the information they need without doing any additional lookups\\n- Doesn\u2019t require a breaking schema change, since no-one is really using channel yet\\nCons\\n- We\u2019d need to update some docs and code in Experimenter to prevent **\u201capplication\u201d** being\\ndifferent things in the DTO v.s. Experimenter i.e. rename the old thing to family.\\n- Release channel is encoded implicitly in a way that isn\u2019t obvious (although this seems fine since\\nDesktop is using the targeting property to figure this out and Jetstream doesn\u2019t need to know)\\n### Option 2 - Identify application family and channel in the DTO\\n\u201cApplication\u201d in the DTO will represent a notion of application family, like \u201cFirefox,\u201d \u201cFenix,\u201d\\n\u201cFirefox iOS,\u201d \u201cFocus Android,\u201d etc.\\n\u201cChannel\u201d will represent a release channel, like \u201crelease,\u201d \u201cbeta,\u201d \u201cnightly.\u201d\\nTim asked for this in [EXP-673](https:\/\/github.com\/mozilla\/experimenter\/issues\/4148).\\nPros\\n- Human-legible\\nCons\\n- No system actually needs release channel exposed this way. Jetstream will need additional\\ninformation in order to decide which dataset to look at, which might require implementing a new\\nExperimenter API and hitting it on each invocation. This _probably_ gives the SDK enough\\ninformation to make a decision but the application bundle identifier would be more convenient.\\nWork involved:\\n- Experimenter: Revert\\n[https:\/\/github.com\/mozilla\/experimenter\/issues\/4235](https:\/\/github.com\/mozilla\/experimenter\/issues\/4235)\\n### Option 3 - Experimenter was right: identify application family and channelbundle in the DTO\\n\u201cApplication\u201d in the DTO will represent a notion of application family, like \u201cFirefox,\u201d \u201cFenix,\u201d\\n\u201cFirefox iOS,\u201d \u201cFocus Android,\u201d etc.\\n\u201cChannel\u201d on desktop will represent a release channel, like \u201crelease,\u201d \u201cbeta,\u201d \u201cnightly.\u201d \u201cChannel\u201d\\non mobile will represent a bundle identifier, like \u201corg.mozilla.firefox.beta.\u201d\\nPros\\n- Consumers have the information they need without any additional lookups\\nCons\\n- Bundle ID and release channel are arguably two separate concepts\\nWork involved:\\n- Experimenter: Revert\\n[https:\/\/github.com\/mozilla\/experimenter\/issues\/4235](https:\/\/github.com\/mozilla\/experimenter\/issues\/4235)\\n# Additional considerations\\n> How do we spell the names of the application families? Are there any interesting systems we should\\n> agree with?\\nWe will align with the data pipeline probe scraper.\\n","Prediction":"## Decision\nThe decision is to present the application and channel fields as a string. The \\napplication field is expected to have the type \\code{String} and the channel field is expected to have the type \\code{Channel}\\nThe \\code{Channel} field is expected to be used by the consumer to specify the channel to use for\\na given data transfer object.\\nThis is the only change to the current behavior.\\n### Positive Consequences\\n* The consumer will be able to specify the data transfer object\u2019s channel in the\\napplication field.\\n* The consumer will be able to specify the data transfer object\u2019s application in the\\nchannel field.\\n### Negative Consequences\\n* The consumer will only be able to specify the channel\u2019s type.\\n* The consumer will not be able to specify the channel\u2019s type.\\n* The consumer will have to specify the data transfer object\u2019s application field.\\n* The consumer will have to specify the data transfer object\u2019s application field.\\n* The consumer will have to specify the data transfer object\u2019s application field.\\n* The consumer will have to specify the data transfer object\u2019s application field.\\n### Alternative approach\\n* The consumer could specify the data transfer object\u2019s application field as a string.\\"}
{"Context":"## Context\\nWe have a requirement to publish data about the performance of the\\nMake an LPA service. The key pieces of data we must publish are:\\n* **User satisfaction:** Ratings of the service provided by users on a 5 point scale,\\nfrom \"very dissatisfied\" to \"very satisfied\". This is currently collected through\\na [centralised government feedback page](https:\/\/www.gov.uk\/done\/lasting-power-of-attorney).\\n* **Completion rate:** Percentage of transactions started on the service by users\\nwhich are subsequently completed. We derive this from Google Analytics and\\nthe database.\\n* **Digital take-up:** Number of users using the online service as a percentage of\\nusers across all channels (online + paper + phone etc.). There is no API for this\\npresently.\\nCurrently, this data is being manually collated. We need to find a more efficient\\nand accurate way of publishing it.\\nAs a side objective, this is a greenfield project which gives us an opportunity\\nto test the waters of our proposed new technology stack\\n(see [0006-modernise-the-code-base](.\/0006-modernise-the-code-base.md)).\\n","Decision":"## Decision\n### Data sources\\nWe agreed the following sources for the data:\\n1. **User satisfaction:** To be collected by us through a new feedback form, based on\\nthe existing central form.\\n2. **Completion rate:** We propose to calculate this by counting the number of\\nLPA applications which reach a \"created\" state. We propose not to count\\napplications completed within a session (i.e. user logs in, starts an application,\\nand completes it). Rather, we would count the applications completed by day,\\nregardless of when they were started (e.g. user logs in on Monday, starts an\\napplication, and completes it on Thursday: it's added to the \"completed\"\\npile for Thursday).\\n3. **Digital take-up:** We will request a new endpoint on the Sirius data API to\\nprovide this data. Investigation showed that this data is currently manually\\ncollected from Sirius (OPG's case management system) through the client\\napplication, so we believe that Sirius has the data we need.\\n### High-level architecture\\nWe agreed that an **embedded** design for the solution's high-level architecture was preferable:\\n![Data platform - system landscape view](..\/images\/structurizr-SystemLandscapeEmbedded.png)\\n![Data platform - container view](..\/images\/structurizr-ContainerEmbedded.png)\\n(The above diagrams use the [C4 model](https:\/\/c4model.com\/).)\\nThis re-uses parts of the existing stack to implement ingress and storage for the data platform,\\nin particular:\\n* The data platform API (which accepts requests to generate the performance data) is\\npositioned behind the api-web proxy currently used by api-app.\\n* The data platform worker (which aggregates the performance data) and API write to the existing\\npostgres database, albeit into new table(s).\\nThe contrasting approach we discussed was a **standalone** one. Using this approach, we would\\nimplement the entire data platform in isolation from the existing application: we would have\\na separate database to store its data, and traffic in to the platform would be separate from\\ntraffic intended for other components, for example:\\n* The data platform has its own proxy which forwards requests to the API.\\n* The API and the worker process write to a separate database.\\nWe decided the latter would add unnecessary complexity, as we would need more\\ningress rules for the API, extra Terraform\/scripting to provision the additional database,\\nand a new load balancer\/proxy. This would be a lot of work and expense for a relatively simple service.\\n(In some respects, the proposed architecture is a little over the top; but part of the reason for that\\nis to provide a gateway into a more modern architecture over the long term.)\\n### Implementation approach\\nAfter agreeing on an embedded archtitecture, we also agreed the following approach to implementation:\\n* We will implement the data platform in Python, using AWS lambdas to run the worker and API.\\n* We will use a queue-based workflow, where we add a job to generate the perf data and notify\\nclients when it is ready. We agreed that SQS was a good choice of queuing technology.\\n* We will not merge the user satisfaction data into the existing feedback table,\\nbut will store it in a separate table in the PostgreSQL database.\\n* We\u2019ll manage the new table, likely using SQLAlchemy, possibly with Alembic for migrations.\\n* We'll provide an interface in the admin UI which enables admin users to view \u201cend of journey\u201d\\nuser satisfaction data. This could either be in parallel with the existing feedback view,\\nor integrated with it (by merging data from the two tables into one view). This is\\nto be decided by the product team.\\n* We'll gather user satisfaction data inside front-app initially, using a PHP form\\nbased on the existing centralised feedback form. This will post to the new API (in Python).\\nLonger term, this would be a good test case for migrating our PHP app to microfrontend\\nslices: as the user satisfaction form needs no authentication, it would be a good place to\\ninsert a first Python UI application into our infrastructure.\\n* We generally agreed to the system landscape and container structure shown in the diagrams above.\\nNote that a \u201ccontainer\u201d in these diagrams doesn\u2019t necessarily mean a docker container:\\nit\u2019s just a logical unit in the C4 level 2 (container) model diagram. These would be\\nlambdas running under Docker or localstack in dev, but native lambdas (either pure code\\nor lambda-ised containers) in live.\\n","Prediction":"## Decision\nWe will use [Gatsby](https:\/\/gatsbyjs.com\/) to build the new LPA service.\\nGatsby is a static site generator that generates static pages based on a\\nrequirement specification. It does this through the use of static site\\ncomponents, which are simple to compose and use.\\nAs we are creating a new service, we will be able to use the built-in\\nsupport for GraphQL which gives us the ability to query the data in\\na more efficient way.\\nWe will use [gatsby-plugin-recompose](https:\/\/github.com\/webpack\/gatsby-plugin-recompose) to compose the components.\\nThis plugin allows us to compose components from the same file\\nusing a simple configuration. It also provides a way to easily\\nrecompose the same component across multiple files.\\nWe will also be using [gatsby-plugin-graphql](https:\/\/github.com\/webpack\/gatsby-plugin-graphql) to fetch data from the\\nnew LPA service's database.\\nWe will use [gatsby-plugin-migrate](https:\/\/github.com\/webpack\/gatsby-plugin-migrate) to migrate the data from the old\\nGatsby service.\\nWe will also be using [gatsby-plugin-graphql-aws-s3](https:\/\/github.com\/"}
{"Context":"## Context\\nJames products rely historically on Spring for dependency injection. It doesn't use last major Spring version (4.x instead of 5.x).\\nJames uses Spring in a way that enables overriding any class via a configuration file thus endangering overall correctness by giving too much\\npower to the user.\\nJames proposes several implementations for each of the interfaces it defines. The number of possible combinations of\\nimplementations is thus really high (like factorial(n) with n > 10). It makes it unpractical to run tests for each\\npossible component combination. We run integration tests for combinations that we decide brings the more value to\\nthe users. Spring product defeats this testing logic\\nby allowing the user arbitrary classes combination, which is likely not being tested.\\nInstead of having a single product allowing all component combination, we rather have\\nseveral products each one exposing a single component combination. Components are defined by code in a static fashion.\\nWe thus can provide a decent level of QA for these products. Overriding components requires explicit code modification\\nand recompilation, warning the user about the impact of the choices he does, and lowering the project's responsibility.\\nGuice had been enacted as a way to reach that goal.\\nWith Guice we expose only supported, well tested combinations of components, thus addressing the combination issue.\\nSpring application often bring dependencies conflicts, for example between Lucene and ElasticSearch\\ncomponents, leading to potential runtime or compile time issues. Instead of having a single big application being able\\nto instantiate each and every component application, we have several products defining their dependencies in a\\nminimalistic way, relying only on the components implementation that are needed.\\nHere is the list of products we provide:\\n- In-Memory: A memory based James server, mainly for testing purposes\\n- Distributed James: A scalable James server, storing data in various data stores. Cassandra is used for metadata,\\nElasticSearch for search, RabbitMQ for messaging, and ObjectStorage for blobs.\\n- Cassandra: An implementation step toward Distributed James. It does not include messaging and ObjectStorage and\\nshould not be run in a cluster way but is still relevant for good performance.\\n- JPA: A JPA and Lucene based implementation of James. Only Derby driver is currently supported.\\n- JPA with SMTP only using Derby: A minimalist SMTP server based on JPA storage technology and Derby driver\\n- JPA with SMTP only using MariaDB: A minimalist SMTP server based on JPA storage technology and MariaDB driver\\nSome components however do have several implementations a user can choose from in a given product. This is the case for:\\n- BlobExport: Exporting a blob from the blobStore to an external user. Two implementations are currently supported:\\nlocalFiles and LinShare.\\n- Text extraction: Extracting text from attachment to enable attachment search. There is a Tika implementation, but\\nlighter JSOUP based, as well as no text extraction options are also available.\\nIn order to keep the number of products low, we decided to use conditional statements in modules based on the\\nconfiguration to select which one to enable at runtime. Eventually defeating the Guice adoption goals mentioned above.\\nFinally, Blob Storing technology offers a wide combination of technologies:\\n- ObjectStorage in itself could implement either Swift APIs or Amazon S3 APIs\\n- We decided to keep supporting Cassandra for blob storing as an upgrade solution from Cassandra product to Distributed\\nJames for existing users. This option also makes sense for small data-sets (typically less than a TB) where storage cost are less\\nof an issue and don't need to be taken into account when reasoning about performance.\\n- Proposals such as [HybridBlobStore](0014-blobstore-storage-policies.md) and then\\n[Cassandra BlobStore cache](0025-cassandra-blob-store-cache.md) proposed to leverage Cassandra as a performance\\n(latency) enhancer for ObjectStorage technologies.\\nYet again it had been decided to use conditional statements in modules in order to lower the number of products.\\nHowever, some components requires expensive resource initialization. These operations are performed via a separate module\\nthat needs to be installed based on the configuration. For instance\\n[Cassandra BlobStore cache](0025-cassandra-blob-store-cache.md) requires usage of an additional cache keyspace that\\nrepresents a cost and an inconvenience we don't want to pay if we don't rely on that cache. Not having the cache module\\nthus enables quickly auditing that the caching cassandra session is not initialized. See\\n[this comment](https:\/\/github.com\/linagora\/james-project\/pull\/3261#pullrequestreview-389804841) as well as\\n[this comment](https:\/\/github.com\/linagora\/james-project\/pull\/3261#issuecomment-613911695).\\n### Audit\\nThe following modules perform conditional statements upon injection time:\\n- BlobExportMechanismModule : Choice of the export mechanism\\n- ObjectStorageDependenciesModule::selectBlobStoreBuilder: Choice between S3 and Swift ObjectStorage technologies\\n- TikaMailboxModule::provideTextExtractor: Choice of text extraction technology\\n- BlobStoreChoosingModule::provideBlobStore: Choice of BlobStore technology: Cassandra, ObjectStorage or Hybrid\\n- [Cached blob store](https:\/\/github.com\/linagora\/james-project\/pull\/3319) represents a similar problem: should the\\nblobStore be wrapped by a caching layer?\\nCassandra and Distributed products are furthermore duplicated to offer a version supporting LDAP authentication. JPA\\nproduct does not offer LDAP support.\\n","Decision":"## Decision\nWe should no longer rely on conditional statements in Guice module.\\nGuice modules combination choice should be decided before starting the dependency injection stage.\\nEach component choice needs to be abstracted by a related configuration POJO.\\nProducts will, given the set of configuration POJOs, generated the modules it should rely on during the dependency\\ninjection stage.\\nAn INFO log with the list of modules used to create its Guice injector. This enables easy diagnose of the running\\ncomponents via the selected module list. It exposes tested, safe choices to the user while limiting the Guice products\\ncount.\\n","Prediction":"## Decision\nWe will use Guice for dependency injection. We will provide a Guice product for each component.\\nWe will provide a Guice configuration file for each component allowing to override the default behavior.\\nWe will expose a minimal set of dependencies that should be taken into account when creating a new component.\\nWe will drop support for Lucene and ElasticSearch in Distributed components.\\nWe will drop support for Tika in Distributed components.\\nWe will drop support for Cassandra in Distributed components.\\nWe will drop support for ObjectStorage in Distributed components.\\nWe will drop support for Hybrid BlobStore in Distributed components.\\nWe will drop support for S3 BlobStore in Distributed components.\\nWe will drop support for S3 ObjectStore in Distributed components.\\nWe will drop support for ObjectStore in Distributed components.\\nWe will drop support for Cassandra and ObjectStore in Distributed components.\\nWe will drop support for Tika in Distributed components.\\nWe will drop support for S3 ObjectStore in Distributed components.\\nWe will drop support for S3 ObjectStore in Distributed components.\\nWe will drop support for S3 ObjectStore in Distributed components.\\nWe will drop support for ObjectStore in Distributed components.\\nWe will drop support for S3 ObjectStore"}
{"Context":"## Context and Problem Statement\\nIn the context of the need to support MDTP domains other than www.tax.service.gov.uk (non-tax domains), facing the need\\nto keep things simple for service developers and for tracking preferences to be stored on the\\ncorrect domain, should we make tracking consent available on non-tax domains and advise using a relative URL to link to\\nit except when running locally, adjusting the helpers in play-ui and play-frontend-hmrc accordingly?\\nAt the moment, if tracking consent is integrated with a service not on www.tax.service.gov.uk via an\\nabsolute URL, users visiting the preferences page will have their preferences set on the incorrect domain.\\nThis decision does not introduce the need for any technical changes to tracking consent itself other than adding routing\\nfor the additional domains as required, but it does require changes to the integration guidance and helpers that are used to link to\\ntracking consent because they currently use an absolute URL derived from the `platform.frontend.host` configuration key.\\n## Decision Drivers\\n* The need to support the MDTP-hosted API platform on developer.service.hmrc.gov.uk and corresponding\\nlower environments\\n* The need to ensure tracking preferences are set on the correct domain\\n* The potential need to support other similar non-tax domains in future\\n* The need to reduce boilerplate code across MDTP services\\n* The need to support local development\\n* The limitation around non-tax users seeing the same content on the banner and settings page.\\n","Decision":"## Decision\n* The need to support the MDTP-hosted API platform on developer.service.hmrc.gov.uk and corresponding\\nlower environments\\n* The need to ensure tracking preferences are set on the correct domain\\n* The potential need to support other similar non-tax domains in future\\n* The need to reduce boilerplate code across MDTP services\\n* The need to support local development\\n* The limitation around non-tax users seeing the same content on the banner and settings page.\\nChosen option: \"Update integration guidance and helpers\", because making this change is technically straightforward, does not\\nundermine the solution, and the API platform are aware of the limitations around content.\\n### Positive Consequences\\n* Third party developers using the API platform will be able to adjust their tracking preferences using tracking consent.\\n* The API platform does not need to source or develop their own consenting solution or develop and review bespoke\\ncookie categories and content.\\n* The API platform can use a consenting solution consistent with the rest of the platform including API\\nplatform frontend microservices that do run under the tax domain\\n* The API platform can use the same data format and technical mechanism for storing tracking preferences.\\n* We are supporting the frontend needs of the API platform consistent with the principle that all MDTP hosted services\\ncan benefit from the same frontend components and services.\\n### Negative Consequences\\n* The API platform will not be able to customise the content on the banner or settings page (this limitation has been communicated\\nto the team).\\n* Content designers will need to bear in mind that the tracking consent content is being used on the developer hub aimed at third\\nparty developers.\\n* The scope increase and risk of future pressure to make additional changes to tracking consent that may undermine the solution's conceptual\\nintegrity.\\n","Prediction":"## Decision\n* The need to support the MDTP-hosted API platform on developer.service.hmrc.gov.uk and corresponding\\nlower environments\\n* The need to ensure tracking preferences are set on the correct domain\\n* The potential need to support other similar non-tax domains in future\\n* The need to reduce boilerplate code across MDTP services\\n* The need to support local development\\n* The limitation around non-tax users seeing the same content on the banner and settings page.\\nChosen option: \"Tracking consent on non-tax domains\", because:\\n* We want to ensure that developers have the option to set preferences on the correct domain, and\\n* We want to be able to support other non-tax domains in the future, without the need for a change to\\ntracking consent.\\n### Positive Consequences\\n* Tracking preferences are set on the correct domain, and on the correct domains where\\nthe service is hosted\\n* The tracking consent middleware is not required on the non-tax domains, so boilerplate code can be\\nremoved from the application\\n* We can support non-tax domains in the future, without the need for a change to tracking consent.\\n### Negative Consequences\\n* We will need to ensure that the tracking consent middleware is not required on the non"}
{"Context":"## Context\\nAndroid and iOS have different folder architectures.\\nTherefore we need to provide *path folder* according to user's plateform.\\n`path-provider` library only provide *Download Folder* for Android Version.\\n","Decision":"## Decision\nWe decided to use `path-provider` if the user is on Android, and use an other library (TBD) if the user is on iOS.\\n","Prediction":"## Decision\n`path-provider` library will provide *Download Folder* for Android Version.\\n`path-provider` library will provide *Download Folder* for iOS Version.\\n`path-provider` library will provide *Download Folder* for Windows Version.\\n`path-provider` library will provide *Download Folder* for Mac Version.\\n`path-provider` library will provide *Download Folder* for Linux Version.\\n`path-provider` library will provide *Download Folder* for Android Version.\\n`path-provider` library will provide *Download Folder* for iOS Version.\\n`path-provider` library will provide *Download Folder* for Windows Version.\\n`path-provider` library will provide *Download Folder* for Mac Version.\\n`path-provider` library will provide *Download Folder* for Linux Version.\\n`path-provider` library will provide *Download Folder* for Android Version.\\n`path-provider` library will provide *Download Folder* for iOS Version.\\n`path-provider` library will provide *Download Folder* for Windows Version.\\n`path-provider` library will provide *Download Folder* for Mac Version.\\n`path-provider` library will provide *Download Folder* for Linux Version.\\n`path-provider` library will provide *Download Folder* for Android"}
{"Context":"## Context\\nOffer an API without a good documentation is not a choice. Document this API with text documents is not efficient, and as the code changes more and more effort are necessary to keep it up-to-date. It's necessary to document APIs in a simply and efficient way, preferably in a way that developers can test it.\\n","Decision":"## Decision\nI decided to use Swagger as a documentation tool for the APIs, as it is a de facto standard. I choosed springfox-swagger2 and springfox-swagger-ui because of its smooth integration with Spring Boot.\\n","Prediction":"## Decision\nWe will use [Swagger](https:\/\/swagger.io\/) to document our API endpoints.\\nSwagger is a popular tool to document APIs.\\nIt's a JSON-based specification that is easy to read and easy to understand.\\nSwagger supports most of the popular HTTP verbs.\\nIt is easy to use and it is supported by most popular programming languages.\\nSwagger has a large community and is actively developed.\\nSwagger is also well documented and has a rich API.\\nSwagger documentation is very easy to use. It is well structured and simple.\\nSwagger documentation is also well documented and has a rich API.\\nSwagger documentation is also well documented and has a rich API.\\nSwagger documentation is also well documented and has a rich API.\\nSwagger documentation is also well documented and has a rich API.\\nSwagger documentation is also well documented and has a rich API.\\nSwagger documentation is also well documented and has a rich API.\\nSwagger documentation is also well documented and has a rich API.\\nSwagger documentation is also well documented and has a rich API.\\nSwagger documentation is also well documented and has a rich API.\\nSwagger documentation is also well documented and has a rich API.\\nSwagger documentation is also well documented and has a rich API.\\n"}
{"Context":"## Context\\nThere are [two main ways to go about testing your networking stack][2]:\\n1. introduce protocols that wrap Apple foundation types (like `URLSession`,\\n`URLSessionDataTask`, etc.)\\n2. introduce a stubbing library like [OHHTTPStubs][1] that leverage\\n`URLProtocol` and a small bit of swizzling to allow using JSON fixture data\\n","Decision":"## Decision\nWhile there are cases where both approaches are appropriate (even in the same\\nproject), we've decided to opt for option 1 (protocol-based testing) of our\\nnetworking layer. This allows a more unit-based approach to testing.\\n","Prediction":"## Decision\nThe testing framework will use the [OHHTTPStubs](https:\/\/github.com\/ohioh\/sweeper) library to wrap Apple foundation types and introduce protocol stubs. This will allow us to test our networking stack without having to introduce a custom protocol. It also allows us to test our networking stack without having to introduce a custom stubbing library.\\n### OHHTTPStubs\\nOHHTTPStubs is a library that stubs out Apple foundation types and wraps them in a single object. It will be used as a replacement for the existing `URLProtocol` library.\\n### JSON Fixture Data\\nJSON fixture data is [implemented using the [JSONFixtureData](https:\/\/github.com\/ohioh\/sweeper\/tree\/master\/projects\/sweeper\/ohhttp\/JSONFixtureData)] library. JSON fixture data is a simple format that is easy to read and write. It also allows for easy parsing and validation of the JSON data.\\n### Installation\\n### OHHTTPStubs\\n### JSONFixtureData\\n### Sweeper\\n### Sweeper is a framework that is used to write and run network tests. It allows you to write tests that wrap Apple foundation types and allows you to write tests that stub out Apple foundation types. It also allows"}
{"Context":"## Context\\nWe need a mechanism to capture and manage application errors. Without an integration our debugging options are to access a console on live environments and try to replicate (something we want to minimise) or by looking through information provided by logs.\\nWe have used Rollbar for a few years now and we have not reviewed this decision since. It is currently being used for 14 applications.\\nFor some projects we use their technical tooling of choice to aid in the transition to business as usual. Due to this we will have a handful of projects using Sentry and possible others.\\nSometimes Rollbar environment names don't match the Rails environment. Dalmatian-<project> and paas-<project> both exist. There also exists both permutations for the same project as we transition. We have used ROLLBAR_ENV to manage this before so making it explicit will hopefully make it clearer how it can be changed.\\n","Decision":"## Decision\nUse Rollbar to collect and manage our application errors.\\n","Prediction":"## Decision\nWe will use Rollbar to capture and manage application errors. This will be managed by a configuration file and a command line tool for managing the environment.\\nWe will use the following configuration to configure Rollbar. This is a minimal configuration that will be used to bootstrap the Rollbar environment and will be used to configure Rollbar for new projects. This configuration will be used to configure Rollbar for existing projects that have been transitioned from Rollbar to Sentry.\\n* Rollbar's default configuration is in config\/rollbar.yml. This can be overridden by a configuration file in config\/rollbar.yml or by a command line argument. This will be used to configure Rollbar for new projects and will be used to configure Rollbar for existing projects that have been transitioned from Rollbar to Sentry.\\n* Rollbar's default configuration is in config\/rollbar.yml. This can be overridden by a configuration file in config\/rollbar.yml or by a command line argument. This will be used to configure Rollbar for new projects and will be used to configure Rollbar for existing projects that have been transitioned from Rollbar to Sentry.\\n* Rollbar's default configuration is in config\/rollbar.yml. This can be overridden by a configuration file"}
{"Context":"## Context\\nIn threebot deployer we hardcode the branch we deploy from in the code, this will be annoying if we need to deploy from another branch.\\n","Decision":"## Decision\nGetting the branch dynamically from the active branch of js-sdk repository\\n","Prediction":"## Decision\n* We will move the branch to a new file, the webhook will check the branch and deploy only to that branch.\\n* We will use the `pull_request` feature to push the new branch to the server.\\n* We will use the `branch` to track the branches we are deploying from.\\n* We will use the `pull_request` feature to push the new branch to the server.\\n* We will check the branch to deploy on every push to the server.\\n* We will check the branch to deploy only to the branch that was pushed.\\n* We will check the branch to deploy only to the `master` branch.\\n* We will use the `pull_request` feature to push the new branch to the server.\\n* We will check the branch to deploy only to the branch that was pushed.\\n* We will check the branch to deploy only to the `master` branch.\\n* We will use the `pull_request` feature to push the new branch to the server.\\n* We will check the branch to deploy only to the branch that was pushed.\\n* We will check the branch to deploy only to the `master` branch.\\n* We will use the `pull_request` feature to push"}
{"Context":"## Context and Problem Statement\\nSince the product is going to be based on Microservices architecture there is a need for good orchestrator to host its different parts.\\n## Decision Drivers\\n* Easy to use\\n* Easy to learn\\n* Possibility to host modules as Docker containers\\n* Well integrated with Azure\\n* Well integrated with Azure DevOps\\n","Decision":"## Decision\n* Easy to use\\n* Easy to learn\\n* Possibility to host modules as Docker containers\\n* Well integrated with Azure\\n* Well integrated with Azure DevOps\\nDecided to use AKS since\\n* It is more powerfull than ASF\\n* It is more suitable for Docker containers hosting\\n* ASF have to be used when you plan to use its programming model (stateless services, stateful services, actors), but I didn't need that\\n* Kubernetes seems more popular and evolved\\n* Kubernetes has managed implementation for Azure - AKS\\n### Positive Consequences\\n* I will be able to use orchestrator to host app's modules.\\n### Negative Consequences\\n* n\/a\\n","Prediction":"## Decision\n* Easy to use\\n* Easy to learn\\n* Possibility to host modules as Docker containers\\n* Well integrated with Azure\\n* Well integrated with Azure DevOps\\nChosen option: \"Azure Service Fabric\", because it is the most mature and well supported orchestrator for Microservices currently available and it has a good integration with Azure DevOps.\\n### Positive Consequences\\n* Easy to use and learn, with good documentation and community support.\\n* Possibility to run multiple microservices on a single host.\\n* Possibility to use different containers, like Docker or Kubernetes.\\n* Possibility to use different orchestration solutions, like Service Fabric or Kubernetes Ingress.\\n### Negative Consequences\\n* Price tag is high.\\n* The documentation is not as good as it could be.\\n* The community support is not as good as it could be.\\n* The community is not as enthusiastic as it could be.\\n* It is not as mature as it could be.\\n* It is not as well integrated with Azure as it could be.\\n* It is not as well integrated with Azure DevOps as it could be.\\n* It is not as well integrated with Kubernetes as it could be.\\n* It is not"}
{"Context":"## Context\\nA portal should have some kind of status page.\\nIt has to tell an administrating person and a health check automation whether the portal is in a good state.\\nA good state can differ from the point of perspective.\\nA configuration might be syntactically correct but is not able to setup an I\/O connection to the datasource.\\n","Decision":"## Decision\n* A portal and the portal extension have to be able to provide new status topics and have impact on the contents they report.\\n* Every reporter has to expose JSON serializable content for easy automation access.\\n* Every reporter should expose a boolean value keyed with the topics' key to determine whether the report displays a good state.\\n* Every portal should expose a status reporter for topic `health` when the portal interacts with a datasource connected via I\/O operations to determine correct configuration and connectivity.\\n* Portal extensions have to prefix their own keys they expose with a reasonable identifier.\\n* Every topic should be accessible on their own.\\n* A status report should act fast and use as little I\/O operations as possible to allow frequent health checks.\\n* A status report should be promoted for the following use cases:\\n* static information that are not part of the providing composer package\\n* health check of the datasource connection\\n* portal internal behaviour analysis (last time usage, remaining API calls by time limitations)\\n* configuration support\\n","Prediction":"## Decision\nThe portal will be in a good state if:\\n* The I\/O connection to the datasource is setup\\n* The portal is on the same host as the datasource\\n* The health check is able to connect to the portal\\n* The health check is able to ping the portal\\n* The portal can be reached by the health check\\n* The portal is reachable from the health check\\n* The portal can be reached from the I\/O connection to the datasource\\n* The portal is reachable from the I\/O connection\\n* The portal has a valid SSL certificate\\n* The portal has a valid TLS certificate\\n* The portal has a valid private key\\n* The portal has a valid private key\\n* The portal has a valid X.509 certificate\\n* The portal has a valid X.509 certificate\\n* The portal has a valid X.509 certificate\\n* The portal has a valid X.509 certificate\\n* The portal has a valid X.509 certificate\\n* The portal has a valid X.509 certificate\\n* The portal has a valid X.509 certificate\\n* The portal has a valid X.509 certificate\\n* The portal has a valid X.509 certificate\\n* The portal has a valid X.509 certificate\\n* The portal"}
{"Context":"## Context\\nThere are a couple external services that need to be communicated with. These are expected to be a source control system and a server running the [Cucumber Wire Protocol](1987e2349b14ca0fe93e879d762df09f1a9b3934). There are other functions that are needed such as authentication and authorization, and an abstraction around the storage and organization of Gherkin-based executable specifications\\n","Decision":"## Decision\nAn API server will be built to handle the following:\\n* Integration with source control systems (initially just `git` but the addition of others needs to be possible)\\n* Communication with Cucumber Wire Protocol service\\n* Required abstractions for creating, modifying, organizing, and executing Gherkin-based executable specifications\\n* Abstractions for authentication and authorization (initially fulfilled by a simple database authentication mechanism, but eventually allowing other authentication sources such as OAuth)\\nThese functions will be independent of any user interface that's presented to facilitate carrying out these actions.\\n","Prediction":"## Decision\nThe external services will be implemented as REST endpoints that can be called from the client application.\\nThis means that the client application can be written in any language that is supported by the external service.\\nThe client application will be responsible for provisioning the necessary resources for the REST endpoint to function properly.\\nThis includes the provision of the credentials to sign the Gherkin executable specification and the provision of the environment variables that are needed to run the Gherkin executable.\\nThe client application will also be responsible for handling the authentication and authorization of the external services.\\nThis includes the provision of the credentials to sign the Gherkin executable specification to the external service, and the provision of the environment variables that are needed to run the Gherkin executable.\\nThe client application will also be responsible for the storage and organization of the Gherkin-based executable specification.\\nThis includes the provision of the Gherkin-based executable specification to the external service, and the provision of the environment variables that are needed to run the Gherkin executable.\\nThe client application will also be responsible for the provision of the Gherkin-based executable specification to the external service.\\nThis includes the provision of the Gherkin-based executable specification to the external service.\\nThis"}
{"Context":"## Context\\nAll my website are updated only a few times per year and I need that will works without backend system.\\n","Decision":"## Decision\nI decide using plain html over Wordpress, Ghost or similar solution for simplify process publishing new articles using only git and my current editor (VSCode). No costs for hosting and better reliability than my own hosting.\\n","Prediction":"## Decision\nI decided to use [Flask](https:\/\/flask.pocoo.org\/) as the backend to serve my website.\\nThis decision is based on the fact that Flask is a well known and maintained framework and it's easy to use.\\nIt's also a framework that can be used to create REST APIs.\\nI will use Flask to serve my website.\\nI will use [Pylons](https:\/\/pypi.python.org\/pypi\/Pylons\/) to deploy my website on Heroku.\\nI will use [Django](https:\/\/www.djangoproject.com\/) to create my REST API.\\nI will use [Django REST Framework](https:\/\/www.djangoproject.com\/projects\/rest_framework\/) to create my API views.\\nPylons will be used to create my website.\\nDjango REST Framework will be used to create my API views.\\nDjango will be used as the ORM (Object Relational Mapper) to manage my data.\\nDjango will be used as the DB (Database) driver to manage my data.\\nDjango will be used as the front-end framework to display my website.\\nDjango will be used as the back-end framework to manage my data.\\nDjango will be used as the database"}
{"Context":"## Context\\nWe want to be able to write log entries (to file, or STDOUT), without them being interleaved.\\nBut also, we want logging to perform well.\\n","Decision":"## Decision\n_Unlike_ the Ruby standard `Logger`, GreenLog will use a [lock-free logging](https:\/\/www.jstorimer.com\/blogs\/workingwithcode\/7982047-is-lock-free-logging-safe) approach. That is, we will:\\n- avoid using of mutexes to serialise output\\n- perform atomic writes to `IO` streams (using `<<`)\\n","Prediction":"## Decision\nWe will use `logger` as the logging library.\\nThis library is designed to be easily extensible, and is already well tested on other projects. It also has good support for logging to files.\\nWe will use `loggers` as the logging library.\\nThis library is designed to be easily extensible, and is already well tested on other projects. It also has good support for logging to files.\\nWe will use `slf4j` as a logging library.\\nThis library is designed to be easily extensible, and is already well tested on other projects. It also has good support for logging to files.\\nWe will use `logback` as a logging library.\\nThis library is designed to be easily extensible, and is already well tested on other projects. It also has good support for logging to files.\\n### Logging to files\\n`loggers` will be used to log to files. This will be done using the `slf4j` library, which is designed to be easily extensible. It also has good support for logging to files.\\n`logback` will be used to log to files. This will be done using the `slf4j` library, which is designed"}
{"Context":"## Context\\nOur CI\/CD Pipeline has a build stage, which contains a `buildBackend` and a `buildFrontend` component.\\n`buildFrontend` is optional, and only run if the project has a frontend, which is determined by the existence of a `package.json` file.\\nIf both are run, they are executed in parallel.\\n`buildFrontend` actually does not build anything, it only downloads dependencies.\\n\"Building\" the frontend is facilitated via maven, which is run in `buildBackend`.\\n","Decision":"## Decision\nBoth `buildBackend` and `buildFrontend` are unified into one `build` stage.\\n","Prediction":"## Decision\nWe will use `frontend-maven-plugin` to build the frontend.\\nThis plugin will be used to build the frontend during the build of the backend.\\nThis plugin will be used to build the frontend during the build of the frontend.\\nThis plugin will be used to build the frontend during the build of the frontend.\\nThis plugin will be used to build the frontend during the build of the backend.\\nThis plugin will be used to build the frontend during the build of the frontend.\\nThis plugin will be used to build the frontend during the build of the backend.\\nThis plugin will be used to build the frontend during the build of the frontend.\\nThis plugin will be used to build the frontend during the build of the backend.\\nThis plugin will be used to build the frontend during the build of the backend.\\nThis plugin will be used to build the frontend during the build of the backend.\\nThis plugin will be used to build the frontend during the build of the backend.\\nThis plugin will be used to build the frontend during the build of the backend.\\nThis plugin will be used to build the frontend during the build of the backend.\\nThis plugin will be used to build the frontend during the build of the backend.\\n"}
{"Context":"## Context\\nWe need to choose framework for tests.\\n","Decision":"## Decision\nWe will use Pytest. It's much more Pythonic and simplier to use than Unittest\\nfrom standrd library.\\n","Prediction":"## Decision\nWe will use [F#](https:\/\/fsharp.org\/) as a test framework.\\nWe will use [F# Test](https:\/\/fsharp.org\/test\/) for unit tests.\\nWe will use [F# Test.Report](https:\/\/fsharp.org\/test\/report\/) for integration tests.\\nWe will use [F# Test.Interpreter](https:\/\/fsharp.org\/test\/interpreter\/) for unit tests that use an external interpreter.\\nWe will use [F# Test.Interpreter.Test](https:\/\/fsharp.org\/test\/interpreter\/test\/) for integration tests that use an external interpreter.\\nWe will use [F# Test.Interpreter.Patches](https:\/\/fsharp.org\/test\/interpreter\/patches\/) for unit tests that use patches.\\nWe will use [F# Test.Report.Test](https:\/\/fsharp.org\/test\/report\/test\/) for integration tests that use patches.\\nWe will use [F# Test.Report.Test](https:\/\/fsharp.org\/test\/report\/test\/) for integration tests that use an external interpreter.\\nWe will use [F# Test.Report.Test](https:\/\/fsharp.org\/test\/report\/test\/) for integration tests that use patches.\\nWe will use [F# Test.Report.Test](https:\/\/fsharp.org\/test\/report\/test"}
{"Context":"## **Context**\\nWhen the microservices need to interact with each other (e.g. to sync common data), the preferable approach is to adopt an event-driven architecture. This approach guarantees loosely coupled services which can be run and deployed in isolation.\\nIn order to implement an events-driven architecture we need to use a message broker which should be responsible for sending the messages.\\nIn AWS, two options can be considered:\\n1. SNS (Simple Notification Service)\\nSNS implements pub\/sub messaging which allow applications to send messages to multiple subscribers through push mechanisms.\\n2. SNS (Simple Notification Service) + SQS (Simple Queue Service)\\nSQS is a message queue service used by distributed applications to exchange messages through a polling model and can be used to decouple sending and receiving components. Using Amazon SNS and Amazon SQS together, messages can be delivered to applications that require immediate notification of an event, and also persisted in an Amazon SQS queue for other applications to process at a later time.\\n","Decision":"## Decision\n**SNS (Simple Notification Service) + SQS (Simple Queue Service)**\\nBy coupling SQS with SNS, the subscriber can receive messages at \"their peace\". It allows subscribers to be offline, tolerant to network and host failures. Although SNS has got a 4-phase retry policy, the message can be lost if the consumer is not available. Instead if the subscriber uses a queue, we are able to achieve guaranteed delivery.\\n","Prediction":"## Decision\nWe will use Amazon SNS and SQS to implement an event-driven architecture in our microservices. We will use SQS to decouple sending and receiving components.\\nWe will use SQS to persist messages in an Amazon S3 bucket. We will use SQS to process messages in an Amazon SQS queue. We will use SQS to send messages to other microservices.\\nWe will use SQS to send messages to other microservices.\\nWe will use SQS to send messages to other microservices.\\nWe will use SQS to send messages to other microservices.\\nWe will use SQS to send messages to other microservices.\\nWe will use SQS to send messages to other microservices.\\nWe will use SQS to send messages to other microservices.\\nWe will use SQS to send messages to other microservices.\\nWe will use SQS to send messages to other microservices.\\nWe will use SQS to send messages to other microservices.\\nWe will use SQS to send messages to other microservices.\\nWe will use SQS to send messages to other microservices.\\nWe will use SQS to send messages to other microservices.\\nWe will use SQS to send"}
{"Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","Prediction":"## Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n\\nWe will use Architecture Decision Maker, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n\\nWe will use Architecture Decision Log, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n\\nWe will use Architecture Decision Tree, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n\\nWe will use Architecture Search Engine, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n\\nWe will use Architecture Search, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n\\nWe will use Architecture Search Service, as described by Michael Nygard in"}
{"Context":"## Context\\nThere are two main strategies for organising source code that I'm aware of:\\n- Organise by kind\\n- Organise by feature\\nWith \"Organise by kind\" we group modules that are related architecturally together. I.e. group all\\n\"Effects\", group all \"Data\", group all \"Scenes\".\\nThis is a fairly common approach, particularly given that website architectures often typify this\\nstyle by grouping all \"modules\", \"views\" and \"controllers\".\\nAlternatively we can \"Organise by feature\", ideally in this style the source code required to\\nimplement a particular \"Feature\" is grouped together instead. Ideally this would mean that\\nremoving a feature would simply mean deleting it's grouping any removing any dependent links.\\nFor this project I want to experiement to see what \"Organise by feature\" would look like for a game.\\n","Decision":"## Decision\nWe're going to organise by *feature*!\\n","Prediction":"## Decision\nI am going to use the \"Organise by feature\" approach for the game. I will create a \"Game\" module that\\ncontains all of the game related modules. I will then create a \"GameModule\" module that contains\\nall of the game related modules. I will then create a \"GameSpec\" module that contains all of the\\ngame related modules. I will then create a \"GameModuleSpec\" module that contains all of the\\ngame related modules. I will then create a \"GameModuleSpec\" module that contains all of the\\ngame related modules and so on. This will allow me to group modules that are related architecturally\\ntogether, and allow me to remove any module that is no longer required.\\nSince the \"GameModuleSpec\" module will contain all of the game related modules, I will\\ncreate a \"GameModule\" module that contains all of the game related modules.\\nThis will allow me to remove any game related module that is no longer required.\\nThis will allow me to remove any game related module that is no longer required.\\nThis will allow me to remove any game related module that is no longer required.\\nThis will allow me to remove any game related module that is no longer required"}
{"Context":"## Context\\nEnsure that TDR users are not able to overwrite\/view another user\u2019s objects in the TDR AWS S3 upload bucket.\\nPotential options to resolve the problem are to use:\\n* AWS Cognito;\\n* Signed URLS.\\n","Decision":"## Decision\nDecided to go with AWS Cognito, with Keycloak as an OpenId authentication provider, using IAM policy to restrict access to a user's objects.\\nAWS Cognito has some advantages over using signed URLs:\\n* Using signed URLs means an eavesdropper, potentially, can grab the signed url and use it until it expires\\n* Note: AWS signed URLs use HTTPS, so an eavesdropper on a public network cannot see the secret part which is in the request parameters or the path.\\n* AWS Cognito will work with AWS KMS encrypted AWS S3, can set up an AWS IAM or key policy allowing access to the AWS KMS key from AWS Cognito identities\\n* A separate signed URL would have to generated for each file. That would mean generating thousands of URLs for large consignments.\\nDetails of how AWS Congito would work are given in the sections below.\\n","Prediction":"## Decision\nUse AWS Cognito to protect the TDR AWS S3 upload bucket.\\nThis will help to mitigate the risk of a TDR user being able to overwrite the contents of another user\u2019s S3 bucket.\\nThis will also ensure that the TDR user is not able to upload files to the TDR AWS S3 upload bucket.\\nCognito will also allow the TDR user to have a unique ID for each S3 bucket that they have access to.\\nThis will allow the TDR user to be able to securely upload to their own bucket.\\nCognito will have the following\\n* A unique ID per S3 bucket.\\n* A unique key for each bucket.\\n* A unique secret for each bucket.\\n* The ability to enable S3 encryption.\\n* The ability to enable S3 SSM integration.\\n* S3 Encryption will be enabled by default for all buckets.\\n* S3 SSM will be enabled by default for all buckets.\\n* S3 DMS will be enabled by default for all buckets.\\n* S3 SSM will be enabled by default for all buckets.\\n* S3 S3FS will be enabled by default for all buckets.\\n* S3 FS will be enabled"}
{"Context":"## Context\\nWe need to build a UI that allows users to \"login\" by entering a PIN - the UI\\nshould allow access to pages if the user logs in. In addition we want to run\\nZenroom code in the browser using the Zenroom WASM build. To build this\\nmoderately complex UX we could use vanilla HTML with sprinklings of jQuery or\\nsomething more structured.\\n","Decision":"## Decision\nTo use Vue (https:\/\/vuejs.org) to build the front end UI as a SPA that sends\\nmessages to Phoenix backend via sockets in order to make use of the required\\nDECODE services.\\n","Prediction":"## Decision\nWe will use HTML and CSS to build the UI. We will use jQuery to provide\\nfeatures that allow us to run Zenroom code in the browser.\\nWe will use a custom build of the [Zenroom WASM](https:\/\/github.com\/zencamp\/zenroom-wasm) to build the UI and the\\nZenroom code.\\nWe will use the [Zenroom WASM Test Coverage](https:\/\/github.com\/zencamp\/zenroom-wasm\/tree\/main\/zenroom-wasm\/testcoverage) to\\ntest the Zenroom WASM build.\\nWe will use the [Zenroom WASM build](https:\/\/github.com\/zencamp\/zenroom-wasm\/tree\/main\/zenroom-wasm) to build the UI and\\nthe Zenroom code.\\nWe will use the [Zenroom WASM Test Coverage](https:\/\/github.com\/zencamp\/zenroom-wasm\/tree\/main\/zenroom-wasm\/testcoverage) to\\ntest the Zenroom WASM build.\\nWe will use the [Zenroom WASM Test Coverage](https:\/\/github.com\/zencamp\/zenroom-wasm\/tree\/main\/zenroom-wasm\/testcoverage) to\\ntest the Zenroom WASM build.\\nWe will use the"}
{"Context":"## Context\\nWe need to avoid side effects on configuration loading and prevent the need to fully configure the settings to run a subset of tests in projects using `konfetti`.\\n","Decision":"## Decision\nWe will use a lazy evaluation approach, similar to [implemented in Django](https:\/\/github.com\/django\/django\/blob\/master\/django\/conf\/__init__.py#L42)\\n","Prediction":"## Decision\nWe will use `konfetti` to configure the tests.\\nWe will use `konfetti` to configure the services.\\nWe will use `konfetti` to configure `konfetti-config.yml`.\\nWe will use `konfetti` to configure `konfetti-config.rb`.\\nWe will use `konfetti` to configure `config.ru`.\\nWe will use `konfetti` to configure `config.rb`.\\nWe will use `konfetti` to configure `config.toml`.\\nWe will use `konfetti` to configure `config.toml.toml`.\\nWe will use `konfetti` to configure `config.toml.toml.toml.toml.toml.toml.toml.toml.toml.toml.toml.toml.toml.toml.toml.toml.toml.toml.toml.toml.toml.toml.toml.toml.toml.toml.toml.toml.toml.toml.toml.toml.toml.toml.toml.toml.toml.toml.toml.toml.toml.toml.toml.toml.toml.toml.toml.toml.toml.toml.toml.toml.toml.tom"}
{"Context":"## Context\\nThe issue motivating this decision, and any context that influences or constrains the decision.\\n","Decision":"## Decision\nThe change that we're proposing or have agreed to implement.\\n","Prediction":"## Decision\nThe change that we're proposing or have agreed to implement.\\n### Positive Consequences\\n* The change that we're proposing or have agreed to implement.\\n### Negative Consequences\\n* We're not sure, we need to think this through. We may change our mind.\\n### Exceptions\\n* This change does not affect me or my team.\\n* This change does not improve the sustainability or robustness of our systems.\\n* This change is only a small part of an overall change that we're considering or have agreed to implement.\\n### Decision\nThe change that we're proposing or have agreed to implement.\\n### Positive Consequences\\n* We're not sure, we need to think this through. We may change our mind.\\n### Negative Consequences\\n* We're not sure, we need to think this through. We may change our mind.\\n### Exceptions\\n* This change does not affect me or my team.\\n* This change does not improve the sustainability or robustness of our systems.\\n* This change does not introduce a breaking change.\\n### Decision\nThe change that we're proposing or have agreed to implement.\\n### Positive Consequences\\n* We're not sure, we need to think this through."}
{"Context":"## Context\\nSince 2015 JavaScript has had a standard for defining and importing modules, as defined in chapter 15 of the [ECMAScript 2015 Language Specification](https:\/\/262.ecma-international.org\/6.0\/) (also known as ECMAScript 6). [Node.js](https:\/\/nodejs.org\/) introduced support for ECMAScript Modules (ESM) in late 2020, with all major supported versions providing ESM support.\\nPrior to the use of ECMAScript Modules, this application was using the `require()` functionality provided by Node.js to import modules. While this worked without any issue, the dawn of a major problem appeared on the horizon: third party libraries started using ESM. The old `require()` module system cannot import modules defined using the new ECMAScript 6 syntax. While ESM is backward compatible, it is not forward compatible. As more libraries migrate to this standard, the application will gradually go stale, using old versions of libraries.\\nAs such, switching the entire application to ESM has become a necessity.\\n","Decision":"## Decision\nThis application will use ECMAScript Modules because it is the only sensible choice going forward.\\n","Prediction":"## Decision\nThe application will be converted to ESM. This will require a complete rewrite and re-architecture of the application.\\nThe decision has been taken to use [Webpack](https:\/\/webpack.js.org\/) to load all modules and build the application. This will allow the application to be built using ESM.\\nWebpack will be responsible for loading the modules, building the application and bundling it into a single file. It will also be responsible for bundling and minifying the application before it is deployed.\\nWebpack will be used because it is the most widely used bundler in the Node.js ecosystem. It is also the bundler that is most widely supported by the Node.js community.\\nWebpack is also the bundler that is most widely used by the [Webpack DevKit](https:\/\/webpack.devkit.io\/) and is therefore the bundler that will be used by the application.\\nWebpack will also be used because it has been shown to significantly improve the performance of the application. While this is not a direct performance impact, it is an indirect one, as the application will be more efficient in the long term.\\nWebpack will also be used because it has been shown to significantly improve the quality of the application. While this is not a direct quality impact"}
{"Context":"## Context\\nPreviously my thoughts regarding a \"streamid\" in the `CloudEvent` has been that since it's not visible to the user.\\nThe \"streamid\" extension property was added on write and removed on read to avoid surprises to the user.\\nHowever this leads to problems implementing snapshotting and sagas since then it's highly likely that we want to use the streamId\\nin these cases (and the user probably wants to know the stream id in a saga).\\n","Decision":"## Decision\nFor this reason the implementors of the `WriteEventStream` api will _add_ a \"streamid\" to each `CloudEvent`.\\n","Prediction":"## Decision\nUse the `stream_id` property instead.\\nThis is the property that we want to use as a stream id because it's intended to be used in a stream.\\nIt's not intended to be used as a stream id in a saga.\\nIn the case of a stream, it's intended to be used as a stream id in a saga.\\nIn the case of a saga, it's intended to be used as a stream id in a saga.\\nIt's recommended to use the stream id property if it's not used in a stream.\\nIf there's no use for it in a stream, it's recommended to use the stream id property.\\nIf the stream id property is used as a stream id in a saga, there's no need to use it in a stream.\\nIf the stream id property is used in a saga, there's no need to use it in a stream.\\nThis is because the stream id property is intended for the user to see in a stream.\\nIn a saga, the stream id property is intended for the user to see in a saga.\\nIn a saga, the stream id property is intended for the user to see in a saga.\\nIn a saga, the stream id property"}
{"Context":"## Context\\nThe point of the tool is to enforce good commit hygiene, which is easy to compromise while trying to just get good code out. We should pick a standard that seems like it's popular or otherwise conventional, and not overly strict for flexibility.\\n","Decision":"## Decision\nUse [Conventional Commits v1.0.0-beta.4]. It's similar enough to [Semantic Commits] and its inspiration [Angular Commits], but loose enough and working on being an independent standard.\\n","Prediction":"## Decision\nWe'll use the [Git Flow](https:\/\/git-scm.com\/book\/en\/v2\/):\\n1. [Pull Requests](https:\/\/git-scm.com\/docs\/git-flow\/#pull-requests)\\n2. [Commits](https:\/\/git-scm.com\/docs\/git-flow\/#commits)\\n3. [Rebase](https:\/\/git-scm.com\/docs\/git-flow\/#rebase)\\n4. [Push (push-requests)](https:\/\/git-scm.com\/docs\/git-flow\/#push-push-requests)\\n5. [Pull Requests (pull-requests)](https:\/\/git-scm.com\/docs\/git-flow\/#pull-requests)\\n6. [Push (push-requests)](https:\/\/git-scm.com\/docs\/git-flow\/#push-push-requests)\\n7. [Pull Requests (pull-requests)](https:\/\/git-scm.com\/docs\/git-flow\/#pull-requests)\\n8. [Push (push-requests)](https:\/\/git-scm.com\/docs\/git-flow\/#push-push-requests)\\n9. [Pull Requests (pull-requests)](https:\/\/git-scm.com\/docs\/git-flow\/#pull-requests)\\n10. [Push (push-requests)](https:\/\/git-s"}
{"Context":"## Context\\nIMLS funded us to experiment with transcription tools. The project [Anti-Slavery Manuscripts](https:\/\/www.antislaverymanuscripts.org\/classify) has a workflow where volunteers transcribe lines of text first by marking the line, then adding the transcription in a sub-task. This task is slightly unique in that:\\n- Incomplete classifications can be submitted\\n- Previous transcriptions from caesar are loaded and presented as an option in the sub-task\\n- volunteers can select the previous transcription, edit it, and submit a new transcription\\n- lines with previous transcriptions or retired lines are displayed visually by color\\n","Decision":"## Decision\nWe will be porting the ASM functionality to the main classifier as a new transcription task. The task will be composite of:\\n- A drawing line task for transcription that is created by two pointer down and up events to mark the points of the line.\\n- the starting point and ending point will have visual indicators communicating the direction of creation.\\n- the line color will indicate the current status of completion\\n- completed lines cannot be edited\\n- Note that ASM leveraged a hacked polygon tool to do this, but we should have a line tool variant specifically for this instead. Downstream aggregation can leverage polygon type aggregations if it fits.\\n- A sub-task will display once the transcription line mark is made\\n- a text input will display suggestions from previous caesar aggregations.\\n- The suggestions can be selected and inserted as an editable value in the text input\\n- Grommet's [TextInput](https:\/\/storybook.grommet.io\/?path=\/story\/textinput--suggestions) with the suggestions prop will be utilized so that it can be styleable with our theme.\\n- sub-task will not be a modal, but a movable div linked to the currently selected line mark\\n[More detailed user stores are in this google doc](https:\/\/docs.google.com\/document\/d\/16abI-wkRlEXsWgACfFQVqwO76aEopohIjQiRfNQKWiw\/edit)\\n","Prediction":"## Decision\nWe will use [Toggl](https:\/\/www.toggl.com\/) to provide a way to mark up completed classifications and upload the completed classifications for display.\\nToggl will collect the transcriptions and classifications for each line and provide a way to display in a dashboard.\\nWe will use [Toggl-Fluent](https:\/\/fluent-toggl.com\/) to provide a way to upload completed classifications to Antislavery Manuscripts for display.\\nToggl-Fluent will provide a way to upload completed classifications to the [Anti-Slavery Manuscripts](https:\/\/www.antislaverymanuscripts.org\/classify) platform. Toggl-Fluent will also provide a way to export the completed classifications to a CSV file.\\nWe will use [Toggl-Fluent-UI](https:\/\/fluent-toggl.com\/3\/ui\/) to provide a way to upload completed classifications to the [Anti-Slavery Manuscripts](https:\/\/www.antislaverymanuscripts.org\/classify) platform. Toggl-Fluent-UI will provide a way to upload completed classifications to Antislavery Manuscripts for display.\\nWe will use [Toggl-Fluent-"}
{"Context":"## Context\\n* In order to develop and test their build and deployment pipeline the teams need be able to run the same build tools as the CI server offline on their machines.\\n* In order to be able to trace user acceptance down to code and to enable efficient and fast pipelines we must not build the same code multiple times for different environments \/ configurations.\\n","Decision":"## Decision\n* We make a **pipeline development environment** available to teams. See #34\\n* The CI server only **build once** per pipeline run, so that the commit hash of the system can be used as reliable reference.\\n","Prediction":"## Decision\nWe will use [Azure Pipelines](https:\/\/docs.microsoft.com\/en-us\/azure\/azure-pipelines\/default-user\/using-azure-pipelines-for-deployment) to build and test the different builds and deployments for the different environments.\\n* Azure Pipelines will be used to build and test the code for the different environments.\\n* Azure Pipelines will be used to run the tests for the different environments.\\n* Azure Pipelines will be used to run the CI\/CD pipeline for the different environments.\\n* Azure Pipelines will be used to run the CI\/CD pipeline for the different builds and deployments.\\n* Azure Pipelines will be used to run the build and deployment pipeline for the different environments and the CI\/CD pipeline for the different builds and deployments.\\n* Azure Pipelines will be used to run the CI\/CD pipeline for the different environments and the CI\/CD pipeline for the different builds and deployments.\\n* Azure Pipelines will be used to run the CI\/CD pipeline for the different builds and deployments and the CI\/CD pipeline for the different builds and deployments.\\n* Azure Pipelines will be used to run the CI\/CD pipeline for the different builds and deployments and the CI\/CD pipeline for the different"}
{"Context":"## Context\\nKeeping the tool simple, minimal, easy to install and understand.\\n","Decision":"## Decision\nDo not depend on any NPM package unless there's a strong reason to do that. Keep the zero dependence counter as much\\nas possible.\\n","Prediction":"## Decision\nUse only the minimum (most basic) tools necessary to perform the task. This will result in a smaller tool, and a faster tool.\\nUse the **Go** programming language for the majority of the tool.\\nUse **[Go Modules](https:\/\/golang.org\/doc\/)** for the **[API](https:\/\/golang.org\/doc\/api\/)**. This will result in a smaller tool, and a faster tool.\\nUse **[Go Test](https:\/\/golang.org\/doc\/)** to perform unit tests and ensure the code is well tested.\\nUse **[Go Test-Driven Development](https:\/\/golang.org\/doc\/tdd\/)** to ensure the code is well tested.\\nUse **[Go Test-Driven Infrastructure](https:\/\/golang.org\/doc\/tdi\/)** to ensure the code is well tested.\\nUse **[Go Test-Coverage](https:\/\/golang.org\/doc\/tdi\/coverage\/)** to ensure the code is well tested.\\nUse **[Go Test-Driven Design](https:\/\/golang.org\/doc\/tdi\/tdi\/)** to ensure the code is well tested.\\nUse **[Go Test-Driven Design-Style](https:\/\/golang.org\/doc"}
{"Context":"## Context and Problem Statement\\nThere is an ongoing project to upgrade Jira to version 8, that potentially may require some migration of workflows and other data. That triggers this discussion, as a good point in time to evaluate all available options.\\n","Decision":"## Decision\nChosen option: **Stay on Jira, revisit idea of moving to github if maintenance of two backlogs feels unmanageable.**\\n","Prediction":"## Decision\nWe will use [Anypoint Platform](https:\/\/www.anypoint.com\/platform\/) to migrate the existing workflows to. As part of the migration, the existing workflows are migrated to the new platform, and the existing workflows are then deprecated. This means that the workflows will be removed from Jira in the future, but will still be available in the Anypoint Platform, and will be migrated to the new platform automatically.\\nAnypoint Platform will be used to store the workflows, and will be the primary means of exporting the workflows to the new platform. The workflow will be stored in the Anypoint Platform, and will be kept in sync with the Jira server.\\nAnypoint Platform will be used as the primary means of exporting the workflows to the new platform, as the existing workflows are deprecated in Jira.\\nWe will use the [Anypoint Platform](https:\/\/www.anypoint.com\/platform\/) workflow upgrade tool to migrate the existing workflows to the new platform. The workflow upgrade tool will be used to migrate existing workflows to the new platform. The workflow upgrade tool will be used to migrate existing workflows to the new platform.\\nThe workflow upgrade tool will be used to migrate existing workflows to the new platform, and will"}
{"Context":"## Context\\nTo improve usability of the map it is required that we include the parcel boundaries as a default layer.\\nData BC currently provides mapping layers (which includes parcel boundaries).\\n- [openmaps.gov.bc.ca](https:\/\/www2.gov.bc.ca\/gov\/content\/data\/geographic-data-services\/web-based-mapping\/map-services)\\n","Decision":"## Decision\nAdd the **Data BC** openmaps parcel boundaries layer to the default map.\\n","Prediction":"## Decision\nWe will use the openmaps.gov.bc.ca map layer for the parcel boundaries.\\nThe parcel boundaries will be included as a default layer in the map.\\nThis will allow the user to easily toggle the parcel boundaries on and off.\\nIf the parcel boundaries are not included in the map then they will be hidden.\\nThis will allow the map to be used in a number of different ways.\\n- The parcel boundaries may be used as a layer in a number of different applications.\\n- The parcel boundaries may be included in a separate map layer in a different application.\\n- The parcel boundaries may be used as a layer in a number of other applications.\\n- The parcel boundaries may be included as a separate layer in a different application.\\n- The parcel boundaries may be included in a separate map layer in a different application.\\n- The parcel boundaries may be included in a separate map layer in a different application.\\n- The parcel boundaries may be included in a separate map layer in a different application.\\n- The parcel boundaries may be included in a separate map layer in a different application.\\n- The parcel boundaries may be included in a separate map layer in a different application.\\n- The parcel boundaries may be included in a separate map"}
{"Context":"**Context**\\nCode quality is very important and we need some way to ensure the code we are sending to production is clean code based on good practices and without bugs that can cause some unexpected behaviors. Sonar is a good option in our Integration server but how we can check our code in the local environment?\\n**Decision**\\nThere are two very famous plugins to help us. [Findbugs](http:\/\/findbugs.sourceforge.net\/) will scan the code and detect any possible bug that can cause an unexpected exception or some vulnerability in our code. Same way [Checkstyle](http:\/\/checkstyle.sourceforge.net\/) will help us to be aligned with the code standard depending on the platform, in this case Java.\\n**Status**\\nAccepted\\n**Consequences**\\nBoth plugins will be executed when the build command is executed. Currently Findbugs is activated to prevent build to be done if detects some code vulnerability or bug. It will generate a report in HTML code to analyze the point of failure. Checkstyle is configured to alert us about the code bad syntax, but will not stop the build.\\nThese two configurations can be modified on the `build.gradle` file.\\n","Decision":"## Decision\nThere are two very famous plugins to help us. [Findbugs](http:\/\/findbugs.sourceforge.net\/) will scan the code and detect any possible bug that can cause an unexpected exception or some vulnerability in our code. Same way [Checkstyle](http:\/\/checkstyle.sourceforge.net\/) will help us to be aligned with the code standard depending on the platform, in this case Java.\\n**Status**\\nAccepted\\n**Consequences**\\nBoth plugins will be executed when the build command is executed. Currently Findbugs is activated to prevent build to be done if detects some code vulnerability or bug. It will generate a report in HTML code to analyze the point of failure. Checkstyle is configured to alert us about the code bad syntax, but will not stop the build.\\nThese two configurations can be modified on the `build.gradle` file.\\n","Prediction":"## Decision\nThere are two very famous plugins to help us. [Findbugs](http:\/\/findbugs.sourceforge.net\/) will scan the code and detect any possible bug that can cause an unexpected exception or some vulnerability in our code. Same way [Checkstyle](http:\/\/checkstyle.sourceforge.net\/) will help us to be aligned with the code standard depending on the platform, in this case Java.\\n**Status**\\nAccepted\\n**Consequences**\\nBoth plugins will be executed when the build command is executed. Currently Findbugs is activated to prevent build to be done if detects some code vulnerability or bug. It will generate a report in HTML code to analyze the point of failure. Checkstyle is configured to alert us about the code bad syntax, but will not stop the build.\\nThese two configurations can be modified on the `build.gradle` file.\\nChosen option: \"SonarLint\", because it is a plugin that we have available in our project and it is a very well known tool for code quality analysis. It has a very good coverage of Java and it is very easy to use.\\n## Decision\nThere are two very famous plugins to help us. [Findbugs](http:\/\/findbugs.sourceforge.net\/) will scan the code and detect"}
{"Context":"## Context\\nThe kops template can be very useful to deploy clusters, but there are\\nsituations where the cluster must be installed in already provisioned\\nAWS resources.  Then, you can find lots of issues.\\nI have found an article about how to solve this issue by integrating\\nterraform with kops in a very clever way.  I'm referencing the article\\nhere for future ideas.\\n* [Deploying Kubernetes clusters with kops and Terraform](https:\/\/medium.com\/becnh-engineering\/deploying-kubernetes-clusters-with-kops-and-terraform-832b89250e8e)\\n","Decision":"## Decision\nI need to review and test the proposed solution before considerating it.\\n","Prediction":"## Decision\nWe will use Terraform to deploy kops to AWS.\\nWe will create a new deployment account for this project.\\nWe will create a new AWS account for this project.\\nWe will create a new Terraform module for this project.\\nWe will create a new Terraform provider for this project.\\nWe will create a new AWS account for this project.\\nWe will create a new AWS VPC for this project.\\nWe will create a new VPC for this project.\\nWe will create a new subnet for this project.\\nWe will create a new VPC gateway for this project.\\nWe will create a new VPC IP for this project.\\nWe will create a new security group for this project.\\nWe will create a new IAM policy for this project.\\nWe will create a new IAM role for this project.\\nWe will create a new secret for this project.\\nWe will create a new secret for this project.\\nWe will create a new IAM policy for this project.\\nWe will create a new IAM role for this project.\\nWe will create a new secret for this project.\\nWe will create a new function for this project.\\nWe will create a new function for this project.\\nWe"}
{"Context":"## Context\\nCurrently throughout PACE, different error identifiers are used to reference the same error.\\nThis makes it difficult in cases of specific try-catch (as described in [ADR 21](.\/0021-errors-and-warnings.md)) to know what to compare against.\\nExamples include:\\n```\\nherbert_core\/applications\/multifit\/@mfclass\/fit.m                                             : Herbert:mfclass:invalid_argument\\nherbert_core\/classes\/data_loaders\/@rundata\/private\/gen_runfiles_.m                            : GEN_GRUNFILES:invalid_arguments\\nherbert_core\/utilities\/general\/catstruct.m                                                    : catstruct:InvalidArgument\\nherbert_core\/classes\/data_loaders\/parse_arg.m                                                 : PARSE_ARG:wrong_arguments\\nherbert_core\/classes\/data_loaders\/@rundata\/get_par.m                                          : RUNDATA:invalid_argument\\nherbert_core\/classes\/data_loaders\/@rundata\/get_rundata.m                                      : RUNDATA:invalid_arguments\\nherbert_core\/utilities\/maths\/noisify.m                                                        : HERBERT:noisify\\nherbert_core\/utilities\/misc\/objdiff.m                                                         : YMA:OBJDIFF:NotEnoughInputs\\nherbert_core\/utilities\/xml_io_tools\/xmlwrite_xerces.m                                         : xml:FileNotFound\\nherbert_core\/admin\/matlab_nbits.m                                                             : MATLAB:NBITS\\n_test\/shared\/matlab_xunit_ISISextras\/@TestCaseWithSave\/private\/get_ref_dataset_.m             : TestCaseWithSave:invalid_argument\\n_test\/shared\/matlab_xunit_ISISextras\/@TestCaseWithSave\/private\/instantiate_methods_to_save_.m : TEST_CASE_WITH_SAVE:invalid_argument\\nerbert_core\/classes\/MPIFramework\/@iMessagesFramework\/iMessagesFramework.m                     : iMESSAGES_FRAMEWORK:invalid_argument\\nherbert_core\/classes\/MPIFramework\/@iMessagesFramework\/iMessagesFramework.m                    : MESSAGES_FRAMEWORK:invalid_argument\\nherbert_core\/classes\/MPIFramework\/@iMessagesFramework\/private\/mix_messages_.m                 : iMESSAGES_FRAMEWOR:invalid_argument\\n```\\nFrom which it should be obvious that this is not a consistent scheme for error identification, even within the same set of routines.\\nAs MATLAB standards do not enforce a particular style of error statement\\n(though it does encourage certain forms [see here](https:\/\/uk.mathworks.com\/help\/matlab\/ref\/mexception.html#mw_e5712c7f-3862-42fa-9a8f-8de992cdc6d4)),\\nthis document seeks to outline the format an error identifier should take as well as establish a list of standard common error identifiers.\\nAlso due to the fact that MATLAB has changed its method of error identification between versions in the past, this document establishes its scheme based on the most recent currently used version (2020b).\\n","Decision":"## Decision\nFollowing discussions it was decided that it would be useful to have a standard format with 3 components, in the following format\\n(expanding upon that initially outlined in [ADR 21](.\/0021-errors-and-warnings.md)).\\n```matlab\\nerror('(HORACE|HERBERT):(function_name|ClassName):error_identifier')\\n```\\nThis scheme is based on the current (2020b) MATLAB error identifiers common to many functions.\\n- `HORACE` or `HERBERT` should be in all caps.\\n- The `function_name` and `ClassName` should exactly match that of the parent function\/class, including casing.\\n- The `error_identifier` should be in `lower_snake` case and should succinctly identify the error message, using one of\\nthe following names if the error falls within their remit.\\n**NB.** This is in contrast to the usual MATLAB format which uses `lowerCamelCase`, but is in keeping with the majority\\nof identifiers currently used in PACE.\\n| Identifier         | Issue                                                                                         |\\n| :----------------- | :-------------------------------------------------------------------------------------------- |\\n| `invalid_argument` | Argument fails validation, there are insufficient arguments or argument is an unexpected flag |\\n| `invalid_output`   | Insufficient outputs                                                                          |\\n| `not_implemented`  | Called function is virtual\/abstract                                                           |\\n| `not_available`    | Method or function is not available on current system or current Horace\/Herbert configuration |\\n| `array_mismatch`   | Array dimensions are incompatible (will usually be identified by MATLAB's error)              |\\n| `file_not_found`   | File not found on system                                                                      |\\n| `io_error`         | Issues with opening, reading or writing files                                                 |\\n| `runtime_error`    | Unspecified issues, caused by running valid code under specific circumstances                 |\\n|                    | (e.g. caught from mex code and propagated to MATLAB)                                          |\\n| `parallel_error`   | Unspecified issues, caused by working with parallel cluster or MPI framework                  |\\n|                    | (e.g. caught from mex code and propagated to MATLAB)                                          |\\n| `system_error`     | Issues when calling system (shell) functions from MATLAB                                      |\\n","Prediction":"## Decision\nThis document will be used as a guideline for error identification in MATLAB. If in doubt, please consult the MATLAB documentation.\\nError identifiers will be written as a string of lowercase letters and numbers, separated by periods.\\nFor example:\\n```\\nErrorType: 'MissingParameter'                             : 'Here's the error message'                      : 'InvalidArgument'                              : 'InvalidParameter'                               : 'Argument is out of range'                           : 'Argument is out of the domain'                     : 'InvalidArgument'                               : 'InvalidArgument'                              : 'Argument is out of the domain'                     : 'Argument is out of the domain'                      : 'Argument is out of the domain'                     : 'Argument is out of the domain'                      : 'Argument is out of the domain'                      : 'Argument is out of the domain'                      : 'Argument is out of the domain'                      : 'Argument is out of the domain'                      : 'Argument is out of the domain'                      : 'Argument is out of the domain'                      : 'Argument is out of the domain'                      : 'Argument is out of the domain'                      : 'Argument is out of the domain'                      : 'Argument is out of the domain'                      : '"}
{"Context":"## Context\\nDfE needs to validate the teacher's identity and qualification before approving\\nthe claim.\\nGOVUK Verify is being depreciated ((ADR\\nNo. 0008)[https:\/\/github.com\/DFE-Digital\/claim-additional-payments-for-teaching\/blob\/master\/docs\/architecture-decisions\/0008-use-govuk-verify-for-claiment-identity-assurance.md])\\nand is being replaced with integration DqT to validate teacher's identity.\\n","Decision":"## Decision\nThe service will replace GOV verify with Database for Qualified Teachers\\n([DQT](https:\/\/teacherservices.education.gov.uk\/SelfService\/Login)) for\\nteacher's identity and QTS check before approving the claims.\\nDQT will provide a DQT dataset according to the policy on a daily basis and the\\nservice will validate the teacher's identity and QTS against the DQT via API.\\nThe claim service team will be responsible to manage the temporary DQT API and\\ndata until actual DQT API ready to server all requests.\\nThis DQT integration automation will also replace the manually verify claims\\nthrough DQT CSV report.\\n","Prediction":"## Decision\nUse DqT to validate teacher's identity and qualification.\\nUse DqT to validate the teacher's contact details.\\nUse DqT to validate the teacher's qualifications.\\nUse DqT to validate the teacher's employment details.\\nUse DqT to validate the teacher's training details.\\nUse DqT to validate the teacher's educational attainment.\\nUse DqT to validate the teacher's employment history.\\nUse DqT to validate the teacher's references.\\nUse DqT to validate the teacher's qualifications.\\nUse DqT to validate the teacher's employment history.\\nUse DqT to validate the teacher's references.\\nUse DqT to validate the teacher's employment history.\\nUse DqT to validate the teacher's employment history.\\nUse DqT to validate the teacher's employment history.\\nUse DqT to validate the teacher's employment history. It should be\\npossible to validate the teacher's employment history by means of a\\nvalidating API such as [DqT](https:\/\/github.com\/DFE-Digital\/claim-additional-payments-for-teaching\/tree\/master\/docs\/api).\\nUse DqT to validate the teacher's employment history.\\"}
{"Context":"## Context and Problem Statement\\nWe have to transfer json data and verify the integrity of the json data model.\\n[ADR-0002](0002-use-sha256-with-base64url-encoding.md) describes how to create a hash of the json.\\nThe hash must be transferred to from the authorization server to the WebSocket API secure.\\nThe validity of the hash must be verified.\\n## Decision Drivers <!-- optional -->\\n* JWT should be used\\n","Decision":"## Decision\n* JWT should be used\\nChosen option: \"Transfer hash in JWT Claim\", because it's the only option when using JWT.\\n### Positive Consequences <!-- optional -->\\n* multiple hashes for different json documents can be added in one JWT\\n","Prediction":"## Decision\n* JWT should be used\\nChosen option: \"JWT 2.0 with HMAC-SHA256 with HMAC-SHA512\", because\\n* JWT is more convenient to use for transfer and verification.\\n* JWT is more secure than base64url encoding\\n* JWT is more convenient to use for transfer and verification.\\n* JWT is more convenient to use for transfer and verification.\\n* JWT is more convenient to use for transfer and verification.\\n* JWT is more convenient to use for transfer and verification.\\n* JWT is more convenient to use for transfer and verification.\\n* JWT is more convenient to use for transfer and verification.\\n* JWT is more convenient to use for transfer and verification.\\n* JWT is more convenient to use for transfer and verification.\\n* JWT is more convenient to use for transfer and verification.\\n* JWT is more convenient to use for transfer and verification.\\n* JWT is more convenient to use for transfer and verification.\\n* JWT is more convenient to use for transfer and verification.\\n* JWT is more convenient to use for transfer and verification.\\n* JWT is more convenient to use for transfer and verification.\\n* JWT is more convenient to use for transfer and verification.\\n* JWT is more convenient"}
{"Context":"## Context\\nFacilitate organizing shell customizations into smaller, more readable, more understandable, more maintainable blocks and to provide a clear and common patter for extensibility.\\n","Decision":"## Decision\nCreate a $ZDOTDEEDIR directory where extension files can be stored. Add a rooutine to .zprofile or .zshrc which sources all files found in this directory in lexagraphical order by file name.\\n","Prediction":"## Decision\nThe shell will be organized into a set of smaller, more readable, more maintainable blocks.\\nEach block will be a file that is sourced in the shell's environment.\\nEach block will have a name, an author, and a description.\\nEach block will have a set of standard attributes (e.g. `--help`, `--version`) and a set of optional attributes.\\nEach block that has optional attributes will have an `--optional` attribute.\\nEach block that has optional attributes will have a `--help` and `--version` attribute.\\nEach block that has optional attributes will have a `--help` and `--version` attribute and a `--no-optional` attribute.\\nEach block that has optional attributes will have a `--help` and `--version` attribute and a `--no-optional` attribute and a `--help` and `--version` attribute and a `--no-optional` attribute.\\nEach block will use `--help` and `--version` to indicate which attributes are optional and which attributes are required.\\nEach block will use `--help` and `--version` to indicate which attributes are optional and which attributes are required.\\nEach block will"}
{"Context":"## Context\\nThe Kyma developers need to select specific technologies. Their purpose is to:\\n* Enable the most lightweight solution and cost-effective solution for the cluster installation.\\n* Facilitate the cross-teams collaboration development.\\n* Enable easy context-switching between teams.\\n","Decision":"## Decision\nThe decision is to use the **Go** language for all new implementations in Kyma. Go allows to create very efficient applications with low memory usage and a vast set of system libraries. Many projects which Kyma depends on are written in Go, including Kubernetes.\\nUse the following frontend technologies within Kyma:\\n* Open UI5\\n* Angular (version 4 and later)\\n* React\\nThe recommended technologies fulfill the Kyma principles.\\n","Prediction":"## Decision\nWe will use the following technologies: \\n* [Docker](https:\/\/www.docker.com\/)\\n* [Docker Swarm](https:\/\/docs.docker.com\/swarm\/)\\n* [Docker Compose](https:\/\/docs.docker.com\/compose\/)\\n* [Docker Networking](https:\/\/docs.docker.com\/network\/)\\n* [Docker Registry](https:\/\/docs.docker.com\/registry\/)\\n* [Docker Compose Deploy](https:\/\/docs.docker.com\/compose\/deploy\/)\\n* [Docker Compose Deploy Image Pull](https:\/\/docs.docker.com\/compose\/compose-images\/)\\n* [Docker Compose Deploy Image Push](https:\/\/docs.docker.com\/compose\/compose-images\/)\\n* [Docker Compose Create Image](https:\/\/docs.docker.com\/compose\/compose-images\/create\/)\\n* [Docker Compose Deploy Image](https:\/\/docs.docker.com\/compose\/compose-images\/deploy\/)\\n* [Docker Compose Deploy](https:\/\/docs.docker.com\/compose\/compose-deploy\/)\\n* [Docker Compose Deploy Network](https:\/\/docs.docker.com\/compose\/compose-network\/)\\n* [Docker Compose Deploy LoadBalancer]("}
{"Context":"## Problem\\nCurrently, an error or warning message in elektra causes the following line to be shown:\\n```\\nIngroup: <group>\\n```\\nIts main purpose is to show the user if the error occurred in either `kdb`, `module` or `plugin`.\\nThe problem is that this message is of no value for the user and increases the verbosity of the message.\\n","Decision":"## Decision\nThe `ingroup` message will be removed as it does not yield any notable benefit.\\nSee [Error concept](error_codes.md)\\n","Prediction":"## Decision\nWe will remove the `Ingroup: <group>` message from the error message.\\n[![Error message](error_message_1.png)](images\/1-error-message.png)\\nThis will help the user to better understand the problem and the solution.\\nThen the user will be able to see the `Ingroup: <group>` message in the error message.\\nThis will be useful to all users.\\nWe will also remove the `Ingroup: <group>` message from the `elektra_config` file.\\nThis will be useful to `elektra` users.\\nWe will also add a new command to the `elektra` configuration file to remove this message:\\n```bash\\nelektra: Ingroup: <group>\\n```\\nThis command will be run as a part of the `elektra` configuration file.\\nThis will be useful to `elektra` users.\\nWe will also add a new command to the `elektra` configuration file to show the error message in the `elektra` error message.\\n```bash\\nelektra: Error message: \\nIngroup: <group>\\n```\\nThis command will be run as"}
{"Context":"## Context\\nWe need a way to have a paper trail of items as they move throughout the system.\\nThe goal is to be able to look at the system at any point in time and, in\\ntheory, re-create all events that have happened so far. An individual item (ie.\\na single diaper) should probably not be represented as a discrete record in the\\ndatabase. This is because both diaperbanks move a lot of volume (hundreds of\\nthousands of units per year) and also because each individual item of inventory\\nis unimportant; only the quantities in aggregate are.\\n","Decision":"## Decision\nWe will use an associative model (\"LineItem\") to represent the contents of an\\nindividual transaction within an organization. All transactions involve Items\\nand Quantities. A separate model (\"InventoryItem\") will be used to track item\\ntotals within StorageLocations, since their purpose there is slightly different.\\n","Prediction":"## Decision\nWe will use the `Item` class as the base class for all of our unique items.\\nThis class will be extended by all of our other classes as necessary. For\\nexample, the `Item` class will be used to track the status of each\\ndiaperbank in the system.\\nWe will also create a `Durable` class that will be used to track all\\nof the data associated with each individual item (eg. the status, the\\nquantity, etc.).\\nWe will create a `Viewable` class that will allow us to display the\\ncontents of the database in a human-readable way (eg. the number of\\ninstances of each item and the total number of units in the\\nsystem).\\nWe will create a `Readable` class that will allow us to use the\\ndatabase to create a list of instances of an individual item.\\nWe will create a `Read` class that will allow us to use the database to\\nretrieve a single instance of an individual item.\\nWe will create a `Listable` class that will allow us to create a list of\\ninstances of an individual item (ie. the number of instances of each\\nitem in the system).\\nWe will create a"}
{"Context":"## Context\\nAll mailboxes implementations rely on `mailbox-store` module that defines some common tools to implement the `mailbox-api`\\n(representing the API defining how to use a mailbox).\\nFor example, a `CassandraMailboxmanager` has to extend `StoreMailboxManager` (that implements `Mailboxmanager` from the\\n`mailbox-api`) that requires the implementation of some `Mapper`s.\\n`Mapper`s are designed to provide low-level functions and methods on mailboxes. It's recurrent that we are tempted in\\nJames, outside of the `mailbox` modules, to rely on some of those common tools located in `mailbox-store` to have an\\neasier access on some user's mailboxes or messages.\\nLike for example, using a `Mapper` outside to be able to retrieve a message with only its `MessageId`, which is not\\ncurrently possible at the `Manager`'s level, which tends to violate `mailbox-api`'s role and primary mission.\\nAs a matter of fact, we have currently such uses of `mailbox-store` in James:\\n* `mailbox-adapter` because `Authenticator` and `Authorizator` are part of the `mailbox-store`\\nThe manager layer do further validation including right checking, event dispatching (resulting in message search index\\nindexing, current quota calculation mainly), quota validation. Not relying on the manager layer is thus error prone\\nand can lead to security vulnerabilities.\\n","Decision":"## Decision\nWe should never rely on classes defined in `mailbox-store` outside of the `mailbox` modules (except on some cases\\nlimited to the test scope). The right way would be to always rely on the `Manager`s defined in `mailbox-api` module to\\naccess mailboxes and messages, as the `mailbox-api` module defines the API on how to use a mailbox.\\nWe should ensure the correctness of `Manager`s implementations by providing contract tests and not by sharing abstract\\nclasses.\\nRegarding the modules wrongly relying already on `mailbox-store`, we can:\\n* `mailbox-adapter`: move `Authenticator` and `Authorizator` to `mailbox-api`\\n","Prediction":"## Decision\nWe will remove the `mailbox-api` from the `mailbox-api` module.\\nInstead, we will rely on the `mailbox-store` module to implement a `Manager` and to retrieve the mailboxes\\ndesired by a user (or a group of users).\\nWe will keep a `mailbox-api` implementation in the `mailbox-api` module to support the `mailbox-api`'s role and\\nprimary mission.\\nWe will remove the `mailbox-api` implementation from the `mailbox-api` module.\\nInstead, we will rely on the `mailbox-store` module to implement a `Manager` and to retrieve the mailboxes\\ndesired by a user (or a group of users).\\nWe will keep a `mailbox-api` implementation in the `mailbox-api` module.\\nInstead, we will rely on the `mailbox-store` module to implement a `Manager` and to retrieve the mailboxes\\ndesired by a user (or a group of users).\\nWe will keep a `mailbox-api` implementation in the `mailbox-api` module.\\nInstead, we will rely on the `mailbox-store` module to implement a `Manager` and to retrieve the mailboxes\\ndesired by a user (or"}
{"Context":"## Context\\nThe Schoenberg Institute won three-year NEH grant to rebuild the existing Schoenberg Database of Manuscripts to\\nreplace the then current SDBM which was written in ColdFusion, lacked critical data model features and user-based editing.\\nThe new application needed a more flexible, user contribution, complex search and an up-to-date technology stack.\\n","Decision":"## Decision\nThe following technologies were select for the following reasons.\\n- Blacklight, which uses Solr, was chosen to provide complex, configurable search, and uses technology common in the library, and for which long term product support could be expected. Blacklight was also used for bookmarking.\\n- Rails is required by Blacklight\\n- MySQL was selected as it was the database of choice within library technology services, and supported by sysadmin staff\\n- Solr is required by Blacklight\\n- Delayed Job (URL) was chosen as for orchestrating background jobs, namely sending updates from the database to the Solr index\\n- Sunspot was chosen for the Rails-Solr interaction\\n- AngularJS was chosen to support complex, dynamic page interactions\\n- cancancan was selected to provide tiered, role-based access for users with different permission levels\\n","Prediction":"## Decision\nWe will use MongoDB, RDS, ElasticSearch, and S3 as the storage layer.  We will work with the existing SDBM code base.\\nWe will use [MongoDB Streams](https:\/\/www.mongodb.com\/stream\/) for event ingestion and streaming.\\nWe will use [MongoDB Streams](https:\/\/www.mongodb.com\/stream\/) as the storage layer for the search index and request\/response\\npatterns.\\nWe will use [MongoDB Streams](https:\/\/www.mongodb.com\/stream\/) as the primary data store for the application.\\nWe will use [MongoDB Streams](https:\/\/www.mongodb.com\/stream\/) as the primary data store for the application.\\nWe will use [MongoDB Streams](https:\/\/www.mongodb.com\/stream\/) as the primary data store for the application.\\nWe will use [MongoDB Streams](https:\/\/www.mongodb.com\/stream\/) as the primary data store for the application.\\nWe will use [MongoDB Streams](https:\/\/www.mongodb.com\/stream\/) as the primary data store for the application.\\nWe will use [MongoDB Streams](https:\/\/www.mongodb.com\/stream\/) as the primary data store for the application.\\nWe will use [MongoDB Streams](https:\/\/www.mongodb.com\/stream\/) as the primary"}
{"Context":"## Context\\nWhilst most created evidence of malicious behavior is self evident such that any individual can verify them independently there are types of evidence, known collectively as global evidence, that require further collaboration from the network in order to accumulate enough information to create evidence that is individually verifiable and can therefore be processed through consensus. [Fork Accountability](https:\/\/github.com\/tendermint\/spec\/blob\/master\/spec\/consensus\/light-client\/accountability.md) has been coined to describe the entire process of detection, proving and punishing of malicious behavior. This ADR addresses specifically what a light client amnesia attack is and how it can be proven and the current decision around handling light client amnesia attacks. For information on evidence handling by the light client, it is recommended to read [ADR 47](https:\/\/github.com\/tendermint\/tendermint\/blob\/master\/docs\/architecture\/adr-047-handling-evidence-from-light-client.md).\\n### Amnesia Attack\\nThe schematic below explains a scenario where an amnesia attack can occur such that two sets of honest nodes, C1 and C2, commit different blocks.\\n![](..\/imgs\/tm-amnesia-attack.png)\\n1. C1 and F send PREVOTE messages for block A.\\n2. C1 sends PRECOMMIT for round 1 for block A.\\n3. A new round is started, C2 and F send PREVOTE messages for a different block B.\\n4. C2 and F then send PRECOMMIT messages for block B.\\n5. F later on creates PRECOMMITS for block A and combines it with those from C1 to form a block\\nThis forged block can then be used to fool light clients trying to verify it. It must be stressed that there are a few more hurdles or dimensions to the attack to consider.For a more detailed walkthrough refer to Appendix A.\\n","Decision":"## Decision\nThe decision surrounding amnesia attacks has both a short term and long term component. In the long term, a more sturdy protocol will need to be fleshed out and implemented. There is already draft documents outlining what such a protocol would look like and the resources it would require (see references). Prior revisions however outlined a protocol which had been implemented (See Appendix B). It was agreed that it still required greater consideration and review given it's importance. It was therefore discussed, with the limited time frame set before 0.34, whether the protocol should be completely removed or if there should remain some logic in handling the aforementioned scenarios.\\nThe latter of the two options meant storing a record of all votes in any height with which there was more than one round. This information would then be accessible for applications if they wanted to perform some off-chain verification and punishment.\\nIn summary, this seemed like too much to ask of the application to implement only on a temporary basis, whilst not having the domain specific knowledge and considering such a difficult and unlikely attack. Therefore the short term decision is to identify when the attack has occurred and implement the detector algorithm highlighted in [ADR 47](https:\/\/github.com\/tendermint\/tendermint\/blob\/master\/docs\/architecture\/adr-047-handling-evidence-from-light-client.md) but to not implement any accountability protocol that would identify malicious validators and allow applications to punish them. This will hopefully change in the long term with the focus on eventually reaching a concrete and secure protocol with identifying and dealing with these attacks.\\n","Prediction":"## Decision\nTo prevent this attack from being able to be used in the current consensus protocol we have decided to implement an amnesia attack mitigation protocol.\\nThis mitigation protocol requires the following steps:\\n1. C1 and C2 send PREVOTE messages for block A.\\n2. C1 and F send PREVOTE messages for block A.\\n3. C2 and F then send PRECOMMIT for block B.\\n4. C2 and F then send PRECOMMIT for block A.\\n5. F later on creates PRECOMMITS for block A and combines it with those from C1 to form a block\\nThis will then be sent to all light clients and they will be able to verify that the block they are trying to verify is the one that is being held by the C1 and C2 light clients. If a light client that has already been affected by this attack is unable to verify that block A is the one that is being held by C1 and C2 then it will be considered a false positive and the attacker will be prohibited from forging more blocks.\\nThis mitigation protocol is not mandatory however if it is not implemented then it will result in a false positive being raised. This will cause the light client to be prevented from"}
{"Context":"## Context\\nThe proposed trust metric will allow Tendermint to maintain local trust rankings for peers it has directly interacted with, which can then be used to implement soft security controls. The calculations were obtained from the [TrustGuard](https:\/\/dl.acm.org\/citation.cfm?id=1060808) project.\\n### Background\\nThe Tendermint Core project developers would like to improve Tendermint security and reliability by keeping track of the level of trustworthiness peers have demonstrated within the peer-to-peer network. This way, undesirable outcomes from peers will not immediately result in them being dropped from the network (potentially causing drastic changes to take place). Instead, peers behavior can be monitored with appropriate metrics and be removed from the network once Tendermint Core is certain the peer is a threat. For example, when the PEXReactor makes a request for peers network addresses from a already known peer, and the returned network addresses are unreachable, this untrustworthy behavior should be tracked. Returning a few bad network addresses probably shouldn\u2019t cause a peer to be dropped, while excessive amounts of this behavior does qualify the peer being dropped.\\nTrust metrics can be circumvented by malicious nodes through the use of strategic oscillation techniques, which adapts the malicious node\u2019s behavior pattern in order to maximize its goals. For instance, if the malicious node learns that the time interval of the Tendermint trust metric is _X_ hours, then it could wait _X_ hours in-between malicious activities. We could try to combat this issue by increasing the interval length, yet this will make the system less adaptive to recent events.\\nInstead, having shorter intervals, but keeping a history of interval values, will give our metric the flexibility needed in order to keep the network stable, while also making it resilient against a strategic malicious node in the Tendermint peer-to-peer network. Also, the metric can access trust data over a rather long period of time while not greatly increasing its history size by aggregating older history values over a larger number of intervals, and at the same time, maintain great precision for the recent intervals. This approach is referred to as fading memories, and closely resembles the way human beings remember their experiences. The trade-off to using history data is that the interval values should be preserved in-between executions of the node.\\n### References\\nS. Mudhakar, L. Xiong, and L. Liu, \u201cTrustGuard: Countering Vulnerabilities in Reputation Management for Decentralized Overlay Networks,\u201d in _Proceedings of the 14th international conference on World Wide Web, pp. 422-431_, May 2005.\\n","Decision":"## Decision\nThe proposed trust metric will allow a developer to inform the trust metric store of all good and bad events relevant to a peer's behavior, and at any time, the metric can be queried for a peer's current trust ranking.\\nThe three subsections below will cover the process being considered for calculating the trust ranking, the concept of the trust metric store, and the interface for the trust metric.\\n### Proposed Process\\nThe proposed trust metric will count good and bad events relevant to the object, and calculate the percent of counters that are good over an interval with a predefined duration. This is the procedure that will continue for the life of the trust metric. When the trust metric is queried for the current **trust value**, a resilient equation will be utilized to perform the calculation.\\nThe equation being proposed resembles a Proportional-Integral-Derivative (PID) controller used in control systems. The proportional component allows us to be sensitive to the value of the most recent interval, while the integral component allows us to incorporate trust values stored in the history data, and the derivative component allows us to give weight to sudden changes in the behavior of a peer. We compute the trust value of a peer in interval i based on its current trust ranking, its trust rating history prior to interval _i_ (over the past _maxH_ number of intervals) and its trust ranking fluctuation. We will break up the equation into the three components.\\n```math\\n(1) Proportional Value = a * R[i]\\n```\\nwhere _R_[*i*] denotes the raw trust value at time interval _i_ (where _i_ == 0 being current time) and _a_ is the weight applied to the contribution of the current reports. The next component of our equation uses a weighted sum over the last _maxH_ intervals to calculate the history value for time _i_:\\n`H[i] =` ![formula1](img\/formula1.png \"Weighted Sum Formula\")\\nThe weights can be chosen either optimistically or pessimistically. An optimistic weight creates larger weights for newer history data values, while the the pessimistic weight creates larger weights for time intervals with lower scores. The default weights used during the calculation of the history value are optimistic and calculated as _Wk_ = 0.8^_k_, for time interval _k_. With the history value available, we can now finish calculating the integral value:\\n```math\\n(2) Integral Value = b * H[i]\\n```\\nWhere _H_[*i*] denotes the history value at time interval _i_ and _b_ is the weight applied to the contribution of past performance for the object being measured. The derivative component will be calculated as follows:\\n```math\\nD[i] = R[i] \u2013 H[i]\\n(3) Derivative Value = c(D[i]) * D[i]\\n```\\nWhere the value of _c_ is selected based on the _D_[*i*] value relative to zero. The default selection process makes _c_ equal to 0 unless _D_[*i*] is a negative value, in which case c is equal to 1. The result is that the maximum penalty is applied when current behavior is lower than previously experienced behavior. If the current behavior is better than the previously experienced behavior, then the Derivative Value has no impact on the trust value. With the three components brought together, our trust value equation is calculated as follows:\\n```math\\nTrustValue[i] = a * R[i] + b * H[i] + c(D[i]) * D[i]\\n```\\nAs a performance optimization that will keep the amount of raw interval data being saved to a reasonable size of _m_, while allowing us to represent 2^_m_ - 1 history intervals, we can employ the fading memories technique that will trade space and time complexity for the precision of the history data values by summarizing larger quantities of less recent values. While our equation above attempts to access up to _maxH_ (which can be 2^_m_ - 1), we will map those requests down to _m_ values using equation 4 below:\\n```math\\n(4) j = index, where index > 0\\n```\\nWhere _j_ is one of _(0, 1, 2, \u2026 , m \u2013 1)_ indices used to access history interval data. Now we can access the raw intervals using the following calculations:\\n```math\\nR[0] = raw data for current time interval\\n```\\n`R[j] =` ![formula2](img\/formula2.png \"Fading Memories Formula\")\\n### Trust Metric Store\\nSimilar to the P2P subsystem AddrBook, the trust metric store will maintain information relevant to Tendermint peers. Additionally, the trust metric store will ensure that trust metrics will only be active for peers that a node is currently and directly engaged with.\\nReactors will provide a peer key to the trust metric store in order to retrieve the associated trust metric. The trust metric can then record new positive and negative events experienced by the reactor, as well as provided the current trust score calculated by the metric.\\nWhen the node is shutting down, the trust metric store will save history data for trust metrics associated with all known peers. This saved information allows experiences with a peer to be preserved across node executions, which can span a tracking windows of days or weeks. The trust history data is loaded automatically during OnStart.\\n### Interface Detailed Design\\nEach trust metric allows for the recording of positive\/negative events, querying the current trust value\/score, and the stopping\/pausing of tracking over time intervals. This can be seen below:\\n```go\\n\/\/ TrustMetric - keeps track of peer reliability\\ntype TrustMetric struct {\\n\/\/ Private elements.\\n}\\n\/\/ Pause tells the metric to pause recording data over time intervals.\\n\/\/ All method calls that indicate events will unpause the metric\\nfunc (tm *TrustMetric) Pause() {}\\n\/\/ Stop tells the metric to stop recording data over time intervals\\nfunc (tm *TrustMetric) Stop() {}\\n\/\/ BadEvents indicates that an undesirable event(s) took place\\nfunc (tm *TrustMetric) BadEvents(num int) {}\\n\/\/ GoodEvents indicates that a desirable event(s) took place\\nfunc (tm *TrustMetric) GoodEvents(num int) {}\\n\/\/ TrustValue gets the dependable trust value; always between 0 and 1\\nfunc (tm *TrustMetric) TrustValue() float64 {}\\n\/\/ TrustScore gets a score based on the trust value always between 0 and 100\\nfunc (tm *TrustMetric) TrustScore() int {}\\n\/\/ NewMetric returns a trust metric with the default configuration\\nfunc NewMetric() *TrustMetric {}\\n\/\/------------------------------------------------------------------------------------------------\\n\/\/ For example\\ntm := NewMetric()\\ntm.BadEvents(1)\\nscore := tm.TrustScore()\\ntm.Stop()\\n```\\nSome of the trust metric parameters can be configured. The weight values should probably be left alone in more cases, yet the time durations for the tracking window and individual time interval should be considered.\\n```go\\n\/\/ TrustMetricConfig - Configures the weight functions and time intervals for the metric\\ntype TrustMetricConfig struct {\\n\/\/ Determines the percentage given to current behavior\\nProportionalWeight float64\\n\/\/ Determines the percentage given to prior behavior\\nIntegralWeight float64\\n\/\/ The window of time that the trust metric will track events across.\\n\/\/ This can be set to cover many days without issue\\nTrackingWindow time.Duration\\n\/\/ Each interval should be short for adapability.\\n\/\/ Less than 30 seconds is too sensitive,\\n\/\/ and greater than 5 minutes will make the metric numb\\nIntervalLength time.Duration\\n}\\n\/\/ DefaultConfig returns a config with values that have been tested and produce desirable results\\nfunc DefaultConfig() TrustMetricConfig {}\\n\/\/ NewMetricWithConfig returns a trust metric with a custom configuration\\nfunc NewMetricWithConfig(tmc TrustMetricConfig) *TrustMetric {}\\n\/\/------------------------------------------------------------------------------------------------\\n\/\/ For example\\nconfig := TrustMetricConfig{\\nTrackingWindow: time.Minute * 60 * 24, \/\/ one day\\nIntervalLength:    time.Minute * 2,\\n}\\ntm := NewMetricWithConfig(config)\\ntm.BadEvents(10)\\ntm.Pause()\\ntm.GoodEvents(1) \/\/ becomes active again\\n```\\nA trust metric store should be created with a DB that has persistent storage so it can save history data across node executions. All trust metrics instantiated by the store will be created with the provided TrustMetricConfig configuration.\\nWhen you attempt to fetch the trust metric for a peer, and an entry does not exist in the trust metric store, a new metric is automatically created and the entry made within the store.\\nIn additional to the fetching method, GetPeerTrustMetric, the trust metric store provides a method to call when a peer has disconnected from the node. This is so the metric can be paused (history data will not be saved) for periods of time when the node is not having direct experiences with the peer.\\n```go\\n\/\/ TrustMetricStore - Manages all trust metrics for peers\\ntype TrustMetricStore struct {\\ncmn.BaseService\\n\/\/ Private elements\\n}\\n\/\/ OnStart implements Service\\nfunc (tms *TrustMetricStore) OnStart() error {}\\n\/\/ OnStop implements Service\\nfunc (tms *TrustMetricStore) OnStop() {}\\n\/\/ NewTrustMetricStore returns a store that saves data to the DB\\n\/\/ and uses the config when creating new trust metrics\\nfunc NewTrustMetricStore(db dbm.DB, tmc TrustMetricConfig) *TrustMetricStore {}\\n\/\/ Size returns the number of entries in the trust metric store\\nfunc (tms *TrustMetricStore) Size() int {}\\n\/\/ GetPeerTrustMetric returns a trust metric by peer key\\nfunc (tms *TrustMetricStore) GetPeerTrustMetric(key string) *TrustMetric {}\\n\/\/ PeerDisconnected pauses the trust metric associated with the peer identified by the key\\nfunc (tms *TrustMetricStore) PeerDisconnected(key string) {}\\n\/\/------------------------------------------------------------------------------------------------\\n\/\/ For example\\ndb := dbm.NewDB(\"trusthistory\", \"goleveldb\", dirPathStr)\\ntms := NewTrustMetricStore(db, DefaultConfig())\\ntm := tms.GetPeerTrustMetric(key)\\ntm.BadEvents(1)\\ntms.PeerDisconnected(key)\\n```\\n","Prediction":"## Decision\nWe will implement the TrustGuard project's Trust Metric in Tendermint Core. We will use the TrustGuard project's `trust_metrics` attribute to store the TrustGuard metric in the Tendermint Core database. The `trust_metrics` attribute will be used to store the TrustGuard metric for peers who have interacted with Tendermint Core in the past. Additionally, the `trust_metrics` attribute will be used to store the TrustGuard metric for peers who have not interacted with Tendermint Core in the past. We will use the `trust_metrics` attribute to determine whether a peer is trustworthy.\\n### Implementation\\n### Peer-to-peer\\n### TrustGuard\\nWe will use the `trust_metrics` attribute to store the TrustGuard metric in the peer-to-peer database. The `trust_metrics` attribute will be used to store the TrustGuard metric for peers who have interacted with Tendermint Core in the past. Additionally, the `trust_metrics` attribute will be used to store the TrustGuard metric for peers who have not interacted with Tendermint Core in the past. We will use the `trust_metrics` attribute to determine whether a peer is trustworthy.\\n### TrustGuard configuration\\n### TrustGuard configuration file\\n### TrustGuard configuration file configuration\\n### Trust"}
{"Context":"## Context\\nWe want to be able to distribute this library to me ingested by TypeScript or\\nplain JavaScript (both commonJS and module) applications.\\n[Rollup](https:\/\/rollupjs.org\/guide\/en\/) is a popular JavaScript bundler with\\nsupport for TypeScript and simple configuration.\\n","Decision":"## Decision\nWe will build distributables using Rollup.js.\\n","Prediction":"## Decision\nWe will use Rollup to bundle our application.\\nWe will use Rollup to bundle our dependencies.\\nWe will use Rollup to bundle our dependencies.\\nWe will use Rollup to bundle our dependencies.\\nWe will use Rollup to bundle our dependencies.\\nWe will use Rollup to bundle our dependencies.\\nWe will use Rollup to bundle our dependencies.\\nWe will use Rollup to bundle our dependencies.\\nWe will use Rollup to bundle our dependencies.\\nWe will use Rollup to bundle our dependencies.\\nWe will use Rollup to bundle our dependencies.\\nWe will use Rollup to bundle our dependencies.\\nWe will use Rollup to bundle our dependencies.\\nWe will use Rollup to bundle our dependencies.\\nWe will use Rollup to bundle our dependencies.\\nWe will use Rollup to bundle our dependencies.\\nWe will use Rollup to bundle our dependencies.\\nWe will use Rollup to bundle our dependencies.\\nWe will use Rollup to bundle our dependencies.\\nWe will use Rollup to bundle our dependencies.\\nWe will use Rollup to bundle our dependencies.\\nWe will use Rollup to bundle our dependencies.\\nWe will use Rollup to bundle our dependencies.\\n"}
{"Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","Prediction":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\nDecision records are short, bulleted lists of decisions made during a project's life cycle. They are useful for managing change and for making sure that everyone understands the decisions made.\\nWe will use [AltConf Architecture Library](https:\/\/github.com\/altconf\/altconf-architecture-library) for storing and retrieving our records.\\nWe will use [Radar](https:\/\/github.com\/bradleyhill\/radar) for displaying our ADRs in a format that we can easily read and search.\\nWe will use [OpenAPI](https:\/\/github.com\/openapipublishing\/openapi) for describing our architecture.\\nWe will use [Swagger](https:\/\/swagger.io\/) for generating documentation from our ADRs.\\nWe will use [Airflow](https:\/\/airflow.apache.org\/) for scheduling our ADRs.\\nWe will use [Ansible](https:\/\/www.ansible.com\/) for provisioning our infrastructure.\\nWe will use [RavenDB](https:\/\/ravendb.com\/) as our database.\\nWe"}
{"Context":"## Context\\nThere are a number of ways an API could allow clients to upload files to S3, the popular ones:\\n- Allow the API to accept Base 64 encoded files in a JSON POST request and subsequently send this blob to S3\\n- Allow the API to accept multipart form uploads, compile the parts on the server then send the file to S3\\n- Use the S3 Presigned URL functionality, which allows the client to act as the IAM which created the URL for a single operation, and upload the file directly to S3 themselves\\n","Decision":"## Decision\nWe decided to use the first option (base 64 encoded uploads), for the following reason:\\n- We do not want to expose any AWS links to outside parties. This ensures that all access to AWS resources is controlled by Hackney authentication mechanisms.\\n","Prediction":"## Decision\nWe will use the multipart form upload capability of the S3 API for uploading files to S3\\nWe will use the S3 Presigned URL functionality for uploading files to S3\\nWe will use the S3 REST API to retrieve the uploaded file and upload it to S3\\nWe will use the S3 SSM to provision the IAM role which will allow the created IAM user to upload files to S3\\nWe will use the S3 API Gateway to provision the S3 bucket\\nWe will use the S3 API Gateway to provision the S3 presigned URL\\nWe will use the S3 API Gateway to provision the S3 IAM user\\nWe will use the S3 API Gateway to provision the S3 IAM group\\nWe will use the S3 API Gateway to provision the S3 IAM policy\\nWe will use the S3 API Gateway to provision the S3 IAM user and IAM group\\nWe will use the S3 API Gateway to provision the S3 IAM policy\\nWe will use the S3 API Gateway to provision the S3 IAM user\\nWe will use the S3 API Gateway to provision the S3 IAM group\\nWe will use the S3 API Gateway to provision the S3 IAM user\\nWe will use the S3"}
{"Context":"## Context and Problem Statement\\nThe current reference implementation will ignore unrecognized fields in a\\nmetadata file when loading it.\\nThis leads to the side effect that if you read a metadata file with unrecognized\\nfields and immediately write it back to the disk, this file will be modified.\\nFurthermore, some TAPs like:\\n- [TAP 6](https:\/\/github.com\/theupdateframework\/taps\/blob\/master\/tap6.md)\\n- [TAP 10](https:\/\/github.com\/theupdateframework\/taps\/blob\/master\/tap10.md)\\n- [TAP 14](https:\/\/github.com\/theupdateframework\/taps\/blob\/master\/tap14.md)\\n- [TAP 15](https:\/\/github.com\/theupdateframework\/taps\/blob\/master\/tap15.md)\\n- [TAP 16](https:\/\/github.com\/theupdateframework\/taps\/blob\/master\/tap16.md)\\nare relying on that unrecognized fields will be accepted to introduce new fields\\nto the specification without making the metadata invalid for older clients who\\ndon't recognize the field.\\n## Decision Drivers\\n- The TUF specification implies support for unrecognized attribute-value fields,\\nsee [Document formats](https:\/\/theupdateframework.github.io\/specification\/latest\/#document-formats)\\n- If we perform the following operations on a metadata file with no\\nintermediate operations:\\n1. read the metadata file\\n2. write the metadata file back to the disk\\nthen, the checksum (the content) of the file must not be changed.\\n- Flexibility to add new fields in the spec without adding breaking changes.\\n- Don't store unrecognized fields when it is not allowed by the specification.\\n","Decision":"## Decision\n- The TUF specification implies support for unrecognized attribute-value fields,\\nsee [Document formats](https:\/\/theupdateframework.github.io\/specification\/latest\/#document-formats)\\n- If we perform the following operations on a metadata file with no\\nintermediate operations:\\n1. read the metadata file\\n2. write the metadata file back to the disk\\nthen, the checksum (the content) of the file must not be changed.\\n- Flexibility to add new fields in the spec without adding breaking changes.\\n- Don't store unrecognized fields when it is not allowed by the specification.\\nChosen option: \"Ignore, but store unrecognized fields as an additional attribute\\nexcept for a couple of places where it's not allowed by the specification.\"\\nThe motivation for this decision is that the TUF specification already implies\\nthat we should accept unrecognized fields for backward compatibility and easier\\nfuture extensibility.\\nAdditionally, it seems unacceptable to change a metadata file content just by\\nreading and writing it back.\\nThere are exceptions however for places in the metadata format when it is not\\nallowed by specification: keys, roles, meta, hashes, and targets are\\nactual dictionaries (vs JSON objects that most structures in the format are)\\nwhere `unrecognized field` is not a meaningful concept.\\n","Prediction":"## Decision\n- The TUF specification implies support for unrecognized attribute-value fields,\\nsee [Document formats](https:\/\/theupdateframework.github.io\/specification\/latest\/#document-formats)\\n- If we perform the following operations on a metadata file with no\\nintermediate operations:\\n1. read the metadata file\\n2. write the metadata file back to the disk\\nthen, the checksum (the content) of the file must not be changed.\\n- Flexibility to add new fields in the spec without adding breaking changes.\\n- Don't store unrecognized fields when it is not allowed by the specification.\\nChosen option: \"Don't store unrecognized fields when it is not allowed by the\\nspecification\", because it is the most flexible option.\\n### Positive Consequences\\n- The TUF specification implies support for unrecognized attribute-value fields,\\nsee [Document formats](https:\/\/theupdateframework.github.io\/specification\/latest\/#document-formats)\\n- If we perform the following operations on a metadata file with no\\nintermediate operations:\\n1. read the metadata file\\n2. write the metadata file back to the disk\\nthen, the checksum (the content) of the file must not be changed.\\n- Flexibility to add new fields in"}
{"Context":"## Context\\nWe get contributed a lot of new integrations, new features to integrations and refactors of integrations. The Home Assistant project is honored to receive so many great contributions to our project!\\nUnfortunately, as a contributor, adding oneself as (the, or one of the) code owners of the integration contributed or contributed to, doesn't always happen spontaneously.\\nNot adding oneself as a code owner has drawbacks for the project:\\n- The contributor doesn't \"own\" (in terms of taking responsibility) his code, and thus contribution, in a more formal fashion.\\n- Without being listed as a code owner, our GitHub bot will not notify the contributor, when an issue for the integration is reported, quite possibly affecting his contribution.\\n- Integrations have ended up or may end up with having a single code owner or no code owners at all.\\nAs a result of this:\\n- Bugs are less likely to be resolved in a timely fashion (turn-around time).\\n- Integrations are more prone to break in the future.\\n- Integration with a single code owner:\\n- Do not benefit from multiple code owners being familiar with the integration in terms of code review and general turn-around time.\\n- Become largely unmaintained when the single listed code owner can no longer contribute to the project.\\nWe have quite a few integrations that haven't got multiple code owners or don't have a code owner.\\nDuring the design discussion of this ADR, it also became clear, that the term \"code owner\" has different meanings to our members and contributors. Some interpret it as an honorable mention of contribution; others see it as \"taking responsibility\".\\n","Decision":"## Decision\nCode ownership for an integration defined:\\nThe willingness of a contributor to try, at best effort, to maintain the integration. Providing the intention for handling issues, providing bug fixes, or other contributions to the integration one is listed on as a code owner.\\n### Rules\\nIn order to support having (multiple) code owners for integration, to raise the quality and interaction on integration in our codebase, we have a set of rules (exceptions are in the next chapter).\\nFor the following cases, adding oneself as a code owner is required:\\n- When contributing a new integration.\\n- When contributing a new platform to an integration.\\n- When contributing a new feature to an integration.\\n- When contributing a significant refactor or rewrite of an integration.\\nContributions to our integrations, in the above-listed scopes, without having the contributor listed or added as the code owner, is no longer accepted.\\n### Exceptions\\nSome exceptions are in place, to prevent contributors to become demotivated to contribute; and are mainly based around smaller, low-impact contributions.\\nIn the following cases, code ownership may be omitted:\\n- Contributions that solely provides a bug fix(es).\\n- Contributions that only provide additional unit test(s).\\n- Contributions to integrations marked as \"internal\". These integrations are code owned by the Home Assistant core team.\\n- Contributions refactoring across multiple integrations, caused by changes to our core codebase. E.g., due to changes to the used platforms.\\n- Small or low impact contributions to an integration. A currently active code owner for the integration or a Home Assistant code reviewer can decide it may be omitted.\\n- The contributor pro-actively rejects to be listed as a code owner; however, a currently active code owner is willing to accept and take code ownership for the contribution provided by the contributor.\\nCode owner(s) and Home Assistant code reviewers are encouraged to ask a contributor to join an integration code ownership, even when the contribution matches one of the exceptions above.\\n### Withdrawing as a code owner\\nWithdrawing code ownership can happen, and it is sad to see an active code owner leaving the project.\\nA code owner may open up a PR to remove oneself as a code owner. However, this should only be accepted after the last contribution to the integration, made by the contributor, is released in a stable Home Assistant release.\\n","Prediction":"## Decision\nWe will add a new property to the [Home Assistant integration](https:\/\/github.com\/home-assistant\/home-assistant\/commits\/master) to indicate the code owner of the integration.\\nThe new property will be a string, consisting of the following form:\\n`'owner' => 'my-own-name'`\\nFor example:\\n`'owner' => 'my-own-name' => 'my-own-name'`\\nWe will add this property to the [Home Assistant integration](https:\/\/github.com\/home-assistant\/home-assistant\/commits\/master) and update the [Home Assistant configuration](https:\/\/github.com\/home-assistant\/home-assistant\/blob\/develop\/requirements.txt)\\nfile to use it.\\nA code owner can only have one `owner` property, and it must be a string.\\nIf the name of the code owner is not a valid string, the integration will fail to start.\\nIf a code owner is already known to Home Assistant, the integration will fail to start.\\nIf the `owner` property is set to `unknown`, the integration will silently fail to start.\\nIf the `owner` property is set to `unknown` and the `homeassistant` package is installed"}
{"Context":"## Context\\nThe responsibility for signaling and acting upon peer behaviour lacks a single\\nowning component and is heavily coupled with the network stack[<sup>1<\/sup>](#references). Reactors\\nmaintain a reference to the `p2p.Switch` which they use to call\\n`switch.StopPeerForError(...)` when a peer misbehaves and\\n`switch.MarkAsGood(...)` when a peer contributes in some meaningful way.\\nWhile the switch handles `StopPeerForError` internally, the `MarkAsGood`\\nmethod delegates to another component, `p2p.AddrBook`. This scheme of delegation\\nacross Switch obscures the responsibility for handling peer behaviour\\nand ties up the reactors in a larger dependency graph when testing.\\n","Decision":"## Decision\nIntroduce a `PeerBehaviour` interface and concrete implementations which\\nprovide methods for reactors to signal peer behaviour without direct\\ncoupling `p2p.Switch`.  Introduce a ErrorBehaviourPeer to provide\\nconcrete reasons for stopping peers. Introduce GoodBehaviourPeer to provide\\nconcrete ways in which a peer contributes.\\n### Implementation Changes\\nPeerBehaviour then becomes an interface for signaling peer errors as well\\nas for marking peers as `good`.\\n```go\\ntype PeerBehaviour interface {\\nBehaved(peer Peer, reason GoodBehaviourPeer)\\nErrored(peer Peer, reason ErrorBehaviourPeer)\\n}\\n```\\nInstead of signaling peers to stop with arbitrary reasons:\\n`reason interface{}`\\nWe introduce a concrete error type ErrorBehaviourPeer:\\n```go\\ntype ErrorBehaviourPeer int\\nconst (\\nErrorBehaviourUnknown = iota\\nErrorBehaviourBadMessage\\nErrorBehaviourMessageOutofOrder\\n...\\n)\\n```\\nTo provide additional information on the ways a peer contributed, we introduce\\nthe GoodBehaviourPeer type.\\n```go\\ntype GoodBehaviourPeer int\\nconst (\\nGoodBehaviourVote = iota\\nGoodBehaviourBlockPart\\n...\\n)\\n```\\nAs a first iteration we provide a concrete implementation which wraps\\nthe switch:\\n```go\\ntype SwitchedPeerBehaviour struct {\\nsw *Switch\\n}\\nfunc (spb *SwitchedPeerBehaviour) Errored(peer Peer, reason ErrorBehaviourPeer) {\\nspb.sw.StopPeerForError(peer, reason)\\n}\\nfunc (spb *SwitchedPeerBehaviour) Behaved(peer Peer, reason GoodBehaviourPeer) {\\nspb.sw.MarkPeerAsGood(peer)\\n}\\nfunc NewSwitchedPeerBehaviour(sw *Switch) *SwitchedPeerBehaviour {\\nreturn &SwitchedPeerBehaviour{\\nsw: sw,\\n}\\n}\\n```\\nReactors, which are often difficult to unit test[<sup>2<\/sup>](#references) could use an implementation which exposes the signals produced by the reactor in\\nmanufactured scenarios:\\n```go\\ntype ErrorBehaviours map[Peer][]ErrorBehaviourPeer\\ntype GoodBehaviours map[Peer][]GoodBehaviourPeer\\ntype StorePeerBehaviour struct {\\neb ErrorBehaviours\\ngb GoodBehaviours\\n}\\nfunc NewStorePeerBehaviour() *StorePeerBehaviour{\\nreturn &StorePeerBehaviour{\\neb: make(ErrorBehaviours),\\ngb: make(GoodBehaviours),\\n}\\n}\\nfunc (spb StorePeerBehaviour) Errored(peer Peer, reason ErrorBehaviourPeer) {\\nif _, ok := spb.eb[peer]; !ok {\\nspb.eb[peer] = []ErrorBehaviours{reason}\\n} else {\\nspb.eb[peer] = append(spb.eb[peer], reason)\\n}\\n}\\nfunc (mpb *StorePeerBehaviour) GetErrored() ErrorBehaviours {\\nreturn mpb.eb\\n}\\nfunc (spb StorePeerBehaviour) Behaved(peer Peer, reason GoodBehaviourPeer) {\\nif _, ok := spb.gb[peer]; !ok {\\nspb.gb[peer] = []GoodBehaviourPeer{reason}\\n} else {\\nspb.gb[peer] = append(spb.gb[peer], reason)\\n}\\n}\\nfunc (spb *StorePeerBehaviour) GetBehaved() GoodBehaviours {\\nreturn spb.gb\\n}\\n```\\n","Prediction":"## Decision\nWe will use the `p2p.Switch` as the owning component for peer behaviour.\\nWe will use the `p2p.AddrBook` as a single, independent peer behaviour\\ncomponent, which will be used to trigger the `MarkAsGood`\\nmethod on peer behaviour. The `p2p.AddrBook` will be responsible for\\nhandling peer behaviour in the reactor, and the `p2p.Switch`\\ncomponent will be responsible for signalling peer behaviour.\\nWe will use a single, independent reactor for peer behaviour, which will\\nbe responsible for signalling peer behaviour.\\nWe will use the `p2p.Switch` as the owning component for peer behaviour.\\nWe will use the `p2p.AddrBook` as a single, independent peer behaviour\\ncomponent, which will be used to trigger the `MarkAsGood`\\nmethod on peer behaviour. The `p2p.AddrBook` will be responsible for\\nhandling peer behaviour in the reactor, and the `p2p.Switch`\\ncomponent will be responsible for signalling peer behaviour.\\nWe will use a single, independent reactor for peer behaviour, which will\\nbe responsible for signalling peer behaviour.\\nWe will use the `p2p.S"}
{"Context":"## Context\\nWe want to allow partial or merge updates of things with a single request.\\n","Decision":"## Decision\nA merge request\\n* uses HTTP `PATCH` method.\\n* has payload in _JSON merge patch_ format defined in [RFC-7396](https:\/\/tools.ietf.org\/html\/rfc7396).\\n* has the request header `content-type` set to `application\/merge-patch+json`.\\n","Prediction":"## Decision\nWe will use a single request to update a thing.  This means that we will not be able to merge things with multiple updates.  This is because merging updates is a hard merge and we don't want to have to handle that.\\nWe will use a `PUT` request to update a thing.  This means that we will need to handle the merge properly.\\nWe will use a `PATCH` request to update a thing.  This means that we will need to handle the merge properly.\\nWe will use a `DELETE` request to update a thing.  This means that we will need to handle the merge properly.\\nWe will use a `POST` request to update a thing.  This means that we will need to handle the merge properly.\\nWe will use a `PATCH` request to update a thing.  This means that we will need to handle the merge properly.\\nWe will use a `DELETE` request to update a thing.  This means that we will need to handle the merge properly.\\nWe will use a `PATCH` request to update a thing.  This means that we will need to handle the merge properly.\\nWe will use a `DELETE` request to update a thing.  This"}
{"Context":"## Context\\nrun-actions run scripts using a platform specific shell:\\n`bash -eo pipefail` on non-windows, and `cmd.exe \/c \/d \/s` on windows\\nThe `shell` option overrides this to allow different flags or completely different shells\/interpreters\\nA small example is:\\n```yml\\njobs:\\nbash-job:\\nactions:\\n- run: echo \"Hello\"\\nshell: bash\\npython-job:\\nactions:\\n- run: print(\"Hello\")\\nshell: python {0}\\n```\\n","Decision":"## Decision\n___\\n### Shell option\\nThe keyword being used is `shell`\\n`shell` can be either:\\n1. Builtins \/ Explicitly supported keywords. It is useful to support at least `cmd`, and `powershell` on Windows. Because `cmd my_cmd_script` and `powershell my_ps1_script` are not valid the same way many Linux\/cross-platform interpreters are, e.g. `bash myscript` or `python myscript`. Those tools (and potentially others) also require the correct file extension to run, or must be run in a particular way to get the exit codes consistently, so we must have first class knowledge about them. We provide default templates for these keywords as follows:\\n- `cmd`: Default is: `%ComSpec% \/D \/E:ON \/V:OFF \/S \/C \"CALL \"{0}\"\"` where the script name is automatically appended with `.cmd` and substituted for `{0}`\\n- Note this is equivalent to the default Windows behavior if no shell option is given\\n- `pwsh`: Default is: `pwsh -command \"& '{0}'\"` where the script is automatically appended with `.ps1`\\n- `powershell`: Default is: `powershell -command \"& '{0}'\"` where the script is automatically appended with `.ps1`\\n- `bash`: Uses `bash --noprofile --norc -eo pipefail {0}`\\n- The default behavior on non-Windows if no shell is given is to attempt this first\\n- `sh`: Uses `sh -e {0}`\\n- This is the default behavior on non-Windows if no shell is given, AND `bash` (see above) was not located on the PATH\\n- `python`: `python {0}`\\n- **NOTE**: The exact command ran may vary by machine. We only provide default arguments and command format for the listed shell. While the above behavior is expected on hosted machines, private runners may vary. For example, `sh` (or other commands) may actually be a link to `\/bin\/dash`, `\/bin\/bash`, or other\\n1. A template string: `command [...options] {0} [...more_options]`\\n- As above, the file name of the temporary script will be templated in. This gives users more control to have options at any location relative to the script path\\n- The first whitespace-delimited word of the string will be interpreted as the command\\n- e.g. `python {0} arg1 arg2` or similar can be used if passing args is needed. Some shells will require other options after the filename for various reasons\\nNote that (1) simply provides defaults that are executed with the same mechanism as (2). That is:\\n- A temporary script file is generated, and the path to that file is templated into the string at `{0}`\\n- The first word of the formatted string is assumed to be a command, and we attempt to locate its full path\\n- The fully qualified path to the command, plus the remaining arguments, is executed\\n- e.g. `shell: bash` expands to `\/bin\/bash --noprofile --norc -eo pipefail \/runner\/_layout\/_work\/_temp\/f8d4fb2b-19d9-47e6-a786-4cc538d52761.sh` on my private runner\\nAt this time, **THE LIST OF WELL-KNOWN SHELL OPTIONS IS**:\\n- cmd - Windows (hosted vs2017, vs2019) only\\n- powershell - Windows (hosted vs2017, vs2019) only\\n- sh - All hosted platforms\\n- pwsh - All hosted platforms\\n- bash - All hosted platforms\\n- python - All hosted platforms. Can use setup-python to configure which python will be used\\n___\\n### Containers\\nFor container jobs, `shell` should just work the same as above, transparently. We will simply `exec` the command in the job container, passing the same arguments in\\n___\\n### Exit codes \/ Error action preference\\nFor builtin shells, we provide defaults that make the most sense for CI, running within Actions, and being executed by our runner\\nbash\/sh:\\n- Fail-fast behavior using `set -e o pipefail` is the default for `bash` and `shell` builtins, and by default when no option is given on non-Windows platforms\\n- Users can opt out of fail-fast and take full control easily by providing a template string to the shell options, eg: `bash {0}`.\\n- sh-like shells exit with the exit code of the last command executed in a script, and is our default behavior. Thus the runner reports the status of the step as fail\/succeed based on this exit code\\npowershell\/pwsh\\n- Fail-fast behavior when possible. For `pwsh` and `powershell` builtins, we will prepend `$ErrorActionPreference = 'stop'` to script contents\\n- We append `if ((Test-Path -LiteralPath variable:\\LASTEXITCODE)) { exit $LASTEXITCODE }` to powershell scripts to get Action statuses to reflect the script's last exit code\\n- Users can always opt out by not using the builtins, and providing a shell option like: `pwsh -File {0}`, or `powershell -Command \"& '{0}'\"`, depending on need\\ncmd\\n- There doesn't seem to be a way to fully opt in to fail-fast behavior other than writing your script to check each error code and respond accordingly, so we can't actually provide that behavior by default, it will be completely up to the user to write this behavior into their script\\n- cmd.exe will exit (return the error code to the runner) with the errorlevel of the last program it executed. This is internally consistent with the previous default behavior (sh, pwsh) and is the cmd.exe default, so we keep that behavior\\n","Prediction":"## Decision\nUse `shell` to run scripts, this is used in the `run.sh` script, but will also be used in the `run.py` script.\\n`shell` is a bash option, but also supports python, and python3.\\n`shell` is used in the `run.sh` script, but will also be used in the `run.py` script.\\n`shell` is used in the `run.sh` script, but will also be used in the `run.py` script.\\n`shell` is used in the `run.sh` script, but will also be used in the `run.py` script.\\n`shell` is used in the `run.sh` script, and will also be used in the `run.py` script.\\n`shell` is used in the `run.sh` script, and will also be used in the `run.py` script.\\n`shell` is used in the `run.sh` script, and will also be used in the `run.py` script.\\n`shell` is used in the `run.sh` script, and will also be used in the `run.py` script.\\n`shell` is used in the `run.sh` script,"}
{"Context":"## Context\\nAs part of our approval from the Architecture Review Board on 2019-10-16, we were heavily steered towards adopting the [Tech Group's implementation](https:\/\/dev-developers-customerengagement.platform.education.gov.uk) of [Azure's Enterprise API Management](https:\/\/azure.microsoft.com\/en-gb\/services\/api-management) gateway service.\\nThe model is as described in the following diagram:\\n![EAPIM model](0007-eapim-model.png)\\nThe position as outlined by Jemma Sheargold is: \"the API Gateway pattern must be applied, EAPIM is the way to this in the first instance\", and we agreed in ARB to experiment with it.\\nWe have experimented, and set up our dev API with the dev EAPIM instance.\\n### Findings\\n#### Pros\\n- for DFE: having all externally-facing APIs listed in one place\\n- for DFE: having a central place to allow\/disallow access to all APIs + users, set policies, etc\\n- for the service: having rate-limiting etc be Someone Else's Problem\\n#### Cons\\n- for the User: the portal looks immature, with a confusing user experience and terminology\\n- for the User: they would now have two tokens to manage in two different places - one for the EAPIM gateway, and another one to authenticate against our backend\\n- for the Service Team: it adds another \"thing\" in between us and our users that we do not have any control over\\n- for the Service Team: it is not clear how we would get notifications of requests for access via the dev portal, or if the model is that the CIP team would approve\/deny them, then how would they know who is a valid user or not?\\n- for the User: there is a risk that future centrally-imposed changes could be incompatible with our vendor's integrations (e.g. central security policies, or Azure dropping support for some things) which, given the long lead times for changes to vendor systems (up to 2yrs in some cases) could effectively take our API offline for some clients.\\n- for the Service Team: the only way to add customised messages to the portal (for instance, how to get a token for the backend) is to add it into the OpenApi spec - but our tech docs are also auto-generated from the OpenApi spec, so we would have confusing messages on one or the other, or both.\\n- for the Service Team: it adds more overhead to managing access (two places to manage tokens rather than one)\\n","Decision":"## Decision\nWe will not be pursuing integration with EAPIM at this time. It adds too much overhead for too little benefit, and gives our users a worse experience than they already have.\\nWe do not object to listing our API in the portal - but we will be directing our users towards our own existing token management & documentation, and not actively pointing our users towards the portal or gateway.\\n","Prediction":"## Decision\nWe will use the EAPIM gateway for all external-facing APIs.\\nWe will set up an EAPIM instance as a virtual service in the same way as an API Gateway.\\nWe will use the [Azure DevOps pipeline](https:\/\/docs.microsoft.com\/en-us\/azure\/devops\/pipelines\/standard\/azure-devops-pipelines-integration) for the build and deployment of the API Gateway.\\nWe will use the [Azure API Management portal](https:\/\/portal.azure.com) as a single point of entry for all external-facing APIs.\\nWe will use the [Azure API Management API Explorer](https:\/\/docs.microsoft.com\/en-us\/azure\/api-management\/api-explorer) to allow the Service Team to view all external-facing APIs.\\nWe will use the [Azure API Management API Gateway](https:\/\/docs.microsoft.com\/en-us\/azure\/api-management\/api-gateway) to allow the Service Team to add external-facing APIs to the portal.\\nWe will allow the Service Team to approve\/disallow external-facing APIs by default.\\nWe will allow the Service Team to allow external-facing APIs via a separate portal page on the portal.\\nWe will allow the Service Team to create new APIs and manage existing APIs via a separate portal page on the portal.\\"}
{"Context":"Context\\n-------\\nWhen using the puppetlabs firewall module, one of the recommended\\nsettings is to purge all rules not managed by puppet:\\n```puppet\\nresources { 'firewall':\\npurge => true,\\n}\\n```\\nSince kubernetes and calico perform networking and load balancing at the\\nkernel level, this means puppet finds itself with hundreds of rules to\\npurge every 30 minutes.\\nThe firewall module also has an undocumented feature that lets you\\nmanage individual netfilter chains:\\n```puppet\\n# This will remove all `-A INPUT` rules unless they're either (a)\\n# managed by puppet or (b) contain `--comment \"you can trust me ;)\"`.\\nfirewallchain { 'INPUT:filter:IPv4':\\nensure => 'present',\\npurge  => true,\\nignore => [\\n'--comment \"you can trust me ;)\"',\\n],\\n}\\n```\\nThis doesn't work in conjunction with `purge => true` for all firewall\\nresources. If we use `firewallchain` resources instead, then any chains\\nwe don't explicitly define will be ignored by puppet. So, for example,\\nif we define only `INPUT:filter:IPv4`, then any existing `OUTPUT` rules\\nwill be ignored rather than purged by puppet.\\nDecision\\n--------\\nWe will only manage INPUT, OUTPUT, and FORWARD chains on kubernetes\\nmachines. We will specifically whitelist particular lines known to be\\nused in those chains by kubernetes and calico.\\nConsequences\\n------------\\nAs of writing this, it means we now have two firewall profiles\\n(`networking::firewall` and `kubernetes::firewall`), and they share most\\nof their code. We should probably split that `purge => true` line out of\\nthe central firewall profile and come up with a single structure that\\nworks more broadly.\\nAdditionally, docker itself performs its own networking, so we'll want\\nto do something like this for the `docker` profile, but with different\\nexceptions in the chains we manage.\\nSecurity consequences: unknown.\\n","Decision":"## Decision\n--------\\nWe will only manage INPUT, OUTPUT, and FORWARD chains on kubernetes\\nmachines. We will specifically whitelist particular lines known to be\\nused in those chains by kubernetes and calico.\\nConsequences\\n------------\\nAs of writing this, it means we now have two firewall profiles\\n(`networking::firewall` and `kubernetes::firewall`), and they share most\\nof their code. We should probably split that `purge => true` line out of\\nthe central firewall profile and come up with a single structure that\\nworks more broadly.\\nAdditionally, docker itself performs its own networking, so we'll want\\nto do something like this for the `docker` profile, but with different\\nexceptions in the chains we manage.\\nSecurity consequences: unknown.\\n","Prediction":"## Decision\n--------\\nWe will only manage INPUT, OUTPUT, and FORWARD chains on kubernetes\\nmachines. We will specifically whitelist particular lines known to be\\nused in those chains by kubernetes and calico.\\nConsequences\\n------------\\nAs of writing this, it means we now have two firewall profiles\\n(`networking::firewall` and `kubernetes::firewall`), and they share most\\nof their code. We should probably split that `purge => true` line out of\\nthe central firewall profile and come up with a single structure that\\nworks more broadly.\\nAdditionally, docker itself performs its own networking, so we'll want\\nto do something like this for the `docker` profile, but with different\\nexceptions in the chains we manage.\\nSecurity consequences: unknown.\\nNone yet.\\n## Decision\n--------\\nWe will only manage INPUT, OUTPUT, and FORWARD chains on kubernetes\\nmachines. We will specifically whitelist particular lines known to be\\nused in those chains by kubernetes and calico.\\nConsequences\\n------------\\nAs of writing this, it means we now have two firewall profiles\\n(`networking::firewall` and `kubernetes::firewall`), and"}
{"Context":"## Context\\nThe modernisation of the Fix & Go infrastructure requires an initial decision on what system to start with and what overall approach to take.\\nThere is a large amount of prior experience and assets that might be used form the Delius AWS migration.\\nThis may or may not be a way of saving time on delivery, and\/or increasing consistency within MoJ.\\nThere is a discussion document (with comments from team members) here https:\/\/docs.google.com\/document\/d\/12p3nUA4NnJplcQyj8p3QOx2je4gTjO_z3ADvH1k5tk0\/edit\\n","Decision":"## Decision\nWe will start by modernising Nomis, as it is the largest and most critical system under our management\\nWe will aim to deliver a working DR environment, with ability to bring it online, as this removes a large piece of operational risk\\nWe will build this and other environments and tooling in Azure, as this reduces the impact of migration on external clients and dependencies of Nomis\\nWe will create a new architecture for Nomis rather than refactoring the existing environments, as we consider the system simple enough that this will be more efficient\\nWe will reuse patterns, practices and any platform-agnostic code from Delius\/AWS wherever possible, as this will save development time\\n","Prediction":"## Decision\nWe will use AWS for the initial migration of the Fix & Go infrastructure.\\nWe will use AWS for the initial migration of the MoJ applications.\\nWe will consider using AWS for the initial migration of the MoJ applications.\\nWe will consider using AWS for the initial migration of the MoJ infrastructure.\\nWe will consider using AWS for the initial migration of the MoJ applications.\\nWe will consider using AWS for the initial migration of the MoJ applications.\\nWe will consider using AWS for the initial migration of the MoJ infrastructure.\\nWe will consider using AWS for the initial migration of the MoJ applications.\\nWe will consider using AWS for the initial migration of the MoJ infrastructure.\\nWe will consider using AWS for the initial migration of the MoJ infrastructure.\\nWe will consider using AWS for the initial migration of the MoJ application.\\nWe will consider using AWS for the initial migration of the MoJ infrastructure.\\nWe will consider using AWS for the initial migration of the MoJ application.\\nWe will consider using AWS for the initial migration of the MoJ application.\\nWe will consider using AWS for the initial migration of the MoJ application.\\nWe will consider using AWS for the initial migration of the MoJ application"}
{"Context":"## Context\\nWe want to be able to ensure our pages are accessible.\\n[Axe](https:\/\/github.com\/dequelabs\/axe-core) is an actively supported\\naccessibility testing engine for HTML-based user interfaces that supports all\\nmodern browsers (including IE 9+).\\n","Decision":"## Decision\nWe will use axe for accessibility testing. We will integrate it with Jest via\\n[`jest-axe`](https:\/\/github.com\/nickcolley\/jest-axe), enabling us to integrate\\nwith all of our tests.\\n","Prediction":"## Decision\nWe will use [axe](https:\/\/github.com\/dequelabs\/axe-core) to test our pages.\\nWe will use [axe-standalone](https:\/\/github.com\/dequelabs\/axe-standalone) to run\\nthe tests.\\nWe will use [axe-standalone-html](https:\/\/github.com\/dequelabs\/axe-standalone-html) to\\nrun the tests on HTML pages.\\nWe will use [axe-standalone-css](https:\/\/github.com\/dequelabs\/axe-standalone-css) to\\nrun the tests on CSS-based user interfaces.\\nWe will use [axe-standalone-rtl](https:\/\/github.com\/dequelabs\/axe-standalone-rtl) to run\\nthe tests on RTL pages.\\nWe will use [axe-standalone-rtl-html](https:\/\/github.com\/dequelabs\/axe-standalone-rtl-html) to run the tests on HTML pages in RTL.\\nWe will use [axe-standalone-rtl-css](https:\/\/github.com\/dequelabs\/axe-standalone-rtl-css) to run the tests on CSS-based user interfaces.\\nWe will use [axe-standalone-rtl-rtl](https:\/\/github.com"}
{"Context":"## Context\\nIn an effort to support [Go-API-Stability](.\/adr-060-go-api-stability.md),\\nduring the 0.35 development cycle, we have attempted to reduce the the API\\nsurface area by moving most of the interface of the `node` package into\\nunexported functions, as well as moving the reactors to an `internal`\\npackage. Having this coincide with the 0.35 release made a lot of sense\\nbecause these interfaces were _already_ changing as a result of the `p2p`\\n[refactor](.\/adr-061-p2p-refactor-scope.md), so it made sense to think a bit\\nmore about how tendermint exposes this API.\\nWhile the interfaces of the P2P layer and most of the node package are already\\ninternalized, this precludes some operational patterns that are important to\\nusers who use tendermint as a library. Specifically, introspecting the\\ntendermint node service and replacing components is not supported in the latest\\nversion of the code, and some of these use cases would require maintaining a\\nvendor copy of the code. Adding these features requires rather extensive\\n(internal\/implementation) changes to the `node` and `rpc` packages, and this\\nADR describes a model for changing the way that tendermint nodes initialize, in\\nservice of providing this kind of functionality.\\nWe consider node initialization, because the current implemention\\nprovides strong connections between all components, as well as between\\nthe components of the node and the RPC layer, and being able to think\\nabout the interactions of these components will help enable these\\nfeatures and help define the requirements of the node package.\\n","Decision":"## Decision\n- To the greatest extent possible, factor the code base so that\\npackages are responsible for their own initialization, and minimize\\nthe amount of code in the `node` package itself.\\n- As a design goal, reduce direct coupling and dependencies between\\ncomponents in the implementation of `node`.\\n- Begin iterating on a more-flexible internal framework for\\ninitializing tendermint nodes to make the initatilization process\\nless hard-coded by the implementation of the node objects.\\n- Reactors should not need to expose their interfaces *within* the\\nimplementation of the node type\\n- This refactoring should be entirely opaque to users.\\n- These node initialization changes should not require a\\nreevaluation of the `service.Service` or a generic initialization\\norchestration framework.\\n- Do not proactively provide a system for injecting\\ncomponents\/services within a tendtermint node, though make it\\npossible to retrofit this kind of plugability in the future if\\nneeded.\\n- Prioritize implementation of p2p-based statesync reactor to obviate\\nneed for users to inject a custom state-sync provider.\\n","Prediction":"## Decision\nWe will move all of the node package's internal logic, such as the\\n`init` function, the `initialize` function, the `init` method, and the\\n`rpc` function, to an internal package called `node`.\\nWe will remove the following `node` package functions from the\\n`default` package:\\n```go\\npackage node\\nimport \"github.com\/tendermint\/node\/node.go\"\\nfunc (n *node) initialize(ctx context.Context, t *tendermint.Tendermint) error\\nfunc (n *node) init(rpc.Rpc) error\\nfunc (n *node) init(p2p.P2P) error\\nfunc (n *node) init(nodeService *nodeService) error\\nfunc (n *node) init(nodeService, rpc.Rpc) error\\nfunc (n *node) init(nodeService, rpc.Rpc) (node, error)\\nfunc (n *node) init(nodeService, rpc.Rpc) (node, error)\\nfunc (n *node) init(nodeService, rpc.Rpc) (node, error)\\nfunc (n *node) init(nodeService, rpc.Rpc) (node, error)\\nfunc ("}
{"Context":"## Context\\nThis document describes the _Lame Duck Mode_ server notification. When a server enters lame duck mode, it removes itself from being advertised in the cluster, and slowly starts evicting connected clients as per  [`lame_duck_duration`](https:\/\/docs.nats.io\/nats-server\/configuration#runtime-configuration). This document describes how this information is notified\\nto the client, in order to allow clients to cooperate and initiate an orderly migration to a different server in the cluster.\\n","Decision":"## Decision\nThe server notififies that it has entered _lame duck mode_ by sending an [`INFO`](https:\/\/docs.nats.io\/nats-protocol\/nats-protocol#info) update. If the `ldm` property is set to true, the server has entered _lame_duck_mode_ and the client should initiate an orderly self-disconnect or close. Note the `ldm` property is only available on servers that implement the notification feature.\\n","Prediction":"## Decision\nWe will use a simple PubSub broker to notify clients of the removal of a server from the cluster. Clients will subscribe to the `server_removed` topic, and will be notified when the removal is complete.\\nIf a client is already subscribed to the topic, it will be removed from the topic, and the client will be notified of a new subscription to the topic.\\nThis will allow clients to either gracefully exit the cluster, or to migrate to another server in the cluster.\\nIf the client is not subscribed to the topic, it will be removed from the topic, and a new subscription will be added to the topic.\\nThis will allow clients to gracefully exit the cluster, or to migrate to another server in the cluster.\\nThis is in contrast to how we currently notify clients when a server is removed from the cluster.\\nWe will use the `lame_duck_duration` field in the `server_removed` topic to determine the number of milliseconds that have passed since the server was removed from the cluster. The `lame_duck_duration` field is a binary field, with the following values:\\n* 0: the server was removed from the cluster\\n* 1: the server was added to the cluster\\n* 2"}
{"Context":"## Context\\nTransfer Digital Records needs a web interface to allow government users to\\nupload files to be transferred to The National Archives.\\nThere are many frameworks to choose from when building user interfaces for the\\nweb. Most modern programming languages have at least one widely-used server-side\\nweb framework, such as Rails (Ruby), Node.js (JavaScript) or Spring (Java). There\\nare also many client-side JavaScript frameworks such as React, Vue or Angular.\\nThese are optimised for presenting dynamic content that can be updated without a\\nfull page reload.\\n","Decision":"## Decision\nUse [Scala Play][play].\\nWe chose Play over other MVC frameworks because we want to continue developing\\nin Scala.\\nWe chose a server-side MVC framework over Next.Js, which was the other main\\noption that we prototyped, because we want to support progressive enhancement as\\nmuch as possible, and this is easier in a framework like Play whose main purpose\\nis server-side rendering.\\nAlthough Next.Js also supports server-side rendering, it's main purpose is to\\nspeed up the rendering of React sites, so its purpose is more to enhance\\nclient-side JavaScript than to support situations where JS isn't available or\\nfails to load.\\nAgain, see [Front end technology considerations][alpha-considerations] and\\nthe [Play MVC vs Next.js comparison][play-vs-next] for more considerations which\\nfed into the decision.\\n[play]: https:\/\/www.playframework.com\/\\n","Prediction":"## Decision\nWe will use React (https:\/\/facebook.github.io\/react\/) for the web interface.\\nReact is a JavaScript library that is designed to create user interfaces. It has\\nbeen used to create many applications such as Facebook, Twitter and Google,\\nand it is used by many other companies such as Netflix and Spotify.\\nReact is written in the popular JavaScript programming language, JavaScript.\\nReact is also one of the most popular web framework for building web\\napplications. This means it is likely to be used by Transfer Digital Records.\\nReact is also a popular client-side JavaScript framework, which means it can be used\\nby other applications such as browsers. This means it is likely to be used by\\nother applications such as Transfer Digital Records.\\nReact also has a community of developers who are actively developing and maintaining\\nthe library. This means it is likely to be used by Transfer Digital Records.\\nReact also has a large ecosystem of components that can be used to create\\napplications. This means it is likely to be used by Transfer Digital Records.\\nReact also has a large ecosystem of third-party libraries that can be used to create\\napplications. This means it is likely to be used by Transfer Digital Records.\\nReact also has a large ecosystem of third"}
{"Context":"## Context\\nAssets in the Cosmos SDK are represented via a `Coins` type that consists of an `amount` and a `denom`,\\nwhere the `amount` can be any arbitrarily large or small value. In addition, the Cosmos SDK uses an\\naccount-based model where there are two types of primary accounts -- basic accounts and module accounts.\\nAll account types have a set of balances that are composed of `Coins`. The `x\/bank` module keeps\\ntrack of all balances for all accounts and also keeps track of the total supply of balances in an\\napplication.\\nWith regards to a balance `amount`, the Cosmos SDK assumes a static and fixed unit of denomination,\\nregardless of the denomination itself. In other words, clients and apps built atop a Cosmos-SDK-based\\nchain may choose to define and use arbitrary units of denomination to provide a richer UX, however, by\\nthe time a tx or operation reaches the Cosmos SDK state machine, the `amount` is treated as a single\\nunit. For example, for the Cosmos Hub (Gaia), clients assume 1 ATOM = 10^6 uatom, and so all txs and\\noperations in the Cosmos SDK work off of units of 10^6.\\nThis clearly provides a poor and limited UX especially as interoperability of networks increases and\\nas a result the total amount of asset types increases. We propose to have `x\/bank` additionally keep\\ntrack of metadata per `denom` in order to help clients, wallet providers, and explorers improve their\\nUX and remove the requirement for making any assumptions on the unit of denomination.\\n","Decision":"## Decision\nThe `x\/bank` module will be updated to store and index metadata by `denom`, specifically the \"base\" or\\nsmallest unit -- the unit the Cosmos SDK state-machine works with.\\nMetadata may also include a non-zero length list of denominations. Each entry contains the name of\\nthe denomination `denom`, the exponent to the base and a list of aliases. An entry is to be\\ninterpreted as `1 denom = 10^exponent base_denom` (e.g. `1 ETH = 10^18 wei` and `1 uatom = 10^0 uatom`).\\nThere are two denominations that are of high importance for clients: the `base`, which is the smallest\\npossible unit and the `display`, which is the unit that is commonly referred to in human communication\\nand on exchanges. The values in those fields link to an entry in the list of denominations.\\nThe list in `denom_units` and the `display` entry may be changed via governance.\\nAs a result, we can define the type as follows:\\n```protobuf\\nmessage DenomUnit {\\nstring denom    = 1;\\nuint32 exponent = 2;\\nrepeated string aliases = 3;\\n}\\nmessage Metadata {\\nstring description = 1;\\nrepeated DenomUnit denom_units = 2;\\nstring base = 3;\\nstring display = 4;\\n}\\n```\\nAs an example, the ATOM's metadata can be defined as follows:\\n```json\\n{\\n\"name\": \"atom\",\\n\"description\": \"The native staking token of the Cosmos Hub.\",\\n\"denom_units\": [\\n{\\n\"denom\": \"uatom\",\\n\"exponent\": 0,\\n\"aliases\": [\\n\"microatom\"\\n],\\n},\\n{\\n\"denom\": \"matom\",\\n\"exponent\": 3,\\n\"aliases\": [\\n\"milliatom\"\\n]\\n},\\n{\\n\"denom\": \"atom\",\\n\"exponent\": 6,\\n}\\n],\\n\"base\": \"uatom\",\\n\"display\": \"atom\",\\n}\\n```\\nGiven the above metadata, a client may infer the following things:\\n* 4.3atom = 4.3 * (10^6) = 4,300,000uatom\\n* The string \"atom\" can be used as a display name in a list of tokens.\\n* The balance 4300000 can be displayed as 4,300,000uatom or 4,300matom or 4.3atom.\\nThe `display` denomination 4.3atom is a good default if the authors of the client don't make\\nan explicit decision to choose a different representation.\\nA client should be able to query for metadata by denom both via the CLI and REST interfaces. In\\naddition, we will add handlers to these interfaces to convert from any unit to another given unit,\\nas the base framework for this already exists in the Cosmos SDK.\\nFinally, we need to ensure metadata exists in the `GenesisState` of the `x\/bank` module which is also\\nindexed by the base `denom`.\\n```go\\ntype GenesisState struct {\\nSendEnabled   bool        `json:\"send_enabled\" yaml:\"send_enabled\"`\\nBalances      []Balance   `json:\"balances\" yaml:\"balances\"`\\nSupply        sdk.Coins   `json:\"supply\" yaml:\"supply\"`\\nDenomMetadata []Metadata  `json:\"denom_metadata\" yaml:\"denom_metadata\"`\\n}\\n```\\n","Prediction":"## Decision\nWe will use `x\/bank` to keep track of the total amount of assets that are associated with\\na given `denom`.\\nTo support this change, we will introduce a new Cosmos SDK dependency called `x\/bank`.\\nThis dependency will be required for all Cosmos SDK and related modules.\\nThis dependency will be required in the `x\/bank` module itself, and will be\\nrequired by all modules that are built on top of the `x\/bank` module.\\nThis dependency will be required in the `x\/bank` codebase itself.\\nThis dependency will be required in the `x\/bank` API.\\nThis dependency will be required in the `x\/bank` documentation.\\nThis dependency will be required in the `x\/bank` tooling.\\nThis dependency will be required in the `x\/bank` buildpack.\\nThis dependency will be required in the `x\/bank` toolchain.\\nThis dependency will be required in the `x\/bank` release.\\nThis dependency will be required in the `x\/bank` release toolchain.\\nThis dependency will be required in the `x\/bank` release file.\\nThis dependency will be required in the"}
{"Context":"## Context\\nThe current approach to collecting a list of things e.g. people that live in a household, and asking questions for each thing in that list, is more complex than it needs to be.\\nCollecting a list of things is currently achieved using repeat until logic with a confirmation question to get out of the repeat - something like \"Is this everyone that lives at ...?\". While the happy path journey is acceptable, it soon falls apart when a user attempts to do something else, for example editing or removing a list item. This also means that every time we evaluate the routing path, we need to follow the list item entry and confirmation combination for every item in the list. Attempting to fit creation of a list of things into the standard question per block model is wrong, as the add\/edit\/remove journeys are not relevant outside of the list creation.\\n","Decision":"## Decision\n- Define list creation in the schema\\n- Store lists and answers within the context of a list\\n- Remove all unnecessary repeat functionality\\n### Define list creation in the schema\\nA new block type will co-ordinate the collection of a list of things:\\n```json\\n{\\n\"id\": \"block-id\",\\n\"type\": \"ListCollector\",\\n\"for_list\": \"people\",\\n\"question\": {},\\n\"add_block\": {},\\n\"edit_block\": {},\\n\"remove_block\": {}\\n}\\n```\\n- The routing path is kept simple as we can ignore the journeys taken adding\/editing\/removing when evaluating the path.\\n- Each add, edit and remove block will contain a Question block type.\\n- Each block can have variants. This is important as the add question can change depending on whether the list is empty or not.\\n- More than one list collector can populate the same list.\\nThe user flows that will need to be implemented in runner from this schema:\\n![Question Flow](0006-question-flow.png)\\n### Store lists and answers within the context of a list\\nA new top level item in the questionnaire store will store each named list and it's identifiers:\\n```json\\n{\\n\"answers\": [\\n{\\n\"answer_id\": \"date-of-birth\",\\n\"value\": \"1982-08-01\",\\n\"list_item_id\": \"a9hd8j\"\\n},\\n{\\n\"answer_id\": \"occupation\",\\n\"value\": \"Teacher\",\\n\"list_item_id\": \"a9hd8j\"\\n},\\n{\\n\"answer_id\": \"date-of-birth\",\\n\"value\": \"1994-02-05\",\\n\"list_item_id\": \"kl9s3y\"\\n},\\n{\\n\"answer_id\": \"occupation\",\\n\"value\": \"Engineer\",\\n\"list_item_id\": \"kl9s3y\"\\n}\\n],\\n\"lists\": [\\n{\\n\"name\": \"people\",\\n\"items\": [\\n\"a9hd8j\",\\n\"kl9s3y\"\\n]\\n}\\n]\\n}\\n```\\n- Removal of `group_instance`, `group_instance_id` and `answer_instance` from answers.\\n- Introduction of a `list_item_id` that identifies the list item that an answer is related to.\\n- The order of the items in the list definiton will determine display order.\\n- The repeat identifier should be a short randomised string (it only has to be unique within the questionnaire).\\n### Remove all unnecessary repeat functionality\\nThere are currently a number of ways to define a repeat that should be removed:\\n- `answer_value` provided a way of repeating based on a user provided value and is not used\\n- `answer_count` and `answer_count_minus_one` were only necessary in the absence of lists\\n- `until` was only ever used to collect a list\\nWhile we're simplifying the repeat logic, `repeat` should be moved out of `routing_rules`.\\n","Prediction":"## Decision\nWe will not use the standard question per block model for creating a list of things. The user will be able to create a list of things using the standard question per block model, and then the user will be able to edit or remove the list item. The user will be able to create a list of things in the following way:\\n- Create a list item (e.g. \"Barbers\") with the following options:\\n- \"Male\", \"Female\", \"Both\" or \"None\"\\n- Add a list of things as a question: \"Is this a barbershop?\". If the user answers \"Yes\" to this, the list item will be created, and the user will be prompted for a confirmation. If the user answers \"No\" to this, the list item will be removed.\\n- Add a list of things as a question: \"Is this a hairdresser?\". If the user answers \"Yes\" to this, the list item will be created, and the user will be prompted for a confirmation. If the user answers \"No\" to this, the list item will be removed.\\n- Add a list of things as a question: \"Is this a hair stylist?\". If the user answers \""}
{"Context":"## Context and Problem Statement\\nSylius is by design application that will be adjusted to customer needs.\\nTherefore each part of it has to be crafted with replaceability or customizability at its core.\\nNonetheless, the current state of API Platform integration requires the replacement of the whole config of the whole resource.\\nIn terms of the more complicated cases like Order or Customer, this practice may become error-prone and hard to maintain for both maintainers and Sylius users.\\n","Decision":"## Decision\nChosen option: \"Config merging\", because it allows us easily overwrite any endpoint, without getting deep into api platform resources.\\n","Prediction":"## Decision\nChosen option: \"Use a custom resource provider\", because:\\n- Sylius has a very large API, and it is not possible to cover all use-cases with a single provider (e.g. Order and Customer are not the same)\\n- The API Platform documentation is not very helpful in understanding how to set up a custom provider\\n- It is possible to create custom providers with the help of the Sylius documentation. However, this is not the recommended approach, as it introduces a dependency on the API Platform and requires a Sylius-specific configuration.\\n- The API Platform documentation is not very helpful in understanding how to set up a custom provider\\n- It is possible to create custom providers with the help of the Sylius documentation. However, this is not the recommended approach, as it introduces a dependency on the API Platform and requires a Sylius-specific configuration.\\n- This option is also supported by the API Platform, so it should be considered as a \"good practice\" and not an \"optional feature\"\\n- This option is also supported by the API Platform, so it should be considered as a \"good practice\" and not an \"optional feature\"\\n- Sylius does not provide a sufficient amount of documentation on this subject\\n-"}
{"Context":"## Context\\nManaged Streaming for Apache Kafka (MSK) cluster in AWS requires a VPC to run in. This means that before provisioning an MSK cluster a VPC needs to be provisioned in AWS. The VPC should be set up to deal with High Availability (HA) and fault tolerance\\n","Decision":"## Decision\nTo make it simpler to use this module we have included an internal VPC module. This module will be used by default when creating an MSK cluster. This removes the need for users to have to provide their own VPC before using this module.\\n","Prediction":"## Decision\nThe VPC will be set up to deal with HA and fault tolerance. The VPC will be set up to allow for the creation of subnets for the containers to be deployed in. The VPC will also allow for the creation of an Internet Gateway for the container to be deployed inside. The VPC will be set up to allow the container to be deployed in the region of the UK. The VPC will allow the container to connect to the AWS API Gateway. The VPC will allow the container to connect to the AWS Load Balancer. The VPC will allow the container to connect to the AWS API Gateway. The VPC will allow the container to connect to the AWS API Gateway. The VPC will allow the container to connect to the AWS API Gateway. The VPC will allow the container to connect to the AWS API Gateway. The VPC will allow the container to connect to the AWS API Gateway. The VPC will allow the container to connect to the AWS API Gateway. The VPC will allow the container to connect to the AWS API Gateway. The VPC will allow the container to connect to the AWS API Gateway. The VPC will allow the container to connect to the AWS API Gateway. The VPC will allow the container"}
{"Context":"## Context\\nWe need a consistent way to add menus within fragments.\\n","Decision":"## Decision\nEvery fragment handles its menu independently. This is achieved through calling setHasOptionsMenu(true)\\nand overriding the specific methods needed for creating the menu and handling selection events.\\n","Prediction":"## Decision\nUse `FragmentActivity` to create menus.\\nCreate the menu in the fragment's `onCreateView()` method.\\nUse `FragmentActivity` to create fragments.\\nCreate a `FragmentManager` to manage the fragments.\\nUse `FragmentActivity` to create fragments.\\nUse `FragmentActivity` to create fragments.\\nUse `FragmentActivity` to create fragments.\\nUse `FragmentActivity` to create fragments.\\nUse `FragmentActivity` to create fragments.\\nUse `FragmentActivity` to create fragments.\\nUse `FragmentActivity` to create fragments.\\nUse `FragmentActivity` to create fragments.\\nUse `FragmentActivity` to create fragments.\\nUse `FragmentActivity` to create fragments.\\nUse `FragmentActivity` to create fragments.\\nUse `FragmentActivity` to create fragments.\\nUse `FragmentActivity` to create fragments.\\nUse `FragmentActivity` to create fragments.\\nUse `FragmentActivity` to create fragments.\\nUse `FragmentActivity` to create fragments.\\nUse `FragmentActivity` to create fragments.\\nUse `FragmentActivity` to create fragments.\\nUse `FragmentActivity` to create fragments.\\nUse `FragmentActivity` to create fragments.\\nUse `FragmentActivity` to"}
{"Context":"## Context\\nWe want to be able to manage DNS records for Kubernetes `Ingress` and `Service type=Loadbalancer` resources declaratively via Kubernetes resources, so that we avoid the overhead and brittleness of manual DNS management with Terraform.\\n[`external-dns`](https:\/\/github.com\/kubernetes-sigs\/external-dns) is the primary project in this space and gives us everything we need; Route53 integration, support for all AWS load balancer types, integration with [`alb-ingress-controller`](https:\/\/github.com\/kubernetes-sigs\/external-dns\/blob\/master\/docs\/tutorials\/alb-ingress.md) and [IRSA](https:\/\/docs.aws.amazon.com\/eks\/latest\/userguide\/iam-roles-for-service-accounts.html) support.\\n","Decision":"## Decision\nUse [`external-dns`](https:\/\/github.com\/kubernetes-sigs\/external-dns).\\n","Prediction":"## Decision\nWe will use [`external-dns`](https:\/\/github.com\/kubernetes-sigs\/external-dns) to provision our DNS records for Kubernetes `Ingress` and `Service type=Loadbalancer` resources.\\nWe will use [`alb-ingress-controller`](https:\/\/github.com\/kubernetes-sigs\/external-dns) to manage our DNS records.\\nWe will use [`alb`](https:\/\/github.com\/kubernetes-sigs\/alk) as the `Ingress` Controller for our `Ingresses` and `LoadBalancer` resources.\\nWe will use [`alb-ingress-controller`](https:\/\/github.com\/kubernetes-sigs\/external-dns) as the `Service` Controller for our `Service` resources.\\nWe will use [`alb`](https:\/\/github.com\/kubernetes-sigs\/external-dns) as the `ClusterIP` resource type for all our `Ingresses` and `LoadBalancer` resources.\\nWe will use [`alb`](https:\/\/github.com\/kubernetes-sigs\/external-dns) as the `ClusterIP` resource type for all our `Service` resources.\\nWe will use [`alb`](https:\/\/github.com\/kubernetes-sigs\/external-dns) as the `ClusterIP` resource type"}
{"Context":"## Context\\nKubernetes requires someone to write very verbose manifest files in YAML. Templating\\ncan allow someone to separate the manifest and data and allow a tool to generate\\nand populate manifest files with ease.\\nIt allows easily understanding the values of a specific application.\\nIt requires us to use a tool on top of plainly writing manifests ourselves.\\n","Decision":"## Decision\nWe will not use templates for the time-being, but will re-evaluate our use of\\npopulating common values in the future.\\n","Prediction":"## Decision\nWe will use the [Flannel](https:\/\/flannel.io\/) templating tool to write\\nmanifests.\\nFlannel is a tool that simplifies the templating of applications and allows\\nthem to be deployed to Kubernetes.\\nFlannel can be used on top of plain YAML files.\\nFlannel will generate a YAML file that can be used as a manifest.\\nFlannel is an open source tool and is maintained by the community.\\nFlannel has been used by the team to write manifests for the following:\\n* Kubernetes (1.18.3)\\n* Terraform (0.12.4)\\n* Ansible (2.7.0)\\n* Consul (1.9.0)\\n* Consul Controller (1.9.0)\\nFlannel is well supported and is maintained by the community.\\nFlannel is a tool that has been well tested and is well known.\\nFlannel is written in Python and is available on Python Package Index (PyPI).\\nFlannel is hosted on Github.\\nFlannel is hosted on [Flannel.io](https:\/\/flannel.io\/), a tool that\\nsimplifies the templating of applications and allows them to be deployed"}
{"Context":"## Context\\nWe are using a mono-repository structure for the Datalabs project, this permits\\nsharing of components across independent services. In NodeJS sharing of code\\nwithout duplication requires linking of modules to the dependant service.\\nThere are a few libraries to manages code-sharing:\\n* NPM Link\\n* Yarn Workspaces\\n* Learna\\n","Decision":"## Decision\nWe have decided to use Yarn Workspaces as is very lightweight and offers\\nmanagement of the links required for module share, including auto discovery of\\nother services.\\nWe have ruled out using straight NPM Links as they are difficult to set-up and\\nshare between development team. We have used Learna on other project and found\\nit to be very heavyweight and requires that is adds git commits to releasing new\\nversions.\\n","Prediction":"## Decision\nWe decided to use Yarn Workspaces to manage dependencies between services.\\nWe will use Yarn workspaces to manage dependencies between components.\\n### Benefits\\n* Sharing of code between components is simple as the same code can be used in\\ndifferent services\\n* Code can be shared across components without duplication\\n* Yarn workspaces is a single point of failure and highly available\\n* Yarn can be used to manage dependencies between services\\n### Limitations\\n* Dependencies between components are not easily managed\\n* Dependencies between components are not easily managed\\n* Dependencies between components are not easily managed\\n* Dependencies between components are not easily managed\\n* Dependencies between components are not easily managed\\n* Dependencies between components are not easily managed\\n* Dependencies between components are not easily managed\\n### Links\\n* Yarn workspaces\\n* Learna\\n* [https:\/\/learna.io\/](https:\/\/learna.io\/) provides a dependency manager for NodeJS\\n* [https:\/\/www.npmjs.com\/](https:\/\/www.npmjs.com\/) provides a package manager for NodeJS\\n* [https:\/\/www.npmjs.com\/package\/links](https:\/\/www.npmjs.com\/package\/links) provides a package manager for NodeJS\\n* ["}
{"Context":"## Context\\nSo that developers do not have to think too hard, we want a simple rule about where the styles related to a React component should live. The options are \"always in a separate styles.js file in the same folder\" or \"always in the same file as the React component, above the component\". (This is referring to JSX or styled-components styles, and not CSS\/SASS styles.)\\n### Separate File\\nPros:\\n- Smaller files, easier to read\\nCons:\\n- Overkill for a single style\\n- More files for Babel to transform and watchers to watch\\n- Extra work for dev (minimal)\\n### Same File\\nPros:\\n- Simple and less work for dev\\n- Fewer files for Babel to transform and watchers to watch\\nCons:\\n- Larger files, harder to read (but you can code fold in IDE)\\n","Decision":"## Decision\nAll styles always live in the same file as the React component, above the component.\\n","Prediction":"## Decision\nWe will use the same style.js file for all style-related styles.\\n### Exceptions\\nThere are a few exceptions to this rule: for example, we may want to use a different style for a component that has a different component's class, or we may want to use a different style for a component that doesn't have a component's class.\\n### Warning\\nWe will not be using the same style file for components that do not have a component's class. It is possible to do this, but it is not recommended.\\n## Decision\nWe will use the same style.js file for all style-related styles.\\n### Exceptions\\nThere are a few exceptions to this rule: for example, we may want to use a different style for a component that has a different component's class, or we may want to use a different style for a component that doesn't have a component's class.\\n### Warning\\nWe will not be using the same style file for components that do not have a component's class. It is possible to do this, but it is not recommended.\\nChosen option: \"always in a separate styles file\", because it allows for the most flexibility and is easy to read. It also has the fewest files, which"}
{"Context":"## Context and Problem Statement\\nIt's probably easiest to describe this using an example. Consider the changes that were implemented for [ADR-0008](0008-restful-failed-message-retrier.md). This added a new interface\\n```java\\npublic interface RetryStore extends ComponentLifecycle, ComponentLifecycleExtension {\\n\/\/ some methods.\\n}\\n```\\nalong with a configurable item that uses the new pluggable interface\\n```java\\npublic class RetryStoreWriteService extends ServiceImp {\\n\/**\\n* Where messages are stored for retries.\\n*\\n*\/\\n@Getter\\n@Setter\\n@NotNull\\n@NonNull\\nprivate RetryStore retryStore;\\n}\\n```\\nThis class requires a lot of boiler plate along the lines of overriding parent methods \/ implementing missing methods.\\n```java\\n@Override\\npublic void prepare() throws CoreException {\\nArgs.notNull(getRetryStore(), \"retry-store\");\\nLifecycleHelper.prepare(getRetryStore());\\n}\\n@Override\\nprotected void initService() throws CoreException {\\nLifecycleHelper.init(getRetryStore());\\n}\\n@Override\\npublic void start() throws CoreException {\\nLifecycleHelper.start(getRetryStore());\\nsuper.start();\\n}\\n@Override\\npublic void stop() {\\nLifecycleHelper.stop(getRetryStore());\\nsuper.stop();\\n}\\n@Override\\nprotected void closeService() {\\nLifecycleHelper.close(getRetryStore());\\n}\\n```\\nThis is prone to mistakes and we should be removing the boilerplate where possible.\\n## Decision Drivers\\n* Backwards compatible; i.e. doesn't break existing code.\\n* Low friction to uptake\\n","Decision":"## Decision\n* Backwards compatible; i.e. doesn't break existing code.\\n* Low friction to uptake\\nAnnotation + LifecycleHelper runtime behavioural change. Introduction of a `InterlokLifecycle` annotation.\\n","Prediction":"## Decision\n* Backwards compatible; i.e. doesn't break existing code.\\n* Low friction to uptake\\nChosen option: \"Use the new RetryStore interface\", because it's the least work for us and it's the most compatible with existing code.\\n### Positive Consequences\\n* Existing code is compatible with the new interface.\\n* The new interface is more flexible than the old one.\\n### Negative Consequences\\n* This makes it harder to implement the new RetryStore interface.\\n* It requires more boilerplate, but it's a small price to pay for backwards compatibility.\\n### Impact on Users\\n* Existing code is compatible with the new interface.\\n* The new interface is more flexible than the old one.\\n### Positive Consequences\\n* Existing code is compatible with the new interface.\\n* The new interface is more flexible than the old one.\\n### Negative Consequences\\n* It requires more boilerplate, but it's a small price to pay for backwards compatibility.\\n### Affected Users\\n* Existing code is compatible with the new interface.\\n* The new interface is more flexible than the old one.\\n### Positive Consequences\\n* Existing code is compatible with the new interface.\\n* The new interface is"}
{"Context":"## Context\\nWe are successfully using lightweight architecture decision records to document decisions being made about architecture.\\nLast year legislation changes around data privacy (GDPR, CCPA, PECR) and their impact on our obligations means we need to increase our technical measures to ensure the safety of our reader's data through:\\n-   Reducing the number of third-party scripts\\n-   Architecting our systems with a [privacy by design](https:\/\/en.wikipedia.org\/wiki\/Privacy_by_design) approach\\n-   Documenting risks and mitigations in place or to be added.\\n-   Adding new controls to prevent issues, i.e being proactive rather than reactive.\\nAdditional to this privacy concern, we have noticed in the past the difficulty to correctly estimate and measure accurately the performance and by association the revenue impact of each third-party. While they usually come with a direct revenue, there is as well indirect revenue loss through performamce impact:\\n-   performance degradation impacts our ad display and our advertising revenue\\n-   performance degradation impacts our contributions conversion rate\\n-   performance degradation impacts our Google SEO which impact our reach and indirectly all of our revenue streams (contributions, subscriptions, advertising)\\nTo address those 2 concerns, we are making a change to the process for deciding if a third-party will be added. Before a decision is made by the business to add a third-party to our website or apps, we will perform an engineering technical review that will be used to inform the business decision.\\nThe suggested format of the review is the following:\\n-   Metadata about the review\\n-   Date of the review\\n-   Author of the review\\n-   Platforms concerned\\n-   Context\\n-   Data privacy engineering review\\n-   Security engineering review\\n-   Perfomance engineering review\\n-   Recommended mitigations\\n","Decision":"## Decision\nFor all third-party technical reviews in this project, a record will be created with a filename of `XXX-technical-review.md` where XXX is the monotonically increasing number described in the linked document.\\nPlease note that as records will be public, specific attention should be paid about information being disclosed.\\n","Prediction":"## Decision\nWe will perform an engineering technical review to inform the business decision on whether a third-party will be added to our website or apps.\\nWe will use the following format:\\n-   Metadata about the review\\n-   Date of the review\\n-   Author of the review\\n-   Platforms concerned\\n-   Context\\n-   Data privacy engineering review\\n-   Security engineering review\\n-   Perfomance engineering review\\n-   Recommended mitigations\\nWe will use the [NIST 800-171](https:\/\/www.nist.gov\/sites\/default\/files\/publication\/800-171-revisions.pdf) as a baseline for our review.\\nWe will also use the [NIST 800-171 appendix A](https:\/\/www.nist.gov\/sites\/default\/files\/publication\/800-171-revisions-1.pdf) to provide additional context to the review.\\nWe will also use the [NIST 800-171 appendix B](https:\/\/www.nist.gov\/sites\/default\/files\/publication\/800-171-revisions-2.pdf) to provide additional context to the review.\\nWe will use the [NIST 800-171 appendix C](https:\/\/www.nist.gov\/sites\/default\/files\/publication\/800-171-revisions-"}
{"Context":"## Context\\nIn order to provide a consistent UI, we need to use a design system. This design system also helps reduce the amount of code required, and ensures designers and developers speak the \"same language\".\\n","Decision":"## Decision\nWe have created our own design system\/pattern librarly, [Franklin](https:\/\/ebi-uniprot.github.io\/franklin-sites). It is built on top [Foundation](https:\/\/foundation.zurb.com\/) (Atomic level components) and uses React.js. The library is published to `npm` as [`franklin-sites`](https:\/\/www.npmjs.com\/package\/franklin-sites) and can be used by any React.js website.\\n","Prediction":"## Decision\nWe will use [Flux by Facebook](https:\/\/facebook.github.io\/flux\/) for building our web UI. We will use [react-router](https:\/\/facebook.github.io\/react-router\/) for routing, and react-redux for state management. We will use [redux](https:\/\/redux.js.org\/) for persisting state.\\n### Design System\\nWe will use [Flux](https:\/\/facebook.github.io\/flux\/) as our design system.\\n### Routing\\nWe will use [react-router](https:\/\/facebook.github.io\/react-router\/) for routing. We will register a router for each of our pages, and route to the appropriate page based on the route parameters. The router will also handle authentication and state management.\\n### State Management\\nWe will use [redux](https:\/\/redux.js.org\/) for persisting state. We will use the [redux-thunk](https:\/\/redux-thunk.com\/) middleware to handle HTTP requests and state management.\\n### Page Templates\\nWe will use [react-page-template](https:\/\/facebook.github.io\/react-page-template\/) as our page template library. We will only use the [create-react-app](https:\/\/facebook.github.io\/create-react-app\/) scaffolding system, and create a default set of page templates.\\n"}
{"Context":"## Context\\nWe want to be able to support full-featured React apps, without reinventing the\\nwheel when it comes to routing and server-side rendering.\\n[Next.js](https:\/\/nextjs.org\/) is a framework that supports these features out\\nof the box.\\n","Decision":"## Decision\nWe will use Next.js as the basis for the project.\\n","Prediction":"## Decision\nWe will use Next.js for our React app.\\nhttps:\/\/nextjs.org\/ is a popular open-source framework that provides a\\nfull-featured development server and client.\\nWe will use [Next](https:\/\/nextjs.org\/) as our framework.\\nNext.js is an open-source framework that is maintained by Facebook.\\n[Next.js](https:\/\/nextjs.org\/) is built on top of [React](https:\/\/reactjs.org\/).\\nReact is a popular React.js framework that is maintained by Facebook.\\nReact is built on top of [React Router](https:\/\/facebook.github.io\/react-router\/).\\nReact is a popular React.js library that is maintained by Facebook.\\nReact is a popular React library that is maintained by Facebook.\\nReact is a popular React library that is maintained by Facebook.\\nReact is a popular React library that is maintained by Facebook.\\nReact is a popular React library that is maintained by Facebook.\\nReact is a popular React library that is maintained by Facebook.\\nReact is a popular React library that is maintained by Facebook.\\nReact is a popular React library that is maintained by Facebook.\\nReact is a popular React library that is maintained by Facebook.\\nReact is a popular React library that is"}
{"Context":"## Context and Problem Statement\\nKnow it before they do! We need a tool to discover, triage, and prioritize errors in real-time.\\n","Decision":"## Decision\nChosen option: `Sentry`, because it ranks higher in a community survey regarding our stack (Javascript). It's also much cheaper and offers the choice to be completely free if we self-host it.\\n","Prediction":"## Decision\nChosen option: \"Error Detective\", because it's easy to use, it's easy to setup, it's easy to learn and it's better than the alternatives (see below).\\n### Positive Consequences <!-- optional -->\\n* We can use it on any platform\\n* We can use it with any language\\n* We can use it with any database\\n* We can use it with any monitoring platform\\n* We can use it with any other tools\\n* We can use it with any other languages\\n* We can use it with any other database platforms\\n* We can use it with any other monitoring platforms\\n* We can use it with any other tools\\n* We can use it with any other languages\\n* We can use it with any other database platforms\\n* We can use it with any other monitoring platforms\\n* We can use it with any other tools\\n* We can use it with any other languages\\n### Negative Consequences <!-- optional -->\\n* It's not easy to use\\n* It's not as easy to use as the alternatives (see below)\\n* It's not as easy to setup as the alternatives (see below)\\n* It's not as easy to learn as the alternatives (see below)\\n"}
{"Context":"## Context\\nEngineers in the team want to have their code changes built as real environments in the Development AWS account so that they can have their work evaluated.\\nEvaluation includes automated and user acceptance testing.\\nWhen evaluation is finished it should be possible to destroy the environment, while retaining important artifacts and data (logs for example).\\nIt will be possible to host multiple PR environments in the Development AWS account\\n","Decision":"## Decision\n### Terraform Configuration\\nWe will identify and manage separately, resources at an Account level and resources at an Environment level.\\nAccount level resources, such as S3 buckets for logs, the default VPC and networking will be used Environment level resources, such as ECS clusters, DNS routes and Load Balancers.\\nThis will enable us to use Terraform Workspaces to contain an Environment completely for it to be built and destroyed simply.\\nAccount level resources will remain in place after an Environment is destroyed.\\n![split terraform configuration diagram](..\/diagrams\/Use-An-LPA_split_Terraform_configurations.png)\\n### Name Spacing\\nWe will use the github PR number and the first few characters of the branch name to namespace the environment and it's resources\\nFor example, DNS records will be created for each environment using the namespace `https:\/\/view.025-UML-93.use-an-lpa.opg.service.justice.gov.uk`\\nwhile an ECS cluster will be named `025-UML-93-use-an-lpa`\\n### Environment lifecycle\\nAn Environment will be built when a PR is opened\\nOnce evaluation is complete, the Envirnoment can be destroyed.\\nTo begin with the pipeline will print instructions for destroying an Environment.\\nLater, destroying an Environment will be automated and scheduled with a maximum life span of a day or two.\\n### CI\/CD Pipeline\\nThe Terraform steps in the pipeline will always first provision the Account level shared resources, then provision the Environment level resources.\\n![pipeline terraform steps](..\/diagrams\/Use-An-LPA_Terraform_pipeline.png)\\n","Prediction":"## Decision\nWe will use AWS CodePipeline and CodeBuild to build the environments in the AWS account.\\nThis will be done using our existing AWS CodeBuild and CodePipeline environments.\\nWe will use CodeBuild to build and test the codebase in the Development AWS account.\\nWe will use CodeBuild to build and test the codebase in the Production AWS account.\\nWe will use CodeBuild to run the code in the Development AWS account.\\nWe will use CodeBuild to run the code in the Production AWS account.\\nWe will use CodeBuild to store the deployment artifacts in a CodeBuild release artifact repository.\\nWe will use CodePipeline to trigger the deployment of the release artifacts.\\nWe will use CodePipeline to trigger the destruction of the release artifacts.\\nWe will use CodePipeline to trigger the destruction of the Development AWS account.\\nWe will use CodePipeline to trigger the destruction of the Production AWS account.\\nWe will use CodeBuild to trigger the destruction of the Development AWS account.\\nWe will use CodeBuild to trigger the destruction of the Production AWS account.\\nWe will use CodeBuild to trigger the destruction of the Development AWS account.\\nWe will use CodeBuild to trigger the destruction of the Production AWS account.\\nWe will use"}
{"Context":"## Context\\nCassandra eventual consistency is all about \"replication\", but \"denormalization\" consistency needs\\nto be handled at the applicative layer (due to the lack of transactions in a NoSQL database).\\nIn the past we did set up \"Solve inconsistency\" tasks that can be assimilated to Cassandra repairs. Such\\ntasks, after being scheduled, ensure that the according entity denormalization is correctly denormalized.\\nHowever, the inconsistencies persist between runs. We experienced inconsistencies in some production platform\\nfor both the mailbox entity, and the mailbox counter entity (whose table structure is exposed in\\n[these](0020-cassandra-mailbox-object-consistency.md), [ADRs](0023-cassandra-mailbox-counters-inconsistencies.md)).\\nMonitoring is required to detect when to run them and is time consuming for the platform administrator.\\nGiven a large dataset, it could even be impossible to run such tasks in a timely fashion.\\nAnother classic eventual consistency mechanism, that enables auto-healing is read-repair. Randomly piggy back upon reads\\nsynchronous or asynchronous consistency checks. If missed a repair is performed.\\nIn order to achieve denormalization auto-healing, we thus need to implement \"applicative read repairs\".\\n","Decision":"## Decision\nProvide a Proof of concept for \"Applicative read repairs\" for the mailbox and mailbox-counters entities.\\nThis enables read path simplification (and performance enhancements) for the mailbox object.\\nIMAP LIST should not read mailbox counters. This information is uneeded and we should avoid paying the\\nprice of read repairs for this operation.\\nProvide a comprehensive documentation page regarding \"Distributed James consistency model\".\\n","Prediction":"## Decision\nWe will use [read-repair](https:\/\/read-repair.readthedocs.io\/en\/latest\/)\\nto handle the \"denormalization\" problem.\\nTo do so we will have to introduce a new [ReplicationStrategy](https:\/\/github.com\/samza\/cassandra\/blob\/7c5f4f3b8f3c1ea1b8c9e9f9a4b5e84a5b5c9c6e\/core\/src\/main\/java\/org\/apache\/cassandra\/solr\/replication\/ReplicationStrategy.java#L113)\\nthat will be used in the replication strategy.\\nThe replication strategy is used to achieve eventual consistency by replicating the data\\nto a number of replicas (for example, 1 replica per data center).\\nThe replication strategy will be defined in a new [CassandraReplicationStrategy](https:\/\/github.com\/samza\/cassandra\/blob\/7c5f4f3b8f3c1ea1b8c9e9f9a4b5e84a5b5c9c6e\/core\/src\/main\/java\/org\/apache\/cassandra\/solr\/config\/CassandraReplicationStrategy.java#L43)\\nclass that"}
{"Context":"## Context\\nDocumenting changes for a release can be challenging. It often involves reading\\nback through commit messages and PRs, looking for and classifying changes, which\\nis a time consuming and error prone process.\\n","Decision":"## Decision\nWe will use a changelog (`CHANGELOG.md`) in the\\n[Keep a Changelog 1.0.0](https:\/\/keepachangelog.com\/en\/1.0.0\/) format to be\\nupdated when code changes happen, rather than at release time.\\n","Prediction":"## Decision\nWe will use `git log` to document changes in a release.\\nWe will use `git log --decorate` to document changes in the commit\\nmessages.\\nWe will use `git log --decorate --all` to document changes in the PR\\ncomments.\\nWe will use `git log --decorate --all --all` to document changes in the\\nrelease notes.\\nWe will use `git log --decorate --all --decorate` to document changes in\\nthe release notes.\\nWe will use `git log --pretty=format:%C, %ad` to document changes in the\\nchanges.\\nWe will use `git log` to run the release process.\\nWe will use `git log --pretty=format:%C, %ad --decorate` to run the release\\nprocess.\\nWe will use `git log --pretty=%C, %ad --decorate --all` to run the release\\nprocess.\\nWe will use `git log --pretty=%C, %ad --decorate --all --all` to run the\\nrelease process.\\nWe will use `git log --pretty=%C, %ad --decorate --all --all --all` to run\\nthe release process.\\nWe will use `git"}
{"Context":"## Context\\nWe're working towards private beta for Pay and working towards implementing\\nhandling Worldpay refund notifications.\\nA decision was made previously to make an 'inquiry' API call to worldpay after\\nreceiving a notification from them. The main motivations for doing this were:\\n- security - in the case that there is a forged notification from worldpay, we\\ncan be more confident in getting the accurate information by calling out to\\nworldpay\\n- robustness if a notification is missed for any reason, then doing an inquiry\\nwill allow us to 'catch up' to the new status.\\nRegarding refunds, we have determined that Worldpay API does not support\\ninquiry about specific refunds, only about an order. Here is a sample response\\nto an inquiry after a refund has occured:\\n```xml\\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n<!DOCTYPE paymentService PUBLIC \"-\/\/WorldPay\/\/DTD WorldPay PaymentService v1\/\/EN\" \"http:\/\/dtd.worldpay.com\/paymentService_v1.dtd\">\\n<paymentService version=\"1.4\" merchantCode=\"MERCHANTCODE\">\\n<reply>\\n<orderStatus orderCode=\"01105e9b-f0e6-446c-a093-a2ef48f5fd39\">\\n<payment>\\n<paymentMethod>VISA-SSL<\/paymentMethod>\\n<paymentMethodDetail>\\n<card type=\"creditcard\" \/>\\n<\/paymentMethodDetail>\\n<amount value=\"500\" currencyCode=\"GBP\" exponent=\"2\" debitCreditIndicator=\"credit\" \/>\\n<lastEvent>REFUNDED<\/lastEvent>\\n<refundReference>refund-1-01105e9b-f0e6-446c-a093-a2ef48f5fd39<\/refundReference>\\n<reference>refund-1-01105e9b-f0e6-446c-a093-a2ef48f5fd39<\/reference>\\n<AuthorisationId id=\"666\" \/>\\n<CVCResultCode description=\"NOT SENT TO ACQUIRER\" \/>\\n<AVSResultCode description=\"NOT SENT TO ACQUIRER\" \/>\\n<cardHolderName><![CDATA[Mr. Payment]]><\/cardHolderName>\\n<issuerCountryCode>N\/A<\/issuerCountryCode>\\n<balance accountType=\"SETTLED_BIBIT_COMMISSION\">\\n<amount value=\"15\" currencyCode=\"GBP\" exponent=\"2\" debitCreditIndicator=\"credit\" \/>\\n<\/balance>\\n<balance accountType=\"SETTLED_BIBIT_NET\">\\n<amount value=\"484\" currencyCode=\"GBP\" exponent=\"2\" debitCreditIndicator=\"credit\" \/>\\n<\/balance>\\n<riskScore value=\"1\" \/>\\n<\/payment>\\n<date dayOfMonth=\"24\" month=\"08\" year=\"2016\" hour=\"9\" minute=\"53\" second=\"0\" \/>\\n<\/orderStatus>\\n<\/reply>\\n<\/paymentService>\\n```\\nthe inquiry command only returns information about the *last* event on an\\norder (`<lastEvent>REFUNDED<\/lastEvent>`). So in the case of an order with two\\npartial refunds, there's a race condition where an inquiry could return\\ninformation about a different refund.\\nConducting an inquiry after a notification adds significant complexity to the\\ncodebase. In particular it opens up the possqibility of a lot of edge cases\\nwhere, for example, an enquiry might indicate a different status than a\\nnotification. This could in theory be legitimate if two status changes occured\\nin quick succession but only the first notification has been delivered. Or it\\ncould indicate a bug or issue.\\nIt's hard to know how to sensibly handle these edge cases, and we don't really\\nhave evidence that they will happen in practice.\\nThere's a further issue in that the current implementation performs inquiries\\nin the same thread that handles the notification HTTP request. This means that\\nthe notification doesn't get an HTTP response until the inquiry roundtrip to\\nWorldpay has also completed. This could be a potential source of bugs.\\nFinally, Worldpay themselves advise in the [Order Modification and Inquiries Guide](http:\/\/support.worldpay.com\/support\/kb\/gg\/pdf\/omoi.pdf):\\n> Although order inquiries can be a useful tool, we recommend that you use\\norder notifications to find out about changes to your transactions. If\\nyou\u2019re set up for notifications, we do all the work - notifications are\\nsent to you automatically when the status of a transaction changes.\\n","Decision":"## Decision\nWe will not do `inquiry` calls after notifications at all, neither for order\\nnotifications, nor refund notifications.\\n","Prediction":"## Decision\nWe will not perform inquiries in the same thread as the notification HTTP request.\\nThis means that we will not be able to detect whether or not a notification has\\nbeen received before performing an inquiry. We will instead use an `inquiryHandler` to\\ncall the Worldpay API every time a notification is received and pass the\\nresponse to the `inquiryHandler`.\\nThis means that we will need to add a new method to the `PaymentService` class:\\n```c#\\npublic class PaymentService : PaymentServiceHandler\\npublic bool InquiryHandler(string inquiryAddress, string inquirySubject, string inquirySubjectText)\\n{...\\n\/\/ do the Worldpay API inquiry...\\nreturn true;\\n}\\n```\\nThis will allow us to detect whether or not a notification has been received.\\nIf it has, the `inquiryHandler` will be called and the response will be passed\\nto the `inquiryHandler` via the `inquiryHandler` `inquiryHandler` parameter.\\nIf it hasn't, the `inquiryHandler` will be called and the response will be\\npassed to the `inquiryHandler` via the `inquiryHandler` `inquiryHandler` parameter.\\nWe"}
{"Context":"## Context and Problem Statement\\nManaging GitHub organizations via UI is restricted to few individuals only and does not offer any review mechanisms. The same apply to repository creation, organization membership management and label management. This ADR focuses on bringing transparency into this aspect of housekeeping within a community and aims to provide a way to individual contributors, so they can influence and propose changes themselves.\\nADR seeks a way to manage GitHub organization as a code, declaratively via GitOps as a natural extension of the core paradigm of Operate First movement.\\n## Decision Drivers\\nImprove transparency, provide auditing and reviews and allow individual contributors to propose changes.\\n","Decision":"## Decision\nImprove transparency, provide auditing and reviews and allow individual contributors to propose changes.\\nChosen option: _\"1. Prow's Peribolos\"_, because it's a declarative solution that is widely embraced in upstream communities like Kubernetes.\\n### Positive Consequences\\n* Repositories can be created declaratively.\\n* Even external users can request themselves to be added to the organization via a pull request.\\n* Teams can be created and maintained declaratively.\\n* Labels can be centrally managed in a unified way as well as updated per repository bases.\\n* Solution can be easily automated via post-submit Prow jobs.\\n### Negative Consequences\\n* Using a declarative configuration may not be as straightforward as clicking buttons in the UI.\\n* Some users can still do manual changes to repositories they own, these changes are undone by the automation.\\n","Prediction":"## Decision\nImprove transparency, provide auditing and reviews and allow individual contributors to propose changes.\\nChosen option: \"Organize code review and organization in GitHub Actions\", because it provides\\nthe most flexibility, allows for better organization of code review and organization in GitHub Actions.\\n### Positive Consequences\\n* Enables GitHub organization management via GitOps\\n* Provides the most flexibility to manage code review and organization in GitHub Actions\\n* Allows for better organization of code review and organization in GitHub Actions\\n### Negative Consequences\\n* Requires a separate action for code review\\n* Requires a separate action for organization in GitHub Actions\\n* Need to create separate action for organization in GitHub Actions\\n* Need to create separate action for code review for organization in GitHub Actions\\n* Need to create separate action for organization in GitHub Actions\\n* Need to create separate action for code review for organization in GitHub Actions\\n* Need to create separate action for organization in GitHub Actions\\n* Need to create separate action for organization in GitHub Actions\\n* Need to create separate action for organization in GitHub Actions\\n* Need to create separate action for organization in GitHub Actions\\n* Need to create separate action for organization in GitHub Actions\\n* Need to create separate action for organization in GitHub Actions\\n* Need to create separate"}
{"Context":"## Context\\nGiven Dapr is going out with language specific Actor SDKs, we formally introduced an Actor API into Dapr to make Actors are first-class citizen in Dapr. The goal of this review was to ensure Dapr can provide strong support of Service Fabric stateful actors programming model so that we can offer a migration path to the majority of existing actor users.\\n","Decision":"## Decision\n### Dapr\\n* A separate Actor interface is defined.\\n* Actors should support multiple reminders and timers.\\n* Actor state access methods are encapsulated in the Actor interface itself.\\n* Actor interface shall support updating a group of key-value states in a single operation.\\n* Actor interface shall support deletion of an actor. If the actor is activated when the method is called, the in-flight transaction is allowed to complete, then the actor is deactivated, deleted, with associated state removed.\\n### Non-Dapr\\n* Transaction across multiple API calls is left for future versions, if proven necessary. Due to single-threaded guarantee, such transaction scope might be unnecessary. However, if developer expects an Actor code to behave atomically (in an implied transaction scope), we may have to implement this.\\n","Prediction":"## Decision\nWe will upgrade the Dapr actor SDK to use Service Fabric Actor SDK 2.1.1. This is the latest version of the actor SDK and has been tested against Service Fabric 2.1.0. We will also update the Dapr actor SDK to use Service Fabric Service Bus. Service Fabric provides a highly available and durable messaging platform that is used by a lot of our services.\\n### Upgrade Guide\\n1. Upgrade Dapr actor SDK to Service Fabric 2.1.1\\n2. Upgrade Dapr actor SDK to use Service Fabric Service Bus\\n3. Upgrade Dapr actor SDK to use Service Fabric Service Bus Service Fabric Services\\ntype and Service Fabric Service Bus Service Fabric Namespace\\n4. Run the `aspnet:upgrade` command to update the Dapr actor SDK to use Service Fabric Service Bus\\n5. Run the `aspnet:upgrade` command to update the Dapr actor SDK to use Service Fabric Service Bus Service Fabric Service Bus Service Fabric Services\\n6. Run the `aspnet:upgrade` command to update the Dapr actor SDK to use Service Fabric Service Bus Service Fabric Service Bus Service Fabric Services\\n7. Run the `aspnet:upgrade` command to update the Dapr actor SDK to use"}
{"Context":"## Context\\nAs of version 0.12.0, our implementation of sweep for Cassandra relied upon the getPageWithRangeCreator method of\\nCassandraKeyValueService, which fetches values for all columns and timestamps from Cassandra. In cases where a cell with\\na large value was overwritten (with other large values) many times, this caused Cassandra to run out of memory, even\\nif only a single row was fetched. (internal issue 44272).\\nWe needed a way to run sweep without Cassandra running out of memory in such cases.\\n","Decision":"## Decision\nWe are unaware of any way to fetch columns and timestamps without also temporarily loading values into memory (within\\nCassandra). Therefore, to avoid running out of memory, we needed to make it possible to fetch only a certain number of\\ncells (rather than rows) from Cassandra at once.\\nWe decided to introduce a more granular batching solution. In particular, we decided to page through the\\ncolumns for each row. Briefly, the algorithm implemented is as follows:\\n1. Fetch a number of rows equal to the row batch size (but only one column per row).\\n2. Use CQL queries with limits to collect the <column, timestamp> pairs.\\nFurther, since performing a number of additional CQL queries for every row will be less efficient, and unnecessary for\\nmost users, we decided to add an optional parameter, timestampsGetterBatchSize, in CassandraKVSConfig, and use the method\\ndescribed above only if this parameter is set.\\nWe chose CQL queries over thrift because CQL queries are simpler to write, and have been empirically shown to have\\nhigher throughput.\\n","Prediction":"## Decision\nWe have removed getPageWithRangeCreator() from the library.\\nThis means that all of our implementations of sweep now rely on the \\n\\texttt{SweepReads} and \\texttt{SweepWrites} methods of the \\n\\texttt{Sweep} class, rather than the\\n\\texttt{getPageWithRangeCreator()}.\\nThis change has no impact on the performance of our implementations of sweep in\\ncases where a single large value is being read, as it has already been\\nperformed by the time the value is written to Cassandra.\\nWe have also removed the\\n\\texttt{getPageWithRangeCreator()}\/\\texttt{getRangeWithPageWithOffsetCreator()} method from our\\nimplementation of sweep, as this was causing confusion among users.\\nThis change has also been included in our nuget package.\\n### 0.12.0\\nThe 0.12.0 release of the library introduces a new\\n\\texttt{SweepReads} and \\texttt{SweepWrites} methods of the\\n\\texttt{Sweep} class.\\nThese methods are not part of the current implementation of sweep, and\\nare only included in this release"}
{"Context":"## Context\\nTerraform has recently released version v0.12. [Upgrading to Terraform\\nv0.12](https:\/\/www.terraform.io\/upgrade-guides\/0-12.html) is significantly more\\neffort than previous Terraform upgrades due to a number of\\n[incompatibilities](https:\/\/www.terraform.io\/upgrade-guides\/0-12.html).\\nHypr has clients using the v0.11 of modules at this time. Whilst these clients\\ncontinue to use v0.11 modules, any new modules created should also be available\\nto these clients.\\nTerraform v0.12 provides a significant number of improvements that enable\\nmodules to be less verbose, with less workarounds to achieve the same results as\\nthose written using v0.11.\\n","Decision":"## Decision\nThis module will support both Terraform v0.11 and v0.12 module users.\\nThe `master` branch of this module will contain the v0.12 version of this\\nmodule, whilst a `0.11` branch will hold the v0.11 version. This is a common\\npattern prevalent in the Terraform Module community at this time.\\nUsers should still be encouraged to upgrade their Terraform to v0.12, but by\\nproviding a v0.11 version of the module they do not miss out on the\\nfunctionality provided by the module at this time.\\n","Prediction":"## Decision\nWe will upgrade to Terraform v0.12 as soon as possible.\\nWe will ensure that any new modules created are compatible with v0.11.\\nWe will test whether any new modules are compatible with v0.11 before\\nto release them to the client.\\nWe will remove any modules we no longer need once they are compatible with\\nTerraform v0.12.\\nWe will notify the client of any compatibility issues as soon as possible.\\nWe will continue to support the current versions of Terraform modules.\\nWe will perform this work as part of a larger project.\\nWe will ensure that this work is documented.\\nWe will ensure that this work is on-going.\\nWe will ensure that we have sufficient capacity to support this work.\\nWe will ensure that we have sufficient capacity to support this work.\\nWe will ensure that we have sufficient capacity to support this work.\\nWe will ensure that we have sufficient capacity to support this work.\\nWe will ensure that we have sufficient capacity to support this work.\\nWe will ensure that we have sufficient capacity to support this work.\\nWe will ensure that we have sufficient capacity to support this work.\\nWe will ensure that we have sufficient capacity to support"}
{"Context":"## Context\\nDeployment requirements need to be expressed somehow\u2014either declaratively or in a programming language.\\n","Decision":"## Decision\nIndividual Marain services express their deployment requirements in the form of a set of PowerShell scripts. These will be run in PowerShell core v6.\\n","Prediction":"## Decision\nWe will use YAML to describe all deployment requirements.\\n## Decision\nWe will use YAML to describe all deployment requirements.\\nUse `yaml` to describe your deployment requirements.\\nUse `yml` to describe your deployment requirements.\\nUse `yaml` to describe your deployment requirements.\\nUse `yml` to describe your deployment requirements.\\nUse `yaml` to describe your deployment requirements.\\nUse `yml` to describe your deployment requirements.\\nUse `yaml` to describe your deployment requirements.\\nUse `yml` to describe your deployment requirements.\\nUse `yaml` to describe your deployment requirements.\\nUse `yml` to describe your deployment requirements.\\nUse `yaml` to describe your deployment requirements.\\nUse `yml` to describe your deployment requirements.\\nUse `yaml` to describe your deployment requirements.\\nUse `yml` to describe your deployment requirements.\\nUse `yaml` to describe your deployment requirements.\\nUse `yml` to describe your deployment requirements.\\nUse `yaml` to describe your deployment requirements.\\nUse `yml` to describe your deployment requirements.\\nUse `yaml` to describe your deployment requirements.\\nUse `yml` to describe your deployment requirements"}
{"Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","Prediction":"## Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\nWe will use [Altair](http:\/\/altair.cs.berkeley.edu\/), a tool for documenting architecture decisions.\\nWe will use [Architecture Decision Records, Third Edition](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions-third-edition)\\nWe will use the [architecture decision language](http:\/\/docs.splunk.com\/Documentation\/Splunk\/5.2) in the Altair documentation.\\nWe will use the [SPLUNK-ADM-ADR](https:\/\/github.com\/kevinhuang\/splunk-adr) package for ADR.\\nWe will use the [SPLUNK-ADM-ADR repository](https:\/\/github.com\/kevinhuang\/splunk-adr) for the ADR repository.\\nWe will use the [SPLUNK-ADM-ADR repository style guide](https:\/\/github.com\/kevinhuang\/splunk-adr\/blob\/master\/adr.md) for the ADR style guide.\\nWe will use the"}
{"Context":"## Context\\nApi will be responsible for administering vehicle timetables. It will use map service for real time vehicle tracking and will recive informations about it localizations from any source.\\n","Decision":"## Decision\nTeam decide to make API wich allows to organize timetables, receive vehicles localizations and send informations to front-end api.\\n","Prediction":"## Decision\nWe will use [Open Street Map](http:\/\/www.openstreetmap.org\/) to provide vehicle tracking information in the form of a map.\\nWe will use [OpenStreetMap\u2019s Geoserver](https:\/\/www.openstreetmap.org\/) to provide tracking information in the form of an overlay on a map.\\nWe will use [OpenStreetMap\u2019s Mapnik](https:\/\/github.com\/mapnik\/mapnik) to provide vehicle tracking information in the form of vector tiles.\\nWe will use [GeoServer](https:\/\/www.geoserver.org) to provide vehicle tracking information in the form of an overlay on a map.\\nWe will use [PostGIS](https:\/\/www.postgis.org) to provide vehicle tracking information in the form of a geometry column in a database table.\\nWe will use [PostGIS\u2019 GeoIP](https:\/\/postgis.net\/geoip\/geoip) to provide vehicle tracking information in the form of an IP address.\\nWe will use [PostGIS\u2019 GeoIP\u2019s GeoIP-Spam](https:\/\/postgis.net\/geoip\/geoip-spam) to provide vehicle tracking information in the form of an email address.\\nWe will use [PostGIS\u2019 GeoIP\u2019s GeoIP-S"}
{"Context":"## Context\\nWe need to be able to specify a target and have that target be the focus\\ne.g.\\ngenes, with variations mapped to them\\nvariations, with genes mapped to them\\ntranscripts, with genes mapped to them\\ngenes, with transcripts mapped to them\\nsequences, with genes mapped to them\\ngenes, with sequences mapped to them\\nUltimately, we may also need 3-stage joins as well (expression->genes->variations). The current implementation is too restrictive.\\n","Decision":"## Decision\nRather than overloading a search with complex behaviour, this boils down to needing to have one endpoint per data type\\ne.g.\\n\/api\/genes\/query?fields=[name,genome,variations:{id}]\\n\/api\/variations\/query?fields=[id,genes:{}]\\nThis might use the same search underneath e.g. a transcript-based ESSearch, which automatically does the retargetting and restructures fields and queries accordingly. This allows a merge search to deal with these in batches as required.\\nA join\/merge search would work as follows:\\n* decompose queries and fields to split them into \"from\" and \"to\"\\n* query the primary db including join field\\n* hash results (in batches)\\n* for the hits, query the secondary and then pull back and add to the results\\nNote that pass-through targets are no longer a consideration - a gene search will contain transcripts as before. Note that we may need to flatten out sub-fields in ESSearch though.\\nA merge strategy can be specified to deal with situations where the \"join object\" is already present in the \"from\" results.\\nFor instance, homologues are merged into the \"from\" homologues.\\nA new implementation `JoinMergeSearch` now replaces `JoinAwareSearch`.\\n","Prediction":"## Decision\n- Use a `Target` enum to specify the target to be processed\\n- Use a `Target` enum to specify the target to be processed\\n- Use a `Target` enum to specify the target to be processed\\n- Use a `Target` enum to specify the target to be processed\\n- Use a `Target` enum to specify the target to be processed\\n- Use a `Target` enum to specify the target to be processed\\n- Use a `Target` enum to specify the target to be processed\\n- Use a `Target` enum to specify the target to be processed\\n- Use a `Target` enum to specify the target to be processed\\n- Use a `Target` enum to specify the target to be processed\\n- Use a `Target` enum to specify the target to be processed\\n- Use a `Target` enum to specify the target to be processed\\n- Use a `Target` enum to specify the target to be processed\\n- Use a `Target` enum to specify the target to be processed\\n- Use a `Target` enum to specify the target to be processed\\n- Use a `Target` enum to specify the target to be processed\\n- Use a `Target` enum to specify the target to be"}
{"Context":"## Context\\nWe need a way for the service to automatically and regularly run a task.\\nWe already have Redis available as our caching layer and Sidekiq works great with it.\\nAn alternative could be to use Cron for scheduled tasks and a postgres backed asynchronous jobs, perhaps even run inline. We know how to get Sidekiq running with Docker and reusing Redis (rather than Postgres) for job data that is ephemeral feels a better fit given we already have Redis.\\n","Decision":"## Decision\nUse Sidekiq for processing asynchronous tasks.\\n","Prediction":"## Decision\nUse Cron for scheduled tasks and a postgres backed asynchronous jobs.\\nWe will use Sidekiq as the job queue and Redis as the data store.\\nSidekiq will be used as the job queue and will be configured to run as a daemon when run\\nas a container.\\nRedis will be used as the data store in the Sidekiq queue.\\nSidekiq will be configured to run as a daemon when run as a container.\\nRedis will be used as the data store in the Sidekiq queue.\\nSidekiq will be configured to run as a daemon when run as a container.\\nRedis will be used as the data store in the Sidekiq queue.\\nSidekiq will be configured to run as a daemon when run as a container.\\nRedis will be used as the data store in the Sidekiq queue.\\nSidekiq will be configured to run as a daemon when run as a container.\\nRedis will be used as the data store in the Sidekiq queue.\\nSidekiq will be configured to run as a daemon when run as a container.\\nRedis will be used as the data store in the Sidekiq queue.\\nSidekiq will be configured to run as"}
{"Context":"## Context\\nFor the project a deployment and development mechanism is need to be chosen and adopted.\\n","Decision":"## Decision\n- [Kubernetes](https:\/\/kubernetes.io\/) will be the deployment technology of choice.\\nThe project will be deployed into a kubernetes cluster using [helm](https:\/\/helm.sh\/).\\n- [Docker](https:\/\/www.docker.com\/) will be the container building technology of choice.\\nEvery thing will be directed through docker containers, from development to deployment. CI Pipeline will run inside a docker container as well.\\n","Prediction":"## Decision\nThe deployment and development mechanism will be a docker image.\\nThis will be used to build and deploy the application to production.\\nTo ensure that the application is kept up to date and supported, Docker will be used to build the image.\\nThis will be used to control the version of the application and prevent backwards compatibility issues.\\nThis decision ensures that the application is kept up to date and is supported.\\nIt also provides a means to test the application.\\nDocker will be used to build the image for this project.\\nThis will be used to build the application in a development environment.\\nThis decision ensures that the application is kept up to date and is supported.\\nIt also provides a means to test the application.\\nDocker will be used to build the image for this project.\\nThis will be used to build the application in a development environment.\\nThis decision ensures that the application is kept up to date and is supported.\\nIt also provides a means to test the application.\\nDocker will be used to build the image for this project.\\nThis will be used to build the application in a development environment.\\nThis decision ensures that the application is kept up to date and is supported.\\nIt also provides a means"}
{"Context":"## Context\\nOur [intermediate architecture](.\/007-screen-architecture-v2.md) was a step above the [v1 screen architecture](.\/001-screen-controllers.md). However,\\nit still has problems of its own:\\n- The `UiStateProducer` is still some sort of a **God Class** since it is reponsible for both business logic (what needs to happen) and effects (\\nchanges to the system). This makes testing it cumbersome since the tests are more of integration tests which end up verifying both business logic\\nand implementation details.\\n- The architecture is heavily dependent on [RxJava](https:\/\/github.com\/ReactiveX\/RxJava\/). While `RxJava` is good for a lot of things, the current\\narchitecture encourages us to use it for everything and this eventually leads to the tests and production code becoming increasingly harder to\\nmaintain and refactor.\\n- In addition, `RxJava` has a steep learning curve which requires significant onboarding effort before a new contributor is able to reach an\\nacceptable level of productivity with the current architecture.\\nWe need a scalable framework to build screens that lets us manage business logic and effects, and which is receptive to change.\\n","Decision":"## Decision\n### Goals\\n- Separate business logic, presentation logic, and effects so that they can tested independently of each other.\\n- Make UI state explicit and save\/restore it manually instead of depending on hidden behaviour.\\n- Restrict `RxJava` to managing events and effects, and let the business logic be implemented as pure functions.\\n### Choosing a framework\\nWe evaluated many patterns and frameworks that are common in the industry, including but not limited to:\\n##### [Android recommended architecture](https:\/\/developer.android.com\/jetpack\/docs\/guide)\\nThe Android recommended architecture depends on a few architecture components to work together:\\n- [`ViewModel`](https:\/\/developer.android.com\/topic\/libraries\/architecture\/viewmodel): To retain the state across configuration changes.\\n- [`LiveData`](https:\/\/developer.android.com\/reference\/androidx\/lifecycle\/LiveData): To provide lifecycle-aware reactive notifications to the UI.\\n- [`Lifecyle`](https:\/\/developer.android.com\/reference\/androidx\/lifecycle\/Lifecycle): To automatically manage subscriptions to the reactive\\nnotifications from the `LiveData` instances provided via the `ViewModel`.\\nThis architecture makes sense for new codebases, but this is a codebase with an already existing architecture which does not lend itself well to this\\nspecific setup. Some of the problems are:\\n- The `Lifecycle` component is designed to work with screens that are built on top of `Activity` or `Fragment` classes. However, our current\\narchitecture uses a single `Activity` setup, where the individual screens are implemented using `View` subclasses. The `LiveData` component will not\\ngive us much benefit unless we transition all the `View` based screens to be `Fragment` instances.\\n- We already use `RxJava` for reactive notifications. Using `LiveData` would mean that we would either have to replace usages of `RxJava`\\nwith `LiveData` which is neither feasible not desirable at this point.\\n##### [MvRx, by Airbnb](https:\/\/github.com\/airbnb\/MvRx)\\n`MvRx` is a library that is built on top of `RxJava` and the Android `ViewModel` architecture component. While this is a good architecture, it has a\\ncouple of limitations which stopped us from choosing this:\\n- Its core model is based on `RxJava`. We already have issues because of an overuse of `RxJava` across the app and part of the goals of this new\\narchitecture is to restrict the usage of `RxJava` to limited sections of the codebase.\\n- It does not support custom views and is designed to be used with `Fragment` based screens. This suffers from the same problem as the Android\\nrecommended architecture.\\n##### [MVI](http:\/\/hannesdorfmann.com\/android\/mosby3-mvi-1)\\n`MVI` (Model-View-Intent) was one of the more promising architectures that we reviewed. The problem with `MVI` however, is that it is generally a set\\nof principles more than an architecture. This means that there are many implementations of `MVI` in the industry, and they are all implemented\\ndifferently based on the needs of the project. There is no \"one-way\" to implement it at all.\\nWe took a look at the core principles which most `MVI` implementations are based on, which are similar\\nto [`Redux`](https:\/\/redux.js.org\/introduction\/three-principles).\\n- Single source of truth\\n- State is read-only\\n- Changes are made with pure functions\\nWe decided to look for libraries\/frameworks which are based on these principles and build our new screen architecture based on them.\\n### Result\\nLooking at the frameworks available in the Android world that are built on the [Redux principles](https:\/\/redux.js.org\/introduction\/three-principles),\\nwe found [Mobius](https:\/\/github.com\/spotify\/mobius), a reactive framework for managing state and side-effects, by Spotify.\\nThe [objectives](https:\/\/github.com\/spotify\/mobius\/wiki\/Objectives) of the framework also aligned very well with what we need from the architecture.\\nThus, we decided to use it as the basis for the v3 screen architecture.\\nBasing our new screen architecture on this framework lets us satisfy the following goals:\\n#### Separation of concerns\\nThis is satisfied by Mobius since it enables us to separate concerns at an even more granular level than before.\\nWe have three core components:\\n- [`Update`](..\/mobius\/implementing-a-new-screen.md#update): Responsible for deciding the business logic.\\n- [`EffectHandler`](..\/mobius\/implementing-a-new-screen.md#effecthandler): Responsible for making changes to the system (or the *Real World*).\\n- [`UiRenderer`](..\/mobius\/implementing-a-new-screen.md#uirenderer): Responsible for updating the UI in response to changes to the state.\\nThese components are responsible for discrete parts of the system. These are smaller and more focused, thereby making them easier to test and\\nmaintain.\\n#### Making UI state explicit and support state restoration\\nThis is handled by the `MobiusDelegate` classes for us and enforced by it since the class expects the `Model` to be `Parcelable` by default.\\n#### Restricting `RxJava` usage\\nThe `Update` component here is solely responsible for the business logic and is implemented only as pure functions. The usage of `RxJava` will be\\nlimited to:\\n- Setting up event sources to feed into the Mobius loop.\\n- Perform asynchronous operations in the `EffectHandler` component\\n### Usage\\nThere are two things to consider when we look at this architecture from an implementation perspective:\\n##### How do we create a new screen\\nMobius has its own core components, and we have created our components on top of them in order to build out our screen architecture. These\\ncomponents (both Mobius' and our own) have been documented on [this page](..\/mobius\/implementing-a-new-screen.md).\\n##### How do we migrate an older screen to the new architecture\\nMigrating to the newer architecture is an involved process that requires us to follow deliberate, measured steps. The migration process is detailed\\nin [this document](..\/mobius\/migrating-to-mobius.md)\\n##### Reference\\nA reference implementation of the complete architecture can be found\\nat [this commit](https:\/\/github.com\/simpledotorg\/simple-android\/tree\/6da548b36c3cceb3e3db344c09a0f5ae588fc2c0\/app\/src\/main\/java\/org\/simple\/clinic\/settings\/changelanguage)\\n.\\n","Prediction":"## Decision\nWe will use [Android Architecture Components](https:\/\/developer.android.com\/about\/versions\/27+apps\/architectural-components.html) to build\\nthe screens. We will use [RxJava](https:\/\/github.com\/ReactiveX\/RxJava) for the messaging layer.\\nWe will use [Android Studio 3.0](https:\/\/developer.android.com\/studio) as the IDE.\\nWe will use [AndroidX](https:\/\/developer.android.com\/projects\/androidx) as the framework for building a modular architecture.\\nWe will use [AndroidX Fragments](https:\/\/developer.android.com\/design\/architecture\/fragments.html) as the view layer.\\nWe will use [AndroidX Utilities](https:\/\/developer.android.com\/studio\/projects\/androidx-utils.html) as the library layer.\\nWe will use [AndroidX Coroutines](https:\/\/developer.android.com\/studio\/projects\/androidx-coroutines.html) for event-based\\nsynchronization between views.\\nWe will use [AndroidX LiveData](https:\/\/developer.android.com\/studio\/projects\/androidx-lifecycle.html) as the view layer.\\nWe will use [AndroidX Fragments](https:\/\/developer.android.com\/studio\/projects\/androidx-fragments.html) as the view layer.\\nWe will"}
{"Context":"## Context\\nFollowing on from [8. Session storage using an encrypted cookie](0008-session-storage-using-an-encrypted-cookie.md)\\n* The encrypted cookie will need to be encrypted using a key.\\n* Keys should be able to be rotated easily and often.\\n* Key rotations should have no effect on active users.\\n","Decision":"## Decision\n* We will use AWS' KMS to manage our encryption keys.\\n* Keys will be cached at the contained level in volatile memory.\\n","Prediction":"## Decision\nWe will use a key rotation mechanism for the encrypted cookie.\\nThis means that keys will be rotated on a regular basis.\\nThis will be done by the `cookie` package and should be done by the web server.\\nThe rotation will be via a cron job (cron) that runs every 24 hours.\\nThis will be secured by using the `cron` package and a password to access the key.\\nThis will be done by the web server and will be stored in the `~\/.zshrc` file.\\nThis will be stored in a file called `cron.sh` in the same directory as the encrypted cookie file.\\nThis will be run by the web server and will be run as root.\\nThis will be stored in the `~\/.zshrc` file.\\nThis will be stored in a file called `cron.sh` in the same directory as the encrypted cookie file.\\nThis will be run by the web server and will be run as root.\\nThis will be stored in the `~\/.zshrc` file.\\nThis will be stored in a file called `cron.sh` in the same directory as the encrypted cookie file.\\nThis will be run by the web server and will be"}
{"Context":"## Context\\nAs part of on-going development, it has been noticed (thanks Norris) that the\\ntranpilation is not what was expected and we should use a later version of\\nECMA script to target.\\n","Decision":"## Decision\nFrom [here](https:\/\/kangax.github.io\/compat-table\/es2016plus\/#node12_11)\\nThe suggestion is to use node v12 and target \"ES2019\" in your `tsconfig.json`.\\nThe minor node version will be updated to match the minor version of the Alpine docker image for node (currently 12.15)\\n### Discussion\\nDone.\\n","Prediction":"## Decision\nWe will use the latest version of ECMA script in our project.\\nThis will allow us to target the latest version of the language\\nwithout having to wait until the next release.\\nThis decision will be revisited in 12 months.\\n### Positive Consequences\\n* **ECMA Script 2018.3** is the latest version of the language and\\nECMA script will be the only supported version in the future.\\n* **ECMA Script 2018.3 is the latest version of the language and\\nECMA script will be the only supported version in the future.\\n* **ECMA Script 2018.3 is the latest version of the language and\\nECMA script will be the only supported version in the future.\\n* **ECMA Script 2018.3 is the latest version of the language and\\nECMA script will be the only supported version in the future.\\n* **ECMA Script 2018.3 is the latest version of the language and\\nECMA script will be the only supported version in the future.\\n* **ECMA Script 2018.3 is the latest version of the language and\\nECMA script will be the only supported version in the future.\\"}
{"Context":"## Context\\nMost of the business logic for the backend services (HTTP APIs, async event pocessing) will be deployed on [Azure Functions](https:\/\/azure.microsoft.com\/en-us\/services\/functions\/) (Azure's serverless, reactive framework).\\nAzure Functions [provides 1st class support](https:\/\/docs.microsoft.com\/en-us\/azure\/azure-functions\/functions-compare-logic-apps-ms-flow-webjobs) for the .NET\/C#\/F# and the Node\/Javascript runtimes.\\n|                  | .NET\/C#\/F# | Node\/Javascript |\\n| ---------------- | ---------- | --------------- |\\n| Support on Azure | 1st        | 1st             |\\n| Open Source      | No (1)     | Yes             |\\n| Typesafe         | Yes        | Yes (2)         |\\n| Performance      | High       | Medium          |\\n| Traction         | Medium     | High            |\\n| Tools            | Best       | Good            |\\n| Ecosystem        | Good       | Best            |\\n*   (1) Only the core .NET runtime is open source (MONO)\\n*   (2) Type safety can be achieved with Typescript or Flow\\n","Decision":"## Decision\nWe decide to implement the backend logic using [TypeScript](https:\/\/www.typescriptlang.org\/), a typed superset of Javascript that compiles to Javascript.\\nWe favor TypeScript due to its superior tooling and workflow, compared to other alternatives (see for instance a [comparison of TypeScript against Flow](https:\/\/github.com\/niieani\/typescript-vs-flowtype)).\\n","Prediction":"## Decision\nUse Typesafe for the .NET runtime and Node\/Javascript for the backend services.\\nThe decision is based on the following considerations:\\n*   .NET Core is the recommended runtime for the .NET backend services (see [dotnet\/core](https:\/\/docs.microsoft.com\/en-us\/aspnet\/core\/)).\\n*   Mono is still supported for some .NET runtime features (see [dotnet\/runtime](https:\/\/docs.microsoft.com\/en-us\/aspnet\/core\/fundamentals\/runtime)).\\n*   Mono is still supported for the Node\/Javascript runtime (see [dotnet\/runtime](https:\/\/docs.microsoft.com\/en-us\/aspnet\/core\/fundamentals\/runtime)).\\n*   Mono is still supported for the Typesafe runtime (see [typesafe\/runtime](https:\/\/www.typesafe.com\/products\/typesafe\/)).\\n*   Mono is still supported for the Typesafe runtime and .NET Core (see [typesafe\/runtime](https:\/\/www.typesafe.com\/products\/typesafe\/)).\\n*   Mono is still supported for the Typesafe runtime and Node\/Javascript (see [typesafe\/runtime](https:\/\/www.typesafe.com\/products\/typesafe\/)).\\n*   Mono is still supported for the Typesafe runtime and ."}
{"Context":"## Context and Problem Statement\\nAPI Platform recommends using IRI as an identifier. That identifier gives more clarity than an id because it contains more information - a full endpoint path to the resource and its unique identifier. On resources, API Platform handles IRI out of the box.\\nWhile we were designing Sylius new API, we have decided to use commands in many endpoints.\\nIn some cases, this solution is more flexible than the default creation approach (as we have control over what we are processing), but it doesn't handle the transformation of IRI to proper `code`\/`id`.\\nIn the past, we have been using `code`\/`id` instead of IRI, then we have been using both approaches.\\nNow we are trying to unify the new API and replace codes and ids with IRI everywhere.\\nThe main challenge is the usage of IRI in requests, where we want to have IRI in the request but `id`\/`code`in its commands and command handlers.\\n","Decision":"## Decision\nChosen option: \"Handling and transforming IRI to `id`\/`code`\". Request that is based on command and needed information like `code`\/`id` should get it as IRI\\n","Prediction":"## Decision\nWe will use `code`\/`id` for commands and handlers.\\nWe will use `code`\/`id` for requests and commands.\\nWe will use `uri` for command handlers.\\nWe will use `uri` for requests and commands.\\nWe will use `uri` for requests and commands.\\nWe will use `uri` for requests and commands.\\nWe will use `uri` for commands.\\nWe will use `uri` for commands.\\nWe will use `uri` for commands.\\nWe will use `uri` for commands.\\nWe will use `uri` for commands.\\nWe will use `uri` for commands.\\nWe will use `uri` for commands.\\nWe will use `uri` for commands.\\nWe will use `uri` for commands.\\nWe will use `uri` for commands.\\nWe will use `uri` for commands.\\nWe will use `uri` for commands.\\nWe will use `uri` for commands.\\nWe will use `uri` for commands.\\nWe will use `uri` for commands.\\nWe will use `uri` for commands.\\nWe will use `uri` for commands.\\nWe will use `"}
{"Context":"## Context\\nWhen building new micro-services defaults were given to some configuration values so\\napplications would work locally 'out of the box'. This was not done for all values\\nor in all services.\\nThere was discussion around whether it would be better for the service to\\nerror\/exit if any configuration value was not provided on startup, to be clear\\non what needs to be set.\\n","Decision":"## Decision\nAll services will be built with default configuration values set - with defaults\\nthat will most likely work when running services locally.\\n","Prediction":"## Decision\nAll configuration values are now provided on startup\\n(see https:\/\/github.com\/0x0d\/0x0d\/pull\/182).\\nThis will also be used to change default values on an application basis, for example\\nif a logging provider is used in a service that is not already configured to use it.\\nThis is currently implemented as a separate service, but will be merged into the\\nservice that is currently used to configure the service and would require\\nupdating that service to use the new service.\\nThis will also be used to change default values on an application basis, for example\\nif a logging provider is used in a service that is not already configured to use it.\\nThis is currently implemented as a separate service, but will be merged into the\\nservice that is currently used to configure the service and would require\\nupdating that service to use the new service.\\n### Changelog\\n* 0.1.0 - Initial version\\n- Initial changes to how configuration is provided on startup\\n- Initial changes to how configuration is changed on an application basis\\n## Decision\nAll configuration values are now provided on startup\\n(see https:\/\/github.com\/0x0d\/0x0d\/pull\/182).\\"}
{"Context":"## Context and Problem Statement\\nWhat are the main components of the library?\\n","Decision":"## Decision\nChosen option is to implement as a **single class**, as the foreseen complexity of the library does not justify the extra complexity of developing a facade pattern implementation.\\n","Prediction":"## Decision\nChosen option: \"Maven\", because it is already used at the project, it is a mature tool with a lot of experience, and it is the one of the most popular build tools on the market. It also allows us to have full control over the build process.\\n### Positive Consequences <!-- optional -->\\n* It is already used by the team\\n* It allows us to have full control over the build process\\n### Negative Consequences <!-- optional -->\\n* It is not the most popular build tool on the market\\n* It is not the most mature build tool on the market\\n* It is not as mature as Ant or Maven\\n* It is not as mature as Gradle\\n* It is not as mature as Ant or Maven\\n* It is not as mature as Gradle\\n* It is not as mature as Gradle\\n* It is not as mature as Gradle\\n* It is not as mature as Gradle\\n* It is not as mature as Gradle\\n* It is not as mature as Gradle\\n* It is not as mature as Gradle\\n* It is not as mature as Gradle\\n* It is not as mature as Gradle\\n* It is not as mature as Grad"}
{"Context":"## Context\\n- Full mode: full mode does not have the capability to become a validator.\\n- Validator mode : this mode is exactly same as existing state machine behavior. sync without voting on consensus, and participate consensus when fully synced\\n- Seed mode : lightweight seed node maintaining an address book, p2p like [TenderSeed](https:\/\/gitlab.com\/polychainlabs\/tenderseed)\\n","Decision":"## Decision\nWe would like to suggest a simple Tendermint mode abstraction. These modes will live under one binary, and when initializing a node the user will be able to specify which node they would like to create.\\n- Which reactor, component to include for each node\\n- full\\n- switch, transport\\n- reactors\\n- mempool\\n- consensus\\n- evidence\\n- blockchain\\n- p2p\/pex\\n- statesync\\n- rpc (safe connections only)\\n- *~~no privValidator(priv_validator_key.json, priv_validator_state.json)~~*\\n- validator\\n- switch, transport\\n- reactors\\n- mempool\\n- consensus\\n- evidence\\n- blockchain\\n- p2p\/pex\\n- statesync\\n- rpc (safe connections only)\\n- with privValidator(priv_validator_key.json, priv_validator_state.json)\\n- seed\\n- switch, transport\\n- reactor\\n- p2p\/pex\\n- Configuration, cli command\\n- We would like to suggest by introducing `mode` parameter in `config.toml` and cli\\n- <span v-pre>`mode = \"{{ .BaseConfig.Mode }}\"`<\/span> in `config.toml`\\n- `tendermint start --mode validator`  in cli\\n- full | validator | seednode\\n- There will be no default. Users will need to specify when they run `tendermint init`\\n- RPC modification\\n- `host:26657\/status`\\n- return empty `validator_info` when in full mode\\n- no rpc server in seednode\\n- Where to modify in codebase\\n- Add  switch for `config.Mode` on `node\/node.go:DefaultNewNode`\\n- If `config.Mode==validator`, call default `NewNode` (current logic)\\n- If `config.Mode==full`, call `NewNode` with `nil` `privValidator` (do not load or generation)\\n- Need to add exception routine for `nil` `privValidator` to related functions\\n- If `config.Mode==seed`, call `NewSeedNode` (seed node version of `node\/node.go:NewNode`)\\n- Need to add exception routine for `nil` `reactor`, `component` to related functions\\n","Prediction":"## Decision\n- We will keep validator mode as a lightweight seed node maintaining an address book keeping the data in the `wallets` subdirectory\\n- Seed mode will be a full node maintaining the state in `wallets` and voting on consensus\\n- Validator mode will become a validator\\n- We will keep the existing `wallets` directory structure to maintain the existing state machine behavior.\\n- We will create a new directory `wallets` and move the existing state machine to it\\n- We will rename `wallets` and `wallet` to `wallets2` and `wallet` to `wallets`\\n- We will rename `wallet` to `wallets2` and `wallet`\\n- We will rename `state_machine` to `wallets`\\n- We will rename `wallet` to `wallets`\\n- We will rename `state_machine` to `wallets`\\n- We will create a new directory `wallets2` and move the existing state machine to it\\n- State machine will keep the existing state machine behavior\\n- State machine will be a validator\\n- State machine will become a validator\\n- State machine will keep the existing state machine behavior\\n- State machine will be a validator\\n- State"}
{"Context":"## Context\\nExisting verify configuration for things like metadata can be quite\\nintimidatingly complex. We'd like the application to be as simple as\\npossible to configure.\\n","Decision":"## Decision\nWe'll try to make the config have sane defaults wherever possible\\nso that the user doesn't have to specify things they shouldn't care\\nabout.\\nThe user shouldn't have to specify things we don't use (e.g. truststorePath).\\n","Prediction":"## Decision\nWe will use the [Rust Type System](https:\/\/docs.rs\/type\/) to define\\nthe type system for our application. We will use the `#[derive]\\n` trait to define the type system.\\nWe will define our own crate `types` module and implement the\\n`#[derive(Struct)]` trait.\\nWe will define our own crate `impl<T> Type for T` and use the\\n`#[derive(Struct)]` trait to define the type system.\\nWe will define our own crate `impl<T> Type for T + Debug` and\\nuse the `#[derive(Struct)]` trait to define the type system.\\nWe will define our own crate `impl<T> Type for T + Info` and use the\\n`#[derive(Struct)]` trait to define the type system.\\nWe will define our own crate `impl<T> Type for T + Known` and use the\\n`#[derive(Struct)]` trait to define the type system.\\nWe will define our own crate `impl<T> Type for T + Known + Error` and use the\\n`#[derive(Struct)]` trait to define the type system.\\nWe will define our own crate `impl<T>"}
{"Context":"## Context\\nBehavior-driven development (BDD) allows for a broader team to collaborate on software development. The business stakeholders have insight into how well the software meets the requirements. Through automated tests they have a way to validate the functionality in a user friendly way. Cucumber or in our case cucumber-js is a top tool to provide a friendly language for all team members. To implement cucumber-js, additional tools are needed to provide a way to run automated browser tests. Here, Puppeteer as well as selenium webdriver both of which are popular tools to enable browser automation were evalueated.\\n","Decision":"## Decision\nTo go along with cucumber-js Puppeteer was selected. Puppeteer offers more control over Chromium based browsers, very fast, headless by default, run mode, and it can take screenshots of the pages, both in an image and PDF formats. Additionally, it measures rendering and load times by Chrome Performance Analysis tool and it is easier to set up than selenium webdriver. The drawback to using Puppeteer is it's lack of full cross-browser support, which is offered by the selenium webdriver, since Puppeteer is specialized for Chromium.\\n","Prediction":"## Decision\nIn order to use cucumber, we will use puppeteer and selenium webdriver as the two most popular tools to enable browser automation for the application. Puppeteer is a popular tool and has a well known integration with selenium webdriver. This will be used to provide browser automation to the application. Selenium webdriver is also well known and has a well developed integration with the selenium framework. It is also well supported by the community and has a well known reputation as a well supported tool to run automated browser tests. This will be used to provide browser automation to the application.\\nPuppeteer will be used to provide browser automation for the application. This will be used to run automated browser tests. Puppeteer will be used to provide browser automation for the application. This will be used to run automated browser tests.\\nSelenium webdriver will be used to provide browser automation for the application. This will be used to run automated browser tests. Selenium webdriver will be used to provide browser automation for the application. This will be used to run automated browser tests.\\nThe decision is to use puppeteer and selenium webdriver as the tools to provide browser automation for the application. This will be used to run automated browser tests.\\nThis decision has been documented in the [decision log](https:\/\/github"}
{"Context":"## Context\\nWe wanted to open up access to tenant applications in our production environment.\\nAs part of an earlier story, Pingdom checks were set up for a healthcheck application in CI, Staging, and Production. At this stage applications were not accessible from non-office IP addresses.\\nThe problem we faced was a need to test the code introduced to make our production environment accessible to the world. Initially, we suggested applying the same change to our staging environment. However, this approach means all applications in staging will be accessible from anywhere.\\nIf we use Pingdom to assert an application is accessible from the outside world then we need to remove the explicit rules (security groups) allowing Pingdom traffic. This means our CI environment would not be accessible to Pingdom probes.\\n* [#116104189 - set up Pingdom](https:\/\/www.pivotaltracker.com\/story\/show\/116104189)\\n* [#115347323 - allow public access to tenant applications](https:\/\/www.pivotaltracker.com\/story\/show\/115347323)\\n","Decision":"## Decision\nIt was decided we would make the staging environment accessible to the outside world as well as production, and define future work for removing the CI Pingdom check and security groups allowing Pingdom probes, and setting up tests from the pipeline which use the Pingdom API.\\nGiven that the advantages relate to the availability of our production environment, they outweigh not having an automated healthcheck on an application in our CI environment. However, we remain open to hearing solutions to providing healthchecks for CI in future.\\n","Prediction":"## Decision\nWe will open up Pingdom access to our tenant applications. This will allow us to set up a healthcheck and\/or load test application in CI, Staging, and Production. This will allow us to assert the application is accessible from the outside world and will allow us to perform tests from Pingdom probes.\\nWe will use the [Pivotal Health Checks](https:\/\/www.pivotal.io\/docs\/monitoring\/health-checks) plugin to set up the Pingdom health checks. We will also use the [Pivotal Health](https:\/\/www.pivotal.io\/docs\/monitoring\/health-checks) plugin to set up the Pingdom load tests.\\nWe will use the [Pivotal Load](https:\/\/www.pivotal.io\/docs\/monitoring\/monitoring-load) and the [Pivotal Health](https:\/\/www.pivotal.io\/docs\/monitoring\/health-checks) plugins to set up the Pingdom health checks. We will also use the [Pivotal Load](https:\/\/www.pivotal.io\/docs\/monitoring\/monitoring-load) and the [Pivotal Health](https:\/\/www.pivotal.io\/docs\/monitoring\/health-checks) plugins to set up"}
{"Context":"## Context\\nThe primary (only) consuming application for this API needs to show both open\\nand nearby services on separate pages (and more of them). Previously the\\napplication had shown a mix of open and nearby services within a\\nsingle page.\\nHaving the API so closely aligned to the needs of the consumer is not ideal.\\nThere is scope to increase the flexibility of the API along with increasing the\\nease with which it can be used both by the current and future consumers.\\n","Decision":"## Decision\nThe decision is to add a new endpoint i.e. `\/open` alongside the current\\n`\/nearby` endpoint. The former endpoint will return only services that are open\\nwhere the latter will be refactored to return only services that are nearby\\nregardless of their opening state.\\n","Prediction":"## Decision\nThe decision is to refactor the current API to use a separate API\\nand associated UI to show the separate services.\\nThe new API will be called `NearbyServices` and will be used as a\\nfallback to the existing API. A new view will be added to the\\nearestServices page to show the services.\\nThis decision is based on the following:\\n* The API needs to allow the consumer to show the nearest\\nservices for a given location.\\n* The API needs to allow the consumer to show the\\nnearest services within a given distance.\\n* The API needs to allow the consumer to show\\nthe nearest services for a given service type.\\n* The API needs to allow the consumer to show\\nthe nearest services for a given service\\nname.\\n* The API needs to allow the consumer to show\\nthe nearest services for a given service\\nlocation.\\n* The API needs to allow the consumer to show\\nthe nearest services for a given service\\ntype.\\n* The API needs to allow the consumer to show\\nthe nearest services for a given service\\nname.\\n* The API needs to allow the consumer to show\\nthe nearest services for a given service\\nlocation.\\n* The API needs to allow"}
{"Context":"## Context\\nThe Modernisation Platform team and its users need a way to store secrets securely. There are several different methods currently used across the MoJ, including [Secrets Manager](https:\/\/aws.amazon.com\/secrets-manager\/), [Parameter Store](https:\/\/docs.aws.amazon.com\/systems-manager\/latest\/userguide\/systems-manager-parameter-store.html), [LastPass](https:\/\/www.lastpass.com\/) and [Git-Crypt](https:\/\/github.com\/AGWA\/git-crypt).\\nThere are also other well known industry solutions such as [HashiCorp Vault](https:\/\/www.vaultproject.io\/). We want to have a consistent solution across the Modernisation Platform.\\n","Decision":"## Decision\nWe've decided to use [Secrets Manager](https:\/\/aws.amazon.com\/secrets-manager\/) for our secrets storage.\\nParameter store can be used to store non secret parameters if needed for environment specific configuration, but the first choice should be using an app_variables.json like [here](https:\/\/github.com\/ministryofjustice\/modernisation-platform-environments\/blob\/main\/terraform\/environments\/sprinkler\/app_variables.json)\\n","Prediction":"## Decision\nWe will use HashiCorp Vault as the storage solution. This will allow us to easily manage secrets across the platform.\\nWe will use a multi-tenancy solution for the Vault to allow us to separate the data from other users.\\nWe will use a self-managed Vault to allow us to set up the Vault and its secrets on every instance.\\nWe will use the [Kubernetes Vault](https:\/\/kubernetes.io\/docs\/concepts\/understand\/kubernetes-apiserver\/vault\/) Helm chart to install the Vault on every instance.\\nWe will use the [Kubernetes Helm Chart](https:\/\/kubernetes.io\/docs\/concepts\/administration\/helm\/) Helm chart to manage the Vault. Helm charts are a popular way to manage Kubernetes components.\\nWe will use the [Kubernetes Helm Chart](https:\/\/kubernetes.io\/docs\/concepts\/administration\/helm\/) Helm chart to manage the Vault secrets.\\nWe will use the [AWS Secrets Manager](https:\/\/aws.amazon.com\/secrets-manager\/) Helm chart to manage the Vault secrets.\\nWe will use the [AWS Secrets Manager](https:\/\/aws.amazon.com\/secrets-manager\/) Helm chart to manage the Vault secrets.\\nWe will use the [AWS Secrets Manager](https:\/\/aws.amazon.com"}
{"Context":"## Context\\nApplications using Docker will sometimes need an ECR repo to store their Docker images.  Since we use a separate AWS account for each application environment, this will need to be in a shared location.\\nSome options which were considered:\\n* ECR repository per application in the shared services account\\n* Use a service provided by Operations Engineering (there is only Docker Hub currently)\\n* Use a hosted service such as Docker Hub\\n* ECR repository per application in one of the applications accounts which is then cross shared to other accounts\\n","Decision":"## Decision\nWe will create application ECR repositories in the shared-services account, and share them to the relevant accounts and account deployment CICD users.\\nFor the moment we will create repositories on behalf of teams as and when they need them [here](https:\/\/github.com\/ministryofjustice\/modernisation-platform\/blob\/main\/terraform\/environments\/core-shared-services\/ecr_repos.tf) using the ECR module.\\n","Prediction":"## Decision\nWe will use a shared ECR repository per application per account.  This will be stored in the shared VPC and managed by the AWS Ops team.\\nWe will use a service such as Docker Hub to manage the ECR repository.\\nWe will use a shared VPC and IAM to manage the ECR repository and the AWS Ops team will provision a public\/private key pair for the ECR service\\nWe will use SSM to manage the ECR service\\nWe will use the AWS CLI to manage the ECR service\\nWe will use Terraform to manage the ECR service\\nWe will use the AWS CloudFormation template to manage the ECR service\\nWe will use the AWS CodeBuild build and deployment pipeline to manage the ECR service\\nWe will use the AWS CodeCommit source code control system to manage the ECR service\\nWe will use the AWS CodePipeline build and release pipeline to manage the ECR service\\nWe will use the AWS CodeBuild build and deployment pipeline to manage the ECR service\\nWe will use the AWS CodePipeline build and release pipeline to manage the ECR service\\nWe will use the AWS CodeBuild build and deployment pipeline to manage the ECR service\\nWe will use the AWS CodeBuild build and deployment pipeline"}
{"Context":"## Context\\nThe software could be developed in one big (Gradle) project.\\nThis would make integration easier.\\nAt the same time this would make re-use of the code outside of this project harder.\\nOne big project would probably lead to worse code since there would not be the need to have defined API boundaries.\\n","Decision":"## Decision\nWe will try to modularize the software and will use multiple projects to achieve this goal.\\n","Prediction":"## Decision\nWe will use a Gradle plugin to bundle the software. This will allow us to reuse the code outside of the project.\\nWe will use the plugin to build and test the software. This will allow us to easily test the software.\\nWe will use the plugin to package the software for distribution. This will allow us to easily distribute the software.\\nWe will use the plugin to package the application for distribution. This will allow us to easily distribute the application.\\nWe will use the Gradle plugin to package the application for distribution. This will allow us to easily distribute the application.\\nWe will use the Gradle plugin to package the application for distribution. This will allow us to easily distribute the application.\\nWe will use the Gradle plugin to package the application for distribution. This will allow us to easily distribute the application.\\nWe will use the Gradle plugin to package the application for distribution. This will allow us to easily distribute the application.\\nWe will use the Gradle plugin to package the application for distribution. This will allow us to easily distribute the application.\\nWe will use the Gradle plugin to package the application for distribution. This will allow us to easily distribute the application.\\nWe will use the Gradle plugin to"}
{"Context":"## Context\\nWe need a authentication system for the frontend service.\\nThe authentication method must be publicly accessed.\\n","Decision":"## Decision\nA few authentication options were suggested and discussed:\\n- username\/password\\n- social authentication (facebook, twitter, etc.)\\n- passwordless email link authentication (also known as magic link)\\nIt was decided that we would use passwordless email link authentication to authenticate our public users (grant\\napplicants).\\nThe user will enter their email address into a login page served by the frontend service. The service will then\\nsend a authentication link containing a one time use token in the url. The user can then visit this url to start or\\ncontinue their application.\\nConstraints\\n- The token will be limited to one time use.\\n- The token will have a datetime expiry assigned to it.\\n- On issuing a new token to a user all previous tokens will be invalidated.\\n","Prediction":"## Decision\nWe chose the [JWT](https:\/\/jwt.io\/) protocol to authenticate users.\\nThe [JWT](https:\/\/jwt.io\/) protocol is a simple, open-source, [JSON Web Token](https:\/\/jwt.io\/) protocol that is widely used and supported by many\\n[OpenID](https:\/\/openid.net\/)\\n[Web Sockets](https:\/\/en.wikipedia.org\/wiki\/Web_Sockets)\\n[REST](https:\/\/en.wikipedia.org\/wiki\/Representational_state_transfer)\\n[HTTP](https:\/\/en.wikipedia.org\/wiki\/HTTP)\\nWe chose JWT over the more standard [OAuth](https:\/\/oauth.net\/)\\n[Authorization Code](https:\/\/en.wikipedia.org\/wiki\/Authorization_code) protocol as it is easier to understand and implement.\\nThe JWT protocol is a little more complex than the authorization code protocol.\\nHowever, JWT is supported by many\\n[OpenID](https:\/\/openid.net\/) clients, and we can use it to replace the authorization code protocol.\\nWe can also use JWT to implement a client-side access token.\\nThis is a way of authenticating a client without requiring them to provide an access\\ntoken in the HTTP request.\\nWe can use the JWT to replace the HTTP Basic Access token\\n[REST](https"}
{"Context":"## Context\\nSnippet Templates have been historically a combination of HTML, CSS and\\nJavascript with Jinja2 variables. Template is saved in the database, through the\\n`SnippetTemplate` Django Model. Upon save Django would extract the Jinja2\\nvariables and use this list to auto-generate the Snippet Admin UI.\\nIn the Snippet Admin UI, the users would select from a drop-down a Template and\\nthen a list of Inputs, Images, Textareas and Booleans would appear to represent the\\nJinja2 variables of the selected template.\\nUpon save the values of those inputs would be combined into a JSON blob and\\nsaved in the database in the Snippets Django Model.\\nThis system allowed for fast iteration on templates without the need of\\nredeploying the application. It served us well for all these years but imposes a\\nlarge number of limitations and shortcomings like:\\n- The template forms are dynamic and not real Django forms. Thus we cannot take\\nbenefit of the excellent Django form validation. Bogus input is a daily\\nproblem Snippet Editors fight with reviews.\\nConverting Templates to real models would allow us to:\\n- Validate type of input: text, link, URLs.\\n- Enforce secure links.\\n- For icons validate, type, size and dimensions of the image and thus\\nincrease the quality of the snippet.\\n- Have complex validations like require an input when another input is set.\\n- Give better error messages.\\n- Have an overall better Admin UI.\\n- Snippet Editors have to re-upload icons over and over again since all images\\nare saved as part of a blob and not in a gallery.\\n- Snippet Bundles are huge because they include the full blobs. Moving images\\noutside of the bundle would drastically decrease the bundle size and reduce\\nthe CDN transfer costs. We also expect benefits on the CPU and Memory\\nrequirements on the server side and faster Activity Stream page on the client\\nside due to the reduced size of IndexedDB.\\n### Why is this now possible?\\n- Deploys used to be painful and now are not.\\n- Also the template code is now part of Firefox and don't change that often.\\n","Decision":"## Decision\nWe decide to move templates into real Django models. Each ASR Template will get\\nit's own Django Model and entry in the Django Admin.\\nSimilarly images will be moved to a new Django model named `Icons` and templates\\nwill link to those icons.\\nEach `ASRSnippet` will link to one `Template` obj using Django's model\\ninheritance.\\nThis change will affect only the `ASRSnippet` models and the pre-ASR and\\n`JSONSnippet` implementations will remain the same until decommissioned.\\nScripts to migrate from the current system the new system for all `ASRSnippets`\\nwill be created.\\n","Prediction":"## Decision\nWe will use the `django-embed-html` and `django-embed-sass` package to create\\nSnippets Templates.\\nIn order to support Snippets Templates we need to add a new Django property\\n`django_extensions` to the `django_extensions_model` Django Model. This\\nproperty will hold the list of extensions that are supported by the Snippets\\nManager and will be passed to the Snippet Template during the\\n`django_extensions_template` Django Property. In order to support\\nSnippets Templates we need to add a new Django property `django_extensions_template_url`. This property will be used during the\\n`django_extensions_template` Django Property to create the Snippet\\nTemplate.\\n### Snippets Templates.\\nSnippets Templates are just HTML files with the following structure:\\n* `snippet.html` - The HTML snippet template.\\n* `snippet_backbone.js` - The JavaScript snippet template.\\n* `snippet_backbone.js.map` - The JavaScript snippet template map.\\n* `snippet_backbone.js.map.js.map` - The JavaScript snippet template map.\\n* `snippet_backbone.js.map.js.map.js.map.js.map.js.map.js.map.js.js.map.js.js.js` - The JavaScript snippet template map.\\n*"}
{"Context":"## Context\\nAutoFlow aims to provide a method for automating workflows involving FlowAPI queries. As executable documents, [Jupyter notebooks](https:\/\/jupyter.org\/) provide a medium for users to define workflows (using FlowClient to communicate with FlowAPI) in a familiar analysis environment, and supply these workflows to AutoFlow to be scheduled and executed. Additionally, Jupyter's markdown cells and inline display of outputs (including images and markdown) make Jupyter notebooks suitable candidates for producing static reports (using [nbconvert](https:\/\/nbconvert.readthedocs.io\/en\/latest\/)).\\nAnother advantage of using Jupyter notebooks for automated workflows is that if any errors occur during execution, these errors will be displayed inline within the notebook, aiding debugging.\\nWith the decision to automate running notebooks comes the need to parametrise the notebooks at execution time (otherwise we would be repeatedly running an identical notebook and getting the same output). AutoFlow will use [Papermill](https:\/\/papermill.readthedocs.io) to parametrise and execute Jupyter notebooks. Additionally, [scrapbook](https:\/\/nteract-scrapbook.readthedocs.io) (formerly a part of the papermill library) can be used to persist data in a notebook so that it can be re-used in a later notebook. This allows for building workflows from multiple notebooks, for example to produce a daily PDF report that compiles results from multiple separate analyses.\\n","Decision":"## Decision\nAutoFlow will use Jupyter notebooks as the central user-defined components of workflows (both for defining queries to be run and for producing the content of output reports), and will use Papermill to parametrise and execute the notebooks. Scrapbook will also be installed in an AutoFlow deployment, so that users can share data between several notebooks in a workflow.\\n","Prediction":"## Decision\nWe will use [Papermill](https:\/\/papermill.readthedocs.io) to parametrise and run Jupyter notebooks in AutoFlow.\\nWe will use [scrapbook](https:\/\/nteract-scrapbook.readthedocs.io) to persist data in a notebook so that it can be re-used in a later notebook.\\nThis decision has been made as part of a larger change to the workflow automation architecture. As part of this change, we will be switching from the [Papermill library](https:\/\/papermill.readthedocs.io\/en\/latest\/) to [scrapbook](https:\/\/nteract-scrapbook.readthedocs.io).\\nPapermill is a well-established tool that is widely used in the data science community, but it is not widely supported by the data science community. In particular, it is not widely supported by the data science community on Python 3.6 and below.\\nscrapbook is a free, widely used and well-supported tool that is used extensively by the data science community. The data science community has been using it for a number of years and it is not widely supported by the data science community.\\nPapermill has been used in AutoFlow since it was first released, but it is"}
{"Context":"## Context\\nIn the current Cosmos SDK documentation on the [Object-Capability Model](https:\/\/docs.cosmos.network\/main\/learn\/advanced\/ocap#ocaps-in-practice), it is stated that:\\n> We assume that a thriving ecosystem of Cosmos SDK modules that are easy to compose into a blockchain application will contain faulty or malicious modules.\\nThere is currently not a thriving ecosystem of Cosmos SDK modules. We hypothesize that this is in part due to:\\n1. lack of a stable v1.0 Cosmos SDK to build modules off of. Module interfaces are changing, sometimes dramatically, from\\npoint release to point release, often for good reasons, but this does not create a stable foundation to build on.\\n2. lack of a properly implemented object capability or even object-oriented encapsulation system which makes refactors\\nof module keeper interfaces inevitable because the current interfaces are poorly constrained.\\n### `x\/bank` Case Study\\nCurrently the `x\/bank` keeper gives pretty much unrestricted access to any module which references it. For instance, the\\n`SetBalance` method allows the caller to set the balance of any account to anything, bypassing even proper tracking of supply.\\nThere appears to have been some later attempts to implement some semblance of OCAPs using module-level minting, staking\\nand burning permissions. These permissions allow a module to mint, burn or delegate tokens with reference to the module\u2019s\\nown account. These permissions are actually stored as a `[]string` array on the `ModuleAccount` type in state.\\nHowever, these permissions don\u2019t really do much. They control what modules can be referenced in the `MintCoins`,\\n`BurnCoins` and `DelegateCoins***` methods, but for one there is no unique object capability token that controls access \u2014\\njust a simple string. So the `x\/upgrade` module could mint tokens for the `x\/staking` module simple by calling\\n`MintCoins(\u201cstaking\u201d)`. Furthermore, all modules which have access to these keeper methods, also have access to\\n`SetBalance` negating any other attempt at OCAPs and breaking even basic object-oriented encapsulation.\\n","Decision":"## Decision\nBased on [ADR-021](.\/adr-021-protobuf-query-encoding.md) and [ADR-031](.\/adr-031-msg-service.md), we introduce the\\nInter-Module Communication framework for secure module authorization and OCAPs.\\nWhen implemented, this could also serve as an alternative to the existing paradigm of passing keepers between\\nmodules. The approach outlined here-in is intended to form the basis of a Cosmos SDK v1.0 that provides the necessary\\nstability and encapsulation guarantees that allow a thriving module ecosystem to emerge.\\nOf particular note \u2014 the decision is to _enable_ this functionality for modules to adopt at their own discretion.\\nProposals to migrate existing modules to this new paradigm will have to be a separate conversation, potentially\\naddressed as amendments to this ADR.\\n### New \"Keeper\" Paradigm\\nIn [ADR 021](.\/adr-021-protobuf-query-encoding.md), a mechanism for using protobuf service definitions to define queriers\\nwas introduced and in [ADR 31](.\/adr-031-msg-service.md), a mechanism for using protobuf service to define `Msg`s was added.\\nProtobuf service definitions generate two golang interfaces representing the client and server sides of a service plus\\nsome helper code. Here is a minimal example for the bank `cosmos.bank.Msg\/Send` message type:\\n```go\\npackage bank\\ntype MsgClient interface {\\nSend(context.Context, *MsgSend, opts ...grpc.CallOption) (*MsgSendResponse, error)\\n}\\ntype MsgServer interface {\\nSend(context.Context, *MsgSend) (*MsgSendResponse, error)\\n}\\n```\\n[ADR 021](.\/adr-021-protobuf-query-encoding.md) and [ADR 31](.\/adr-031-msg-service.md) specifies how modules can implement the generated `QueryServer`\\nand `MsgServer` interfaces as replacements for the legacy queriers and `Msg` handlers respectively.\\nIn this ADR we explain how modules can make queries and send `Msg`s to other modules using the generated `QueryClient`\\nand `MsgClient` interfaces and propose this mechanism as a replacement for the existing `Keeper` paradigm. To be clear,\\nthis ADR does not necessitate the creation of new protobuf definitions or services. Rather, it leverages the same proto\\nbased service interfaces already used by clients for inter-module communication.\\nUsing this `QueryClient`\/`MsgClient` approach has the following key benefits over exposing keepers to external modules:\\n1. Protobuf types are checked for breaking changes using [buf](https:\/\/buf.build\/docs\/breaking-overview) and because of\\nthe way protobuf is designed this will give us strong backwards compatibility guarantees while allowing for forward\\nevolution.\\n2. The separation between the client and server interfaces will allow us to insert permission checking code in between\\nthe two which checks if one module is authorized to send the specified `Msg` to the other module providing a proper\\nobject capability system (see below).\\n3. The router for inter-module communication gives us a convenient place to handle rollback of transactions,\\nenabling atomicy of operations ([currently a problem](https:\/\/github.com\/cosmos\/cosmos-sdk\/issues\/8030)). Any failure within a module-to-module call would result in a failure of the entire\\ntransaction\\nThis mechanism has the added benefits of:\\n* reducing boilerplate through code generation, and\\n* allowing for modules in other languages either via a VM like CosmWasm or sub-processes using gRPC\\n### Inter-module Communication\\nTo use the `Client` generated by the protobuf compiler we need a `grpc.ClientConn` [interface](https:\/\/github.com\/grpc\/grpc-go\/blob\/v1.49.x\/clientconn.go#L441-L450)\\nimplementation. For this we introduce\\na new type, `ModuleKey`, which implements the `grpc.ClientConn` interface. `ModuleKey` can be thought of as the \"private\\nkey\" corresponding to a module account, where authentication is provided through use of a special `Invoker()` function,\\ndescribed in more detail below.\\nBlockchain users (external clients) use their account's private key to sign transactions containing `Msg`s where they are listed as signers (each\\nmessage specifies required signers with `Msg.GetSigner`). The authentication checks is performed by `AnteHandler`.\\nHere, we extend this process, by allowing modules to be identified in `Msg.GetSigners`. When a module wants to trigger the execution a `Msg` in another module,\\nits `ModuleKey` acts as the sender (through the `ClientConn` interface we describe below) and is set as a sole \"signer\". It's worth to note\\nthat we don't use any cryptographic signature in this case.\\nFor example, module `A` could use its `A.ModuleKey` to create `MsgSend` object for `\/cosmos.bank.Msg\/Send` transaction. `MsgSend` validation\\nwill assure that the `from` account (`A.ModuleKey` in this case) is the signer.\\nHere's an example of a hypothetical module `foo` interacting with `x\/bank`:\\n```go\\npackage foo\\ntype FooMsgServer {\\n\/\/ ...\\nbankQuery bank.QueryClient\\nbankMsg   bank.MsgClient\\n}\\nfunc NewFooMsgServer(moduleKey RootModuleKey, ...) FooMsgServer {\\n\/\/ ...\\nreturn FooMsgServer {\\n\/\/ ...\\nmodouleKey: moduleKey,\\nbankQuery: bank.NewQueryClient(moduleKey),\\nbankMsg: bank.NewMsgClient(moduleKey),\\n}\\n}\\nfunc (foo *FooMsgServer) Bar(ctx context.Context, req *MsgBarRequest) (*MsgBarResponse, error) {\\nbalance, err := foo.bankQuery.Balance(&bank.QueryBalanceRequest{Address: fooMsgServer.moduleKey.Address(), Denom: \"foo\"})\\n...\\nres, err := foo.bankMsg.Send(ctx, &bank.MsgSendRequest{FromAddress: fooMsgServer.moduleKey.Address(), ...})\\n...\\n}\\n```\\nThis design is also intended to be extensible to cover use cases of more fine grained permissioning like minting by\\ndenom prefix being restricted to certain modules (as discussed in\\n[#7459](https:\/\/github.com\/cosmos\/cosmos-sdk\/pull\/7459#discussion_r529545528)).\\n### `ModuleKey`s and `ModuleID`s\\nA `ModuleKey` can be thought of as a \"private key\" for a module account and a `ModuleID` can be thought of as the\\ncorresponding \"public key\". From the [ADR 028](.\/adr-028-public-key-addresses.md), modules can have both a root module account and any number of sub-accounts\\nor derived accounts that can be used for different pools (ex. staking pools) or managed accounts (ex. group\\naccounts). We can also think of module sub-accounts as similar to derived keys - there is a root key and then some\\nderivation path. `ModuleID` is a simple struct which contains the module name and optional \"derivation\" path,\\nand forms its address based on the `AddressHash` method from [the ADR-028](https:\/\/github.com\/cosmos\/cosmos-sdk\/blob\/main\/docs\/architecture\/adr-028-public-key-addresses.md):\\n```go\\ntype ModuleID struct {\\nModuleName string\\nPath []byte\\n}\\nfunc (key ModuleID) Address() []byte {\\nreturn AddressHash(key.ModuleName, key.Path)\\n}\\n```\\nIn addition to being able to generate a `ModuleID` and address, a `ModuleKey` contains a special function called\\n`Invoker` which is the key to safe inter-module access. The `Invoker` creates an `InvokeFn` closure which is used as an `Invoke` method in\\nthe `grpc.ClientConn` interface and under the hood is able to route messages to the appropriate `Msg` and `Query` handlers\\nperforming appropriate security checks on `Msg`s. This allows for even safer inter-module access than keeper's whose\\nprivate member variables could be manipulated through reflection. Golang does not support reflection on a function\\nclosure's captured variables and direct manipulation of memory would be needed for a truly malicious module to bypass\\nthe `ModuleKey` security.\\nThe two `ModuleKey` types are `RootModuleKey` and `DerivedModuleKey`:\\n```go\\ntype Invoker func(callInfo CallInfo) func(ctx context.Context, request, response interface{}, opts ...interface{}) error\\ntype CallInfo {\\nMethod string\\nCaller ModuleID\\n}\\ntype RootModuleKey struct {\\nmoduleName string\\ninvoker Invoker\\n}\\nfunc (rm RootModuleKey) Derive(path []byte) DerivedModuleKey { \/* ... *\/}\\ntype DerivedModuleKey struct {\\nmoduleName string\\npath []byte\\ninvoker Invoker\\n}\\n```\\nA module can get access to a `DerivedModuleKey`, using the `Derive(path []byte)` method on `RootModuleKey` and then\\nwould use this key to authenticate `Msg`s from a sub-account. Ex:\\n```go\\npackage foo\\nfunc (fooMsgServer *MsgServer) Bar(ctx context.Context, req *MsgBar) (*MsgBarResponse, error) {\\nderivedKey := fooMsgServer.moduleKey.Derive(req.SomePath)\\nbankMsgClient := bank.NewMsgClient(derivedKey)\\nres, err := bankMsgClient.Balance(ctx, &bank.MsgSend{FromAddress: derivedKey.Address(), ...})\\n...\\n}\\n```\\nIn this way, a module can gain permissioned access to a root account and any number of sub-accounts and send\\nauthenticated `Msg`s from these accounts. The `Invoker` `callInfo.Caller` parameter is used under the hood to\\ndistinguish between different module accounts, but either way the function returned by `Invoker` only allows `Msg`s\\nfrom either the root or a derived module account to pass through.\\nNote that `Invoker` itself returns a function closure based on the `CallInfo` passed in. This will allow client implementations\\nin the future that cache the invoke function for each method type avoiding the overhead of hash table lookup.\\nThis would reduce the performance overhead of this inter-module communication method to the bare minimum required for\\nchecking permissions.\\nTo re-iterate, the closure only allows access to authorized calls. There is no access to anything else regardless of any\\nname impersonation.\\nBelow is a rough sketch of the implementation of `grpc.ClientConn.Invoke` for `RootModuleKey`:\\n```go\\nfunc (key RootModuleKey) Invoke(ctx context.Context, method string, args, reply interface{}, opts ...grpc.CallOption) error {\\nf := key.invoker(CallInfo {Method: method, Caller: ModuleID {ModuleName: key.moduleName}})\\nreturn f(ctx, args, reply)\\n}\\n```\\n### `AppModule` Wiring and Requirements\\nIn [ADR 031](.\/adr-031-msg-service.md), the `AppModule.RegisterService(Configurator)` method was introduced. To support\\ninter-module communication, we extend the `Configurator` interface to pass in the `ModuleKey` and to allow modules to\\nspecify their dependencies on other modules using `RequireServer()`:\\n```go\\ntype Configurator interface {\\nMsgServer() grpc.Server\\nQueryServer() grpc.Server\\nModuleKey() ModuleKey\\nRequireServer(msgServer interface{})\\n}\\n```\\nThe `ModuleKey` is passed to modules in the `RegisterService` method itself so that `RegisterServices` serves as a single\\nentry point for configuring module services. This is intended to also have the side-effect of greatly reducing boilerplate in\\n`app.go`. For now, `ModuleKey`s will be created based on `AppModule.Name()`, but a more flexible system may be\\nintroduced in the future. The `ModuleManager` will handle creation of module accounts behind the scenes.\\nBecause modules do not get direct access to each other anymore, modules may have unfulfilled dependencies. To make sure\\nthat module dependencies are resolved at startup, the `Configurator.RequireServer` method should be added. The `ModuleManager`\\nwill make sure that all dependencies declared with `RequireServer` can be resolved before the app starts. An example\\nmodule `foo` could declare it's dependency on `x\/bank` like this:\\n```go\\npackage foo\\nfunc (am AppModule) RegisterServices(cfg Configurator) {\\ncfg.RequireServer((*bank.QueryServer)(nil))\\ncfg.RequireServer((*bank.MsgServer)(nil))\\n}\\n```\\n### Security Considerations\\nIn addition to checking for `ModuleKey` permissions, a few additional security precautions will need to be taken by\\nthe underlying router infrastructure.\\n#### Recursion and Re-entry\\nRecursive or re-entrant method invocations pose a potential security threat. This can be a problem if Module A\\ncalls Module B and Module B calls module A again in the same call.\\nOne basic way for the router system to deal with this is to maintain a call stack which prevents a module from\\nbeing referenced more than once in the call stack so that there is no re-entry. A `map[string]interface{}` table\\nin the router could be used to perform this security check.\\n#### Queries\\nQueries in Cosmos SDK are generally un-permissioned so allowing one module to query another module should not pose\\nany major security threats assuming basic precautions are taken. The basic precaution that the router system will\\nneed to take is making sure that the `sdk.Context` passed to query methods does not allow writing to the store. This\\ncan be done for now with a `CacheMultiStore` as is currently done for `BaseApp` queries.\\n### Internal Methods\\nIn many cases, we may wish for modules to call methods on other modules which are not exposed to clients at all. For this\\npurpose, we add the `InternalServer` method to `Configurator`:\\n```go\\ntype Configurator interface {\\nMsgServer() grpc.Server\\nQueryServer() grpc.Server\\nInternalServer() grpc.Server\\n}\\n```\\nAs an example, x\/slashing's Slash must call x\/staking's Slash, but we don't want to expose x\/staking's Slash to end users\\nand clients.\\nInternal protobuf services will be defined in a corresponding `internal.proto` file in the given module's\\nproto package.\\nServices registered against `InternalServer` will be callable from other modules but not by external clients.\\nAn alternative solution to internal-only methods could involve hooks \/ plugins as discussed [here](https:\/\/github.com\/cosmos\/cosmos-sdk\/pull\/7459#issuecomment-733807753).\\nA more detailed evaluation of a hooks \/ plugin system will be addressed later in follow-ups to this ADR or as a separate\\nADR.\\n### Authorization\\nBy default, the inter-module router requires that messages are sent by the first signer returned by `GetSigners`. The\\ninter-module router should also accept authorization middleware such as that provided by [ADR 030](https:\/\/github.com\/cosmos\/cosmos-sdk\/blob\/main\/docs\/architecture\/adr-030-authz-module.md).\\nThis middleware will allow accounts to otherwise specific module accounts to perform actions on their behalf.\\nAuthorization middleware should take into account the need to grant certain modules effectively \"admin\" privileges to\\nother modules. This will be addressed in separate ADRs or updates to this ADR.\\n### Future Work\\nOther future improvements may include:\\n* custom code generation that:\\n* simplifies interfaces (ex. generates code with `sdk.Context` instead of `context.Context`)\\n* optimizes inter-module calls - for instance caching resolved methods after first invocation\\n* combining `StoreKey`s and `ModuleKey`s into a single interface so that modules have a single OCAPs handle\\n* code generation which makes inter-module communication more performant\\n* decoupling `ModuleKey` creation from `AppModule.Name()` so that app's can override root module account names\\n* inter-module hooks and plugins\\n","Prediction":"## Decision\nWe will implement OCAPs for the `x\/bank` keeper module to allow it to control access to the modules it references.\\nWe will create an object capability token for each module, and store the tokens in the `x\/bank` keeper module\u2019s state.\\nWe will implement a module-level `SetBalance` method to allow the caller to set the balance of a module\u2019s account.\\nWe will implement a `MintCoins` method that will mint tokens for the caller and return a new `mintCoins` method that\\nreturns the new minted token, and a `BurnCoins` method that will burn the minted token.\\nWe will implement a `DelegateCoins` method to allow the caller to delegate to a module\u2019s account.\\nWe will implement a `MintCoins` method that will mint tokens for the caller and return a new `mintCoins` method that\\nreturns the new minted token, and a `BurnCoins` method that will burn the minted token.\\nWe will implement a `MintCoins` method that will mint tokens for the caller and return a new `mintCoins` method that\\nreturns the new minted token, and a `BurnCoins` method that will burn the mint"}
{"Context":"Context\\n-------\\nThe public API uses a callback to intercept file modifications:\\n```php\\n$watcher->monitor($paths, function (ModifiedFile $file) {\\n\/\/ do something\\n});\\n```\\nA promise based API would be better:\\n```php\\n$stream = $watcher->monitor($paths);\\nwhile (null !== $modifiedFile = yield $stream->wait()) {\\n\/\/ do something\\n}\\n```\\nThe problem is that the internals for this code are far more complicated as we\\nneed to manage the backlog. Some of the strategies involve multiple processes\\nfrom which we need to read the streams.\\nSo for example, for the find watcher we would need to maintain the state of\\neach of the find processes. As we can only return one promise at a time, and a\\nprocess may return multiple files at a time, we need to maintain a queue of\\nfile modifications for the next call to `wait`.\\nAnother simple Amp solution is to use an emitter, in which case the API would\\nbe consumed as:\\n```php\\n$iterator = $watcher->watch($paths);\\nwhile (yield $iterator->advance()) {\\n$modifiedFile = $iterator->getCurrent();\\n\/\/ do something\\n}\\n```\\nThis has about the same complexity as the callback solution, but the API\\ndoesn't seem to be much better - and at least with the callback we can use a\\ntype hint.\\nDecision\\n--------\\nKeep the simple callback interface.\\nConsequences\\n------------\\nIt's not the best API, but it keeps the rest of the code simple and relatively\\neasy to understand.\\n","Decision":"## Decision\n--------\\nKeep the simple callback interface.\\nConsequences\\n------------\\nIt's not the best API, but it keeps the rest of the code simple and relatively\\neasy to understand.\\n","Prediction":"## Decision\n--------\\nKeep the simple callback interface.\\nConsequences\\n------------\\nIt's not the best API, but it keeps the rest of the code simple and relatively\\neasy to understand.\\nDone in 2016-10-11.\\n## Decision\n--------\\nKeep the simple callback interface.\\nConsequences\\n------------\\nIt's not the best API, but it keeps the rest of the code simple and relatively\\neasy to understand.\\nDone in 2016-10-11.\\nTODO: add some examples of how to use the new API.\\n```\\n- [Dockerfile](https:\/\/github.com\/ethereum\/ethereum\/blob\/master\/ethereum\/ethereum-1.9.1\/dockerfile) --\\n- [Docker Compose](https:\/\/github.com\/ethereum\/ethereum\/blob\/master\/ethereum\/ethereum-1.9.1\/docker-compose.yml) --\\n- [Docker Compose file](https:\/\/github.com\/ethereum\/ethereum\/blob\/master\/ethereum\/ethereum-1.9.1\/docker-compose.yml#L2) --\\n- [Dockerfile.lock](https:\/\/github.com\/ethereum\/ethereum\/blob\/master\/ethereum\/ethereum-1."}
{"Context":"## Context\\nWe feel the need to define a uniform way of setting entrypoints for Cloud Functions within the Google Cloud Platform\\n","Decision":"## Decision\n- The entrypoint should have the same name as the function directory.\\n- The entrypoint name should be clear and descriptive.\\n- The entrypoint should be in `main.py`.\\n- The entrypoint should not contain business logic.\\n","Prediction":"## Decision\nWe have decided to use the [GCP Application Load Balancer](https:\/\/cloud.google.com\/appengine\/docs\/lambda\/#app-lb) as the entrypoint for Cloud Functions. This allows us to use a load-balancer for each function, and route traffic to the function based on the function name.\\nWe have also decided to use the [Lambda API Gateway](https:\/\/cloud.google.com\/appengine\/docs\/lambda\/#api) as the API Gateway for Cloud Functions. This allows us to use a load-balancer for each function, and route traffic to the function based on the function name.\\nWe have decided to use the [Cloud Endpoints API](https:\/\/cloud.google.com\/endpoints\/) as the API Gateway for Cloud Functions. This allows us to use a load-balancer for each function, and route traffic to the function based on the function name.\\nWe have decided to use the [Cloud Functions Runbook](https:\/\/cloud.google.com\/functions\/docs\/runbooks) as the CI\/CD tool for Cloud Functions. This allows us to use a load-balancer for each function, and route traffic to the function based on the function name.\\nWe have decided to use the [Google Cloud Run](https:\/\/cloud.google.com\/run) as"}
{"Context":"## Context\\nA microservices architecture is complex and we'll need to make many decisions.\\nWe'll need a way to keep track of the important decision we make, so that we can revisit and re-evalute them in the future.\\nWe'd prefer to use a lightweight, text-based solution so that we don't have to install any new software tools.\\n","Decision":"## Decision\nWe've decided to use [Michael Nygard's lightweight architectural decision record (LADR)](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions) format.\\nLADR is text based and is lightweight enough to meet our needs.\\nWe'll keep each LADR record in its own text file and manage the files like code.\\nWe also considered the following alternative solutions:\\n* Project management tooling (not selected, because we didn't want to install tools)\\n* Informal or \"word of mouth\" record keeping (not reliable)\\n","Prediction":"## Decision\nWe will use [trello](https:\/\/trello.com\/) for decision support.\\nWe will use [slack](https:\/\/slack.com\/) for notifications.\\nWe will use [json](https:\/\/json.org\/) for configuration.\\nWe will use [d3.js](https:\/\/d3js.org) for visualization.\\nWe will use [react-socket.io](https:\/\/socket.io) for notification and chat.\\nWe will use [react-router](https:\/\/facebook.github.io\/react-router\/) for routing.\\nWe will use [redux-thunk](https:\/\/redux.js.org) for state management.\\nWe will use [redux-saga](https:\/\/redux-saga.io) for asynchronous messaging.\\nWe will use [redux-thunk-legacy](https:\/\/redux-thunk.js.org\/legacy) for legacy state management.\\nWe will use [redux-form](https:\/\/redux-form.com) for form handling.\\nWe will use [react-router-dom](https:\/\/facebook.github.io\/react-router\/) for routing.\\nWe will use [react-router-dom](https:\/\/facebook.github.io\/react-router\/) for routing.\\nWe will use [react-select](https:\/\/facebook.github.io\/react-select\/) for user interface.\\nWe will use ["}
{"Context":"## Context\\nThe first draft release has to be highly configurable with few efforts.\\n","Decision":"## Decision\nAll the deployment phases are done using docker (compose). Local and prod yml\\nfiles govern such a deployment.\\n","Prediction":"## Decision\nThe first release will be a [minimal](https:\/\/github.com\/kriskowal\/minimal) version of the application.\\nIt will have a minimal set of features, that can be configured with the `config.yml` file.\\nIt will be a [single page application](https:\/\/en.wikipedia.org\/wiki\/Single_page_application) with a REST API.\\nIt will use [REST](https:\/\/en.wikipedia.org\/wiki\/Representational_state_transfer) and [JSON](https:\/\/en.wikipedia.org\/wiki\/JSON) as the underlying transport protocol for data requests and responses.\\nIt will be built using [Django](https:\/\/www.djangoproject.com\/).\\n### Features\\n* [Minimal](https:\/\/github.com\/kriskowal\/minimal) and lightweight application with minimal configuration.\\n* [REST API](https:\/\/en.wikipedia.org\/wiki\/Representational_state_transfer) and JSON transport protocol for data requests and responses.\\n* Minimal configuration for setting up the application.\\n* Minimal configuration for setting up the database.\\n* Minimal configuration for setting up the database authentication.\\n* Minimal configuration for setting up the database schema.\\n* Minimal configuration for setting up the database schema validation.\\n* Minimal configuration for setting up the database"}
{"Context":"## Problem\\nA key name is made out of a sequence of key part names, and can be constructed with `keyAddBaseName\/keySetBaseName`.\\nBoth applications and configuration file formats might need arbitrary strings to be encoded within a key name part.\\nFor example:\\n- an application uses names of internal components as sections within the configuration.\\n- a parser reads an empty string, to be encoded as base name.\\n","Decision":"## Decision\n`keyAddBaseName\/keySetBaseName` never fail with any argument, so any character sequence can be escaped except of NULL bytes.\\nThe argument goes unmodified to the unescaped key name.\\nFor arrays there is no escaping needed because an array is only an array if the metadata `array` is appended to the direct parent key.\\nSee [array](array.md).\\n- [Array](array.md)\\n","Prediction":"## Decision\nWe will use base name encoded as a 32 bit integer, with the key part encoded as a 16 bit integer.\\nAn example is:\\n- keyAddBaseName(0x00000001, 0x00000002)\\n- keyAddBaseName(0x00000003, 0x00000004)\\n- keyAddBaseName(0x00000005, 0x00000006)\\n- keyAddBaseName(0x00000007, 0x00000008)\\n- keyAddBaseName(0x00000009, 0x00000009)\\n- keyAddBaseName(0x0000000a, 0x0000000b)\\n- keyAddBaseName(0x0000000c, 0x0000000d)\\n- keyAddBaseName(0x0000000d, 0x0000000e)\\n- keyAddBaseName(0x0000000f, 0x0000000f)\\n- keyAddBaseName(0x00000010, 0x00000010)\\n- keyAddBaseName(0x00000011, 0"}
{"Context":"## Context\\nDefine a supported installation method as per [ADR-0012](https:\/\/github.com\/home-assistant\/architecture\/blob\/master\/adr\/0012-define-supported-installation-method.md).\\n","Decision":"## Decision\nHome Assistant OS is the full installation of our all-inclusive home automation system. Best in class home automation is complemented with a UI for configuring your system, making backups and safe updates with automatic rollback.\\nThis is the generally recommended installation method and the one that our website and documentation should be focused on.\\n### Supported boards\/hardware\/machines\\n- Home Assistant Yellow\\n- Home Assistant Green\\n- Raspberry Pi 3 Model B and B+ 32-bit\\n- Raspberry Pi 3 Model B and B+ 64-bit\\n- Raspberry Pi 4 Model B 32-bit\\n- Raspberry Pi 4 Model B 64-bit\\n- Tinkerboard\\n- ODROID-C2\\n- ODROID-C4\\n- ODROID-N2\\n- ODROID-M1\\n- ODROID-XU4\\n- Bare-metal on x86-64 PCs (e.g. Intel NUC, via `generic-x86-64` image)\\n- Virtual Machine (x86-64\/AMD64 based by `ova` image)\\n### Supported Operating Systems and versions\\nThis installation method is only available and supported using the Home Assistant Operating System. Only the latest major version is supported.\\nWhen a new major version is released, the previous major version will be dropped with a deprecation period of 2 months. The last 3 minor releases are supported.\\n### Supported Hypervisors\\nThe Home Assistant Operating System can be run on a Hypervisor and thus be run as a virtual machine. The following Hypervisors are supported:\\n- KVM\/QEMU\\n- VirtualBox\\n- VMWare\\n- Xen\\nWe will provide documentation for the following systems build on top of these technologies:\\n- Proxmox (KVM\/QEMU based)\\n- Unraid (KVM\/QEMU based)\\n- VirtualBox\\n- VMWare\\n### Required Expertise\\n- **Installation**\\nInstallation requires the user to install the disk image for their device, install an image flasher (Etcher) and flash the image to the disk. Then insert image and boot their device.\\n* **Start when the system is started:** Managed by installer\\n* **Run with full network access:** Managed by installer\\n* **Access USB devices:** Managed by supervisor\\n* **Maintaining the Home Assistant installation**\\nOngoing maintenance can be done via the user interface. User will be notified if an update is available for the operating system or for Home Assistant. Updates will be automatically rolled back if they fail.\\n- **Python upgrades:** Managed via HA updates\\n- **Installing Python dependencies:** Managed via HA updates\\n- **Updating Home Assistant:** Via a UI\\n- **Maintaining the Operating System**\\nOngoing maintenance can be done via the user interface. User will be notified if an update is available for the operating system. Updates will be automatically rolled back if they fail.\\n* **Security updates for OS:** Managed via HA OS updates\\n* **Maintaining the components required for the Supervisor:** Managed via HA OS updates\\n**Conclusion:** low expertise required. Besides the installation methods, everything is done via the UI.\\n","Prediction":"## Decision\nAssumptions:\\n* We are using a docker image.\\n* We deploy to a docker registry.\\n* We use a python package to install the components we need.\\n* We use the home-assistant component installer to install components.\\n* We use the home-assistant component installer to provision the devices.\\n* We use the home-assistant component installer to provision the services.\\n* We set up and provision the services using the `home-assistant` component installer.\\n* We use the `home-assistant` component installer to provision the services on the controller.\\n* We use the `home-assistant` component installer to provision the services on the thermostat.\\n* We use the `home-assistant` component installer to provision the services on the switches.\\n* We use the `home-assistant` component installer to provision the services on the thermostat.\\n* We use the `home-assistant` component installer to provision the services on the thermostat\\n* We use the `home-assistant` component installer to provision the services on the switches\\n* We use the `home-assistant` component installer to provision the services on the switches\\n* We use the `home-assistant` component installer to provision the services on"}
{"Context":"Context\\n=======\\nWe use RDS for tenant databases. Tenants can bind more than one application to a database instance created via the services console. Database migrations were broken when a binding was removed and re-added, or when another bound application other than the one that created the database tables attempted to modify them.\\nPreviously the RDS broker PostgreSQL engine copied the MySQL engine and granted all rights on the database to the newly created user. In PostgreSQL this will give the user rights to create tables, but because it has a more finely-grained permission model than MySQL this does not give the user rights on existing tables, or tables newly created by other users.\\nOnly the owner of a table can alter\/drop it, and you cannot grant this permission to other users. Users who are the owners of tables cannot be removed until the table ownership is changed.\\nWe attempted to work around the PostgreSQL permissions system in the following ways:\\n* Using [`ALTER DEFAULT PRIVILEGES`](https:\/\/www.postgresql.org\/docs\/9.5\/static\/sql-alterdefaultprivileges.html) on every newly created user to `GRANT ALL ON PUBLIC` - this means that every user can `SELECT`, `INSERT`, and `DELETE`, but because only the table owner can `ALTER` or `DROP` this will not allow other bound users to run migrations. This is also limited to the `PUBLIC` (default) schema, so would fail to work for any applications that have custom schemas.\\n* Making the group the owner of the `PUBLIC` schema. This allowed members of the group to `DROP` tables within the schema, but still did not allow them to `ALTER` these tables.\\n* Creating a \"parent\" group that is given ownership of all tables, sequences, and functions. New users are granted rights on this group and thus can carry out all owner actions on group-owned items. A [DDL event trigger](https:\/\/www.postgresql.org\/docs\/9.5\/static\/event-trigger-definition.html) using the built-in `REASSIGN OWNED BY` command handed over ownership of all resources owned by the current user to the parent `$DBNAME_owner` group. This worked perfectly on all the PostgreSQL databases we tested with, however RDS does not allow use of event triggers so it was unworkable in production.\\nDecision\\n========\\nWe decided the simplest solution was to issue the same credentials to all applications bound to the database. This means that the same user will be used by each application, and they will not suffer ownership problems when issuing `DROP` or `ALTER` commands.\\nRather than deterministically deriving the credentials we decided to store them in the backing database itself meaning they can be changed at any time in case of a breach. To protect against database breaches that may leak stored credentials we also decided to encrypt the credentials using a symmetric key held by the broker.\\nWe accepted that in the future we may wish to extend this by allowing the broker to issue read-only credentials, or credentials that are suitable for `SELECT`, `INSERT`, and `DELETE` operations, but not schema modifications.\\nStatus\\n======\\nAccepted\\nConsequences\\n============\\nWe return the same credentials to all apps bound to the same PostgreSQL database (RDS) instance.\\n","Decision":"## Decision\n========\\nWe decided the simplest solution was to issue the same credentials to all applications bound to the database. This means that the same user will be used by each application, and they will not suffer ownership problems when issuing `DROP` or `ALTER` commands.\\nRather than deterministically deriving the credentials we decided to store them in the backing database itself meaning they can be changed at any time in case of a breach. To protect against database breaches that may leak stored credentials we also decided to encrypt the credentials using a symmetric key held by the broker.\\nWe accepted that in the future we may wish to extend this by allowing the broker to issue read-only credentials, or credentials that are suitable for `SELECT`, `INSERT`, and `DELETE` operations, but not schema modifications.\\nStatus\\n======\\nAccepted\\nConsequences\\n============\\nWe return the same credentials to all apps bound to the same PostgreSQL database (RDS) instance.\\n","Prediction":"## Decision\n========\\nWe decided the simplest solution was to issue the same credentials to all applications bound to the database. This means that the same user will be used by each application, and they will not suffer ownership problems when issuing `DROP` or `ALTER` commands.\\nRather than deterministically deriving the credentials we decided to store them in the backing database itself meaning they can be changed at any time in case of a breach. To protect against database breaches that may leak stored credentials we also decided to encrypt the credentials using a symmetric key held by the broker.\\nWe accepted that in the future we may wish to extend this by allowing the broker to issue read-only credentials, or credentials that are suitable for `SELECT`, `INSERT`, and `DELETE` operations, but not schema modifications.\\nStatus\\n======\\nAccepted\\nConsequences\\n============\\nWe return the same credentials to all apps bound to the same PostgreSQL database (RDS) instance.\\n## Decision\n========\\nWe decided the simplest solution was to issue the same credentials to all applications bound to the database. This means that the same user will be used by each application, and they will not suffer ownership problems when issuing `DROP` or `ALTER` commands.\\nRather than deterministically"}
{"Context":"## Context\\nThe OpenAPI Specification (OAS) defines a standard, language-agnostic interface to RESTful APIs which allows both humans and computers to discover and understand the capabilities of the service without access to source code, documentation, or through network traffic inspection. When properly defined, a consumer can understand and interact with the remote service with a minimal amount of implementation logic.\\nAn OpenAPI definition can then be used by documentation generation tools to display the API, code generation tools to generate servers and clients in various programming languages, testing tools, and many other use cases.\\n","Decision":"## Decision\nWe will use (when possible) OpenAPI Spec version 3\\n","Prediction":"## Decision\nGiven the importance of OpenAPI to our ecosystem, and the importance of the OpenAPI Specification as a standard for describing APIs, we will use OpenAPI to define and document our APIs.\\nWe will also use OpenAPI to generate documentation and code generation.\\nWe will use the OpenAPI specification as the primary format for our API documentation.\\nWe will use the OpenAPI specification to generate code generation for our API clients and servers.\\nWe will use OpenAPI to generate unit tests for our API clients and servers.\\nWe will use OpenAPI to generate code coverage reports for our API clients and servers.\\nWe will use OpenAPI to generate API test results for our API clients and servers.\\nWe will use OpenAPI to generate a set of API test results that can be used by our CI\/CD system.\\nWe will use OpenAPI to generate a set of API test results that can be used by our automated UI tests.\\nWe will use OpenAPI to generate a set of API test results that can be used by our automated UI tests.\\nWe will use OpenAPI to generate a set of API test results that can be used by our automated UI tests.\\nWe will use OpenAPI to generate a set of API test results that can"}
{"Context":"## Context\\nWe have several TB of images to deliver through a IIIF service, more than any of\\nour physical boxes can currently hold all in one place. We have to pay\\nKakadu a licensing fee in order to get acceptable tile delivery, and the\\nCantaloupe server we use has been proven to be easily Denial-of-Service'd to the\\npoint of freezing and no images being able to be served.\\nWe will remedy this problem of scale by storing Pyramidal Tiffs in the cloud\\nwhich an AWS Lambda service (scale-able to at least a thousand simultaneous\\nrequest) will use to respond to IIIF image server requests, using Northwestern's\\n[serverless-iiif](https:\/\/github.com\/nulib\/serverless-iiif) project.\\n","Decision":"## Decision\n1. Generate Pyramidal tiffs using VIPS and upload them to an AWS bucket using\\n`valkyrie-shrine`.\\n* Any image greater than 15k pixels on the long-side will be downsized by\\nhalf to reduce the derivative size and allow the image server to respond\\nin a reasonable amount of time.\\n2. Configure and deploy\\n[serverless-iiif](https:\/\/github.com\/nulib\/serverless-iiif) to serve IIIF\\nImage API requests using those pyramidal tiffs.\\n3. Configure an Amazon CloudFront cache in front of the lambda to automatically\\ncache tiles and info.jsons for one year.\\n","Prediction":"## Decision\nWe will use the [serverless-iiif](https:\/\/github.com\/nulib\/serverless-iiif) project to store\\nand serve Pyramidal Tiffs from the cloud.\\nWe will use the [iiif-couchdb](https:\/\/github.com\/nulib\/iiif-couchdb) project to store\\nimages in the cloud, allowing them to be served directly from the cloud.\\nWe will use the [iiif-galaxy](https:\/\/github.com\/nulib\/iiif-galaxy) project to store, and\\nserve, images in the cloud.\\nWe will use the [iiif-gzip](https:\/\/github.com\/nulib\/iiif-gzip) project to compress and\\nstore Pyramidal Tiffs, using the [iiif-gzip-caddy](https:\/\/github.com\/nulib\/iiif-gzip-caddy) project\\nto serve them.\\nWe will use the [iiif-s3](https:\/\/github.com\/nulib\/iiif-s3) project to store Pyramidal Tiffs in the\\ncloud. We will use the [iiif-s3-aws-iam](https:\/\/github.com\/nulib\/iiif-s3"}
{"Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"## Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in\\nthis\\n[article](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions)\\n","Prediction":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\nWe will use a tool that supports ADRs, [ADR-Kit](https:\/\/adr-kit.org\/) for this purpose.\\nADR-Kit is a free, open-source tool that supports a lightweight ADR format that is easy to use and understand. The ADRs are created by consensus of the project team. ADRs are not intended to be a full record of the decision-making process, but are intended to be a concise record of the decision itself.\\nWe will use ADR-Kit to create the ADRs.\\nADR-Kit will be used to create the ADRs.\\nADR-Kit will be used to create the ADRs.\\nADR-Kit will be used to create the ADRs.\\nADR-Kit will be used to create the ADRs.\\nADR-Kit will be used to create the ADRs.\\nADR-Kit will be used to create the ADRs.\\nADR-Kit will be used to create the ADRs.\\nADR-Kit will be"}
{"Context":"## Context\\nThroughout the process of development and release, the deployed components, codebase and data may vary in stability and quality (despite all used effort by the teams \ud83d\ude1b). Also, since several teams with different use cases work on the same project, potentially at the same time, there might be collision(s) in development and integration.\\n","Decision":"## Decision\nIn order to ensure that only stable and quality code is shipped to the public viewer (app visitor), we will use several environments. This will allow siloting the code updates in a controlled manner.\\nWe will use the following environment:\\n- **Production**: Visitor facing environment. **It must be stable**, i.e. code and data shipped to production must be of the best quality and validated by product team.\\n- **Staging**: Internal purposed environment. _Components, code and data therein might be unstable_ and thus could potentially not reflect what would be deployed to production. This is the environment used by the product team to validate new improvements, fix and so... Since there is \"only\" one environment of that kind, several updates might concurrently live there.\\n- **Local**: Development purposed environment. Each developer (independently of its team) might use such an environment while developing. Since our stack involves components on GCP and Algolia, the following has been decided: GCP CloudFunctions can be ran locally (refer to `README.md` for usage); Algolia indices can only exist in Algolia (i.e. SAAS), thus one must create its own (i.e. developer bound) index (refer to `functions\/README.md` for usage) and refer to it locally by updating `algolia.index` in the `functions\/.runtimeconfig.json` file.\\n","Prediction":"## Decision\nTo avoid collision(s) in development and integration, we will use CI\/CD pipeline to deploy the components of the application to the staging environment and to the production environment.\\nWe will use the following pipeline to achieve this:\\n- [Build pipeline](https:\/\/docs.microsoft.com\/en-us\/azure\/devops\/pipelines\/artifacts\/build\/)\\n- [Release pipeline](https:\/\/docs.microsoft.com\/en-us\/azure\/devops\/pipelines\/artifacts\/release)\\n- [Staging pipeline](https:\/\/docs.microsoft.com\/en-us\/azure\/devops\/pipelines\/artifacts\/staging)\\n- [Continuous integration pipeline](https:\/\/docs.microsoft.com\/en-us\/azure\/devops\/pipelines\/artifacts\/continuous-integration)\\n- [Continuous deployment pipeline](https:\/\/docs.microsoft.com\/en-us\/azure\/devops\/pipelines\/artifacts\/deployment)\\n- [Reference pipeline](https:\/\/docs.microsoft.com\/en-us\/azure\/devops\/pipelines\/artifacts\/reference)\\n- [Continuous deployment pipeline](https:\/\/docs.microsoft.com\/en-us\/azure\/devops\/pipelines\/artifacts\/deployment)\\n- [Reference pipeline](https:\/\/docs.microsoft.com\/en-us\/azure\/devops\/pipelines\/artifacts\/reference)\\n- [Continuous deployment pipeline](https:\/\/docs.microsoft.com\/en"}
{"Context":"## Context\\nThe final effect of the POC will be static HTML page, with css and javascripts included in the file. Therefore, it will be easy to host it online for demonstration purposes, with configuration for multiple environments.\\n","Decision":"## Decision\nUse S3 static webpage hosting as deployment infrastructure.\\n","Prediction":"## Decision\nWe will build a static website using a simple Ruby on Rails application. The application will serve static HTML to a user, and the application will be deployed to Heroku.\\nWe will use the Heroku CLI to deploy the application to Heroku. Heroku will then serve the static HTML to the user.\\nHeroku will then use the Ruby on Rails application to serve the static HTML to the user.\\nHeroku will then use the Heroku CLI to deploy the static HTML to the user.\\nHeroku will then use the Heroku CLI to deploy the static HTML to the user.\\nHeroku will then use the Heroku CLI to deploy the static HTML to the user.\\nHeroku will then use the Heroku CLI to deploy the static HTML to the user.\\nHeroku will then use the Heroku CLI to deploy the static HTML to the user.\\nHeroku will then use the Heroku CLI to deploy the static HTML to the user.\\nHeroku will then use the Heroku CLI to deploy the static HTML to the user.\\nHeroku will then use the Heroku CLI to deploy the static HTML to the user.\\nHeroku will then use the Heroku CLI to deploy the static HTML to the user.\\"}
{"Context":"## Context\\nThe cloud platforms team currently use a self-host Jenkins server for CI\/CD pipeline. This solution is not cloud native. There is a large amount of custom configuration which has equated to a vast amount of user request tickets and an opaque service which would be very difficult to recover [CPT-364](https:\/\/dsdmoj.atlassian.net\/browse\/CPT-364)  There is also a reoccurring issue which has caused a number of outages [CPT-384](https:\/\/dsdmoj.atlassian.net\/browse\/CPT-384).\\nReasons behind this move were:\\n* Average of almost one week per month spent on debugging, fixing and reviving jenkins\\n* Unnecessary downtimes\\n* With the move to Kubernetes a cloud native CI\/CD solution is needed, all jobs are written as code\\n* Non working jenkins was a blocker for product teams\\n","Decision":"## Decision\nReplace self hosted Jenkins with self hosted Concourse CI pipeline\\n3. Automate deployment of concourse using Terraform and Helm\\n4. WIP\\n~~Product teams have confirmed their success builds and deploys. zero downtime and minimal maintenance from Cloud Platforms side.~~\\n","Prediction":"## Decision\nWe will use [Kubernetes](https:\/\/kubernetes.io\/), which is an open-source container orchestration system, and Jenkins as a self-hosted CI\/CD solution.\\nKubernetes will be used to manage the application infrastructure and services. This will include:\\n* Deployment of the application\\n* Provisioning of the services\\n* Monitoring of the application and services\\n* Configuration management of the application and services\\n* Continuous integration and deployment of the application\\n* Continuous deployment of the application\\n* Continuous deployment of the services\\n* Continuous monitoring of the application and services\\n* Continuous monitoring of the services\\n* Continuous health checks of the application and services\\n* Continuous health checks of the services\\n* Continuous monitoring of the application and services\\n* Continuous monitoring of the services\\n* Continuous monitoring of the applications\\n* Continuous monitoring of the services\\n* Continuous monitoring of the services\\n* Continuous monitoring of the applications\\n* Continuous monitoring of the services\\n* Continuous monitoring of the services\\n* Continuous monitoring of the applications\\n* Continuous monitoring of the services\\n* Continuous monitoring of the services\\n* Continuous monitoring of the services\\n* Continuous monitoring of the services\\n* Continuous monitoring of the services\\n* Continuous monitoring of the services\\n* Continuous monitoring of"}
{"Context":"## Context\\nWe don\u2019t want to put business logic inside Android framework classes (like an `Activity` or `Fragment`) because those cannot be unit tested. To enable\\na fast feedback loop (i.e. tests that run on the JVM and not Android VM), we separate screens and controllers using\\nthe [MVI architecture](https:\/\/medium.com\/@ragunathjawahar\/mvi-series-a-pragmatic-reactive-architecture-for-android-87ca176387d1) [pattern](https:\/\/medium.com\/@ragunathjawahar\/mvi-series-a-pragmatic-reactive-architecture-for-android-87ca176387d1)\\n.\\n","Decision":"## Decision\nEvery screen has one controller that consumes user events, performs business logic with the help of data repositories and communicates UI changes back\\nto the screen.\\nUser interactions happening on the screen are abstracted inside data classes of type `UiEvent`. These events flow to the controller in the form of\\nRxJava streams.\\n```kotlin\\n\/\/ Create the UsernameTextChanged event by listening to the EditText\\nRxTextView\\n.textChanges(usernameEditText)\\n.map { text -> UsernameTextChanged(text) }\\n\/\/ Event\\ndata class UsernameTextChanged(text: String) : UiEvent\\n```\\nThe screen sends a single stream of `UiEvent`s to the controller and gets back a transformed stream of UI changes. The flow of data is\\nuni-directional. To merge multiple streams into one, RxJava\u2019s `merge()`  operator is used.\\n```kotlin\\n\/\/ Login screen\\nObservable.merge(usernameChanges(), passwordChanges(), submitClicks())\\n.compose(controller)\\n.takeUntil(screenDestroy)\\n.subscribe { uiChange -> uiChange(this) }\\n```\\nIn the controller, `UiEvent`s are transformed as per the business logic and `UiChange`s are sent back to the screen. The `UiChange` is a simple lambda\\nfunction that takes the screen itself as an argument, which can call a method implemented by the screen interface.\\n```kotlin\\ntypealias Ui = LoginScreen\\ntypealias UiChange = (LoginScreen) -> Unit\\nclass LoginScreenController : ObservableTransformer<UiEvent, UiChange>() {\\nfun apply(events: Observable<UiEvent>): Observable<UiChange> {\\nevents.ofType<UsernameTextChanged>\\n.map { isValidUsername(it.text) }\\n.map { isValid ->\\n{ ui: Ui -> ui.setSubmitButtonEnabled(isValid) } \/\/ UiChange lambda!\\n}\\n}\\n}\\n```\\nWhen the events have to observed across multiple functions in the controller, the stream is shared using `replay()` + `refCount()` so that the UI\\nevents aren't recreated once for every subscription. `replay()` shares a single subscription to the screen by replaying the events to every observer\\nand `refCount()` keeps the subscription alive as long as there is at least one observer.\\n```kotlin\\nclass LoginScreenController : ObservableTransformer<UiEvent, UiChange>() {\\nfun apply(events: Observable<UiEvent>): Observable<UiChange> {\\nval replayedEvents = events.replay().refCount()\\nreturn Observable.merge(\\nenableSubmitButton(replayedEvents),\\nloginUser(replayedEvents))\\n}\\nfun enableSubmitButton(events: Observable<UiEvent>): Observable<UiChange>()\\nfun loginOnSubmit(events: Observable<UiEvent>): Observable<UiChange>()\\n}\\n```\\n![data flow from the Ui to the controller](images\/diagram_screen_controller.png)\\n([diagram source](https:\/\/docs.google.com\/drawings\/d\/1I_VdUM8Pf9O3nOYViqVF6kiyqFaYFD2fHmKRyvwmEl4\/edit?usp=sharing))\\n","Prediction":"## Decision\nWe will use the MVI pattern to separate our screens and controllers.\\nWe will use [MVI Router](https:\/\/github.com\/ragunathjawahar\/mvi-router) to route\\nrequests to the correct screens and controllers.\\nWe will use [MVI Router](https:\/\/github.com\/ragunathjawahar\/mvi-router) to route requests to\\nthe correct screens and controllers.\\nWe will use [MVI Router](https:\/\/github.com\/ragunathjawahar\/mvi-router) to route requests to the correct screens and controllers.\\nWe will use [MVI Router](https:\/\/github.com\/ragunathjawahar\/mvi-router) to route requests to the correct screens and controllers.\\nWe will use [MVI Router](https:\/\/github.com\/ragunathjawahar\/mvi-router) to route requests to the correct screens and controllers.\\nWe will use [MVI Router](https:\/\/github.com\/ragunathjawahar\/mvi-router) to route requests to the correct screens and controllers.\\nWe will use [MVI Router](https:\/\/github.com\/ragunathjawahar\/mvi-router) to route requests to the correct screens and controllers"}
{"Context":"## Context\\nAs part of the lib-classifier, we are developing a component for displaying multi-frame subjects in the classification interface. Initially, we referred to this viewer as the PagesViewer, but changed the name to multiFrame Viewer to reflect more general use cases. In the first iteration, we will focus on multiImage features necessary for transcription projects. As projects with different requirements, such as ecology projects, move to the new classifier, we can add new features to the filmstrip component (see decisions 6 an 7).\\nSee the [InVision document for the Pages Viewer](https:\/\/projects.invisionapp.com\/d\/main#\/console\/12924056\/393421254\/preview) and Issue #1142.\\n","Decision":"## Decision\n1. **When to Render:** There will be an explicit workflow configuration (i.e., `{ subject_viewer: 'multiFrame' }`) for the multiFrame Viewer. However, as a fallback, the multiFrame subject viewer will render if (1) the subject delivered to the client has multiple frames and (2) the workflow is not configured to display all frames at once.\\n2. **Number of Frames:** The multiFrame Viewer will display up to ten frames. All frames after the tenth frame will be ignored.\\n3. **Frame Display**: For every frame that is an image, the filmstrip will render a thumbnail of the image. The fallback for non image frames will be a dot. Later, we will need to think about subjects with mixed media frames. HTML Inputs will be used instead of HTML buttons to display each frame.\\n4. **Selected Image Display**: The selected image will be displayed using the SingleImageViewer.\\n5. **Location of Filmstrip:** The filmstrip will be located to the left of the displayed image. This will help for images that are tall as well as make the interface easier to use for volunteers navigating by keyboard or screen reader (see https:\/\/github.com\/zooniverse\/front-end-monorepo\/issues\/1262).\\n6. **Playing Slideshow**: Although adding a slideshow play button will be necessary for non-transcription projects, at this time, the play button is a necessary enhancement\/future feature.\\n7. **Collapsing Filmstrip**: Adding a button to collapse the image previews into pagination dots is an enhancement\/future feature.\\n","Prediction":"## Decision\nWe will develop a multiFrame Viewer component. We will use a template component to display the multiImage viewer. The template component will provide the ability to show multiple images in a single frame. The template component will be a React component. We will use [vue-multiframe](https:\/\/github.com\/ProteusSoft\/vue-multiframe) for the multiImage viewer.\\n### Decision Process\\nWe will use [vue-multiframe](https:\/\/github.com\/ProteusSoft\/vue-multiframe) for the multiImage viewer.\\n### Usage\\n### Using the template component to display the multiImage viewer\\nTo use the template component, add the following to your project's components folder:\\n```vue\\n<template lang=\"ts\"><div class=\"multiImageContainer\" style=\"width: 100%; height: 100%; margin-top: 50px\">...<\/div><\/template>\\n```\\n### Using the template component to display the multiImage viewer\\nTo use the template component, add the following to your project's components folder:\\n```vue\\n<template lang=\"ts\"><div class=\"multiImageContainer\" style=\"width: 100%; height: 100%; margin-top: 50px\">...<\/div><\/template>\\n"}
{"Context":"## Context and Problem Statement\\nHow does a client use the interface?\\n","Decision":"## Decision\nChosen option is to implement as a **fluent API**, as early use of the library showed that this was easier to understand and provided a means\\nto expand the evolve the functionality.\\n","Prediction":"## Decision\nChosen option: \"Use a [client-side library](https:\/\/www.npmjs.com\/package\/axios)\", because: \"It's a known and tested way to pass data between the server and the client and it's a good way to avoid unnecessary overhead when dealing with large data sets\".\\n### Positive Consequences <!-- optional -->\\n* [Client-side library](https:\/\/www.npmjs.com\/package\/axios) is used for client-side communication\\n* [Client-side data binding](https:\/\/www.npmjs.com\/package\/react-native-data-binding) is used for data binding\\n### Negative Consequences <!-- optional -->\\n* It's a known and tested way to pass data between the server and the client and it's a good way to avoid unnecessary overhead when dealing with large data sets\\n* [Client-side library](https:\/\/www.npmjs.com\/package\/axios) is not used for data binding\\n* [React Native](https:\/\/facebook.github.io\/react\/) is used for data binding\\n* [React](https:\/\/facebook.github.io\/react\/), [react-native](https:\/\/facebook.github.io\/react\/) and [react](https:\/\/facebook.github.io\/react\/) are used for the client-side rendering of the UI\\n* [React"}
{"Context":"## Context\\nStatic type-checking is available for Python, making us of the type annotations already in the codebase.\\n","Decision":"## Decision\nType-check via [mypy](http:\/\/mypy-lang.org\/).\\n","Prediction":"## Decision\nWe will use static type-checking for Python.\\nThis will make it easier to understand the code, and will help us catch bugs and improve the codebase.\\nThis will also make it easier to debug the code. Code that uses the same type-checking will be easier to find.\\nThis will also make it easier to maintain the codebase, since we will be able to catch bugs and improve the codebase.\\nWe will use the following syntax for type-checking Python code:\\n```\\nif foo is not None: print('foo is not None')\\n```\\nThis will cause the code to be type-checked, and if it is not, it will raise a type error.\\nThis will also cause the code to be debugged.\\nThis will also make it easier to understand the code, since the code will be type-checked and the type of the variable will be indicated.\\nThis will also make it easier to maintain the code, since the code will be type-checked and the type of the variable will be indicated.\\nThis will also make it easier to find bugs and improve the codebase, since the code will be type-checked and the type of the variable will be indicated.\\nThis will also make"}
{"Context":"## Context\\nA primary component of OpenBounty is a web application. As part of our work on this web application we regularly need to implement new UI elements or flows to support overall product development. This frontend work requires usage of CSS to specify positioning, text styles and many more variables.\\nA common problem with CSS is that developers try to generalize CSS classes so that they can be reused (see e.g. [BEM](http:\/\/getbem.com\/)). Arguably the intention is great but inevitably the time will come when constraints change and so the component's CSS is modified. By that time other people may have used that component in other places relying on the current implementation.\\nIn programming languages breaking a collaborator's expectation like this can be mitigated using assertions or automatic tests but this is less easily done when working with CSS.\\n","Decision":"## Decision\nIn order to avoid the problems outlined above we will adopt the approach of using atomic, immutable utility classes as promoted by the [Tachyons](http:\/\/tachyons.io\/) library.\\nTachyons provides safe-to-reuse, single-purpose classes that help with achieving consistent scales of whitespace and font-sizes.\\nBy not modifying the definition of CSS classes anymore we can safely build out UI components using those classes without needing to worry if we're breaking someone else's expectations.\\n","Prediction":"## Decision\nWe will use a style checker to enforce that the current implementation of the CSS conforms to the style rules we have defined. We will also use a style checker to automatically detect style changes that may occur in the future.\\nThis style checker will be written in a language that we are familiar with and that we have access to. It will be written using the [CSS Modules](https:\/\/github.com\/css-modules\/css-modules) style checker library.\\nWe will use [SASS](https:\/\/sass-lang.com\/) as the CSS language to help enforce that the current CSS conforms to the style rules we have defined.\\nWe will use [Webpack](https:\/\/webpack.js.org\/) to bundle our CSS files together into a single file for deployment. Bundling will help us to reduce the size of the CSS file and make it easier to maintain in the future when we are working on other parts of the OpenBounty web application.\\nWe will use [Prettier](https:\/\/prettier.io\/) to automatically format our CSS. Prettier will help us to reduce the amount of errors in our CSS by highlighting syntax errors and style issues.\\nWe will use [React-Bootstrap](https:\/\/reactjs.org\/docs\/bootstrap\/) as a UI framework to help"}
{"Context":"## Context\\nThe SpatioTemporal Asset Catalog (STAC) API specification allows for the use of `next` links in the responses for the the\\n`\/search` and `\/collections\/<id>\/items` endpoints. The specific implementation for creating a token and scheme for constructing\\nthe parameters to get the next page of results is left open to different implementations.\\nAccording to the OGC API - Features specification for links in responses the `next` relation signifies:\\n> the link\u2019s context is a part of a series, and that the next in the series is the link target\\nSince the parameters of the `href` itself is not specified to follow a certain pattern or any additional requirements\\nimplementation itself is left to `franklin`. There are a few common options that have been [documented](https:\/\/www.moesif.com\/blog\/technical\/api-design\/REST-API-Design-Filtering-Sorting-and-Pagination\/) [by](https:\/\/nordicapis.com\/everything-you-need-to-know-about-api-pagination\/) [a](https:\/\/phauer.com\/2015\/restful-api-design-best-practices\/) [number](https:\/\/phauer.com\/2018\/web-api-pagination-timestamp-id-continuation-token\/) [of](https:\/\/developers.facebook.com\/docs\/graph-api\/using-graph-api\/#paging)\\n[existing](https:\/\/developer.github.com\/v3\/#pagination) [resources](https:\/\/developer.spotify.com\/documentation\/web-api\/reference\/object-model\/#paging-object).\\n### limit & offset\\nUsing `offset` and `limit` parameters is a common patten for paginating results. With this approach the `limit`\\nparameter specifies the number of results in a response and the `offset` parameter specifies which particular set of\\nresults out of the total to return. This maps closely to how `limit` and `offset` work in `SQL` already.\\n#### Benefits\\n**Easy to implement**: maps closely to `SQL` and therefore easy to implement\\n**Stateless**: requires no additional context\\n#### Disadvantages\\n**Performance**: can be slow for large datasets\\n**Inconsistent**: depending on sort ordering and the inclusion\/deletion of items the same results can be returned multiple times\\n### keyset\/cursor\/continuation token\\nThe `keyset`, `cursor`, or `continuation` token approach uses an opaque-*ish* token to control the page of results\\nreturned and can also be combined with a `limit` parameter to control the number of results. Often the token used will\\nrepresent a unique identifier for the last result of the current page returned to the client. This token is then used in\\nsubsequent requests use the token in a query parameter (e.g. `\/?next=<token>`).\\n#### Benefits\\n**Performance**: on large datasets with indices performs better than scanning previous results\\n**Consistent Ordering**: even when data is added\/removed results are stable\\n**Can page forward and backwards**: tokens can be used to page before and after search results\\n**Implementation logic hidden from users**: while there may be an interpretation of a continuation token, it is not\\nguaranteed to be consistent or long-lived, which frees up back-end optimizations and changes without breaking the API\\n#### Disadvantages\\n**Implementation more complex**: needs to manage sort ordering to ensure that the continuation token equality check is\\nconsistent even if data is added, removed, or sorted by additional fields\\n**Tokens become invalid**: continuation tokens can become invalid, users should not rely on them for long periods of\\ntime\\n","Decision":"## Decision\nWe will use **continuation** tokens for managing pagination. Keeping the implementation flexible\/hidden\\nfrom end-users so that it can be optimized or changed later without breaking functionality for existing users outweighs\\ndisadvantages around increased complexity for implementation. Additionally, since `franklin` APIs are expected to hold\\nlarge amounts of data (tens of millions of rows) it is important that the pagination approach scales accordingly.\\nLastly, since eventually `franklin` may move beyond relying solely on `postgresl` for managing state it may actually be\\nbetter that we do not use `offset\/limit` which relies heavily on `SQL`.\\n","Prediction":"## Decision\nUse the `next` link to indicate the current page of results in the `search` and `collections\/<id>\/items` endpoints.\\nUse the `keyset`\/`cursor` approach to page forward and backward in the results.\\nUse the `continuation` token approach to page forward and backward in the results.\\nUse the `keyset`\/`cursor` approach to page forward and backward in the results.\\nUse the `keyset`\/`cursor` approach to page forward and backward in the results.\\nUse the `keyset`\/`cursor` approach to page forward and backward in the results.\\nUse the `keyset`\/`cursor` approach to page forward and backward in the results.\\nUse the `keyset`\/`cursor` approach to page forward and backward in the results.\\nUse the `keyset`\/`cursor` approach to page forward and backward in the results.\\nUse the `keyset`\/`cursor` approach to page forward and backward in the results.\\nUse the `keyset`\/`cursor` approach to page forward and backward in the results.\\nUse the `keyset`\/`cursor` approach to page forward and backward in"}
{"Context":"## Context\\nThe resources, database, storage and API endpoints needs to be managed as a unit.\\nWhen build our application with the Serverless framework we get CloudFormation templates that manages our infrastructure in AWS.\\n","Decision":"## Decision\nTo help package the individual AWS resource into a serverless application we will use [Serverless](https:\/\/www.serverless.com\/) framework.\\nWe also considered the following alternative solutions:\\n* Manual creation of resource in AWS throurgh the AWS Console.\\n* AWS Serverless Application Model (AWS SAM)\\n","Prediction":"## Decision\nWe have decided to use the Serverless platform to manage our infrastructure.\\nWe will use the Serverless platform to create the following:\\n- API Gateway:\\n- API Gateway is a service that enables API consumption and management.\\n- API Gateway enables us to deploy our API in an isolated and managed environment.\\n- API Gateway allows us to expose our API in a self-hosted and managed environment.\\n- API Gateway allows us to expose our API in a multi-account environment.\\n- API Gateway allows us to load balance our API, we can use a load balancer and a health check to monitor our API performance.\\n- API Gateway allows us to provision a load balanced environment.\\n- API Gateway allows us to deploy a load balanced environment.\\n- API Gateway allows us to expose our API in an autoscaling environment.\\n- API Gateway allows us to provision an autoscaling environment.\\n- API Gateway allows us to provision a multi-region environment.\\n- API Gateway allows us to provision an autoscaling environment.\\n- API Gateway allows us to provision a multi-region environment.\\n- API Gateway allows us to provision a multi-account environment.\\n- API Gateway allows us to provision a multi-account environment.\\n- API Gateway allows us"}
{"Context":"## Context\\nHaving the ability to sign messages off-chain has proven to be a fundamental aspect of nearly any blockchain. The notion of signing messages off-chain has many added benefits such as saving on computational costs and reducing transaction throughput and overhead. Within the context of the Cosmos, some of the major applications of signing such data includes, but is not limited to, providing a cryptographic secure and verifiable means of proving validator identity and possibly associating it with some other framework or organization. In addition, having the ability to sign Cosmos messages with a Ledger or similar HSM device.\\nFurther context and use cases can be found in the references links.\\n","Decision":"## Decision\nThe aim is being able to sign arbitrary messages, even using Ledger or similar HSM devices.\\nAs a result signed messages should look roughly like Cosmos SDK messages but **must not** be a valid on-chain transaction. `chain-id`, `account_number` and `sequence` can all be assigned invalid values.\\nCosmos SDK 0.40 also introduces a concept of \u201cauth_info\u201d this can specify SIGN_MODES.\\nA spec should include an `auth_info` that supports SIGN_MODE_DIRECT and SIGN_MODE_LEGACY_AMINO.\\nCreate the `offchain` proto definitions, we extend the auth module with `offchain` package to offer functionalities to verify and sign offline messages.\\nAn offchain transaction follows these rules:\\n* the memo must be empty\\n* nonce, sequence number must be equal to 0\\n* chain-id must be equal to \u201c\u201d\\n* fee gas must be equal to 0\\n* fee amount must be an empty array\\nVerification of an offchain transaction follows the same rules as an onchain one, except for the spec differences highlighted above.\\nThe first message added to the `offchain` package is `MsgSignData`.\\n`MsgSignData` allows developers to sign arbitrary bytes valid offchain only. Where `Signer` is the account address of the signer. `Data` is arbitrary bytes which can represent `text`, `files`, `object`s. It's applications developers decision how `Data` should be deserialized, serialized and the object it can represent in their context.\\nIt's applications developers decision how `Data` should be treated, by treated we mean the serialization and deserialization process and the Object `Data` should represent.\\nProto definition:\\n```protobuf\\n\/\/ MsgSignData defines an arbitrary, general-purpose, off-chain message\\nmessage MsgSignData {\\n\/\/ Signer is the sdk.AccAddress of the message signer\\nbytes Signer = 1 [(gogoproto.jsontag) = \"signer\", (gogoproto.casttype) = \"github.com\/cosmos\/cosmos-sdk\/types.AccAddress\"];\\n\/\/ Data represents the raw bytes of the content that is signed (text, json, etc)\\nbytes Data = 2 [(gogoproto.jsontag) = \"data\"];\\n}\\n```\\nSigned MsgSignData json example:\\n```json\\n{\\n\"type\": \"cosmos-sdk\/StdTx\",\\n\"value\": {\\n\"msg\": [\\n{\\n\"type\": \"sign\/MsgSignData\",\\n\"value\": {\\n\"signer\": \"cosmos1hftz5ugqmpg9243xeegsqqav62f8hnywsjr4xr\",\\n\"data\": \"cmFuZG9t\"\\n}\\n}\\n],\\n\"fee\": {\\n\"amount\": [],\\n\"gas\": \"0\"\\n},\\n\"signatures\": [\\n{\\n\"pub_key\": {\\n\"type\": \"tendermint\/PubKeySecp256k1\",\\n\"value\": \"AqnDSiRoFmTPfq97xxEb2VkQ\/Hm28cPsqsZm9jEVsYK9\"\\n},\\n\"signature\": \"8y8i34qJakkjse9pOD2De+dnlc4KvFgh0wQpes4eydN66D9kv7cmCEouRrkka9tlW9cAkIL52ErB+6ye7X5aEg==\"\\n}\\n],\\n\"memo\": \"\"\\n}\\n}\\n```\\n","Prediction":"## Decision\nThe decision is to use the [signing-off-chain](https:\/\/github.com\/ethereum\/sig-signature\/blob\/master\/docs\/signing-off-chain.md) functionality of the [signature library](https:\/\/github.com\/ethereum\/sig-signature) to sign Cosmos messages.\\nAdditionally, the signing-off-chain functionality of the sign library should be used to sign Cosmos messages with the `sign` function. For example, the `sign` function can be used to sign Cosmos messages with the `from` and `to` parameters.\\nAdditionally, the `sign` function can be used to sign Cosmos messages with the `from` and `to` parameters. For example, the `sign` function can be used to sign Cosmos messages with the `from` and `to` parameters.\\nAdditionally, the `sign` function can be used to sign Cosmos messages with the `from` and `to` parameters.\\nAdditionally, the `sign` function can be used to sign Cosmos messages with the `from` and `to` parameters.\\nAdditionally, the `sign` function can be used to sign Cosmos messages with the `from` and `to` parameters.\\nAdditionally, the `sign` function can be used to sign Cosmos messages with"}
{"Context":"## Context and Problem Statement\\nThe new FxA settings page introduces a new service in the form of a GQL API that will provide account and profile data to the browser. This service needs to make api requests to the auth-server on behalf of logged in users. The auth-server uses hawk authentication so adding another service between the browser and auth-server is not straightforward. In [ADR-0017](0017-switch-settings-auth-to-sessiontoken.md) we decided to allow the browser to share the session token with the GQL server so that it can make hawk authenticated requests. This leads us to question whether we should deprecate our use of hawk altogether.\\nLong ago, the precursor to onepw had 2-phase signup\/login that would establish a secret key that would be used to encrypt\/decrypt the sessionToken so that it was never in the clear on the wire. It was secure enough that you could login over HTTP and still be safe. We'd still use TLS, but - defense in depth. Once we had the sessionToken we used Hawk for API requests to maintain that level of security. We implemented this protocol and it worked as designed. The problem at the time was we wanted to support low-end phones on slow networks and the extra round trips meant we were too slow; bad UX. Instead of scrapping the whole thing we modified the protocol to the one we use today. The sessionToken is now sent over TLS in the signup\/login response, but the \"secure even over HTTP\" dream is gone.\\nHawk is designed for authentication over HTTP. Quoting from the design goals of Hawk:\\n- simplify and improve HTTP authentication for services that are unwilling or unable to deploy TLS for all resources,\\n- secure credentials against leakage (e.g., when the client uses some form of dynamic configuration to determine where to send an authenticated request), and\\n- avoid the exposure of credentials sent to a malicious server over an unauthenticated secure channel due to client failure to validate the server's identity as part of its TLS handshake.\\nNone of those scenarios apply to auth-server.\\nDefining the replacement for hawk is out of scope, but let's consider the properties of the interim replacement: using the sessionToken as a bearer token.\\nThe new surface area to which the token is exposed is in the HTTP header of API requests, sent over TLS. Both the client and server storage of the token remains unchanged. A compromised TLS connection would expose the token on more API calls where it would not have been with hawk. The sessionToken is in the body of the login response, so even with hawk that endpoint could leak the token if the TLS connection is compromised.\\nA leaked sessionToken would allow access to personal information, email addresses and device metadata, but not the FxA password, secret keys, or Firefox Sync data.\\n## Decision Drivers\\n- Security\\n","Decision":"## Decision\n- Security\\nWe will stop using hawk in future work that requires authentication. Selecting a preferred scheme is out of scope of this ADR. Sharing the session token with trusted services is an acceptable interim solution.\\n","Prediction":"## Decision\n- Security\\nChosen option: \"Use the sessionToken as a bearer token\", because it's the only option that meets the goal of \"keeping the session token out of the clear\" and also meets the goal of \"making it easy to get a session token\".\\n### Positive Consequences\\n* The sessionToken is never in the clear in the clear\\n* We can use the sessionToken in any request that needs it\\n* The sessionToken is in the clear on the wire\\n### Negative Consequences\\n* The sessionToken is not encrypted\\n* The sessionToken is not shared with the client\\n* The sessionToken is not shared with the server\\n* The sessionToken is not sent over TLS\\n* The sessionToken is not in the clear on the wire\\n* The sessionToken is not shared with the client\\n* The sessionToken is not in the clear on the wire\\n* The sessionToken is not in the clear on the wire\\n* The sessionToken is not in the clear on the wire\\n* The sessionToken is not in the clear on the wire\\n* The sessionToken is not in the clear on the wire\\n* The sessionToken is not in the clear on the wire\\n* The sessionToken is not"}
{"Context":"## Context\\nThe more katas come to life and the more overlap between es6, es7, es8, etc. versions exist the more a structure is\\nneeded on how to allow grouping the katas in any other form.\\nOne example. ES6 has katas in the group \"Array API\", \"Object API\", \"String API\", etc. ES1, ES7, ES10 and other also\\nhave katas that have the same group names and that is logically correct.\\nIn order to make these easy to use and allow grouping, for example all Array-API-katas into one group and present\\nthem on the website (or somewhere else), the groups must be used consistently.\\n","Decision":"## Decision\nMany katas can be put into the following X groups and this is done across the board (not only for language katas,\\nalso for libraries, etc.) in order to have a coherent structure and make it easier for the user's to understand\\nwhere to look for katas.\\n### The \"Xxx API\" katas\\nThe ES6 Promise katas are a very good example (not perfect at this point in time, but very good). The standards\\nare like this:\\n1. The group name is \"Xxx API\", for example \"Promise API\", \"String API\", ...\\n1. The directory name is `...\/xxx-api\/`, for example `es6\/language\/array-api`\\n1. The group contains API katas, such as the `Xxx.method()`, `Xxx.property` katas\\n1. Two additional katas should be provided:\\n1. The \"Basics\" kata, this kata describes basic knowledge that is needed for understanding this topic,\\nfor example promises need some explaination, see the [ES6 Promise \"Basics\" kata][1], which does that very well.\\n1. \"The API\" kata gives a quick overview of the API to understand what can be done with it. Just pick the\\nmost common use case for every method\/property and show it briefly, the explicit kata like \"Promise.all()\" will\\ncover the feature more detailed, and if needed there might even be an [in-depth kata][2] for it.\\nkatas for good examples\\n### Syntax katas\\nKatas that help to learn syntax of JavaScript are in this group.\\nThese are for example \"Block scope\", \"Template string\", \"Rest operator\", \"Destructuring\" and others.\\nTBD: should they adhere to a naming scheme?\\n### TBD: Other katas???\\nAre there other kata types? If so, must they be described or even standardized?\\n","Prediction":"## Decision\nAll katas will be grouped in one of the following groups:\\n* Array API\\n* Object API\\n* String API\\n* Other\\nEach of these katas will have a different group name. For example, Array API-katas are grouped in the\\n`array` group and Array-katas are grouped in the `array`-group.\\nThis grouping is based on the `name` property of the katas. In the `array`-group, the katas will be named\\n`Array` and in the `array`-group the katas will be named `Array` and so on.\\nFor example, `Array`-katas will be named `Array-a` and `Array-b` and `Array-c` and so on.\\nThis is a general structure and it will be used for all katas. The structure is not limited to this.\\nThis structure will be used for all katas and all other pieces of data that are related to katas.\\nIt will be used as a reference for the structure to allow grouping and other things to be done.\\nFor example, the structure will not be used for the name of the katas.\\nThis structure will be used to group katas"}
{"Context":"## Context\\nAs part of our efforts to improve our testing across GOV.UK Pay,\\nwe wish to move browser testing from being tested against a docker-compose version of the entire GOV.UK Pay stack to a stubbed environment testing our Node.js apps in isolation.\\nThis should make it:\\n- less resource intensive to test an individual app\\n- make tests across the stack less brittle\\n- help to better delineate tests across the stack\\n- allow us to effectively validate client-side javascript enhancements\\n- allow a greater focus on UI\\nThis is intended to take place alongside or after the implementation of Pact contract testing.\\nThis affords us the opportunity to use generated Pact contracts to stub our other micro-services for the purposes of\\nthese tests. This may be helpful or a hindrance, it is something we need to keep in mind as we proceed.\\nThere are a number of technologies we could use for running our browser tests:\\n- selenium based test runners i.e webdriver.io, nightwatch.js\\n- casper.js\\n- cypress.io\\n","Decision":"## Decision\n### Browser Testing Tooling\\nWe should use cypress.io, because:\\n- this will help us to more cleanly separate integration and unit tests in our app\\n- gives the nicest feedback loop\\n- gives the nicest developer workflow\\n### Stubbing Downstream Services\\nInitially we should try using our Pact stubs. This may make our tests too brittle or too difficult to write,\\nif so we should investigate how else we can generate stubs.\\n","Prediction":"## Decision\nWe will use casper.js for browser testing.\\nWe will use cypress.io for UI testing.\\nWe will use nightwatch.js for integration testing.\\nWe will use cypress for UI and integration tests.\\nWe will use casper for stubbing tests.\\nWe will use cypress for all other tests.\\nWe will use cypress for running our browser tests.\\nWe will use casper for running our browser tests.\\nWe will use casper for running our integration tests.\\nWe will use casper for running our UI tests.\\nWe will use casper for running our UI tests.\\nWe will use casper for running our UI tests.\\nWe will use casper for running our UI tests.\\nWe will use casper for running our UI tests.\\nWe will use casper for running our UI tests.\\nWe will use casper for running our UI tests.\\nWe will use casper for running our UI tests.\\nWe will use casper for running our UI tests.\\nWe will use casper for running our UI tests.\\nWe will use casper for running our UI tests.\\nWe will use casper for running our UI tests.\\nWe will use casper for"}
{"Context":"## Context and Problem Statement\\nIn v4 we started using Apollo due to its appealing promises that it has better docs, is easier to begin, and does not enforce strict patterns like Relay does. Apollo was first used in a naive way where each component that requires data would execute it's own query resulting in a large number of queries and round trips which performed badly.\\nWe then transitioned to use Fat Queries that utilize fragments to compose a big query whose data response would then be distributed manually to the components that needed the data (Colocation).\\nHuge effort and complexity needed to be introduced to support this, as Apollo provided limited to no support for this use case. The lack of documentation of our home grown graphql framework is hindering quality contributions from the community.\\nIt also revealed large performance issues inside Apollo that required an elaborate fork to fix.\\nApollo is moving in a different direction, preventing us from upgrading apollo itself.\\nIt is very easy to deteriorate performance, introduce bugs and break maintainability due to lack of proper Colocation support and prevention of misuse from the framework.\\nWe reached out to the Apollo team but haven't received the required attention to fix the issues upstream.\\nFor v5 we needed a better solution.\\n## Decision Drivers\\nA framework that:\\n- has first class support for Colocation\\n- is performant\\n- encourages best practices\\n- prevent bugs\\n- proven to work in a large and complex application\\n- easy to maintain\\n- good documentation\\n","Decision":"## Decision\nA framework that:\\n- has first class support for Colocation\\n- is performant\\n- encourages best practices\\n- prevent bugs\\n- proven to work in a large and complex application\\n- easy to maintain\\n- good documentation\\nAt that time the only viable alternative to Apollo is Relay. In fact all the solutions we came up to make up for the missing Colocation support in Apollo looked a lot like Relay Classic anyway. A huge advantage was that Relay Modern was just released that has large improvments over Relay Classic and removed more Facebook related internals.\\n### Positive Consequences\\n* First class Colocation support means we were able to simplify our graphql framework to a handful of lines mostly related to typing and wrapping.\\n* Typescript support eventually came out, which was a dramatic improvement in maintainability.\\n* A huge chunk of bugs in v4 was related to Apollo and how Data Colocation was not used properly. Relay entirely prevents these bugs from happening.\\n* Persisted Queries support for better performance, bundle size and security.\\n* Precompilation lead to better performance and bundle size.\\n* Relay moves in our direction, every version upgrade is exciting news for us.\\n* To understand how our graphql framework works we can now just point to the Relay documentation. It was an almost impossible task to explain our framework before.\\n* Relay detects problems in Queries during build time, whereas previously problems were only highlighted during runtime. This has led to faster development.\\n* Relay's Client Schema Extensions allowed us to replace Redux entirely. Wow.\\n### Negative Consequences\\n* Supporting a plugin architecture becomes much more difficult but not impossible.\\n* To use advanced features in Relay, we have to adhere to some conventions when structuring the schema. It's not a big issue however as our experience in v4 with Apollo has mostly led to the same or similar conventions.\\n","Prediction":"## Decision\nA framework that:\\n- has first class support for Colocation\\n- is performant\\n- encourages best practices\\n- prevent bugs\\n- proven to work in a large and complex application\\n- easy to maintain\\n- good documentation\\nWe decided to use Apollo 5.0.0.\\nIt is a first class library that is used in the Apollo SDK along with Relay for Apollo's graphql\\nand Apollo's graphql-graphql-rails for Apollo's graphql-graphql-rails. It is also used in the Apollo\\nlibrary.\\nApollo is the first library that supports colocation in the Apollo SDK. It is also the first library\\nthat supports Apollo's graphql-graphql-rails.\\nIt has a good amount of documentation and has been used in production for years.\\nIt is a first class library that is used in the Apollo SDK along with Relay for Apollo's graphql\\nand Apollo's graphql-graphql-rails for Apollo's graphql-graphql-rails. It is also used in the Apollo\\nlibrary.\\nApollo is the first library that supports colocation in the Apollo SDK. It is also the first library\\nthat supports Apollo's graphql-graphql-rails.\\nIt has a good amount of documentation and has been used in production"}
{"Context":"## Context:\\nWe need a way for player characters to be generated intentionally where certain statistics can be decided on by the player instead of having a random outcome. This will give the player some opportunity to have some control over their character's strengths and weaknesses and add to the playability of the game.\\n","Decision":"## Decision\nCreate a player module with user interaction to affect the starting statistics of a character.\\n","Prediction":"## Decision\nWe will use the [Character Builder](https:\/\/github.com\/Alphakronix\/CharacterBuilder) library to generate characters for the game.\\nThis library is a fork of [Character Generator](https:\/\/github.com\/Alphakronix\/CharacterGenerator), which is a fork of the [Character Generator](https:\/\/github.com\/Alphakronix\/CharacterGenerator) library.\\nWe will use the [Character Builder](https:\/\/github.com\/Alphakronix\/CharacterBuilder) library to generate characters for the game.\\nWe will also use the [Character Generator](https:\/\/github.com\/Alphakronix\/CharacterGenerator) library to generate new characters for the game.\\nWe will use the [Character Generator](https:\/\/github.com\/Alphakronix\/CharacterGenerator) library to generate new characters for the game.\\nWe will use the [Character Builder](https:\/\/github.com\/Alphakronix\/CharacterBuilder) library to generate new characters for the game.\\nWe will use the [Character Generator](https:\/\/github.com\/Alphakronix\/CharacterGenerator) library to generate new characters for the game.\\nWe will use the [Character Builder](https:\/\/github"}
{"Context":"## Context\\nContext for rate limiting pattern\\n","Decision":"## Decision\nProsa about the decision in the ADD\\n","Prediction":"## Decision\nWe will use rate limiting pattern to enforce a maximum number of requests per second (RPS) for each request. This is the default behaviour for all application servers.\\nThis is the behaviour that we will enforce on our own application servers.\\nWe will use the `max_rate` and `max_rate_inbound` properties of the `MaxRate` and `MaxRateInbound` endpoints. These will be set to the maximum request rate that is allowed for the application server.\\nWe will use the `rate_limit` property of the `MaxRate` endpoint to define the maximum number of requests that can be made in a given time period.\\nWe will use the `rate_limit_inbound` property of the `MaxRateInbound` endpoint to define the maximum number of requests that can be made in a given time period in inbound connections.\\nWe will use the `rate_limit` and `rate_limit_inbound` properties to define the maximum number of requests that can be made by a single request.\\nWe will use the `rate_limit_inbound` and `rate_limit` properties to define the maximum number of requests that can be made by a single request.\\nWe will use the `rate_limit_inbound` and `rate_limit"}
{"Context":"## Context\\nHere I'm adding some ideas to add as a cul-de-sac about resilience and\\nprevious experiences in order to keep a production environment under\\ncontrol.\\n### Mechanisms for resilience and fault tolerance\\n#### What can go wrong?\\nTo identify possible points of failure, I will go upstreams from production to development.\\n#### Production\\nAttending to Murphy's Law: Anything that can possibly go wrong, does, I should meter anything.  So first point of failure here is monitoring.\\n##### Monitoring\\nIf is not monitorized, it is not in production\\nAnything that goes in production is a result of an effort to earn money, so we should be consciuous of the benefits of the feature as soon as it is deployed.\\nFor me, any new feature request should have a way to meter the success or failure rate of the solution, and that check must be defined before the feature runs in production, just to show it is not in production yet.\\nTo handle this, I would add a requirement in the specs for any feature request that should establish what is a success and what is a failure for the product owner that requested it.\\nA product owner dashboard should exist so any product owner should see a way to measure the success rates of her decisions.\\nThis monitoring comes from Business, but as technicians we need to measure how the platform is performing, and the capacity of the current resources in order to answer the following question:\\nWill the next feature get into the current platform?\\nYou can use [RED method](https:\/\/thenewstack.io\/monitoring-microservices-red-method\/) to monitoring features.\\n##### Name Resolution\\nWhat if you have a website nobody can reach because your nameservers are\\ndown?\\nYou should have a primary and secondary nameservers in different\\nproviders.\\n##### Name System Attacks\\nWhat if your nameservers have been poisoned to deliver different\\naddresses?\\nYour nameservers must verify and check their data consistency.\\n##### Name Changes Response Time\\nYour servers must be quite fast to respond queries, and fast enough to\\npropagate changes when endpoints are affected.\\n##### Network and Security\\nYour services should have enough bandwidth to avoid queueing requests and deliver quick responses.  Your network is vulnerable to several attacks, from DDoS to specific website malformed urls or known product vulnerabilities.\\nMonitor your network capacity and get a provider that can handle all your needs.  In cloud environments is easy if you have the money.\\nMonitor your traffic using Firewalls or Web Application Firewalls, that inspect the requests looking for known vulnerabilities.\\nIf you are using web services, perform the following recomendations:\\n- Use HTTP2\\n- Use CDN solutions\\n- Use WAF solutions that can automatically ban attackers IP addresses\\n##### SSL certificates\\nYour web applications and sites won't work if your SSL certificates are not valid.\\nValid certificates means they are in working conditions and they are recognized by the users as trusted certificates.\\nMonitor your SSL certificates expiration dates and set a prodedure to renew them and publish them automatically.\\nUse global solutions like [Cloudflare](https:\/\/cloudflare.com) that provide solutions for DNS, CDN and Certificate management.  But keep an eye on other providers to have a failover.\\n##### Know your servers, avoid stupid downtimes\\nSome servers can create unexpected conditions when you don't know their limits.  For example: nginx is known for be a good load balancer and reverse proxy, but it's also true that it caches the IP addresses of the upstream servers.  If the IP address of an upstream server changes (because reprovisioning a node), then nginx can behaves unexpectedly and it can return error codes.\\nOther issues can be related to timeouts.  If a web request last more than 40 seconds, nginx automatically rejects the request with a 504 (Gateway Timeout) error code.\\nTo solve this, use your preproduction \/ staging environments to reproduce possible scenarios, and get knowledge about the limits of the servers, services and platforms used.\\n##### Invest in your logs\\nGet all the possible info from the servers and any service involved in the production environment.\\nProcess all your logs in order to aggregate data and set a baseline to detect anomalies.\\nUse solutions like Splunk, Logentries, Devo, Airbrake\\n#### Preproduction\\n##### Schedule backups from production to preproduction \/ staging\\nTBD\\n##### Pseudoanonymize backups from production to upstream environments\\nTBD\\n##### Perform load tests\\nTBD\\n##### Perform releases in staging and take times\\nTBD\\n#### QA\\n##### Match specs with tests\\nTBD\\n#### Development\\nTBD\\n","Decision":"## Decision\nHere are lots of ideas to explore for next discussions.\\n","Prediction":"#### Production\\n##### Run production load tests\\nTBD\\n##### Run preproduction load tests\\nTBD\\n##### Run preproduction releases\\nTBD\\n##### Run preproduction in staging\\nTBD\\n##### Run releases in preproduction\\nTBD\\n##### Run releases in staging in preproduction\\nTBD\\n##### Run releases in preproduction in preproduction\\nTBD\\n##### Run releases in preproduction in preproduction\\nTBD\\n##### Run releases in preproduction in preproduction\\nTBD\\n##### Run releases in preproduction in preproduction\\nTBD\\n##### Run releases in preproduction in preproduction\\nTBD\\n##### Run releases in preproduction in preproduction\\nTBD\\n##### Run releases in preproduction in preproduction\\nTBD\\n##### Run releases in preproduction in preproduction\\nTBD\\n##### Run releases in preproduction in preproduction\\nTBD\\n##### Run releases in preproduction in preproduction\\nTBD\\n##### Run releases in preproduction in preproduction\\nTBD\\n##### Run releases in preproduction in preproduction\\nTBD\\n##### Run releases in preproduction in preproduction\\nTBD\\n##### Run releases in preproduction in preproduction\\nTBD\\n##### Run releases in preproduction in preproduction\\nT"}
{"Context":"## Context\\nWe originally provisioned a discourse instance alongside DataLab as a user forum, however\\nin practise we have found that it is not used as discussion takes place either in\\nperson or on Slack, and we can use the documentation page where required.\\n","Decision":"## Decision\nDiscourse will be removed from the stack.\\n","Prediction":"## Decision\nWe will use Discourse for the discussion forum.\\nDiscourse is a well known open source platform with a large user base and\\nsupport.\\nIt is well documented and has a good community behind it.\\nWe will use the Discourse Python SDK to access the Discourse API for\\naccessing the forums.\\nWe will use the Discourse Admin SDK to manage the Discourse instance.\\nWe will use the Discourse REST API for accessing the documentation and\\nother resources.\\nWe will use the Discourse REST API to provide a RESTful API for the DataLab\\napplication.\\nWe will use the Discourse REST API to provide a GraphQL API for the DataLab\\napplication.\\nWe will use the Discourse REST API to provide a GraphQL API for the DataLab\\napplication.\\nWe will use the Discourse REST API to provide a GraphQL API for the DataLab\\napplication.\\nWe will use the Discourse REST API to provide a GraphQL API for the DataLab\\napplication.\\nWe will use the Discourse REST API to provide a GraphQL API for the DataLab\\napplication.\\nWe will use the Discourse REST API to provide a GraphQL API for the DataLab\\napplication.\\nWe will use the Discourse"}
{"Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"## Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","Prediction":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\nWe will use [Architecture Code Review Process](http:\/\/thinkrelevance.com\/blog\/2011\/11\/01\/documenting-architecture-code-review-process).\\nWe will use [Architecture Style Guide](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-style-guide).\\nWe will use [Pivotal Tracker](https:\/\/trac.pivotal.io\/projects\/industries\/finance).\\nWe will use [Google Closure Compiler](https:\/\/github.com\/google\/closure-toolkit).\\nWe will use [Google Cloud Platform](https:\/\/cloud.google.com\/).\\nWe will use [Google Test](https:\/\/github.com\/google\/GoogleTest).\\nWe will use [Google Test Driven Development](https:\/\/github.com\/google\/GoogleTest\/tree\/master\/docs\/tdd).\\nWe will use [GitHub Actions](https:\/\/github.com\/cloudbees\/gh-actions).\\nWe will use [AWS CodeCommit](https:\/\/aws.amazon.com\/codecommit\/).\\nWe will use [AWS CodeBuild](https:\/\/github.com"}
{"Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","Prediction":"## Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\nWe will use [YAML](https:\/\/yaml.org\/), the standard format for configuration files. We will use the [Swagger](https:\/\/swagger.io\/) specification to document the format of the documents.\\nWe will use the [Red Hat Decision Portal](https:\/\/portal.redhat.com) to host the architecture decisions.\\nWe will use the [Red Hat Decision Portal](https:\/\/portal.redhat.com) to host the architecture decisions.\\nWe will use the [Red Hat Decision Portal](https:\/\/portal.redhat.com) to host the architecture decisions.\\nWe will use the [Red Hat Decision Portal](https:\/\/portal.redhat.com) to host the architecture decisions.\\nWe will use the [Red Hat Decision Portal](https:\/\/portal.redhat.com) to host the architecture decisions.\\nWe will use the [Red Hat Decision Portal](https:\/\/portal.redhat.com) to host the architecture decisions.\\nWe will use the [Red Hat Decision Portal](https:\/\/portal.redhat.com) to host the architecture decisions.\\nWe will use the"}
{"Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","Prediction":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\nWe will use [SAD-Record](https:\/\/github.com\/TeamSAD\/sad-record) to generate the files.\\nWe will use [p4a](https:\/\/github.com\/jakepritchard\/p4a) to generate the files.\\nWe will use [P4A](https:\/\/github.com\/jakepritchard\/p4a) to generate the files.\\nWe will use [Rational ClearQuest](https:\/\/www.ibm.com\/software\/rational\/products\/clarity\/) to store the ADRs.\\nWe will use [Schema.org](https:\/\/schema.org\/) to store the ADRs.\\nWe will use [SAD-Record](https:\/\/github.com\/TeamSAD\/sad-record) to generate the ADRs.\\nWe will use [P4A](https:\/\/github.com\/jakepritchard\/p4a) to generate the ADRs.\\nWe will use [SAD-Record](https:\/\/github.com\/TeamSAD\/sad-record) to generate the ADR"}
{"Context":"## Context\\nhttps:\/\/github.com\/suzaku-io\/diode\\nDiode provides a\\n","Decision":"## Decision\nDecision here...\\n","Prediction":"`Diode::Logger` and a\\n`Diode::Logger::Logger` interface. A\\n`Diode::Logger` implements\\n`Diode::Logger::Logger` and\\n`Diode::Logger::Logger` is a\\n`Diode::Logger` with a\\n`Diode::Logger` instance as its\\n`Logger` object. The\\n`Diode::Logger` interface\\n`Diode::Logger::Logger` interface\\nhas a `get_logger` method that\\nreturns a\\n`Diode::Logger` object. The\\n`Diode::Logger` object is\\ninitialized with a\\n`Diode::Logger` instance as its\\n`Logger` object.\\n## Decision\nThe new \\code{Diode::Logger} class will be\\nintroduced in the next stable release.\\nThis class will be used to\\nprovide a\\n`Diode::Logger` instance to\\nthe \\code{Diode::Logger} interface.\\nIn order to avoid confusion, this\\nclass will be called\\n`Diode::Logging::Logger`.\\nIn addition, all\\n`Diode::Logging::Logger`\\nmethods will be renamed\\n`Diode::Logging::Logger::get_logger()`"}
{"Context":"## Context\\nWe will be generating a number of classes as per [ADR 1](adr-001.md).\\nGenerated classes should be generated during the build process and not committed to GitHub.\\nThis ensures we have reproducable builds and makes it clear that generated classes are not to be edited manually.\\nWe wish to publish artifats to the central maven repository.\\nThis does not necessarily limit us to working with Maven, but it encourages us to stick it.\\nUsing a compiled language to run the code generation would require two passes of compilation\\n(or another\/project module) - one to compile the codegen scripts, another to run it.\\n","Decision":"## Decision\nWe will use ruby to write the code generation scripts.\\nWe will run them using the [jruby-maven-plugins](https:\/\/github.com\/torquebox\/jruby-maven-plugins) as part of the\\n`generate-sources` phase of the\\n[maven lifecycle](https:\/\/maven.apache.org\/guides\/introduction\/introduction-to-the-lifecycle.html)\\n","Prediction":"## Decision\nWe will generate code during build time using the [Java 8](java-8) compiler, and then run it\\nduring the build using the [Java 8](java-8) compiler. This will be achieved by using the\\n[Javac](http:\/\/docs.oracle.com\/javase\/7\/docs\/api\/java\/lang\/Class.html#compile(java.lang.String))\\nand [javac](https:\/\/docs.oracle.com\/javase\/8\/docs\/javadoc\/api\/java\/lang\/Class.html) methods\\nto compile the codegen scripts to bytecode. This will be achieved by using the\\n[Java 8](java-8) compiler's `new` and `run` methods.\\nThis is preferred over the approach of compiling the codegen scripts to source files\\nduring the build, because it will allow us to run the codegen scripts during the build\\nprocess, and then compile them during the build.\\nWe will not generate code for the following:\\n- [ADR 2](adr-002.md), which is a standalone build script that will generate a class\\nthat can be run standalone.\\n- [ADR 3](adr-003.md), which is a Maven project that will generate a class that can be run\\n"}
{"Context":"## Context\\nWe currently have a lot of integrations that use or are based on GPIO (including SPI and I2C busses).\\n- All of these integrations have a low usage count (source Home Assistant Analytics),\\nbut add the same amount of maintenance and review time to the core.\\n- Most of these integrations are unmaintained.\\n- The use of GPIO via containers (including our Home Assistant OS, Container\\n& Supervised installation methods) is often unstable or complicated.\\n- Support for GPIO on specific platforms\/SBCs: mileage varies (e.g., Raspberry Pi on 64 bits platforms).\\n- The use of these sensors, in general, is better supported and done via dedicated projects like ESPHome.\\n","Decision":"## Decision\n- We no longer accept integrations that integrate with devices over GPIO.\\n- We identify, deprecate and remove integrations that use GPIO.\\n- It will still be possible to have custom integrations that use GPIO.\\nGPIO, in this case, means interfacing directly with the individual GPIO pins\\n(or I\/O lines) on the board running Home Assistant. This includes bus\\ncommunications like I2C and SPI.\\nDevices connected on GPIO that are exposed on the system as regular serial\\ndevices are exempted from this ADR.\\n","Prediction":"## Decision\nWe will use a dedicated GPIO device for all our GPIO integration (including sensors).\\nThe following devices will be used for integration:\\n- [GPIO Controller](https:\/\/www.amazon.com\/Mini-Controller-2-Channel-GPIO-Compatible\/dp\/B00G3W4S6Y\/ref=sr_1_1?ie=UTF8&qid=1482328513&sr=8-1&keywords=GPIO+Controller)\\n- [GPIO Breakout](https:\/\/www.amazon.com\/Mini-Controller-2-Channel-GPIO-Breakout\/dp\/B00G3W4S6Y\/ref=sr_1_2?ie=UTF8&qid=1482328513&sr=8-2&keywords=GPIO+Breakout)\\n- [GPIO Header](https:\/\/www.amazon.com\/Mini-Controller-2-Channel-GPIO-High-Speed+Breakout\/dp\/B00G3W4S6Y\/ref=sr_1_3?ie=UTF8&qid=1482328513&sr=8-3&keywords=GPIO+Header)\\n- [GPIO Cable](https:\/\/www.amazon.com\/Mini-Controller-2-Channel-GPIO-Cable\/dp\/B00"}
{"Context":"## Context\\nVundle hasn't been updated for a while (Aug 2019 at time of writing) and I'd\\nread a bit about how vim-plug was pretty good. And it felt like it was time for\\na change.\\n","Decision":"## Decision\n[vim-plug](https:\/\/github.com\/junegunn\/vim-plug) will be used to manage Vim\\nplugins in favour of [Vundle](https:\/\/github.com\/VundleVim\/Vundle.vim).\\n","Prediction":"## Decision\nI'm going to use vim-plug as my plugin manager. It's a pretty mature project and\\nhas a lot of adoption.\\nI'm also going to use the `:PluginManager` plugin for just managing plugins. This\\nwill allow me to use `:PluginManager` to manage plugins and plugins.vim.\\nI'm also going to use `:PluginManager::PluginManager` for managing plugins in a\\nplugin-specific way rather than globally. This way, I can use the `:PluginManager`\\nplugin to manage a plugin's dependencies.\\nI'm also going to use `:PluginManager::PluginManager::PluginManager` for managing\\nplugins in a plugin-specific way. This way, I can use the `:PluginManager::Plugin`\\nplugin to manage a plugin's dependencies.\\nI'm also going to use `:PluginManager::PluginManager::PluginManager` for managing\\nplugins in a plugin-specific way. This way, I can use the `:PluginManager::Plugin`\\nplugin to manage a plugin's dependencies.\\nI'm also going to use `:PluginManager::PluginManager::PluginManager` for managing\\nplugins in a plugin-specific way. This way, I can use the `"}
{"Context":"## Context\\nNAV is paid for by the public for the purpose of providing public services. What\\nwe produce should be open and available for the public.\\n","Decision":"## Decision\nWe should strive to be as transparent as possible, and work by the principle\\n\"Open by default\".\\n","Prediction":"## Decision\nWe will make the development of our services open and available to the public.\\nWe will publish the code of the services we produce.\\nWe will provide a way for developers to contribute to our services.\\nWe will make the source code of our services available for download.\\nWe will make it easy for developers to contribute to our services.\\nWe will make it easy for developers to understand the code of our services.\\nWe will make it easy for developers to understand the impact of our services.\\nWe will make it easy for developers to understand the impact of our services on their\\nwork and personal environment.\\nWe will make it easy for developers to understand the impact of our services on\\ntheir business and the wider economy.\\nWe will make it easy for developers to understand the impact of our services on\\ntheir health, safety, and wellbeing.\\nWe will make it easy for developers to understand the impact of our services on\\ntheir professional development.\\nWe will make it easy for developers to understand the impact of our services on\\ntheir personal development.\\nWe will make it easy for developers to understand the impact of our services on\\ntheir learning and growth.\\nWe will make it easy for developers to understand the impact of our services on"}
{"Context":"## Context\\nWe need to get a postcode from the coordinates we get from the browser.\\n","Decision":"## Decision\nTo use postcodes.io instead of geocoder gem and just make a simple AJAX call from the browser.\\n# Consequences\\nWe avoid creating an endpoint on the server, therefore reducing the load we have to manage. On the other side we rely on a service that is less trusted than Google Maps, but open source and based on Open Data.\\n","Prediction":"## Decision\nWe will use the Postcode API to get the postcode.\\nIt is a public API, so we will use it in the same way as we use the Google Maps API.\\nWe will use the [Postcode API](https:\/\/developers.google.com\/places\/postcodes) to get the postcode.\\nWe will use the [Postcode Geocoding API](https:\/\/developers.google.com\/places\/postcodes\/postcode-geocoding) to get the postcodes.\\nWe will use the [Postcode Geocoding API](https:\/\/developers.google.com\/places\/postcodes\/postcode-geocoding) to get the postcodes.\\nWe will use the [Postcode Geocoding API](https:\/\/developers.google.com\/places\/postcodes\/postcode-geocoding) to get the postcodes.\\n### API endpoints\\nWe will use the following endpoints for the Postcode Geocoding API:\\n* `https:\/\/maps.googleapis.com\/maps\/api\/geocode\/json?address=postcode`\\n* `https:\/\/maps.googleapis.com\/maps\/api\/place\/nearbysearch\/json?address=postcode`\\n* `https:\/\/maps.googleapis.com\/maps\/api\/place\/extractlat?address=postcode`\\n* `https:\/\/maps.googleapis.com\/maps"}
{"Context":"## Context\\nBefore GOV.UK Frontend, you needed to use 3 different libraries to build a\\nservice that looked like GOV.UK:\\n- GOV.UK Frontend Toolkit\\n- GOV.UK Template\\n- GOV.UK Elements\\nGOV.UK Template provides the boilerplate, header, footer and related assets like\\nthe favicon, and is available in a number of different formats:\\n- a gem containing a Rails engine\\n- a tarball containing Play Framework templates\\n- a folder containing Mustache templates\\n- a tarball containing Liquid templates\\n- a tarball containing Mustache Inheritance templates\\n- a tarball containing Jinja templates\\n- a tarball containing plain HTML and assets\\n- a tarball containing Embedded JavaScript (EJS) templates\\n- a JAR file containing assets but no templates, structured as per WebJars\\n- a tarball containing Django templates\\nMaking GOV.UK Template available in a number of formats provides a convenient\\nway for service teams across government to start a new GOV.UK project, despite\\nthe number of different tech-stacks being used.\\nThese different formats are automatically generated from a base ruby ERB\\ntemplate through the use of different '[compilers][template-compilers]',\\n'[packagers][template-packagers]' and '[publishers][template-publishers]' for\\neach target format.\\nGOV.UK Elements, which contains the rest of the UI Elements that you would need\\nto use, does not support any sort of templating language. The only thing that it\\nprovides is an [npm package containing the Sass][elements-sass]. It relies on\\nusers copying and pasting HTML from the examples in the online documentation.\\nThe way that the different formats in GOV.UK Template are generated is fairly\\nsimple. It supports the substitution of blocks and variables, but is unable to\\nhandle loops, complex logic or many of the other features provided by most\\ntemplating languages. For this reason, we do not think that we could use the\\nsame technique to provide the components in GOV.UK Frontend in multiple formats.\\nWe investigated other ways of doing this in alpha, which are outlined below, but\\nfound that doing so in a reliable way would involve significant work.\\nOverall we do not think that the pain points involved in maintaining a setup of\\nthis nature are worth the benefit if we are only able to provide the 'header,\\nfooter and boilerplate in multiple formats,  given how rarely they change. The\\n[HTML in GOV.UK Template has not changed very much][template-changes] in the\\nyears it has existed.\\n### Pain points\\n- The team do not have working knowledge of a number of the target languages and\\nframeworks. This would make it difficult to fix issues that might be reported,\\nor to update the template to work with new versions of the different\\nframeworks that might be released.\\n- There are integration tests for some of the target languages and frameworks,\\nbut not all. This makes it difficult to make substantial changes to Template\\nwhilst being confident that those changes will work everywhere unless you do\\nsubstantial manual testing. It would be possible to set up test harnesses for\\nevery target language and framework, but this would be significant work.\\n- The build pipeline is reasonably complex with a number of downstream builds\\nbeing kicked off in other repositories, making it hard to diagnose when issues\\narise. It also publishes to a number of different package managers using a\\nvariety of different methods.\\n### Transpiling templates\\n[Transpilation][transpilation] is the act of taking source code written in one\\nlanguage and producing an equivalent version in another language.\\nIn [alpha][alpha] the team explored ways of doing this by using [shawnbot\/meta-\\ntemplate][meta-template] which converts the source template into an [abstract\\nsyntax tree][ast] and uses that to generate the destination template. We created\\na [working example][alpha-transpilation] which involved  [adding an ERB\\nformatter][meta-template-pr] to meta-template.\\nHowever, meta-template was in its infancy and not something we could adopt\\nwithout doing significant work. It was also difficult to establish which\\ntemplating language features, for example, default variables for variables, were\\ngoing to be available in every templating language we wanted or might want to\\nsupport, and therefore which features were safe to use in the source template.\\nThere is also the risk of making contribution more difficult, as contributors\\nmay discover issues with transpilation during development that they do not know\\nhow to solve. For example, tests may fail because they're using a new feature\\nthat either doesn't work in another templating language, or that the transpiler\\ndoesn't know how to transpile.\\n","Decision":"## Decision\nWe will use a single templating language which will allow us to define the\\nmarkup for a component in a single place.\\nWe will not build integrations into other languages or frameworks. Our\\n'deliverable' will be in the form of one or more npm packages containing the\\nSass and JavaScript.\\nWe may decide to look at transpilation again in the future, based on the\\nfeedback that we get from our users.\\n","Prediction":"## Decision\nWe will not use transpilation to transform the GOV.UK Frontend templating from\\none language to another. We will instead use the [GOV.UK Template][template-\\ntemplate] template format as described in the GOV.UK Template documentation.\\nThis will allow us to re-use the boilerplate, header, footer and related assets in\\nmultiple languages, and to support a number of different templating languages\\nwithout the need to transpile them. We will also be able to provide the\\n[HTML in GOV.UK Template has not changed very much][template-changes] template\\nformat, which will allow us to provide a consistent way to generate the boilerplate,\\nfooter and header assets in all cases.\\nWe will continue to work on transpilation as part of the [GOV.UK Template\\nworkstream][template-workstream].\\nIf you are interested in contributing to transpilation, please contact the team on\\n[send feedback][feedback].\\nIf you are interested in contributing to the GOV.UK Frontend templating\\narchitecture, please see [the GOV.UK Frontend architecture][architecture]\\nfor more information.\\n### Translating templates\\n["}
{"Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","Prediction":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\nSee also [Cascading Architecture Decisions](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/cascading-architecture-decisions).\\nSee also [Architectural Decision Records](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/architectural-decision-records).\\nSee also [Decisions That Work](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/decisions-that-work).\\nSee also [Architectural Decisions](http:\/\/thinkrelevance.com\/blog\/2009\/11\/16\/architectural_decisions).\\nSee also [Architectural Decision Process](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/architectural_decision_process).\\nSee also [AIA Document A100](http:\/\/www.aia.org\/architecture-architecture\/what-is-architecture-documentation\/aia-documentation-guide-architecture-decisions).\\nSee also [Architectural Decision Support](http:\/\/thinkrelevance.com\/blog\/2011\/11"}
{"Context":"## Context\\nWe have to provide a registration for merchant.\\nThe definition of a customer, which is defined as a merchant, we want to realize via customer groups.\\nHowever, this is not \"merchant\" specific, because we do not react to \"merchant customer groups\" in any way in the core. In other words, we implement a customer group registration system.\\nThe process should work as follows:\\n* The shop owner enables customer group registration for a customer group and generates an url\\n* This url must be shared by the shop owner to customers (footer, social media, mails, etc.)\\n* Customer registers on an individual registration page on an individual url\\n* The customer will be created with the default customer group\\n* The shop operator can accept \/ decline the \"merchant\" registration in the admin\\nFor this I would suggest the following:\\n* At the customer we store another Foreign Key (desired customer group)\\n* This is then considered in the StoreApiRoute and stored at the customer\\n* In Administration we extend the current customer module with an accept \/ decline button\\n* Upon activation, we switch the customer group of the customer and set \"desired customer group\" back to zero.\\n","Decision":"## Decision\n### Headless Frontend\\n* Headless sales channel can resolve the url to get the foreign key using the seo-url store api route\\n* Call the customer-group-registration config endpoint with the foreign key to get the form configuration\\n* Sends a registration to customer registration endpoint with the `requestedGroupId`\\n","Prediction":"## Decision\nWe will create a customer group registration page which allows the shop operator to create a customer group and register a customer.\\nThis page is not a \"consumer\" page. The customer is a merchant. The customer group is not a \"consumer\" group. The customer group is the merchant group.\\nThe customer group registration page should be accessible via an individual url, which is configured in the admin.\\nThe customer group registration page should not be a part of the admin, but the admin should be able to set the customer group registration page.\\nThe customer group registration page is designed as a \"module\" which is used by the \"merchant group registration\" page.\\nThis module contains the following actions:\\n* Create a customer group\\n* Create a customer\\n* Create a merchant\\n* Create a merchant group\\n* Create a merchant customer\\n* Add a merchant group to a merchant\\n* Remove a merchant group from a merchant\\n* Remove a merchant customer\\n* Create a customer group\\n* Add a customer\\n* Remove a customer\\n* Create a merchant group\\n* Add a merchant\\n* Remove a merchant group\\n* Remove a merchant customer\\n* Create a merchant customer\\n* Remove a merchant\\n* Create a customer group\\n* Add a"}
{"Context":"## Context \ud83e\udd14\\nCf-k8s-networking was designed to be integrated with\\n[cf-for-k8s](https:\/\/github.com\/cloudfoundry\/cf-for-k8s\/). The Istio\\ninstallation used to be maintained by\\n[cf-for-k8s](https:\/\/github.com\/cloudfoundry\/cf-for-k8s\/), but the networking\\nteam needed to be able to easily make changes to [Istio](https:\/\/istio.io\/)\\nconfiguration to enable more networking features for [Cloud\\nFoundry](https:\/\/www.cloudfoundry.org\/).\\n","Decision":"## Decision\nWe decided to move the scripts to build Istio configuration, and maintain a\\ngenerated Istio configuration within the cf-k8s-networking repository.\\nThe build scripts and `ytt` overlays for Istio live in this repo (links removed\\nas they are no longer relevant or accurate). **UPDATE** This configuration has\\nmoved as a result of [ADR\\n017](.\/0017-moving-istio-configuration-out-of-this-repo.md).\\n","Prediction":"## Decision\nWe will use Istio as the networking platform for Cf-k8s-networking.\\nIstio will be used as the orchestration layer for the Istio network\\nin addition to the Istio service mesh as the platform for the Istio\\nAPI. Istio will be used as the default network for Cf-k8s-networking.\\nTo enable the Istio network to work with Cf-k8s-networking, we will\\nrequire Istio to be installed on the Istio cluster. Istio will be\\ninstalled on the Istio cluster as a separate role, and the Istio\\nservice mesh will be installed in the [Istio](https:\/\/istio.io\/) cluster.\\nThe Istio service mesh will be used to manage the Istio network\\nin the Istio cluster. The Istio service mesh will provide the\\nIstio API and Istio services to Istio and the Istio cluster.\\nThe Istio service mesh will be installed in the Istio cluster as a\\nstandalone role. The Istio service mesh will be installed in the\\nIstio cluster as a container in the [Istio](https:\/\/istio.io\/) namespace.\\nThe Istio service mesh will be installed"}
{"Context":"## Context\\nWe want to be confident that any changes to how our React components display are\\nintentional. Jest has built in support for snapshot testing.\\n","Decision":"## Decision\nWe will use snapshot testing as part of testing components.\\n","Prediction":"## Decision\nWe will use Jest snapshot testing for our React components.\\nSnapshots will be stored in the Jest repository.\\nWe will run Jest tests using Jest snapshots.\\nThis will allow us to run tests in parallel, and if we need to run\\nmultiple tests in a test suite, we can.\\nWe will run Jest tests using Jest snapshots.\\nThis will allow us to run tests in parallel, and if we need to run\\nmultiple tests in a test suite, we can.\\nWe will run Jest tests in parallel to avoid race conditions.\\nWe will run Jest tests in parallel to avoid race conditions.\\nWe will run Jest tests in parallel to avoid race conditions.\\nWe will run Jest tests in parallel to avoid race conditions.\\nWe will run Jest tests in parallel to avoid race conditions.\\nWe will run Jest tests in parallel to avoid race conditions.\\nWe will run Jest tests in parallel to avoid race conditions.\\nWe will run Jest tests in parallel to avoid race conditions.\\nWe will run Jest tests in parallel to avoid race conditions.\\nWe will run Jest tests in parallel to avoid race conditions.\\nWe will run Jest tests in parallel to avoid race conditions.\\nWe will run Jest tests in parallel to avoid race conditions"}
{"Context":"## Context\\nWhen preparing data for the rendering components we currently have up to three possible locations to do so: (1) the frontend Scala backend, (2) the dotcom rendering backend and (3) the end user's client side.\\nIn the interest of the user, we should avoid postponing computation to the client side and precompute data and state on either of the two backends whenever possible.\\n","Decision":"## Decision\n- Favour computation in frontend over computation in dotcom-rendering\\n- Favour computation on dotcom-rendering server than computation on the client\\n","Prediction":"## Decision\nFor the frontend frontend we will use the Scala backend in order to avoid the client side precomputed data.\\nFor the backend, we will use the dotcom backend.\\nFor the frontend, we will use the dotcom rendering backend.\\nFor the backend, we will use the dotcom rendering backend.\\nFor the frontend, we will use the Scala backend.\\nFor the backend, we will use the Scala backend.\\nFor the frontend, we will use the Scala backend.\\nFor the backend, we will use the Scala backend.\\nFor the frontend, we will use the Scala backend.\\nFor the backend, we will use the Scala backend.\\nFor the frontend, we will use the Scala backend.\\nFor the backend, we will use the Scala backend.\\nFor the frontend, we will use the Scala backend.\\nFor the backend, we will use the Scala backend.\\nFor the frontend, we will use the Scala backend.\\nFor the backend, we will use the Scala backend.\\nFor the frontend, we will use the Scala backend.\\nFor the backend, we will use the Scala backend.\\nFor the frontend, we will use the Scala backend.\\nFor the backend, we will use the Scala backend"}
{"Context":"## Context\\nWe want the logging API used by applications to be consistent, while allowing for logs to be filed, forwarded, filtered and formatted in a variety of ways.\\n","Decision":"## Decision\nDe-couple generation of log message\/entries from how they are handled.\\n* A `logger` object provides an API that can be used to generate log entries.\\n* Log \"entries\" are strongly typed structures.\\n* Log entry \"handlers\" provide a simple, consistent interface.\\n```mermaid\\nsequenceDiagram\\nApp ->> Logger:       info(\"Message\")\\nLogger ->> Handler:   <<(entry)\\n```\\n","Prediction":"## Decision\nWe will use the [nifi-log-api](https:\/\/github.com\/nifi-ni\/nifi-log-api) package to provide a logging API that is consistent with the current logging system. We will also provide a logging API that is useful in an application, while providing a consistent interface to the log handler.\\nWe will use [Docker Compose](https:\/\/docs.docker.com\/compose\/) to manage the containerised applications and services. This will allow us to easily deploy the logging API and logging handler to an application.\\nWe will use [Docker Compose](https:\/\/docs.docker.com\/compose\/) to manage the containerised services. This will allow us to easily deploy the logging API and logging handler to the application.\\nWe will use the [nifi-log-api](https:\/\/github.com\/nifi-ni\/nifi-log-api) package to provide the logging API to applications. We will use the [nifi-log-api](https:\/\/github.com\/nifi-ni\/nifi-log-api) package to provide a consistent logging API to applications.\\nWe will use the [nifi-log-api](https:\/\/github.com\/nifi-ni\/nifi-log-api) package to provide a consistent logging API to applications.\\nWe will use the"}
{"Context":"## Context\\nRelative paths for images won't work when the README\u00a0is displayed somewhere where the images are not hosted. From experience developing npm libraries, images won't appear on the package manager website.\\nTo solve this, we use absolute paths to hosted images. Since images are hosted in GitHub along the rest of the code, we use these absolute paths.\\nVS Code has a way to deal with that issue. It won't allow you to create a package if README contains relative paths. It will emit this error:\\n> Couldn't detect the repository where this extension is published. The image '.\/docs\/logo\/abracadabra-logo.svg' will be broken in README.md. Please provide the repository URL in package.json or use the --baseContentUrl and --baseImagesUrl options.\\nThus, we could technically use relative paths and provide the correct arguments to make images work on VS Code Marketplace.\\nHowever, using absolute paths is something we're used to, and doesn't come up with a lot of downsides. So we decide to go with absolute paths, until downsides justify otherwise.\\n","Decision":"## Decision\nWe use absolute paths to GitHub hosted images in the README (and other documentation files).\\nWe don't use relative paths to images.\\n","Prediction":"## Decision\nWe will use absolute paths for images hosted on GitHub along the rest of the code.\\nThis will allow us to provide the correct arguments to images on the package manager website.\\nIt will also allow us to use relative paths for images that are hosted elsewhere.\\nWe will use relative paths for images that are hosted elsewhere only when they are not in the repository. We will always use absolute paths when they are.\\nThis is a compromise between absolute paths and relative paths. Relative paths are used for images that are hosted elsewhere. Absolute paths are used for images that are hosted in the repository.\\nThis is a compromise between relative paths and absolute paths. Relative paths are used for images that are hosted elsewhere. Absolute paths are used for images that are hosted in the repository.\\nThis is a compromise between relative paths and absolute paths. Relative paths are used for images that are hosted elsewhere. Absolute paths are used for images that are hosted in the repository.\\nThis is a compromise between relative paths and absolute paths. Relative paths are used for images that are hosted elsewhere. Absolute paths are used for images that are hosted in the repository.\\nThis is a compromise between relative paths and absolute paths. Relative paths are used for images that are hosted elsewhere. Absolute paths are"}
{"Context":"## Context\\nWe want users of the cloud platform to be able to access Kibana so that they can see the logs for their applications in a central place. AWS Kibana does not provide easy ways for users to authenticate. We need to put a proxy in front of Kibana so that users can authenticate with Github and then be redirected to the [Kibana dashboard][kibana-webconsole] to access their logs.\\n","Decision":"## Decision\nIt has been decided to use a combination of Auth0 and an OIDC proxy app. The application is managed in the [cloud-platform-terraform-monitoring repo][kibana-proxy] and configured ministryofjustice GitHub organization users to access Kibana.\\n","Prediction":"## Decision\nWe will use [CloudFront](https:\/\/docs.aws.amazon.com\/elasticbeanstalk\/latest\/dg\/using-cloudfront-elasticity.html) to proxy to AWS Kibana. CloudFront is a content delivery network that will cache the content from the origin server for a short period of time. It will then deliver the cached content to the client. It is a very fast CDN and will cache the contents of the origin server and any other origin server that it is connected to, so that the content will be delivered as quickly as possible.\\nWe will configure CloudFront to proxy to [Kibana][kibana-webconsole] using the [Kibana proxy configuration][kibana-proxy-config].\\nWe will configure [Kibana][kibana-webconsole] to use the [Kibana dashboard][kibana-webconsole] as the default dashboard\\nWe will configure [Kibana][kibana-webconsole] to use the [Kibana dashboard][kibana-dashboard] to display the log files of the applications. We will configure [Kibana][kibana-dashboard] to display the log files of the instances. We will configure [Kibana][kibana-dashboard] to"}
{"Context":"## Context and Problem Statement\\nIn scaling Nimbus for 2021 and beyond, we need to provide experiment owners, data scientists, and desktop and Fenix engineers documentation to better streamline working with the platform. Following the same line of thought in [`nimbus-shared` ADR 0001](https:\/\/github.com\/mozilla\/nimbus-shared\/blob\/main\/docs\/adr\/0001-one-console-app.md), users should expect a single source of truth for their documentation needs.\\nThe documentation that will be part of the hub includes:\\n- A new front-page or home-page doc, likely written in MD, with a brief overview of Nimbus and instructions for users on the workflow for doc maintenance\\n- New docs, likely written in MD, and depending on the chosen option, possibly optionally with React components (if Docusaurus or Storybook) or custom JS (if Docusaurus, Storybook, or `mdBook`)\\n- For Fenix engineers: SDK docs for integration in Fenix\\n- For experiment owners: How to use the console, either living here or in Mana\\n- For data scientists: How to add Jetstream configs\\n- How to read automated analysis, either living here or in Mana\\n- In the future, iOS and Gecko integration docs\\n- Existing docs\\n- For Desktop engineers: [SDK docs for integration in desktop front end](https:\/\/www.notion.so\/k88hudson\/Nimbus-Migration-Guide-for-Desktop-Front-End-Experiments-d36c21e505f84550aad1202897fc4ba3)\\n- Various Google docs and Mana links\\n- Possibly existing MD docs living in other repos\\n## Decision Drivers\\n- Docs should be kept in a central location and cross-linking should be easy\\n- Finding relevant documentation should be painless, quick, and clear for all of our users, and cover their specific doc needs, like copying a code snippet in various languages\\n- Docs should be easily editable by anyone; maintenance and workflow should be straightforward and scalability should be taken into account\\n- Set up, hosting, and deployment efforts should be minimal\\n","Decision":"## Decision\n- Docs should be kept in a central location and cross-linking should be easy\\n- Finding relevant documentation should be painless, quick, and clear for all of our users, and cover their specific doc needs, like copying a code snippet in various languages\\n- Docs should be easily editable by anyone; maintenance and workflow should be straightforward and scalability should be taken into account\\n- Set up, hosting, and deployment efforts should be minimal\\nChosen option: \"Docusaurus + GH pages\", because Docusaurus provides a flexible and easily\/automatically updated sidebar, allows MD and custom JS when needed, and provides a robust search bar. To minimize maintenance efforts, customization will be kept to a minimum and MD files can be edited directly in the GH UI. GitHub Pages takes care of deployment efforts and we can make the URL memorable with a custom domain if we wish to.\\nThe top three choices based on discussions held in the PR for this ADR were Docusaurus, the Experimenter repo Wiki, and `mdBook`. Docusaurus was chosen over the Experimenter repo Wiki because a repo Wiki would not allow custom JS, it's less scalable, and more difficult to search. Docusaurus was chosen over `mdBook` because `mdBook` does not allow external links in the sidebar and requires a separate file edited for sidebar updates which would add one more step to the maintenance workflow.\\n","Prediction":"## Decision\n- Docs should be kept in a central location and cross-linking should be easy\\n- Finding relevant documentation should be painless, quick, and clear for all of our users, and cover their specific doc needs, like copying a code snippet in various languages\\n- Docs should be easily editable by anyone; maintenance and workflow should be straightforward and scalability should be taken into account\\n- Set up, hosting, and deployment efforts should be minimal\\nChosen option: \"Nimbus hub\", because we want to keep the Nimbus documentation separate from the Nimbus Hub, and it is easy to set up and maintain. It also has the advantage of being the default location of the Nimbus documentation so that we can easily refer to it in other docs.\\n### Positive Consequences\\n- Docs are kept in a central location and cross-linking is easy\\n- Maintenance is easy, as we can simply include a new doc in the Nimbus Hub\\n- We can easily reference other docs in the Nimbus Hub, making it easier to find what we need (e.g., for new users)\\n- It is easy to set up and maintain, and can easily be hosted and deployed by other teams (e.g., in [Manuscript](https:\/\/manuscript.io\/))"}
{"Context":"## Context\\nWe applied the CQRS style (see [ADR 7. Use CQRS architectural style](007-use-cqrs-architectural-style.md)), now we need to decide how to handle reading (querying) requests.\\n","Decision":"## Decision\nWe will use 2 layered architecture to handle queries: API layer and Application Service layer. As we applied the CQRS and created a separated read model, querying should be straightforward so 2 layers are enough. The API layer is responsible for Query creation based on HTTP request and the module Application layer is responsible for query handling.\\n","Prediction":"## Decision\nWe will use the [RxJS](https:\/\/github.com\/Reactive-Extensions\/RxJS) library for asynchronous query processing.\\nRxJS is a library for asynchronous programming in JavaScript. It provides a stream of events that can be consumed by subscribing to a particular source. It also provides a way to subscribe to a particular source.\\nWe will use RxJS for asynchronous query processing.\\nRxJS also provides a library for asynchronous async\/await. This library is designed for asynchronous programming and is designed to be used in asynchronous queries.\\nWe will use RxJS for asynchronous async\/await.\\nRxJS provides a library for asynchronous queries. This library is designed for asynchronous programming and is designed to be used in asynchronous queries.\\nWe will use RxJS for asynchronous queries.\\nRxJS provides a library for asynchronous queries. This library is designed for asynchronous programming and is designed to be used in asynchronous queries.\\nWe will use RxJS for asynchronous queries.\\nRxJS provides a library for asynchronous queries. This library is designed for asynchronous programming and is designed to be used in asynchronous queries.\\nWe will use RxJS for asynchronous queries.\\nRxJS provides a library for asynchronous queries. This library is designed for asynchronous programming and is designed to"}
{"Context":"### Context\\n- linting for CSS\\n- After running it, it didn't pick up a single error\\n### Decision\\nDon't start using stylelint\\n","Decision":"## Decision\nDon't start using stylelint\\n","Prediction":"## Decision\nDon't start using stylelint\\nChosen option: \"use ESLint as linting tool\", because it's easier to setup and doesn't require any external tools. It also has a good community and is actively maintained.\\n### Positive impact\\n- No external tools are needed to lint CSS\\n- No external tools are needed to lint JavaScript\\n- Less configuration and less maintenance\\n- ESLint is also used for linting code\\n### Negative impact\\n- No external tools are needed to lint CSS\\n- No external tools are needed to lint JavaScript\\n- Less configuration and less maintenance\\n- ESLint is also used for linting code\\n## Decision\nDon't start using stylelint\\nChosen option: \"use ESLint as linting tool\", because it's easier to setup and doesn't require any external tools. It also has a good community and is actively maintained.\\n### Positive impact\\n- No external tools are needed to lint CSS\\n- No external tools are needed to lint JavaScript\\n- Less configuration and less maintenance\\n- ESLint is also used for linting code\\n### Negative impact\\n- No external tools are needed to lint CSS\\n- No external tools are needed to lint JavaScript\\n- Less configuration and less maintenance\\n-"}
{"Context":"## Context and Problem Statement\\nMultiple projects need to show data visually using charts and graphs. In order to provide unified look and feel across island.is we should commit to a single approach to implementing charts, i.e. choose one library for the whole repository.\\n## Decision Drivers\\n- Meet all requirements listed above\\n- API quality\\n- Pricing\\n- Bundle size\\n- Typescript support\\n","Decision":"## Decision\n- Meet all requirements listed above\\n- API quality\\n- Pricing\\n- Bundle size\\n- Typescript support\\nChosen option: \"Recharts\", because it meets all requirements, and overall has a very nice, dev-friendly API. It is the most popular (downloads per week) react charting library on github, and recommended across the community. We can customize how it looks, and start using it quickly without much groundwork.\\n### Positive Consequences\\n- We can start implementing charts and graphs as needed in island.is\\n### Negative Consequences\\n- It is a big dependency, but almost all chart libraries are big due to their nature. We will minimize the impact of this by enforcing charts to be lazy loaded in our codebase.\\n","Prediction":"## Decision\n- Meet all requirements listed above\\n- API quality\\n- Pricing\\n- Bundle size\\n- Typescript support\\nChosen option: \"Charts.js\", because it provides the best balance between performance and features.\\n### Positive Consequences\\n* Supports all features required by island.is\\n* The choice of Graphviz for chart rendering is a good option for island.is since it's already installed on the machine and is easy to use.\\n### Negative Consequences\\n* Bundle size is slightly larger than for other libraries. However, it's expected to be the same for all charts and graphs.\\n* The choice of Graphviz for chart rendering is a good option for island.is since it's already installed on the machine and is easy to use.\\n### Impact on other Software Engineering Decisions\\n* None\\n* None\\n* None\\n* None\\n* None\\n* None\\n* None\\n* None\\n* None\\n* None\\n* None\\n* None\\n* None\\n* None\\n* None\\n* None\\n* None\\n* None\\n* None\\n* None\\n* None\\n* None\\n* None\\n* None\\n* None\\n* None\\n* None\\n* None\\n* None\\n* None\\n* None\\n"}
{"Context":"## Context\\nScratch supports images\/sprites.\\nThe ftduino has no way to show images\/sprites.\\n","Decision":"## Decision\nScratch images\/sprites and image\/sprite related blocks are not supported.\\n","Prediction":"## Decision\nUse the [Image](https:\/\/github.com\/jimmyt\/ScratchImage) package to load images.\\nUse the [SpriteImage](https:\/\/github.com\/davidsharpe\/scratch-spriteimage) package to create sprites.\\nUse the [ImageAnimation](https:\/\/github.com\/davidsharpe\/scratch-imageanimation) package to animate the sprites.\\nUse the [SpriteAnimation](https:\/\/github.com\/davidsharpe\/scratch-imageanimation) package to create and animate sprites.\\nUse the [ImageAnimation](https:\/\/github.com\/davidsharpe\/scratch-imageanimation) package to create and animate sprites.\\nUse the [ImageAnimation](https:\/\/github.com\/davidsharpe\/scratch-imageanimation) package to create and animate sprites.\\nUse the [ImageAnimation](https:\/\/github.com\/davidsharpe\/scratch-imageanimation) package to create and animate sprites.\\nUse the [ImageAnimation](https:\/\/github.com\/davidsharpe\/scratch-imageanimation) package to create and animate sprites.\\nUse the [ImageAnimation](https:\/\/github.com\/davidsharpe\/scratch-imageanimation) package to create and animate sprites.\\nUse the [ImageAnimation](https"}
{"Context":"## Context\\nHigher-level HTTP clients (in Java) fundamentally try to bridge the programming model with HTTP network communication. Most of these clients tend to *squeeze* HTTP into the traditional method call style: single return value and exception cases. That has several issues. On the one hand it unnecessarily restricts the use of HTTP: There is no need to have a single success case. An API can return `200 OK`, `201 Created` and `202 Accepted` - all on the same endpoint. On the other hand it's not supportive enough: Error cases are usually neglected and while you usually get rich deserialization support for the happy cases, any kind of 4xx\/5xx response will just trigger a generic `HttpClientErrorException` or something alike. If the response body is exposed, it's at most as an `InputStream` or a byte array.\\nKnown HTTP clients that suffer from this are [Spring's `RestTemplate`](https:\/\/docs.spring.io\/spring\/docs\/current\/spring-framework-reference\/integration.html#rest-resttemplate), [Feign](https:\/\/github.com\/OpenFeign\/feign), [Retrofit](https:\/\/square.github.io\/retrofit\/) and JAX-RS.\\n","Decision":"## Decision\nRiptide is build around the concept of [response routing](https:\/\/github.com\/zalando\/riptide\/blob\/master\/docs\/concepts.md) which fundamentally dictates that:\\n- All responses are created equal\\n- fail to handle `200 OK` will produce the same error as `400 Bad Request`\\n- each response will map to exactly one route\\n- Most routes are general purpose and applicable to any kind of response\\n- A route that deserializes the body and triggers a callback will work for any status code\\n- Multiple happy cases are just multiple routes for different *2xx* status codes\\n","Prediction":"## Decision\nWe will not implement any HTTP clients. We will instead focus on the HTTP spec, which is more flexible and extensible than HTTP clients. We will use [HTTP\/2](https:\/\/en.wikipedia.org\/wiki\/HTTP_2) for all network communication, which is faster and more efficient than HTTP. We will use [JAX-RS](https:\/\/docs.oracle.com\/javaee\/7\/api\/javax\/servlet\/http\/jaxrs.html) for RESTful web services implementation.\\n### HTTP\/2\\nHTTP\/2 is the new HTTP protocol. It's faster and more efficient, and it's backwards compatible with HTTP. It supports [connection reuse](https:\/\/www.w3.org\/TR\/2017\/WD-httpbis-20171030\/#connection-reuse) and [streaming](https:\/\/www.w3.org\/TR\/2014\/REC-httpbis-20140918\/#streaming), which is very important for network communication in our case. Our network communication will be mostly HTTP requests and responses.\\n### JAX-RS\\nJAX-RS is a Java API for RESTful web services. It's a great choice for REST services implementation, it's very easy to learn and it's already supported by Spring. It's also"}
{"Context":"## Context\\nWe use the `utils-demo` package to deploy the whole Smarthub system. After receiving some feedback, we noticed that the name `utils-demo` didn't reflect the true intention of that package.\\n","Decision":"## Decision\nWe've decided to rename the `utils-demo` package to `migrations` and publish it on NPM.\\n","Prediction":"## Decision\nWe will rename the package `utils-demo` to `smarthub-demo`.\\nThis package will be used for the deployment of the `smarthub` service. It will be used by the `main` application. The `main` application will deploy the `smarthub` service to the `utils-demo` service.\\nThis package will be used to deploy the `smarthub` service. It will be used by the `main` application. The `main` application will deploy the `smarthub` service to the `utils-demo` service.\\nThe `main` application will use the `smarthub` service to deploy the `smarthub` service.\\n### Use Case\\n* The `main` application will deploy the `smarthub` service to the `smarthub-demo` service.\\n* The `main` application will use the `smarthub` service to deploy the `smarthub` service.\\n### Deployment\\n* The `main` application will use the `smarthub` service to deploy the `smarthub` service.\\n* The `main` application will use the `smarthub` service to deploy the `smarthub` service.\\"}
{"Context":"## Context and Problem Statement\\nWe need to be able to regenerate versions from snapshots. As documents is aim to change over time (location or filters) we can't rely on the last version of the declaration to regenerate the version from an old snapshot. So we need a system to keep track of declaration changes, that's what we called declarations and filters versioning.\\n","Decision":"## Decision\n[After consulting the community](https:\/\/github.com\/OpenTermsArchive\/engine\/issues\/156), the options 2A is retained as it hide complexity (compared to Option 1) of the history while increasing its discoverability (compared to Option 3) for contributors who might become more \u201cadventurous\u201d.\\n","Prediction":"## Decision\nThe system will be called `declarations` and will be used to keep track of the version of a declaration. Each declaration will have a `version` field. We will need to create a new `version` field for each version of the declaration. For example, the declaration `A` has a `version` of `1.0` and a `version` of `2.0`.\\nWe will use a `Version` class to store the information about the version of a declaration. The `Version` class will be a child class of `Declaration`. We will use the `@Entity` annotation to make it a table in the database.\\n### Use of `@Entity` annotation\\nThe `@Entity` annotation will be used to make the declaration a child class of `Version`. The `Version` class will be a child class of `Declaration`. The `Version` class will have the following fields:\\n* `version`\\n* `version_label`\\n* `version_description`\\n* `version_version`\\n### Using `@Version`\\nWe will use the `@Version` annotation to make the declaration version a child class of `Version`. The `Version` class will have the following fields:\\n"}
{"Context":"## Context\\nWe require symmetric ciphers to handle how we encrypt keys in the sdk,\\nand to potentially encrypt `priv_validator.json` in tendermint.\\nCurrently we use AEAD's to support symmetric encryption,\\nwhich is great since we want data integrity in addition to privacy and authenticity.\\nWe don't currently have a scenario where we want to encrypt without data integrity,\\nso it is fine to optimize our code to just use AEAD's.\\nCurrently there is not a way to switch out AEAD's easily, this ADR outlines a way\\nto easily swap these out.\\n### How do we encrypt with AEAD's\\nAEAD's typically require a nonce in addition to the key.\\nFor the purposes we require symmetric cryptography for,\\nwe need encryption to be stateless.\\nBecause of this we use random nonces.\\n(Thus the AEAD must support random nonces)\\nWe currently construct a random nonce, and encrypt the data with it.\\nThe returned value is `nonce || encrypted data`.\\nThe limitation of this is that does not provide a way to identify\\nwhich algorithm was used in encryption.\\nConsequently decryption with multiple algoritms is sub-optimal.\\n(You have to try them all)\\n","Decision":"## Decision\nWe should create the following two methods in a new `crypto\/encoding\/symmetric` package:\\n```golang\\nfunc Encrypt(aead cipher.AEAD, plaintext []byte) (ciphertext []byte, err error)\\nfunc Decrypt(key []byte, ciphertext []byte) (plaintext []byte, err error)\\nfunc Register(aead cipher.AEAD, algo_name string, NewAead func(key []byte) (cipher.Aead, error)) error\\n```\\nThis allows you to specify the algorithm in encryption, but not have to specify\\nit in decryption.\\nThis is intended for ease of use in downstream applications, in addition to people\\nlooking at the file directly.\\nOne downside is that for the encrypt function you must have already initialized an AEAD,\\nbut I don't really see this as an issue.\\nIf there is no error in encryption, Encrypt will return `algo_name || nonce || aead_ciphertext`.\\n`algo_name` should be length prefixed, using standard varuint encoding.\\nThis will be binary data, but thats not a problem considering the nonce and ciphertext are also binary.\\nThis solution requires a mapping from aead type to name.\\nWe can achieve this via reflection.\\n```golang\\nfunc getType(myvar interface{}) string {\\nif t := reflect.TypeOf(myvar); t.Kind() == reflect.Ptr {\\nreturn \"*\" + t.Elem().Name()\\n} else {\\nreturn t.Name()\\n}\\n}\\n```\\nThen we maintain a map from the name returned from `getType(aead)` to `algo_name`.\\nIn decryption, we read the `algo_name`, and then instantiate a new AEAD with the key.\\nThen we call the AEAD's decrypt method on the provided nonce\/ciphertext.\\n`Register` allows a downstream user to add their own desired AEAD to the symmetric package.\\nIt will error if the AEAD name is already registered.\\nThis prevents a malicious import from modifying \/ nullifying an AEAD at runtime.\\n","Prediction":"## Decision\nWe will use the `aes-128-cbc` cipher with a 128-bit key.\\nThis will be the default aes cipher for the sdk.\\nWe will support aes-256-cbc and aes-128-cbc for our SDK. We will add support\\nfor aes-256-gcm and aes-128-gcm in the future.\\nThis is because aes-128-cbc is also supported by the sdk.\\nThis will also allow us to support non-english alphabets.\\nWe will also support aes-256-gcm for the sdk, but not for our client.\\nThis will allow us to support non-english alphabets.\\nThis ADR is only for the sdk.\\nFor the client we will use aes-256-gcm for aes encryption.\\nThis will only be supported by the client, not the sdk.\\nWe will use the `aes-128-cbc` cipher with a 128-bit key.\\nThis will be the default aes cipher for the sdk.\\nWe will support aes-256-gcm and aes-128-gcm for our SDK. We will add support\\nfor aes-256-gcm and aes-256-gcm in the future.\\nThis is because aes"}
{"Context":"## Context\\nApplication configuration can be overridden by command line flags and environment variables. Is this something we want to take advantage of?\\n","Decision":"## Decision\nNo, configuration will be limited to the configuration file for sake of simplicity and having a single way to configure the application.\\n","Prediction":"## Decision\nWe will not make it easy to override configuration files with command line flags or environment variables. This will help to maintain the separation of concerns between configuration and application logic.\\n### Command Line Flags\\nCommand line flags are passed to the application at runtime and must be specified in the command line argument. For example, the --config flag could be specified to override the configuration file or environment variable. This allows configuration to be overridden for a single command line argument, but not for all command line arguments.\\n### Environment Variables\\nEnvironment variables are not passed to the application at runtime and must be specified in the environment variable. For example, the --config flag could be specified to override the configuration file or environment variable. Environment variables can be specified in a separate file or environment variable file. Environment variables are not passed to the application at runtime and must be specified in the environment variable. For example, the --config flag could be specified to override the configuration file or environment variable. Environment variables can be specified in a separate file or environment variable file.\\n### CLI Flags\\nCLI flags are passed to the application at runtime and are not passed to the application at runtime. For example, the --config flag could be specified to override the configuration file or environment variable. CLI flags"}
{"Context":"## Context\\nIn SAML RPs can provide some extra data along with the request. This is\\ncalled RelayState. Some existing RPs use this, but we're not sure what\\nthey use it for.\\nWe're not aware of any need for the service-provider to use relay state itself.\\n","Decision":"## Decision\nUsers will be able to specify whatever relay state they want to and it will be\\nprovided in the response.\\n","Prediction":"## Decision\nWe will not use relay state in the SAML RPs.\\nWe will use the [SAML 1.1 SP1](https:\/\/tools.ietf.org\/html\/rfc6749#section-4.4) protocol\\nfor SAML RPs. This is the standard protocol for SAML RPs.\\nWe will use the [SAML 1.1 SP1 Assertion](https:\/\/tools.ietf.org\/html\/rfc6749#section-4.4.1) format\\nfor SAML RPs.\\nWe will use the [SAML 1.1 SP1 Assertion Clause](https:\/\/tools.ietf.org\/html\/rfc6749#subsection-4.4.1) format\\nfor SAML RPs.\\nWe will use the [SAML 1.1 SP1 SP Single Logout](https:\/\/tools.ietf.org\/html\/rfc6749#section-4.4.1) format\\nfor SP SSO.\\nWe will use the [SAML 1.1 SP1 SP Single Sign Out](https:\/\/tools.ietf.org\/html\/rfc6749#section-4.4.1) format\\nfor SP SSO.\\nWe will use the [SAML 1.1 SP"}
{"Context":"## Context\\nSee [ADR1](.\/001-formatting-katas.md), which describes the formatting of a kata.\\nThis would normally lead to using a linter.\\n","Decision":"## Decision\nBut I am not a big fan of a huge set of rules\\nand currently configuring a linter just for those couple of rules is overhead.\\nSomething like prettier which automatically applies a rule set is fine, but a linter\\nis kinda like someone who just says that a cleanup is needed, an auto-fixer (like prettier)\\ndoes clean up instead of just talking about it, my 2 cents.\\n","Prediction":"## Decision\nWe will use [Linter](https:\/\/pypi.org\/project\/linter\/) to format the kata.\\nWe will use [Prettier](https:\/\/prettier.io\/) to lint the code.\\nWe will use [eslint](https:\/\/eslint.org\/) to enforce a style for the code.\\nWe will use [stylelint](https:\/\/stylelint.com\/) to enforce a style for the linting.\\nWe will use [Pylint](https:\/\/pypi.org\/project\/pylint\/) to enforce a style for the linting.\\nWe will use [Codecov](https:\/\/cov.io\/) to monitor the linting of the code.\\nWe will use [Deno](https:\/\/deno.land\/x\/Denodo) to run the tests.\\nWe will use [JSHint](https:\/\/www.jshint.com\/) to enforce a style for the tests.\\nWe will use [TypeMock](https:\/\/pypi.org\/project\/typemock\/) to mock the service.\\nWe will use [Mockito](https:\/\/mockito.org\/) to mock the service.\\nWe will use [Mockery](https:\/\/mockery.io\/) to mock the service.\\nWe will use [Presto](https:\/\/pre"}
{"Context":"## Context and Problem Statement\\nFor over the past 5 years (at least), we have been using [InternJS](https:\/\/github.com\/theintern\/intern) for our functional tests in the content-server. While this has worked well and caught several bugs, we should reevaluate it since we are migrating to react and have an opportunityto use newer\/faster\/better supported testing frameworks.\\n## Decision Drivers\\n- Ease of writing tests\\n- Good community support\\n- Learning curve of settings V2 development\\n","Decision":"## Decision\n- Ease of writing tests\\n- Good community support\\n- Learning curve of settings V2 development\\nUse InternJS for functional testing in the settings v2 app, but also update the tests to support async\/await. The biggest factor was not having to introduce another testing framework for developers to learn.\\n","Prediction":"## Decision\n- Ease of writing tests\\n- Good community support\\n- Learning curve of settings V2 development\\nChosen option: [Manual Testing](https:\/\/docs.microsoft.com\/en-us\/azure\/devops\/continuous-integration\/daily-tests\/manual-testing), because it is easy to write tests with the [InternJS](https:\/\/github.com\/theintern\/intern) package and has more features than [TestCafe](https:\/\/testcafe.io).\\n### Positive Consequences\\n- Test coverage will be higher than with [TestCafe](https:\/\/testcafe.io) because it is easier to write tests with InternJS\\n### Negative Consequences\\n- InternJS is not as widely supported as TestCafe\\n- InternJS has a learning curve that may take some time to get used to\\n- InternJS requires more setup than TestCafe\\n- InternJS does not have a test runner\\n- InternJS does not have a test harness\\n- InternJS does not have a test suite\\n- InternJS does not have a test suite in the same way that [Jest](https:\/\/jestjs.com) does\\n### Positive Consequences\\n- Test coverage will be higher than with [TestCafe](https:\/\/testcafe.io"}
{"Context":"## Context\\nDeployment can be done into CDN and static pages (Github Pages, S3, Netlify)\\n","Decision":"## Decision\nI choose Netlify for custom domain with TLS and very good support for Hugo.\\n","Prediction":"## Decision\nIn order to achieve the goal of low cost and fast deployment, we will use CDN and S3 for static assets.\\nWe will use Netlify as a static site generator.\\nWe will use AWS Lambda functions to run the deployment scripts.\\nWe will use CloudFront to serve the static files to end users.\\nWe will use S3 to store the static files in.\\nWe will use CloudFront to serve static files to end users.\\nWe will use CloudFront to serve static files to end users.\\nWe will use CloudFront to serve static files to end users.\\nWe will use CloudFront to serve static files to end users.\\nWe will use CloudFront to serve static files to end users.\\nWe will use CloudFront to serve static files to end users.\\nWe will use CloudFront to serve static files to end users.\\nWe will use CloudFront to serve static files to end users.\\nWe will use CloudFront to serve static files to end users.\\nWe will use CloudFront to serve static files to end users.\\nWe will use CloudFront to serve static files to end users.\\nWe will use CloudFront to serve static files to end users.\\nWe will use CloudFront to serve"}
{"Context":"## Context\\nInstalling Connaisseur isn't as simple as one might think. There is more to it then just applying some yaml files, all due to the nature of being an admission controller, which might block itself in various ways. This ADR depicts some issues during installation of Connaisseur and shows solutions, that try make the process simpler and easier to understand.\\n## Problem 1 - Installation order\\nConnaisseur's installation order is fairly critical. The webhook responsible for intercepting all requests is dependent on the Connaisseur pods and can only work, if those pods are available and ready. If not and  `FailurePolicy` is set to `Fail`, the webhook will block anything and everything, including the Connaisseur pods themselves. This means, the webhook must be installed *after* the Connaisseur pods are ready. This was previously solved using the `post-install` Helm hook, which installs the webhook configuration after all other resources have been applied and are considered ready. Just for installation purposes, this solution suffices. A downside of this is, every resource installed via a Helm hook isn't natively considered to be part of the chart, meaning a `helm uninstall` would completely ignore those resources and leave the webhook configuration in place. Then the situation of everything and anything being blocked arises again. Additionally, upgrading won't be possible, since you can't tell Helm to temporarily delete resources and then reapply them. That's why the `helm-hook` image and `bootstrap-sentinel` where introduced. They were used to temporarily delete the webhook and reapply it before and after installations, in order to beat the race conditions. Unfortunately, this solution always felt a bit clunky and added a lot of complexity for a seemingly simple problem.\\n### Solution 1.1 - Empty webhook as part of Helm release\\nThe `bootstrap sentinel` and `helm-hook` image won't be used anymore. Instead, an empty webhook configuration (a configuration without any rules) will be applied along all other resources during the normal Helm installation phase. This way the webhook can be normally deleted with the `helm uninstall` command. Additionally, during the `post-install` (and `post-upgrade`\/`post-rollback`) Helm hook, the webhook will be updated so it can actually intercept incoming request. So in a sense an unloaded webhook gets installed, which then gets \"armed\" during `post-install`. This also works during an upgrade, since the now \"armed\" webhook will be overwritten by the empty one when trying to apply the chart again! This will obviously be reverted back again after upgrading, with a `post-upgrade` Helm hook.\\n**Pros:** Less clunky and more k8s native.\\n**Cons:** Connaisseur will be deactivated for a short time during upgrading.\\n### Solution 1.2 - Bootstrap Sentinel and Helm hook\\nEverything stays as is! The Helm hook image is still used to (un)install the webhook, while the bootstrap sentinel is there to mark the Connaisseur pods as ready for initial installation.\\n**Pros:** Never change a running system.\\n**Cons:** Clunky, at times confusing for anyone not familiar with the Connaisseur installation order problem, inactive webhook during upgrade.\\n### Solution 1.3 - (Un)installation of webhook during Helm hooks\\nThe webhook can be easily installed during the `post-install` step of the Helm installation, but then it isn't part of the Helm release and can't be uninstalled, as mentioned above. With a neat little trick this is still possible: in the `post-delete` step the webhook can be reapplied in an empty (\"unarmed\") form, while setting the `hook-delete-policy` to delete the resource in either way (no matter if the Helm hook step fails or not). So in a way the webhook gets reapplied and then immediately deleted. This still works with upgrading Connaisseur if a rolling update strategy is pursued, meaning the old pods will still be available for admitting the new ones, while with more and more new pods being ready, the old ones get deleted.\\n**Pros:** Less clunky and more k8s native, no inactivity of the webhook during upgrade.\\n**Cons:** Slower upgrade of Connaisseur compared to solution 1.\\n### Decision outcome (1)\\nSolution 1.3 was chosen, as it is the more Kubernetes native way of doing things and Connaisseur will be always available, even during its own upgrade.\\n### Problem 2\\nAll admission webhooks must use TLS for communication purposes or they won't be accepted by Kubernetes. That is why Connaisseur creates its own self signed certificate, which it uses for communication between the webhook and its pods. This certificate is created within the Helm chart, using the native `genSelfSignedCert` function, which makes Connaisseur pipeline friendly as there is no need for additional package installation such as OpenSSL. Unfortunately, this certificate gets created every time Helm is used, whether that being a `helm install` or `helm upgrade`. Especially during an upgrade, the webhook will get a new certificate, while the pods will get their new one written into a secret. The problem is that the pods will only capitalize on the new certificate inside the secret once they are restarted. If no restart happens, the pods and webhook will have different certificates and any validation will fail.\\n### Solution 2.1 - Lookup\\nInstead of always generating a new certificate, the `lookup` function for Helm templates could be used to see whether there already is a secret defined that contains a certificate and then use this one. This way the same certificate is reused the whole time so no pod restarts are necessary. Should there be no secret with certificate to begin with, a new one can be generated within the Helm chart.\\n**Pros:** No need for restarts and changing of TLS certificates.\\n**Cons:** The lookup function takes some time to gather the current certs.\\n### Solution 2.2 - Restart\\nOn each upgrade of the Helm release, all pods will be restarted so they incorporate the new TLS secrets.\\n**Pros:** -\\n**Cons:** Restarting takes time and may break if too many Connaisseur pods are unavailable at the same time.\\n### Solution 2.3 - External TLS\\nGo back to using an external TLS certificate which is not being generated within the Helm chart, but by pre-configuring it or using OpenSSL.\\n**Pros:** Fastest solution.\\n**Cons:** More configurational effort and\/or not pipeline friendly (may need OpenSSL).\\n### Decision outcome (2)\\nSolution 2.1 is being implemented, as it is important that Connaisseur works with as little configuration effort as possible from the get-go. Nonetheless an external configuration of TLS certificates is still considered for later development.\\n--\\n","Decision":"## Decision\nSolution 1.3 was chosen, as it is the more Kubernetes native way of doing things and Connaisseur will be always available, even during its own upgrade.\\n### Problem 2\\nAll admission webhooks must use TLS for communication purposes or they won't be accepted by Kubernetes. That is why Connaisseur creates its own self signed certificate, which it uses for communication between the webhook and its pods. This certificate is created within the Helm chart, using the native `genSelfSignedCert` function, which makes Connaisseur pipeline friendly as there is no need for additional package installation such as OpenSSL. Unfortunately, this certificate gets created every time Helm is used, whether that being a `helm install` or `helm upgrade`. Especially during an upgrade, the webhook will get a new certificate, while the pods will get their new one written into a secret. The problem is that the pods will only capitalize on the new certificate inside the secret once they are restarted. If no restart happens, the pods and webhook will have different certificates and any validation will fail.\\n### Solution 2.1 - Lookup\\nInstead of always generating a new certificate, the `lookup` function for Helm templates could be used to see whether there already is a secret defined that contains a certificate and then use this one. This way the same certificate is reused the whole time so no pod restarts are necessary. Should there be no secret with certificate to begin with, a new one can be generated within the Helm chart.\\n**Pros:** No need for restarts and changing of TLS certificates.\\n**Cons:** The lookup function takes some time to gather the current certs.\\n### Solution 2.2 - Restart\\nOn each upgrade of the Helm release, all pods will be restarted so they incorporate the new TLS secrets.\\n**Pros:** -\\n**Cons:** Restarting takes time and may break if too many Connaisseur pods are unavailable at the same time.\\n### Solution 2.3 - External TLS\\nGo back to using an external TLS certificate which is not being generated within the Helm chart, but by pre-configuring it or using OpenSSL.\\n**Pros:** Fastest solution.\\n**Cons:** More configurational effort and\/or not pipeline friendly (may need OpenSSL).\\n### Decision outcome (2)\\nSolution 2.1 is being implemented, as it is important that Connaisseur works with as little configuration effort as possible from the get-go. Nonetheless an external configuration of TLS certificates is still considered for later development.\\n--\\n","Prediction":"## Decision\nIf the Connaisseur webhook configuration is not defined in the Helm chart, the Connaisseur will be deactivated. Otherwise, the Connaisseur webhook configuration will be updated, so it can be used during the Helm installation.\\n**Decision:** Connaisseur webhook will be deactivated during Helm installation.\\n**Reason:** The Connaisseur webhook configuration is not defined in the Helm chart. During the Helm installation, Connaisseur will be deactivated, while the Connaisseur webhook configuration can be updated during the Helm update step.\\n**Decision:** Connaisseur webhook will be updated during Helm installation.\\n**Reason:** Connaisseur webhook configuration is defined in the Helm chart, but not updated during the Helm installation.\\n**Decision:** Connaisseur webhook will be updated during Helm installation.\\n**Reason:** Connaisseur webhook configuration is defined in the Helm chart, but not updated during the Helm update step.\\n**Decision:** Connaisseur webhook will be updated during Helm installation.\\n**Reason:** Connaisseur webhook configuration is defined in the Helm chart, but not updated during the Helm update step.\\n**Decision:** Connais"}
{"Context":"## Context and Problem Statement\\nViskuausan is proving to be more complex and larger platform than just\\na simple documentation site from static content.\\nWhich React framework provides the most out-of-the-box features that we need?\\n## Decision Drivers\\n- Should use NodeJS and React as outlined in [S\u00cd technical direction]\\n- Should be able to support markdown content rendered to HTML\\n- Should be open source\\n- Should be customizable to island.is UI design\\n","Decision":"## Decision\n- Should use NodeJS and React as outlined in [S\u00cd technical direction]\\n- Should be able to support markdown content rendered to HTML\\n- Should be open source\\n- Should be customizable to island.is UI design\\nChosen option: NextJS + NestJS\\nNextJS is the chosen web framework for all island.is websites needing server side rendering.\\nAs Viskuausan will probably be merged with island.is main website, creating it using same\\nframeworks makes it easy to merge later on.\\nIt is easier to reuse islandis-ui components using NextJS over Docusaurus.\\nDocusaurus main advantage over Next is out-of-the-box markdown support but\\nit is easy to add markdown support in NextJS using [Remark] library.\\nNestJS is used to create backend services and Viskuausan needs\\nfew backend services related to the X-Road and API GW integrations.\\nProvides functionalities like ORM, dependency injection, unit testing.\\n","Prediction":"## Decision\n- Should use NodeJS and React as outlined in [S\u00cd technical direction]\\n- Should be able to support markdown content rendered to HTML\\n- Should be open source\\n- Should be customizable to island.is UI design\\nChosen option: \"Next.js\", because it is a popular framework that is\\nsupported by the island.is team and is a popular choice amongst\\nother companies in the industry.\\n### Positive Consequences\\n- We will be able to create a next.js website in a matter of minutes\\n- We will be able to choose a framework that is supported by the\\nisland.is team and that will be maintained by island.is team in\\nthe future\\n- We will be able to customize the template to fit our\\nrequirements\\n### Negative Consequences\\n- We will need to learn how to use the framework\\n- We will need to learn how to customize the template\\n- We will need to learn how to create a new website\\n- We will need to learn how to maintain the website\\n- We will need to learn how to deploy the website\\n- We will need to learn how to handle security issues\\n- We will need to learn how to create a developer environment\\n- We will need to learn how to create a"}
{"Context":"## Context and Problem Statement\\nPayment data used in Stripe, PayPal, Apple, and Google IAP are handled as individual documents. We typically need to access different portions of these at different times. In addition, their schema occasionally changes (API updates). Stripe and PayPal have API limits, which means we typically need to duplicate\/cache data we retrieve from them. At the moment we cache this data in Redis. This mostly works, but restricts our ability to run queries across payment data sets without pulling it all into BigQuery, or making substantial amounts of Stripe\/PayPal requests.\\nTo handle IAP, we need to store Apple receipts and Google subscription objects. These should be organized with the FxA user they belong to. They're arbitrary JSON documents that we also need copied to BigQuery for ETL jobs. To future-proof our payment data storage needs we should have the flexibility needed to store Stripe\/Paypal payment data in the same data store with the same ETL capability.\\n## Decision Drivers\\n- Engineering resources\\n- Arbitrary JSON document querying ability\\n- Ease of loading data for ETL jobs\\n- Capable of storing arbitrary JSON data\\n- Integration with Stripe and other payment systems\\n","Decision":"## Decision\n- Engineering resources\\n- Arbitrary JSON document querying ability\\n- Ease of loading data for ETL jobs\\n- Capable of storing arbitrary JSON data\\n- Integration with Stripe and other payment systems\\nGoogle Firestore. We've used Firestore in the event-broker, it has a rich API, and extensions that overlap with our requirements that reduces engineering and operations resources.\\n","Prediction":"## Decision\n- Engineering resources\\n- Arbitrary JSON document querying ability\\n- Ease of loading data for ETL jobs\\n- Capable of storing arbitrary JSON data\\n- Integration with Stripe and other payment systems\\nChosen option: \"Store payment data as individual objects\", because it seems to be the easiest solution for now. We may have to change this in the future, but it's not worth the risk now.\\n### Positive Consequences\\n- We can query payment data easily without pulling it all into BigQuery\\n- We can use any JSON schema we want, so we can store payment data in any format\\n- We can store Stripe\/Paypal payment data in the same data store as the FxA user's payment data (if we ever need that flexibility), so it's easy to load Stripe\/Paypal data into BigQuery for ETL jobs\\n### Negative Consequences\\n- We can't query payment data quickly, because it's in a single JSON document\\n- We can't store Stripe\/Paypal payment data in the same format, so it's not a good fit for other data stores (e.g. Google IAP)\\n- We can't query payment data easily, because it's all in a single JSON document\\n- We can"}
{"Context":"## Context\\nKubernetes does not provide an overlay network out of the box and we need to choose\\nwhich one to use from [here](https:\/\/kubernetes.io\/docs\/concepts\/cluster-administration\/networking\/).\\n","Decision":"## Decision\nWe have decided to use the [WeaveNet](https:\/\/www.weave.works\/oss\/net\/) network as this\\nhas already been used by the JASMIN team. It also appears easy to use and there is good\\ndocumentation.\\n","Prediction":"## Decision\nWe will use the [Kubernetes Ingress Controller](https:\/\/kubernetes.io\/docs\/concepts\/ingress\/ingress-controller\/) to configure our ingress endpoints.\\nThe ingress controller will manage the deployment of the ingress controller and the\\ncluster. It will also manage the deployment of the ingress controller\\npod. It will also manage the deployment of the ingress controller\\ncontroller application. It will also handle the cluster\\nmanagement. It will also handle the cluster health check.\\n* Kubernetes Ingress Controller\\n* Kubernetes Ingress Controller\\n* Kubernetes Ingress Controller\\n* Kubernetes Ingress Controller\\n* Kubernetes Ingress Controller\\n* Kubernetes Ingress Controller\\n* Kubernetes Ingress Controller\\n* Kubernetes Ingress Controller\\n* Kubernetes Ingress Controller\\n* Kubernetes Ingress Controller\\n* Kubernetes Ingress Controller\\n* Kubernetes Ingress Controller\\n* Kubernetes Ingress Controller\\n* Kubernetes Ingress Controller\\n* Kubernetes Ingress Controller\\n* Kubernetes Ingress Controller\\n* Kubernetes Ingress Controller\\n* Kubernetes Ingress Controller\\n* Kubernetes Ingress Controller\\n* Kubernetes Ingress Controller\\n* Kubernetes Ingress Controller\\n* Kubernetes Ingress Controller\\n* Kubernetes Ingress Controller\\n* Kubernetes Ingress Controller\\n* Kubernetes Ingress Controller\\n* Kubernetes Ingress Controller\\n*"}
{"Context":"## Context\\nThe _ordering system_ should accept a wide variety of payment methods to satisfy the user's expectations and increase the user base. The number of payment systems is significant and each of them have nuances in the API and ways of communication.\\n","Decision":"## Decision\nUse a 3rd party payment provider to delegate dealing with different payment systems.\\n","Prediction":"## Decision\nUse the most widely accepted and widely used payment systems. These are: PayPal, Stripe, and Braintree.\\nPayPal is the most widely accepted payment system in the world and it is the most widely used payment system in the UK and Europe.\\nStripe is the most widely used payment system in the US and is the most widely used payment system in the EU.\\nBraintree is the most widely used payment system in the US and is the most widely used payment system in the UK.\\nThese payment systems are widely supported by the _Open Source_ community.\\nPayPal has a 100% open source implementation and is the most widely used payment system in the world.\\nStripe has a 100% open source implementation and is the most widely used payment system in the US.\\nBraintree has a 100% open source implementation and is the most widely used payment system in the US.\\nStripe is the most widely used payment system in the US and is the most widely used payment system in the EU.\\nStripe is the most widely used payment system in the US and is the most widely used payment system in the UK.\\nStripe is the most widely used payment system in the UK and is the most widely used payment system in"}
{"Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\nSome other useful links:\\n[https:\/\/github.blog\/2020-08-13-why-write-adrs\/](https:\/\/github.blog\/2020-08-13-why-write-adrs\/)\\n[https:\/\/understandlegacycode.com\/blog\/earn-maintainers-esteem-with-adrs\/](https:\/\/understandlegacycode.com\/blog\/earn-maintainers-esteem-with-adrs\/)\\n","Prediction":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\nWe will use the [Decision Tool](https:\/\/www.atlassian.com\/software\/jira\/documentation\/decision-tools\/en\/1.8.0\/en-US\/decision-tool.html) to create ADRs.\\nWe will use the [AADM](https:\/\/github.com\/airbrake\/adr) library to generate ADRs from the architecture decision files.\\nWe will use the [Semantic Versioning](https:\/\/semver.org\/) format for versioning ADRs.\\nWe will use the [Semantic Versioning 1.0.0](https:\/\/semver.org\/2016\/11\/15\/semantic-versioning-1-0\/) format for ADR names.\\nWe will use the [Semantic Versioning 1.0.0-alpha.1](https:\/\/semver.org\/2017\/05\/11\/semantic-versioning-1-0-0-alpha-1\/) format for ADRs.\\nWe will use the [Semantic Versioning 1.0.0-alpha.1](https:\/\/semver"}
{"Context":"## Context\\nArchitectural decisions are design decisions that address architecturally-significant requirements.\\n### Glossary\\n- **Architecture Knowledge Management (AKM)**: the efficient handling of information and resources pertaining to software architecture.\\n- **Architecture Decision Log (ADL)**: a collection of architecture decision records created and maintained for a particular project.\\n- **Architecture Decision Record (ADR)**: a document that captures an important architectural decision.\\n- **Architecture Decision (AD)**: a software design choice that addresses a significant requirement.\\n- **Architecturally-Significant Requirement (ASR)**: a requirement that has a measurable effect on a software system's architecture and quality.\\n","Decision":"## Decision\n- **Architecture Decision Record (ADR)**: a document that captures an important architectural decision.\\n- **Architecture Decision (AD)**: a software design choice that addresses a significant requirement.\\n- **Architecturally-Significant Requirement (ASR)**: a requirement that has a measurable effect on a software system's architecture and quality.\\nRecord architectural decisions in a decision log in this repository.\\nA [decision backlog](index.md) will be maintained for less pertinent decisions.\\n- [ThoughtWorks Technology Radar](https:\/\/www.thoughtworks.com\/radar\/techniques\/lightweight-architecture-decision-records)\\n","Prediction":"## Decision\nWe will use the following principles to guide architecture decisions made in this project:\\n- **Don't reinvent the wheel.**\u202f\\n- **Make it easy to reuse existing components.**\u202f\\n- **Don't do something that's not needed.**\u202f\\n- **Don't make assumptions.**\u202f\\n- **Don't build something that's not needed.**\u202f\\n- **Don't build something that's not needed.**\u202f\\n- **Don't make assumptions.**\u202f\\n- **Don't build something that's not needed.**\u202f\\n- **Don't build something that's not needed.**\u202f\\n- **Don't build something that's not needed.**\u202f\\n- **Don't build something that's not needed.**\u202f\\n- **Don't build something that's not needed.**\u202f\\n- **Don't build something that's not needed.**\u202f\\n- **Don't build something that's not needed.**\u202f\\n- **Don't build something that's not needed.**\u202f\\n- **Don't build something that's"}
{"Context":"## Context\\nThe Childrens Social Care Placement Service is in the [Alpha] phase of delivery. As such it is not expected to process real data for users, more to the point, it is an exploratory phase for building prototypes and testing different ideas.\\n","Decision":"## Decision\nThe placement alpha prototypes and development work will only process fake data.\\nThe reasoning for this is to allow the Service Team to focus on developing features in fast iteration feedback loops, rather than undertaking the additional non-functional needs for processing live data. The Security needs, including passing the \"Authority to Operate\" governance gateway alone, would jeopardise the desired delivery timeframe for Alpha of January 2021.\\n","Prediction":"## Decision\nAs such, we will not be using the [Childrens Social Care Placement Service](https:\/\/github.com\/ChildrensSocialCare\/childrens-social-care-placement-service) for real data processing.\\nWe will instead use the [Childrens Social Care Placement Service](https:\/\/github.com\/ChildrensSocialCare\/childrens-social-care-placement-service) for our [self service](https:\/\/github.com\/ChildrensSocialCare\/self-service) component.\\nThis will allow us to test our environment for the next few months.\\nWe will also be using the [Childrens Social Care Placement Service](https:\/\/github.com\/ChildrensSocialCare\/childrens-social-care-placement-service) for our [service discovery](https:\/\/github.com\/ChildrensSocialCare\/service-discovery) component.\\nThis will allow us to test our environment for the next few months.\\nWe will also be using the [Childrens Social Care Placement Service](https:\/\/github.com\/ChildrensSocialCare\/childrens-social-care-placement-service) for our [service integration](https:\/\/github.com\/ChildrensSocialCare\/service-integration) component.\\nThis will allow us to test our environment for the next few months.\\nWe will also be using the [Childrens Social Care"}
{"Context":"## Context\\nWe sought to identify a single templating language that would provide a source of truth across our frontend & content management platforms. This aim seeks to reduce the effort in developing on the content management platform by mitigating the burden of markup integration. We also aimed to find a templating language that allowed us to utilise the components already created as part of the NHSUK frontend framework.\\n### Options\\nWe identified three main candidates to use as our templating language:\\n#### Freemarker:\\nFreemarker - https:\/\/freemarker.apache.org\/ is the native templating language used by BloomReach and although there are projects that provide the ability to parse Freemarker templates with Javascript - this would be a prerequisite to using a tool like StoryBook to manage the frontend components - choosing Freemarker and utlising the prexisting work of the NHSUK Frontend project would require implementing and maintaining Freemarker versions of those components.\\n#### Nunjucks:\\nNunjucks - https:\/\/mozilla.github.io\/nunjucks\/ is the native templating language used by the NHSUK Frontend project. It's syntax is similar to the Jinja templating language which is widely used in Python application development. Choosing Nunjucks would require implementing a custom integration with BloomReach using that platforms SPA\/headless functionality. Integrating with StoryBook would require the creation of a custom StoryBook renderer.\\n#### React:\\nReact - https:\/\/reactjs.org\/ is a fuller javascript framework for building UIs. BloomReach provides an out of the box, if perhaps somewhat nascent integration with React through its SPA\/headless functionality. StoryBook provides full support for React and NHS Digital have developed and maintain an implementation of the NHSUK Frontend in React\\n- https:\/\/github.com\/NHSDigital\/nhsuk-react-components\\n","Decision":"## Decision\nWe will use React to develop the HEE Frontend platform.\\n","Prediction":"## Decision\nWe will use React for our templating language.\\n- https:\/\/reactjs.org\/ is widely used within the NHSUK Frontend project and has the support of the NHS Digital team.\\n- https:\/\/reactjs.org\/docs\/ is well documented.\\n- https:\/\/facebook.github.io\/react\/ is widely used within the NHSUK Frontend project and has the support of the NHS Digital team.\\n- https:\/\/reactjs.org\/docs\/ provides a well documented, widely supported framework.\\n- https:\/\/www.npmjs.com\/package\/react\\n- https:\/\/www.npmjs.com\/package\/react-dom\\n- https:\/\/www.npmjs.com\/package\/react-router\\n- https:\/\/www.npmjs.com\/package\/react-select\\n- https:\/\/www.npmjs.com\/package\/reselect\\n- https:\/\/www.npmjs.com\/package\/validate\\n- https:\/\/www.npmjs.com\/package\/react-helmet\\n- https:\/\/www.npmjs.com\/package\/sass\\n- https:\/\/www.npmjs.com\/package\/less\\n- https:\/\/www.npmjs.com\/package\/less-styles\\n- https:\/\/www.npmjs.com\/package\/less-sass\\n- https:\/\/www.npmjs.com\/package\/babel\\n- https:\/\/www.npm"}
{"Context":"## Context\\nThe issue motivating this decision, and any context that influences or constrains the decision.\\nWe need to distinguish between decorator and presenter objects in order to clarify which would be used in a given\\nsituation.\\n","Decision":"## Decision\nDecorators extend SimpleDelegator and will always delegate undefined methods to the delegated object.\\nPresenters take the form of \"plain ol' Ruby objects\" (POROs) and would generally not delegate methods to an object.\\nTheir usage is designed to be more flexible when the rendering of content isn't tied specifically to one object.\\n","Prediction":"## Decision\nDecorator objects will be used to decorate a given method, function, or class.\\nDecorator objects will not be used to annotate a given class with additional methods, fields, or properties.\\nDecorator objects will be used to decorate a given method, function, or class.\\nDecorator objects will not be used to annotate a given class with additional methods, fields, or properties.\\nDecorator objects will be used to decorate a given method, function, or class.\\nDecorator objects will not be used to annotate a given class with additional methods, fields, or properties.\\nDecorator objects will be used to decorate a given method, function, or class.\\nDecorator objects will not be used to annotate a given class with additional methods, fields, or properties.\\nDecorator objects will be used to decorate a given method, function, or class.\\nDecorator objects will not be used to annotate a given class with additional methods, fields, or properties.\\nDecorator objects will be used to decorate a given method, function, or class.\\nDecorator objects will not be used to annotate a given class with additional methods, fields, or properties.\\nDecorator objects will be used to decorate a given method, function, or class.\\nDecorator objects will not be used to annotate"}
{"Context":"Context\\n-------\\nDifferent projects have followed different testing approaches on a\\ncontinuum from \"test all the things!\" to \"only test some of the things\".\\n\"Test all the things!\" is higher cost and higher maintenance if expected\\nfeature behavior changes, while \"only test some of the things\" risks\\nmissing some tests that should exist. Picking a point on that continuum\\nthat gets the best of both worlds without the worst of either is hard.\\nDecision\\n--------\\nTesting rules should vary based on what is being tested. Our testing\\nphilosophy should emphasize meaningful, readable tests. To the extent\\npossible, some sort of narrative testing framework\\n([ScalaTest](http:\/\/www.scalatest.org\/),\\n[Jasmine](http:\/\/jasmine.github.io\/2.4\/introduction.html), etc.) should\\nbe used to make explanation of features easily communicable to\\nnon-development staff. \"Only test some of the things\" can work as long\\nas there are rules for when tests are necessary.\\n#### New features\\nThese should almost always include automated tests demonstrating the\\nfeature's behavior. The exception is for \"small\" features. Features\\nshould be labeled \"small\" when their cards are created. If a feature\\nisn't labeled \"small,\" it needs a test.\\nFrontend testing should test behavior rather than presence of elements\\non a page except when element presence testing is the only way to test\\nbehavior.\\n#### Bug fixes\\nThese should always include automated tests to protect against\\nregression. Commit messages *and* docstrings for the test must include\\nissue numbers to make discussion around the original bug more accessible\\nif the test regresses in the future.\\n#### Refactors\\nThese shouldn\u2019t need new tests unless the refactor is fixing a buggy\\nbehavior, in which case it\u2019s more properly a bug fix and requires a test\\naccordingly.\\n#### Non-application changes\\nThis category includes changes to AWS configurations, documentation,\\ncorrections to misspelled words, etc. These changes do not require\\ntests. For changes most like the former, review should ensure\\nappropriate behavior.\\n#### Retirement of worthless tests\\nTests that stop providing information should be retired. This category\\nrefers primarily to tests that don't address bug fixes. It may be\\nimportant initially to include a test that demonstrates that a feature\\nfunctions as intended; however, over time, this test becomes less\\nimportant, and it's likely that the test has never failed outside the\\ndevelopment environment in which the feature was developed. We will\\ndevelop standards and seek out tools to enforce worthless test\\nretirement.\\nConsequences\\n------------\\nThe team will need to:\\n1.  carefully maintain links between cards and issues\\n2.  make testing requirements explicit up-front, when cards are created\\n3.  enforce new testing philosophy through code review and available\\ntools\\n4.  find tools for worthless test retirement where tools don't currently\\nexist\\n","Decision":"## Decision\n--------\\nTesting rules should vary based on what is being tested. Our testing\\nphilosophy should emphasize meaningful, readable tests. To the extent\\npossible, some sort of narrative testing framework\\n([ScalaTest](http:\/\/www.scalatest.org\/),\\n[Jasmine](http:\/\/jasmine.github.io\/2.4\/introduction.html), etc.) should\\nbe used to make explanation of features easily communicable to\\nnon-development staff. \"Only test some of the things\" can work as long\\nas there are rules for when tests are necessary.\\n#### New features\\nThese should almost always include automated tests demonstrating the\\nfeature's behavior. The exception is for \"small\" features. Features\\nshould be labeled \"small\" when their cards are created. If a feature\\nisn't labeled \"small,\" it needs a test.\\nFrontend testing should test behavior rather than presence of elements\\non a page except when element presence testing is the only way to test\\nbehavior.\\n#### Bug fixes\\nThese should always include automated tests to protect against\\nregression. Commit messages *and* docstrings for the test must include\\nissue numbers to make discussion around the original bug more accessible\\nif the test regresses in the future.\\n#### Refactors\\nThese shouldn\u2019t need new tests unless the refactor is fixing a buggy\\nbehavior, in which case it\u2019s more properly a bug fix and requires a test\\naccordingly.\\n#### Non-application changes\\nThis category includes changes to AWS configurations, documentation,\\ncorrections to misspelled words, etc. These changes do not require\\ntests. For changes most like the former, review should ensure\\nappropriate behavior.\\n#### Retirement of worthless tests\\nTests that stop providing information should be retired. This category\\nrefers primarily to tests that don't address bug fixes. It may be\\nimportant initially to include a test that demonstrates that a feature\\nfunctions as intended; however, over time, this test becomes less\\nimportant, and it's likely that the test has never failed outside the\\ndevelopment environment in which the feature was developed. We will\\ndevelop standards and seek out tools to enforce worthless test\\nretirement.\\nConsequences\\n------------\\nThe team will need to:\\n1.  carefully maintain links between cards and issues\\n2.  make testing requirements explicit up-front, when cards are created\\n3.  enforce new testing philosophy through code review and available\\ntools\\n4.  find tools for worthless test retirement where tools don't currently\\nexist\\n","Prediction":"## Decision\n--------\\nTesting rules should vary based on what is being tested. Our testing\\nphilosophy should emphasize meaningful, readable tests. To the extent\\npossible, some sort of narrative testing framework\\n([ScalaTest](http:\/\/www.scalatest.org\/),\\n[Jasmine](http:\/\/jasmine.github.io\/2.4\/introduction.html), etc.) should\\nbe used to make explanation of features easily communicable to\\nnon-development staff. \"Only test some of the things\" can work as long\\nas there are rules for when tests are necessary.\\n#### New features\\nThese should almost always include automated tests to demonstrate the\\nfeature's behavior. The exception is for \"small\" features. Features\\nshould be labeled \"small\" when their cards are created. If a feature\\nisn't labeled \"small,\" it needs a test.\\nFrontend testing should test behavior rather than presence of elements\\non a page except when element presence testing is the only way to test\\nbehavior.\\n#### Bug fixes\\nThese should always include automated tests to protect against\\nregression. Commit messages *and* docstrings for the test must include\\nissue numbers to make discussion around the original bug more accessible\\nif the test regresses in the future"}
{"Context":"## Context\\nWe need to be able to assign a location when creating an employee and to have autocompletion that list all availables location on cmdlet.\\nBut there are issues related to the location_path attributes on users in easyvista's API.\\nAccording to the documentation ([employees creation](https:\/\/wiki.easyvista.com\/xwiki\/bin\/view\/Documentation\/REST+API+-+Create+an+employee)) the field can't be\\nused in a request to create an employee.\\nAlso filtering on location_path when querying \/locations does not work : queries ignore the field an simply returns the 100 first results.\\n","Decision":"## Decision\nInstead of using the [search option](https:\/\/wiki.easyvista.com\/xwiki\/bin\/view\/Documentation\/REST+API+-+See+a+list+of+locations) from the API we will rely instead on powershell\\nWhere-Object cmdlet with the max_rows parameter from the API.\\n","Prediction":"## Decision\nWe will use the `location` attribute on the users object to be able to use the location path in our request.\\nWe will also add a method to allow to get the location paths.\\nFor the moment we will add the method on the `User` class and use the `Get-User` cmdlet to retrieve all the users from the database.\\nWe will add a method on the `Location` class to allow to get the location paths.\\nWe will add a method on the `Location` class to allow to get the location info.\\nWe will add a method on the `Location` class to allow to get the location path.\\nWe will add a method on the `Location` class to allow to get the location info.\\nWe will add a method on the `Location` class to allow to get the location path.\\nWe will add a method on the `Location` class to allow to get the location path.\\nWe will add a method on the `Location` class to allow to get the location path.\\nWe will add a method on the `Location` class to allow to get the location path.\\nWe will add a method on the `Location` class to allow to get the location path.\\"}
{"Context":"## Problem\\nOn structures like maps or [arrays](array.md) there are different\\npossibilities which keys are looked up in the KeySet and which\\nare simply iterated.\\nWithout any guidelines, applications would provide arbitrary inconsistent\\nbehavior.\\n","Decision":"## Decision\nEvery key that an application wants to use, must be looked up\\nwith `ksLookup` using a cascading lookup key.\\n- [Arrays](array.md)\\n","Prediction":"## Decision\nThe default behavior will be to look up the keys in the KeySet\\nin the order they are provided in the `:key` parameter.\\nKeys are looked up in the order they are provided in the\\n`:key` parameter.\\nThis decision is based on the following rules:\\n* The keyset is a set of keys.\\n* The keys are ordered according to the order they are\\nprovided in the `:key` parameter.\\n* Keys in the set are not iterated.\\n* Keys in the set are not added.\\n* Keys not in the set are not looked up.\\n* Keys in the set are not removed.\\n* Keys in the set are not changed.\\n* Keys in the set are not removed.\\n* Keys in the set are not changed.\\n* Keys in the set are not changed.\\n* Keys in the set are not changed.\\n* Keys in the set are not changed.\\n* Keys in the set are not changed.\\n* Keys in the set are not changed.\\n* Keys in the set are not changed.\\n* Keys in the set are not changed.\\n* Keys in the set are not changed.\\n* Keys in the set are not"}
{"Context":"## Decision Drivers\\n* Time and complexity\\n* Do we have bandwidth to make these changes?\\n* Do we have enough capacity handle the unknowns or deal with future issues this decision may cause?\\n* Do we need more information?\\n* Do we have enough future capacity to delay this?\\n* Flexibility\\n* How likely is this solution to support new shipment types or even existing shipments?\\n* Consistency\\n* How consistent are our design patterns?\\n* Are they intuitive?\\n","Decision":"## Decision\n* Time and complexity\\n* Do we have bandwidth to make these changes?\\n* Do we have enough capacity handle the unknowns or deal with future issues this decision may cause?\\n* Do we need more information?\\n* Do we have enough future capacity to delay this?\\n* Flexibility\\n* How likely is this solution to support new shipment types or even existing shipments?\\n* Consistency\\n* How consistent are our design patterns?\\n* Are they intuitive?\\n* Chosen Alternative: [Create a new table](#create-a-new-table)\\n* Positive Outcomes: We will have a pattern that is easier to understand and will potentially be helpful in the future when implementing new shipments. The tentative plan is to test this new pattern out with PPMs, and if things go smoothly, revisit this to be the default DB pattern for all new shipment types moving forward.\\n* Consequences: If this new pattern is not utilized elsewhere we may be adding more complexity by introducing another pattern that is only partially used.\\n","Prediction":"## Decision\n* Time and complexity\\n* Do we have bandwidth to make these changes?\\n* Do we have enough capacity handle the unknowns or deal with future issues this decision may cause?\\n* Do we have enough future capacity to delay this?\\n* Flexibility\\n* How likely is this solution to support new shipment types or even existing shipments?\\n* Consistency\\n* How consistent are our design patterns?\\n* Are they intuitive?\\nChosen option: \"Use a new service\", because it's the most obvious choice.\\n### Positive Consequences\\n* We can use a new service without making any changes to our architecture\\n* We can use a new service without having to touch our existing API\\n* We can use a new service with a new API and a new set of rules\\n* We can use a new service with a new architecture, but it should be easy to move to a new one later\\n* We can use a new service without having to change our design patterns\\n* We can use a new service without having to change our architecture\\n* We can use a new service with a new API and a new set of rules\\n* We can use a new service with a new architecture, but it should be easy to move to"}
{"Context":"## Context\\nIt can be easy to miss an inefficient database query during code review. These\\ncan build up and have detremental performance on the application and effect the\\nuser experience.\\n","Decision":"## Decision\nAdd an automatic check to the test suite to ensure (through CI) that these are\\nfixed before being deployed.\\n","Prediction":"## Decision\nUse [Rapyd](http:\/\/rapyd.org)\\nto check for and report database queries that are inefficient.\\nThis will help you spot and fix these queries as they arise.\\n### Use cases\\nRapyd can be used to check for the following query types:\\n* Queries that perform a single call to a database\\n* Queries that perform a single operation on a database\\n* Queries that perform an expensive operation on a database\\n* Queries that perform an expensive operation on a database\\n* Queries that perform a large operation on a database\\n* Queries that perform an expensive operation on a database\\n* Queries that perform a large operation on a database\\n* Queries that perform a large operation on a database\\n* Queries that perform a large operation on a database\\n* Queries that perform a large operation on a database\\n* Queries that perform a large operation on a database\\n* Queries that perform a large operation on a database\\n* Queries that perform a large operation on a database\\n* Queries that perform a large operation on a database\\n* Queries that perform a large operation on a database\\n* Queries that perform a large operation on a database\\n* Queries that perform a large operation on a database\\n* Queries that perform a large operation"}
{"Context":"## Context\\nWe have already decided to use Ruby for our new applications (see [ADR 0007](0007-use-ruby-for-new-applications-for-manage-offenders-in-custody.md)).\\nThe team are already very familiar with Rails and it is widely used within MOJ.\\n","Decision":"## Decision\nWe will use Rails as our web framework for our new applications.\\n","Prediction":"## Decision\nWe will use Ruby for our new applications.\\nWe will use Rails as our framework.\\nWe will use Rubygems as our package manager.\\nWe will use Capistrano for deployment.\\nWe will use Capybara for automated testing.\\nWe will use RSpec for testing.\\nWe will use RSpec-Cucumber for integration tests.\\nWe will use Capistrano Action Cucumber for integration tests.\\nWe will use Capybara for UI tests.\\nWe will use Selenium for UI tests.\\nWe will use Capybara for UI tests.\\nWe will use RSpec for UI tests.\\nWe will use RSpec-Cucumber for UI tests.\\nWe will use RSpec for UI tests.\\nWe will use RSpec-Cucumber for UI tests.\\nWe will use RSpec for UI tests.\\nWe will use RSpec for UI tests.\\nWe will use RSpec for UI tests.\\nWe will use RSpec for UI tests.\\nWe will use RSpec for UI tests.\\nWe will use RSpec for UI tests.\\nWe will use RSpec for UI tests.\\nWe will use RSpec for UI tests.\\nWe will use RSpec for UI tests.\\nWe will"}
{"Context":"## Context\\nTo make this module user-friendly we need to support a number of identified use cases\\n1. MSK Cluster \/ Default Internal VPC\\n2. MSK Cluster \/ Configured Internal VPC\\n3. MSK Cluster \/ External VPC\\n4. MSK Cluster using Custom Configuration\\n5. MSK Cluster using Client Authentication\\nWe should make it simple for users to create an MSK cluster in each of these configurations.\\n","Decision":"## Decision\nTo enable this the module will utilise feature toggles. The proposed feature toggles are\\n1. Create VPC\\n2. Use Custom Configuration\\n3. Use Client Authentication\\nThe configured internal VPC will utilise the Create VPC feature flag with additional configuration settings.\\n","Prediction":"## Decision\nWe will use [Azure Resource Manager](https:\/\/docs.microsoft.com\/en-us\/azure\/azure-resource-manager\/overview) to create and configure the MSK clusters.\\nWe will use the [Azure Resource Manager API](https:\/\/docs.microsoft.com\/en-us\/azure\/azure-resource-manager\/api) to create the MSK clusters.\\nWe will use the [Azure Resource Manager API Management](https:\/\/docs.microsoft.com\/en-us\/azure\/azure-resource-manager\/management\/api-management) to manage the MSK clusters.\\nWe will use the [Azure Resource Manager CLI](https:\/\/docs.microsoft.com\/en-us\/azure\/azure-resource-manager\/cli) to create the MSK clusters.\\nWe will use the [Azure Resource Manager Deployment](https:\/\/docs.microsoft.com\/en-us\/azure\/azure-resource-manager\/deployment) to deploy the MSK clusters to Azure.\\nWe will use the [Azure Resource Manager Service Fabric](https:\/\/docs.microsoft.com\/en-us\/azure\/service-fabric\/azure-resource-manager-service-fabric) to deploy the MSK clusters to Azure.\\nWe will use the [Azure Resource Manager Template](https:\/\/docs.microsoft.com\/en-us\/azure\/azure-resource-manager\/template) to create the MSK clusters.\\nWe will use the [Azure"}
{"Context":"## Problem\\nWhen `kdbSet()` is called, plugins implementing the commit role need to\\ntrack their state to distinguish between carrying out that role and\\ncarrying out potential other roles (commit and setresolver for the\\nresolver plugin, for example). This limits the possibilities of plugin\\nreuse and the ways plugins can be combined.\\n","Decision":"## Decision\nCommitting will no longer be done by `kdbSet()`. Instead, the functionality\\nwill be implemented by its own function, `kdbCommit()`.\\n","Prediction":"## Decision\nWe will use the notion of a commit commit to distinguish between\\ndifferent commits that are part of the same commit commit.\\nFor example, a plugin that implements the `commit` role and\\n`setresolver` can be used to manage a set of commits and\\na set of plugins. In this example, `kdbSet()` can be called to set\\nthe plugins in the set resolver commit.\\n### Commit Commit\\nA commit commit is a commit that is part of a commit commit and\\nis a commit commit. A commit commit can be a commit that is\\npart of a commit commit or it can be a commit commit itself.\\nWhen a commit commit is a commit commit, it is a commit commit.\\nCommit commits are created by the commit commit itself, not by\\nplugins. For example, a plugin that implements the `commit` role\\ncan be created by a commit commit and can be used to manage\\ncommit commits.\\n### Commit Commit and Commit Commit\\nCommit commits and commit commits are the same thing. Commit commits\\nand commit commits are created by commit commit.\\nCommit commits and commit commits are the same thing. Commit commits and commit commits are\\nthe same thing. Commit commits and commit commits are the same"}
{"Context":"## Context\\nThe `p2p` package responsible for peer-to-peer networking is rather old and has a number of weaknesses, including tight coupling, leaky abstractions, lack of tests, DoS vulnerabilites, poor performance, custom protocols, and incorrect behavior. A refactor has been discussed for several years ([#2067](https:\/\/github.com\/tendermint\/tendermint\/issues\/2067)).\\nInformal Systems are also building a Rust implementation of Tendermint, [Tendermint-rs](https:\/\/github.com\/informalsystems\/tendermint-rs), and plan to implement P2P networking support over the next year. As part of this work, they have requested adopting e.g. [QUIC](https:\/\/datatracker.ietf.org\/doc\/draft-ietf-quic-transport\/) as a transport protocol instead of implementing the custom application-level `MConnection` stream multiplexing protocol that Tendermint currently uses.\\nThis ADR summarizes recent discussion with stakeholders on the scope of a P2P refactor. Specific designs and implementations will be submitted as separate ADRs.\\n","Decision":"## Decision\nThe P2P stack will be refactored and improved iteratively, in several phases:\\n* **Phase 1:** code and API refactoring, maintaining protocol compatibility as far as possible.\\n* **Phase 2:** additional transports and incremental protocol improvements.\\n* **Phase 3:** disruptive protocol changes.\\nThe scope of phases 2 and 3 is still uncertain, and will be revisited once the preceding phases have been completed as we'll have a better sense of requirements and challenges.\\n","Prediction":"## Decision\nWe will refactor the Tendermint p2p layer to use QUIC as the transport protocol for peer-to-peer communication. This includes all existing peer-to-peer implementation and will also allow us to remove the peer-to-peer layer from the Tendermint backend.\\nWe will refactor the peer-to-peer layer to use QUIC as the transport protocol for peer-to-peer communication. This includes all existing peer-to-peer implementation and will also allow us to remove the peer-to-peer layer from the Tendermint backend.\\nWe will refactor the peer-to-peer layer to use QUIC as the transport protocol for peer-to-peer communication. This includes all existing peer-to-peer implementation and will also allow us to remove the peer-to-peer layer from the Tendermint backend.\\nWe will refactor the peer-to-peer layer to use QUIC as the transport protocol for peer-to-peer communication. This includes all existing peer-to-peer implementation and will also allow us to remove the peer-to-peer layer from the Tendermint backend.\\nWe will refactor the peer-to-peer layer to use QUIC as the transport protocol for peer-to-peer communication. This includes all existing peer-to-peer implementation and will also allow us to remove the peer-to-peer layer from the Tendermint backend.\\nWe will refactor the peer-to"}
{"Context":"## Context\\n1. DDOS \u5bfc\u81f4\u4e1a\u52a1\u5168\u7ebf\u4e2d\u65ad\uff0c\u8d2d\u4e70\u9ad8\u9632 IP \uff0c\u4ecd\u7136\u9700\u8981\u91cd\u542f\u673a\u5668\u8fdb\u884c IP \u66f4\u6362\uff08nginx \u548c\u4e1a\u52a1\u7cfb\u7edf\u5728\u540c\u4e00\u53f0\u673a\u5668\u4e0a\uff09\uff1b\\n2. \u5165\u53e3\u8fc7\u591a\uff0c\u9488\u5bf9 DDOS\uff0c\u9700\u8981\u591a\u7ebf\u89e3\u51b3\uff1b\\n3. \u673a\u5668\u51fa\u95ee\u9898\uff0c\u4e1a\u52a1\u4e2d\u65ad\uff0c\u5355\u70b9\uff1b\\n4. \u65b0\u529f\u80fd\u90e8\u7f72\uff0c\u4e1a\u52a1\u4e2d\u65ad\uff0c\u6ca1\u6709\u5bf9\u591a\u5b9e\u4f8b\u8fdb\u884c\u7ba1\u7406\uff0c\u6bcf\u6b21\u90fd\u662f\u6740\u6389\u6240\u6709\uff0c\u7136\u540e\u91cd\u65b0\u542f\u52a8\u3002\\n","Decision":"## Decision\n* \u5165\u53e3\u6536\u655b\uff0c\u4fdd\u8bc1\u5bf9\u5916\u53ea\u6709\u4e00\u4e2a\u57df\u540d\uff1b\\n* \u505a\u4e1a\u52a1\u670d\u52a1\u7684\u9ad8\u53ef\u7528\uff1b\\n* \u4f7f\u7528 slb + nginx \u505a\u9ad8\u53ef\u7528\uff0c\u5e76\u4e14\u5728\u906d\u9047 DDOS \u653b\u51fb\u65f6\uff0c\u53ef\u4ee5\u901a\u8fc7\u5207\u6362 slb \u4f4e\u6210\u672c\u7684\u89e3\u51b3 DDOS\uff1b\\n* \u4f7f\u7528 supervisor \u8fdb\u884c\u4efb\u52a1\u7ba1\u7406\uff0c\u53ef\u9010\u6b65\u8fdb\u884c\u670d\u52a1\u5b9e\u4f8b\u7684\u91cd\u542f\u3002\\n","Prediction":"## Decision\n1. \u91cd\u542f\u673a\u5668\u81f3\u5c11\u4e24\u6b21\uff0c\u5bf9\u4e8e\u9ad8\u9632\uff0c\u66f4\u591a\u7684\u5b9e\u4f8b\u53ef\u4ee5\u589e\u52a0\uff1b\\n2. \u91cd\u542f\u673a\u5668\u65f6\uff0c\u5e94\u8be5\u5148\u5bf9\u5e94\u4e1a\u52a1\u4e2d\u7684\u6240\u6709\u670d\u52a1\u8fdb\u884c\u91cd\u542f\uff0c\u7136\u540e\u5bf9\u5e94\u5404\u4e2a\u670d\u52a1\u8fdb\u884c\u91cd\u542f\uff1b\\n3. \u91cd\u542f\u673a\u5668\u65f6\uff0c\u5e94\u8be5\u662f\u5bf9\u5e94\u670d\u52a1\u7684\u91cd\u542f\uff1b\\n4. \u91cd\u542f\u673a\u5668\u65f6\uff0c\u5e94\u8be5\u662f\u5bf9\u5e94\u4e1a\u52a1\u7684\u91cd\u542f\uff1b\\n5. \u91cd\u542f\u673a\u5668\u65f6\uff0c\u5e94\u8be5\u662f\u5bf9\u5e94\u4e1a\u52a1\u7684\u91cd\u542f\uff1b\\n6. \u91cd\u542f\u673a\u5668\u65f6\uff0c\u5e94\u8be5\u662f\u5bf9\u5e94\u4e1a\u52a1\u7684\u91cd\u542f\uff1b\\n7. \u91cd\u542f\u673a\u5668\u65f6\uff0c\u5e94\u8be5\u662f\u5bf9\u5e94\u4e1a\u52a1\u7684\u91cd\u542f\uff1b\\n8. \u91cd\u542f\u673a\u5668\u65f6\uff0c\u5e94\u8be5\u662f\u5bf9\u5e94\u4e1a\u52a1\u7684\u91cd\u542f\uff1b\\n9. \u91cd\u542f\u673a\u5668\u65f6\uff0c\u5e94\u8be5\u662f\u5bf9\u5e94\u4e1a\u52a1\u7684\u91cd\u542f\uff1b\\n10. \u91cd\u542f\u673a\u5668\u65f6\uff0c\u5e94\u8be5\u662f\u5bf9\u5e94\u4e1a\u52a1\u7684\u91cd\u542f\uff1b\\n11. \u91cd\u542f\u673a\u5668\u65f6\uff0c\u5e94\u8be5"}
{"Context":"## Context\\n32-bits systems were all the rage when this project was concieved back in 1999.\\n64-bits systems were not available yet.\\n16-bits and 8-bit systems were not interesting.\\n","Decision":"## Decision\nIKForth is implemented as 32-bits code with 32-bits CELL size.\\n","Prediction":"## Decision\nWe will use 32-bit systems for the project.\\nWe will use 64-bit systems for the project.\\nWe will use 128-bit systems for the project.\\nWe will use 256-bit systems for the project.\\nWe will use 512-bit systems for the project.\\nWe will use 1,024-bit systems for the project.\\nWe will use 2,048-bit systems for the project.\\nWe will use 4,037-bit systems for the project.\\nWe will use 8,048-bit systems for the project.\\nWe will use 16,096-bit systems for the project.\\nWe will use 32,048-bit systems for the project.\\nWe will use 64,096-bit systems for the project.\\nWe will use 128,192-bit systems for the project.\\nWe will use 256,384-bit systems for the project.\\nWe will use 512,768-bit systems for the project.\\nWe will use 1,024,160-bit systems for the project.\\nWe will use 2,048,384-bit systems for the project.\\nWe will use 4,097,152-bit systems for the project.\\nWe will use 8"}
{"Context":"## Context\\nRapidoid was an interesting framework to try back in 2016, but since then I have wanted to switch this project back to Spring Boot for demonstration purposes.\\n","Decision":"## Decision\nThis project will be switched to Spring Boot.\\n","Prediction":"## Decision\nI will use Spring Boot 2.4.9 for this project.\\nSpring Boot 2.4.9 is the latest version of Spring Boot supported by the Rapid development team.\\nSpring Boot 2.4.9 is the latest version of Spring Boot supported by the Rapid development team.\\nSpring Boot 2.4.9 is the latest version of Spring Boot supported by the Rapid development team.\\nSpring Boot 2.4.9 is the latest version of Spring Boot supported by the Rapid development team.\\nSpring Boot 2.4.9 is the latest version of Spring Boot supported by the Rapid development team.\\nSpring Boot 2.4.9 is the latest version of Spring Boot supported by the Rapid development team.\\nSpring Boot 2.4.9 is the latest version of Spring Boot supported by the Rapid development team.\\nSpring Boot 2.4.9 is the latest version of Spring Boot supported by the Rapid development team.\\nSpring Boot 2.4.9 is the latest version of Spring Boot supported by the Rapid development team.\\nSpring Boot 2.4.9 is the latest version of Spring Boot supported by the Rapid development team.\\nSpring Boot 2.4."}
{"Context":"## Context\\nA sprout node fails before receiving a request from the P-CSCF, and the request fails.\\n","Decision":"## Decision\nP-CSCF retries to sprout2\\n","Prediction":"## Decision\nThe sprout node will retry the request after a configurable amount of time, and if it fails again, it will abort the request.\\nIf the sprout node fails a second time, it will be removed from the cluster.\\nIf the sprout node is removed from the cluster, the request will fail.\\nIf the request fails again, the request will be retried with a retry interval of 1 second.\\nIf the request fails again, the request will be removed from the cluster.\\nIf the request fails again, the request will be retried with a retry interval of 5 seconds.\\nIf the request fails again, the request will be removed from the cluster.\\nIf the request fails again, the request will be removed from the cluster.\\nIf the request fails again, the request will be removed from the cluster.\\nIf the request fails again, the request will be removed from the cluster.\\nIf the request fails again, the request will be removed from the cluster.\\nIf the request fails again, the request will be removed from the cluster.\\nIf the request fails again, the request will be removed from the cluster.\\nIf the request fails again, the request will be removed from the cluster.\\nIf"}
{"Context":"## Context\\n`service-stac` needs to serve some static assets for the admin pages (css, images, icons, ...). Django is not appropriate to serve static files on production environment. Currently Django is served directly by `gunicorn`. As a good practice to avoid issue with slow client and to avoid Denial of Service attacks, `gunicorn` should be served behind a Reversed proxy (e.g. Apache or Nginx).\\n","Decision":"## Decision\nBecause it is to us not clear yet if a Reverse Proxy is really necessary for our Architecture (CloudFront with Kubernetes Ingress), we decided to use WhiteNoise for static assets. This middleware seems to performs well with CDN (like CloudFront) therefore we will use it to serve static files as it is very simple to uses and take care of compressing and settings corrects Headers for caching.\\n","Prediction":"## Decision\nDjango will not be served directly by `gunicorn`. Instead, we will use `nginx` to serve Django as reverse proxy. This will allow us to serve Django as static pages (css, images, icons, ...).\\nWe will use the `nginx` module to serve Django. We will also use the `ninja` module to serve static files. This will allow us to serve Django as static pages (css, images, icons, ...).\\nWe will use the `nginx` module to serve Django. This will allow us to serve Django as static pages (css, images, icons, ...).\\nWe will use the `nginx` module to serve Django. This will allow us to serve Django as static pages (css, images, icons, ...).\\nWe will use the `nginx` module to serve Django. This will allow us to serve Django as static pages (css, images, icons, ...).\\nWe will use the `nginx` module to serve Django. This will allow us to serve Django as static pages (css, images, icons, ...).\\nWe will use the `nginx` module to serve Django. This will allow us to serve Django as static pages (css, images,"}
{"Context":"## Context\\nWe need to make sure our code is written in a standard style for clarity, consistency across a project and to avoid back and forth between developers about code style.\\n","Decision":"## Decision\nWe will use [Standard.rb](https:\/\/github.com\/testdouble\/standard) and run the standard.rb rake task to lint the code as part of the test suite.\\n","Prediction":"## Decision\nWe will use the [Microsoft Style Guide](https:\/\/docs.microsoft.com\/en-us\/learn\/standard\/style-guide) as the standard for style within our codebase.\\nWe will use the [recommended style guide](https:\/\/docs.microsoft.com\/en-us\/cpp\/standard\/cpp-styles) as the standard for style within our codebase.\\nWe will use the [recommended style guide](https:\/\/docs.microsoft.com\/en-us\/cpp\/standard\/cpp-styles) as the standard for style within our codebase.\\nWe will use the [recommended style guide](https:\/\/docs.microsoft.com\/en-us\/cpp\/standard\/cpp-styles) as the standard for style within our codebase.\\nWe will use the [recommended style guide](https:\/\/docs.microsoft.com\/en-us\/cpp\/standard\/cpp-styles) as the standard for style within our codebase.\\nWe will use the [recommended style guide](https:\/\/docs.microsoft.com\/en-us\/cpp\/standard\/cpp-styles) as the standard for style within our codebase.\\nWe will use the [recommended style guide](https:\/\/docs.microsoft.com\/en-us\/cpp\/standard\/cpp-styles) as the standard for style within our codebase.\\nWe will use the [recommended style guide](https:\/\/docs.microsoft.com\/en"}
{"Context":"## Context\\nWe've avoided using React hooks for some time in order to ensure un-desired complexity is avoided in the code base. But as hooks are now standard fare in React applications, it makes sense to review our usage of them in DCR.\\n","Decision":"## Decision\n- Prefer non-stateful components if possible\\n- Prefer React's official hooks to custom hooks\\n- Avoid abstractions that could lead to hooks within hooks within hooks.\\n- Prefer hooks to classes with component lifecycle methods\\n- Try to build hooks that are generic and reusable\\n","Prediction":"## Decision\nWe will use hooks, but only for the most essential use cases.\\nWe will only use hooks to modify state.\\nWe will not use hooks for state management.\\nWe will not use hooks to perform actions on state.\\nWe will not use hooks to perform validation.\\nWe will not use hooks to render components.\\nWe will not use hooks to render views.\\nWe will not use hooks to render templates.\\nWe will not use hooks to render components in a specific way.\\nWe will not use hooks to render views in a specific way.\\nWe will use hooks for state management where it makes sense, e.g. to persist data between routes.\\nWe will use hooks for actions where it makes sense, e.g. to perform actions on state.\\nWe will use hooks for validation where it makes sense, e.g. to prevent invalid data being sent.\\nWe will use hooks for rendering components where it makes sense, e.g. to render a component in the correct way.\\nWe will use hooks for rendering views where it makes sense, e.g. to render a component in the correct way.\\nWe will use hooks for rendering templates where it makes sense, e.g. to render a component in the correct way"}
{"Context":"## Context\\nA particular database technology needs to be chosen for the storage of data.\\nHistorically Microsoft's SQL Server has been the default choice for ASP.NET applications. SQL Server could only be run on a machine running Windows until the release of SQL Server 2017.\\nPostgreSQL is a popular choice for use with other web frameworks (i.e. Rails) and is widely used on a range of platforms, including Linux, MacOS and Windows.\\nPostgreSQL is open source and free to use for commercial use. SQL Server has a free version for development purposes but require the purchase of a license for commercial use.\\n","Decision":"## Decision\nWe will use PostgreSQL for the storage of data.\\n","Prediction":"## Decision\nPostgreSQL will be used for the database storage for the project.\\nPostgreSQL will be used as the database for the CI\/CD pipeline.\\nPostgreSQL will be used as the database for the staging and production environments.\\nPostgreSQL will be used as the database for the admin and user portal.\\nPostgreSQL will be used as the database for the Azure AD authentication provider.\\nPostgreSQL will be used as the database for the Azure DevOps build and release pipelines.\\nPostgreSQL will be used as the database for the Azure Kubernetes Service (AKS) cluster.\\nPostgreSQL will be used as the database for the Kubernetes cluster.\\nPostgreSQL will be used as the database for the Azure SQL Database.\\nPostgreSQL will be used as the database for the Azure Cosmos DB.\\nPostgreSQL will be used as the database for the Azure Service Fabric cluster.\\nPostgreSQL will be used as the database for the Azure IoT Hub.\\nPostgreSQL will be used as the database for the Azure Service Bus Queue.\\nPostgreSQL will be used as the database for the Azure Service Bus Topic.\\nPostgreSQL will be used as the database for the Azure Service Fabric Reliability group.\\nPostgreSQL will be used as the database for the Azure"}
{"Context":"## Context\\nDevelopers need to be able to run application on their local machines in isolated environment.\\nDockerized DB provides required service without the need to setup\\nfull-blown DB server on each local machine.\\n","Decision":"## Decision\nWe will provide docker compose setup for Postgre SQL server.\\n","Prediction":"## Decision\nWe will use Dockerized DB provided by [Docker](https:\/\/www.docker.com\/).\\nDB will be run inside container.\\nDockerized DB will be used as a local database.\\nDockerized DB will be used as a local database.\\nDockerized DB will be used as a local database.\\nDockerized DB will be used as a local database.\\nDockerized DB will be used as a local database.\\nDockerized DB will be used as a local database.\\nDockerized DB will be used as a local database.\\nDockerized DB will be used as a local database.\\nDockerized DB will be used as a local database.\\nDockerized DB will be used as a local database.\\nDockerized DB will be used as a local database.\\nDockerized DB will be used as a local database.\\nDockerized DB will be used as a local database.\\nDockerized DB will be used as a local database.\\nDockerized DB will be used as a local database.\\nDockerized DB will be used as a local database.\\nDockerized DB will be used as a local database.\\nDockerized DB"}
{"Context":"## Context\\nBefore we created this template repo, we had [WPC](https:\/\/github.com\/dxw\/wpc), a repo which served two main purposes:\\n* Generating the files necessary within a Whippet-based WordPress app for local development in Docker, using the \"scripts to rule them all\" model. This meant e.g. generating the `docker-compose.yml` file, and the relevant files for the script folder (e.g. `script\/setup`, `script\/server`, etc.)\\n* Acting as a source for the `thedxw\/wpc-wordpress` docker image we use for running WordPress in docker in local development environments\\nNow we have this template repo, which also contains all files necessary for running a docker-based development environment (but not the `wpc-wordpress` docker image itself). We shouldn't duplicate those files in two separate repos.\\nWe no longer have the need to generate those files within existing projects, as all existing Whippet apps have already been converted for docker-based development. New projects (or projects we  inherit, e.g. old Helpful repos) will use this template repo to provide that infrastructure.\\n","Decision":"## Decision\nUse this repository as the primary source for the files that were previously generated by WPC.\\nRemove those files from the [WPC repo](https:\/\/github.com\/dxw\/wpc), so that repo is only responsible for the `wpc-wordpress` docker image, and nothing else.\\n","Prediction":"## Decision\nWe will use this template repo to generate the files necessary for running a docker-based development environment (but not the `wpc-wordpress` docker image itself).\\nWe will use this template repo to provide the infrastructure needed to run a docker-based development environment.\\nWe will move the existing `wpc-wordpress` repository to this template repo (to make it easier to maintain and easier for new developers to adopt).\\nWe will remove the `wpc-wordpress` repository from the `dxw\/whippet` repository.\\nWe will move the `dxw\/whippet` repository to a new repository, `dxw\/wordpress-template`.\\nWe will remove the `dxw\/wordpress-template` repository from the `dxw\/whippet` repository.\\nWe will move the `dxw\/whippet` repository to a new repository, `dxw\/wordpress-template`.\\nWe will remove the `dxw\/wordpress-template` repository from the `dxw\/whippet` repository.\\nWe will move the `dxw\/wordpress-template` repository to a new repository, `dxw\/wordpress-template`.\\nWe will remove the `dxw\/wordpress-template` repository from the `dxw\/wh"}
{"Context":"## Context\\nThe case for a physical card ID: https:\/\/docs.google.com\/document\/d\/1TFsXBtWU8SA3MnmJUcGK35ubcv-HvsM_H2zfRe5tDZo\/edit#\\nSummarizing the above, there is a need for a patient card ID that quickly, and uniquely identifies a patient. The card can be used as a vehicle for BP history in the clinic, and serve as a reminder about the next visit date for the patient.\\nWe want to allow a patient to cross simple networks, say, from IHMI to Path.\\n### Card IDs, not Patient IDs\\nPatients often lose their cards, and the nurses re-issue them. Since we ideally want the cards to have pre-printed QRs, these will have to be different QRs that a single patient can be associated with. So, a patient can have multiple cards.\\nSome places already have patient IDs. Like kerala, and maharashtra (dell system). These IDs can also be considered card IDs, that we can use to lookup a patient in Simple.\\n### Characteristics of the Card ID\\n1. The number shouldn't be guessable\\n- Nurses shouldn't be able to lookup PHI randomly. The number shouldn't have any implicity meaning associated with it, eg. date of registration, date of birth, colony, village etc.\\n2. Clinics should be able to print their own IDs\\n- In the future, we shouldn't be on the hook for printing these IDs.\\n3. We can associate multiple cards with a patient\\n- Patients will lose cards, and we shouldn't need to print duplicate cards. Like hotel cards, we should be able to use another new card and associate it with the patient. The old card would also remain usable.\\n4. Scanning works cross simple networks\\n- Scanning the card ID, when patient presents it, should allow clinics to gain access to PHI, even if they aren't in the same network. Example: a patient should be able to move from a private clinic at PATH, to a govt facility in IHMI.\\n5. Have a fallback for when scanning doesn't work\\n- The card ID should be usable even if scanning doesn't work, and the camera is not operational. Ideally, this fallback should be a 7 digit number, that is easy for a nurse to type on a numeric keypad on a phone.\\n","Decision":"## Decision\nWe will use [UUID](https:\/\/en.wikipedia.org\/wiki\/Universally_unique_identifier)s as simple card IDs.\\nUUIDs (version 4) are random, unique, and can be generated algorithmically. These are used as IDs in distributed systems for the same reasons.\\nThey are not sequential, are not guessable, and aren't dependent on a central system that guarrantees uniqueness. Clinics can generate their own UUIDs for their cards, in an offline system.\\nThe density of the QRs that resolve to UUIDs is quite good, and scanning them doesn't have issues thus far.\\n### Fallback ID, numeric representation\\nWe'll use a 6 or 7 digit ID, for when the QR scanning doesn't work. This number will be derived from the corresponding UUID by picking the digits that appear in the UUID, in sequence. For example, the UUID `17be9760-bd4c-4a31-9c21-c28b66919f52` will have the `179-7604` as the numeric ID.\\n#### Collisions\\nUnlike the UUID, which is unique, the numeric representation isn't unique. 2 UUIDs can have the same numeric representation. When searching by the digits, the app will show matches in the search results, and the nurse can pick the patient from the results. Since the patients in the search results aren't going to be similar (different names, etc), and there won't be many such collisions, this shouldn't be hard.\\nThe space for this collision is roughly 16^7, or ~270 million (16^6 is ~16 million), which is comfortable for a large clinic in a metropolitan city.\\n#### Associating by the numeric representation\\nThe following are the different ways in which a card ID can be associated with a patient.\\n| # | notes                          | patient-id | simple-card-id | digit-id |\\n|---|--------------------------------|------------|----------------|----------|\\n| 1 | associated by scanning QR      | uuid-1     | card-id-1      | derived  |\\n| 2 | issued new card by scanning QR | uuid-1     | card-id-2      | derived  |\\n| 3 | associated by numeric ID       | uuid-1     | ?              | entered  |\\nIn the case that a card is associated using a numeric ID, the simple-card-id (UUID) isn't known, and the association is left dangling. If in the next visit the card is scanned, the app would show search results, and the nurse can confirm the association.\\n### Composition of the QR\\nWe will have a simple json as the QR content:\\n```\\n{version: 'v1', id: '17be9760-bd4c-4a31-9c21-c28b66919f52'}\\n```\\nHaving the version in the json lets us iterate over the content of the QR, while being backward compatible.\\n### Example simple card\\n![simple-card](resources\/simple-card-example.png)\\n### Simple card ID states\\nThe state machine for a scanned card from the experiments app\\n![simple-state-machine](resources\/simple-card-states.png)\\n","Prediction":"## Decision\n### Use a random card ID\\nWe will use a random card ID when a patient presents the card to the clinician. This will allow the clinician to associate PHI with the card, and use it to lookup the patient in Simple.\\n### Use a pre-printed QR\\nWe will use a pre-printed QR when a patient presents the card to the clinician. This will allow the clinician to associate PHI with the card, and use it to lookup the patient in Simple.\\n### Use a fallback for when scanning doesn't work\\n- If the camera is not operational, the card ID will be 7 digits long, and we will use the last number as the fallback. So, if a patient presents the card, and the camera is not operational, the card will be 6 digits long.\\n- If the camera is operational, the card ID will be 7 digits long, and the last number will be truncated to 6 digits. So, if a patient presents the card, and the camera is operational, the card will be 7 digits long.\\n### Use a private card ID\\nWe will use a private card ID when a patient presents the card to the clinician. This will allow the clinician to associate"}
{"Context":"## Context\\nThere are a variety of whales with many attributes, thus it may be necessary to sort these whales into\\nvarious different groupings. In order to sort these objects, a _function object_ must be implemented using\\none of three possible designs: nested classes, anonymous classes, and lambda expressions.\\n","Decision":"## Decision\nImplement `Comparable<Whale>`, create `compareTo` default method for field `species` (Species) and a nested comparator\\nclass for field `whaleId` (long) in Whale.\\n","Prediction":"## Decision\nAnonymous classes will be used.\\nA nested class will be used when the attributes are not related to a single whale, but instead to a\\nsub-set of attributes.\\nA lambda expression will be used when the attributes are related to a single whale and are\\nnot nested.\\n[![Function Design](..\/assets\/design\/front-end\/design_1.png) ](..\/assets\/design\/front-end\/design_1.png)\\n[![Lambda Design](..\/assets\/design\/front-end\/design_2.png) ](..\/assets\/design\/front-end\/design_2.png)\\n[![Anonymous Design](..\/assets\/design\/front-end\/design_3.png) ](..\/assets\/design\/front-end\/design_3.png)\\n### Nested Classes\\nAn anonymous class can be used to encapsulate a single attribute of a whale, but it will\\nnot be able to nest. An anonymous class can be used to encapsulate a single attribute of a\\nsub-set of attributes, but it will be able to nest.\\n### Anonymous Classes\\nAn anonymous class will be used to encapsulate a single attribute of a whale and will\\nbe able to nest. An anonymous class will not be able to encapsulate a single attribute\\nof a sub-set of attributes.\\n### Lambda Expressions"}
{"Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"## Decision\nThis will allow us to get a wider range of opinions on our decisions which we could not have got before.\\n### Interrogate your data decisions\\nN\/A\\n### Decision\\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n### Consequences\\nSee Michael Nygard's article, linked above.\\nADRs will be public for visibility and collaboration.\\nPull requests and Github issues can be used to drive conversations.\\n","Prediction":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\nWe will use [Bryant Nygard's Architecture Decision Records Template](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\nWe will use [AIA Document A-130](http:\/\/www.aianet.org\/publications\/aianet-documents\/AIA-Document-A130-2015-architecture-decision-records.pdf) to manage ADRs.\\nWe will use [APM tooling](https:\/\/github.com\/ProjectManagement\/Project-Management-Tooling) to manage ADRs.\\nWe will use [JIRA](https:\/\/www.atlassian.com\/software\/jira) to manage ADRs.\\nWe will use [Gerrit](https:\/\/gerrit.org) to manage ADRs.\\nWe will use [Pivotal Tracker](https:\/\/www.pivotaltracker.com) to manage ADRs.\\nWe will use [GitHub Issues](https:\/\/github.com\/) to manage ADRs.\\nWe will use [Google Sheets"}
{"Context":"## Context\\nAs we are re-implementing our search in ElasticSearch v7,\\nwe must re-implement Ask a Question (AAQ) search.\\nThere is one primary use-case for storing AAQ documents in ES which Search v1 supports,\\nwhich we must continue to be able to do in the redesigned Search:\\nsearching for an AAQ thread as a unit.\\nThere are other secondary use-cases which we may want to support when storing AAQ documents in ES.\\nA non-exhaustive list of these are:\\n-   Searching within AAQ threads\\n-   Searching within questions and their solutions\\n-   Aggregating answers to create contribution data\\nWe also want search to be _fast_,\\nso should model our data to avoid nested fields and parent-child relationships,\\nand use de-normalization wherever possible:\\nhttps:\/\/www.elastic.co\/guide\/en\/elasticsearch\/reference\/7.9\/tune-for-search-speed.html#_document_modeling\\n","Decision":"## Decision\nWe will model our data in ES based on what makes most sense for our expected use-cases,\\nand what will make those fast and efficient,\\nrather than feeling like we must have a 1:1 copy of our data structure in our database.\\nIn this vein, we will use a structure of two document \"types\" within one index,\\n`QuestionDocument` and `AnswerDocument`,\\nwhere a `QuestionDocument` will exist for each `Question` which exists in the database,\\nand an `AnswerDocument` will exist for each `Answer` which exists in the database.\\n`AnswerDocument` will be a subclass of `QuestionDocument` so will inherit all of its fields,\\nand we will set the value of those fields to the value of the `Question` associated with its `Answer`.\\nFor instance, if in database:\\n```\\nanswer.created => 2020-10-27\\nanswer.question.created => 2020-11-01\\n```\\nin elastic:\\n```\\nanswer_document.created => 2020-10-27\\nanswer_document.question_created => 2020-11-01\\n```\\n`QuestionDocument` will also have an `answer_content` field,\\nwhich contains the content of all a Question's Answers.\\nWe will set this to null in the `AnswerDocument`.\\n","Prediction":"## Decision\nWe will use the `search` API in Elasticsearch\\nfor searching Ask a Question documents.\\nWe will create a new field type in Elasticsearch which will allow us to search for AAQ documents.\\nThis will be stored as a `text` field in the document.\\nWe will also create a new field type in Elasticsearch which will allow us to search for solutions to AAQ questions.\\nThis will be stored as a `text` field in the document.\\nWe will implement a custom analyzer for the `text` fields, which will allow us to perform complex search queries.\\nThis will be stored as a `text` field in the document.\\nWe will also implement a custom analyzer for the `solutions` field, which will allow us to perform complex search queries.\\nThis will be stored as a `text` field in the document.\\nThis will enable us to support complex queries such as:\\n-   Searching for answers to a question\\n-   Searching for a specific answer to a question\\n-   Searching for a specific question to a question\\n-   Searching for a specific answer to a question in a specific question\\n-   Searching for a specific question in a specific question\\n-   Searching for a specific answer to a"}
{"Context":"## Context\\nThe call to the prison-api to get custody data requires a nomsNumber as the identifier. This is a new identifier for the court-case-service and two options were identified as ways we could get this:\\n1. Incorporate nomsNumber into the case model, retrieve it as part of the matching process and have prepare-a-case pass it in the request to the custody endpoint\\n2. Use CRN to retrieve this data on demand from the community-api\\n","Decision":"## Decision\nOption 2 chosen - Use CRN to retrieve this data on demand from the community-api.\\n","Prediction":"## Decision\nOption 2 was preferred as it allows us to keep the nomsNumber as a unique identifier internally in the system and also provide an API for the community to use to retrieve the data.\\nThis decision has been made based on the following:\\n1. The community-api has limited functionality at the moment and we would like to increase its functionality to be used by the community in the future\\n2. We would like to use the repository pattern to support the use of this identifier in the future\\n3. The community-api has a relatively low barrier of entry to use, so we would like to use it as soon as possible\\n4. The community-api has limited functionality at the moment and we would like to increase its functionality to be used by the community in the future\\n5. The community-api has a relatively low barrier of entry to use, so we would like to use it as soon as possible\\n6. The community-api has a relatively low barrier of entry to use, so we would like to use it as soon as possible\\n7. The community-api has a relatively low barrier of entry to use, so we would like to use it as soon as possible\\n8. The community-api has a relatively low barrier of entry to use, so"}
{"Context":"## Context\\nWhen a transcription is approved, a set of flat files containing the transcription data will be saved to Azure. Users will have the option to download a zip file containing their requested subject, group, workflow, or project. Depending on the speed at which we are able to zip the necessary files, we will either trigger a direct download, or provide a link to the location of the zip file to the user.\\nThe goal is to investigate Azure\u2019s storage options (specifically Blob Storage and File Services) and decide which tool is best suited for our needs.\\n### Factors to consider:\\n* How easy is it to share a file to the end user? What is the process for this?\\n* Ease of use, how complicated is it to set up, maintain, edit\\n* access permission features\\n* Speed of accessing and iterating through files (e.g. getting all files in a given directory)\\n### Terminology:\\n**Blob:** acronym for \u201cBinary Large Object\u201d\\n**Container:** synonym for \u201dS3 Bucket\u201d\\n**Shared Access Signature:** similar functionality as \u201cS3 Presigned URLs\u201d\\n","Decision":"## Decision\nWe don't appear to have any need for most of the additional functionality that comes with File Service, which makes me reluctant to want to use it. In addition, the number of articles and resources available on communicating with Blob Storage to set up file zipping is much greater than what's available for File Service. My initial understanding of Blob Storage led me to believe that permissions could only be set at the container level, but this turned out to be wrong. With the ability to set blob-specific permissions, we will be able to use a single container to store the transcription-specific files, and the user-requested zip files.\\nUltimately, my choice is to go with Blob Storage: the more basic, simple storage tool that gives us what we need and nothing more. That being said, I'd still like to keep the option of using Azure File Service on the table, in case it turns out that we *would* benefit from the additional functionality that it offers.\\nAs for what type of blob we will use, my choice would be to store each data file in its own block blob. If we were to choose to store multiple files within a single blob (and have each file be associated with a block ID on that blob), we would lose the ability to name each individual file. Hypothetically, it would be possible to create a database table with columns \u201cblock ID\u201d and \u201cname\u201d, to emulate a naming functionality, but this seems far more complicated than its worth. In addition, the [azure-storage-blob](https:\/\/github.com\/azure\/azure-storage-ruby\/tree\/master\/blob) gem gives us a simple interface for working with block blobs and saves us the trouble of having to write HTTP requests ourselves.\\nFinal questions:\\n1. Q: Blob Storage doesn't have any concrete hierarchy beyond Storage Account\/Blob Container - within a container, directories are virtual, demarcated by prefixes in the file name. Will this end up being problematic for us? Will it complicate file retrieval?\\nA: Retrieving files from a file system with virtual directories shouldn't be any different than retrieving files from a normal file system. As long as blob prefixes are constructed in a way that reflects the organizational system used within the application\/database, there should be no trouble. File retrieval may be helped by append blobs - final decision on blob type is still TBD.\\n2. Q: Would there be any benefit to caching files on on-premises file servers? If this sounds like something we'd like to employ, it would be worth reconsidering Azure File Service.\\nA: This doesn't appear to be something we will need.\\n### Links and Articles:\\n1. [Microsoft: Deciding when to use Azure Blobs, Azure Files, or Azure Disks](https:\/\/docs.microsoft.com\/en-us\/azure\/storage\/common\/storage-decide-blobs-files-disks)\\n2. [Azure Files FAQ](https:\/\/docs.microsoft.com\/en-us\/azure\/storage\/files\/storage-files-faq) (see \u2018Why would I use an Azure file share versus Azure Blob Storage for my data?\u2019)\\n3. [Stack Overflow: Blob Storage vs File Service](https:\/\/stackoverflow.com\/questions\/24880430\/azure-blob-storage-vs-file-service)\\n4. [Microsoft: Introducing Azure File Service](https:\/\/blogs.msdn.microsoft.com\/windowsazurestorage\/2014\/05\/12\/introducing-microsoft-azure-file-service\/) (scroll to When to use Azure Files vs Azure Blobs vs Azure Disks)\\n5. [Microsoft: Azure Storage scalability and performance targets for storage accounts](https:\/\/docs.microsoft.com\/en-us\/azure\/storage\/common\/storage-scalability-targets)\\n6. [Azure Blob Overview](https:\/\/docs.microsoft.com\/en-us\/azure\/storage\/blobs\/storage-blobs-overview)\\n7. [Azure Blob Introduction](https:\/\/docs.microsoft.com\/en-us\/azure\/storage\/blobs\/storage-blobs-introduction)\\n8. [How to mount Blob storage as a file system with blobfuse](https:\/\/docs.microsoft.com\/en-us\/azure\/storage\/blobs\/storage-how-to-mount-container-linux)\\n9. [Block blobs, append blobs, and page blobs](https:\/\/docs.microsoft.com\/en-us\/rest\/api\/storageservices\/understanding-block-blobs--append-blobs--and-page-blobs)\\n10. [Azure Blob Storage gem](https:\/\/github.com\/azure\/azure-storage-ruby\/tree\/master\/blob)\\n","Prediction":"## Decision\nWe have decided to use Azure File Services for our file storage.\\nWe will use Azure Storage Blob to store flat files containing our subject, group, workflow, and project data.\\nWe will use Azure File Storage to store files that have been approved for a given subject, group, workflow, or project.\\nWe will use Azure File Storage to store files that have not been approved for a given subject, group, workflow, or project.\\nWe will use Azure File Storage to store files that are currently in a pending state.\\nWe will use Azure File Storage to store files that are currently in a deleted state.\\nWe will use Azure File Storage to store files that are currently in a versioned state.\\nWe will use Azure File Storage to store files that are currently in a versioned state.\\nWe will use Azure File Storage to store files that are currently in a versioned state.\\nWe will use Azure File Storage to store files that are currently in a versioned state.\\nWe will use Azure File Storage to store files that are currently in a versioned state.\\nWe will use Azure File Storage to store files that are currently in a versioned state.\\nWe will use Azure File Storage to store files that are currently in"}
{"Context":"## Context\\nNow that the repo subcommand is able to add, remove, update and list\\nrepositories... maybe the barebones `list` command should be removed in favor of\\n`repo list`. That may make things simpler.\\nAlso the `--with-manifest` command is less useful now that the repository\\nmanagement is easier.\\n","Decision":"## Decision\n- Remove the explicit `list` command\\n- Remove the `--with-manifest` argument\\n- Simplify listing the commands when none is provided (removing versions)\\n- List commands when listing repositories\\n","Prediction":"## Decision\nRemove the barebones `list` command and instead use the `repo` command\\nin the `repo` subcommand.\\nIt's also worth noting that we're going to be using `--with-manifest` as\\nan argument to the `repo` command, so the command will look like:\\n```\\nrepo list --with-manifest repo_repo --with-manifest=repo_repo.manifest\\n```\\nwhich is the same as:\\n```\\nrepo list repo_repo --my-manifest repo_repo.manifest\\n```\\nwhich is the same as:\\n```\\nrepo list repo_repo repo_repo.manifest\\n```\\nwhich is the same as:\\n```\\nrepo list repo_repo repo_repo.manifest --my-manifest repo_repo.manifest\\n```\\nwhich is the same as:\\n```\\nrepo list repo_repo repo_repo.manifest repo_repo.manifest --my-manifest repo_repo.manifest\\n```\\nwhich is the same as:\\n```\\nrepo list repo_repo repo_repo.manifest repo_repo.manifest --my-manifest repo_repo.manifest repo_repo.manifest\\n```\\nwhich is the same as:\\n```\\nrepo list repo_repo repo_repo"}
{"Context":"## Context\\nFor this project, we would like to package up the API endpoints and logic into a small Flask app in a single lambda function.\\nWe found whilst working on the Documents integration that managing multiple lambda functions quickly became quite hard work,\\nespecially as in that project there was a lot of shared code about the place. In hindsight, we should have refactored the\\nshared code into separate lambda functions, but we never got to it due to time constraints. Also this would just give us\\nmore lambda functions to maintain.\\n#### Why Flask (or any other WSGI app) in a Lambda is a good idea\\n* We can re-use most of the infra setup from the last project so we can get started fast\\n* Easy to run and develop locally, just run like a normal Flask app\\n* Easy to test as it's a normal Flask app\\n* I have written a lot of Flask apps(!) and so have a LOT of other people, so documentation is plentiful and well established\\n* Is it an anti-pattern? AWS have a library called Chalice which does a very similar thing to how we propose to use Flask\\n(but with more deployment stuff in that we don't need), so if AWS have an official package that does it, can it be an\\nanti-pattern?\\n* Maintaining a single lambda function is much easier than maintaining many\\n* This project is very small and has very well defined boundaries. None of its data or other artifacts will be accessed\\nby anything other than its own API, meaning it's completely independent of any other service (excluding AWS security) so\\nit's a good candidate for experimenting with this, as nothing will need to be reused outside of this app\\n#### Why Flask (or any other WSGI app) in a Lambda is a bad idea\\n* A whole app in a FaaS seems like an anti-pattern (can't find any evidence to support\/contradict this)\\n* Potentially longer cold-start as it has to initialise the whole app not just a single function\\n* [Flask-Lambda](https:\/\/github.com\/sivel\/flask-lambda) is a small package with only 1 contributor, though it is a pretty\\nsimple script that just maps some request variables depending on the source of the request\\n","Decision":"## Decision\n* Single lambda function containing small Flask app that provides all endpoints\\n* Flask-Lambda to help easily switch between local dev and AWS\\n","Prediction":"## Decision\nWe will use Flask-Lambda to run the API endpoints in a FaaS as a part of our Lambda functions. The Flask-Lambda\\npackage is fairly simple to install and configure and thus should be fairly easy to maintain.\\nFlask-Lambda is also well documented and has a lot of examples in it, so we should be able to get started fairly quickly.\\nWe will not be using [Flask-Lite](https:\/\/github.com\/ian\/Flask-Lite) as it appears to be a wrapper around Flask-Lambda\\nand we don't need a wrapper around Flask-Lambda.\\nFlask-Lambda is a little more complicated to setup than Flask-Lambda, but we can use it as a learning exercise to\\nget a handle on the basics of running a Flask app in a Lambda function.\\nIn the future, we can then refactor the Flask-Lambda code into a separate Flask app.\\nFlask-Lambda is available as a PyPI package, [Flask-Lambda](https:\/\/pypi.org\/project\/Flask-Lambda)\\nand can be installed using pip:\\n```{code}\\npip install flask-lambda\\n\\end{code}\\nWe have not yet decided what version of Flask-L"}
{"Context":"## Context\\nMeadow needs a mechanism to track user privileges.\\n","Decision":"## Decision\nUse existing Library Active Directory group membership to map users to\\nsets of privileges and access controls.\\n","Prediction":"## Decision\nWe will use the [SecureRandom](https:\/\/docs.oracle.com\/javase\/8\/docs\/api\/java\/security\/srand.html) class to generate the user's credentials.\\nWe will use the [SecureRandomGenerator](https:\/\/docs.oracle.com\/javase\/8\/docs\/api\/java\/security\/srandgenerator.html) class to generate a random number to be used in the password.\\nWe will use the [SecureRandomGeneratorFactory](https:\/\/docs.oracle.com\/javase\/8\/docs\/api\/java\/security\/srandgeneratorfactory.html) factory to create the seed for the random number generator.\\nWe will use the [SecureRandomGeneratorFactory](https:\/\/docs.oracle.com\/javase\/8\/docs\/api\/java\/security\/srandgeneratorfactory.html) to create a random number generator to be used in the password.\\nWe will use the [SecureRandomGenerator](https:\/\/docs.oracle.com\/javase\/8\/docs\/api\/java\/security\/srand.html) to create a random number generator to be used in the password.\\nWe will use the [SecureRandomGeneratorFactory](https:\/\/docs.oracle.com\/javase\/8\/docs\/api\/java\/security\/srandgeneratorfactory.html) to create a random number generator to be used in the password.\\nWe will use the [SecureRandomGeneratorFactory](https"}
{"Context":"## Context\\nThe release pipeline of Wikibase will produce a tarball release package containing a tested version of the Wikibase extension. The package will normally include the necessary files to run the software and sometimes documentation of changes to the software and how to use them.\\nIn order to get a overview on the practices by the open-source php community a brief survey was done on the releases of some popular projects.\\n| Name | Release notes | Historic release notes | Installation instructions | Upgrade instructions | Hidden files | Vendor folder |\\n| --- | --- | --- | --- | --- | --- | --- |\\n| mediawiki | yes | yes | yes | yes | no | yes (9) |\\n| laravel | no (1) | no | no | no | some (6) | no |\\n| nextcloud | no (2) | no | no | no | some (7) | yes |\\n| joomla | no (3) | no (3) | yes (3) | no (4) | no | yes |\\n| wordpress | no (5) | no | yes (8) | no | no | - |\\n---\\n1. Online https:\/\/laravel.com\/docs\/5.8\/releases\\n2. Online https:\/\/nextcloud.com\/changelog\/\\n3. Installation\/INSTALL & simple readme with links to https:\/\/docs.joomla.org\/Special:MyLanguage\/Joomla_3.9_version_history, https:\/\/downloads.joomla.org\/technical-requirements\\n4. Specific upgrade release packages\\n5. Online https:\/\/wordpress.org\/news\/2020\/11\/wordpress-5-6-beta-4\/\\n6. gitignore, editorconfig etc.\\n7. Necessary htaccess files\\n8. Readme.html with installation & upgrade instructions and links to documentation for further reading.\\n9. https:\/\/releases.wikimedia.org\/mediawiki\/1.35\/mediawiki-1.35.0.tar.gz\\n---\\nMediaWiki sticks out from the crowd by including most of their release specific documentation in each of the tarball releases while all other projects in the survey do not. At first glance this feels like a undesirable pattern to adopt as it's uncommon among other projects. In reality this points out a tested and proven methodology for documenting changes and shipping release notes by the mediawiki community. Release notes are added to the release branch and refined throughout the release process and then used as a template for the documentation on mediawiki.org.\\nThe `vendor` folder comes populated in some of the release packages for the different projects and this is also the behavior of [Extension Distributor]. The reasons given for this behavior dates back to a [request from 2014] to include the composer dependencies in tarballs as composer could for various reasons not be available on the system the software was supposed to be installed on.\\n","Decision":"## Decision\nSeeing that there is an already defined process for release notes by the mediawiki community it feels natural to use existing tools and adopting this process.\\nThe `vendor` folder will be included in the tarball as this aligns with the mediawiki tarballs and the behavior of [Extension Distributor].\\nThe tarball package _WILL_ include apart from the source-code the following additions.\\n- `vendor` folder populated by composer\\n- Release notes (RELEASE-NOTES-N.NN) (example RELEASE-NOTES-1.36)\\n- Historic changes (HISTORY) (Previous release notes in one file)\\n- git submodules\\nThe tarball will _NOT_ include.\\n- `node_modules`\\n- `build` folder\\n- hidden files\/folders in the root folder\\n- `.gitignore`\\n- `.gitreview`\\n- `.git` folder\\n- `.github` folder\\n- `.vscode` folder\\n- `.phan` folder\\n- etc.\\n","Prediction":"## Decision\nWe will include the `vendor` folder in the release tarball for mediawiki. We will also include a README file in the tarball that explains the contents of the release tarball and provides links to the documentation as described above.\\nThis will be a part of the `mediawiki-1.35\/CHANGELOG.md` file. This README file will be included in the tarball release.\\nIf the `vendor` folder is not included in the tarball release and the `vendor` folder is not present in the `composer.json` file then the tarball will be skipped.\\nIf the `vendor` folder is present in the tarball release and the `vendor` folder is not present in the `composer.json` file then the tarball will be skipped.\\nIf the `vendor` folder is present in the tarball release and the `vendor` folder is present in the `composer.json` file then the tarball will be skipped.\\nIf the `vendor` folder is present in the tarball release and the `vendor` folder is present in the `composer.json` file then the tarball will be skipped.\\nIf the `vendor` folder is present in the tarball release and the `vendor`"}
{"Context":"## Context\\nWe faced a problem while injecting new parameters to presenters' initializer. The list of init params are getting much longer.\\n","Decision":"## Decision\nEach module has it's own `createModule` or `setupModule` static method in its router. If we have 3 or more properties to be injected to any of these methods, we must create `XYZArguments` structure that contains those properties and pass the argument object to router.\\nWe must also inject the same arguments variable to the presenters' initializer and use the variables inside through the arguments object, and not copy each of variables inside arguments to the presenter one by one.\\n**Note**: Argument object must only include the values to be injected from outer module to initialize the target module (e.g. any kind of `Config\/AB flags`, `Managers` and `Helpers` must not be stored inside arguments, and these variables must be injected to the presenters' initializer directly)\\n","Prediction":"## Decision\nWe decided to replace the current approach with a new one. We will use new `init!()` and `init!(_:)` methods to pass new parameters to the presenter. The new init methods will be called from the `presenters` initializer.\\nIn order to make it easier to test the new approach, we will use new `init!()` and `init!(_:)` methods for the `presenters` initializer.\\nWhen the new approach becomes available, we will remove the current `init!()` and `init!(_:)` methods from the `presenters` initializer.\\nThis will allow us to create a new initializer with `init!()` and `init!(_:)` methods.\\n### New initializer\\nThe new initializer will be called from the `presenters` initializer. It will take the new init parameters and create a new presenter with them.\\n### Where to put the new initializer\\nThe new initializer will be placed in the `presenters` initializer. It will be called from the `presenters` initializer.\\n### How to test the new initializer\\n### How to create a new presenter\\n### How to inject new parameters to a presenter\\n### How to remove old parameters\\n### How to create a"}
{"Context":"## Context\\nHistorically Alfresco has had a bad experience of having a monolithic codebase in SVN. The main issue with this was the\\ninability to easily work on feature branches, resulting in frequent conflicting changes. To make matters worse this was\\nseen as hard to move to git, since GitHub has\/had a maximum limit on repository size and there were several large\\nbinary artifacts stored in the Alfresco history [[1]].\\nMore recently the Alfresco codebase has been split into a large number of small git repositories [[2]].\\nThis proposal is to merge the various git repositories together. By doing this we aim to:\\n1. Remove the effort of creating releases of library projects.\\n1. Allow tests to be written on the same branch (in the same repository) as the production code (i.e. facilitate TDD).\\n1. Make it easier for everyone (especially people outside the team and new starters) to find the 'right bit' of code.\\n[1]: https:\/\/community.alfresco.com\/community\/ecm\/blog\/2015\/04\/01\/so-when-is-alfresco-moving-to-github\\n[2]: https:\/\/ts.alfresco.com\/share\/proxy\/alfresco\/api\/node\/content\/versionStore\/version2Store\/a0c2492f-6354-4b98-adfc-e63d5c2209f5\/SearchCodeBase.png\\n","Decision":"## Decision\nWe will merge the search-related repositories together and preserve their history.  We will not attempt to merge code\\nthat other teams also need (for example the TAS test utilities or the alfresco-data-model projects).\\n","Prediction":"## Decision\nThe Alfresco codebase will be split into a number of git repositories, each of which will be updated\\nwith a new release. This will be done in the following manner:\\n- The release will be named after the date it was released. For example, the release for 1.0.0 was\\nreleased on 2015-04-01.\\n- The release will be named in a format which is human-readable and which does not contain any\\nconflicting characters, such as spaces.\\n- The release will be named with the date in the YYYY-MM-DD format, for example, 2015-04-01-1.0.0\\n- The release will be named in a format which is human-readable and which does not contain any\\nconflicting characters, such as spaces.\\n- The release will be named with the date in the YYYY-MM-DD format, for example, 2015-04-01-1.0.0\\n- The release will be named in a format which is human-readable and which does not contain any\\nconflicting characters, such as spaces.\\n- The release will be named with the date in the YYYY-MM-DD format, for example, 2015-04-"}
{"Context":"## Context\\nOkta is an Identity as a Service provider (similar to Auth0) that can provide authentication and authorization support to an application.  They have a forever free developer account that allows up to 7000 active users\/mo (well below our anticipated usage).\\nOkta provides the ability to authenticate against a variety of providers (okta, social (facebook, github, microsoft azure, etc), SAML, AD\/LDAP).  We would use the Microsoft social OpenConnect ID provider.  It will map the Microsoft user into an Okta user, essentially acting as a proxy or facade to Microsoft while providing a more uniform and standards adhering API.\\nThey have integrations into Spring Boot 1.x (via okta spring security starter) and Spring Security 5.x (via Spring Boot 2.x) supports Okta out of the box.\\nThey have a React client SDK instead of using our handrolled client code for Microsoft Azure AD.\\nThe real value add comes when we need to add roles\/groups (admin level users).  You get admin screens\/dashboards \"for free\" to add\/remove users etc.\\nLater on, we could add additional social logins (or support AD directly if needed).\\nSpike can be found at: https:\/\/github.com\/buildit\/bookit-api\/tree\/okta-spring-boot-1.x\\n","Decision":"## Decision\n* Delay for now - because we think we still need a local user table, we can add this in later\\n* Use Okta as IdP for Bookit\\n* Use Okta's React client for client side code\\n* Use Okta's Spring Boot 1.x starter for\\n* Use Okta Access Tokens as Bearer tokens\\n","Prediction":"## Decision\nUse Okta as our Identity Provider to support the authentication of users to our service.\\nWe will use the Okta Spring Boot starter to quickly get up and running with Okta and Spring Security.\\nWe will use the Okta React SDK to add the Okta login to our app.\\nWe will use the Okta Spring Security Starter to add Okta SSO to our app.\\nWe will use the Okta Spring Security 5.x starter to add Okta SSO to our app.\\nWe will use the Okta Spring Security 5.x starter to add Okta SSO to our app.\\nWe will use the Okta Spring Security 5.x starter to add Okta SSO to our app.\\nWe will use the Okta Spring Security 5.x starter to add Okta SSO to our app.\\nWe will use the Okta Spring Security 5.x starter to add Okta SSO to our app.\\nWe will use the Okta Spring Security 5.x starter to add Okta SSO to our app.\\nWe will use the Okta Spring Security 5.x starter to add Okta SSO to our app.\\nWe will use the Okta Spring Security 5.x starter"}
{"Context":"## Context\\nTypes of State:\\n- Local State\\n- e.g. React `useState` Hook\\n- Shared State\\n- e.g. React Context, Redux\\n- Remote State (i.e. API)\\n- e.g. React Context, Redux\\nShared State Options:\\n- `redux` + `redux-observable` + `rxjs` + `react-redux`\\n- Drawbacks\\n- Boilerplate\\n- React Context\\n- Drawbacks\\n- \"Provider Hell\"\\n- Unintentional Re-Renders\\n- `constate` \/ `recoil` \/ `react-tracked`\\nRemote State Options:\\n- `redux` + `redux-observable` + `rxjs` + `react-redux`\\n- Drawbacks\\n- Boilerplate\\n- React Context\\n- Drawbacks\\n- \"Provider Hell\"\\n- Unintentional Re-Renders\\n- `constate` \/ `recoil` \/ `react-tracked` \/ `@tanstack\/react-query`\\n","Decision":"## Decision\n- Local State: React `useState` Hook\\n- Shared State: `recoil`\\n- Remote State: `@tanstack\/react-query`\\n","Prediction":"## Decision\n* Use Redux Context\\n* Use Redux-Redux-Redux-Redux-Redux\\n* Use Redux-Redux-Redux-Redux-Redux\\n* Use Redux-Redux-Redux-Redux\\n* Use Redux-Redux-Redux-Redux\\n* Use Redux-Redux-Redux-Redux\\n* Use Redux-Redux-Redux-Redux\\n* Use Redux-Redux-Redux-Redux\\n* Use Redux-Redux-Redux-Redux\\n* Use Redux-Redux-Redux-Redux\\n* Use Redux-Redux-Redux-Redux\\n* Use Redux-Redux-Redux-Redux\\n* Use Redux-Redux-Redux-Redux\\n* Use Redux-Redux-Redux-Redux\\n* Use Redux-Redux-Redux-Redux\\n* Use Redux-Redux-Redux-Redux\\n* Use Redux-Redux-Redux-Redux\\n* Use Redux-Redux-Redux-Redux\\n* Use Redux-Redux-Redux-Redux\\n* Use Redux-Redux-Redux-Redux\\n* Use Redux-Redux-Redux-Redux\\n* Use Redux-Redux-Redux-Redux\\n* Use Redux-Redux-Redux-Redux\\n* Use Redux-Redux-Redux-Redux\\n* Use Redux-Redux-Redux-Redux\\n* Use Redux-"}
{"Context":"## Context and Problem Statement\\nNumber of module count has been increased our build times. Enabling unused plugins and using **gradle.kts** is causing to longer build times.\\n","Decision":"## Decision\n* Disable generating BuildConfig file if its not needed in module.\\n* Only enable *databinding* if you're going to use DataBinding in that module.\\n* Do not apply *kapt* plugin if you're not going to use.\\n* Do not create new variants other than *debug* and *release*.\\n* Use groovy scripts on *build.gradle* files.\\n","Prediction":"## Decision\nChosen option: **\"Use Gradle + Kustomize + Gradle Ks\"**, because it's the most common way to provide multiple plugins and it's easy to use. Also, we don't need to worry about the update process, which is the main issue we had before. This solution does not require any manual configuration and is easy to use for developers.\\n### Positive Consequences <!-- optional -->\\n* It's easy to use and understand\\n* The build time is reduced\\n* We don't need to worry about updating plugins\\n### Negative Consequences <!-- optional -->\\n* Can't use Gradle Ks (we need to use gradle.kts)\\n* We need to manually update plugins\\n* We need to manage plugins in our project\\n### Other Consequences <!-- optional -->\\n* We can't use plugins that are not in the Gradle Ks\\n* We need to manage plugins in our project\\n* We need to use **gradle.kts** file (for more details see [Gradle Plugin Management](https:\/\/docs.gradle.org\/current\/userguide\/plugins_plugin_management.html))\\n* We need to manage plugins in our project\\n* We can't use plugins that are not in the"}
{"Context":"## Context\\nCode formatting is a problem when scaling a team. Each developer has his own code styling preferences, causing unconsented code.\\nWe need a tool for normalize our javascript code formatting.\\n","Decision":"## Decision\nUse prettier as the tool for javascript code formatting\\n","Prediction":"## Decision\nWe will use [eslint](https:\/\/eslint.org\/) to format our code.\\nESLint is an open-source, community-driven plugin that provides code formatting and linting for JavaScript, TypeScript, and Node.js.\\nWe will use the default configuration, but we will add custom rules to enforce our own formatting.\\nWe will use [eslint-plugin-standard](https:\/\/eslint.org\/docs\/plugin\/standard) to enforce our own rules.\\nWe will use [eslint-config-standard](https:\/\/github.com\/eslint\/eslint-config-standard) to enforce our own rules.\\nWe will use [eslint-plugin-import](https:\/\/eslint.io\/docs\/plugins\/import) to enforce our own rules.\\nWe will use [eslint-plugin-import](https:\/\/github.com\/eslint\/eslint-plugin-import) to enforce our own rules.\\nWe will use [eslint-plugin-jsx-a11y](https:\/\/github.com\/eslint\/eslint-plugin-jsx-a11y) to enforce our own rules.\\nWe will use [eslint-plugin-jsx-a11y](https:\/\/github.com\/eslint\/eslint-plugin-jsx-a11y) to enforce our own rules.\\nWe will use [eslint-plugin-jsx-a11y](https:\/\/github.com\/eslint\/eslint-plugin-js"}
{"Context":"## Context\\nWe need to make sure that whatever we build meets the users' needs. To make sure of this we need to\\nconduct some user research, which should involve putting software in front of users and observing them using it.\\n","Decision":"## Decision\nWe will build a simple \"prototype\" which we will use to test our assumptions about whether our proposed\\nsolution is the best way of meeting our users needs.\\nThe prototype will be architecturally similar to the product we envisage building, but won't be able to\\ndo the SAML interactions with Verify at this stage.\\nADRs for the first prototype will live in the [prototype-0](prototype-0) directory.\\n","Prediction":"## Decision\nWe will use [Tactile](https:\/\/tactile.io\/) to conduct user research. Tactile is an open source tool that enables us to\\ncollect user feedback in a variety of ways.\\nWe will use Tactile to collect feedback on the KODI repo. We will use it to collect feedback on the README.md file. We will use it to collect feedback on the website.\\nWe will use it to collect feedback on the [Kodi](https:\/\/kodi.tv\/) application. We will use it to collect feedback on the [KODI](https:\/\/kodi.tv\/) package.\\nWe will use it to collect feedback on the [KODI](https:\/\/kodi.tv\/) plugin.\\nWe will use it to collect feedback on the [KODI](https:\/\/kodi.tv\/) repository.\\nWe will use it to collect feedback on the [KODI](https:\/\/kodi.tv\/) repository URL.\\nWe will use it to collect feedback on the [KODI](https:\/\/kodi.tv\/) package URL.\\nWe will use it to collect feedback on the [KODI](https:\/\/kodi.tv\/) repository URL.\\nWe will use it to collect feedback"}
{"Context":"## Context\\nBy switching the task manager to a distributed implementation, we need to be able to execute a `Task` on any node of the cluster.\\nWe need to have a way to describe the `Task` to be executed and serialize it in order to be able to store it in the `Created` event. Which will be persisted in the Event Store, and will be send in the event bus.\\nAt this point in time a `Task` can contain any arbitrary code. It's not an element of a finite set of actions.\\n","Decision":"## Decision\n* Create a `Factory` for one `Task`\\n* Inject a `Factory` `Registry` via a Guice Module\\n* The `Task` `Serialization` will be done in JSON, We will get inspired by `EventSerializer`\\n* Every `Task`s should have a specific integration test demonstrating that serialization works\\n* Each `Task` is responsible of eventually dealing with the different versions of the serialized information\\n","Prediction":"## Decision\nFor now we will serialize the `Task` using a JSON object and store it in a DynamoDB table. We will use the `DynamoDB` API to read and write the `Task`.\\nWe will not have a way to create a new `Task` or update an existing one. We will rely on the `DynamoDB` API to perform these operations.\\nWe will have a way to describe the `Task` to be executed using a JSON object. The `Task` will be serialized as a YAML object, and stored in the DynamoDB table. This will be used in the event bus to consume the `Task` and send it to the consumer.\\nWe will have a way to create a new `Task` or update an existing one. We will rely on the `DynamoDB` API to perform these operations. We will not have a way to create a new `Task` or update an existing one.\\nWe will have a way to specify a `TaskId` for each `Task` in the `Created` event. This will be used to identify the `Task` in the event bus to consume the `Task`.\\nWe will have a way to specify a `Task` in the `"}
{"Context":"**Context**: most APIs in the GCS library have a number of optional parameters,\\nfor example, the API can use `ifMetagenerationMatch` to apply an operation only\\nif the metadata generation matches a given number. The question arose of how to\\nrepresent these parameters, as properties that we modify in the client or\\nobject, or as per-request parameters in the function used to access the API.\\nThat is, we had two proposals, one where the application would write code like\\nthis:\\n```C++\\nvoid AppCode(Bucket b, FixedArgument1 foo, FixedArgument2 bar) {\\nb.ApiName(\\nfoo, bar, storage::IfMetagenerationMatch(7), UserProject(\"my-project\"));\\n```\\nvs. code like this:\\n```C++\\nvoid AppCode(Bucket b, FixedArgument1 foo, FixedArgument2 bar) {\\n\/\/ Create a new bucket handle that applies the given optional parameters to\\n\/\/ all requests.\\nauto bucket = b.ApplyModifiers(\\nstorage::IfMetagenerationMatch(7), UserProject(\"my-project\"))\\nbucket.ApiName(foo, bar);\\n}\\n```\\n**Decision**: The parameters are passed as variadic arguments into any function\\nthat needs them. That is, all APIs look like this:\\n```C++\\nclass Bucket \/* or Object as applicable *\/ { public:\\ntemplate <typename... Parameters>\\nReturnType ApiName(\\nFixedArgument1 a1, FixedArgument2 a2,\\nParameters&&... p);\\n```\\nand are used like this:\\n```C++\\nvoid AppCode(Bucket b, FixedArgument1 foo, FixedArgument2 bar) {\\nb.ApiName(\\nfoo, bar, storage::IfMetagenerationMatch(7), UserProject(\"my-project\"));\\n```\\n**Consequences**: The advantages of this approach include:\\n- It is easier to use parameters in an API, it does not require to create a new\\nbucket or object or client handle just for changing the parameters in one\\nrequest.\\nThe downsides include:\\n- All APIs become templates, we should be careful not to create massive header\\nfiles that are slow to compile.\\n- It is harder to overload APIs.\\n- It is not clear how other optional parameters of the APIs, such as timeouts,\\nfit with this structure.\\n","Decision":"## Decision\nthat needs them. That is, all APIs look like this:\\n```C++\\nclass Bucket \/* or Object as applicable *\/ { public:\\ntemplate <typename... Parameters>\\nReturnType ApiName(\\nFixedArgument1 a1, FixedArgument2 a2,\\nParameters&&... p);\\n```\\nand are used like this:\\n```C++\\nvoid AppCode(Bucket b, FixedArgument1 foo, FixedArgument2 bar) {\\nb.ApiName(\\nfoo, bar, storage::IfMetagenerationMatch(7), UserProject(\"my-project\"));\\n```\\n**Consequences**: The advantages of this approach include:\\n- It is easier to use parameters in an API, it does not require to create a new\\nbucket or object or client handle just for changing the parameters in one\\nrequest.\\nThe downsides include:\\n- All APIs become templates, we should be careful not to create massive header\\nfiles that are slow to compile.\\n- It is harder to overload APIs.\\n- It is not clear how other optional parameters of the APIs, such as timeouts,\\nfit with this structure.\\n","Prediction":"## Decision\nThe proposal is to use the first approach for the GCS API.  The other approach\\nshould be possible, but is not a priority.\\nWe will document the decision in the issue.\\n### Decision process\\nWe will discuss the decision in the issue.\\n### Decision\nWe will discuss the decision in the issue.\\n### Decision process\\nWe will discuss the decision in the issue.\\n### Decision\nWe will discuss the decision in the issue.\\n### Decision\nWe will discuss the decision in the issue.\\n### Decision\nWe will discuss the decision in the issue.\\n### Decision\nWe will discuss the decision in the issue.\\n### Decision\nWe will discuss the decision in the issue.\\n### Decision\nWe will discuss the decision in the issue.\\n### Decision\nWe will discuss the decision in the issue.\\n### Decision\nWe will discuss the decision in the issue.\\n### Decision\nWe will discuss the decision in the issue.\\n### Decision\nWe will discuss the decision in the issue.\\n### Decision\nWe will discuss the decision in the issue.\\n### Decision\nWe will discuss the decision in the issue.\\n### Decision\nWe will discuss the decision in the issue.\\n### Decision\n"}
{"Context":"## Context\\nThe smart fridge system is an external system that provides vital information for the customer and the *ordering system*. But, we do not control the frequency of updates, data structure, and management of available positions. Nonetheless, we have to provide our service based on the provided data and try to keep inventory on our side as precise as possible to avoid collision and situations where the user might be dissatisfied with our service.\\nThere is a risk that at some point in time we can't get actual data (offline, maintenance, throttling, caching, etc). How will the ordering system handle such situations?\\nThe occasional user (who pays cash) is a major factor in this issue. Any pre-paid meals that can still be picked up in an offline scenario don't really contribute to the stale data issue as the payment server must already have done its job.\\n","Decision":"## Decision\nWe will cache and maintain data integrity about the available stock in fridges on our side. Until we get new data from the *smart fridge system* at which point the data will be adjusted.\\n","Prediction":"## Decision\nWe will implement a **notification** system that will notify the *ordering system* whenever an offline user is trying to order something. In other words, the *order* notification will be sent to the *ordering system* whenever the *ordering system* is offline, but it will be sent only once.\\nThis means that, if the *ordering system* is offline for a long time, the *ordering system* will not be able to handle the order again. The *ordering system* will send a notification to the *ordering system* if it is offline for a long time, and the *ordering system* will have to react to the notification and check if it's offline or not.\\nThe *ordering system* will be able to handle the order again if the *ordering system* is able to get the order data from the *ordering system* (in this case, the *ordering system* will still be offline).\\nThe *ordering system* will be able to handle the order again if the *ordering system* has a valid connection with the *ordering system* (in this case, the *ordering system* will be able to get the order data).\\nIf the *ordering system* is offline and a notification is sent to the"}
{"Context":"## Context\\nOur current command line interface is awkward to use and contains a considerable amount of business logic. Rather than listing out every command, this ADR will provide guidelines for adding new commands.\\n","Decision":"## Decision\nThe mario command itself will be a collection of subcommands. Only use global options for values that can truly be applied for every subcommand. All other options should be attached to the subcommand. Provide reasonable default values when it makes sense.\\nA few examples:\\n```\\n$ mario ingest --v4 --index aleph-2020-01-01 s3:\/\/bucket\/key.mrc\\n$ mario reindex --url http:\/\/example.com -s aleph-01 -d aleph-02\\n```\\nAdditionally, the `main.go` file should be kept small and all business logic should reside elsewhere in the application.\\n","Prediction":"## Decision\nWe will add a new command line tool to the [Chef Server](https:\/\/chef.io) platform. Chef Server will provide a command line interface for the server management process. This tool will be called Chef Command.\\nChef Command will be a standalone tool that will be available to all Chef Server instances. Chef Server will also provide a web interface that will allow for the management of Chef Server instances.\\nChef Command will be used to perform the following tasks:\\n1. Manage Chef Server\\n2. Manage Chef Client\\n3. Manage Chef Resources\\n4. Manage Chef Resources in a Chef Client\\n5. Manage Chef Resources in a Chef Server\\n6. Manage Chef Modules\\n7. Manage Chef Modules in a Chef Client\\n8. Manage Chef Templates\\n9. Manage Chef Templates in a Chef Client\\n10. Manage Chef Cookbooks\\n11. Manage Chef Cookbooks in a Chef Client\\n12. Manage Chef Source\\n13. Manage Chef Source in a Chef Client\\n14. Manage Chef Sudo\\n15. Manage Chef Sudo in a Chef Client\\n16. Manage Chef Provider\\n17. Manage Chef Provider in a Chef Client\\n18. Manage Chef Supplementary\\n19. Manage Chef Supplementary in a Chef Client\\n20. Manage Chef Resources in"}
{"Context":"## Context\\nHackney have a preference for using this tool across their digital services: https:\/\/github.com\/LBHackney-IT\/API-Playbook#centralised-exception-logging\\n","Decision":"## Decision\nPingdom will be the third party service we use for collecting and managing performance data for all environments.\\n","Prediction":"## Decision\nWe will use [PTLR](https:\/\/github.com\/ptlrlabs\/ptlr) to log exceptions to the `exception` table in the database.\\nThis tool is already used by other services within the LB Hackney IT estate and has been proven to be a good fit for this use case.\\nWe will need to set up a database migration to move the logging from `exception` to `exception2` in the near future.\\nWe will need to update the `exception` table to reflect the new log type, as well as adding a new column to `exception2` to store the error message.\\nWe will also need to run a `plogger` script to convert the log files from `exception` to `exception2`.\\nWe will need to ensure that the new log format is compatible with the current `exception` log format before committing to using it.\\nWe will need to ensure that the `exception2` log format is compatible with the current `exception` log format before committing to using it.\\nWe will need to ensure that the `exception2` log format is compatible with the current `exception` log format before committing to using it.\\nWe will need to ensure that the `exception2"}
{"Context":"## Context\\nSymfony provides a very useful tool called [Autowiring](https:\/\/symfony.com\/doc\/3.4\/service_container\/autowiring.html). It allows to magically bind classes and their dependencies as long as both are declared as services, and the dependencies are declared using their FQCN as service identifier.\\nAdvantages:\\n- Less boilerplate configuration code for every service as you don't have to manually bind dependencies manually.\\nDisadvantages:\\n- Dependencies must be declared using the FQCN instead of a service identifier like \"prestashop.core.foo.bar\".\\n- Currently existing services would have to be aliased in order to have service names follow the required naming convention for autowiring. This would lead to confusion as to which service name use in code, and in case a module wanted to replace them, they would have to replace both.\\n- Dependencies type-hinted as interfaces can have one and **only one** implementation for autowiring to work.\\n","Decision":"## Decision\nActiviting autoriwing is rejected for the 1.7 version.\\n","Prediction":"## Decision\nWe will use [AutoInjection](https:\/\/symfony.com\/doc\/current\/service_container\/autowiring.html) to autowire dependencies. This will give us the following benefits:\\n- More consistent service naming\\n- Ability to use more sophisticated autowiring strategies\\n- Ability to handle dependency injection in more complex scenarios\\n- Ability to use more complex autowiring strategies for services that use custom types\\n- Ability to use more complex autowiring strategies for services that don't use autowiring\\n- Ability to reuse autowiring for services that don't use autowiring\\n- Ability to reuse autowiring for services that use custom types\\n- Ability to reuse autowiring for services that don't use autowiring\\n- Ability to reuse autowiring for services that don't use autowiring\\n- Ability to reuse autowiring for services that don't use autowiring\\n- Ability to reuse autowiring for services that don't use autowiring\\n- Ability to reuse autowiring for services that don't use autowiring\\n- Ability to reuse autowiring for services that don't use autowiring\\n- Ability to reuse autowiring for services that don't use autowiring\\n"}
{"Context":"## Context\\nWe need to know what the application is doing in a more light weight way than\\nscraping logs. We need to be able to monitor KPIs of the application in order\\nto understand the health of the application. This will allow us to react and\\npotentially pro-actively initiate measures as to ensure the application's\\nhealth if sound. Ultimately providing a better service for our users.\\n","Decision":"## Decision\nWe will use Prometheus to monitor and alert on the state of the application.\\n","Prediction":"## Decision\nWe will use [Kiali](https:\/\/kiali.io\/), a tool that allows us to visualize\\nKPIs in a variety of ways. Kiali is a tool that is used by a large number of\\npeople in the data science community and is quite popular within the\\nScikit-learn community. We will use it in conjunction with [Scikit-learn](https:\/\/scikit-learn.org\/).\\nWe will use Kiali to visualize the application's KPIs and to monitor\\nhealth.\\nWe will use [Kiali Health Dashboard](https:\/\/kiali.io\/kiali-health-dashboard) to\\nvisualize the application's health and KPIs.\\nWe will use [Kiali Alert](https:\/\/kiali.io\/alerts) to alert us of any issues.\\nWe will use [Kiali Alert](https:\/\/kiali.io\/alerts) to monitor the health of\\nour application.\\nWe will use [Kiali Alert](https:\/\/kiali.io\/alerts) to monitor the health of\\nour application in a way that allows us to pro-actively react to issues.\\nWe will use [Kiali Alert](https:\/\/kiali.io\/alerts) to monitor the health of\\nour application"}
{"Context":"## Two Unique Problems to Solve\\n* A radio or checkbox field needs to optionally have an \"Other\" input field which consists of a label and freeform text input. All existing radio\/checkbox functionality must be preserved.\\n* Address can exist as a form group which can contain any combination of street address, city, state, zip, etc. It would be useful to have pre-made templates of form groups.\\n","Decision":"## Decision\n* Create additional versions for each individual form field and treat them accordingly in the HTML helper. \"version\" will be a new attribute, \"formtype\" and the rest of the JSON will stay the same.\\n```\\nExample of field variation\\n\"data\":\\n[\\n{\\n\/\/ otherless radio, notice no version attribute (default)\\n\"label\":\"Icecream?\",\\n\"placeholder\":\"placeholder\",\\n\"help\":\"Supporting help text\",\\n\"id\":\"radio_1\",\\n\"formtype\":\"s08\",\\n\"name\":\"icecream\",\\n\"radios\":\"yes\\nno\",\\n\"type\":\"radio\",\\n\"required\":\"true\",\\n\"class\":\"custom-class\"\\n},\\n{\\n\/\/ radio with other\\n\"label\":\"Icecream flavor?\",\\n\"placeholder\":\"placeholder\",\\n\"help\":\"Supporting help text\",\\n\"id\":\"radio_2\",\\n\"formtype\":\"s08\",\\n\"name\":\"icecream_flavor\",\\n\"radios\":\"vanilla\\nchocolate\",\\n\"type\":\"radio\",\\n\"version\": \"other\", \/\/ Variation version\\n\"required\":\"true\",\\n\"class\":\"custom-class\"\\n},\\n]\\n```\\n* Add a new \"groupid\" attribute to the saved JSON form data object.\\n* The new \"groupid\" will act as a dynamic id which will group all fields with the same groupid together.\\n* The \"groupid\" value will be generated once it is dragged\/added to the editing form and be a concatenation of the form group template name (see below) and an incremental number, ie: g_address_streetonly_1\\n* Versions of form groups do not depend on each other.\\n* Added form groups to the form will be non-editable but will otherwise appear as regular fields within the JSON form data object.\\n```\\nExample of form data with a group\\n\"data\":\\n[\\n{\\n\/\/ pizza is not in the group\\n\"label\":\"Pizza\",\\n\"placeholder\":\"placeholder\",\\n\"help\":\"Supporting help text\",\\n\"id\":\"pizza_1\",\\n\"formtype\":\"s08\",\\n\"name\":\"pizza\",\\n\"radios\":\"unpopular\\nreally unpopular\",\\n\"type\":\"radio\",\\n\"required\":\"true\",\\n\"class\":\"custom-class\"\\n},\\n{\\n\"label\":\"Icecream?\",\\n\"placeholder\":\"placeholder\",\\n\"help\":\"Supporting help text\",\\n\"id\":\"radio_1\",\\n\"formtype\":\"s08\",\\n\"name\":\"icecream\",\\n\"radios\":\"yes\\nno\",\\n\"type\":\"radio\",\\n\"required\":\"true\",\\n\"class\":\"custom-class\",\\n\"groupid\":\"g_icecream_all_1\"\\n},\\n{\\n\"label\":\"Icecream flavor?\",\\n\"placeholder\":\"placeholder\",\\n\"help\":\"Supporting help text\",\\n\"id\":\"radio_2\",\\n\"formtype\":\"s08\",\\n\"name\":\"icecream_flavor\",\\n\"radios\":\"vanilla\\nchocolate\",\\n\"type\":\"radio\",\\n\"version\": \"other\", \/\/ Variation version\\n\"required\":\"true\",\\n\"class\":\"custom-class\",\\n\"groupid\":\"g_icecream_all_1\"\\n},\\n]\\n```\\n* HTML Generation: Create a helper listing of form group templates which will describe which form fields belong to which predefined group.\\n* Format would be a JSON with an array of formtypes: each formtype would adopt the same attributes as the existing form field JSON.\\n* Undefined attributes would be filled in by the default value and defined attributes would override the default.\\n* Naming groups will start with g_groupname_templatename\\n```\\nExample of form group templates\\n\"address\" : {\\n\"full\" : [\\n{\\n\"formtype\":\"c08\",\\n\"label\":\"Address 1\",\\n},\\n{\\n\"formtype\":\"c08\",\\n\"label\":\"Address 2\",\\n},\\n{\\n\"formtype\":\"c10\", \/\/city\\n},\\n{\\n\"formtype\":\"s14\", \/\/state\\n},\\n{\\n\"formtype\":\"c14\", \/\/zip\\n}\\n],\\n\"streetzip\" : [\\n{\\n\"formtype\":\"c08\",\\n},\\n{\\n\"formtype\":\"c14\", \/\/zip\\n}\\n]\\n}\\n```\\n","Prediction":"## Decision\n* Address fields should be pre-made templates of form groups.\\n* Address fields should be pre-made templates of form groups.\\n* Address fields should be pre-made templates of form groups.\\n* Address fields should be pre-made templates of form groups.\\n* Address fields should be pre-made templates of form groups.\\n* Address fields should be pre-made templates of form groups.\\n* Address fields should be pre-made templates of form groups.\\n* Address fields should be pre-made templates of form groups.\\n* Address fields should be pre-made templates of form groups.\\n* Address fields should be pre-made templates of form groups.\\n* Address fields should be pre-made templates of form groups.\\n* Address fields should be pre-made templates of form groups.\\n* Address fields should be pre-made templates of form groups.\\n* Address fields should be pre-made templates of form groups.\\n* Address fields should be pre-made templates of form groups.\\n* Address fields should be pre-made templates of form groups.\\n* Address fields should be pre-made templates of form groups.\\n* Address fields should be pre-made templates of form groups.\\n* Address fields should be pre-made templates of form groups.\\n* Address fields should be pre"}
{"Context":"## Context and Problem Statement\\nIn the implementation of payment features for subscription services, we've\\ndecided to use third-party JavaScript for payment widgets.\\nBest practices established by our security team indicate that third-party JS\\nshould not be included on the highly-sensitive pages - i.e. such as those used\\nfor general account management on Firefox Accounts.\\nSo, we need a way to isolate the pages responsible for subscription sign-up and\\nmanagement from the rest of Firefox Accounts.\\n## Decision Drivers\\n* Security when dealing with financial transactions.\\n* Security when including with third-party JS code for payment widgets.\\n* Simplicity in user experience flows.\\n* Delivering against the subscription services deadline.\\n","Decision":"## Decision\n* Security when dealing with financial transactions.\\n* Security when including with third-party JS code for payment widgets.\\n* Simplicity in user experience flows.\\n* Delivering against the subscription services deadline.\\nChosen option: \"Option A - Payment pages as separate app supplied with\\npre-generated access token\", because\\n* Further refinements to access token delivery mechanism in Option A do not\\nsignificantly affect the rest of the payments app.\\n* Doesn't preclude an upgrade to Option B in the future - i.e. once [Issue\\n#640](https:\/\/github.com\/mozilla\/fxa\/issues\/640) is resolved.\\n* Doesn't preclude Option C as a future option - e.g. offering embedded\\nsubscription widgets to third-parties.\\n* Fastest practical option given existing record of reviews by security & UX and\\nwork completed so far.\\n* Fresh start with a more modern web stack (i.e. React).\\n","Prediction":"## Decision\n* Security when dealing with financial transactions.\\n* Security when including with third-party JS code for payment widgets.\\n* Simplicity in user experience flows.\\n* Delivering against the subscription services deadline.\\nChosen option: \"Separate the sign-up and account management pages as separate\\nmodules\", because it:\\n* Is a best practice.\\n* Is the simplest solution.\\n* Is in line with the design decisions made by the team.\\n* Is the most secure option in regards to both the server-side code and the browser-side\\ncode.\\n* Is the least disruptive to the user experience.\\n* Has the lowest impact on the subscription service's deadline.\\n* Is the least expensive option.\\n* Is the easiest to implement.\\n* Is the best choice for the next major release of Firefox Accounts.\\n### Positive Consequences\\n* Client-side code is isolated from the rest of the code.\\n* The client-side code is not considered a security issue, since it is not\\ninvolved in any of the subscription sign-up or account management API calls.\\n* The client-side code is not included in the sign-up and account management API\\ncalls.\\n* The client-side code is not considered a security"}
{"Context":"## Context\\nIn the course of trying to standardize how we do database development, we have had lots of discussion around schemas (much of the conversation around how oracle specifically views schemas, but the conversation *may* be relavant to other databases). This conversation has been mostly around how do do our database development, and how do we provide appropriate access to the required data.\\nThrougout this discussion (mostly happening in our DB Working Group), we have agreed on a set of definitions for different types of schemas.\\nHere are the notes from the original discussion:\\n","Decision":"## Decision\nWe will use the following definitions for the different types of schemas in our databases:\\n### System Schemas\\nThese are schemas in the database that are completely outside of our control, used by the database itself as necessary. Even though we don't create or manage these, we are including them here for completness and categorization.\\n### DBA User Schemas\\nDBA User Schemas will exist for the DBAs to perform necessary functions in our databases.\\nThese schemas will have the highest level of access in our systems, and thus need to be the most careful about credentials and access.\\nDBA User Schemas should follow this naming convention:\\n```\\n{}_DBA\\n```\\nWhere {} is some useful identifier (i.e. `EXAMPLE_DBA`).\\nUsually, we have only one of these per database, `EXAMPLE_DBA`. If more are necessary, they should follow this convention.\\n### Application User Schemas\\nApplication user schemas will exist for each application that needs to access data in our databases.\\nApplication users should be granted the appropriate roles to access the necessary data. They should not be granted individual object grants.\\nApplication users should not have any object creation permissions (i.e. they should not be able to perform DDL operations).\\nApplication User Schemas should follow this naming convention:\\n```\\n{}_APP\\n```\\nWhere {} is the application name (i.e. `CALENDAR_APP`).\\n### Individual User Schemas\\nIndividual user schemas represent unique individual people that need access to database systems.\\nThese types of schemas are primarly used by developers and people in the organization that perform data analytics functions.\\nIndividual users should be granted the appropriate roles to access the necessary data. They should not be granted individual object grants.\\nIndividual users should have object creation permissions only for their own user schema.\\nIndividual User Schemas should be named the same as the user's email address. We prefer this to the AD account credentials because it follows a human-readable format, and better allows us to understand who the user is.\\n### Domain Schemas\\nDomain schemas are where the actual objects in the database exist (i.e. tables, views, functions, packages).\\nDomain schemas should have object creation permissions only for their own schema.\\nWhen domain schemas are created, the appropriate roles should also be created to be able to control access to the objects.\\nNote that these might exist in multiple database instances, e.g. custorder might have extract logic in POSP and tables in DSSP.\\nDomain Schemas should be named logically so their purpose can at least be guessed at.\\n### Replicated Schemas\\nReplicated Schmeas are similar to Domain schemas, except they will *only* include data whose system of record is elsewhere. They should not include any objects that aren't represented in the \"parent\" schema.\\nLike Domain schemas, the appropriate roles should also be created to be able to control access to the objects.\\nReplicated Schemas should follow this naming convention:\\n```\\n{}_REPL\\n```\\nWhere {} is the application name (i.e. `CALENDAR_REPL`).\\n","Prediction":"## Decision\n* Schema: a collection of data, that is owned by a single user or application, and is not shared with other users or applications\\n* Schema: a collection of data, that is owned by a single user or application, and is shared with other users or applications\\n* Schema: a collection of data, that is owned by a single application, and is shared with other applications\\n* Schema: a collection of data, that is owned by a single application, and is shared with other applications\\n* Schema: a collection of data, that is owned by a single application, and is shared with other applications\\n* Schema: a collection of data, that is owned by a single application, and is shared with other applications\\n* Schema: a collection of data, that is owned by a single application, and is shared with other applications\\n* Schema: a collection of data, that is owned by a single application, and is shared with other applications\\n* Schema: a collection of data, that is owned by a single application, and is shared with other applications\\n* Schema: a collection of data, that is owned by a single application, and is shared with other applications\\n* Schema: a collection of data, that is owned by a single"}
{"Context":"## Context\\nThere are possible solutions related to validation topic:\\n- always valid approach. Domain model is encapsulated, all requests are validated before executing logic in domain model;\\n- deferred validation. Write all data to domain model from request and validate everything in domain model;\\n- return validation object.\\n","Decision":"## Decision\nAlways valid approach will be used. So domain model will be changed from one valid state to another valid state. Technically, validation rules are defined on `Command` models and executed during processing the command. Javax validation-api is used for defining the validation rules via annotations.\\nUseful links:\\n- https:\/\/danielwhittaker.me\/2016\/04\/20\/how-to-validate-commands-in-a-cqrs-application\/\\n- https:\/\/enterprisecraftsmanship.com\/posts\/validate-commands-cqrs\/\\n- https:\/\/enterprisecraftsmanship.com\/posts\/validation-and-ddd\/\\n- https:\/\/enterprisecraftsmanship.com\/posts\/always-valid-vs-not-always-valid-domain-model\/\\n- http:\/\/www.kamilgrzybek.com\/design\/domain-model-validation\/\\n- http:\/\/www.kamilgrzybek.com\/design\/rest-api-data-validation\/\\n","Prediction":"## Decision\nUse deferred validation for validation of all data. Domain model is not validated until the request is received. This approach has several advantages:\\n- no request is sent to server until all data is validated;\\n- domain model is validated only once (during request processing);\\n- no data is sent to server until the request is received.\\n- when the request is received, domain model is validated only once.\\n- domain model is not validated when the request is sent.\\n- domain model is validated only if it's a valid request.\\n- domain model is validated only once.\\n- domain model is not validated when the request is sent.\\n- domain model is validated only if it's a valid request.\\n- domain model is validated only once.\\n- domain model is not validated when the request is sent.\\n- domain model is validated only if it's a valid request.\\n- domain model is validated only once.\\n- domain model is not validated when the request is sent.\\n- domain model is not validated when the request is sent.\\n- domain model is not validated when the request is sent.\\n- domain model is not validated when the request is sent.\\n- domain model is not validated when the request is"}
{"Context":"## [Context](https:\/\/github.com\/libero\/community\/issues\/13)\\nLibero needs automated and human feedback over pull requests and release candidates.\\n","Decision":"## Decision\nWe will provide Travis CI builds for all repositories, covering both testing and deployment to a demo environment.\\n","Prediction":"## Decision\nLibero will use the [Jenkins](https:\/\/jenkins.io\/) continuous integration (CI) and continuous delivery (CD) system to manage and deploy the codebase. Jenkins is a free, open-source continuous integration and continuous delivery (CI\/CD) platform that runs in a Docker container on every developer's machine.\\n### Positive Consequences\\n* CI and CD systems will be used to improve the build, test, and deployment process.\\n* The codebase will be versioned.\\n### Negative Consequences\\n* The CI and CD systems will be tested on a separate machine, which will make it harder to reproduce issues.\\n* The versioning system will require manual intervention.\\n* The CI and CD systems will require manual intervention.\\n### Impact on other projects\\nLibero is one of the few Librem 5 components that uses Jenkins. As a result, it will be used to manage the deployment of other Librem 5 components. For example, the [Librem 5](https:\/\/github.com\/librem5\/librem5) project uses Jenkins to manage the deployment of its own components.\\n### Consequences for the project\\nLibrem 5 users will be impacted by the change. The systems used by Lib"}
{"Context":"### Context\\nOTRv4 is a protocol that aims to:\\n1. Be an alternative to current messaging applications that work in synchronous\\nand asynchronous messaging environments.\\n2. Be a comprehensive and up-to-date specification: it updates cryptographic\\nprimitives and increase the security level of the whole protocol to 224 bits.\\n3. Provide better deniability properties.\\n4. Be compatible with OTRv3 and be useful for instant messaging protocols\\n(e.g. XMPP).\\nIn order to be an alternative to current messaging applications and to be\\ncompatible with OTRv3, OTRv4 protocol must define three modes in which it can be\\nimplemented: a only OTRv4 mode and a OTRv3-compatible mode. These are the three\\nmodes enforced by the protocol, but, it must be taken into account, that OTRv4\\ncan and may be also implemented in other modes.\\n### Decision\\nTo attain all of the purposes of OTRv4, the specification can work in three\\nmodes:\\n1. OTRv4-standalone mode: an always encrypted mode. This mode will not know how\\nto handle any kind of plaintext message, including query messages and\\nwhitespace tags.\\n2. OTRv3-compatible-mode: a mode with backwards compatibility with OTRv3.\\nThis mode will know, therefore, how to handle plaintext messages, including\\nquery messages and whitespace tags.\\n3. OTRv4-interactive-only-mode: a always encrypted mode that provides higher\\ndeniability properties when compared to the previous two modes. It only\\nsupports interactive conversations.\\n### Consequences\\nAs a result, OTRv4' state machine will need to know the mode is working on when\\ninitialized. It will also need to take this mode into account every time it\\nmakes a decision on how to transition from every state. This increases the\\ncomplexity of the specification and implementation.\\nFurthermore, \"OTRv4-standalone\" mode will only support version 4 of OTR. The\\nClient Profile, therefore will only allow the 1-byte version string \"4\". It will\\nalso not allow the Transitional Signature parameter on the same Client Profile.\\nIn addition to only supporting the version 4 of OTR (and imposing the same\\nrestrictions to the Client Profile as the \"OTRv4-standalone\" mode), the\\n\"OTRv4 interactive-only\" mode will only support the interactive DAKE. This mode\\nwill not handle or generate any Prekey Profile or prekey messages, nor implement\\na Prekey Sever, not retrieve prekey ensembles.\\nIt should be taken into account, also, that some clients might implement\\ndifferent modes when talking with each other. In those cases:\\n* If a client implements \"OTRv4-standalone\" mode or \"OTRv4-interactive-only\"\\nmode and a request for an OTRv3 conversation arrives, reject this request.\\n* If a client implements \"OTRv4-interactive-only\" mode and a request for an\\noffline conversation arrives, reject this request.\\n","Decision":"## Decision\nTo attain all of the purposes of OTRv4, the specification can work in three\\nmodes:\\n1. OTRv4-standalone mode: an always encrypted mode. This mode will not know how\\nto handle any kind of plaintext message, including query messages and\\nwhitespace tags.\\n2. OTRv3-compatible-mode: a mode with backwards compatibility with OTRv3.\\nThis mode will know, therefore, how to handle plaintext messages, including\\nquery messages and whitespace tags.\\n3. OTRv4-interactive-only-mode: a always encrypted mode that provides higher\\ndeniability properties when compared to the previous two modes. It only\\nsupports interactive conversations.\\n### Consequences\\nAs a result, OTRv4' state machine will need to know the mode is working on when\\ninitialized. It will also need to take this mode into account every time it\\nmakes a decision on how to transition from every state. This increases the\\ncomplexity of the specification and implementation.\\nFurthermore, \"OTRv4-standalone\" mode will only support version 4 of OTR. The\\nClient Profile, therefore will only allow the 1-byte version string \"4\". It will\\nalso not allow the Transitional Signature parameter on the same Client Profile.\\nIn addition to only supporting the version 4 of OTR (and imposing the same\\nrestrictions to the Client Profile as the \"OTRv4-standalone\" mode), the\\n\"OTRv4 interactive-only\" mode will only support the interactive DAKE. This mode\\nwill not handle or generate any Prekey Profile or prekey messages, nor implement\\na Prekey Sever, not retrieve prekey ensembles.\\nIt should be taken into account, also, that some clients might implement\\ndifferent modes when talking with each other. In those cases:\\n* If a client implements \"OTRv4-standalone\" mode or \"OTRv4-interactive-only\"\\nmode and a request for an OTRv3 conversation arrives, reject this request.\\n* If a client implements \"OTRv4-interactive-only\" mode and a request for an\\noffline conversation arrives, reject this request.\\n","Prediction":"## Decision\nTo attain all of the purposes of OTRv4, the specification can work in three\\nmodes:\\n1. OTRv4-standalone mode: an always encrypted mode. This mode will not know how\\nto handle any kind of plaintext message, including query messages and\\nwhitespace tags.\\n2. OTRv3-compatible-mode: a mode with backwards compatibility with OTRv3.\\nThis mode will know, therefore, how to handle plaintext messages, including\\nquery messages and whitespace tags.\\n3. OTRv4-interactive-only-mode: a always encrypted mode that provides higher\\ndeniability properties when compared to the previous two modes. It only\\nsupports interactive conversations.\\n### Consequences\\nAs a result, OTRv4' state machine will need to know the mode is working on when\\ninitialized. It will also need to take this mode into account every time it\\nmakes a decision on how to transition from every state. This increases the\\ncomplexity of the specification and implementation.\\nFurthermore, \"OTRv4-standalone\" mode will only support version 4 of OTR. The\\nClient Profile, therefore will only allow the 1-byte version string \"4\". It"}
{"Context":"## Context\\nCurrently (not counting Tendermint's internal storage or wallets), two processes maintain their internal storage:\\n* chain-abci: stores the node state, Merkle trie of staking states, transaction metadata (whether spent or not),\\nvalidator tracking, etc.\\n* tx-validation enclave (TVE): sealed transaction data (of valid obfuscated transactions that have outputs)\\nThe reason for having two processes is that SGX SDK compilation is different and needs Intel SGX SDK tooling\\n(and the subsequent process execution requires Intel SGX PSW tooling, such as AESM service),\\nso for the development convenience, the transaction validation code that needs to execute in an enclave\\nis isolated. (For example, one can build and run chain-abci on any platform (e.g. macOS),\\nand run the enclave parts inside a docker container or on a remote Linux host.)\\nThe inter-process communication is over a simple REQ-REP 0MQ socket.\\n*Problem 1: These two storage locations need to be \"in sync\"*:\\nwhen an obfuscated transaction arrives that spends some transaction outputs, chain-abci will do a basic validation and check if they are unspent and forward it to TVE (assuming its storage contains\\nsealed transaction data of respective outputs). There is currently a naive check that TVE\\nstores the latest app hash provided by chain-abci; and upon a startup, chain-abci cross-checks if TVE is in sync with it. This leads to various errors and annoyances that are usually resolved by removing all storage and syncing from scratch (in certain cases, there may be a better mechanism, but wasn't implemented).\\n*Problem 2: Transaction querying*:\\nAs wallet \/ client-* may be lightweight client and not have access to TEE directly, it will connect to one remotely.\\nFor this purpose, there is transaction query enclave (TQE). See [this document](https:\/\/github.com\/crypto-com\/chain-docs\/blob\/master\/plan.md#transaction-query-enclave-tqe-optional--for-client-infrastructure) for more details.\\nThere are two flows (over an attested secure channel):\\n1. retrieving transactions: client submits transaction identifiers signed by its view key, and TQE replies with matching transaction data. For this workflow, TQE contacts TVE over REQ-REP 0MQ socket to retrieve data.\\n2. submitting new transactions: client submits a new transaction, TQE forwards it to TVE that checks it (so that it doesn't obfuscate random \/ invalid data) and if it's valid, it encrypts it with the obfuscation key (currently compile-time mock, but planned to be periodically regenerated\\nby another type of enclave) and returns the obfuscated transaction to TQE that forwards it to the client.\\nIn the first flow, TQE only talks to the TVE's application wrapper that handles the persistence -- it can unseal\\nthe transaction data, because the key policy is MRSIGNER.\\nIn the second flow, TVE holds the obfuscation key inside the enclave memory, so the payload goes to TVE.\\nCurrently, TVE cannot check everything, e.g. staked state or if a transaction output was spent or not\\n-- in the future, it may internally have app hash components and at least require some lightweight proofs\\nfor these things.\\nFor the first flow, it's unnecessary for TQE to talk to TVE. For the second flow, it'll be desirable\\nto do a more complete verification (currently there are a few hacks and workarounds).\\n","Decision":"## Decision\nThis will be a bit big change, so it can be done in several steps:\\n1. separate out the storage functionality from chain-abci into chain-storage crate\\n2. TBD: separate storage process? moving enclave app wrappers to chain-abci?\\n3. change enclave-protocol for chain-abci\/storage to add sealed transaction data, and TVE to return Ok(sealed transaction data) back to chain-abci\/storage\\n4. give TQE some way to talk to chain-abci\/storage (TBD: another 0MQ socket or abci_query?)\\n5. change the first flow of TQE to use that instead of talking to TVE\\n6. TBD: change the second flow to go through chain-abci (it'd still need to forward to TVE)\\n7. remove storage from TVE (as it should be all handled by chain-abci\/storage)\\n","Prediction":"## Decision\nUse the [TQE](https:\/\/github.com\/crypto-com\/chain-docs\/blob\/master\/plan.md#transaction-query-enclave-tqe-optional--for-client-infrastructure) enclaves to implement the transaction validation and query operations.\\nThis enclaves are implemented as a separate package, and deployed on the TEE. The TQE enclaves are:\\n* pre-compiled into a single binary (TQE binary)\\n* wrapped inside a Docker image (TQE binary image)\\n* uploaded to the TEE using a [S3-compatible bucket](https:\/\/docs.aws.amazon.com\/AmazonS3\/latest\/userguide\/AmazonS3Bucket.html)\\n* accessible via the [API](https:\/\/docs.aws.amazon.com\/AmazonS3\/latest\/userguide\/S3Bucket.html)\\n* exposed to the TEE via a [S3-compatible bucket API](https:\/\/docs.aws.amazon.com\/AmazonS3\/latest\/userguide\/AmazonS3Bucket.html)\\n* deployed using the [AWS CodeDeploy](https:\/\/aws.amazon.com\/codeDeploy\/)\\n* available via the [AWS CodeCommit](https:\/\/docs.aws.amazon.com\/code-commit-apis\/latest\/developerguide\/aws-codecommit.html)\\n* deployed on the"}
{"Context":"## Context\\nThe following minor language improvements need to be made:\\n* querying by number explicitly for a field\\n* querying by text match vs an exact term match\\n* generic NOT support\\n","Decision":"## Decision\nTo support these, `QueryHandler` needs to have a `DataTypeInfo` instance to figure out what type each field is. This has some knock-on effects in parsing a string into a query.\\nIn order to make testing simpler, `DefaultQueryHandler` will remain `DataTypeInfo` agnostic and a new sub-class `DataTypeAwareQueryHandler` created which uses an internal `DataTypeInfo` to figure out the type.\\nThe main place where this comes in is in `FetchParams` which transforms a string into a list of `Query` objects. This functionality will be shifted out of the `FetchParams` object and into a generic `SearchBasedService` method.\\nIn addition, `FieldType` and `QueryType` have now converged so that `QueryType` can be retired. This means that some of the query builders will need updating to deal with things like `GENOME`, `ID` and `ONTOLOGY`.\\n`Query` now also has a `not` property so that a query handler can decide if a query is \"not\" or not. The search implementation should deal with this.\\n","Prediction":"## Decision\n* Querying by number explicitly for a field\\n* Querying by text match vs an exact term match\\n* Generic NOT support\\n* Support for multiple language files\\n* Support for custom SQL dialect\\n* Support for custom HTTP verbs\\n* Support for custom HTTP methods\\n* Support for custom HTTP headers\\n* Support for custom HTTP status codes\\n* Support for custom HTTP status codes for custom HTTP methods\\n* Support for custom HTTP methods with custom HTTP headers\\n* Support for custom HTTP status codes with custom HTTP headers\\n* Support for custom HTTP status codes with custom HTTP status codes\\n* Support for custom HTTP status codes with custom HTTP status codes\\n* Support for custom HTTP status codes with custom HTTP status codes\\n* Support for custom HTTP status codes with custom HTTP status codes\\n* Support for custom HTTP status codes with custom HTTP status codes\\n* Support for custom HTTP status codes with custom HTTP status codes\\n* Support for custom HTTP status codes with custom HTTP status codes\\n* Support for custom HTTP status codes with custom HTTP status codes\\n* Support for custom HTTP status codes with custom HTTP status codes\\n* Support for custom HTTP status codes with custom HTTP status codes\\n* Support for custom HTTP status_codes\\n* Support for custom HTTP status_codes\\n* Support"}
{"Context":"- [Context](#context)\\n- [Alternative Approaches](#alternative-approaches)\\n- [Decision](#decision)\\n- [Detailed Design](#detailed-design)\\n- [EventSink](#eventsink)\\n- [Supported Sinks](#supported-sinks)\\n- [`KVEventSink`](#kveventsink)\\n- [`PSQLEventSink`](#psqleventsink)\\n- [Configuration](#configuration)\\n- [Future Improvements](#future-improvements)\\n- [Consequences](#consequences)\\n- [Positive](#positive)\\n- [Negative](#negative)\\n- [Neutral](#neutral)\\n- [References](#references)\\n","Decision":"## Decision\n- [Detailed Design](#detailed-design)\\n- [EventSink](#eventsink)\\n- [Supported Sinks](#supported-sinks)\\n- [`KVEventSink`](#kveventsink)\\n- [`PSQLEventSink`](#psqleventsink)\\n- [Configuration](#configuration)\\n- [Future Improvements](#future-improvements)\\n- [Consequences](#consequences)\\n- [Positive](#positive)\\n- [Negative](#negative)\\n- [Neutral](#neutral)\\n- [References](#references)\\nWe will adopt a similar approach to that of the Cosmos SDK's `KVStore` state\\nlistening described in [ADR-038](https:\/\/github.com\/cosmos\/cosmos-sdk\/blob\/master\/docs\/architecture\/adr-038-state-listening.md).\\nWe will implement the following changes:\\n- Introduce a new interface, `EventSink`, that all data sinks must implement.\\n- Augment the existing `tx_index.indexer` configuration to now accept a series\\nof one or more indexer types, i.e., sinks.\\n- Combine the current `TxIndexer` and `BlockIndexer` into a single `KVEventSink`\\nthat implements the `EventSink` interface.\\n- Introduce an additional `EventSink` implementation that is backed by\\n[PostgreSQL](https:\/\/www.postgresql.org\/).\\n- Implement the necessary schemas to support both block and transaction event indexing.\\n- Update `IndexerService` to use a series of `EventSinks`.\\nIn addition:\\n- The Postgres indexer implementation will _not_ implement the proprietary `kv`\\nquery language. Users wishing to write queries against the Postgres indexer\\nwill connect to the underlying DBMS directly and use SQL queries based on the\\nindexing schema.\\nFuture custom indexer implementations will not be required to support the\\nproprietary query language either.\\n- For now, the existing `kv` indexer will be left in place with its current\\nquery support, but will be marked as deprecated in a subsequent release, and\\nthe documentation will be updated to encourage users who need to query the\\nevent index to migrate to the Postgres indexer.\\n- In the future we may remove the `kv` indexer entirely, or replace it with a\\ndifferent implementation; that decision is deferred as future work.\\n- In the future, we may remove the index query endpoints from the RPC service\\nentirely; that decision is deferred as future work, but recommended.\\n","Prediction":"## Decision\nWe will use `KvEventSink` for the sink, which is a high performance, low-latency event sink. It is designed to be a drop-in replacement for `KvStore`, which is what we are using now, and it supports the same protocol and transport layer. It will also be a drop-in replacement for `KvStore` in the future.\\nWe will also use `PSQLEventSink` to connect to the sink. It is a higher-level event sink that is designed to be used when an event is being sent to a specific system. It supports the same protocol and transport layer, and will provide a drop-in replacement for `KvStore` in the future.\\nWe will use the `KvEventSink` for the sink. This will allow us to drop-in the `KvStore` event sink, and have a single, unified event sink in the system. We will then be able to drop-in the `KvStore` event sink in the future, if needed.\\nWe will use the `PSQLEventSink` for the sink. This will allow us to drop-in the `KvStore` event sink, and have a single, unified event sink in the system"}
{"Context":"## Context\\nThe issue motivating this decision, and any context that influences or constrains the decision.\\n","Decision":"## Decision\nThe change that we're proposing or have agreed to implement.\\n","Prediction":"## Decision\nThe change that we're proposing or have agreed to implement.\\n### Positive Consequences\\n* The change that we're proposing or have agreed to implement.\\n### Negative Consequences\\n* We're not sure yet.\\n* We have a plan to address negative consequences.\\n### Change description\\nWe're making a change to the way we're storing and querying data in a way that\\nreduces the number of data points that we have to maintain. It also provides\\na number of benefits over the current approach.\\n### Positive Consequences\\n* We're not sure yet.\\n* We have a plan to address negative consequences.\\n### Negative Consequences\\n* We're not sure yet.\\n* We have a plan to address negative consequences.\\n### Change description\\nWe're making a change to the way we're querying data in a way that\\nreduces the number of data points that we have to maintain. It also provides\\na number of benefits over the current approach.\\n### Positive Consequences\\n* We're not sure yet.\\n* We have a plan to address negative consequences.\\n### Negative Consequences\\n* We're not sure yet.\\n* We have a plan to address negative consequences.\\n###"}
{"Context":"## Context\\nYarn and NPM can both manage the Node packages for a project. Recent updates to NPM mean that Yarn only has a negligible performance advantage over NPM.\\n","Decision":"## Decision\nWe will use `yarn`, `yarn start`, `yarn add`, `yarn remove` etc. for the management of Node packages in our project.\\n","Prediction":"## Decision\nWe will use NPM to manage the Node packages for the project.\\nThis decision is made in the context of a larger architecture decision that is being considered.\\n* The architecture is being considered in the context of a larger architecture decision that is being considered.\\n* The architecture is being considered in the context of a larger architecture decision that is being considered.\\n## Decision\nWe will use NPM to manage the Node packages for the project.\\nThis decision is made in the context of a larger architecture decision that is being considered.\\n* The architecture is being considered in the context of a larger architecture decision that is being considered.\\n* The architecture is being considered in the context of a larger architecture decision that is being considered.\\n* The architecture is being considered in the context of a larger architecture decision that is being considered.\\n* The architecture is being considered in the context of a larger architecture decision that is being considered.\\n* The architecture is being considered in the context of a larger architecture decision that is being considered.\\n* The architecture is being considered in the context of a larger architecture decision that is being considered.\\n* The architecture is being considered in the context of a larger architecture decision that is being considered.\\n* The architecture is"}
{"Context":"## Context\\nA lot of Home Assistant integrations use config options like `CONF_MONITORED_CONDITIONS` to allow the user to select which data the integration should expose from the data. This means that the user needs to know what the different data points mean while setting up the integration. While configuring its Lovelace UI, the user has the option to include the entity or not. This means that we allow the user to pick twice.\\n","Decision":"## Decision\nIntegrations should expose all available data to the backend if that data is fetched in a single API request.\\nIntegrations should only include selector logic if it make sense in the context of interface, like if it would require extra requests. User should not have to read the available documentation and API descriptions to find out which data they want have.\\n```\\nLayer model:\\nFetch            Manage           View\\n-------------      ---------      ------------\\n| Interface | ---> | Core  | ---> | Frontend |\\n-------------      ---------      ------------\\n```\\nIntegrations can set the `entity_registry_enabled_default` property on their entity objects to instruct Home Assistant to disable certain entities by default ([docs](https:\/\/developers.home-assistant.io\/docs\/entity_registry_disabled_by\/#integrations-setting-default-value-of-disabled_by-for-new-entity-registry-entries)).\\n","Prediction":"## Decision\nWe have decided to remove the option to include the entity in the config options. This will remove the inconsistency between the Lovelace UI and the configuration options and will allow the user to pick the data. This will also allow the Lovelace UI to be more flexible and allow the user to easily add new data points in the future.\\n### Why we decided to remove the entity option\\nThe entity option has been removed because it is not necessary. The user has the option to include the entity in the config options, so there is no reason to keep it. The user can also choose to remove the entity in the config options, which was the case in the past. Removing the entity option will allow the user to pick the data and thus the Lovelace UI can be more flexible.\\n### Why we decided to keep the config options for the entity\\nThe config options include the entity and the entity data. The user can choose whether or not to include the entity in the config options. This ensures that the Lovelace UI is consistent with the configuration options.\\n### Why we decided to remove the entity option from the config options\\nThe entity option was removed because it was not necessary. The user has the option to include the entity in the"}
{"Context":"## Context and Problem Statement\\nSome entry titles are composed of multiple sentences, for example: \"Whose Music? A Sociology of Musical Language\", therefore, it is necessary to first split the title into sentences and process them individually to ensure proper formatting using '[Sentence Case](https:\/\/en.wiktionary.org\/wiki\/sentence_case)' or '[Title Case](https:\/\/en.wiktionary.org\/wiki\/title_case#English)'\\n","Decision":"## Decision\nChosen option: \"Regular expression\", because we can use Java internal classes (Pattern, Matcher) instead of adding additional dependencies\\n### Positive Consequences\\n* Less dependencies on third party libraries\\n* Smaller project size (ICU4J is very large)\\n* No need for model data (OpenNLP is a machine learning based toolkit and needs a trained model to work properly)\\n### Negative Consequences\\n* Regular expressions can never cover every case, therefore, splitting may not be accurate for every title\\n","Prediction":"## Decision\nSplit the title into sentences using 'Sentence Splitter' application (see [link](https:\/\/cutt.ly\/CMzWg1b)) and process each sentence to ensure proper formatting using '[Sentence Case](https:\/\/en.wiktionary.org\/wiki\/sentence_case)' or '[Title Case](https:\/\/en.wiktionary.org\/wiki\/title_case#English)'\\n### Positive Consequences\\n* Each entry title will be well-formatted\\n* The title will be consistent in formatting across all entry types\\n### Negative Consequences\\n* The title will not be well-formatted in some cases (e.g. 'Who's Music? A Sociology of Musical Language')\\n### Positive Consequences:\\n* Each entry title will be well-formatted\\n* The title will be consistent in formatting across all entry types\\n### Negative Consequences:\\n* The title will not be well-formatted in some cases (e.g. 'Who's Music? A Sociology of Musical Language')\\n### Positive Consequences:\\n* Each entry title will be well-formatted\\n* The title will be consistent in formatting across all entry types\\n### Negative Consequences:\\n* The title will not be well-formatted in some cases (e.g. '"}
{"Context":"## Context\\nIf the ETL if is interrupted it will need to start over again, i.e. a 6 hour ETL is stopped in the 5th hour, restarting will take another 6 hours to finish.\\n","Decision":"## Decision\nThe ETL will be re-entrant, storing state on a regular basis.\\nRestarting the ETL will pick up from the last point.\\n","Prediction":"## Decision\nWe will use a [timeout strategy](https:\/\/en.wikipedia.org\/wiki\/Timeout_%28computer%29) of 6 hours to restart the ETL, but the user can choose to restart it sooner than 6 hours.\\nThis is done to avoid the ETL being restarted twice in a short period of time.\\nFor example, if the ETL is stopped in the 5th hour, the user can choose to restart it in the 6th hour.\\nThis is just a simple example, and if the user chooses to restart the ETL in the 6th hour, the ETL will be restarted in the 7th hour.\\nIf the ETL is stopped in the 6th hour, the user can choose to restart it in the 7th hour.\\nThis is just a simple example, and if the user chooses to restart the ETL in the 7th hour, the ETL will be restarted in the 8th hour.\\nIf the ETL is stopped in the 7th hour, the user can choose to restart it in the 8th hour.\\nThis is just a simple example, and if the user chooses to restart the ETL in the 8th hour,"}
{"Context":"## Context\\nHaving selected to run all services and applications in Docker containers we need a\\nregistry to store them in.\\n","Decision":"## Decision\nWe have decided to store the Docker containers in [DockerHub](https:\/\/hub.docker.com\/u\/nerc\/dashboard\/). Given the project is Open Source this seemed to be the easiest option\\nas most tools default to this registry.\\n","Prediction":"## Decision\nWe have decided to use Docker Hub to host the registry.\\n### Installation\\nTo install Docker Hub we will use the Helm package manager\\nhttps:\/\/helm.sh\/.\\n### Usage\\n### Using Helm to install\\nWe will use Helm to install the registry.\\n### Using Docker Compose to install\\nWe will use Docker Compose to install the registry.\\n### Using Docker to install\\nWe will use Docker to install the registry.\\n### Using Docker to install\\nWe will use Docker to install the registry.\\n### Using Docker to install\\nWe will use Docker to install the registry.\\n### Using Docker to install\\nWe will use Docker to install the registry.\\n### Using Docker to install\\nWe will use Docker to install the registry.\\n### Using Docker to install\\nWe will use Docker to install the registry.\\n### Using Docker to install\\nWe will use Docker to install the registry.\\n### Using Docker to install\\nWe will use Docker to install the registry.\\n### Using Docker to install\\nWe will use Docker to install the registry.\\n### Using Docker to install\\nWe will use Docker to install the registry.\\n### Using Docker to install\\nWe will use Docker to install the registry.\\n### Using Docker"}
{"Context":"## Context\\nThe work to ship cloud foundry platform logs to Logit was started in 2018 Q1.\\nIt was paused because some IA issues with Logit were not resolved. At one point\\nRE recommended that PaaS should host our own logstash as this part of the\\nservice was not widely available by market Elastic SaaS providers. The PaaS\\nteam was also considering to use Elasticsearch on AWS with our accounts.\\n","Decision":"## Decision\nAn updated conversation with the RE tool team has confirmed that the IA issues\\nhad been resolved, and that GDS can continue to use Logit for now.\\nIt is a GDS strategy to use a consistent logging solution. Hence, we should\\ncontinue our migration of platform logs to logit, including our logstash\\nfilters.\\nThere is considerably less maintenance work for us if we use Logit's logstash\\nfilter rather than hosting the bosh release one. In the future if GDS choose to\\nuse another vendor that do not have a hosted logstash solution, they would need\\nto provide a migration strategy for all the current logstash users.\\n","Prediction":"## Decision\nWe will host our own logstash and elastic search on the PaaS.\\nWe will use Logit to monitor the logstash and elastic search components.\\nWe will use Logit to track the logs from the cloud foundry platform.\\nWe will use Logit to monitor the logstash and elastic search components.\\nWe will use Logit to monitor the logs from the cloud foundry platform.\\nWe will use Logit to monitor the logs from the cloud foundry platform.\\nWe will use Logit to monitor the logs from the cloud foundry platform.\\nWe will use Logit to monitor the logs from the cloud foundry platform.\\nWe will use Logit to monitor the logs from the cloud foundry platform.\\nWe will use Logit to monitor the logs from the cloud foundry platform.\\nWe will use Logit to monitor the logs from the cloud foundry platform.\\nWe will use Logit to monitor the logs from the cloud foundry platform.\\nWe will use Logit to monitor the logs from the cloud foundry platform.\\nWe will use Logit to monitor the logs from the cloud foundry platform.\\nWe will use Logit to monitor the logs from the cloud foundry platform.\\nWe"}
{"Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","Prediction":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\nWe will use the [Format for Architecture Decisions](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions) as a guide.\\nWe will use the [Pivotal Labs Architecture Decision Service](https:\/\/github.com\/pivotallabs\/adr) to store our records.\\nWe will use the [Pivotal Labs Architecture Library](https:\/\/github.com\/pivotallabs\/adr-lib) to help us keep the rules for our records in sync.\\nWe will use the [Pivotal Labs Architecture Toolkit](https:\/\/github.com\/pivotallabs\/adr-lib) to help us keep our ADRs up-to-date.\\nWe will use the [Pivotal Labs Architecture Toolchain](https:\/\/github.com\/pivotallabs\/adr-lib) to help us build our ADRs.\\nWe will use the [Pivotal Labs Architecture Decision Maker](https:\/\/github.com\/pivotallabs\/adr-lib) to help us quickly build our ADRs.\\nWe"}
{"Context":"## Context\\nSpotlight uses the term \"Exhibits\" because its primary use case is generating\\nmultiple exhibit sites. However, as per\\n[ADR #2](.\/0002-build-on-spotlight.md) we want to use it as landing pages for\\ncollections.\\nThe term \"Collection\" was used everywhere else, including PUDL, so we decided to\\nuse that instead. Originally reported in\\n[#89](https:\/\/github.com\/pulibrary\/pomegranate\/issues\/89).\\n","Decision":"## Decision\n* Use \"Collection\" in the UI everywhere \"Exhibit\" is mentioned in Spotlight.\\n","Prediction":"## Decision\nWe will use the term \"Collection\" for our POMs and our Expos. As we have no\\nexplanations for why we use it over \"Exhibit\", we will use the term \"Exhibit\" in\\ntheir place.\\nWe will use the term \"Exhibit\" for all other POMs and Expos that use it. This\\nincludes all other POMs and Expos that use the word \"Exhibit\" in the title\\nand\/or description.\\nWe will use the term \"Exhibit\" for all other POMs and Expos that are not POMs\\nto be hosted on POMservers. This includes all other POMs and Expos that are\\nnot POMs to be hosted on POMservers that are used for collecting POMs and\\neXpos.\\nWe will use the term \"Exhibit\" only in the context of a collection. For\\nexample, if we wanted to create a collection that only includes POMs, we'd use\\nthe term \"Exhibit\" for that collection.\\nWe will use the term \"Exhibit\" in the POMs and Expos that are hosted on\\nPOMs"}
{"Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","Prediction":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\nWe will use [Schema.org](http:\/\/schema.org\/) for metadata about architectural decisions.\\nWe will use [Jira](https:\/\/www.atlassian.com\/software\/jira) for issue tracking.\\nWe will use the [Architecture Decision Table](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions-architecture-decision-table).\\nWe will use the [Architectural Style Guide](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions-architectural-style-guide) for style.\\nWe will use the [Architecture Style Guide](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions-architecture-style-guide) as a starting point and use it as a reference to help us write our own style guide.\\nWe will use the [Architectural Decision Table](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions-architecture-decision-table)"}
{"Context":"## Context\\nIdentifiers (the names and keys used to identify applications and handlers) must\\nbe compared by engines to determine if two such entities are to be considered\\nequivalent.\\nThe documentation specifies that such keys must be non-empty UTF-8 strings\\nconsisting of printable characters without whitespace, but it did not previously\\nspecify how such strings would be compared.\\nThese identifiers are either mostly or entirely immutable and generated as part\\nof the source code. They do not need to be parsed and validated from user input.\\n","Decision":"## Decision\nIn keeping with current behavior, we've decided to specify byte-wise comparison\\nsemantics for identifiers.\\n","Prediction":"## Decision\nWe will use the following rules to determine whether two identifiers are equal:\\n* if the strings are equal, we will return true\\n* if the strings are not equal, but one is longer, we will return a length greater than zero\\n* if the strings are equal, but one is shorter, we will return a length less than zero\\n* if the strings are equal, but one is less than the length of the other, we will return a length of zero\\n* if the strings are equal, but the first is greater than the second, we will return a length of zero\\n* if the strings are equal, but the first is less than the second, we will return a length of zero\\n* if the strings are equal, but the first is greater than the second, but the second is less than the second, we will return a length of zero\\n* if the strings are equal, but the first is less than the second, but the second is greater than the second, we will return a length of zero\\n* if the strings are equal, but the first is less than the second, but the second is less than the second, we will return a length of zero\\n* if the strings are equal, but the first"}
{"Context":"## Context\\nThe tool will have a `help` subcommand to provide documentation\\nfor users.\\nIt's nice to have usage documentation in the script files\\nthemselves, in comments.  When reading the code, that's the first\\nplace to look for information about how to run a script.\\n","Decision":"## Decision\nWrite usage documentation in comments in the source file.\\nDistinguish between documentation comments and normal comments.\\nDocumentation comments have two hash characters at the start of\\nthe line.\\nThe `adr help` command can parse comments out from the script\\nusing the standard Unix tools `grep` and `cut`.\\n","Prediction":"## Decision\nUse the `usage` subcommand to provide usage documentation for the\\nscript, rather than the `help` subcommand.  It's more convenient to\\nuse the `usage` subcommand than the `help` subcommand, because\\nusers can more easily determine which subcommand to use from the\\nusage.\\nThis is particularly important for scripts that are being used\\non a large scale.  For example, a tool that sends email messages\\nmay need to provide `help` documentation for sending email on\\ndemand, while a tool that reads email messages may not need\\nto provide `help` documentation for that.\\nThis also makes it easier to provide documentation for a script\\nthat is being used on a local machine, since the local machine\\nisn't always on the same network as the master.\\nIn the future, we may want to provide documentation for a script\\nthat is being used on a remote machine, since the remote machine\\nmay not be on the same network as the master.\\nThis decision is driven by the fact that the `help` command\\nhas a very large and complex command line, and is more likely to\\nbe used in a script that is being run on a larger scale.\\n`usage"}
{"Context":"## Context\\nCache control headers can be used to prevent a client's browser from\\nre-requesting a page that has not changed, and may be leveraged by proxies to\\nreturn the same cached pages for multiple clients.\\n","Decision":"## Decision\nCache-control headers will be added to all valid requests. 500 and 404 errors\\nwill not be cached.\\n","Prediction":"## Decision\n* Cache-Control: no-cache\\n* Pragma: no-cache\\n* Expires: no-cache\\n* Cache-Control: must-revalidate\\n* Cache-Control: max-age\\n* Cache-Control: no-store\\n* Cache-Control: no-cache\\n* Cache-Control: max-age\\n* Cache-Control: max-age\\n* Cache-Control: max-age\\n* Cache-Control: max-age\\n* Cache-Control: max-age\\n* Cache-Control: max-age\\n* Cache-Control: max-age\\n* Cache-Control: max-age\\n* Cache-Control: max-age\\n* Cache-Control: max-age\\n* Cache-Control: max-age\\n* Cache-Control: max-age\\n* Cache-Control: max-age\\n* Cache-Control: max-age\\n* Cache-Control: max-age\\n* Cache-Control: max-age\\n* Cache-Control: max-age\\n* Cache-Control: max-age\\n* Cache-Control: max-age\\n* Cache-Control: max-age\\n* Cache-Control: max-age\\n* Cache-Control: max-age\\n* Cache-Control: max-age\\n* Cache-Control: max-age\\n* Cache-Control: max-age\\n* Cache-Control: max-age\\n* Cache-Control: max-age\\n* Cache-Control: max-age\\n* Cache-Control: max-age\\n*"}
{"Context":"## Context\\nWe need to decide what data structure to use as the identifier for domain entities.\\n","Decision":"## Decision\nWe will use a [GUID](https:\/\/msdn.microsoft.com\/en-us\/library\/system.guid%28v=vs.110%29.aspx?f=255&MSPPError=-2147217396) type.\\n","Prediction":"## Decision\nThe domain entity identifier will be the `id` of the `User` entity.\\nThis is not a unique identifier, but it is a unique key. We will use the `id` as the primary key.\\nThis will allow us to easily create new entities, but also to uniquely identify existing ones.\\nFor example, we would be able to create a `Contact` entity, and then later on, a `Customer`.\\nThis is also useful for debugging, as it's not possible to create two entities of the same id.\\n### Use of the `id` as the primary key\\nThe `id` is a unique identifier, and it's used as the primary key for the domain entity.\\nIn other words, the `id` is the primary key for the `User` entity.\\nWhen creating a new entity, the `id` is used as the primary key for that entity.\\nWhen updating an existing entity, the `id` is used as the primary key for that entity.\\n### Use of the `id` as the foreign key to another domain entity\\nThe `id` is the foreign key to another domain entity.\\nIn other words, the `id` is the primary key for the"}
{"Context":"## Context\\nWe need to decide whether Dogma's examples should reside in the `dogma`\\nrepository itself, or a separate `examples` repository.\\n","Decision":"## Decision\nWe've decided to move the examples to a separate repository, so that we can\\nprovide fully-functional examples that depend on modules\/packages that we would\\nnot want to have as dependants of `dogma` itself, such as `mysql`, etc.\\n","Prediction":"## Decision\nWe will create a separate `examples` repository as per the following\\nadvice from the [Dogma docs](https:\/\/github.com\/DAK\/DOGMA\/tree\/master\/examples)\\n### Example 1: Admin Page\\nThe `admin` repository should contain examples for the `admin`\\npage.\\n### Example 2: Admin Page with a List of Objects\\nThe `admin` repository should contain examples for the `admin`\\npage with a list of objects.\\n### Example 3: Admin Page with a List of Objects and a List of\\nObjects\\nThe `admin` repository should contain examples for the `admin`\\npage with a list of objects and a list of objects.\\n### Example 4: Admin Page with a List of Objects and a List of\\nObjects and a List of Objects\\nThe `admin` repository should contain examples for the `admin`\\npage with a list of objects and a list of objects and a list of\\nobjects.\\n### Example 5: Admin Page with a List of Objects and a List of\\nObjects and a List of Objects\\nThe `admin` repository should contain examples for the `admin`\\npage with a list of objects and a list of objects and a list"}
{"Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"## Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","Prediction":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\nWe will use [The Architecture Journal](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions) as our guide.\\nWe will use [Architectures.io](http:\/\/architectures.io\/) as our source of truth.\\nWe will use [Joshua Kerievsky's Architecture Decision Tool (ADDIT)](https:\/\/github.com\/joshuakerievsky\/ADDIT) as our tool for creating ADRs.\\nWe will use [Jira](https:\/\/marketplace.atlassian.com\/plugins\/solutions\/2q5q7hng) as our issue tracker.\\nWe will use [Confluence](https:\/\/confluence.atlassian.com\/display\/AEC\/AECD) as our source of truth.\\nWe will use [Maven Archetype Repository](https:\/\/mvnrepository.com\/artifact\/org.apache.maven.architeture\/) as our source of truth.\\nWe will use [Postman](https:\/\/www.postman.com\/) as our API test client.\\"}
{"Context":"## Context\\nThe Data Submission Service is a new digital system for suppliers to submit\\nmanagement information to CCS. It replaces the current Management Information\\nSystem Online (MISO).\\nThis paper seeks approval for the overall technical approach for the new\\nservice.\\n### Management Information System Online (MISO)\\nMISO is an online service run by CCS for suppliers to submit data about work\\nundertaken through CCS commercial agreements. Suppliers submit data on a monthly\\nbasis via a complex Excel spreadsheet.\\nThe data submitted through MISO is used to calculate the management fee that is\\ncharged to suppliers. This data is also used by Commercial Agreement Managers\\n(CAMs) to help with management of suppliers and agreements as well as inform\\nfuture agreement design.\\nMISO was developed in 2011 with only minor updates since.\\nThe service frequently experiences poor performance with frequent errors. It\\ndelivers a poor user experience, and requires a large amount of manual work to\\nadd new commercial agreements and new suppliers.\\nThe number of transactions taken by MISO has increased significantly over recent\\nyears, rising from around 5000 per month in 2015, to over 8000 per month\\nin 2018. Transactions peak around G-Cloud commercial agreement boundaries (with\\nnearly 10,000 transactions in June 2017). This increase puts a further strain on\\nan already stretched service.\\n### Estate in flux\\nThe technology estate within CCS is in a state of flux. Several important parts\\nare being replaced over the next 6 months, including eSourcing (how commercial\\nagreements are established), Finance & HR and the roll-out of the Crown\\nMarketplace.\\nAdditionally, there is ongoing work to replace the CCS website, develop the\\norganisation's data strategy, technical architecture and early plans to develop\\nand improve the identity and access management services.\\n### Data Submission Service Alpha\\nThe Digital Services Team has been working on a prototyping phase to identify\\noptions for a new service to replace MISO.\\nThis phase has involved user research with suppliers and staff within CCS who\\nadminister MISO. It's looked at ways of improving processes and investigated\\npotential technology choices.\\nThe team is now ready to start producing a Minimum Viable Service with an aim to\\ntake real data submissions from suppliers later this year. The medium-term aim\\nis to switch off the existing MISO service in 2019.\\n","Decision":"## Decision\nThe new service will need to have the following characteristics:\\n- **Scalability** - the service must cope with high peaks in demand -\\nparticularly around submission deadlines. Outside of peak times, the service is\\ninfrequently used. Over time, the number of transactions is likely to grow,\\nparticularly with the introductions of G-Cloud 10, Digital Outcomes and\\nSpecialists 3 and Dynamic Purchasing System (DPS) where suppliers can leave and\\njoin a commercial agreement at any time.\\n- **Flexibility** - the service needs to collect different types of data for\\ndifferent commercial agreements. This includes invoice data, contract data,\\ndata to support decision making. Over time, CCS may need to collect different\\ntypes of data, particularly as agreements and industries change.\\n- **Adaptable** - as the CCS technical landscape changes, the service will need\\nto change along with it. For example, in future it will need to support a new\\nfinance system, and potential integration with Crown Marketplace, eSourcing and\\nthe CCS website.\\nThese characteristics lend themselves to developing a system with small pieces\\nthat can be easily iterated or replaced as requirements change over time.\\nThe new data submission service will comprise the following pieces:\\n1. **Submission app** - a public facing front-end for suppliers to submit\\nreturns. This will comprise a specially designed user-interface to make the\\njourney for suppliers as simple as possible, providing timely and useful\\nfeedback to help reduce errors, using well-tested design patterns from the\\nGOV.UK toolkit alongside the CCS branding.\\n1. **Data storage layer** - to store the data submitted to CCS. This will act as\\nthe 'core' of the new service, and the source of truth for submission data. This\\nwill be accessed via an API, allowing other CCS services to use the data as\\nrequired.\\n1. **File transformation service** - a small service to take various file inputs\\nfrom suppliers, extract required data and store it in the data storage layer.\\nThis service will need to support various file formats including custom Excel\\ndocuments, CSVs, PDFs and Open Office formats.\\n1. **Data validation service** - a small service to validate data submitted by\\nsuppliers and calculate the approximate management fee required. This will need\\nto adapt over time as different data is collected from suppliers and different\\nmethods of calculating the management fee are introduced.\\n1. **Notification service** - a small service to manage sending of notifications\\nto users. This service will automatically generate notifications, and handle\\nresponses (eg bounces, out of office replies etc), reducing the burden on the\\nCCS support staff. The service will integrate with GOV.UK Notify for sending\\nnotifications.\\n1. **Data exports** - a series of regular 'batch' data exports to provide\\nsubmission data to other systems, including the Data Warehouse and the legacy\\nfinance system.\\n1. **Administration app** - a front-end for CCS staff to support the operation\\nof the data submission service. This will comprise a customised interface to\\nmake support tasks as simple as possible, allowing staff to focus on more\\nimportant work.\\n1. **Onboarding app** - a front-end for CCS staff to manage the onboarding of\\nnew commercial agreements and suppliers. This will compromise a customised\\ninterface to allow CCS to specify commercial agreement data collection\\nrequirements, validation rules, management fee calculation formula, and handle\\nthe onboarding of new users\/suppliers.\\nEach of these components will make use of the most appropriate and simplest\\ntechnology to perform the required functions and will be integrated using well\\ndefined JSON APIs. Each piece should be easy to maintain, iterate and enhance,\\nand should also be easy to replace in future.\\nWhere possible, the service will make use of managed cloud services to provide\\nfeatures such as automatic scaling, high availability and resiliency.\\nThe service will make use of existing government and CCS services where possible\\n- for example GOV.UK Notify, the Supplier Registration Service, Digital\\nMarketplace etc.\\n","Prediction":"## Decision\nThe Data Submission Service will be replaced with a new service.\\nThis will be a modern, multi-tenant, cloud-based service which will provide a\\nreliable, consistent and secure way to submit data to CCS.\\nThe new service will provide the following key features:\\n* A single sign-on (SSO) experience for suppliers using MISO with a\\nreliable, consistent and secure service\\n* A single sign-on (SSO) experience for CAMs and their suppliers\\n* A single sign-on (SSO) experience for the central data warehouse\\n* A single sign-on (SSO) experience for the central services data\\nwarehouse\\n* A single sign-on (SSO) experience for the central procurement data\\nwarehouse\\n* A single sign-on (SSO) experience for the central contracts data\\nwarehouse\\n* A single sign-on (SSO) experience for the central contracts and supplier\\nmanagement data\\n* A single sign-on (SSO) experience for the central contract and supplier\\nteaming data\\n* A single sign-on (SSO) experience for the central contractor data\\n* A single sign-on (SSO) experience for the central contact details\\n* A single sign-on (SSO) experience"}
{"Context":"## Context\\nTo perform asynchronous operations we use [Azure Storage Queues](https:\/\/azure.microsoft.com\/en-us\/services\/storage\/queues\/):\\nenqueued messages are processed by the methods exposed by Azure Functions using [triggers](https:\/\/docs.microsoft.com\/en-us\/azure\/azure-functions\/functions-triggers-bindings).\\nIn case some error occurs in the message handler, it is necessary to re-schedule the message in the queue\\nto be processed again at a later time.\\nOur goal is to re-schedule the processing at increasing intervals, using an exponential back off strategy for retries.\\nThe default behavior is specified in the [triggers documentation](https:\/\/docs.microsoft.com\/en-us\/azure\/azure-functions\/functions-bindings-storage-queue#trigger---poison-messages):\\n> When a queue trigger function fails, the Azure functions retry the function up to five times for a given queued message, including the first attempt.\\n> If all five attempts fail, the runtime> functions adds a message to a queue called <originalqueuename> -poison.\\nCurrently the Function SDK does not provide an *output binding* to delay the processing of an enqueued message.\\nIt is therefore necessary to implement a custom retry policy to schedule messages over a longer period of time.\\nThe following possibilities, to delay message processing, have been considered:\\n1. Switching from Storage Queues to [ServiceBus](https:\/\/docs.microsoft.com\/en-us\/azure\/service-bus\/):\\nthat allows to schedule messages at specified point in time bue [there are some known issues](https:\/\/github.com\/Azure\/Azure-Functions\/issues\/454#issuecomment-324785962) that suggest to avoid this approach. Moreover the ServiceBus is more expensive\\nand has less storage capability than Storage Queues.\\n1. Using [Durable Functions](https:\/\/docs.microsoft.com\/en-us\/azure\/azure-functions\/durable-functions-overview#language-support): this is [only supported\\nin .NET environment](https:\/\/github.com\/Azure\/azure-functions-durable-extension\/issues\/13#issuecomment-359448481).\\n1. Using a [Custom Queue Processor](https:\/\/stackoverflow.com\/questions\/26937782\/queuetrigger-attribute-visibility-timeout): this is only supported in .NET environments and is not documented.\\n1. Update the *visibilityTimeout* parameter in enqueued messages (https:\/\/github.com\/Azure\/azure-webjobs-sdk\/issues\/1040)\\nAt the time of this writing Functions don't provide an [output binding to set the `visibilityTimeout`](https:\/\/github.com\/Azure\/azure-webjobs-sdk\/issues\/1040)\\non one individual message, but only a global setting in the [host.json file](https:\/\/docs.microsoft.com\/en-us\/azure\/azure-functions\/functions-host-json).\\n","Decision":"## Decision\nTo manage processing errors, we decided to update `visibilityTimeout` value using the methods provided by the\\n[Azure Storage SDK](https:\/\/github.com\/Azure\/azure-storage-node) and re-enqueue the messages that caused the error.\\nThe implemented strategy, that uses the [SDK method `updateMessage()`](http:\/\/azure.github.io\/azure-storage-node\/QueueService.html#updateMessage__anchor), lets us re-schedule messages with an exponential back off and a maximum time to live of 7 days.\\n","Prediction":"## Decision\nWe will switch to using a [Durable Function](https:\/\/docs.microsoft.com\/en-us\/azure\/azure-functions\/durable\/functions-overview#language-support) to schedule messages over a longer period of time.\\nThis will allow us to delay message processing until the host is ready.\\nWe will use a [Custom Queue Processor](https:\/\/stackoverflow.com\/questions\/26937782\/queuetrigger-attribute-visibility-timeout) to schedule messages over a longer period of time.\\nThis will allow us to delay message processing until the host is ready.\\nWe will use a [Custom Queue Processor](https:\/\/stackoverflow.com\/questions\/26937782\/queuetrigger-attribute-visibility-timeout) to schedule messages over a longer period of time.\\nThis will allow us to delay message processing until the host is ready.\\nWe will use a [Custom Queue Processor](https:\/\/stackoverflow.com\/questions\/26937782\/queuetrigger-attribute-visibility-timeout) to schedule messages over a longer period of time.\\nThis will allow us to delay message processing until the host is ready.\\nWe will use a [Custom Queue Processor](https:\/\/stackoverflow.com\/questions\/26937782\/queuetrigger-attribute-visibility"}
{"Context":"## Context\\nSee also ADR 0002 \"Database migrations are standalone releases\"\\nAs our system is designed for zero downtime, we have to be careful that\\nwe don't change the database in a way that causes production issues\\n","Decision":"## Decision\nWhere possible, we should avoid database migrations that will lock the database\\nfor any significant amount of time.  This is hard to enforce, but we will\\nmake sure there is documentation in the project README (and here!) on ways\\nto achieve this.\\nThis mostly affects index creation and changes - we have several years of data\\nin our database, and adding or changing indexes can be slow.  In general,\\nyou should use the `CREATE INDEX CONCURRENTLY` option to let indexes be\\ncreated in a non-blocking way.  See https:\/\/www.postgresql.org\/docs\/current\/static\/sql-createindex.html\\nIf you want to `ALTER INDEX` or `REINDEX`, they can't be concurrent - in this\\ncase you'll need to look at stopping the Event Recorder lambda, allowing messages\\nto queue up while the index change is made.  *BEWARE* however that SQS queues\\nonly allow 100,000 messages, and at peak load we have historically sent 75,000\\nmessages an hour, so you have a somewhat limited amount of time to run such a change.\\nIf you have a very complex change, you should consider:\\n- Dropping the index then running `CREATE INDEX CONCURRENTLY` rather than\\naltering indexes - generally our reports run intermittently, so it is safe to have\\nno indexes for a period of time, data will still be appended with no problems\\n- Performance testing the change - we have a large fake dataset available that\\ncan be used to simulate a production database in a test environment\\n- Duplicating the database - you could apply the change to a new database containing\\na copy of production data, then switch databases, and migrate any missed changes\\nfrom the old database to the new.\\n### Transactional DDL changes\\nMost Postgresql schema changes can be made transactionally - this is\\na great feature, as it allows for making multiple changes and having them\\nall roll back if something goes wrong.  For example:\\n```\\nBEGIN;\\nALTER TABLE fizzbuzz RENAME COLUMN foo TO bar;\\nUPDATE TABLE fizzbuzz set foo = 'splat';\\nCOMMIT;\\n```\\nIn this case the `UPDATE` will fail, so the column rename will be reverted.\\n*However* note that `CREATE INDEX CONCURRENTLY` does not work in a transaction -\\nit depends on being able to change the table incrementally, which doesn't fit\\nthe transaction model.  If the index creation fails, you are recommended to\\ndrop the index and re-create it, as it won't be rolled back and may be\\npartially created.\\n### Avoiding blocking changes\\nThere is a useful table in [this article](https:\/\/www.citusdata.com\/blog\/2018\/02\/15\/when-postgresql-blocks\/) which I've reproduced below (as the article may disappear):\\n| Runs concurrently with           | SELECT | INSERT UPDATE DELETE | CREATE INDEX CONC VACUUM ANALYZE | CREATE INDEX | CREATE TRIGGER | ALTER TABLE DROP TABLE TRUNCATE VACUUM FULL |\\n|----------------------------------|--------|----------------------|----------------------------------|--------------|----------------|---------------------------------------------|\\n| SELECT                           | Y      | Y                    | Y                                | Y            | Y              | No                                          |\\n| INSERT UPDATE DELETE             | Y      | Y                    | Y                                | No           | No             | No                                          |\\n| CREATE INDEX CONC VACUUM ANALYZE | Y      | Y                    | No                               | No           | No             | No                                          |\\n| CREATE INDEX                     | Y      | No                   | No                               | Y            | No             | No                                          |\\n| CREATE TRIGGER                   | Y      | No                   | No                               | No           | No             | No                                          |\\n| ALTER TABLE etc                  | No     | No                   | No                               | No           | No             | No                                          |\\nOur reports should only use SELECT so most operations won't block them.\\nThe Event Recorder however needs to insert data, so you can't run\\nany of the index modification changes (apart from `CREATE INDEX CONCURRENTLY`) without risking blocking.\\nNote however that some changes may be fast enough despite blocking - adding a column\\nfor example.  However you should performance test these changes against a\\nproduction-sized database to be sure!\\n","Prediction":"## Decision\nWe will use the [sequelize-migrate](https:\/\/github.com\/Sequelize\/sequelize-migrate) npm package to perform database migrations.\\nSee also ADR 0003 \"Migrations are standalone releases\"\\nThis is the recommended way to do database migrations, as it allows you to create\\na new migration and roll back to a previous version of a database, without having to create a new production\\ndeployment. It also allows you to do things like create a new migration that runs before a migration that\\nthrows an error, and then rollback to the previous version of the database.\\nSee also ADR 0004 \"Migrations are not standalone releases\"\\nIf you want to roll back to a previous version of a database, you must create a production\\ndeployment first, and then run `sequelize migrate` from the root of your project.\\nSee also ADR 0005 \"Rolling back migrations is not recommended\"\\n### Migrations are not standalone releases\\nMigrations are only used to create new database tables and alter existing ones. If you want to create a new\\ndatabase table, you must first create a migration that does that, and then run `sequelize migrate` from the root of your project.\\nSee also"}
{"Context":"## Context\\nTeaching Vacancies will soon start to develop a new set of features which ultimately will allow job seekers to apply for jobs directly on the TVS website.\\nThis feature constitutes an important milestone for the service and will require job seekers users to create accounts to manage their job applications. Account functionality will permit teachers to initiate web sessions on TVS website as well as manage their account (password resets, email verification and account closing).\\n","Decision":"## Decision\nTVS will adopt Devise as authentication library.\\n","Prediction":"## Decision\nTVS will implement the following functionality in the new account creation and web session management:\\n- Enable users to create a new account with a unique username and password\\n- Allow users to update their existing account details (i.e. email address, phone number, address, etc) with the intention of creating a new account on the TVS website\\n- Enable users to close their account and remove their details from the TVS website\\n- Support for the use of a single email address for multiple accounts\\n- Support for the use of multiple phone numbers for multiple accounts\\n- Support for the use of multiple addresses for multiple accounts\\n- Support for the use of multiple addresses for a single user\\n- Support for the use of multiple email addresses for a single user\\n- Support for the use of a single phone number for a single user\\n- Support for the use of multiple addresses for a single user\\n- Support for the use of multiple email addresses for a single user\\n- Support for the use of multiple phone numbers for a single user\\n- Support for the use of multiple addresses for a single user\\n- Support for the use of multiple email addresses for a single user\\n- Support for the use of multiple phone numbers for a single user\\n- Support for"}
{"Context":"## Context\\nRead [Event Bus ADR](0037-eventbus.md) for context.\\nGiven several James servers, we need them to share a common EventBus.\\nThis:\\n- Ensures a better load balancing for `group mailbox listners`.\\n- Is required for correctness of notifications (like IMAP IDLE).\\n","Decision":"## Decision\nProvide a distributed implementation of the EventBus leveraging RabbitMQ.\\nEvents are emitted to a single Exchange.\\nEach group will have a corresponding queue, bound to the main exchange, with a default routing key. Each eventBus\\nwill consume this queue and execute the relevant listener, ensuring at least once execution at the cluster level.\\nRetries are managed via a dedicated exchange for each group: as we need to count retries, the message headers need to\\nbe altered and we cannot rely on rabbitMQ build in retries. Each time the execution fails locally, a new event is emitted\\nvia the dedicated exchange, and the original event is acknowledged.\\nEach eventBus will have a dedicated exclusive queue, bound to the main exchange with the `registrationKeys` used by local\\nnotification mailboxListeners (to only receive the corresponding subset of events). Errors are not retried for\\nnotifications, failures are not persisted within `DeadLetter`, achieving at most once event delivery.\\n","Prediction":"## Decision\nIn order to achieve the first goal, we use an `EventBus` with a shared `Queue` in the James `Connector`.\\nWe use a `JMS` connection factory to connect each James server to the same `JMS Topic`.\\nThis way, we can ensure that the `EventBus` is always up and running on all James servers.\\nWe then use a `JMS Topic` to connect each `EventBus` to each James server.\\nWe use a `JMS` connection factory to connect each James server to the same JMS Topic.\\nWe then use a `JMS Connection` to connect each `EventBus` to each JMS Connection.\\nThis way, we can ensure that the `EventBus` is always up and running on each James server.\\nWe then use a `JMS Connection` to connect each `EventBus` to each James server.\\nThis way, we can ensure that the `EventBus` is always up and running on each James server.\\nWe then use a `JMS Connection` to connect each `EventBus` to each James server.\\nThis way, we can ensure that the `EventBus` is always up and running on each James"}
{"Context":"## Context\\nA [proposal](https:\/\/lists.cloudfoundry.org\/g\/cf-dev\/topic\/75070528#9059) was\\nmade on the cf-dev mailing list to rename our `master` branch to `main` to make\\nthe CF community a more welcoming and inclusive environment.\\n","Decision":"## Decision\nWe are going to rename our `master` branch to `develop`. `develop` was chosen\\ninstead of `main` by team consensus because it better describes the use of the\\nbranch.\\n","Prediction":"## Decision\nWe will rename our `master` branch to `main` in order to:\\n* enable cross team collaboration\\n* provide a more clear and consistent name for the branch\\n* promote a more inclusive and welcoming environment\\nWe will rename our `master` branch to `main` in order to:\\n* enable cross team collaboration\\n* provide a more clear and consistent name for the branch\\n* promote a more inclusive and welcoming environment\\nWe will rename our `master` branch to `main` in order to:\\n* enable cross team collaboration\\n* provide a more clear and consistent name for the branch\\n* promote a more inclusive and welcoming environment\\nWe will rename our `master` branch to `main` in order to:\\n* enable cross team collaboration\\n* provide a more clear and consistent name for the branch\\n* promote a more inclusive and welcoming environment\\nWe will rename our `master` branch to `main` in order to:\\n* enable cross team collaboration\\n* provide a more clear and consistent name for the branch\\n* promote a more inclusive and welcoming environment\\nWe will rename our `master` branch to `main` in order to:\\n* enable cross team collaboration\\n* provide a more clear and consistent name for the"}
{"Context":"## Context\\nThere are slight inconsistencies in naming between existing op class definitions and factory methods. For example a\\nfactory method called `bernoulli` in the `random` namespace with a corresponding op class called\\n`BernoulliDistribution`.\\nTwo possible solutions where suggested:\\n1. Add an additional property that provides us with the correct class name\\n2. Rename classes in ND4J to ensure consistency and provide backwards compatibility via deprecated subclasses\\n","Decision":"## Decision\nFor now we will introduce a `javaOpClass` property which in cases of inconsistency provides us with the correct class\\nname.\\n","Prediction":"## Decision\nWe will rename classes in ND4J to ensure consistency and provide backwards compatibility via deprecated\\nsubclasses.\\nWe will not rename classes in ND4J to remove the ambiguity that exists at present.\\nWe will not move classes from the `random` namespace to the `op` namespace.\\nWe will provide a `@Deprecated` annotation for classes that are deprecated and will be removed in the future.\\nWe will provide a `@Deprecated` annotation for classes that are deprecated and will be removed in the future.\\nWe will provide a `@Deprecated` annotation for classes that are deprecated and will be removed in the future.\\nWe will provide a `@Deprecated` annotation for classes that are deprecated and will be removed in the future.\\nWe will provide a `@Deprecated` annotation for classes that are deprecated and will be removed in the future.\\nWe will provide a `@Deprecated` annotation for classes that are deprecated and will be removed in the future.\\nWe will provide a `@Deprecated` annotation for classes that are deprecated and will be removed in the future.\\nWe will provide a `@Deprecated` annotation for classes that are deprecated and will be removed in the future.\\nWe will provide a `@Deprecated` annotation"}
{"Context":"## Context\\nTo quickly check database connection and created objects as a result of Northwind database reverse engineering.\\n","Decision":"## Decision\nSimple call from handler to DbContext is used without any middle layer.\\n","Prediction":"## Decision\nWe will use the [dotConnect for SQL Server](https:\/\/www.microsoft.com\/en-us\/download\/details.aspx?id=43189) library to allow us to connect to the database and create the objects.\\nWe will use the [dotConnect for SQL Server](https:\/\/www.microsoft.com\/en-us\/download\/details.aspx?id=43189) client library to allow us to execute queries on the database.\\nWe will use the [dotConnect for SQL Server](https:\/\/www.microsoft.com\/en-us\/download\/details.aspx?id=43189) Entity Framework provider to allow us to use the `DbContext` class to define and use the stored procedures.\\nWe will use the [dotConnect for SQL Server](https:\/\/www.microsoft.com\/en-us\/download\/details.aspx?id=43189) Entity Framework provider to allow us to use the `DbContext` class to define and use the stored procedures.\\nWe will use the [dotConnect for SQL Server](https:\/\/www.microsoft.com\/en-us\/download\/details.aspx?id=43189) Entity Framework provider to allow us to use the `dbContext` property to easily access the context.\\nWe will use the [dotConnect for SQL Server](https:\/\/www.microsoft.com\/en-us\/download\/details.aspx?id=43189) Entity Framework provider to allow"}
{"Context":"## Context\\nWe need a platform that can help us with the following requirements:\\n- Cross-platform connection to the input form: Information collection can happen through phone or tablet in real time or be entered into manually through a computer at a later stage. The database we use should be able to connect to all input media.\\n- Collate information of all types (text, videos, photographs, etc): When making observations out at sea, researchers capture pictures and videos along with textual information such as weather conditions and distance. Being able to bring together different types of data will greatly reduce post-collection maintenance efforts.\\n- Work around intermittent internet connection: Given the environmental conditions on a boat in the middle of the ocean, consistent internet connection is not a given. Thus, our tech stack must provide offline capabilities with minimal-to-no data loss.\\n- Inexpensive scalability: As they move into different forms of information collection such as drones, it is possible that the size of data becomes exponentially bigger than it is today. Thus, the datastore should not only be able to handle storage of that data but also does not put a cost overhead on BMMRO.\\n","Decision":"## Decision\nFirebase is a mobile and web application development platform that is currently owned and maintained by Google.\\nWe've decided to use Firebase because it offers a variety of services under the Google ecosystem that can be leveraged including hosting, databases, authentication, cloud storage, analytics, etc. along with beta features such as translate and email trigger. It also has a free plan that seems to be enough for our needs.\\n","Prediction":"## Decision\nWe will use [Kafka](https:\/\/kafka.apache.org\/) as our messaging platform. It is a distributed, highly available, reliable message broker that supports reliable message delivery, fault-tolerance, and high throughput. It supports structured data (like the ones we have on this platform) and unstructured data (like images and videos) and can be easily integrated with other technologies such as [Kafka Connect](https:\/\/kafka.apache.org\/kafka-connect.html) and [Kafka Streams](https:\/\/kafka.apache.org\/kafka-streams.html).\\nKafka will allow us to connect to the database on a per-user basis and will be able to scale up and down as needed. It will also be able to handle the large amounts of data that we expect to collect. It will also provide a low latency connection to the database so that we can have real-time data collection from the field.\\nKafka will also be able to connect to other technologies such as [Kafka Connect](https:\/\/kafka.apache.org\/kafka-connect.html) and [Kafka Streams](https:\/\/kafka.apache.org\/kafka-streams.html).\\nKafka Streams will be used to connect to the database and will also act as a bridge between the database and"}
{"Context":"## Context\\nMesh Configuration Protocol (MCP) is a [protocol](https:\/\/github.com\/istio\/api\/tree\/master\/mcp) for transferring configuration among Istio components during runtime. MCP is meant to defer all the logics and complexities back to the server (copilot) as oppose to the original design which all the logic was embeded in the client (Pilot). Another goal of MCP is to create a unified contract for all the Custom Resource Definitions and Service Discovery and the way they are communicated with Pilot.\\n","Decision":"## Decision\nCopilot will implement a MCP server to send configuration to Pilot. We will be sending definitions for Gateways, VirtualServices and DestinationRules over bi-directional GRPC.\\n","Prediction":"## Decision\nWe have decided to adopt the [MCP](https:\/\/github.com\/istio\/mcp) protocol for the Mesh Configuration Protocol.\\nWe have also decided to add support for the `MeshConfigProto` to the `ServiceDefinitionProto`.\\nWe have decided to use the `MeshConfigProto` to communicate with pilot during runtime.\\nWe have decided to use the `MeshConfigProto` to communicate with the server during runtime.\\nWe are also considering to add support for the `MeshConfigProto` to the `ServiceDefinitionProto`.\\nWe are also considering to add support for the `MeshConfigProto` to the `ServiceDefinitionProto`.\\nWe are also considering to add support for the `MeshConfigProto` to the `ServiceDefinitionProto`.\\nWe have decided to use the `MeshConfigProto` to communicate with pilot during runtime.\\nWe have decided to use the `MeshConfigProto` to communicate with the server during runtime.\\nWe have decided to use the `MeshConfigProto` to communicate with the server during runtime.\\nWe have decided to use the `MeshConfigProto` to communicate with the server during runtime.\\nWe have decided to use the `MeshConfigProto` to communicate with the"}
{"Context":"## Context\\nGoals:\\n* Use a set of microservices to deploy to the platform\\nI have invested some time to learn about microservices as I was not very\\nconscious about what implies today and what is different between languages.\\nI have coded Python and Springboot apps to see which one will give me better\\nfit in my example here.\\nSpring has several components to create microservices that can be really\\nuseful, like:\\n- eureka server: as discovery server\\n- eureka client: a library for self-registering into eureka server\\n- feing rest client: a powerful but simple REST client\\n- zuul gateway: a reverse proxy application that performs as api gateway\\n- config server: a configuration management server for spring cloud apps.\\n","Decision":"## Decision\nI have added a small set of SpringBoot microservices to help me work on the\\ntest.\\nThis example creates 4 small microservices:\\n* servicio-productos: models a product and provides a REST interface\\n* servicio-items: consume products, ask for quantities and models an item, exposes data as REST interface\\n* servicio-eureka-server: discovery service from netflix where all apps can self-register and find for other services addresses.\\n* servicio-zuul-server: a reverse-proxy to access the item and product APIs in a common way.\\n* servicio-config-server: a configuration management server configured against a git\\nrepository to set the variables used by the applications.\\n","Prediction":"## Decision\nI will use Springboot to deploy and manage microservices on the platform\\nI will use Zuul to create reverse proxies on top of the services I have\\ndeployed.\\nI will use Spring Cloud Data Flow as a way to manage data flows between\\nservices.\\nI will use Spring Cloud Data Flow as a way to enable data flows between\\nservices.\\nI will use Spring Cloud Stream to enable data flows between services.\\nI will use Spring Cloud Stream to enable data flows between services.\\nI will use Spring Cloud Stream to enable data flows between services.\\nI will use Spring Cloud Stream to enable data flows between services.\\nI will use Spring Cloud Stream to enable data flows between services.\\nI will use Spring Cloud Stream to enable data flows between services.\\nI will use Spring Cloud Stream to enable data flows between services.\\nI will use Spring Cloud Stream to enable data flows between services.\\nI will use Spring Cloud Stream to enable data flows between services.\\nI will use Spring Cloud Stream to enable data flows between services.\\nI will use Spring Cloud Stream to enable data flows between services.\\nI will use Spring Cloud Stream to enable data flows between services.\\nI will use Spring Cloud Stream to enable data flows between"}
{"Context":"## Context\\nTo perform Continual Integration and development, with weekly releases, it would be convenient and useful to have a testing branch as well. Accidental pull requests into the main branch may introduce features that have not been tested from the interfac\/front-end. It is difficult to automate these front-end interface tests, and there may be factors not present in a localhost\/express server that only become apparent in an online scanario.\\nThe use of **master** branch as the release branch is useful, as 'master' is usually the most protected on GitHub, with the most warnings about deleting, modifying, etc.\\nCode reviews ar essential from all developers, to become familiar with each other's code, and to learn about javascript, and web-development. THis way we all learn from each other, and also learn good review and communicaton practice.\\n","Decision":"## Decision\n**master** will be the release branch\\n**development** will be the main development\/test branch. This will also be made into the \"default\" branch for all pull requests, to avoid accidentaly PR into master\\n**feature** branches must be made off development, with unique names. All pull requests for completed features to be made into \"development\".\\n* All PRs must be reviewed by at least two developers to merge into \"development\"\\n* All PRs must be reviewed by at the three other developers to merge into \"master\"\\n* All PRs must pass all tests (Jest, Travis, and Coveralls) in order to be considered valid for a merge\\n* Stale reviews will be automatically dismissed if a new commit is pushed to the same branch\\n* Accepted PRs for completed features (User Stories) should be deleted after sucessfully merging\\n","Prediction":"## Decision\nUse the **master** branch as the release branch. This will allow us to test the release branch without having to push a pull request to the main branch. We can then use the **master** branch as the release branch for all future releases.\\nWhen a pull request is merged into the master branch, it will automatically be merged to the release branch. This will allow us to quickly test the release branch without needing to push a pull request to the main branch.\\nWe will use **yarn** to manage the deployment of the released code to the server. This will allow us to easily run the release tests in the browser, and deploy the release to the server.\\nWe will use **yarn test** to run the front-end tests on the server, so that we can quickly test the release without needing to push a pull request to the main branch.\\nWe will use **yarn test** to run the back-end tests on the server, so that we can quickly test the release without needing to push a pull request to the main branch.\\nWe will use **yarn test** to run the front-end tests in the browser, so that we can quickly test the release without needing to push a pull request to the main branch.\\"}
{"Context":"## Context and Problem Statement\\nThe backend needs to serialize the REST resources before sending them via http to the client.\\n## Decision Drivers\\n- the format should be REST\/[HATEOAS](https:\/\/spring.io\/understanding\/HATEOAS) compatible (e.g. allow hyperlinks)\\n- the format should be easy to parse\/serialize for the backend and the client\\n","Decision":"## Decision\n- the format should be REST\/[HATEOAS](https:\/\/spring.io\/understanding\/HATEOAS) compatible (e.g. allow hyperlinks)\\n- the format should be easy to parse\/serialize for the backend and the client\\nWe will use json+hal (or the spring implementation for json with hyperlinks) without the [`_embedded`](https:\/\/tools.ietf.org\/html\/draft-kelly-json-hal-08#section-4.1.2) attribute (because of its complexity to implement correctly).\\n```eval_rst\\n.. seealso::\\n* :doc:`0010-rest-api-design`\\n* `Richardson Maturity Model <https:\/\/martinfowler.com\/articles\/richardsonMaturityModel.html>`_\\n* `HATEOAS is for Humans <http:\/\/intercoolerjs.org\/2016\/05\/08\/hatoeas-is-for-humans.html>`_\\n```\\n","Prediction":"## Decision\n- the format should be REST\/[HATEOAS](https:\/\/spring.io\/understanding\/HATEOAS) compatible (e.g. allow hyperlinks)\\n- the format should be easy to parse\/serialize for the backend and the client\\nChosen option: \"RESTpath\", because it is the only option that allows hyperlinks to the resources.\\n### Positive Consequences\\n* the backend can serialize the resources in a different format (e.g. JSON)\\n* the client can parse the resources in a different format (e.g. JSON)\\n### Negative Consequences\\n* the client needs to know the format for the backend\\n* the client needs to know the format for the backend\\n* the client needs to know the format for the backend\\n* the client needs to know the format for the backend\\n* the client needs to know the format for the backend\\n* the client needs to know the format for the backend\\n* the client needs to know the format for the backend\\n* the client needs to know the format for the backend\\n* the client needs to know the format for the backend\\n* the client needs to know the format for the backend\\n* the client needs to know the format for the backend\\n*"}
{"Context":"## Context\\nWe need to define how to save and query project, subproject and workflow data efficiently.\\n### Definition of terms\\n- Project: Is the root element of the hierarchy. It contains metadata and Subprojects\\n- Subproject: Are a subset of Projects and collect a number of Workflowitems\\n- Workflowitems: Are a number of steps which need to be executed in a certain order\\n- Resources: Projects, Subprojects, Workflowitems are resources. This means they contain permissions, a log and metadata.\\n### Relations\\n```mermaid\\ngraph TD;\\nProject-->Subproject_A;\\nProject-->Subproject_B;\\nSubproject_A-->Workflowitem_A;\\nSubproject_A-->Workflowitem_B;\\nSubproject_B-->Workflowitem_C;\\nSubproject_B-->Workflowitem_D;\\n```\\n### Options\\n1. Each resource has its own stream. Each resource has an array which links to the underlying resources (e.g. A Projects has an array of the ids of its Subrojects). That means if we want to query Subprojects of a specific Project, we need to lookup the Subproject ID's inside the projects to afterswards fetch all the underlying Subproject streams. **Pro:** We are only fetching the data which is necessary, changes inside the resources are storage efficent. **Con:** Resources with a high number of relations result in multiple (maybe hundreds) of queries.\\n2. Each Project has its own stream. All relational resources of the projects are stored inside a JSON and resolved on the API Layer. **Pro:** Only one query necessary to resolve the whole resource tree for one project. **Con:** Large amount of storage needed, since every change inside a single resource would trigger a whole copy of the project tree. Large datastructures have to be kept in memory\\n3. Each project has its own stream. All relational resources are organized with multiple key-items inside the project stream. This allows to specifically query resources with a single query. Updates to resources will only result in the copy of the affected resource. **Pro:** Easy to query, low storage \/ memory footprint **Con:** None\\n","Decision":"## Decision\nSince we need to be able to scale out to larger projects, it is important to be able to execute fast queries and having a low storage \/ memory footprint. The goal is to exploit the Multichain API as much as possible, since we can expect it to be more performant than our API. Option 3 is the option, which covers the required capabilities in the best way.\\n","Prediction":"## Decision\nWe will use the following approach:\\n* Each resource will be stored with a stream which contains the following fields:\\n- Resource ID\\n- Resource Name\\n- Resource Type (e.g. WorkflowItem, Subproject)\\n- Resource Description\\n* Each resource will have a stream which contains the following fields:\\n- Resource ID\\n- Resource Name\\n- Resource Type (e.g. WorkflowItem, Subproject)\\n- Resource Description\\n* Each project will have a stream which contains the following fields:\\n- Project ID\\n- Project Name\\n- Project Description\\n- Project Stream\\n* Each project will have a stream which contains the following fields:\\n- Project ID\\n- Project Name\\n- Project Description\\n- Project Stream\\n* Each subproject will have a stream which contains the following fields:\\n- Subproject ID\\n- Subproject Name\\n- Subproject Description\\n- Subproject Stream\\n* Each workflowitem will have a stream which contains the following fields:\\n- WorkflowItem ID\\n- WorkflowItem Name\\n- WorkflowItem Description\\n- WorkflowItem Stream\\n* Each workflowitem will have a stream which contains the following fields:\\n- WorkflowItem ID\\n- WorkflowItem Name\\n- WorkflowItem Description\\n- WorkflowItem"}
{"Context":"## Context\\nBoth link-admin and link-user need some way to communicate with the link-api.  Instead of re-inventing the wheel, we've decided to use an off the shelf solution to manage requests to our back end.\\n### Which Library?\\nWe have found a few options which look well supported and have many of the features we're looking for.  We are optimizing for compatibility, package size, and feature set.\\nIn no particular order, they are Fetch, Axios, Superagent, and Apollo Link Rest.\\n#### Fetch\\nhttps:\/\/developer.mozilla.org\/en-US\/docs\/Web\/API\/Fetch_API\\nFetch is a promise based library that is considered a new developing web standard.  It exists in two formats:  the official standard package `fetch`, and a polyfill implementation from Github called `whatwg-fetch`.  Its feature set is relatively limited. While it does support promises, request cancellation, and blob upload, it doesn't feature request interception or data transformation.  It seems like failed requests also fail silently.\\nBecause fetch has native support on most desktop browsers, [compabitility is very good](https:\/\/github.com\/github\/fetch#browser-support).  The Github polyfill implementation should allow it to be compatibile with most mobile browsers.\\nLastly, with `whatwg-fetch`'s [package size of 7.9kB](https:\/\/bundlephobia.com\/result?p=whatwg-fetch@3.0.0), it is extremely light weight.\\n#### Axios\\nhttps:\/\/github.com\/axios\/axios\\nAxios is an enormously popular open source HTTP client built upon Fetch.  It seems to support all of the same features as Fetch plus some, including request interception and canceling, automatic transforms, and XSRF protection.\\nOfficial browser compatibility is [on par with Fetch](https:\/\/github.com\/axios\/axios#browser-support) for desktop browsers, but documentation on mobile browsers is very difficult to find.  There is no polyfill required for Axios.\\nAxios' package size [sits around 12kB](https:\/\/bundlephobia.com\/result?p=axios@0.18.0), making it good enough for our use in Link.\\n#### Superagent\\nhttps:\/\/github.com\/visionmedia\/superagent\\nSuperagent is another very popular open source HTTP client.  It has an extensive set of plugins supporting a wide range of features from caching to request throttling.  Multipart requests, request retry, and native auth support (which we may not actually need) set it apart from many of the other libraries under consideration.\\nAlthough [the compatibility section](https:\/\/github.com\/visionmedia\/superagent#supported-browsers-and-node-versions) is sparse, it quotes Firefox, Chrome, Safari and IE10 with no polyfill as well as Android and iPhone support.  IE9 is supported with polyfill.\\nSuperagent has a slightly larger [package size of 16.8kB](https:\/\/bundlephobia.com\/result?p=superagent@4.1.0).\\n#### Apollo Link Rest\\nApollo Link Rest incorporates REST features into the well known GraphQL client Apollo.  The Link Rest portion allows the use of GraphQL queries in Javascript while still having a REST server while bringing with many of the advantages of the original Apollo client.  This includes a variety of benefits such as request caching, data normalization, easy state management, and seamless integration with React.\\nBrowser compatiblity information it difficult to find for Apollo, but it sounds like it's compatibile with all major browsers including IE11 with a polyfill.  Mobile support is unknown.\\nApollo Link Rest has a [package size of 13kB](https:\/\/bundlephobia.com\/result?p=apollo-link-rest@0.7.0), but requires `apollo-link` and `graphql` as peer dependencies, which brings package size to around 200kB.\\n","Decision":"## Decision\nThe real debate is between Github's `whatwg-fetch` Fetch polyfill and Axios.  While `whatwg-fetch` offers the widest possible range of compatible browsers, I think that the Axios provides us with enough support while offering a more rich feature set and a better developer API.\\nI propose we use Axios as our Javascript HTTP client.\\n","Prediction":"## Decision\nWe will use Axios as our HTTP client.  Fetch and Superagent have been considered but not found to meet the needs of this project.\\nFetch is useful, but it requires a polyfill.  Superagent is more popular and has a much larger package size.\\nApollo Link Rest seems likely to be the best option for our needs, but it requires us to use Apollo as a client (which we won't be using in this project).  The polyfill is still under active development, but it seems to be stable and documented well.\\nFetch is the most widely supported library, but it has the smallest package size, and the polyfill is still being actively developed.  Superagent seems to be the most popular HTTP client, but it has a larger polyfill size.\\nFetch is the most widely supported HTTP client, but it has a larger polyfill size.\\nFetch is the most widely supported HTTP client, but it has a larger polyfill size.\\nFetch is the most widely supported HTTP client, but it has a larger polyfill size.\\nFetch is the most widely supported HTTP client, but it has a larger polyfill size.\\nFetch is the most widely supported HTTP client, but it has a larger polyfill size.\\"}
{"Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"## Decision\nWe will use Architecture Decision Records (ADR), as described by Michael Nygard in this article: <http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions>.\\n","Prediction":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\nWe will use [Zero Architecture](https:\/\/github.com\/ZeroArchitecture\/Zero-Architecture) to handle architecture decisions.\\nWe will use [Adopting Architecture Decision Records](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions#architecture-decision-records).\\nWe will use [Lighthouse](https:\/\/github.com\/lighthouseapp\/lighthouse) to manage our architecture decisions.\\nWe will use [Pivotal Tracker](https:\/\/www.pivotaltracker.com\/), an open source, hosted, versioned, and extensible architecture decision management system.\\nWe will use [Maven Architecture Project](https:\/\/maven.apache.org\/), an open source, extensible, dependency management system that is used by the Pivotal Container Service to manage the lifecycle of application components.\\nWe will use [Pivotal Cloud Foundry](https:\/\/cf.pivotal.io), an open source platform as a service that enables the rapid development, deployment, and management of applications and services.\\"}
{"Context":"## Context and Problem Statement\\nWhen should we spread props in components?\\n**Example**\\n```\\nconst Component = ({ foo, bar, ...restProps}) => {\\n<element foo={foo} bar={bar} {...restProps} \/>\\n}\\n```\\nIf we spread rest props on every component, we create an opportunity for developers to\\ncustomize components via code instead of explicitly defined props.\\nFor components that render attribute-heavy DOM elements (like `button`), it may make\\nsense to spread props. For more complex components, spreading props could lead to bugs,\\nside effects, and unnecessary customization.\\n","Decision":"## Decision\nChosen option: \"Only components that render attribute-heavy DOM elements spread rest props\", because...\\n- Compromise that allows us to create the right prop interface for the job of each component\\n- Components that conceptually map to attribute-heavy DOM elements (i.e. html form elements)\\nwill allow spread props to keep the explicitly defined props easier to reason about.\\n- Components that **do not** conceptually map to attribute-heavy DOM elements (e.g. `Toast`)\\nwill be able to continue to define an explicit prop interface instead of spreading props.\\n### Negative Consequences <!-- optional -->\\n- Not all components will behave the same way.\\n- **We must document when a component supports spread props**.\\n","Prediction":"## Decision\nWe decided to **not** spread props to components.\\nWe don't want to create a dependency on the component creator for a component's props.\\nThis is especially true for complex components that render a lot of DOM elements.\\nWe will use `@Component` to mark props as being used by a component.\\nWe will use `@View` to mark a component view that renders a specific component.\\nWe will use `mixin` to create a class to wrap a component's props.\\nThis approach will provide a clean separation between the component's code and its props.\\nIt will also allow us to easily create a component that renders a specific type of\\nprop.\\n**Example**\\n```\\nconst Component = ({ foo, bar, ...restProps }) => {\\nconst { x } = foo;\\nconst { y } = bar;\\nreturn <div>{x}<\/div>\\n}\\n```\\nIf we spread props to every component, we create an opportunity for developers to\\ncustomize components via code instead of explicitly defined props.\\nFor components that render attribute-heavy DOM elements (like `button`), it may make\\nsense to spread props. For more complex components, spreading props could"}
{"Context":"## Context\\nEngaging Crowds allows volunteers to browse and search subject sets, linked to a workflow, in order to pick which subjects they want to work on. We needed a solution that would allow us to search a set, via indexed subject metadata, and present the results to a volunteer.\\nThe subjects table in Panoptes is too large to allow for fast querying, so we also needed to build our own indexing system for indexed sets.\\n","Decision":"## Decision\n- Project owners can flag subject metadata columns as searchable by prefixing the heading with `%` in the manifest eg. these manifest headings `subject_id,image_name_1,%origin,link,%attribution,license,#secret_description` mark `metadata.origin` and `metadata.attribution` as searchable.\\n- Subject sets with indexed subjects have `metadata.indexFields` set to a list of indexed fields eg. `indexFields: 'origin,attribution'`.\\n- Subject metadata for indexed sets is copied to a separate database running on [Datasette](https:\/\/datasette.io). Each set is given its own table, named by subject set ID. Datasette gives us a RESTful API out of the box, allowing us to browse and search subject data as HTML or JSON. See https:\/\/subject-set-search-api.zooniverse.org\/subjects.\\n- From a volunteers point-of-view, the Datasette service is used to find specific subject IDs to work on. Those IDs are then sent to the Panoptes API `\/subjects\/selection` endpoint, which returns those subjects, in order, for classification.\\n","Prediction":"## Decision\nWe will use Elasticsearch to index our indexed subject sets.\\nWe will use ElasticSearch Indexers to index the indexed sets. We will use ElasticSearch Indexers to index the indexed subject sets.\\nWe will use Elasticsearch Shards to aggregate the indexed subject sets. We will use Elasticsearch Shards to aggregate the indexed subject sets.\\nWe will use Elasticsearch Kibana to visualize the indexed subject sets. We will use Elasticsearch Kibana to visualize the indexed subject sets.\\nWe will use Elasticsearch Logstash to aggregate the indexed subject sets. We will use Elasticsearch Logstash to aggregate the indexed subject sets.\\nWe will use Elasticsearch Beats to aggregate the indexed subject sets. We will use Elasticsearch Beats to aggregate the indexed subject sets.\\nWe will use Elasticsearch Dashboard to aggregate the indexed subject sets. We will use Elasticsearch Dashboard to aggregate the indexed subject sets.\\nWe will use Elasticsearch Service to aggregate the indexed subject sets. We will use Elasticsearch Service to aggregate the indexed subject sets.\\nWe will use Elasticsearch CloudWatch to aggregate the indexed subject sets. We will use Elasticsearch CloudWatch to aggregate the indexed subject sets.\\nWe will use Elasticsearch API Gateway to aggregate the indexed subject sets. We will use Elasticsearch API Gateway to aggregate the indexed subject sets.\\nWe"}
{"Context":"## Context\\nSince this project targets a local workstation and is aimed to be run by the same\\nexact localhost, should we use ansible's connection local for ansible.cfg and localhost\\nas a host in all the playbooks?\\n","Decision":"## Decision\nWe will model the ansible playbook hosts as separate groups and form a full inventory\\nof them. The inventory will be assigned to localhost or 127.0.0.1 ip.\\nThe logical explanation of this is having a set of playbooks that can be run from everywhere\\nfor two reasons\\n* learn and expirement with the main ansible goal, i.e. setup remote hosts\\n* re-use this project in the future for more than our workstation\\n","Prediction":"## Decision\nNo.\\nThe ansible.cfg file will be stored in the same directory as the playbook and will\\nonly be accessible to the playbook.\\nThis will allow for a more consistent config, and will allow the playbook to be stored\\nin a version control system.\\nThis decision will be revisited when the project becomes more complex.\\n### Install\\nThe playbook will be installed to a local directory, using pip to install the\\nrequired dependencies. This will allow for easy upgrades and maintenance.\\n### Run\\nThe playbook will be run using the ansible-playbook command. This will\\nallow the playbook to be run locally, without the need for a network connection.\\n### Configuration\\nThe playbook will be stored in the same directory as the playbook. This\\nwill allow for easy upgrades and maintenance.\\n### Ansible\\nThe playbook will be stored in the same directory as the playbook. This\\nwill allow for easy upgrades and maintenance.\\n### Ansible roles\\nThe playbook will be stored in the same directory as the playbook. This\\nwill allow for easy upgrades and maintenance.\\n### Ansible roles and variables\\nThe playbook will be stored in the same directory as the playbook. This\\nwill allow for easy upgrades and maintenance.\\n### Ansible inventory"}
{"Context":"## Context and Problem Statement\\nSpring HATEOAS includes several classes that encapsulate domain objects, adding support for links.\\nConstructing such objects, as well as adding the desired links to them is a common operation that\\nrequires entity-specific boilerplate code.\\nHow can duplicate code in nearly all controller methods be avoided?\\n## Decision Drivers <!-- optional -->\\n* Avoid duplicate code to create HATEOAS models\\n* Decouple link creation from normal entity logic\\n","Decision":"## Decision\n* Avoid duplicate code to create HATEOAS models\\n* Decouple link creation from normal entity logic\\nSeparate model assemblers were chosen, as the former option would require us to have a deep coupling between HATEOAS types\\nand our DTO classes.\\nDue to the assembler classes being initially only used for links they all reside in the `linkassembler` package.\\n","Prediction":"## Decision\n* Avoid duplicate code to create HATEOAS models\\n* Decouple link creation from normal entity logic\\nChosen option: \"Use the [LinkResolver] annotation\", because it allows to annotate every entity\\nwith a link resolver without creating duplicate code.\\n### Positive Consequences <!-- optional -->\\n* Can be used with any type of entity, not just the [Entity](https:\/\/spring.io\/guides\/gs\/annotation\/annotation.html)\\n### Negative Consequences <!-- optional -->\\n* Requires a bit more boilerplate code to create links\\n* Requires a bit more boilerplate code to create links\\nChosen option: \"Use the [EntityLinkResolver] annotation\", because it allows to annotate every entity with\\na link resolver without creating duplicate code.\\n### Positive Consequences <!-- optional -->\\n* Requires a bit more boilerplate code to create links\\n* Requires a bit more boilerplate code to create links\\nChosen option: \"Use the [EntityLinkResolver] annotation\", because it allows to annotate every entity with\\na link resolver without creating duplicate code.\\n### Positive Consequences <!-- optional -->\\n* Requires a bit more boilerplate code to create links\\n* Requires a bit more boilerplate code"}
{"Context":"## Context and Problem Statement\\nAs we have multiple services\/applications deployed and monitored in the Operate First environment (ex. Jupyterhub, Argo, Superset, Observatorium, Project Thoth, AICoE CI pipelines etc), we need to implement an incident reporting setup for handling outages\/incidents related to these services.\\nAll the services are being monitored by [Prometheus](https:\/\/prometheus.io\/). Prometheus scrapes and stores time series data identified by metric key\/value pairs for each of the available services. These metrics can be used for measuring the service performance and alerting on any possible service degradation such as basic availability, latency, durability and any other applicable SLI\/SLOs. These SLI\/SLOs for the various services are defined and documented in the [SRE repository](https:\/\/github.com\/operate-first\/SRE\/tree\/master\/sli-slo).\\nAlerting with Prometheus is separated into two parts. Alerting rules in Prometheus servers send alerts to an [Alertmanager](https:\/\/prometheus.io\/docs\/alerting\/latest\/alertmanager\/). The Alertmanager then manages those alerts, including silencing, inhibition, aggregation and sending out notifications via methods such as email, on-call notification systems, and chat platforms.\\nWhether its a major bug, capacity issues, or an outage, users depending on the services expect an immediate response. Having an efficient incident management process is critical in ensuring that the incidents are always communicated to the users (via on-call notification systems) and handled by the team immediately. An on-call notification system is a system\/software that provides an automated means of contacting users and communicating pertinent information during an incident. It also has additional on-call scheduling features that can be used to ensure that the right people on the team are available to address a problem during an incident. There are multiple on-call notification systems such as [PagerDuty](https:\/\/www.pagerduty.com\/), [JIRA](https:\/\/www.atlassian.com\/software\/jira) etc that can be used for incident reporting, but which of these tools are best suitable for reporting the outages\/incidents to our users?\\n## Decision Drivers <!-- optional -->\\n* Visibility of incident reporting\\n* How can the incidents be tracked and reported in an open and transparent manner for our users?\\n* Compatibility with Prometheus\\n* Is the incident reporting tool compatible with Prometheus i.e. can it handle\/receive Prometheus alerts?\\n* Complexity and cost of incident reporting tool\\n* How easy\/hard is it to manage and operate the incident reporting tool?\\n* Is it a free or paid tool?\\n* Is it open source?\\n","Decision":"## Decision\n* Visibility of incident reporting\\n* How can the incidents be tracked and reported in an open and transparent manner for our users?\\n* Compatibility with Prometheus\\n* Is the incident reporting tool compatible with Prometheus i.e. can it handle\/receive Prometheus alerts?\\n* Complexity and cost of incident reporting tool\\n* How easy\/hard is it to manage and operate the incident reporting tool?\\n* Is it a free or paid tool?\\n* Is it open source?\\nChosen option: **Option 3**, because:\\n* The [GitHub alertmanager receiver](https:\/\/github.com\/m-lab\/alertmanager-github-receiver) can easily be configured and operated to function with Prometheus alerts. It automatically creates issues in GitHub repositories for any active alerts being fired, making it visible for any user to track\\n* All communication\/updates\/concerns related to the incident can be easily handled by adding comments in the issues created by the GitHub receiver\\n* Unlike Option 1, there is no additional cost involved\\n* There is no requirement for using JIRA\/Slack for incident tracking, which are the only supported options in some of the tools listed in Option 2 (such as [Dispatch](https:\/\/github.com\/Netflix\/dispatch) and [Response](https:\/\/github.com\/monzo\/response)) In any case that such a requirement surfaces, we can use GitHub bots for different platforms such as [GitHub for Slack](https:\/\/slack.github.com\/) and [Google Chat](https:\/\/support.google.com\/chat\/answer\/9632291?co=GENIE.Platform%3DAndroid&hl=en) to notify us of the issues immediately\\n* It is actively being maintained and supported compared to some of the tools in Option 1 (such as [Cabot](https:\/\/github.com\/arachnys\/cabot) and [OpenDuty](https:\/\/github.com\/openduty\/openduty)) which lack community support\\n","Prediction":"## Decision\n* Visibility of incident reporting\\n* How can the incidents be tracked and reported in an open and transparent manner for our users?\\n* Compatibility with Prometheus\\n* Is the incident reporting tool compatible with Prometheus i.e. can it handle\/receive Prometheus alerts?\\n* Complexity and cost of incident reporting tool\\n* How easy\/hard is it to manage and operate the incident reporting tool?\\n* Is it a free or paid tool?\\n* Is it open source?\\nChosen option: \"Prometheus\", because it is open source and has a well established API. Prometheus is a widely used open source service that has been used by the Operate First team to monitor other services such as Jupyterhub and Argo. It is also well documented and has a community that is very knowledgeable and capable of answering questions.\\n### Positive Consequences <!-- optional -->\\n* We have a well established incident reporting tool that is well documented and open source\\n* We can use Prometheus to monitor the services and pull metrics from Prometheus\\n* We can use Prometheus to create alerts based on metrics that are visible to the users\\n* We can use Prometheus to send out alerts to users via PagerDuty, JIRA, Slack etc.\\n* We can use Prometheus"}
{"Context":"## Context\\nWith the upcoming of [notaryv2](https:\/\/github.com\/notaryproject\/nv2) and similar projects like [Cosign](https:\/\/github.com\/sigstore\/cosign) the opportunity for Connaisseur arises to support multiple signing mechanisms, and combine all into a single validation tool. For that to work, the internal validation mechanism of connaisseur needs to be more modular, so we can easily swap in and out different methods.\\n","Decision":"## Decision\nWe are going with this structure (**option 1.1.1.1**) due to the lack of other alternatives. It provides all needed information and the flexibility to use multiple validation methods, as needed.\\n##### 1.1.2 Sensitive values\\nIf we allow multiple validators that may contain different forms of sensitive values, i.e. notary credentials, symmetric keys, service principals, ..., they need to be properly handled within the Helm chart with respect to ConfigMaps and Secrets. Currently, the distinction is hard-coded.\\n##### Option 1.1.2.1\\nAdd an optional `sensitive([-_]fields)` field at the validator config top level. Any sensitive values go in there and will be handled by the Helm chart to go into a secret. Any other values are treated as public and go into the ConfigMap.\\nAdvantages:\\n- Generic configuration\\n- Could be used by potential plugin validators to have their data properly handled (potential future)\\n- Hard to forget the configuration for newly implemented validators\\nDisadvantage: If implemented in a `config = merge(secret, configmap)` way, might allow sensitive values in configmap and Connaisseur still working\\n##### Option 1.1.2.2\\nHard-code sensitive values based on validator type\\nAdvantages: Can do very strict validation on fields without extra work\\nDisadvantages:\\n- Helm chart change might be forgotten for new validator\\n- Helm chart release required for new validator\\n- Does not \"natively\" allow plugins\\n##### Decision\\nWe are going with **option 1.1.2.2** and hard code the sensitive fields, to prevent users from misconfigure and accidentally but sensitive parts into configmaps.\\n#### Image policy (1.2)\\nFor the image policy similar changes to the notary configuration have to be made.\\n##### Proposition\\nThe previous `notary` field in the image policy will be changed to `validator`, referencing a `name` field of one item in the validators list. Any additional fields, e.g. required delegation roles for a notaryv1 validator will be given in a `with` field. This will look similar to this:\\n```yaml\\npolicy:\\n- pattern: \"docker.harbor.io\/*:*\"\\nvalidator: \"harbor-nv1\"\\nwith:\\nkey: \"default\"\\ndelegations:\\n- lou\\n- max\\n- pattern: \"docker.io\/*:*\"\\nvalidator: \"dockerhub-nv2\"\\n```\\n##### Option 1.2.1.1\\nBesides the self configured validator, two additional validators will be available: _allow_ and _deny_. The allow validator will allow any image and the deny validator will deny anything.\\nAdvantages: More powerful than `verify` flag, i.e. has explicit deny option.\\nDisadvantages: More config changes for users\\n##### Option 1.2.1.2\\nStick with current `verify` flag.\\nAdvantages: Config known for current users\\nDisadvantages: No explicit deny option\\n##### Decision\\nWe are going with **option 1.2.1.1**, as we don't have to use additional fields and offer more powerful configuration options.\\n##### Option 1.2.2.1\\nWhen no validator given, default to deny validator.\\nAdvantages: Easy\\nDisadvantages: Not explicit\\n##### Option 1.2.2.2\\nRequire validator in policy config.\\nAdvantages: Explicit configuration, no accidental denying images\\nDisadvantages: ?\\n#### Decision\\nWe are going with **option 1.2.2.1** as it reduces configurational effort and is consistent with the key selection behavior.\\n#### Option 1.2.3.1\\nThe validators from option 1.2.1.1 (_allow_ and _deny_) will be purely internal, and additional validator can not be named \"allow\" or \"deny\".\\nAdvantages: Less configurational effort\\nDisadvantage: A bit obscure for users\\n#### Option 1.2.3.2\\nThe _allow_ and _deny_ validator will be added to the default configuration as `type: static` with an extra argument (name up for discussion) that specifies whether everything should be denied or allowed. E.g.:\\n```yaml\\nvalidators:\\n- name: allow\\ntype: static\\napprove: true\\n- name: deny\\ntype: static\\napprove: false\\n- ...\\n```\\nAdvantages: No obscurity, if user don't need these they can delete them.\\nDisadvantage: Bigger config file ...?\\n#### Decision\\nWe are going with **option 1.2.3.2** as we favor less obscurity over the \"bigger\" configurational \"effort\".\\n### Validator interface (Choice 2)\\nSee [validator interface](https:\/\/github.com\/sse-secure-systems\/connaisseur\/blob\/master\/connaisseur\/validators\/validator.py)\\nShould validation return JSON patch or digest?\\n#### Option 2.1.1\\n`Validator.validate` creates a JSON patch for the k8s request. Hence, different validators might make changes in addition to transforming tag to digest.\\nAdvantages: More flexibility in the future\\nDisadvantages: We open the door to changes that are not core to Connaisseur functionality\\n#### Option 2.1.2\\n`Validator.validate` returns a digest and Connaisseur uses the digest in a \"standardized\" way to create a JSON patch for the k8s request.\\nAdvantage: No code duplication and we stay with core feature of translating input data to trusted digest\\nDisadvantages: Allowing additional changes would require additional work if we wanted to allow them in the future\\n#### Decision\\nWe are going with **option 2.1.2** as all current and upcoming validation methods return a digest.\\n","Prediction":"## Decision\nWe will use the [notaryv2](https:\/\/github.com\/notaryproject\/nv2) as our internal validation mechanism. We will use [Cosign](https:\/\/github.com\/sigstore\/cosign) as the external validation mechanism.\\nThis will allow us to write a plugin that will allow for a custom validation mechanism to be used by the Connaisseur client. The plugin can then be used as a standalone module, and\/or be used in conjunction with the Connaisseur client.\\nThis will enable us to support any of the following validation mechanisms:\\n* [Notaryv2](https:\/\/github.com\/notaryproject\/nv2) - the default\\n* [Cosign](https:\/\/github.com\/sigstore\/cosign) - optionally\\n* [Nucleus](https:\/\/github.com\/nucleusjs\/cosign) - optional\\n* [Serpent](https:\/\/github.com\/serpentjs\/cosign) - optional\\n* [JWPKOY](https:\/\/github.com\/jwpkoy\/jwpkoy) - optional\\n* [IOTA](https:\/\/github.com\/crypto-standards\/iota) - optional\\n* [Graal](https"}
{"Context":"## Context and Problem Statement\\n### Context\\nThe Versions repository has several purposes:\\n- Display differences between two versions, in particular when users receive a notification of change, so that they can simply see the changes.\\n- Explore significant changes in tracked documents.\\n- Offer a corpus of the latest versions of all the documents of the monitored services.\\n- Serve as a dataset for research.\\nIt is therefore important that repository constitutes a quality dataset, to provide relevant information to users.\\nFor this purpose, the following constraints are considered necessary:\\n- Versions must be ordered chronologically, so that navigation through the history of a document is intuitive.\\n- Versions should not contain noise, only significant changes.\\n- Each version must contain a link to the snapshot that was used to generate it.\\nCurrently, the following problems with the repository of Versions are identified:\\n- Noise in the versions: URL or structure changes in the tracked documents.\\n- Presence of refilter commits: related to URL and selector updates in service declarations or to Open Terms Archive code evolution.\\n- Presence of commits due to code changes: type renaming, service renaming, documentation changes in the repository.\\n- Presence of unordered commits: consequence of the import of the ToSBack history in snapshots or to the import of snapshots corresponding to archived documents provided by the services themselves.\\nThe solution considered in order to provide a quality dataset therefore consists of being able to regenerate the `versions` from the `snapshots`, that's what we call rewriting history.\\n#### Rewriting history\\nTo rewrite history, we go through the snapshot commits one by one after reordering them (in memory) and we create a version commit each time, avoiding commits corresponding to noise and performing any renaming.\\nThis implies being able to version the service filters (used to generate the version from the snapshot).\\nSee https:\/\/github.com\/OpenTermsArchive\/engine\/issues\/156.\\n### Problem\\nCurrently, `git` is used as database for storing snapshots and versions.\\nOne year ago, the process to rewrite history was estimated to take about 16 hours for 100,000 commits. It has also been noted that the evolution of the time is not linear, the more commits there are in `snapshots` the more the average time per commit increases.\\nIt appears that the most costly operation is accessing the contents of a commit (checkout).\\nIt also appears that the older the commit is in the git history, the longer this operation takes.\\n> For example, on a history containing about 100,000 commits, accessing the contents of the oldest commit takes about 1,000\u00a0ms while accessing the most recent commit takes only 100\u00a0ms.\\nAt the date of this document, the number of commits entries approaches the million and to iterate over these snapshots, to rewrite versions history, it currently takes more or less 3 months.\\nAlso, `git` implies to store data in a hash tree in the form of chronologically ordered commits. So to insert snapshots in the history, it implies to rewrite the whole snapshots history which also takes the same time as reading them.\\nAs described previously, we need to be able to regenerate versions from snapshots (for example to [rename services](https:\/\/github.com\/OpenTermsArchive\/engine\/issues\/314)) and to be able to insert snapshots in the history (for example to [import databases](https:\/\/github.com\/OpenTermsArchive\/engine\/pull\/214)).\\n**This cannot take 6 months.**\\nMoreover, as the number of snapshots will keep on growing, we need a system which allows scaling, potentially across multiple servers.\\nThus, we need a database management system meeting the following requirements:\\n- Access time to a snapshot should be constant and independent from its authoring date.\\n- Inserting time of a snapshot should be constant and independent from its authoring date.\\n- Support concurrent access.\\n- Scale horizontally.\\n### Solutions considered\\n#### 1. Keep the system under git\\n##### Splitting into sub-repos\\nSince accessing the contents of a commit takes longer the older it is in the history considered, the idea would be to work successively on ordered subsets of this history.\\nThis means truncating the history, browsing the remaining commits and regenerating the corresponding versions. Then creating another subset of the history which contains an arbitrary number of commits following the commits already browsed and perform the processing.\\nTo create a history subset with git :\\n- Create a clone of a subset of N commits from the local snapshot: `git clone --depth <N> \"file:\/\/local\/path\/snapshots\" snapshots-tmp` with `N` corresponding to the position of the first commit you want in the block relative to the last commit in the history\\n- Remove all commits older than the last commit you want to keep in the block: `git reset --hard <sha>` with `sha` corresponding to the id of the last commit you want to have in the block.\\n- Clean up git to ensure that history navigation is efficient: `git gc`.\\nSo we need to split the history into chronologically ordered blocks, which leads us to the next problem.\\n##### Splitting and reordering blocks of snapshots\\nBecause snapshot commits are unordered, we can't simply create blocks of a fixed size from the git history (otherwise we'd process commits out of order).\\nIt is necessary to create blocks whose commits are ordered within the block but also in relation to the other blocks: for example, all the commits of the first block processed must be older than the commits of all the other blocks.\\nThe solution would be to create blocks in order: from the git history, we look for commits that are not in their place (whose date is earlier than that of its predecessor).\\nEach of these commits represents the first commit of a block. This block extends to the previous one, the starting point of the next block.\\nWe thus obtain blocks whose commits are ordered.\\nWe still have to order the blocks between them (note, it is possible to have to cut a block to be able to place another).\\nThese chronologically ordered commit blocks, without overlap, can then be used with the previous approach (it may be necessary to re-split these blocks so that they have a reasonable size).\\n#### 2. Move snapshots to a document-oriented database\\nThe idea of this solution is to keep the `versions` under git in order to continue to enjoy the benefits that GitHub provides in terms of browsing and viewing diffs, but to save the snapshots in a database, since we don't really need to browse the snapshots via a graphical interface nor to see the diff between two snapshots, which would allow us to be able to access the content more efficiently.\\nMongoDB seems to meet the constraints:\\n- It natively allows horizontal scaling with [replica sets](https:\/\/docs.mongodb.com\/manual\/replication\/) and [sharding](https:\/\/docs.mongodb.com\/manual\/sharding\/).\\n- It supports concurrent access.\\n- It has [In-Memory storage engine](https:\/\/docs.mongodb.com\/manual\/core\/inmemory\/) as an option for performance.\\nWe also did a simple test to ensure that access time and insert time also meets the requirements. We populated a database with one million entries and tried accessing snapshots with random dates and we found that access times remained stable. In our test on 1000 sequential access to random snapshot, the average access time was ~3.5ms with a maximum of ~50ms.\\nMoreover, MongoDB has the following benefits:\\n- Easy to use: offers a simple query syntax SQL and has a quick learning curve, especially for JavaScript developers.\\n- Flexible and evolutive: allows to manage data of any structure, not just tabular structures defined in advance.\\n- Widely used in the JavaScript ecosystem.\\nAs downside, joining documents in MongoDB is no easy task and pulling data from several collections requires a number of queries, which will lead to long turn-around times. This is not a problem in our case as we do not currently envision a need for such complex queries.\\n","Decision":"## Decision\nAs MongoDB meets the requirements it is retained as a solution.\\n### Benchmark\\nWith MongoDB implementation, refilter takes around ~3m where it took around ~1h20 with the Git version.\\n","Prediction":"## Decision\nThe solution proposed is to move the Versions repository to MongoDB.\\nThis will allow us to:\\n- Provide a consistent and stable versioning scheme, with the ability to regenerate the version from a snapshot.\\n- Provide a consistent and stable querying scheme, with the ability to retrieve only the most recent version of a document.\\n- Provide a consistent and stable replication scheme, with the ability to replicate the Versions repository to multiple servers.\\n- Provide a consistent and stable backup scheme with the ability to restore a snapshot to a different server.\\n- Provide a consistent and stable synchronization scheme with the ability to replicate the Versions repository to a different server.\\n- Provide a consistent and stable replication scheme with the ability to replicate the Versions repository to a different server.\\n- Provide a consistent and stable replication scheme with the ability to replicate the Versions repository to a different server.\\n- Provide a consistent and stable replication scheme with the ability to replicate the Versions repository to a different server.\\n- Provide a consistent and stable replication scheme with the ability to replicate the Versions repository to a different server.\\n- Provide a consistent and stable replication scheme with the ability to replicate the Versions repository to a different server.\\n- Provide a consistent and stable replication scheme with the ability to"}
{"Context":"## Context\\nGitHub Actions are the new shiny and have been in Beta for sometime. They are\\ngoing to be generally available on\\n[2019-11-13](https:\/\/github.blog\/2019-08-08-github-actions-now-supports-ci-cd\/).\\nGitHub Actions will have a long term future. It is likely GitHub Actions\\nwill become the default CI mechanism (and possibly more) for projects hosted on\\nGitHub. Using them in this repo, which has a basic use case will provide some\\nexposure to the service.\\n","Decision":"## Decision\nThe decision is to replace Travis CI with GitHub Actions.\\n","Prediction":"## Decision\nWe will use GitHub Actions for CI\/CD.\\nWe will use the [GitHub Actions plugin](https:\/\/github.com\/github\/gh-actions) to\\ntest our CI\/CD pipeline.\\nWe will use the [GitHub Actions CLI](https:\/\/github.com\/github\/actionclint) to\\ntest our CI\/CD pipeline.\\nWe will use the [GitHub Actions Runner](https:\/\/github.com\/github\/actionssuite\/runners) to run our CI\/CD pipeline.\\nWe will use the [GitHub Actions Runner](https:\/\/github.com\/github\/actionssuite\/runners) to run our CI\/CD pipeline.\\nWe will use the [GitHub Actions Runner](https:\/\/github.com\/github\/actionssuite\/runners) to run our CI\/CD pipeline.\\nWe will use the [GitHub Actions Runner](https:\/\/github.com\/github\/actionssuite\/runners) to run our CI\/CD pipeline.\\nWe will use the [GitHub Actions Runner](https:\/\/github.com\/github\/actionssuite\/runners) to run our CI\/CD pipeline.\\nWe will use the [GitHub Actions Runner](https:\/\/github.com\/github\/actionssuite\/runners) to run our CI\/CD pipeline.\\nWe will use the [GitHub Actions Runner](https"}
{"Context":"## Context\\nAs mentioned in [ADR # 8. Use CSS modules to create scoped CSS for components](0008-use-css-modules-to-create-scoped-css-for-components.md) this application has separate CSS files for each component's CSS. We now require a need to define commonly used variables such as colours, spacing and so on in a centralised location so that these values are not duplicated across the codebase.\\nThere were two solutions considered:\\n1. [Sass](https:\/\/sass-lang.com\/)\\n- Superset of CSS that compiles to CSS.\\n- Supports variables, nesting, partials, operators, and so on.\\n- Has two major implementations in JavaScript:\\n- [`sass`](https:\/\/www.npmjs.com\/package\/sass) which is implemented in Dart.\\n- [`node-sass`](https:\/\/www.npmjs.com\/package\/node-sass) which provides bindings to the C library LibSass.\\n2. [PostCSS](https:\/\/postcss.org\/)\\n- Transforms CSS with JavaScript plugins.\\n- Plugins that add support for Sass features such as variables, nesting, imports, and so on are available.\\n- Allows for auto-vendor prefixing via [`autoprefixer`].\\n","Decision":"## Decision\nThis system will use PostCSS to process CSS.\\n","Prediction":"## Decision\nWe will use Sass to define the CSS for the application, and postcss to define the CSS for components.\\nThis will allow us to reuse CSS across the application, and will also make it easier to update components in the future.\\nWe will use Sass to define the CSS for the application, and postcss to define the CSS for components.\\nThis will allow us to reuse CSS across the application, and will also make it easier to update components in the future.\\nWe will use Sass to define the CSS for the application, and postcss to define the CSS for components.\\nThis will allow us to reuse CSS across the application, and will also make it easier to update components in the future.\\nWe will use Sass to define the CSS for the application, and postcss to define the CSS for components.\\nThis will allow us to reuse CSS across the application, and will also make it easier to update components in the future.\\nWe will use Sass to define the CSS for the application, and postcss to define the CSS for components.\\nThis will allow us to reuse CSS across the application, and will also make it easier to update components in the future.\\nWe will use Sass to define the CSS for the application, and"}
{"Context":"## Context\\nGoal: I need to select a platform for the infrastructure.\\nA platform?\\nOk, at first, it came to my mind the idea of Platform as a Service.  I have\\nthree options here:\\n- Heroku and Cloud Foundry\\n- Kubernetes\\n- Openshift\\nAlso I think it could be possible to consider other solutions, like:\\n- AWS ECS\\n- AWS EKS\\n- Google GKS\\n- Azure AKS\\nAll of them are good options for managing microservices architectures,\\nbut it depends on the perspective.  As SRE, I want to keep things under\\ncontrol, so I'm going to use something I can create from scratch.  So\\nI will discard Heroku now and all the managed solutions (*KS and AWS ECS).\\nCloud Foundry is really a good idea from the developer point of view, as\\nthe only thing she has to do is just focus on development.  Cloud Foundry\\nuses the concept of code deployment (it takes a directory or repository and\\nbuilds the code and deploy it to the cells.)\\nPros: it's very easy from a end user point of view, and fully compatible with\\nother Cloud Foundry providers (think of IBM Cloud, Pivotal, Anynines...).\\nCons: it requires license to use it and it requires lots of resources (it's for\\nvery large deployments).\\nKubernetes: it's a standard de-facto of the container world.  Google uses it,\\nAmazon uses it, Azure uses it.  And the open source community is pushing it\\na lot.  It has lots of documents, addons, blogs, meetups...\\nPros: best documented solution, and more affordable (it's free).  Lots of tools\\nto help with the deployment and administration of the platform.\\nCons: complex. Very complex.\\nOpenshift: I know it's more a platform as a service solution than a container\\norchestrator or platform.  Internally it uses kubernetes, but with more layers.\\nPros: It must be very complete\\nCons: I don't have any experience at all with openshift.\\n### Let's talk about costs\\nI would like to add here an interesting topic: Costs.\\nTo design a System Architecture, you must cover the following set of features:\\n- Availability\\n- Economy\\nAvailability\\nAvailability for me includes these topics:\\n- Maintenability\\n- Testability\\n- Usability\\n- Scalability\\n- Performance\\n- Security\\nEconomy\\nBecause at the end, a business must earn money with the value added to the market.\\nSo there is a complex formula to balance Availability and Economy.\\nPublic cloud is cheaper at the beginning, but it becomes expensive quickly.\\nPrivate cloud is expensive at the short and middle scope, but it can be cheaper when the business is mature.\\n","Decision":"## Decision\nHere is a good branch point.\\nI have experience managing apps in Cloud Foundry and Kubernetes, and I like how\\neasy is for a developer or a non SRE to deploy an app in CloudFoundry.\\nBut Kubernetes is more appropiate to answer this test, as it will allows me to\\ndescribe all the internals and the steps to make solve the test.\\nOpenshift... It would be nice to try it.\\nAnd about economy, it's not so easy to decide between public or private.  I'm\\ngoing to use my own servers, and my AWS account.\\nSo, I will create a main branch for a solution based on Kubernetes, and I will try\\nto keep an eye on Openshift if I have enough time.\\n","Prediction":"Economy is a combination of Availability and Economy.\\nSo the best way to balance both of them is to have a hybrid solution.\\nFor a business that is not mature, it is better to look for a private cloud.\\nFor a mature business, it is better to look for a public cloud.\\nFor a mature business, it is better to look for a private cloud.\\nFor a mature business, it is better to look for a public cloud.\\nFor a mature business, it is better to look for a private cloud.\\nFor a mature business, it is better to look for a private cloud.\\nFor a mature business, it is better to look for a private cloud.\\nFor a mature business, it is better to look for a private cloud.\\nFor a mature business, it is better to look for a private cloud.\\nFor a mature business, it is better to look for a private cloud.\\nFor a mature business, it is better to look for a private cloud.\\n## Decision\nI will use Kubernetes as the platform for the infrastructure.\\nI will use Openshift as the platform for running the microservices.\\nI will use Cloud Foundry as the container orchestrator.\\nI will use Openshift as"}
{"Context":"## Context\\nWe can identify different components all making part of the finality gadget\\nprotocol we introduced:\\n- _Validator(s)_ as the actors voting to finalize blocks\\n- _Finalization_ as the process of making a certain part of the chain irreversible\\n- _Finalization transaction(s)_ as the transactions types newly introduced specifically\\nfor this process (deposit, vote, logout, slash, withdraw, admin).\\n- _FinalizationState_ as the class that handles the process of finalization and its\\nglobal state.\\n- _ValidatorState_ as the current state of the node acting as a validator, possibilities\\nare like \"waiting for deposit to be included in a block\", \"waiting for deposit to\\nbe finalized\", \"voting\", etc..\\n- _Commits_ as a group of finalization transactions passed by peers while reconstructing\\nthe state of the blockchain while syncing.\\n- _Esperanza_ as the codename for the whole protocol, derived from Ethereum's _casper_.\\nIt is evident that, even if all those components belong together, the naming does\\nnot well reflect that. Also the terms \"validate\", \"validation\" are quite ambiguous\\nsince it has to be cleared with context if is about finalization or just another\\nvalidation logic (as per transaction validation i.e.).\\n","Decision":"## Decision\nWe define a new, more consistent naming for all the components mentioned above\\nin the following way:\\n- _Finalizer(s)_: MUST replace _Validator_ everywhere in the codebase. The term _Validator_\\ncomes from the _casper_ lingo, but it does not really represent the actions\\nperformed by the actors in this role. The _Finalizers_ are such because they\\nall concur in finalizing the blockchain.\\n- _Finalization_: remains unchanged.\\n- _FinalizerCommit(s)_: MUST replace all the instances of _Finalization transaction_\\nor similar in the codebase. Even if they look like a special case of a bitcoin\\ntransaction, they do not really transact as there is no exchange of money [1].\\nThose are instead actions representing finalizers performing (committing to) an action.\\nPlural or singular form has to be chosen according to the situation.\\n- _FinalizationState_: remains unchanged.\\n- _FinalizerState_: MUST replace _ValidatorState_ and similar everywhere in the codebase.\\nThe rationale is the same as for _Finalizer(s)_.\\n- _FinalizerCommit(s)_: MUST replace simple _Commits_ everywhere in the codebase.\\nThe specific concept of _Commits_, as a batch of transactions sent for syncing the\\n_FinalizationState_ will be effectively absorbed into the more general definition\\nof _FinalizerCommit(s)_.\\n- _Esperanza_: remains unchanged as for the informal codename, but any trace of it\\nMUST be completely removed from the codebase.\\nGiving some practical examples from the codebase:\\n- the tests `esperanza_vote.py` and the similar, will be renamed to `finalization_vote.py`\\nand so on. The reason is that tests follow a `namespace_testname.py` convention\\nand finalization is the correct namespace.\\n- the folder `src\/esperanza` has to be replaced with `src\/finalization`.\\n- the start up parameter `-validating` will be replaced by `-finalizing`.\\n","Prediction":"## Decision\nWe will use the name _finalization_ for all the components of the protocol.\\nWe will use _finalization_ for the _transaction_ component of the protocol.\\nWe will use _finalization_ for the _chain_ component of the protocol.\\nWe will use _finalization_ for the _finality_ component of the protocol.\\nWe will use _finality_ for the _finalization_ transaction.\\nWe will use _finality_ for the _finality_ transaction.\\nWe will use _finality_ for the _finality_ transaction.\\nWe will use _finality_ for the _finality_ transaction.\\nWe will use _finality_ for the _finality_ transaction.\\nWe will use _finality_ for the _finality_ transaction.\\nWe will use _finality_ for the _finality_ transaction.\\nWe will use _finality_ for the _finality_ transaction.\\nWe will use _finality_ for the _finality_ transaction.\\nWe will use _finality_ for the _finality_ transaction.\\nWe will use _finality_ for the _finality_ transaction.\\nWe will use _"}
{"Context":"### Problem to solve\\nCover with API all `ProductOption`s related functionality provided by the Admin panel.\\n### Context\\nDuring the development of a new Sylius API (based on API Platform) for Product Options, we had to decide how we should handle\\nProduct Option's values collection, to make it efficient and easy to use.\\n### Possible solutions\\nValues from collection could be embedded as objects within a `ProductOption` resource **or** provided by\\n[IRIs](https:\/\/en.wikipedia.org\/wiki\/Internationalized_Resource_Identifier)\\n### Decision and reasoning\\nProduct option values should always be provided with their IRIs. They can exist outside of the `ProductOption` resource\\n(e.g. be [related with](https:\/\/github.com\/Sylius\/Sylius\/blob\/master\/src\/Sylius\/Component\/Product\/Model\/ProductVariant.php#L44) `ProductVariant`).\\nMoreover, values embedded together with their translations in the `ProductOption` that already has its translations would\\nresult in a massive and unmanageable response.\\n","Decision":"## Decision\nProduct option values should always be provided with their IRIs. They can exist outside of the `ProductOption` resource\\n(e.g. be [related with](https:\/\/github.com\/Sylius\/Sylius\/blob\/master\/src\/Sylius\/Component\/Product\/Model\/ProductVariant.php#L44) `ProductVariant`).\\nMoreover, values embedded together with their translations in the `ProductOption` that already has its translations would\\nresult in a massive and unmanageable response.\\n","Prediction":"## Decision\nValues from collection should be provided with their IRIs. They can exist outside of the `ProductOption` resource\\n(e.g. be [related with](https:\/\/github.com\/Sylius\/Sylius\/blob\/master\/src\/Sylius\/Component\/Product\/Model\/ProductVariant.php#L44) `ProductVariant`).\\nMoreover, values embedded together with their translations in the `ProductOption` that already has its translations would\\nresult in a massive and unmanageable response.\\nWe decided to provide the values of a `ProductOption` as well as their translations in a `ProductOption` resource\\nfor the following reasons:\\n* It would be easier to manage and use translations for a `ProductOption` than for a `ProductVariant`.\\n* We would be able to use the `IRIs` of the `ProductOption` as well as its translations in other parts of Sylius.\\n* It would be easier to implement a `get` method for a `ProductOption` than for a `ProductVariant`.\\n* It would be easier to integrate the `IRIs` of a `ProductOption` with the `get` methods of a `ProductVariant`.\\n* It would be easier to handle multiple translations of"}
{"Context":"Context\\n=======\\nWe need to pass correct client IP and requested protocol to applications deployed to the platform. To achieve this we want to use X-Forwarded-For and X-Forwarded-Proto headers.\\nIn the current setup we've got HAProxy behind ELB to allow insert HSTS headers, and ELB is configured in SSL mode (not HTTPS) because it does not support WebSockets. In SSL\/TCP mode ELB is not able to set any `X-Forwarded` header.\\nThe solution is to use ProxyProtocol to pass information about recorded client IP and protocol to HAProxy which can set required headers for us. Unfortunately [ELB sets ProxyProtocol header inside SSL stream and HAProxy expects it outside](http:\/\/serverfault.com\/questions\/775010\/aws-elb-with-ssl-backend-adds-proxy-protocol-inside-ssl-stream).\\nThere are two options to workaround this:\\n* Use a more complex configuration of HAProxy with two frontends\/listeners chained\\n* Disable SSL between ELB and HAProxy\\nDecision\\n========\\nWe have decided to disable SSL encryption between internal IP of ELB and HAProxy to allow us to use ProxyProtocol.\\nWe don't think this has any significant increase in risk because:\\n* gorouter to cell traffic is already HTTP (it has to be because we can't do termination in app containers)\\n* the inner interface of the ELB is on an internal network in our VPC\\nStatus\\n======\\nAccepted\\nConsequences\\n============\\nThe http traffic between ELB and HAProxy will not be encrypted.\\n","Decision":"## Decision\n========\\nWe have decided to disable SSL encryption between internal IP of ELB and HAProxy to allow us to use ProxyProtocol.\\nWe don't think this has any significant increase in risk because:\\n* gorouter to cell traffic is already HTTP (it has to be because we can't do termination in app containers)\\n* the inner interface of the ELB is on an internal network in our VPC\\nStatus\\n======\\nAccepted\\nConsequences\\n============\\nThe http traffic between ELB and HAProxy will not be encrypted.\\n","Prediction":"## Decision\n========\\nWe have decided to disable SSL encryption between internal IP of ELB and HAProxy to allow us to use ProxyProtocol.\\nWe don't think this has any significant increase in risk because:\\n* gorouter to cell traffic is already HTTP (it has to be because we can't do termination in app containers)\\n* the inner interface of the ELB is on an internal network in our VPC\\nStatus\\n======\\nAccepted\\nConsequences\\n============\\nThe http traffic between ELB and HAProxy will not be encrypted.\\nIn the current setup we've got HAProxy behind ELB to allow insert HSTS headers, and ELB is configured in SSL mode (not HTTPS) because it does not support WebSockets. In SSL\/TCP mode ELB is not able to set any `X-Forwarded` header.\\nThe solution is to use ProxyProtocol to pass information about recorded client IP and protocol to HAProxy which can set required headers for us. Unfortunately [ELB sets ProxyProtocol header inside SSL stream and HAProxy expects it outside](http:\/\/serverfault.com\/questions\/775010\/aws-elb-with-ssl-backend-adds-proxy-protocol-inside-ssl-stream).\\nThere are two options to workaround this"}
{"Context":"## Context\\nThis ADR presents a technical plan for creation of a 2.0 version of edgex-cli which supports the new V2 REST APIs developed as part of the Ireland release of EdgeX.\\n","Decision":"## Decision\n1. Use standardized command-line args\/flags\\n| Argument\/Flag      | Description |\\n| ----------- | ----------- |\\n| `-d`, `--debug`      | show additional output for debugging purposes (e.g. REST URL, request JSON, \u2026). This command-line arg will replace -v, --verbose and will no longer trigger output of the response JSON (see -j, --json).       |\\n| `-j`, `--json`   | output the raw JSON response returned by the EdgeX REST API and *nothing* else. This output mode is used for script-based usage of the client.    |\\n| `--version`   | output the version of the client and if available, the version of EdgeX installed on the system (using the version of the metadata data service)   |\\n2. Restructure the Go code hierarchy to follow the [most recent recommended guidelines](https:\/\/github.com\/golang-standards\/project-layout). For instance \/cmd should just contain the main application for the project, not an implementation for each command - that should be in \/internal\/cmd\\n3. Take full advantage of the features of the underlying command-line library, [Cobra](https:\/\/github.com\/spf13\/cobra), such as tab-completion of commands.\\n4. Allow overlap of command names across services by supporting an argument to specify the service to use: `-m\/--metadata`, `-c\/--command`, `-n\/--notification`, `-s\/--scheduler` or `--data` (which is the default). Examples:\\n- `edgex-cli ping --data`\\n- `edgex-cli ping -m`\\n- `edgex-cli version -c`\\n5. Implement all required V2 endpoints for core services\\n**Core Command**\\n- **`edgex-cli command`** `read | write | list`\\n**Core Data**\\n- **`edgex-cli event`** `add | count | list | rm | scrub**`\\n- **`edgex-cli reading`** `count | list`\\n**Metadata**\\n- **`edgex-cli device`**  `add | adminstate | list | operstate | rm | update`\\n- **`edgex-cli deviceprofile`**  `add | list | rm | update`\\n- **`edgex-cli deviceservice`** ` add | list | rm | update`\\n- **`edgex-cli provisionwatcher`**  `add | list | rm | update`\\n**Support Notifications**\\n- **`edgex-cli notification`** `add | list | rm`\\n- **`edgex-cli subscription`** `add | list | rm`\\n**Support Scheduler**\\n- **`edgex-cli interval`** `add | list | rm | update`\\n**Common endpoints in all services**\\n- **`edgex-cli version`**\\n- **`edgex-cli ping`**\\n- **`edgex-cli metrics`**\\n- **`edgex-cli status`**\\nThe commands will support arguments as appropriate. For instance:\\n- `event list` using `\/event\/all` to return all events\\n- `event list --device {name}` using `\/event\/device\/name\/{name}` to return the events sourced from the specified device.\\n6.  Currently, some commands default to always displaying GUIDs in objects when they're not really needed. Change this so that by default GUIDs aren't displayed, but add a flag which causes them to be displayed.\\n7. **scrub** may not work with Redis being secured by default. That might also apply to the top-level `db` command (used to wipe the entire db). If so, then the commands will be disabled in secure mode, but permitted in non-secure mode.\\n8. Have built-in defaults with port numbers for all core services and allow overrides, avoiding the need for static configuration file or configuration provider.\\n9. *(Stretch)* implement a `-o`\/`--output` argument which could be used to customize the pretty-printed objects (i.e. non-JSON).\\n10. *(Stretch)* Implement support for use of the client via the API Gateway, including being able to connect to a remote EdgeX instance. This might require updates in go-mod-core-contracts.\\n","Prediction":"## Decision\nThe plan is to create a 2.0 version of edgex-cli which implements the new V2 REST APIs.\\nThe 1.0 version of edgex-cli will be deprecated.\\nThe 2.0 version of edgex-cli will be maintained for the duration of the Ireland release.\\nThe 2.0 version of edgex-cli will be developed in a separate GitHub repository.\\nThe 2.0 version of edgex-cli will be released as a separate package.\\nThis will be done using the `edge-cli` package.\\nThis will be done using the `edge-cli` package.\\nThis will be done using the `edge-cli` package.\\nThis will be done using the `edge-cli` package.\\nThis will be done using the `edge-cli` package.\\nThis will be done using the `edge-cli` package.\\nThis will be done using the `edge-cli` package.\\nThis will be done using the `edge-cli` package.\\nThis will be done using the `edge-cli` package.\\nThis will be done using the `edge-cli` package.\\nThis will be done using the `edge-cli` package.\\nThis will be done"}
{"Context":"## Context\\nSeveral applications for OpenFIDO will require APIs with application level\\nauthentication and authorization. These include the pipeline API itself, its\\nworkers, the OpenFIDO services and web client, and the Blob service.\\nThese other services will want to import and use the application roles tables\\nand functions defined in this project for their own API endpoints, but might\\nnot the pipeline endpoints.\\nDocs:\\n* [OpenFIDO Db\/Service Model](https:\/\/app.lucidchart.com\/documents\/edit\/5dcaf4fa-7cad-4ce1-9275-ab86110fc2a6\/0_0?shared=true)\\n","Decision":"## Decision\nCreate a separate 'application roles' package and set of models within this\\nproject that can be imported separately from the pipelines API.\\n- [x] Ensure that both logic to enforce a permission (a decorator enforcing a\\nrequirement of a specific set of SystemPermissions) is included in this\\npackage.\\n- [x] Models are included in such a way that they can be included in an existing\\nAlembic database schema (have one central 'db' that is configured by the\\nimporting app)\\n- [x] Create a setup.py file and reference flask project that imports this project\\n(basis example for other projects).\\n","Prediction":"## Decision\nWe will use the `role` table and `role` functions to map the application\\nroles tables and functions to OpenFIDO's API endpoints and internal\\nservices. We will also generate the application's application roles tables\\nand functions as part of the pipeline API itself.\\nWe will generate the roles tables and functions as part of the pipeline\\nAPI's `schema` schema file so that they can be used directly in the\\npipeline API's `schema` file. The `schema` file will be generated by the\\npipeline API's `schema` generator.\\n### Pipeline API\\nThe pipeline API will generate the following tables and functions:\\n* `role`: the application's application roles table\\n* `role\\_id`: the application's application role identifier\\n* `role\\_type`: the application's application role type\\n* `role\\_id` is the primary key of the application's application roles table\\n* `role\\_type` is the application's application role type\\n* `role`: the application's role table\\n* `id`: the application's application role identifier\\n* `role\\_type`: the application's application role type\\n* `role`: the application's role table\\n* `id`: the application's application role"}
{"Context":"## Context\\nPUDL has [collection landing\\npages](http:\/\/pudl.princeton.edu\/collections\/pudl0058) because it's a digital collections site which\\nstaff pushed content into. PUDL required all items be in a single collection,\\nand couldn't provide good search-across. Figgy is a staff back-end, so we need some sort of\\napplication to provide that functionality.\\nCurators also had a history of either requesting or creating ad-hoc websites to showcase\\ntheir material or accompany on-site exhibits. We wanted instead to provide a CMS for\\nthem to create those experiences based on material they curate which wouldn't\\ncreate metadata silos and increased maintenance.\\nFurther, curators often had different use cases about how metadata should\\ndisplay in different contexts. For example, items with the same title in the catalog may need to be differentiated in an exhibit. It was important that certain fields display a certain\\nway, but be cataloged according to best practices.\\nSpotlight is an exhibit building platform that provides controlled CMS\\nfunctionality and is built on Blacklight similar to our new catalog. It allows\\nfor local overrides of fields.\\n","Decision":"## Decision\nWe will use Spotlight to fulfill both the requirements of a Collection Landing\\nPage as well as exhibits. Curators of collections will generate the collection\\npages. Staff who wish to generate exhibits will be able to manage membership in\\nFiggy but have the tools to create those exhibits in Pomegranate.\\n","Prediction":"## Decision\nWe will use Spotlight as a CMS for the PUDL exhibit building site.\\nWe will use Blacklight as the exhibit building CMS.\\nWe will use the [PUDL Curator's Toolkit](https:\/\/github.com\/princeton\/pudl-curator\\%27s-toolkit) for the PUDL curators to create content. These curators will\\nbe responsible for creating and maintaining content for the PUDL site.\\nWe will use the PUDL Curator's Toolkit for the PUDL exhibit building site.\\nWe will use the PUDL Curator's Toolkit for the PUDL catalog site.\\nWe will use the PUDL Curator's Toolkit for the PUDL Curator's Guide.\\nWe will use the PUDL Curator's Toolkit for the PUDL Curator's Guide\\nfor the PUDL exhibit building site.\\nWe will use the PUDL Curator's Toolkit for the PUDL Curator's Guide\\nfor the PUDL catalog site.\\nWe will use the PUDL Curator's Guide for the PUDL exhibit building site.\\nWe will use the PUDL Curator's Guide for the PUDL catalog"}
{"Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","Prediction":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\nWe will use [style guide](http:\/\/www.visionmedia.com\/blog\/2011\/11\/15\/documenting-architecture-decisions-with-style-guides) for style.\\nWe will use [test-first approach](http:\/\/www.developsense.com\/2011\/11\/15\/test-first-architecture\/) for testing.\\nWe will use [Jira](https:\/\/www.atlassian.com\/software\/jira) for project management.\\nWe will use [Jenkins](https:\/\/jenkins.io\/) for continuous integration and continuous deployment.\\nWe will use [Ansible](https:\/\/www.ansible.com\/) for infrastructure as code configuration.\\nWe will use [CircleCI](https:\/\/circleci.com\/) for continuous integration and continuous deployment.\\nWe will use [GitLab](https:\/\/gitlab.com\/) as a continuous integration and continuous deployment tool.\\nWe will use [Pulumi](https:\/\/pulumi.com\/) for cloud infrastructure as code.\\nWe will use [Grafana](https:\/\/grafana.com\/) for monitoring and"}
{"Context":"## Context\\n> This ADR details the mitigation of brute force attacks on certain endpoints - other types of protection are out of\\n> scope.\\nWe have a need to mitigate brute force attacks on certain endpoints in the application. Currently these are:\\n* Actor Login\\n* One Time Passcode (adding an LPA)\\n* Viewer Code (viewing an LPA)\\nBrute force can be defined as multiple failed attempts to carry out these actions.\\n","Decision":"## Decision\nProtecting against these kinds of attacks is a problem in two parts; reliable identification of users and the recording\\nof attempts by that user against protected endpoints.\\n#### Identification\\nThe bare minimum of information that we can use to identify a user is their originating IP address. This is not without\\nissue as it can be spoofed, or more likely the user is behind a NAT or proxy layer (as will likely be the case with our\\ncorporate users). We can couple the browser sent headers that offer extra user identifying information (such as 'Accept'\\n'DNT', 'User-Agent' etc) in a hashing function to generate an ID that should more uniquely identify a user.\\nWe will identify users using a hash calculated using the incoming IP and associated headers.\\nWe will track this identity in the session to guard against changing headers within a session.\\n#### Attempt Tracking\\nWe will use a cache service (AWS Elasticache) to track failure attempts at each of the three endpoints defined above.\\nWe will use a per-endpoint moving window rate-limit to reduce brute force impact - these will be individually\\nconfigurable in terms of window size and request limit.\\n","Prediction":"## Decision\nWe will create a custom role that will be used to control the access to the endpoints. This role will be\\nassociated to the application and will require a password. The password will be stored in a\\nseparate database associated to the application.\\nThis role will require a password to be entered upon login to the application. This password\\nwill be used to protect the application and will be required in order to read the LPA.\\nWe will create a role that will require a password to view the LPA. This role will be\\nassociated to the application and will require a password. The password will be stored in a\\nseparate database associated to the application.\\nWe will create a role that will require a password to view the LPA. This role will be\\nassociated to the application and will require a password. The password will be stored in a\\nseparate database associated to the application.\\nWe will create a role that will require a password to view the LPA. This role will be\\nassociated to the application and will require a password. The password will be stored in a\\nseparate database associated to the application.\\nWe will create a role that will require a password to view the LPA. This role will be\\n"}
{"Context":"## Context and Problem Statement\\nAs of today, kernel and unity have huge dependencies in business logic: sagas in TS, requests are made in TS, and everything is used from Unity.\\nThat generates coupling between technologies and teams, generating blockers and planning problems.\\n","Decision":"## Decision\n* Go for this proposal \u2705\\n### Actionables\\n- When starting development on new features, aim for this approach from the get-go\\n- Features that are already advanced in development or finished will be refactored gradually\\n- We will start by exposing signed requests and migrating the quests feature saga\\nNew features for the ECS\/SDK will have the common product process (RFC, roadmap, backlog, refinement). Alvaro will gather together the team when necessary to discuss implementation of new changes to the SDK.\\nCLI should be handled in the same way as the kernel and SDK developments.\\n","Prediction":"## Decision\nWe will use a [common domain model](https:\/\/en.wikipedia.org\/wiki\/Common_domain_model) to define the domain of the application and the business logic of the app.\\nThis model will be used by all the team, and it will be used across all the SDKs.\\nThis model will be used for the following:\\n* [Domain Models](https:\/\/en.wikipedia.org\/wiki\/Domain_model)\\nThe domain model will be used by all the team to define the application domain and the domain of the business logic.\\n* [Domain-Driven Design](https:\/\/en.wikipedia.org\/wiki\/Domain-driven_design)\\nThe domain model will be used by all the team to define the application domain and the domain of the business logic.\\n* [Domain-First Design](https:\/\/en.wikipedia.org\/wiki\/Domain-first_design)\\nThe domain model will be used by all the team to define the application domain and the domain of the business logic.\\n* [Domain-Object Design](https:\/\/en.wikipedia.org\/wiki\/Domain-object_design)\\nThe domain model will be used by all the team to define the application domain and the domain of the business logic.\\n* [Domain-Object-Relational Mapping](https:\/\/en.wikipedia.org\/wiki\/Domain-Object-"}
{"Context":"## Context\\nOur two feeds for court case data, Libra and Common Platform, both send data to us in a way that can result in concurrent database updates leading to errors.\\n- **Libra** - Lists of court cases received as a batch at specified times throughout the day. In practice, we quite often see multiple updates to the same cases coming either in the same payloads or in very quick succession. The effect of this is that multiple updates to the same case are processed at `court-case-service` within milliseconds of each. This can lead to race-conditions when updating records causing persistence to fail on some of these requests so they have to be retried.\\n- **Common Platform** - Individual updates to cases are received in real time throughout the day. Again, we very frequently see multiple updates for the same cases coming through in immediate succession leading to the same issues as seen with Libra data.\\nPrior to this ADR being adopted, the most common failures we saw as a result of this concurrency were:\\n1. **[OptimisticObjectLockingExceptions](https:\/\/www.baeldung.com\/jpa-optimistic-locking)**: Thrown when a change happens between reading the state of an object and subsequently saving it.\\n2. **DataIntegrityViolationException**: Thrown when an attempted update violates a database constraint. Specifically `offender_match_group_uq1` - compound key `defendantId` and `caseId` must be unique for a give offender match group\\nNeither of these are fatal errors within the context of the larger system because the `court-case-matcher` will retry failed requests and the retries will generally succeed. The frequency of these issues is a definite problem though as our alerts channel is rife with these errors, potentially obscuring more serious issues.\\n# Decision\\nThough the `court-case-matcher` will also retry on failed HTTP calls, we have decided to retry within `court-case-service` as these are known and recoverable exceptions. This is also preferable to updating our alerting to filter out these exceptions which is the other alternative but isn't really dealing with the underlying problem.\\n1. Apply `@Transactional(isolation = Isolation.REPEATABLE_READ)` to force transactions to acquire a lock before attempting an update (see docs [here](https:\/\/www.baeldung.com\/spring-transactional-propagation-isolation) for isolation level). If this acquisition fails then a `CannotAcquireLockException` will be thrown. Though this does not prevent the transaction from failing in the first place it does provide a guarantee using database level mechanisms that no bad updates can be applied. Also, as a lock is required before beginning the transaction it will fail fast rather than by doing the query portion of the transaction and falling over on the save as it does currently.\\n2. Apply `@Retryable(value = CannotAcquireLockException.class)`. This in conjunction with `1` this will allow recovery from failed transactions caused by concurrent updates where a lock could not be acquired. If the application fails to get the lock, it then simply tries again after a second when the conflicting transaction should hopefully have completed.\\n3. Apply `@Retryable(value = DataIntegrityViolationException.class)` to catch instances of failure `2`. These happen as a result of two new records with the same compound key being created concurrently. This differs from the first failure mode in that it cannot be guarded against with a row level lock as the conflicting row causing the failure does not exist when the transaction is started. As the code checks for existing records prior to creating new ones, if we retry here it will now update the conflicting record rather than attempting to create a new one.\\n","Decision":"## Decision\nThough the `court-case-matcher` will also retry on failed HTTP calls, we have decided to retry within `court-case-service` as these are known and recoverable exceptions. This is also preferable to updating our alerting to filter out these exceptions which is the other alternative but isn't really dealing with the underlying problem.\\n1. Apply `@Transactional(isolation = Isolation.REPEATABLE_READ)` to force transactions to acquire a lock before attempting an update (see docs [here](https:\/\/www.baeldung.com\/spring-transactional-propagation-isolation) for isolation level). If this acquisition fails then a `CannotAcquireLockException` will be thrown. Though this does not prevent the transaction from failing in the first place it does provide a guarantee using database level mechanisms that no bad updates can be applied. Also, as a lock is required before beginning the transaction it will fail fast rather than by doing the query portion of the transaction and falling over on the save as it does currently.\\n2. Apply `@Retryable(value = CannotAcquireLockException.class)`. This in conjunction with `1` this will allow recovery from failed transactions caused by concurrent updates where a lock could not be acquired. If the application fails to get the lock, it then simply tries again after a second when the conflicting transaction should hopefully have completed.\\n3. Apply `@Retryable(value = DataIntegrityViolationException.class)` to catch instances of failure `2`. These happen as a result of two new records with the same compound key being created concurrently. This differs from the first failure mode in that it cannot be guarded against with a row level lock as the conflicting row causing the failure does not exist when the transaction is started. As the code checks for existing records prior to creating new ones, if we retry here it will now update the conflicting record rather than attempting to create a new one.\\n","Prediction":"## Decision\nThough the `court-case-matcher` will also retry on failed HTTP calls, we have decided to retry within `court-case-service` as these are known and recoverable exceptions. This is also preferable to updating our alerting to filter out these exceptions which is the other alternative but isn't really dealing with the underlying problem.\\n1. Apply `@Transactional(isolation = Isolation.REPEATABLE_READ)` to force transactions to acquire a lock before attempting an update (see docs [here](https:\/\/www.baeldung.com\/spring-transactional-propagation-isolation) for isolation level). If this acquisition fails then a `CannotAcquireLockException` will be thrown. Though this does not prevent the transaction from failing in the first place it does provide a guarantee using database level mechanisms that no bad updates can be applied. Also, as a lock is required before beginning the transaction it will fail fast rather than by doing the query portion of the transaction and falling over on the save as it does currently.\\n2. Apply `@Retryable(value = CannotAcquireLockException.class)`. This in conjunction with `1` this will allow recovery from failed transactions caused by concurrent updates where a lock could not be acquired. If the application fails to get"}
{"Context":"## Context\\nWe wanted a test framework to provide some scaffolding for testing new k8s code.\\nSome folks on the team had explained Ginkgo's problems with test pollution and\\nconcurrency to many-a-pair. @christarazi wanted to use tools with broader\\nacceptance in the k8s\/go community. Testify vs Gomega also came into play.\\n","Decision":"## Decision\nWe will try Stephen Levine's [\"spec\" library](https:\/\/github.com\/sclevine\/spec)\\nWe will try [testify](https:\/\/github.com\/stretchr\/testify) while we're at it.\\n","Prediction":"## Decision\nWe will use Testify to write and run tests for new code.\\nWe will use Gomega to run tests for existing code.\\nWe will use Gopkg.in to find and use the right tooling in the community.\\nWe will use Github Actions to run tests.\\nWe will use Codecov to monitor code coverage.\\nWe will use [Gopkg.in](https:\/\/gopkg.in) for dependency management.\\nWe will use [Testify](https:\/\/testify.dev) for testing new code.\\nWe will use [Gomega](https:\/\/github.com\/awesomemock\/go-gomega) for testing\\nexisting code.\\nWe will use [Gopkg.in](https:\/\/gopkg.in) for dependency management.\\nWe will use [Testify](https:\/\/testify.dev) for testing new code.\\nWe will use [Gomega](https:\/\/github.com\/awesomemock\/go-gomega) for testing\\nexisting code.\\nWe will use [Gopkg.in](https:\/\/gopkg.in) for dependency management.\\nWe will use [Testify](https:\/\/testify.dev) for testing new code.\\nWe will use [Gomega](https"}
{"Context":"## Context\\nWe need a way to manage our infrastructure as code (IaC) to support CI\/CD and manage and provision our computing, storage and networking resources in the cloud.\\nWe use Terraform extensively on all our existing services, as well as a central Terraform repository for managing all our services resources, permissions and security needs.\\n","Decision":"## Decision\nWe should continue to use Terraform taking advantage of our existing lessons learnt and best practices.\\n### Consequences\\nWe will be able to take advantage of our existing best practices and knowledge to quickly setup any new infrastructure.\\n","Prediction":"## Decision\nWe will use Terraform to manage our infrastructure as code (IaC) for our services.\\nWe will use AWS CodeBuild and CodePipeline to provision infrastructure as code (IaC) build and deployment artifacts, and to deploy our infrastructure as code (IaC) manifests to the appropriate regions.\\nWe will use Terraform to manage our existing services.\\nWe will use Terraform to manage our existing services.\\nWe will use Terraform to manage our existing services.\\nWe will use Terraform to manage our existing services.\\nWe will use Terraform to manage our existing services.\\nWe will use Terraform to manage our existing services.\\nWe will use Terraform to manage our existing services.\\nWe will use Terraform to manage our existing services.\\nWe will use Terraform to manage our existing services.\\nWe will use Terraform to manage our existing services.\\nWe will use Terraform to manage our existing services.\\nWe will use Terraform to manage our existing services.\\nWe will use Terraform to manage our existing services.\\nWe will use Terraform to manage our existing services.\\nWe will use Terraform to manage our existing services.\\nWe will use Terraform to manage our existing"}
{"Context":"## Context\\nSubstepsList needs to be animated between showing and hiding the substeps of a step.\\nAnimating proves convenient on `max-height`\\nWe were initially hard coding the `max-height` property of the substeps list to have a maximum to animate too (from 0).\\nHowever, hard-coding that value proved not flexible, especially when the substep was also holding an `ErrorMessage`, which could span an undeterminate height (multiple lines, responsiveness, etc).\\nThe height of the list all determines the height of the progress bar which at this point is settled and working with little work.\\n","Decision":"## Decision\nWe chose to follow the second approach which requires less code to work. It is however creating issues with the `html` templating function of `lit-element`, since we change the state of the attribute without informing the library.\\nDoing the latter would mean re-rendering via a state prop which would have to be updated at the `_didRender` moment, and would create complains from Polymer.\\nWe thus decided to trick the component:\\n- we only calculate the `totalHeight` once, since content will not change once rendered.\\n- we only force the `max-height` with the calculated height once, if the component was forced rendered and if we haven't applied the trick already\\nWe discovered that it would then require 3 clicks on the `hide` button to finally close the list. Some research led us to discover that the `html` templating function handles the values of the attributes, and does not trigger a change (cf: https:\/\/github.com\/Polymer\/lit-html\/blob\/master\/src\/core.ts#L538). As such, in the state of the code, the active value is `0` and not `[totalHeight]px` as we forced it to be. Since the closing value is 0 too, the code didn't see the reason for the change.\\nTo circumvent this issue, we are now setting the `hidden` state value to be 1px, which allows for the initial closing.\\n","Prediction":"## Decision\n* Animations are only used after the `max-height` property is set to a value which is not set to `0`.\\n* Animations are only used after the `max-height` property is set to a value which is not set to `0`.\\n* The `max-height` property is set to the value of the progress bar's `height` at the time the animation is triggered.\\n* The animation is triggered after the `max-height` property is set to a value which is not set to `0`.\\n* The animation is triggered after the `max-height` property is set to a value which is not set to `0`.\\n* If the animation is triggered before the `max-height` property is set to a value which is not set to `0` (i.e.\\ the progress bar is not at `0`), the animation is triggered after the `max-height` property is set to a value which is not set to `0`.\\n* If the animation is triggered before the `max-height` property is set to a value which is not set to `0` (i.e.\\ the progress bar is not at `0`), the animation is triggered after the `max"}
{"Context":"## Context\\nENA presentation requires a top-level, numerical only taxID.\\n","Decision":"## Decision\nWe have added a taxID field at the top-level of all biosamples.\\n- This is a single, top-level, numeric field named 'taxId'.\\n- If no taxon available the value of taxId will be 0.\\n- There is no support for multiple taxId as there is no data that meets this requirement.\\n","Prediction":"## Decision\nA unique taxID will be used to uniquely identify each record. This will be generated by a unique GUID.\\n### Example\\nA record for a single country will have a taxID of `EN_AUS_2019-2020`. This will be generated as follows:\\n`EN_AUS_2019-2020_0001`\\n`EN_AUS_2019-2020_0002`\\n`EN_AUS_2019-2020_0003`\\netc.\\n### Example for a country with two taxIDs (e.g. the UK and Australia)\\nEach taxID will be unique to a country. For example, a taxID for the UK would be `EN_UK_2019-2020_0001`\\nand a taxID for the UK would be `EN_UK_2019-2020_0002`\\n### Example for a country with three taxIDs (e.g. Canada, New Zealand, and Australia)\\nEach taxID would be unique to a country. For example, a taxID for the UK would be `EN_UK_2019-2020_0001`\\nand a taxID for the UK would be `EN"}
{"Context":"## Context\\nWe are testing how deploy in diferente databases per envirtoment, and don't find the way of do it using the pluggin\\n","Decision":"## Decision\nDevelop the migration part in the code\\n","Prediction":"## Decision\nWe decided to use the [Docker Compose](https:\/\/docs.docker.com\/compose\/)\\nto deploy the app\\nWe are using the [Kubernetes](https:\/\/kubernetes.io\/) to create the environment\\nWe are using the [Docker Compose](https:\/\/docs.docker.com\/compose\/)\\nto create the services\\nWe are using the [Docker Compose](https:\/\/docs.docker.com\/compose\/)\\nto create the volumes\\nWe are using the [Docker Compose](https:\/\/docs.docker.com\/compose\/)\\nto create the services\\nWe are using the [Docker Compose](https:\/\/docs.docker.com\/compose\/)\\nto create the volumes\\nWe are using the [Docker Compose](https:\/\/docs.docker.com\/compose\/)\\nto create the services\\nWe are using the [Docker Compose](https:\/\/docs.docker.com\/compose\/)\\nfor the services\\nWe are using the [Docker Compose](https:\/\/docs.docker.com\/compose\/)\\nfor the volumes\\nWe are using the [Docker Compose](https:\/\/docs.docker.com\/compose\/)\\nfor the services\\nWe are using the [Docker Compose](https"}
{"Context":"## Context and Problem Statement\\nThe need to provide an HMRC footer translated into Welsh (PLATUI-752)\\n## Decision Drivers\\n* the lack of support for Welsh in the GDS govukFooter\\ncomponent\\n* the desire to maintain play-frontend-hmrc as a pure port of hmrc-frontend\\nwith some non-presentational helpers to aid use in Scala\/Play - conceptual integrity\\n* the desire to maintain hmrc-frontend as the source of truth for all presentational\\nmarkup - separation of concerns\\n","Decision":"## Decision\n* the lack of support for Welsh in the GDS govukFooter\\ncomponent\\n* the desire to maintain play-frontend-hmrc as a pure port of hmrc-frontend\\nwith some non-presentational helpers to aid use in Scala\/Play - conceptual integrity\\n* the desire to maintain hmrc-frontend as the source of truth for all presentational\\nmarkup - separation of concerns\\nChosen option \"maintain parity\" because we want hmrc-frontend to be the source of truth for presentation and\\nmaintain the separation of concerns between hmrc-frontend and\\nplay-frontend-hmrc\\n### Positive Consequences\\n* Maintain separation of concerns between hmrc-frontend and play-frontend-hmrc\\n* Maintain conceptual integrity for play-frontend-hmrc\\n* Be able to test markup using existing test strategy\\n### Negative Consequences\\n* We will need to add and maintain a new hmrcFooter component in\\nhmrc-frontend and play-frontend-hmrc providing a mirror of govukFooter with localised content,\\n* The new hmrcFooter component in hmrc-frontend will not be able to make use of any I18n features,\\n* We will need to create a new `helpers` package within play-frontend-hmrc to clearly demarcate them\\nfrom presentational components,\\n* We will need to redesign the existing hmrcFooter helper to wrap the new hmrcFooter component\\nand move to the helpers package,\\n* The new hmrcFooter component will need to be deprecated when GDS provide a localised version\\nof govukFooter,\\n* We will need to liaise and get approval from the HMRC design system team for adding\\nhmrcFooter to the hmrc\/design-system and hmrc\/hmrc-frontend\\n* We will be adding features that are unlikely to be useful or used by designers because\\nat the prototyping phase content is not stable enough for translation into Welsh\\n","Prediction":"## Decision\n* the lack of support for Welsh in the GDS govukFooter\\ncomponent\\n* the desire to maintain play-frontend-hmrc as a pure port of hmrc-frontend\\nwith some non-presentational helpers to aid use in Scala\/Play - conceptual integrity\\n* the desire to maintain hmrc-frontend as the source of truth for all presentational\\nmarkup - separation of concerns\\nChosen option: \"Play-frontend-hmrc\", because it is the most obvious and\\nconvenient option for the immediate future and because the need for a Welsh\\nfooter is not urgent.\\n### Positive Consequences\\n* play-frontend-hmrc is a pure play-frontend port of hmrc-frontend\\n### Negative Consequences\\n* hmrc-frontend will be deprecated\\n* hmrc-frontend will not be maintained as a pure play-frontend\\n* hmrc-frontend will not be maintained as the source of truth for all presentational\\nmarkup\\n### Impact on other systems\\n* HMRC will not be able to use hmrc-frontend directly to generate a Welsh\\nfooter\\n* hmrc-frontend will be removed from play-frontend\\n* hmrc-frontend will not be maintained as the source of truth for all"}
{"Context":"## Context\\nWikibase uses cache in different parts of PHP code base. Wikibase itself does not provide any abstraction for cache implementation. Instead MediaWiki-specific classes, such as abstract `BagOStuff`, or `WANObjectCache`, are used in code.\\nThe [PSR-16] standard defines an abstract Simple Cache interface for use in PHP code.\\nMore formal and verbose Caching interface has been defined as [PSR-6].\\nWe find it overly verbose compared to simpler [PSR-16], which we favour here.\\nPossible use of [PSR-16] in MediaWiki is out of scope of this decision.\\nIt might be a possible next step.\\nIn this regard it is possibly worth mentioning that using [PSR-6] interface in MediaWiki has been proposed in 2016, but it has been declined (see: [T130528]).\\nIt should also be noted that there is already a PSR-6 adapter for the `BagOStuff`: https:\/\/packagist.org\/packages\/addshore\/psr-6-mediawiki-bagostuff-adapter.\\nThere are already adapters that can use a PSR-16 cache as a PSR-6 cache and vice versa: https:\/\/symfony.com\/doc\/current\/components\/cache\/psr6_psr16_adapters.html.\\nAlso, as `Psr\\Cache\\CacheItemPoolInterface` and `Psr\\SimpleCache\\CacheInterface` declare only one method of the same name, `clear()`, and as both declarations have the same signature, any cache system may implement both interfaces.\\n","Decision":"## Decision\nWe will introduce PSR-16-compliant cache interface to Wikibase.\\nNew PHP code using cache will use this abstraction, instead of binding directly to MediaWiki cache classes, or any other specific third-party implementations.\\nWe will use [psr\/simple-cache] library to add `CacheInterface` to Wikibase.\\n","Prediction":"## Decision\nWe will use [PSR-16] interface in MediaWiki.\\nThis will enable MediaWiki to use cache in different parts of PHP code base.\\nWe will use [PSR-6] interface in MediaWiki.\\nMediaWiki will use cache in MediaWiki code.\\nWe will use [PSR-16] cache interface in MediaWiki.\\nMediaWiki will use cache in MediaWiki code.\\nWe will use [PSR-6] cache interface in MediaWiki.\\nMediaWiki will use cache in MediaWiki code.\\nWe will use [PSR-16] cache interface in MediaWiki.\\nMediaWiki will use cache in MediaWiki code.\\nWe will use [PSR-6] cache interface in MediaWiki.\\nMediaWiki will use cache in MediaWiki code.\\nWe will use [PSR-16] cache interface in MediaWiki.\\nMediaWiki will use cache in MediaWiki code.\\nWe will use [PSR-6] cache interface in MediaWiki.\\nMediaWiki will use cache in MediaWiki code.\\nWe will use [PSR-16] cache interface in MediaWiki.\\nMediaWiki will use cache in MediaWiki code.\\nWe will use [PSR"}
{"Context":"## Context and Problem Statement\\nDifferent projects have their EditorConfig files organized inconsistently.\\nThe order of key-value pairs is arbitrary.\\nThis makes it difficult to compare and merge settings.\\n## Decision Drivers\\n* Easy to compare and merge lists of key-value pairs\\n* Work nicely with version control systems\\n* Consistent and conventional look\\n* Easy to \u201cnormalize\u201d the list of settings\\n* Easy to find desired setting\\n","Decision":"## Decision\n* Easy to compare and merge lists of key-value pairs\\n* Work nicely with version control systems\\n* Consistent and conventional look\\n* Easy to \u201cnormalize\u201d the list of settings\\n* Easy to find desired setting\\nChosen option: \u201cNormalize EditorConfig by removing in-section comments and sorting key-value pairs alphabetically\u201d, because it is easy to maintain and to come to agreement between different editors and IDEs.\\nThe same applies for .gitignore and .gitattributes files: keep them sorted, remove groupings and comments.\\n### Negative Consequences\\n* Cannot attach relevant information in comments.\\n","Prediction":"## Decision\n* Easy to compare and merge lists of key-value pairs\\n* Work nicely with version control systems\\n* Consistent and conventional look\\n* Easy to \u201cnormalize\u201d the list of settings\\n* Easy to find desired setting\\nChosen option: \"A convention of the EditorConfig files\", because it's easy to read and work with.\\n### Positive Consequences\\n* The EditorConfig files are consistent and conventional\\n* Easy to compare and merge key-value pairs\\n* Easy to work with version control systems\\n* Easy to find desired settings\\n### Negative Consequences\\n* The EditorConfig files are not standardized\\n* Some key-value pairs are not well organized\\n* Difficult to compare and merge key-value pairs\\n* Difficult to find desired settings\\n* Difficult to normalize the list of settings\\n* Difficult to work with version control systems\\n* Difficult to find desired settings\\n## Decision\n* Easy to compare and merge key-value pairs\\n* Easy to work with version control systems\\n* Easy to find desired settings\\n### Positive Consequences\\n* The EditorConfig files are consistent and conventional\\n* Easy to compare and merge key-value pairs\\n* Easy to work with version control systems\\n* Easy to find desired settings\\n### Negative"}
{"Context":"## Context\\nIn order to create a filter, a user must provide a unique way of referencing\\na specific version\/instance document. Instance IDs uniquely identify the document\\nbut it may not be clear to users what this ID represents.\\nIt is likely that users understand which version, of which edition, of which\\ndataset they are interested in - and this also uniquely identifies the document.\\n","Decision":"## Decision\nFilters should be created using a combination of dataset\/edition\/version IDs\\n","Prediction":"## Decision\nWe will use a version number as the unique ID for each instance document\\nand will use this as the basis for the unique ID for each filter.\\nWe will use a version number for each dataset (rather than a separate document)\\nfor the unique ID.\\nExample:\\n- 1.0.0\\n- 1.0.1\\n- 1.0.2\\n- 1.0.3\\n- 1.0.4\\n- 1.0.5\\n- 1.0.6\\n- 1.0.7\\n- 1.0.8\\n- 1.0.9\\n- 1.0.10\\n- 1.0.11\\n- 1.0.12\\n- 1.0.13\\n- 1.0.14\\n- 1.0.15\\n- 1.0.16\\n- 1.0.17\\n- 1.0.18\\n- 1.0.19\\n- 1.0.20\\n- 1.0.21\\n- 1.0.22\\n- 1.0.23\\n- 1."}
{"Context":"## Context\\nThe teams need timely notifications based on key indicators in order that they can ensure reliability and respond to issues.\\nThe prometheus operator included in the GSP cluster can provide Alertmanager however we would like to manage alert routing across GDS and not duplicate routing rules or manage multiple sets of alert targets.\\n","Decision":"## Decision\nWe will route alerts to a separately hosted shared [Alertmanager](https:\/\/prometheus.io\/docs\/alerting\/alertmanager\/) to handle platform alert routing\\n","Prediction":"## Decision\nAlertmanager will be used to manage alert routing.\\nAlertmanager will be used for data collection and management.\\nAlertmanager will be used for alert routing.\\nAlertmanager will be used to manage and deploy alerts.\\nAlertmanager will be used to create and manage alerts in GDS.\\nAlertmanager will be used to manage and deploy alerts in GDS.\\nAlertmanager will be used to manage and deploy alerts in GDS.\\nAlertmanager will be used to manage and deploy alerts in GDS.\\nAlertmanager will be used to manage and deploy alerts in GDS.\\nAlertmanager will be used to manage and deploy alerts in GDS.\\nAlertmanager will be used to manage and deploy alerts in GDS.\\nAlertmanager will be used to manage and deploy alerts in GDS.\\nAlertmanager will be used to manage and deploy alerts in GDS.\\nAlertmanager will be used to manage and deploy alerts in GDS.\\nAlertmanager will be used to manage and deploy alerts in GDS.\\nAlertmanager will be used to manage and deploy alerts in GDS.\\nAlertmanager will be used to manage and deploy alerts in GDS.\\nAlertmanager will be used to manage and deploy alerts in GDS"}
{"Context":"## Context\\nAll current IAPT services should have an ODS code. Any future services must\\nhave an ODS code assigned. There for it is no longer necessary to use local\\ndata to relate CCGs to IAPT services.\\n","Decision":"## Decision\nRemove local data for all CCGs. The source for CCG to IAPT service relationship\\ndata will only be the central data store.\\n","Prediction":"## Decision\nAll CCGs will have an ODS code assigned. Any future IAPT services must have an\\ndata relation to the ODS code.\\nThis will allow for the following:\\n* Linking IAPT services to CCGs\\n* Linking CCGs to IAPT services\\n* Using CCGs as a source of data for IAPT services\\n* Linking IAPT services to CCGs\\n* Linking CCGs to IAPT services\\n* Using CCGs as a source of data for IAPT services\\n## Decision\nAll CCGs will have an ODS code assigned. Any future IAPT services must have an\\ndata relation to the ODS code.\\nThis will allow for the following:\\n* Linking IAPT services to CCGs\\n* Linking CCGs to IAPT services\\n* Using CCGs as a source of data for IAPT services\\n* Linking IAPT services to CCGs\\n* Linking CCGs to IAPT services\\n* Using CCGs as a source of data for IAPT services\\n### Positive impact\\n* Linking CCGs to IAPT services\\n* Linking CCGs to IAPT"}
{"Context":"## Context\\nShopware offers no way of unified refund handling. This results in every payment extension either implementing it themselves or not at all.\\n","Decision":"## Decision\nWe want to implement the following structure to offer a unified refund handling for all extension types.\\n","Prediction":"## Decision\nShopware will provide a unified refund handling by implementing a `CustomCheckout` abstraction. This abstraction will allow the developer to choose which refund handling is chosen when a customer returns an item.\\nThis abstraction will also allow the developer to choose a custom refund handling that will be applied to any unfulfilled refund requests.\\nFor example, a developer can choose to use the `StripeCheckout` abstraction, but also choose to apply a `PayPalCheckout` abstraction to also allow refunds to be issued via the PayPal payment gateway.\\nThis abstraction will be available in the following ways:\\n- In the `CoreCheckout` abstraction\\n- In the `Checkout` abstraction\\n- In the `CheckoutAction` abstraction\\n- In the `CheckoutAction` abstraction\\n- In the `CheckoutAction` abstraction\\n- In the `CheckoutAction` abstraction\\n- In the `CheckoutAction` abstraction\\n- In the `CheckoutAction` abstraction\\n- In the `CheckoutAction` abstraction\\n- In the `CheckoutAction` abstraction\\n- In the `CheckoutAction` abstraction\\n- In the `CheckoutAction` abstraction\\n- In the `CheckoutAction` abstraction\\n- In the `CheckoutAction` abstraction\\n- In the `CheckoutAction` abstraction\\n- In"}
{"Context":"## Context\\nWe are currently using a role based access control mechanism with the following roles\\n- Owner\\n- Organization Owner\\n- Supervisor\\n- Analyst\\n- Counsellor\\nAs the application grows, we foresee an explosion in the roles and their abilities. This\\nwould be very difficult to track with static roles. This also causes confusion to users\\nas we add more nuanced roles.\\n","Decision":"## Decision\nWe propose moving to a permissions based access control model that can give granular access\\nto dashboard features. Each feature on the dashboard is backed by its on permission and user\\naccess can be granted and revoked accordingly.\\n","Prediction":"## Decision\nWe will use the [github.com\/Heroku\/annotations](https:\/\/github.com\/Heroku\/annotations) to\\nprovide role based access control to our application. This will allow us to track\\nwhich users can perform which roles.\\n### Use cases\\n- Users are able to read and write data in the application\\n- Users are able to create data in the application\\n- Users are able to delete data in the application\\n- Users are able to view data in the application\\n- Users are able to update data in the application\\n- Users are able to delete data in the application\\n- Users are able to view data in the application\\n- Users are able to update data in the application\\n- Users are able to delete data in the application\\n- Users are able to view data in the application\\n- Users are able to update data in the application\\n- Users are able to delete data in the application\\n- Users are able to view data in the application\\n- Users are able to update data in the application\\n- Users are able to delete data in the application\\n- Users are able to view data in the application\\n- Users are able to update data in the application\\n- Users are able to delete data in the application\\n"}
{"Context":"## Context\\nJMAP messages have a handy preview property displaying the firsts 256 characters of meaningful test of a message.\\nThis property is often displayed for message listing in JMAP clients, thus it is queried a lot.\\nCurrently, to get the preview, James retrieves the full message body, parse it using MIME parsers, removes HTML and keep meaningful text.\\n","Decision":"## Decision\nWe should pre-compute message preview.\\nA MailboxListener will compute the preview and store it in a MessagePreviewStore.\\nWe should have a Cassandra and memory implementation.\\nWhen the preview is precomputed then for these messages we can consider the \"preview\" property as a metadata.\\nWhen the preview is not precomputed then we should compute the preview for these messages, and save the result for later.\\nWe should provide a webAdmin task allowing to rebuild the projection. The computing and storing in MessagePreviewStore\\nis idempotent and the task can be run in live without any concurrency problem.\\nSome performance tests will be run in order to evaluate the improvements.\\n","Prediction":"## Decision\nWe will remove the preview property and use the text of the message body for the preview.\\nThis will allow James to use the preview property only if a message has been sent and has a complete body.\\nThis will also allow James to use other properties for the preview.\\n### ChangeLog\\n### 1.0.0\\n* Initial release\\n### 1.1.0\\n* Fixed a bug that James was not parsing the first 256 characters of a message body correctly\\n### 1.2.0\\n* Removed the preview property\\n### 1.3.0\\n* Removed the preview property\\n### 1.4.0\\n* Removed the preview property\\n### 1.5.0\\n* Removed the preview property\\n### 1.6.0\\n* Removed the preview property\\n### 1.7.0\\n* Removed the preview property\\n### 1.8.0\\n* Removed the preview property\\n### 1.9.0\\n* Removed the preview property\\n### 1.10.0\\n* Removed the preview property\\n### 1.11.0\\n* Removed the preview property\\n### 1.12.0\\n* Removed the preview property"}
{"Context":"## Context\\nFor our initial iteration we should discuss whether we use a Puppetmaster architecture,\\nor use a masterless Puppet approach.\\nUsing the former approach, we would have to take the following into account:\\n- We would set up a Puppetmaster for the rest of the provisioned instances to connect to\\n- We would deploy new Puppet code to this machine only\\n- It matches our current architecture and approach\\n- We would need to take into account certificate management\\n- Each machine would \"check in\" to the Puppetmaster every 30 minutes, ensuring\\na consistent state\\nIf we used a masterless approach:\\n- We would need to deploy Puppet code and secrets to each instance that we provision\\n- We would run Puppet only when we deploy new code\\n- It differs from our current approach\\n- We would not have to manage a Puppetmaster which means no single point of failure\\n","Decision":"## Decision\nUse a Puppetmaster to not diverge from current architecture in the initial iteration.\\n","Prediction":"## Decision\nWe will use a masterless Puppet approach.\\nThis means we will set up the following structure on each of the provisioned instances:\\n- A Puppetmaster\\n- A Puppet Agent\\n- One or more Puppet manifests\\n- A Puppet manifest will be deployed to each Puppet Agent\\n- A Puppet manifest will contain the Puppet code to be deployed\\n- A Puppet manifest will contain the secrets to be deployed\\n- A Puppet manifest will contain the Puppet user credentials to be deployed\\n- If we deploy new Puppet code, we will deploy Puppet manifests for it\\n- If we deploy new Puppet code and secrets, we will deploy Puppet manifests for them\\n- A Puppet manifest will contain a list of Puppet manifests to deploy\\n- A Puppet manifest will contain a list of Puppet manifests to uninstall\\n- A Puppet manifest will contain a list of Puppet manifests to upgrade\\n- A Puppet manifest will contain a list of Puppet manifests to revert\\n- A Puppet manifest will contain a list of Puppet manifests to rollback\\n- A Puppet manifest will contain a list of Puppet manifests to apply\\n- A Puppet manifest will contain a list of Puppet manifests to delete\\n- A Puppet manifest will contain a list of Puppet manifests to destroy\\n- A Puppet manifest will contain a list of Puppet manifests"}
{"Context":"## Context\\nWe need a way to document major architecture decisions; in the past we have used the [Architecture Decision Record (ADR) format](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions). On past projects, we have found the ADR format to be a useful way to write and manage architecture decisions.\\nWe have written ADRs using both reStructuredText and Markdown formats on past projects. Certain documentation generators, such as Sphinx, can only use one of RST \/ Markdown. It is currently unknown which documentation generators we are likely to use for this project. The team is somewhat more comfortable writing in Markdown than RST.\\n","Decision":"## Decision\nWe have written ADRs using both reStructuredText and Markdown formats on past projects. Certain documentation generators, such as Sphinx, can only use one of RST \/ Markdown. It is currently unknown which documentation generators we are likely to use for this project. The team is somewhat more comfortable writing in Markdown than RST.\\nWe will continue to use the ADR format for writing architecture decisions for this project. We will use Markdown for formatting ADR documents.\\n","Prediction":"## Decision\nWe will use Markdown to document architecture decisions.\\nWe will use RST to document architecture decisions.\\nWe will use Sphinx to generate documentation.\\nWe will use Sphinx to generate documentation.\\nWe will use Sphinx to generate documentation.\\nWe will use Sphinx to generate documentation.\\nWe will use Sphinx to generate documentation.\\nWe will use Sphinx to generate documentation.\\nWe will use Sphinx to generate documentation.\\nWe will use Sphinx to generate documentation.\\nWe will use Sphinx to generate documentation.\\nWe will use Sphinx to generate documentation.\\nWe will use Sphinx to generate documentation.\\nWe will use Sphinx to generate documentation.\\nWe will use Sphinx to generate documentation.\\nWe will use Sphinx to generate documentation.\\nWe will use Sphinx to generate documentation.\\nWe will use Sphinx to generate documentation.\\nWe will use Sphinx to generate documentation.\\nWe will use Sphinx to generate documentation.\\nWe will use Sphinx to generate documentation.\\nWe will use Sphinx to generate documentation.\\nWe will use Sphinx to generate documentation.\\nWe will use Sphinx to generate documentation.\\nWe will use Sphinx to generate documentation.\\nWe will use Sphinx to generate documentation.\\nWe will use Sphinx to generate documentation.\\nWe will use Sphinx to generate documentation"}
{"Context":"## Context\\nCore developers are using JS maps files to keep all the selectors they need to interact with the BO theme. Here is an example of such a file:\\n```\\nexport default {\\nmainDiv: '#order-view-page',\\norderPaymentDetailsBtn: '.js-payment-details-btn',\\norderPaymentFormAmountInput: '#order_payment_amount',\\norderPaymentInvoiceSelect: '#order_payment_id_invoice',\\nviewOrderPaymentsBlock: '#view_order_payments_block',\\nprivateNoteToggleBtn: '.js-private-note-toggle-btn',\\nprivateNoteBlock: '.js-private-note-block'\\n}\\n```\\nQA team is keeping their own set of selectors to interact with during automated tests. Here is an example of such a file:\\n```\\nthis.documentTab = 'a#orderDocumentsTab';\\nthis.documentsTableDiv = '#orderDocumentsTabContent';\\nthis.documentsTableRow = `${this.documentsTableDiv} table tbody tr:nth-child(%ROW)`;\\nthis.documentNumberLink = `${this.documentsTableRow} td:nth-child(3) a`;\\nthis.documentName = `${this.documentsTableRow} td:nth-child(2)`;\\n```\\nSelectors should be mutualized so that when a dev changes a UI component or update a selector, the changes are reflected in the corresponding JS Map file and automated tests using this selector will keep working as intented without human intervention.\\n","Decision":"## Decision\nQA team and Core developers will populate and maintain their own set of selectors. There is no clear advantage to use a fusioned set of selectors.\\nHere are the main arguments against this decision:\\n* Devs use ES6, QA team use ES5. This means using a transpiler and adding libraries (babel).\\n* There is no clear conventions in existing JS Map files (names and content architecture vary).\\n* QA team need some selectors with modifiable input (with strings like %ROW or %COLUMN that must be replaced when used), which mean these types of selectors will be unusable for the Core dev team.\\n* Not the same needs: QA team need navigation selectors, Core dev team need interactive selectors. There is only a few selectors in common.\\n* Low risk: there is no evidence of any selector modification that caused automated tests to fail. When a test break, it's mostly because of a relatively big UI revamp, something that couldn't be fixed with just a selector modification anyway.\\n","Prediction":"## Decision\nWe will keep all JS maps files under src\/ and will use the following naming convention:\\n```\\nsrc\/JSMap.js\\n```\\nWe will also add a new attribute to the JS map file to indicate the selector that will be used in the corresponding JS map file. This will allow us to take advantage of the selector mutability.\\n```\\nexport default {\\nmainDiv: '#order-view-page',\\norderPaymentDetailsBtn: '.js-payment-details-btn',\\norderPaymentFormAmountInput: '#order_payment_amount',\\norderPaymentInvoiceSelect: '#order_payment_id_invoice',\\nviewOrderPaymentsBlock: '#view_order_payments_block',\\nprivateNoteToggleBtn: '.js-private-note-toggle-btn',\\nprivateNoteBlock: '.js-private-note-block'\\n}\\n```\\nWe will also add a new attribute to the JS map file to indicate the selector that will be used in the corresponding JS map file. This will allow us to take advantage of the selector mutability.\\n```\\nexport default {\\nmainDiv: '#order-view-page',\\norderPaymentDetailsBtn: '.js-payment-details-btn',\\norderPaymentFormAmountInput: '#order_payment_amount',"}
{"Context":"## Problem\\nA standard way of describing REST APIs offered by tools and plugins for Elektra is required to ease development for and usage of these. Because many good standards for describing APIs are out there already, an existing one shall be used.\\n","Decision":"## Decision\nThe decision is to use [API blueprints](https:\/\/apiblueprint.org\/) together with additional tools from its ecosystem.\\n","Prediction":"## Decision\nThe \"elektra-rest-api\" shall be used.\\nThis will allow for a consistent API description and API documentation, and will also allow for future extensions to be defined.\\nIt is recommended to create a new file called \"elektra-rest-api.yaml\", and add the following content to this file:\\n```yaml\\napiVersion: v1\\nkind: Service\\nmetadata:\\nname: elektra-rest-api\\nnamespace: elektra\\n```\\nThis file shall be stored in the same directory as the plugin or tool, and shall be named \"elektra-rest-api.yaml\".\\nThis file should contain the following fields:\\n* `apiVersion`: The version of the REST API (e.g. `v1`).\\n* `kind`: The name of the REST API.\\n* `namespace`: The namespace of the REST API.\\n* `service`: The name of the REST API service.\\n* `description`: The REST API description.\\n* `tags`: The REST API tags.\\n* `dependsOn`: The REST API service that depends on the REST API service.\\n* `dependsOnNamespace`: The REST API service that depends on the REST API service and namespace.\\n*"}
{"Context":"## Context\\nEven though Multichain (like Bitcoin) has to be bootstrapped from a single node, we're aiming at a system design that doesn't rely on a single node being involved in most administrative tasks, such as adding additional nodes to an existing deployment.\\nThis ADR discusses terminology and setup of the TruBudget network.\\n","Decision":"## Decision\n### Terminology\\nEach **node** is associated to a single _organization_ (e.g. ACMECorp, UmbrellaCorp).\\nThe **origin node** refers to the one node, which mines the first block in the whole network. Nodes that belong to the same organization are called _internal peers_, nodes that belong to other organizations are called _external peers_.\\nThe **organization's address** is used to identify the organization in the network.\\nThe **private key** is used to authenticate members of the organization, it is encrypted using the ORGANIZATION*VAULT_SECRET.\\nThe **ORGANIZATION_VAULT_SECRET** is a shared key between internal peers used to en-\/decrypt the private key. It is saved in an environment variable on the API host of a node.\\nA **user** is a member of an organization. To access the Trubudget API, the user is given a username and password. When performing actions (which are described by their \\_intent*, e.g. \"project.create\" or \"global.addUser\") the API writes this action in form of an _event_ to the blockchain.\\nAn **event** describes a performed action like creating a project or adding users and holds meta data like timestamp or creator.\\nExample for creating a group \"MyGroup\" with users \"user1\" and \"user2\":\\n```json\\n{\\n\"key\": \"123\",\\n\"intent\": \"global.createGroup\",\\n\"createdBy\": \"root\",\\n\"createdAt\": \"2018-12-17T14:52:11.511Z\",\\n\"dataVersion\": 1,\\n\"data\": {\\n\"group\": {\\n\"groupId\": \"123\",\\n\"displayName\": \"MyGroup\",\\n\"users\": [\"user1\", \"user2\"]\\n}\\n}\\n}\\n```\\nThe first data added to the blockchain is the organization's address and (encrypted) private key:\\n```json\\n{\\n\"publishers\": [\"1bNueyVy4j7V6yRSa6SDyHhkfVBHyFJ4QoAiD4\"],\\n\"keys\": [\"address\"],\\n\"offchain\": false,\\n\"available\": true,\\n\"data\": {\\n\"json\": {\\n\"address\": \"1bNueyVy4j7V6yRSa6SDyHhkfVBHyFJ4QoAiD4\",\\n\"privkey\": \"151fca5bbb689321a410c5646cc81a582c7bdade7365a806c994b530b9f28689e67b29af306996f0e95ea47a0ec66fc0bca9c78b7ce7510f71e95111a476d0cb6799b431b249d5a632ddc45aa1984f8fe8c2bcbd903bc6d9c6b8ba8458efb5b5\"\\n}\\n},\\n\"confirmations\": 8,\\n\"blocktime\": 1545054793,\\n\"txid\": \"c3ce3a37669d7ce005c7813a1e9a97152fac873cc9149ad81eadea5f78c6718e\"\\n}\\n```\\n### Distributing secret keys\\nIn order to prevent other organizations from using the private key, it is encrypted.\\nThe encryption key is a shared secret known to all internal peers, called the **organization vault secret** (OVS).\\nEach internal peer uses the address and private key of the first node of its organization after [joining the network](#joining-as-new-node-of-an-existing-organization).\\nEach action of any user is published to the multichain using the corresponding organization's private key ,no matter through which internal peer he\/she is connected.\\nSidenote:\\nEach user has their own pair of address and private key which is held by a streamitem on the _users_ stream, which currently is not in use.\\n(_In the future_ whenever a user logs in, the user's address should be read from the _users_ stream. Then this keypair is used to publish any data using command _publishfrom_)\\n### Creating a network\\nTo create a new network the origin node creates the genesis block (i.e. the first block of a blockchain) and the multichain creates the organization's address and private key. The new organization creates its own stream (e.g. org:ACMECorp) and writes its address and encrypted private key to the first streamitem ([see above](# Terminology)).\\n### Joining the network\\n#### Joining as new organization\\nIn this case, a new organization wants to join an already existing network. Example: ACMECorp already has one note in the network and UmbrellaCorp wants to join.\\nA = Orga A (e.g. ACMECorp)\\nB = Orga B (new organization, e.g. UmbrellaCorp)\\n```mermaid\\nsequenceDiagram\\nparticipant A as Orga A\\nparticipant B as Orga B\\nB->>A:connect\\nA->>A:Multichain: B has no connect permission\\nA->>A:Add connection attempt to dashboard\\nA-->>B:connection denied!\\nA->>A:Admin validates the address. If more than 50% of all organizations in the network validates the address, the new organization is approved.\\nB->>A:connect\\nA->>A:Multichain: B has connect permission\\nA-->>B:connection established!\\nB->>B:Initialize own organization stream\\nB->>B:Encrypt own private key with *organization vault secret* and save it to its organization stream\\n```\\n#### Joining as new node of an existing organization\\nIn this case a new node from an organization that already has a node in the network wants to join. Example: ACMECorp has one node in the network and a new ACMECorp node wants to join the network.\\nThe main difference here is, that you only need the approval of one admin user from another node in the network.\\nImportant: Even though the new node of the organization has its own address, only the address and private key of the already existing node is used for future transactions. It is therefore absolutely necessary that both nodes have the same ORGANIZATION_VAULT_SECRET.\\nNode A = first node of Orga A (e.g. ACMECorp)\\nNode A2 = second node of Orga A\\n```mermaid\\nsequenceDiagram\\nparticipant A as Node A\\nparticipant B as Node A2\\nA->>A: creates organization stream with organization key-pair\\nB->>A: connect with A2 node address\\nA->>A: Multichain: A2 has no connect permission\\nA->>A: Add connection attempt to dashboard\\nA-->>B: connection denied!\\nA->>A: *One* admin of any organization in the network approves the new node.\\nB->>A: connect\\nB->>B: read organization key-pair from stream and import into local wallet\\n```\\n","Prediction":"## Decision\n* We will use [TruCoin](https:\/\/trucoin.com\/) as our main blockchain.\\n* We will use [TruWallet](https:\/\/trucoin.com\/wiki\/TruWallet) as our main wallet.\\n* We will use [TruBlock](https:\/\/trucoin.com\/wiki\/TruBlock) as our main consensus layer.\\n* We will use [TruData](https:\/\/trucoin.com\/wiki\/TruData) as our main transaction layer.\\n* We will use [TruRelay](https:\/\/trucoin.com\/wiki\/TruRelay) as our main relay layer.\\n* We will use [TruCluster](https:\/\/trucoin.com\/wiki\/TruCluster) as our main cluster layer.\\n* We will use [TruBalance](https:\/\/trucoin.com\/wiki\/TruBalance) as our main balance layer.\\n* We will use [TruLedger](https:\/\/trucoin.com\/wiki\/TruLedger) as our main blockchain client.\\n* We will use [TruBlockWallet](https:\/\/trucoin.com\/wiki\/TruBlockWallet) as our main wallet client.\\n* We"}
{"Context":"## Context\\nAfter the offsite, there was a bit of confusion on what the default behaviour for running a Celestia Full node should be in the devnet. Since we decided on an architecture where Core nodes will communicate with Celestia Full nodes *exclusively* via RPC (over an HTTP connection, for example), it is necessary that a certain percentage of Celestia Full nodes in the devnet run with either an embedded Core node process or are able to fetch block information from a remote Core endpoint, otherwise there would be no way for the two separate networks (Core network and Celestia network) to communicate.\\n","Decision":"## Decision\nSince the flow of information in devnet is unidirectional, where Core nodes provide block information to Celestia Full nodes, the default behaviour for running a Celestia Full node is to have an embedded Core node process running within the Full node itself. Not only will this ensure that at least some Celestia Full nodes in the network will be communicating with Core nodes, it also makes it easier for end users to spin up a Celestia Full node without having to worry about feeding the Celestia Full node a remote Core endpoint from which it would fetch information.\\nIt is also important to note that for devnet, it should also be possible to run Celestia Full nodes as `standalone` processes (without a trusted remote or embedded Core node) as Celestia Full nodes should also be capable of learning of block information on a P2P-level from other Celestia Full nodes.\\n","Prediction":"## Decision\nCelestia Full nodes will run with a Core node process and should be configured to fetch block information from a remote Core endpoint.\\nThis will allow the devnet to run full nodes with the minimum amount of Core nodes required to run a full Celestia Full node. This will also allow for the devnet to run a full Celestia Full node without having to have a Core node process running.\\nThis decision was made after a number of experiments were performed with Core nodes running with Celestia Full nodes, and it was found that the Celestia Full node process was not able to communicate with the Core node process. The Core node process was unable to communicate with the Celestia Full node process because the Celestia Full node process was not able to communicate with the Core node process.\\nThis decision was made because there are a number of known issues with the Celestia Full node process (e.g. it is slow to start and stop, and has a high CPU usage). It was felt that the Celestia Full node process would be unable to communicate with the Celestia Full node process, which would result in an unresponsive devnet.\\nThis decision was made because it was found that the Celestia Full"}
{"Context":"## Context\\nA lot of different functions are required to support the business. These functions can be grouped into coherent sets, referred to as a solution. The functionality defined by the set of functions can then be implemented on the platform, fulfilling the business requirements.\\nThe solution can also implement domain crossing functionality, e.g. a service provided from one domain that is consumed in another domain.\\n","Decision":"## Decision\nWe define a solution as the implementation of a coherent set of business functions on the platform.\\n","Prediction":"## Decision\nThe solution will represent the business logic as a set of functional blocks, which can be implemented on the platform. This functionality can be implemented as a service that is consumed from the other domains.\\nThis allows the solution to be implemented on the platform, and also to be consumed from other domains.\\nFunctional blocks will be implemented using the [Functional Reactive Programming](https:\/\/en.wikipedia.org\/wiki\/FunrP) pattern.\\n### Use Cases\\nThe following use cases outline the use cases for the solution.\\n### Functional Blocks\\nFunctional blocks are used to implement the functionality. They are composed of a set of functions that are executed in order.\\nFunctional blocks are also used to provide the domain crossing functionality, which is required to consume the services provided by the functional blocks.\\n### Domain Crossing\\nThe solution will represent the domain crossing functionality as a set of functional blocks, which can be implemented on the platform.\\n### Domain Crossing Service\\nThe domain crossing service will act as the service that is consumed from the other domains. It will return the domain that it is crossing to the functional block, and the domain that the functional block is crossing to.\\n### Functional Block\\nFunctional blocks will be implemented using the [Functional Reactive Programming](https:\/\/en.wikipedia"}
{"Context":"## Context\\nThe Children's Social Care Alpha Service is seeking to pull together key data assets\\nsuch as Looked After Children and Foster Families. These will be sourced from Local\\nAuthority (LA) and Independent Foster Agency (IFA) Care Management Systems (CMS'),\\nas well as central Departmental data collections such as SSDA903.\\nThe SSDA903 collection and the CMS at LAs highlight the significance of individual\\npathways through the Care System. Each individuals pathway can be regarded as a\\nseries of events and touchpoints that the individual has had with various other actors\\nin the system, such as IFAs and Foster Families.\\nThe existing SSDA903 dataset is analogous to an event stream, with the Local\\nAuthorities (LAs) and Indedependent Fostering Agencies (IFAs) being event producers.\\n","Decision":"## Decision\n* Develop an API Gateway (see [Kong], [NGinx], [AWS API Gateway]) and REST Proxy, available primarily\\nto LAs and IFAs, serving an [Event Architecture] available internally.\\n* Use an Event Backbone (see [Kafka], [AWS Event Bridge]) to store and publish these events internally.\\n* Publish example client code on the Internet to ease LA and IFA adoption.\\n* Events will become the durable and persistent data which can be used to hydrate\\nthe Department's services, such as Diary Log and Matchmaker Assistant,\\n* [Event Sourcing] should be trialled as a solution.\\n","Prediction":"## Decision\nThis decision will be applied to the existing SSDA903 data asset and will not be\\napplied to the new data asset. This decision will be applied to the new data asset\\nfollowing the completion of the SSDA903 data asset.\\nThe new data asset will be built upon the existing data asset with the following\\nchanges:\\n* The LAs and IFAs will be represented as `Providers` in the new data asset\\n* The LAs and IFAs will be represented as `Sources` in the new data asset\\n* A new `Contact` table will be added to the existing `Provider` table\\n* The `Pathway` table will be added to the existing `Provider` table\\n* The `Pathway` table will be split into two tables: one for the `Pathway` and one\\nfor the `Pathways` that are created by the LAs and IFA's.\\n* The `Pathway` table will contain the following columns:\\n* `ProviderID` - the unique identifier for the provider\\n* `PathID` - the unique identifier for the path that the provider has created\\n* `PathTitle` - the title of the provider's path\\n* `PathDescription` - the"}
{"Context":"## Context and Problem Statement\\nHow to manage abbreviations\\n","Decision":"## Decision\nChosen option: \"glossaries-extra\", because seems to be best.\\n","Prediction":"## Decision\nChosen option: \"Use markdown\", because it is a standard for technical documentation and it is easy to use with the [Markdown formatter](https:\/\/markdown-editor.org\/). The Markdown formatter is included in the [Markdown formatter](https:\/\/markdown-editor.org\/) package and can be installed with the command `install.packages(\"markdown-formatter\")`.\\n### Positive Consequences <!-- optional -->\\n* [Markdown formatter](https:\/\/markdown-editor.org\/)\\n* [Markdown formatter](https:\/\/github.com\/markdudley\/markdown-formatter)\\n### Negative Consequences <!-- optional -->\\n* [Markdown formatter](https:\/\/github.com\/markdudley\/markdown-formatter)\\n* [Markdown formatter](https:\/\/github.com\/philw\/Markdown-formatter)\\n* [Markdown formatter](https:\/\/github.com\/philw\/Markdown-formatter)\\n### See also\\n* [Markdown](https:\/\/www.w3.org\/TR\/markdown\/)\\n* [Markdown formatter](https:\/\/github.com\/markdudley\/markdown-formatter)\\n* [Markdown formatter](https:\/\/github.com\/philw\/Markdown-formatter)\\n### License\\n* [MIT license](https:\/\/opensource.org\/licenses\/MIT)\\n* [See"}
{"Context":"## Context\\nCurrently, we have a fixed mapping between the inner goal identificator (actually, just an index in `Goals.goals` dictionary) and the user-visible goal number that is used for selecion (also called \"enumeration\"). This decision was easy to implement, but it seems to have significant downsides.\\n1. Goal numbers can only grow and grow. This makes long-living trees look ugly. We may have only few open goals in a tree, but most of them could have 3-digit numbers (because of a big amount of closed goals which are also used to generate enumeration).\\n2. It makes difficult to create nested goaltrees (see issue #6). It seems reasonably for the nested goaltree to have its own enumeration starting from 1. But the current enumeration function is not flexible enough to support this naturally.\\n","Decision":"## Decision\nWe decide to extract the enumeration function `id_mapping` out from the `Goals` class. It should be transformed into the wrapper which transforms actual goal identificators into user-visible numbers and, respectively, keeps backward mapping (which is used for goal selection).\\n","Prediction":"## Decision\nWe decided to remove the enumeration functionality from the `Goals` package and to use a custom `Goal` struct instead. The new `Goal` struct will have a `GoalId` field and will be used to determine the goal number. It will also have a `GoalTree` field that will have the tree structure of the goal. The `GoalTree` will contain a single `GoalId` that will be used to determine the goal number.\\n### Use a custom `Goal` struct\\nWe will use a custom `Goal` struct to represent a goal. A custom `Goal` struct will have two fields: a `GoalId` and a `GoalTree`. A `GoalId` is an integer that will be used to determine the goal number. A `GoalTree` will have a `GoalId` field that will be used to determine the goal number for a specific `GoalId`.\\n### Use a custom `GoalTree` structure\\nWe will use a custom `GoalTree` structure to represent a goal. A custom `GoalTree` structure will have two fields: a `GoalId` and a `GoalTree`. A `GoalId` is an integer that will be used to determine the goal number. A"}
{"Context":"## Context\\nContext here...\\n","Decision":"## Decision\nDecision here...\\n","Prediction":"## Decision\nDecision here...\\n### Positive Consequences\\n* We will be able to use the same tooling for both development and testing\\n### Negative Consequences\\n* We will have to maintain two separate tools, which is a bit of a pain\\n* There is more overhead in running the tests\\n* We have to run the tests before we run the code\\n### Impact on other systems\\n* We will have to run the tests before we run the code\\n### Decision\nDecision here...\\n### Positive Consequences\\n* We will be able to use the same tooling for both development and testing\\n### Negative Consequences\\n* There is more overhead in running the tests\\n* We have to run the tests before we run the code\\n* We have to run the tests before we run the code\\n* We have to run the tests before we run the code\\n* We have to run the tests before we run the code\\n* We have to run the tests before we run the code\\n### Impact on other systems\\n* We will have to run the tests before we run the code\\n* There is more overhead in running the tests\\n* We have to run the tests before we run the code\\n* We have to run the tests before we"}
{"Context":"## Context and Problem Statement\\nThe original goal for the reference implementation refactor was to provide an\\nimplementation which is both an aid to understanding the specification and a\\ngood architecture for other implementations to mimic.\\nDuring refactoring efforts on the metadata API and ngclient, several friction\\npoints have arisen where a safe object-oriented API would result in a less\\ndirect mapping to the [Document formats] in the specification.\\nThe archetypal example friction point is that [Timestamp] lists snapshot _only_\\nin a `meta` dictionary of `METAPATH` -> attribute fields. The dictionary will\\nonly ever contain one value and creates an extra level of indirection for\\nimplementations which try to map to the file format.\\nWhen presented with such cases, we have considered multiple options:\\n* Strict mapping to the [Document formats]\\n* Simple and safe API in preference to mapping to the [Document formats]\\n* Strict mapping to the [Document formats] with additional convenience API\\nwhich is documented as the preferred interface for users\\nSo far implementation has tended towards the final option, but this is\\nunsatisfying because:\\n* the API contains traps for the unsuspecting users\\n* two code paths to achieve the same goal is likely to result in inconsistent\\nbehaviour and bugs\\nTherefore, we would like to define our primary purpose so that we can make\\nconsistent decisions.\\n[Document formats]: https:\/\/theupdateframework.github.io\/specification\/latest\/#document-formats\\n[Timestamp]: https:\/\/theupdateframework.github.io\/specification\/latest\/#file-formats-timestamp\\n## Decision Drivers\\n* The reference implementation is often the starting point for new\\nimplementations, porting architecture of the reference implementation to new\\nlanguages\/frameworks\\n* Reading reference implementation code is a common way to learn about TUF\\n* The TUF formats include non-intuitive JSON object formats when mapping to OOP\\nobjects\\n* Multiple code paths\/API for the same feature is a common source of bugs\\n","Decision":"## Decision\n* The reference implementation is often the starting point for new\\nimplementations, porting architecture of the reference implementation to new\\nlanguages\/frameworks\\n* Reading reference implementation code is a common way to learn about TUF\\n* The TUF formats include non-intuitive JSON object formats when mapping to OOP\\nobjects\\n* Multiple code paths\/API for the same feature is a common source of bugs\\nPrimary purpose of the reference implementation is as an exemplary reference:\\nproviding a safe, consistent API for users and a good architecture for other\\nimplementations to mimic.\\n","Prediction":"## Decision\n* The reference implementation is often the starting point for new\\nimplementations, porting architecture of the reference implementation to new\\nlanguages\/frameworks\\n* Reading reference implementation code is a common way to learn about TUF\\n* The TUF formats include non-intuitive JSON object formats when mapping to OOP\\nobjects\\n* Multiple code paths\/API for the same feature is a common source of bugs\\nChosen option: \"Strict mapping to the [Document formats]\", because:\\n* The API contains traps for the unsuspecting users\\n* two code paths to achieve the same goal is likely to result in inconsistent\\nbehaviour and bugs\\nTherefore, we would like to define our primary purpose so that we can make\\nconsistent decisions.\\n[Document formats]: https:\/\/theupdateframework.github.io\/specification\/latest\/#document-formats\\n[Timestamp]: https:\/\/theupdateframework.github.io\/specification\/latest\/#file-formats-timestamp\\nWe will create a new `meta` dictionary that maps to the [Document formats] and\\nwill be used by all existing implementations that need to read, write and\\ntransmit data to and from the TUF specification.\\nWe will then create a new `meta` dictionary that maps to the [Timestamp] and\\n"}
{"Context":"## Context\\nTDR will have multiple deployment environments: integration, staging and\\nproduction. We want to be able to deploy new versions of our services to each\\nenvironment in turn.\\n","Decision":"## Decision\nConfigure each project's build and Jenkinsfile so that when code is merged to\\nmaster and the tests pass, Jenkins runs the following steps:\\n1. Create a git tag with a version number based on the Jenkins build number, at\\nthe commit that master is pointing to\\n2. Push the version tag to GitHub\\n3. If the repo is deployed as a Docker image (such as the [consignment\\nAPI][tdr-api] or the [frontend]), build the Docker image, tag it\\nwith the same version number and push it to the Docker image repository\\n(currently Docker Hub, but may soon be AWS ECR)\\nThis version is now available to deployed to each environment in turn.\\nThe master build should start an automated deployment to the Integration\\nenvironment, passing the version tag as a parameter. The deployment build will:\\n1. Pull the Docker image tagged with the given version tag\\n2. Retag the Docker image with the environment name (e.g. \"intg\")\\n3. Trigger an update in AWS, e.g. by calling the AWS API to restart the ECS\\nservice, which will pull the environment-specific tag\\n4. Push the git release branch matching the environment name to the same commit\\nas the given version tag\\nA developer can then manually start a release to Staging then Production,\\npassing the same version tag as a parameter OR the \"intg\" branch\/tag\\nname.\\n[tdr-api]: https:\/\/github.com\/nationalarchives\/tdr-consignment-api\\n[frontend]: https:\/\/github.com\/nationalarchives\/tdr-transfer-frontend\\n### Version numbers\\nThe version numbers will come from Jenkins build numbers, rather than using a\\nsemantic versioning system. This is because semantic versions aren't as\\nmeaningful for applications as for libraries, and because updating the minor or\\nmajor version would need a developer to bump the version number. It's more\\nuseful to have an auto-incrementing version number generated by something like\\nJenkins.\\nIf the build numbers are ever reset, for example because we rebuild Jenkins from\\nscratch, we can set the next build numbers so they start from the next version\\nrather than resetting back to build 1. To do this, you can run a script in the\\n[Jenkins script console][script-console]:\\n```\\ndef job = Jenkins.instance.getItemByFullName(\"name-of-jenkins-build\/master\")\\njob.nextBuildNumber = 123\\njob.save()\\n```\\n[script-console]: https:\/\/jenkins.tdr-management.nationalarchives.gov.uk\/script\\n### Docker retagging step\\nThe process described above is slightly inefficient, because the deployment\\nbuild needs to pull the Docker image tagged with the version number, retag it\\nwith the environment name, and then push it back to Docker Hub. This is because\\nthere's currently no simple way to add a new tag to a remote image without\\npulling and pushing it.\\nDocker Hub might support this in the future, which would make the deployment\\nstep slightly faster. Alternatively, [AWS ECR supports image\\nretagging][ecr-retag], so this step would be simpler when we switch to ECR.\\n[ecr-retag]: https:\/\/docs.aws.amazon.com\/AmazonECR\/latest\/userguide\/image-retag.html\\n","Prediction":"## Decision\nWe will use the [dotenv](https:\/\/github.com\/ehsan\/resolve-dotenv) library\\nto handle environment variables. The library will be used to read and write\\nenvironment variables to files, and will be used to inject environment\\nvariables into the runtime.\\n### How to use\\nEnvironment variables can be set at runtime by using a `dotenv` file. A\\n`dotenv` file can be configured using the environment variable `DOTENV`\\nvariable. A dotenv file can be configured using the environment variable\\n`DOTENV`.\\n### Example usage\\nThe dotenv library is already installed on the server. To set the environment\\nvariable variables, we can use the dotenv library to read the environment\\nvariables, and then use the environment variable variables to inject\\nenvironment variables into the runtime.\\n### Example usage\\nThe dotenv library is already installed on the server. To set the environment\\nvariable variables, we can use the dotenv library to read the environment\\nvariables, and then use the environment variable variables to inject\\nenvironment variables into the runtime.\\n### Example usage\\nThe dotenv library is already installed on the server. To set the environment\\nvariable variables, we can use the dotenv library to"}
{"Context":"## Context\\nThe app primarily displays information for medical staff working in hospitals.  The information needs to come from somewhere and there are a few options:\\n### Download content from a CMS\/S3 bucket\/Website\\nWe could pull the information in the app from somewhere on the web.  Some kind of CMS, S3 bucket or website where the files are hosted would be the most obvious choices.\\nThis does make it easier to quickly the information in the app, but would require the build out of more infrastructure and architecture to support all this.\\nIt also could mean that if the internet connection goes down to a hospital in  a crisis any information that isn't already cached in the app would be unavailable to users.\\n### Embed the content in the app\\nThe alternative method is to embed all of the information in the app so that all of the information is included, after downloading the app.\\nThis is architecturally much simpler and means all the content will be available offline if the internet should go down.\\nSubsequently, a new version of the app needs to be released to update the information.\\n","Decision":"## Decision\nFor the initial versions of the application, we will embed the content in the app.\\nOnce the situation stabilises, and we can provide support to more hospitals, we can re-visit the decision and decide if we want to download content.\\n","Prediction":"## Decision\nWe will use the first option to embed the content into the app.\\nThis will mean that the app will be updated in the background without any interruption to the user experience.\\nThis option also means that the app will be able to be updated to include new information as it becomes available.\\nThis will also mean that the app will be able to be updated to include new information as it becomes available.\\nThis will also mean that the app will be able to be updated to include new information as it becomes available.\\nThis will also mean that the app will be able to be updated to include new information as it becomes available.\\nThis will also mean that the app will be able to be updated to include new information as it becomes available.\\nThis will also mean that the app will be able to be updated to include new information as it becomes available.\\nThis will also mean that the app will be able to be updated to include new information as it becomes available.\\nThis will also mean that the app will be able to be updated to include new information as it becomes available.\\nThis will also mean that the app will be able to be updated to include new information as it becomes available.\\nThis will also mean that the app will be able"}
{"Context":"## Context\\nWe need a mechanism for highlighting security vulnerabilities in our code before\\nit reaches production environments\\n","Decision":"## Decision\nUse the [Brakeman](https:\/\/brakemanscanner.org\/) static security analysis tool\\nto find vulnerabilities in development and test\\n","Prediction":"## Decision\nWe will use [Vulnerability Tooling](https:\/\/vulnerabilitytooling.org\/) to detect and\\nreport security vulnerabilities in our code\\nWe will use [Kubernetes Security](https:\/\/kubernetes.io\/docs\/concepts\/understanding-security\/)\\nto monitor and manage security vulnerabilities in Kubernetes\\nWe will use [Ceph Security](https:\/\/ceph.com\/docs\/securing-ceph\/) to monitor and manage\\nvulnerabilities in Ceph\\nWe will use [Grafana](https:\/\/grafana.com\/) to monitor and manage security vulnerabilities\\nWe will use [Puppet](https:\/\/puppet.com\/), a tool for automating configuration management, to manage\\nvulnerabilities\\nWe will use [Puppet Labs Inc](https:\/\/puppetlabs.com\/) to automate vulnerability management\\nWe will use [Puppet Labs Inc](https:\/\/puppetlabs.com\/) to automate security patching\\nWe will use [Puppet Labs Inc](https:\/\/puppetlabs.com\/) to automate vulnerability remediation\\nWe will use [Puppet Labs Inc](https:\/\/puppetlabs.com\/) to automate incident response\\nWe will use [Puppet Labs Inc](https:\/\/puppetlabs.com\/) to automate vulnerability management\\nWe will use ["}
{"Context":"## Context\\nVerify's [doc-checking service](https:\/\/github.com\/alphagov\/doc-checking) is\\nsecured in part using mTLS. Currently, our clusters are fronted by ALBs which\\ncannot provide mTLS.\\nThe doc-checking service currently runs an nginx that provides the mTLS\\nfunctionality. In order for GSP to be able to allow something within the\\ncluster to perform mTLS we must run a load balancer that forwards unaltered TCP\\npackets in addition to, or instead of an ALB.\\n","Decision":"## Decision\nWe will optionally create and run an NLB in addition to the current ALB for\\nclusters that have a requirement to terminate their own TLS. This NLB will be\\navailable at `nlb.$CLUSTER_DOMAIN`.\\n","Prediction":"## Decision\nWe will use a load balancer that forwards unaltered TCP packets to the doc-checking\\nservice.\\nThis will allow GSP to run the doc-checking service without the need for an ALB.\\nThis will also allow GSP to use the existing load balancer that is already in use.\\nThis will also allow GSP to use the existing gRPC API that is already in use.\\nThis will also allow GSP to use the existing gRPC API that is already in use.\\n### Benefits\\n* We can run a load balancer that forwards unaltered TCP packets to the doc-checking\\nservice.\\n* We can use the existing gRPC API that is already in use.\\n* We can use the existing gRPC API that is already in use.\\n### Consequences\\n* We will need to add an ALB to the cluster to serve the doc-checking service.\\n* We will need to add an ALB to the cluster to serve the doc-checking service.\\n* We will need to modify the gRPC API to serve the doc-checking service.\\n* We will need to modify the gRPC API to serve the doc-checking service.\\n### Decision\nWe will use"}
{"Context":"## Context and Problem Statement\\nDue to the use of two different object types for transfering and storing data, the conversion between the types becomes a necessity. Therefore we need means to perform the respective conversion.\\n","Decision":"## Decision\nChosen option: \"[Model Mapper](http:\/\/www.modelmapper.org)\", because it fulfils our requirements, is highly configurable and less error prone then manually written converters.\\n### Positive Consequences <!-- optional -->\\n* Less boilerplate code\\n","Prediction":"## Decision\nWe will use [json-schema](https:\/\/www.json-schema.org\/) for the conversion of the data from the object type `Transfer` to the object type `JSONSchema`. This conversion is necessary to be able to interact with the `Transfer` object in the client.\\n[JSON Schema](https:\/\/www.json-schema.org\/) will be used to describe the data types and their possible values. This schema will be stored in the `transfer.json` file.\\nThe JSON Schema definition will be generated by a tool called [Schema.org](https:\/\/www.schema.org\/).\\nThe JSON Schema definition will be stored in the `transfer.json` file.\\nFor more information on how to generate Schema.org definitions, please see the [Schema.org](https:\/\/www.schema.org\/) page.\\n### Example of JSON Schema definition\\nThe JSON Schema definition for `Transfer` will be stored in the `transfer.json` file.\\nFor more information on how to generate Schema.org definitions, please see the [Schema.org](https:\/\/www.schema.org\/) page.\\n### Example of JSON Schema validation\\nWhen the `Transfer` object is used in the client, the `Transfer` schema will be validated against the `Transfer` schema. The `Transfer` schema will"}
{"Context":"## Context and Problem Statement\\nEach community has its own `contributing.md` file which outlines how are users expected to contribute to the community. That guide for contributions should also clearly state what will happen once a PR is submitted and how reviews are performed.\\n","Decision":"## Decision\nChosen option: \"Building upon Prow and modifying and evolving them to operations needs\"\\n### Prerequisites\\nWe use Prow to automate review process. That means we use following setup:\\n1. Each repository contains an `OWNERS` file:\\n1. There is always one `OWNERS` file in a repository root folder\\n2. If needed, additional `OWNERS` files can be created within nested folders. This file takes precedence for all resources within this folder.\\n2. `OWNERS` file contains several entries:\\n1. `approvers` is a list of GitHub users that are allowed to use the `\/approve` chat-ops command. Role of approvers is to gate PRs - they verify that changes in PRs respect our architectural decisions and are in line with the heading of the project. See [upstream docs][4] for better understanding of this role.\\n2. `reviewers` is a list of GitHub users that are allowed to use the `\/lgtm` chat-ops command. Their role is to ensure changes in PRs are implemented properly and meet our coding standards. See [upstream docs][4] for details.\\n### Process\\n1. A PR is submited\\n2. Prow (represented by a Sesheta bot-account) will pick a selection of reviewers from corresponding `OWNERS` file (based on modified files in the PR).\\n3. Reviewers assess the code for general code quality, correctness, sane software engineering, and style. They either provide feedback or respond with `\/lgtm` chat-ops command.\\n1. Once a `\/lgtm` command is sent, Prow (Sesheta) will add a `lgtm` label to the PR. This label will get removed if there are any new pushes to the PR.\\n2. Additionally Prow will ask users to assign a specific `approver` to the PR if there's no assignment yet.\\n4. Approvers evaluate architectural aspects of the PR and holistic acceptance criteria, including dependencies with other features, forwards\/backwards compatibility, API and flag definitions. They either provide feedback or respond with `\/approve` chat-ops command.\\n1. Once an `\/approve` command is sent, Prow (Sesheta) will add an `approve` label to the PR. This label stays after subsequent pushes to the PR.\\n5. If a PR is large in nature or introduces a breaking change we exercise an **informal rule**:\\n1. PR is put on hold via `\/hold`.\\n2. We wait for a second reviewer to come by and approve it via `\/lgtm` (2 different people have commented with `\/lgtm`).\\n3. PR is held in review for at least a day to give more people chance to look at it.\\n6. Prow (Sesheta) waits for conditions to be met to make PR eligible for merge. Required conditions (See [here][2] for details):\\n1. Required CI checks need to finish and pass. Which CI checks are required is configured per repository.\\n2. Both `approve` and `lgtm` labels are present.\\n7. Prow merges the pull request.\\nAdditional chat-ops commands to help with PR review process are:\\n* `\/retest` - re-run failed CI checks\\n* `\/cc @username` - request a review from an user\\n* `\/close` - close pull request\\n* `\/label` - add a label to the PR\\n* `\/ok-to-test` - if a PR was submitted by somebody who is not a member of the operate-first GitHub organization, Prow doesn't start CI until this command is used by a member\\n* `\/hold` - prevent PR from being merged\\n","Prediction":"## Decision\n* We will use the `contributing.md` file from each community in the repository to document how we expect users to contribute to the project\\n* When a PR is submitted, we will make a note in the `contributing.md` file to let users know that they should read the `contributing.md` file and follow the guidelines outlined in it\\n* The `contributing.md` file will be made available in the README of each project\\n* The `contributing.md` file will be included in the PR description in the `README` of each project\\n* If a PR is rejected, we will make a note in the `contributing.md` file to let users know that they should read the `contributing.md` file and follow the guidelines outlined in it\\n* If a PR is approved, we will make a note in the `contributing.md` file to let users know that they should read the `contributing.md` file and follow the guidelines outlined in it\\n* We will make the `contributing.md` file available in the `README` of each project\\n* We will make the `contributing.md` file available on the project's `index.html` page\\n* We will make the `contributing.md"}
{"Context":"## Context\\nWe need to create a riglet for our new bookit project so that we practice what we preach.\\n","Decision":"## Decision\nWe will use the AWS Bare Metal Riglet from bookit-riglet as a starting point for our riglet.  We will keep the previous bookit-riglet and create a new bookit-infrastructure project\/repo.\\nTechnologies:\\n* AWS: CloudFormation, ECR, ECS, Route53, VPC, ALB\\n* Deployment Mechanism: Docker images\\n* Build: Travis\\n","Prediction":"## Decision\nWe will use the [riglet package](https:\/\/github.com\/andrewcooke\/riglet) to create a riglet for our booklet.\\nWe will use the [booklet package](https:\/\/github.com\/andrewcooke\/booklet) to create a riglet for our booklet.\\nWe will use the [booklet-riglet package](https:\/\/github.com\/andrewcooke\/booklet-riglet) to create a riglet for our booklet.\\nWe will use the [booklet-riglet-riglet package](https:\/\/github.com\/andrewcooke\/booklet-riglet-riglet) to create a riglet for our booklet.\\nWe will use the [booklet-riglet-riglet-riglet package](https:\/\/github.com\/andrewcooke\/booklet-riglet-riglet-riglet) to create a riglet for our booklet.\\nWe will use the [booklet-riglet-riglet-riglet-riglet package](https:\/\/github.com\/andrewcooke\/booklet-riglet-riglet-riglet-riglet) to create a riglet for our booklet.\\n"}
{"Context":"## Context\\nSREs sometimes need to change various things about a cluster's worker nodes, and therefore replace all of the nodes:\\n* AMI (e.g. for an EKS update)\\n* instance type\\n* anything else in the launch template in some way (e.g. instance role)\\nIn addition to this, in our Terraform we get the latest AMI ID for our EKS version from AWS - this means that as soon as AWS releases a new AMI for our current EKS version, the next time the cluster's deployer step runs for Terraform it will also replace all the nodes.\\nWe have a CloudFormation UpdatePolicy on our worker node auto-scaling-groups that tells it to replace 1 node at a time in each ASG. This would be fine if we didn't have 3 ASGs with a small number of nodes in each - right now this practice terminates too many instances at once and results in the cluster becoming unstable. This in turn causes outages in the applications running in the cluster.\\nWithout such a policy we'd be able to update the launch template for the ASG and it wouldn't remove any existing nodes, just set up any new future nodes correctly.\\nThe reason we have 3 small ASGs is so the cluster auto-scaler can scale nodes independently in each AZ - it will know that a pod attempting (and failing) to schedule is tied to a persistent volume claim in a particular availability zone and can scale up the ASG for that zone.\\n","Decision":"## Decision\nWe will do action 3 (lifecycle hooks) (and therefore not action 2 (some other node rolling process)).  This will mean that the ASG will know how to wait for nodes to be ready in the cluster when launching, and how to wait for nodes to be drained before terminating.  It also means we can choose our method of node rolling with relative freedom.  We will keep our existing CloudFormation UpdatePolicy as our actual mechanism of rolling nodes.\\nWe will do action 4 (turn off autoscaler while rolling nodes).  This will probably need to be a script run before we run terraform (to scale the autoscaler down) and afterwards (to scale the autoscaler back up).\\n","Prediction":"## Decision\nWe will implement a CloudFormation UpdatePolicy on our worker node auto-scaling-groups that will update all of the worker nodes in the cluster in one go.\\nWe'll also make sure the auto-scaling groups are only created on the worker nodes that are currently available, so that they can be scaled up in a single operation.\\nWe'll also make sure the auto-scaling group names are unique within their availability zone, and that the auto-scaling group name doesn't include any characters that are known to be reserved for use in auto-scaling groups (e.g. a-z, A-Z, 0-9, a-z, etc) - this is to help prevent the auto-scaling group from being accidentally renamed or deleted.\\nWe'll also make sure the auto-scaling group name is in lowercase and uses all lowercase letters, and that the auto-scaling group name contains no spaces, special characters, or underscores.\\nWe'll also make sure the auto-scaling group name doesn't include any characters that are known to be reserved for use in auto-scaling groups (e.g. a-z, A-Z, 0-9, a-z, etc) - this is to help prevent the auto-scaling group from being accidentally renamed"}
{"Context":"## Context\\nWe are doing a rewrite.\\nHolochain Go code shows many implicit dependencies between different modules and stateful objects. In conjunction with the complexity of a p2p network of agents, this leads to a level of overall complexity that feels too much to manage. A clean and fitting architecture for this Rust rebuild is needed.\\nHaving a single global state within the agent feels appropriate and even balancing the distributed nature of the network of agents.\\n","Decision":"## Decision\nThe new holochain architecture will follow a redux architecture in order for an agent to have one global state.\\nWe will apply nested state objects which represent a state tree, with sub states for each module.\\nWe use reference counting smart pointers for the sub\\nstates such that it is possible for each module's\\nreducer to decide if the sub state is to be mutated or reused.\\n","Prediction":"## Decision\nWe will use [Distributed State Management](https:\/\/distributedstate.info\/) as our state store.\\nDistributed State Management is a [relatively new Rust library](https:\/\/github.com\/distributedstate\/distributed-state-management) that stores the state of the system in a single file. It also provides an API that is very similar to the one we have in Holochain Go; however, it is designed to be more efficient and more flexible.\\nWe will use [Distributed State Management](https:\/\/distributedstate.info\/) as our state store.\\nDistributed State Management is a [relatively new Rust library](https:\/\/github.com\/distributedstate\/distributed-state-management) that stores the state of the system in a single file. It also provides an API that is very similar to the one we have in Holochain Go; however, it is designed to be more efficient and more flexible.\\nWe will use [Distributed State Management](https:\/\/distributedstate.info\/) as our state store.\\nDistributed State Management is a [relatively new Rust library](https:\/\/github.com\/distributedstate\/distributed-state-management) that stores the state of the system in a single file. It also provides an API that is very similar to the"}
{"Context":"## Context\\nThe existing site (Frontend) includes Javascript in a variety of ways:\\n-   inline\\n-   script tags\\n-   dynamically added script tags\\nThere is a single (universal) boot process which conditionally loads additional\\nJS as required based on page metadata.\\nFor Dotcom Rendering we want to simplify things so that it is easy for a\\ndeveloper to know where to add a script\/functionality. We also want an approach\\nthat helps ensure good performance and, related, which is less monolithic.\\n","Decision":"## Decision\n-   Load scripts in the head as deferred, or as async at the bottom of the HTML\\nbody section\\n-   All other script loading (other than for commercial third-party code) is\\nprohibited\\n-   We will not use a universal 'boot' JS file, but load scripts which have a\\nunique purpose and which are self-contained aside from a few shared helpers\\nFuller discussion of this approach can be found here:\\nhttps:\/\/docs.google.com\/document\/d\/1EnkowJ7BFumrqkpkRacRl-gSkiJs9fUgRCixQTLxBvo\/edit#.\\n","Prediction":"## Decision\nUse a single boot process to load additional Javascript.\\nUse a single boot process for the core of the application.\\nUse a single boot process for the common JS libraries.\\nUse a single boot process for the common JS templating libraries.\\nUse a single boot process for the common JS static libraries.\\nUse a single boot process for the common JS dynamic libraries.\\nUse a single boot process for the common JS static libraries.\\nUse a single boot process for the common JS dynamic libraries.\\nUse a single boot process for the common JS common libraries.\\nUse a single boot process for the common JS common libraries.\\nUse a single boot process for common JS common libraries.\\nUse a single boot process for common JS common libraries.\\nUse a single boot process for common JS common libraries.\\nUse a single boot process for common JS common libraries.\\nUse a single boot process for common JS common libraries.\\nUse a single boot process for common JS common libraries.\\nUse a single boot process for common JS common libraries.\\nUse a single boot process for common JS common libraries.\\nUse a single boot process for common JS common libraries.\\nUse a single boot process for common JS common libraries.\\nUse a single boot process"}
{"Context":"## Context:\\nWe have been using `Theme.Design.*` theme from the start, while that worked perfectly it did not allow us to use new material components because those\\nare dependent on `Theme.MaterialComponents.*` app theme. Using new material components allows us to have much more control over component theming like\\ntext styles, icon & icon gravity, strokes, corners, etc.\\n- Right now we are using 3 different kind of buttons to satisfy various design requirements `Button`, `OmegaCenterIconButton` & `PrimarySolidButton`.\\n- Since all our Figma designs are made using material specs\/material components, the design specs for certain components didn\u2019t translate properly to\\nappcompat variants or there are no components available in appcompat at all. Moving to MDC will make the design to layout\/style much easier.\\nYou can find more information on setting up MDC Android [here](https:\/\/material.io\/develop\/android\/docs\/getting-started\/).\\n","Decision":"## Decision\nMoving to MDC will allow us to use `MaterialButton` & other material components if needed. You can find more material components supported by Android\\nand how to theme them [here](https:\/\/material.io\/components).\\nWith `MaterialButton` we can finally use icon, icon gravity, icon tint. This migration will also open better theming support for material components\\nsuch as corner radius, strokes, shapes etc., Since we no longer need 3 types of buttons we will remove\\n`PrimarySolidButton` & `OmegaCenterIconButton`, we will be using `MaterialButton` in place of `Button`.\\nThese are the changes that occur due to this migration.\\n### Button tag change:\\n- We will start using `com.google.android.material.button.MaterialButton` in views instead of `Button`.\\n- In order to use icon in a `MaterialButton` we will use the `icon` attribute instead of `drawableStart` or `drawableEnd` attr.\\n- We will use `iconGravity` (`textStart`, `viewStart`, `textEnd`, `viewEnd`) for setting the icon position, `iconTint` for setting the icon color.\\n### Style change:\\n- `theme_material.xml` will house the main app theme which now extends `Theme.MaterialComponents.Light.NoActionBar`, some of the new attrs we set in\\napp theme.\\n```\\n<style name=\"Clinic.V2.Theme\" parent=\"Theme.MaterialComponents.Light.NoActionBar\">\\n<item name=\"materialButtonStyle\">@style\/Clinic.V2.MaterialButton<\/item>\\n<item name=\"materialButtonOutlinedStyle\">@style\/Clinic.V2.OutlineButton<\/item>\\n<item name=\"borderlessButtonStyle\">@style\/Clinic.V2.TextButton<\/item>\\n<item name=\"toolbarStyle\">@style\/Clinic.V2.ToolbarStyle<\/item>\\n<\/style>\\n```\\nThese attrs can be directly referenced in the view styles instead of using the entire style tag in the view.\\n- We have 4 primary button styles that we use in app\\n- `Clinic.V2.MaterialButton`\\n- `Clinic.V2.TextButton`\\n- `Clinic.V2.OutlineButton`\\n- `Clinic.Button.Flat`\\n- You can then style `MaterialButton` using any of these styles or attr.\\n- Filled Button : `?attr\/materialButtonStyle` or `@style\/Clinic.V2.MaterialButton`\\n- Text Button: `attr\/borderlessButtonStyle` or `@style\/Clinic.V2.TextButton`\\n- Outline Button: `?attr\/materialButtonOutlinedStyle` or `Clinic.V2.OutlineButton`\\n- Un-Elevated\/Flat Button: `@style\/Clinic.Button.Flat`\\n- You can extend any of those primary button styles to override certain attrs like color, for example:\\n```\\n<style name=\"Clinic.V2.MaterialButton.Green3\">\\n<item name=\"backgroundTint\">@color\/green3<\/item>\\n<item name=\"android:textColor\">@color\/green1<\/item>\\n<item name=\"iconTint\">@color\/green1<\/item>\\n<\/style>\\n```\\n","Prediction":"## Decision\nWe will use MDC for all our buttons and components.\\nWe will use MaterialComponents for all our icons.\\nWe will use the `Theme.MaterialComponents.*` for our main app theme.\\nWe will use `Theme.Design.*` for custom design components like buttons.\\nWe will use `Theme.Design.*` for our app icons.\\nWe will use `Theme.Design.*` for our app navigation bars.\\nWe will use `Theme.Design.*` for our app header.\\nWe will use `Theme.Design.*` for our app footer.\\nWe will use `Theme.Design.*` for our app tabs.\\nWe will use `Theme.MaterialComponents.*` for our toolbar.\\nWe will use `Theme.Design.*` for our toolbar icons.\\nWe will use `Theme.MaterialComponents.*` for our custom toolbar buttons.\\nWe will use `Theme.Design.*` for our app tabs.\\nWe will use `Theme.Design.*` for our app tabs buttons.\\nWe will use `Theme.MaterialComponents.*` for our app tabs buttons.\\nWe will use `Theme.Design.*` for our app tabs icons.\\nWe will use `Theme.MaterialComponents.*` for our app tabs icons.\\nWe will use `Theme"}
{"Context":"## Context\\nDuring the course of release strategy hike a brief [investigation](https:\/\/phabricator.wikimedia.org\/T272418) into signing of release packages was done.\\nSigning the artifacts our release pipeline produces is a step that can be taken to ensure their source and integrity. Our pipeline produces two different kinds of artifacts that both have the possibility to have a signature.\\n- Tarball packages\\n- Docker images\\n### Signing tarball packages with GPG\\nTarball files can be signed with [GPG](https:\/\/gnupg.org\/gph\/en\/manual\/x135.html) in a similar way to which MediaWiki are signing theirs. In the case of MediaWiki a private and public key-pair is generated for release engineering members and published on https:\/\/www.mediawiki.org\/keys\/keys.html.\\nBy importing the list of public keys the end-user can then verify the release tarball and it's signature by issuing the following commands.\\nImport keys:\\n```sh\\n$ gpg --fetch-keys \"https:\/\/www.mediawiki.org\/keys\/keys.txt\"\\n```\\nVerify release package:\\n```sh\\n$ gpg --verify mediawiki-core-1.35.2.zip.sig mediawiki-core-1.35.2.zip\\ngpg: Signature made tor  8 apr 2021 20:40:08 CEST\\ngpg:                using DSA key 1D98867E82982C8FE0ABC25F9B69B3109D3BB7B0\\ngpg: Good signature from \"Sam Reed <reedy@wikimedia.org>\" [unknown]\\ngpg: WARNING: This key is not certified with a trusted signature!\\ngpg:          There is no indication that the signature belongs to the owner.\\nPrimary key fingerprint: 1D98 867E 8298 2C8F E0AB  C25F 9B69 B310 9D3B B7B0\\n```\\n### Signing docker images with Docker Content Trust\\nDocker images can be signed with [DCT](https:\/\/docs.docker.com\/engine\/security\/trust\/) which allows you to sign tags associated with images.\\nDCT works by using a set of keys to sign a specific tag in a docker repository. Using signatures is optional and only apply to a specific tag, there can be signed and unsigned tags within the same repository.\\n#### Root key\\nThe root-key or \"offline\" key as named by the docker documentation is what is used to add new signers and repositories and is generally consider that it should be kept safe on offline hardware.\\n#### Repository key\\nFor each image repository a specific key is used to sign the tags.\\n#### Signer key\\nThe signer key is what is added to the docker registry and can be delegated in such a way that more than one signer can sign images for a single repository.\\n### Managing DCT and GPG keys\\nFor DCT the signer and the publisher does not necessarily have to be the same person. A image can be published and then the signer can add a signature later by pulling and signing. (This way managing keys does not have to be a burden on everyone).\\nRoot keys and GPG keys that represent WMDE as an organization should be generated and kept in a secure location. As there has been no previous signing of release packages the methods and processes required for maintaining, using and backing up these keys would have to be defined and formalized before they are applied.\\nTherefore the decision for the release pipeline will be not to sign any of the release packages in the first version (wmde.0 and wmde.1).\\nHowever the benefits of signing are obvious and therefore it's probably a good idea to take the time and answer the following questions to have a solid foundation to stand on when doing release signing.\\n- Which engineers are to be considered release engineers?\\n- Needs access to organization in docker registry\\n- Needs access to organization in github\\n- Need access to `releasers-wikibase` group\\n- Would need to publish and maintain a GPG key used for signing Docker and release tarballs\\n- How do we securely store\/generate offline \"organization\" private keys?\\n- Where does WMDE publish public keys?\\n","Decision":"## Decision\n- Do not sign docker images now\\n- Do not sign tarball artifacts now\\n","Prediction":"## Decision\n- Signing of release tarball packages with GPG\\n- Signing of release tarball packages with Docker Content Trust\\n- Signing of release tarball packages with Docker Images\\n- Signing of release tarball packages with DCT\\n- Signing of release tarball packages with GPG\\n- Signing of release tarball packages with Docker Content Trust\\n- Signing of release tarball packages with DCT\\n- Signing of release tarball packages with DCT\\n- Signing of release tarball packages with DCT\\n- Signing of release tarball packages with DCT\\n- Signing of release tarball packages with DCT\\n- Signing of release tarball packages with DCT\\n- Signing of release tarball packages with DCT\\n- Signing of release tarball packages with DCT\\n- Signing of release tarball packages with DCT\\n- Signing of release tarball packages with DCT\\n- Signing of release tarball packages with DCT\\n- Signing of release tarball packages with DCT\\n- Signing of release tarball packages with DCT\\n- Signing of release tarball packages with DCT\\n- Signing of release tarball packages with DCT\\n- Signing of release tarball packages with DCT\\n- Signing of release tarball packages with D"}
{"Context":"## Context and Problem Statement\\n<!-- [Describe the context and problem statement, e.g., in free form using two to three sentences. You may want to articulate the problem in form of a question.] -->\\nTo build a timed text editor, for editing audio or video transcriptions it can be useful to use an open source text editor and add functionalities to it.\\n## Decision Drivers <!-- optional -->\\n<!-- * [driver 1, e.g., a force, facing concern, \u2026] -->\\n* A simple and straight forward way to keep words and time in sync, during playback and text editing.\\n","Decision":"## Decision\n<!-- * [driver 1, e.g., a force, facing concern, \u2026] -->\\n* A simple and straight forward way to keep words and time in sync, during playback and text editing.\\nChosen option: **Draft.js**, because there is previous work in this space that has explored and stressed tests this editor.\\nAltho Quilljs has a straight forward API, despite [a quircky data model internal  representation](https:\/\/quilljs.com\/docs\/delta). And has been used to quickly make [text based prototypes](https:\/\/github.com\/pietrop\/annotated_article_generator) (see [demo](http:\/\/pietropassarelli.com\/annotated_article_generator\/)) even with multi user collaboration support [through togetherjs](https:\/\/togetherjs.com\/).\\nIt's hard  to tell whether Quilljs is going to be around long term, while draft.js being core part of facebook is starting to see more of a community around it,  and a growing number of plugins.\\nThere also other more advanced features, like adding speakers label, timed text, and other non timed text in the transcription that has workarounds in Draftjs but is not immediatly obvious how it would be implemented in Quilljs.\\n<!-- because [justification. e.g., only option, which meets k.o. criterion decision driver | which resolves force force | \u2026 | comes out best (see below)]. -->\\n<!--\\n### Positive Consequences\\n* [e.g., improvement of quality attribute satisfaction, follow-up decisions required, \u2026]\\n* \u2026\\n### Negative consequences\\n* [e.g., compromising quality attribute, follow-up decisions required, \u2026]\\n* \u2026\\n","Prediction":"<!-- * [driver 2, e.g., a force, facing concern, \u2026] -->\\n* To be able to add a new language, that has to be translated to English, it needs to be possible to generate the text in the language.\\n<!-- * [e.g., the only answer is option 2, i.e., the only option that meets the decision driver, \u2026] -->\\n<!-- * [e.g., only option 2, \u2026] -->\\n## Decision\n<!-- * [e.g., only option 2, \u2026] -->\\n<!-- * [e.g., the only answer option, \u2026] -->\\n<!-- * [e.g., only option 1, \u2026] -->\\n<!-- * [e.g., the only option 3, \u2026] -->\\n<!-- * [e.g., the only option 4, \u2026] -->\\n<!-- * [e.g., the only option 5, \u2026] -->\\n<!-- * [e.g., the only option 6, \u2026] -->\\n<!-- * [e.g., the only option 7, \u2026] -->\\n<!-- * [e.g., the only option 8, \u2026] -->\\n<!-- * ["}
{"Context":"## Context\\nSome mailbox implementations of James store already parsed attachments for faster retrieval.\\nThis attachment storage capabilities are required for two features:\\n- JMAP attachment download\\n- JMAP message search \"attachment content\" criteria\\nOnly Memory and Cassandra backends can be relied upon as a JMAP backend.\\nOther protocols relies on dynamic EML parsing to expose message subparts (IMAP)\\nHere are the POJOs related to these attachments:\\n- **Attachment** : holds an attachmentId, the attachment content, as well as the content type\\n- **MessageAttachment** : composes an attachment with its disposition within a message (cid, inline and name)\\n- **Message** exposes its list of MessageAttachment when it is read with FetchType Full.\\n- **Blob** represents some downloadable content, and can be either an attachment or a message. Blob has a byte array\\npayload too.\\nThe following classes work with the aforementioned POJOs:\\n- **AttachmentMapper** and **AttachmentManager** are responsible of storing and retrieving an attachment content.\\n- **BlobManager** is used by JMAP to allow blob downloads.\\n- Mailbox search exposes attachment content related criteria. These criteria are used by the JMAP protocol.\\nThis organisation causes attachment content to be loaded every time a message is fully read (which happens for instance\\nwhen you open a message using JMAP) despite the fact that it is not needed, as attachments are downloadable through a\\nseparate JMAP endpoint, their content is not attached to the JMAP message JSON.\\nAlso, the content being loaded \"at once\", we allocate memory space to store the whole attachment, which is sub-optimal. We\\nwant to keep the consumed memory low per-message because a given server should be able to handle a high number of messages\\nat a given time.\\nTo be noted that JPA and maildir mailbox implementations do not support attachment storage. To retrieve attachments of a\\nmessage, these implementations parse the messages to extract their attachments.\\nCassandra mailbox prior schema version 4 stored attachment and its metadata in the same table, but from version 5 relies\\non the blobStore to store the attachment content.\\n","Decision":"## Decision\nEnforce cassandra schema version to be 5 from James release 3.5.0. This allows to drop attachment management prior version\\n5.\\nWe will re-organize the attachment POJOs:\\n- **Attachment** should hold an attachmentId, a content type, and a size. It will no longer hold the content. The\\ncontent can be loaded from its **AttachmentId** via the **AttachmentLoader** API that the **AttachmentManager**\\nimplements.\\n- **MessageAttachment** : composes an attachment with its disposition within a message (cid, inline and name)\\n- **Blob** would no longer hold the content as a byte array but rather a content retriever (`Supplier<InputStream>`)\\n- **ParsedAttachment** is the direct result of attachment parsing, and composes a **MessageAttachment** and the\\ncorresponding content as byte array. This class is only relied upon when saving a message in mailbox. This is used as\\nan output of `MessageParser`.\\nSome adjustments are needed on class working with attachment:\\n- **AttachmentMapper** and **AttachmentManager** need to allow from an attachmentId to retrieve the attachment content\\nas an `InputStream`. This is done through a separate `AttachmentLoader` interface.\\n- **AttachmentMapper** and **AttachmentManager** need the Attachment and its content to persist an attachment\\n- **MessageManager** then needs to return attachment metadata as a result of Append operation.\\n- **InMemoryAttachmentMapper** needs to store attachment content separately.\\n- **MessageStorer** will take care of storing a message on the behalf of `MessageManager`. This enables to determine if\\nattachment should be parsed or not on an implementation aware fashion, saving attachment parsing upon writes for JPA\\nand Maildir.\\nMaildir and JPA no longer support attachment content loading. Only the JMAP protocol requires attachment content loading,\\nwhich is not supported on top of these technologies.\\nMailbox search attachment content criteria will be supported only on implementation supporting attachment storage.\\n","Prediction":"## Decision\nWe will use the blobStore to store the attachment content, and parse the attachment content during mailbox retrieval.\\nWe will use the mailbox schema to define the attachment content type and the attachment content's disposition.\\nThe content type will be defined as a string, and the disposition will be defined as a boolean.\\nThe mailbox will be partitioned into multiple queues, each queue will be partitioned into multiple replicas.\\nWhen a mailbox is retrieved, the mailbox will be read from the first replica, and then the mailbox will be split into\\nmultiple replicas. Each replica will have its own copy of the mailbox, and will be responsible for\\nretrieving the attachment content from its replicas.\\nFor each mailbox, we will store the attachment content in a different table.\\nFor example, the mailbox `com.example.mailbox` will store the attachment content in the table `com.example.mailbox:\\nattachments`.\\nFor each mailbox, we will load the attachment content from the mailbox's `attachments` table.\\nFor each mailbox, we will load the attachment content from the mailbox's `attachments` table\\nand dump it into the mailbox's `messages` table.\\nFor each mailbox, we will load the attachment content from the mailbox's `"}
{"Context":"## Context\\nFollowing the release of v3.5.0 of Horace,\\nit was found that breaking changes were not effectively communicated to users.\\nA discussion on the best way to communicate these changes,\\nas well as the best way for developers to track such changes, was had.\\nIt was decided release notes provided a solution to this problem.\\nThe workflow for producing release notes needs to be decided.\\n","Decision":"## Decision\nA file is to be kept in the repository to track changes between releases.\\nThis file is to be updated on every pull request in which:\\n- an API change is made.\\n- a feature is added.\\n- a user-facing bug is fixed.\\nAdditions to the file will be written as if addressing users.\\nThese additions will document behaviour changes,\\nas well as changes users will be required to make to their workflows\/scripts.\\nBefore every release, this file will be used to compile release notes.\\nThese release notes will be published alongside releases on GitHub\\nand with user documentation.\\n","Prediction":"## Decision\nRelease notes will be produced in a similar fashion to the current version of\\nHorace.\\nA template will be provided to the community outlining the scope of release\\nnotes and what needs to be in them.\\nThis template will be made available via the Horace GitHub repository.\\nThis will allow for the release notes to be versioned and tracked, and\\nreproduced by other developers.\\nThese release notes will be versioned using the \\`horace version\\`\\ncommand.\\nRelease notes will be produced on the 1st of each month.\\nThis will allow for the release notes to be versioned and tracked, and\\nreproduced by other developers.\\nThese release notes will be versioned using the \\`horace version\\`\\ncommand.\\nRelease notes will be produced on the 1st of each month.\\nThis will allow for the release notes to be versioned and tracked, and\\nreproduced by other developers.\\nThese release notes will be versioned using the \\`horace version\\`\\ncommand.\\nRelease notes will be produced on the 1st of each month.\\nThis will allow for the release notes to be versioned and tracked, and\\nreproduced by other developers"}
{"Context":"## Context and Problem Statement\\nWhat to use as basis for the generator?\\n","Decision":"## Decision\nChosen option: \"vojtechhabarta\/typescript-generator\", because\\n* actively maintained\\n* typescript\\n","Prediction":"## Decision\nChosen option: `ng2-generator` because it has a lot of features and it's easy to use. It's also an open source project, which means that it will be maintained. It also has a lot of examples.\\nIt's also the easiest to use, because it's more similar to Angular 1 than Angular 2, which is the latest version. It also supports Angular 4.\\nIt also has a lot of plugins, which means that you can easily extend the generator with your own plugins.\\nIt's also the only generator that can be used with ESLint and Babel. ESLint is a code quality tool that helps you write better code and it's free, which means that it's easy to use. Babel is a transpiler that helps you write more readable code, which is important for a generator.\\nIt's also the only generator that has a lot of examples to follow, which is important for beginners.\\nIt's also the only generator that's an Angular CLI project, which means that it's easy to use.\\nIt's also the only generator that has a lot of plugins, which means that you can easily extend the generator with your own plugins.\\nIt's also the only generator that"}
{"Context":"## Context\\nThere are many IAM resources of which most, if not all, are relatively small in size & complexity. In order to minimise repo churn, the module will contain a collection of modules which can be invoked as required.\\n","Decision":"## Decision\nThe module will be a mono repo of specific IAM resources.\\n","Prediction":"## Decision\nThe module will contain a collection of modules which can be invoked as required.\\nThis will allow the module to be used as a library of resources which can be invoked as required.\\nThis will also allow the module to be used as a library of resource versions which can be used as a backup for the module.\\nIn addition, it will allow the module to be used as a library of the resource being invoked.\\nThis means that the module can be used as a library of resources which can be used as a backup for the module.\\nThis also means that the module can be used as a library of the resource being invoked.\\nThis means that the module can be used as a library of the resource being invoked.\\nThis means that the module can be used as a library of the resource being invoked.\\nThis means that the module can be used as a library of the resource being invoked.\\nThis means that the module can be used as a library of the resource being invoked.\\nThis means that the module can be used as a library of the resource being invoked.\\nThis means that the module can be used as a library of the resource being invoked.\\nThis means that the module can be used as a library of the resource being invoked"}
{"Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","Prediction":"## Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\nWe will use [Confluence](https:\/\/confluence.atlassian.com\/display\/DOC\/Document+Architecture+Decision+Record) to keep a record of our ADRs.\\nWe will use a tool like [adrr](https:\/\/github.com\/peck\/adrr) to generate ADRs from the Architecture Decision List.\\nWe will use the [architecture decision tool](https:\/\/github.com\/peck\/architecture_decision_tool) to generate ADRs from the ADRs of other teams.\\nWe will use the [decision tree](https:\/\/github.com\/peck\/architecture_decision_tree) to generate ADRs from the Architecture Decision List.\\nDecision trees are a great way to capture complex decisions and are easier to read than ADRs.\\nWe will use the [decision matrix](https:\/\/github.com\/peck\/architecture_decision_matrix) to visualize the ADRs.\\nWe will use the [decision tool](https:\/\/github.com\/peck\/architecture_decision_tool) to generate ADRs from the A"}
{"Context":"## Context\\nIn HTML5, `link` and `script` nodes have an attribute named `integrity`, which lets the browser check if the remote file is valid, mostly for the purpose of enhancing page security.\\n","Decision":"## Decision\nIn order to replicate the browser's behavior, the program should perform integrity check the same way it does, excluding the linked asset from the final result if such check fails.\\nThe `integrity` attribute should be removed from nodes, as it bears no benefit for resources embedded as data URLs.\\n","Prediction":"## Decision\nWe will use the `integrity` attribute to check if a file is valid.\\n### Use `href` instead of `rel`\\nIn HTML5, you can use `rel` to indicate a link to another document. This is not possible in JavaScript.\\nHowever, `href` can be used to check if a file is valid.\\nThe `href` attribute is more specific than `rel`, so it can be used to check if a file is valid.\\nThis is not the case with `rel`.\\nWe will use `href` to check if a file is valid.\\n### Use `class` instead of `id`\\nIn HTML5, you can use `id` to indicate a link to another document. This is possible in JavaScript.\\nHowever, `id` can be used to check if a file is valid.\\nThe `id` attribute is more specific than `class`, so it can be used to check if a file is valid.\\nThis is not the case with `class`.\\nWe will use `class` to check if a file is valid.\\n### Use `data-*` instead of `class-*`\\nIn HTML5, you can use `class-*` to"}
{"Context":"## Context\\nBoxwise is in the middle of a planned migration from the old PHP-based Dropapp, to a new app based on a new stack (Python\/React). In the old app, the backend and the data model were closely intertwined, with SQL strings written directly within PHP. This made it challenging to evolve the data model for the app, which in turn imposed many product functionality constraints and slowed development time - especially since the Dropapp data model was a prototype rather than designed for scalability and product-market fit. As the team migrates to the new app and explores the possibility of entering new markets, it is time for us to reexamine if and how the team might benefit from a separation of concerns on the data layer.\\n","Decision":"## Decision\n1. **Scalability:** how well will it support expected future changes such as DB restructuring, database migrations, etc.? What timescale is the technology expected to be defunct on?\\n2. **Developer experience:** given the rotating environment of loosely affiliated developers of different backgrounds, what will support the rapid onboarding of developers with our data structure? Once onboarded, is the chosen technology pleasant to use? Is it useful from a career progression standpoint?\\n3. **Maintainability:** we expect to have rapid changes to the DB structure as we expand functionality. How easy is our solution to maintain and evolve? What about documentation?\\n4. **Support and production-readiness:** Is the library mature enough to use in a production environment? Is there an active community or support channel should we run into problems?\\nGraphQL with single endpoint for everything was selected, paired with Ariadne server-side and Apollo client side.\\nReasoning: while GraphQL may have a steeper learning curve for professional developers who are not familiar with the standard, in the long run this should be more scalable for iterations and easier to maintain than  multiple REST endpoints. In the future, should we end up ingesting external data APIs such as the UNHCR data, it will be easier to pull that all from the GraphQL endpoint as well. This should also be more favorable from a developer experience standpoint for both onboarding and maintaining the codebase, due to GraphQL's introspective capabilities, human-readable JSON query structure, and degree of client-side specificity in requesting field-level data.\\nApollo was selected on the client-side due to its maturity as a product, robust features including sophisticated caching, excellent documentation, and huge community. Ariadne was selected over Graphene on the server side due to it being designed to deliberately intended to mimic Apollo Server. With Ariadne being under active development by Mirumee software, and its excellent documentation that developers can cross-reference with Apollo server documentation, I believe this outweighs any cons that come from it being a less mature library than Graphene.\\nFinally, I believe any performance concerns that could result from queries being abstracted from SQL into resolvers will be compensated for by less load on the network due to overfetching, as long as no N+1 queries are created.\\n","Prediction":"## Decision\nTo separate the data model from the application logic, we will adopt the Separation of Concerns principle and split the model into two independent components:\\n* The \"data\" component - which will be the core of the application, and thus the only component that will be exposed to the user\\n* The \"user\" component - which will be the only component that will be exposed to the application\\nIt is important to note that the separation of concerns is not a hard and fast rule. In some cases, the application logic might be more closely aligned to the data model, e.g. the user interface, and thus the separation of concerns may not be necessary. In other cases, the separation of concerns may not be possible, e.g. the database model. In this case, the data model will be exposed to the user, and the application logic will be abstracted away and the data model will be used to define the application logic.\\n* The \"application\" component - which will be the only component that will be exposed to the user\\n* The \"application logic\" component - which will be the only component that will be exposed to the application\\nIn order to isolate the data model and the application logic, the \"application\" component will be written"}
{"Context":"## Context\\nEach terraform `aws_security_group_rule` requires a name. We need these to be both unique and useful so we\\nneed to define a basic standard for them. We have a few basic patterns shown below:\\n```\\nRule 1:\\n`allow_${source-name}_from_${dest-name}_$service_(ingress|egress)`\\nExample:\\nallow_bastion-elb_from_bastion-asg_ssh_ingress\\nRule 2:\\nallow_${source-name}_(ingress|egress)_${dest-name}_$service\\nExample:\\nallow_bastion-elb_ingress_bastion-asg_ssh\\nRule 3:\\n${source-name}_(ingress|egress)_${dest-name}_$service\\nExample:\\nmonitoring2_ingress_webapps-elb_nrpe\\nRule 4:\\n${source-name}_$service_(ingress|egress)_${dest-name}\\nExample:\\nmonitoring2_nrpe_ingress_webapps-elb\\n```\\nThere are a few other guidelines that will be the same between rules. `$service` will be a named service,\\nnot just a port number. If the rule uses a protocol other than TCP we will add an additional `_$proto` to the end.\\n","Decision":"## Decision\nWe will use \"rule 3\" and change all the existing rule names to comply with it.\\n","Prediction":"## Decision\nThe basic naming convention is:\\n```\\naws_security_group_rule_name = {\\nallow_from_address = {\\nsource_address = {\\nsource_port = {\\nprotocol = \"TCP\"}\\n},\\nservice = {\\nservice = {\\nprotocol = \"HTTP\"}\\n}\\n},\\nallow_to_address = {\\nsource_address = {\\nsource_port = {\\nprotocol = \"TCP\"}\\n},\\nservice = {\\nservice = {\\nprotocol = \"HTTP\"}\\n}\\n}\\n}\\n}\\n```\\nIn this case we have a simple `allow_from_address` rule that will allow any IP address to access the\\n`aws_security_group` without specifying a port number. If the service is not HTTP we will include the\\n`protocol` in the `allow_from_address`.\\n```\\naws_security_group_rule_name = {\\nallow_from_address = {\\nsource_address = {\\nsource_port = {\\nprotocol = \"TCP\"}\\n},\\nservice = {\\nservice = {\\nprotocol = \"HTTP\"}\\n}\\n},\\nallow_to_address = {\\nsource_address = {\\nsource_port = {\\nprotocol = \"TCP\"}\\n"}
{"Context":"## Context\\nWe sought to determine whether to deliver our document management capabilities using the content management platform natively or through the integration of an external document management platform.\\nWe sought to determine whether Bloomreach's 'Channel' concept would be suitable for managing the various sites required to be brought onto the platform both at MVP and in the future, such as Deenary and Speciality sights.\\nAs part of this, considerations were made around:\\n* Ease of use for creating new sites\\n* Ability to share components\\n* Ability to segregate content for specific channels (sites)\\n* Ability to share content up and down the stack where needed and appropriate\\n* Permissions model required to support this model\\n","Decision":"## Decision\nBloomreach's concept of channels is well suited to meet the needs of running the sites required under the NWP platform umbrella. Channels offer the ability to build new sites that share components and modules, which enables for greater consistency. By utilising roles and permissions from within BR, content can be segregated to be available only where it is most relevant, whilst allowing for content to be made available up or down the organisational stack (e.g. national content being aggregated at a regional level).\\nBR's 'blueprinting' functionality allows for sites to be created using a series of parameters, further standardising the creation of sites where needed in an easy fashion.\\n","Prediction":"## Decision\nWe will deliver our document management capabilities as a component of the natively built site. This will allow us to support the various sites required to be brought onto the platform at MVP and in the future.\\nWe will require the following permissions for this:\\n* Create new sites\\n* Create new site assets\\n* Create a new document\\n* Create a new document asset\\n* Create a new document version\\n* Create a new document version asset\\n* Create a new document version history\\n* Create a new document version history asset\\n* Create a new document version history history\\n* Create a new document version history history asset\\n* Create a new document version history history history\\n* Create a new document version history history asset\\n* Create a new document version history history history\\n* Create a new document version history history asset\\n* Create a new document version history history history\\n* Create a new document version history history asset\\n* Create a new document version history history history\\n* Create a new document version history history asset\\n* Create a new document version history history history\\n* Create a new document version history history asset\\n* Create a new document version history history history\\n* Create a new document version history history asset\\n* Create a new document version history"}
{"Context":"## Context or problem to solve\\n* How should git histories look like?\\n* What format should we use to write commit messages?\\n","Decision":"## Decision\nWe agreed on normalizing commit messages to master branches to avoid things like `Merge pull request #15 from client\/menduz-patch-10` in persuit of more semantic messages like `fix: commit style guide, closes #15`. That is particularly helpful in repositories with several contributors and fosters professionalism in open source repositories.\\n### Branches\\nWhen you work on a branch on a specific issue, we keep the spirit of [semantic branch naming](https:\/\/medium.com\/@hanuman_95739\/how-to-integrate-branch-naming-commit-message-and-push-rules-in-gitlab-fe9cd642cc1a). Think of this as writing what is and what you are doing in a three word sentence The first one must be oune of the list. For instance:\\n```\\nfix\/wrong_host\\n^  ^^------------^\\n|  ||\\n|  |+----> Summary in present tense.\\n|  +-----> Slash\\n+--------> Type: chore, docs, feat, fix, refactor, style, or test.\\n```\\nOther examples are:\\n```\\ndocs\/update_readme\\nrefactor\/new_welcome_message\\n```\\nLook for the *Examples* in section *Commit messages* for a description of the allowed branch types.\\nIt's OK to use hyphens (`-`) or underscores (`_`) to replace spaces. Avoid any other special characters, like `#` or `$`, as they might lead to problems, for example, when deploying the content using the branch name as part of the URL. The branch name should match this regexp: `(chore|docs|feat|fix|refactor|style|test)\/[0-9a-zA-Z_-]+`\\n#### Exceptions:\\nThis rules are ignored when you work on *environment* branches like `master`, `development`, `staging` and so on.\\n","Prediction":"## Decision\n* Use the [meta-repo](https:\/\/github.com\/rgreene\/meta-repo) format for versioning and commit messages.\\n* Write the commit messages in Markdown.\\n* Use a `git log` command to view the history of a repository.\\n* Use `git reflog` to view the history of a branch.\\n* Use `git log --oneline` to view the history of a file.\\n* Use `git log --format=%s [file]` to view the history of a file.\\n* Use `git log --format=%s [file] [commits]` to display a commit message in a human-readable format.\\n* Use `git log --format=%s [file] [commits] --decorate` to show a human-readable format of a commit message.\\n* Use `git log --format=%s [file] [commits] --decorate --author`, `git log --format=%s [file] [commits] --author`, or `git log --format=%s [file] [commits] --author,author,author`\\n* Use `git log --format=%s [file] [commits] --author,author,"}
{"Context":"## Context\\nCurrently, a Cosmos SDK application's CLI directory stores key material and metadata in a plain text database in the user\u2019s home directory.  Key material is encrypted by a passphrase, protected by bcrypt hashing algorithm. Metadata (e.g. addresses, public keys, key storage details) is available in plain text.\\nThis is not desirable for a number of reasons. Perhaps the biggest reason is insufficient security protection of key material and metadata. Leaking the plain text allows an attacker to surveil what keys a given computer controls via a number of techniques, like compromised dependencies without any privilege execution. This could be followed by a more targeted attack on a particular user\/computer.\\nAll modern desktop computers OS (Ubuntu, Debian, MacOS, Windows) provide a built-in secret store that is designed to allow applications to store information that is isolated from all other applications and requires passphrase entry to access the data.\\nWe are seeking solution that provides a common abstraction layer to the many different backends and reasonable fallback for minimal platforms that don\u2019t provide a native secret store.\\n","Decision":"## Decision\nWe recommend replacing the current Keybase backend based on LevelDB with [Keyring](https:\/\/github.com\/99designs\/keyring) by 99 designs. This application is designed to provide a common abstraction and uniform interface between many secret stores and is used by AWS Vault application by 99-designs application.\\nThis appears to fulfill the requirement of protecting both key material and metadata from rogue software on a user\u2019s machine.\\n","Prediction":"## Decision\nWe will use the [Golang](https:\/\/golang.org\/) secret store.  The secret store will be provided by [Golang\u2019s official secret store](https:\/\/github.com\/golang\/golang-secrets) and will be included in the `go1.13` distribution.\\nWe will use the `os` package to provide a way to store the secret store.  We will also use the `os` package to provide a way to access the secret store.  The `os` package is not a secret store, but rather provides a way to store and access the secret store.  The `os` package is also included in the `go1.13` distribution.\\nWe will use the `golang` package to provide a way to encrypt the secret store.  We will use the `golang` package as the secret store's default encryption algorithm.\\nWe will use the `oauth2` package to provide a way to access the secret store.  We will use the `oauth2` package as the secret store's default access token provider.  We will use the `oauth2` package as the secret store's default token expiration mechanism.  We will follow [the OAuth 2.0 standard]("}
{"Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","Prediction":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\nWe will use [Decision Records by Mike Cohn](https:\/\/github.com\/mccohndecisionrecords\/decision-records-architectural-decisions).\\nWe will use [Decision Support Systems](http:\/\/decisiontools.org\/decision-support-systems.php).\\nWe will use [Atomic Design](https:\/\/github.com\/atomicdesign\/atomic-design) to make architecture decisions.\\nWe will use [Design Decisions](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions) to make architecture decisions.\\nWe will use [Architecture Style Guide](http:\/\/www.sei.cmu.edu\/copyright\/standards\/architecture_style_guide.html) to make architecture decisions.\\nWe will use [Architecture Charter](http:\/\/www.sei.cmu.edu\/copyright\/standards\/architecture_charter.html) to make architecture decisions.\\nWe will use [Architecture Decision Process](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions) to make architecture"}
{"Context":"## Context\\nThe raw HES data is large, complex, unweildly and potentially dirty. A way to\\nclean, store and later easily access the data for analysis required, which means\\nthat repeated cleaning and prep of the raw data is avoided.\\nThere are multiple datasets (with different variables), which once cleaned, will\\nbe accompanied by additional derived variables, and additional derived datasets.\\n","Decision":"## Decision\nThe raw data will be read into a SQLite database through R. R is The Health\\nFoundation's open-source language of choice, and is well equiped to\\nhandle later analysis of said data. An SQLite database is not stored in memory,\\navoiding holding very large datasets an R session. It is a standard database\\nformat which can be easily queried with SQL which can in turn be written within\\nmost other common languagses, including R.\\nIn an SQLite database the database consistents of all the data, and within that\\nthere are tables; here each table corresponds to a HES dataset or a derived\\ndataset.\\n","Prediction":"## Decision\nUse a Hadoop-based system to handle the raw data.\\nUse a Hadoop-based system to handle the raw data. When data is no longer required, it can be moved to a separate HDFS cluster, which will be deleted.\\nUse a Hadoop-based system to handle the raw data. When data is no longer required, it can be moved to a separate HDFS cluster, which will be deleted.\\nUse a Hadoop-based system to handle the raw data. When data is no longer required, it can be moved to a separate HDFS cluster, which will be deleted.\\nUse a Hadoop-based system to handle the raw data. When data is no longer required, it can be moved to a separate HDFS cluster, which will be deleted.\\nUse a Hadoop-based system to handle the raw data. When data is no longer required, it can be moved to a separate HDFS cluster, which will be deleted.\\nUse a Hadoop-based system to handle the raw data. When data is no longer required, it can be moved to a separate HDFS cluster, which will be deleted.\\nUse a Hadoop-based system to handle the raw data. When data is no longer required, it can be moved to"}
{"Context":"## Context\\nThe application needs a way to store objects in preservation buckets that facilitate upload and retrieval and allows for duplicate file names among FileSets associated with a Work object.\\n","Decision":"## Decision\nUse a combination of the pairtree of a FileSet ULID plus its sha256 checksum as the key for S3 objects in the preservation bucket. So an FileSet with a ULID `01dpxt2xajvkdsbckqrs8ry677`\\nand sha256 checksum `87d2c7faf7774a8b07c43be06055c67c4bd602b8ec0e9d6b15241967d500d356`will be stored in s3 as\\n`s3:\/\/[preservation-bucket]\/01\/dp\/xt\/2x\/01dpxt2xajvkdsbckqrs8ry677\/87d2c7faf7774a8b07c43be06055c67c4bd602b8ec0e9d6b15241967d500d356`\\n","Prediction":"## Decision\nThe application uses the `persistence` gem to store the objects in the file system. The `persistence` gem is responsible for the following:\\n* It provides a way to persist objects in a file system\\n* It provides a way to persist the storage of multiple files and directories\\n* It provides a way to persist the storage of multiple files and directories\\n* It provides a way to persist the storage of multiple files and directories\\n* It provides a way to persist the storage of multiple files and directories\\n* It provides a way to persist the storage of multiple files and directories\\n* It provides a way to persist the storage of multiple files and directories\\n* It provides a way to persist the storage of multiple files and directories\\n* It provides a way to persist the storage of multiple files and directories\\n* It provides a way to persist the storage of multiple files and directories\\n* It provides a way to persist the storage of multiple files and directories\\n* It provides a way to persist the storage of multiple files and directories\\n* It provides a way to persist the storage of multiple files and directories\\n* It provides a way to persist the storage of multiple files and directories\\n* It provides a way to persist the storage of multiple files and directories"}
{"Context":"## Context and Problem Statement\\nA Simple Composition Component is a [Generic Component](https:\/\/github.com\/UST-MICO\/mico\/issues\/724), that follows the concept of a certain [Enterprise Integration Pattern](https:\/\/www.enterpriseintegrationpatterns.com) by invoking a user-defined (or predefined) openFaaS function and acting upon its results. In contrast to the [Complex Integration Components](https:\/\/github.com\/UST-MICO\/docs\/blob\/master\/adr\/0025-implementation-of-complex-eai-patterns-with-faas.md) there are no further efforts (such as maintaining a state) needed for implementing the corresponding Simple Composition Component.\\n## Decision Drivers\\nWe want to have a list of all Enterprise Integration Patterns, that can be implemented as Simple Composition Components with a reasonable amount of effort.\\n","Decision":"## Decision\nWe want to have a list of all Enterprise Integration Patterns, that can be implemented as Simple Composition Components with a reasonable amount of effort.\\nWhich of those do we want to implement??\\n### Routing Patterns\\nThe following tables describes for all Routing, Transformation and Management patterns, if they can be implemented as Simple Composition Components (As defined above).\\nThe following assumptions are made:\\n- We have two Generic Components:\\n- A Generic Transformation Component: Provide each message to a user-defined function and forward the result to a predefined target topic\\n- A Generic Routing Component: Provides each message to a user-defined function and forward the result to the topic, that is returned by the user-defined function.\\n- Both components read from a single topic only\\n- There are no external resources available (e.g. for storing a state)\\n| Name                       | Implementation Strategy                                                                                                              | Return Value                                                                |         Possible as <br\/>Simple Composition Component         |\\n|----------------------------|--------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------|:--------------------------------------------------------:|\\n| Content-Based router       | All rules are implemented in the user-defined function                                                                               | The topic, to which it is routed                                            | Yes                                                      |\\n| Message Filter             | All rules are implemented in the user-defined function                                                                               | Either nothing, or the topic to which it is routed                          | Yes                                                      |\\n| Dynamic Router             | All rules are implemented in the user-defined function                                                                               | The topic, to which it is routed                                            | No                                                       |\\n| Recipient List             | The recipients are hard coded in the user-defined function | All topics, to which the message shall be forwarded                         | Yes                                                       |\\n| Splitter                   | The splitting rules are implemented in the user-defined function                                                                     | The sub messages, that contain the splitted <br\/>content of the original message | Yes                                                      |\\n| Aggregator                 | Would require that the user provides a message storage, <br\/>in which the messages can be stored until they are aggregated                                   | The aggregated message                                                      | No                                                       |\\n| Resequencer                | Would require that the user provides a message storage, <br\/>in which the messages can be stored until they are forwarded in the right order                 | The (correct) sequence of messages                                          | No                                                       |\\n| Composed Message <br\/>Processor | Could be implemented by combining a Splitter, Router and Aggregator                                                                    | The composed message                                                        | No (since the aggregator <br\/>is not considered to be simple) |\\n| Scatter-Gather             | Could be implemented by combining an aggregator, multiple <br\/>transformers and a topic, to which all transformers subscribe                | the transformed and aggregated message                                      | No (since the aggregator <br\/>is not considered to be simple) |\\n| Routing Slip               | Probably too complex <br\/>for Simple Composition Component                                                                                      | -                                                                           | No                                                       |\\n| Process Manager            | Probably too complex <br\/>for Simple Composition Component                                                                                      | -                                                                           | No                                                       |\\n| Message Broker             | Probably too complex <br\/>for Simple Composition Component                                                                                      | -                                                                           | No                                                       |\\n### Transformation Patterns\\n| Name                 | Implementation Strategy                                                                                                                                                                        | Return Value                     | Possible as Simple <br\/>Composition Component |\\n|----------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------|------------------------------------------|\\n| Content Enricher     | The user-defined function contains all the information, that may eventually be added to a message  | The transformed message          | Yes                                       |\\n| Content Filter       | The user-defined function implements the whole process of filtering <br\/>out content of a message                                                                                                   | The filtered message             | Yes                                      |\\n| Envelope Wrapper     | A Content Enricher for wrapping a message in an envelope. A <br\/>Content Filter for unwrapping                                                                                                      | -                                | Yes                                      |\\n| Claim Check          | Combination of a special Content Filter for replacing message data <br\/>with a 'claim', and a Content Enricher <br\/>for recovering the data. Also a data storage for storing the data until it is recovered  | -                                | No                                       |\\n| Normalizer           | A combination of a Router, and a set of Translators                                                                                                                                            | The transformed message          | Yes                                      |\\n| Canonical Data Model | The user-defined function is implemented in such a way, that <br\/>receives\/returns messages with a canoncial data format                                                                            | Message in canonical data format | Yes                                      |\\n### System Management\\n| Name            | Implementation Strategy                                                                      | Return Value                                | Possible as Simple Composition Component |\\n|-----------------|----------------------------------------------------------------------------------------------|---------------------------------------------|:----------------------------------------:|\\n| Control Bus     | It would be necessary to receive from more than one topic                                    | -                                           | No                                       |\\n| Detour          | Would require a control bus                                                                  | -                                           | No                                       |\\n| Wire Tap        | Special case of Recipient List                                                               | The topic to which the message is forwarded | Yes                                      |\\n| Message History | Every user-defined function adds a reference of itself to the message history of a function  | The modified message                        | Yes                                      |\\n| Message Store   | Would require, that a data storage is available                                              | -                                           | NO                                      |\\n| Smart Proxy     | Would require a data storage for storing the return address                                  | -                                           | No                                       |\\n| Test Message    | Would require, that the Test Data Verifier receives from two topics                          | -                                           | No                                       |\\n| Channel Purger  | This would be a feature, that is implemented in the Generic Component. It is not planned yet | -                                           | No                                       |\\n","Prediction":"## Decision\nWe want to have a list of all Enterprise Integration Patterns, that can be implemented as Simple Composition Components with a reasonable amount of effort.\\nChosen option: \"Simple composition components with a [Common Implementation](https:\/\/github.com\/UST-MICO\/docs\/blob\/master\/adr\/0025-implementation-of-complex-eai-patterns-with-faas.md#common-component-implementations)\", because it satisfies the following criteria:\\n* It is easy to understand and implement for new users, because the concepts are well-known and well-documented. The component's implementation can be found in the [Mico Components repository](https:\/\/github.com\/UST-MICO\/docs\/tree\/master\/adr\/0025-implementation-of-complex-eai-patterns-with-faas.md#mico-components).\\n* It is a [Simple Composition Component](https:\/\/github.com\/UST-MICO\/docs\/blob\/master\/adr\/0025-implementation-of-complex-eai-patterns-with-faas.md#simple-composition-component).\\n* The component's public API is well-documented, it is well-structured and well-organized.\\n* It is easy to extend the component's functionality, because the component's public API is well-documented and the implementation"}
{"Context":"## Context\\nGovUK Notify is used as the email provider for Keycloak: [0012 Keycloak Email Provider](0012-keycloak-email-provider.md)\\n","Decision":"## Decision\nIt was decided to pass the entire Keycloak message into the GovUK Notify template via the `keycloakMessage` personalisation parameter, rather than having a mix of text and parameters in the Keycloak template.\\nThis is because the Keycloak message is passed in as a complete constructed string. It would be very difficult to extract particular portions of this string, for example a link for use within the GovUK Notify template.\\nThis approach does mean it is harder for non-developers to make changes to the email messages. The decision may be revisited in the future if this proves to be causing issues.\\n","Prediction":"## Decision\nWe are going to use the [Keycloak Email Provider](https:\/\/keycloak.org\/)\\nextension to enable the use of GovUK Notify for email communication with the [Keycloak Users](https:\/\/keycloak.org\/extension\/users).\\nThis extension will provide a way to configure email notifications, for the following use cases:\\n* Notifications for new keycloak users\\n* Notifications for existing users when a new keycloak user is created\\n* Notifications for existing keycloak users that have had an email sent to them\\n* Notifications for existing keycloak users that have had an email sent to them that have expired\\n* Notifications for existing keycloak users that have had their password reset by email\\n* Notifications for existing keycloak users that have had their password reset by email that have expired\\n* Notifications for existing keycloak users that have had their password reset by email that have expired\\n* Notifications for existing keycloak users that have had their password reset by email that have expired\\n* Notifications for existing keycloak users that have had their password reset by email that have expired\\n* Notifications for existing keycloak users that have had their password reset by email that have expired\\n* Notifications for existing keycloak users that have had their password reset by email that have expired"}
{"Context":"## Context\\nIn order to send email notifications about user actions, such as publishing a document, we need to:\\n- Work out who should receive the email\\n- Generate the content for the email (subject and body)\\n- Send the email to the recipient(s)\\nWe don't anticipate sending email to be a critical step of any user action, but it is still a step that is prone to error and delay, due to interaction with external systems. We should ensure sending emails is robust against transient failure and that emails are only sent when we are confident they are accurate i.e. after DB updates and critical API calls.\\nWe should have a means of sending notifications in non-production environments, which allows us to see what notifications would be sent in production, but prevents the emails from being sent to the intended recipient(s). It should also be possible to see the notifications we have sent in production in order to debug issues if users report they are not receiving notifications, or that the content of the notifications is incorrect.\\n","Decision":"## Decision\nWe will use [GOV.UK Notify][notify] in combination with [Action Mailer][action-mailer], [Active Job][active-job] and [Sidekiq][sidekiq].\\n### GOV.UK Notify\\nWe will use [GOV.UK Notify][notify] to handle the low-level concerns of sending emails:\\n- This avoids having to setup our own mechanism for sending email, including logic to handle low-level errors and retries, which are [handled by Notify by default][notify-features]\\n- Notify provide a [Ruby gem][notify-gem] that makes it easy to integrate the service into our app, without specific knowledge of the underlying APIs\\n- We can use the [Notify dashboard][notify-features] to see emails we have sent, logs for individual emails, as well as aggregate stats over all emails\\n- Notify provides the facility to restrict the recipients of emails to a [specific list][notify-allowed-list], which we can use to restrict notifications outside of production\\nIn order to integrate Notify with our app, we will setup 3 'GOV.UK Publishing' accounts as follows:\\n- One account will be for production use by any GOV.UK publishing app\\n- The other two accounts will be for testing notifications on integration and staging\\nWe agreed to use the generic 'GOV.UK Publishing' name as part of a wider GOV.UK strategy to have a single Notify account for all GOV.UK publishing apps, which helps to limit future infrastructure growth.\\n### Action Mailer\\nWe will use [Action Mailer][action-mailer] to generate the individual emails to send to notify:\\n- Using Action Mailer to process email is the recommended approach for Rails apps\\n- Action Mailer comes with in-built support for testing and sending email asynchronously\\nAlthough Notify provides a [templating feature][notify-features], we will use a single, generic template for all 'GOV.UK Publishing' notifications, with recipient, subject and body parameters; the body supports a limited form of Markdown. Using Action Mailer to handle the generation of emails means we limit our reliance on Notify templates; we can make most of our changes in code, with the usual benefits of testing, faster debugging and version control.\\nNotify only supports a [single recipient for each email][notify-gem-send-email], so we will need to generate and send separate emails for each recipient. Although this requires more API calls, it's easier to reason about failure for individual emails.\\nNotify will reject an email when a recipient is not in a specific list, if one has been setup. In order to retain end-to-end email functionality but avoid emailing users outside of production, we will use an [Action Mailer Interceptor][action-mailer-interceptor] to redirect emails to a Google Group in integration and staging environments. The Google Group(s) will act as a dumping ground for viewing the emails that have been generated, which enables us to check they appear correctly in text and HTML forms.\\n### Active Job\\nWe will use [Active Job][active-job] in combination with [Action Mailer][action-mailer] to send emails asynchronously.\\n- This means any delay in calling the Notify APIs does not impact on the user request\\n- We can use the [retry feature][active-job-retry] of Active Job to cope with transient failures from Notify\\nAction Mailer integrates with Active Job instead of specific queueing systems, and recent [improvements to the Active Job framework][active-job-improvements] also make it a viable for other background jobs, so we can avoid having to use a mixture of frameworks.\\nActive Job does not handle asynchronous processing directly and instead delegates to a queueing provider. Initially, we will use [Sidekiq][sidekiq], which is already common on GOV.UK and one of the [in-built queueing adapters][active-job-sidekiq] provided with Active Job.\\n### Sidekiq\\nWe will use [Sidekiq][sidekiq] in order to handle the actual processing of background jobs. Sidekiq is already used by other apps on GOV.UK for background job processing, to the extent that including it in apps is managed by a [wrapper gem][sidekiq-govuk].\\nIn order to avoid [duplicate retry behaviour][active-job-sidekiq-retry], we will disable Sidekiq retries and rely solely on the retry behaviour of Active Job. When a failure occurs, the job is marked as 'processed' by Sidekiq and any retry manifests as a newly enqueued job; when the retries are exhausted, the exception will propagate to Sidekiq and the job will be marked as 'dead'. Normally, the job would be marked as 'failed' by Sidekiq and requeued; this means we lose visibility on the number of retries.\\nWe already use [Raven] to report errors to Sentry, which automatically [integrates with Sidekiq][sidekiq-sentry]. Note that Raven also [integrates with Active Job][raven-active-job], but this is disabled when Sidekiq is present. Active Job catches exceptions as part of its retry behaviour, so an error will only get reported to Sentry when the exception is not handled by Active Job, or the retries for an exception we do handle are exhausted. We already [export Sidekiq metrics to Graphite][sidekiq-grafana].\\n","Prediction":"## Decision\nWe will use [Postman](https:\/\/www.postman.com) to send email notifications in production.\\nWe will enable Postman's [send email](https:\/\/www.postman.com\/docs\/api\/using\/send-email) functionality in a non-production environment.\\nWe will use the Postman API to send notifications in a non-production environment. We will not use the [send email](https:\/\/www.postman.com\/docs\/api\/using\/send-email) API to send notifications in a production environment.\\nWe will only send emails in a non-production environment where we are confident that the email content is correct. We will not send emails in a non-production environment where we are not confident that the email content is correct.\\nWe will use the [send email](https:\/\/www.postman.com\/docs\/api\/using\/send-email) API to send emails in a non-production environment. We will only use the [send email](https:\/\/www.postman.com\/docs\/api\/using\/send-email) API to send emails in a production environment.\\nWe will run the email testing suite in the [email testing](https:\/\/github.com\/abacus\/abacus-email-standalone\/tree\/master\/email-testing) branch of the [abacus-email-standalone](https:\/\/"}
{"Context":"## Context\\n1. Team `devops` set up recently, we need some change to improve the performance;\\n2. No communication in the team, always person to person;\\n3. Tasks not organized well which spread among different projects;\\n4. Some teams require daily reports for their management, but lose the communication between team members.\\n","Decision":"## Decision\nWe're going to have 10 minutes daily standup meetings with each team so that We're on the same page about what everyone in the team is working on and we can strategize on how to approach the work items on the board for the day to come.\\nEffective standups are all about communication and commitment, not about status updates. So before every scrum take 5 minutes to actually think and note down what you were planning to accomplish yesterday, did you actually accomplish what you've committed to, if not why?\\nTake some time to plan your day and decide what you're going to accomplish today, commit to it and state it to the team (not the management), so the team members will keep each other accountable for the commitments they make to make the whole team improve together and make sure to remove impediments that prevent team members to accomplish what they plan.\\nScrum master should pay more attention to the status of each card, encourage team members to deploy in time.\\nWe change scrum master every two week to make sure all team members understand the details as an owner.\\n### Keynotes\\n* Safe time - make sure everyone free that time;\\n* Everyone should standup;\\n* Focus - Every should watching the board;\\n* Less than 10 minutes for the whole meeting;\\n* One minute per person, no more than two minutes;\\n* Remove impediments;\\n* What done yesterday;\\n* What will do today.\\n","Prediction":"## Decision\nWe will use [Azure Boards](https:\/\/docs.microsoft.com\/en-us\/azure\/boards) to manage the change.\\nWe will use [Azure DevOps](https:\/\/docs.microsoft.com\/en-us\/azure\/devops) for the daily communication between team members.\\nWe will use [Azure Pipelines](https:\/\/docs.microsoft.com\/en-us\/azure\/azure-pipelines\/ci) for the continuous integration between Azure Boards and Azure DevOps.\\nWe will use [Azure Repos](https:\/\/docs.microsoft.com\/en-us\/azure\/azure-virtual-machines\/troubleshoot\/azure-versions\/azure-ssh-keypair) for the private version control of source code.\\nWe will use [Azure Boards](https:\/\/docs.microsoft.com\/en-us\/azure\/boards) for the daily communication between team members.\\nWe will use [Azure DevOps](https:\/\/docs.microsoft.com\/en-us\/azure\/devops) for the continuous integration between Azure Boards and Azure DevOps.\\nWe will use [Azure Pipelines](https:\/\/docs.microsoft.com\/en-us\/azure\/azure-pipelines\/ci) for the continuous integration between Azure Boards and Azure DevOps.\\nWe will use [Azure Boards](https:\/\/docs.microsoft.com\/en-us\/azure\/boards"}
{"Context":"## Context\\nCurrently, when user requests data point sync from either DataPointsMapFragment or SurveyedLocaleListFragment and a service,\\nSurveyedDataPointSyncService is started and for each batch of datapoints both fragments get notified of new data available via local Broadcast which both subscribe too. While data is being loaded a non-blocking \"loading\" ui is shown. Once the loading is finished\\nthat loading needs to be dismissed. We also display a snackBar with the number of updated datapoints or some eventual errors.\\n","Decision":"## Decision\nThe current implementation is not ideal since if user leaves the fragments view, the service will still send the broadcast even\\nif there is no ui waiting for it. Both fragments have to subscribe to the broadcast even if only one of them that will display\\nthe error\/success snackBars. It would be much better to have the ui subscribe to changes in the database data instead of broarcasts.\\nThat way the service does not need to notify ui of new data being available instead when the service inserts data into the database\\nthe ui will automatically be notified. RX Java can be used for this, an example is this library https:\/\/github.com\/square\/sqlbrite.\\nThis task also requires the refactor of this feature (independently from the rest of the application). We will separate the ui and\\nview (fragment and presenter) logic from domain logic (business logic, use cases) and the data logic (data sources sych as database\\nand network). This separation of concerns will allow us to add unit tests to separate components which will have a Single\\nResponsability.\\n","Prediction":"## Decision\nIn order to reduce the amount of messages sent to the server and to make the app more responsive we decided to use\\n[Channel Messaging](https:\/\/developer.android.com\/reference\/android\/os\/Channel.html) library. Channel Messaging\\nis a lightweight, cross-platform messaging library for Android, which provides a simple way to send notifications\\nin the background. Channel Messaging is a great way to send notifications to the user without blocking the app.\\nWe decided to use Channel Messaging because it supports offline messaging (see here for more details) and it is\\ntypically used in the background. In addition, we can use Channel Messaging to send notifications with a\\nbackground service. In order to send notifications with a background service we need to use a service. We will use\\n`com.google.android.gms.tasks` library to create a background service. We can create a new service with the following\\nstructure:\\n```\\ncom.google.android.gms.tasks.Task<com.google.android.gms.tasks.datapoint.SurveyedDataPointSyncService> createSurveyedDataPointSyncService()\\n```\\n```\\nWe can use the following code to send notifications with a background service:\\n```\\n\/\/ Create a background service\\nnew BackgroundService().executeTaskOnUIT"}
{"Context":"## Context\\nWe needed to test parallel login of users to ensure that\\nsession data isn't accidentally shared between users\\n(see [decision 0002](.\/0002-custom-save-handler-in-service-front.md)).\\nAlso, Make an LPA currently has no load tests. These are useful\\nfor capacity planning and finding bottlenecks which may be\\ncausing errors for users on the site. While we are auto-scaling,\\nwe don't have a way to verify that it is sufficient to manage\\nexpected load, which load testing can provide.\\nLoad testing can also provide a benchmark we can use to ensure\\nthat any new work we do does not degrade performance of the stack\\nas a whole.\\n","Decision":"## Decision\nUse [locust.io](https:\/\/locust.io\/) to test parallel logins.\\nWhile there are alternatives to locust, it is implemented in Python\\n(our chosen language going forward), has an intuitive API, and some\\nmembers of the team have experience with it already (and can reuse\\npreviously-written code).\\nLoad tests will be added to the tests\/load directory in the project,\\nas they are not component-specific and apply to the whole stack.\\nDoing the above has the happy side effect of opening up the possibility of\\nload testing the whole application stack in future.\\nInitially, this will only run locally and is not integrated into\\nthe CI pipeline. We will consider extending this testing into CI\\nin future.\\n","Prediction":"## Decision\nWe will run load tests to ensure that the site is able to handle\\nexpected load. We will use [GoTest](https:\/\/github.com\/go-test\/go-test) to test the app.\\nWe will run these tests every 30 minutes.\\nWe will run the tests in parallel for each load group.\\nWe will run the load tests in a separate queue for the front-end.\\nWe will run the load tests on a separate server to the app server.\\nWe will only run load tests for the front-end, to limit the impact on the app.\\nWe will run the load tests for a 5 minute window, to ensure that users\\nare not experiencing any significant load.\\nWe will run load tests for a 1 minute window, to ensure the load is\\nnot too high.\\nWe will run load tests for a 5 minute window, to ensure that the load is\\nnot too low.\\nWe will run load tests for a 1 minute window, to ensure that the load is\\nnot too high.\\nWe will run load tests for a 5 minute window, to ensure that the load is\\nnot too low.\\nWe will run load tests for a 1 minute window, to ensure that the"}
{"Context":"## Context\\nHaving a \"preservation first\" mindset has been decided upoan as a stated goal for Meadow. Generally, this means that digital preservation\\npolicies, processes and deliverables should be planned and implemented in tandem with development of the applications features, processeses and infrastructure.\\n","Decision":"## Decision\nThe digital preservation lifecycle for a Work and its FileSets begin as soon as an Ingest Sheet is \"approved\". Actions in the ingest pipeline are used to\\nadd digital preservation \"artifacts\" to Work and FileSet metadata (such as checksum and timestamps) and move objects to preservation storage buckets in S3. Additionally, the\\nsuccess and failure outcomes of these actions are added as AuditEntries that can be used verification, future audits and problem resolution.\\n","Prediction":"## Decision\n**Preservation first**. This means that\\nthe preservation of digital assets and content will be a first priority for all development efforts, including the development of the Meadow\\napplication. This is to ensure that the current and future preservation needs of the Meadow application can be met, and that any future\\nimprovements to the application can be planned and implemented in a timely manner.\\nThis also includes the development of the Meadow service: the Meadow REST API and the Meadow Web Services.\\n**Preservation first**. This means that\\nthe preservation of digital assets and content will be a first priority for all development efforts, including the development of the Meadow\\napplication. This is to ensure that the current and future preservation needs of the Meadow application can be met, and that any future\\nimprovements to the application can be planned and implemented in a timely manner.\\nThis also includes the development of the Meadow service: the Meadow REST API and the Meadow Web Services.\\nThis includes the use of the following tools and practices:\\n* **Metadata Management**. As part of the Meadow application's development process, the metadata of digital assets and content will be\\nmanaged using a metadata management tool. This tool will be used to manage metadata for all digital assets"}
{"Context":"## Context\\nCurrently, the `RepoHooks` class remains largely untested due to a combination of two factors:\\n1. The methods in this class are static, and we do not own the contract under which they should be called, as they are\\ndefined as hooks in `extension.json` or as global variables in the entrypoints e.g. [extensions\/Wikibase\/repo\/Wikibase.php:1020](https:\/\/github.com\/wikimedia\/mediawiki-extensions-Wikibase\/blob\/7b20d22b3c0bbc37ad23f63e38fadc9b1f2ca057\/repo\/Wikibase.php#L1020), which means we cannot easily refactor the methods to increase testability\\n2. Methods rely heavily on the `WikibaseRepo` singleton and it's store, which make it harder to test, as there is no\\nway to to inject a mock of `WikibaseRepo` without dependency injection.\\nA [RFC for enabling dependency injection](https:\/\/phabricator.wikimedia.org\/T240307) in hooks is currently under way.\\nHowever, an interim solution is needed in order to mitigate the amount of untested logic that exists in that file\\nand other places in the codebase.\\nWhile reviewing this issue, two initial solutions were considered:\\n- Refactor `RepoHooks` into a singleton itself, so that when instantiated, we can inject a Mock of `WikibaseRepo`\\ninstead of using the real deal.\\n- Adopt a pattern used in `WikibaseClient` Which enables us to mock several parts of it (namely the store), and replace\\nthe real store by creating an `overrideStore` method. See in following:\\n- [`client\/tests\/phpunit\/includes\/MockClientStore.php`](https:\/\/github.com\/wikimedia\/mediawiki-extensions-Wikibase\/blob\/master\/client\/tests\/phpunit\/includes\/MockClientStore.php)\\n- [`client\/tests\/phpunit\/includes\/DataAccess\/ParserFunctions\/PropertyParserFunctionIntegrationTest.php:42`](https:\/\/github.com\/wikimedia\/mediawiki-extensions-Wikibase\/blob\/master\/client\/tests\/phpunit\/includes\/DataAccess\/ParserFunctions\/PropertyParserFunctionIntegrationTest.php#L42)\\nHowever, after a discussion, it was decided to implement a middle ground, that would enable us to gradually refactor\\nhooks, rather than a one time big change.\\n","Decision":"## Decision\nIt was decided to adopt an existing pattern in Wikibase repo where each hook handler gets its own singleton class and\\nprovides at least four methods:\\n- A constructor to make dependency injection easier.\\n- A public static method to bootstrap an instance from global settings. For consistency, this would typically be named\\n`newFromGlobalSettings`.\\n- (optional) A public static method to get a cached instance of the handler object (rather than instantiate it each time):\\nThis is useful for hooks handlers which are called several times.\\n- A public static method to wire up functionality into the hooks system and should contain little to no logic (as it is\\nhard to test without complete integration or e2e tests).\\n- A public method to perform the actual handler logic in a testable way.\\nThis class should be placed under the `includes\/Hooks` directory. An example of this pattern can be seen in:\\nhttps:\/\/gerrit.wikimedia.org\/r\/c\/mediawiki\/extensions\/Wikibase\/+\/574495\\n","Prediction":"## Decision\n- `RepoHooks` will be split into two classes, `RepoHooks` and `RepoHooksImpl`\\n- `RepoHooksImpl` will be responsible for creating the `WikibaseRepo` and injecting it into the `RepoHooks` instance.\\n- `RepoHooksImpl` will be responsible for creating the `overrideStore` method, and injecting it into the `RepoHooks` instance.\\n- `RepoHooksImpl` will be responsible for injecting the mock of the real `WikibaseRepo` into the `RepoHooks` instance.\\n- `RepoHooksImpl` will be responsible for creating the `WikibaseClient` instance, and injecting it into the `RepoHooks` instance.\\n- `RepoHooksImpl` will be responsible for creating the `WikibaseClient` instance, and injecting it into the `RepoHooks` instance.\\n- `RepoHooksImpl` will be responsible for creating the `WikibaseClient` instance, and injecting it into the `RepoHooks` instance.\\n- `RepoHooksImpl` will be responsible for creating the `WikibaseClient` instance, and injecting it into the `RepoHooks` instance.\\n- `RepoHooksImpl` will be responsible for creating the `WikibaseClient`"}
{"Context":"## Context\\nI see found both `yarn.lock` and `package-lock.json` in the repository.\\n","Decision":"## Decision\nSticking to `yarn` for `web` for now.\\n","Prediction":"## Decision\nWe will keep the `package-lock.json` in the repository.\\n[![Docker Pull Command](https:\/\/img.shields.io\/badge\/docker-Container-Verified-blue.svg)](https:\/\/hub.docker.com\/r\/akosia\/aws-s3\/)\\n[![Docker Compose](https:\/\/img.shields.io\/badge\/docker-Compose-Container-Container-Image-blue.svg)](https:\/\/hub.docker.com\/r\/akosia\/aws-s3\/)\\n[![Docker Pipelines](https:\/\/img.shields.io\/badge\/docker-Pipelines-Container-Container-Image-blue.svg)](https:\/\/hub.docker.com\/r\/akosia\/aws-s3\/)\\n[![Docker Registry](https:\/\/img.shields.io\/badge\/docker-Registry-Container-Container-Image-blue.svg)](https:\/\/hub.docker.com\/r\/akosia\/aws-s3\/)\\n[![Docker Registry Push](https:\/\/img.shields.io\/badge\/docker-Registry-Container-Container-Image-blue.svg)](https:\/\/hub.docker.com\/r\/akosia\/aws-s3\/)\\n[![Docker Registry Pull](https:\/\/img.shields.io\/badge\/docker-Registry-Container-Container-Image"}
{"Context":"## Context\\nIn horizontally scaled processes problems of parallelization can happen like race conditions (a resource is accessed simultaneously by multiple processes and can have a different value for each process as a previous one wrote to the resource before).\\n","Decision":"## Decision\nTo support horizontal scaling there is also the need to allow resource locking.\\nAs the resource accessing is part of transmitting data over time or space, the storages and portals should be able to use this feature.\\nTherefore we will put in the contracts into the portal-base.\\nAs locks need some sort of storage to maintain a lock state an additional repository has to be added to the storages.\\nThe featured methods shall be:\\n* isLocked\\n* lock\\n* release\\nA utility class or methods to easily write spinlocks by time or iteration shall be added.\\nLocks shall rather run out of time or a different measurement instead to unintentionally lock a resource.\\n","Prediction":"## Decision\nThis issue will be closed as won't be solved with this change.\\nWe will add a new field `is_parallel` to the `Process` class that will return true if the process is part of a distributed system like a cluster.\\nWe will then use a new process property `is_parallel` to check if a process is part of a distributed system.\\nFor example, if we have a `Data` class that has a `is_parallel` property, then it would be possible to have multiple threads accessing the same property at the same time.\\nThis property will be used to determine if a process is part of a cluster or not.\\nWe will also need to add a new method to `Process` to check if a process is part of a distributed system, but this is a different task that will be left to a different issue.\\nWe will also need to add a new method to `Process` to check if a process is part of a cluster, but this is a different task that will be left to a different issue.\\nThis issue will be closed as won't be solved with this change.\\nWe will add a new field `is_parallel` to the `Process` class that will return true if the process is part"}
{"Context":"## Context\\n`pyHorace` ([prototype](https:\/\/github.com\/mducle\/hugo)) is the proposed Python module for PACE\\nwhich packages the Matlab Horace\/Herbert codes as a \"compiled\" library.\\nThe Matlab code within this library needs to interact with Python codes in the host.\\nSpecifically for fitting and model evaluation, Horace has to be able to call a user defined model function.\\nAs the compiled Matlab library does not allow users to use their own `m-file`,\\nuser model functions would have to be written in Python (or C\/C++\/Fortran - subject of a separate ADR).\\nHowever, the implementation of the compiled Matlab library is such that compiled Matlab functions cannot call\\nPython functions directly using the standard API.\\nThis is because the standard API for Matlab to call Python assumes that Matlab can spawn its own Python interpreter.\\nIn `pyHorace`, this would clash with the user-launched parent Python intepreter and cause a crash.\\n[This design document](..\/..\/01_pace_python_high_level_discussion.md#-calling-user-defined-functions)\\noutlined two main methods for the compiled Matlab code to call a Python function.\\n1. Using a `mex` file.\\n2. Using a \"reverse communications interface\" (RCI).\\nIn the `mex` method, a reference to the Python function is given to the `mex` file which uses the CPython\\nAPI to call that function (in Python) and then automatically converts (copies) the results to Matlab (`Array`) format.\\nIn the RCI approach, the compiled Matlab program never explicitly calls a Python function.\\nInstead whenever the Matlab code needs to call a Python function, the user instead calls a **Python** wrapper\\nwhich only exchanges data with Matlab. This is explained in detail in the [RCI section](#rci) below.\\nBecause of the need to the wrapper, this is not a general purpose approach but requires code in each specific\\ninstance where we want to employ it (e.g. in `multifit`).\\nThis may prove too restrictive for integrating Brille.\\nDespite this, the RCI approach uses only the documented Matlab-Python API which should make it more stable and robust.\\nOn the other hand, the `mex` option allows the current Matlab code to be used without modifications and is more flexible.\\nBut, as it uses a `mex` file, unhandled exceptions in the `mex` file can lead to hard crashes.\\nIn addition, `mex` functions _cannot_ use non-Matlab (non-`Array` types) data as input or outputs.\\nThus all data must be converted to Matlab format from Python, and vice versa which involves at least one data copy.\\nThis restriction means that not all Python functions can be supported by this method\\n(e.g. those which return non-convertible types such as Python classes)\\nexcept with special work-arounds as in the case of Brille.\\nThis also means that the reference to the Python function cannot be passed directly to the `mex` function.\\nRather, the Python code stores it in a global dictionary and the (string) key is passed to the `mex`.\\n","Decision":"## Decision\nThe decision was taken to use the `mex` method for compiled Matlab code to call Python functions.\\nThis is because using the RCI approach would require extensive changes to the `multifit` and `SpinW` Matlab code to\\nhandle the pyHorace use case.\\nFuthermore these changes would be incompatible with the Matlab interface which would result in two different versions\\nof the multifit `mfclass` and SpinW `spinw` classes.\\nOn the other hand, using the `mex` approach, the current Matlab code can be used \"as-is\" in pyHorace.\\n","Prediction":"## Decision\nWe will use a `mex` approach for interfacing with the compiled Matlab code.\\nThis will be implemented using the `mex` method.\\nThis will be implemented using a reverse communications interface (RCI) approach in the form of a wrapper around the\\n`mex` method which converts the results of the `mex` function to a C-style array.\\nThis wrapper will be used to call the Python functions in the host.\\nThis wrapper will also be used to call the Python functions in the host for the `eval` and `fit` functions.\\nThis wrapper will _not_ be used to call the Python functions in the host for the `fit` function.\\nThis wrapper will _not_ be used to call the Python functions in the host for the `fit` function.\\nThis wrapper will _not_ be used to call the Python functions in the host for the `fit` function.\\nThis wrapper will _not_ be used to call the Python functions in the host for the `fit` function.\\nThis wrapper will _not_ be used to call the Python functions in the host for the `fit` function.\\nThis wrapper will _not_ be used to call the Python functions in"}
{"Context":"## Context\\nThe issue motivating this decision, and any context that influences or constrains the decision.\\n","Decision":"## Decision\nThe change that we're proposing or have agreed to implement.\\n","Prediction":"## Decision\nThe change that we're proposing or have agreed to implement.\\nChosen option: \"Change the default page layout to use a single column\\nwith a fixed width\", because it is expected to perform best in the\\ncontext of the application.\\n### Positive Consequences\\n* The default page layout is fixed-width\\n* The default page layout is responsive\\n* The default page layout is SEO-friendly\\n* The default page layout is easy to maintain\\n* The default page layout is easy to test\\n### Negative Consequences\\n* The default page layout is not responsive\\n* The default page layout is not SEO-friendly\\n* The default page layout is difficult to test\\n* The default page layout is not easy to maintain\\n* The default page layout is difficult to test\\n* The default page layout is not easy to maintain\\n* The default page layout is not easy to maintain\\n* The default page layout is not easy to maintain\\n* The default page layout is not easy to maintain\\n* The default page layout is not easy to maintain\\n* The default page layout is not easy to maintain\\n* The default page layout is not easy to maintain\\n* The default page layout is not easy to maintain\\n* The default page layout is not easy to"}
{"Context":"## Context\\nTo enhance the ability to import data from any source that is compliant with the OpenReferral\\nmodle use strings for fields that are keys.\\n","Decision":"## Decision\nConvert existing models to use strings instead of integers as the id fields and references.\\n","Prediction":"## Decision\nField keys will be strings.\\n### Use of String Keys\\nWhen importing data fields, the values of the fields should be strings.\\n### Use of Numbers for Field Values\\nNumbers will be used when importing data fields, however, if a field has a numeric value, the field name will be replaced with the number.\\n### Use of Numeric Values for Field Names\\nField names will be strings.\\n### Field Names\\nField names will be strings.\\n### Field Type\\nField type will be strings.\\n### Field Names in List Format\\nField names will be strings.\\n### Field Names in Table Format\\nField names will be strings.\\n### Additional Data Types\\nAdditional data types will be strings.\\n### Field Keys in Table Format\\nField keys will be strings.\\n### Field Keys in List Format\\nField keys will be strings.\\n### Field Keys in Table Format\\nField keys will be strings.\\n### Field Names in Table Format\\nField names will be strings.\\n### Field Names in List Format\\nField names will be strings.\\n### Field Names in List Format\\nField names will be strings.\\n### Field Names in List Format\\nField names will be strings.\\n### Field Names in List Format\\nField names will"}
{"Context":"## Context\\nThis ADR defines the motiviation and approach used to secure access\\nto the Consul component in the EdgeX architecture\\nfor *security-enabled configurations only*.\\nNon-secure configuations continue to use Consul in\\nanonymous read-write mode.\\nAs this Consul security feature requires Vault to function,\\nif `EDGEX_SECURITY_SECRET_STORE=false` and Vault is not present,\\nthe legacy behavior (unauthenticated Consul access) will be preserved.\\nConsul provides several services for the EdgeX architecture:\\n- Service registry (see ADR in references below)\\n- Service health monitoring\\n- Mutable configuration data\\nUse of the services provided by Consul is optional on a service-by-service basis.\\nUse of the registry is controlled by the `-r` or `--registry` flag provided to an EdgeX service.\\nUse of mutable configuration data is controlled by the `-cp` or `--configProvider` flag provided to an EdgeX service.\\nWhen Consul is enabled as a configuration provider,\\nthe `configuration.toml` is parsed into individual settings\\nand seeded into the Consul key-value store on the first start of a service.\\nConfiguration reads and writes are then done to Consul if it is specified as the configuration provider,\\notherwise the static `configuration.toml` is used.\\nWrites to the `[Writable]` section in Consul trigger per-service callbacks\\nnotifying the application of the changed data.\\nUpdates to non-`[Writable]` sections are parsed only once at startup\\nand require a service restart to take effect.\\nSince configuration data can affect the runtime behavior of services,\\ncompensating controls must be introduced in order to mitigate the risks introduced\\nby moving configuration from a static file into to an HTTP-accessible service with mutable state.\\nThe current practice is that Consul is exposed via unencrypted HTTP in anonymous read\/write mode\\nto all processes and EdgeX services running on the host machine.\\n","Decision":"## Decision\nConsul will be configured with access control list (ACL) functionality enabled,\\nand each EdgeX service will utilize a Consul access token to authenticate to Consul.\\nConsul access tokens will be requested from the Vault Consul secrets engine\\n(to avoid introducing additional bootstrapping secrets).\\nDNS will be disabled via configuration as it is not used in EdgeX.\\n**Consul Access Via API Gateway**\\nIn security enabled EdgeX, the API gateway will be configured to\\nproxy the Consul service over the `\/consul` path,\\nusing the `request-transformer` plugin\\nto add the global management token to incoming requests\\nvia the `X-Consul-Token` HTTP header.\\nThus, ability to access remote APIs also grants the ability\\nto modify Consul's key-value store.\\nAt this time, service access via API gateway is all-or-nothing,\\nbut this does not preclude future fine-grained authorization\\nat the API gateway layer to specific microservices, including Consul.\\nProxying of the Consul UI is problematic and there is no current solution,\\nwhich would involve proper balacing of the externally-visible URL,\\nthe path-stripping effect (or not) of the proxy,\\nConsul's `ui_content_path`,\\nand UI authentication\\n(the `request-transfomer` does not work on the UI).\\n","Prediction":"## Decision\nConsul will be exposed via unencrypted HTTP in anonymous read\/write mode\\nto all processes and EdgeX services running on the host machine.\\nConsul will be exposed via unencrypted HTTP in anonymous read\/write mode\\nto all processes and EdgeX services running on the host machine.\\nConsul will be exposed via unencrypted HTTP in anonymous read\/write mode\\nto all processes and EdgeX services running on the host machine.\\nConsul will be exposed via unencrypted HTTP in anonymous read\/write mode\\nto all processes and EdgeX services running on the host machine.\\nConsul will be exposed via unencrypted HTTP in anonymous read\/write mode\\nto all processes and EdgeX services running on the host machine.\\nConsul will be exposed via unencrypted HTTP in anonymous read\/write mode\\nto all processes and EdgeX services running on the host machine.\\nConsul will be exposed via unencrypted HTTP in anonymous read\/write mode\\nto all processes and EdgeX services running on the host machine.\\nConsul will be exposed via unencrypted HTTP in anonymous read\/write mode\\nto all processes and EdgeX services running on the host machine.\\nConsul will be exposed via unencrypted HTTP in anonymous read\/write mode\\nto all processes and EdgeX"}
{"Context":"## Context\\nA given mail is often written to the blob store by different components. And mail traffic is heavily duplicated (several recipients receiving similar email, same attachments). This causes a given blob to often be persisted several times.\\nCassandra was the first implementation of the blobStore. Cassandra is a heavily write optimized NoSQL database. One can assume writes to be fast on top of Cassandra. Thus we assumed we could always overwrite blobs.\\nThis usage pattern was also adopted for BlobStore on top of ObjectStorage.\\nHowever writing in Object storage:\\n- Takes time\\n- Is billed by most cloud providers\\nThus choosing a right strategy to avoid writing blob twice is desirable.\\nHowever, ObjectStorage (OpenStack Swift) `exist` method was not efficient enough to be a real cost and performance saver.\\n","Decision":"## Decision\nRely on a StoredBlobIdsList API to know which blob is persisted or not in object storage. Provide a Cassandra implementation of it.\\nLocated in blob-api for convenience, this it not a top level API. It is intended to be used by some blobStore implementations\\n(here only ObjectStorage). We will provide a CassandraStoredBlobIdsList in blob-cassandra project so that guice products combining\\nobject storage and Cassandra can define a binding to it.\\n- When saving a blob with precomputed blobId, we can check the existence of the blob in storage, avoiding possibly the expensive \"save\".\\n- When saving a blob too big to precompute its blobId, once the blob had been streamed using a temporary random blobId, copy operation can be avoided and the temporary blob could be directly removed.\\nCassandra is probably faster doing \"write every time\" rather than \"read before write\" so we should not use the stored blob projection for it\\nSome performance tests will be run in order to evaluate the improvements.\\n","Prediction":"## Decision\nWe will use Cassandra to store our mail blobs. Cassandra `exists` method will be used to find mail blobs that are already present in the blobStore.\\nMail blobs will be written to ObjectStorage, and a corresponding `exists` method will be used to check if a mail blob already exists in the ObjectStore.\\nMail blobs will be deleted if they are no longer needed.\\nThis is a good tradeoff between performance and storage space, since mail blobs are not large.\\nWe will also use a lightweight caching mechanism to avoid unnecessary writes to the ObjectStore.\\nMail blobs will be persisted in the blobStore without an `exists` method, which will be updated by the mail processing code.\\nMail processing code will also check the existence of mail blobs before persisting them, to avoid writing to the ObjectStore twice.\\nMail processing code will not be aware of the mail blobStore or ObjectStore, so it will not be able to intelligently decide if a mail blob already exists in the ObjectStore.\\nMail processing code will also not be able to intelligently decide if a mail blob is a duplicate of another mail blob, so it will not be able to intelligently decide if a mail blob should be deleted.\\nMail processing code"}
{"Context":"# Context\\nScott recently added a `path_url` prompt to get the eventual url a web1 app would end up. The purpose of this property was to update the css and javascript url paths in a web app html file. The problem is that typical useage of the lifecorp-init app is to press enter through each prompt and accept the defaults. For a web1 app, there is no default for the url path because there is no standard for where the app will be placed on the lifecorp network. If this field does NOT get filled in when the app is intiated, then the developer must manually fill in this information. The purpose of lifecorp-init is to get a project skeleton up and running to a \"Hello World\" state with little to no modifications.\\n# Decision\\nScott will add a parameter called `--require` and `-r` that will allow a string to be passed to tell lifecorp-init to require a specific prompt to not be empty for some projects, but remain empty for others.\\n# Consequences\\nThe lifcorp-init will be able to block users from skipping required parameters.\\n# Status\\nAccepted\\n","Decision":"## Decision\nScott will add a parameter called `--require` and `-r` that will allow a string to be passed to tell lifecorp-init to require a specific prompt to not be empty for some projects, but remain empty for others.\\n# Consequences\\nThe lifcorp-init will be able to block users from skipping required parameters.\\n# Status\\nAccepted\\n","Prediction":"## Decision\nScott will add a parameter called `--require` and `-r` that will allow a string to be passed to tell lifecorp-init to require a specific prompt to not be empty for some projects, but remain empty for others.\\n# Consequences\\nThe lifcorp-init will be able to block users from skipping required parameters.\\n# Status\\nAccepted\\n## Decision\nScott will add a parameter called `--require` and `-r` that will allow a string to be passed to tell lifecorp-init to require a specific prompt to not be empty for some projects, but remain empty for others.\\n# Consequences\\nThe lifcorp-init will be able to block users from skipping required parameters.\\n# Status\\nAccepted\\n# Decision\nScott will add a parameter called `--require` and `-r` that will allow a string to be passed to tell lifecorp-init to require a specific prompt to not be empty for some projects, but remain empty for others.\\n# Consequences\\nThe lifcorp-init will be able to block users from skipping required parameters.\\n# Status\\nAccepted\\n# Decision\nScott will add a parameter called `--require` and `-r` that will allow a string to be passed to tell lif"}
{"Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","Prediction":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\nWe will use [Design Decisions](http:\/\/www.designdecisions.de\/2011\/01\/11\/documenting-architecture-decisions\/) as a reference.\\nWe will use [Architectural Styles](http:\/\/www.oreilly.com\/catalog\/9780596804419) as a reference.\\nWe will use [Architecture Styles Explained](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\/) as a reference.\\nWe will use [Architecture Styles in Practice](http:\/\/www.oreilly.com\/catalog\/9780596528414) as a reference.\\nWe will use [Architecture at Work](http:\/\/www.oreilly.com\/catalog\/9780596528421) as a reference.\\nWe will use [The Cathedral and the Bazaar](http:\/\/www.amazon.com\/Cathedral-Bazaar-Adrian-King\/dp\/0672333131) as a reference.\\nWe will use [The Design of Business Processes](http:\/\/www.amazon.com\/Design-B"}
{"Context":"### Context and Problem Statement\\nCurrently Technical Documentation lives in the Handbook, with an [entry point](https:\/\/planet4.greenpeace.org\/create\/tech\/) and everything else organized under the [Development tag](https:\/\/planet4.greenpeace.org\/tag\/development\/). On the other hand Gitbook is a platform specifically for maintaining documentation, based on Markdown. We created a [Planet 4 space](https:\/\/app.gitbook.com\/@greenpeace\/s\/planet4\/) in GP Gitbook account to test how a potential structure of our documentation would look.\\n### Decision Drivers\\n* Community Management Team should be consulted, since this affects the internal development community too.\\n### Considered Options\\n* Move to Gitbook\\n* Stay in the Handbook\\n### Decision Outcome\\nChosen option: **Decided to move to Gitbook all Technical documentation \\(Development & Infrastructure\\)**\\n### Pros and Cons of the Options\\n#### Move to Gitbook\\n* Good, because Gitbook is meant to be used as a documentation platform providing better UI\/UX for that task.\\n* Good, because docs can be grouped in categories, or browsed as a series of a guide, creating a user journey.\\n* Good, because it\u2019s connected and synced with a public Github repo, so it\u2019s easy for an external contributor to update the documentation by opening a Pull Request.\\n* Good, because the documentation on the repo is just Markdown so we are not locked into Gitbook, if we ever choose to leave.\\n* Good, because it\u2019s the choice of [other teams](https:\/\/app.gitbook.com\/@greenpeace\/spaces) in the Org \\(eg. Global Support\\), so people can discover it easier but they are also used to it.\\n#### Stay in the Handbook\\n* Good, because everything regarding P4 is in one place.\\n* Bad, because the perception is that the Handbook is for internal audience. That\u2019s not always the case for the technical documentation.\\n* Bad, because we sometimes need to do custom development just for the Handbook.\\nBad, because it demands manual styling customization for technical documentation \\(e.g scripts\/coding pieces\\) or instructions.\\n### Links\\n* [P4 space on Gitbook](https:\/\/app.gitbook.com\/@greenpeace\/s\/planet4\/)\\n* [Git Repo](https:\/\/github.com\/greenpeace\/planet4-docs\/)\\n","Decision":"## Decision\n* Community Management Team should be consulted, since this affects the internal development community too.\\n### Considered Options\\n* Move to Gitbook\\n* Stay in the Handbook\\n### Decision Outcome\\nChosen option: **Decided to move to Gitbook all Technical documentation \\(Development & Infrastructure\\)**\\n### Pros and Cons of the Options\\n#### Move to Gitbook\\n* Good, because Gitbook is meant to be used as a documentation platform providing better UI\/UX for that task.\\n* Good, because docs can be grouped in categories, or browsed as a series of a guide, creating a user journey.\\n* Good, because it\u2019s connected and synced with a public Github repo, so it\u2019s easy for an external contributor to update the documentation by opening a Pull Request.\\n* Good, because the documentation on the repo is just Markdown so we are not locked into Gitbook, if we ever choose to leave.\\n* Good, because it\u2019s the choice of [other teams](https:\/\/app.gitbook.com\/@greenpeace\/spaces) in the Org \\(eg. Global Support\\), so people can discover it easier but they are also used to it.\\n#### Stay in the Handbook\\n* Good, because everything regarding P4 is in one place.\\n* Bad, because the perception is that the Handbook is for internal audience. That\u2019s not always the case for the technical documentation.\\n* Bad, because we sometimes need to do custom development just for the Handbook.\\nBad, because it demands manual styling customization for technical documentation \\(e.g scripts\/coding pieces\\) or instructions.\\n### Links\\n* [P4 space on Gitbook](https:\/\/app.gitbook.com\/@greenpeace\/s\/planet4\/)\\n* [Git Repo](https:\/\/github.com\/greenpeace\/planet4-docs\/)\\n","Prediction":"## Decision\n* Community Management Team should be consulted, since this affects the internal development community too.\\n### Considered Options\\n* Move to Gitbook\\n* Stay in the Handbook\\n### Decision Outcome\\nChosen option: **Decided to move to Gitbook all Technical documentation \\(Development & Infrastructure\\)**\\n### Pros and Cons of the Options\\n#### Move to Gitbook\\n* Good, because Gitbook is meant to be used as a documentation platform providing better UI\/UX for that task.\\n* Good, because docs can be grouped in categories, or browsed as a series of a guide, creating a user journey.\\n* Good, because it\u2019s connected and synced with a public Github repo, so it\u2019s easy for an external contributor to update the documentation by opening a Pull Request.\\n* Good, because the documentation on the repo is just Markdown so we are not locked into Gitbook, if we ever choose to leave.\\n* Good, because it\u2019s the choice of [other teams](https:\/\/app.gitbook.com\/@greenpeace\/spaces) in the Org \\(eg. Global Support\\), so people can discover it easier but they are also used to it.\\n#### Stay in the Handbook\\n* Good, because everything regarding"}
{"Context":"## Context\\n_The issue motivating this decision, and any context that influences or constrains the decision._\\nWe have two teams developing the CLI.\\nOne (CLI) is building out functionality (such as a better UAA and login experience) for the current, stable v6 releases.\\nOne (VAT) is dropping compatibility with CAPI v2 to more quickly integrate with and build out CAPI v3.\\nVAT is doing so in v7 releases of the CLI.\\nFor the last three weeks,\\nVAT has been developing on a long-running _vat_ branch based on v6.39.\\nPart of the goals for VAT include\\nincorporating all 7-10 new epics that will be built on _main_,\\nalong with sharing any bugfixes back and forth between the two branches.\\nVAT has been tracking which commits should be cherry-picked back and forth each day,\\nresulting in significant overhead and bookkeeping.\\nRather than continuously merging two long-running branches into each other,\\nthe VAT team proposes to develop on _main_ simultaneously with CLI.\\nThis would allow the CAPI v3 acceleration work to proceed faster.\\nThe CLI team is constrained by needing to continue shipping stable versions of v6.\\nThere would be increased risk of shipping unstable versions,\\ngiven that the VAT team's work involves modifying existing code\\nfor which there are no integration tests.\\nThe VAT team would bear little of this risk\\nand would reap most of the benefit.\\n","Decision":"## Decision\n_The change that we're proposing or have agreed to implement._\\nVAT is proposing to have both teams develop in the _main_ branch simultaneously.\\nOne code base would be building two different versions of the binary.\\nThe v6 binary would not include any v7 actors or commands.\\nIntegration tests will need to split to represent v6 behaviour vs v7 behaviour.\\nWe will need two separate command lists.\\nThe main.go file can either be copied as two separate packages\\nor as a single file that relies on go build flags to determine the version.\\nAny v6 commands that are already backed by v3 endpoints will need to be carefully dealt with\\nin order to ensure backwards compatibility when working with the v6 CLI.\\nSeveral packages are differentiated by a v2\/v3 prefix,\\nbut should be v6\/v7 going forward to avoid confusion.\\nRemoval of v3 prefix commands from the v6 CLI to reduce overlapping surface area.\\n","Prediction":"## Decision\n_The change that we're proposing or have agreed to implement._\\n_The change that we're proposing or have agreed to implement._\\n### Decision\n_The change that we're proposing or have agreed to implement._\\n_The change that we're proposing or have agreed to implement._\\n### Decision\n_The change that we're proposing or have agreed to implement._\\n_The change that we're proposing or have agreed to implement._\\n### Decision\n_The change that we're proposing or have agreed to implement._\\n_The change that we're proposing or have agreed to implement._\\n### Decision\n_The change that we're proposing or have agreed to implement._\\n_The change that we're proposing or have agreed to implement._\\n### Decision\n_The change that we're proposing or have agreed to implement._\\n_The change that we're proposing or have agreed to implement._\\n### Decision\n_The change that we're proposing or have agreed to implement._\\n_The change that we're proposing or have agreed to implement._\\n### Decision\n_The change that we're proposing or have agreed to implement._\\n_The change that we're proposing"}
{"Context":"## Context\\nCode must be organized to be compatible with:\\n- Static type-checking via [mypy](https:\/\/github.com\/python\/mypy)\\n- Runtime execution during normal usage and running tests via [pytest](https:\/\/docs.pytest.org\/en\/stable\/)\\n- Static doc generation via [sphinx-autodoc-typehints](https:\/\/github.com\/agronholm\/sphinx-autodoc-typehints)\\nAdditionally:\\n- Functions should be able to refer to any type\\n- Most types depend on other types non-recursively, but some types (e.g. `SubAttribute` and `AttributeType`) do depend on each other recursively \/ cyclically.\\n","Decision":"## Decision\nPut types (`@dataclass(frozen=True)`) into the `_types` module\\nand have all function modules depend on the `_types` module to define their inputs and outputs.\\n","Prediction":"## Decision\nUse `typehints` to provide static type hints for types.\\nUse `types.py` to provide a module that contains the static type hints.\\nUse `pytypes` to provide a library that provides static type hints.\\nUse `pydoc` to provide a library that provides static doc generation.\\nUse `pylint` to provide a library that provides static linting.\\nUse `pydoc` to provide a library that provides static doc generation.\\nUse `pylint` to provide a library that provides static linting.\\nUse `pydoc` to provide a library that provides static doc generation.\\nUse `pydoc` to provide a library that provides static doc generation.\\nUse `pydoc` to provide a library that provides static doc generation.\\nUse `pydoc` to provide a library that provides static doc generation.\\nUse `pydoc` to provide a library that provides static doc generation.\\nUse `pydoc` to provide a library that provides static doc generation.\\nUse `pydoc` to provide a library that provides static doc generation.\\nUse `pydoc` to provide a library that provides static doc generation.\\nUse `pydoc` to provide a library that provides static doc generation"}
{"Context":"## Context\\nThe fundraising department wants to address donors and members in emails\\nand other communication with a personalized greeting, tailored to the\\npreferred way people want to be adressed.\\nOur external data warehousing provider only allows for a limited selection\\nof three German genders: \"Herr\", \"Frau\" and \"Divers\".\\nWe want to keep the business logic (bounded contexts) free from gender\\nnorms and I18N concerns. We also want to keep the UI extensible (for\\nfuture options in the selection field) and translatable.\\n","Decision":"## Decision\nWe put salutation information into the content repository that contains\\nthe following information for each of the three possible salutations:\\n- The \"label\", used in the selection field in forms and for display on\\nconfirmation pages.\\n- The \"value\", used in the domain (will probably the same as label)\\n- Translation keys for addressing in varying degrees of formality\\nOur frontend code will use the label and value to construct selection fields.\\nOur server code will use the value and the translation keys for creating\\nemails and writing to the database. The export script will map the\\ndifferent values from the database to the allowed 3 values for the data\\nwarehousing provider.\\n","Prediction":"## Decision\nWe will support the following three gender options:\\n- \"Herr\" (masculine)\\n- \"Frau\" (feminine)\\n- \"Divers\" (neutral) (no gender specified)\\nWe will display the gender of the user as part of the user name.\\nFor example, \"David\" or \"Diana\" would be displayed as \"David\" in the\\nemail and \"Diana\" in the user name.\\nWe will use the German language as a fallback for the third gender, if\\nthe user name does not contain the required gender.\\nWe will also set the default culture to the German language in the\\nuser profile.\\nFor example, if David is the user name, the default culture will be\\n\"de_DE\" in any emails and user name.\\nWe will use the German locale for the user profile, if it does not\\ncontain the required gender.\\nFor example, if David is the user name, the user profile will be\\ndisplayed as \"de_DE\" in the emails and user name.\\nWe will use the German locale for the email and user profile, if it\\ndoes not contain the required gender.\\nFor example, if David is the user"}
{"Context":"## **Context**\\nIn order to be able to release code safely as part of a CI\/CD pipeline, it is necessary to have a feature management capability to externalise the ability to enable features.\\nThis can be achieved by including feature toggles in code, and by versioning APIs to account for breaking changes.\\nThe options available are:\\n- Environment Variables\\n- AWS AppConfig\\n","Decision":"## Decision\n**AWS AppConfig**\\nWhat is the change that we're proposing and\/or doing?\\n","Prediction":"## Decision\nWe will use AWS AppConfig to enable feature toggles.\\nThis will be provided as an environment variable to the application, and the feature toggles will be enabled\/disabled by running the following command:\\n```\\naws config get-config --environment=AWS AppConfig --type json --quiet --output=json --query 'features[*].enabled' --query 'features[*].disabled'```\\nThis will return:\\n```\\n\"features\": [ \"AWS AppConfig\", \"AWS::IAM::IAMPolicy\", \"AWS::RouteTable::RouteTable\" ]```\\nThis will be used to enable\/disable features in the AWS AppConfig service.\\nThis will also be used to version the feature toggles.\\nThis feature toggles will be stored as a secret in the AWS AppConfig service.\\nAs a secret, it will be encrypted using the AWS Secrets Manager.\\nThis will be used to enable\/disable features in the AWS AppConfig service.\\nThis will also be used to version the feature toggles.\\nThis feature toggles will be stored as a secret in the AWS AppConfig service.\\nAs a secret, it will be encrypted using the AWS Secrets Manager.\\nThis will be used to enable\/disable features in the AWS AppConfig service.\\nThis"}
{"Context":"## Context\\nThis is a response from IITD Architecture to the IITD Infrastructure Team, and others, for an Official Statement on OpenJDK positioning.\\n* Status: proposed\\n* Deciders: Licence Change\\n* Date: 2020-08-27\\nTechnical Story: [description | <https:\/\/apps.nrs.gov.bc.ca\/int\/jira\/browse\/ARCH-62]>\\n## Context and Problem Statement\\nThis is a response from IITD Architecture to the IITD Infrastructure Team, and others, for an Official Statement on OpenJDK positioning.\\n## Decision Drivers\\n* An official Information Innovation & Technology Division (IITD) statement is needed on the direction forward for target versions of Java\/JDKs for all IITD hosted java applications.  Oracle has changed its licensing model, and will now be using a subscription model for charging for the use of its Oracle Java SE product releases.\\n* Java SE 8 is the end of the legacy versioning and release cadence model; Java 9 was the new beginning.\\n* Oracle extended the public updates of its Oracle JDK to January 2019 for commercial production use, and at least the end of 2020 for individual desktop use.\\n* Oracle JDK should be, for the most part, interchangeable with Oracle\u2019s OpenJDK builds for those who want to move up to the next releases, as they did for Java 6->7 and Java 7->8.\\n* Java 10 is the suggested release by Oracle\\n* Oracle has posted more information on the roadmap for the Java Client (including Applets and Web Start).\\n* Oracle plans to discontinue contributions to the JDK 8 Updates project until January 2019.\\n* For security reasons, IITD Architecture encourages upgrade\/migration of all java applications to at least JDK 11.\\n* Scope - IITD all server and client side applications owned by IITD that run Java.\\n","Decision":"## Decision\n* An official Information Innovation & Technology Division (IITD) statement is needed on the direction forward for target versions of Java\/JDKs for all IITD hosted java applications.  Oracle has changed its licensing model, and will now be using a subscription model for charging for the use of its Oracle Java SE product releases.\\n* Java SE 8 is the end of the legacy versioning and release cadence model; Java 9 was the new beginning.\\n* Oracle extended the public updates of its Oracle JDK to January 2019 for commercial production use, and at least the end of 2020 for individual desktop use.\\n* Oracle JDK should be, for the most part, interchangeable with Oracle\u2019s OpenJDK builds for those who want to move up to the next releases, as they did for Java 6->7 and Java 7->8.\\n* Java 10 is the suggested release by Oracle\\n* Oracle has posted more information on the roadmap for the Java Client (including Applets and Web Start).\\n* Oracle plans to discontinue contributions to the JDK 8 Updates project until January 2019.\\n* For security reasons, IITD Architecture encourages upgrade\/migration of all java applications to at least JDK 11.\\n* Scope - IITD all server and client side applications owned by IITD that run Java.\\nJDK 8 & Later\\nOracles OpenJDK JDK binaries for Windows, macOS, and Linux are available on release-specific pages of jdk.java.net as .tar.gz or .zip archives.\\nAs an example, the archives for JDK 13 may be found on jdk.java.net\/13 and may be extracted on the command line using\\n$ tar xvf openjdk-13*_bin.tar.gz\\nor\\n$ unzip openjdk-13*_bin.zip\\ndepending on the archive type.\\n### Positive Consequences\\n* removes the dependencies on Oracle JDK Licensing\\n* reduces security vulnerabilities of older JDK versions\\nJava 7 is still in predominant use. It goes without saying that any version of Java below 7 should be updated immediately even version 7 needs significant remediation for its fleet of vulnerabilities.\\nFurther vulnerabilities -\\n* <https:\/\/www.cvedetails.com\/product\/19117\/Oracle-JRE.html?vendor_id=93>\\n*\t<https:\/\/www.cvedetails.com\/product\/23642\/Oracle-Openjdk.html?vendor_id=93>\\n### Negative Consequences\\n* slow performance may occur\\n* migration issues will need to be addressed\\n* Migrate all Java JDK dependencies from Oracle JDK to OpenJDK.\\n* Upgrade all older versions to at least JDK 8, preference is to encourage teams to target move to JDK 11.\\n","Prediction":"## Decision\n* An official Information Innovation & Technology Division (IITD) statement is needed on the direction forward for target versions of Java\/JDKs for all IITD hosted java applications.  Oracle has changed its licensing model, and will now be using a subscription model for charging for the use of its Oracle Java SE product releases.\\n* Java SE 8 is the end of the legacy versioning and release cadence model; Java 9 was the new beginning.\\n* Oracle extended the public updates of its Oracle JDK to January 2019 for commercial production use, and at least the end of 2020 for individual desktop use.\\n* Java 10 is the suggested release by Oracle\\n* Oracle has posted more information on the roadmap for the Java Client (including Applets and Web Start).\\n* Oracle plans to discontinue contributions to the JDK 8 Updates project until January 2019.\\n* For security reasons, IITD Architecture encourages upgrade\/migration of all java applications to at least JDK 11.\\n* Scope - IITD all server and client side applications owned by IITD that run Java.\\nChosen option: \"Java 10\", because it is the latest supported version and there is currently no known"}
{"Context":"## Context\\nThere are a number of different ways we could approach indexing in Elasticsearch. We would like to choose a path that allows us some flexibility to adjust as future needs arise. We also need to think about how to maintain index uptime while modifying the contents of the index.\\n","Decision":"## Decision\nUse an index alias for searching that points to a separate index for each source.\\n","Prediction":"## Decision\nWe will use the InfluxDB database to store the index.\\nWe will use the [InfluxDB Query Language](https:\/\/www.influxdata.com\/blog\/2016\/11\/15\/influxql-query-language) to query the database.\\nWe will use the [InfluxDB Export](https:\/\/www.influxdata.com\/blog\/2016\/11\/15\/influxdb-export) to export the data from the database to a CSV file.\\nWe will use a [Docker Compose](https:\/\/docs.docker.com\/compose\/) stack to provision the required services and run the export and import scripts.\\nWe will use a [Kibana](https:\/\/www.elastic.co\/products\/kibana) dashboard to monitor the Elasticsearch instance.\\nWe will use a [Kibana Dashboard](https:\/\/www.elastic.co\/products\/kibana#dashboard) plugin to allow us to easily create dashboards for querying the data stored in the InfluxDB database.\\nWe will use a [Kibana Dashboards](https:\/\/www.elastic.co\/products\/kibana#dashboard) plugin to allow us to easily create dashboards for querying the data stored in the InfluxDB database.\\nWe will use a [D"}
{"Context":"## Context\\nNode Exporter needs to be installed on Verify's infrastructure so that machine metrics can be gathered.\\nVerify runs Ubuntu Trusty which does not have an existing node exporter package.\\nVerify has an existing workflow for packaging binaries which can be leveraged to package node exporter.\\n","Decision":"## Decision\nNode exporter will be packaged as a deb using FPM following Verify's exiting packaging workflow.\\n","Prediction":"## Decision\nVerify will package and distribute node exporter binary packages via the Verify package manager.\\nPackage names will be generated using `package` command from the Verify package manager.\\nPackage names will be generated using `package` command from the Verify package manager.\\nPackage names will be generated using `package` command from the Verify package manager.\\nPackage names will be generated using `package` command from the Verify package manager.\\nPackage names will be generated using `package` command from the Verify package manager.\\nPackage names will be generated using `package` command from the Verify package manager.\\nPackage names will be generated using `package` command from the Verify package manager.\\nPackage names will be generated using `package` command from the Verify package manager.\\nPackage names will be generated using `package` command from the Verify package manager.\\nPackage names will be generated using `package` command from the Verify package manager.\\nPackage names will be generated using `package` command from the Verify package manager.\\nPackage names will be generated using `package` command from the Verify package manager.\\nPackage names will be generated using `package` command from the Verify package manager.\\nPackage names will be generated using `package` command from the Verify package manager.\\"}
{"Context":"## Context\\nVerify provide a \"compliance tool\" which allows relying parties to check that their\\nimplementations are compatible with the SAML profile of Verify.\\nCurrently relying parties must test their service against the compliance tool as part\\nof the onboarding process.\\nBy writing tests against the compliance tool we can:\\n* make sure our own implementation is compliant\\n* demonstrate to relying parties how to write tests against the compliance tool\\n* learn more about the user experience of using the compliance tool\\n","Decision":"## Decision\nWe'll write acceptance tests against the compliance tool for the verify service provider\\nand the stub relying parties.\\nTests for the service provider will give us direct feedback on whether it's compliant.\\nTests for the stub relying parties will give us confidence they work end-to-end and should\\nprovide a template for real relying parties to write their own tests.\\n","Prediction":"## Decision\nWe will use the [Check Verify](https:\/\/checkverify.io) compliance tool for\\ntesting our own implementation.\\nWe will use the [Check Verify](https:\/\/checkverify.io) CLI tool to run our tests\\nagainst the compliance tool.\\nWe will use the [Check Verify](https:\/\/checkverify.io) SDK for our own\\nimplementation.\\nWe will use the [Check Verify](https:\/\/checkverify.io) CI\/CD pipeline to\\nautomate running our tests against the compliance tool.\\nWe will use the [Check Verify](https:\/\/checkverify.io) continuous integration pipeline to\\nautomate running our tests against the compliance tool.\\nWe will use the [Check Verify](https:\/\/checkverify.io) release pipeline to release\\nnew versions of our implementation to our users.\\nWe will use the [Check Verify](https:\/\/checkverify.io) static site to host\\ndocumentation and examples of our implementation.\\nWe will use the [Check Verify](https:\/\/checkverify.io) repository on\\nGitHub to host our implementation.\\nWe will use the [Check Verify](https:\/\/checkverify.io) documentation and examples of our\\nimplementation.\\nWe will use the [Check Verify](https:\/\/checkverify.io) release and"}
{"Context":"## Context and problem statement\\nPhysics models usually include assumptions that simplify the structure of the\\nmodel. For example, splitting a model into a product of independent parts, in\\nwhich every part contains a certain responsibility. In case of partial wave\\namplitude models, we can make a separation into a spin part and a dynamical\\npart. While the spin part controls the probability w.r.t angular kinematic\\nvariables, the dynamics controls the probability on variable like the invariant\\nmass of states.\\nGenerally, a dynamics part is simply a function, which is defined in complex\\nspace, and consists of:\\n- a mathematical **expression** (`sympy.Expr`)\\n- a set of **parameters** in that expression that can be tweaked (optimized)\\n- a set of (kinematic) **variables** to which the expression applies\\n### Technical story\\n- [#124](https:\/\/github.com\/ComPWA\/expertsystem\/issues\/124): see\\n{ref}`adr\/002:Issues with existing set-up`.\\n- [#440](https:\/\/github.com\/ComPWA\/expertsystem\/issues\/440): no way to supply\\ncustom dynamics. Or at least, {mod}`tensorwaves` does not understand those\\ncustom dynamics.\\n- [ADR-001](.\/001.md): parameters _and_ variables are to be expressed as\\n`sympy.Symbol`s.\\n- [#454](https:\/\/github.com\/ComPWA\/expertsystem\/pull\/454): dynamics are\\nspecified as a mapping of `sympy.Function` to a `sympy.Expr`, but now there\\nis no way to supply those `sympy.Expr`s with expected `sympy.Symbol`s\\n(parameters and variables).\\n### Issues with existing set-up\\n- There is no clear way to apply dynamics functions to a specific decaying\\nparticle, that is, to a specific edge of the `StateTransitionGraph`s (`STG`).\\nCurrently, we just work with a mapping of `Particle`s to some dynamics\\nexpression, but this is not feasible when there there are identical particles\\non the edges.\\n- The set of variables to which a dynamics expression applies, is determined by\\nthe position within the `STG` that it is applied to. For instance, a\\nrelativistic Breit-Wigner that is applied to the resonance in some 1-to-3\\nisobar decay (described by an `STG` with final state edges 2, 3, 4 and\\nintermediate edge 1) would work on the invariant mass of edge 3 and 4\\n(`mSq_3+4`).\\n- Just like variables, parameters need to be identifiable from their position\\nwithin the `STG` (take a relativistic Breit-Wigner _with form factors_, which\\nwould require break-up momentum as a parameter), but also require some\\nsuggested starting value (e.g. expected pole position). These starting values\\nare usually taken from the edge and node properties within the `STG`.\\n","Decision":"## Decision\nThe following points are nice to have or can influence the decision but are not\\nessential and can be part of the users responsibility.\\n1. The parameters that a dynamics functions requires, are registered\\nautomatically and linked together.\\n2. Kinematic variables used in dynamics functions are also linked\\nappropriately.\\n3. It is easy to define custom dynamics (no boilerplate code).\\n### Solution requirements\\n1. It is easy to apply dynamics to specific components of the `STG`s. Note:\\nit's important that the dynamics can be applied to resonances of some\\n**selected** graphs and not generally all graphs in which the resonance\\nappears.\\n2. Where possible, suggested (initial) parameter values are provided as well.\\n3. It is possible to use and inspect the dynamics expression itself\\nindependently from the `expertsystem`.\\n4. Follow open-closed principle. Probably the most important decision driver.\\nThe solution should be flexible enough to handle any possible scenario,\\nwithout having to change the interface defined in requirement 1!\\nUse a composition based solution from group 2.\\nImportant is the definition of the interface following solution group 2. This\\nensures to be open-closed and keep the responsibilities separated.\\nThe `expertsystem` favors **composition over inheritance**: we intend to use\\ninheritance only to define interfaces, not to insert behavior. As such, the\\ndesign of the `expertsystem` is fundamentally different than that of SymPy.\\nThat's another reason to favor composition here: the interfaces are not\\ndetermined by the dependency and instead remain\\n{doc}`contained within the dynamics class <002\/composition>`.\\nWe decide to keep responsibilities as separated as possible. This means that:\\n1. The only responsibility of `set_dynamics` method is to attribute some\\nexpression (`sympy.Expr`) the correct symbol within the complete amplitude\\nmodel. For now, this position is specified using some `StateTransitionGraph`\\nand an edge ID, but this syntax may be improved later (see\\n[#458](https:\/\/github.com\/ComPWA\/expertsystem\/pull\/458)):\\n```python\\ndef set_dynamics(\\nself,\\ngraph: StateTransitionGraph,\\nedge_id: int,\\nexpression: sp.Expr,\\n) -> None:\\n# dynamics_symbol = graph + edge_id\\n# self.dynamics[dynamics_symbol] = expression\\npass\\n```\\nIt is assumed that the `expression` is correct.\\n2. The user has the responsibility of formulating the `expression` with the\\ncorrect symbols. To aid the user in the construction of such expressions\\nsome building code can handle some of the common tasks, such as\\n- A `VariablePool` can facilitate finding the correct symbol names (to avoid\\ntypos).\\n```python\\nmass = variable_pool.get_invariant_mass(graph, edge_id)\\n```\\n- A `dynamics` module provides descriptions of common line-shapes as well as\\nsome helper functions. An example would be:\\n```python\\ninv_mass, mass0, gamma0 = build_relativistic_breit_wigner(graph, edge_id, particle)\\nrel_bw: sympy.Expr = relativistic_breit_wigner(inv_mass, mass0, gamma0)\\nmodel.set_dynamics(graph, edge, rel_bw)\\n```\\n3. The `SympyModel` has the responsibility of defining a the full model in\\nterms of an expression and keeping track of variables and parameters, for\\ninstance:\\n```python\\n@attr.s(kw_only=True)\\nclass SympyModel:\\ntop: sp.Expr = attr.ib()\\n# intensities: Dict[sp.Symbol, sp.Expr] = attr.ib(default={})\\n# amplitudes: Dict[sp.Symbol, sp.Expr] = attr.ib(default={})\\ndynamics: Dict[sp.Symbol, sp.Expr] = attr.ib(default={})\\nparameters: Set[sp.Symbol]\\nvariables: Set[sp.Symbol]  # or: VariablePool\\ndef full_expression(self) -> sp.Expr:\\n...\\n```\\n","Prediction":"## Decision\n- [#440](https:\/\/github.com\/ComPWA\/expertsystem\/issues\/440): see\\n{ref}`adr\/002:Issues with existing set-up`.\\n- [#454](https:\/\/github.com\/ComPWA\/expertsystem\/pull\/454): dynamics are\\nspecified as a mapping of `Particle`s to a `sympy.Expr`, but now there\\nis no way to supply those `sympy.Expr`s with expected `sympy.Symbol`s\\n(parameters and variables).\\n### Issues with existing set-up\\n- There is no clear way to apply dynamics functions to a specific decaying\\nparticle, that is, to a specific edge of the `StateTransitionGraph`s (`STG`).\\nCurrently, we just work with a mapping of `Particle`s to some dynamics\\nexpression, but this is not feasible when there there are identical particles\\non the edges.\\n- The set of variables to which a dynamics expression applies, is determined by\\nthe position within the `STG` that it is applied to. For instance, a\\nrelativistic Breit-Wigner that is applied to the resonance in some 1-to-3\\nisobar decay (described by an `STG`"}
{"Context":"## Context\\nWe have to make a decision, how we support e-signatures for work contracts.\\n","Decision":"## Decision\nWe use [Signature Pad](https:\/\/github.com\/szimek\/signature_pad) to implement HTML5 canvas-based e-signatures.\\n","Prediction":"## Decision\nWe will use [e-signature.js](https:\/\/github.com\/evn\/evn-e-signature) for the e-signature component.\\nWe will use the [e-signature-provider-library](https:\/\/github.com\/evn\/evn-e-signature-provider-library) for the e-signature provider.\\nWe will use the [e-signature-provider](https:\/\/github.com\/evn\/evn-e-signature) for the e-signature provider.\\nWe will use the [e-signature-provider-library](https:\/\/github.com\/evn\/evn-e-signature-provider-library) for the e-signature provider.\\nWe will use the [e-signature-provider](https:\/\/github.com\/evn\/evn-e-signature) for the e-signature provider.\\nWe will use the [e-signature-provider-library](https:\/\/github.com\/evn\/evn-e-signature-provider-library) for the e-signature provider.\\nWe will use the [e-signature-provider](https:\/\/github.com\/evn\/evn-e-signature) for the e-signature provider.\\nWe will use the [e-signature-provider-library](https:\/\/github.com\/evn\/evn"}
{"Context":"## Context and Problem Statement\\nWe need a persistent state for the system. Previously I've used Azure SQL but it is quite pricey but has it's own set of features including relational model. But given that system is not very complex I can model persistence mechanism for NoSQL model.\\n## Decision Drivers\\n* Cost\\n* Backup capabilities\\n","Decision":"## Decision\n* Cost\\n* Backup capabilities\\nChosen option: \"Azure Table Storage\", because cost.\\n### Positive Consequences <!-- optional -->\\n* Persistence bill should be down from ~$5\/month to less than $1\/month\\n### Negative Consequences <!-- optional -->\\n* Students in IT Academy won't be exposed to a relational model, which at this moment dominates work they will have to be doing. We will have to come up with a strategy to get them good experience.\\n","Prediction":"## Decision\n* Cost\\n* Backup capabilities\\nChosen option: \"Azure Cosmos DB\", because it has built-in support for NoSQL model and it is backed by Microsoft.\\n### Positive Consequences\\n* Cost is low, we can use it for free for 1 year (for our current instance).\\n* Backup capabilities are available.\\n### Negative Consequences\\n* It is not as mature as SQL Azure, but it's going to be the case for the foreseeable future.\\n* It is not yet available in Azure Regions.\\n* It has limited support for NoSQL models (they are not in the roadmap yet).\\n* It has a more limited set of features.\\n### Impact on the System Architecture\\n* No change in the architecture, it's just a different persistence mechanism.\\n* It's not a change in the model, but a change in the persistence mechanism.\\n## Decision\n* Cost\\n* Backup capabilities\\nChosen option: \"Azure Cosmos DB\", because it has built-in support for NoSQL model and it is backed by Microsoft.\\n### Positive Consequences\\n* Cost is low, we can use it for free for 1 year (for our current instance).\\n* Backup capabilities are available.\\n### Negative"}
{"Context":"## Context\\nTo make the benchmark more meaningful we will want to make them to be more like real world Lambdas. A typical pattern we see with lambdas is fetching data from Amazon DynamoDB.\\n","Decision":"## Decision\nWith the deployment of the Lambdas and API gateway we will also deploy a DynamoDB table.\\n","Prediction":"## Decision\nWe will use the following patterns for fetching data from DynamoDB:\\n- [Get a list of all the queues](https:\/\/docs.aws.amazon.com\/lambda\/latest\/dg\/getting-started-with-dynamodb.html)\\n- [Get a list of all the tables](https:\/\/docs.aws.amazon.com\/lambda\/latest\/dg\/getting-started-with-dynamodb.html#table-list)\\n- [Get a list of all the tables in a specific region](https:\/\/docs.aws.amazon.com\/lambda\/latest\/dg\/getting-started-with-dynamodb.html#table-list-region)\\n- [Get a list of all the tables in a specific table group](https:\/\/docs.aws.amazon.com\/lambda\/latest\/dg\/getting-started-with-dynamodb.html#table-group-list)\\n- [Get a list of all the tables in a specific table](https:\/\/docs.aws.amazon.com\/lambda\/latest\/dg\/getting-started-with-dynamodb.html#table-list)\\n- [Get a list of all the tables in a specific table group](https:\/\/docs.aws.amazon.com\/lambda\/latest\/dg\/getting-started-with-dynamodb.html#table-group-list)\\n- [Get a list of all the tables in a specific table](https:\/\/docs"}
{"Context":"## Context\\nOriginally, we utilized a Spring's JdbcTemplate to quickly CRUD our entities against the data store.  While this was quick and easy, we did most of our filtering in the application as opposed to SQL WHERE clauses.  As we continued, each addition to our entities required a lot of boilerplate code.\\nSpring has great JPA support and Boot uses Hibernate out of the box.  Our entity models are still relatively simple, but using JPA reduces a lot of the boilerplate code, and opens up a lot of additional features \"for free.\"  Specifically, we can utilize JPA to manage our schema updates (naively, later if we need something more robust we can look to Liquibase or Flyway).  It also simplifies joins, where clauses, and gives us more database independence.\\n","Decision":"## Decision\n* Use JPA to map objects into database tables\\n* Use Hibernate as the JPA implementation - Spring Boot's default\\n* Leverage Spring Data's JPA support to implement queries via Repository interface patterns\\n","Prediction":"## Decision\nWe will continue to use Spring's JPA implementation for all CRUD operations.  We will continue to use Hibernate for persisting entities, and we will continue to use LiquidBabel to generate our SQL statements.\\nWe will also continue to use Spring's JDBCTemplate to filter the data returned by our repository.  This is done in the application, and is not exposed to the user.  This is a simpler approach, and it allows us to use Spring's built-in filtering, which is much more robust than the Spring's JDBCTemplate's own filtering.  It also allows us to filter based on the entity's id, which is more robust than Spring's JDBCTemplate's id parameter.  This is because Spring's JDBCTemplate allows a user to pass in a string as an id parameter, which is not supported by Hibernate.  In addition, Spring's JDBCTemplate allows us to use a different parameter type for the id parameter, which is much more robust than Hibernate's id parameter type.  This allows us to filter based on the entity's id, which is more robust than Hibernate's id parameter type.  It also allows us to filter based on the entity's id, which is more robust than"}
{"Context":"## Context\\nUp to now Ditto used Pekko's distributed publish\/subscribe in order to emit e.g. `ThingEvent`s to interested other services:\\n* gateway\\n* websocket\/SSE sessions publishing events\\n* connectivity\\n* AMQP 1.0 \/ AMQP 0.9.1 \/ MQTT \/ Kafka sessions publishing events\\n* things-search\\n* ThingUpdater updating the search index\\nThat naive approach works, but does not provide *horizontal scalability*:\\n* each single service instance generally interested in `ThingEvent`s gets all of them, regardless of whether someone is actually interested in them\\n* as a result a lot of avoidable JSON deserialization is done\\n* when Ditto needs to scale the event publishing horizontally, adding new gateway or connectivity instances will not help scaling the event publishing\\n* still all instances will have to process each `ThingEvent` and discard if not relevant\\n","Decision":"## Decision\nWe will implement a custom Ditto pub\/sub which\\n* uses \"authorization subjects\" as topics when subscribing\\n* uses \"read subjects\" as topics when publishing\\n* manages and distributes the active subscriptions via Pekko Distributed Data (ddata)\\n* emits `ThingEvent`s only to service instances where at least one consumer consumes the event\\n","Prediction":"## Decision\nDitto will use a PubSubPubSub architecture to publish and subscribe to `ThingEvent`s, and will use a dedicated event sink to consume the published events.\\n* Ditto will use a PubSubPubSub architecture to publish and subscribe to `ThingEvent`s, and will use a dedicated event sink to consume the published events.\\n* Ditto will use a PubSubPubSub architecture to publish and subscribe to `ThingEvent`s, and will use a dedicated event sink to consume the published events.\\n* Ditto will use a PubSubPubSub architecture to publish and subscribe to `ThingEvent`s, and will use a dedicated event sink to consume the published events.\\n* Ditto will use a PubSubPubSub architecture to publish and subscribe to `ThingEvent`s, and will use a dedicated event sink to consume the published events.\\n* Ditto will use a PubSubPubSub architecture to publish and subscribe to `ThingEvent`s, and will use a dedicated event sink to consume the published events.\\n* Ditto will use a PubSubPubSub architecture to publish and subscribe to `ThingEvent`s, and will use a dedicated event sink to consume the published events.\\n* Ditto will use a PubSubPub"}
{"Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"## Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","Prediction":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\nWe will use the [Confluence](https:\/\/confluence.atlassian.com\/display\/DOC\/Architecture+Decision+Record) tool to store and organize our ADRs.\\nWe will use the [AIA Document A-160](http:\/\/www.aia.org\/cio\/article\/adr-architecture-decision-records) as our standard format for ADRs.\\nWe will use the [AIA Document E-152](http:\/\/www.aia.org\/cio\/article\/adr-architecture-decision-records-e-152) as our standard format for ADRs.\\nWe will use the [AIA Document C-102](http:\/\/www.aia.org\/cio\/article\/adr-architecture-decision-records-c-102) as our standard format for ADRs.\\nWe will use the [AIA Document C-101](http:\/\/www.aia.org\/cio\/article\/adr-architecture-decision-records-c-101) as our standard format for ADRs.\\nWe will use the [AIA Document C"}
{"Context":"## Context\\nWe want to be confident about the code we write, and for it to be\\nself-documenting as much as possible.\\n[TypeScript](https:\/\/www.typescriptlang.org\/) is a compiled language with\\noptional typing. It's a superset of JavaScript, so is familiar to developers who\\nknow JavaScript. It has wide editor support.\\nAs of writing, TypeScript is used by over\\n[1.4 million repositories](https:\/\/github.com\/microsoft\/TypeScript\/network\/dependents?package_id=UGFja2FnZS01MTE3ODUxNjg%3D)\\non GitHub.\\n","Decision":"## Decision\nWe will use TypeScript.\\n","Prediction":"## Decision\nWe will use TypeScript as the programming language for this project.\\nWe will use it as the build tool.\\nWe will use it as the testing tool.\\nWe will use it as the code review tool.\\nWe will use it as the deployment tool.\\nWe will use it as the CI\/CD tool.\\nWe will use it as the CI\/CD pipeline.\\nWe will use it as the infrastructure for managing dependencies.\\nWe will use it as the infrastructure for managing version numbers.\\nWe will use it as the infrastructure for managing dependencies.\\nWe will use it as the infrastructure for managing version numbers.\\nWe will use it as the infrastructure for managing version numbers.\\nWe will use it as the infrastructure for managing version numbers.\\nWe will use it as the infrastructure for managing version numbers.\\nWe will use it as the infrastructure for managing version numbers.\\nWe will use it as the infrastructure for managing version numbers.\\nWe will use it as the infrastructure for managing version numbers.\\nWe will use it as the infrastructure for managing version numbers.\\nWe will use it as the infrastructure for managing version numbers.\\nWe will use it as the infrastructure for managing version numbers.\\nWe will use it as the infrastructure for"}
{"Context":"## Context\\nRouting is a system to allow a user to describe conditions as to where the survey respondent should go after they complete the answers on a question page.\\nAs these conditions can be any boolean expression we need a more complex routing structure to handle\\n","Decision":"## Decision\nImplement a new version of the routing in the API that can handle nested expressions.\\nThis is heavily inspied by https:\/\/github.com\/ONSdigital\/eq-author-app\/wiki\/Routing,-MK2\\n### Terminology\\n- Routing - all rules etc for a page with a fallback destination when no rules match.\\n- Rule - a series of expressions that when evaluated to true will go to the destination specified.\\n- Expression - Either a binary expression or expression group\\n- Expression Group - A group of BinaryExpressions combined as either `and` or `or`.\\n- Binary Expression - The base item with a left hand side, condtion and right hand side.\\n- Left hand side - The item being compared against (e.g. a question)\\n- Condition - A way of comparing left and right e.g. `=`, `>`, `>=`, `includes`, `not_includes`\\n- Right hand side - The value being compared against a left.\\n### Example\\nOn page 1, when the user enters 5 go to page 2, when they select red or white go to page 3, otherwise go to page 4.\\n```\\npage: {\\ntitle: \"Page 1\",\\nanswers: [\\n{ id: \"answer1\", type: NUMBER, label: \"Answer\" }\\n],\\nrouting: {\\nrules: [\\n{\\nexpressionGroup: {\\noperator: \"AND\",\\nexpressions: [\\n{\\nleft: {\\nanswer: {\\nid: \"Answer1\",\\ntype: CURRENCY\\n}\\n},\\ncondition: \"Equals\",\\nright: {\\nnumber: 5\\n}\\n}\\n]\\n},\\ndestination: {\\npage: {\\nid: \"Page2\"\\n}\\n}\\n},\\n{\\nexpressionGroup: {\\noperator: \"OR\",\\nexpressions: [\\n{\\nleft: {\\nanswer: {\\nid: \"Answer2\",\\ntype: RADIO\\n}\\n},\\ncondition: \"OneOf\",\\nright: {\\noptions: [\\n{\\nvalue: \"red\"\\n},\\n{\\nvalue: \"white\"\\n}\\n]\\n}\\n}\\n]\\n},\\ndestination: {\\npage: {\\nid: \"Page3\"\\n}\\n}\\n}\\n],\\nelse: {\\npage: {\\nid: \"Page4\"\\n}\\n}\\n}\\n}\\n```\\n","Prediction":"## Decision\nInstead of using a single condition for the `:to` endpoint we will now have multiple conditions:\\n- `:to` will be `:new` if the question has not been answered yet\\n- `:to` will be `:answered` if the question has been answered\\n- `:to` will be `:completed` if the question has been answered and all of the answers have been selected\\n- `:to` will be `:failed` if the question has been answered and one of the answers has been selected\\n- `:to` will be `:failed` if the question has been answered and all of the answers have been selected\\n- `:to` will be `:failed` if the question has been answered and the user has selected the `failed` answer\\n- `:to` will be `:failed` if the question has been answered and the user has selected the `failed` answer\\n- `:to` will be `:failed` if the question has been answered and all of the answers have been selected\\n- `:to` will be `:failed` if the question has been answered and the user has selected the `failed` answer\\n- `:to` will"}
{"Context":"## Context\\nAs part of the AtlasDB transaction protocol, write transactions acquire locks from the lock service. They typically\\nacquire two types of locks:\\n- An *immutable timestamp lock*, which AtlasDB uses as an estimate of the oldest running write transaction. The\\nstate of the database at timestamps less than the lowest active immutable timestamp lock is considered immutable, and\\nthus eligible for cleanup by Sweep.\\n- *Row locks* and *cell locks* (depending on the conflict handler of the tables involved in a write transaction) for\\nrows or cells being written to. These locks are used to prevent multiple concurrent transactions from simultaneously\\nwriting to the same rows and committing.\\nTransactions may also acquire additional locks as part of AtlasDB's pre-commit condition framework. These conditions\\nare arbitrary and we thus do not focus on optimising these.\\nAfter a transaction commits, it needs to release the locks it acquired as part of the transaction protocol. Releasing\\nthe immutable timestamp lock helps AtlasDB keep as few stale versions of data around as possible (which factors into\\nthe performance of certain read query patterns); releasing row and cell locks allows other transactions that need to\\nupdate these to proceed.\\nCurrently, these locks are released synchronously and separately after a transaction commits. Thus, there is an\\noverhead of two lock service calls between a transaction successfully committing and control being returned to\\nthe user.\\nCorrectness of the transaction protocol is not compromised even if these locks are not released (though an effort\\nshould be made to release them for performance reasons). Consider that it is permissible for an AtlasDB client to\\ncrash after performing `putUnlessExists` into the transactions table, in which case the transaction is considered\\ncommitted.\\n","Decision":"## Decision\nInstead of releasing the locks synchronously, release them asynchronously so that control is returned to the user very\\nquickly after transaction commit. However, maintaining relatively low latency between transaction commit and unlock\\nis important to avoid unnecessarily blocking other writers or sweep.\\nTwo main designs were considered:\\n1. Maintain a thread pool of `N` consumer threads and a work queue of tokens to be unlocked. Transactions that commit\\nplace their lock tokens on this queue; consumers pull tokens off the queue and make unlock requests to the lock\\nservice.\\n2. Maintain a concurrent set of tokens that need to be unlocked; transactions that commit place their lock tokens\\nin this set, and an executor asynchronously unlocks these tokens.\\nSolution 1 is simpler than solution 2 in terms of implementation. However, we opted for solution 2 for various reasons.\\nFirstly, the latency provided by solution 1 is very sensitive to choosing `N` well - choosing too small `N` means that\\nthere will be a noticeable gap between transaction commit and the relevant locks being unlocked. Conversely, choosing\\ntoo large `N` incurs unnecessary overhead. Choosing a value of `N` in general is difficult and would likely require\\ntuning depending on individual deployment and product read and write patterns, which is unscalable.\\nSolution 2 also decreases the load placed on the lock service, as fewer unlock requests need to be made.\\nIn our implementation of solution 2, we use a single-threaded executor. This means that on average the additional\\nlatency we incur is about 0.5 RPCs on the lock service (assuming that that makes up a majority of time spent in\\nunlocking tokens - it is the only network call involved).\\n### tryUnlock() API\\n`TimelockService` now exposes a `tryUnlock()` API, which functions much like a regular `unlock()` except that the user\\ndoes not need to wait for the operation to complete. This API is only exposed in Java (not over HTTP).\\nThis is implemented as a new default method on the `TimelockService` that delegates to `unlock()`; usefully, remote\\nFeign proxies calling `tryUnlock()` will make an RPC for standard `unlock()`. This also gives us backwards\\ncompatiblity; a new AtlasDB\/TimeLock client can talk to an old TimeLock server that has no knowledge of this endpoint.\\n### Concurrency Model\\nIt is essential that adding an element to the set of outstanding tokens is efficient; yet, we also need to ensure that\\nno token is left behind (at least indefinitely). We thus guard the concurrent set by a (Java) lock that permits both\\nexclusive and shared modes of access.\\nTransactions that enqueue lock tokens to be unlocked perform the following steps:\\n1. Acquire the set lock in shared mode.\\n2. Read a reference to the set of tokens to be unlocked.\\n3. Add lock tokens to the set of tokens to be unlocked.\\n4. Release the set lock.\\n5. If no task is scheduled, then schedule a task by setting a 'task scheduled' boolean flag.\\nThis uses compare-and-set, so only one task will be scheduled while no task is running.\\nFor this to be safe, the set used must be a concurrent set.\\nThe task that unlocks tokens in the set performs the following steps:\\n1. Un-set the task scheduled flag.\\n2. Acquire the set lock in exclusive mode.\\n3. Read a reference to the set of tokens to be unlocked.\\n4. Write the set reference to point to a new set.\\n5. Release the set lock.\\n6. Unlock all tokens in the set read in step 3.\\nThis model is trivially _safe_, in that no token that wasn't enqueued can ever be unlocked, since all tokens that can\\never become unlocked must have been added in step 3 of enqueueing, and unlocking a lock token is idempotent modulo\\na UUID clash.\\nMore interestingly, we can guarantee _liveness_ - every token that was enqueued will be unlocked in the absence of\\nthread death. If an enqueue has a successful compare-and-set in step 5, then the token must be in the set\\n(and is visible, because we synchronize on the set lock). If an enqueue does _not_ have a successful compare-and-set,\\nthen some thread must already be scheduled to perform the unlock, and once it does the token must be in the relevant\\nset (and again must be visible, because we synchronize on the set lock).\\nTo avoid issues with starving unlocks, we use a fair lock scheme. Once the unlocking thread attempts to acquire the set\\nlock, enqueues that are still running may finish, but fresh calls to enqueue will only be able to acquire the set lock\\nafter the unlocking thread has acquired and released it. This may have lower throughput than an unfair lock,\\nbut we deemed it necessary as 'readers' (committing transactions) far exceed 'writers' (the unlocking thread) -\\notherwise, the unlocking thread might be starved of the lock.\\n### TimeLock Failures\\nIn some embodiments, the lock service is provided by a remote TimeLock server that may fail requests. There is retry\\nlogic at the transport layer underneath us.\\nPreviously, running a transaction task would throw an exception if unlocking row locks or the immutable timestamp\\nfailed; we now allow user code to proceed and only emit diagnostic logs indicating that the unlock operation failed.\\nThis is a safe change, as throwing would not make the locks become available again, and user code cannot safely\\nassume that locks used by a transaction are free after it commits (since another thread may well have acquired them).\\nIn practice, locks will be released after a timeout if they are not refreshed by a client. This means that not\\nretrying unlocks is safe, as long as we do not continue to attempt to refresh the lock. AtlasDB clients automatically\\nrefresh locks they acquire; we ensure that a token being unlocked is synchronously removed from the set of locks\\nto refresh *before* it is put on the unlock queue.\\n","Prediction":"## Decision\nWe will introduce a new transaction protocol to the AtlasDB protocol that will allow for the release of the immutable\\ntimestamp locks in a manner that is more efficient than currently possible.\\nThis new transaction protocol will allow for lock release in a manner that is separate from the commit operation.\\nThis means that a transaction will be able to release a lock for an entire row or cell in a single\\ntransaction, rather than needing to release a lock for each row or cell that it needs to change. Lock release\\nwill be performed synchronously, so that the lock release is completed before the commit operation is\\ndone.\\nLock release will be performed by the lock service, so that a transaction can block waiting for lock release\\nbefore it commits. The lock service can then continue to block waiting for lock release while the transaction\\ndoes its work.\\n### Lock release\\nLock release will be performed by the lock service, so that a transaction can block waiting for lock release\\nbefore it commits. The lock service can then continue to block waiting for lock release while the transaction\\ndoes its work.\\n### Transaction protocol\\nThe transaction protocol will be split into two parts: a new transaction protocol to release the locks\\ninvolved in the transaction and"}
{"Context":"## Context\\nApplications that consume `Marain.UserNotifications` need control over the appearance of the notifications that end users receive. At a bare minimum, notifications must convey all relevant information, but organizations also typically want communications to look 'on brand'. For example, they will want full control over the layout and design of email notifications. Furthermore, each communication channel is likely to have different requirements: SMS notifications need brevity and have very little design flexibility compared to email, for example.\\nThis ADR describes how `Marain.UserNotifications` provides a templating mechanism to give consuming applications control over the presentation of notifications. Templating is a widely-used solution to this problem. It enables a separation of concerns: application code can focus on deciding when to send a notification, and collecting the relevant information; the concern of how to present that information is handled by templates.\\nA template defines the design and layout for a notification. Typically, most notifications of a particular type will share layout and even most of the content, but there will be places where values must be inserted to convey the particulars to the end user. For example, in a birthday email notification, most of the design and formatting of the email\/subject will be fixed, but certain custom data (eg. the recipient's name) will be different in each notification. So an application defines templates with placeholders for this variable data, and the templating engine plugs the real data into place each time a message is to be sent.\\n","Decision":"## Decision\nCode in a consuming application that raises a notification will build a property bag\u2014a key\/value collection containing all of the information that can vary across instances of a particular notification type. For example, this might include the display name of the message recipient, and the name of a customer. The application sends this, along with the notification type, to the `Marain.UserNotifications` Management API as a request to send a notification.\\nTo enable `Marain.UserNotifications` to transform this property bag into an actual message (email\/SMS\/etc.), consuming applications define a set of templates for each distinct notification type they produce. This set must include one template for each communication channel (SMS, email, web push notifications etc.) through which notifications of that type will be sent. E.g., if an application defines 10 messages types, and if each of these could be sent either by SMS or email, the application will need to supply 20 templates. Whenever the application sends a notification, `Marain.UserNotifications` will apply the relevant template to produce a communication object of the appropriate type (Web push, Sms, Email etc.), which it will then send via the desired delivery channel (SendGrid, Twillio, Airship, etc.).\\nFor example, if a notification is going to be sent as a web push message, the delivery channel requires an object that includes three things: a title, the message body, and an action URL. A web push notification template describes how to form all three of these from the property bag for a particular notification type.\\n### Creation and storage of templates\\nTemplates are stored in tenanted blob storage, using the notification type and communication type as a filename. For example, the template describing how a notification of type `marain.lead.created.v1` would be presented as a web push message would live in a blob named:\\n`marain.lead.created.v1:WebPush`.\\nThe blob would look something like this:\\n```json\\n{\\n\"body\": \"You have been assigned a new lead by {{assignedByUserName}}\",\\n\"title\": \"New Lead\",\\n\"contentType\": \"application\/vnd.marain.usernotifications.notificationtemplate.webpushtemplate.v1\",\\n\"notificationType\": \"marain.lead.created.v1\",\\n\"actionUrl\": \"{{actionUrl}}\"\\n}\\n```\\nSince different communication channels require different sets of information, the exact template structure will depend on the channel. For example, an email template would, like a web push template, have a `body` and a `title`, but would not have an `actionUrl`.\\nThere are no default templates. Each organization must define a complete set of templates covering each communication channel for each notification type that it wishes to raise. It is an error for an application to attempt to send a notification if it has not defined a template for the notification type\/channel combination. (If configuration dictates that the message will be delivered through multiple channels, then templates for every channel must be supplied for that notification type.) If certain channels will not be used for particular notification types, the application does not need to supply a template for those notification type\/channel combinations.\\n### Rendering templates\\nWhen applications send a notification request to the Management API, they specify the notification type, and provide a property bag. The `Marain.UserNotifications` service is responsible for determining which channels are to be used, and then for each channel, it will locate the correct template (WebPush, Email), and apply the template engine for each of the fields that the channel requires.\\n`Marain.UserNotifications` uses the [`Corvus.DotLiquidAsync`](https:\/\/github.com\/corvus-dotnet\/Corvus.DotLiquidAsync) templating engine, supplying properties from the notification property bag. It invokes the engine for each required field. For example, with the Web Push template shown above, the template engine will be run three times for that template (with the same property bag each time) on the `body`, `title`, and `actionUrl` fields.\\n","Prediction":"## Decision\nWe will support the following templating mechanisms:\\n- HTML template: a template that is generated from an HTML document using a simple templating engine such as Twig or Pug, and that contains the relevant content and layout.\\n- JSON template: a template that is generated from an JSON document using a simple templating engine such as Node.js templates (such as Pug), and that contains the relevant content and layout.\\n- YAML template: a template that is generated from an YAML document using a simple templating engine such as Babel, and that contains the relevant content and layout.\\n- Markdown template: a template that is generated from an Markdown document using a simple templating engine such as Hugo, and that contains the relevant content and layout.\\n- CSS template: a template that is generated from an CSS document using a simple templating engine such as SASS, and that contains the relevant content and layout.\\n- Image template: a template that is generated from an image using a simple templating engine such as Gulp, and that contains the relevant content and layout.\\n- CSS variables: a template that is generated from an HTML document using a simple templating engine such as SASS, and that contains the relevant content and layout.\\"}
{"Context":"* [Context](#context)\\n* [Decision](#decision)\\n* [Status](#status)\\n* [Consequences](#consequences)\\n* [More reading](#more-reading)\\n* [Updates](#updates)\\n","Decision":"## Decision\n* [Status](#status)\\n* [Consequences](#consequences)\\n* [More reading](#more-reading)\\n* [Updates](#updates)\\nCommand Query Responsibility Segregation and Event-Driven Architecture (CQRS+ES) solves those problems with ease. When we took a look, especially at [Martin's Event-Sourcing technique](https:\/\/martinfowler.com\/articles\/201701-event-driven.html), we understood that we could create an event-driven application that can react only to events.\\nThe idea is simple: since we're dealing with compliance and audit departments, we can see **every** action in the platform as **commands** and dispatch **events** based on the result of those commands. That way, we can restore **any** entity from scratch. Even if we suffer from a mass failure in all our servers in this architecture, we could still reconstruct every entity from their event store.\\nThis pattern adds an extra layer of complexity: reading data. If we try to build our entities from scratch at every query, we would have a poor performance. That's where CQRS (Comand Query Responsibility Segregation) comes in handy.\\nCQRS segregates (as the name suggests) the **command** and **query** stacks. Before digging into the concept, you must understand the definition of those two concepts:\\n* **Command:** is an intention, an action. Always imperative, like: `RaiseSalary`. Our domain layer receives commands to execute side effects based on those.\\n* **Event:** is the result of a command. Always in the past, like: `SalaryRaised`. Our domain dispatches events based on resulting side-effects.\\nIn a nutshell, the CQRS architecture focuses our domain layer in handling commands (received from a message broker queue). In contrast, our application layer can return our entities' last know state by accessing the database directly. Like the following diagram:\\n![Diagram explaining the CQRS Layered Architecture hierarchy](..\/assets\/0005-cqrs+es\/cqrs-layered-architecture.png)\\nOur domain layer will not have any updated copy of our entities. Executing all actions again could cost a lot in compute time, so we're going to create some snapshots to consolidate our entities' state in time. This strategy is a pretty common CQRS standard, and most databases support it.\\nThe last concept we must cover is the sync between our command and query stacks. In our architecture, we will do it using a consumer that will consume dispatched events and update our query database accordingly.\\n**IMPORTANT:** This architecture is not mandatory across all bounded contexts. Some of those may not have any benefits from event-sourcing at all. Those could use a more standard approach instead of the suggested in this ADR.\\n","Prediction":"## Decision\n* [Status](#status)\\n* [Consequences](#consequences)\\n* [More reading](#more-reading)\\n* [Updates](#updates)\\nWe decided to use the [Docker Compose](https:\/\/docs.docker.com\/compose\/) architecture to build and run our applications. Docker Compose is a tool that allows us to easily configure and manage multiple layers of services in a single, portable container image. It also allows us to easily control networking, security, and other aspects of our deployment pipeline, making it easier to scale and deploy applications in a reliable and repeatable manner.\\nDocker Compose also allows us to easily build, test, and deploy microservices, which enables us to deploy and run our applications in a scalable, reliable manner. It also allows us to easily scale our services and components, while maintaining high levels of availability and compliance.\\nDocker Compose also helps us to easily manage the deployment of a diverse range of applications, from simple web applications to highly scalable, mission-critical applications.\\n## Decision\n* [Status](#status)\\n* [Consequences](#consequences)\\n* [More reading](#more-reading)\\n* [Updates](#updates"}
{"Context":"## Context\\nUse scala for the server side processes as it's an approved language for use in the Home Office,\\n","Decision":"## Decision\nUse scala\\n","Prediction":"and the majority of the team are using it.\\n## Decision\nUse scala for the server side processes and python for the client side.\\nThis will allow the majority of the team to continue using scala.\\nUse python for command line tools and libraries as it's an approved language for use in the Home Office.\\nThis will allow the majority of the team to continue using python.\\nUse Docker for the CI\/CD pipeline as it's an approved tool for use in the Home Office.\\nThis will allow the majority of the team to continue using Docker.\\nUse Jupyter notebooks for the development of the client side and server side code as it's an approved tool for use in the Home Office.\\nThis will allow the majority of the team to continue using Jupyter notebooks.\\nUse a continuous integration and continuous delivery tool such as Travis CI, Circle CI or Jenkins as it's an approved tool for use in the Home Office.\\nThis will allow the majority of the team to continue using continuous integration and continuous delivery tools.\\nUse a code review tool such as Circle Review, GitHub Actions or Jenkins as it's an approved tool for use in the Home Office.\\nThis will allow the majority of the team to continue using code review tools.\\nUse a"}
{"Context":"## Context\\nWe are tightly coupled to `lbh-frontend` and `govuk-frontend`. We want to ensure\\nusers having different versions doesn't cause problems.\\nIt's also likely that users of this library will have no direct need of either\\n`lbh-frontend` or `govuk-frontend` as neither supports React. They also both use\\nSass for their stylesheets, and we don't want to force users to do the tooling\\nto also support Sass themselves.\\n","Decision":"## Decision\nWe will bundle `lbh-frontend` and `govuk-frontend` into our distributables.\\n","Prediction":"## Decision\nWe will make `lbh-frontend` and `govuk-frontend` compatible with each other. We will\\ntarget the same version of the library, and use the same version of the Sass\\ntoolchain for all Sass and SCSS files.\\nWe will use the same version of the Sass compiler for both Sass and SCSS. This\\nmeans that if you have a SCSS file in your project, you will be able to compile it with\\nthe Sass compiler for that version.\\nWe will use `sass-preset` for compiling Sass. This means that we will have the same\\n`sass-preset` for both Sass and SCSS compiled files, and we will be able to run the\\nsame Sass compiler on both Sass and SCSS.\\nWe will use the same version of the Sass loader for both Sass and SCSS. This means\\nthat if you have a Sass file in your project, you will be able to load it with the same\\ntoolchain and Sass compiler. This means that if you have a SCSS file in your project, you will\\ntarget the same version of the Sass compiler for that version.\\nWe will use the latest version of the Sass loader for both Sass"}
{"Context":"## Context\\nAfter implementing several real business domains we have found that it can be\\ndifficult to use `AggregateCommandScope.Destroy()` effectively.\\nIdeally, `Destroy()` could be called after recording any event that effectively\\n\"resets\" the aggregate's state, from the perspective of the business logic.\\nIn practice, logic that results in a call to `Destroy()` may be followed by some\\nconditional logic that records a new event. In existing implementations this\\nresults in a panic from the engine, though as of ADR-16, which removed\\n`Create()` this is no longer documented as required engine behavior.\\n","Decision":"## Decision\nRemove the requirement that any call to `Destroy()` be preceeded by a call to\\n`RecordEvent()` within the same scope.\\nCalling `RecordEvent()` *after* `Destroy()` event should \"negate\" the call to\\n`Destroy()`, as though it were never called.\\n","Prediction":"## Decision\nWe will add a new command `AggregateCommandScope.Destroy()` which will be\\nimplemented as a separate class and called from the `AggregateCommandScope`\/\\n`AggregateCommandScopeManager`.\\nThis command will be implemented using the command's `Destroy()` method\\nwhich will be called by the command's `Dispose()` method.\\nThis command will be called when the aggregate's state is changed from\\nthe aggregate's current state, to the aggregate's next state, or the aggregate's\\nnext command is being issued.\\nThis command will be called when the aggregate's state is changed to\\nthe aggregate's next state, or the aggregate's next command is being\\nissued.\\nThis command will be called when the aggregate's state is changed to\\nthe aggregate's next state, or the aggregate's next command is being\\nissued.\\nThis command will be called when the aggregate's state is changed to\\nthe aggregate's next state, or the aggregate's next command is being\\nissued.\\nThis command will be called when the aggregate's state is changed to\\nthe aggregate's next state, or the aggregate's next command is being\\nissued.\\nThis command will be called when the aggregate's state is changed to\\nthe aggregate's"}
{"Context":"## Context\\nFor our current monitoring to work we need Icinga to be able to communicate with hosts and services.\\n","Decision":"## Decision\nEach stack will have an internal, private, zone for internal services such as the puppetmaster. These\\nwill be in the following format:\\n$servicename.$stackname.internal\\npuppet.perftesting.internal\\nmonitoring.mystack.internal\\n","Prediction":"## Decision\nWe will use the Icinga2 library to communicate with our monitoring tools.\\nWe will use the Icinga2 library to communicate with our monitoring tools.\\nWe will use the Icinga2 library to communicate with our monitoring tools.\\nWe will use the Icinga2 library to communicate with our monitoring tools.\\nWe will use the Icinga2 library to communicate with our monitoring tools.\\nWe will use the Icinga2 library to communicate with our monitoring tools.\\nWe will use the Icinga2 library to communicate with our monitoring tools.\\nWe will use the Icinga2 library to communicate with our monitoring tools.\\nWe will use the Icinga2 library to communicate with our monitoring tools.\\nWe will use the Icinga2 library to communicate with our monitoring tools.\\nWe will use the Icinga2 library to communicate with our monitoring tools.\\nWe will use the Icinga2 library to communicate with our monitoring tools.\\nWe will use the Icinga2 library to communicate with our monitoring tools.\\nWe will use the Icinga2 library to communicate with our monitoring tools.\\nWe will use the Icinga2 library to communicate with our monitoring tools"}
{"Context":"## Context\\nGrout is currently tightly coupled to a PostgreSQL dependency. Postgres is a powerful\\ndatabase engine, but it's also difficult to deploy, and potentially a serious\\nbarrier to adoption for Grout.\\nTo make Grout even easier to use, we would like to [support NoSQL\\nbackends](https:\/\/github.com\/azavea\/grout-2018-fellowship\/issues\/13). However,\\nfor a NoSQL backend to be feasible, it must provide support for a few key\\nGrout functions:\\n1. Storing geospatial data\\n2. Querying geospatial data\\n3. Querying complex JSON data, including:\\n- Nested search (e.g. find the value of `baz` in`{\"foo\": {\"bar\": {\"baz\": 1}}}`)\\n- Compound rules (`foo AND bar OR baz`)\\n4. (Optional) JSONSchema support\\nIn addition, NoSQL backends should be considered based on how much value they\\nbring to the project, including:\\n4. How easy the backend is to configure and deploy\\n5. How cheap it is to use, if not free\\nIn this document, I look at a few different options for [NoSQL\\nbackends](#backends), including:\\n- MongoDB\\n- CouchDB\\n- Google Cloud Firestore\\n- AWS DynamoDB\\n- JSONBin\\nAfter eliminating options that cannot support the basic requirements of Grout,\\nI then evaluate [cloud providers](#providers). Finally, I [consider the\\nvalue](#value) that a NoSQL backend would bring to the project.\\n### Backends\\n#### MongoDB\\n[MongoDB](https:\/\/www.mongodb.com\/what-is-mongodb) is an open-source\\nnonrelational database that stores data in a JSON-like format. It translates\\nJSON objects into \"documents,\" which are organized in \"collections.\"\\nHere's how it scores on the requirements:\\n1. Geospatial data \u2714\ufe0f\\n- MongoDB can store [arbitrary\\nGeoJSON](https:\/\/docs.mongodb.com\/manual\/geospatial-queries\/#geospatial-data),\\nmeaning that points and polygons are supported.\\n2. Geospatial queries \u2714\ufe0f\\n- MongoDB supports geospatial querying for [`intersection`, `within`, and\\n`near` queries](https:\/\/docs.mongodb.com\/manual\/geospatial-queries\/#id1).\\n- Queries make use of GeoJSON objects for comparison operations.\\n3. Querying JSON \u2714\ufe0f\\n- Run queries on [nested fields](https:\/\/docs.mongodb.com\/manual\/tutorial\/query-embedded-documents\/#query-on-nested-field).\\n- Supports [AND conditions](https:\/\/docs.mongodb.com\/manual\/tutorial\/query-embedded-documents\/#specify-and-condition).\\n- Supports [logical OR conditions](https:\/\/docs.mongodb.com\/manual\/reference\/operator\/query\/or\/).\\n4. JSONSchema support \u2714\ufe0f\\n- As of v3.6, MongoDB supports JSONSchema for [validating `create` and `update`\\noperations](https:\/\/docs.mongodb.com\/manual\/reference\/operator\/query\/jsonSchema\/).\\n- Draft 4 is supported, but with \"some limitations\". The documentation\\ndoesn't describe what those limitations are, however.\\n#### CouchDB\\n[CouchDB](https:\/\/couchdb.apache.org\/) is an open-source nonrelational\\ndatabase, similar to MongoDB. Its focus is on supporting highly-distributed\\ndatabases. All data is stored as JSON-like \"documents\" in the database, which is\\nqueryable over a REST API.\\n1. Geospatial support \u2753\\n- Provided through an official plugin, [GeoCouch](https:\/\/github.com\/couchbase\/geocouch\/)\\n- Documentation is [not great](https:\/\/github.com\/couchbase\/geocouch\/blob\/master\/gc-couchdb\/README.md),\\nonly distributed through GitHub, no new tagged releases since 2013\\n2. Geospatial queries \u2753\\n- GeoCouch can [save points and return bounding box\\nrequests](https:\/\/github.com\/couchbase\/geocouch\/blob\/master\/gc-couchdb\/README.md)\\n3. Querying JSON \u2714\ufe0f\\n- Query capability is powerful but [quite\\nesoteric](http:\/\/docs.couchdb.org\/en\/2.1.2\/query-server\/protocol.html#map-doc)\\n- A third-party JavaScript API, [PouchDB](https:\/\/pouchdb.com\/), is\\navailable.\\n4. JSONSchema support \u274c\\n- External references are possible, and you can define validation functions\\nfor `create` and `update` operations, so we could [hack together our own\\nschema solution](https:\/\/developer.ibm.com\/dwblog\/2012\/schemas-in-couchdb\/).\\nBut nothing comes out of the box.\\n#### Google Cloud Firestore\\n[Google Cloud Firestore](https:\/\/cloud.google.com\/firestore\/docs\/) is\\na proprietary nonrelational database service provided by Google. It uses a data model\\nvery similar to MongoDB, where objects are stored as \"documents\" inside of\\n\"collections.\" It is currently in beta.\\n1. Geospatial support \u2753\\n- Firestore has a type for [lat\/lng\\ncoordinates](https:\/\/cloud.google.com\/firestore\/docs\/concepts\/data-types).\\nPresumably they're stored as WebMercator, but there's not much\\ninformation.\\n- Firestore does not support polygons.\\n2. Querying geospatial data \u274c\\n- Seems to not be possible. I can't find any documentation or blog posts\\non the issue.\\n3. Querying JSON \u2753\\n- Firestore has a few [serious\\nlimitations](https:\/\/firebase.google.com\/docs\/firestore\/query-data\/queries#query_limitations):\\n- Cannot query across multiple collections\\n- Cannot support range filters on multiple fields\\n- Cannot support logical `OR` queries (reccommends performing multiple\\nqueries and merging them client-side, but that may not work for all\\nuse cases)\\n4. JSONSchema support \u274c\\n- No results [in the\\nsearch](https:\/\/cloud.google.com\/s\/results\/?q=jsonschema&p=%2Ffirestore%2F)\/\\n- You can define validation patterns as [security\\nrules](https:\/\/cloud.google.com\/firestore\/docs\/manage-data\/transactions?authuser=0&hl=ru#data_validation_for_atomic_operations),\\nbut you have to use Firestore's idiomatic security rule syntax.\\n#### AWS DynamoDB\\n[DynamoDB](https:\/\/aws.amazon.com\/dynamodb\/) is a proprietary nonrelational database\\nprovided as a service by Amazon Web Services. It supports key-value- and\\ndocument-based data storage. Objects are stored as _items_ in _rows_, where they\\nare queried by a primary key. It is designed to easily integrate with other\\nAWS products.\\n1. Geospatial support \u2753\\n- DynamoDB supports [geohashes for point\\ngeometries](https:\/\/aws.amazon.com\/about-aws\/whats-new\/2013\/09\/05\/announcing-amazon-dynamodb-geospatial-indexing\/).\\n- No support for polygons.\\n2. Geospatial queries \u2753\\n- Relies on [a third-party Java library that is sparsely\\ndocumented](https:\/\/blog.corkhounds.com\/2017\/06\/19\/geospatial-queries-on-aws-dynamodb\/).\\n- Allows searching for coordinates within a radius or a box.\\n3. Querying JSON \u2753\\n- Some limitations:\\n- Queries only search items attached to a primary key.\\n- Queries can retrieve a maximum of 1MB of data, which raises a\\nquestion for images.\\n- True to AWS form, [the query docs are a slog to\\ndecipher](https:\/\/docs.aws.amazon.com\/amazondynamodb\/latest\/developerguide\/Query.html).\\nAs a result, I'm not confident I fully understand the capability here.\\n4. JSONSchema support \u274c\\n- [No schema validation\\nwhatsoever](https:\/\/stackoverflow.com\/questions\/42563529\/is-there-a-way-to-enforce-a-schema-constraint-on-an-aws-dynamodb-table).\\n#### JSONBin\\nJSONBin is a [JSON-store-as-a-service](https:\/\/jsonbin.org\/) that allows\\ndevelopers to easily store and retrieve JSON data over a REST API. In this\\nsense, it is most like a serverless JSON datastore. It requires\\nno configuration, and has a built-in permissions and token authentication\\nsystem. Its main focus is simplicity and ease-of-use.\\n1. Geospatial support \u2714\ufe0f\\n- Since JSONBin stores documents as pure JSON, GeoJSON is inherently\\nsupported.\\n2. Geospatial queries \u274c\\n- Querying is [on the product roadmap](https:\/\/jsonbin.io\/roadmap), but\\nis not currently supported.\\n3. Querying JSON \u274c\\n- Querying is [on the product roadmap](https:\/\/jsonbin.io\/roadmap), but\\nis not currently supported.\\n4. JSONSchema support \u274c\\n- No mention of validation in the [API documentation](https:\/\/jsonbin.io\/api-reference).\\n### Providers\\nBased my research summarized above, **MongoDB is the only backend I considered\\nthat supports all project requirements.** As such, I only considered cloud\\nproviders for MongoDB.\\nThere are two main cloud providers for MongoDB: MongoDB Atlas, a hosting service\\nprovided by MongoDB itself, and mLab, a third-party service.\\n#### mLab\\nTo read an argument in favor of mLab, see [mLab's account of why it's better than MongoDB\\nAtlas](https:\/\/mlab.com\/mlab-vs-atlas\/)\\n- **Pros**:\\n- Free, non-expiring [sandbox account](https:\/\/mlab.com\/plans\/pricing\/#plan-type=sandbox).\\n- Up to 500MB storage with shared hosting and variable RAM.\\n- [Integrates with AWS](https:\/\/docs.mlab.com\/), and is quick and easy\\nto set up.\\n- **Cons**:\\n- Provided by a third-party.\\n- Lags behind the current release of MongoDB.\\n- However, 3.6 will be available for sandbox accounts as of July 20th.\\n#### MongoDB Atlas\\nTo read an argument in favor of MongoDB Atlas, see [MongoDB Atlas' account of\\nwhy it's better than mLab](https:\/\/www.mongodb.com\/cloud\/atlas\/compare).\\n- **Pros**:\\n- Provided by the MongoDB team.\\n- [Integrates with AWS](https:\/\/docs.atlas.mongodb.com\/getting-started\/#for-cloud-provider-region-select-your-preferred-cloud-provider).\\n- [Slightly complicated to set up](https:\/\/docs.atlas.mongodb.com\/getting-started\/),\\nbut still very easy relative to hosting your own database.\\n- Integrates with [MongoDB Stitch](https:\/\/docs.mongodb.com\/stitch\/), a\\nserverless backend-as-a-service for MongoDB applications.\\n- **Cons**:\\n- The free tier comes with [some major\\nlimitations](https:\/\/docs.atlas.mongodb.com\/reference\/free-shared-limitations\/):\\n- \"Idle instances\" may be terminated.\\n- No backups.\\n- Max throughput of 100 writes\/sec.\\n- Data transfer limit of 10GB\/week.\\n### Value\\nThe two main goals of supporting a NoSQL backend include:\\n1. Making Grout easier to configure and deploy\\n2. Abstracting the data model such that it could be applied to more backends\\nin the future\\nI'll look at each of these goals in detail.\\n#### Making Grout easier to configure and deploy\\n- **Pros**:\\n- MongoDB is slightly easier to configure than Postgres. However, it\\nstill requires [some configuration](https:\/\/docs.mongodb.com\/manual\/reference\/configuration-options\/)\\nfrom the developer.\\n- **Cons**:\\n- A database server still must be deployed to interact with the database.\\nThis means that the developer must still deploy a database and a database\\nserver in addition to the frontend.\\n- MongoDB offers a serverless framework, MongoDB Stitch, which exposes an API\\n[that can be queried from the client\\nside](https:\/\/docs.mongodb.com\/stitch\/getting-started\/configure-rules-based-access-to-mongodb\/).\\nUsing a \"serverless\" pattern like this would be a substantial improvement on\\nthe existing development framework, but moving in this direction would\\nrequire the Grout NoSQL backend to be tightly coupled to a certain provider\\n(MongoDB Stitch), and would introduce another layer of integration that would\\nslow the development process.\\n**Overall score**: Medium-low.\\n#### Abstracting the data model\\n- **Pros**:\\n- Using MongoDB would let us represent the data model in JSONSchema\\nand validate it on the backend. This would be a big win for\\ngeneralizability.\\n- **Cons**:\\n- JSONSchema is not widely supported among NoSQL providers, according to\\nmy research.\\n**Overall score**: Medium.\\n","Decision":"## Decision\nBased on my research, I recommend that if we move forward with a NoSQL backend,\\nwe should build it on top of **MongoDB**. While there is promising work being\\ndone on many nonrelational databases, MongoDB is currently the only NoSQL database\\nthat satisfies all project requirements.\\nWhile MongoDB satisifes the project requirements, however, I'm still uncertain\\nwhether the value proposition is strong enough that we should prioritize this\\nwork. It seems to me that MongoDB will only present a strong value proposition\\nif the backend component can be eliminated completely from the infrastructure\\nprovisioning; otherwise, deploying a NoSQL database requires essentially the\\nsame amount of work. With MongoDB, this is technically possible with the **Atlas and\\nStitch serverless stack**; however, choosing this stack will require\\nintegrating with two separate services, making the work much more complex and\\nintroducing an undesirable coupling to the application.\\n","Prediction":"#### MongoDB Stitch\\nThis serverless framework is built on top of MongoDB's existing\\nserverless database, and provides a familiar MongoDB API. It is\\ntightly integrated with MongoDB Atlas, so it's easy to set up.\\n## Decision\nI will use MongoDB as a database backend for Grout.\\nThis decision is supported by the following factors:\\n1. MongoDB is the most widely supported NoSQL database, and\\nis the most widely deployed NoSQL database.\\n2. MongoDB has a serverless stack, MongoDB Stitch, which will\\ntake care of most of the configuration for Grout.\\n3. MongoDB's MongoDB Atlas account is free, and comes with\\nunlimited storage and a 99.99% uptime SLA.\\n4. MongoDB's MongoDB Stitch account is free, and comes with\\nunlimited storage and a 99.99% uptime SLA.\\n5. MongoDB's MongoDB Atlas account is supported by the MongoDB team,\\nand is well-documented.\\n6. MongoDB has a serverless stack, MongoDB Stitch, which will take care of\\nmost of the configuration for Grout.\\n7. MongoDB Atlas accounts are supported by the MongoDB team,\\nand are well-documented.\\n8. MongoDB Atlas"}
{"Context":"## Context\\nThis tool had, up until this point, made assumptions about how its users might interpret the information it presents (for example, using colour as the main mechanism of distinguishing record types)\\n","Decision":"## Decision\nAccessibility will now be a first-class concern of this project. All future design decisions should bear this in mind.\\n","Prediction":"## Decision\nWe will use the [blind trust](https:\/\/en.wikipedia.org\/wiki\/Blind_trust) principle to ensure that our users only trust us to present information that they can understand.\\nThis means that we will present the data in a way that is not obvious to the user (for example, if we present a list of records in a table, we will not make a distinction between the records that have the same name, or the same email address).\\nWe will also present data in a way that is not obvious to the user (for example, if we present a list of records in a table, we will not present the same records twice).\\nWe will also present data in a way that is not obvious to the user (for example, if we present a list of records in a table, we will not present the same records twice).\\nWe will present the data in a way that is not obvious to the user (for example, if we present a list of records in a table, we will present the same records twice).\\nWe will present the data in a way that is not obvious to the user (for example, if we present a list of records in a table, we will present the same records twice).\\nWe will"}
{"Context":"## Context\\nWe were interested in pushing the project until we felt we needed redux to see how necessary it really was.\\n","Decision":"## Decision\nWe added redux to the project relatively early\\n","Prediction":"## Decision\nWe will use redux to handle state and navigation.\\nWe will use redux-thunk to handle actions and promises.\\nWe will use redux-saga to handle the asynchronous nature of the application.\\nWe will use redux-observable to handle the data.\\nWe will use redux-saga to handle the asynchronous nature of the application.\\nWe will use redux-observable to handle the data.\\nWe will use redux-saga to handle the asynchronous nature of the application.\\nWe will use redux-observable to handle the data.\\nWe will use redux-saga to handle the asynchronous nature of the application.\\nWe will use redux-observable to handle the data.\\nWe will use redux-saga to handle the asynchronous nature of the application.\\nWe will use redux-observable to handle the data.\\nWe will use redux-saga to handle the asynchronous nature of the application.\\nWe will use redux-observable to handle the data.\\nWe will use redux-saga to handle the asynchronous nature of the application.\\nWe will use redux-observable to handle the data.\\nWe will use redux-saga to handle the asynchronous nature of the application.\\nWe will use redux-observable to handle the data.\\nWe will use redux-s"}
{"Context":"## Context\\nThere are several databases within the system. One per module.\\n","Decision":"## Decision\nUnder no circumstance should a module read\/write from more than one database.\\n","Prediction":"## Decision\nUse PostgreSQL as the database for the system. The choice of PostgreSQL was based on\\n[the fact that it is the most popular database in the world](https:\/\/www.percona.com\/blog\/2020\/03\/10\/why-is-postgresql-so-popular).\\nIt also has a mature community, a mature release cycle, and a mature release of\\nthe database itself.\\nIt is also a mature, well-documented database.\\nIt has a mature tooling ecosystem, including a mature tooling ecosystem for\\nusing it as the database of record. This includes the ability to use\\n[PostgreSQL as the database of record](https:\/\/www.postgresql.org\/docs\/10\/privatesg.html#privatesg-postgresql)\\nfor storing the data. There is also a mature tooling ecosystem for using\\nPostgreSQL as the database of record, including [PostgreSQL\\nCluster Manager](https:\/\/www.postgresql.org\/docs\/13\/cluster-managers.html)\\nand [PostgreSQL Cluster Resource Manager](https:\/\/www.postgresql.org\/docs\/13\/cluster-managers.html#postgresql-cluster-resource-manager).\\nPostgreSQL is also a mature, well-documented database.\\nPostgreSQL can be used as the database of record"}
{"Context":"## Context\\nWe need some building blocks to model the domain in the software\\n","Decision":"## Decision\nWe will adopt tactical design patterns from DDD, that is we'll model the domain in terms of following objects:\\n- ValueObject - immutable object without identity\\n- Entity - object encapsulating some business entity with identity\\n- Aggregate - graph of Entities and ValueObjects enforcing consistency boundaries and business invariants\\n- AggregateRoot - entity acting as a root object in the Aggregate\\n- Command - immutable object representing the intention to trigger the behavior of the aggregate\\n- Event - immutable object representing the important fact that happened in the domain\\n- Repository - collection-like storage for aggregates\\n- DomainService - encapsulates the behavior that does not belong to any particular aggregate\\n- Saga - long-running business transaction\\n","Prediction":"## Decision\nWe will use the [Entity-Relationship model](https:\/\/en.wikipedia.org\/wiki\/Entity-relationship_model) to model the domain.\\nWe will use the [Oriented-Relational Database Model](https:\/\/en.wikipedia.org\/wiki\/Oriented-relational_database_model) to model the relationships between the entities in the domain.\\nWe will use the [Oriented-Graph Database Model](https:\/\/en.wikipedia.org\/wiki\/Oriented-Graph_database_model) to model the entities in the domain.\\nWe will use the [Bounded Context Model](https:\/\/en.wikipedia.org\/wiki\/Bounded_context_model) to model the entities in the domain.\\nWe will use the [Entity-Attribute-Value (EAV) Model](https:\/\/en.wikipedia.org\/wiki\/Entity-attribute-value_model) to model the domain.\\nWe will use the [Entity-Graph Database Model](https:\/\/en.wikipedia.org\/wiki\/Entity-graph_database_model) to model the domain.\\n### Building blocks\\nThe entities in our domain will be modelled as the entities of a relational database. For example, a `Person` entity in an `Employee` model would be the combination of an `Id` (primary key) and a `Name` (name"}
{"Context":"** Context\\nDWP performed an investigation into matching using their dataset. They propose a\\nmatching strategy which works well for their particular use case.\\nThe following is a much-simplified version of the proposed matching strategy.\\n#+BEGIN_SRC plantuml :exports results :file images\/flow.png\\nskinparam shadowing false\\nskinparam activity {\\nBorderColor black\\n}\\nstart\\n:Select all records matching any historical surname and date of birth;\\nif (One or more match?) then (yes)\\nif (Postcode present in Verify matching dataset?) then (yes)\\n:Filter only records matching any historical postcode;\\nif (One or more match?) then (yes)\\nelse (no)\\n#DF3034:No match;\\nstop\\nendif\\nelse (no)\\nendif\\n:Filter only records matching given name or middlename;\\nif (Exactly one match?) then (yes)\\n#00823B:Match;\\nstop\\nelseif (More than one match?) then (yes)\\nif (Cycle 3 attribute present?) then (yes)\\n:Filter only records matching Cycle 3 attribute;\\nif (Exactly one match?) then (yes)\\n#00823B:Match;\\nstop\\nelse (no)\\n#DF3034:No match;\\nstop\\nendif\\nelse (no)\\n#DF3034:No match;\\nstop\\nendif\\nelse (no)\\n#DF3034:No match;\\nstop\\nendif\\nelse (no)\\n#DF3034:No match;\\nstop\\nendif\\n#+END_SRC\\n#+RESULTS:\\n[[file:images\/flow.png]]\\n** Decision\\nWe will implement the above matching strategy.\\nFor simplicity, we will ignore any attributes where \"verified\" is set to \"false\".\\n** Consequences\\nThis strategy should be a good starting point for services in similar situations\\nto DWP (large, pre-existing dataset, high chance of new users already being\\npresent in the dataset).\\nThe strategy may work less well for services in other situations, for example\\nthose without a pre-existing database of users who plan on creating a new\\ndataset from Verify users.\\n","Decision":"## Decision\nWe will implement the above matching strategy.\\nFor simplicity, we will ignore any attributes where \"verified\" is set to \"false\".\\n** Consequences\\nThis strategy should be a good starting point for services in similar situations\\nto DWP (large, pre-existing dataset, high chance of new users already being\\npresent in the dataset).\\nThe strategy may work less well for services in other situations, for example\\nthose without a pre-existing database of users who plan on creating a new\\ndataset from Verify users.\\n","Prediction":"## Decision\nWe will implement the above matching strategy.\\nFor simplicity, we will ignore any attributes where \"verified\" is set to \"false\".\\n** Consequences\\nThis strategy should be a good starting point for services in similar situations\\nto DWP (large, pre-existing dataset, high chance of new users already being\\npresent in the dataset).\\nThe strategy may work less well for services in other situations, for example\\nthose without a pre-existing database of users who plan on creating a new\\ndataset from Verify users.\\nChosen option: \"Use a different strategy for each dataset\", because\\nwe have a limited number of datasets and the strategy can be easily adapted to\\neach dataset.\\n* [Image](https:\/\/i.imgur.com\/1hX6mHi.png)\\n* [Image](https:\/\/i.imgur.com\/7B2ZpT0.png)\\n* [Image](https:\/\/i.imgur.com\/OpiKZw4.png)\\n* [Image](https:\/\/i.imgur.com\/6ZGpM1B.png)\\n* [Image](https:\/\/i.imgur.com\/jd0e9q2.png)\\n* [Image](https:\/\/i.imgur.com\/2vqpz"}
{"Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"## Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","Prediction":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\nWe will use [Pivotal Tracker](https:\/\/www.pivotaltracker.com\/) for version control and issue tracking.\\nWe will use [GitHub](https:\/\/github.com\/) as a source control repository.\\nWe will use [Confluence](https:\/\/www.confluence.org\/) as a wiki.\\nWe will use [JIRA](https:\/\/www.atlassian.com\/software\/jira) for issue tracking.\\nWe will use [Ansible](https:\/\/www.ansible.com\/) for configuration management.\\nWe will use [OpenWhisk](https:\/\/openwhisk.com\/) for a microservices architecture dashboard.\\nWe will use [Azure DevOps](https:\/\/azure.microsoft.com\/en-us\/services\/devops\/)\\nWe will use [Docker](https:\/\/docs.docker.com\/) for container orchestration.\\nWe will use [Docker Compose](https:\/\/docs.docker.com\/compose\/) for infrastructure as code.\\nWe will use [Docker Compose](https:\/\/docs.docker.com\/compose\/) for application deployment.\\nWe will"}
{"Context":"## Context\\nsuperwerker creates an OpsItem in Systems Manager OpsCenter for each email received by the [RootMail](rootmail.md) feature. Without notifications for new OpsItems, users need to check the OpsCenter for new items manually and might miss important information regarding their AWS accounts and resources.\\n","Decision":"## Decision\n- Use CloudWatch Events to trigger an AWS Lambda function whenever a new OpsItem is created.\\n- OpsCenter \/ OpsItem supports SNS notifications, but the desired SNS topic Arn needs to be provided explicitly whenever an OpsItem is created. Therefore, we decided against this native feature.\\n- Publish a message to an SNS topic for every new OpsItem.\\n- Use SNS, since subscriptions and email verification work out-of-the-box with CloudFormation tooling.\\n- Use native email subscriptions for SNS to notify a specified email address about new messages.\\n- We decided against using SES for email notifications since this would lead to several additional steps like verifying sender and recipient domains or email addresses.\\n- We decided against (re-)using the existing root email address of the management AWS account since we would need to keep this in sync with the SNS subscription (because the management account root email adress can be changed). And we wanted to keep the notification feature simple.\\n- If no email address is provided, no SNS topic is created.\\n","Prediction":"## Decision\nWe will provide a [new feature](https:\/\/aws.amazon.com\/blogs\/compute\/superwerker-opscenter\/), that will notify the user when a new OpsItem is created for their AWS account and resources.\\nThis feature will be enabled by default for users with an OpsManager role. We will provide a configuration option to disable this feature for other users.\\nThis feature will be enabled by default for users with an OpsManager role.\\nThis feature will be enabled by default for users with an OpsManager role.\\nThis feature will be enabled by default for users with an OpsManager role.\\nThis feature will be enabled by default for users with an OpsManager role.\\nThis feature will be enabled by default for users with an OpsManager role.\\nThis feature will be enabled by default for users with an OpsManager role.\\nThis feature will be enabled by default for users with an OpsManager role.\\nThis feature will be enabled by default for users with an OpsManager role.\\nThis feature will be enabled by default for users with an OpsManager role.\\nThis feature will be enabled by default for users with an OpsManager role.\\nThis feature will be enabled by default for users with an OpsManager role.\\nThis"}
{"Context":"## Context\\nSmarthub project consist of multiple packages which are the part of Smarthub SDK. Complex dependency graph forces us to update dependent packages manually every time dependency has changed.\\n","Decision":"## Decision\nMigrate code base to monorepo structure and use `lerna` for versioning management.\\n","Prediction":"## Decision\nWe decided to use [Dependency Graph](https:\/\/github.com\/google\/dependency-graph) to keep dependency graph in sync with the dependency graph of Smarthub SDK.\\nDependency Graph is a tool which allows us to identify the dependencies between the packages and maintain the dependency graph in sync.\\nDependency Graph is a graph based tool which is supported by all the major version of the Depgraph library. Dependency Graph is also compatible with other dependency graph libraries like [Dependency Walker](https:\/\/github.com\/binarytree\/dependency-walker) and [Dependency Checker](https:\/\/github.com\/cyberciti\/dependency_checker).\\nDependency Graph is written in Go and it is used by other dependency graph libraries which are written in Go as well. Dependency Graph uses Go's concurrency to avoid race conditions.\\nDependency Graph is also compatible with other dependency graph libraries like [Dependency Checker](https:\/\/github.com\/cyberciti\/dependency_checker) which use Dependency Graph.\\nDependency Graph also provides the ability to check the dependency graph of a given package with a single command. This makes dependency graph checking easier and faster.\\nDependency Graph is also compatible with other dependency graph libraries like [Dependency Checker](https:\/\/github.com\/cyberciti\/dependency_checker) which"}
{"Context":"## Context\\nWe use architectural decision records (ADR) on this project.\\nThe project's code base is owned by the team, the team organizes itself (see Journey model) into smaller, short lived \"journey\" sub-units to - amongst others - increase focus. Architecture decisions affect the project for a long time, and will likely soon be faced by developers that were not part of the journey at the time. Consequently their feedback about the architectural decision is inevitable. Additionally, given the intentionally small size of an individual journey's group, the amount of opinions concerning any given ADR could be as small as one or two, should the ADR be voted upon by members of the same journey exclusively. To avoid a flood of ADRs trying to unwrite each other and to increase the standing of ADRs in general and the quality of the individual ADR they should be vetted (RFC) by the entire team.\\nTo avoid long-running feedback loops that block the individual journey team from fulfilling their commitments RFCs should be time-limited.\\n","Decision":"## Decision\nWe put proposed Architecture Decision Records up for feedback by the entire team. ADRs will be proposed as dedicated changes and iterated upon through the tools provided by our code review system. ADR RFCs will have a deadline; it should be no sooner than one working day after initial request.\\n","Prediction":"## Decision\nWe will use ADRs to document architectural decisions made by the team. The ADRs will be time-limited, and should be voted upon by the team. The ADRs will be time-limited and should be voted upon by the team. The ADRs will be time-limited and should be voted upon by the team.\\nTo avoid long-running feedback loops that block the individual journey team from fulfilling their commitments RFCs should be time-limited.\\nWe will use ADRs to document architectural decisions made by the team. The ADRs will be time-limited, and should be voted upon by the team. The ADRs will be time-limited and should be voted upon by the team.\\nTo avoid long-running feedback loops that block the individual journey team from fulfilling their commitments RFCs should be time-limited.\\nWe will use ADRs to document architectural decisions made by the team. The ADRs will be time-limited, and should be voted upon by the team. The ADRs will be time-limited and should be voted upon by the team.\\nTo avoid long-running feedback loops that block the individual journey team from fulfilling their commitments RFCs should be time-limited.\\nWe"}
{"Context":"## Context\\nSoftware-defined everything (SDx) is the definition of technical computing infrastructure entirely under the control of software with no operator or human intervention. It operates independent of any hardware-specific dependencies and is programmatically extensible.\\nIn the SDx approach, an application's infrastructure requirements are defined declaratively (both functional and non-functional requirements) such that sufficient and appropriate hardware can be automatically derived and provisioned to deliver those requirements.\\nThe benefits of SDx is that it lowers\/eliminates effort towards infrastructure maintenance, allows companies to move focus to other parts of the software, ensures consistence while also allowing for extensibility, remote deployment through configuration without downtime, and allows you to leverage the power of versioning such as git.\\n","Decision":"## Decision\nWhere possible software and infrastructure (or whatever) are deployed from source code. No human management of software and infrastructure is performed.\\n","Prediction":"## Decision\nWe will use SDx for the platform infrastructure.\\nWe will use Kubernetes for the cluster management.\\nWe will use Terraform for the infrastructure configuration.\\nWe will use Helm for the Kubernetes application management.\\nWe will use Helm charts for the application versioning.\\nWe will use Terraform for the application configuration.\\nWe will use Helm charts for the application versioning.\\nWe will use Helm charts for the application versioning.\\nWe will use Terraform for the application configuration.\\nWe will use Helm charts for the application versioning.\\nWe will use Helm charts for the application versioning.\\nWe will use Helm charts for the application versioning.\\nWe will use Helm charts for the application versioning.\\nWe will use Helm charts for the application versioning.\\nWe will use Helm charts for the application versioning.\\nWe will use Helm charts for the application versioning.\\nWe will use Helm charts for the application versioning.\\nWe will use Helm charts for the application versioning.\\nWe will use Helm charts for the application versioning.\\nWe will use Helm charts for the application versioning.\\nWe will use Helm charts for the application versioning.\\nWe will use Helm charts for the application"}
{"Context":"## Context\\nCurrently, we have a main class called [KonduitServingMain](https:\/\/github.com\/KonduitAI\/konduit-serving\/blob\/45af79d15abe4912ccd81e78c9d215306472036e\/konduit-serving-core\/src\/main\/java\/ai\/konduit\/serving\/configprovider\/KonduitServingMain.java) that is the entrypoint for a konduit-serving application to run. The main command line arguments are defined inside [KonduitServingNodeConfigurer](https:\/\/github.com\/KonduitAI\/konduit-serving\/blob\/e791741b80721980f8b66a35ed42f20b30612d5c\/konduit-serving-core\/src\/main\/java\/ai\/konduit\/serving\/configprovider\/KonduitServingNodeConfigurer.java) class. We also have an additional [Python CLI](https:\/\/github.com\/KonduitAI\/konduit-serving\/blob\/7965965b58217f2b4d983fd41aaea013264491ee\/python\/cli.py) that can be just implemented in Java. Vert.x Launcher supports the ability to start, stop and list running verticles out of the box.\\n","Decision":"## Decision\n- Extend `KonduitServingNodeConfigurer` from [Vert.x Launcher](https:\/\/vertx.io\/docs\/vertx-core\/java\/#_the_vert_x_launcher) class.\\n- Write all the initialization\/tear-down logic inside the lifecycle methods of Vert.x Launcher which is present inside both of the above classes (KonduitServingMain and KonduitServingNodeConfigurer).\\n- Extend the CLI and write separate classes for every command we want to add to the CLI (see the documentation [here](https:\/\/vertx.io\/docs\/vertx-core\/java\/#_extending_the_vert_x_launcher)).\\n- Depending on the final decided API, register or unregister commands in Vert.x Launcher.\\n### Example CLI:\\nThe following CLI will be the start for how this will be look:\\n#### Starting Verticles\\nEvery verticle will be named some sensible value instead of the full classpath so that the verticles can be started with a simple name. For example:\\nkonduit serve inference-server --config '{}' # With a JSON string\\nkonduit serve inference-server --config config.json # With a JSON file\\nkonduit serve inference-server --config config.json --name=mnist-prediction-server # With a server name. If no names are given then a random name\\n# is generated.\\nThe CLI can also have options for setting up deployment options like:\\nkonduit serve inference-server --config config.json --instances 3 # For running 3 instances of a Verticle. Usually it runs on a single port with\\n# round robin fashion requests transfer for load balancing\\n#### Stopping Verticles\\nkonduit stop inference --name=mnist-prediction-server # stops the server by the name of \"mnist-prediction-server\"\\n#### Listing Verticles\\nkonduit list # Lists all of the running verticle services with their names, host, port, configuration\\n#### Inspecting Verticles\\nPossible details will include:\\n- configuration\\n- host and port\\n- current resource usage\\n```bash\\nkonduit inspect # Give details of all the running verticles\\nkonduit inspect --name=mnist-prediction-server # Details of a specific verticle\\n```\\n#### Running Predictions\\n#### Predict with JSON (application\/json)\\nkonduit predict --name=mnist-prediction-server --input '{\"key\":\"value\"}' # With JSON string\\nkonduit predict --name=mnist-prediction-server --input input.json # With JSON file\\n#### Predict with Files (multipart\/form-data)\\nkonduit predict --name=mnist-prediction-server --input file.npy # Numpy\\nkonduit predict --name=mnist-prediction-server --input file.zip # DL4J\\nkonduit predict --name=mnist-prediction-server --input image.jspg # Image\\nkonduit predict --name=mnist-prediction-server --input file1.zip,file2.zip # DL4J with multiple inputs\\n","Prediction":"## Decision\nWe will use the [Vert.x](https:\/\/vertx.io\/) Python CLI to start, stop, and list running verticles.\\nWe will use the [Vert.x Launcher](https:\/\/github.com\/vertx\/vertx) Python CLI to start, stop, and list running verticles.\\nWe will use the [Vert.x Verticles](https:\/\/github.com\/vertx\/vertx\/tree\/master\/verticles) Verticles package to start, stop, and list running verticles.\\nWe will use the [Vert.x VerticleProviders](https:\/\/github.com\/vertx\/vertx\/tree\/master\/verticle-providers) Verticle Providers package to start, stop, and list running verticle providers.\\nWe will use the [Vert.x VerticlesProvider](https:\/\/github.com\/vertx\/vertx\/tree\/master\/verticles-provider) Verticle Providers package to start, stop, and list running verticle providers.\\nWe will use the [Vert.x VerticleService](https:\/\/github.com\/vertx\/vertx\/tree\/master\/verticle-service) Verticle Services package to start, stop, and list running verticle services.\\nWe will use the [Vert.x VerticleServiceProvider](https:\/\/"}
{"Context":"## Context\\nWe wanted to agree a structure for Kubernetes applications so that we could create\\nconsistent layouts when creating new applications.\\nThis structure should be iterated upon when we start to work more heavily with\\nmigrating apps over to Kubernetes.\\nWe need to also be aware that developers may want to create their own applications\\nand so must be easily understood.\\nWe want to split open and private repositories easily when we come to start publishing\\nour work.\\n","Decision":"## Decision\nWe have agreed on the following tree:\\n```\\n\u251c\u2500\u2500 data\\n\u2502   \u2514\u2500\u2500 my-awesome-app\\n\u2502   |   \u251c\u2500\u2500 development_secrets.yaml\\n|   |   \u2514\u2500\u2500 development_configmaps.yaml\\n|   \u2514\u2500\u2500 global\\n|       \u2514\u2500\u2500 development_configmaps.yaml\\n\u2514\u2500\u2500 manifests\\n\u2514\u2500\u2500 my-awesome-app\\n\u251c\u2500\u2500 deployment.yaml\\n\u2514\u2500\u2500 service.yaml\\n```\\nThis allows us to split the manifests section of Kubernetes configuration from the\\ndata values that get created and mounted into any deployments.\\n","Prediction":"## Decision\nWe will use the [kubernetes-logging](https:\/\/github.com\/kubernetes-logging\/kubernetes-logging) project to\\nto provide a readable log format for each application. This will allow us to create\\nand publish our own log format in the future.\\nWe will use the [kubernetes-logging-templates](https:\/\/github.com\/kubernetes-logging\/kubernetes-logging-templates) project to\\nto provide a template for each application.\\nWe will use the [kubernetes-logging-templates-external](https:\/\/github.com\/kubernetes-logging\/kubernetes-logging-templates-external) project to\\nto provide a template for each application that has a log output. This will allow us to provide\\na custom log output for each application.\\nWe will use the [kubernetes-logging-templates-external-internal](https:\/\/github.com\/kubernetes-logging\/kubernetes-logging-templates-external-internal) project to\\nto provide a template for each application that has a log output and a custom log output. This will allow\\nus to create custom log outputs for each application.\\nWe will use the [kubernetes-logging-templates-external-external](https:\/\/github.com\/kubernetes-logging\/kubernetes-"}
{"Context":"## Context\\nThe various components deployed via `cloud_controller_ng` interact with the\\nKubernetes API to perform CRUD operations on a variety of Kubernetes Custom\\nResource Definitions (CRDs) which are created in the context of a `cf-for-k8s`\\ndeployment such that the Kubernetes API becomes the source of truth for those CF\\ndomain objects (e.g. route resources for apps and build\/image resources for app\\nbuilds).\\nCurrently, we have individual clients which interact with a specific Kubernetes\\nAPI group which translates to a client which is scoped to only a particular\\ngrouping of Kubernetes resources. Currently there are two clients: one to\\ninteract with `kpack`-related resources and one to to interact with app\\nnetworking-related resources.\\nWe have found that there is an increasing amount of code duplication and shared\\nlogic\/concerns between these two clients and any other such clients we might\\nintroduce, so we would like to take note of some refactoring ideas to mitigate\\nsuch concerns.\\n","Decision":"## Decision\nThe existing clients currently take on two responsibilities which we'd like to\\nsplit out:\\n1. Translate an object from the CC database into a Kubernetes resource JSON\\nobject\\n1. Propagate Kubernetes resource JSON to the Kubernetes API and perform any\\nrequired error handling\\n#### Translate an object from the CC database\\nProposing creating some sort of translation layer which will take various CC\\ndomain objects from the CC database and construct their equivalent Kubernetes\\nCRs (e.g. CC route object => Route CR).\\nSeems like this would be difficult to achieve generically, so our first pass can\\nprobably just create a class for each object which needs to be translated.\\n#### Propagate the Kubernetes resource JSON\\nWe would like to propose creating a single, generic Kubernetes client which\\naccepts JSON form of any Kubernetes resource and simply propagates the desired\\nresource to the Kubernetes API. This single, generic client should also continue\\nto provide elegant, traceable error handling at least as well as we currently\\ndo in the existing `route_crd_client` and the `kpack_client`.\\nThis client should also be the single place where Kubernetes-related properties\\nwe expose are configured and validated.\\nWould be helpful for this generic client to also provide some sort of\\nvalidation, especially if we're providing it with raw JSON.\\nSomething to consider is also providing a generic Kubernetes resource template\\nwith common resource keys to extend from instead of providing the entire JSON\\ncontent for a particular resource. For example, all Kubernetes resource contain\\na top-level `metadata` key with various nested keys that are often defined\\nsuch as `name` or `labels` or `namespace`.\\n","Prediction":"## Decision\nWe will refactor the existing client code which interacts with the\\n`kpack` API group to use a separate client which is scoped to a\\nspecific `kpack` API group.\\nWe will also refactor the existing client code which interacts with\\n`kubernetes_api` to use a separate client which is scoped to a\\nspecific `kubernetes_api` API group.\\nWe will also refactor the existing client code which interacts with\\n`kubernetes` to use a separate client which is scoped to a\\nspecific `kubernetes` API group.\\nWe will also refactor the existing client code which interacts with\\n`kubernetes` to use a separate client which is scoped to a\\nspecific `kubernetes` API group.\\nWe will also refactor the existing client code which interacts with\\n`kubernetes` to use a separate client which is scoped to a\\nspecific `kubernetes` API group.\\nWe will also refactor the existing client code which interacts with\\n`kubernetes` to use a separate client which is scoped to a\\nspecific `kubernetes` API group.\\nWe will also refactor the existing client code which interacts with\\n`kubernetes` to use a separate client which is scoped to a\\nspecific `kubernetes` API"}
{"Context":"## Context\\nBaseApp's implementation of ABCI `{Check,Deliver}Tx()` and its own `Simulate()` method call the `runTx` method under the hood, which first runs antehandlers, then executes `Msg`s. However, the [transaction Tips](https:\/\/github.com\/cosmos\/cosmos-sdk\/issues\/9406) and [refunding unused gas](https:\/\/github.com\/cosmos\/cosmos-sdk\/issues\/2150) use cases require custom logic to be run after the `Msg`s execution. There is currently no way to achieve this.\\nAn naive solution would be to add post-`Msg` hooks to BaseApp. However, the Cosmos SDK team thinks in parallel about the bigger picture of making app wiring simpler ([#9181](https:\/\/github.com\/cosmos\/cosmos-sdk\/discussions\/9182)), which includes making BaseApp more lightweight and modular.\\n","Decision":"## Decision\nWe decide to transform Baseapp's implementation of ABCI `{Check,Deliver}Tx` and its own `Simulate` methods to use a middleware-based design.\\nThe two following interfaces are the base of the middleware design, and are defined in `types\/tx`:\\n```go\\ntype Handler interface {\\nCheckTx(ctx context.Context, req Request, checkReq RequestCheckTx) (Response, ResponseCheckTx, error)\\nDeliverTx(ctx context.Context, req Request) (Response, error)\\nSimulateTx(ctx context.Context, req Request (Response, error)\\n}\\ntype Middleware func(Handler) Handler\\n```\\nwhere we define the following arguments and return types:\\n```go\\ntype Request struct {\\nTx      sdk.Tx\\nTxBytes []byte\\n}\\ntype Response struct {\\nGasWanted uint64\\nGasUsed   uint64\\n\/\/ MsgResponses is an array containing each Msg service handler's response\\n\/\/ type, packed in an Any. This will get proto-serialized into the `Data` field\\n\/\/ in the ABCI Check\/DeliverTx responses.\\nMsgResponses []*codectypes.Any\\nLog          string\\nEvents       []abci.Event\\n}\\ntype RequestCheckTx struct {\\nType abci.CheckTxType\\n}\\ntype ResponseCheckTx struct {\\nPriority int64\\n}\\n```\\nPlease note that because CheckTx handles separate logic related to mempool priotization, its signature is different than DeliverTx and SimulateTx.\\nBaseApp holds a reference to a `tx.Handler`:\\n```go\\ntype BaseApp  struct {\\n\/\/ other fields\\ntxHandler tx.Handler\\n}\\n```\\nBaseapp's ABCI `{Check,Deliver}Tx()` and `Simulate()` methods simply call `app.txHandler.{Check,Deliver,Simulate}Tx()` with the relevant arguments. For example, for `DeliverTx`:\\n```go\\nfunc (app *BaseApp) DeliverTx(req abci.RequestDeliverTx) abci.ResponseDeliverTx {\\nvar abciRes abci.ResponseDeliverTx\\nctx := app.getContextForTx(runTxModeDeliver, req.Tx)\\nres, err := app.txHandler.DeliverTx(ctx, tx.Request{TxBytes: req.Tx})\\nif err != nil {\\nabciRes = sdkerrors.ResponseDeliverTx(err, uint64(res.GasUsed), uint64(res.GasWanted), app.trace)\\nreturn abciRes\\n}\\nabciRes, err = convertTxResponseToDeliverTx(res)\\nif err != nil {\\nreturn sdkerrors.ResponseDeliverTx(err, uint64(res.GasUsed), uint64(res.GasWanted), app.trace)\\n}\\nreturn abciRes\\n}\\n\/\/ convertTxResponseToDeliverTx converts a tx.Response into a abci.ResponseDeliverTx.\\nfunc convertTxResponseToDeliverTx(txRes tx.Response) (abci.ResponseDeliverTx, error) {\\ndata, err := makeABCIData(txRes)\\nif err != nil {\\nreturn abci.ResponseDeliverTx{}, nil\\n}\\nreturn abci.ResponseDeliverTx{\\nData:   data,\\nLog:    txRes.Log,\\nEvents: txRes.Events,\\n}, nil\\n}\\n\/\/ makeABCIData generates the Data field to be sent to ABCI Check\/DeliverTx.\\nfunc makeABCIData(txRes tx.Response) ([]byte, error) {\\nreturn proto.Marshal(&sdk.TxMsgData{MsgResponses: txRes.MsgResponses})\\n}\\n```\\nThe implementations are similar for `BaseApp.CheckTx` and `BaseApp.Simulate`.\\n`baseapp.txHandler`'s three methods' implementations can obviously be monolithic functions, but for modularity we propose a middleware composition design, where a middleware is simply a function that takes a `tx.Handler`, and returns another `tx.Handler` wrapped around the previous one.\\n### Implementing a Middleware\\nIn practice, middlewares are created by Go function that takes as arguments some parameters needed for the middleware, and returns a `tx.Middleware`.\\nFor example, for creating an arbitrary `MyMiddleware`, we can implement:\\n```go\\n\/\/ myTxHandler is the tx.Handler of this middleware. Note that it holds a\\n\/\/ reference to the next tx.Handler in the stack.\\ntype myTxHandler struct {\\n\/\/ next is the next tx.Handler in the middleware stack.\\nnext tx.Handler\\n\/\/ some other fields that are relevant to the middleware can be added here\\n}\\n\/\/ NewMyMiddleware returns a middleware that does this and that.\\nfunc NewMyMiddleware(arg1, arg2) tx.Middleware {\\nreturn func (txh tx.Handler) tx.Handler {\\nreturn myTxHandler{\\nnext: txh,\\n\/\/ optionally, set arg1, arg2... if they are needed in the middleware\\n}\\n}\\n}\\n\/\/ Assert myTxHandler is a tx.Handler.\\nvar _ tx.Handler = myTxHandler{}\\nfunc (h myTxHandler) CheckTx(ctx context.Context, req Request, checkReq RequestcheckTx) (Response, ResponseCheckTx, error) {\\n\/\/ CheckTx specific pre-processing logic\\n\/\/ run the next middleware\\nres, checkRes, err := txh.next.CheckTx(ctx, req, checkReq)\\n\/\/ CheckTx specific post-processing logic\\nreturn res, checkRes, err\\n}\\nfunc (h myTxHandler) DeliverTx(ctx context.Context, req Request) (Response, error) {\\n\/\/ DeliverTx specific pre-processing logic\\n\/\/ run the next middleware\\nres, err := txh.next.DeliverTx(ctx, tx, req)\\n\/\/ DeliverTx specific post-processing logic\\nreturn res, err\\n}\\nfunc (h myTxHandler) SimulateTx(ctx context.Context, req Request) (Response, error) {\\n\/\/ SimulateTx specific pre-processing logic\\n\/\/ run the next middleware\\nres, err := txh.next.SimulateTx(ctx, tx, req)\\n\/\/ SimulateTx specific post-processing logic\\nreturn res, err\\n}\\n```\\n### Composing Middlewares\\nWhile BaseApp simply holds a reference to a `tx.Handler`, this `tx.Handler` itself is defined using a middleware stack. The Cosmos SDK exposes a base (i.e. innermost) `tx.Handler` called `RunMsgsTxHandler`, which executes messages.\\nThen, the app developer can compose multiple middlewares on top on the base `tx.Handler`. Each middleware can run pre-and-post-processing logic around its next middleware, as described in the section above. Conceptually, as an example, given the middlewares `A`, `B`, and `C` and the base `tx.Handler` `H` the stack looks like:\\n```text\\nA.pre\\nB.pre\\nC.pre\\nH # The base tx.handler, for example `RunMsgsTxHandler`\\nC.post\\nB.post\\nA.post\\n```\\nWe define a `ComposeMiddlewares` function for composing middlewares. It takes the base handler as first argument, and middlewares in the \"outer to inner\" order. For the above stack, the final `tx.Handler` is:\\n```go\\ntxHandler := middleware.ComposeMiddlewares(H, A, B, C)\\n```\\nThe middleware is set in BaseApp via its `SetTxHandler` setter:\\n```go\\n\/\/ simapp\/app.go\\ntxHandler := middleware.ComposeMiddlewares(...)\\napp.SetTxHandler(txHandler)\\n```\\nThe app developer can define their own middlewares, or use the Cosmos SDK's pre-defined middlewares from `middleware.NewDefaultTxHandler()`.\\n### Middlewares Maintained by the Cosmos SDK\\nWhile the app developer can define and compose the middlewares of their choice, the Cosmos SDK provides a set of middlewares that caters for the ecosystem's most common use cases. These middlewares are:\\n| Middleware              | Description                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |\\n| ----------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\\n| RunMsgsTxHandler        | This is the base `tx.Handler`. It replaces the old baseapp's `runMsgs`, and executes a transaction's `Msg`s.                                                                                                                                                                                                                                                                                                                                                                             |\\n| TxDecoderMiddleware     | This middleware takes in transaction raw bytes, and decodes them into a `sdk.Tx`. It replaces the `baseapp.txDecoder` field, so that BaseApp stays as thin as possible. Since most middlewares read the contents of the `sdk.Tx`, the TxDecoderMiddleware should be run first in the middleware stack.                                                                                                                                                                                   |\\n| {Antehandlers}          | Each antehandler is converted to its own middleware. These middlewares perform signature verification, fee deductions and other validations on the incoming transaction.                                                                                                                                                                                                                                                                                                                 |\\n| IndexEventsTxMiddleware | This is a simple middleware that chooses which events to index in Tendermint. Replaces `baseapp.indexEvents` (which unfortunately still exists in baseapp too, because it's used to index Begin\/EndBlock events)                                                                                                                                                                                                                                                                         |\\n| RecoveryTxMiddleware    | This index recovers from panics. It replaces baseapp.runTx's panic recovery described in [ADR-022](.\/adr-022-custom-panic-handling.md).                                                                                                                                                                                                                                                                                                                                                  |\\n| GasTxMiddleware         | This replaces the [`Setup`](https:\/\/github.com\/cosmos\/cosmos-sdk\/blob\/v0.43.0\/x\/auth\/ante\/setup.go) Antehandler. It sets a GasMeter on sdk.Context. Note that before, GasMeter was set on sdk.Context inside the antehandlers, and there was some mess around the fact that antehandlers had their own panic recovery system so that the GasMeter could be read by baseapp's recovery system. Now, this mess is all removed: one middleware sets GasMeter, another one handles recovery. |\\n### Similarities and Differences between Antehandlers and Middlewares\\nThe middleware-based design builds upon the existing antehandlers design described in [ADR-010](.\/adr-010-modular-antehandler.md). Even though the final decision of ADR-010 was to go with the \"Simple Decorators\" approach, the middleware design is actually very similar to the other [Decorator Pattern](.\/adr-010-modular-antehandler.md#decorator-pattern) proposal, also used in [weave](https:\/\/github.com\/iov-one\/weave).\\n#### Similarities with Antehandlers\\n* Designed as chaining\/composing small modular pieces.\\n* Allow code reuse for `{Check,Deliver}Tx` and for `Simulate`.\\n* Set up in `app.go`, and easily customizable by app developers.\\n* Order is important.\\n#### Differences with Antehandlers\\n* The Antehandlers are run before `Msg` execution, whereas middlewares can run before and after.\\n* The middleware approach uses separate methods for `{Check,Deliver,Simulate}Tx`, whereas the antehandlers pass a `simulate bool` flag and uses the `sdkCtx.Is{Check,Recheck}Tx()` flags to determine in which transaction mode we are.\\n* The middleware design lets each middleware hold a reference to the next middleware, whereas the antehandlers pass a `next` argument in the `AnteHandle` method.\\n* The middleware design use Go's standard `context.Context`, whereas the antehandlers use `sdk.Context`.\\n","Prediction":"## Decision\nWe will add a new `BaseApp` API to the Cosmos SDK to provide a way to hook into the behavior of `Msg` calls without modifying the BaseApp implementation. We will also add a new `BaseApp` implementation to the `CosmosBuilder` to allow for custom BaseApp implementations.\\nThis change will require the following changes to the BaseApp implementation:\\n* `BaseApp`::`BaseApp::BaseApp`\\n* `BaseApp::BaseApp::BaseApp::runTx`\\n* `BaseApp::BaseApp::BaseApp::runTransaction`\\nThis change will require the following changes to the BaseApp::BaseApp::BaseApp::BaseApp::BaseApp::BaseApp::BaseApp::BaseApp implementation:\\n* `BaseApp::BaseApp::BaseApp::BaseApp::runTx`\\n* `BaseApp::BaseApp::BaseApp::runTransaction`\\n* `BaseApp::BaseApp::BaseApp::BaseApp::runTx`\\n* `BaseApp::BaseApp::BaseApp::runTx`\\n* `BaseApp::BaseApp::BaseApp::runTx`\\n* `BaseApp::BaseApp::BaseApp::runTx`\\n* `"}
{"Context":"## Context\\nAs of version 0.35.0 , our implementation of `DbKvs.getTimestampsByCell` was creating a MultiMap with all pairs of\\n(Cell, timestamp) from the range determined by the row batch size. In case of wide rows, or simply a row batch size\\nthat is too large, this could cause us to run out of memory; see [issue #982](https:\/\/github.com\/palantir\/atlasdb\/issues\/982).\\n","Decision":"## Decision\nWe decided to use a block (a triple of row name, column name, and timestamp) batch size, defaulting to 1000000 to avoid\\nloading too many blocks into memory. The algorithm will fetch blocks by row, column, and increasing timestamp until the\\nbatch size is met. In that case:\\n1. If at least one row was fully processed, we return RowResults with all timestamps each of those rows, and continue\\nprocessing from the beginning of current row when more RowResults are necessary.\\n2. If no rows were fully processed, we continue processing blocks until the end of the current cell (row, column),\\nfetching all timestamps for that cell. We return a (partial) RowResult that contains all timestamps for that row\\nuntil and including the final cell processed. To create the next RowResult we then continue processing the row after\\nthe last cell processed, effectively splitting the wide row into several RowResults.\\nExample:\\nAssuming a batch size of 10 and the following table\\n|   |         1          |          2         |     3     |\\n|   | ------------------ | ------------------ | --------- |\\n| 1 |     (1, 2, 3)      |      (4, 5, 6)     | (7, 8, 9) |\\n| 2 |    (1, 2, 3, 4)    |    (4, 5, 6, 7)    | (7, 8, 9) |\\n| 3 | (1, 2, 3, 4, 5, 6) | (4, 5, 6, 7, 8, 9) | (7, 8, 9) |\\n| 4 |     (1, 2, 3)      |                    |           |\\nThe RowResults will be as follows:\\n- (1 -> (1 -> (1, 2, 3); 2 -> (4, 5, 6); 3 -> (7, 8, 9)))\\n- (2 -> (1 -> (1, 2, 3, 4); 2 -> (4, 5, 6, 7); 3 -> (7, 8, 9)))\\n- (3 -> (1 -> (1, 2, 3, 4, 5, 6); 2 -> (4, 5, 6, 7, 8, 9)))\\n- (3 -> (3 -> (7, 8, 9)))\\n- (4 -> (1 -> (1, 2, 3)))\\nOther options considered:\\n1. Partial row batching: fetch up to 1000000 blocks. If the block batch size is hit, stop immediately (cells can be split across multiple batches).\\n2. Full row batching: fetch up to 1000000 blocks. If the block batch size is hit in the middle of a row, continue fetching until the row is exhausted.\\n3. No batching, but throw an error when the block batch hint is reached\\n- Option 1 was considered, but was replaced by the modified version above. This is because sweep must ensure that all blocks\\nexcept for the most recent (before the immutable timestamp) are deleted. This can be achieved by repeating the last\\nblock from one batch in the next batch, or by keeping this last result in sweep's memory, so that it knows to remove it,\\nif further blocks for the same cell are encountered. Neither option is compelling: the first is hard to reason about,\\nand the second reduces the scope for parallelisation, and risks introducing a correctness bug.\\n- Option 2 was not chosen, because it does not guard well against wide rows (many cells) that have many overwrites.\\n- Option 3 was considered, but was ultimately discarded because it relies on properties of the code that calls getCellTimestamps.\\nIn particular, there needs to be retry logic that detects that an error was thrown, and reduces the batch size accordingly.\\n","Prediction":"## Decision\nWe decided to return a `MultiMap` of the same type as `TimestampsByCell` to eliminate the need for creating\\na `MultiMap` with all the `TimestampsByCell` instances from the range determined by the row batch size.\\nThis means that we can increase the row batch size without fear of running out of memory.\\nWe will continue to create a `MultiMap` with all the `TimestampsByCell` instances from the range determined by\\nthe row batch size, but we will only return it when we need to return a `TimestampsByCell` instance.\\nWe will continue to return a `MultiMap` with a single `TimestampsByCell` instance.\\nWe will continue to use the `TimestampsByCell` type.\\nWe will continue to use the `TimestampsByCell` type as a base type for all MultiMap types.\\nIn order to make this change as painless as possible, we will continue to use the `TimestampsByCell` type as a\\nbase type for all `MultiMap` types.\\n### 0.35.0\\n* [Deprecation](https:\/\/github.com\/palantir\/atlasdb\/issues\/982)\\"}
{"Context":"## Context\\n`wbeditentity` summary message for Wikibase Items and Properties provides\\nvery little information on what have been changed on the entity, with the main\\nmessage in the summary being \"Item Changed\".\\nIn [T220696], we want to update those summary\\nmessages generated for Wikibase Items and Properties when using `wbeditentity`\\nto be more detailed and informative.\\nWikibase Lexeme achieved generating detailed summary messages by:\\n1. making its  implementations update the summary object passed to\\nthe change op instance with granular information about the change.\\n2. implementing [SummaryAggregator] class and using it internally in some\\n[ChangeOp] implementations (some of those that are non-leaf nodes and\\ncontain further change ops) in order to combine those summary messages\\ngenerated by leaf nodes (the effective change ops in the tree).\\n","Decision":"## Decision\nWe decided to go with the second option, the visitor pattern design.\\nIn order to achieve that, we will make [ChangeOp::apply()] return\\n[ChangeOpResult] that is defined by the following IDL:\\n```php\\n\/\/ encapsulates the result of applying a change op to an entity document\\ninterface ChangeOpResult {\\n\/\/ the id of the entity document that the change op was applied to\\nEntityId getEntityId();\\n\/\/ whether the entity document was actually changed in any way\\n\/\/ as a result of applying the change op to it\\nbool isEntityChanged();\\n}\\n```\\n### Next steps\\n1. Change [ChangeOp::apply()] to return [ChangeOpResult]\\n2. Provide needed implementations of [ChangeOpResult] that can capture the result\\nof applying the different [ChangeOp] implementations.\\nExample:\\n[ChangeOpLabel] will probably return something like [ChangeOpLabelResult] that\\nis defined as:\\n```php\\ninterface ChangeOpLabelResult : ChangeOpResult {\\nstring getOldLabel();\\nstring getNewLabel();\\nstring getLanguageCode();\\n}\\n```\\n3. Update implementations of [ChangeOp] in Wikibase and [WikibaseLexeme] to conform\\nwith the changes to the interface.\\n","Prediction":"## Decision\n- Implement Wikibase Lexeme's SummaryAggregator class and add it to the Wikibase\\nLexeme core.\\n- Implement Wikibase Lexeme's ChangeOp implementations and add them to the\\nWikibase Lexeme core.\\n- Update the Wikibase Lexeme core to use the new SummaryAggregator\\nimplementation.\\n- Update the Wikibase Lexeme core to use the new ChangeOp\\nimplementation.\\n- Update the Wikibase Lexeme core to call the new SummaryAggregator\\nimplementation in `ChangeOp` implementations.\\n- Update the Wikibase Lexeme core to call the new ChangeOp\\nimplementation in `WikibaseLexemeItem` implementations.\\n- Update the Wikibase Lexeme core to use the new SummaryAggregator\\nimplementation in `WikibaseLexemeItem` implementations.\\n- Update the Wikibase Lexeme core to use the new ChangeOp\\nimplementation in `WikibaseLexemeItem` implementations.\\n- Update the Wikibase Lexeme core to use the new SummaryAggregator\\nimplementation in `WikibaseLexemeItem` implementations.\\n- Update the Wikibase Lexeme core to use the new ChangeOp\\nimplementation in `WikibaseLexemeItem` implementations.\\n- Update the Wikibase Lexeme"}
{"Context":"## Context\\n* We have several different roles for using a Holochain app (through a *container*):\\n* local, QML based UI components\\n* as a special case of the above: administration UI\\n* externally located (living in a browser) web UIs connected through some form of IPC\\n* *bridged* apps, i.e. Holochain app as the user of another Holochain app\\n* Services hosting multiple Holochain apps and exposing their zome functions to UIs selectively\\n* With HoloSqape and the Holochain-nodejs, we already have two container implementations\\nA Holochain app cannot be used directly. Holochain is built as a library that exposes an interface for\\ncreating Holochain instances and calling their zome functions (see [container_api](\/container_api\/src\/lib.rs)).\\nThis *container_api* requires its client to provide a context, which holds the representation of the agent\\n(name and keys), a logger and persistence and will also include a networking proxy in the future.\\nWhile it is possible to use this library directly in a monolithic app where the UI is tightly\\ncoupled to the app and everything linked to the Holochain library into the same executable, we regard this\\nas a special case, since this does not allow for app composability (which we think is a crucial\\nconcept for the Holochain app ecosystem).\\nInstead, in the context of the Rust iteration of Holochain, we have been following the notion of\\nproviding a *Container* as a relevant concept (and implemention) for the deployment of Holochain apps:\\nwe provide a Container (i.e. [HoloSqape](https:\/\/github.com\/holochain\/holosqape)) for each supported platform (Linux, MacOS, Windows, iOS, Android)\\nthat gets installed on a host machine. Holochain apps get installed into the container.\\nThe Container:\\n* manages installed hApps (Holochain Apps),\\n* manages agent identities (keys) ,\\n* should also enable hApps to call functions of other hApps - what we call *bridging*,\\n* has to implement access permissions to installed hApps.\\nSo far, the interface our Container implementation provides was growing organically\\nin [container.h](https:\/\/github.com\/holochain\/holosqape\/blob\/master\/bindings\/container.h).\\nWith upcoming alternative container implementations (based on [Holochain-nodejs](https:\/\/github.com\/holochain\/holochain-nodejs)\\nor a custom one for HoloPorts) we should drive the process of building this Container API\\nconsciously and with more coherence and visibility amongst our separate dev teams.\\nWe need a protocol for communication with a Holochain container and a specification of what upcoming\\ncontainers have to implement, so that apps and UIs can be build against a standardised interface.\\n","Decision":"## Decision\nWe establish the **Container as a fundamental module of the Holochain architecture\/stack**\\nby specifying its **API**, that can be assumed by UI components, Holochain apps (i.e. zome\\nfunctions in the case of bridging)\\nand remote processes alike to be implemented by the context a Holochain app is executed in.\\nFundamental to this API is **user\/client roles and permissions**.\\nClients will be able to use different subsets of the Container's API depending on their specific permissions.\\nSo an implicit aspect of this API is that every potential API call happens in the context of a known\\nclient identified through a client ID that the API manages and returns to the admin client as handles.\\nWe will specify this API in a separate *specification document*.\\nThe following subsection provide examples for how this *could* look like:\\n### Example API\\nEvery client (of the Container API, i.e. QML root widgets, admin UI in the case of HoloSqape,\\nand network services built on top of the Container)\\nwill have these functions available, though functions can return with a permission denied\\nerror in case an app or capability was given that the current client is not allowed to use.\\n* `call(app_hash, function_name, parameters) -> {results}`\\n* `connect(app_hash, capability_name, signal_name, callback_function)`\\n* `installed_apps() -> [hash]`\\n* `capabilities_of(app_hash) -> [String]`\\n* `functions_of(app_hash, capability_name) -> [String]`\\n* `request_access(app_hash, capability)`\\n* `can_access?(app_hash, capability) -> bool`\\n### Admin\\nOnly the admin user can manage apps and permissions:\\n* `install_app(Dna) -> hash`\\n* `uninstall_app(hash)`\\n* `start_app(hash)`\\n* `stop_app(hash)`\\n* `promote_to_admin(client_id)`\\n* `retract_admin(client_id)`\\n* `commit_capability_grant(client_id, app_hash, capability)`\\n* `deny_capability(client_id, app_hash, capability)`\\n### Extensible\\nMore API capabilities might be added in the future.\\n","Prediction":"## Decision\nWe will use the [container_api](https:\/\/github.com\/holochain\/holosqape\/blob\/master\/bindings\/container_api.md)\\nas a protocol for communication with Holochain containers and HoloPorts.\\nThis will be a Rust library that will be available to Holochain apps and UIs.\\nIt will provide a standardised API to interact with Holochain containers\\nand allow Holochain apps to be built against a common interface.\\nWe will use the [container_api](https:\/\/github.com\/holochain\/holosqape\/blob\/master\/bindings\/container_api.md)\\nas a protocol for communication with Holochain containers and HoloPorts.\\nThis will be a Rust library that will be available to Holochain apps and UIs.\\nIt will provide a standardised API to interact with Holochain containers\\nand allow Holochain apps to be built against a common interface.\\nWe will use the [container_api](https:\/\/github.com\/holochain\/holosqape\/blob\/master\/bindings\/container_api.md)\\nas a protocol for communication with Holochain containers and HoloPorts.\\nThis will be a Rust library that will be available to Holochain apps"}
{"Context":"## Context\\nThe choice of which programming language to use is governed by a host of different factors. Most languages can be made to work for whatever task is required, though some are better at certain tasks than others.\\nWe expect the nature of the work in this project to benefit from concurrency, so choosing a language with good support for this is important. Since we will be deploying to AWS Lambda (See [5. Use AWS Lambda](0005-use-aws-lambda.md)), we are further limited to using one of the supported languages. Other considerations include ease of packaging and distribution, excellent data streaming abilities, and a healthy ecosystem of 3rd party libraries.\\n","Decision":"## Decision\nUse Go for the core application language.\\n","Prediction":"## Decision\nWe will use Python 3 for the project code.\\nWe will use Go for the API code.\\nWe will use JavaScript for the front end code.\\nWe will use Java for the database code.\\nWe will use Go for the logging and monitoring code.\\nWe will use Python 3 for the AWS Lambda code.\\nWe will use Ruby for the tests.\\nWe will use Go for the CI\/CD pipeline.\\nWe will use Perl for the CI\/CD pipeline.\\nWe will use Python 3 for the AWS Lambda code.\\nWe will use PHP for the CI\/CD pipeline.\\nWe will use Python 3 for the AWS Lambda code.\\nWe will use Python 3 for the CI\/CD pipeline.\\nWe will use Perl for the CI\/CD pipeline.\\nWe will use Python 3 for the CI\/CD pipeline.\\nWe will use Ruby for the CI\/CD pipeline.\\nWe will use Python 3 for the CI\/CD pipeline.\\nWe will use Python 3 for the CI\/CD pipeline.\\nWe will use Ruby for the CI\/CD pipeline.\\nWe will use Ruby for the CI\/CD pipeline.\\nWe will use Ruby for the CI\/CD pipeline"}
{"Context":"## Context\\nOur component prop APIs have, at times been a bit of a mess. We've seen:\\n- Implicit conventions not documented anywhere but consistently reflected in our code (e.g., the type of the `sx` prop)\\n- Explicit plans to change some of those (e.g., the deprecation of Styled System props)\\n- Inconsistencies in our implementation (e.g., when components accept a `ref` prop)\\nThis ADR aims to unify some of these conversations about prop APIs, codify our decisions, and sequence the work to get there.\\n","Decision":"## Decision\n### \ud83d\udfe2 `sx`\\nAll components that ultimately render to the DOM should accept an `sx` prop.\\nThe `sx` prop (of type `SystemStyleObject`) should generally set styles for the root HTML element rendered by the component. An exception would be components like `<Dialog>`, whose outermost HTML element is a backdrop. In that case, it would be appropriate for `sx` styles to apply to child of the backdrop that is more likely to need styling overrides.\\n### \ud83d\udfe2 `ref`\\nAll components that ultimately render to the DOM should accept a `ref` prop. That `ref` prop should most often be passed to the root HTMLElement rendered by the component, although occasionally a different descendent node may make more sense.\\nSee also: [Discussion on `ref` props (internal)](https:\/\/github.com\/github\/primer\/discussions\/131)\\n### \ud83d\udfe1 `as`\\nOnly components with a clear need for polymorphism should accept an `as` prop. Reasonable cases include:\\n- Components that need functionality from the component passed to the `as` prop, like a `<Button>` that renders a React Router link.\\n- Components whose accessibility are improved by using semantically appropriate HTML elements, like an ActionList\\nWhen a Primer component user passes an `as` prop to a component, it should be done in a way that is consistent with the component\u2019s intended use. In some situations we can enforce that with a narrowed type for our `as` prop.\\nSee also: [Discussion on `as` props (internal)](https:\/\/github.com\/github\/primer\/discussions\/130)\\n### \ud83d\udfe1 DOM props: Limited\\nAll components that accept an `as` prop should accept props en masse for the element specified by the `as` prop (excluding props of the same name already used by the component). _Additionally_, some other elements that do _not_ accept an `as` prop should accept the props for their root HTML element when those props are fundamental to the component\u2019s function (e.g., `<TextInput>` should accept DOM props for its underlying `<input>`).\\n### \ud83d\udd34 Styled System props\\nComponents should not accept Styled System props (except our utility components: `Box` and `Text`)\\n_Reasoning:_ Utility components are meant to provide a convenient API for writing styles (including styles that reference theme and other context managed within Primer). Non-utility components implement specific design patterns where additional styling is available for exceptional cases.\\nSee also: [Discussion on the deprecation of styled-system props (internal)](https:\/\/github.com\/github\/primer\/discussions\/132)\\n### \ud83d\udd34 `theme`\\nComponents should not accept a `theme` prop (with the exception of `ThemeProvider`).\\n_Reasoning:_ The `theme` prop doesn't enable anything that can't be done with `<ThemeProvider>`, and promotes the anti-pattern of per-component theme overrides.\\n### `children`\\nI'm intentionally withholding advocacy about `children` prop types because I expect that topic will be covered by a future ADR.\\n### Sequencing\\n1. Deprecate remaining unwanted Styled System props (should be done? Let's verify.)\\n1. Release an eslint rule to disallow Styled System props\\n1. Release an eslint rule to disallow `theme`\\n1. Migrate all usage within PRC\\n1. Assist GitHub projects with migration\\n1. Remove support for unwanted props\\n1. Update docs to reflect the standards in this ADR\\n","Prediction":"## Decision\nWe will use the [Prop API Specification](https:\/\/github.com\/prop-registry\/prop-api-spec) as the official specification for prop APIs.\\nWe will document the decision-making process and use the [Prop API Specification](https:\/\/github.com\/prop-registry\/prop-api-spec) as the official specification for prop APIs.\\nWe will use the [Prop API Specification](https:\/\/github.com\/prop-registry\/prop-api-spec) as the official specification for prop APIs.\\nWe will use the [Prop API Specification](https:\/\/github.com\/prop-registry\/prop-api-spec) as the official specification for prop APIs.\\nWe will use the [Prop API Specification](https:\/\/github.com\/prop-registry\/prop-api-spec) as the official specification for prop APIs.\\nWe will use the [Prop API Specification](https:\/\/github.com\/prop-registry\/prop-api-spec) as the official specification for prop APIs.\\nWe will use the [Prop API Specification](https:\/\/github.com\/prop-registry\/prop-api-spec) as the official specification for prop APIs.\\nWe will use the [Prop API Specification](https:\/\/github.com\/prop-registry\/prop-api-spec) as the official specification for prop APIs.\\"}
{"Context":"## Context\\n`adr-cli` needs to store the markdown files somewhere.\\n[Michael Nygard's article](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions) proposes `doc\/arch\/adr-NNN.md` as directory and filename pattern.\\n[adr-tools](https:\/\/github.com\/npryce\/adr-tools) uses `docs\/adr\/NNNN-title-with-dashes.md` as pattern.\\n","Decision":"## Decision\nUse `docs\/adr\/NNNN-title-with-dashes.md` as pattern to be\\n1) consistent with adr-tools and\\n2) enable rendering in GitHub pages, because GitHub pages can be rendered from the `docs\/` sub folder, but not from `doc` subfolder.\\n","Prediction":"## Decision\n`adr-cli` will use `docs\/adr\/NNNN-title-with-dashes.md` as pattern.\\n`adr-cli` will use `docs\/adr\/NNNN-title-with-dashes.md` as pattern.\\n`adr-cli` will use `docs\/adr\/NNNN-title-with-dashes.md` as pattern.\\n`adr-cli` will use `docs\/adr\/NNNN-title-with-dashes.md` as pattern.\\n`adr-cli` will use `docs\/adr\/NNNN-title-with-dashes.md` as pattern.\\n`adr-cli` will use `docs\/adr\/NNNN-title-with-dashes.md` as pattern.\\n`adr-cli` will use `docs\/adr\/NNNN-title-with-dashes.md` as pattern.\\n`adr-cli` will use `docs\/adr\/NNNN-title-with-dashes.md` as pattern.\\n`adr-cli` will use `docs\/adr\/NNNN-title-with-dashes.md` as pattern.\\n`adr-cli` will use `docs\/adr\/NNNN-title-with-dashes.md` as pattern.\\n`adr-cli` will use `docs\/adr\/NNNN-title-with-dashes.md` as pattern.\\n"}
{"Context":"## Context\\nThe ViewModel is responsible for the data, so i tried an attempt to notify the ViewModel from within the view that the\\nuser clicked the \"edit\" menu item. The ViewModel exposed a LiveData object which was observed by the view to trigger the\\nnavigation to a edit fragment.\\nAfter editing the workout and clicked saved, the edit fragment was opened again instantly. This was caused by the\\nViewModel which triggered the edit event again.\\n","Decision":"## Decision\nThe view (DetailviewWorkoutFragment) no longer notifies the ViewModel about the edit action but directly calls the\\nnavigation component to open the EditWorkoutFragment, and therefore decides what should be done after the edit action\\nwas clicked (normally the view should not make this kind of decision).\\n","Prediction":"## Decision\nA LiveData is created for the ViewModel and the view. The LiveData is then used to notify the view. This means that\\nthe view can be updated from the ViewModel without waiting for the ViewModel to be notified of the change.\\nThis also means that the view can be updated from another ViewModel. This is useful when the view is a child of another\\nViewModel.\\n### Example\\nThe view is the workout. The view is a child of the workout activity. The workout activity has a viewModel as a child. The\\nworkout activity is the ViewModel. The workout activity has a LiveData which is used to notify the view. The workout activity\\nhas a button to edit the workout. The view has a button to edit the workout. The view is the workout activity. The view\\nis the workout activity. The view has a viewModel as a child. The workout activity has a LiveData which is used to notify the\\nworkout activity. The workout activity has a button to edit the workout. The view has a button to edit the workout. The view\\nhas a viewModel as a child. The workout activity has a LiveData which is used to notify the view. The workout activity has a button to\\nedit the workout. The view has a button to edit"}
{"Context":"## Context\\nThere is a mechanism to ignore discovered config flows using unique ids\\nattached to config entries but not all integrations may set an unique id\\ndue to lack of truly unique identifiers or just not having been updated for\\nconcerns of backward compatibility.\\nThis creates a UX issue that has started to become more pronounced with more config flows\\nbeing added every release (which is awesome!). The real issue is that not all discovery processes\\nare created equal and that becomes an issue for consistency.\\n","Decision":"## Decision\nIntegrations that are discoverable must provide a unique id (via `async_set_unique_id`)\\nto allow the user to ignore the discovered config entry.\\nThese rules apply to all new integrations. Existing integrations should be reviewed\\nfor possible ways to improve identification during discovery.\\n","Prediction":"## Decision\nWe will be using the [sticky id](https:\/\/github.com\/fluxdata\/fluxdata-oss\/blob\/master\/docs\/fluxdata\/fluxdata-oss\/fluxdata-oss-sticky-id.md) to\\nidentify config flows and will be using the .id suffix to uniquely identify them.\\nThis will allow for discovery to be done in a consistent manner.\\nThis will also allow for a future change to be made that will be backwards compatible.\\nThis will also allow for the id to be reused in the future.\\nThis will also allow for the ids to be removed in the future.\\n* Sticky ids are generated on a per-configuration basis and will not be used\\nto uniquely identify config flows. They will be removed in the future.\\n* The id is generated by concatenating the namespace and the flow name.\\n* The id is generated by concatenating the namespace and the flow name, with a period\\nin front of the namespace.\\n* The id is generated by concatenating the namespace and the flow name, with a period\\nin front of the namespace, and a dash in the middle.\\n* If the namespace and the flow name are the same, the id will be the concatenation of the two"}
{"Context":"## Context\\nWe used to have a monorepo with multiple packages, including `guui` and `design`. However, once these packages were migrated to `src-foundation` we were left with a monorepo with only one package.\\n","Decision":"## Decision\nRemove the use of Yarn Workspaces which was being used for the monorepo. Restore a basic yarn package, merging dependencies.\\n","Prediction":"## Decision\nWe will remove the monorepo and use a single package for the foundation. This will give us a much simpler build, test, and deployment process. It will also allow us to use a single package to provide a consistent UI for the front end.\\n* We will remove `guui` and `design` packages.\\n* We will create a single `package.json` file for the foundation.\\n* We will create a single `package-lock.json` file for the foundation.\\n* We will create a single `package.json` file for the front end.\\n* We will create a single `package-lock.json` file for the front end.\\n* We will create a single `package.json` file for a single package.\\n* We will create a single `package.json` file for a single package.\\n* We will create a single `package.json` file for a single package.\\n* We will create a single `package.json` file for a single package.\\n* We will create a single `package.json` file for a single package.\\n* We will create a single `package.json` file for a single package.\\n* We will create a single `package.json` file for a single package.\\"}
{"Context":"## Context\\nUntil recently, frontend dependencies in Wikibase were managed through globally registered [ResourceLoader] modules with\\nno distinction between internal dependencies and modules that needed to be publicly available. This resulted in more\\nthan 250 [ResourceLoader] modules with a very complex dependency relationship. The names of all modules are loaded with\\nevery page view, affecting page load time and network overhead. See [T228513]\\nWe considered two options to reduce the number of globally registered ResourceLoader modules:\\nwebpack and [ResourceLoader] [Package Modules].\\nSolving the problem with webpack would have involved the introduction of a build step to create bundles for each of the\\ntop-level (=entry point) modules. Advantages of this approach are superior minification at build time - better than the\\nResourceLoader minification, and all the other benefits of introducing a build step such as transpilation from more modern ES versions,\\nand easier reuse of 3rd party npm libraries. The risks we identified for this approach include;\\n- Having to generate and check in the bundled js files in addition to their source. That would lead to most patches having a merge conflict.\\n- Having to rewrite tests for internal files that can no longer run within the QUnit ResourceLoader test suite.\\n- Webpack is not yet widely adopted within the MediaWiki ecosystem.\\n- Putting in too much work into (legacy) parts of the code that are not going to change any time soon.\\nFor [Package Modules], we agreed that we would be missing out many of the positive side effects of introducing webpack and\\nthe build step. On the other hand, this solution would require fewer code changes, and would be the recommended MediaWiki way to solve the problem at hand.\\n","Decision":"## Decision\nWe decided to go with [Package Modules] instead of webpack and reduced the number of modules that Wikibase registers from 260 to 85.\\nThe reason we went with Package Modules was:\\n- It's easier to migrate such a huge codebase to [Package Modules].\\n- The tests are easier to adapt given the current paradigm of running QUnit tests in RL and browser context.\\n- If we change our mind later and switch to webpack, it would be still doable and easier than now given that both understand require.\\n- The rebase hell that would come with checking the dist files in VCS.\\nWe still went with webpack in the submodules that work independently of Wikibase (like WikibaseSerializationJavaScript and\\nWikibaseDataModelJavaScript) to run the tests in the context of karma + webpack + node but at the same time ResourceLoader understands\\nthose notations when the code is being pulled as a RL module in the Wikibase extension (as a submodule).\\n","Prediction":"## Decision\nWe will not use webpack for the MediaWiki modules. We will instead use [ResourceLoader] [Package Modules] to bundle our modules. We will use\\n[Package Modules] for the MediaWiki core and external modules only. We will use [ResourceLoader] [Resource Bundles] for any\\nother modules that need to be publicly available.\\nWe have decided to change the MediaWiki version of this issue from T228513 to T228514. This is because the configuration of\\n[Package Modules] will not affect MediaWiki's build configuration. However, we should still consider the\\npossibility of a merge conflict in future, and consider how we can alleviate that risk.\\nTo do this, we will need to:\\n- Change the MediaWiki version of this issue to T228514, and include a [Severity] of `major` in the subject.\\n- Include a [Summary] of the issue in the [ChangeLog]. This summary should include the following information:\\n- the MediaWiki version(s) affected by the issue, as seen in the `mediawiki_version()` function\\n- an introduction to the issue, including a link to the issue itself\\n- a description of the issue, including the problem and the"}
{"Context":"## Context\\nThe majority of client side developers at the Guardian have some [React](https:\/\/reactjs.org\/) experience, and it is succesfully used in several projects. Its [JSX syntax](https:\/\/reactjs.org\/docs\/introducing-jsx.html) is expressive and readable.\\nThe [Preact](https:\/\/preactjs.com\/) library has a similar API to React, and is an order of magnitude smaller in terms of file size (3.35KB vs 29.07KB Gzipped). It excludes a lot of React features that we will likely never use.\\nHowever, React has a larger community, and is driven by a dedicated team at Facebook. It comes with synthetic events that offer better cross browser support. It is easier to configure, as tools such as Babel, Webpack and Flow assume that JSX implies React. It is easier to [profile using Chrome dev tools](https:\/\/reactjs.org\/blog\/2016\/11\/16\/react-v15.4.0.html#profiling-components-with-chrome-timeline). It also has better integration with the [Enzyme](http:\/\/airbnb.io\/enzyme\/) test utilities.\\nThe React team is working on a number of new features (async rendering, time slicing, suspense). It is [unlikely](https:\/\/github.com\/developit\/preact-compat\/issues\/432) that Preact will be able to keep up with the pace of React's development.\\n","Decision":"## Decision\nWe will build the dotcom rendering layer in React.\\n","Prediction":"## Decision\nWe will use React as our client side rendering library, and Preact as a library to provide a consistent API for rendering HTML. We will use React Hooks where appropriate.\\nWe will use React Router where appropriate.\\nWe will use [Enzyme](http:\/\/airbnb.io\/enzyme\/) to profile React components.\\nWe will use [Babel](https:\/\/babel.pypa.io\/en-US\/) to transpile ES6 to ES5.\\nWe will use [Webpack](https:\/\/webpack.js.org\/) to reduce the size of our client side bundles.\\nWe will use [ESLint](https:\/\/eslint.org\/) to enforce configuration standards.\\nWe will use [Enzyme testing](https:\/\/github.com\/facebook\/react\/blob\/master\/docs\/enzyme-testing.md) to isolate bugs in our components and their dependencies.\\nWe will use [Enzyme testing with Jest](https:\/\/github.com\/facebook\/react\/blob\/master\/docs\/enzyme-testing-with-jest.md) to test our components and their dependencies.\\nWe will use [Enzyme testing with Mocha](https:\/\/github.com\/facebook\/react\/blob\/master\/docs\/enzyme-testing-with-mocha.md) to test our components and their dependencies.\\nWe will use [En"}
{"Context":"## Context\\nWhen building pipelines using concourse, we investigated using the [pool\\nresource](https:\/\/github.com\/concourse\/pool-resource) in order to control flow\\nthrough jobs. This was an alternative to the use of the\\n[semver resource](https:\/\/github.com\/concourse\/semver-resource).\\nThese 2 resources are both workarounds to solve the problem of triggering jobs\\nwhen we haven't made changes to a resource.\\nThe problem is that the pool resource relies on write access to a github repo,\\nwhich means we must pass public keys that allow this access into the pipeline\\nand deployed concourse instance - we want to minimise the number of credentials\\nwe pass, and the semver resource relies on AWS credentials that are already\\npassed.\\n","Decision":"## Decision\nWe will not use the pool resource for flow between jobs - instead we will use\\nthe semver resource\\n","Prediction":"## Decision\nWe will use the pool resource to control the flow of jobs through the pipeline\\nand will use the semver resource to trigger them.\\nWe will also deploy an additional node to the cluster (which will be the\\ncontrolling node) and set up a custom IAM role to allow access to the\\npool resource.\\nThis will allow us to use the pool resource in a more granular manner, and\\nlimit the number of credentials that are passed to the pipeline.\\nWe will also use the pool resource to trigger the pipeline itself. The\\npool resource will be used to trigger the pipeline when a new\\nconfiguration file is added to the project, rather than when the job is\\nstarted.\\nThis will allow us to avoid the need to trigger the pipeline from the\\ncontrolling node.\\nWe will also use the pool resource to trigger the pipeline when a new\\nfile is added to the project, rather than when the job is triggered.\\nThis will allow us to avoid the need to trigger the pipeline from the\\ncontrolling node.\\nWe will also use the pool resource to trigger the pipeline when a new\\nfile is added to the project, rather than when the job is triggered.\\nThis will allow us to avoid the need to"}
{"Context":"## Context\\nWe want to be confident about the code we write, and for it to be\\nself-documenting as much as possible.\\n[TypeScript](https:\/\/www.typescriptlang.org\/) is a compiled language with\\noptional typing. It's a superset of JavaScript, so is familiar to developers who\\nknow JavaScript. It has wide editor support.\\nAs of writing, TypeScript is used by over\\n[1.4 million repositories](https:\/\/github.com\/microsoft\/TypeScript\/network\/dependents?package_id=UGFja2FnZS01MTE3ODUxNjg%3D)\\non GitHub.\\n","Decision":"## Decision\nWe will use TypeScript.\\n","Prediction":"## Decision\nWe will use TypeScript for code development.\\nThis will be the primary language used for all code development.\\nWe will use TypeScript as a superset of JavaScript where possible.\\nWe will use TypeScript as the primary language for writing tests.\\nWe will use TypeScript for the majority of the code that we write.\\nWe will use TypeScript as the primary language for writing documentation.\\nWe will use TypeScript as a superset of JavaScript where possible.\\nWe will use TypeScript as the primary language for writing code.\\nWe will use TypeScript as the primary language for writing tests.\\nWe will use TypeScript for the majority of the code that we write.\\nWe will use TypeScript as a superset of JavaScript where possible.\\nWe will use TypeScript as the primary language for writing documentation.\\nWe will use TypeScript as a superset of JavaScript where possible.\\nWe will use TypeScript as the primary language for writing code.\\nWe will use TypeScript as the primary language for writing tests.\\nWe will use TypeScript for the majority of the code that we write.\\nWe will use TypeScript as a superset of JavaScript where possible.\\nWe will use TypeScript as the primary language for writing code.\\nWe will use TypeScript as the primary language for writing tests.\\"}
{"Context":"### Context\\nThe strategy accepted in [ADR-001](https:\/\/github.com\/slackwing\/feathers\/blob\/master\/adr\/001-tracking-external-files.md) is difficult to recall, therefore difficult to be confide in. An alternative is proposed.\\n### Approaches\\n#### 1. Document logical changes\\nThe main kind of external files we want to track are configuration files, such as .zshrc and .vimrc. For these, it would be better to document logical changes, _e.g._ as steps in [tooling](https:\/\/github.com\/slackwing\/feathers\/tree\/master\/tooling), for at least 3 advantages: (1) logical changes are easier to comprehend than a raw configuration; (2) steps are pickable; and (3) layering configurations in steps is more robust, especially across different systems, than dropping them all in at once.\\n#### 2. Track snapshots\\nHowever, documenting logical changes comes with the overhead of having to prepare and finalize configurations for commiting. This in turn carries the risk of piling up uncommitted configurations. We can balance things by committing snapshots of raw configurations, _e.g._ in [tooling\/snapshots](https:\/\/github.com\/slackwing\/feathers\/tree\/master\/tooling\/snapshots).\\n#### 3. Other external files\\nCurrently there are no other kinds of external files we want to track. Disregarding.\\n### Decision\\nAdopting #1 and #2.\\n### Accepted Tradeoffs\\nNone known.\\n### Retrospective\\n_TBD_\\n","Decision":"## Decision\nAdopting #1 and #2.\\n### Accepted Tradeoffs\\nNone known.\\n### Retrospective\\n_TBD_\\n","Prediction":"## Decision\nAdopting #1 and #2.\\n### Accepted Tradeoffs\\nNone known.\\n### Retrospective\\n_TBD_\\n#### Decision\nAdopting #1 and #2.\\n### Accepted Tradeoffs\\nNone known.\\n### Retrospective\\n_TBD_\\n#### Decision\\nAdopting #1 and #2.\\n### Accepted Tradeoffs\\nNone known.\\n### Retrospective\\n_TBD_\\n#### Decision\\nAdopting #1 and #2.\\n### Accepted Tradeoffs\\nNone known.\\n### Retrospective\\n_TBD_\\n#### Decision\\nAdopting #1 and #2.\\n### Accepted Tradeoffs\\nNone known.\\n### Retrospective\\n_TBD_\\n#### Decision\\nAdopting #1 and #2.\\n### Accepted Tradeoffs\\nNone known.\\n### Retrospective\\n_TBD_\\n#### Decision\\nAdopting #1 and #2.\\n### Accepted Tradeoffs\\nNone known.\\n### Retrospective\\n_TBD_\\n#### Decision\\nAdopting #1 and #2.\\n### Accepted Tradeoffs\\nNone known.\\n### Retrospective\\n_TBD_\\n#### Decision\\nAdopting #1"}
{"Context":"## Context and Problem Statement\\nWe want to realize the following user story: As a user, I want to connect two components of a distributed app. Both components write\/listen to different topics and\\nuse a different message format.\\nTo realize this, a message router and a message transformer ([EAI Pattern](https:\/\/www.enterpriseintegrationpatterns.com\/)) is needed as depicted in the following sketch.\\n![First scenario](images\/FirstScenario.png)\\nThe transformation and the routing logic is provided via functions which are hosted on a FaaS solution. This functions are user specific and could be provided via a function store. The transformer doesn't need to know how to transform\/split\/enrich\/filter the messages. It simply reads the message from a specific topic, hands it over to the function and sends the (transformed) result of the function to an output topic. Likewise, the router does not need to know how to route the messages (content or context based, e.g. if `itemPrice>50000` than route to high-value department). It reads the message from a topic, hands it over to a function and sends the message to a function determined topic. Both components have in common that they only read messages from a topic, hand the message to a function and act upon the result. We could draw the image above like this:\\n![generic component sketch](images\/genericComponent.png)\\n","Decision":"## Decision\nChosen option: Kafka-faas-connector, because we don't want to mix messaging and business logic in the functions and need to have dynamic topics which are not supported by the OpenFaaS Kafka connector at the moment.\\n","Prediction":"## Decision\nChosen option: \"message router and transformer\", because this approach allows to define generic components which can be used in the distributed app. The components can be provided via a function store and the transformer and router are defined in the component. This approach allows to easily change the routing logic without changing the components. The components can be provided by a cloud function, which can be used by the distributed app. The components can be provided via a function store, which can be used by the distributed app. The components can be provided by a cloud function, which can be used by the distributed app.\\n### Positive Consequences\\n* The components are defined by a function and can be provided with a function store. This allows us to provide generic components which can be used in the distributed app.\\n* The components can be provided by a cloud function, which can be used by the distributed app. This allows us to provide generic components which can be used in the distributed app.\\n* The components can be provided by a function store. This allows us to provide generic components which can be used in the distributed app.\\n* The components can be provided by a function store. This allows us to provide generic components which can be used in the distributed app.\\n### Negative"}
{"Context":"## Context\\nWe want to be able to run feature and end to end tests in browsers and as part\\nof continuous integration. We only want to write tests once, and have them run\\nin all environments.\\n[Selenium WebDriver](https:\/\/docs.seleniumhq.org\/projects\/webdriver\/\/) is a\\nbrowser automation framework. It supports all major browsers and is supported by\\ncloud based browser testing services like\\n[BrowserStack](https:\/\/www.browserstack.com\/).\\n","Decision":"## Decision\nUse Selenium WebDriver to write feature and end to end tests via Jest.\\n","Prediction":"## Decision\nWe will use Selenium WebDriver to run tests.\\nSelenium WebDriver was chosen because it is the most widely used browser automation\\nframework for test automation. It is also the framework that has the most\\nsupport from cloud based browser testing services and is supported by\\nBrowserStack.\\nSelenium WebDriver provides a high level of abstraction over the underlying\\nbrowser and has been designed to work with a range of different browsers\\nand operating systems. This allows us to write tests that run in a wide\\nrange of environments without having to worry about the underlying\\nbrowser.\\nSelenium WebDriver is a popular framework for writing test automation and is\\nsupported by a number of organisations, including Microsoft, Google, and\\n[BrowserStack](https:\/\/www.browserstack.com).\\nSelenium WebDriver is an open source framework that is maintained by the\\nSelenium Project. Selenium WebDriver is used by many other well known\\ncompanies, including Google, Microsoft, and Amazon.\\nSelenium WebDriver is used by BrowserStack to run tests in browsers\\noutside of the browser it is written for.\\nSelenium WebDriver has a high level of abstraction over the underlying\\nbrowser. This means that we can write tests that run in the browser, but\\nalso test the browser behaviour.\\n"}
{"Context":"## Context\\nA [finalizer](https:\/\/kubernetes.io\/docs\/tasks\/access-kubernetes-api\/custom-resources\/custom-resource-definitions\/#finalizers) allows you to write asynchronous pre-delete logic, such as deleting associated resources. Here's an [example](https:\/\/book.kubebuilder.io\/reference\/using-finalizers.html) of finalizers can be used with Kubebuilder.\\nWithout a finalizer, we could rely on cascading deletion of child objects when Routes are deleted and `RequeueAfter` to rebuild the virtual services and services  from its routes every `ResyncInterval` seconds. `ResyncInterval` is currently set to 30 seconds.\\nHowever, this doesn\u2019t meet our SLO to handle changes within 10 seconds. [Cascading deletes don\u2019t work within 10 seconds](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/af67408c172630d59996207a2f3587ea88c96572\/test\/integration\/garbagecollector\/garbage_collector_test.go#L385-L392), so meeting the SLO would require `ResyncInterval` to be less than 10 seconds, which seems unreasonable.\\nCascading deletion alone also doesn\u2019t handle the case of a virtual service being owned by >1 Route. This means a cascading delete cannot update the virtual service\u2019s contents to not include the paths related to that deleted route. So we would need to rely on `RequeueAfter` on a different route with the same FQDN for those updates, which would be slow, and a strange behavior to support.\\n","Decision":"## Decision\nIn order to handle all of the cases: deleting services, deleting virtual services owned by only that route and updating virtual services owned by many routes, we rely on a finalizer, so we can have a \u201cfast path\u201d to all of these cases.\\nFinalizers do a \u201csoft delete\u201d to keep the route in the K8s API while handling deletion\/updates to the route\u2019s child objects.\\nUsing finalizers allows us to implement all of the cases in our route deletion logic. This helps us meet our SLO of having 95% of route changes being reflected within 10 seconds.`RequeueAfter` serves as a \u201csync\u201d to handle disaster recovery scenarios when unexpected operations outside of normal controller reconciliation happen (i.e child resources are deleted in etcd).\\n","Prediction":"## Decision\nWe will use a finalizer with a `RequeueAfter` to handle updating the virtual services. We will use the `Route` interface to provide the `RequeueAfter` to the `finalizer`.\\n* `finalizer` will delete the virtual service when the `Route` is deleted.\\n* `finalizer` will update the virtual service if the `Route` is deleted and a new route is added to the virtual service. In this case, the `finalizer` will delete the virtual service and update the virtual service with the new `Route`.\\n* `finalizer` will update the virtual service if the `Route` is deleted and the virtual service is removed from the virtual cluster. In this case, the `finalizer` will delete the virtual service and update the virtual service with the removal of the `Route`.\\n* `finalizer` will update the virtual service if the virtual service is removed from the virtual cluster. In this case, the `finalizer` will delete the virtual service and update the virtual service with the removal of the `Route`.\\n* `finalizer` will update the virtual service if the virtual service is removed from the virtual cluster and the virtual service is removed from the"}
{"Context":"## Context\\nThis ADR documents the implementation of the `v0.1` [relayer lib crate]\\n[ibc-relayer].\\nThis library is instantiated in the [Hermes][hermes] binary of the\\n[ibc-relayer-cli crate][ibc-relayer-cli] (which is not the focus of this discussion).\\nAs a main design goal, `v0.1` is meant to lay a foundation upon which we can\\nadd more features and enhancements incrementally with later relayer versions.\\nThis is to say that `v0.1` may be deficient in terms of features or\\nrobustness, and rather aims to be simple, adaptable, and extensible.\\nFor this reason, we primarily discuss aspects of concurrency and architecture.\\n### Relayer versioning scheme\\nOn the mid-term, the relayer architecture is set out to evolve across three\\nversions.\\nThe first of these, `v0.1`, makes several simplifying assumptions\\nabout the environment of the relayer and its features. These assumptions\\nare important towards limiting the scope that `v0.1` aims to\\ncover, and allowing a focus on the architecture and concurrency model to\\nprovide for growth in the future.\\nThese assumptions are documented below in the [decision](#decision) section.\\n","Decision":"## Decision\n### Configuration\\nFor the most part, the relayer configuration will be\\nstatic: the configuration for chains and their respective objects (clients,\\nconnections, or channels) will be fully specified in the relayer\\nconfiguration file and will not change throughout execution.\\nLight clients are also statically defined in the config file, and cannot be\\nswitched dynamically at runtime.\\nRecent changes to the ICS protocol specifies identifier\\nselection for clients, connections, and channels to be [deterministic][ids].\\nFor this reason, we will not need to specify any identifiers in the\\nconfiguration file.\\nWe only specify which pairs of chains should communicate with one\\nanother, and the port identifier to use for that purpose.\\nThis pair of chains plus their corresponding port identifiers is called a\\n__relaying path__.\\nAny relaying path is unidirectional.\\nAn example with the relevant section of the configuration file follows.\\n```toml\\n[[connections]]\\na_chain = 'ibc-0'\\nb_chain = 'ibc-1'\\n[[connections.paths]]\\na_port = 'transfer'\\nb_port = 'transfer'\\n```\\nHere there are two chains, ith one connection between them, and a path for\\nrelaying on the port called `transfer` on both chains, from chain `ibc-0`\\nto `ibc-1`.\\n### Links\\nA [link][link] is a relayer-level protocol that implements packet relay across\\none relaying path.\\nThe relayer at `v0.1` will focus on a single link.\\nThis limitation will be lifted in subsequent versions.\\n### Chain State\\nEach chain is assumed to start with an empty IBC state.\\nThis means that the relayer will take care of creating the client,\\nconnection, and channel objects respectively on each side of a link.\\n### Proof Verification\\nThe `v0.1` relayer will _not_ do proof verification.\\n### Feature set\\nThe [complete list of features is documented elsewhere][features] in detail.\\n","Prediction":"## Decision\nWe will use the `relayer` crate to implement `v0.1`.\\nWe will use the `relayer` crate to implement `v0.1`.\\nWe will use the `relayer` crate to implement `v0.1`.\\nWe will use the `relayer` crate to implement `v0.1`.\\nWe will use the `relayer` crate to implement `v0.1`.\\nWe will use the `relayer` crate to implement `v0.1`.\\nWe will use the `relayer` crate to implement `v0.1`.\\nWe will use the `relayer` crate to implement `v0.1`.\\nWe will use the `relayer` crate to implement `v0.1`.\\nWe will use the `relayer` crate to implement `v0.1`.\\nWe will use the `relayer` crate to implement `v0.1`.\\nWe will use the `relayer` crate to implement `v0.1`.\\nWe will use the `relayer` crate to implement `v0.1`.\\nWe will use the `rel"}
{"Context":"**Context**: operations that change state in the server may complete with an\\nindeterminate status code. For example: an error in the 500 range may be\\nproduced by a middle-tier server after the operation was started by the backend.\\nRe-issuing the request may not work as expected for the application developer.\\n**Decision**: by default the library will retry all requests, including\\nnon-idempotent ones. The library will allow application developers to override\\nthe definition of what operations are automatically retried.\\n**Consequences**: most operations become easier to use for application\\ndevelopers. In very rare cases the operation will result in double uploads, or\\nin a new generation of the object or metadata being created. In even more\\nrare cases the operation may fail, for example, an operation to create an object\\nwith `IfGenerationMatch(0)` would fail on the second attempt.\\n","Decision":"## Decision\nnon-idempotent ones. The library will allow application developers to override\\nthe definition of what operations are automatically retried.\\n**Consequences**: most operations become easier to use for application\\ndevelopers. In very rare cases the operation will result in double uploads, or\\nin a new generation of the object or metadata being created. In even more\\nrare cases the operation may fail, for example, an operation to create an object\\nwith `IfGenerationMatch(0)` would fail on the second attempt.\\n","Prediction":"## Decision\nThe library will automatically retry all operations that are not idempotent.\\nThis includes requests that are not idempotent and operations that are not\\nnon-idempotent. This means that if an operation fails, the application\\ndeveloper can still retry it using the retry function. For example, if an\\noperation fails, the application developer can retry it using the retry\\nfunction with a retry rate of 1.2 seconds.\\nIn order to prevent the request from being retried twice, the retry function\\nwill always return a `RetryStatus` with an `HttpStatusCode` of 400 or 500.\\nIf the request is retried with a `RetryStatus` of 400 or 500, the operation will\\nalso return an `HttpStatusCode` of 400 or 500 to the application developer.\\nIf the operation is retried with a `RetryStatus` of 400 or 500, the operation\\nwill also return an `HttpStatusCode` of 400 or 500 to the application developer.\\nIf the operation fails, the application developer will still be able to retry\\nit using the retry function. For example, if an operation fails, the application\\ndeveloper can retry it using the retry function with a retry"}
{"Context":"## Context\\nRunning a service like cljdoc requires infrastructure of various kinds. For\\nthe initial prototype I generated a static site stack using [a library I\\ncreated](https:\/\/github.com\/confetti-clj\/confetti) which uses AWS\\nCloudFormation under the hood. This has been useful while prototyping but\\nwith the addition of [a server component](\/doc\/adr\/0009-introduce-server-component.md)\\nand potentially other infrastructure components in the future it becomes\\nharder to stick to Cloudformation.\\n[Terraform](https:\/\/www.terraform.io\/) seems to be a popular tool in this space\\nand provides all the functionality of my library as well as support for using\\nmultiple cloud providers.\\n","Decision":"## Decision\nRebuild the static site stack using Terraform. Once that has been transitioned,\\nextend the Terraform configuration to include a server that we can use to host\\nthe server component of cljdoc.\\n","Prediction":"## Decision\nI will use Terraform to manage infrastructure of the cljdoc service.\\nThis includes the creation of a new stack and provision of all the services\\nthat are necessary to run the service. It also includes the creation of\\na new stack which will be used to manage the cljdoc service.\\nTerraform will be used in conjunction with AWS CloudFormation to provision\\nthe stack.\\nThis decision is made with the understanding that the cljdoc service\\nwill be moved to a managed service in the future and will be deployed\\nusing CloudFormation.\\n### Decision\nI will use Terraform to manage infrastructure of the cljdoc service.\\nThis includes the creation of a new stack and provision of all the services\\nthat are necessary to run the service. It also includes the creation of\\na new stack which will be used to manage the cljdoc service.\\nTerraform will be used in conjunction with AWS CloudFormation to provision\\nthe stack.\\nThis decision is made with the understanding that the cljdoc service\\nwill be moved to a managed service in the future and will be deployed\\nusing CloudFormation.\\n### References\\n[1]: https:\/\/medium.com\/terraform-blog\/terraform-architecture-"}
{"Context":"## Context and Problem Statement\\nWe would like to generally flatten the directory structure in FDS as well as improving the\\nimport statements for client applications. This will be a breaking change for a major\\nversion, as clients will need to update imports once we make the change.\\n## Decision Drivers\\n- Remove unnecessary hierarchy in components dir\\n- Simplify documentation\\n- Adopt the unofficial CBI standard of organizing by component directories (each component\\nhas its own directory).\\n- Improve ergonomics and performance of import statements for client applications\\n","Decision":"## Decision\n- Remove unnecessary hierarchy in components dir\\n- Simplify documentation\\n- Adopt the unofficial CBI standard of organizing by component directories (each component\\nhas its own directory).\\n- Improve ergonomics and performance of import statements for client applications\\n### Import statements\\nBecause individual imports for each component has a negative performance impact, we decided\\nto use destructured imports and rely on a tree shaking strategy:\\n```diff\\n-import Flex from '@cbinsights\/fds\/lib\/components\/layout\/Flex';\\n-import FlexItem from '@cbinsights\/fds\/lib\/components\/layout\/FlexItem';\\n-import TextInput from '@cbinsights\/fds\/lib\/components\/forms\/TextInput';\\n+import { Flex, FlexItem, TextInput } from '@cbinsights\/fds\/lib\/components';\\n```\\nThe \"root\" file for each import corresponds to an FDS code module.\\n| code module   | Import example                                                                        |\\n| ------------- | ------------------------------------------------------------------------------------- |\\n| `dictionary`  | `import { COLOR_TIMBERWOLF, COLOR_PURPLE } from '@cbinsights\/fds\/lib\/dictionary';`    |\\n| `components`  | `import { Avatar, TextInput } from '@cbinsights\/fds\/lib\/components\/';`                |\\n| `icons`       | `import { CloudIcon, BellIcon } from '@cbinsights\/fds\/lib\/icons';`                    |\\n| `assets`      | Use path `@cbinsights\/fds\/lib\/assets\/<file>` where applicable                         |\\n| `base-styles` | not applicable; we only publish this CSS as part of the main styles file in `assets\/` |\\n### `components` file structure\\nCategorization directories will be removed.\\nEach component will have its own directory under `components\/`.\\nTests, stories, private components, and helpers will be colocated in component\\ndirectories.\\n```\\ncomponents\/\\n\u251c\u2500\u2500 Foo\/\\n\u2502   \u251c\u2500\u2500 FooHelper.jsx\\n\u2502   \u251c\u2500\u2500 foo.test.jsx\\n\u2502   \u251c\u2500\u2500 foo.stories.jsx\\n\u2502   \u251c\u2500\u2500 utils.js\\n\u2502   \u2514\u2500\u2500 index.js  \/\/ contains Foo component code\\n...\\n```\\n### Documentation\\nThe \"Components\" section in Storybook will no longer have categorization. All components\\nwill be documented flat under the section heading.\\n","Prediction":"## Decision\n- Remove unnecessary hierarchy in components dir\\n- Simplify documentation\\n- Adopt the unofficial CBI standard of organizing by component directories (each component\\nhas its own directory).\\n- Improve ergonomics and performance of import statements for client applications\\nChosen option: \"Separate components by subdirectories\", because\\n- It is the most straightforward and easy to understand\\n- It is the best option for improving the documentation\\n- It is the option that will be most backwards compatible with older versions\\n- It will provide the best performance and\\n- It is the option that will most effectively enable us to improve the performance of\\nimport statements for client applications.\\n### Positive Consequences\\n- All existing import statements will be changed to use component names\\n- All existing test cases will be changed to use component names\\n- All existing documentation will be changed to use component names\\n- Existing CBI structure will be preserved\\n- Existing FDS structure will be preserved\\n### Negative Consequences\\n- Existing import statements will not be changed to use component names\\n- Existing test cases will not be changed to use component names\\n- Existing FDS structure will be preserved\\n- Existing CBI structure will be preserved\\n- Existing FDS import statements will not be changed"}
{"Context":"## Context\\nIn order to support building highly secure, robust and interoperable blockchain\\napplications, it is vital for the Cosmos SDK to expose a mechanism in which arbitrary\\nevidence can be submitted, evaluated and verified resulting in some agreed upon\\npenalty for any misbehavior committed by a validator, such as equivocation (double-voting),\\nsigning when unbonded, signing an incorrect state transition (in the future), etc.\\nFurthermore, such a mechanism is paramount for any\\n[IBC](https:\/\/github.com\/cosmos\/ibc) or\\ncross-chain validation protocol implementation in order to support the ability\\nfor any misbehavior to be relayed back from a collateralized chain to a primary\\nchain so that the equivocating validator(s) can be slashed.\\n","Decision":"## Decision\nWe will implement an evidence module in the Cosmos SDK supporting the following\\nfunctionality:\\n* Provide developers with the abstractions and interfaces necessary to define\\ncustom evidence messages, message handlers, and methods to slash and penalize\\naccordingly for misbehavior.\\n* Support the ability to route evidence messages to handlers in any module to\\ndetermine the validity of submitted misbehavior.\\n* Support the ability, through governance, to modify slashing penalties of any\\nevidence type.\\n* Querier implementation to support querying params, evidence types, params, and\\nall submitted valid misbehavior.\\n### Types\\nFirst, we define the `Evidence` interface type. The `x\/evidence` module may implement\\nits own types that can be used by many chains (e.g. `CounterFactualEvidence`).\\nIn addition, other modules may implement their own `Evidence` types in a similar\\nmanner in which governance is extensible. It is important to note any concrete\\ntype implementing the `Evidence` interface may include arbitrary fields such as\\nan infraction time. We want the `Evidence` type to remain as flexible as possible.\\nWhen submitting evidence to the `x\/evidence` module, the concrete type must provide\\nthe validator's consensus address, which should be known by the `x\/slashing`\\nmodule (assuming the infraction is valid), the height at which the infraction\\noccurred and the validator's power at same height in which the infraction occurred.\\n```go\\ntype Evidence interface {\\nRoute() string\\nType() string\\nString() string\\nHash() HexBytes\\nValidateBasic() error\\n\/\/ The consensus address of the malicious validator at time of infraction\\nGetConsensusAddress() ConsAddress\\n\/\/ Height at which the infraction occurred\\nGetHeight() int64\\n\/\/ The total power of the malicious validator at time of infraction\\nGetValidatorPower() int64\\n\/\/ The total validator set power at time of infraction\\nGetTotalPower() int64\\n}\\n```\\n### Routing & Handling\\nEach `Evidence` type must map to a specific unique route and be registered with\\nthe `x\/evidence` module. It accomplishes this through the `Router` implementation.\\n```go\\ntype Router interface {\\nAddRoute(r string, h Handler) Router\\nHasRoute(r string) bool\\nGetRoute(path string) Handler\\nSeal()\\n}\\n```\\nUpon successful routing through the `x\/evidence` module, the `Evidence` type\\nis passed through a `Handler`. This `Handler` is responsible for executing all\\ncorresponding business logic necessary for verifying the evidence as valid. In\\naddition, the `Handler` may execute any necessary slashing and potential jailing.\\nSince slashing fractions will typically result from some form of static functions,\\nallow the `Handler` to do this provides the greatest flexibility. An example could\\nbe `k * evidence.GetValidatorPower()` where `k` is an on-chain parameter controlled\\nby governance. The `Evidence` type should provide all the external information\\nnecessary in order for the `Handler` to make the necessary state transitions.\\nIf no error is returned, the `Evidence` is considered valid.\\n```go\\ntype Handler func(Context, Evidence) error\\n```\\n### Submission\\n`Evidence` is submitted through a `MsgSubmitEvidence` message type which is internally\\nhandled by the `x\/evidence` module's `SubmitEvidence`.\\n```go\\ntype MsgSubmitEvidence struct {\\nEvidence\\n}\\nfunc handleMsgSubmitEvidence(ctx Context, keeper Keeper, msg MsgSubmitEvidence) Result {\\nif err := keeper.SubmitEvidence(ctx, msg.Evidence); err != nil {\\nreturn err.Result()\\n}\\n\/\/ emit events...\\nreturn Result{\\n\/\/ ...\\n}\\n}\\n```\\nThe `x\/evidence` module's keeper is responsible for matching the `Evidence` against\\nthe module's router and invoking the corresponding `Handler` which may include\\nslashing and jailing the validator. Upon success, the submitted evidence is persisted.\\n```go\\nfunc (k Keeper) SubmitEvidence(ctx Context, evidence Evidence) error {\\nhandler := keeper.router.GetRoute(evidence.Route())\\nif err := handler(ctx, evidence); err != nil {\\nreturn ErrInvalidEvidence(keeper.codespace, err)\\n}\\nkeeper.setEvidence(ctx, evidence)\\nreturn nil\\n}\\n```\\n### Genesis\\nFinally, we need to represent the genesis state of the `x\/evidence` module. The\\nmodule only needs a list of all submitted valid infractions and any necessary params\\nfor which the module needs in order to handle submitted evidence. The `x\/evidence`\\nmodule will naturally define and route native evidence types for which it'll most\\nlikely need slashing penalty constants for.\\n```go\\ntype GenesisState struct {\\nParams       Params\\nInfractions  []Evidence\\n}\\n```\\n","Prediction":"## Decision\nThe Cosmos SDK will expose a mechanism for validators to submit and evaluate arbitrary\\nproofs. Validators will be able to submit proofs in the form of JSON-LD that will be\\nevaluated by the validator's validator submission contract. Validators will be able to\\nsubmit proofs in the form of a JSON-RPC API call.\\nValidators will have the ability to submit proofs in the form of JSON-LD and\/or the\\nJSON-RPC API call. Validators will be able to submit proofs in the form of JSON-LD\\nand\/or the JSON-RPC API call.\\nValidators will submit proofs in the form of JSON-LD and\/or the JSON-RPC API call.\\nValidators will submit proofs in the form of JSON-LD and\/or the JSON-RPC API call.\\nValidators will submit proofs in the form of JSON-LD and\/or the JSON-RPC API call.\\nValidators will submit proofs in the form of JSON-LD and\/or the JSON-RPC API call.\\nValidators will submit proofs in the form of JSON-LD and\/or the JSON-RPC API call.\\nValidators will submit proofs in the form of JSON-LD and\/or the JSON-RPC API call.\\nValidators will submit proofs in the form of JSON-LD"}
{"Context":"## Context and Problem Statement\\nThe Identity platform consists of numerous interconnected components,\\neach responsible for generating their own log outputs. Some of these log\\noutputs will be driven by the implementation of logging within a\\ncomponent, and others will be provided \"out-of-the-box\" by a managed\\nservice. In either case, these logs files must be readily accessible,\\nqueryable, and persisted over a long period.\\n## Decision Drivers\\n* Collation of logs to a single, centralised platform utilised by all\\nplatforms in Wellcome Collection's technical estate.\\n* Minimal infrastructure \/ configuration required in components to\\nutilise the centralised logging platform.\\n","Decision":"## Decision\n* Collation of logs to a single, centralised platform utilised by all\\nplatforms in Wellcome Collection's technical estate.\\n* Minimal infrastructure \/ configuration required in components to\\nutilise the centralised logging platform.\\n- For containers deployed into AWS ECS \/ AWS Fargate, Wellcome\\nCollection's existing ELK stack will be utilised. Logs will be pushed\\nto the stack via a Firelens sidecar containers.\\n- For all log generating services (e.g. Lambda Function and API Gateway\\nlogs), the default AWS CloudWatch configurations will be used.\\n","Prediction":"## Decision\n* Collation of logs to a single, centralised platform utilised by all\\nplatforms in Wellcome Collection's technical estate.\\n* Minimal infrastructure \/ configuration required in components to\\nutilise the centralised logging platform.\\nChosen option: \"Use a managed service to expose logs\", because\\nit provides the best trade-off between ease of integration and\\nprovisioning of log management.\\n### Positive Consequences\\n* Logging platform can be deployed and provisioned in minutes\\n* Log management can be integrated with other components in a\\ntransparent manner\\n* No additional configuration required to manage log\\nmanagement\\n* Log files can be persisted in a managed service\\n* Self-service log ingestion is enabled\\n* Logs can be queried using standard API calls\\n* Logs can be consumed using standard API calls\\n### Negative Consequences\\n* Logs are not generated by the platform\\n* No solution exists to ingest logs from a platform into a\\nmanaged service\\n* Logging platform requires a managed service to be provisioned\\n* No management \/ control over log ingestion is provided\\n* Logging platform requires configuration to be provided to\\na managed service\\n* Logging platform requires a managed service to be provisioned\\n* Logging platform requires a managed service to be provision"}
{"Context":"## Context\\nJames offers several implementation for MailQueue, a component allowing asynchronous mail processing upon smtp mail\\nreception. These includes:\\n- Default embedded ActiveMQ mail queue implementation, leveraging the JMS APIs and using the filesystem.\\n- RabbitMQMailQueue allowing several James instances to share their MailQueue content.\\n- And FileMailQueue directly leveraging the file system.\\nWe introduced a junit5 test contract regarding management features, concurrency issues, and FileMailQueue do not meet this\\ncontract. This results in some tests being disabled and in an unstable test suite.\\nFileMailQueue tries to implement a message queue within James code, which does not really makes sense as some other projects\\nalready provides one.\\n","Decision":"## Decision\nDeprecate FileMailQueue components.\\nDisable FileMailQueue tests.\\nTarget a removal as part of 3.6.0.\\n","Prediction":"## Decision\nWe will remove FileMailQueue from the James codebase, and instead rely on the existing MailQueue implementation in James\\ncode.\\nMailQueue will be moved to a separate JAR, which will be packaged as part of James. MailQueue will be removed from the JUnit\\ntests.\\nMailQueue will be removed from the JIRA ticket.\\nMailQueue will be removed from the codebase.\\nMailQueue will be removed from the test contract.\\nMailQueue will be removed from the test suite.\\nMailQueue will be removed from the JIRA ticket.\\nMailQueue will be removed from the codebase.\\nMailQueue will be removed from the test contract.\\nMailQueue will be removed from the test suite.\\nMailQueue will be removed from the JIRA ticket.\\nMailQueue will be removed from the codebase.\\nMailQueue will be removed from the test contract.\\nMailQueue will be removed from the test suite.\\nMailQueue will be removed from the JIRA ticket.\\nMailQueue will be removed from the codebase.\\nMailQueue will be removed from the test contract.\\nMailQueue will be removed from the test suite.\\nMailQueue will be removed from the JIRA ticket.\\nMail"}
{"Context":"## Context\\nOur queries are mostly bounded to a mailbox or a user. We can easily\\nlimit the number of ElasticSearch nodes involved in a given query by\\ngrouping the underlying documents on the same node using a routing key.\\nWithout a routing key, each shard needs to execute the query. The coordinator\\nneeds also to be waiting for the slowest shard.\\nUsing the routing key unlocks significant throughput enhancement (proportional\\nto the number of shards) and also a possible high percentile latency enhancement.\\nAs most requests are restricted to a single coordination, most search requests will\\nhit a single shard, as opposed to non routed searches which would have hit each shards\\n(each shard would return the number of searched documents, to be ordered and limited\\nagain in the coordination node). This allows to be more linearly scalable.\\n","Decision":"## Decision\nEnable ElasticSearch routing.\\nMessages should be indexed by mailbox.\\nQuota Ratio should be indexed by user.\\n","Prediction":"## Decision\nWe will use the routing key to limit the number of shards used in a search request.\\nWe will use the routing key to limit the number of shards used in a coordination request.\\nWe will use the routing key to limit the number of shards used in a search request.\\nWe will use the routing key to limit the number of shards used in a coordination request.\\nWe will use the routing key to limit the number of shards used in a search request.\\nWe will use the routing key to limit the number of shards used in a search request.\\nWe will use the routing key to limit the number of shards used in a search request.\\nWe will use the routing key to limit the number of shards used in a search request.\\nWe will use the routing key to limit the number of shards used in a search request.\\nWe will use the routing key to limit the number of shards used in a search request.\\nWe will use the routing key to limit the number of shards used in a search request.\\nWe will use the routing key to limit the number of shards used in a search request.\\nWe will use the routing key to limit the number of shards used in a search request.\\nWe will use the routing key"}
{"Context":"## Context\\nRiptide started as a monolithic project. That worked good enough as long as there were only few features and minimal dependencies. But with a growing feature set several issues came up:\\n1. The single artifact became bloated and it was hard to tell core functionality apart from additional, optional features.\\n2. Picking features *\u00e0 la carte* was unnecesarily hard since it usually involved the need to carefully include\/exclude optional dependencies.\\n","Decision":"## Decision\nIn order to counter both of these aspects Riptide is fundamentally build on the idea of modules. That manifests itself in two aspects:\\n1. Each module is a sub-directory, hosting a Maven sub-module, containing a separate package with a dedicated README.\\n2. The core has several API extension points or SPIs that encourage modularization:\\n1. The `Plugin` interface to extend the processing pipeline of requests and responses\\n2. The `Route` interface to build, compose and reuse response handling functions.\\n3. The `Navigator` interface to build custom routing algorithms.\\n4. The `Attribute` interface to provide public interaction points between plugins (usually).\\nThe criteria for when to extract something into a separate module include but are not limited to:\\n- Isolate rarely used dependencies to ideally a single module\\n- Group functionality logically (Single Responsibility Principle)\\n- Different maturity level and therefore higher probabilty to change (Open\/Closed Principle)\\n","Prediction":"## Decision\nWe decided to split the monolithic application into multiple microservices. Each microservice is responsible for a specific functionality. A microservice is responsible for a specific data model, a specific service bus, an event bus, a message queue, etc.\\nThis way we can easily consume each microservice and we can easily scale and change the service architecture to better fit our requirements.\\nWe will also separate the data model from the service architecture to make it easier to change.\\nThis way we can easily switch to a new microservice architecture without having to change the way we consume it.\\nWe will also separate the messaging system from the microservices. This will allow us to change the messaging system without having to change the way we consume it.\\nWe will also separate the data model from the messaging system. This will allow us to change the data model without having to change the way we consume it.\\nWe will also separate the service architecture from the microservices. This will allow us to easily switch to a new microservice architecture without having to change the way we consume it.\\nWe will also separate the data model from the service architecture. This will allow us to easily switch to a new microservice architecture without having to change the way we consume it.\\"}
{"Context":"## Context\\nThe POC assumes generation of the table of contents based on the content presented on the page. Data need to be fetched from somewhere, and then presented on the view in some way. Data model won't be simple data structure, as it can be multi-leveled list, which might also contain some behavior, to make it easier to prepare for rendering. It's not going to be simple CRUD application, even if model will not contain much behavior.\\n","Decision":"## Decision\nUse ports and adapters architecture style.\\n","Prediction":"## Decision\nWe will use [Laravel Query Builder](https:\/\/laravel.com\/docs\/8.x\/queries#query-builder) to build the query. It's a well known, well maintained library and has a very wide support for data model that will be used.\\nIt will be used to build the data model for the POC, and will be used to build the view.\\nThere is a good chance to use [ORM features](https:\/\/laravel.com\/docs\/8.x\/orm) in the future, but we don't want to make the assumption that it will be implemented at the time of the POC.\\nQuery builder will be used to build the data model for the POC, and will be used to build the view.\\nThere is a good chance to use [ORM features](https:\/\/laravel.com\/docs\/8.x\/orm) in the future, but we don't want to make the assumption that it will be implemented at the time of the POC.\\nQuery builder will be used to build the data model for the POC, and will be used to build the view.\\nThere is a good chance to use [ORM features](https:\/\/laravel.com\/docs\/8.x\/orm) in the future"}
{"Context":"## Context\\nForm variations are tackled through a mixture of routing rules (go here if...), skip conditions (skip this if...) and splitting of schemas into \"form types\" (e.g. Retail Sales Index (RSI) form types 0102, 0112 are survey runner schemas 1_0102.json and 1_0112.json). Compared to existing surveys Monthly Business Survey (MBS) has a large number of form types which makes splitting and maintaining them by hand too onerous. The Author service is the long term solution but we need something more manageable in the interim for the MBS.\\nWe need a solution that lightens the load of authoring form variations.\\n","Decision":"## Decision\nWe will spilt schemas into manifests and blocks and generate the survey runner JSON at build time.\\n- Each manifest file will represent a form type. A manifest will define the survey metadata, one or more groups and the block ids that each group composes. Manifests will be stored in YAML.\\n- A block file will define a single survey runner block and the question it contains. Blocks which have different metadata but contain the same question will duplicate the question. Blocks which contain the same question should have the same file name with a suffix that makes identifying potential duplicate\/similar questions easy, e.g. for RSI `total-internet-sales-0102`, `total-internet-sales-0112`. Blocks will be stored in YAML.\\n- The following directory structure will be used\\n```\\ndata\/\\nsources\/\\nblocks\/\\nrsi\/\\nintroduction-0102.yaml\\nintroduction-0112.yaml\\nsummary.yaml\\n...\\nmci\/\\nintroduction-0203.yaml\\ninternet-sales.yaml\\n...\\n...\\nmanifests\/\\nrsi\/\\n0102.manifest.yaml\\n0112.manifest.yaml\\n...\\nmci\/\\n0203.manifest.yaml\\n0213.manifest.yaml\\n...\\n...\\n```\\n- The scripts which aid the build process and other useful tasks will live in the `scripts\/` folder.\\n- The json files produced by the manifests will continue to be tested by the test_schema_validation.py suite.\\n- This solution is temporary and will be superseded once Author goes live.\\n","Prediction":"## Decision\nWe will use a form schema to describe form variations. We will use the following form schema:\\n- [0102](0102.json) - survey runner\\n- [0112](0112.json) - survey runner\\n- [1001](1001.json) - survey runner\\n- [1002](1002.json) - survey runner\\n- [1011](1011.json) - survey runner\\n- [1012](1012.json) - survey runner\\n- [1013](1013.json) - survey runner\\n- [1015](1015.json) - survey runner\\n- [1017](1017.json) - survey runner\\n- [1018](1018.json) - survey runner\\n- [1019](1019.json) - survey runner\\n- [1020](1020.json) - survey runner\\n- [1021](1021.json) - survey runner\\n- [1022](1022.json) - survey runner\\n- [1023](1023.json) - survey runner\\n- [1024](1024.json) - survey runner\\n- [1025](1025.json) - survey runner\\n- [1026](1026.json) - survey runner"}
{"Context":"## Context and Problem Statement\\nThis proposal aims to outline possible approaches to report the Elasticsearch sidecar health in combination with the cluster state for cluster-wide monitoring and alerting.\\nAs for now an Elasticsearch pod is composed of 2 containers:\\n- a main container for Elasticsearch\\n- a sidecar container for running the keystore-updater\\nWhat is the keystore-updater in the sidecar doing?\\nIt calls the Elasticsearch endpoint `\/_nodes\/reload_secure_settings` to decrypt and re-read the entire keystore used by the snapshotter job.\\nTo connect to ES it depends on:\\n- an environment variable for the username\\n- secrets mounted as readonly files for the password and the CA certificate\\n- the Elasticsearch readiness\\nCurrently there is no health check based on the state of the sidecar. The sidecar can error without anyone ever noticing this state.\\nSo there is a need to check that everything is correctly setup in the sidecar container and the call to the ES API succeeds.\\nIf the sidecar container is not ready, the Elasticsearch container is impacted because the pod is considered not ready and\\nKubernetes stops to send traffic to the pod. We must accept that the two containers are intimately linked. A sidecar failure\\ncan impact the Elasticsearch availability by design.\\nHowever Go binaries that do simple things are very fast to start and very reliable.\\nFrom that we could admit that the probability to have a failure in the sidecar that runs a simple go binary is very low\\ncompared to have an Elasticsearch failure.\\nAnother challenge is to take into account that some sidecar errors are to be expected when ES is not ready yet.\\nThis can be mitigated by considering a start-up delay during which it is accepted that ES is not ready and\\ndo not report errors during this period. Then how to detect that ES has never started?\\nThe ES readiness probe will fail if ES never becomes ready.\\n## Decision Drivers\\n* Error distinction: a sidecar failure should be easily identified from an Elasticsearch failure\\n* Error side effect: a sidecar failure should not increase the unavailability of Elasticsearch compared to the current situation\\n* Promote reliability and simplicity because health-checking is a critical part of the system\\n","Decision":"## Decision\n* Error distinction: a sidecar failure should be easily identified from an Elasticsearch failure\\n* Error side effect: a sidecar failure should not increase the unavailability of Elasticsearch compared to the current situation\\n* Promote reliability and simplicity because health-checking is a critical part of the system\\nChosen option: option 4, because it gives us more flexibility to take decisions in case of failure, it does not depend on Kubernetes probes\/kubelet and it does not depend on external components.\\n### Positive Consequences\\n* Collecting the sidecar health from the operator side gives us more options to react to failures\\n* Having an HTTP server in the sidecar brings more flexibility to interact with the pod\\n* Does not depend on the Kubernetes probes or the Kubelet\\n* Minimize external dependencies\\n### Negative Consequences\\n* Increase a little the failure domain of the sidecar with the presence of the HTTP server\\n* Add complexity and responsibility to the operator\\n","Prediction":"## Decision\n* Error distinction: a sidecar failure should be easily identified from an Elasticsearch failure\\n* Error side effect: a sidecar failure should not increase the unavailability of Elasticsearch compared to the current situation\\n* Promote reliability and simplicity because health-checking is a critical part of the system\\nChosen option: \"Use the health-checking API to check for the state of the sidecar on a periodic basis\", because:\\n- health-checking is part of the sidecar\\n- health-checking is a critical part of the system\\n- the health-checking API is widely used by a large number of projects\\n- the health-checking API is simple to use and doesn't require any configuration\\n- the health-checking API is easy to test (see below)\\n- the health-checking API is easy to maintain (see below)\\n- the health-checking API is easy to document (see below)\\n- the health-checking API can be integrated with an external service (see below)\\n- the health-checking API is easy to integrate with a monitoring tool (see below)\\n- the health-checking API is easy to integrate with a logging tool (see below)\\n- the health-checking API is easy to integrate"}
{"Context":"## Context\\nWhen releasing some features, we might want to only make them available to a sub set of users initially, to gain feedback and reduce the potential impact of bugs.\\nWe also want to be able to continue development of a feature in master over a longer period of time, without it being available in an unfinished state.\\n","Decision":"## Decision\nWe will associate a set of feature toggles with a Pocket Account.\\n","Prediction":"## Decision\nWe will use the [code-review](https:\/\/github.com\/brunolabs\/code-review) tool to ensure code quality.\\nWe will create a branch to allow us to test new features on a per-feature basis.\\nWe will create pull requests for new features that we want to release.\\nWe will use the [travis-ci](https:\/\/travis-ci.org\/) CI platform to automatically test new features,\\nand deploy to production when they are ready for production.\\nWe will use the [Grafana metrics](https:\/\/www.grafana.org\/) dashboard to collect data and metrics about the code quality of our features.\\nWe will use a [staging environment](https:\/\/www.grafana.org\/docs\/en\/staging\/) to test new features on a staging server.\\nWe will use a [pre-release](https:\/\/www.grafana.org\/docs\/en\/pre-release\/) channel to test new features on a pre-release server.\\nWe will create a [pull request](https:\/\/github.com\/brunolabs\/code-review\/pull\/2) for every feature that is released.\\nWe will run the unit tests for all new features, and we will run them on all of the supported environments.\\nWe will run"}
{"Context":"## Context\\nWe need to push our Meadow resources to the Elasticsearch `common` index so that they can be discovered on the Digital Collections website. We also want the ability to do full text searching and faceting from within the Meadow ingest application. We investigated using Postgres through our GraphQL API, but concluded there would potentially be performance issues at scale.\\n","Decision":"## Decision\n- Meadow will write to same `common` index that Donut does\\n- The `source.model.application` property exists to identify source application\\n- Items will be indexed with a `published` `true` or `false` flag\\n- Requests through the `elastic-proxy` will refuse to return any unpublished items.\\n- For Elasticsearch requests through from the Meadow front end, the Elixir app backend will provide a plug\/route and add AWS signing credentials to the request as well as filter results limiting to Meadow items only.\\n","Prediction":"## Decision\nWe will use Elasticsearch as our search engine.\\nWe will use the `indices` and `indices` metadata to create our index configuration. We will also create a `schema` and `mappings` metadata to define the fields and types that will be present in the index.\\nWe will use the `indices` and `indices` metadata to define how the fields and types are accessed. We will use the `indices` and `indices` metadata to define the fields that will be indexed in the `common` index.\\nWe will use the `indices` and `indices` metadata to define the fields that will be searchable in the `common` index.\\nWe will use the `indices` and `indices` metadata to define the fields that will be filtered in the `common` index.\\nWe will use the `indices` and `indices` metadata to define the fields that will be collapsed in the `common` index.\\nWe will use the `indices` and `indices` metadata to define the fields that will be collapsed in the `common` index.\\nWe will use the `indices` and `indices` metadata to define the fields that will be sorted in the `common` index.\\nWe will use the `indices"}
{"Context":"## Context\\nWe need a mechanism to capture and manage application errors. Without an integration our debugging options are to access a console on live environments and try to replicate (something we want to minimise) or by looking through information provided by logs.\\nWe have used Rollbar for a few years now and we have not reviewed this decision since. It is currently being used for 14 applications.\\nFor some projects we use their technical tooling of choice to aid in the transition to business as usual. Due to this we will have a handful of projects using Sentry and possible others.\\nSometimes Rollbar environment names don't match the Rails environment. Dalmatian-<project> and paas-<project> both exist. There also exists both permutations for the same project as we transition. We have used ROLLBAR_ENV to manage this before so making it explicit will hopefully make it clearer how it can be changed.\\n","Decision":"## Decision\nUse Rollbar to collect and manage our application errors.\\n","Prediction":"## Decision\nWe will use Rollbar to monitor errors and warnings for all applications. We will use this to monitor a single Rollbar account.\\nWe will monitor the following:\\n- Application error\\n- Application warning\\n- Environment variable error\\n- Environment variable warning\\n- Deployed application status\\n- Deployment status\\n- Deployment error\\n- Deployment warning\\n- Deployment environment\\n- Deployment environment variable error\\n- Deployment environment variable warning\\n- Application error\\n- Application warning\\n- Deployment error\\n- Deployment warning\\n- Application environment\\n- Deployment environment variable error\\n- Deployment environment variable warning\\n- Deployment error\\n- Deployment warning\\n- Deployment environment\\n- Deployment environment variable error\\n- Deployment environment variable warning\\n- Deployment error\\n- Deployment warning\\n- Deployment environment\\n- Deployment environment variable error\\n- Deployment environment variable warning\\n- Deployment error\\n- Deployment warning\\n- Deployment environment\\n- Deployment environment variable error\\n- Deployment environment variable warning\\n- Deployment error\\n- Deployment warning\\n- Deployment environment\\n- Deployment environment variable error\\n- Deployment environment variable warning\\n- Deployment error\\n- Deployment warning\\n- Deployment environment\\n- Deployment environment variable error\\n- Deployment environment variable warning\\n- Deployment error\\n- Deployment warning\\n- Deployment environment\\n- Deployment environment"}
{"Context":"## Context\\nWe are working on a new feature of Form Builder where a form creator (user) can\\nopt to have submissions (citizen answers) sent as JSON to an endpoint of their choosing.\\nThese endpoints can be any url, but will likely be CMS or case management systems.\\nThe first use case of this is the HMCTS complaints form. This form requires an\\nintegration to HMCTS' Optics system. We will be creating a custom adapter that receives\\nthe JSON output from Form Builder, and then generates the complaint in Optics. This\\nadapter will live outside the Form Builder infrastructure. In the future, the hope\\nis that teams will create their own adapters in order to integrate Form Builder\\nwith various external systems.\\nThis ADR is regarding how to secure this JSON output from Form Builder to the\\nendpoint set by the user.\\nThreats\\n- Payload may be intercepted, exposing sensitive user answers.\\n- Payload may be modified in transit, with potentially damaging consequences.\\n- POST requests could be made to the receiver (adapter) from sources other than Form Builder,\\ncreating records in their systems that don't reflect user submissions.\\nRisks\\n- Reputational damage due to data breaches\\n- Security threats from modified payloads\\nMistakes from the User\\n- Users could potentially enter an incorrect endpoint, which would deliver sensitive\\ndata to an incorrect endpoint.\\n- Users could share security information accidentally, allowing malicious actors\\nto receive payloads.\\n","Decision":"## Decision\n### TLS over HTTP\\nUse TLS over HTTP (HTTPS). This would ensure\\nencrypted data in transit, which [satisfies MOJ standards](https:\/\/ministryofjustice.github.io\/security-guidance\/standards\/cryptography\/#cryptography).\\nWe came to the conclusion that mutual TLS doesn\u2019t really bring any benefits and\\ninvolves way too much management overhead in this context, so agreed on normal\\nTLS + payload encryption with a pre-shared secret.\\n### Encrypted JSON Payload\\nEncryption of the payload is possible with JWE (JOSE toolkit). This would require\\ndecryption at the other end. Payload encryption is preferred over TLS alone as TLS\\nis often terminated at the edge of a large network with communications travelling in the clear\\ninside that network. Given the sensitivity of the submitted data for some forms\\nand that it may need to be relied on in court, we need more confidence that only\\nthe intended recipient can read the data and that other actors on that network\\ncan\u2019t impersonate FB by sending other requests to the endpoint.\\n### Shared Secret\\nThe shared secret will be set as an ENV var in the publisher to be consumed by\\nthe form's runner instance. The shared secret will be used to encrypt \/ decrypt\\nthe payload. This shared secret would ideally be system generated rather than user\\ngenerated to ensure that it is appropriate for the encryption method we choose.\\n### Certificates vs Shared Secret\\nBoth certificates and shared secrets can be used for signing and de-serialising the payload.\\nGiven users may not be technical, a shared secret would be preferred as requiring users\\nto generate and upload certificates may be too much to ask.\\n### Validation of remote endpoint\\nThe Form Builder system should validate the endpoint that is entered by the user.\\nAt minimum this should be an HTTPS endpoint and `*.gov.uk`.\\n### Overview of Solution\\nForm Builder:\\n- Connects with adapter via HTTPS using a ruby library such as [Net:HTTP](https:\/\/ruby-doc.org\/stdlib-2.6.3\/libdoc\/net\/http\/rdoc\/Net\/HTTP.html).\\n- Connection with TLS using `Net::HTTP` gem or similar.\\n- Encrypts JSON payload using a shared secret and JWE protocol.\\n- Sends as POST request.\\nAdapter:\\n- Receives HTTPS POST request from Form Builder.\\n- Decrypts JSON payload using shared secret.\\n","Prediction":"## Decision\nWe will use a custom adapter that will intercept the Form Builder request and\\nre-encode it to a JSON payload. We will then use the adapter to authenticate the\\nsender and the receiver and ensure that the data sent is indeed the correct\\nform of data.\\nWe will use the AES-256 encryption algorithm, as recommended by the NIST.\\nWe will use a secret key to encrypt the payload. We will store this key in a\\nseparate file in the file system. This key will be used only by the adapter and\\nthe Form Builder application.\\nWe will use a secret key to encrypt the request, and we will store this key\\nin a separate file in the file system. This key will be used only by the adapter and\\nthe Form Builder application.\\nWe will use a secret key to encrypt the response, and we will store this key\\nin a separate file in the file system. This key will be used only by the adapter and\\nthe Form Builder application.\\nWe will use a secret key to encrypt the response, and we will store this key\\nin a separate file in the file system. This key will be used only by the adapter and\\nthe Form Builder application.\\nWe will use a secret"}
{"Context":"## Context\\nDiscussed by entire team, but predominately Sam Smith, Joaquin Hernandez and\\nJon Robson.\\nAs our JavaScript becomes more complex we are making it increasingly difficult\\nto maintain dependencies via extension.json. Dependencies and file order have\\nto be managed and every new file creation requires an edit to extension.json.\\nThis slows down development. In Vagrant for instance NTFS file systems\\nexperience slowdown when loading many files.\\nThere are many tools that bundle JavaScript out there that can do this for us.\\n**Pros**\\n* mw.popups no longer needs to be exposed as a global object\\n* Dependency management is no longer a manual process but automated by webpack\\n* Would allow us to explore template pre-compiling\\n* More reliable debug via source map support\\n* For non-MediaWiki developers it should be easier to understand our\\ndevelopment workflow.\\n**Cons**\\n* There is now a build step. New developers to the extension may try to\\ndirectly edit the distribution files.\\n* Likely to be more merge conflicts, but this could be addressed by additional\\ntooling (e.g. post-merge build step)\\n","Decision":"## Decision\nThere are various bundlers to choose from, but Webpack was chosen on the basis\\nthat\\n1) It was easy to switch to another\\n2) It is popular and well maintained.\\n3) Many members of the team are familiar with it.\\nhttps:\/\/medium.com\/@tomchentw\/why-webpack-is-awesome-9691044b6b8e#.mi0mmz75y\\nprovides a good write up.\\n","Prediction":"## Decision\n* Enable bundling of JavaScript via extension.json as a global object.\\n* Add a build step to bundle JavaScript to the distribution.\\n* Add a dependency management tool to the distribution.\\n* Add a post-merge build step to bundle JavaScript.\\n* Remove dependency management tool from distribution.\\n* Remove build step from distribution.\\n* Remove bundle.js from distribution.\\n* Remove bundle.js from distribution.\\n* Remove bundle.json from distribution.\\n* Remove dependency.json from distribution.\\n* Remove dependency.json from distribution.\\n* Remove dependency.json from distribution.\\n* Remove dependency.json from distribution.\\n* Remove dependency.json from distribution.\\n* Remove dependency.json from distribution.\\n* Remove dependency.json from distribution.\\n* Remove dependency.json from distribution.\\n* Remove dependency.json from distribution.\\n* Remove dependency.json from distribution.\\n* Remove dependency.json from distribution.\\n* Remove dependency.json from distribution.\\n* Remove dependency.json from distribution.\\n* Remove dependency.json from distribution.\\n* Remove dependency.json from distribution.\\n* Remove dependency.json from distribution.\\n* Remove dependency.json from distribution.\\n* Remove dependency.json from distribution.\\n* Remove dependency.json from distribution.\\n* Remove dependency.json from distribution.\\n"}
{"Context":"## Context\\nThere are two possible tokenization methods when making a payment request to Google Pay: [PAYMENT_GATEWAY and DIRECT](https:\/\/developers.google.com\/pay\/api\/web\/guides\/tutorial#tokenization).\\nFeatures of PAYMENT_GATEWAY tokenization:\\n- Merchant id identifying GOV.UK Pay with Google Pay needs to be set up\\n- We simply pass the encrypted response from Google Pay through to the payment gateway as is\\n- No need of the operational overhead of key management as is already handled between Google Pay and the payment gateway\\nFeatures of DIRECT tokenization:\\n- No need for a Merchant id between GOV.UK Pay and Google Pay\\n- Need to be PCI DSS compliant (which we already are)\\n- Need to decrypt the response to get information to send to the payment gateway\\n- Need the operational overhead of key management as we need to [roll keys once a year and register these with Google Pay](https:\/\/developers.google.com\/pay\/api\/web\/guides\/resources\/payment-data-cryptography#key-management)\\nWith either tokenization method, we get a [number of data](https:\/\/developers.google.com\/pay\/api\/web\/reference\/object#response-objects), but the ones relevant to us are:\\n- Last 4 card digits\\n- Billing address (which includes name)\\n- Email address\\n- Card Network\\nWith DIRECT tokenization we decrypt to get:\\n- PAN\\n- expiration month and year\\n[and other things](https:\/\/developers.google.com\/pay\/api\/web\/guides\/resources\/payment-data-cryptography#encrypted-message).\\nChoosing whether to decrypt the payload from Google Pay or not is informed by the following two deal breakers:\\n- We need to be able to retrieve the cardholder name, email, and last 4 digits of a card\\n- We don't want services to have to manage any sort of keys\/certificates\\n","Decision":"## Decision\nUnlike the current implementation of Apple Pay, we have chosen to go with not decrypting the payload for the following reasons:\\n- We get all the information we require (Last 4 card digits, billing address name, email address, card network) in unencrypted form\\n- Services won't need to manage any keys\/certificates. This is because when a payment request is made to Google Pay we specify the payment gateway, e.g. \"Worldpay\".\\nGoogle Pay will return a response encrypted with the payment gateway's public key.\\nThis response is decryptable by the payment gateway as they have their private encryption key.\\nSo there is already a key management process between Google Pay and the payment gateway.\\nOur initial assumption that Google Pay encrypts with the merchant id's public key was incorrect.\\n- No operational process is needed on our part to manage keys\/certs.\\n","Prediction":"## Decision\nWe will use PAYMENT_GATEWAY tokenization for our GOV.UK Pay with Google Pay payments.\\nWe will use DIRECT tokenization for all other payments.\\nWe will use [GOV.UK Pay with Google Pay](https:\/\/developers.google.com\/pay\/api\/web\/guides\/) as the service provider for GOV.UK Pay with Google Pay.\\nWe will implement [GOV.UK Pay with Google Pay](https:\/\/developers.google.com\/pay\/api\/web\/guides\/) as a [service](https:\/\/developers.google.com\/pay\/api\/web\/reference\/service) in our [service account](https:\/\/developers.google.com\/identity\/sign-in\/web\/guides\/service-account).\\nWe will use [GOV.UK Pay with Google Pay](https:\/\/developers.google.com\/pay\/api\/web\/guides\/) as the service provider for [GOV.UK Pay](https:\/\/developers.google.com\/pay\/api\/web\/guides\/).\\nWe will use [GOV.UK Pay with Google Pay](https:\/\/developers.google.com\/pay\/api\/web\/guides\/) as the service provider for [GOV.UK Pay with Google Pay](https:\/\/developers.google.com\/pay\/api\/web\/guides\/).\\nWe will use [GOV.UK Pay"}
{"Context":"## Context and Problem Statement\\nThe performance tests for `contact-frontend` and `hmrc-deskpro` live in the repository\\n[contact-hmrcdeskpro-performance-tests](https:\/\/github.com\/hmrc\/contact-hmrcdeskpro-performance-tests). As part of the performance test journey, they were previously making calls that didn't reflect a real user journey:\\n1. Making a call to the endpoint `\/ticket\/$id` in `hmrc-deskpro` to get the IDs for the ticket in MongoDB and external Deskpro\\n2. Failing the test if there was no ID yet available from external Deskpro for the ticket\\nThe problems with this are:\\n1. The calls to POST ticket data to external Deskpro happens asynchronously as a scheduled job, with queueing in place to handle high load. In the real world, it is not coupled the an end user's journey, so this test is not representative of a real user journey.\\n2. The Staging instance of external DeskPro ends up tightly coupled to our performance tests, and our performance tests fail if external DeskPro is not.\\nas performant as `contact-frontend` and `hrmc-deskpro`. We should not be performance testing external services, especially as we have queueing in place to manage load.\\nIn short, by coupling in this way to external DeskPro:\\n* we test a service that we don't own\\n* we don't reflect the user journey\\n* we put unnecessary load on the system by calling `\/ticket\/$id`.\\n## Decision Drivers <!-- optional -->\\n* Upcoming SA peak 2021 requires load testing to 700,000 tickets created from1 to 31 Jnuary inclusive\\n","Decision":"## Decision\n* Upcoming SA peak 2021 requires load testing to 700,000 tickets created from1 to 31 Jnuary inclusive\\nChosen option: \"Remove calls to external DeskPro from the tests\", because our tests should be testing the\\nservices that we own, not external DeskPro which is owned by DDCOps. Additionally, we have queueing backed by MongoDB in place within `hmrc-deskpro` precisely so that if there is high load on `contact-frontend` and `hmrc-deskpro`, this can handle the throttling of tickets POSTed to external DeskPro.\\n### Positive Consequences <!-- optional -->\\n* The performance tests are an accurate representation of the user journey\\n* We as PlatUI are only testing the services that we own\\n### Negative Consequences <!-- optional -->\\n* Team DDC Ops no longer get performance testing of their DeskPro instances as part of the\\nPlatUI performance tests. However, a separate repo can be created to performance test\\nexternal DeskPro.\\n","Prediction":"## Decision\n* Upcoming SA peak 2021 requires load testing to 700,000 tickets created from1 to 31 Jnuary inclusive\\nChosen option: \"Don't use external Deskpro\", because it is not representative of a real user journey, and it is not a good idea to test services that we don't own.\\n### Positive Consequences <!-- optional -->\\n* The test is not representative of a real user journey\\n* We don't have to deal with the complexities of an end user's journey\\n* We can focus on the tests we care about\\n* We can use the queueing functionality to manage load\\n### Negative Consequences <!-- optional -->\\n* We're not reflecting the user journey\\n* We're not testing services that we don't own\\n* We're loading an external service that we don't own\\n* We're not testing the end user's journey\\n* We don't have a good test suite to check that our tests are correct\\n* We potentially introduce a poorly performing service into production\\n* We introduce an additional point of failure for the service\\n* We introduce a test that doesn't reflect the user journey\\n* We introduce an additional point of failure for the service\\n* We introduce an additional"}
{"Context":"## Context\\nFor implementing event-driven application, the platform should be available for communication with integration events.\\n","Decision":"## Decision\nWe will start from standard Spring Events classes: ApplicationListener, ApplicationEvent without dependency to external middleware component. We can add custom features to Spring functionality when it's needed.\\ntodo: For now, all events will be stored in integration-events module. But this solution should be reviewed. Integration events should be published after successful transaction.\\n- https:\/\/devblogs.microsoft.com\/cesardelatorre\/domain-events-vs-integration-events-in-domain-driven-design-and-microservices-architectures\/\\n","Prediction":"## Decision\nIn the platform, the integration events will be published to the integration broker.\\nIn the integration broker, the integration events will be consumed by the event-driven application.\\nIntegration events will be published to the integration broker using the [apigateway](https:\/\/github.com\/GoogleCloudPlatform\/apigateway) service.\\nIntegration events will be consumed by the integration application using the [apigateway](https:\/\/github.com\/GoogleCloudPlatform\/apigateway) service.\\nIntegration events will be published to the integration broker using the [apigateway](https:\/\/github.com\/GoogleCloudPlatform\/apigateway) service.\\nIntegration events will be consumed by the integration application using the [apigateway](https:\/\/github.com\/GoogleCloudPlatform\/apigateway) service.\\nIntegration events will be published to the integration broker using the [apigateway](https:\/\/github.com\/GoogleCloudPlatform\/apigateway) service.\\nIntegration events will be consumed by the integration application using the [apigateway](https:\/\/github.com\/GoogleCloudPlatform\/apigateway) service.\\nIntegration events will be published to the integration broker using the [apigateway](https:\/\/github.com\/GoogleCloudPlatform\/apigateway) service.\\nIntegration events"}
{"Context":"## Context and problem statement\\nThe core of Archivematica is a server application that coordinates work across\\nmultiple workers. The collection of related tasks (also known as *workflow\\ndata*) is stored in the application database using a relation model.\\nIntroducing changes to the workflow data is hard because the model is clumsy and\\nconvoluted. Changes to the data are delivered as data and schema migrations\\nwhich are difficult to implement and they add risk to the process of upgrading\\nthe software.\\nAdditionally, we want to add full internationalisation support in order to make\\nthe system more attractive to a wider range of users and increase adoption. We\\nneed a simpler mechanism to deliver translations and other workflow data changes\\nin general.\\n","Decision":"## Decision\n* Archivematica 1.9 will implement the proposed solution replacing the existing\\ndatabase tables.\\n* We do not currently intend to expose the ability to modify the workflow data\\nto Archivematica users - but this may change in the future as the solution\\nmatures and becomes interoperable.\\n* The visualization and manipulation of the workflow data will be facilitated by\\na new tool that we're calling amflow.\\n","Prediction":"## Decision\nWe will use the [CustomFields](https:\/\/github.com\/Archivematica\/Archivematica\/wiki\/CustomFields) package to provide translations and other workflow data changes. This package is a\\n[fully-featured] field storage solution that can be used to store\\nany type of data and is the core of the Archivematica data model. It can be used to store\\nany type of data and is the core of the Archivematica data model.\\nWe will use the CustomFields package to store translations and other workflow data changes as\\ndescribed in this [PR](https:\/\/github.com\/Archivematica\/Archivematica\/pull\/1194).\\nWe will also use the [CustomFieldsPlugin](https:\/\/github.com\/Archivematica\/Archivematica\/issues\/1194) package to provide translations and other workflow data changes.\\nThis package can be used to provide translations and other workflow data changes in\\nvarious other ways, including in the `archivematica-core` package and the\\n`archivematica` package.\\nWe will also use the `archivematica-core` package to provide translations and other workflow data changes.\\nWe will use"}
{"Context":"## Context\\nIn ADR008 and ADR014 we decided to use HAProxy, for three reasons:\\n- Writing HSTS header if they are not present in the upstream request\\n- Implementing HTTP -> HTTPS redirect\\n- Custom health check for Gorouter\\nThese problems have since been fixed:\\n- Header rewriting was implemented in [v0.183.0](https:\/\/github.com\/cloudfoundry\/routing-release\/releases\/tag\/0.183.0)\\n- HTTP healthchecking was implemented in [v0.139.0](https:\/\/github.com\/cloudfoundry\/routing-release\/releases\/tag\/0.139.0)\\n- HTTP -> HTTPS redirect can be done using AWS ALBs\\nWe currently use multiple ELBs (classic) which we want to replace with ALBs.\\nWe want to use ALBs because:\\n- ELBs are deprecated in terraform and cause crashes\\n- ALBs can give us more metrics in CloudWatch\\n- ALBs have better support for X-Forwarded-For\\n- ALBs support fixed-response which can be used for HTTP -> HTTPS rewriting\\nHAProxy adds significant complexity to our routing deployment and maintenance:\\n- Proxy Protocol is non-standard and hard to understand\\n- HTTP -> HTTPS rewriting is hard to understand\\n- HAProxy config is rarely touched\\n- We have to maintain our own HAProxy BOSH release\\n- HAProxy duplicates the number of logs we receive because every platform request is written twice\\n- HAProxy adds an extra network hop for every request\\n","Decision":"## Decision\n- Replace ELBs with ALBs\\n- Use ALB fixed-response to redirect HTTP -> HTTPS\\n- Use Gorouter directly for:\\n- TLS termination\\n- HSTS header rewriting\\n- Healthchecking the router instance\\n- Remove HAProxy\\n","Prediction":"## Decision\nWe will use HAProxy for our HTTP -> HTTPS routing.\\nWe will use ALBs for HAProxy health checking and health logging.\\nWe will use ALBs for HAProxy transport encryption (TLS).\\nWe will use ALBs for HAProxy load balancing.\\nWe will use ALBs for HAProxy health checks and health logging.\\nWe will use ALBs for HAProxy transport encryption (TLS).\\nWe will use ALBs for HAProxy load balancing.\\nWe will use ALBs for HAProxy health checks and health logging.\\nWe will use ALBs for HAProxy transport encryption (TLS).\\nWe will use ALBs for HAProxy health checks and health logging.\\nWe will use ALBs for HAProxy transport encryption (TLS).\\nWe will use ALBs for HAProxy health checks and health logging.\\nWe will use ALBs for HAProxy transport encryption (TLS).\\nWe will use ALBs for HAProxy health checks and health logging.\\nWe will use ALBs for HAProxy transport encryption (TLS).\\nWe will use ALBs for HAProxy health checks and health logging.\\nWe will use ALBs for HAProxy health checks and health logging.\\nWe will use ALBs for HAProxy health checks and health"}
{"Context":"## Context and Problem Statement\\n* The current API implementations is spread across multiple modules (such as webapp, config, db and query) without a formal definition of the inputs\/outputs. This induces inconsistencies and we foresee technical difficulties to maintain and evole the service.\\n* We would like to use an interface description language to define the API between the various component (such as between the webapi and webclients, worker and database).\\n* How do we define and maintain the interfaces?\\n","Decision":"## Decision\nChosen option: \"Protobuf\", because it comes out best (see below).\\n### Positive Consequences\\n- We need to start defining interfaces outside of their implementations.\\n- We need to use a simpler HTTP api (e.g. the path for the action function name and the body for its input), instead of encoding the inputs with a mix of path, querystring and body components.\\nFor example, use `POST \/get_projects BODY { \"index\": \"name\" }` instead of `GET \/projects?index=name`.\\n### Negative Consequences\\n- The team needs to learn a new language.\\n- Follow-up decisions required for the actual implementation.\\n","Prediction":"## Decision\n* We will use [Swagger](https:\/\/swagger.io\/)\\n* To define the API we will use [Swagger UI](https:\/\/swagger.io\/swagger-ui\/).\\n* To validate the API we will use [Swagger Cloud](https:\/\/swaggercloud.io\/).\\n* To ingest the APIs into the service we will use [Swagger API](https:\/\/swaggerapi.com\/).\\n* To consume the APIs we will use [Swagger API Proxy](https:\/\/swaggerapi.com\/).\\n* To define the API we will use [Swagger Data Model](https:\/\/swagger.io\/docs\/data-model\/).\\n* To validate the API we will use [Swagger Data Model Validator](https:\/\/swaggerapi.com\/validator\/).\\n* To ingest the APIs into the service we will use [Swagger Data Model Validator](https:\/\/swaggerapi.com\/validator\/).\\n* To consume the APIs we will use [Swagger API Proxy](https:\/\/swaggerapi.com\/).\\n* We will define the API as a Swagger definition.\\n* We will define the Swagger UI as a Swagger definition.\\n* We will define the Swagger API Proxy as a Swagger definition.\\n* We will define the Swagger Data Model as a Swagger definition.\\n* We will define the Swagger"}
{"Context":"## Context\\nThis ADR supersedes `ADR-006`.\\nBack in August of 2016, we went through an extensive evaluation process for a workflow manager. After looking at a number of open source and hosted solutions, we settled on Apache Airflow. Since then, we've had some success with Airflow, but also a number of setbacks.\\nFirst, we've encountered a number of deeply rooted issues with Airflow's scheduler. At times it would lock up and cease scheduling queued jobs, emitting cryptic error messages in the process. Other times, it would consume a large percentage of an instance's CPU while emitting many valueless log messages.\\nSecond, Airflow's UI was a big differentiator in our initial decision making process. In practice, the UI proved confusing to use, and hasn't lended itself well to running in a distributed environment (e.g., task log centralization, real-time log reporting).\\nLastly, our troubleshooting processes often led us to community supported help resources, like the Airflow mailing lists and issues. The more time we spent reading through these entries, the lower our confidence in Airflow became. Leadership was in a constant state of turmoil around releases, and a presence of production grade users became less and less visible.\\n","Decision":"## Decision\nWe are replacing all of the following Airflow components with AWS Batch:\\n- Scheduler\\n- Web server\\n- Worker\\n- Executor (Celery)\\nAWS Batch is a hosted service produced by Amazon Web Services (AWS). When used in `MANAGED` mode, it assists in scheduling, executing, and reporting status on submitted jobs. In addition, it provides a minimal UI for visually tracking job status. Under the covers, its execution environment is Amazon ECS.\\n","Prediction":"## Decision\nWe will not adopt Airflow as a workflow manager. We will instead focus on the following alternatives:\\n* [Kafka](https:\/\/kafka.apache.org\/)\\n* [Flume](https:\/\/flume.apache.org\/)\\n* [Django Airflow](https:\/\/airflow.apache.org\/)\\n* [Flume + Zookeeper](https:\/\/flume.apache.org\/docs\/latest\/zookeeper.html)\\n* [Flume + Zookeeper + Airflow](https:\/\/flume.apache.org\/docs\/latest\/zookeeper-airflow.html)\\n* [Flume + Airflow + Zookeeper + Kafka](https:\/\/flume.apache.org\/docs\/latest\/zookeeper-airflow.html)\\n* [Flume + Airflow + Kafka + Zookeeper](https:\/\/flume.apache.org\/docs\/latest\/zookeeper-airflow.html)\\n* [Flume + Airflow + Kafka + Kafka + Zookeeper](https:\/\/flume.apache.org\/docs\/latest\/zookeeper-airflow.html)\\n* [Flume + Airflow + Kafka + Kafka + Kafka + Zookeeper](https:\/\/flume.apache.org\/docs\/latest\/zookeeper-airflow.html)\\n* [Flume + Kafka + Kafka + Kafka + Kafka + Kafka + Zookeeper](https:\/\/flume.apache"}
{"Context":"## Context\\nIt's handy to use tooling to create ADRs. There are a couple of options available.\\n","Decision":"## Decision\nThese are a branch of npryce's adr-tools. See\\n[adr\/madr](https:\/\/github.com\/adr\/madr). They seem to have added a whole bunch\\nof extra stuff that I'm not sure we need, although I do like some of the ideas\\nin their ADR template which I want to re-use in our own template.\\n#### Pros\\n- flexible\\n#### Cons\\n- seems quite a bit more complicated (not necessarily in usage, didn't get that\\nfar, just looking at their plethora of repos)\\n- lots of extra features we may not need\\n- JavaScript based, so more complicated than just a bunch of bash scripts\\nUse npryce's adr-tools.\\n","Prediction":"## Decision\nWe will use [ADRTool](https:\/\/adrtool.org\/) to create ADRs.\\nADRTool is a tool to create ADRs, and is well documented and maintained.\\nADRTool is written in Python and is hosted on PyPI.\\nADRTool has a command line tool to generate ADRs, and can also be used\\nto generate ADRs from other tools. We will use this to generate ADRs\\nfrom other tools.\\nADRTool has a Python API to allow it to be used from other tools.\\nWe will use this to allow it to be used from other tools.\\nADRTool is used as the tool to generate ADRs.\\nADRTool is used as the tool to generate ADRs.\\nADRTool is used as the tool to generate ADRs.\\nADRTool is used as the tool to generate ADRs.\\nADRTool is used as the tool to generate ADRs.\\nADRTool is used as the tool to generate ADRs.\\nADRTool is used as the tool to generate ADRs.\\nADRTool is used as the tool to generate ADRs.\\nADRTool is used as the tool to generate ADRs.\\"}
{"Context":"## Context and Problem Statement\\nSince launching `play-frontend-hmrc` we have included deprecated static helpers which were required for Play 2.5 projects.\\n## Decision Drivers\\n* There is complexity involved in maintaining multiple interfaces for the same behaviour\\n* these helpers have been deprecated for 2 years, they were only added originally for Play 2.5 users\\n* the platform opinion is to use DI\\n* soft advice from Play has been to use DI since Play 2.6 (https:\/\/www.playframework.com\/documentation\/2.8.x\/ScalaDependencyInjection)\\n* Guice DI is completely standard in the Play Framework and not new - all services will be using Guice\\n* some teams may regard the deprecation notices as small print\\n","Decision":"## Decision\n* There is complexity involved in maintaining multiple interfaces for the same behaviour\\n* these helpers have been deprecated for 2 years, they were only added originally for Play 2.5 users\\n* the platform opinion is to use DI\\n* soft advice from Play has been to use DI since Play 2.6 (https:\/\/www.playframework.com\/documentation\/2.8.x\/ScalaDependencyInjection)\\n* Guice DI is completely standard in the Play Framework and not new - all services will be using Guice\\n* some teams may regard the deprecation notices as small print\\n* we should not re-instate static helpers for Play 2.8\\n* we should remove them in version 1.0.0\\n* we should publish a blog article sooner rather than later that we will be removing them in 1.0.0\\n### Positive Consequences\\n* Our codebase becomes easier to maintain\\n* We have to use DI in our tests which makes our tests more accurate to how these components will be used\\n### Negative Consequences\\n* Anyone who is using the static helpers will have to update - that's why we're including this in version 1.0.0\\n","Prediction":"## Decision\n* there is complexity involved in maintaining multiple interfaces for the same behaviour\\n* these helpers have been deprecated for 2 years, they were only added originally for Play 2.5 users\\n* the platform opinion is to use DI\\n* soft advice from Play has been to use DI since Play 2.6 (https:\/\/www.playframework.com\/documentation\/2.8.x\/ScalaDependencyInjection)\\n* Guice DI is completely standard in the Play Framework and not new - all services will be using Guice\\n* some teams may regard the deprecation notices as small print\\nChosen option: \"Use DI\", because it is idiomatic Scala, and it was the only option that was recommended by Play for 2.5.\\n### Positive Consequences\\n* We are using DI, which is standard in the Play Framework\\n* We are using Play's own DI\\n* We are using Play's own DI, which is standard in the Play Framework\\n* We are using Play's own DI, which is standard in the Play Framework\\n* We are using Play's own DI, which is standard in the Play Framework\\n* We are using Play's own DI, which is standard in the Play Framework\\n* We are using Play's"}
{"Context":"## Context\\nTo speed up the UI development we need to select a UI Framework that has a good community as well as good functionality.\\n","Decision":"## Decision\nWe use Ant Design as the UI Framework.\\n","Prediction":"## Decision\nWe will use Angular 9 as our UI Framework.\\nWe will also use Material Design as our UI Framework.\\nWe will also use the [Angular Material](https:\/\/material.angularjs.org) package for the UI.\\nWe will use [Angular CLI](https:\/\/angular.io\/cli) for the project.\\nWe will use [Angular CLI Buildpacks](https:\/\/angular.io\/cli\/buildpacks) to manage the build process.\\nWe will use [Angular CLI 9.0.0-beta.67.0.10](https:\/\/angular.io\/cli\/9.0.0-beta.67.0.10) for the project.\\nWe will use [Angular CLI 9.0.0-beta.67.0.10 | GitHub](https:\/\/github.com\/angular\/angular-cli) as the source code repository.\\nWe will use [Angular CLI 9.0.0-beta.67.0.10 | Angular Marketplace](https:\/\/www.npmjs.com\/package\/angular) for the package.\\nWe will use [Angular CLI 9.0.0-beta.67.0.10 | Angular CLI 9.0.0-beta.67.0.10 | Angular CLI 9.0.0"}
{"Context":"## Context\\nWe have received a couple of Out of Memory errors in relation to large pdfs which cause a problem themselves or when they are combined through the `count` parameter.\\n","Decision":"## Decision\nGiven the low number of occurances it has been decided to simply increase the memory available to the JVM in the pod.\\nShould this problem continue to occur we will need to look at options such as:\\n* Streaming the zip file data to the SFTP server directly\\n* Storing binary data in Blob Storage instead of Postgresql and potentially shifting processing to a tmpfs volume\\n","Prediction":"## Decision\nWe will not add any more checks to prevent this.\\nWe will only allow a single pdf to be opened at a time, and will attempt to close any that are opened.\\nWe will allow the user to specify the maximum size of the pdf that will be opened.\\nWe will also allow the user to specify the maximum number of combined pdfs that will be opened.\\nWe will check the combined pdfs are not larger than the maximum size specified, and if they are, the user will be informed that the combined pdf is too large.\\nWe will not attempt to handle the case where the combined pdf is too large, but will instead exit the application.\\nWe will only attempt to close the combined pdf if the maximum size has been exceeded.\\nWe will only attempt to close the combined pdf if the combined pdf is not a pdf.\\nWe will not attempt to attempt to combine pdfs with a name that already exists in the database.\\nWe will not attempt to attempt to combine pdfs with a name that is too long.\\nWe will not attempt to attempt to attempt to combine pdfs that contain a certain number of pages.\\nWe will not attempt to attempt to attempt to attempt to combine pdfs that contain a certain number"}
{"Context":"## Context\\nOne of the core components in Kyma is the API Gateway. This DR focuses on [Tr\u00e6fik](https:\/\/traefik.io), one of the candidates for an API Gateway.\\nTr\u00e6fik is an HTTP reverse proxy shipped with load-balancing, rate-limiting, monitoring, and several other useful features. In comparison to the other reverse proxies, Traefik stands out with its dynamic configuration abilities.\\nTr\u00e6fik supports the most common orchestration backends (for example: Docker, Kubernetes, Mesos, or Consul). It is written in Go. The evaluated version of Traefik is 1.4.0 [rc4].\\n","Decision":"## Decision\nDuring our evaluation process, it turned out that Traefik does not support one of the key features required in Kyma - JWT based authentication. Throughout the implementation, the lack of extensibility capabilities was also noticed. There is absolutely no notion of custom middlewares so introduction of additional features (like JWT-based authentication middleware) requires direct changes in the source code.\\nAdvantages:\\n- few dependencies\\n- dynamic configuration\\n- Kubernetes-based orchestration\\n- built in circuit breaker\\n- rate limiter\\nDisadvantages:\\n- poor extensibility\\nAlthough Traefik has a lot of interesting features, due to the poor extensibility capabilities, the decision is to not use it as an API Gateway in Kyma.\\n","Prediction":"## Decision\nWe will use [Tr\u00e6fik](https:\/\/traefik.io) for the API Gateway.\\nWe will use [Kyma](https:\/\/kyma.io) for configuration.\\nWe will use [Kyma-Cloud](https:\/\/kyma.io\/infra\/kyma-cloud) for load-balancing.\\nWe will use [Kyma-Cloud-LoadBalancer](https:\/\/kyma.io\/infra\/kyma-cloud-load-balancer) for application load-balancing.\\nWe will use [Kyma-Cloud-Route53](https:\/\/kyma.io\/infra\/kyma-cloud-route53) for Route53 load-balancing.\\nWe will use [Kyma-Cloud-LoadBalancer-Route53](https:\/\/kyma.io\/infra\/kyma-cloud-route53-load-balancer) for Route53 load-balancing.\\nWe will use [Kyma-Cloud-Route53-LoadBalancer](https:\/\/kyma.io\/infra\/kyma-cloud-route53-load-balancer-route53) for Route53 load-balancing.\\nWe will use [Kyma-Cloud-Route53-LoadBalancer](https:\/\/kyma.io\/infra\/k"}
{"Context":"## Context\\nIt is becoming more common to recognise that a specific format of a resource is the combination of content type AND version.\\n","Decision":"## Decision\nFor the reasons explored in [2. API versioning strategy](0002-api-versioning-strategy.md),\\n* We will implement versioning via Content Negotiation using the Accept header, as per option 5, below. This seems the most future proof, most RESTful solution.\\nThis necessitates our own vendor content type, which we will call `opg-data`\\n`application\/vnd.opg-data.[version]+[representation]`\\nRepresentation will default to `json` and version will default to `latest`\\nSome Examples:\\n* `application\/vnd.opg-data.v1+json` (v1 presented as JSON)\\n* `application\/vnd.opg-data.v1+yml` (v1 presented as YAML)\\n* `application\/vnd.opg-data.v1` (v1 presented as JSON)\\n* `application\/vnd.opg-data` (latest version, presented as JSON)\\n* `application\/json` (latest version, as JSON)\\n","Prediction":"## Decision\nThe resource will be represented as a combination of a content type and the version.\\nThis is the same as the [REST API](https:\/\/dev.twitter.com\/docs\/api\/1.1) for Twitter.\\nThis is also consistent with the [REST API](https:\/\/dev.twitter.com\/docs\/api\/1.1) for LinkedIn.\\nThis is also consistent with the [REST API](https:\/\/dev.twitter.com\/docs\/api\/1.1) for Facebook.\\nThis is also consistent with the [REST API](https:\/\/dev.twitter.com\/docs\/api\/1.1) for Google.\\nThis is also consistent with the [REST API](https:\/\/dev.twitter.com\/docs\/api\/1.1) for Instagram.\\nThis is also consistent with the [REST API](https:\/\/dev.twitter.com\/docs\/api\/1.1) for Pinterest.\\n### Content Types\\nContent types will be the combination of the following:\\n* `application\/vnd.github.v1+json` - a JSON representation of the resource\\n* `application\/vnd.github.v2+json` - a JSON representation of the resource with additional fields\\n* `application\/vnd.github.v3+json` - a JSON representation of the resource with additional fields\\n* `application\/vnd.github"}
{"Context":"## Context\\nWe need a way to describe the structure of Javascript objects for readers of the code to be able to understand the structure.\\nAnd we need a way to detect null references at build time Javascript.\\nOur Javascript passes though a build step.\\nOur Javascript is maintained by people who did not write the code.\\n","Decision":"## Decision\nWe will use Typescript as a tool to describe object structure.\\n","Prediction":"## Decision\nWe will use the [JavaScript Object Notation](https:\/\/en.wikipedia.org\/wiki\/JavaScript_Object_Notation) format for describing the structure of our Javascript objects.\\nWe will use the [JavaScriptSerializer](https:\/\/docs.microsoft.com\/en-us\/dotnet\/api\/system.object.serialization.json?) library to serialize our Javascript objects.\\nWe will use the [JavaScriptSerializer](https:\/\/docs.microsoft.com\/en-us\/dotnet\/api\/system.object.serialization.json?) library to deserialize our Javascript objects.\\nWe will use the [JavaScriptSerializer](https:\/\/docs.microsoft.com\/en-us\/dotnet\/api\/system.object.serialization.json?) library to validate the structure of the objects.\\nWe will use the [JavaScriptSerializer](https:\/\/docs.microsoft.com\/en-us\/dotnet\/api\/system.object.serialization.json?) library to validate the structure of the objects.\\nWe will use the [JavaScriptSerializer](https:\/\/docs.microsoft.com\/en-us\/dotnet\/api\/system.object.serialization.json?) library to validate the structure of the objects.\\nWe will use the [JavaScriptSerializer](https:\/\/docs.microsoft.com\/en-us\/dotnet\/api\/system.object.serialization.json?) library to validate the structure of the objects.\\nWe will use the [JavaScriptSerializer](https:\/\/docs.microsoft.com\/en"}
{"Context":"## Context\\nIn order to run webapps within a fully private and secure Application Service Environment we needed to come up with a DNS solution so that when web applications are put into service these should be discoverable and available for consumption by other INTERNAL services. The \"Internal\" part of the description is fundamental within this context and it defines the scope of this proposal. In this context we are also assuming that each web application will have it's own DNS name, which will be dynamically generated by convention meanind DNS records will need to be updated dinamically.\\n","Decision":"## Decision\nAlthough DNS offerings like BIND, unbound or dnsmasq allow us to provide name resolution services for our problem, we felt that none of them provided options good enough to update name records dynamically out of the box. As a solution for this particular challenge we have decided to start leveraging Consul's DNS interface, which allows applications to make use of service discovery without any high-touch integration with Consul.\\nFor our particular case we are proposing to have DNS service lookups directed to Internal domains to be forwarded from a current\/new DNS server (i.e BIND or Dnsmasq) to our Consul service. For example a host can use the DNS server directly via name lookups like `rhubarb-frontend.service.internaldomain`. This query automatically translates to a lookup of the IP that provide the rhubarb-frontend service, are located in the <internal domain> ASE. The following drawing give us a high level overview of the proposed solution:\\n![Internal DNS](..\/..\/img\/internal-dns-proposal.png)\\nThis approach give us a number of advantages including simplification of the DNS server configuration (a single config entry can forward all queries about internaldomain to our consul service) or reusing existing DNS infrastructure (i.e. Support for BIND, Dnsmasq, Unbound, etc ...), but more importantly it allow us to update DNS records with very simple API calls as demonstrated below.\\nConsider an example setup where a Dnsmasq server is running on `10.0.1.4` and for demonstration purposes the Consul server is running on the same node. Consul is listening on tcp port 8500 for app registration and tcp port 8600 for DNS queries forwarded from Dnsmasq, we can then configure dnsmasq to forward all lookup queries for domain `internal` to consul:\\n```code\\n[dsanabria@dnsserver ~]$ cat \/etc\/dnsmasq.d\/10-consul\\nserver=\/internal\/127.0.0.1#8600\\n[dsanabria@dnsserver ~]$\\n```\\nIf a new service wants to register to our DNS system this can be achieved with a very simple API call as shown in the following snippet:\\n```code\\n[dsanabria@dnsserver ~]$ host mojwaftest-dev.service.internal\\nHost mojwaftest-dev.service.internal.3ipn3ped5wke5cbmc154jrnarb.zx.internal.cloudapp.net not found: 5(REFUSED)\\n[dsanabria@dnsserver ~]$ cat service.json\\n{\\n\"ID\": \"mojwaftest-dev\",\\n\"Name\": \"mojwaftest-dev\",\\n\"Tags\": [],\\n\"Address\": \"10.0.4.9\",\\n\"Port\": 443\\n}\\n[dsanabria@dnsserver ~]$ curl --request PUT --data @service.json http:\/\/localhost:8500\/v1\/agent\/service\/register\\n[dsanabria@dnsserver ~]$ host mojwaftest-dev.service.internal\\nmojwaftest-dev.service.internal has address 10.0.4.9\\n[dsanabria@dnsserver ~]$\\n```\\nAnother advantage of embracing consul is that it lead us to start adopting best naming practices by following specific methods for service lookups. In consul service queries support two lookup methods: standard and strict RFC 2782, for our use case we are recommending the \"standard\" service lookup method.\\nThe format of a standard service lookup is:\\n```code\\n[tag.]<service>.service[.datacenter].<domain>\\n```\\nThe tag and datacenter parts are optional but we can see, however, that the service part is static and can't be changed. At first this can be seen as a limitation on how services are named at provisioning time but by leveraging Azure's webapp custom domains we can implement naming patterns driven by good industry practices.\\nAll our webapp based services can easily follow these conventions since we, as operators, have good control over their settings, but the Azure Source Control Management (SCM) is an exception to this rule mainly because we have very little control over its naming conventions (i.e. Azure force us to use `<service name>.scm.<internal domain>`. In order to comply with both, Industry best practices and Azure's own conventions, we can register SCM as a service in consul and update its tags when services are registered. The following cli session illustrates how this can be accomplished:\\n```code\\n[dsanabria@dnsserver ~]$ host mojwaftest-dev.scm.service.internal\\nHost mojwaftest-dev.scm.service.internal.3ipn3ped5wke5cbmc154jrnarb.zx.internal.cloudapp.net not found: 5(REFUSED)\\n[dsanabria@dnsserver ~]$ cat scm.json\\n{\\n\"ID\": \"scm\",\\n\"Name\": \"scm\",\\n\"Tags\": [\"mojwaftest-dev\"],\\n\"Address\": \"10.0.4.9\",\\n\"Port\": 443\\n}\\n[dsanabria@dnsserver ~]$ curl --request PUT --data @scm.json http:\/\/localhost:8500\/v1\/agent\/service\/register\\n[dsanabria@dnsserver ~]$ host mojwaftest-dev.scm.service.internal\\nmojwaftest-dev.scm.service.internal has address 10.0.4.9\\n[dsanabria@dnsserver ~]$\\n```\\n","Prediction":"## Decision\nWe will implement a DNS service that will be managed by the ServiceNow Application Platform and that will be used to manage the DNS records of the web application. This DNS service will be provided as a managed service within the Application Platform and will be managed by the Application Platform. We will also create a DNS zone in the AWS Route 53 service for the application and will configure this zone to point to the DNS service. We will also create a DNS record in Route 53 that points to the DNS service to enable the DNS service to be used to manage the DNS records of the web application.\\nWe will also create a DNS zone in AWS Route 53 that will be used to manage the DNS records of the web application. This DNS zone will be created by Route 53 and will be managed by the AWS Route 53 service. We will also create a DNS record in Route 53 that will point to the DNS service to enable the DNS service to be used to manage the DNS records of the web application.\\nWe will also create a DNS zone in AWS Route 53 that will be used to manage the DNS records of the web application. This DNS zone will be created by Route 53 and will be managed by the AWS Route 53 service. We"}
{"Context":"## Context\\nWe need dir to save ADRs into.\\n","Decision":"## Decision\nWe will save ADRs into top level `adr` dir. Another option was to put them into `docs\/adr`\\nalong with Sphinx docs, but they might be bit hidden there.\\n","Prediction":"## Decision\nWe will use the [Dir](https:\/\/github.com\/alexandravdovich\/dir) package to save ADRs into.\\nIt's a [node package](https:\/\/nodejs.org\/en\/download\/) that comes with a [node.js module](https:\/\/nodejs.org\/en\/docs\/v7.0.0\/).\\nIt's a [node package](https:\/\/nodejs.org\/en\/download\/) that comes with a [node.js module](https:\/\/nodejs.org\/en\/docs\/v7.0.0\/).\\nIt's a [node package](https:\/\/nodejs.org\/en\/download\/) that comes with a [node.js module](https:\/\/nodejs.org\/en\/docs\/v7.0.0\/).\\nIt's a [node package](https:\/\/nodejs.org\/en\/download\/) that comes with a [node.js module](https:\/\/nodejs.org\/en\/docs\/v7.0.0\/).\\n### Installation\\nInstall the `Dir` package using `npm`:\\n```js\\nnpm install -g dir\\n```\\n### Usage\\n```js\\nconst dir = require('dir');\\nconst dir = dir(dir.resolve(__dirname));\\n```\\n### Example\\n```js\\nconst dir"}
{"Context":"## Context\\nWe are an open source project and as a result can possibly accept outside contribution to our codebase. To this end, we need to communicate our requirements for the code that is submitted and our process for acceptance of this code. The end goal for the rewrite is to be more modular which we believe will enable both the team and outside contributors flexibility and ease of adding enhanced features.\\n","Decision":"## Decision\nWe will add a contributing guide to our documentation files. Once available, Github will show links to this guide automatically to any outside user visiting the front-end-monorepo repository. Any outside contributions must follow this guide to submit code for review.\\n","Prediction":"## Decision\nWe will use [Jira](https:\/\/www.atlassian.com\/software\/jira) to document our requirements and process for the rewrite. Requirements will be documented as acceptance criteria. Once all the criteria have been met, the code will be accepted into the codebase.\\nWe will require that the following features be included in the rewritten code\\n* Modular code\\n* Unit testing\\n* Code readability\\n* Consistent naming\\n* Code style\\n* Code conventions\\n* Code formatting\\n* Code guidelines\\n* Code tests\\n* Code coverage\\n* Code coverage tooling (e.g. [Jenkins](https:\/\/plugins.jenkins.io\/))\\n* Code quality tooling (e.g. [Sonar](https:\/\/sonarqube.org\/))\\n* Code review\\n* Code review guidelines\\n* Code review tooling (e.g. [SonarLint](https:\/\/sonarlint.com\/))\\n* Code review tooling (e.g. [Prettier](https:\/\/prettier.io\/))\\n* Code review tooling (e.g. [eslint](https:\/\/eslint.org\/))\\n* Code review tooling (e.g. [eslint-plugin](https:\/\/github.com\/eslint\/es"}
{"Context":"## Context\\nWe will be developing new code and applications during the build of the Data\\nSubmission Service.\\nIt is good practice to use version control to allow changes to be tracked over\\ntime.\\nThe Digital Service Standard [recommends][service-manual-version-control] the\\nuse of version control for government services. It also recommends\\n[making all new source code open][service-standard-point-8].\\nCrown Commercial Service already uses Git for some systems, and has a\\n[GitHub][ccs-github] account.\\n","Decision":"## Decision\nWe will use Git and GitHub for storing our code.\\nWe will use public repositories by default, unless there is a convincing reason\\nnot to (which must be documented in the README of the private repository).\\nWe will use the main [CCS GitHub][ccs-github] account and use their standards\\nfor user account management.\\n","Prediction":"## Decision\nWe will use Git for version control of software that we develop.\\nGit will be used as the version control system for all new code and\\napplications on the Data Submission Service.\\nWe will make all new source code open in GitHub (see Service Standard\\n[point 8]).\\nWe will commit code to GitHub every time we make a change to the codebase.\\nWe will use the following naming convention for new code:\\n* code-name: version number\\n* version: 1.0.0\\n* minor: 1\\n* patch: 2\\n* major: 3\\n* patch: 4\\n* major: 5\\n* minor: 6\\n* patch: 7\\n* patch: 8\\n* minor: 9\\n* patch: 10\\n* minor: 11\\n* minor: 12\\n* minor: 13\\n* minor: 14\\n* minor: 15\\n* minor: 16\\n* minor: 17\\n* minor: 18\\n* minor: 19\\n* minor: 20\\n* minor: 21\\n* minor: 22\\n* minor: 23\\n* minor: 24\\n* minor:"}
{"Context":"* [Context](#context)\\n* [Decision](#decision)\\n* [Status](#status)\\n* [Consequences](#consequences)\\n* [More reading](#more-reading)\\n* [Updates](#updates)\\n","Decision":"## Decision\n* [Status](#status)\\n* [Consequences](#consequences)\\n* [More reading](#more-reading)\\n* [Updates](#updates)\\nWe've decided to use [Storybook](https:\/\/storybook.js.org\/). This tool helps us document our components and develop them since we can make a simple canvas to interact with it.\\nSome major companies with mature design systems use this tool, which could be a precious asset.\\n","Prediction":"## Decision\n* [Status](#status)\\n* [Consequences](#consequences)\\n* [More reading](#more-reading)\\n* [Updates](#updates)\\nWe will use `react-redux` to handle the Redux store.\\nThis is the recommended library for Redux, because it does a lot of the heavy lifting for you.\\nIt's also very popular, and the team is actively working on it.\\nIt's still in beta, but it's very stable and very well supported.\\n### Benefits of using Redux\\n* It's already implemented in most of the popular React applications.\\n* It's easy to use and learn.\\n* It's backed by the Redux team.\\n### Consequences of using Redux\\n* The state management is not as good as Redux, but it's still pretty good.\\n* It's a little slower than Redux, because you have to build up the store in memory.\\n### More reading\\n* [Redux docs](https:\/\/redux.js.org\/)\\n* [Redux docs](https:\/\/redux.js.org\/)\\n* [Redux docs](https:\/\/redux.js.org\/)\\n* [Redux docs](https:\/\/redux.js.org\/)\\n* [Redux docs]("}
{"Context":"## Context and Problem Statement\\nTax users are sometimes, without warning, unnecessarily signed out when accessing\\nMDTP services using multiple browser tabs or windows. This is a violation of [WCAG 2.1\\nsuccess criterion 2.1.1 (Timing adjustable)](https:\/\/www.w3.org\/WAI\/WCAG21\/Understanding\/timing-adjustable.html).\\nThis problem is a consequence of the fact that a user's session can be refreshed via user activity in any\\ntab or window belonging to the same browser profile. However, the Javascript-powered [hmrcTimeoutDialog](https:\/\/github.com\/hmrc\/play-frontend-hmrc#warning-users-before-timing-them-out)\\ncurrently has no way of knowing about this activity following initial page load.\\nSolving this issue requires providing the timeout dialog component with knowledge of the actual time\\nremaining on the user's active session via an endpoint that is itself excluded from\\nsession management.\\nHow can we achieve this cost-effectively while minimising impact for service teams, limiting duplication of\\nknowledge and avoiding introducing additional coupling between frontend microservices?\\n## Decision Drivers\\n* The need to minimise code changes for service teams other than a library upgrade.\\n* The avoidance of requiring service teams to add any additional routing rules.\\n* The avoidance of requiring service teams to add any additional configuration.\\n* The need to limit duplication of knowledge between SessionTimeoutFilter in bootstrap-play and\\nany other library or service.\\n* The preference for maintaining loose coupling of frontend services and avoiding adding single points\\nof failure between frontend microservices.\\n* The need for the endpoint used to interrogate the session to not itself affect the session.\\n","Decision":"## Decision\n* The need to minimise code changes for service teams other than a library upgrade.\\n* The avoidance of requiring service teams to add any additional routing rules.\\n* The avoidance of requiring service teams to add any additional configuration.\\n* The need to limit duplication of knowledge between SessionTimeoutFilter in bootstrap-play and\\nany other library or service.\\n* The preference for maintaining loose coupling of frontend services and avoiding adding single points\\nof failure between frontend microservices.\\n* The need for the endpoint used to interrogate the session to not itself affect the session.\\nChosen option: option 9, because it is an option PlatUI and PlatOps agree on, is technically feasible,\\nsatisfies most of the decision drivers and is the smallest possible change with the least impact to service teams. The intention would be\\nto revisit option 7 (play-session) at a later date in order to address any outstanding concerns\\naround knowledge duplication.\\n","Prediction":"## Decision\n* The need to minimise code changes for service teams other than a library upgrade.\\n* The avoidance of requiring service teams to add any additional routing rules.\\n* The avoidance of requiring service teams to add any additional configuration.\\n* The need to limit duplication of knowledge between SessionTimeoutFilter in bootstrap-play and\\nany other library or service.\\n* The preference for maintaining loose coupling of frontend services and avoiding adding single points\\nof failure between frontend microservices.\\n* The need for the endpoint used to interrogate the session to not itself affect the session.\\nChosen option: \"Allow service teams to configure the timeout dialog via the `apmTimeoutDialog` endpoint\\nand then use this to limit the number of times a user is signed out of a session\", because the\\napproach described above requires no changes to the `apmTimeoutDialog` endpoint.\\n### Positive Consequences\\n* Service teams can configure the timeout dialog via an endpoint that is not exposed to\\nother services.\\n* Service teams can configure the timeout dialog to limit the number of times a user is\\nsignaled out of a session.\\n### Negative Consequences\\n* Service teams will need to configure the timeout dialog via an endpoint. This limits\\nthe flexibility"}
{"Context":"## Context\\nDifferent tenant applications have differing ingress and egress requirements. Examples include:\\n* mutual TLS\\n* egress controls (e.g. limiting what can egress from a namespace \/ cluster)\\n* certificate operations (renewal etc.)\\nThere is also the vision of the GSP that provides a common platform for all tenants based off a shared codebase. This platform allows suitable customisations to be suitable for the tenants running on it and, where possible, those customisations are controlled via kube-yaml.\\nAt present, a single ingress gateway and egress gateway are defined by the GSP. In the case of the ingress gateway, all ingress traffic is routed through this which creates a range of difficulties:\\n* secrets relating to TLS certs (mutual or otherwise) have to be stored in `istio-system`\\n* having some endpoints use mutual TLS and others using non-mutual TLS is difficult\/awkward\/impossible on a single gateway\\n* a gateway is able to load certificates from a single secret (by default) so different applications in different namespaces will inevitably conflict\\n* different tenants in the same cluster will need to make changes to common \"system\" parts of the code (i.e. `istio-system`)\\n* controlling DNS entries and TLS configuration per tenant is difficult\/awkward\/impossible\\n* DNS is currently controlled via terraform in the GSP codebase (a single \"*.\" record for the domain for the cluster provided as a convenience).\\n* TLS is currently terminated outside the cluter in an Application Load Balancer (ALB) in AWS, which doesn't suit all tenants\\nOther, indirect features:\\n* istio routing rules for single gateways in `istio-system` are:\\n* difficult to reason about and become quite brittle as a result\\n* don't align very well with istio's documentation\\nOptions\\n1. \"Snowflake\" load balancers and DNS records in tenant-specific terraform, bypass the `istio-system` ingress gateway as needed\\nPros:\\n* Quick\\n* Relatively easy & familar\\nCons:\\n* Custom terraform for a tenant for something likely to be common (e.g. managing TLS termination and DNS ingress)\\n* Doesn't make istio routing easier to work with\\n* Doesn't improve certificate management (provisioning, renewing etc.)\\n1. Allow tenants to configure multiple secrets on the `istio-system` ingress gateways\\nPros:\\n* Addresses the single-secret certificate problem\\nCons:\\n* Tenants need to know about gateway configuration\\n* Tenants still manipulating shared namespaces & resources\\n* Doesn't make istio routing easier to work with\\n* Doesn't improve certificate management (provisioning, renewing etc.)\\n1. Cert-manager and external-dns with single `istio-system` gateways\\nPros:\\n* Certificate management\\n* DNS management (potentially)\\nCons:\\n* Tenants still manipulating shared namespaces & resources\\n* Doesn't make istio routing easier to work with\\n1. Per-namespace gateways with cert-manager and external-dns\\nPros:\\n* Certificate management\\n* DNS management\\n* Ingress control\\n* Improves istio routing management (as it aligns more with the docs)\\n* Alows tenants direct control (hence it's more inline with the vision)\\nCons:\\n* A complex change\\n","Decision":"## Decision\nWe will create an ingress gateway and an egress gateway in each tenant (\"managed\") namespace (option 4 above).\\n","Prediction":"## Decision\nPer-namespace gateways with cert-manager and external-dns\\nPros:\\n* Certificate management\\n* DNS management\\n* Ingress control\\n* Improves istio routing management (as it aligns more with the docs)\\n* Alows tenants direct control (hence it's more inline with the vision)\\nCons:\\n* A complex change\\nWe will use cert-manager and external-dns to create per-namespace load balancers for tenants. Cert-manager will be used to provision a single load balancer per namespace, which will be configured with a single secret in the `istio-system` ingress gateway. External-dns will be used to provision an external load balancer for each namespace and will be configured with a single secret in an external DNS provider (e.g. Route53). This will allow tenants to configure their load balancers in a similar way to how they configure a load balancer in the GSP.\\nThis approach will allow tenants to configure their own load balancers without having to modify the GSP or the ingress gateway. It also allows tenants to use their own DNS providers as they see fit, without needing to modify the ingress gateway. A tenant's own DNS provider will be used to provide external DNS for the load"}
{"Context":"## Context\\nAlthough Kubernetes manages service accounts, the solution leaves the management of regular users to an independent, outside service.\\nFor this reason, Kyma needs an authentication provider for end-users.\\nKubernetes has the following requirements for authentication providers:\\n- The authentication provider must support the [OpenID connect discovery](https:\/\/openid.net\/specs\/openid-connect-discovery-1_0.html).\\n- The authentication provider must run in TLS with non-obsolete ciphers.\\n- The authentication provider must support CA-signed certificates, even for commercial and self-signed CAs.\\nThe solution must also fulfill the general platform requirements:\\n- It must be a cloud-native solution.\\n- It must be lightweight, preferably written in Go.\\n- It must be open-source, with the ability to contribute to the project development.\\nAdditionally, the solution must allow to delegate authentication to SAML2-compliant Identity Providers.\\n","Decision":"## Decision\nThe decision is to use [Dex](https:\/\/github.com\/coreos\/dex) as an authentication provider for end-users in Kyma.\\n","Prediction":"## Decision\nWe will use [Kibana](https:\/\/www.kibana.org\/) as a standalone authentication provider to provide end-user authentication.\\nKibana will be used as the authentication provider for all cloud-native services that need to authenticate end-users.\\nKibana will be the primary authentication provider for the Kyma Cloud.\\nKibana will be used as the primary authentication provider for the Kyma Service Dashboard.\\nKibana will be used as the primary authentication provider for the Kyma Cloud Dashboard.\\nKibana will be used as the primary authentication provider for the Kyma Service Account and Service Provider UI.\\nKibana will be used as the primary authentication provider for the Kyma Service Account and Service Provider UI.\\nKibana will be used as the primary authentication provider for the Kyma Service Account and Service Provider UI.\\nKibana will be used as the primary authentication provider for the Kyma Service Account and Service Provider UI.\\nKibana will be used as the primary authentication provider for the Kyma Service Account and Service Provider UI.\\nKibana will be used as the primary authentication provider for the Kyma Service Account and Service Provider UI.\\nK"}
{"Context":"Context\\n-------\\nWe have a number of applications and servers that need numerous Perl libraries\\ninstalled. We have historically managed the libraries at a system level and\\ncoordinated versions with application teams. While there are now compelling\\noptions like [Carton](https:\/\/metacpan.org\/pod\/Carton), which allow an\\nappliction to manage and isolate its dependencies, most of our apps would\\nrequire changes to be ready to use something like this.\\nTo aid in keeping the versions stable and ensuring that underlying system\\nlibraries like OpenSSL are in sync, Debian packages many CPAN libraries at\\nspecific versions for a release. These are still at a system level, but we\\nhave an already-running process that would catch updates in the case of a\\nmajor bug fix or security release.\\nIn service of completing OS and hardware upgrades, we are seeking primarily to\\nidentify the dependencies and to be able to reproduce our application\\nenvironments. In the long term, applications are likely to be containerized or\\notherwise placed under PSGI\/Plack, where many of these concerns could be\\nisolated more readily. This decision is a record of current realities, rather\\nthan a long-term position.\\nDecision\\n--------\\nWhere possible, when installing Perl libraries at the system level, we will\\nuse the Debian-released package. Where practical, we will use\\n[dh-make-perl](https:\/\/manpages.debian.org\/stretch\/dh-make-perl\/dh-make-perl.1p.en.html)\\nto create local .deb packages from those in CPAN, but not released by Debian.\\nWe will use `nebula::cpan` as a last resort.\\nConseqeuences\\n-------------\\nUsing Debian-released libraries means that we are limited to the versions\\nselected in their process. This could be a stabilizing or limiting force.\\nProducing our own packages requires additional maintenance and monitoring.\\nUsing bare CPAN installs could result in different versions on different\\nsystems, depending on when the installation happens.\\n","Decision":"## Decision\n--------\\nWhere possible, when installing Perl libraries at the system level, we will\\nuse the Debian-released package. Where practical, we will use\\n[dh-make-perl](https:\/\/manpages.debian.org\/stretch\/dh-make-perl\/dh-make-perl.1p.en.html)\\nto create local .deb packages from those in CPAN, but not released by Debian.\\nWe will use `nebula::cpan` as a last resort.\\nConseqeuences\\n-------------\\nUsing Debian-released libraries means that we are limited to the versions\\nselected in their process. This could be a stabilizing or limiting force.\\nProducing our own packages requires additional maintenance and monitoring.\\nUsing bare CPAN installs could result in different versions on different\\nsystems, depending on when the installation happens.\\n","Prediction":"## Decision\n--------\\nWhere possible, when installing Perl libraries at the system level, we will use the Debian-released package. Where practical, we will use\\n[dh-make-perl](https:\/\/manpages.debian.org\/stretch\/dh-make-perl\/dh-make-perl.1p.en.html)\\nto create local .deb packages from those in CPAN, but not released by Debian.\\nWe will use `nebula::cpan` as a last resort.\\nConseqeuences\\n-------------\\nUsing Debian-released libraries means that we are limited to the versions\\nselected in their process. This could be a stabilizing or limiting force.\\nProducing our own packages requires additional maintenance and monitoring.\\nUsing bare CPAN installs could result in different versions on different\\nsystems, depending on when the installation happens.\\nDecision\\n--------\\nWhere possible, when installing Perl libraries at the system level, we will use the Debian-released package. Where practical, we will use\\n[dh-make-perl](https:\/\/manpages.debian.org\/stretch\/dh-make-perl\/dh-make-perl.1p.en.html)\\nto create local .deb packages from those in CPAN, but not released by Debian.\\nWe"}
{"Context":"## Context\\nThe custom AMF validator needs to show the location of the file from which each error was generated.\\nGiven the current state of the amf model and emission of jsonld, there was no way to obtain the location of a specific node.\\n","Decision":"## Decision\nA new node was defined as a field in BaseUnit call BaseUnitSourceInformation, which has the necessary information to obtain the source location of any node.\\nInternally, this node has two fields, one that stores the root location, and another that stores LocationInformation nodes which contain alternative locations with the ids of all the elements parsed from that location.\\nA new render option was included making the emission of this node to jsonld optional and not activated by default.\\nAn alternative solution was to serialize SourceLocation annotation in each node, but this leads to a 25% or more increase in size of the resulting jsonld, as the paths are stored in a redundant fashion.\\n","Prediction":"## Decision\nThe validator will use the jsonld to get the location of the file that caused the error.\\nThis is also the only way to get the location of a specific file.\\nThis avoids having to duplicate the current code to get the location of each file.\\nThis is also a standard way of getting the location of a file, as it is used by the jsonld.\\nThis will make it easier to debug the code and to understand the code.\\nThis also means that the validator will not be dependent on the amf model to get the location of a specific file.\\nThis will allow the validator to be extended easily in the future.\\n### Positive Consequences\\n- The validator will be able to get the location of a specific file from the jsonld.\\n- The validator will not depend on the amf model to get the location of a specific file.\\n### Negative Consequences\\n- The validator will be dependent on the amf model to get the location of a specific file.\\n- The validator will have a dependency on the amf model.\\n- The validator will be dependent on the amf model to get the location of a specific file.\\n- The validator will be dependent on a file that is not part of"}
{"Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","Prediction":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\nWe will use [Stash](https:\/\/stash.rethinkproject.org\/)\\nfor version control and [Hipchat](https:\/\/github.com\/hipchat\/hipchat) for chat.\\nWe will use [JIRA](https:\/\/www.atlassian.com\/software\/jira) for bug tracking.\\nWe will use [CircleCI](https:\/\/circleci.com\/)\\nfor continuous integration and deployment.\\nWe will use [Google Cloud Storage](https:\/\/cloud.google.com\/storage\/), and will use the [Google Cloud IAM role](https:\/\/cloud.google.com\/iam\/role\/iam-role\/iam-read-write) to grant the necessary access.\\nWe will use the [Google Cloud Dataprep](https:\/\/cloud.google.com\/dataprep\/) service for data scrubbing and analysis.\\nWe will use the [Google Cloud Endpoint](https:\/\/cloud.google.com\/endpoints\/) service for identity management.\\nWe will use the [Google Cloud Datastore](https:\/\/cloud.google.com\/datastore) service for storing application data.\\nWe will use"}
{"Context":"## Context\\nLibero's products are assembled from product specific, general libero and external container images.\\nIntegration testing and reproducible deploys require uniquely identifiable image tags.\\nAs a starting point for users each product has an umbrella repo. This is not only used to collect product issues and documents. I also contains e.g. docker-compose files, helm charts that combine various components to a useable whole.\\nTo automatically update images in these files tools like [dependabot](https:\/\/dependabot.com\/), [renovate](https:\/\/docs.renovatebot.com\/docker\/) and [flux](https:\/\/github.com\/fluxcd\/flux\/). These tools need way to determine which images to update, when to do so and whether human interaction is required. [Semantic Versioning](https:\/\/semver.org\/) is ideally suited for this, but not all projects\/components have\/will implement automated semver from the start.\\nTherefor  this ADR also defines branch name, git hash and timestamp based tags. Branch names let us track specific branches. Hashes allow for unique identification. Timestamps are needed by tools like renovate that require some form of numeric versioning.\\nContainer images always have a digest hash that uniquely identifies their build and is independent from git hashes, git tags and image tags.\\nThe image associated with an image tag is allowed to change. This is useful for tracking semver image tags in deploys, but results in the image digest being required for pinning etc.\\nThis ADR does not define how the image tags are used for integration tests or deploys. Nor does it define the consequences of their test results. But by defining a tagging convention is allows us to develop these strategies.\\n","Decision":"## Decision\n### Tagging\\n- libero container images shall __always__ be tagged with both:\\n- `${branch}-${short_git_hash}-${timestamp}`\\n- `${branch}-${short_git_hash}`\\n- `timestamp` format is `%Y%m%d.%H%M`\\n- images shall receive semver tags when the git repo receives them\\n- semver tagging edits\/creates three tags i.e.:\\n- image gets tagged `1.2.43`,  `1.2` and `1`\\n- images previously associated with major\/minor tag loose this tag\\n- (see example below)\\n- git tags shall consists of the version preceeded by a `v`\\ne.g. `refs\/tags\/v1.2.43`\\n- only images from `master` branch shall be pushed as `latest`\\nExamples:\\n```sh\\nliberoadmin\/reviewer-client:master-${short_git_hash}-${timestamp}\\nliberoadmin\/reviewer-client:master-${short_git_hash}\\nliberoadmin\/reviewer-client:develop-${short_git_hash}-${timestamp}\\nliberoadmin\/reviewer-client:develop-${short_git_hash}\\nliberoadmin\/reviewer-client:1.2.43\\n```\\nExample Workflow:\\n1. head of `libero\/reviewer-client` `master` moves\\nthe hash is `2d57c085`\\n1. new container image gets built\\n2. image gets pushed to registry as\\n`liberoadmin\/reviewer-client:master-2d57c085-20200512.0834` __and__\\n`liberoadmin\/reviewer-client:master-2d57c085`\\nits short digest is: `dd75d2e1bbfc`\\n3. same image gets tagged and pushed as `latest`\\n2. `master` gets tagged as `v1.2.43`\\n3. image `dd75d2e1bbfc` receives additional tags\\n- `liberoadmin\/reviewer-client:1.2.43`\\n- `liberoadmin\/reviewer-client:1.2`\\n- `liberoadmin\/reviewer-client:1`\\n","Prediction":"## Decision\nThe image tags in image repositories will be based on the [semantic versioning](https:\/\/semver.org\/) convention. Each version of a component is assigned a unique tag. This tag is incremented by one at each major version of the component.\\nWhen the component is updated, the image tag is incremented by one. The image will not be updated if the image tag is already incremented. An image tag can be incremented multiple times for different major versions of the component, but not multiple times in the same major version. This is to prevent the image being used as a tag for multiple components at the same time.\\nWhen a new version is released, the image tag is incremented by one. An image is then pulled from the image repository and the image tag is incremented by one. If the image is already incremented, the image is replaced with the new image. If the image is not yet incremented, the image is added to the image repository. This ensures that the image can be pulled from the repository when updating the image tag so the image can be reused when updating the image tag.\\nIf an image is already incremented in the repository, it will be ignored when pulling from the repository. This means that images can be pulled from the public image repository without being"}
{"Context":"## Context\\nDuring some refactoring of our static file versioning code, there existed several issues which caused static files to be served incorrectly.\\nThese errors propagated in several different ways across the site, but the root issue was always that there was a bug around how JS files were being served.\\nFull context can be found in the following PRs:\\n- Initial refactoring: https:\/\/github.com\/dimagi\/commcare-hq\/pull\/24938\\n- Failed attempts at fixing `force_update_static`:\\n- https:\/\/github.com\/dimagi\/commcare-cloud\/pull\/3148\\n- https:\/\/github.com\/dimagi\/commcare-cloud\/pull\/3186\\n- Removal of `hotfix_deploy` and `force_update_static`: https:\/\/github.com\/dimagi\/commcare-cloud\/pull\/3204\\n- Simplification of the deploy static file process: https:\/\/github.com\/dimagi\/commcare-hq\/pull\/25284\\n","Decision":"## Decision\nWe are removing the previous commands `hotfix_deploy` and `force_update_static`.\\nAny future deploys must go through `commcare-cloud <env> deploy`\\n","Prediction":"## Decision\nAfter discussing the issues with the team, we decided that the best course of action was to remove the `hotfix_deploy` and `force_update_static` code from the static file versioning code.\\nInstead, we will use the following code:\\n- A static file versioning backend, which will be used to track all static files.\\n- A static file versioning endpoint, which will be used to serve static files.\\n- A static file versioning hook, which will be used to automatically trigger the removal of the `hotfix_deploy` and `force_update_static` code.\\n- A static file versioning tool, which will be used to automatically run the static file versioning process.\\n- A static file versioning pipeline, which will be used to automatically deploy the static file versioning changes to the production environment.\\n- A static file versioning build, which will be used to automatically deploy the static file versioning changes to the GitHub repository.\\n- A static file versioning CI, which will be used to automatically run the static file versioning process and deploy the static file versioning changes to the Jenkins environment.\\n- A static file versioning release, which will be used to automatically deploy the static file versioning"}
{"Context":"## Context and Problem Statement\\nThe study definition file is formulated as a YAML document.\\nTo accessed the definition within JabRef this document has to be parsed.\\nWhat parser should be used to parse YAML files?\\n","Decision":"## Decision\nChosen option: Jackson, because as it is a dedicated library for parsing YAML. yamlbeans also seem to be viable. They all offer similar functionality\\n","Prediction":"## Decision\nChosen option: \"YAML\", because it is the most simple and widely supported format for definition files.\\n### Positive Consequences\\n* It is the simplest format for definition files.\\n* It is supported by JabRef.\\n### Negative Consequences\\n* It is not supported by JabRef.\\n* It is not supported by any other JabRef plugins.\\n* It is not possible to use JabRef plugins to parse definition files.\\n### Positive Consequences\\n* It is the standard format for definition files in JabRef\\n* It is supported by JabRef\\n* It is supported by other JabRef plugins\\n### Negative Consequences\\n* It is not supported by JabRef\\n* It is not supported by any other JabRef plugins.\\n* It is not possible to use JabRef plugins to parse definition files.\\nChosen option: \"YAML\", because it is the most simple and widely supported format for definition files.\\n### Positive Consequences\\n* It is the simplest format for definition files.\\n* It is supported by JabRef.\\n### Negative Consequences\\n* It is not supported by JabRef.\\n* It is not supported by any other JabRef plugins.\\n* It is not possible to"}
{"Context":"## Context\\nIn [ADR 061](adr-061-p2p-refactor-scope.md) we decided to refactor the peer-to-peer (P2P) networking stack. The first phase is to redesign and refactor the internal P2P architecture, while retaining protocol compatibility as far as possible.\\n","Decision":"## Decision\nThe P2P stack will be redesigned as a message-oriented architecture, primarily relying on Go channels for communication and scheduling. It will use a message-oriented transport to binary messages with individual peers, bidirectional peer-addressable channels to send and receive Protobuf messages, a router to route messages between reactors and peers, and a peer manager to manage peer lifecycle information. Message passing is asynchronous with at-most-once delivery.\\n","Prediction":"## Decision\nThe first phase of the refactor will be to refactor the P2P stack to a more pluggable architecture. This will involve removing the `P2P` and `P2P::Broker` protocol types from the protocol stack, and replacing them with a `P2P::Broker`-specific interface. The first implementation will be a `P2P::BrokerImpl` that is a wrapper around a `P2P::Broker` implementation, and can be used in a variety of different ways.\\nThe second phase will be to refactor the `P2P::Broker` implementation to expose a new `P2P::BrokerImpl` interface. This will allow the `P2P::Broker` implementation to be replaced with a custom implementation in the future.\\nIn the future, the new `P2P::BrokerImpl` interface will be used as the basis for all P2P networking support in the future.\\n### New `P2P::BrokerImpl` interface\\nThe new `P2P::BrokerImpl` interface will replace the `P2P::Broker` interface in the protocol stack. It will provide a new constructor that takes an `Broker` to create a new `BrokerImpl` implementation,"}
{"Context":"## Context\\nIdea [142-Wallet-Compatibility](https:\/\/ideas.status.im\/ideas\/142-wallet-compatibility\/README) changes the way we generate keys starting from the 12 mnemonic words.\\nThis change allows users to create a wallet in other apps and import the same address in Status using the 12 mnemonic words.\\n### Breaking Changes\\nThe 12 mnemonic words will generate different addresses and whisper identities (public key). We should communicate this properly and recommend users to create new accounts.\\n### Features\\nIf users used a dapp in an external app\/wallet, they would be able to continue to use the same dapp with the same account in Status.\\n","Decision":"## Decision\n* update the BIP39 seed generation to use the salt `\"mnemonic\"` instead of `\"status-im\"` following [BIP39](https:\/\/github.com\/bitcoin\/bips\/blob\/master\/bip-0039.mediawiki#from-mnemonic-to-seed).\\n* update the master key generation using the constant `\"Bitcoin seed\"` instead of `\"status-im\"`, following [BIP32](https:\/\/github.com\/bitcoin\/bips\/blob\/master\/bip-0032.mediawiki#master-key-generation).\\n* remove the use of the password in the salt during the BIP39 seed generation (password is not removed from the app in general, and it's still required to encrypt the keys on the device).\\n### Relevant reading\\n* [BIP39](https:\/\/github.com\/bitcoin\/bips\/blob\/master\/bip-0039.mediawiki)\\n* [BIP32](https:\/\/github.com\/bitcoin\/bips\/blob\/master\/bip-0032.mediawiki)\\n* [BIP44](https:\/\/github.com\/bitcoin\/bips\/blob\/master\/bip-0044.mediawiki)\\n","Prediction":"## Decision\nWe will recommend users to create new accounts in Status.\\nWe will add a warning if users are not sure which dapp to use.\\nWe will provide a link to the dapp in the Status README.\\nWe will provide an example of a wallet in the README.\\nWe will provide an example of a wallet in the README.\\nWe will use the 12 mnemonic words for the wallets in the README.\\nWe will use the 12 mnemonic words for the wallet in the README.\\nWe will add a warning if users are not sure which dapp to use.\\nWe will add a warning if users are not sure which dapp to use.\\nWe will add a warning if users are not sure which dapp to use.\\nWe will add a warning if users are not sure which dapp to use.\\nWe will add a warning if users are not sure which dapp to use.\\nWe will add a warning if users are not sure which dapp to use.\\nWe will add a warning if users are not sure which dapp to use.\\nWe will add a warning if users are not sure which dapp to use.\\nWe will add a warning if users are not sure which dapp to use"}
{"Context":"## Context\\nWorking with OpenSource software for commercial purposes requires\\nevery developer to check the licenses of the packages that he uses.\\nSome licenses might be not-compatible with the project needs and\\ncause some legal issue if not properly handled.\\nUsing open-source libraries speeds up development in a\\nsignificant way but has a drawback, nested dependencies.\\nEach open-source library could depend on further open-source libraries\\nthat could have different licenses for usage and re-distribution.\\n","Decision":"## Decision\nGiven the previous concerns I suggest to use an automated\\nlicenses checker on the installed packages.\\nIn python you can use `pip-licenses` (which is distributed with MIT license)\\n","Prediction":"## Decision\nWe will use the [LGPLv3+](http:\/\/www.gnu.org\/licenses\/lgpl.html)\\nlicense for all our open-source software.\\nIf it is not possible to use the LGPLv3+ license for a given\\npackage then we will provide a license compatible with the\\nLGPLv3+ license. This license will be clearly indicated in the\\npackage's README.md file.\\nIf we are not able to provide a license compatible with the\\nLGPLv3+ license for a given software package then we will\\nprovide a license compatible with the MIT license.\\nIf it is not possible to provide a license compatible with the\\nMIT license for a given software package then we will\\nprovide a license compatible with the Apache License.\\nIf a license compatible with the Apache License is not\\navailable for a given software package then we will\\nprovide an alternative license compatible with the Apache License.\\nIf a license compatible with the Apache License is not\\navailable for a given software package then we will\\nprovide a license compatible with the MIT license.\\nIf a license compatible with the MIT license is not\\navailable for a given software package then we will\\nprovide a license compatible with the GNU General Public License.\\nIf a license compatible"}
{"Context":"## Context\\nWe want our [API](.\/0004-api.md) to be self-documenting and testable as we go.\\n","Decision":"## Decision\nUse an [OpenAPI hex package](https:\/\/hexdocs.pm\/open_api_spex) to automate OpenAPI tasks.\\n","Prediction":"## Decision\nWe will use [Swagger](https:\/\/swagger.io\/), a tool for generating API documentation from Swagger specifications.\\nSwagger is a tool for generating API documentation from Swagger specifications.\\nSwagger is highly customizable, so you can generate documentation in many different ways.\\nSwagger is also simple to use, so you don't need to be a developer to use it.\\nSwagger is also open-source, so we can contribute to it if we need to.\\nThis will be useful for us because we can add custom documentation to our API.\\nWe will also use Swagger to generate documentation for our [status API](.\/0001-status-api.md).\\nSwagger is also used by [Swagger Cloud](https:\/\/swagger.io\/cloud\/) to generate documentation for their API.\\nSwagger Cloud is a hosted service that allows us to generate documentation out-of-the-box.\\nSwagger Cloud is also the preferred way to generate Swagger documentation for [Swagger Hub](https:\/\/swaggerhub.com\/).\\nSwagger Cloud is open-source, so we can contribute to it as we need to.\\nWe will also use Swagger to generate documentation for our [deployment dashboard](.\/0002-deployment-dashboard.md).\\nDeployment Dashboard is a web application that allows us to view the status of our deployments.\\"}
{"Context":"## Context\\nEven a self-discoverable API needs to be documented to let consumers understand the underlying application features and\\nthe resources attributes significance.\\nA good API documentation will provide requests and responses structure, defining each attribute, giving examples and\\nmixing functional and technical information so that consumers do not need to traverse different documents to get a\\ncomplete overview of the features and technical concerns.\\nThe documentation must also provide links between different parts of the document, so that it is easy to go back and\\nforth between those parts.\\nThe documentation must be kept in sync with the features. Ideally, a documentation is generated from the code.\\n","Decision":"## Decision\n`menu-generation` will generate its API documentation with [Spring Rest-Docs](https:\/\/spring.io\/projects\/spring-restdocs).\\n","Prediction":"## Decision\nWe will use [docs.rockyou.com](docs.rockyou.com).\\nThe purpose of the documentation is to provide a concise overview of the features and technical concerns of the\\ncurrent state of the application. This is a living document, and we will update it from time to time.\\nWe will also use [docs.rockyou.com](docs.rockyou.com), but we will also use [docs.k8s.io](docs.k8s.io) and [docs.terraform.io](docs.terraform.io).\\nWe will create documentation for every feature, and update it when needed. We will also create documentation for\\nthe code that supports the feature.\\nWe will create documentation for every technical concern, and update it when needed. We will also create\\ndocumentation for the code that addresses the concern.\\nWe will create documentation for every deployment step, and update it when needed. We will also create\\ndocumentation for the command-line tools that make the deployment step possible.\\nWe will create documentation for every testing step, and update it when needed. We will also create\\ndocumentation for the command-line tools that make the testing step possible.\\nWe will create documentation for every configuration step, and update it when needed. We will"}
{"Context":"## Context\\n#### Request permission\\nApplication need `READ_EXTERNAL_STORAGE` to be allowed to perform file upload.\\nEvery time upload is performed, application must check that permission\\n```kotlin\\nwhen (ContextCompat.checkSelfPermission(this, Manifest.permission.READ_EXTERNAL_STORAGE)) {\\nPackageManager.PERMISSION_GRANTED -> { performUpload() }\\nelse -> { requestReadStoragePermission() }\\n}\\n```\\nIf the app has the permission, the method will returns `PERMISSION_GRANTED`.\\nOtherwise, the app has to explicit ask the user for permission.\\n#### Explain why the app needs permission\\n> One approach you might use is to provide an explanation only if the user\\nhas already denied that permission request. Android provides a utility method,\\n`shouldShowRequestPermissionRationale()`, that returns `true` if the user has previously denied the request,\\nand returns `false` if a user has denied a permission and selected the Don't ask again option\\nin the permission request dialog, or if a device policy prohibits the permission.\\nFollowing code check if the app has permission to read storage:\\n```kotlin\\n\/\/ Here, thisActivity is the current activity\\nif (ContextCompat.checkSelfPermission(thisActivity,\\nManifest.permission.READ_EXTERNAL_STORAGE)\\n!= PackageManager.PERMISSION_GRANTED) {\\n\/\/ Permission is not granted\\n\/\/ Should we show an explanation?\\nif (ActivityCompat.shouldShowRequestPermissionRationale(thisActivity,\\nManifest.permission.READ_EXTERNAL_STORAGE)) {\\n\/\/ Show an explanation to the user *asynchronously* -- don't block\\n\/\/ this thread waiting for the user's response! After the user\\n\/\/ sees the explanation, try again to request the permission.\\n} else {\\n\/\/ No explanation needed, we can request the permission.\\nActivityCompat.requestPermissions(thisActivity,\\narrayOf(Manifest.permission.READ_EXTERNAL_STORAGE),\\nMY_PERMISSIONS_REQUEST_READ_EXTERNAL_STORAGE)\\n\/\/ MY_PERMISSIONS_REQUEST_READ_EXTERNAL_STORAGE is an\\n\/\/ app-defined int constant. The callback method gets the\\n\/\/ result of the request.\\n}\\n} else {\\n\/\/ Permission has already been granted\\n}\\n```\\n#### Problem\\n>`shouldShowRequestPermissionRationale()`, that returns `true` if the user has previously denied the request\\nThis method return `false` in the first time app call it.\\nSo that, System Permission request can not show and Explanation dialog always show.\\n","Decision":"## Decision\nStore a flags to indicate User has denied permission or not and change the code above\\n","Prediction":"## Decision\nWe will use `shouldShowRequestPermissionRationale()` method to check if the app has permission to read storage.\\nWe will provide an option to allow the app to ask the user for permission.\\nWe will use `ActivityCompat.requestPermissions()` method to request the permission.\\nWe will use `ActivityCompat.requestPermissions()` method to check if the app has permission to read storage.\\nWe will use `ActivityCompat.requestPermissions()` method to check user permission.\\nWe will use `ActivityCompat.requestPermissions()` method to check if the app has permission to read storage.\\nWe will use `ActivityCompat.requestPermissions()` method to check user permission.\\nWe will use `ActivityCompat.requestPermissions()` method to check user permission.\\nWe will use `ActivityCompat.requestPermissions()` method to check user permission.\\nWe will use `ActivityCompat.requestPermissions()` method to check user permission.\\nWe will use `ActivityCompat.requestPermissions()` method to check user permission.\\nWe will use `ActivityCompat.requestPermissions()` method to check user permission.\\nWe will use `ActivityCompat.requestPermissions()` method to check user permission.\\nWe will use `ActivityCompat.requestPermissions()` method to check user permission.\\nWe will use `ActivityCompat.requestPermissions"}
{"Context":"## Context\\nWe want to be able to loop simply over Ontologies \/ Terms results, without bothering if a\\nnew call is made to change page.\\nOLS API results are paginated, the page size is a parameter in Query.\\nThere is no simple way to loop over all elements, and returning all results is not a solution, considering amount of data\\nThe actual calls to API are hidden from final users.\\n```python\\nfrom ebi.ols.api.client import OlsClient\\nclient = OlsClient()\\nontology = client.ontology('fpo')\\nterms = ontology.terms()\\nindividuals = ontology.individuals()\\nproperties = ontology.properties()\\n# work with all 'list' item types\\nfor term in terms:\\n# do whatever\\nprint(term)\\n# Direct List'like access on all list types\\nterm = terms[1254]\\nindividual = individuals[123]\\n# ...\\n```\\n","Decision":"## Decision\nTo Implement\\n","Prediction":"## Decision\nLoop over all results, and return them in a list\\n```python\\nfor result in individual.items():\\nprint(result)\\n```\\n```python\\nfor result in individual.items():\\nprint(result)\\n```\\n```python\\nfor result in individual.items():\\nprint(result)\\n```\\n```python\\nfor result in individual.items():\\nprint(result)\\n```\\n```python\\nfor result in individual.items():\\nprint(result)\\n```\\n```python\\nfor result in individual.items():\\nprint(result)\\n```\\n```python\\nfor result in individual.items():\\nprint(result)\\n```\\n```python\\nfor result in individual.items():\\nprint(result)\\n```\\n```python\\nfor result in individual.items():\\nprint(result)\\n```\\n```python\\nfor result in individual.items():\\nprint(result)\\n```\\n```python\\nfor result in individual.items():\\nprint(result)\\n```\\n```python\\nfor result in individual.items():\\nprint(result)\\n```\\n```python\\nfor result in individual.items():\\nprint(result)\\n```\\n```python\\nfor result in individual"}
{"Context":"## Context\\nBefore using ADR we decided that we would use vue. Vue needs a form of state management to work with. State in vue can be persisted on a vue component level as raw data objects but with growing scale and complexity of the application the need to share this state between components, without additional safety measures, can lead to code that is hard to understand and\/or debug.\\nThere is an officially recommended state management implementation for vue [[0]] - Vuex; which is community-supported, free, and follows the renowned Flux pattern [[1]].\\nVuex offers using a \"single source of truth\" for the application [[2]] with added guards. Only mutation through events are possible. This helps to protect from the down-sides of scattered code holding reference to the same objects.\\nVuex ships with an existing best practice for structure [[3]] and out-of-the-box integration with debug tools. The nature of event-based state mutation provides benefits like ease of debugging (\"time-travel\") and promises maintainability in a growing application.\\nWe already use Vuex within the WikibaseLexeme codebase. See: `resources\/widgets\/LexemeHeader.newLexemeHeaderStore.js`\\nThere is also an existing approach within WikibaseLexeme to persist, pass and partially validate state using rich models (`wikibase.lexeme.datamodel.Lexeme` et al.). These were written to mimic the backend data model but did so incompletely. To accomodate the need for components in a (temporarily while editing) \"dirty\" state, another `wikibase.lexeme.LemmaList` kind of state was added pointing out the need for flexibility beyond the model's capability in this regard.\\nLosing object-oriented accessors and existing \"validation\" rules, for an apparent anemic model [[4]], could feel cumbersome at first [[5]] and consequences of actions are not always immediately obvious. But Vuex\u2019s event-based store system provides us with an extensible interface for us and 3rd parties that offers flexibility, robustness, existing & well-maintained documentation.\\n","Decision":"## Decision\nWe will use Vuex as state management.\\n","Prediction":"## Decision\nWe will use Vuex as our state management system for the WikibaseLexeme application. This is the recommended state management system for vue. The choice of Vuex is based on the existing experience with Vuex and the community support for the library. We are also using Vuex because it is well-maintained and has a good reputation within the Vue community. We also chose Vuex because it is a well-supported library with a large community and active development. It is also free and open-source. We also chose Vuex because it is the recommended state management library for vue in the official vue-guide. It is also the recommended state management library for vue in the official vue-cli. It is also well-documented and has a good reputation within the Vue community. We also chose Vuex because it is a well-supported library with a large community and active development. It is also free and open-source. We also chose Vuex because it is the recommended state management library for vue in the official vue-cli. It is also well-documented and has a good reputation within the Vue community.\\n### State management in Vue\\nThe state management system in Vue is the `vue-state` library. It is well-established, well-maintained, and has a large community and active development. It is"}
{"Context":"## Context\\nThere are various requirements for the database containing master data\\nthat TEET is authoritative for.  These include ACID properties\\n(Atomicity, Consistency, isolation, Durability), preservation of\\nhistory, extensibility, GIS data support, file attachments.\\n","Decision":"## Decision\nThe master database containing business data, workflows, etc is\\nDatomic, as the data model providing history preservation and data\\nmodel extensibility is a good fit for TEET business requirements\\nidentified so far. In addition the AWS-native deployment and operating\\nmodel and and excellent fit with Clojure data structure paradigms are\\nsignificant pluses.\\nPostgreSQL + PostGIS and AWS S3 are used as auxiliary databases for\\nimported GIS data (that we are not the master of) and file\\nattachments.  PostgreSQL (as provided by AWS RDS) is used via\\nPostgREST to provide an API for map layer and geometry data.\\nPostgreSQL will also be used as our full text search engine by caching\\nfull-text data set in it.\\n","Prediction":"## Decision\nThe database will be stored in Amazon RDS for S3 and DynamoDB for RDBMS.\\nThese are both managed by AWS and have been pre-configured to allow\\ntelemetry, so are ready to go.\\nA single database will be used, as this will allow the most flexibility\\nfor how the data is stored and accessed.\\nWe will use the following configuration to store the database\\nconfiguration:\\n* DynamoDB Table: master\\_data\\n* DynamoDB Table: master\\_data\\_id\\n* DynamoDB Table: master\\_data\\_type\\n* DynamoDB Table: master\\_data\\_table\\n* DynamoDB Table: master\\_data\\_type\\n* DynamoDB Table: master\\_data\\_id\\n* DynamoDB Table: master\\_data\\_table\\n* DynamoDB Table: master\\_data\\_type\\n* DynamoDB Table: master\\_data\\_type\\n* DynamoDB Table: master\\_data\\n* DynamoDB Table: master\\n* DynamoDB Table: master\\n* DynamoDB Table: master\\n* DynamoDB Table: master\\n* DynamoDB Table: master\\n* DynamoDB Table: master\\n* DynamoDB Table: master\\n* DynamoDB Table: master\\n* DynamoDB Table: master"}
{"Context":"## Context\\nKeycloak provides a flexible way to organise access for users to its administration functions.\\nTDR requires user configuration that allows the following:\\n* Set of users permission to manage transferring body users, ie the users of the TDR application.\\n* Set of users permission to manage keycloak users, ie those internal TNA users who can administer Keycloak itself, including setting up TDR application users.\\nThe configuration needs to consider the following principles:\\n* *Least privilege*: only allow the user the minimum level of permissions to perform the requires operations\\n* *Elevated privileges*: prevent a user accruing additional permissions above what they require\\n* *User data protection*: ensure user data is secure\\n","Decision":"## Decision\nDecided on Option 2.\\nHaving user administrators in the top level realm provides a clear demarcation between transferring body users which will be defined in the TDR realm users.\\nThis provides additional protection against inadvertently giving incorrect permissions to transferring body users.\\n","Prediction":"## Decision\n* Set the configuration up so that the TNA application can only manage the configuration for the TDR application users.\\n* Set the configuration up so that the TNA application can only manage the configuration for the Keycloak users.\\n* Set the configuration up so that the TNA application can only manage the configuration for the TNA users.\\n* Set the configuration up so that the TNA application can only manage the configuration for the TNA users.\\n* Set the configuration up so that the TNA application can only manage the configuration for the TNA users.\\n* Set the configuration up so that the TNA application can only manage the configuration for the TNA users.\\n* Set the configuration up so that the TNA application can only manage the configuration for the TNA users.\\n* Set the configuration up so that the TNA application can only manage the configuration for the TNA users.\\n* Set the configuration up so that the TNA application can only manage the configuration for the TNA users.\\n* Set the configuration up so that the TNA application can only manage the configuration for the TNA users.\\n* Set the configuration up so that the TNA application can only manage the configuration for the TNA users.\\"}
{"Context":"## Context\\nWe want to have API documentation for the code we publish. Rather than writing\\nseparate API docs that quickly get out of date, we would like to be able to\\ngenerate it from our code and comments in our code that live next to the thing\\nthey refer to. For JavaScript, the standard is [JSDoc](https:\/\/jsdoc.app\/), but\\nwith TypeScript, we're already defining the types in our code, and duplicating\\nthat in the associated comments is repeated effort and requires manual action to\\nkeep up-to-date.\\n[TypeDoc](https:\/\/typedoc.org\/) is a documentation generator based on JSDoc. It\\nuses a combination of comments and TypeScripts own types to generate API\\ndocumentation automatically.\\n","Decision":"## Decision\nWe will use TypeDoc to generate documentation.\\nWe will document all exported code for the benefit of end users.\\nWe will commit the documentation we generate to the repository alongside changes\\nto behaviour.\\n","Prediction":"## Decision\nWe will use TypeScript and TypeDoc to generate API documentation.\\nTypeDoc will be used for comments, which are indented to the next level, and\\ngenerated as inline comments. TypeDoc comments are indented to the next\\nlevel, and will be used as the source of truth for the generated API documentation.\\nComments will be generated in the same order as the code, so that the\\ndocumentation generated by TypeDoc is the same as the documentation generated by JSDoc.\\nTypeDoc comments will be generated as inline comments, and will be\\nindented to the next level. TypeDoc comments will be the source of\\ntruth for the generated API documentation.\\nTypeDoc comments will be generated as inline comments, and will be\\nindented to the next level. TypeDoc comments will be the source of\\ntruth for the generated API documentation.\\nTypeDoc comments will be generated as inline comments, and will be\\nindented to the next level. TypeDoc comments will be the source of\\ntruth for the generated API documentation.\\nTypeDoc comments will be generated as inline comments, and will be\\nindented to the next level. TypeDoc comments will be the source of\\ntruth for the generated API documentation.\\nTypeDoc"}
{"Context":"## Context\\nThe verify-service-provider will be used by number of relying parties (RPs).\\nCurrently to distribute the artifact to RPs we upload them manually to some server which is not a secure way of doing it.\\nSo we wanted to automate the release process for verify service provider and make it more secure by signing the artifact.\\nThis will make RPs confident that they are not running any compromised software in their infrastructure.\\n","Decision":"## Decision\nWe decided to follow the below release process for verify service provider:\\n* During development, maintain #NEXT release notes :\\n- Update the release notes with any breaking changes \/ new feature \/ anything important that RPs should know about\\n* When we decide to release the following steps have to be followed :\\n- Finalise the release notes\\n- Decide appropriate release number\\n- Include diff link in release notes\\n- Commit, tag it with release number and push release notes to master\\n- The release jenkins job should be triggered and if it passes, it should produce an artifact\\n- Ask for approval (in Google \/ Slack)\\n- Release approver should check the artifact to ensure its proper one and sign it using the certificate\\n- Approver should also upload the signed artifact to github releases\\n- Jenkins job to check if release artifact has been signed. If not, should send email to appropriate people\\n- If release artifact is proper it should send an email to the RP's regarding new VSP.\\n","Prediction":"## Decision\nWe will use the [deployer](https:\/\/github.com\/jeffreyhenderson\/verify-service-provider-deployer) to deploy the artifact to the RPs.\\nDeployer will be run as a service in the same container that will be used to run the verify service provider.\\nDeployer will use the [deployer-auth](https:\/\/github.com\/jeffreyhenderson\/verify-service-provider-deployer\/blob\/master\/requirements\/deployer-auth.md) and [deployer-secret](https:\/\/github.com\/jeffreyhenderson\/verify-service-provider-deployer\/blob\/master\/requirements\/deployer-secret.md) to create the artifact which will be signed by the verify service provider.\\nDeployer will use the [deployer-agent](https:\/\/github.com\/jeffreyhenderson\/verify-service-provider-deployer\/blob\/master\/requirements\/deployer-agent.md) to communicate with the verify service provider.\\nDeployer will use the [deployer-agent-sandbox](https:\/\/github.com\/jeffreyhenderson\/verify-service-provider-deployer\/blob\/master\/requirements\/deployer-agent-sandbox.md) to test the deployer before deploying it to the RPs and will be run"}
{"Context":"## Context\\nNetwork connection might be lost, when a meal is delivered to a fridge and a user comes to grab it. Then the fridge can't check data of the user online by card swapping, or in-app distance opening.\\nBut the fridge has a pin pad keyboard and still has quite sophisticated software and internal memory to process orders.\\nUpdate 2020-11-24:\\nWe expect that every meal has its own unique id provided by the kitchen because some meals might be customized from the general catalog. Let's say lactose-free lasagna should be addressed to a specific user.\\nThen, at the purchase or production process, we can update the user's device with the meal's unique id and generate an access code based on meal ID.\\n","Decision":"## Decision\nMeals dispatched from a Ghost Kitchen, will have a special 6-8 digit code.\\n","Prediction":"## Decision\nWe will store the meal's unique id in the database, so we can generate an access code even if the kitchen is out of network or the fridge is not connected to the internet.\\nWe will store the meal's unique id in the database, so we can generate an access code even if the kitchen is out of network or the fridge is not connected to the internet.\\nWe will generate an access code for each unique meal, and store it in the database. To check the code, we will use the unique meal id to check if the code is valid.\\nWe will generate an access code for each unique meal, and store it in the database. To check the code, we will use the unique meal id to check if the code is valid.\\nWe will generate an access code for each unique meal, and store it in the database. To check the code, we will use the unique meal id to check if the code is valid.\\nWe will generate an access code for each unique meal, and store it in the database. To check the code, we will use the unique meal id to check if the code is valid.\\nWe will generate an access code for each unique meal, and store it in the database. To check the"}
{"Context":"## Context\\nThe `service` command is has become a useful way for us to start, stop, restart,\\nand check the status of the many service processes behind a commcare cluster.\\nFor the most part, these commands map very predictably to a start\/stop\/restart\/status\\ncommand on either [`service`](https:\/\/linux.die.net\/man\/8\/service) or `supervisorctl`.\\nRecently and in the past, we have found that `elasticsearch` does not stop or restart\\nreliably. In particular, after running `service elasticsearch stop`,\\nusing `ps aux | grep elasticsearc[h]` may still show a running Elasticsearch\\nprocess, even if `service` is reporting a stopped status. This then requires\\nkilling the rogue process in order for a `start` to bring up a genuinely new process.\\nBefore the addition of the `service` command, we already had the `restart-elasticsearch`\\ncommand, which is conceptually (though currently not functionally) equivalent to\\n`... service elasticsearch restart`. It does not address the above issue of rogue\\nprocesses, but it does address some other issues, including a reminder to stop all pillows,\\nas well as theoretically providing rolling (and thus no-downtime) restart,\\nwhich we currently do not benefit from because we do not store data in duplicate.\\n","Decision":"## Decision\nWe will be changing `... service elasticsearch ...` from using the default\\nbehavior to performing a more targeted set of actions that are guaranteed to achieve the\\nstop\/start\/restart\/status action, and we will also be deprecating the `restart-elasticsearch`\\nentry point. In particular, `... service elasticsearch stop` will\\n- stop pillows\\n- guarantee the process has stopped and `kill -9` it after a period of time if it has not\\nand `start` will\\n- start pillows\\nand `restart` will be functionally a `stop` followed by a `start`.\\nPros:\\n- Someone with little context will be able to run a `... service elasticsearch ...`\\nand have it have the intended overall consequence\\n- The tool will incorporate what is currently heard knowledge\\nCons:\\n- A \"power\" user may think of the command as executing a very specific other command,\\nand may be taken aback by the tool trying to outsmart them.\\nTo mitigate this con, the command will also explain itself beforehand,\\nand print out the command you could run to execute the narrower command,\\nand ask for confirmation before continuing.\\nSee https:\/\/github.com\/dimagi\/commcare-cloud\/pull\/1721#discussion_r187730933\\nfor a short exchange that demonstrates this trade-off.\\n","Prediction":"## Decision\nWe will no longer use the `service` command. Instead, we will use the `restart-elasticsearch`\\ncommand, which is conceptually (though currently not functionally) equivalent to\\n`... service elasticsearch restart`. It does not address the above issue of rogue\\nprocesses, but it does address some other issues, including a reminder to stop all pillows,\\nas well as theoretically providing rolling (and thus no-downtime) restart,\\nwhich we currently do not benefit from because we do not store data in duplicate.\\nIt is worth noting that the `restart-elasticsearch` command is effectively the\\n\"fork\" of the `service` command. This means that it is possible to use the\\ntool `kill` command to stop a running `restart-elasticsearch` process,\\nwithout the need to kill the `service` process itself. This is useful for\\ndebugging, for example, when we need to isolate an issue.\\nWe will continue to use the `service` command to start services, but we will\\nbegin to use the `restart-elasticsearch` command to stop and restart them.\\nThis change will be rolled out gradually, starting with the `elasticsearch`\\nservice.\\nWe will continue to"}
{"Context":"## Context\\nWe need a log rotation for the nginx webserver. As the nginx does not support log rotation out of the box, we need our own solution.\\n","Decision":"## Decision\nWe implement the log rotation using the programm logrotate inside the docker container for the nginx webserver. We do not trigger the log rotation from outside of the docker container, because we want a solution that is independed from the host system.\\n","Prediction":"## Decision\nWe will use logrotate to rotate the logs. We will use the [logrotate](https:\/\/www.gnu.org\/software\/logrotate\/) package to manage the rotation of the logs.\\nWe will use the [logrotate](https:\/\/www.gnu.org\/software\/logrotate\/) package to manage the rotation of the logs.\\nWe will use the [logrotate](https:\/\/www.gnu.org\/software\/logrotate\/) package to manage the rotation of the logs.\\nWe will use the [logrotate](https:\/\/www.gnu.org\/software\/logrotate\/) package to manage the rotation of the logs.\\nWe will use the [logrotate](https:\/\/www.gnu.org\/software\/logrotate\/) package to manage the rotation of the logs.\\nWe will use the [logrotate](https:\/\/www.gnu.org\/software\/logrotate\/) package to manage the rotation of the logs.\\nWe will use the [logrotate](https:\/\/www.gnu.org\/software\/logrotate\/) package to manage the rotation of the logs.\\nWe will use the [logrotate](https:\/\/www.gnu.org\/software\/logrotate\/) package to manage the rotation of the logs.\\nWe will use the [logrotate](https:\/\/www.gnu.org\/software\/logrotate\/) package to manage the rotation of the logs.\\n"}
{"Context":"## Context\\nGeoprocessing calls for large shapes can take a long time to complete. While users can draw custom shapes, they can also pick from a list of predefined shapes in the system. Geoprocessing the same shape over and over when multiple users select it is wasteful and unnecessary. By caching the geoprocessing results of predefined shapes, or Well Known Areas of Interest (WKAoIs), we can improve user experience and application performance.\\nOutside of geoprocessing, there are also [some](https:\/\/github.com\/WikiWatershed\/model-my-watershed\/blob\/dea12ff9a6fb234e30978e97ed1d0f6266a406c9\/src\/mmw\/apps\/modeling\/calcs.py#L28) [database](https:\/\/github.com\/WikiWatershed\/model-my-watershed\/blob\/dea12ff9a6fb234e30978e97ed1d0f6266a406c9\/src\/mmw\/apps\/modeling\/calcs.py#L51) [calls](https:\/\/github.com\/WikiWatershed\/model-my-watershed\/blob\/dea12ff9a6fb234e30978e97ed1d0f6266a406c9\/src\/mmw\/apps\/modeling\/calcs.py#L97), but these are much faster in comparison and are not likely to see significant improvement by caching, especially since the cache could be a table in the database itself.\\nIn this ADR we consider the following questions:\\n* What will be cached?\\n* How will it be cached?\\n* Where will it be cached?\\n* When will it be cached?\\n* How will the cache be invalidated?\\nWe also sketch out an implementation plan, and consider consequences and side-effects.\\n","Decision":"## Decision\n### What will be cached?\\nThe output of the geoprocessing service for WKAoIs will be cached. This is almost always a JSON blob of key-value pairs, where the key is a combination of overlaid cell values in a set of rasters, and the value is a count of such cells. The inputs are a GeoJSON shape and a [set of related arguments](https:\/\/github.com\/WikiWatershed\/model-my-watershed\/blob\/develop\/src\/mmw\/mmw\/settings\/base.py#L412) that specify rasters, operation type, CRS, etc.\\nWhile we could cache the entire output of a MapShed or TR-55 run, an update to some of the constituent raster or vector data would force recalculation of the whole. By caching only the time-consuming geoprocessing results, we ensure that any updates to constituent rasters would invalidate only that specific cached result, leaving the others still current. And since vector data results would never be cached, they will always be current upon update as well.\\nIn case of MapShed, the modifications do not change any of the geoprocessing queries, thus those requests can be cached easily. For TR-55, the modifications _do_ change the geoprocessing queries. For these cases, we will cache only the Current Conditions (modification-less) run, not other Scenarios, since storing arbitrary shapes can balloon the size of the cache very quickly. This decision may be revisited in the future.\\nIn case we do not foresee updating the rasters very often, it may be beneficial to consider caching the entire JSON response of the API.\\n### How will it be cached?\\nThe current stack already has [Django Caching](https:\/\/github.com\/WikiWatershed\/model-my-watershed\/blob\/a49cc115d18d2c67cbd0721cf62faa08e98d13fc\/src\/mmw\/mmw\/settings\/base.py#L76-L90) setup. This allows us to cache with a single line of code:\\n```python\\nfrom django.core.cache import cache\\ncache.set(key, value, None)\\n```\\nwhere the `key` is a unique identified consisting of WKAoI id and the geoprocessing operation, `value` is the result of the geoprocessing operation, and `None` is the timeout value which ensures the values don't ever expire.\\nRetrieval is as simple as:\\n```python\\nvalue = cache.get(key)\\nif not value:\\n# Calculate and cache the value\\nreturn value\\n```\\nThe `key` should be prefixed with `geop` to namespace it from other cache entries in the app, composed of the WKAoI id, which consists of a table name and an integer id, and the [geoprocessing operation name](https:\/\/github.com\/WikiWatershed\/model-my-watershed\/blob\/a49cc115d18d2c67cbd0721cf62faa08e98d13fc\/src\/mmw\/mmw\/settings\/base.py#L387). For example, `geop__boundary_huc08__1__nlcd_soils`. In practice, the `key` may actually be something like `:1:geop__boundary_huc08__1__nlcd_soils`, with a configurable app-wide prefix (which defaults to `''`) and a version number are prefixed to our key. However, as long as we always access the cache via `django.core.cache.cache`, we shouldn't need to worry about that.\\n### Where will it be cached?\\nDjango Caching Framework can be configured to use a number of cache backends, including Redis and Database. It is currently setup to use Redis \/ [ElastiCache](https:\/\/aws.amazon.com\/elasticache\/redis\/) in production.\\nThe advantages of using Redis are:\\n* It is already configured\\n* It allows us to take advantage of the Django Caching Framework which is configured to use Redis\\n* Redis is really fast, and a good candidate for storing key\/value pairs like we intend to\\n* It is designed to be a cache, and thus comes with mechanisms for timeout, LRU, and cache misses out of the box\\nThe disadvantages of using Redis are:\\n* In the case of system failure, the cached values will be lost, and will need to be cached again\\n* If it purges least recently used values, it might not be a good candidate for storing hard-to-process large WKAoIs that are rarely used\\n### When will it be cached?\\nWhen a user selects a WKAoI, and a request is made to `\/analyze` or `\/modeling`, if the WKAoI results haven't already been cached, we will run and cache them. Over time, we will build up the cache, so that when new users request the same WKAoIs, they will get the cached results. These will be cached in the Celery Tasks where calculation would otherwise happen.\\nFor a select few WKAoIs which are too large to process in the production infrastructure, such as HUC-8s which time-out during MapShed gathering phase, we can run their geoprocessing steps on more powerful infrastructure with longer timeouts in a batch process, cache the results, and then decommission it. This will make them available to regular users of the production app without needing the extra power to render them.\\nFor this purpose, we'll need a pair of new Django management commands. The first should run the geoprocessing steps for given WKAoIs and save the results to a file. This will be run in the super environment. The second should take a file of pre-processed results for given shapes and operations, and add them to the cache. This will be run in the production environment.\\n### How will the cache be invalidated?\\nSince every cache entry is tagged with its type, if a certain raster is updated, we can remove all related cache entries. For example, if there are updates to `us-percent-slope-30m-epsg5070` raster, which is used in the [`nlcd_slope`](https:\/\/github.com\/WikiWatershed\/model-my-watershed\/blob\/a49cc115d18d2c67cbd0721cf62faa08e98d13fc\/src\/mmw\/mmw\/settings\/base.py#L449) and [`slope`](https:\/\/github.com\/WikiWatershed\/model-my-watershed\/blob\/a49cc115d18d2c67cbd0721cf62faa08e98d13fc\/src\/mmw\/mmw\/settings\/base.py#L462) requests, we could simply run\\n```python\\ncache.delete_pattern('geop__*__nlcd_slope')\\ncache.delete_pattern('geop__*__slope')\\n```\\nTo refresh a specific shape we could do\\n```python\\ncache.delete_pattern('geop__boundary_huc08__1__*')\\n```\\nThese could be management commands as well.\\n","Prediction":"## Decision\nWe will implement a cache for the [WKAoIs](https:\/\/github.com\/WikiWatershed\/model-my-watershed\/blob\/dea12ff9a6fb234e30978e97ed1d0f6266a406c9\/src\/mmw\/apps\/modeling\/calcs.py#L27) and [Well Known Areas of Interest](https:\/\/github.com\/WikiWatershed\/model-my-watershed\/blob\/dea12ff9a6fb234e30978e97ed1d0f6266a406c9\/src\/mmw\/apps\/modeling\/calcs.py#L30) [WKAoIs](https:\/\/github.com\/WikiWatershed\/model-my-watershed\/blob\/dea12ff9a6fb234e30978e97ed1d0f6266a406c9\/src\/mmw\/apps\/modeling\/calcs.py#L29) [WKAoIs](https:\/\/github.com\/WikiWatershed\/model-my-watershed\/blob\/dea12ff9a6fb234e30978e97ed1d0f6266a406c9\/src\/mmw\/apps\/modeling\/calcs.py#L31) and [WKAoIs](https"}
{"Context":"## Context\\nWe would like to have logical groupings of code that allow both separation of\\nreusable modules and combines each projects code and configuration in to a\\nconsistent but isolated unit.\\n","Decision":"## Decision\nStore units of code and configuration in directories. The initial version of the layout is still\\nbeing iterated on but currently expects your directory structure to look like the below:\\nmodules                         # each local module should be placed under here\\nmodules\/networking\/             # resource containing tf files should be placed here\\nmodules\/networking\/main.tf\\nmodules\/networking\/overlays.tf\\nmodules\/networking\/README.md\\nprojects                                    # each project has a directory under here\\nprojects\/master_dbs\/\\nprojects\/master_dbs\/resources               # common resources should be placed here\\nprojects\/master_dbs\/resources\/main.tf\\nprojects\/master_dbs\/resources\/rds.tf\\nprojects\/master_dbs\/resources\/production\/   # environment specific resources should be placed here\\nprojects\/master_dbs\/resources\/production\/read-replicas.tf\\n","Prediction":"## Decision\nWe will use [GitFlow](https:\/\/en.wikipedia.org\/wiki\/Flow_(software)) to\\nprovide the separation of concerns between our modules.\\nGitFlow will allow us to have separate branches for each module, with each branch\\nbeing used for a specific purpose. For example, we may have a `frontend` branch, a\\n`backend` branch and a `backend-frontend` branch. All three branches will be\\nused by different developers, but the code will be merged into the same codebase.\\nWe will also use a configuration file to control the flow of code between modules.\\nThis file will not be version controlled, but will be used to allow developers to\\nrun different versions of the application.\\nWe will use a [GitHub Flow](https:\/\/help.github.com\/en\/articles\/flow) convention for\\nthe flow of code between projects.\\nEach project will have a `main` branch that will contain the `main` file.\\nThis file will contain the code for the application, which can be used by\\nother projects.\\nEach module will have a `test` branch that will contain the `test` file.\\nThis file will contain the unit tests for the module.\\nEach module will have a `production"}
{"Context":"## Context\\nLerna guidance on how to use the `bootstrap` command with or without hoisting and the resulting `package-lock.json`s is sparse.\\nCalling `lerna bootstrap --hoist` changes the root `package-lock.json`, calling `lerna bootstrap` created `package-lock.json`s in the packages, that do not contain references to other local packages and are therefore incorrect in terms of standard npm.\\nTherefore tools like dependabot cannot handle these files.\\nGenerally, there is quite some confusion about how lerna handles the package-locks, when they are updated and how to make them work with other tools.\\nFurther, because `lerna bootstrap --hoist` changes the root `package-lock.json` we are not using hoisting to ensure we respect the `build once` principle.\\nTherefore, we currently don't benefit from hoisting.\\n","Decision":"## Decision\nUse yarn workspaces with hoisting.\\n","Prediction":"## Decision\nWe will not use hoisting when calling `lerna bootstrap`.\\nInstead, we will use the `lerna bootstrap` command with the `--hoist` flag, as described in the [documentation](https:\/\/github.com\/lerna\/lerna#hoisting).\\nWe will continue to use this flag when calling `lerna bootstrap`.\\nWe will use the `lerna` command to generate packages.\\nWe will use the `lerna` command to generate packages that include hoisted dependencies.\\nWe will use the `lerna` command to generate packages that do not include hoisted dependencies.\\nWe will use the `lerna` command to generate packages that are intended for `npm publish`.\\nWe will use the `lerna` command to generate packages that are intended for `npm run build`.\\nWe will use the `lerna` command to generate packages that are intended for `npm run build --production`.\\nWe will use the `lerna` command to generate packages that are intended for `npm run build --production --test`.\\nWe will use the `lerna` command to generate packages that are intended for `npm run build --production --test --log`.\\nWe"}
{"Context":"## Context\\nOur current implementation of the link-user application is built on React via [create-react-app](https:\/\/github.com\/zendesk\/link_platform\/blob\/master\/doc\/architecture\/decisions\/0005-create-react-app-npm-no-yarn.md).\\nThis ADR reviews the possibility of ejecting from `create-react-app` and using Preact for the link-user application going forward.\\n### What is Preact?\\n[Preact describes itself as](https:\/\/preactjs.com\/) a \"fast 3kB alternative to React with the same modern API.\" React comes in at about 45kB.\\n### Differences to React\\nPreact achieves a 95% reduction in size by differing in several areas from React.\\nOne significant difference between Preact and React is that Preact does not rely on [Synthetic Events](https:\/\/reactjs.org\/docs\/events.html) for browser event handling. Rather, it relys directly on the browser's native `addEventListener`. A cross-browser API is still provided in that event handler registration and removal are abstracted; however, there is no custom event propagation scheme like in React.\\nAnother difference is that Preact does not use [React.Children](https:\/\/reactjs.org\/docs\/react-api.html#reactchildren). Instead, `props.children` is always implemented as an array. See Preact's \"What's Missing\" section of their article on [Differences to React](https:\/\/preactjs.com\/guide\/differences-to-react) for more detail.\\n### Similarities to React\\nPreact includes many of the same concepts that developers familiar with React are used to. Here are some examples from the [Preact website](https:\/\/preactjs.com\/guide\/differences-to-react):\\n- ES6 Class Components\\n- High-Order Components\\n- Stateless Pure Functional Components\\n- Contexts\\n- Refs\\n- Virtual DOM Diffing\\nThe code itself is extremely similar to React, so much so that one might mistake it for React code at first glance. Below is a Preact code snippet from the Preact [website](https:\/\/preactjs.com\/).\\n```\\nexport default class Stars extends Component {\\nasync componentDidMount() {\\nlet stars = await githubStars(this.props.repo);\\nthis.setState({ stars });\\n}\\nrender({ repo }, { stars=0 }) {\\nlet url = `\/\/github.com\/${repo}`;\\nreturn (\\n<a href={url} class=\"stars\">\\n\u2b50\ufe0f {stars} Stars\\n<\/a>\\n);\\n}\\n}\\n```\\nOne notable difference here is in the component's `render()` function. In Preact, `props` and `state` are passed into the `render()` function.\\n```\\nimport { Component } from 'preact';\\nclass MyComponent extends Component {\\nrender(props, state) {\\n\/\/ props === this.props\\n\/\/ state === this.state\\nreturn <h1>Hello, {props.name}!<\/h1>;\\n}\\n}\\n```\\n### Case Studies\\nThere are many existing examples of companies taking their React applications and converting them to Preact applications in pursuit of smaller bundle sizes and performance gains.\\nOne excellent example is the Uber mobile web app. Angus Croll of Uber engineering details their decision to switch to Preact and the benefits gained in a post titled [Building m.uber: Engineering a High-Performance Web App for the Global Market](https:\/\/eng.uber.com\/m-uber\/)\\nUber's mobile web app is intended to be fast on low-performance devices and slow, 2G connections, much like link-user. By switching to Preact and implementing a few other performance enhancing features such as rendering Preact on the server, Uber was able to reduce their bundle size from 280kB to 17kB and greatly increase their time to first pain.\\nTreebo, a budget Hotel chain out of India, has [wrote up a great performance case](https:\/\/medium.com\/dev-channel\/treebo-a-react-and-preact-progressive-web-app-performance-case-study-5e4f450d5299) study on switching their mobile site from React to Preact. Treebo was able to achieve a 70% improvement in time to first paint and a 31% improvement in time to interactive.\\n","Decision":"## Decision\nNo decision yet. I believe using Preact will be beneficial.\\n","Prediction":"## Decision\nWe will continue to use `create-react-app` to build and test our link-user application. We will continue to use Preact for the link-user application.\\nThis decision is based on the following considerations:\\n- Preact is a great library with a lot of benefits for building fast and maintainable applications. We have had a small number of issues with it in the past and it has a great reputation.\\n- We have a small number of existing link-user applications that are already running on Preact. Many of these applications are well maintained and have had many years of experience with Preact.\\n- We have a small number of existing link-user applications that are using Preact. Many of these applications are well maintained and have had many years of experience with Preact.\\n- We have a small number of existing link-user applications using Preact that are using Preact in production. Many of these applications are well maintained and have had many years of experience with Preact.\\n- We have a small number of existing link-user applications using Preact that are using Preact in production that are not well maintained. Many of these applications are not well maintained and have had many years of experience with Preact.\\n- We have a small number of"}
{"Context":"## Context and Problem Statement\\nWe no longer want to depend on SQLCipher and want to use SQLite directly for build complexity and concerns over the long term future of the rust bindings. The encryption approach taken by SQLCipher means that in practice, the entire database is decrypted at startup, even if the logins functionality is not interacted with, defeating some of the benefits of using an encrypted database.\\nThe per-field encryption in autofill, which we are planning to replicate in logins, separates the storage and encryption logic by limiting the storage layer to the management of encrypted data. Applying this approach in logins will break the existing validation and deduping code so we need a way to implement per-field encryption while supporting the validation and de-duping behavior.\\n## Decision Drivers\\n* Addressing previously identified deficiencies in the logins API while we are breaking the API for the encryption work\\n* Continuing to support the existing logins validation and deduping logic\\n* Avoiding the implementation of new security approaches that may require additional time and security resources\\n* Establishing a standard encyrption approach across components\\n","Decision":"## Decision\n* Addressing previously identified deficiencies in the logins API while we are breaking the API for the encryption work\\n* Continuing to support the existing logins validation and deduping logic\\n* Avoiding the implementation of new security approaches that may require additional time and security resources\\n* Establishing a standard encyrption approach across components\\nChosen Option: \"Reduce the API functions that require the encryption key and pass the key to the remaining functions\" because it will not require a security review as similar to the approach we have established in the codebase.\\n","Prediction":"## Decision\n* Addressing previously identified deficiencies in the logins API while we are breaking the API for the encryption work\\n* Continuing to support the existing logins validation and deduping logic\\n* Avoiding the implementation of new security approaches that may require additional time and security resources\\n* Establishing a standard encyrption approach across components\\nWe will implement per-field encryption by creating a new struct type for each field being encrypted.\\nEach field will have a field type which will be defined in the encryption module and will be used by the logging API to decrypt and encrypt the fields.\\nEach field type will also have a field name which will be defined in the encryption module and will be used by the logging API to store the encrypted data and the field name.\\nFields will be stored in a unique manner based on the field name, for example:\\n```rust\\n| field_name::Name = \"email\"\\n| field_name::Email = \"test@example.com\"\\n| field_name::Password = \"test123\"\\n| field_name::Password::Hex = \"04 55 66 77 89 99 88 88 44 33 22 11 22 33 44 55 77 88 99"}
{"Context":"## Context\\nWe want to support ESLint of the administration vue app to keep and increase code quality.\\nBy adding ESLint to the administration app, every developer will get instant feedback and best practices on his code by writing.\\n","Decision":"## Decision\nFor the `*.js` linting, we want to get pretty close to a standard vue2 app linting. In the `*.js` linting chapter, we explain where and why we choose to leave the common way.\\nFor the `*.html.twig` linting, we need to create a custom solution, which can handle or circumstance the twig syntax in our templates. We decided to convert all twig syntax to HTML comments within the linting process. This way, the linter ignores the twig parts and can handle the twig files like typical vue templates. The most significant tradeoff with this solution is that the linter cannot take the twig blocks into account on computing indentation levels.\\n### `*.js` linting\\nFor the `*.js` files we try to follow a standard vue cli linting way, with this adjustments:\\n* [`'vue\/require-prop-types': 'error'`](https:\/\/eslint.vuejs.org\/rules\/require-prop-types.html) - always use proper types definitions for `props`\\n* [`'vue\/require-default-prop': 'error'`](https:\/\/eslint.vuejs.org\/rules\/require-default-prop.html) - always provide a default value for optional `props`\\n* [`'vue\/no-mutating-props': ['off']`](https:\/\/eslint.vuejs.org\/rules\/no-mutating-props.html) - this is a tradeoff to allow mutating properties because it is already heavily used\\n* [`'vue\/component-definition-name-casing': ['error', 'kebab-case']`](https:\/\/eslint.vuejs.org\/rules\/component-definition-name-casing.html) - write component names in kebab-casing\\n### `*.spec.js` linting\\nDuring writing unit test files, we do not want to get a `max-len` warning.\\nA `max-len` rule may lead to hard understandable output in test names only to suit the `max-len` rules.\\nIn a test itself, you sometimes have `selector` phrases or something else where you exceed the `max-len` rule without a chance to solve it.\\n### `*.html.twig` linting\\nBesides the _twig-to-html-comment_ tradeoff, these exceptions are also made:\\n* `'vue\/component-name-in-template-casing': ['error', 'kebab-case']` - write vue component names in kebab-case in templates\\n* `'vue\/no-multiple-template-root': 'off',` - due to external template files and component inheritance\\n* `'vue\/attribute-hyphenation': 'error'` - write `hello-word=\"\"` attributes instead of `helloWorld=\"\"`\\n* `'vue\/no-parsing-error': ['error', {'nested-comment': false}]` - ignore nested html comments, which may be a result of the twig-to-html-comment workflow\\n* `'vue\/valid-template-root': 'off'` - @see `vue\/no-multiple-template-root`\\n* `'vue\/valid-v-slot': ['error', { allowModifiers: true }]` - allow `.`s in template slot names\\n* `'vue\/no-unused-vars': 'off'` - the twig parser cannot understand if a scoped slot value is used or not used properly\\n* `'vue\/no-template-shadow': 'off'` - for providing scoped values into another template scope\\n* `'vue\/no-lone-template': 'off'` - in some composition cases lone template tags are used\\n* `'vue\/no-v-html': 'off'` - for i18n and other reasons v-html is often used\\n### twig block indentation\\nTo accomplish the twig syntax being able to be linted, we needed to create a custom [`eslint-twig-vue-plugin`](..\/src\/Administration\/Resources\/app\/administration\/twigVuePlugin\/lib\/processors\/twig-vue-processor.js) and to accept the following changes in template writing:\\n_before_\\n``` html\\n\u2026\\n<div>\\n{% block block_name %}\\n<div>\\n\u2026\\n\u2026\\n```\\n_now_\\n``` html\\n\u2026\\n<div>\\n{% block block_name %}\\n<div>\\n\u2026\\n```\\nTo be able to lint the twig templates, we replace the twig syntax with HTML comments during the lint process, and thus every `twig` syntax is treated as an HTML comment and not recognised for indentation.\\n### self-closing components\\n_before_\\n``` html\\n\u2026\\n<sw-language-switcher><\/sw-language-switcher>\\n\u2026\\n```\\n_now_\\n``` html\\n\u2026\\n<sw-language-switcher \/>\\n\u2026\\n```\\n### attribute alignment\\nAs soon as more than 1 attribute exists, every attribute gets its own line:\\n_before_\\n``` html\\n\u2026\\n<div v-for=\"strategy in strategies\" class=\"sw-app-app-url-changed-modal__content-choices\">\\n\u2026\\n<sw-icon small color=\"#189eff\" name=\"default-basic-shape-circle-filled\"><\/sw-icon>\\n\u2026\\n```\\n_now_\\n``` html\\n\u2026\\n<div\\nv-for=\"strategy in strategies\"\\nclass=\"sw-app-app-url-changed-modal__content-choices\"\\n>\\n\u2026\\n<sw-icon\\nsmall\\ncolor=\"#189eff\"\\nname=\"default-basic-shape-circle-filled\"\\n\/>\\n\u2026\\n```\\n","Prediction":"## Decision\nWe will use ESLint for code quality.\\nWe will use ESLint as a code style checker.\\nWe will use ESLint as a code formatter.\\nWe will use ESLint as a code linting tool.\\nWe will use ESLint as a code formatter linting tool.\\nWe will use ESLint as a code style checker linting tool.\\nWe will use ESLint as a code formatter linting tool.\\nWe will use ESLint as a code style checker linting tool.\\nWe will use ESLint as a code style checker linting tool.\\nWe will use ESLint as a code style checker linting tool.\\nWe will use ESLint as a code style checker linting tool.\\nWe will use ESLint as a code style checker linting tool.\\nWe will use ESLint as a code style checker linting tool.\\nWe will use ESLint as a code style checker linting tool.\\nWe will use ESLint as a code style checker linting tool.\\nWe will use ESLint as a code style checker linting tool.\\nWe will use ESLint as a code style checker linting tool.\\nWe will use ESLint as a code style checker linting tool.\\"}
{"Context":"## Context and Problem Statement\\nHow should Axiomatic ensure that dir2consul, when run as a Nomad batch job, has access to configuration repos hosted on GitHub?\\n## Decision Drivers\\n* GitHub prevents deployment key reuse. The SSH key can only be attached to one repo.\\n* There is no appetite for spending money on Service User accounts.\\n* Option 2 needs a process for creating the ssh key pair, adding the public key to the repo, and storing the private key in Vault (potentially automated via Terraform)\\n* Option 2 setup is an infrequent activity as would be key rotation\\n* Option 3 fundamentally cannot be done due to bugs\/non-support in Nomad\\n* Option 4 was eliminated due to possible violation of our acceptable use policies\\n","Decision":"## Decision\n* GitHub prevents deployment key reuse. The SSH key can only be attached to one repo.\\n* There is no appetite for spending money on Service User accounts.\\n* Option 2 needs a process for creating the ssh key pair, adding the public key to the repo, and storing the private key in Vault (potentially automated via Terraform)\\n* Option 2 setup is an infrequent activity as would be key rotation\\n* Option 3 fundamentally cannot be done due to bugs\/non-support in Nomad\\n* Option 4 was eliminated due to possible violation of our acceptable use policies\\nChosen option: Option 2, Axiomatic Retrieves Deployment Keys\\n### Positive Consequences\\n* Allows us to provide a repeatable solution for configurationn repos in the short term\\n* We will default to using read-only ssh access\\n### Negative Consequences\\n* Requires codifying a key management process and a potential automation effort\\n","Prediction":"## Decision\n* GitHub prevents deployment key reuse. The SSH key can only be attached to one repo.\\n* There is no appetite for spending money on Service User accounts.\\n* Option 2 needs a process for creating the ssh key pair, adding the public key to the repo, and storing the private key in Vault (potentially automated via Terraform)\\n* Option 2 setup is an infrequent activity as would be key rotation\\n* Option 3 fundamentally cannot be done due to bugs\/non-support in Nomad\\n* Option 4 was eliminated due to possible violation of our acceptable use policies\\nOption 2 is not supported for Nomad due to the following:\\n- GitHub does not provide the infrastructure to support this\\n- Nomad does not support the creation of SSH keys\\n- Nomad does not provide the infrastructure to support the creation of a secret\\n- Nomad does not support the creation of a private key in Vault\\n- Nomad does not support the creation of a secret in Vault\\n- Nomad does not support the creation of a secret in Vault\\n- Nomad does not support the creation of a secret in Vault\\n- Nomad does not support the creation of a secret in Vault\\n- Nomad does not support the creation"}
{"Context":"## Context\\nCurrently, the messages exchanged between tendermint and a (potentially remote) signer\/validator,\\nnamely votes, proposals, and heartbeats, are encoded as a JSON string\\n(e.g., via `Vote.SignBytes(...)`) and then\\nsigned . JSON encoding is sub-optimal for both, hardware wallets\\nand for usage in ethereum smart contracts. Both is laid down in detail in [issue#1622].\\nAlso, there are currently no differences between sign-request and -replies. Also, there is no possibility\\nfor a remote signer to include an error code or message in case something went wrong.\\nThe messages exchanged between tendermint and a remote signer currently live in\\n[privval\/socket.go] and encapsulate the corresponding types in [types].\\n[privval\/socket.go]: https:\/\/github.com\/tendermint\/tendermint\/blob\/d419fffe18531317c28c29a292ad7d253f6cafdf\/privval\/socket.go#L496-L502\\n[issue#1622]: https:\/\/github.com\/tendermint\/tendermint\/issues\/1622\\n[types]: https:\/\/github.com\/tendermint\/tendermint\/tree\/master\/types\\n","Decision":"## Decision\n- restructure vote, proposal, and heartbeat such that their encoding is easily parseable by\\nhardware devices and smart contracts using a  binary encoding format ([amino] in this case)\\n- split up the messages exchanged between tendermint and remote signers into requests and\\nresponses (see details below)\\n- include an error type in responses\\n### Overview\\n```\\n+--------------+                      +----------------+\\n|              |     SignXRequest     |                |\\n|Remote signer |<---------------------+  tendermint    |\\n| (e.g. KMS)   |                      |                |\\n|              +--------------------->|                |\\n+--------------+    SignedXReply      +----------------+\\nSignXRequest {\\nx: X\\n}\\nSignedXReply {\\nx: X\\nsig: Signature \/\/ []byte\\nerr: Error{\\ncode: int\\ndesc: string\\n}\\n}\\n```\\nTODO: Alternatively, the type `X` might directly include the signature. A lot of places expect a vote with a\\nsignature and do not necessarily deal with \"Replies\".\\nStill exploring what would work best here.\\nThis would look like (exemplified using X = Vote):\\n```\\nVote {\\n\/\/ all fields besides signature\\n}\\nSignedVote {\\nVote Vote\\nSignature []byte\\n}\\nSignVoteRequest {\\nVote Vote\\n}\\nSignedVoteReply {\\nVote SignedVote\\nErr  Error\\n}\\n```\\n**Note:** There was a related discussion around including a fingerprint of, or, the whole public-key\\ninto each sign-request to tell the signer which corresponding private-key to\\nuse to sign the message. This is particularly relevant in the context of the KMS\\nbut is currently not considered in this ADR.\\n[amino]: https:\/\/github.com\/tendermint\/go-amino\/\\n### Vote\\nAs explained in [issue#1622] `Vote` will be changed to contain the following fields\\n(notation in protobuf-like syntax for easy readability):\\n```proto\\n\/\/ vanilla protobuf \/ amino encoded\\nmessage Vote {\\nVersion       fixed32\\nHeight        sfixed64\\nRound         sfixed32\\nVoteType      fixed32\\nTimestamp     Timestamp         \/\/ << using protobuf definition\\nBlockID       BlockID           \/\/ << as already defined\\nChainID       string            \/\/ at the end because length could vary a lot\\n}\\n\/\/ this is an amino registered type; like currently privval.SignVoteMsg:\\n\/\/ registered with \"tendermint\/socketpv\/SignVoteRequest\"\\nmessage SignVoteRequest {\\nVote vote\\n}\\n\/\/  amino registered type\\n\/\/ registered with \"tendermint\/socketpv\/SignedVoteReply\"\\nmessage SignedVoteReply {\\nVote      Vote\\nSignature Signature\\nErr       Error\\n}\\n\/\/ we will use this type everywhere below\\nmessage Error {\\nType        uint  \/\/ error code\\nDescription string  \/\/ optional description\\n}\\n```\\nThe `ChainID` gets moved into the vote message directly. Previously, it was injected\\nusing the [Signable] interface method `SignBytes(chainID string) []byte`. Also, the\\nsignature won't be included directly, only in the corresponding `SignedVoteReply` message.\\n[Signable]: https:\/\/github.com\/tendermint\/tendermint\/blob\/d419fffe18531317c28c29a292ad7d253f6cafdf\/types\/signable.go#L9-L11\\n### Proposal\\n```proto\\n\/\/ vanilla protobuf \/ amino encoded\\nmessage Proposal {\\nHeight            sfixed64\\nRound             sfixed32\\nTimestamp         Timestamp         \/\/ << using protobuf definition\\nBlockPartsHeader  PartSetHeader     \/\/ as already defined\\nPOLRound          sfixed32\\nPOLBlockID        BlockID           \/\/ << as already defined\\n}\\n\/\/ amino registered with \"tendermint\/socketpv\/SignProposalRequest\"\\nmessage SignProposalRequest {\\nProposal proposal\\n}\\n\/\/ amino registered with \"tendermint\/socketpv\/SignProposalReply\"\\nmessage SignProposalReply {\\nProp   Proposal\\nSig    Signature\\nErr    Error     \/\/ as defined above\\n}\\n```\\n### Heartbeat\\n**TODO**: clarify if heartbeat also needs a fixed offset and update the fields accordingly:\\n```proto\\nmessage Heartbeat {\\nValidatorAddress Address\\nValidatorIndex   int\\nHeight           int64\\nRound            int\\nSequence         int\\n}\\n\/\/ amino registered with \"tendermint\/socketpv\/SignHeartbeatRequest\"\\nmessage SignHeartbeatRequest {\\nHb Heartbeat\\n}\\n\/\/ amino registered with \"tendermint\/socketpv\/SignHeartbeatReply\"\\nmessage SignHeartbeatReply {\\nHb     Heartbeat\\nSig    Signature\\nErr    Error     \/\/ as defined above\\n}\\n```\\n","Prediction":"## Decision\nWe will use the [signer\/validator](https:\/\/github.com\/tendermint\/tendermint\/blob\/d419fffe18531317c28c29a292ad7d253f6cafdf\/privval\/socket.go#L462-L482) and\\n[heartbeat](https:\/\/github.com\/tendermint\/tendermint\/blob\/d419fffe18531317c28c29a292ad7d253f6cafdf\/privval\/heartbeat.go#L107-L112) protocols\\nfor signing and verifying.\\nThis will allow tendermint to use the existing JSON encoding scheme for messages\\nbetween tendermint and a remote signer, while providing a consistent\\nimplementation of the signer\/validator and heartbeat protocols.\\nThis change will also allow for the implementation of a secure channel\\nbetween tendermint and a remote signer, as a separate issue.\\nThe following changes will be made:\\n* Tendermint will use the `signer\/validator` and `heartbeat` protocols\\n* Tendermint will expose a new `signer\/validator` and `heartbeat` interface\\n* Tendermint will expose a new `signer\/validator` and `heartbeat` function\\n* Tendermint will expose a new `signer\/validator`"}
{"Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"## Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","Prediction":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\nWe will use [The Architecture Journal](http:\/\/thinkrelevance.com\/blog\/architecture-journal) to keep a running list of our decisions.\\nWe will use [Architecture Code Complete](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions) to keep a running list of our changes.\\nWe will use [Architecture Code Search](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions) to search for and find our architecture decisions.\\nWe will use [Architecture Code Search](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions) to search for and find our architecture decisions.\\nWe will use [Architecture Code Search](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions) to search for and find our architecture decisions.\\nWe will use [Architecture Code Search](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting"}
{"Context":"## Context\\nFrom [12factor.net](https:\/\/12factor.net\/config):\\n#### Store config in the environment\\nAn app\u2019s config is everything that is likely to vary between deploys (staging,\\nproduction, developer environments, etc). This includes:\\n* Resource handles to the database, Memcached, and other backing services\\n* Credentials to external services such as Amazon S3 or Twitter\\n* Per-deploy values such as the canonical hostname for the deploy\\nApps sometimes store config as constants in the code. This is a violation of\\ntwelve-factor, which requires strict separation of config from code. Config\\nvaries substantially across deploys, code does not.\\nA litmus test for whether an app has all config correctly factored out of the\\ncode is whether the codebase could be made open source at any moment, without\\ncompromising any credentials.\\nThe twelve-factor app stores config in environment variables (often shortened\\nto env vars or env). Env vars are easy to change between deploys without\\nchanging any code; unlike config files, there is little chance of them being\\nchecked into the code repo accidentally; and unlike custom config files, or\\nother config mechanisms such as Java System Properties, they are a language-\\nand OS-agnostic standard.\\nAnother aspect of config management is grouping. Sometimes apps batch config\\ninto named groups (often called \u201cenvironments\u201d) named after specific deploys,\\nsuch as the development, test, and production environments in Rails. This\\nmethod does not scale cleanly: as more deploys of the app are created, new\\nenvironment names are necessary, such as staging or qa. As the project grows\\nfurther, developers may add their own special environments like joes-staging,\\nresulting in a combinatorial explosion of config which makes managing deploys\\nof the app very brittle.\\nIn a twelve-factor app, env vars are granular controls, each fully orthogonal\\nto other env vars. They are never grouped together as \u201cenvironments\u201d, but\\ninstead are independently managed for each deploy. This is a model that scales\\nup smoothly as the app naturally expands into more deploys over its lifetime.\\n","Decision":"## Decision\nFollow the twelve-factor advice and use configuration from the environment.\\nWe will centralize the interface for the environment into a single location so\\nthat we can enforce strict requirements on vars. For example, the application\\nshould fail fast if a requirement is not valid. A `PORT` environment variable\\nshould be validated that it is positive number within the accepted port values.\\n#### Strictness\\nWe're currently using the npm package [`envalid`][1] to provide this\\nfunctionality. There are several libraries that serve this purpose.\\n[`envalid`][1] was specifically chosen because it doesn't have any\\nfunctionality that would allow developers to break this decision.\\nSome libraries allow merge of config files. Some libraries allow usage of\\n`NODE_ENV`. These features allow (if not encourage) developers to do the wrong\\nthing.\\n","Prediction":"## Decision\nUse a system like [Azure DevOps](https:\/\/azure.microsoft.com\/en-us\/services\/devops\/) or\\n[Google Cloud](https:\/\/cloud.google.com) to manage your config, instead of relying on\\nthe app\u2019s own config file.\\n### Config management for the application\\nThe Azure DevOps project configuration management system (CMS) is\\ndesigned for large-scale, long-lived applications. It is well-suited to\\nproviding consistent control over the configuration of your application,\\nand has many built-in features to support the application\u2019s lifecycle.\\n### Config management for the deployment process\\nThe deployment process relies on the codebase to provide consistent\\nconfigurations. This is achieved through the deployment process itself,\\nwhich involves deploying to a production server and pulling the code from\\nthe code repository. The deployment process itself is a well-understood\\nprocess that has been extensively documented in the Docker ecosystem.\\nThis makes it straightforward to use a CMS to manage the deployment process.\\n### Config management for the build process\\nThe build process is a well-understood process that has been extensively\\ndocumented in the Docker ecosystem. It is also well-understood in\\nthe context of the twelve-factor app. It is also well-understood in"}
{"Context":"## Context\\nWith the release of Tendermint 1.0 we will adopt [semantic versioning](https:\/\/semver.org). One major implication is a guarantee that we will not make backwards-incompatible changes until Tendermint 2.0 (except in pre-release versions). In order to provide this guarantee for our Go API, we must clearly define which of our APIs are public, and what changes are considered backwards-compatible.\\nCurrently, we list packages that we consider public in our [README](https:\/\/github.com\/tendermint\/tendermint#versioning), but since we are still at version 0.x we do not provide any backwards compatiblity guarantees at all.\\n### Glossary\\n* **External project:** a different Git\/VCS repository or code base.\\n* **External package:** a different Go package, can be a child or sibling package in the same project.\\n* **Internal code:** code not intended for use in external projects.\\n* **Internal directory:** code under `internal\/` which cannot be imported in external projects.\\n* **Exported:** a Go identifier starting with an uppercase letter, which can therefore be accessed by an external package.\\n* **Private:** a Go identifier starting with a lowercase letter, which therefore cannot be accessed by an external package unless via an exported field, variable, or function\/method return value.\\n* **Public API:** any Go identifier that can be imported or accessed by an external project, except test code in `_test.go` files.\\n* **Private API:** any Go identifier that is not accessible via a public API, including all code in the internal directory.\\n","Decision":"## Decision\nFrom Tendermint 1.0, all internal code (except private APIs) will be placed in a root-level [`internal` directory](https:\/\/golang.org\/cmd\/go\/#hdr-Internal_Directories), which the Go compiler will block for use by external projects. All exported items outside of the `internal` directory are considered a public API and subject to backwards compatibility guarantees, except files ending in `_test.go`.\\nThe `crypto` package may be split out to a separate module in a separate repo. This is the main general-purpose package used by external projects, and is the only Tendermint dependency in e.g. IAVL which can cause some problems for projects depending on both IAVL and Tendermint. This will be decided after further discussion.\\nThe `tm-db` package will remain a separate module in a separate repo. The `crypto` package may possibly be split out, pending further discussion, as this is the main general-purpose package used by other projects.\\n","Prediction":"## Decision\nWe will not provide backwards compatiblity guarantees for any of our Go packages.\\nThis decision is based on the following observations:\\n* All of our Go packages are intended for use by external projects.  We do not intend to make any changes to them that would affect an internal project.\\n* We do not intend to make any changes to our internal Go packages that would affect external projects.\\n* We do not intend to make any changes to our public Go packages that would affect external projects.\\n* We do not intend to make any changes to our public Go packages that would affect internal projects.\\n* We do not intend to make any changes to our public Go packages that would affect external projects.\\n* Our internal Go packages are intended for use by internal projects.  We do not intend to make any changes to them that would affect an external project.\\n* Our public Go packages are intended for use by external projects.  We do not intend to make any changes to them that would affect internal projects.\\n* Our internal Go packages are intended for use by internal projects.  We do not intend to make any changes to them that would affect external projects.\\n* Our public Go packages are intended for use by external projects. "}
{"Context":"## Context\\nWhen we decided to start off with separate API and user-facing applications\\n(see [ADR 0004](0004-separate-api-and-user-facing-applications.md)) we thought\\nthat we would need to build more than one service as part of this work, to\\nmeet different needs of different users. That may still be true in general, but\\nfrom what we've learned in the Handover discovery it's looking more likely at\\nthe moment that the Handover phase of our work will involve expanding on\\nAllocations rather than being an entirely separate journey and service.\\nWe were also intending that the allocation manager would be a fairly minimal\\nuser-facing application, which would fetch all the data it needed from the\\nallocation API and then render pages based directly on it.\\nWe've been looking at how we should set up authentication on our two\\napplications and have realised that that split of responsibilities between the\\nallocation manager and API could lead to some confusions:\\n- Calling the Custody API from the allocation API means that the allocation API\\ncan be seen as sitting in two different layers depending on the context:\\neither above the Custody API from the point of view of the allocation manager,\\nbut on the same level as the Custody API from the point of view of other\\nservices which need to use allocation data. This lack of clarity raises\\nquestions around which OAuth2 grant type to use for authentication on the\\nallocation API.\\n- Responsibility for access control may be split between the allocation\\nmanager and API. The user-facing application would need to know what the user\\nshould have access to in order to show the right options, but the API may\\nalso need to know what the user should have access to in order to fetch the\\nright data from other APIs.\\nWe still think there's value in having a separate allocation API, but it would\\nbe easier to understand if it were a more RESTful interface to the data it\\nstores.\\n","Decision":"## Decision\nThe allocation API will be a simpler interface to the data held in its database.\\nThe allocation API will not call other APIs to return data from them to the\\nallocation manager.\\nThe allocation manager will call the allocation API and other APIs (Custody,\\nDelius etc) directly.\\nThe allocation manager will be entirely responsible for access control for its\\nusers.\\n","Prediction":"## Decision\nWe will split the allocation API into two services:\\n- Allocation API: used to fetch data from allocations\\n- Allocation API Access: used to retrieve data from allocations\\nWe will use the `access` grant type on the allocation API, and a `read` grant\\ntype on the allocation API Access. This will allow us to keep the allocation\\nAPI as a single service, but separate the API Access from the allocation\\nAPI service.\\nWe will use the `access` grant type on the allocation API, and a `read` grant\\ntype on the allocation API Access. This will allow us to keep the allocation\\nAPI as a single service, but separate the API Access from the allocation\\nAPI service.\\nWe will use the `access` grant type on the allocation API, and a `read` grant\\ntype on the allocation API Access. This will allow us to keep the allocation\\nAPI as a single service, but separate the API Access from the allocation\\nAPI service.\\nWe will use the `access` grant type on the allocation API, and a `read` grant\\ntype on the allocation API Access. This will allow us to keep the allocation\\nAPI as a single service, but separate the API Access from the allocation"}
{"Context":"## Context and Problem Statement\\nPersistence and retrieval of SKOS taxonomies require a storage layer that supports storing rich, free-form linked data.\\nSuch a data model could be represented in a traditional RDBMS, however, doing so would require a specialized serialization and deserialization implementation whereas graph databases can typically store RDF natively.\\n## Decision Drivers <!-- optional -->\\n* High availability\/Fault tolerance\\n* Learning curve\\n* Maintenance overhead\\n* Vendor lock-in\\n","Decision":"## Decision\n* High availability\/Fault tolerance\\n* Learning curve\\n* Maintenance overhead\\n* Vendor lock-in\\nChosen option: Apache Jena with underlying PostgreSQL persistence store and a custom persistence layer, because it provides a highly available database persistence that is widely available as a managed service. We opted against using Apache Jena's SDB to achieve this as it has been in a state of \"maintenance only\" since June 2013.\\n### Positive Consequences\\n* PostgreSQL can be clustered, making it highly available\\n* PostgreSQL is broadly available as a managed service\\n* We can utilise the same PostgreSQL database for storing additional data (e.g. users, roles, etc)\\n### Negative Consequences\\n* We have to design our own schema and code for interacting with the PostgreSQL database\\n","Prediction":"## Decision\n* High availability\/Fault tolerance\\n* Learning curve\\n* Maintenance overhead\\n* Vendor lock-in\\nChosen option: \"RDBMS\", because\\n* High availability is supported\\n* No vendor lock-in is expected\\n* It is easy to get started with graph databases\\n* It is easy to migrate to graph databases from relational\\n* It is easy to use the graph database as a data store for SKOS\\n* It is easy to use SKOS as a data store for graph databases\\n* It is easy to write custom serialization\/deserialization for SKOS\\n* It is easy to use SKOS as a data store for graph databases\\n* It is easy to use SKOS as a data store for graph databases\\n* It is easy to use SKOS as a data store for graph databases\\n* It is easy to use SKOS as a data store for graph databases\\n* It is easy to use SKOS as a data store for graph databases\\n* It is easy to use SKOS as a data store for graph databases\\n* It is easy to use SKOS as a data store for graph databases\\n* It is easy to use SKOS as a data store for graph databases\\n* It is easy to use SKOS as"}
{"Context":"## Context\\nThere are currently many different ways of structuring an integration and its config in Home Assistant. We allow config YAML, either under the integration key or under platform keys with the integration name. We allow a config flow with configuration entered via the GUI, stored in config entries. We allow importing config YAML to a config entry via config flow.\\nThis ADR focuses on the configuration YAML structure and its use in integrations.\\nThe many options for configuration impact how the integration is structured and what Home Assistant backend APIs are used.\\n1. Integrations that only have a single platform, place the configuration under a platform key and use `async_setup_platform` to set up the platform.\\n2. Integrations that have multiple platforms, sometimes centralize the configuration to the integration domain name key and load platforms via `discovery.async_load_platform`, which in turn calls `async_setup_platform`.\\n3. If the integration has multiple platforms but doesn't have a centralized config, the user needs to add configuration under the integration domain to set up the integration and each platform key to set up each platform. This loads each platform via `async_setup_platform`.\\nAll backend APIs in point 1-3 above also have sync versions, to further increase the options.\\nThis multitude of options for config and integration structure makes it:\\nA. Harder for users to know how to configure the integrations.\\nB. Harder for contributors to know what way to implement and support.\\nC. Harder for Home Assistant to support confused users and contributors and harder to support and extend a disparate code base.\\n---\\n","Decision":"## Decision\nWe limit the configuration YAML structure to one way for new integrations. We require all YAML configuration, if present, for an integration to be located under the integration domain key in configuration YAML, for all new integrations (point 2 above).\\nFor existing integrations we don't allow changes to YAML configuration in platform sections until the integration has been refactored and the configuration moved under the integration domain key.\\n```yaml\\n# Allowed\\nawesome_integration:\\nusername: user\\n# Not allowed\\nsensor:\\n- platform: awesome_integration\\nusername: user\\n```\\n### Exceptions\\n- For integrations that do not fall under [ADR-0010](https:\/\/github.com\/home-assistant\/architecture\/blob\/master\/adr\/0010-integration-configuration.md), implementing an optional `unique_id` configuration key to set the unique ID of the entity created by the platform is allowed.\\n","Prediction":"## Decision\nWe will keep the current approach to configuration and configuration flow.\\nWe will support a centralized configuration option for platforms and integrations.\\nWe will support a centralized configuration option for platform key configuration.\\nWe will support a centralized configuration option for configuration flow.\\nWe will support a configuration flow that loads the configurations via the config key.\\nWe will support a configuration flow that loads the configurations via the platform key.\\nWe will support a configuration flow that loads the configurations via the configuration flow.\\nWe will support a configuration flow that loads the configurations via the user or config key.\\nWe will support a configuration flow that loads the configurations via the platform key.\\nWe will support a configuration flow that loads the configurations via the configuration flow.\\nWe will support a configuration flow that loads the configurations via the user or config key.\\nWe will support a configuration flow that loads the configurations via the platform key.\\nWe will support a configuration flow that loads the configurations via the configuration flow.\\nWe will support a configuration flow that loads the configurations via the user or config key.\\nWe will support a configuration flow that loads the configurations via the platform key.\\nWe will support a configuration flow that loads the configurations via the configuration flow.\\nWe will support"}
{"Context":"## Context and Problem Statement\\n**What kind of database should be used to storage aggregates data ?**\\nConsidered options:\\n1. Document database (MongoDB)\\n2. Relational database\\nDrivers:\\n* Simple objects will be saved.\\n* There will be no relationship between objects .\\n* A Read Model combining data from different aggregates is required .\\n","Decision":"## Decision\nOption no 1 - document database - MongoDB.\\n","Prediction":"## Decision\nWe will use MongoDB as the database.\\nMongoDB as the database is the most used database in the world and it is the most mature database.\\nMongoDB is a NoSQL database and it is the most popular NoSQL database.\\nMongoDB is the only NoSQL database that is SQL compatible.\\nMongoDB is the only NoSQL database that is built on top of the NoSQL document database.\\nMongoDB supports the following features:\\n* NoSQL\\n* JSON\\n* JSONB\\n* JSONSchema\\n* Server-Side Encryption\\n* Sharding\\n* Schema Evolution\\n* Schema History\\n* Sharding\\n* Sharded ColumnStore\\n* Sharded ColumnStore\\n* Sharded ColumnStore\\n* Sharded ColumnStore\\n* Sharded ColumnStore\\n* Sharded ColumnStore\\n* Sharded ColumnStore\\n* Sharded ColumnStore\\n* Sharded ColumnStore\\n* Sharded ColumnStore\\n* Sharded ColumnStore\\n* Sharded ColumnStore\\n* Sharded ColumnStore\\n* Sharded ColumnStore\\n* Sharded ColumnStore\\n* Sharded ColumnStore\\n* Sharded ColumnStore\\n* Sharded ColumnStore\\n* Sharded ColumnStore\\n* Sharded ColumnStore\\n* Sharded Column"}
{"Context":"## Context\\nDevelopers tend to go for getting things working, ending up with a working program, lacking the quality you would like to see.\\n![Build breaker](buildbreaker.jpg \"Agent stopping construction worker from adding wrong quality component to the project\")\\n(Picture by Jo\u00eblle van Veen)\\nQuality software is not only software that is doing what it should do. Next to the functionality, a maybe even larger amount of non-functional requirements determine the quality of software. The ISO\/IEC 25010 software quality model describes performance, compatibility, portability, usability, reliability, security and maintainability as additional aspects of software quality (1). But assuring the required quality in all of these aspects appears to be hard when finishing the last few lines of code of the new feature at the end of a long working day.\\nThe result of low quality software can be performance issues, increased maintenance cost, bugs, difficulty in changing the software and security vulnerabilities. These are all things you would rather avoid, but that requires building good quality software. So, despite high workload and despite the urge to finish that new feature today, we do not want anyone to write low quality code. How to stop a developer from doing that?\\n_The only way to stop a developer is to break the build!_\\nEvery time new code is written and added to the program, the build pipeline is running, a practice known as continuous integration (2). The build pipeline builds the program and runs various checks to see if the program is behaving according to the requirements. If something is wrong, the build pipeline will stop and report about the problem, which is referred to as a broken build. The problem will have to be fixed first to get to a successful build.\\nSo if we want to stop a developer from writing low quality code, we must make sure the build breaks as soon as low quality code is added. Therefore, we add build-breaking checks (build-breakers), to the build pipeline that check the quality of the code added to the program. Examples of these checks are _static application security tests (SAST)_, which examine the programming code for security issues, static code analysis, which check for coding style and coding issues, _dynamic application security tests (DAST)_, which examine the running program for security issues, _unit tests_, which automatically perform small tests on the program, _fuzzing tests_, which send random input to the program and check for unwanted behaviour, _performance tests_ and _end-to-end tests_, which test the program as a whole.\\n","Decision":"## Decision\nWe will add build-breakers to the build pipeline performing SAST and DAST scans and also other non-security checks.\\n","Prediction":"## Decision\nWe will use the [build-breaking check](https:\/\/github.com\/hamlapi\/build-breakers) to prevent a developer from writing low quality code.\\nWe will use the [build-breaking check](https:\/\/github.com\/hamlapi\/build-breakers) to prevent a developer from writing low quality code.\\nWe will use the [build-breaking check](https:\/\/github.com\/hamlapi\/build-breakers) to prevent a developer from writing low quality code.\\nWe will use the [build-breaking check](https:\/\/github.com\/hamlapi\/build-breakers) to prevent a developer from writing low quality code.\\nWe will use the [build-breaking check](https:\/\/github.com\/hamlapi\/build-breakers) to prevent a developer from writing low quality code.\\nWe will use the [build-breaking check](https:\/\/github.com\/hamlapi\/build-breakers) to prevent a developer from writing low quality code.\\nWe will use the [build-breaking check](https:\/\/github.com\/hamlapi\/build-breakers) to prevent a developer from writing low quality code.\\nWe will use the [build-breaking check](https:\/\/github.com\/hamlapi\/build-breakers) to prevent a developer from writing low quality"}
{"Context":"## Context\\nWe need to know what the application is doing in a more light weight way than\\nscraping logs. We need to be able to monitor KPIs of the application in order\\nto understand the health of the application. This will allow us to react and\\npotentially pro-actively initiate measures as to ensure the application's\\nhealth if sound. Ultimately providing a better service for our users.\\n","Decision":"## Decision\nWe will use Prometheus to monitor and alert on the state of the application.\\n","Prediction":"## Decision\nTo introduce the use of `log4j2` as a logging framework. This will provide us with a\\nlightweight logging framework that can be used in conjunction with the\\nmicroservices architecture. We will use this framework rather than the standard\\ntext based log4j. This will allow us to monitor the data that is being\\nlogged in a lightweight way.\\nAs a result of this, we will be able to:\\n- Reduce the number of log lines in the application\\n- Monitor metrics in a lightweight way\\n- Pro-actively initiate measures to ensure the application's health if sound\\n- Provide a better service for our users\\n- Reduce the amount of log lines in the application\\n- Monitor metrics in a lightweight way\\n- Pro-actively initiate measures to ensure the application's health if sound\\n- Provide a better service for our users\\n- Support the use of microservices in the application\\n- Support the use of logging in microservices\\n- Provide a lightweight framework for logging\\n- Provide a lightweight framework for monitoring\\n- Provide a lightweight framework for pro-actively initiating measures\\n- Support the use of logging in microservices\\n- Support the use of microservices\\n- Support the use of logging in microservices\\n-"}
{"Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","Prediction":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\nWe will use [CockroachDB](http:\/\/cockroachdb.com\/) to manage the ADRs.\\nWe will use [PostgreSQL](https:\/\/www.postgresql.org\/) for the data layer.\\nWe will use [PostgreSQL-JSON](https:\/\/www.postgresql.org\/JSON\/) for the data layer.\\nWe will use [PostgreSQL-JSON-P](https:\/\/www.postgresql.org\/JSON-P\/) for the data layer.\\nWe will use [PostgreSQL-JSON-SNAP](https:\/\/www.postgresql.org\/JSON-SNAP\/) for the data layer.\\nWe will use [PostgreSQL-JSON-Tabular](https:\/\/www.postgresql.org\/JSON-Tabular\/) for the data layer.\\nWe will use [PostgreSQL-JSON-MapReduce](https:\/\/www.postgresql.org\/JSON-MapReduce\/) for the data layer.\\nWe will use [PostgreSQL-JSON-Map](https:\/\/www.postgresql.org\/JSON-Map\/) for the data layer.\\nWe will"}
{"Context":"## Context\\nTDP application requires strong multi-factor authentication (MFA) for all users, and Personal Identity Verification (PIV) authentication must be used as the 2nd factor for all internal ACF staff. To date, TDP has been leveraging Login.gov authentication for all users because (1) it requires MFA for all user accounts by default and accepts PIV for authentication, (2) it has a FedRAMP ATO on file, and (3) HHS already has an IAA for this service.\\nSince then, ACF OCIO (TDP's Authorizing Official) has recommended use of HHS-vetted authentication services for both internal ACF and external (i.e.non-ACF) users.\\nWe initiated the integratation with NextGen XMS for external users in Spring 2022 because of the estimated cost savings, and because the refactoring was expected to be minimal (since NextGen leverages login.gov as one of its credential service providers). However, the time\/effort to integrate with this newer service led to significant roadmap delays.\\n","Decision":"## Decision\nWe will use [ACF AMS](https:\/\/hhsgov.sharepoint.com\/:w:\/s\/TANFDataPortalOFA\/EYsh4YvAE0hLrr_rVhYKsGABgeA_yuHzt-v7TGbxaBF2jA?e=zriBjY) authentication service for ACF users and [Login.gov](login.gov) authentication service for all non-ACF users.\\n","Prediction":"## Decision\nWe will use the HHS-vetted NextGen XMS service for all user accounts and will not implement the recommended integration with Login.gov.\\nWe will continue to leverage login.gov for external users.\\nWe will re-evaluate the need to integrate with NextGen XMS for internal users after we have completed the migration of the TDP application to the new NextGen XMS platform.\\nWe will explore the use of the HHS-vetted SAML SSO for external users to improve the single-sign-on experience.\\nWe will evaluate the use of the HHS-vetted SAML SSO for internal users after we have completed the migration of the TDP application to the new NextGen XMS platform.\\nWe will evaluate the use of the HHS-vetted SAML SSO for internal users after we have completed the migration of the TDP application to the new NextGen XMS platform.\\nWe will evaluate the use of the HHS-vetted SAML SSO for internal users after we have completed the migration of the TDP application to the new NextGen XMS platform.\\nWe will evaluate the use of the HHS-vetted SAML SSO for internal users after we have completed the migration"}
{"Context":"## Context\\nRoad nodes, road segments and grade separated junctions are brought into existence by an operator.\\nIn doing so, an operator assigns (often thru tooling) an identifier in his \/ her local environment for a newly created entity.\\n","Decision":"## Decision\nWe call these identifiers created by the operator temporary identifiers. The tooling usually creates them as rather big numbers as to not collide with the already allocated identifiers of existing entities.\\nIt's important to note that the notion of a temporary identifier only exists when adding a type of entity.\\nWhile changing the road network we assign a permanent identifier for each temporary identifier. This is usually the next free number. This happens for each of the type of entities separately (nodes, segments, grade separated junctions).\\nWe remember the mapping between the temporary and the permanent identifier by recording it in the events that add the specific entity type.\\nWe give UI feedback to the operator about a particular upload using the temporary identifiers since those are the only ones that make sense to them.\\nOnce they re-download the road registry they will start using the permanent identifiers.\\n","Prediction":"## Decision\nRoad nodes, road segments and grade separated junctions are brought into existence by an operator.\\nIn doing so, an operator assigns (often thru tooling) an identifier in his \/ her local environment for a newly created entity.\\n### Road nodes\\nRoad nodes are created by operators when they are first created.\\n### Road segments\\nRoad segments are created by operators when they are first created.\\n### Grade separated junctions\\nGrade separated junctions are created by operators when they are first created.\\n### Road segments\\nRoad segments are created by operators when they are first created.\\n### Grade separated junctions\\nGrade separated junctions are created by operators when they are first created.\\n### Road nodes\\nRoad nodes are created by operators when they are first created.\\n### Road segments\\nRoad segments are created by operators when they are first created.\\n### Grade separated junctions\\nGrade separated junctions are created by operators when they are first created.\\n### Road nodes\\nRoad nodes are created by operators when they are first created.\\n### Road segments\\nRoad segments are created by operators when they are first created.\\n### Grade separated junctions\\nGrade separated junctions are created by operators when they are first created.\\n### Road nodes"}
{"Context":"## Context\\nWe need an ORM, especially to use Flask-Security.\\nOnly 4 are available at the moment:\\n* Flask-SQLAlchemy: too bloated\\n* Flask-MongoEngine: relies on Mongo (harder migrations)\\n* Flask-Peewee: initial choice, not maintained anymore\\n* PonyORM: looks great but the code is hard to contribute to\\n","Decision":"## Decision\nWe will use Flask-MongoEngine and do migrations manually as of now.\\nWe might want to find\/code a tool for that in the future.\\n","Prediction":"## Decision\nUse PonyORM, a lightweight ORM for SQLAlchemy. It's an open source project and has a great community (see https:\/\/github.com\/pony\/pony\/issues).\\nIt's also the ORM of choice for Flask-Security, see https:\/\/github.com\/Flask\/Flask\/issues\/3083.\\nPonyORM is also used by the Flask-Security project.\\nPonyORM is currently the only ORM that can be used with SQLAlchemy and SQLAlchemy supports SQLAlchemy migrations. This means that we can use it with Flask-Security.\\nPonyORM is not bloated, it's the opposite. It's also the only ORM that is actively maintained.\\nPonyORM is also the only ORM that is used by the Flask-Security project.\\nPonyORM is also the only ORM that supports SQLAlchemy migrations, see https:\/\/github.com\/pony\/pony\/issues.\\nIt also comes with a nice web interface.\\nPonyORM is also used by the Flask-Security project.\\nPonyORM is used by the Flask-Security project.\\nPonyORM is used by the Flask-Security project.\\nPonyORM is used by the Flask-Security project.\\nPonyORM is used by the"}
{"Context":"## Context\\nWe use [Sidekiq](https:\/\/github.com\/mperham\/sidekiq) to do background processing in Email Alert API. This includes critical tasks, such as generating and sending email. However, it's possible for these jobs to be lost:\\n1. If the code to enqueue the job in Redis fails, either for transient reasons, or due to a bug.\\n2. If a worker process quits ungracefully, while processing a job.\\nThe second issue has occurred frequently due to workers being [forcibly restarted][icinga-memory] when they consume too much memory. A memorable example was the loss of a Travel Advice content change, which was detected by the [alert][travel-advice-alert] for travel advice emails, and lead to an incident. Conversely, we don't think the first issue occurs frequently.\\nTo help support engineers cope with certain kinds of lost work, we had [manual steps][old-recovery-steps] to recover it. We had this for the jobs to process content changes and [messages][old-recovery-steps-messages]. The manual steps involved checking the state of the database, as an indication of whether jobs might have been lost. However, it's possible checking the database will turn up false positives e.g. if the system is running slowly. Both workers would [double check][content-change-psuedo-idempotent] to prevent accidentally duplicating work.\\nIdeally these issues wouldn't exist in the first place. Previously we considered upgrading to Sidekiq Pro, which [persists][sidekiq-reliability] jobs in Redis while they are being processed. Using Sidekiq Pro would therefore prevent the second issue, but not the first, which is unrelated to Sidekiq. It's also worth noting that we previously had [concerns][sidekiq-pro-issue] about switching to Sidekiq Pro, thinking it would be hard to use in practice. Still, it's possible the benefits could outweigh the drawbacks.\\n[content-change-psuedo-idempotent]: https:\/\/github.com\/alphagov\/email-alert-api\/commit\/e4a8fedcaf2ce504f238ef53ee9fdf2e3319e30a#diff-ccd4e2b91347f03e81b4ccfae7983d17c508c7cce216a51596ccaf7093848a2cR6\\n[old-recovery-steps]: https:\/\/github.com\/alphagov\/govuk-developer-docs\/blob\/7feaedc9bd9f786662055bba45327ddc8b318525\/source\/manual\/alerts\/email-alert-api-unprocessed-content-changes.html.md\\n[old-recovery-steps-messages]: https:\/\/github.com\/alphagov\/govuk-developer-docs\/blob\/7feaedc9bd9f786662055bba45327ddc8b318525\/source\/manual\/alerts\/email-alert-api-unprocessed-messages.html.md\\n[sidekiq-reliability]: https:\/\/github.com\/mperham\/sidekiq\/wiki\/Reliability#using-super_fetch\\n[icinga-memory]: https:\/\/github.com\/alphagov\/govuk-puppet\/blob\/1258fc65da264191d01b4280cd4422f90085c371\/modules\/monitoring\/files\/usr\/local\/bin\/event_handlers\/govuk_app_high_memory.sh#L22\\n[travel-advice-alert]: https:\/\/github.com\/alphagov\/govuk-puppet\/blob\/be2e3733d9cad619d9cf57e19b190cd5d6116342\/modules\/govuk_jenkins\/manifests\/jobs\/email_alert_check.pp\\n[sidekiq-pro-issue]: https:\/\/github.com\/mperham\/sidekiq\/issues\/4612\\n","Decision":"## Decision\nWe decided to implement a new [worker][recovery-worker] to automatically recover all work associated with generating and sending email. This resolves both of the above issues with lost work, and [codifies][old-recovery-steps] the previously manual recovery steps. Since there is little urgency around sending email, we decided to run the worker only infrequently (currently [every half hour][recovery-schedule]), for work that's [over an hour old][recovery-threshold] - we expect most work to have been processed within an hour.\\nAn edge case for recovery is the initiation of the [daily][daily-init] and [weekly][weekly-init] digest runs. If one of these scheduled jobs is lost before it can [create][digest-run-create] a DigestRun record, our normal strategy of checking the database won't work. For this scenario, we decided to have a [separate][digest-init-recovery] recovery strategy that's coupled to the [schedule][digest-schedule] for the initiator jobs. The strategy involves looking back over the previous week to see if any DigestRun records are missing for that period.\\nWe decided to pursue recovering lost work instead of acquiring Sidekiq Pro, which would help avoid the loss in the first place. We had the impression that the process to pay for, acquire and integrate Sidekiq Pro into Email Alert API would take longer to complete than implementing our own solution.\\n[recovery-worker]: https:\/\/github.com\/alphagov\/email-alert-api\/blob\/101f813d97c3838199e40643e681f1aca6adf67b\/app\/workers\/recover_lost_jobs_worker.rb\\n[recovery-schedule]: https:\/\/github.com\/alphagov\/email-alert-api\/blob\/59c37a1571d1564d1c63fd913274c810b0742ee6\/config\/sidekiq.yml#L39\\n[recovery-threshold]: https:\/\/github.com\/alphagov\/email-alert-api\/blob\/ea21e7cfd4f6131e2db55e45b923dee3895b081a\/app\/workers\/recover_lost_jobs_worker\/unprocessed_check.rb#L13\\n[daily-init]: https:\/\/github.com\/alphagov\/email-alert-api\/blob\/bae78a3cb7e970d290d4a95613149c1d5d34e4f1\/app\/workers\/daily_digest_initiator_worker.rb\\n[weekly-init]: https:\/\/github.com\/alphagov\/email-alert-api\/blob\/bae78a3cb7e970d290d4a95613149c1d5d34e4f1\/app\/workers\/weekly_digest_initiator_worker.rb\\n[digest-init-recovery]: https:\/\/github.com\/alphagov\/email-alert-api\/blob\/bae78a3cb7e970d290d4a95613149c1d5d34e4f1\/app\/workers\/recover_lost_jobs_worker\/missing_digest_runs_check.rb\\n[digest-schedule]: https:\/\/github.com\/alphagov\/email-alert-api\/blob\/fe6ce7a61fa8ab6692786086c91d4d6017c9b2fa\/config\/sidekiq.yml#L17-L22\\n[digest-run-create]: https:\/\/github.com\/alphagov\/email-alert-api\/blob\/fe6ce7a61fa8ab6692786086c91d4d6017c9b2fa\/app\/services\/digest_initiator_service.rb#L8\\n","Prediction":"## Decision\nWe will use Sidekiq to handle background processing in Email Alert API. This means we will no longer use the [manual steps][old-recovery-steps] to recover lost jobs.\\nWe will investigate how to [freeze the state of the database][freeze-the-state-of-the-database] in Sidekiq. We will also investigate how to [check the state of the database][check-the-state-of-the-database] in Sidekiq. We will investigate whether Sidekiq is a good fit for Email Alert API. We will not commit to Sidekiq until this work is complete.\\nWe will [update the state of the database][update-the-state-of-the-database] to [freeze the state of the database][freeze-the-state-of-the-database] in Sidekiq. This will ensure that lost jobs are recoverable in Sidekiq. We will [restore lost jobs][restore-lost-jobs] in Sidekiq. The [restore-lost-jobs] script will be written as a [Sidekiq action][sidekiq-action].\\nWe will [automatically retry lost jobs][automatically-retry-lost-jobs] in Sidekiq. This will help avoid memory errors when workers"}
{"Context":"## Context and Problem Statement\\nHamlet is currently built on two application stacks, a set of bash scripts which run deployments and invoke the generation of outputs, and a freemarker wrapper with supporting templates which handles the creation of outputs. There is a tight coupling between these components and it is difficult to determine the authoritative source of context which we share between the these to stacks.\\n## Decision Drivers\\n* To improve user experience we have introduced a cli tool which is based on the [click library](https:\/\/click.palletsprojects.com) which is built in python. Integrating this cli into the environment has highlighted the need for this cli tool but also exposed the tight coupling between these two application stacks\\n","Decision":"## Decision\n* To improve user experience we have introduced a cli tool which is based on the [click library](https:\/\/click.palletsprojects.com) which is built in python. Integrating this cli into the environment has highlighted the need for this cli tool but also exposed the tight coupling between these two application stacks\\nChosen option: \"Create a clearly defined abstraction layer between the existing bash scripts and the freemarker wrapper\"\\nMakes our architecture cleaner and provides a clear way for hamlet to scale in the future\\n### Positive Consequences\\n* We have more explicit definitions of what hamlet is made of and how it works\\n* We can make choices on appropriate tools for specific services within hamlet\\n### Negative Consequences\\n* Defining and building the layer separation will add complexity to hamlet\\n","Prediction":"## Decision\n* To improve user experience we have introduced a cli tool which is based on the [click library](https:\/\/click.palletsprojects.com) which is built in python. Integrating this cli into the environment has highlighted the need for this cli tool but also exposed the tight coupling between these two application stacks\\nChosen option: \"Hamlet as a cli tool\", because: * The cli tool is built on top of click and is familiar to developers working with the click library * The cli tool is built on top of the freemarker template engine which is well known to the click library developers * The cli tool is built on top of python which is well known to the click library developers * The cli tool is built on top of the freemarker templating system which is well known to the click library developers * The cli tool is built on top of the freemarker templating system which is well known to the click library developers * The cli tool is built on top of python which is well known to the click library developers * The cli tool is built on top of python which is well known to the click library developers * The cli tool is built on top of python which is well known to the click library developers * The cli tool is built on"}
{"Context":"## Context\\nWe want to be able to distribute this library to me ingested by TypeScript or\\nplain JavaScript (both commonJS and module) applications.\\n[Rollup](https:\/\/rollupjs.org\/guide\/en\/) is a popular JavaScript bundler with\\nsupport for TypeScript and simple configuration.\\n","Decision":"## Decision\nWe will build distributables using Rollup.js.\\n","Prediction":"## Decision\nWe will use Rollup as our bundler.\\nRollup supports commonJS, ES2015, and CommonJS modules and is the recommended\\nbundler for TypeScript applications. It also supports commonJS modules in\\nplain JavaScript applications.\\nWe will use Rollup with a configuration file to configure Rollup. See\\n[Rollup configuration](https:\/\/rollupjs.org\/guide\/en\/configuration.html#configuration)\\nfor more information on configuration.\\nWe will use Rollup with a package.json file to configure Rollup. See\\n[Rollup package.json](https:\/\/rollupjs.org\/guide\/en\/package.json.html#rollup-package-json)\\nfor more information on package.json.\\nWe will use Rollup with a .d.ts file to configure Rollup. See\\n[Rollup .d.ts](https:\/\/rollupjs.org\/guide\/en\/rollup.d.ts.html#rollup-file) for more information on .d.ts files.\\nWe will use Rollup with a package.json file to configure Rollup. See\\n[Rollup package.json](https:\/\/rollupjs.org\/guide\/en\/package.json.html#rollup-package-json) for more information on package.json.\\nWe will use Rollup with a"}
{"Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"## Decision\nWe will use Architecture Decision Records, as described by Michael Nygard\\nin this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","Prediction":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\nWe will use [Jira](https:\/\/marketplace.atlassian.com\/plugins\/support\/1.0\/jira) as the issue tracking system.\\nWe will use the [Jira Architecture Decision Framework](https:\/\/marketplace.atlassian.com\/plugins\/support\/1.0\/project-architecture-decision-framework\/)\\nfor architecture decisions.\\nWe will use [CircleCI](https:\/\/circleci.com\/) for continuous integration and deployment.\\nWe will use [CircleCI's Continuous Integration](https:\/\/circleci.com\/docs\/1.0\/integrations\/continuous-integration\/) feature to automatically run tests when CI is enabled.\\nWe will use the [CircleCI's Continuous Deployment](https:\/\/circleci.com\/docs\/1.0\/integrations\/continuous-deployment\/) feature to automatically deploy code changes to production when CI is enabled.\\nWe will use the [CircleCI's Continuous Testing](https:\/\/circleci.com\/docs\/1.0\/integrations\/continuous-testing\/) feature to automatically test code changes before they are merged to master"}
{"Context":"## Context\\nWe need somewhere to store state, and we need a way to get data in \/ out of that store.\\n","Decision":"## Decision\nTypeorm implements DataMapper, which is a low-coupled way to work with databases. It seems pretty mature and well-used. It's good enough for now.\\n","Prediction":"## Decision\nWe will use DynamoDB as our database.\\nWe will use DynamoDB tables for storing our state data.\\nWe will use DynamoDB views for exposing our data to clients.\\nWe will use DynamoDB Kinesis for streaming data to clients.\\nWe will use DynamoDB Streams for reading data from clients.\\nWe will use DynamoDB Streams for writing data to clients.\\nWe will use DynamoDB Kinesis Streams for streaming data to clients.\\nWe will use DynamoDB Streams for reading data from clients.\\nWe will use DynamoDB Kinesis Streams for writing data to clients.\\nWe will use DynamoDB Kinesis Streams for streaming data to clients.\\nWe will use DynamoDB Kinesis Streams for reading data from clients.  This will be used to create a state machine that will be used to manage our state.\\nWe will use DynamoDB Kinesis Streams for reading data from clients.  This will be used to create a state machine that will be used to manage our state.\\nWe will use DynamoDB Kinesis Streams for reading data from clients.  This will be used to create a state machine that will be used to manage our state.\\nWe will use DynamoDB Kinesis Streams for reading data from clients.  This"}
{"Context":"## Context\\nWe want to have holochain run in the browser (for Holo).\\nRust can compile to WASM. Rust is language for experts. Rust is trending.\\nThere is a WASM interpreter in Rust (WASMI).\\nOnly problem might be problems integration with Promether \/ linking against C++:\\n* https:\/\/doc.rust-lang.org\/nomicon\/ffi.html\\n* https:\/\/users.rust-lang.org\/t\/linking-with-custom-c-library\/637\\n","Decision":"## Decision\nRewrite Holochain in the Rust programming language\\n","Prediction":"## Decision\nWe will use WASM and build it with Cargo.\\nWe will use WASMI to link against C++.\\nWe will use WASM to link against C++ with optional optimizations.\\nWe will use WASMI to link against C++ with optional optimizations.\\nWe will use WASM to link against C++ with optional optimizations.\\nWe will use WASM to link against C++ with optional optimizations.\\nWe will use WASM to link against C++ with optional optimizations.\\nWe will use WASM to link against C++ with optional optimizations.\\nWe will use WASM to link against C++ with optional optimizations.\\nWe will use WASM to link against C++ with optional optimizations.\\nWe will use WASM to link against C++ with optional optimizations.\\nWe will use WASM to link against C++ with optional optimizations.\\nWe will use WASM to link against C++ with optional optimizations.\\nWe will use WASM to link against C++ with optional optimizations.\\nWe will use WASM to link against C++ with optional optimizations.\\nWe will use WASM to link against C++ with optional optimizations.\\nWe will use WASM to link against C++ with optional optimizations.\\nWe will use WAS"}
{"Context":"## Context\\nMailQueue is a central component of SMTP infrastructure allowing asynchronous mail processing. This enables a short\\nSMTP reply time despite a potentially longer mail processing time. It also works as a buffer during SMTP peak workload\\nto not overload a server.\\nFurthermore, when used as a Mail Exchange server (MX), the ability to add delays to be observed before dequeing elements\\nallows, among others:\\n- Delaying retries upon MX delivery failure to a remote site.\\n- Throttling, which could be helpful for not being considered a spammer.\\nA mailqueue also enables advanced administration operations like traffic review, discarding emails, resetting wait\\ndelays, purging the queue, etc.\\nSpring implementation and non distributed implementations rely on an embedded ActiveMQ to implement the MailQueue.\\nEmails are being stored in a local file system. An administrator wishing to administrate the mailQueue will thus need\\nto interact with all its James servers, which is not friendly in a distributed setup.\\nDistributed James relies on the following third party softwares (among other):\\n- **RabbitMQ** for messaging. Good at holding a queue, however some advanced administrative operations can't be\\nimplemented with this component alone. This is the case for `browse`, `getSize` and `arbitrary mail removal`.\\n- **Cassandra** is the metadata database. Due to **tombstone** being used for delete, queue is a well known anti-pattern.\\n- **ObjectStorage** (Swift or S3) holds byte content.\\n","Decision":"## Decision\nDistributed James should ship a distributed MailQueue composing the following softwares with the following\\nresponsibilities:\\n- **RabbitMQ** for messaging. A rabbitMQ consumer will trigger dequeue operations.\\n- A time series projection of the queue content (order by time list of mail metadata) will be maintained in **Cassandra** (see later). Time series avoid the\\naforementioned tombstone anti-pattern, and no polling is performed on this projection.\\n- **ObjectStorage** (Swift or S3) holds large byte content. This avoids overwhelming other softwares which do not scale\\nas well in term of Input\/Output operation per seconds.\\nHere are details of the tables composing Cassandra MailQueue View data-model:\\n- **enqueuedMailsV3** holds the time series. The primary key holds the queue name, the (rounded) time of enqueue\\ndesigned as a slice, and a bucketCount. Slicing enables listing a large amount of items from a given point in time, in an\\nfashion that is not achievable with a classic partition approach. The bucketCount enables sharding and avoids all writes\\nat a given point in time to go to the same Cassandra partition. The clustering key is composed of an enqueueId - a\\nunique identifier. The content holds the metadata of the email. This table enables, from a starting date, to load all of\\nthe emails that have ever been in the mailQueue. Its content is never deleted.\\n- **deletedMailsV2** tells wether a mail stored in *enqueuedMailsV3* had been deleted or not. The queueName and\\nenqueueId are used as primary key. This table is updated upon dequeue and deletes. This table is queried upon dequeue\\nto filter out deleted\/purged items.\\n- **browseStart** store the latest known point in time from which all previous emails had been deleted\/dequeued. It\\nenables to skip most deleted items upon browsing\/deleting queue content. Its update is probability based and\\nasynchronously piggy backed on dequeue.\\nHere are the main mail operation sequences:\\n- Upon **enqueue** mail content is stored in the *object storage*, an entry is added in *enqueuedMailsV3* and a message\\nis fired on *rabbitMQ*.\\n- **dequeue** is triggered by a rabbitMQ message to be received. *deletedMailsV2* is queried to know if the message had\\nalready been deleted. If not, the mail content is retrieved from the *object storage*, then an entry is added in\\n*deletedMailsV2* to notice the email had been dequeued. A dequeue has a random probability to trigger a browse start\\nupdate. If so, from current browse start, *enqueuedMailsV3* content is iterated, and checked against *deletedMailsV2*\\nuntil the first non deleted \/ dequeued email is found. This point becomes the new browse start. BrowseStart can never\\npoint after the start of the current slice. A grace period upon browse start update is left to tolerate clock skew.\\nUpdate of the browse start is done randomly as it is a simple way to avoid synchronisation in a distributed system: we\\nensure liveness while uneeded browseStart updates being triggered would simply waste a few resources.\\n- Upon **browse**, *enqueuedMailsV3* content is iterated, and checked against *deletedMailsV2*, starting from the\\ncurrent browse start.\\n- Upon **delete\/purge**, *enqueuedMailsV3* content is iterated, and checked against *deletedMailsV2*. Mails matching\\nthe condition are marked as deleted in *enqueuedMailsV3*.\\n- Upon **getSize**, we perform a browse and count the returned elements.\\nThe distributed mail queue requires a fine tuned configuration, which mostly depends of the count of Cassandra servers,\\nand of the mailQueue throughput:\\n- **sliceWindow** is the time period of a slice. All the elements of **enqueuedMailsV3** sharing the same slice are\\nretrieved at once. The bigger, the more elements are going to be read at once, the less frequent browse start update\\nwill be. Lower values might result in many almost empty slices to be read, generating higher read load. We recommend\\n**sliceWindow** to be chosen from users maximum throughput so that approximately 10.000 emails be contained in a slice.\\nOnly values dividing the current *sliceWindow* are allowed as new values (otherwize previous slices might not be found).\\n- **bucketCount** enables spreading the writes in your Cassandra cluster using a bucketting strategy. Low values will\\nlead to workload not to be spread evenly, higher values might result in uneeded reads upon browse. The count of Cassandra\\nservers should be a good starting value. Only increasing the count of buckets is supported as a configuration update as\\ndecreasing the bucket count might result in some buckets to be lost.\\n- **updateBrowseStartPace** governs the probability of updating browseStart upon dequeue\/deletes. We recommend choosing\\na value guarantying a reasonable probability of updating the browse start every few slices. Too big values will lead to\\nuneeded update of not yet finished slices. Too low values will end up in a more expensive browseStart update and browse\\niterating through slices with all their content deleted. This value can be changed freely.\\nWe rely on eventSourcing to validate the mailQueue configuration changes upon James start following the aforementioned rules.\\n","Prediction":"## Decision\nWe will use RabbitMQ to manage our mailqueue. RabbitMQ is a message broker which allows to easily create queues and\\npublish messages to them. RabbitMQ also provides a client library allowing to interact with RabbitMQ servers.\\nRabbitMQ will be used as a message broker and a Mail Queue server.\\nRabbitMQ will be used as a message broker and a Mail Queue server.\\nRabbitMQ will be used as a message broker and a Mail Queue server.\\nRabbitMQ will be used as a message broker and a Mail Queue server.\\nRabbitMQ will be used as a message broker and a Mail Queue server.\\nRabbitMQ will be used as a message broker and a Mail Queue server.\\nRabbitMQ will be used as a message broker and a Mail Queue server.\\nRabbitMQ will be used as a message broker and a Mail Queue server.\\nRabbitMQ will be used as a message broker and a Mail Queue server.\\nRabbitMQ will be used as a message broker and a Mail Queue server.\\nRabbitMQ will be used as a message broker and a Mail Queue server.\\nRabbitMQ will be used as a message broker and a Mail Queue server.\\nRabbitMQ"}
{"Context":"## Context and Problem Statement\\nWhile developing the new, unified API, there weren't clear guidelines for structuring new API endpoints. The first approach\\nwas introducing two different endpoint prefixes, similar to what is currently done in a regular shop. On the 30th of April,\\nwe have merged a unification of our endpoints in https:\/\/github.com\/Sylius\/Sylius\/pull\/11397. This move was dictated by\\nthe fact that we wanted to push unification even further. Depending on the currently logged in user, we had to expose different\\ndata for available entities in both contexts. Besides, it ought to simplify API usage (as it would reduce the number of endpoints)\\nand simplify the relations between resources. However, this change rose a lot of new issues to solve:\\n* Different serialization for different users is not trivial. Some relations between objects are also contextual, so\\nadmin can see many more links than the regular visitor or shop user.\\n* Endpoints like \"Product Show\" are resolved differently depending on the context as well. The shop products are determined\\nbased on the slug (taking into account the current channel), while the administrator is browsing products by code. This\\nseparation blurs the product identifier, and it is not clear which route should be considered as IRI.\\n* the split was not complete. Two separate log in endpoints have to be preserved, due to our user architecture and the\\ndecision that we want to be explicit, which user is trying to log in.\\nUsing one unified identifier in both admin and shop panel is a no go as well. In the shop we should use a `slug`, to be able\\nto expose product data based on URL (or provide redirect to proper resource based on the channel and locale), while in admin\\nresources are channel and locale agnostic, and we should use `codes` instead.\\n## Decision Drivers\\n* Driver 1: There is not strict requirement to have a direct 1 to 1 correlation between entities in database and exposed API Resources.\\nOne resource in the database may aggregate several API Resources\\n* Driver 2: There should be a clear way to determine an IRI for each of exposed API Resources\\n* Driver 3: A clear separation of available resources and paths for visitor and admin may be expected\\n","Decision":"## Decision\n* Driver 1: There is not strict requirement to have a direct 1 to 1 correlation between entities in database and exposed API Resources.\\nOne resource in the database may aggregate several API Resources\\n* Driver 2: There should be a clear way to determine an IRI for each of exposed API Resources\\n* Driver 3: A clear separation of available resources and paths for visitor and admin may be expected\\nChosen option: \"Moving back to prefixed paths for back-office and shop\". This option was selected because it allows us to\\neasily leverage the Symfony Security component's benefits like firewalls and ACL. The only second and third option provides\\npredictable behavior and clear separation of concerns for admin and shop. Still, suffixes on most of the routes seem like\\na worse solution compared to the common prefix. The common prefix will indicate that these endpoints are grouped in the\\nsame context.\\n","Prediction":"## Decision\n* Driver 1: There is not strict requirement to have a direct 1 to 1 correlation between entities in database and exposed API Resources.\\nOne resource in the database may aggregate several API Resources\\n* Driver 2: There should be a clear way to determine an IRI for each of exposed API Resources\\n* Driver 3: A clear separation of available resources and paths for visitor and admin may be expected\\nChosen option: \"Use a URL without the ID\", because it's the most straightforward approach, and it's also more flexible. It\\nsimplifies the API usage, as we don't have to worry about the IRI generation, and it can be used to expose resources to\\nexternal services as well.\\n### Positive Consequences\\n* We don't have to worry about the IRI generation\\n* It's more flexible, as we can expose resources to external services\\n* It's more extensible, as we can expose resources to external services\\n* It's more flexible, as we can expose resources to external services\\n* It's more extensible, as we can expose resources to external services\\n### Negative Consequences\\n* We don't have to worry about the IRI generation\\n* It's more ext"}
{"Context":"## Context\\nSince both developers will be working in paralel, and time is very limited,\\nan architecture that allows paralel work is highly beneficial.\\n","Decision":"## Decision\nWe will implement the backend using REST API principles, allowing both modules to work independently.\\n","Prediction":"## Decision\n* The application will be split into two parts: the API layer and the front-end layer.\\n* The API layer will be implemented using the [RESTful API](https:\/\/en.wikipedia.org\/wiki\/Representational_state_transfer)\\n* The front-end layer will be implemented using the [React](https:\/\/facebook.github.io\/react\/) library.\\n* The API layer will be implemented using the [GraphQL](https:\/\/graphql.org\/)\\n* The front-end layer will be implemented using the [React-Router](https:\/\/facebook.github.io\/react-router\/)\\n* The API layer will be implemented using the [Backbone](https:\/\/facebook.github.io\/backbone\/) library.\\n* The front-end layer will be implemented using the [Angular](https:\/\/angular.io\/)\\n* The API layer will be implemented using the [Angular-CLI](https:\/\/angular.io\/)\\n* The front-end layer will be implemented using the [Angular CLI](https:\/\/angular.io\/)\\n* The API layer will be implemented using the [HTTP 2](https:\/\/en.wikipedia.org\/wiki\/HTTP\/2)\\n* The front-end layer will be implemented using the [HTTP\/2](https:\/\/en.wikipedia.org\/wiki\/HTTP\/2) protocol"}
{"Context":"## Context\\nCurrently there are no restrictions on what options a service team\\nputs into pod specifications.  This means that a pod in a tenant\\nnamespace could escalate its privileges to access resources that it\\nshould not have access to.\\nExamples include:\\n* host namespaces\\n* privileged containers\\n* host networking\\n* arbitrary host volume mounts\\nFor a full list see [pod security policies][] in the kubernetes\\ndocumentation.\\nCurrently, our defence against this is that we enforce deploying\\ninfrastructure as code, and we require 2-eyes review on every code\\nchange.  However, kubernetes offers pod security policies as an extra\\nlayer of defence.\\n[pod security policies]: https:\/\/kubernetes.io\/docs\/concepts\/policy\/pod-security-policy\/\\n","Decision":"## Decision\nWe will implement pod security policies according to the following:\\n### Cluster defaults\\n```\\napiVersion: policy\/v1beta1\\nkind: PodSecurityPolicy\\nmetadata:\\nname: gsp-default\\nannotations:\\nseccomp.security.alpha.kubernetes.io\/allowedProfileNames: '*'\\nspec:\\nprivileged: false\\nallowPrivilegeEscalation: true\\nallowedCapabilities:\\n- '*'\\n# Allow core volume types.\\nvolumes:\\n- 'configMap'\\n- 'emptyDir'\\n- 'projected'\\n- 'secret'\\n- 'downwardAPI'\\n# Assume that persistentVolumes set up by the cluster admin are safe to use.\\n- 'persistentVolumeClaim'\\nhostNetwork: false\\nhostIPC: false\\nhostPID: false\\nrunAsUser:\\nrule: 'RunAsAny'\\nseLinux:\\nrule: 'RunAsAny'\\nsupplementalGroups:\\nrule: 'RunAsAny'\\nfsGroup:\\nrule: 'RunAsAny'\\nreadOnlyRootFilesystem: false\\n```\\n### System namespaces\\nSystem namespaces, including `gsp-system`, `kube-system` and `istio-system`,\\nshould use a less restrictive policy.\\n```\\napiVersion: policy\/v1beta1\\nkind: PodSecurityPolicy\\nmetadata:\\nname: system\\nannotations:\\nseccomp.security.alpha.kubernetes.io\/allowedProfileNames: '*'\\nspec:\\nprivileged: true\\nallowPrivilegeEscalation: true\\nallowedCapabilities:\\n- '*'\\nvolumes:\\n- '*'\\nhostNetwork: true\\nhostPorts:\\n- min: 0\\nmax: 65535\\nhostIPC: true\\nhostPID: true\\nrunAsUser:\\nrule: 'RunAsAny'\\nseLinux:\\nrule: 'RunAsAny'\\nsupplementalGroups:\\nrule: 'RunAsAny'\\nfsGroup:\\nrule: 'RunAsAny'\\n```\\n","Prediction":"## Decision\nWe will enforce pod security policies as part of the deployment process.\\nThis will be enforced by the Kubernetes CLI and the Kubernetes\\nAPI.  We will also enforce it by enforcing a minimum version of\\nthe Kubernetes API on the service.  This is a good idea, as we can then\\ntest the policy enforcement by running `kubectl` with the `-n` argument.\\nThis should be a good way of ensuring that the policy is enforced and\\nthat it is enforced consistently across all pods.\\nWe will also enforce a minimum version of the Kubernetes API on\\nthe service.  This is a good idea, as we can then test the policy\\nenforcement by running `kubectl` with the `-n` argument.\\nThis should be a good way of ensuring that the policy is enforced and\\nthat it is enforced consistently across all pods.\\nThis will be enforced by the Kubernetes CLI and the Kubernetes\\nAPI.  We will also enforce it by enforcing a minimum version of\\nthe Kubernetes API on the service.  This is a good idea, as we can then\\ntest the policy enforcement by running `kubectl` with the `-n` argument.\\nThis should be a good way of ensuring that the policy is enforced"}
{"Context":"## **Context**\\nIn order to\\n","Decision":"## Decision\nWhat is the change that we're proposing and\/or doing?\\n","Prediction":"## Decision\n## Decision\n* Use the [Node.js](https:\/\/nodejs.org\/) framework\\n* Use [Express](https:\/\/expressjs.com\/en\/)\\n* Use [Mongoose](https:\/\/www.npmjs.com\/package\/mongoose)\\n* Use [MongoDB](https:\/\/www.mongodb.com)\\n* Use the [Mongoose-Express-SDK](https:\/\/github.com\/mongoosejs\/mongoose-express-sdk) package for ORM\\n* Use the [Mongoose-Server](https:\/\/github.com\/mongoosejs\/mongoose-server) package for server-side data access\\n* Use the [MongoDB-Express-SDK](https:\/\/github.com\/mongodb\/express-sdk) package for database access\\n* Use the [MongoDB-Server](https:\/\/github.com\/mongodb\/server) package for database access\\n* Use the [Express-Router](https:\/\/expressjs.com\/en\/featured\/router.html) package for routing\\n* Use the [Express-Bootstrap](https:\/\/expressjs.com\/en\/featured\/bootstrap.html) package for front-end development\\n* Use the [Express-Validator](https:\/\/expressjs.com\/en\/featured\/validator.html) package for validation\\n* Use the [Express-Router-Ext](https:\/\/expressjs.com"}
{"Context":"## Context\\n1) We have commonly used existing Jenkins libraries\\n1) We want to be able to migrate a large number of projects with minimal effort\\n","Decision":"## Decision\nWe provide a compatibility layer for the current library\\n","Prediction":"## Decision\nTo use the existing Jenkins plugins for our project\\n[Image: :arrow_right:] [Jenkins plugins](https:\/\/plugins.jenkins.io\/)\\n[Image: :arrow_right:] [Jenkinsfile](https:\/\/plugins.jenkins.io\/jenkinsfile)\\n[Image: :arrow_right:] [Jenkinsfile plugin](https:\/\/plugins.jenkins.io\/jenkinsfile-plugin)\\nWe are using the [Jenkinsfile plugin](https:\/\/plugins.jenkins.io\/jenkinsfile\/) to make it easy to copy existing Jenkinsfile projects and reuse them. We also have a [Jenkinsfile plugin](https:\/\/plugins.jenkins.io\/jenkinsfile-plugin\/) to help us migrate existing Jenkinsfile projects.\\n### Jenkinsfile plugin\\nWe use the [Jenkinsfile plugin](https:\/\/plugins.jenkins.io\/jenkinsfile\/) to make it easy to copy existing Jenkinsfile projects and reuse them. We also have a [Jenkinsfile plugin](https:\/\/plugins.jenkins.io\/jenkinsfile-plugin\/) to help us migrate existing Jenkinsfile projects.\\n### Jenkinsfile plugin\\nWe use the [Jenkinsfile plugin](https:\/\/plugins.jenkins.io\/jenkinsfile\/) to make it easy to copy existing Jenkinsfile projects and reuse them. We also"}
{"Context":"## Context\\nWe are currently working towards meeting these two main user needs:\\n-   Candidates want to apply to teacher training courses.\\n-   Training providers want to view applicants who have applied to their courses.\\nIn order to do this the Apply service needs to retrieve data for `Training Providers`,\\n`Courses` and `Sites` from the Find service.\\nThe main areas where the Apply Service needs data from the Find Service are:\\n-   The Apply Service Start Page.\\n- Users arrive with a course course code as a URL parameter and the corresponding course needs to be retrieved.\\n-   The Add a course to Application Page.\\n- All current courses need to be retrieved to populate a autocomplete field.\\n-   The Application requirements require the Course type (`Secondary` or `Primary`).\\n-   The Provider UI needs to know information about the currently signed in Provider.\\n- The Applications to a certain Provider's courses need to be scoped for the currently signed in Provider.\\nFrom an initial discussion and planning session we have decided on two main methods from retrieving data from the Find Service:\\n### Ask the Find API for all information.\\nKeeping all information in the Find Service and contacting the `Find API` every time the Apply Service needs information for `Provider`, `Course` and `Site`.\\n#### Pros\\n- This reduces the need to store information within the Apply Service.\\n- The data from Find is always the latest up-to-date data.\\n#### Cons\\n- This can be slow if making many requests to the `Find API`. (Caching could be a solution to this.)\\n- Apply Service is dependent on the Find Service to function.\\n### Create a local copy from Find data\\nCreate local models for `Provider`, `Course` and `Site`. Populate these models from information from the `Find API` which is called periodically.\\n#### Pros\\n- Apply Service is not dependent on the Find Service to function.\\n- Faster to lookup local copy than make API request.\\n- We have more control of local models and only need to request and save data that the Apply Service requires.\\n- Easier to test local models.\\n#### Cons\\n- Requires method to retrieve data from Find and create and update local models.\\n- Local models can drift from information stored in find if not updated frequently.\\n- Newly created courses on Find may not be created on the Apply Service until they are retrieved.\\n(Will need to check Find service often for new data.)\\n- Data deleted from the Find service will need to be reflected in the Apply Service somehow.\\n(Possibly deleting and rebuilding Apply models from  Find Data Periodically).\\n","Decision":"## Decision\nCreate local copies of `Provider`, `Course` and `Site` that can be populated with data from the Find Service.\\n","Prediction":"## Decision\nThe Apply Service will use the Find API to retrieve data from the Find Service for all data that is required for the Apply Service.\\nThe Find API will be called every 5 minutes and the current data will be cached for the next 5 minutes.\\nThe Find API will be called using a cron job from the `deployer` job.\\nThe application will make the following calls to the Find API:\\n-   Retrieve all courses from Find.\\n-   Retrieve all providers from Find.\\n-   Retrieve all courses and providers in a single call.\\n-   Retrieve all courses in a single call.\\n-   Retrieve all providers in a single call.\\n-   Retrieve all courses and providers in a single call.\\n-   Retrieve all courses and providers in a single call.\\n-   Retrieve all providers in a single call.\\n-   Retrieve all courses and providers in a single call.\\n-   Retrieve all courses and providers in a single call.\\n-   Retrieve all providers in a single call.\\n-   Retrieve all courses and providers in a single call.\\n-   Retrieve all providers in a single call.\\n-   Retrieve all courses and providers in a single call.\\n-   Retrieve all courses and providers"}
{"Context":"## Context\\nThere is a lot of things to learn, not only the ecosystem of clojure, we what create a high scalable application deploy in cloud, with autoscale, resilience, geographic distribution, multyples clouds providers, etc.\\n","Decision":"## Decision\nWe start with the most simple solution, a console application, after that we try to evolve to an API, Web Page, Microservices, etc.\\n","Prediction":"## Decision\nWe decided to use the cloud infrastructure of AWS.\\nThis will help us with the management of the resources, scaling, and the security.\\nWe will use the following services:\\n* ECS (Elastic Container Service) to build our microservices\\n* ELB (Elastic Load Balancer) to manage the load balancer\\n* RDS (Relational Database Service) to manage the database\\n* SQS (Simple Queue Service) to manage the message\\n* SQS SQS (Simple Queue Service Queue Service) to manage the queues\\n* AKS (Amazon Kubernetes Service) to manage the services\\n* IAM (Identity and Access Management) to manage the resources\\n* EKS (Elastic Container Service) to manage the cluster\\n* ECR (Elastic Container Registry) to manage the registry\\n* KMS (Kubernetes Mesos Service) to manage the secrets\\n* ECS (Elastic Container Service) to manage the services\\n* ELB (Elastic Load Balancer) to manage the load balancer\\n* RDS (Relational Database Service) to manage the database\\n* SQS (Simple Queue Service) to manage the message\\n* SQS SQS (Simple Queue Service Queue Service) to manage the"}
{"Context":"## Context and Problem Statement\\n- How can we maintain a clean design?\\n- How will we be able to maintain our architecture iteratively in accordance with the TDD practice?\\n## Decision Drivers\\n* We employ TDD, which favors small iterations.\\n* We aim to implement full Continuous Delivery.\\n","Decision":"## Decision\n* We employ TDD, which favors small iterations.\\n* We aim to implement full Continuous Delivery.\\n","Prediction":"## Decision\n* We employ TDD, which favors small iterations.\\n* We aim to implement full Continuous Delivery.\\nChosen option: \"Use a tool like Babel, Webpack and Gulp to define the architecture\", because:\\n- It is the most widely used tool in the Javascript ecosystem.\\n- It is well documented and has a large community.\\n- It is easy to use and can be used with a simple command line.\\n- It has a very good test coverage with Jest.\\n- It has a very good TDD coverage with Mocha.\\n- It has a very good ability to unit test the code.\\n- It is a very popular tool for bundling assets.\\n- It provides a very good build system for building React apps.\\n- It has a very good test coverage with React Testing Library.\\n- It has a very good test coverage with Jest Testing Library.\\n- It is a very popular tool for hosting the code.\\n- It has a very good test coverage with Webpack Testing Library.\\n- It provides a very good test coverage with Cypress.\\n- It provides a very good test coverage with Jest Testing Library.\\n- It has a very good ability to run the tests.\\n-"}
{"Context":"## Context\\n<!--- What is the issue that we're seeing that is motivating this decision or change? -->\\nThis project contains a large number of classes, making up the various constructs and patterns. The project is intended to be used as a component library and, therefore, used by a number of people who don't have extensive knowledge of either the CDK or this library. It is therefore important to make the experience of using these classes as intuitive as possible.\\n","Decision":"## Decision\n<!-- What is the change that we're proposing and\/or doing? -->\\nConstructors should follow the following rules for consistency.\\n1. The first parameter should be a `scope` of type `GuStack`:\\n:white_check_mark: Valid\\n```ts\\nclass MyConstruct {\\nconstructor(scope: GuStack) {\\n...\\n}\\n}\\n```\\n:x: Invalid\\n```ts\\nclass MyConstruct {\\nconstructor(scope: Stack) {\\n...\\n}\\n}\\n```\\nThe construct\/pattern will then have a static `id` as it will never change, for example the `Stage` parameter.\\n2. They can also take a `props` object which should be correctly typed:\\n:white_check_mark: Valid\\n```ts\\nclass MyConstruct {\\nconstructor(scope: GuStack, props: MyConstructProps) {\\n...\\n}\\n}\\n```\\n:x: Invalid\\n```ts\\nclass MyConstruct {\\nconstructor(scope: Stack, props: object) {\\n...\\n}\\n}\\n```\\nThe construct\/pattern will then derive `id` from `props` as it will never change, for example `InstanceTypeFor${props.app}`.\\n3. They can also take an `id` of type string and a `props` object which should be correctly typed\\n:white_check_mark: Valid\\n```ts\\ninterface MyConstructProps {...}\\nclass MyConstruct {\\nconstructor(scope: GuStack, id: string, props: MyConstructProps) {\\n...\\n}\\n}\\n```\\n:x: Invalid\\n```ts\\nclass MyConstruct {\\nconstructor(scope: GuStack, id: any, props: object) {\\n...\\n}\\n}\\n```\\n4. Where all `props` are optional, the `props` object should be optional as a whole\\n:white_check_mark: Valid\\n```ts\\ninterface MyConstructProps {\\nprop1?: string;\\nprop2?: string\\n}\\nclass MyConstruct {\\nconstructor(scope: GuStack, id: string, props?: MyConstructProps) {\\n...\\n}\\n}\\n```\\n:x: Invalid\\n```ts\\ninterface MyConstructProps {\\nprop1?: string;\\nprop2?: string\\n}\\nclass MyConstruct {\\nconstructor(scope: GuStack, id: string, props: MyConstructProps) {\\n...\\n}\\n}\\n```\\n","Prediction":"## Decision\n<!--- What changes need to be made to the code or design to make it work with the new style of module structure. -->\\nWe will use the [Module::injectable decorator](https:\/\/github.com\/felixgeorg\/Module#injectable-decorator) to inject the classes into the modules. It has the advantage of providing a simple way to inject the classes, and it is already used by the CDK.\\n### Example\\n<!--- Example of using the Module::injectable decorator. -->\\n<!--- A small module that will contain the core CDK classes. -->\\n<!--- A module that will contain the core CDK classes, but will also contain a number of other classes. -->\\n<!--- Both modules will be used in the same project. -->\\n<!--- The CDK classes will be injected into the modules that they are used by. -->\\n<!--- The CDK classes will be injected into the modules that are used in the CDK. -->\\n<!--- The CDK classes will be injected into the modules that are used in another CDK module. -->\\n<!--- The CDK classes will be injected into the modules that are used in the CDK."}
{"Context":"## Context\\nWe need to identify how and where we will store packages that describe our front end framework. There are a number of NHS UK and NHS Digital projects that already use NPM as their package repository for front end code.\\n","Decision":"## Decision\nWe have chosen to store our packages on NPM.\\n","Prediction":"## Decision\nWe will use [NPM](https:\/\/www.npmjs.com\/) as the package repository for our front end framework. The repository will be managed by the NHS Digital team.\\nWe will use the `NPM` package repository for our front end framework. The repository will be managed by the NHS Digital team.\\nWe will use the `NPM` package repository for our front end framework. The repository will be managed by the NHS Digital team.\\nWe will use the `NPM` package repository for our front end framework. The repository will be managed by the NHS Digital team.\\nWe will use the `NPM` package repository for our front end framework. The repository will be managed by the NHS Digital team.\\nWe will use the `NPM` package repository for our front end framework. The repository will be managed by the NHS Digital team.\\nWe will use the `NPM` package repository for our front end framework. The repository will be managed by the NHS Digital team.\\nWe will use the `NPM` package repository for our front end framework. The repository will be managed by the NHS Digital team.\\nWe will use the `NPM` package repository for our front end framework. The repository will be"}
{"Context":"## Context and Problem Statement\\nThis design proposal is focused on how best to handle Elasticsearch process restart inside a pod, to catch up with configuration changes.\\nThe primary use case for this is switching for one license type to another with network configuration change. For example, switching from a cluster with a Basic license (no TLS) to a cluster with a Gold license (TLS) requires a full cluster restart. It's not possible to \"just\" replace the pods with new ones here: we would need data to be migrated between pods that cannot communicate with each others. Reusing PersistentVolumes could be a solution here, but may not play well with local persistent volumes bound to a particular node: what if the node gets scheduled to another pod in-between the migration?\\nReusing pods may also be useful in other situations, where simply restarting Elasticsearch process can be way faster and more efficient than replacing the pod:\\n- minor configuration changes\\n- plugin changes\\n## Decision Drivers\\n* It must be possible to perform a \"full cluster restart\" and reuse existing pods.\\n* Configuration changes should be propagated through a volume mount, not through the pod spec.\\n* Operator client cache inconsistencies must be taken into consideration (for ex. don't restart a pod 3 times because of a stale cache).\\n* Volume propagation time must be taken into consideration (it can take more than 1 minute, the ES restart must not happen before the new configuration is there).\\n* It should be resilient to operator restart.\\n","Decision":"## Decision\n* It must be possible to perform a \"full cluster restart\" and reuse existing pods.\\n* Configuration changes should be propagated through a volume mount, not through the pod spec.\\n* Operator client cache inconsistencies must be taken into consideration (for ex. don't restart a pod 3 times because of a stale cache).\\n* Volume propagation time must be taken into consideration (it can take more than 1 minute, the ES restart must not happen before the new configuration is there).\\n* It should be resilient to operator restart.\\nChosen option: option 1, because that's the only one we have here? :)\\n### Positive Consequences\\n* Handles pod and cluster restart, rolling or coordinated\\n* Allows humans to trigger a restart through annotations\\n* Safe from cache inconsistencies, operator restart, reconciliation retries, volume propagation\\n### Negative Consequences\\n* Needs to be implemented!\\n* Additional complexity.\\n* Need to be extra careful about chaining steps in the right order, and make them idempotent.\\n* Once a restart is scheduled, it will go through completion. If the user modifies settings again, we'll wait for the current restart to be done.\\n","Prediction":"## Decision\n* It must be possible to perform a \"full cluster restart\" and reuse existing pods.\\n* Configuration changes should be propagated through a volume mount, not through the pod spec.\\n* Operator client cache inconsistencies must be taken into consideration (for ex. don't restart a pod 3 times because of a stale cache).\\n* Volume propagation time must be taken into consideration (it can take more than 1 minute, the ES restart must not happen before the new configuration is there).\\n* It should be resilient to operator restart.\\nChosen option: \"Reusing PersistentVolumes\", because it's the easiest solution for now, and it's the fastest in terms of volume propagation time. It may not be the best solution in the future, but in the short term it's the best.\\n### Positive Consequences\\n* The cluster restart can be performed with a single command, making it easier to manage the cluster.\\n* The volume mounting is not required: the ES process can be restarted with the persistent volumes intact.\\n* Volume propagation time is not an issue: we can restart the cluster in a matter of seconds.\\n### Negative Consequences\\n* The volume propagation time may be too long: it's not possible to predict the propagation"}
{"Context":"## Context\\nCurrently, the results of emitted runtime messages are `MessageEvent`s, which\\nonly provide information whether the message execution was successful or not.\\nFor various use-cases additional information about message results would be\\nuseful.\\nOne of such is supporting staking by runtimes. Currently, a runtime can emit an\\n`AddEscrow` message, but is unaware of the actual amount of shares it obtained\\nas a result of the added escrow. For some use-cases (e.g. runtime staking user\\ndeposited funds) this information is crucial for accounting.\\nSimilarly, for `ReclaimEscrow`, the runtime doesn't have the direct information\\nat which epoch the stake gets debonded.\\nThe only way to currently obtain this data is to subscribe to consensus events,\\nsomething which runtime doesn't have access to.\\nAdding results to `MessageEvent` solves both of the mentioned use cases:\\n- for `AddEscrow` the result should contain amount of shares obtained with the\\nescrow\\n- for `ReclaimEscrow` the result should contain the amount of shares and epoch\\nat which the stake gets debonded\\n","Decision":"## Decision\nImplement support for arbitrary result data in `MessageEvent` runtime message\\nresults.\\n","Prediction":"## Decision\nTo improve the current situation we will implement a new runtime result type\\n`MessageResult` which will contain the result of a runtime message.\\nThis will allow the runtime to obtain the information it needs for accounting.\\nAdditionally, the `MessageResult` will also contain the result of a consensus\\nevent, which will be used for staking by the runtime.\\nAdditionally, the `MessageResult` will contain the result of a runtime\\nmessage that is emitted by the runtime, e.g. for `AddEscrow` or `ReclaimEscrow`\\nIf the runtime needs to obtain more information about the message it can emit\\na `MessageResult` with additional information that will be used by the\\nruntime.\\nAdditionally, the `MessageResult` will contain the result of a runtime\\nmessage that is emitted by the runtime, e.g. for `AddEscrow` or `ReclaimEscrow`\\nIf the runtime needs to obtain more information about the message it can emit\\na `MessageResult` with additional information that will be used by the\\nruntime.\\nAdditionally, the `MessageResult` will contain the result of a runtime\\nmessage that is emitted by the runtime, e.g. for `AddEscrow` or"}
{"Context":"## Context\\nGiven the premise that those images are designed to run in production, in a clustered environment and managed by an orchestrator like Kubernetes or Docker Swarm, it becomes necessary to shape the PHP installation towards those needs.\\nFor instance, images that ship without any PHP extensions (like the official ones) are not able to handle posix signals (like `SIGTERM` or `SIGINT`) from an orchestrator. Moreover they lack in-memory user cache, which can be useful for projecting data into a service.\\n","Decision":"## Decision\nShip the Docker images with extensions that contribute to the vision of the microservices use case, being:\\n- `PCNTL` in order to deal with user signals. [PCNTL Manual page](http:\/\/php.net\/manual\/en\/book.pcntl.php)\\n- This is meant for long running non-interactive php applications. `fpm` is not impacted, since it [can deal with these signals natively](https:\/\/linux.die.net\/man\/8\/php-fpm).\\n- `APCU` in order to provide a user in-memory caching namespace. [APCU Manual page](http:\/\/php.net\/manual\/en\/book.apcu.php)\\n- `OPcache` for PHP bytecode cache, given the Docker image immutability principle it's good not to re-parse the code. [OPcache Manual page](http:\/\/php.net\/manual\/en\/book.opcache.php)\\n","Prediction":"## Decision\nWe will build a custom PHP image that bundles the necessary PHP extensions and configuration.\\nThis is a base for all other PHP images that will be built in the future.\\nThis base will allow us to ensure that it is compliant with the needs of the orchestrator and will be easy to extend in the future to support other services.\\nThis base will also allow us to avoid the need to bundle PHP extensions with the base PHP image.\\nThis base will also allow us to build PHP images that are closer to the needs of the orchestrator.\\nThis base will also allow us to build PHP images that are closer to the needs of the orchestrator.\\nThis base will also allow us to build PHP images that are closer to the needs of the orchestrator.\\nThis base will also allow us to build PHP images that are closer to the needs of the orchestrator.\\nThis base will also allow us to build PHP images that are closer to the needs of the orchestrator.\\nThis base will also allow us to build PHP images that are closer to the needs of the orchestrator.\\nThis base will also allow us to build PHP images that are closer to the needs of the orchestrator.\\nThis base will also allow us to build PHP images"}
{"Context":"## Context\\nAt least one user is currently using node js and passport. We want to provide as\\nfrictionless as possible an integration for them.\\nOther users will be using other languages and frameworks.\\n","Decision":"## Decision\nWe will initially build only a node \/ passport client. We will want to build\\nanother client in another language as soon as possible to make sure the API\\nis well designed.\\nUsers should also be able to interact with the API directly if we haven't built\\nan appropriate client for their use case.\\n","Prediction":"## Decision\nWe decided that the best approach is to use the passport library for authentication\\nand authorization.\\nWe will also use the [passport-local](https:\/\/github.com\/airbnb\/airbnb-local) library for\\nlocal authentication.\\nThis library will be used as a service layer to provide a simple way to authenticate and\\ntokenize users.\\nThis library will also be used to provide a simple way to create new users.\\nTo do this, it will provide a simple way to store the user's credentials in a\\ndatabase.\\nThis library will also be used to provide a simple way to create new roles.\\nThis library will also be used to provide a simple way to create new permissions.\\nThis library will also be used to provide a simple way to create new users.\\nThis library will also be used to provide a simple way to delete users.\\nThis library will also be used to provide a simple way to delete roles.\\nThis library will also be used to provide a simple way to delete permissions.\\nThis library will also be used to provide a simple way to delete users.\\nThis library will also be used to provide a simple way to delete roles.\\nThis library will also be used to provide a simple way to"}
{"Context":"## Context\\nWe've implemented 2 different methods in the Editor interface:\\n1. `write()`\\n1. `readThenWrite()`\\nMost refactorings use `write()`. They create new code from the AST, then write it in the editor. Still, some refactorings use `readThenWrite()`: they first read some code `string` from the editor, and use it to write new code.\\nThe main problem of `readThenWrite()` is that we end up manipulating strings instead of the AST. This usually means the implementation is more complex. Instead of manipulating the AST, we need to retrieve the correct selection we need to read, then we have to mix this read code with the transformation to produce the final output. Final code is less straightforward and more cases need to be tackled by us, instead of relying on the AST parser to do the job.\\nHowever, there is one key advantage of doing so: it absolutely preserve the code as it was written in the editor. Even though [we use recast to preserve code style as much as possible][recast-usage], it's not perfect. If the refactoring consist in re-using exactly code that was written, `readThenWrite()` is the only way we know to preserve exactly the original style.\\n","Decision":"## Decision\nWe'll use `readThenWrite()` if we need to preserve a code exactly as it was written. E.g. if the refactoring consists in moving existing code, without transforming it (Extract Variable, Inline Variable).\\nFor other refactorings, we'll use `write()`. E.g. if the refactoring transforms the code, it's OK to change it while preserving the original style as much as possible with recast.\\n","Prediction":"## Decision\nAt this point, we decided to implement `readThenWrite()` in the Editor interface.\\nIt will be used to preserve the AST as it was written in the editor, but it will also handle the refactoring, to preserve the code style as it was written in the AST.\\nThis means we end up with 2 different interfaces, `Editor::write` and `Editor::readThenWrite()`\\nWe will add a new method called `refactor()` that will be called on the AST before it's written to the Editor.\\nThe AST will be converted to a `Map<String, Editor>` (or a `Map<String, Editor, ASTNode>>`) by the AST Parser. Each AST node will be converted to a `Node` by the AST Parser, and the AST node's `editor` will be set to the Editor that created it.\\nThen, the AST will be converted to a `Map<String, Editor, ASTNode>>` by the AST Parser, and the AST node's `editor` will be set to the Editor that created it.\\nFinally, the AST will be converted to a `Map<String, Editor, ASTNode>>` by the AST Parser, and the AST node's `editor` will be"}
{"Context":"## Context and Problem Statement\\nThe API has to be documented in human and machine readable form. If the API is documented in machine readable form, the automatic generation of client services is possible.\\n## Decision Drivers\\n* readable API documentation\\n* effort of manually creating client services\\n","Decision":"## Decision\n* readable API documentation\\n* effort of manually creating client services\\nChosen option: Use OpenAPI, because the API is described in a standardized format which is human and machine readable.\\n### Positive Consequences\\n* Standardized documentation of the API\\n* Automatic service generation for clients is possible\\n### Negative Consequences <!-- optional -->\\n* OpenAPI annotations have to be maintained\\n","Prediction":"## Decision\n* readable API documentation\\n* effort of manually creating client services\\nChosen option: \"Manually\", because it is the best option at the moment. It is the easiest way to document the API and to create client services.\\n### Positive Consequences\\n* client services are generated automatically\\n* no effort is needed from the developer's side\\n* the API is documented in machine readable form\\n### Negative Consequences\\n* time-consuming effort of manually creating client services\\n* no automatic generation of client services is possible\\n* the API is not documented in machine readable form\\n### Impact on other systems\\n* Dependencies on client services are not known for the time being.\\n* The API is not documented in machine readable form and will have to be manually created.\\n* There are dependencies on the client services and the API is documented in machine readable form, so the developer can create client services.\\n* The API is not documented in machine readable form, so the developer has to manually create client services.\\n### Alternative\\n* Use the [API documentation generator](https:\/\/github.com\/alexandersteinberg\/aspnet-docs) to generate API docs.\\n* Use [an API documentation generator](https:\/\/github.com\/alexandersteinberg"}
{"Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"## Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions.\\n","Prediction":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\nWe will use [Lighthouse](https:\/\/lighthouseproject.io\/).\\nWe will use [Pivotal Tracker](https:\/\/www.pivotaltracker.com) for version control and reporting.\\nWe will use the [Google Cloud Platform](https:\/\/cloud.google.com) architecture library to provision the server environment.\\nWe will use a standard configuration file for the server, so that we can easily move the server between environments.\\nWe will use a standard configuration file for the client, so that we can easily change the client's configuration.\\nWe will use a standard configuration file for the database, so that we can easily change the database schema.\\nWe will use a standard configuration file for the logging (a.k.a. metrics) service, so that we can easily change the logging configuration.\\nWe will use a standard configuration file for the caching service, so that we can easily change the caching configuration.\\nWe will use a standard configuration file for the DNS service, so that we can easily change the DNS configuration.\\nWe will use standard configuration"}
{"Context":"Context\\n-------\\n`event-routing-backends` transmits events to configured recipients (Learning Record Stores) via http protocol in near real-time. A strategy needs to be implemented to handle the case when an LRS's link is down.\\nDecision\\n--------\\n1. A celery task will be created for each transformation (xAPI or Caliper) of an event. Once the transformation is complete, nested celery tasks will be created, one for each recipient, to route the event.\\n2. Retry attempts shall be made for each recipient, for all events types, and for a configured number of retries and delay between each retry.\\n3. A limited type of events (namely *business critical events*) shall be persisted even after all retry attempts have been exhausted. Celery tasks, that failed to route the transformed event to their intended recipients, will be stored in a database. Each of these tasks (persisted via `celery-utils`) will include just enough information about the event that it gets resent appropriately after persistence. Events that consumers of LRS may use for record keeping such as course enrollment and completion events, shall be classified as *business critical events*.\\n4. A scheduled process will retry transmitting all persisted events in the database to respective recipient(s) at a configured frequency (e.g. once a day). This process will also check if the number of persisted events is higher than a configured threshold. If so, it will generate an alert for the admin.\\n5. An interface shall be provided for admin to view the list of recipient(s) whose events are persisting in the database. The admin may choose to contact the recipient(s) to try and resolve the communication issue.\\nConsequences\\n------------\\n1. All but *business critical events*, will be lost after the time and number of retry attempts in decision # 2 expire.\\n2. Decision # 1 is necessary to enable decision # 3 but will also increase the number of celery workers in use.\\n3. The admin will need to respond to alert discussed in decision # 4 to avoid unnecessary utilisation of storage space.\\n","Decision":"## Decision\n--------\\n1. A celery task will be created for each transformation (xAPI or Caliper) of an event. Once the transformation is complete, nested celery tasks will be created, one for each recipient, to route the event.\\n2. Retry attempts shall be made for each recipient, for all events types, and for a configured number of retries and delay between each retry.\\n3. A limited type of events (namely *business critical events*) shall be persisted even after all retry attempts have been exhausted. Celery tasks, that failed to route the transformed event to their intended recipients, will be stored in a database. Each of these tasks (persisted via `celery-utils`) will include just enough information about the event that it gets resent appropriately after persistence. Events that consumers of LRS may use for record keeping such as course enrollment and completion events, shall be classified as *business critical events*.\\n4. A scheduled process will retry transmitting all persisted events in the database to respective recipient(s) at a configured frequency (e.g. once a day). This process will also check if the number of persisted events is higher than a configured threshold. If so, it will generate an alert for the admin.\\n5. An interface shall be provided for admin to view the list of recipient(s) whose events are persisting in the database. The admin may choose to contact the recipient(s) to try and resolve the communication issue.\\nConsequences\\n------------\\n1. All but *business critical events*, will be lost after the time and number of retry attempts in decision # 2 expire.\\n2. Decision # 1 is necessary to enable decision # 3 but will also increase the number of celery workers in use.\\n3. The admin will need to respond to alert discussed in decision # 4 to avoid unnecessary utilisation of storage space.\\n","Prediction":"## Decision\n--------\\n1. A celery task will be created for each transformation (xAPI or Caliper) of an event. Once the transformation is complete, nested celery tasks will be created, one for each recipient, to route the event.\\n2. Retry attempts shall be made for each recipient, for all events types, and for a configured number of retries and delay between each retry.\\n3. A limited type of events (namely *business critical events*) shall be persisted even after all retry attempts have been exhausted. Celery tasks, that failed to route the transformed event to their intended recipients, will be stored in a database. Each of these tasks (persisted via `celery-utils`) will include just enough information about the event that it gets resent appropriately after persistence. Events that consumers of LRS may use for record keeping such as course enrollment and completion events, shall be classified as *business critical events*.\\n4. A scheduled process will retry transmitting all persisted events in the database to respective recipient(s) at a configured frequency (e.g. once a day). This process will also check if the number of persisted events is higher than a configured threshold. If so, it will generate an alert for the admin.\\n5. An"}
{"Context":"## Context\\nThe eIDAS specification states that member states (MS) requesting an eIDAS identity can operate one or more connector nodes. The MS exporting the identity, us in this case, should only operate one proxy node from the point of view of the requesting MS.\\nTrust between the connector node and proxy node is established by the connector consuming and verifying the proxy node's SAML metadata and vice versa. Verification of the metadata is accomplished using public keys exchanged by both parties out of band.\\nTherefore, from every MS that wishes to consume UK identities we will need:\\n- the URL from which to fetch their metadata\\n- a certificate which can be used to verify the signature of their metadata\\nIn turn, we will provide our URL and certificate to the requesting MS.\\n","Decision":"## Decision\nRunning a single instance of the proxy node services and infrastructure to handle requests from all potential requesting MS would need some way of aggregating their metadata. An additional service would most likely be required to do this, bringing with it the added cost of having to secure and administer that service, as it would end up being the root of trust for _all_ requesting MS. In our current implementation, a single proxy node would also mean a single signing key being used to export all UK identities to every requesting MS, making that key all the more valuable.\\nInstead, we will run one instance of the proxy node services and infrastructure isolation _per_ requesting MS connector node. They will share a single hardware security module (HSM) but will each have independent credentials to access the HSM and an independent signing key held by the HSM.\\nHaving multiple instances running, one per requesting MS, would be fairly easy to implement and provides additional benefits, such as:\\n- a compromise to the infrastructure of one proxy node should not easily affect another\\n- the damage of compromising one proxy node's signing key is limited to a single requesting connector node\\n- signing key and metadata rotations can be performed on an individual connector node basis, removing the need to coordinate with multiple requesting MS\\n","Prediction":"## Decision\nWe will provision an eIDAS connector node using the following URL:\\nhttps:\/\/ukidentity.ukgateway.eu\/\\nWe will provision the certificate for this URL using the following private key:\\n![](https:\/\/ukidentity.ukgateway.eu\/.well-known\/openid-configuration\/3.0\/keys\/uk-gateway-ukidentity-01-0001)!\\nWe will provision a certificate for the eIDAS connector node using the following public key:\\n![](https:\/\/ukidentity.ukgateway.eu\/.well-known\/openid-configuration\/3.0\/keys\/uk-gateway-ukidentity-01-0002)!\\nWe will provision the eIDAS connector node using the following URL:\\nhttps:\/\/ukgateway.ukidentity.eu\/\\nWe will provision the certificate for this URL using the following private key:\\n![](https:\/\/ukgateway.ukidentity.eu\/.well-known\/openid-configuration\/3.0\/keys\/uk-gateway-ukidentity-01-0003)!\\nWe will provision the eIDAS connector node using the following public key:\\n![](https:\/\/ukgateway.ukidentity.eu\/.well-known\/openid-configuration\/3.0\/keys\/uk-gateway-ukidentity"}
{"Context":"## Context\\nThere are 2 possible solutions within the table loading part of the Armadillo.\\n- Make use of `.RData` files to store the data frames you want to use\\n- Make use of `.parquet` files to store the data frame you want to use\\nIn the Armadillo we have a general concept of projects. These projects represent all the data of a cohort or study specific data. e.g. It is the only level you can set permissions.\\n- gecko-all\\n- gecko-diabetes\\n### Managing data with `.RData` files\\nWithin the projects we have `.RData` files containing all the tables (R data frames) that are bound in a specific context.\\n- gecko-all\\n- 1_0_core_1_1.RData\\n- non_rep\\n- yearly_rep\\n- monthly_rep\\n- trimester_rep\\n- 1_0_outcome_1_1_non_rep.RData\\n- gecko-diabetes\\n- 1_0_core_1_1.RData\\n- non_rep\\n- yearly_rep\\n#### Make the data available\\nWhen you login to the Armadillo you specify which `.RData` files you want to have available in your analysis environment.\\n`folder=gecko-diabetes\/1_0_core_1_1&folder=gecko-all\/1_0_outcome_1_1`\\nSpecifying these folders (`.RData` files) allow you to assign all the columns that are available in the `data.frame` that are in the `.RData` files.\\nTechnically it loads the `RData` files in memory to be able to see which tables are available.\\n#### Assigning the data\\nWhen you want to use the data in a DataSHIELD analysis you need to assign the data. So when you login the data becomes available in a private environment in R. When you assign data it will be copied to the analysis environment in R.\\nYou need to assign all data or parts of the data depending on the analysis you want to do. Assigning parts of the data is more efficient and analysis will become faster.\\nIn the `RData` solution all the data in the private and the analysis environment is in memory. When you assign the data, it will be copied to the analysis environment. You can specify a subset of the data as well. The copy will contain only the selected variables.\\n#### Advantages\\n- You can use native `R` components to get the data available for your analysis\\n- You can implement *resources* fairly easy in `.RData`\\n#### Disadvantages\\n- It uses large amounts of data per user when loading large files. Which means you can not work with a large number of users at the same time on the Armadillo.\\n- Logging in takes a lot of time, because folders are `.RData` files and contain all the tables. Tables need to be loaded to be checked by the DataSHIELD package.\\n### Managing data in `.parquet` files\\nManaging data in `.parquet` files allows us to deal with larger data more efficiently. This changes the concepts a little within our file storage (minio). We already work with buckets as described above. `Parquet` files do not allow more than one `data.frame` in one `parquet` file. The `parquet` files are more or less substitutes for `RData` files. The logical structure is the same, however the technical structure is somewhat different.\\n- gecko-all\\n- 1_0_core_1_1 (minio -> folder concept)\\n- non_rep.parquet\\n- yearly_rep.parquet\\n- monthly_rep.parquet\\n- trimester_rep.parquet\\n- 1_0_outcome_1_1_non_rep (minio -> folder concept)\\n- gecko-diabetes\\n- 1_0_core_1_1 (minio -> folder concept)\\n- non_rep.parquet\\n- yearly_rep.parquet\\n#### Make the data available\\nWhen you login you only specify the fully quelified names of the tables you want to load:\\n`project\/folder\/table.parquet`\\nFor example:\\n`gecko-all\/1_0-core-1_1\/non-rep.parquet`\\nSpecifying these projects and folders allow you to assign all the columns that are available in the `data.frames` that are in the `.parquet` files.\\nThis is not loading the folders (`RData` files) in memory, but loads them when you assign the tables. At login time you only check wether you may access the projects.\\n#### Assigning the data\\nThe difference with the `RData` solution is that the files will not be loaded in memory initially only when the data is assigned. More specifically only the selected data will be assigned. You do not have a copy of the whole data in memory in a private environment.\\n#### Storing complex structures\\nAnother format to store complex structures is `rds`. The `rds` format is capable to store 1 object in 1 file. You can directly see what is in the file. With `RData` you need to load the file when you want to know what it is containing. In `rds` you see directly what the contents is.\\n#### Advantages\\n- You do not need to specify the folder(s) (`RData` files) or table(s) during login which makes it faster\\n- You are able to delay loading the data in the R environment, which makes it possible to to login faster\\n- When you specify specific columns from the dataset it is highly efficient to load the data in the R environment\\n- Loading and saving parquet files is faster, so data handling actions will be more efficient.\\n#### Disadvantages\\n- An extra dependency in the package\\n- More risk when converting that is not R-native (think of dates, number of significant bits, etc.)\\n","Decision":"## Decision\nThe principle we are going implement is to store 1 object per file. Which means that for table-like object we are going to use `.parquet`and for more complex structures we are going to use `.rds` (such as *resources* used in Opal).\\nWe are going to use the parquet files in the Armadillo package. As data manager you do not *have* to work with the parquet format.\\n","Prediction":"## Decision\nWe will use the `.parquet` format to store the data. This will allow us to use the native `R` components to get the data available for our analysis.\\nWe will store the data in the same folder as the `.RData` files. This is the only level that can be set permissions for the data.\\nThe `.parquet` files are a replacement for the `.RData` files. The logical structure is the same, however the technical structure is somewhat different. The `.parquet` files are more or less substitutes for the `RData` files. The logical structure is the same, however the technical structure is somewhat different.\\nWhen loading the `.parquet` files the `data.frame` is loaded in memory. The `.parquet` files are loaded when you assign the data. The `.parquet` files are used to load the data. When you load the `.parquet` files you need to know the size of the `.parquet` files. The `.parquet` files are not loaded in memory.\\nWhen you save the `.parquet` files you need to know the size of the `.parquet` files. The `.parquet` files are not loaded in memory.\\nWhen you"}
{"Context":"## Context\\nWe have introduced the concept of academic cycles. All trainees, courses and funding rules will need to be linked to an academic cycle.\\n","Decision":"## Decision\nWe need a way to \"associate\" all of the given entities above to reliably know a trainee's academic cycle and the associated funding rules applied for that cycle.\\nOur `funding_methods` table will have a foreign key linking to the `academic cyles` table. Since funding rules are cycle specific, it made sense to have a hard association between these two tables as their start and end dates correlate.\\n","Prediction":"## Decision\nWe will introduce the concept of academic cycles in the form of a 12-month cycle. This will allow for the following:\\n* All funding rules to be linked to academic cycles\\n* All trainees to be linked to academic cycles\\n* All courses to be linked to academic cycles\\n* Training and funding will be linked across the academic cycles of all trainees and courses\\n* All training and funding will be linked to the academic cycle of the funding body\\n* All funding bodies will be linked to the academic cycle of the academic programme\\n* All funding bodies will be linked to the academic cycle of the funding period\\n* All funding bodies will be linked to the academic cycle of the funding period\\n* All funding periods will be linked to the academic cycle of the funding period\\n* All funding periods will be linked to the funding period\\n* All funding periods will be linked to the funding period\\n* All funding periods will be linked to the funding period\\n* All funding periods will be linked to the funding period\\n* All funding periods will be linked to the funding period\\n* All funding periods will be linked to the funding period\\n* All funding periods will be linked to the funding period\\n* All funding periods will be linked to the funding period\\n"}
{"Context":"## Context\\nWhen rapidly iterating on API for prerelease features, we will commonly\\nwant to make breaking API changes without revving the API endpoints and\\nmaintaining the previous versions of the endpoints indefinitely for\\nversions of the app that will never be seen by customers.\\nIn that context, remote kills stand to be an incredibly useful tool.\\nWhen you decide to make a breaking API change, you simply create\/update\\na remote kill for the app from version `0` through the first client\\nversion conforming to the new contract. Then, instead of getting crash\\nbugs on dev builds, the feature simply turns itself off, and testers\\nwill have to upgrade in order to see the feature again.\\nThis stands to eliminate false bug reports of crash bugs from\\ninternal\/alpha testers who simply need to upgrade.\\nBut if we want to rely on this as a safety mechanism, we need to make\\nsure that chrome extension assignment overrides don't have the\\nopportunity to win-out over a remote kill, because they are too blunt of\\na tool. As an alpha tester, you might see that the feature went away and\\nperform another override in the Chrome extension. If that happens,\\nyou'd be back in crash bug land.\\n","Decision":"## Decision\nOn TestTrack server, remote kills will be absolute and won't be\\noverridden by force assignments regardless of the relative recency of\\nthe override and the remote kill.\\n","Prediction":"## Decision\nWe will use chrome extensions to override remote kills.\\nWe will use chrome extensions to track which remote kill was overridden.\\nWe will mark which remote kill was overridden as a warning for the\\nchrome extension.\\nWe will create a separate chrome extension to track which remote kill was\\noverridden.\\nWe will create a separate chrome extension to track which remote kill was\\nnot overridden.\\nWe will create a separate chrome extension to track which remote kill was\\nnot overridden.\\nWe will create a separate chrome extension to track which remote kill was\\nnot overridden.\\nWe will create a separate chrome extension to track which remote kill was\\nnot overridden.\\nWe will create a separate chrome extension to track which remote kill was\\nnot overridden.\\nWe will create a separate chrome extension to track which remote kill was\\nnot overridden.\\nWe will create a separate chrome extension to track which remote kill was\\nnot overridden.\\nWe will create a separate chrome extension to track which remote kill was\\nnot overridden.\\nWe will create a separate chrome extension to track which remote kill was\\nnot overridden.\\nWe will create a separate chrome extension to track which remote kill was\\nnot overridden.\\nWe will create a separate chrome extension to track which remote kill"}
{"Context":"## Context and Problem Statement\\ntracking-consent-frontend provides a solution for tax users to express their cookie consent preferences via a\\nbanner and cookie settings page, storing these preferences in a `userConsent` cookie with\\nthe following structure:\\n```\\n{\\nversion: string, \/\/ Currently 2020.1\\ndatetimeSet: string, \/\/ e.g. 2021-03-04T15:16:47.077Z\\npreferences: {\\nacceptAll?: boolean, e.g. true\\nmeasurement?: boolean,\\nsettings?: boolean\\n}\\n};\\n```\\nThe `acceptAll` category was added to allow for future differentiation between users that chose to accept all\\nvia the banner vs users that accepted all by individually selecting 'Use' for each cookie type on the settings page.\\nWe now believe this feature may lead to services assuming the user has consented to the use of types of cookie that\\nwere not actually listed at the point the user consented. For example, if marketing cookies are added in 2022,\\nis it possible services may assume `acceptAll: true` set in 2021 means that the user has consented to the use of marketing cookies?\\nServices would need to rely on the cookie version or datetimeSet in order to determine what cookie types\\nexisted at the point the user consented.\\nGiven the need to differentiate between users accepting all cookies via the banner vs the cookie settings is not currently a requirement\\nshould we therefore, remove the acceptAll category and instead set individual categories, so that it is explicit in the userConsent\\nwhich categories the user has consented to?\\nSuch a change would result in a new version of the userConsent cookie with the following structure:\\n```\\n{\\nversion: string, \/\/ 2021.1\\ndatetimeSet: string, \/\/ e.g. 2021-03-04T15:16:47.077Z\\npreferences: {\\nmeasurement?: boolean,\\nsettings?: boolean\\n}\\n};\\n```\\n## Decision Drivers\\n* the preference for keeping the consenting data model simple, unambiguous and free from duplication\\n* the need to mitigate the risk of services misinterpreting the preferences stored in the userConsent cookie\\n* the lack of a current or anticipated future need to differentiate how a user has consented\\n* that storing preferences individually is consistent with the functionality implemented on GOV.UK (see links below)\\n","Decision":"## Decision\n* the preference for keeping the consenting data model simple, unambiguous and free from duplication\\n* the need to mitigate the risk of services misinterpreting the preferences stored in the userConsent cookie\\n* the lack of a current or anticipated future need to differentiate how a user has consented\\n* that storing preferences individually is consistent with the functionality implemented on GOV.UK (see links below)\\nChosen option: \"Remove 'acceptAll'\", because for the reasons above we believe it is the right thing to do and makes sense\\nto do before the solution goes live.\\n### Positive Consequences\\n* There is less risk user cookie preferences will be misinterpreted by services\\n* Teams are likely to be less confused and need to ask fewer questions around the meaning of the categories\\n### Negative Consequences\\n* The need to increment the userConsent version number and introduce logic for dealing\\nwith old cookie versions.\\n","Prediction":"## Decision\n* the preference for keeping the consenting data model simple, unambiguous and free from duplication\\n* the need to mitigate the risk of services misinterpreting the preferences stored in the userConsent cookie\\n* the lack of a current or anticipated future need to differentiate how a user has consented\\n* that storing preferences individually is consistent with the functionality implemented on GOV.UK (see links below)\\nChosen option: \"Don't distinguish between users that accept or reject cookies\", because it:\\n* allows for the cookie settings page to support both cookie acceptance and cookie rejection\\n* allows for the cookie settings page to be updated to support cookie acceptance and cookie rejection\\n* allows for the cookie settings page to be updated to distinguish between users that have consented to the use of marketing cookies and users that have not consented to the use of marketing cookies\\n* allows for the cookie settings page to be updated to differentiate between users that have consented to the use of specific cookie types (e.g. marketing cookies) and users that have not consented to the use of specific cookie types (e.g. marketing cookies)\\n* allows for the cookie settings page to distinguish between users that have consented to the use of specific cookie types (e.g."}
{"Context":"## Context\\n![Concepts](0005-old-concepts.png)\\n### Credentials are available as both files and environment variables\\nCredentials are made available in the clusters as secrets with an expected name.\\nNaiserator deploys applications with their requested Credential's properties as both _environment variables_ and _files_ in a mounted volume.\\nSome properties in the Credential are not made available as environment variables, because their names make for invalid environment variables.\\nAll properties of a Credential are available as files in the mounted volume.\\n### Credential rotation triggered by manual topic edit and on a schedule\\nKafkarator processes topics based on events from the Kubernetes API server.\\nThese events are `Added`, `Modified`, and `Deleted`.\\nIn addition, it will request a full list of topics from the Kubernetes API server every hour.\\nEvents will typically happen when a developer makes changes to the Topic's Kubernetes resource.\\nFor instance when:\\n- adding an application to the topic's ACLs\\n- changing topic configuration\\n- removing an application from the topic's ACLs\\n### Credentials expire on a \"faulty schedule\"\\nWhen Kafkarator processes a topic, it will generate any necessary service users, ACLs, and Credentials.\\nWhen making (or rotating) a Credential, it records the expiration timestamp of the Credential in the status-field of the Topic's Kubernetes resource.\\nThus, when processing a topic Kafkarator checks the expiration timestamp recorded on the Topic's Kubernetes resource, and if that timestamp is in the past, rotate _all_ the Credentials in use for accessing the Topic being processed.\\nWith the current implementation, an application can use the same Credential to access multiple topics, and thus risk multiple independent expiration timestamps (one per Topic) triggering rotation of its Credentials.\\nThis implementation detail brings not only the consequence of more frequent rotation of Credentials than intended, but also triggers rotation of Credentials at unforeseen\/unexpected times.\\n### All applications attached to a topic has their credentials rotated at the same time\\nDue to the same implementation details as [discussed here](#credentials-expire-on-a-faulty-schedule), when the expiration timestamp recorded on a topic has passed, all applications with access to that topic will have their Credentials rotated.\\nThis has resulted in massive Credential purges (and subsequent restarts) of applications with access to topics with many producers and consumers.\\n### Kafkarator purges all service users when processing a topic\\nWhen a topic is updated, the synchronization loop in Kafkarator kicks in.\\nAs part of this loop, all service users are re-created, rotating their credentials.\\n### Some teams have legacy systems that can't make use of auto-rotated credentials\\nSome teams have legacy systems that are not able to run in the NAIS platform's clusters at this time.\\nThis makes it difficult for them to make use of Aiven Kafka, since their applications can not get automated access to rotated Credentials.\\n### Current flow\\n![Current flow](.\/0005-old-kafkarator-flow.png)\\nKafkarator has two modes of operation: Primary and Follower.\\nFollower is only responsible for maintaining secrets in the non-gcp clusters, in response to events sent to it from the Primary.\\nPrimary takes care of managing topics, service users, ACLs and credentials.\\nIn the Primary main loop, the steps are roughly these:\\n1. Create or update topics\\n2. Create and\/or delete ACLs\\n3. Re-create service users\\n4. Generate secrets\\n5. Produce encrypted secrets to send to Follower\\n6. Write back status and sync hash\\n### Deterministic names\\nNames of service users, and secrets are deterministically generated and possible to recreate with only name and namespace of the owning application.\\n### There is a limit to the number of service users we can have in each Aiven project\\nAiven has a limit to the number of service users allowed. This current limit is 1000 service users for nav-dev and nav-prod projects. This is \"the maximum that has been applied so far for any customer\".\\n### Rotation is a security feature\\nOne way to solve these problems would be to drop rotation entirely.\\nInstead we could rely on manual rotation when we discover that we need it.\\nFrom a security perspective, that is not something we consider good enough.\\n### Other NAIS operators dealing with credentials\\nNAIS operates a number of operators, some of which deal with credentials.\\nA common pattern is Just-in-time provisioning of credentials.\\nIn this model credentials are generated\/rotated on each deploy, which would remove much of the problems we have today.\\n","Decision":"## Decision\n### New synchronization flow\\n![New synchronization flow](0005-new-synchronization-flow.png)\\n![New concepts](0005-new-concepts.png)\\nWe will split the loop into two parts.\\nThe existing main loop will now perform these steps:\\n1. Create or update topics\\n2. Create and\/or delete ACLs, using wildcards to match a set of service users\\n3. Write back status and sync hash\\nWhen Naiserator deploys an application that requests kafka access, Naiserator will create the Deployment with a reference to a new secret.\\nAt the same time, it will create an AivenApplication resource, naming that secret.\\nA new loop or possibly an entirely new operator will respond to AivenApplication resources and perform these steps:\\n1. Create service user in Aiven\\n2. Create secret in cluster\\n3. Write back status and sync hash\\n4. Delete obsolete secrets in cluster\\n5. Delete obsolete service users in Aiven\\nThis solves the problem of a topic's expiration timestamp triggering rotation, and provides a place to store additional information for each application.\\nInformation in AivenApplication:\\n- Name of secret\\n- Aiven service (starting with only Kafka)\\n- Pool (Kafka cluster in Aiven) this application uses\\n### Overlapping valid credentials\\nOn every deploy of an application, a service user will be created for it.\\nThis ensures fresh credentials on every deploy.\\nExisting service users are not touched unless no longer in use.\\n### Deterministic names for service users\\nService users need to have a fixed prefix, generated from application name, so that ACLs can be written to match using wildcards.\\n### Monitor number of service users\\nSince service users are a limited resource (at Aiven.io), we need to ensure that we monitor their numbers.\\nWhen we approach this limit, we must take steps to handle it - like requesting Aiven.io to increase the limit.\\n","Prediction":"## Decision\n### Rotation is a security feature\\nWhen a topic is updated, the synchronization loop in Kafkarator kicks in.\\nAs part of this loop, all service users are re-created, rotating their credentials.\\n### Some teams have legacy systems that can't make use of auto-rotated credentials\\nSome teams have legacy systems that are not able to run in the NAIS platform's clusters at this time.\\nThis makes it difficult for them to make use of Aiven Kafka, since their applications can not get automated access to rotated Credentials.\\n### Current flow\\n![Current flow](.\/0005-old-kafkarator-flow.png)\\nKafkarator has two modes of operation: Primary and Follower.\\nFollower is only responsible for maintaining secrets in the non-gcp clusters, in response to events sent to it from the Primary.\\nPrimary takes care of managing topics, service users, ACLs and credentials.\\nIn the Primary main loop, the steps are roughly these:\\n1. Create or update topics\\n2. Create and\/or delete ACLs\\n3. Re-create service users\\n4. Generate secrets\\n5. Produce encrypted secrets to send to Follower\\n6. Write back status and sync hash\\n### Determin"}
{"Context":"## Context\\nTo guide our work, we need to decide whether we use AWS\u2019 Elastic File System\\n(EFS) or Elastic Block Store (EBS) as the cloud storage service for our Jenkins\\nbuild.\\nThe decision is needed as it will impact on the team\u2019s work and may be affected\\nby wider GDS or Government strategy.\\n","Decision":"## Decision\nBased on the below pros and cons, we have decided to remain with EFS as the\\ncloud storage service, until such a time that there is a strategic need to\\nchange.\\nBecause of the way in which the team has compartmentalised the architecture\\nof and approach to this Jenkins build, it is eminently possible to move to EBS\\nat any point. It also means that further development of the system will not make\\nit more complicated to do so.\\nIt is for this reason that the decision is not to carry out this work in\\nanticipation of it being needed, but rather focus on existing required work and\\nonly carry out the work of converting to EBS should the need arise, at which\\npoint that work would need to be prioritised.\\n","Prediction":"## Decision\nWe will use EFS as the cloud storage service for our Jenkins build.\\nWe will use EBS as the cloud storage service if we need to scale up our\\nstorage.\\nWe will use EBS as the cloud storage service for our Jenkins\\ncontinuous integration (CI) jobs.\\nWe will use EBS as the cloud storage service for our Jenkins\\ncontinuous deployment (CD) jobs.\\nWe will use EFS as the cloud storage service for our Jenkins\\ncontinuous delivery (CD) jobs.\\nWe will use EFS as the cloud storage service for our Jenkins\\nimage builds.\\nWe will use EFS as the cloud storage service for our Jenkins\\nimage builds if we need to scale up our storage.\\nWe will use EFS as the cloud storage service for our Jenkins\\nimage builds if we need to scale up our storage.\\nWe will use EFS as the cloud storage service for our Jenkins\\nimage builds if we need to scale up our storage.\\nWe will use EFS as the cloud storage service for our Jenkins\\nimage builds if we need to scale up our storage.\\nWe will use EFS as the cloud storage service for our Jenkins\\nimage builds if we need to scale up our storage.\\nWe will"}
{"Context":"## Context\\nWhen you create a new Wicket project [like described in their documentation](https:\/\/wicket.apache.org\/start\/quickstart.html) it will use Maven as build tool.\\nThe examples in the [wicket-spring-boot-examples](https:\/\/github.com\/MarcGiffing\/wicket-spring-boot-examples) are applying the [wicket-spring-boot-starter](https:\/\/github.com\/MarcGiffing\/wicket-spring-boot\/tree\/master\/wicket-spring-boot-starter) as parent project, e.g. [spring-actuator\/pom.xml#L6](https:\/\/github.com\/MarcGiffing\/wicket-spring-boot-examples\/blob\/master\/spring-actuator\/pom.xml#L6).\\nThis has the implication that the version from the `wicket-spring-boot-starter` is also applied as version number for the actual project.\\nHowever, the project version number should be independent of this.\\nIt seem like this is not possible (see [stackoverflow.com](https:\/\/stackoverflow.com\/questions\/10582054\/maven-project-version-inheritance-do-i-have-to-specify-the-parent-version)).\\n","Decision":"## Decision\nMaybe it is not necessary to apply the Spring Boot Wicket Starter project as parent project.\\nMaybe there is some workaround to for the issue with the version number.\\nHowever, Gradle might still be a good choice as it has a more concise syntax and offers nice features like a handy sub-module system.\\nThus the build process of the project will be implemented with the help of [Gradle](https:\/\/gradle.org\/) tasks.\\n","Prediction":"## Decision\nThe project version number will not be inherited from the parent project.\\nThis will allow us to have a single project version for the parent, and a custom version for each child.\\nThis will also allow us to have a single project version for each module, and a custom version for each submodule.\\nThis will also allow us to have a single project version for the parent, and a custom version for each dependency.\\nThis will also allow us to have a single module version for the parent, and a custom version for each dependency.\\nThis will also allow us to have a single project version for the parent, and a custom version for each dependency.\\nThis will also allow us to have a single module version for the parent, and a custom version for each dependency.\\nThis will also allow us to have a single module version for the parent, and a custom version for each dependency.\\nThis will also allow us to have a single module version for the parent, and a custom version for each dependency.\\nThis will also allow us to have a single module version for the parent, and a custom version for each dependency.\\nThis will also allow us to have a single module version for the parent, and a custom version for each dependency.\\n"}
{"Context":"## Context (Discussion)\\nWe want to give support to any new Froala major version. For this reason, we must decide whether to create a new plugin for the new version of Froala, or use an existing one.\\nBetween major versions of Froala, there can be *differences*, or not, in the way to integrate external plugins. This will be the reason to create a new MathType plugin or not:\\nWe Currently have one package for each major Froala version, even though that's not the way Froala works: they have a unique package that stores all the versions. Even if we decided to follow the conventions Froala does, we would have to rename or delete our current modules, and npm wouldn't allow it, meaning that once a name is used, it can't be used never again for a new package.\\n","Decision":"## Decision\nWe will create and give the Froala MathType packages for any new major version, the following name: *@wiris\/mathtype-froalaX*, where X equals the oldest Froala major version. Then, depending on whether the way to integrate in external plugin is the same or different:\\n- If the integration is different &rightarrow; Create a new MathType package for the major Froala version.\\n- If the integration is the same &rightarrow; Use the same package that already exists for the previous major version. A good example is the situation we are in with the versions 3 and 4, where the integration is the same, so the package for both versions is the mathtype-froala3.\\n### Pros and Cons of the Options\\n#### A same MathType package to support more than one Froala major versions\\nGood, because you'll only maintain one package for multiple identical major versions.\\nGood, because it'll reduce DRI.\\nBad, because the package name is not self-explanatory.\\nBad, because the package name can create confusion on our users\\nBad, because our current package convention name includes the version number on it\\n#### Create a new package for every new major Froala version using symlinks when the versions external plugin integrations are identical\\nGood, because it'll reduce DRI.\\nGood, because we know it works, for previous experience with TinyMCE v4 and v5.\\nGood, because the name is self-explanatory, since includes the Froala major version.\\nBad, because, we'll have to maintain more than one package, at least.\\nBad, because the infrastructure is more complex.\\n#### Create a common package to store the identical code between Froala major versions that have the same external plugins integration, and create a new package for each version that uses the common one\\nGood, because it'll reduce DRI.\\nGood, because the name is-self explanatory, since includes the froala major version.\\nBad, because, we'll have to maintain more than one package, at least thee.\\n","Prediction":"## Decision\nWe will create a MathType plugin, which will be the same for every Froala version.\\nWe will create a package for every MathType library we want to integrate, which will be the same for every Froala version.\\nWe will use the MathType library as a dependency for our MathType plugin, which will be the same for every Froala version.\\nWe will rename MathType as MathTypePlugin, and the MathType library as MathType.\\nWe will use the MathType library as a dependency for our MathType plugin, which will be the same for every Froala version.\\nWe will rename MathType as MathTypePlugin, and the MathType library as MathType.\\nWe will use the MathType library as a dependency for our MathType plugin, which will be the same for every Froala version.\\nWe will rename MathType as MathTypePlugin, and the MathType library as MathType.\\nWe will use the MathType library as a dependency for our MathType plugin, which will be the same for every Froala version.\\nWe will rename MathType as MathTypePlugin, and the MathType library as MathType.\\nWe will use the MathType library as a dependency for our MathType"}
{"Context":"## Context\\nWe require a method of managing and directing external internet traffic into the cluster. Kubernetes provides [several options for handling inbound traffic](https:\/\/medium.com\/google-cloud\/kubernetes-nodeport-vs-loadbalancer-vs-ingress-when-should-i-use-what-922f010849e0).\\nWe do not want to expose the cluster directly to the internet, and so require an AWS load balancer in front of the cluster. The load balancer must support TLS termination and integration with our DNS provider (AWS Route 53).\\nHistorically Kubernetes has supported provisioning of ALBs and NLBs for `Service` resources of `type=LoadBalancer` via the in-tree (built-in) [AWS cloud provider](https:\/\/github.com\/kubernetes\/cloud-provider-aws), with out-of-tree controllers required for `Ingress` resources. Built-in cloud providers are now [considered deprecated overall, in favour of out-of-tree providers](https:\/\/kubernetes.io\/blog\/2019\/04\/17\/the-future-of-cloud-providers-in-kubernetes\/), so an [Ingress Controller](https:\/\/kubernetes.io\/docs\/concepts\/services-networking\/ingress-controllers\/) with support for `Service` resources is required.\\nThe primary and recommended ingress controller for AWS\/EKS is the [AWS Load Balancer Controller](https:\/\/docs.aws.amazon.com\/eks\/latest\/userguide\/aws-load-balancer-controller.html), which can provision and manage [ALBs for `Ingress` resources and NLBs for `Service` resources](https:\/\/kubernetes-sigs.github.io\/aws-load-balancer-controller\/v2.2\/#aws-load-balancer-controller).\\nWe must also consider how Kubernetes edge services and AWS load balancers will interact with the existing [GOV.UK Router service](https:\/\/github.com\/alphagov\/router), as there is significant overlap in their functionality and responsibilities. This will require further investigation and likely experimentation, and so that end we should ensure that we're able to use both `Ingress` and `Service` Kubernetes resources so that we have the flexibility to support a wide range of use cases in the immediate term - L4 & L7 traffic, name-based routing, HTTP->HTTPS redirection, etc.\\n","Decision":"## Decision\nUse the [AWS Load Balancer Controller](https:\/\/github.com\/kubernetes-sigs\/aws-load-balancer-controller).\\n","Prediction":"## Decision\nWe will use the AWS Load Balancer Controller to provision and manage ALBs and NLBs for `Service` resources of `type=LoadBalancer`.\\nWe will also use the AWS Load Balancer Controller to provision and manage `AWS ECS Service` resources of `type=LoadBalancer`.\\nWe will use the AWS Load Balancer Controller to provision and manage `AWS Load Balancers` for both `AWS ECS Service` and `AWS ECS Load Balancers`.\\nWe will use the AWS Load Balancer Controller to provision and manage `AWS ECS DaemonSet` resources of `type=NodePort`.\\nWe will use the AWS Load Balancer Controller to provision and manage `AWS ECS Service` resources of `type=LoadBalancer`.\\nWe will use the AWS Load Balancer Controller to provision and manage `AWS ECS Load Balancers` for both `AWS ECS Service` and `AWS ECS DaemonSet` resources.\\nWe will use the AWS Load Balancer Controller to provision and manage `AWS ECS DaemonSet` resources of `type=NodePort`.\\nWe will use the AWS Load Balancer Controller to provision and manage `AWS ECS DaemonSet` resources of `type=NodePort`.\\nWe will use the"}
{"Context":"## Context\\nDesigning modern scalable cloud based applications requires intentionally\\ndesigning the architecture to take advantage of the cloud.\\nOne leading way to do that is\\n[The Twelve Factor](https:\/\/12factor.net) methodology.\\n","Decision":"## Decision\nWe will follow Twelve Factor methodology.\\n","Prediction":"## Decision\nWe will use the 12 Factor Methodology to architect our application.\\nWe will also use the [Cloud Formation](https:\/\/cloud.google.com\/formation) stack template to provision\\nour application.\\nWe will use [GKE](https:\/\/cloud.google.com\/kubernetes-engine) for our Kubernetes cluster.\\nWe will also use [Docker Compose](https:\/\/docs.docker.com\/engine\/recipes\/) to create our environment.\\nWe will use [Vagrant](https:\/\/www.vagrantup.com\/), [Ansible](https:\/\/www.ansible.com\/), and [Kubernetes](https:\/\/kubernetes.io\/).\\nWe will use [Kubernetes Ingress Rules](https:\/\/kubernetes.io\/docs\/concepts\/glossary\/) to route traffic\\nto our application.\\nWe will use [Kubernetes DNS](https:\/\/kubernetes.io\/docs\/concepts\/administration\/configuration\/) to provide\\na DNS service for our application.\\nWe will use [Kubernetes Load Balancing](https:\/\/kubernetes.io\/docs\/concepts\/administration\/configuration\/) to\\nprovide an HTTP load balancing service for our application.\\nWe will use [Kubernetes Secrets Management](https:\/\/kubernetes.io\/docs\/concepts\/administration\/configuration\/) to\\nprovide a way to securely"}
{"Context":"## Context\\nI feel that ADRs in some form may be very beneficial at Envisage, but two things make me reluctant to push the practice right now.  First, ignorance--I've never done this, so I can't talk personally about the experience.  Second, the enthusiasm of first encounter.  Both of these are addressed by actually doing it for a while.\\nCopper Bend is a learning\/entertainment project.  Trying new techniques and technologies is a primary goal.\\nThe Envisage products are very different from this one.  What works well here won't necessarily cross over.\\n","Decision":"## Decision\nI will avoid tooling, and simply handcraft the documents.  I'll keep them in the main project repo.  I'll review periodically, trimming the present-but-useless and adding the missing-yet-promising.\\nI won't worry about synchronizing the format between ADR docs.\\n","Prediction":"## Decision\nIf we want to push the envelope of ADR in Envisage, we should start with Copper Bend.\\nWe should start with a specific, well-defined project, not a general \"we should try to do things better\" project.\\nWe should start with a specific, well-defined use case.\\nWe should start with a specific, well-defined problem to solve.\\nWe should start with a specific, well-defined approach.\\nWe should start with a specific, well-defined stack.\\nWe should start with a specific, well-defined technology stack.\\nWe should start with a specific, well-defined methodology.\\nWe should start with a specific, well-defined process.\\nWe should start with a specific, well-defined team.\\nWe should start with a specific, well-defined decision-making process.\\nWe should start with a specific, well-defined decision tree.\\nWe should start with a specific, well-defined template.\\nWe should start with a specific, well-defined template.  This is the most important decision.\\nWe should start with a specific, well-defined template.  This is the most important decision.\\nWe should start with a specific, well-defined template.  This is the most important decision.\\nWe should start with a"}
{"Context":"## Context\\nTraffic flows for frontend applications in the following way:\\n![Frontend traffic flow](.\/0027-move-publicapi-away-from-frontend-lb-img01.jpg?raw=true \"Frontend traffic flow\")\\nMost frontend applications can go straight from the cache instances to the application instances, bypassing frontend-lb. They do this by creating a DNS entry for each application service ELB, and setting the appropriate upstream DNS entry for the nginx configuration on the cache machines.\\nThe exception to this rule is the \"publicapi\" application ([reference](https:\/\/github.com\/alphagov\/govuk-puppet\/blob\/34c587b76abee63d0c84b9f67212ac2f1c65ba2e\/modules\/govuk\/manifests\/apps\/publicapi.pp)).\\nThis is a set of nginx configuration that sits on frontend-lb, and forwards specific paths to applications upstream.\\nRouter on the cache instances forwards all traffic bound for `\/api` to frontend-lb, and then publicapi handles the forwarding from there.\\nThe configuration for the paths is [relatively simple](https:\/\/github.com\/alphagov\/govuk-puppet\/blob\/34c587b76abee63d0c84b9f67212ac2f1c65ba2e\/modules\/govuk\/templates\/publicapi_nginx_extra_config.erb).\\n","Decision":"## Decision\nMove publicapi application to either the cache instances themselves, or replace the configuration with an Application Load Balancer.\\n","Prediction":"## Decision\nWe will move all publicapi instances to publicapi-nginx, and remove publicapi from publicapi.\\nWe will move publicapi-nginx to publicapi, and remove publicapi from publicapi.\\nWe will remove the `publicapi` configuration from the `publicapi-nginx` configuration, and add an `upstream` configuration to publicapi-nginx that will use the `publicapi-nginx` configuration to route traffic to the correct application.\\nThis will be done by creating a new `publicapi-nginx` configuration, that will be added to the `publicapi-nginx` configuration.\\nThis configuration will then be used to route traffic to the correct application, and remove the old `publicapi` configuration.\\nThis will be done by creating a new `publicapi-nginx` configuration, that will be added to the `publicapi-nginx` configuration.\\nThis configuration will then be used to route traffic to the correct application, and remove the old `publicapi` configuration.\\nThis will be done by creating a new `publicapi-nginx` configuration, that will be added to the `publicapi-nginx` configuration.\\nThis configuration will then be used to route traffic to the correct application, and remove"}
{"Context":"## Context\\nA [solution facilitates a coherent set of business functions](0026-solution-facilitates-a-coherent-set-of-business-functions.md). Those functions can originate from multiple domains. A [project always belongs to a single domain](0027-a-gcp-project-belongs-to-a-single-domain.md). Therefore, a solution can be implemented by multiple projects, either due to the fact that it requires functions from multiple domains, or because projects allow better modularization of the solution, or both.\\n![Structure of projects, domains and solutions](solution_project_domain.png \"Projects in different domains implementing a solution\")\\n","Decision":"## Decision\nWe implement a solution by one or more projects.\\n","Prediction":"## Decision\nThe [solution will be implemented in multiple projects](0028-solution-implementation.md). The solution will be implemented in one domain, which is a subdomain of `projects.domain`.\\n### Subdomains\\nSubdomains are used to separate the solution from the other projects implementing the same solution. This way, the solution can be easily tested, and its implementation can be modified without breaking other projects implementing the same solution.\\n### Solution\\nThe solution will be implemented in multiple projects. Each project will implement the solution in its own domain. The solution will be implemented in one domain, which is a subdomain of `projects.domain`. The solution will be implemented in a single file, and the solution will be implemented in a single directory. The solution will be implemented by a single class, with a single method, which will be implemented by each project implementing the solution. The solution will be implemented in a single file, and the solution will be implemented in a single directory. The solution will be implemented in a single class, with a single method, which will be implemented by each project implementing the solution. The solution will be implemented in a single file, and the solution will be implemented in a single directory. The solution will be implemented in a single class,"}
{"Context":"## Context\\n> This section contains all the context one needs to understand the current state, and why there is a problem. It should be as succinct as possible and introduce the high level idea behind the solution.\\n","Decision":"## Decision\n> This section explains all of the details of the proposed solution, including implementation details.\\nIt should also describe affects \/ corollary items that may need to be changed as a part of this.\\nIf the proposed change will be large, please also indicate a way to do the change to maximize ease of review.\\n(e.g. the optimal split of things to do between separate PR's)\\n","Prediction":"## Decision\nThe current state is a web application running on Heroku, which is currently using Cloudflare to proxy requests to the app. The proxy is configured to drop all the requests that are not on the http\/1.1 protocol.\\nThe problem is that the proxy is configured to drop requests that are not on the http\/1.1 protocol, but the proxy is configured to drop requests that are on the http\/1.1 protocol. This means that the proxy will drop requests that are not on the http\/1.1 protocol, but will still drop requests that are on the http\/1.0 protocol. This means that the proxy is still dropping the requests that are not on the http\/1.1 protocol.\\n### Solution\\nWe will configure the proxy to drop all the requests that are not on the http\/1.1 protocol. We will then configure the proxy to drop all the requests that are not on the http\/1.0 protocol.\\n### Decision\n### Solution\\nWe will configure the proxy to drop all the requests that are not on the http\/1.1 protocol. We will then configure the proxy to drop all the requests that are not on the http\/1.0 protocol.\\n### Decision\n###"}
{"Context":"## Context and Problem Statement\\nThe hybrid object-oriented\/functional language Scala is ideally suited for developing LaTiS.\\n## Decision Drivers <!-- optional -->\\n* merits of functional programming languages\\n* merits of strongly typed languages\\n* availability of developers\\n* consensus of community of scientific programmers\\n* suitability for processing large data sets\\n* concepts involving code correctnes\\n* transformations to data are explicit and can be reasoned about\\n* support for DSLs (Domain Specific Languages)\\n","Decision":"## Decision\n* merits of functional programming languages\\n* merits of strongly typed languages\\n* availability of developers\\n* consensus of community of scientific programmers\\n* suitability for processing large data sets\\n* concepts involving code correctnes\\n* transformations to data are explicit and can be reasoned about\\n* support for DSLs (Domain Specific Languages)\\nChosen option: Scala is the language-of-choice for LaTiS development.  The only decision driver above that is not in alignment with this decision is the fact that Python is the preferred language for most scientific programmers.\\n### Positive Consequences <!-- optional -->\\n* immutable data types in Scala reduce the challenges of introducing parallelism\\n* for an extensible library like LaTiS a functional programming language like Scala provides better abstractions and tools to reason about transformations of large data sets\\n* a strongly typed language like Scala helps to prevent code rot and reduces the chance that defects will slip into the code base\\n* well designed Scala applications are based on sound software engineering principles even though they may take more effort to build than similar Java or Python programs\\n* that being said, Scala programs tend to be easier to maintain and refactor which actually minimizes effort over the life of a project\\n* LaTiS is primarily a framework for transforming large datasets in a performant manner, this meshes nicely with the functional approach that emphasizes creating abstractions that do things to data rather than creating specific recipes for specific use cases\\n* Apache Spark, written in Scala, is the leading framework for processing large amounts of data and is becoming on of the primary framework for implementing machine learning algorithms\\n* Scala supports the creation of DSLs which will allow users of LaTiS to specify processing instructions for datasets without themselves nedding to learn Scala\\n* the full gamut of Java libraies is available to Scala developers\\n* many successful Java libraries eventually get re-written in Scala\\n* Scala encourages software craft-people to follow sound engineering principles instead of just developing something that just gets the job done, this is probably the most important factor in choosing Scala, but the hardest to document\\n### Negative Consequences\\n* Scala developers are not as common as Python developers, especially in the field of scientific computing\\n* Python is clearly the dominant language used by scientific programmers\\n","Prediction":"## Decision\n* merits of functional programming languages\\n* merits of strongly typed languages\\n* availability of developers\\n* consensus of community of scientific programmers\\n* suitability for processing large data sets\\n* concepts involving code correctnes\\n* transformations to data are explicit and can be reasoned about\\n* support for DSLs (Domain Specific Languages)\\nChosen option: \"Scala with Java interfaces\", because\\n* Scala is a strongly typed language\\n* Java is an object-oriented programming language with a strong type system\\n* Scala provides a DSL implementation of the Java interface\\n* Scala is the language of choice for scientific programmers\\n* Scala is the language of choice for scientific computing\\n* Scala is the language of choice for scientific computing\\n* Scala is the language of choice for scientific computing\\n* Scala is the language of choice for scientific computing\\n* Scala is the language of choice for scientific computing\\n* Scala is the language of choice for scientific computing\\n* Scala is the language of choice for scientific computing\\n* Scala is the language of choice for scientific computing\\n* Scala is the language of choice for scientific computing\\n* Scala is the language of choice for scientific computing\\n* Scala is the language of choice for scientific computing\\n* Scala is the language of choice for scientific computing\\n"}
{"Context":"## Context\\nSitting directly on the `\/find-a-pharmacy` path is not in keeping with the organisations plans for\\ninformation architecture going forward. When the service moves from beta.nhs.uk to nhs.uk we want\\nit to sit on path consistent with other finder services.\\n","Decision":"## Decision\nThe site root will be changed to be `\/service-search\/find-a-pharmacy`\\n","Prediction":"## Decision\nWe will move from using the `\/find-a-pharmacy` path to `\/pharmacy`.\\nThis will be the new URL for the service and will be maintained by the NHS.\\nPharmacy finder services will be moved to `pharmacy.nhs.uk`.\\nWe will migrate the `pharmacy.nhs.uk` URL to `pharmacy.nhs.uk` in the near future.\\nWe will use the `pharmacy.nhs.uk` URL as the primary endpoint of the NHS\\nPharmacy Finder service.\\nWe will use the `pharmacy.nhs.uk` endpoint to pass the `pharmacy.nhs.uk` URL\\nto the `pharmacy.nhs.uk` endpoint.\\nWe will use the `pharmacy.nhs.uk` endpoint to pass the `pharmacy.nhs.uk` URL\\nto the `pharmacy.nhs.uk` endpoint.\\nWe will use the `pharmacy.nhs.uk` endpoint to pass the `pharmacy.nhs.uk` URL\\nto the `pharmacy.nhs.uk` endpoint.\\nWe will use the `pharmacy.nhs.uk` endpoint to pass the `pharmacy.nhs.uk` URL\\nto the `pharmacy.nhs.uk` endpoint.\\nWe will"}
{"Context":"## Context\\nThe UI might request a resource after it's creation or alteration, while the async projections are not yet up to date with this change.\\nIn that case, the user would see stale data without knowing why, perhaps thinking its changes were not accepted by the application.\\nTo prevent this, the client needs a mechanism to check whether or not its latest update has already been processed by the projections.\\n","Decision":"## Decision\nWe will handle eventual consistency concerns using ETags. On succesful POST or PUT to the API, the client will receive a response containing an ETag header. This header will contain the highest event number caused by the client's request.\\nAll Projections will update their ProjectionState with this number.\\nIf a client wants to know if the resource is up to date with their changes, they will have to include the ETag in a `If-Match` header. Depending on whether the Projections is up to date or not, the API will return a `200 OK` or `412 PRECONDITION FAILED`.\\n","Prediction":"## Decision\nThe UI will send a request to the projections, which will return a boolean to indicate whether or not the latest update has been processed.\\nIf the update has already been processed, the UI will display a red background instead of the expected blue.\\nIf the update has not yet been processed, the UI will display a green background.\\nIn either case, the user will not be able to interact with the UI.\\nThis decision was made because the UI will need to be able to handle any number of updates, and the UI should not risk blocking the projections.\\nThis decision was made because the UI should be able to handle any number of updates, and the UI should not risk blocking the projections.\\nThis decision was made because the UI should be able to handle any number of updates, and the UI should not risk blocking the projections.\\nThis decision was made because the UI should be able to handle any number of updates, and the UI should not risk blocking the projections.\\nThis decision was made because the UI should be able to handle any number of updates, and the UI should not risk blocking the projections.\\nThis decision was made because the UI should be able to handle any number of updates, and the UI should not risk blocking the projections"}
{"Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"## Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in\\n[documenting architecture decisions](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions)\\n","Prediction":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\nWe will use [Architectural Styles and Patterns](http:\/\/www.amazon.com\/Architectural-Styles-Patterns-Michael-Nygard\/dp\/0672333647) as a reference, but we will not be following them precisely.\\nWe will use a style guide, [Architectural Styles and Patterns](http:\/\/www.amazon.com\/Architectural-Styles-Patterns-Michael-Nygard\/dp\/0672333647), as a reference, but we will not be following them precisely.\\nWe will use a style guide, [Architectural Styles and Patterns](http:\/\/www.amazon.com\/Architectural-Styles-Patterns-Michael-Nygard\/dp\/0672333647), as a reference, but we will not be following them precisely.\\nWe will use a style guide, [Architectural Styles and Patterns](http:\/\/www.amazon.com\/Architectural-Styles-Patterns-Michael-Nygard\/dp\/0672333647), as a reference, but we will not"}
{"Context":"## Context and Problem Statement\\nWhen using JSON Web Tokens generating signatures and verifying them is an important task.\\nJWTs can be signed using a secret (with the HMAC algorithm) or a public\/private key pair using RSA or ECDSA.\\nWhen implementing JWTs one must decide which method to use.\\n## Decision Drivers\\n* Multi tenant support with own keys for each tenant\\n","Decision":"## Decision\n* Multi tenant support with own keys for each tenant\\nChosen option: \"Asymmetric JWT signing\", because it the only option which allow to use different keys for different tenants.\\n### Positive Consequences\\n* multiple keys are supported\\n### Negative Consequences\\n* complex management of keys\\n","Prediction":"## Decision\n* Multi tenant support with own keys for each tenant\\nChosen option: \"JWT with a private\/public key pair\", because it allows to generate and verify JWTs even if the private key is not available.\\n### Positive Consequences\\n* JWTs can be generated and verified with the private key\\n* JWTs can be generated with the public key\\n### Negative Consequences\\n* JWTs with the public key are not easy to generate and verify\\n* The private key is not available for each tenant\\n* A single tenant can generate only one JWT at a time\\n### Impact on other Systems\\n* The private key is not available for each tenant\\n* A single tenant can generate only one JWT at a time\\n### References\\n* [JWT with a private\/public key pair](https:\/\/tools.ietf.org\/html\/rfc7519#section-5.1)\\n* [JWT with a private\/public key pair](https:\/\/tools.ietf.org\/html\/rfc7519#section-5.1)\\n* [JWT with a private\/public key pair](https:\/\/tools.ietf.org\/html\/rfc7519#section-5.1)\\n* [JWT with a private\/public key pair](https:\/\/tools.ietf.org\/html\/rfc7519"}
{"Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","Prediction":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\nWe will use [Confluence](https:\/\/www.confluence.atlassian.com\/display\/DOC\/Architecture+Decision+Record) for version control.\\nWe will use a [standard template](https:\/\/github.com\/rockafellar\/adr-template) for formatting our ADRs.\\nWe will use a [standard format](https:\/\/github.com\/rockafellar\/adr-template\/blob\/master\/doc\/adr-template.md) for formatting our ADRs.\\nWe will use a [standard language](https:\/\/github.com\/rockafellar\/adr-template\/blob\/master\/doc\/adr-template.md#adr-template-language) for formatting our ADRs.\\nWe will use a [standard template language](https:\/\/github.com\/rockafellar\/adr-template\/blob\/master\/doc\/adr-template.md#adr-template-language) for formatting our ADRs.\\nWe will use a [standard template language](https:\/\/github.com\/rockafellar\/adr-template\/blob\/master\/doc\/adr-template.md#adr-template-language) for formatting our ADRs.\\nWe will"}
{"Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"## Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","Prediction":"## Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\nWe will use Architecture Toolkit to help us with the formatting.\\nWe will use Architecture Decision Manager to help us with the tracking and versioning.\\nWe will use Architecture Style Guide to help us with the style of the architecture decisions.\\nWe will use Architecture Tools to help us with the code.\\nWe will use Architecture Review to help us with the review process.\\nWe will use Architecture Verification to help us with the verification process.\\nWe will use Architecture Verification Types to help us with the verification process.\\nWe will use Architecture Verification Tools to help us with the verification process.\\nWe will use Architecture Verification Types to help us with the verification process.\\nWe will use Architecture Verification Types to help us with the verification process.\\nWe will use Architecture Decision Manager to help us with the tracking and versioning.\\nWe will use Architecture Decision Manager to help us with the tracking and versioning.\\nWe will use Architecture Decision Manager to help us with the tracking and versioning.\\nWe will use Architecture Decision Manager to help us with the tracking and version"}
{"Context":"## Context\\nBookit needs to persist the locations, bookables, and bookings so that the data survives multiple instances and deployments over time.\\n","Decision":"## Decision\n* Use SQL approach as opposed to NoSQL solution - the model is simple and ACID transactions keep multiple users separate\\n* Use H2 for unit testing & local development - speeds up execution time and reduces external dependencies\\n* Use AWS RDS Aurora (MySQL) for integration\/staging\/production - better HA & continuous snapshots (enabled for production)\\n* Use MariaDB JDBC driver - has native Aurora support for failover\\n","Prediction":"## Decision\nBookit will store data in the following format:\\n* [bookings](bookings.json), where each bookable has a list of locations\\n* [bookings](bookings.json), where each bookable has a list of bookings\\n* [bookings](bookings.json), where each bookable has a list of locations\\n* [bookings](bookings.json), where each bookable has a list of bookings\\n* [bookings](bookings.json), where each bookable has a list of locations\\n* [bookings](bookings.json), where each bookable has a list of bookings\\n* [bookings](bookings.json), where each bookable has a list of locations\\n* [bookings](bookings.json), where each bookable has a list of bookings\\n* [bookings](bookings.json), where each bookable has a list of bookings\\n* [bookings](bookings.json), where each bookable has a list of locations\\n* [bookings](bookings.json), where each bookable has a list of bookings\\n* [bookings](bookings.json), where each bookable has a list of bookings\\n* [bookings](bookings.json), where each bookable has"}
{"Context":"## Context\\nIn order to allow apps to ship independently, we need to ensure that\\nsplit names defined in different apps don't collide. The strategy until\\nnow has been for developers to run all the apps that might have\\nconflicting splits in local dev, but this doesn't scale well as they may\\nnot have the latest version of each app at all times and a\\nfully-upgraded local testtrack server to detect conflicts that might\\narise in production.\\nWe need to make sure that migrations don't get jammed up in production\\neven if a developer only has their own app downloaded locally.\\n","Decision":"## Decision\nNew migration runners will be expected to prefix their split names with\\ntheir app names and a dot. Legacy runners will be grandfathered out of\\nthis constraint, but we will begin soft enforcing that any new split\\nname conform to using at most a single dot, and that the text before\\nthat dot be the app name of the split's owner_app.\\n","Prediction":"## Decision\nWe will allow migrations to be run in production when they're\\nconfigured to run in production. If a developer is running\\nmultiple migrations in production, we will only run the\\nmigration that was configured to run in production.\\nThis means that if a developer is running multiple\\nmigrations and they're in production, we will only run the\\nmigration that was configured to run in production.\\nThis means that if a developer is running multiple\\nmigrations and they're in production, we will only run the\\nmigration that was configured to run in production.\\nThis means that if a developer is running multiple\\nmigrations and they're in production, we will only run the\\nmigration that was configured to run in production.\\nThis means that if a developer is running multiple\\nmigrations and they're in production, we will only run the\\nmigration that was configured to run in production.\\nThis means that if a developer is running multiple\\nmigrations and they're in production, we will only run the\\nmigration that was configured to run in production.\\nThis means that if a developer is running multiple\\nmigrations and they're in production, we will only run the\\nmigration that was configured to run in production.\\nThis means that"}
{"Context":"## Context\\nIn the future, once all product features are built, there will be a lot of work on building and enhancing health modules. This might eventually also become its own project.\\nThere needs to be a contract that a health module exposes. We will have interfaces defined in the health module, and communication using the models (which will be common across openchs-android and openchs-health-modules). This will allow common domain logic to live in modules and health module specific business logic to live in the models.\\n","Decision":"## Decision\nThe android client will be broken down into 3 components - openchs-android for the app, openchs-health-modules for the health modules and openchs-models for the contract objects that will be exchanged by openchs-android to openchs-health-modules.\\n","Prediction":"## Decision\nWe will use the `@Contract` annotation to specify contracts in the health module.\\nWe will use the `@Contract` annotation to specify contracts in the health module.\\nWe will use the `@Contract` annotation to specify contracts in the health module.\\nWe will use the `@Contract` annotation to specify contracts in the health module.\\nWe will use the `@Contract` annotation to specify contracts in the health module.\\nWe will use the `@Contract` annotation to specify contracts in the health module.\\nWe will use the `@Contract` annotation to specify contracts in the health module.\\nWe will use the `@Contract` annotation to specify contracts in the health module.\\nWe will use the `@Contract` annotation to specify contracts in the health module.\\nWe will use the `@Contract` annotation to specify contracts in the health module.\\nWe will use the `@Contract` annotation to specify contracts in the health module.\\nWe will use the `@Contract` annotation to specify contracts in the health module.\\nWe will use the `@Contract` annotation to specify contracts in the health module.\\nWe will use the `@Contract` annotation to specify contracts in the health module.\\nWe"}
{"Context":"## Context\\nWe need to adopt system architecture style adjecent to our architectural drivers\\n","Decision":"## Decision\nWe will use modular monolith architecture style\\n","Prediction":"## Decision\nWe will use [OpenAPI](https:\/\/www.openapi.org\/)\\nWe will use [Swagger](https:\/\/swagger.io\/)\\nWe will use [Swagger UI](https:\/\/swagger.io\/swagger-ui\/) for the UI\\nWe will use [Swagger Cloud](https:\/\/swagger.cloud) for code generation and publishing of generated API documentation\\nWe will use [Swagger Cloud](https:\/\/swagger.cloud) for code generation and publishing of generated API documentation\\nWe will use [Swagger Cloud](https:\/\/swagger.cloud) for code generation and publishing of generated API documentation\\nWe will use [Swagger Cloud](https:\/\/swagger.cloud) for code generation and publishing of generated API documentation\\nWe will use [Swagger Cloud](https:\/\/swagger.cloud) for code generation and publishing of generated API documentation\\nWe will use [Swagger Cloud](https:\/\/swagger.cloud) for code generation and publishing of generated API documentation\\nWe will use [Swagger Cloud](https:\/\/swagger.cloud) for code generation and publishing of generated API documentation\\nWe will use [Swagger Cloud](https:\/\/swagger.cloud) for code generation and publishing of generated API documentation\\nWe will use [Swagger Cloud](https:\/\/swagger.cloud) for code generation and publishing of generated API documentation\\nWe will use [Swagger Cloud](https"}
{"Context":"## Context\\nThe issue motivating this decision, and any context that influences or constrains the decision.\\n","Decision":"## Decision\nThe change that we're proposing or have agreed to implement.\\n","Prediction":"## Decision\nThe change that we're proposing or have agreed to implement.\\n### Positive Consequences\\n* The change that we're proposing or have agreed to implement.\\n### Negative Consequences\\n* We're not sure if we really need this change, or if it will have unintended consequences.\\n* We're not sure if we really understand the problem we're trying to solve, or if we've considered trade-offs.\\n* We're not sure if we've considered all the possible changes that we can make to our system to get it to work the way we want it to work.\\n### Alternative or additional solutions\\n* We're not sure if we really need this change, or if it will have unintended consequences.\\n* We're not sure if we really understand the problem we're trying to solve, or if we've considered trade-offs.\\n* We're not sure if we've considered all the possible changes that we can make to our system to get it to work the way we want it to work.\\n### Decision\nThe change that we're proposing or have agreed to implement.\\n### Positive Consequences\\n* We're not sure if we really need this change, or if it will have unintended consequences.\\n* We're not sure if"}
{"Context":"## Context\\nAsyncApis are not semantically the same that other kinds of apis, like Web Apis. Some adopters need to know if they are looking specifically an async api, regarding the structure is the same or no.\\nAlso, we need to prepare our model to support different kinds of Apis.\\n","Decision":"## Decision\nCreate a hierarchy based on Apis. A Document unit will encodes an Api. This Api is abstract. There will be different types of Apis depending on the specifications.\\n","Prediction":"## Decision\n- We will keep the same structure of the current model\\n- We will use the `async` field to indicate that is an async api\\n- We will keep the `name` field to indicate the name of the api\\n- We will keep the `type` field to indicate the type of the api\\n- We will add a new field `async` to the API class\\n- We will add a new method to the API class to return the async version of the api\\n- We will add a new method to the API class to return the non-async version of the api\\n- We will add a new method to the API class to return an error\\n- We will add a new method to the API class to return the error code\\n- We will add a new method to the API class to return the error description\\n- We will add a new method to the API class to return the error type\\n- We will add a new method to the API class to return the error message\\n- We will add a new method to the API class to return the error log\\n- We will add a new method to the API class to return the error status\\n- We will add a new method to the API class to return the error"}
{"Context":"## Context and Problem Statement\\nThe Subscription Platform was implemented with Stripe as the payment processor and driver of subscription logic. Stripe is used as the source of truth for whether an account has subscriptions associated with it. We would like to add PayPal as a payment option for subscriptions, and integrate it appropriately with the existing system in a least-effort manner that is ideally flexible enough to support future expansions of payment processors, such as IAP.\\n## Decision Drivers\\n- Engineering resources\\n- Future effort needed to integrate IAP\\n- Code complexity as it relates to existing Subscription Platform infrastructure\\n","Decision":"## Decision\n- Engineering resources\\n- Future effort needed to integrate IAP\\n- Code complexity as it relates to existing Subscription Platform infrastructure\\nStripe-driven out-of-band invoice processing. This was originally considered to be the slower of the approaches, but using PayPal subscriptions ended up being approximately the same engineering effort. Given additional user experience benefits of the Stripe-driven approach, it is now the recommend implementation path.\\n","Prediction":"## Decision\n- Engineering resources\\n- Future effort needed to integrate IAP\\n- Code complexity as it relates to existing Subscription Platform infrastructure\\nChosen option: \"PayPal + Stripe\", because\\n- It is the most likely to work with Stripe's payment processor API, and it is the most likely to integrate with the Subscription Platform as a least effort payment processor. Stripe's payment processor API is also the most likely to be widely supported in the long term. (This is a factor in the \"future effort\" column, as we will need to add support for IAP payments within the Subscription Platform in the future.)\\n- IAP payments can be very flexible in how they are implemented, and can be used to provide a subscription payment experience that is more similar to how a user would pay for an app. (e.g. the user can pay only once with the same credit card number, or even update the credit card number with a different expiration date, or add an additional card to the account, etc.)\\n- IAP payments are also supported by the payment processors, and can be integrated directly into Stripe's payment processor API. This reduces the friction for the customer in paying for subscriptions and makes it easier to use the standard Stripe payment processor API.\\"}
{"Context":"## Context and Problem Statement\\nShould using directives be inside or outside a namespace?\\n## Decision Drivers\\n* Easy to concatenate files\\n","Decision":"## Decision\n* Easy to concatenate files\\nChosen option: \u201cPlace using directives inside a namespace\u201d, because this allows to concatenate files into a single amalgam. Otherwise the compiler complains: \u201c[CS1529] A using clause must precede all other elements defined in the namespace except extern alias declarations\u201d.\\nIt can be useful in some programming competitions where you have to consume a library in a single source file.\\nThis decision is enforced with a setting in an EditorConfig file:\\ncsharp_using_directive_placement = inside_namespace:suggestion\\n","Prediction":"## Decision\n* Easy to concatenate files\\nChosen option: \"Inside a namespace\", because it's the only option that\\nallows the use of `import` and `export` in the same file.\\n### Positive Consequences\\n* Files inside the namespace are not cluttered with useless code.\\n* Files inside the namespace are easier to understand.\\n* Files inside the namespace are easier to understand and maintain.\\n### Negative Consequences\\n* Files outside the namespace are cluttered with useless code.\\n* Files outside the namespace are harder to understand.\\n* Files outside the namespace are harder to understand and maintain.\\n### Positive Consequences\\n* Files outside the namespace are easier to understand.\\n* Files outside the namespace are easier to understand and maintain.\\n### Negative Consequences\\n* Files outside the namespace are cluttered with useless code.\\n* Files outside the namespace are harder to understand.\\n* Files outside the namespace are harder to understand and maintain.\\n### Positive Consequences\\n* Files outside the namespace are easier to understand and maintain.\\n### Negative Consequences\\n* Files outside the namespace are cluttered with useless code.\\n* Files outside the namespace are harder to understand.\\n* Files outside the namespace are harder to understand and"}
{"Context":"### Context\\nTypescript offers better IDE integration and type checking.\\n### Decision\\nStart using Typescript.\\n","Decision":"## Decision\nStart using Typescript.\\n","Prediction":"## Decision\nStart using Typescript.\\nWe are using Typescript in the frontend of this project.\\n### Decision\nStart using Typescript.\\nWe are using Typescript in the frontend of this project.\\n### Decision\nStart using Typescript.\\nWe are using Typescript in the frontend of this project.\\n### Decision\nStart using Typescript.\\nWe are using Typescript in the frontend of this project.\\n### Decision\nStart using Typescript.\\nWe are using Typescript in the frontend of this project.\\n### Decision\nStart using Typescript.\\nWe are using Typescript in the frontend of this project.\\n### Decision\nStart using Typescript.\\nWe are using Typescript in the frontend of this project.\\n### Decision\nStart using Typescript.\\nWe are using Typescript in the frontend of this project.\\n### Decision\nStart using Typescript.\\nWe are using Typescript in the frontend of this project.\\n### Decision\nStart using Typescript.\\nWe are using Typescript in the frontend of this project.\\n### Decision\nStart using Typescript.\\nWe are using Typescript in the frontend of this project.\\n### Decision\nStart using Typescript.\\nWe are using Typescript"}
{"Context":"## Context\\nWe needed some persistent storage of audit results.\\nWe considered:\\n### Schemaless - DynamoDb\\nThis would be the most obvious choice for a lambda based\\nservice.\\nThe dynamo data model is tables of key value pairs.\\nThe main problem with Dynamo is the limit of I think 4K\\nper value.\\nOne of the things we wanted to do was briefly cache API\\nresponses which could easily breach that 4K limit.\\nWith Dynamo the access control is via IAM which would be\\nrelatively easy to manage and encryption at rest can\\nbe easily configured.\\n### Schemaless - MongoDb\\nMongo was a better fit for our service, saving JSON\\nrepresentations of API responses and resources.\\nThe problem with Mongo is it's not AWS native so we'd\\nhave to provision a server, manage the access controls,\\nmaintenance and availability.\\n### Relational - RDS MySQL \/ PostgreSQL\\nRDS gives us the benefit of running a managed service so\\nAWS are responsible for backups and patching minor\\nversion.\\nRelational databases give us the ability to do on-the-fly\\nanalysis more easily.\\nWe can store JSON as blob data although not ideal.\\nIf we want to re-use the database instance as a shared\\nresource across multiple services RDS is more capable.\\nIt's not unlikely that a future development may require\\ndjango.\\nPostgreSQL seems to be the general direction of travel\\nin GDS and is much more capable for a wider range of\\nuse-cases where we don't know what we're building in the\\nfuture.\\n","Decision":"## Decision\nWhilst none of the options were perfect we decided that\\na PostgreSQL RDS was the best option given the\\ninformation available to give us an infrastructure to\\nsupport multiple tools and services.\\n","Prediction":"## Decision\nWe will use RDS and Mongo as our persistent storage\\nplatform.\\nWe will use RDS as the persistent storage layer for\\nour lambda based services.\\nWe will use RDS as the persistent storage layer for\\nour lambda based services.\\nWe will use RDS as the persistent storage layer for\\nour lambda based services.\\nWe will use RDS as the persistent storage layer for\\nour lambda based services.\\nWe will use RDS as the persistent storage layer for\\nour lambda based services.\\nWe will use RDS as the persistent storage layer for\\nour lambda based services.\\nWe will use RDS as the persistent storage layer for\\nour lambda based services.\\nWe will use RDS as the persistent storage layer for\\nour lambda based services.\\nWe will use RDS as the persistent storage layer for\\nour lambda based services.\\nWe will use RDS as the persistent storage layer for\\nour lambda based services.\\nWe will use RDS as the persistent storage layer for\\nour lambda based services.\\nWe will use RDS as the persistent storage layer for\\nour lambda based services.\\nWe will use RDS as the persistent storage layer for\\nour lambda based services.\\nWe will use R"}
{"Context":"## Context\\nSimulations are generated based on the lambda value provided by the user. It seemed that the simulations\\nwe generated were not realistic enough. It was thought that slightly modifying the lambda value every\\nso often would create more realistic simulations, and this code was implemented.\\n","Decision":"## Decision\nWe will simply base all simulated families on the given lambda and not perturb the lambda.\\n","Prediction":"## Decision\nWe decided to use the `time` library to generate simulations. We also decided to use the `time.time`\\nmethod to create the timestamps.\\nIn order to generate a simulation, the lambda value is passed as an argument to the `time` method.\\nA lambda value of 0 means that the simulation has never run, while a lambda value of 1 means that it has\\nrun at least once. The `time.time` method simply returns the time of the last time that the\\nsimulation ran. We decided to use the `time` method to create the timestamps, rather than using\\nthe `time.time` method to create the simulation itself.\\n`time.time` is a function that takes a `datetime` object as an argument, and returns the time of the\\nlast time that the datetime object was called. The `time` function is defined as:\\n```\\ndef time(self, datetime):\\nreturn datetime.time() - self.start_time\\nd```\\nwhere `self.start_time` is the time, in seconds, that the simulation started.\\n`time.time` is not required to be defined, but it is a good idea to do so in order to ensure\\nthat the `time`"}
{"Context":"## Context\\nWe optionally generate certain solution-level files (e.g., `PackageIcon.png`, `stylecop.json`) and where appropriate or\\nnecessary, we also arrange for these files to show up in project. (This is necessary for `PackageIcon.png`\u2014the file\\nneeds to be in the project for the corresponding `<PackageIcon>` property to work.) However, this causes problems\\nwhen first adding `Endjin.RecommendedPractices` to a project.\\nThe problem is that we don't get the opportunity to generate these files until a build occurs. However, Visual Studio\\nwill attempt to display any files we add to the project immediately after the reference to `Endjin.RecommendedPractices`\\nhas been added, and before any build has occurred. The upshot is that for these kinds of files, Visual Studio shows\\nthem with a big red X, because the physical files they refer to don't exist. Worse, even after the build creates the\\nfiles, the crosses remain, because Visual Studio appears not to update such things when the filesystem changes.\\nThe effect of this was that after adding `Endjin.RecommendedPractices` to a project you would need to build, then\\nunload the solution, and then reload it, before everything looked OK.\\n","Decision":"## Decision\nWe now make all additions of files to a project conditional on the file existing. E.g.:\\nWe are going to require all projects to contain this line at the top of the `csproj`:\\n```xml\\n<ItemGroup Condition=\"($(EndjinDisableCodeAnalysis) != 'true') and (Exists('$(SolutionDir)stylecop.json'))\">\\n<AdditionalFiles Include=\"$(SolutionDir)stylecop.json\" Link=\"stylecop.json\" \/>\\n<\/ItemGroup>\\n```\\nThis is how we make a file link to `stylecop.json` appear in a project. It used to be conditional only on the\\n`EndjinDisableCodeAnalysis` build variable. But now we have an additional `Exists` test.\\n","Prediction":"## Decision\nWe will generate the files we need to generate, but we will not generate them when a build is performed.\\nWe will create a `PackageIcon` property that, when set, will generate `PackageIcon.png` and `PackageIcon.woff`.\\nWe will also add a `RecommendedPractices` property that, when set, will generate the appropriate files.\\nWe will use a `RecommendedPractices` file to configure the `Endjin.RecommendedPractices` property, and we will also\\nadd a `RecommendedPractices` file to configure the `PackageIcon` property.\\nWe will use a `RecommendedPractices` file to configure the `PackageIcon` property, and we will also add a `RecommendedPractices` file to\\nconfigure the `Endjin.RecommendedPractices` property.\\nWe will use a `RecommendedPractices` file to configure the `PackageIcon` property, and we will also add a `RecommendedPractices` file to\\nconfigure the `Endjin.RecommendedPractices` property.\\nWe will use a `RecommendedPractices` file to configure the `PackageIcon` property, and we will also add a `RecommendedPractices` file to\\nconfigure the `Endjin.Re"}
{"Context":"## Context\\nOrigin project consist of multiple packages which are the part of Origin SDK. Complex dependency graph forces us to update dependent packages manually every time dependency has changed.\\n","Decision":"## Decision\nMigrate code base to monorepo structure and use `lerna` for versioning management.\\n","Prediction":"## Decision\nWe will use [Dependency Graph](https:\/\/github.com\/microsoft\/DependencyGraph) to track dependency updates in packages that are part of Origin SDK. Dependency graph will be updated when dependency changes are made in other packages.\\nDependency graph will be updated on every change in package dependency graph. Every time dependency has changed the dependency graph will be updated with new dependency.\\nDependency graph will be updated on every change in package dependency graph. Every time dependency has changed the dependency graph will be updated with new dependency.\\nDependency graph will be updated on every change in package dependency graph. Every time dependency has changed the dependency graph will be updated with new dependency.\\nDependency graph will be updated on every change in package dependency graph. Every time dependency has changed the dependency graph will be updated with new dependency.\\nDependency graph will be updated on every change in package DependencyGraph\\nDependencyGraph\\n[![Dependency Graph](..\/assets\/images\/DependencyGraph.png)](..\/images\/DependencyGraph.png)\\n## Decision\nWe will use [Dependency Graph](https:\/\/github.com\/microsoft\/DependencyGraph) to track dependency updates in packages that are part of Origin SDK. Dependency graph will be updated on every dependency update.\\nDependency graph will be updated on every change in package"}
{"Context":"## Context\\nDuring a delivery for a mother in the mother program, we will need to create one or more new children. Filling in details during delivery and during PNC visits will need switching between them easy.\\nAt the same time, we are also thinking of creating the concept of a family. Here, the individuals of a family will be linked to the head of the household through a relationship.\\nWe need the modeling of a relationship to be a generic structure that can support both these use cases.\\n","Decision":"## Decision\nCreate an option to map relationships between individuals, with relationship being a concept orthogonal to families. Relationships between individuals can be anything (family relationships, or even relationships to ASHA worker etc if required).\\nRelationships will be two-way with different values between the two. We will not build (atleast for now) the ability to automatically deduce transitive relationships.\\n","Prediction":"## Decision\nUse the [Relationships](..\/relationships) model to model the relationship between the child and the parent.\\nThis will allow us to provide a generic structure that can be used for the new concept of families.\\nThis will allow us to create a generic model that can be used to create new children.\\nThis will allow us to create a generic model that can be used to create new parents.\\nThis will allow us to provide a generic model that can be used to create new children.\\nThis will allow us to provide a generic model that can be used to create new parents.\\nThis will allow us to create a generic model that can be used to create new children.\\nThis will allow us to create a generic model that can be used to create new parents.\\nThis will allow us to provide a generic model that can be used to create new children.\\nThis will allow us to create a generic model that can be used to create new parents.\\nThis will allow us to create a generic model that can be used to create new children.\\nThis will allow us to create a generic model that can be used to create new parents.\\nThis will allow us to create a generic model that can be used to create new parents.\\nThis"}
{"Context":"## Context\\nOne of the biggest challenges this project faces is to proof that the\\nimplementations of the specifications are correct, much like we strive to\\nformaly verify our alogrithms and protocols we should work towards high\\nconfidence about the correctness of our program code. One of those is the core\\nof Tendermint - Consensus - which currently resides in the `consensus` package.\\nOver time there has been high friction making changes to the package due to the\\nalgorithm being scattered in a side-effectful container (the current\\n`ConsensusState`). In order to test the algorithm a large object-graph needs to\\nbe set up and even than the non-deterministic parts of the container makes will\\nprevent high certainty. Where ideally we have a 1-to-1 representation of the\\n[spec](https:\/\/github.com\/tendermint\/spec), ready and easy to test for domain\\nexperts.\\nAddresses:\\n- [#1495](https:\/\/github.com\/tendermint\/tendermint\/issues\/1495)\\n- [#1692](https:\/\/github.com\/tendermint\/tendermint\/issues\/1692)\\n","Decision":"## Decision\nTo remedy these issues we plan a gradual, non-invasive refactoring of the\\n`consensus` package. Starting of by isolating the consensus alogrithm into\\na pure function and a finite state machine to address the most pressuring issue\\nof lack of confidence. Doing so while leaving the rest of the package in tact\\nand have follow-up optional changes to improve the sepration of concerns.\\n### Implementation changes\\nThe core of Consensus can be modelled as a function with clear defined inputs:\\n* `State` - data container for current round, height, etc.\\n* `Event`- significant events in the network\\nproducing clear outputs;\\n* `State` - updated input\\n* `Message` - signal what actions to perform\\n```go\\ntype Event int\\nconst (\\nEventUnknown Event = iota\\nEventProposal\\nMajority23PrevotesBlock\\nMajority23PrecommitBlock\\nMajority23PrevotesAny\\nMajority23PrecommitAny\\nTimeoutNewRound\\nTimeoutPropose\\nTimeoutPrevotes\\nTimeoutPrecommit\\n)\\ntype Message int\\nconst (\\nMeesageUnknown Message = iota\\nMessageProposal\\nMessageVotes\\nMessageDecision\\n)\\ntype State struct {\\nheight      uint64\\nround       uint64\\nstep        uint64\\nlockedValue interface{} \/\/ TODO: Define proper type.\\nlockedRound interface{} \/\/ TODO: Define proper type.\\nvalidValue  interface{} \/\/ TODO: Define proper type.\\nvalidRound  interface{} \/\/ TODO: Define proper type.\\n\/\/ From the original notes: valid(v)\\nvalid       interface{} \/\/ TODO: Define proper type.\\n\/\/ From the original notes: proposer(h, r)\\nproposer    interface{} \/\/ TODO: Define proper type.\\n}\\nfunc Consensus(Event, State) (State, Message) {\\n\/\/ Consolidate implementation.\\n}\\n```\\nTracking of relevant information to feed `Event` into the function and act on\\nthe output is left to the `ConsensusExecutor` (formerly `ConsensusState`).\\nBenefits for testing surfacing nicely as testing for a sequence of events\\nagainst algorithm could be as simple as the following example:\\n``` go\\nfunc TestConsensusXXX(t *testing.T) {\\ntype expected struct {\\nmessage Message\\nstate   State\\n}\\n\/\/ Setup order of events, initial state and expectation.\\nvar (\\nevents = []struct {\\nevent Event\\nwant  expected\\n}{\\n\/\/ ...\\n}\\nstate = State{\\n\/\/ ...\\n}\\n)\\nfor _, e := range events {\\nsate, msg = Consensus(e.event, state)\\n\/\/ Test message expectation.\\nif msg != e.want.message {\\nt.Fatalf(\"have %v, want %v\", msg, e.want.message)\\n}\\n\/\/ Test state expectation.\\nif !reflect.DeepEqual(state, e.want.state) {\\nt.Fatalf(\"have %v, want %v\", state, e.want.state)\\n}\\n}\\n}\\n```\\nHeight           int64\\nRound            int\\nBlockID          BlockID\\n}\\ntype TriggerTimeout struct {\\nHeight           int64\\nRound            int\\nDuration         Duration\\n}\\ntype RoundStep int\\nconst (\\nRoundStepUnknown RoundStep = iota\\nRoundStepPropose\\nRoundStepPrevote\\nRoundStepPrecommit\\nRoundStepCommit\\n)\\ntype State struct {\\nHeight           int64\\nRound            int\\nStep             RoundStep\\nLockedValue      BlockID\\nLockedRound      int\\nValidValue       BlockID\\nValidRound       int\\nValidatorId      int\\nValidatorSetSize int\\n}\\nfunc proposer(height int64, round int) int {}\\nfunc getValue() BlockID {}\\nfunc Consensus(event Event, state State) (State, Message, TriggerTimeout) {\\nmsg = nil\\ntimeout = nil\\nswitch event := event.(type) {\\ncase EventNewHeight:\\nif event.Height > state.Height {\\nstate.Height = event.Height\\nstate.Round = -1\\nstate.Step = RoundStepPropose\\nstate.LockedValue = nil\\nstate.LockedRound = -1\\nstate.ValidValue = nil\\nstate.ValidRound = -1\\nstate.ValidatorId = event.ValidatorId\\n}\\nreturn state, msg, timeout\\ncase EventNewRound:\\nif event.Height == state.Height and event.Round > state.Round {\\nstate.Round = eventRound\\nstate.Step = RoundStepPropose\\nif proposer(state.Height, state.Round) == state.ValidatorId {\\nproposal = state.ValidValue\\nif proposal == nil {\\nproposal = getValue()\\n}\\nmsg =  MessageProposal { state.Height, state.Round, proposal, state.ValidRound }\\n}\\ntimeout = TriggerTimeout { state.Height, state.Round, timeoutPropose(state.Round) }\\n}\\nreturn state, msg, timeout\\ncase EventProposal:\\nif event.Height == state.Height and event.Round == state.Round and\\nevent.Sender == proposal(state.Height, state.Round) and state.Step == RoundStepPropose {\\nif event.POLRound >= state.LockedRound or event.BlockID == state.BlockID or state.LockedRound == -1 {\\nmsg = MessageVote { state.Height, state.Round, event.BlockID, Prevote }\\n}\\nstate.Step = RoundStepPrevote\\n}\\nreturn state, msg, timeout\\ncase TimeoutPropose:\\nif event.Height == state.Height and event.Round == state.Round and state.Step == RoundStepPropose {\\nmsg = MessageVote { state.Height, state.Round, nil, Prevote }\\nstate.Step = RoundStepPrevote\\n}\\nreturn state, msg, timeout\\ncase Majority23PrevotesBlock:\\nif event.Height == state.Height and event.Round == state.Round and state.Step >= RoundStepPrevote and event.Round > state.ValidRound {\\nstate.ValidRound = event.Round\\nstate.ValidValue = event.BlockID\\nif state.Step == RoundStepPrevote {\\nstate.LockedRound = event.Round\\nstate.LockedValue = event.BlockID\\nmsg = MessageVote { state.Height, state.Round, event.BlockID, Precommit }\\nstate.Step = RoundStepPrecommit\\n}\\n}\\nreturn state, msg, timeout\\ncase Majority23PrevotesAny:\\nif event.Height == state.Height and event.Round == state.Round and state.Step == RoundStepPrevote {\\ntimeout = TriggerTimeout { state.Height, state.Round, timeoutPrevote(state.Round) }\\n}\\nreturn state, msg, timeout\\ncase TimeoutPrevote:\\nif event.Height == state.Height and event.Round == state.Round and state.Step == RoundStepPrevote {\\nmsg = MessageVote { state.Height, state.Round, nil, Precommit }\\nstate.Step = RoundStepPrecommit\\n}\\nreturn state, msg, timeout\\ncase Majority23PrecommitBlock:\\nif event.Height == state.Height {\\nstate.Step = RoundStepCommit\\nstate.LockedValue = event.BlockID\\n}\\nreturn state, msg, timeout\\ncase Majority23PrecommitAny:\\nif event.Height == state.Height and event.Round == state.Round {\\ntimeout = TriggerTimeout { state.Height, state.Round, timeoutPrecommit(state.Round) }\\n}\\nreturn state, msg, timeout\\ncase TimeoutPrecommit:\\nif event.Height == state.Height and event.Round == state.Round {\\nstate.Round = state.Round + 1\\n}\\nreturn state, msg, timeout\\n}\\n}\\nfunc ConsensusExecutor() {\\nproposal = nil\\nvotes = HeightVoteSet { Height: 1 }\\nstate = State {\\nHeight:       1\\nRound:        0\\nStep:         RoundStepPropose\\nLockedValue:  nil\\nLockedRound:  -1\\nValidValue:   nil\\nValidRound:   -1\\n}\\nevent = EventNewHeight {1, id}\\nstate, msg, timeout = Consensus(event, state)\\nevent = EventNewRound {state.Height, 0}\\nstate, msg, timeout = Consensus(event, state)\\nif msg != nil {\\nsend msg\\n}\\nif timeout != nil {\\ntrigger timeout\\n}\\nfor {\\nselect {\\ncase message := <- msgCh:\\nswitch msg := message.(type) {\\ncase MessageProposal:\\ncase MessageVote:\\nif msg.Height == state.Height {\\nnewVote = votes.AddVote(msg)\\nif newVote {\\nswitch msg.Type {\\ncase Prevote:\\nprevotes = votes.Prevotes(msg.Round)\\nif prevotes.WeakCertificate() and msg.Round > state.Round {\\nevent = EventNewRound { msg.Height, msg.Round }\\nstate, msg, timeout = Consensus(event, state)\\nstate = handleStateChange(state, msg, timeout)\\n}\\nif blockID, ok = prevotes.TwoThirdsMajority(); ok and blockID != nil {\\nif msg.Round == state.Round and hasBlock(blockID) {\\nevent = Majority23PrevotesBlock { msg.Height, msg.Round, blockID }\\nstate, msg, timeout = Consensus(event, state)\\nstate = handleStateChange(state, msg, timeout)\\n}\\nif proposal != nil and proposal.POLRound == msg.Round and hasBlock(blockID) {\\nevent = EventProposal {\\nHeight: state.Height\\nRound:  state.Round\\nBlockID: blockID\\nPOLRound: proposal.POLRound\\nSender: message.Sender\\n}\\nstate, msg, timeout = Consensus(event, state)\\nstate = handleStateChange(state, msg, timeout)\\n}\\n}\\nif prevotes.HasTwoThirdsAny() and msg.Round == state.Round {\\nevent = Majority23PrevotesAny { msg.Height, msg.Round, blockID }\\nstate, msg, timeout = Consensus(event, state)\\nstate = handleStateChange(state, msg, timeout)\\n}\\ncase Precommit:\\n}\\n}\\n}\\ncase timeout := <- timeoutCh:\\ncase block := <- blockCh:\\n}\\n}\\n}\\nfunc handleStateChange(state, msg, timeout) State {\\nif state.Step == Commit {\\nstate = ExecuteBlock(state.LockedValue)\\n}\\nif msg != nil {\\nsend msg\\n}\\nif timeout != nil {\\ntrigger timeout\\n}\\n}\\n```\\n### Implementation roadmap\\n* implement proposed implementation\\n* replace currently scattered calls in `ConsensusState` with calls to the new\\n`Consensus` function\\n* rename `ConsensusState` to `ConsensusExecutor` to avoid confusion\\n* propose design for improved separation and clear information flow between\\n`ConsensusExecutor` and `ConsensusReactor`\\n","Prediction":"## Decision\nUse the [consensus container](https:\/\/github.com\/tendermint\/tendermint\/blob\/master\/docs\/concepts\/concurrency\/consensus\/container.md#consensus-container) to create a container which will be used to host the\\nconsensus state.\\nThis container will expose the `ConsensusState` and `ConsensusStateProto` classes.\\nThis container will be used to host the state in the `ConsensusStateProto` class.\\nThis container will be used to host the state and the consensus state.\\nThis container will be used to host the state and the consensus state.\\nThis container will be used to host the state and the consensus state.\\nThis container will be used to host the state and the consensus state.\\nThis container will be used to host the state and the consensus state.\\nThis container will be used to host the state and the consensus state.\\nThis container will be used to host the state and the consensus state.\\nThis container will be used to host the state and the consensus state.\\nThis container will be used to host the state and the consensus state.\\nThis container will be used to host the state and the consensus state.\\nThis container will be used to host the state and the consensus"}
{"Context":"## Context\\nThe data warehouse was intended to house data for all CommCare HQ reports.\\nThe warehouse would replace Elasticsearch in almost all contexts that it is currently used.\\nThe migration began in 2017 with the Application Status report and the effort\\nto move the report to the warehouse and ensure it is stable, performs well and\\nprovides the same features as the ES-backed reports was much higher than\\nanticipated.\\n","Decision":"## Decision\nTo reduce our infrastructure dependencies and focus our efforts on existing databases,\\nwe have decided to remove the warehouse and stop any efforts to iterate on it.\\nThis decision is not because we believe that the warehouse is a worse implementation than Elasticsearch.\\nThis decision is because we believe that with our current priorities, we will\\nnot be able to spend the appropriate amount of time to make the warehouse a\\nrobust solution for generic reports in the near future.\\nBecause no current reports are backed by the warehouse, it is an important time\\nto reconsider our approach and decide on what will be appropriate long term.\\nWhen there are more dedicated resources for generic reports, we believe that\\na warehouse-style approach should be considered when implementing.\\n","Prediction":"## Decision\nWe will migrate the Application Status report to the data warehouse.\\nThe warehouse will be used to store the data for all CommCare reports.\\nThe warehouse will be used as the data source for the other reports that will\\nuse the warehouse.\\nThe warehouse will be used as a data source for the Data Mart and other\\nreports that will access the data in the warehouse.\\nWe will migrate the warehouse data to the data mart and other reports.\\nWe will migrate the warehouse data to the data mart and other reports.\\nWe will migrate the warehouse data to the data mart and other reports.\\nWe will migrate the warehouse data to the data mart and other reports.\\nWe will migrate the warehouse data to the data mart and other reports.\\nWe will migrate the warehouse data to the data mart and other reports.\\nWe will migrate the warehouse data to the data mart and other reports.\\nWe will migrate the warehouse data to the data mart and other reports.\\nWe will migrate the warehouse data to the data mart and other reports.\\nWe will migrate the warehouse data to the data mart and other reports.\\nWe will migrate the warehouse data to the data mart and other reports.\\nWe will migrate the warehouse data to the data mart and other reports"}
{"Context":"## Context\\nI started off using `yarn` workspaces because it was convenient for running scripts inside packages using `--cwd`. But it has no `audit fix` and deployments started failing because of missing packages.\\n```shell\\n2019-08-09T07:25:06+01:00 code: 'MODULE_NOT_FOUND',\\n2019-08-09T07:25:06+01:00 at Module.require (internal\/modules\/cjs\/loader.js:683:19) {\\n2019-08-09T07:25:06+01:00 at Module.load (internal\/modules\/cjs\/loader.js:643:32)\\n```\\nI tried [changing node engines][node-issue] which failed. I switched to `pnpm` because it had a reputation for solving dependency issues but I ran into problems with the way Hapi and its dependencies get [dynamically imported][hapi-issue].\\nIn order to use `pnpm` I had to use Docker deployments because [Clever cloud][clever-cloud] only support `npm` and `yarn` package managers. I could not get `pnpm` to [work with Docker][docker-issue] (recursive installs kept failing) but `npm` worked fine. I decided to keep Docker for learning and portablity purposes.\\n","Decision":"## Decision\nIn the context of deployments failing because of missing packages. And facing the concern of wanting to have this projects' dependencies reliably installed in production, I've adopted `npm` as the default package manager. I accept that I will have to write more scripts to maintain the monorepo structure. Docker is no longer necessary but is convenient for portability and learning.\\n","Prediction":"## Decision\nI will use [Docker][docker] for deploying packages.\\nI will use [PNPM][pnpm] for running scripts.\\nI will use [Clever Cloud][clever-cloud] to deploy scripts.\\nI will use [Hapi][hapi-issue] to run the application.\\nI will use [Node.js][node-issue] to deploy the application.\\nI will use [Docker][docker] for building and deploying the application.\\nI will use [Docker Compose][docker-compose] for deploying the application.\\nI will use [Docker Swarm][docker-swarm] for running the application on multiple nodes.\\nI will use [Docker Swarm Helm Charts][docker-helm-charts] to deploy the application on multiple nodes.\\nI will use [Docker Registry][docker-registry] to host and push code to the registry.\\nI will use [Docker Vault][docker-vault] to store secrets.\\nI will use [Docker Compose Vault][docker-compose-vault] to store secrets.\\nI will use [Docker Helm Charts Vault][docker-helm-vault] to store secrets.\\nI will use [Docker Secrets][docker-secrets]"}
{"Context":"## Context and Problem Statement\\nIn explorer, most of the world assets are resolved using the following elements:\\n#### Base url\\nThe content server URL. This will be always the first part of the resolved URL for downloading the asset.\\nExamples:\\n`https:\/\/peer.decentraland.org\/contents\/`\\n#### Local asset path\\nThe local relative path pointing to the folder in which the asset was originally stored SDK side.\\nThis relative path is also used internally to resolve references between a GLTF file and its textures. Internally, the GLTF file will store those relative paths, so we have to map them so a valid hash at import time. More on this will be explained later.\\nExamples can include: `asset.gltf` `assets\/cube.glb` `assets\/cube.png` etc.\\n#### Asset Hash\\nThe hash used for composing the full url. This hash is usually assigned by the catalyst content server, and is guaranteed to be unique per individual asset.\\nAny full asset url may look like e.g.: `https:\/\/peer.decentraland.org\/contents\/QmHash`\\n### Content mappings and scene flow\\nWhen a new scene is created, a message is sent from Kernel to Renderer specifying the data that's going to be needed to assemble a full asset url from a single local asset path. The data looks roughly like this:\\n```\\n{\\n\/\/ Here the value can vary depending on the connected realm\\n\"baseUrl\":\"https:\/\/peer.decentraland.org\/contents\",\\n\"contents\":\\n{\\n\"asset.gltf\":\"QmHash1\",\\n\"asset.png\":\"QmHash2\",\\n\"weapons\/firearm1.gltf\":\"QmHash3\"\\n}\\n}\\n```\\nThis data is stored in a `ContentProvider` object [(see code).](https:\/\/github.com\/decentraland\/explorer\/blob\/b1545f8ae65d8e1198caeb18086fcde1d0be20f3\/unity-client\/Assets\/Scripts\/MainScripts\/DCL\/Controllers\/Scene\/ContentProvider\/ContentProvider.cs#L9)\\nThen, further down the line, a component can be sent with a payload that looks like this:\\n```\\n{\\n\"src\":\"asset.gltf\"\\n}\\n```\\nIn runtime we can receive this payload for any scene object. In order to download the object, we must resolve the full URL for it, like this:\\n```\\nbaseUrl + resolved hash\\n```\\nThe resolved hash is obtained querying a dictionary with the local asset path.\\nThis dictionary is scoped to the scene. Meaning that any scene will have a specific dictionary of content mapping pairs and a determined `baseUrl`. The mapping pairs and the baseUrl are fetched from Kernel using our catalyst scene endpoints and then passed over to explorer's renderer.\\n![resources\/adr-18-content-mappings-flow-for-explorer-and-builder-in-world\/fig-mappings-flow.svg](resources\/adr-18-content-mappings-flow-for-explorer-and-builder-in-world\/fig-mappings-flow.svg)\\n<!--\\n```sequence\\nCatalyst->Kernel: Kernel picks realm url\\nCatalyst->Kernel: Kernel fetches mapping pairs\\nKernel->Renderer: send LoadParcelScenes message\\nwith baseUrl and content mappings\\nKernel->Renderer: create any sdk component\\nwith src payload\\nRenderer->Renderer: Use baseUrl and mappings\\nto solve full asset url.\\n```\\n!-->\\nThis approach works fine for most of the cases. However, some issues can be observed:\\n1) For big scenes, a pretty big content mappings payload have to be sent over to Renderer. This can produce performance hiccups.\\n2) For content coming from dynamic and user made sources (i.e.: builder or builder in world) the chance of local asset path collisions is very high.\\n3) For the builder in-world feature we need to update the mappings in real time, and currently we don't have a nice mechanism of doing that without sending the monolithic mappings message scoped to the scene. Also, see (2).\\nIn this document we are going to review the decided approach for dealing with those issues in short and long term stages.\\n","Decision":"## Decision\nThe outcome is to advance on the Stage 1 of the **robust asset catalog approach**.\\nWe argue that this approach has the best return of investment, because:\\n* Stage 1 task size is about the same compared with the other options discussed.\\n* Stage 1 gives a clear roadmap on improving the system scalability while keeping tabs on simplicity for Stage 2.\\n* Reaching Stage 2 will solve all the presented issues.\\n","Prediction":"## Decision\nWe will use a custom ContentProvider to resolve the full asset path for downloaded assets. This contentProvider will be used to resolve the full asset path for downloaded assets in the global scene and in the local scene. We will also have a custom ContentProvider to resolve the full asset path for downloaded assets in the local scene, in the global scene and in the builder in-world scenes. This contentProvider will resolve the full asset path for downloaded assets in the global scene and in the local scene, and will resolve the full asset path for the downloaded assets in the local scene in the global scene when the downloaded assets are used in a global scene. The content provider for downloaded assets in the local scene will resolve the full asset path for downloaded assets in the local scene and will resolve the full asset path for downloaded assets in the builder in-world scene. The content provider for downloaded assets in the global scene will resolve the full asset path for downloaded assets in the global scene and will resolve the full asset path for downloaded assets in the builder in-world scene. The content provider for downloaded assets in the builder in-world will resolve the full asset path for downloaded assets in the global scene and will resolve the full asset path for downloaded assets in the builder in-world scene. The content provider for"}
{"Context":"## Context and Problem Statement\\nSections of the Koshvani platfrom to be considered in scope for scoping and analysing the data.\\n- Expenditure\\n- Receipts\\n","Decision":"## Decision\nExplore both sections to identify sub-sections from which data needs to be extracted and analysed.\\n","Prediction":"## Decision\nChosen option: \"Accounts\", because the data structure is more familiar to the team and the data is already in the database, so it will be easier to understand and use the data in the future. It will also be easier to implement the suggested API.\\n### Positive Consequences <!-- optional -->\\n* The existing data structure will be easier to use and understand.\\n### Negative Consequences <!-- optional -->\\n* The existing data structure will be harder to use and understand.\\n* It will be harder to understand the data structure of the database.\\n* It will be harder to implement the suggested API.\\n* It will be harder to understand the API.\\n### Positive Consequences <!-- optional -->\\n* The data structure will be easier to use and understand.\\n* It will be easier to understand the API.\\n* It will be easier to understand the data structure of the database.\\n* It will be harder to implement the suggested API.\\n* It will be harder to understand the API.\\n* It will be harder to understand the data structure of the database.\\n* It will be harder to implement the suggested API.\\n* It will be harder to understand the API.\\n* It will be harder to understand"}
{"Context":"## Context and Problem Statement\\nPeople want to include time taken for voice overs to estimate the time of the rough cut.\\n## Decision Drivers <!-- optional -->\\n* Ease of implementation\\n* Accuracy of time estimated\\n* Extensibility\\n","Decision":"## Decision\n* Ease of implementation\\n* Accuracy of time estimated\\n* Extensibility\\nChosen option: 1, because it's the simplest implementation and a standardised way of calculating voice over estimations within the company.\\n### Positive Consequences <!-- optional -->\\n* A fairly easy implementation.\\n### Negative consequences <!-- optional -->\\n* Less accurate than other options.\\n* Would require us to change the icon and description of the Voice over, such that it's features are not misunderstood. (This was a point raised in [UX investigations: how to better signpost Voice over\/link](https:\/\/github.com\/bbc\/digital-paper-edit-client\/issues\/63))\\n","Prediction":"## Decision\n* Ease of implementation\\n* Accuracy of time estimated\\n* Extensibility\\nChosen option: \"Custom time estimation\", because it is the only option that allows us to estimate the time of the rough cut.\\n### Positive Consequences <!-- optional -->\\n* Time estimation is possible and accurate.\\n* We can use the time estimation in the rough cut.\\n* We can use the time estimation in the post-production.\\n### Negative Consequences <!-- optional -->\\n* It is not possible to use the time estimation for the final cut.\\n* We cannot use the time estimation in the rough cut.\\n* We cannot use the time estimation in the post-production.\\n* We cannot use the time estimation in the rough cut for the final cut.\\n* It is not possible to use the time estimation in the post-production.\\n* It is not possible for the final cut.\\n* It is not possible for the post-production.\\n* It is not possible in the rough cut.\\n* It is not possible for the final cut.\\n* It is not possible for the post-production.\\n* It is not possible in the rough cut.\\n* It is not possible for the final cut.\\n* It is not"}
{"Context":"## Context\\nPreviously we've kept Terraform code in a separate Github repository (nulterra) and this has had the effect of making changes more cumbersome.\\n","Decision":"## Decision\nWe've decided to keep Meadow Terraform code inside the Meadow repository in a root directory called `terraform`.\\n","Prediction":"## Decision\nWe will move all Terraform code to a single repository (nulterra) and will use that repository for all future work. This will allow changes to be made quickly and efficiently.\\nWe will also move all Terraform configuration files into the same location. This will allow for easier configuration and configuration sharing.\\nWe will also move all Terraform code files into a single location. This will allow for easier configuration and configuration sharing.\\nWe will also move all Terraform configuration files into a single location. This will allow for easier configuration and configuration sharing.\\nWe will also move all Terraform configuration files into a single location. This will allow for easier configuration and configuration sharing.\\nWe will also move all Terraform configuration files into a single location. This will allow for easier configuration and configuration sharing.\\nWe will also move all Terraform configuration files into a single location. This will allow for easier configuration and configuration sharing.\\nWe will also move all Terraform configuration files into a single location. This will allow for easier configuration and configuration sharing.\\nWe will also move all Terraform configuration files into a single location. This will allow for easier configuration and configuration sharing.\\nWe will also move all Terraform configuration files into a single location. This"}
{"Context":"## Context and Problem Statement\\nWe need a way to send emails to users for several workflows.\\n","Decision":"## Decision\nChosen option: SendGrid, because it is SaaS and has generous free tier. Integration is easy.\\n### Positive Consequences <!-- optional -->\\n* Fast time to develop\\n### Negative Consequences\\n* We need to think about setting up development environment\\n","Prediction":"## Decision\nWe will use [Postmark](https:\/\/postmarkapp.com\/).\\nPostmark is a web-based email service that allows you to send emails from a web interface. It's easy to use and has a rich API to handle all the email sending needs of developers.\\nPostmark also has an excellent support team that is available 24\/7 to help you. They are very responsive and very knowledgeable.\\nPostmark is also free to use for personal email sending. There are paid plans available for organizations that need more features.\\nPostmark has been used by many companies in the past and is well known in the web development industry.\\nPostmark has been used by [Rackspace](https:\/\/www.rackspace.com\/products\/marketing\/email-services\/) for many years.\\nPostmark is also an official Google App Engine Partner and a Google Cloud Partner. This means that it's easy to use and integrates easily with other services.\\nPostmark also works with most major email clients, including Gmail, Outlook, and Apple Mail.\\nPostmark is also available on the Google Play Store.\\nPostmark is a Google App Engine Cloud Service, which means that it's easy to use and integrates easily with other services.\\nPostmark is"}
{"Context":"## Context\\nOff-chain data is accessible via REST API. Currently all system components uses direct REST calls in various places making unit test hard.\\n","Decision":"## Decision\nCreate client library and use it as dependency in components that want to read the off-chain data. Include the mocked version of the service so unit-tests does not have to rely on the implementation.\\n","Prediction":"## Decision\nUse [Swagger](https:\/\/swagger.io\/), which is a specification for describing and validating REST APIs.\\nSwagger is a tool for annotating and validating APIs. It is used by many major open-source projects, including:\\n- [Swagger UI](https:\/\/swagger.io\/), which is the official Swagger UI plugin for Visual Studio Code\\n- [Swagger Cloud](https:\/\/swagger.io\/), which is a hosted Swagger service\\n- [Swagger Docs](https:\/\/docs.swagger.io\/), which is the official Swagger Docs service\\n- [Swagger Test](https:\/\/swagger.io\/), which is the official Swagger Test service\\n- [Swagger Client](https:\/\/github.com\/swagger-api\/swagger-client), which is the client library for Swagger\\n- [Swagger Generator](https:\/\/github.com\/swagger-generator\/swagger-generator), which is the generator of Swagger docs from Swagger specification\\n- [Swagger Schema](https:\/\/github.com\/swagger-schema\/swagger-schema), which is the Swagger schema for the API\\n- [Swagger C#](https:\/\/github.com\/swagger-csharp\/swagger-csharp), which is the C# client library for Swagger\\n- [Swagger Server](https:\/\/github.com\/swagger-server\/swagger-server), which is the server library for Swagger\\n- [Swagger Client for Node.js]("}
{"Context":"## Context\\nDevelopers need some tool for DB versioning and safe migrations.\\n","Decision":"## Decision\nWe will introduce dockerized flyway setup for local development.\\nMigration scripts will also be provided.\\n","Prediction":"## Decision\nWe will use PostgreSQL versioning. It is the most widely used and highest used database in the world. It has a mature feature set and is well supported.\\nWe will use the [PostgreSQL 12 feature set](https:\/\/www.postgresql.org\/docs\/12\/FEATURES.html#postgresql-12) and only support the latest supported version.\\nWe will use versioning on the primary key field.\\nWe will use `pg_upgrade` when migrating from a previous version of PostgreSQL to the new version.\\nWe will use `pg_upgrade_all` to migrate a database from a previous version to the new version.\\nWe will use `pg_drop` to drop a database.\\nWe will use `pg_backups` to create a backup of a database.\\nWe will use `pg_repair` to repair a database.\\nWe will use `pg_upgrade` to upgrade a database.\\nWe will use `pg_drop_all` to drop all tables in a database.\\nWe will use `pg_restore` to restore a database.\\nWe will use `pg_commit` to commit changes to a database.\\nWe will use `pg_status` to list a database's current state.\\nWe will use `pg_stat_activity"}
{"Context":"## Context and Problem Statement\\nGLAM requires authentication to protect sensitive metrics, make some metrics and\\nviews visible to non-Mozillians, and in the future to potentially add\\nuser-oriented features. Currently we handle the authentication in the app layer,\\nalbeit with some issues. This ADR attempts to resolve these issues.\\n## Decision Drivers\\n- Authentication should work without issue, including infinite redirects or\\nconflicts with enhanced tracking protection and 3rd party cookies.\\n- Authentication should allow some areas of the site to be public via URL\\nmatching rules.\\n- Authentication should pass information about the authenticated user to the\\nbackend for future use-cases.\\n","Decision":"## Decision\n- Authentication should work without issue, including infinite redirects or\\nconflicts with enhanced tracking protection and 3rd party cookies.\\n- Authentication should allow some areas of the site to be public via URL\\nmatching rules.\\n- Authentication should pass information about the authenticated user to the\\nbackend for future use-cases.\\nIn the GLAM meeting, nginx was chosen as the preferred option for the following\\nreasons:\\n- It removes code complexity and maintenance from the app layer.\\n- It would remove a necessary security review by using something that has\\nalready been reviewed.\\n- It offers the quickest path to fix the authentication issues we are facing.\\n- In the future it satifies the requirement of offering user-facing features.\\n- In the future it satifies the requirement of optional non-authenticated\\nendpoints.\\n","Prediction":"## Decision\n- Authentication should work without issue, including infinite redirects or\\nconflicts with enhanced tracking protection and 3rd party cookies.\\n- Authentication should allow some areas of the site to be public via URL\\nmatching rules.\\n- Authentication should pass information about the authenticated user to the\\nbackend for future use-cases.\\nWe will use the [OpenIDConnect](https:\/\/openid.net\/developers\/identity\/identity-provider\/) API\\nfor authentication. This API provides a standard authentication protocol that\\ncan be used across all of our services. It allows for the use of\\nauthentication credentials (anonymous and non-anonymous) and can handle\\nnon-standard authentication schemes.\\nWe will use the [Google OpenID Connect](https:\/\/openid.net\/specs\/openid-connect-core-1_0.html) API for\\nauthentication. This API provides a standard authentication protocol\\nthat can be used across all of our services. It allows for the use of\\nauthentication credentials (anonymous and non-anonymous) and can handle\\nnon-standard authentication schemes.\\nWe will use the [Microsoft OpenID Connect](https:\/\/openid.net\/specs\/microsoft-openid-connect-1_0.html) API for\\nauthentication. This API provides a standard authentication protocol\\nthat can be"}
{"Context":"## Context and Problem Statement\\nUniFFI currently manages object instances using the `HandleMap` struct in the ffi-support crate.\\nThis means that external consumers of UniFFI-wrapped interfaces never see\\nany pointers to structs - instead, they get what is (roughly) an index into\\nan array, with the struct being stored in (and owned by) that array.\\nThis has a number of safety characteristics which are particularly important for\\nhand-written FFI interfaces, but it does cause some issues in evolving UniFFI in\\ndirections we consider important. In addition to the slight performance overhead,\\nthe use of `HandleMap`s makes it difficult to support:\\n* Passing object instances as arguments ([#40](https:\/\/github.com\/mozilla\/uniffi-rs\/issues\/40)).\\nGetting objects out of a `HandleMap` involves a closure, so accepting multiple\\nobject-typed arguments would involve code-generating nested closures.\\n* Returning object instances from functions ([#197](https:\/\/github.com\/mozilla\/uniffi-rs\/issues\/197)).\\nDoes the returned object already exist in the handlemap? If so, what is its handle?\\nHow will we manage the lifetime of multiple references to the object?\\nThese restrictions mean that UniFFI's `Object` type is currently only suitable\\nas the `self` argument for method calls, and is forbidden in argument position,\\nas record fields, etc.\\nThis ADR considers ways to evolve the handling of object instances and their\\nlifetimes, so that references to structs can be used more widely than currently allowed.\\n## Decision Drivers\\n* We desire the ability to have more flexible lifetimes for object interfaces, so\\nthey can be stored in dictionaries or other interfaces, and be returned by\\nfunctions or methods other than constructors.\\n* We would like to keep the UniFFI implementation as simple as possible while\\nproviding a suitable degree of safety - in particular, a promise that it\\nshould be impossible to misuse the generated bindings in a way that triggers\\nRust's \"undefined behavior\" or otherwise defeats Rust's safety\\ncharacteristics and ownership model (and in particular, avoiding things like\\nuse-after-free issues).\\n* We would like to keep the overhead of UniFFI as small as possible so that it\\nis a viable solution to more use-cases.\\n","Decision":"## Decision\n* We desire the ability to have more flexible lifetimes for object interfaces, so\\nthey can be stored in dictionaries or other interfaces, and be returned by\\nfunctions or methods other than constructors.\\n* We would like to keep the UniFFI implementation as simple as possible while\\nproviding a suitable degree of safety - in particular, a promise that it\\nshould be impossible to misuse the generated bindings in a way that triggers\\nRust's \"undefined behavior\" or otherwise defeats Rust's safety\\ncharacteristics and ownership model (and in particular, avoiding things like\\nuse-after-free issues).\\n* We would like to keep the overhead of UniFFI as small as possible so that it\\nis a viable solution to more use-cases.\\nChosen option:\\n* **[Option 2] We replace `HandleMap<T>` with raw pointers to Rust's builtin `Arc<T>`**\\nThis decision is taken because:\\n* We believe the additional safety offered by `HandleMap`s is far less\\nimportant for this use-case, because the code using these pointers is\\ngenerated instead of hand-written.\\n* Correctly implementing better lifetime management in a thread-safe way is not\\ntrivial and subtle errors there would defeat all the safety mechanisms the\\n`HandleMap`s offer. Ultimately we'd just end up reimplementing `Arc<>` anyway,\\nand the one in the stdlib is far more likely to be correct.\\n* There are usability and familiarity benefits to using the stdlib `Arc<>` rather\\nthan a special-purpose container like `triomphe::Arc`, and the way we currently\\ndo codegen means we're unlikely to notice any potential performance improvements\\nfrom using a more specialized type.\\n### Positive Consequences\\n* There will be less overhead in our generated code - both performance overhead\\nand cognitive overload - it will be much easier to rationalize about how\\nthe generated code actually works and performs.\\n### Negative Consequences\\n* Errors in our generated code might cause pointer misuse and lead to \"use\\nafter free\" type issues.\\n* Misuse of generated APIs may be able to create reference cycles between Rust\\nobjects that cannot be deallocated, and consumers coming from a garbage-collected\\nlanguage may assume that such cycles will be collected.\\n","Prediction":"## Decision\n* We desire the ability to have more flexible lifetimes for object interfaces, so\\nthey can be stored in dictionaries or other interfaces, and be returned by\\nfunctions or methods other than constructors.\\n* We would like to keep the UniFFI implementation as simple as possible while\\nproviding a suitable degree of safety - in particular, a promise that it\\nshould be impossible to misuse the generated bindings in a way that triggers\\nRust's \"undefined behavior\" or otherwise defeat Rust's safety\\ncharacteristics and ownership model (and in particular, avoiding things like\\nuse-after-free issues).\\n* We would like to keep the overhead of UniFFI as small as possible so that it\\nis a viable solution to more use-cases.\\nChosen option: \"Use `HandleMap`\" because it offers the most flexibility, and\\nprovides a reasonable balance between safety and performance.\\n### Positive Consequences\\n* The `HandleMap` struct can be passed as an argument to function\\narguments, which are then passed to functions or methods, without needing to\\nduplicate the data in the handlemap.\\n* `HandleMap` is not a closure, so it's much easier to pass it as an argument to\\"}
{"Context":"## Context\\nThe goal of this challenge is to implement a support plaform for a\\nmicroservices architecture with several requirements:\\n* is scalable on load\\n* is fully automated\\n* identifies Single Point of Failures\\n* implements resilience and fault tolerance mechanisms\\nWe need to record the architectural decisions made on this project as\\na way to document all the thinking and building process.  This will help\\nothers to answer their wondering questions about why and how this solution was\\nimplemented.\\nWe need to record the architectural decisions made on this project.\\n","Decision":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","Prediction":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\nWe will use [Confluence](https:\/\/confluence.atlassian.com\/display\/DEV\/Documenting+Architecture+Decisions)\\nWe will use [Jira](https:\/\/www.atlassian.com\/software\/jira)\\nWe will use [Jenkins](https:\/\/jenkins.io\/)\\nWe will use [SonarQube](https:\/\/www.atlassian.com\/software\/sonarqube)\\nWe will use [Docker](https:\/\/docs.docker.com)\\nWe will use [Kubernetes](https:\/\/kubernetes.io\/)\\nWe will use [CloudFormation](https:\/\/cloudformation.com\/)\\nWe will use [Ansible](https:\/\/www.ansible.com\/)\\nWe will use [Puppet](https:\/\/puppet.com\/)\\nWe will use [Salt](https:\/\/saltstack.com\/)\\nWe will use [Docker Compose](https:\/\/docs.docker.com\/compose\/)\\nWe will use [Docker Swarm](https:\/\/docs.docker.com\/compose\/containers\/)\\nWe will use"}
{"Context":"## Context and Problem Statement\\nA mechanism is needed to inform service providers (SP) that a user has paid for a given subscription. A SP could fetch the user's profile information, however, we prevent some 3rd party SPs from doing so, and instead give them all the information they need in a JWT format access token. Adding subscription information in JWT access tokens will give SPs the information they need to verify users have paid for a subscription.\\n## Decision Drivers\\n- Security - Users should not be able to get access to subscriptions they have not paid for.\\n- Extensibility - Adding subscription info should not inhibit future extensions to JWT access tokens.\\n- Standards - The [JWT access token draft spec][#jwt-draft-spec] format should be followed as closely as possible.\\n","Decision":"## Decision\n- Security - Users should not be able to get access to subscriptions they have not paid for.\\n- Extensibility - Adding subscription info should not inhibit future extensions to JWT access tokens.\\n- Standards - The [JWT access token draft spec][#jwt-draft-spec] format should be followed as closely as possible.\\nChosen option: Adding subscription info into its own claim was chosen because FxA's lax scope checking means bad acting users could grant themselves access to subscriptions they have not paid for. See [this bug regarding FxA's lax scope checking][#lax-scope-checking].\\n### Positive Consequences\\n- The JWT access token contains all the information an SP needs to verify the user has paid for a subscription.\\n- Users are unable to grant themselves access to subscriptions they have not paid for.\\n### Negative Consequences\\n- SPs must now check two claims from the JWT to ensure a user is able to access a protected resource.\\n- An additional claim is added that is not defined in [the JWT access token draft spec][#jwt-draft-spec].\\n","Prediction":"## Decision\n- Security - Users should not be able to get access to subscriptions they have not paid for.\\n- Extensibility - Adding subscription info should not inhibit future extensions to JWT access tokens.\\n- Standards - The [JWT access token draft spec][#jwt-draft-spec] format should be followed as closely as possible.\\nChosen option: \"Add the subscription information to the JWT access token\", because it addresses all of the decision drivers and is the most flexible option.\\n### Positive Consequences\\n* Users will be able to get access to their subscriptions without having to provide subscription information in their JWT access tokens.\\n* Users are not prevented from providing subscription information in their JWT access tokens, which is a security concern.\\n* Existing JWT access token extensions can be maintained without modification, and can be extended to add new information.\\n### Negative Consequences\\n* Existing JWT access token extensions will need to be modified to add subscription information.\\n* Existing JWT access token extensions will only support JWT access tokens, not JSON Web Tokens (JWT) bearer tokens.\\n* Existing JWT access token extensions will not support JWT bearer tokens, which is a security concern.\\n* Existing JWT access token extensions will need to be modified to add subscription information.\\n"}
{"Context":"## Context\\nThe Java implementation of the protocol uses `final static` properties to define\\nthe schema for requests and responses. These are defined using objects, making\\nit easy to support multiple versions for each API call.\\nPHP doesn't have this feature so we need decide how to solve this.\\n","Decision":"## Decision\nWe're still going to use objects to manipulate data and write\/read content\\nto\/from Kafka, however it would be simpler to use arrays in constants of each\\nrequest\/response class.\\nEach field would be an array item, the key would be the field name and the value\\nwould be the field type (or another array for more complex configuration).\\nThe mapping would like this:\\n```php\\nuse Lcobucci\\Kafka\\Protocol\\Type;\\nfinal class DoSomethingRequest\\n{\\nprivate const SCHEMAS = [\\n[\\n'error_code'   => Type\\Int16::class,\\n'api_versions' => [\\n'type'     => Type\\ArrayOf::class,\\n'nullable' => false, \/\/ optional, default = false\\n'items'    => [ \/\/ just type name if items don't have complex structure\\n'api_key'     => Type\\Int16::class,\\n'min_version' => Type\\Int16::class,\\n'max_version' => Type\\Int16::class,\\n],\\n],\\n],\\n];\\n}\\n```\\n","Prediction":"## Decision\nWe will use the `enum` type to define the parameters to each API call.\\nThis will allow us to easily define the parameters for each request and response.\\nThis also means that the `enum` type can be used for the schema definition.\\nThis will allow us to use the same schema for the API calls across all\\nversions of the library.\\nThis means that we can have a version of the `Serializable` interface and\\nhave it be used for all calls to the library.\\nThis also means that we can use the same interface definition for the\\n`Serializable` interface across all versions of the library.\\nThis also means that the `Serializable` interface can be used for all calls to\\nthe library.\\nThis also means that PHP has the same interface definition for the\\n`Serializable` interface across all versions of the library.\\nThis means that we can use the same interface definition for the\\n`Serializable` interface across all versions of the library.\\nThis means that PHP has the same interface definition for the\\n`Serializable` interface across all versions of the library.\\nThis means that PHP has the same interface definition for the\\n`Serializable` interface across all versions of the library.\\nThis means that PHP has the"}
{"Context":"## Context\\nAt present, any rows in a spatial query are dropped if they return an aggregate of 15 subscribers or less. With new\\nlabelling disaggregation queries being added to Flowkit, there is an increased risk of deanonymization attacks\\nusing the increased resolution of information - we need to consider further redaction strategies to mitigate this.\\n","Decision":"## Decision\nFor any labelled spatial aggregate queries, we drop any aggregation zone that contains any disaggregation less than 15\\n(for consistency with the rest of the dissagregation strategy).\\n","Prediction":"## Decision\n* Remove the 15 subscriber limit on the aggregate aggregation query\\n* Implement a new entity type to hold aggregated results. This type will be a `SpatialAggregate` with a `low_level` of 2.\\n* Implement a new entity type to hold aggregated results. This type will be a `SpatialAggregate` with a `low_level` of 1.\\n* Create a new entity type to hold aggregated results. This type will be a `SpatialAggregate` with a `high_level` of 1.\\n* Create a new entity type to hold aggregated results. This type will be a `SpatialAggregate` with a `high_level` of 2.\\n* Implement a new entity type to hold aggregated results. This type will be a `SpatialAggregate` with a `low_level` of 1.\\n* Create a new entity type to hold aggregated results. This type will be a `SpatialAggregate` with a `high_level` of 2.\\n* Create a new entity type to hold aggregated results. This type will be a `SpatialAggregate` with a `high_level` of 3.\\n* Create a new entity type to hold aggregated results. This type will be a `SpatialAggregate` with a `"}
{"Context":"**Context**: because the underlying protocol for GCS is JSON, any field can be\\nabsent from the responses received from the server. Furthermore, the application\\ncan use the `fields` query parameter to select a subset of the fields in a\\nresponse. A natural question is whether the fields should be represented as\\n`optional<T>`. That is, whether the class to represent object attributes should\\nlook like this:\\n```C++\\nclass ObjectMetadata { public:\\nstd::string const& name() const;\\nstd::chrono::system_clock::time_point time_created() const;\\nCustomerEncryption const& customer_encryption() const;\\n};\\n```\\nor it should look like this:\\n```C++\\nusing google::cloud::optional;\\nclass ObjectMetadata { public:\\noptional<std::string> const& name() const;\\noptional<std::chrono::system_clock::time_point> time_created() const;\\noptional<CustomerEncryption> const& customer_encryption() const;\\n};\\n```\\n**Decision**:\\n- For string fields where there is no semantic difference between an empty\\nstring and the field not present we just use `std::string<>`.\\n- For array fields where there is no semantic difference between field not\\npresent and an empty array we just use `std::vector<>`.\\n- For integer and boolean fields we default to `0` (and `false`) if the field is\\nnot present.\\n- For object fields we default to wrapping the field in `optional<>`.\\nFor fields wrapped in `optional<>` we offer convenience functions to make it\\neasier to operate on these fields. For a field called `foo` these are:\\n- `has_foo()` returns true if the field is set.\\n- `foo()` returns the field value if set, the behavior is undefined if the value\\nis not set.\\n- `foo_as_optional()` returns the optional field.\\n- `reset_foo()` resets the field (for writable fields).\\n- `set_foo()` sets the field (for writable fields).\\n**Consequences**: The advantage of this approach is that most fields are easy to\\nuse most of the time. The disadvantage of this approach include:\\n- The ambiguity when the application filters the returned fields, the value may\\nbe the default value because the client did not get the field.\\n- As applications change over time and they start filtering different fields the\\ncode may assume that the value of a field is valid, but it has a default\\nvalue. With optionals the application should crash during testing, or may be\\nprogrammed defensively since the start.\\n- It also requires more thought designing the classes a field has different\\nsemantics for \"not there\" vs. \"the default value\".\\n**Reference**\\nThis was originally discussed in\\n[#934](https:\/\/github.com\/googleapis\/google-cloud-cpp\/issues\/934).\\nThe original [PR](https:\/\/github.com\/googleapis\/google-cloud-cpp\/pull\/1358) also\\nhas some interesting discussions.\\n","Decision":"## Decision\n- For string fields where there is no semantic difference between an empty\\nstring and the field not present we just use `std::string<>`.\\n- For array fields where there is no semantic difference between field not\\npresent and an empty array we just use `std::vector<>`.\\n- For integer and boolean fields we default to `0` (and `false`) if the field is\\nnot present.\\n- For object fields we default to wrapping the field in `optional<>`.\\nFor fields wrapped in `optional<>` we offer convenience functions to make it\\neasier to operate on these fields. For a field called `foo` these are:\\n- `has_foo()` returns true if the field is set.\\n- `foo()` returns the field value if set, the behavior is undefined if the value\\nis not set.\\n- `foo_as_optional()` returns the optional field.\\n- `reset_foo()` resets the field (for writable fields).\\n- `set_foo()` sets the field (for writable fields).\\n**Consequences**: The advantage of this approach is that most fields are easy to\\nuse most of the time. The disadvantage of this approach include:\\n- The ambiguity when the application filters the returned fields, the value may\\nbe the default value because the client did not get the field.\\n- As applications change over time and they start filtering different fields the\\ncode may assume that the value of a field is valid, but it has a default\\nvalue. With optionals the application should crash during testing, or may be\\nprogrammed defensively since the start.\\n- It also requires more thought designing the classes a field has different\\nsemantics for \"not there\" vs. \"the default value\".\\n**Reference**\\nThis was originally discussed in\\n[#934](https:\/\/github.com\/googleapis\/google-cloud-cpp\/issues\/934).\\nThe original [PR](https:\/\/github.com\/googleapis\/google-cloud-cpp\/pull\/1358) also\\nhas some interesting discussions.\\n","Prediction":"## Decision\n- For string fields where there is no semantic difference between an empty\\nstring and the field not present we just use `std::string<>`.\\n- For array fields where there is no semantic difference between field not\\npresent and an empty array we just use `std::vector<>`.\\n- For integer and boolean fields we default to `0` (and `false`) if the field is\\nnot present.\\n- For object fields we default to wrapping the field in `optional<>`.\\nFor fields wrapped in `optional<>` we offer convenience functions to make it\\neasier to operate on these fields. For a field called `foo` these are:\\n- `has_foo()` returns true if the field is set.\\n- `foo()` returns the field value if set, the behavior is undefined if the value\\nis not set.\\n- `foo_as_optional()` returns the optional field.\\n- `reset_foo()` resets the field (for writable fields).\\n- `set_foo()` sets the field (for writable fields).\\n**Consequences**: The advantage of this approach is that most fields are easy to\\nuse most of the time. The disadvantage of this approach include:\\n- The"}
{"Context":"## Context\\nThe proposed trust metric will allow Tendermint to maintain local trust rankings for peers it has directly interacted with, which can then be used to implement soft security controls. The calculations were obtained from the [TrustGuard](https:\/\/dl.acm.org\/citation.cfm?id=1060808) project.\\n### Background\\nThe Tendermint Core project developers would like to improve Tendermint security and reliability by keeping track of the level of trustworthiness peers have demonstrated within the peer-to-peer network. This way, undesirable outcomes from peers will not immediately result in them being dropped from the network (potentially causing drastic changes to take place). Instead, peers behavior can be monitored with appropriate metrics and be removed from the network once Tendermint Core is certain the peer is a threat. For example, when the PEXReactor makes a request for peers network addresses from a already known peer, and the returned network addresses are unreachable, this untrustworthy behavior should be tracked. Returning a few bad network addresses probably shouldn\u2019t cause a peer to be dropped, while excessive amounts of this behavior does qualify the peer being dropped.\\nTrust metrics can be circumvented by malicious nodes through the use of strategic oscillation techniques, which adapts the malicious node\u2019s behavior pattern in order to maximize its goals. For instance, if the malicious node learns that the time interval of the Tendermint trust metric is _X_ hours, then it could wait _X_ hours in-between malicious activities. We could try to combat this issue by increasing the interval length, yet this will make the system less adaptive to recent events.\\nInstead, having shorter intervals, but keeping a history of interval values, will give our metric the flexibility needed in order to keep the network stable, while also making it resilient against a strategic malicious node in the Tendermint peer-to-peer network. Also, the metric can access trust data over a rather long period of time while not greatly increasing its history size by aggregating older history values over a larger number of intervals, and at the same time, maintain great precision for the recent intervals. This approach is referred to as fading memories, and closely resembles the way human beings remember their experiences. The trade-off to using history data is that the interval values should be preserved in-between executions of the node.\\n### References\\nS. Mudhakar, L. Xiong, and L. Liu, \u201cTrustGuard: Countering Vulnerabilities in Reputation Management for Decentralized Overlay Networks,\u201d in _Proceedings of the 14th international conference on World Wide Web, pp. 422-431_, May 2005.\\n","Decision":"## Decision\nThe proposed trust metric will allow a developer to inform the trust metric store of all good and bad events relevant to a peer's behavior, and at any time, the metric can be queried for a peer's current trust ranking.\\nThe three subsections below will cover the process being considered for calculating the trust ranking, the concept of the trust metric store, and the interface for the trust metric.\\n### Proposed Process\\nThe proposed trust metric will count good and bad events relevant to the object, and calculate the percent of counters that are good over an interval with a predefined duration. This is the procedure that will continue for the life of the trust metric. When the trust metric is queried for the current **trust value**, a resilient equation will be utilized to perform the calculation.\\nThe equation being proposed resembles a Proportional-Integral-Derivative (PID) controller used in control systems. The proportional component allows us to be sensitive to the value of the most recent interval, while the integral component allows us to incorporate trust values stored in the history data, and the derivative component allows us to give weight to sudden changes in the behavior of a peer. We compute the trust value of a peer in interval i based on its current trust ranking, its trust rating history prior to interval _i_ (over the past _maxH_ number of intervals) and its trust ranking fluctuation. We will break up the equation into the three components.\\n```math\\n(1) Proportional Value = a * R[i]\\n```\\nwhere _R_[*i*] denotes the raw trust value at time interval _i_ (where _i_ == 0 being current time) and _a_ is the weight applied to the contribution of the current reports. The next component of our equation uses a weighted sum over the last _maxH_ intervals to calculate the history value for time _i_:\\n`H[i] =` ![formula1](img\/formula1.png \"Weighted Sum Formula\")\\nThe weights can be chosen either optimistically or pessimistically. An optimistic weight creates larger weights for newer history data values, while the the pessimistic weight creates larger weights for time intervals with lower scores. The default weights used during the calculation of the history value are optimistic and calculated as _Wk_ = 0.8^_k_, for time interval _k_. With the history value available, we can now finish calculating the integral value:\\n```math\\n(2) Integral Value = b * H[i]\\n```\\nWhere _H_[*i*] denotes the history value at time interval _i_ and _b_ is the weight applied to the contribution of past performance for the object being measured. The derivative component will be calculated as follows:\\n```math\\nD[i] = R[i] \u2013 H[i]\\n(3) Derivative Value = c(D[i]) * D[i]\\n```\\nWhere the value of _c_ is selected based on the _D_[*i*] value relative to zero. The default selection process makes _c_ equal to 0 unless _D_[*i*] is a negative value, in which case c is equal to 1. The result is that the maximum penalty is applied when current behavior is lower than previously experienced behavior. If the current behavior is better than the previously experienced behavior, then the Derivative Value has no impact on the trust value. With the three components brought together, our trust value equation is calculated as follows:\\n```math\\nTrustValue[i] = a * R[i] + b * H[i] + c(D[i]) * D[i]\\n```\\nAs a performance optimization that will keep the amount of raw interval data being saved to a reasonable size of _m_, while allowing us to represent 2^_m_ - 1 history intervals, we can employ the fading memories technique that will trade space and time complexity for the precision of the history data values by summarizing larger quantities of less recent values. While our equation above attempts to access up to _maxH_ (which can be 2^_m_ - 1), we will map those requests down to _m_ values using equation 4 below:\\n```math\\n(4) j = index, where index > 0\\n```\\nWhere _j_ is one of _(0, 1, 2, \u2026 , m \u2013 1)_ indices used to access history interval data. Now we can access the raw intervals using the following calculations:\\n```math\\nR[0] = raw data for current time interval\\n```\\n`R[j] =` ![formula2](img\/formula2.png \"Fading Memories Formula\")\\n### Trust Metric Store\\nSimilar to the P2P subsystem AddrBook, the trust metric store will maintain information relevant to Tendermint peers. Additionally, the trust metric store will ensure that trust metrics will only be active for peers that a node is currently and directly engaged with.\\nReactors will provide a peer key to the trust metric store in order to retrieve the associated trust metric. The trust metric can then record new positive and negative events experienced by the reactor, as well as provided the current trust score calculated by the metric.\\nWhen the node is shutting down, the trust metric store will save history data for trust metrics associated with all known peers. This saved information allows experiences with a peer to be preserved across node executions, which can span a tracking windows of days or weeks. The trust history data is loaded automatically during OnStart.\\n### Interface Detailed Design\\nEach trust metric allows for the recording of positive\/negative events, querying the current trust value\/score, and the stopping\/pausing of tracking over time intervals. This can be seen below:\\n```go\\n\/\/ TrustMetric - keeps track of peer reliability\\ntype TrustMetric struct {\\n\/\/ Private elements.\\n}\\n\/\/ Pause tells the metric to pause recording data over time intervals.\\n\/\/ All method calls that indicate events will unpause the metric\\nfunc (tm *TrustMetric) Pause() {}\\n\/\/ Stop tells the metric to stop recording data over time intervals\\nfunc (tm *TrustMetric) Stop() {}\\n\/\/ BadEvents indicates that an undesirable event(s) took place\\nfunc (tm *TrustMetric) BadEvents(num int) {}\\n\/\/ GoodEvents indicates that a desirable event(s) took place\\nfunc (tm *TrustMetric) GoodEvents(num int) {}\\n\/\/ TrustValue gets the dependable trust value; always between 0 and 1\\nfunc (tm *TrustMetric) TrustValue() float64 {}\\n\/\/ TrustScore gets a score based on the trust value always between 0 and 100\\nfunc (tm *TrustMetric) TrustScore() int {}\\n\/\/ NewMetric returns a trust metric with the default configuration\\nfunc NewMetric() *TrustMetric {}\\n\/\/------------------------------------------------------------------------------------------------\\n\/\/ For example\\ntm := NewMetric()\\ntm.BadEvents(1)\\nscore := tm.TrustScore()\\ntm.Stop()\\n```\\nSome of the trust metric parameters can be configured. The weight values should probably be left alone in more cases, yet the time durations for the tracking window and individual time interval should be considered.\\n```go\\n\/\/ TrustMetricConfig - Configures the weight functions and time intervals for the metric\\ntype TrustMetricConfig struct {\\n\/\/ Determines the percentage given to current behavior\\nProportionalWeight float64\\n\/\/ Determines the percentage given to prior behavior\\nIntegralWeight float64\\n\/\/ The window of time that the trust metric will track events across.\\n\/\/ This can be set to cover many days without issue\\nTrackingWindow time.Duration\\n\/\/ Each interval should be short for adapability.\\n\/\/ Less than 30 seconds is too sensitive,\\n\/\/ and greater than 5 minutes will make the metric numb\\nIntervalLength time.Duration\\n}\\n\/\/ DefaultConfig returns a config with values that have been tested and produce desirable results\\nfunc DefaultConfig() TrustMetricConfig {}\\n\/\/ NewMetricWithConfig returns a trust metric with a custom configuration\\nfunc NewMetricWithConfig(tmc TrustMetricConfig) *TrustMetric {}\\n\/\/------------------------------------------------------------------------------------------------\\n\/\/ For example\\nconfig := TrustMetricConfig{\\nTrackingWindow: time.Minute * 60 * 24, \/\/ one day\\nIntervalLength:    time.Minute * 2,\\n}\\ntm := NewMetricWithConfig(config)\\ntm.BadEvents(10)\\ntm.Pause()\\ntm.GoodEvents(1) \/\/ becomes active again\\n```\\nA trust metric store should be created with a DB that has persistent storage so it can save history data across node executions. All trust metrics instantiated by the store will be created with the provided TrustMetricConfig configuration.\\nWhen you attempt to fetch the trust metric for a peer, and an entry does not exist in the trust metric store, a new metric is automatically created and the entry made within the store.\\nIn additional to the fetching method, GetPeerTrustMetric, the trust metric store provides a method to call when a peer has disconnected from the node. This is so the metric can be paused (history data will not be saved) for periods of time when the node is not having direct experiences with the peer.\\n```go\\n\/\/ TrustMetricStore - Manages all trust metrics for peers\\ntype TrustMetricStore struct {\\ncmn.BaseService\\n\/\/ Private elements\\n}\\n\/\/ OnStart implements Service\\nfunc (tms *TrustMetricStore) OnStart(context.Context) error { return nil }\\n\/\/ OnStop implements Service\\nfunc (tms *TrustMetricStore) OnStop() {}\\n\/\/ NewTrustMetricStore returns a store that saves data to the DB\\n\/\/ and uses the config when creating new trust metrics\\nfunc NewTrustMetricStore(db dbm.DB, tmc TrustMetricConfig) *TrustMetricStore {}\\n\/\/ Size returns the number of entries in the trust metric store\\nfunc (tms *TrustMetricStore) Size() int {}\\n\/\/ GetPeerTrustMetric returns a trust metric by peer key\\nfunc (tms *TrustMetricStore) GetPeerTrustMetric(key string) *TrustMetric {}\\n\/\/ PeerDisconnected pauses the trust metric associated with the peer identified by the key\\nfunc (tms *TrustMetricStore) PeerDisconnected(key string) {}\\n\/\/------------------------------------------------------------------------------------------------\\n\/\/ For example\\ndb := dbm.NewDB(\"trusthistory\", \"goleveldb\", dirPathStr)\\ntms := NewTrustMetricStore(db, DefaultConfig())\\ntm := tms.GetPeerTrustMetric(key)\\ntm.BadEvents(1)\\ntms.PeerDisconnected(key)\\n```\\n","Prediction":"## Decision\nWe will implement the proposed trust metric in the Tendermint Core codebase.\\n### Decision Process\\n### Decision\\n### Decision Process\\n### Decision\\n### Decision\\n### Decision\\n### Decision\\n### Decision\\n### Decision\\n### Decision\\n### Decision\\n### Decision\\n### Decision\\n### Decision\\n### Decision\\n### Decision\\n### Decision\\n### Decision\\n### Decision\\n### Decision\\n### Decision\\n### Decision\\n### Decision\\n### Decision\\n### Decision\\n### Decision\\n### Decision\\n### Decision\\n### Decision\\n### Decision\\n### Decision\\n### Decision\\n### Decision\\n### Decision\\n### Decision\\n### Decision\\n### Decision\\n### Decision\\n### Decision\\n### Decision\\n### Decision\\n### Decision\\n### Decision\\n### Decision\\n### Decision\\n### Decision\\n### Decision\\n### Decision\\n### Decision\\n### Decision\\n### Decision\\n### Decision\\n### Decision\\n### Decision\\n### Decision\\n### Decision\\n### Decision\\n### Decision\\n### Decision\\n### Decision\\n### Decision\\n### Decision\\n### Decision\\n### Decision\\n### Decision\\n### Decision\\n### Decision\\n### Decision\\n### Decision\\n### Decision\\n### Decision\\n### Decision\\n### Decision\\n### Decision\\n### Decision\\n### Decision\\n### Decision\\n### Decision\\n###"}
{"Context":"## Context\\nA full test suite run may take > 10 minutes, so we should only run tests that are necessary.\\nWhile testing project A, running tests that only check projects B, C, ... does not contribute to the testing of A. Especially if the code from A is not exercised.\\nWhile testing project A, tests that fail due to instability on other services B and C do not contribute to the testing of A in the same way.\\n","Decision":"## Decision\nFor each project, run a subset of the tests marked with `@pytest.mark.projectname`.\\n","Prediction":"## Decision\nWe will run tests for projects A only. Tests that are necessary to test A will be run as part of the test suite.\\nWe will run tests for projects B and C only if the tests for B and C are dependent on the tests for A and C.\\nWe will run tests for project A only if the tests for A can fail in a predictable way.\\nWe will run tests for project A only if the tests for A can fail in a predictable way.\\nWe will run tests for project A only if the tests for A can fail in a predictable way.\\nWe will run tests for project A only if the tests for A can fail in a predictable way.\\nWe will run tests for project A only if the tests for A can fail in a predictable way.\\nWe will run tests for project A only if the tests for A can fail in a predictable way.\\nWe will run tests for project A only if the tests for A can fail in a predictable way.\\nWe will run tests for project A only if the tests for A can fail in a predictable way.\\nWe will run tests for project A only if the tests for A can fail in a predictable way.\\nWe will run tests for project A only if"}
{"Context":"## Context\\n### Background\\nOur service frequently needs to take a user through a journey which involves asking them a series of questions over multiple pages, and then playing all the answers back to the user before finally submitting all the answers to the interventions service API.\\nThis means that we need somewhere to store the user\u2019s answers as they progress through the journey. For some journeys, the interventions service provides this storage. For example, in the journey of submitting a referral, the interventions service provides endpoints for creating and updating a draft referral.\\nHowever, we have other journeys where the interventions service does not provide this storage. For example, assigning a referral to a caseworker, or cancelling a referral. For these journeys, the UI application needs to provide its own storage.\\n### Why not have the interventions service store everything?\\nWe _could_ make the interventions service provide storage for the data of every page of every journey in the UI. However, this would be very tied to the user journeys of this UI and less of a general-purpose interventions API. We should reserve interventions service storage for long journeys which the user might want to complete in multiple sittings.\\n### The solution so far\\nWe pass data from page A to page B to page C of a journey by making sure that the HTTP request for page C includes all of the data that was submitted on pages A and B. We do this either by:\\n- accepting the data from previous pages as data encoded in the request URL\u2019s query, and then passing it in a GET request to subsequent pages by placing the data from previous pages into the query of the URL that\u2019s used for navigating to the next page\\n- accepting the data from previous pages as `application\/x-www-form-urlencoded` form data, and then passing it in a POST request to subsequent pages by placing `<input type=\"hidden\">` fields on each page, replaying the data from the previous pages\\n### The problem with this solution\\nUsing a GET request limits the amount of data a user can submit on a page, since many clients and servers do not support URLs over 2000 bytes long.\\nUsing a POST request means that we cannot redirect to different pages in the journey (for example, check your answers) based on the user\u2019s input, since a redirect response cannot instruct the browser to make a POST request.\\nAlso, the approach of embedding the data in the HTML is laborious. We have to make sure that every possible route through the pages in the journey preserves the data. This becomes particularly easy to get wrong when we have non-linear sequences of pages \u2014 for example, a link on the check your answers page that allows the user to edit a previous answer.\\n### Other requirements for a solution\\nThe solution must also:\\n- make sure that a user is not able to access data entered by a different user\\n- not prevent the user from performing the same journey multiple times concurrently \u2014 for example, they should be able to assign two different interventions at the same time in different browser tabs\\n- preserve the data that the user entered on previous pages when they use the browser\u2019s Back button\\n- give us maximum flexibility in deciding how to meet [WCAG 2.1\u2019s Success Criterion 2.2.1 Timing Adjustable](https:\/\/www.w3.org\/TR\/WCAG21\/#timing-adjustable) \u2014 for example, by making sure that the data is kept for at least 20 hours before it expires\\nIt would also be good if the solution could:\\n- not introduce new dependencies to the service \u2014 for example a database\\n- allow us to continue using the same kinds of coding patterns as we do when interacting with the interventions service API \u2014 creating and updating a resource\\n- allow us to identify and clean up old data\\n","Decision":"## Decision\nWe\u2019ll use the UI application\u2019s existing Redis server as our storage. It allows us to store essentially unlimited amounts of data.\\nIn Redis, we\u2019ll store \u201cdraft\u201d objects. These are containers of arbitrary JSON data, along with:\\n- a globally unique identifier\\n- the unique identifier of the user who created the draft\\n- timestamps of creation and last update\\n- an identifier explaining what type of data the draft represents\\nWe\u2019ll:\\n- provide a \u201cCRUD\u201d API for creating, fetching, updating and deleting these draft objects\\n- include the draft\u2019s ID in all URLs in a journey\\n- allow these drafts to be created by a GET request, so that we can use `<a>` tags to link to journeys\\n- prefer to use POSTs to pass data to a page instead of GET, so we don't have to worry about body size constraints\\nThe aforementioned API will:\\n- enforce access control \u2014 making sure that a user is only allowed to access drafts that they created\\n- make sure that drafts are automatically removed after they are no longer accessed for a certain amount of time (for example, 1 day), using Redis\u2019s expiry functionality, to make sure that drafts do not consume storage in Redis indefinitely\\n### Alternatives\\nWe might consider using the (Redis-backed) Express `session` object. However, this object expires after 2 hours of inactivity, which is insufficient for our needs. We don\u2019t want to increase this timeout since there are security implications to increasing the amount of time that a user remains logged in to the service.\\n","Prediction":"## Decision\nWe will use a solution that uses the [HTTP 2.0 protocol](http:\/\/www.w3.org\/Protocols\/rfc2616\/rfc2616-sec14.html#sec14.3.4) to transfer the user\u2019s answers to the interventions service API.\\nWe will use the [HTTP\/2.0 specification](http:\/\/www.w3.org\/Protocols\/rfc2616\/rfc2616-sec14.html#sec14.3) to store the user\u2019s answers in the interventions service API, as well as the [HTTP\/2.0 client library](http:\/\/www.w3.org\/Protocols\/rfc2616\/rfc2616-sec14.html#sec14.3.5) we use to communicate with the intervention service API.\\nWe will use the [HTTP\/2.0 server library](https:\/\/www.w3.org\/Protocols\/rfc2616\/rfc2616-sec14.html#sec14.3.6) that comes with the [HTTP\/2.0 specification](http:\/\/www.w3.org\/Protocols\/rfc2616\/rfc2616-sec14.html#sec14.3.4) to handle the HTTP\/2.0 protocol, and we will test our implementation against the"}
{"Context":"## Context\\nDatasets are made up of observations and dimensions. Each of the options for a\\ndimension have wider definitions and metadata which apply to that option in all\\nscenarios, across multiple datasets. These code definitions live in code-lists.\\nWhen storing both datasets and code-lists in a graph database, there is some\\noverlap between the information stored on a code and on a dimension option. A\\ndesign decision was needed as to whether the dimension option and code nodes\\nin the graph database should be collapsed into a single node, with the relevant\\nrelationships for each coming off that single node.\\n","Decision":"## Decision\nDimension options and codes should not be collapsed into a single node. Dimension\\noptions have relationships to every observation within an imported dataset, and\\nadding those relationships to the code node would result in relationships to\\nevery observation across all datasets - a significant and unnecessary burden.\\n","Prediction":"## Decision\nThe decision was made to store both the code and the dimensions in a single\\nnode. This was done by creating a new node type that contains the information\\nfor both the dimension and the code. The new node type is then attached to the\\nexisting node type and the information is stored on that node.\\n### Code\\nThe new node type is a `NumpyArray` and it contains the following fields:\\n- `id` - the id of the dimension\\n- `code` - the code for the dimension\\n- `dimension` - the dimension option for the dimension\\n### Dimensions\\nThe `NumpyArray` is stored in a separate column in the graph database. It is\\nattached to the `code` node and the `dimension` node. The `id` of the\\ndimension is the same as the `code` node, and the `dimension` is the\\nname of the dimension. The `dimension` field contains the name of the\\ndimension and the `code` field contains the name of the dimension option.\\n### Code List\\nThe `NumpyArray` is stored in a separate column in the graph database. It is\\nattached to the `code` node and the `dimension` node. The"}
{"Context":"## Context\\nTo avoid running out of disk space for messages in the MSK Cluster it is [best\\npractice](https:\/\/docs.aws.amazon.com\/msk\/latest\/developerguide\/bestpractices.html) to monitor disk space. This can be done using a CloudWatch alarm, or third party support.\\n","Decision":"## Decision\nWe will create a Broker CloudWatch Alarm to monitor the `KafkaDataLogsDiskUsed`\\nmetric provided for each broker. An Alarm will be created for each broker in the\\ncluster. This Alarm will only be created if the module is using a CloudWatch\\nDashboard as the assumption is that if you use another monitoring tool you would\\nset up the alarm in the tooling being used.\\nAt this time we do not plan to provide any notification or actions to be\\nperformed when this Alarm is breached as there are a number of options to use\\nwhen this occurs.\\n","Prediction":"## Decision\nWe will monitor disk space using [CloudWatch Alarms](https:\/\/aws.amazon.com\/cloudwatch\/alarms\/).\\nCloudWatch Alarms can be configured to be triggered based on any of the following criteria:\\n* [Alarm](https:\/\/docs.aws.amazon.com\/lambda\/latest\/dg\/alarms.html) - This is the most basic way to monitor disk space. It will trigger an alarm if the disk space falls below a certain threshold. The threshold can be set to any value, and an alarm can be set for any number of days in the future.\\n* [Threshold](https:\/\/docs.aws.amazon.com\/lambda\/latest\/dg\/limits.html) - The minimum disk space that a CloudWatch Alarm will trigger an alarm for. The threshold can be set to any value, and an alarm can be set for any number of days in the future.\\n* [AlarmSet](https:\/\/docs.aws.amazon.com\/lambda\/latest\/dg\/limits.html#alarmset) - A set of CloudWatch Alarms that are triggered together. This can be used to monitor disk space across a number of Alarms.\\n* [AlarmGroup](https:\/\/docs.aws.amazon.com\/lambda\/latest\/dg\/limits.html#alarmgroup) - A"}
{"Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","Prediction":"## Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\nWe will use the [Confluence](https:\/\/confluence.atlassian.com\/display\/ARCHITECT\/Document+Architecture+Decision+Records)\\nand [JIRA](https:\/\/confluence.atlassian.com\/display\/ARCHITECT\/Architecture+Decision+Records).\\nWe will use the [Architectural Styles Framework](http:\/\/www.thoughtworks.com\/radar\/architecture\/2011\/11\/15\/architectural-style-framework-2)\\nand the [Architectural Styles Guide](http:\/\/www.thoughtworks.com\/radar\/architecture\/architectural-style-guide\/)\\nto guide us in how we document our decisions.\\nWe will also use a [style guide](https:\/\/github.com\/tonygreenberg\/typography-style-guide) for style.\\nWe will use [the [JIRA](https:\/\/confluence.atlassian.com\/display\/ARCHITECT\/Architectural+Decision+Records) project](https:\/\/github.com\/jira-projects\/jira\/issues\/1106).\\nWe will use the ["}
{"Context":"## Context\\nWikibase Manifest needs to provide essential metadata and configuration options about a Wikibase instance.\\nWe need to decide on the format of the information the manifest will provide.\\nWe took into consideration the following projects:\\n- the recent [REST API Prototype](https:\/\/github.com\/wmde\/wikibase-rest-fantasies) (by WMDE) and its [OpenAPI spec](https:\/\/raw.githubusercontent.com\/wmde\/wikibase-rest-fantasies\/gh-pages\/openapi.json)\\n- [OpenRefine's initiative](https:\/\/github.com\/OpenRefine\/wikibase-manifests) to collect manifests from different wikibases and their [json-schema spec](https:\/\/github.com\/OpenRefine\/wikibase-manifests\/blob\/master\/wikibase-manifest-schema-v1.json)\\nThe MediaWiki REST API which we are using (please refer to ADR 1 for more info) implements neither json schema nor openAPI.\\n### OpenAPI (swagger)\\nThe OpenAPI Specification (formerly Swagger Specification) is an API description format for REST APIs. An OpenAPI file allows you to describe your entire API. [Swagger](swagger.io) is a set of open-source tools built around the OpenAPI Specification, like the [api editor](https:\/\/editor.swagger.io\/).\\n- It\u2019s popularly used for mocking services and generating SDKs. It's not commonly used for run-time functionality.\\n- Useful when you want to describe your entire API.\\n- **OpenAPI is both a subset of JSON Schema Draft 5 and a superset**\\n### Json Schema\\nJSON Schema is a vocabulary that allows you to validate, annotate, and manipulate JSON documents.\\nThe specification is split into three parts, [Core](https:\/\/json-schema.org\/draft\/2019-09\/json-schema-core.html), [Validation](https:\/\/json-schema.org\/draft\/2019-09\/json-schema-validation.html), and [Hyper-Schema](https:\/\/json-schema.org\/draft\/2019-09\/json-schema-hypermedia.html).\\n- JSON Schema is a good option when there are data models whose schema needs to be defined\\n","Decision":"## Decision\nUse OpenAPI spec.\\nWe acknowledge that both are good options. We chose OpenAPI because the Wikidata team has created several products (e.g. [termbox](https:\/\/gerrit.wikimedia.org\/r\/plugins\/gitiles\/wikibase\/termbox\/+\/refs\/heads\/master\/openapi.json)) using the OpenAPI spec and plan on continue to do so when we have the opportunity.\\nThere're tools (e.g. [OpenAPI Schema to JSON Schema](https:\/\/github.com\/openapi-contrib\/openapi-schema-to-json-schema)) for converting from OpenAPI Schema Object or Parameter Object to JSON Schema in case the need arises to use json schema.\\n","Prediction":"- JSON Schema is a good option when there are problems with the data model\\n## Decision\nWe will use Json Schema as our data model. We will use it for all other data model as well, but only as a reference to Json Schema.\\nWe will use the following conventions for the data model:\\n- **Type**: The type of the data. Usually the type of the property is the same as the data type, but it can be different if the property has a different type.\\n- **Value**: The value of the property. Usually the value of the property is the same as the data type, but it can be different if the property has a different type.\\n- **Key**: The name of the property. Usually the name of the property is the same as the name of the property in the data type, but it can be different if the property has a different name.\\n- **Default**: The value of the property if the property has no value. Usually the value of the property is the same as the data type, but it can be different if the property has a different type.\\n- **Comment**: A short description of the property. Usually the property name is the same as the name of the property in the data type,"}
{"Context":"## Context\\nWe need a standard format for messages to be exchanged by the routing protocol.\\nThis format would be used by routers on different implementations.\\n","Decision":"## Decision\nWe use the following formats:\\nFor a message:\\n```\\n{\\nonward_route: Route,\\nreturn_route: Route,\\npayload: Any\\n}\\n```\\nWhere\\n`Route` - an ordered list of addresses.\\nFor an address:\\n```\\n{\\ntype: Integer,\\ndata: Any\\n}\\n```\\n","Prediction":"## Decision\nWe will use the [RFC 793](http:\/\/www.rfc-editor.org\/rfc\/rfc793)\\nformat as a base for the routing protocol messages.\\nThis will allow us to easily exchange routing protocol messages.\\n[RFC 3437](https:\/\/tools.ietf.org\/html\/rfc3437) provides an extensible format for routing protocol messages that can be used by the routing protocol.\\nWe will use this format for messages exchanged by the routing protocol.\\nWe will use `message` fields in the routing protocol messages to indicate the type of message.\\nWe will use `message_type` fields in the routing protocol messages to indicate the type of routing protocol message.\\nWe will use `routing_protocol` fields in the routing protocol messages to indicate the routing protocol.\\nWe will use `routing_protocol` fields in the routing protocol messages to indicate the routing protocol.\\nWe will use `routing_protocol_message` fields in the routing protocol messages to indicate the routing protocol message.\\nRouting protocol messages will be encoded in the format of the `message` fields.\\nRouting protocol messages will be encoded in the format of the `message_type` fields.\\nRouting protocol messages will be encoded in the format of the `routing_protocol` fields.\\nRouting"}
{"Context":"## Context\\nCurrently, the themes can only inherit config fields from the default Storefront theme.\\nAlso, this inheritance is only a snapshot by activation time of the theme - The configs are copied to the new theme and changes to the default theme config will not appear in the new theme without a re-activation.\\nThe different possibilities to inherit different parts of a theme, like scripts, templates and config, can also cause problems on later updates.\\n","Decision":"## Decision\nTo take this points into account, we have decided to add a new inheritance key for the `configFields` in the `theme.json` which allow a theme to inherit its config from other themes in a given order:\\n```json\\n\"configInheritance\": [\\n\"@Storefront\",\\n\"@PreviousTheme\",\\n\"@MyDevelopmentTheme\"\\n],\\n```\\n<details>\\n<summary>Complete theme.json with part inheritances<\/summary>\\n```json\\n{\\n\"name\": \"MyDevelopmentTheme\",\\n\"author\": \"Shopware AG\",\\n\"views\": [\\n\"@Storefront\",\\n\"@Plugins\",\\n\"@MyDevelopmentTheme\"\\n],\\n\"style\": [\\n\"app\/storefront\/src\/scss\/overrides.scss\",\\n\"@Storefront\",\\n\"app\/storefront\/src\/scss\/base.scss\"\\n],\\n\"script\": [\\n\"@Storefront\",\\n\"app\/storefront\/dist\/storefront\/js\/my-development-theme.js\"\\n],\\n\"asset\": [\\n\"@Storefront\",\\n\"app\/storefront\/src\/assets\"\\n],\\n\"configInheritance\": [\\n\"@Storefront\",\\n\"@PreviousTheme\",\\n\"@MyDevelopmentTheme\"\\n],\\n\"config\": {\\n\"blocks\": {\\n\"exampleBlock\": {\\n\"label\": {\\n\"en-GB\": \"Example block\",\\n\"de-DE\": \"Beispiel Block\"\\n}\\n}\\n},\\n\"sections\": {\\n\"exampleSection\": {\\n\"label\": {\\n\"en-GB\": \"Example section\",\\n\"de-DE\": \"Beispiel Sektion\"\\n}\\n}\\n},\\n\"fields\": {\\n\"my-single-test-select-field\": {\\n\"editable\": false\\n},\\n\"my-single-select-field\": {\\n\"label\": {\\n\"en-GB\": \"Select a font size\",\\n\"de-DE\": \"W\u00e4hle ein Schriftgr\u00f6\u00dfe\"\\n},\\n\"type\": \"text\",\\n\"value\": \"24\",\\n\"custom\": {\\n\"componentName\": \"sw-single-select\",\\n\"options\": [\\n{\\n\"value\": \"16\",\\n\"label\": {\\n\"en-GB\": \"16px\",\\n\"de-DE\": \"16px\"\\n}\\n},\\n{\\n\"value\": \"20\",\\n\"label\": {\\n\"en-GB\": \"20px\",\\n\"de-DE\": \"20px\"\\n}\\n},\\n{\\n\"value\": \"24\",\\n\"label\": {\\n\"en-GB\": \"24px\",\\n\"de-DE\": \"24px\"\\n}\\n}\\n]\\n},\\n\"editable\": true,\\n\"block\": \"exampleBlock\",\\n\"section\": \"exampleSection\"\\n},\\n\"usps-positions\": {\\n\"label\":\\n{\\n\"en-GB\": \"Position\",\\n\"de-DE\": \"Position\"\\n},\\n\"scss\": true,\\n\"type\": \"text\",\\n\"value\": [\\n\"top\",\\n\"bottom\"\\n],\\n\"custom\": {\\n\"componentName\": \"sw-multi-select\",\\n\"options\": [\\n{\\n\"value\": \"bottom\",\\n\"label\":\\n{\\n\"en-GB\": \"bottom\",\\n\"de-DE\": \"unten\"\\n}\\n},\\n{\\n\"value\": \"top\",\\n\"label\":\\n{\\n\"en-GB\": \"top\",\\n\"de-DE\": \"oben\"\\n}\\n},\\n{\\n\"value\": \"middle\",\\n\"label\":\\n{\\n\"en-GB\": \"middle\",\\n\"de-DE\": \"mittel\"\\n}\\n}\\n]\\n},\\n\"editable\": true,\\n\"tab\": \"usps\",\\n\"block\": \"exampleBlock\",\\n\"section\": \"exampleSection\"\\n}\\n}\\n}\\n}\\n```\\n<\/details>\\n","Prediction":"## Decision\nWe will add an extension point to Storefront so that themes can inherit config from other themes.\\nThis will allow themes to inherit config from other themes and override the default Storefront theme if they want to.\\nWe will also add a way for themes to override the default Storefront theme in the `storefront_theme_override` config field.\\nThis will allow themes to override the default Storefront theme for a specific store and overrides the theme in the main theme config.\\nWe will also add a way to enable\/disabling the inheritance of config from other themes.\\nThis will allow themes to enable or disable inheriting config from other themes, so that they can override the default Storefront theme if they want to.\\n### Themes that inherit from the default Storefront theme\\nThemes that inherit from the default theme will inherit all config fields from the default theme. They will override the default Storefront theme if they want to.\\nFor example, a theme that inherits from the Storefront theme could have the following config fields:\\n- `storefront_theme` (should be set to `StorefrontTheme::default`)\\n- `storefront_store` (should be set to `StorefrontStore::default`)\\n- `storefront_store"}
{"Context":"## Context\\nOur deploys to production are currently not very well tested beyond the built-in\\nfeature tests, which while providing good feature coverage, do not test how the\\nservice works as a whole in the production environment.\\n","Decision":"## Decision\nWe've decided on option 2 above.\\n","Prediction":"## Decision\nWe will add a new test suite to the CI pipeline that will test the service in the\\nproduction environment.\\nThis new test suite will run on Travis CI as part of the build process and\\nwill run the service as part of the deployment process.\\nThis new test suite will be included in the `deploy` branch of the project\\nrepository.\\nThis new test suite will be included in the `master` branch of the project\\nrepository.\\nThis new test suite will be included in the `test` branch of the project\\nrepository.\\nThis new test suite will be included in the `test` branch of the project\\nrepository.\\nThis new test suite will be included in the `test` branch of the project\\nrepository.\\nThis new test suite will be included in the `test` branch of the project\\nrepository.\\nThis new test suite will be included in the `test` branch of the project\\nrepository.\\nThis new test suite will be included in the `test` branch of the project\\nrepository.\\nThis new test suite will be included in the `test` branch of the project\\nrepository.\\nThis new test suite will be included in the `test` branch of the project\\nrepository.\\nThis"}
{"Context":"## Context\\nAdditional screens for the publishing workflow were designed given all the\\npossible fields regardless of how they are retrieved or edited from the API.\\nThese fields were broken down into suggested logical groupings for users to edit.\\nThis separation means multiple API calls could be needed to save updates on a\\nsingle screen, and that datasets will have to be added with versions depending\\non which fields are updated (going against [decision 0004](0004.md)).\\n","Decision":"## Decision\nScreens should be separated to update either the dataset or version metadata\\nbut not both in one.\\n","Prediction":"## Decision\nWe will split the publishing workflow into two screens, one for editing\\nthe fields of a single dataset and one for editing the fields of the\\nentire dataset. This will allow for better separation of data\\nmanagement and the display of the data to the user.\\n### Screen for editing a single dataset\\nA single dataset will be displayed on the screen for editing. The\\nfields which are being edited will be displayed in a list on the\\nscreen. The user will be able to select one or more fields from the\\nlist. A single dataset will be saved with the selected fields.\\n### Screen for editing the entire dataset\\nThe entire dataset will be displayed on the screen for editing. The\\nfields which are being edited will be displayed in a list on the\\nscreen. The user will be able to select one or more fields from the\\nlist. A single dataset will be saved with the selected fields.\\n### Screen for editing a single dataset\\nA single dataset will be displayed on the screen for editing. The\\nfields which are being edited will be displayed in a list on the\\nscreen. The user will be able to select one or more fields from the\\nlist. A single dataset will be saved with the selected fields.\\n###"}
{"Context":"## Context\\nThe Middleware team has strived to practice Infrastructure-As-Code, meaning DLP's AWS account should be reflected entirely within GitHub.\\nThis has mostly been accomplished but Ansible playbooks are currently run in Ansible Core on individual laptops of the Middleware team.\\nThe eventual goal should be GitOps, automated infrastructure changes triggered by a commit.\\nFor that purpose we will need a platform capable of running playbooks automatically.\\nThis platform would need to be able to run the playbooks on demand or scheduled.\\nRedHat provides two solutions: AWX and Ansible Tower.\\nAWX is the upline of Ansible Tower, meaning it is a more advanced version that lacks official support.\\n","Decision":"## Decision\nThe Middleware team will use AWX to manage DLP's infrastructure automation needs.\\nA different tool may be used for application automation.\\n","Prediction":"## Decision\nAWS will be used as the AWS provider for the Middleware team's Ansible infrastructure.\\nAWS will be the primary provider of the AWS resources that the Middleware team uses.\\nAWS will be the primary provider of the Middleware team's Ansible playbooks.\\nAWS will be the primary provider of the Middleware team's Ansible templates.\\nAWS will be the primary provider of the Middleware team's Ansible configuration management.\\nAWS will be the primary provider of the Middleware team's Ansible infrastructure as code (IAC).\\nAWS will be the primary provider of the Middleware team's Ansible configuration management.\\nAWS will be the primary provider of the Middleware team's Ansible infrastructure as code (IAC).\\nAWS will be the primary provider of the Middleware team's Ansible infrastructure as code (IAC).\\nAWS will be the primary provider of the Middleware team's Ansible infrastructure as code (IAC).\\nAWS will be the primary provider of the Middleware team's Ansible infrastructure as code (IAC).\\nAWS will be the primary provider of the Middleware team's Ansible infrastructure as code (IAC).\\nAWS will be the primary provider of the Middleware team's Ansible infrastructure as code (IAC).\\nAWS will be the"}
{"Context":"## Context\\nThe plan around UI was always to create a separate NPM library that developers could pull in to implement and build out components. The intended result of this was to create a solution that made it easy to implement and maintain the desired design. Having a separate NPM library that developers can pull in means that we have greater control over library versioning and a single source of truth.\\nWhile the first set of components were implemented via Material UI React, Martin and Soheima consulted each other and decided to go for React Semantic UI, with tailwind.css to fill in the gaps. This was due to the fact that Martin had a good experience with Semantic in the past, and because its documentation and visual examples are easy to follow. Overall, Martin has said that he prefers a larger community with a more robust system out of the box for its easy and fat deployment.\\nHowever, after attempting to work with Semantic for some time, Soheima found that it uses a standard palette with extremely limited customization - only two other colors for primary\/secondary are allowed. We are therefore revisiting this question of UI libraries to see if we can identify a better alternative.\\n","Decision":"## Decision\n1. Branding Compliance: does the library allow for us to apply our branding guidelines to the product (colors, font, etc.)?\\n2. Fit for Use Case: does it have components that support the functionality we need? Does it work across both mobile and desktop?\\n3. Developer Experience: How easy is it for front-end developers to render the UI as provided in the mockups in a clean and consistent way?\\n4. Maintainability: As the team grows and changes, will it be easy to maintain and evolve the UI?\\n5. Speed of Development: How fast is it to get the design we need up and running?\\nRebassJS. Although in an ideal world, we would have liked to start with a libaray that has many pre-built components out of the box, in reality, libraries that had this sort of set up were either too restrictive what their components could do (Semantic - weak theme customizations; Material, Ant Design - UI components do not fit and cannot be modified to behave the way we want them to behave) or too incomplete in their pre-built components (Grommet), meaning that a separate library would have to be brought in anyway to create custom components for Boxtribute. Since there did not seem to be an easy or elegant way to integrate new custom components with an existing library, we decided to go directly with the library that was created with custom UI building in mind.\\n","Prediction":"## Decision\nWe will be using React Semantic UI for our UI components. We will be using React's [Semantic UI Components](https:\/\/facebook.github.io\/react\/docs\/components.html) and tools to build and maintain our components. We will be using the [react-dom package](https:\/\/facebook.github.io\/react\/docs\/components.html#dom) to render our components.\\nWe will be using Tailwind CSS to style our components. This will not be a requirement of the project, but it is a good tool to have in our toolbelt and allows us to easily customize our style without having to use Sass.\\nWe will be using [react-router-dom](https:\/\/facebook.github.io\/react\/react-router\/3.0.0-alpha.28\/docs\/overview.html) for routing and state management. This will allow us to easily switch between screens and provide a consistent user experience across the app.\\nWe will be using [react-router](https:\/\/facebook.github.io\/react\/) as our router library. This will allow us to easily navigate between pages, and will also allow us to provide a consistent user experience across the app.\\nWe will be using [react-router-dom](https:\/\/facebook.github.io\/react\/react-router\/3.0.0-alpha.28\/docs\/overview"}
{"Context":"## Context and Problem Statement\\nWe want to start with a set of components each dedicated to a task to ensure Monocle being able to scale and also to fit well in a cloud like deployment. No monolithic app, but microservices. As we start we also don't want to overengineer, so just keep the basic components and architecture.\\n","Decision":"## Decision\nChoosen Option: \"A database, a crawler, a, CLI, a WEB API, a WEB UI\".\\nBecause, it fits the described context. The database (Elasticsearch) is scalable, and the other components are stateless, each of them use the database or the API as backend so this will ease in term of scalability. This architecture makes it easy to deploy using docker\/podman or even k8s.\\n","Prediction":"## Decision\nWe will use Docker containers to host the different components of Monocle.\\nWe will use a single application and a single database for the whole Monocle infrastructure.\\nWe will use a single application for the whole Monocle infrastructure.\\nWe will use a single application and a single database for the whole Monocle infrastructure.\\nWe will use a single application and a single database for the whole Monocle infrastructure.\\nWe will use a single application and a single database for the whole Monocle infrastructure.\\nWe will use a single application and a single database for the whole Monocle infrastructure.\\nWe will use a single application and a single database for the whole Monocle infrastructure.\\nWe will use a single application and a single database for the whole Monocle infrastructure.\\nWe will use a single application and a single database for the whole Monocle infrastructure.\\nWe will use a single application and a single database for the whole Monocle infrastructure.\\nWe will use a single application and a single database for the whole Monocle infrastructure.\\nWe will use a single application and a single database for the whole Monocle infrastructure.\\nWe will use a single application and a single database for the whole"}
{"Context":"## Context\\nWe want to ensure our platform remains available when a single AZ fails. This means that we need to have enough spare memory capacity left on cells to cover deploying apps from the failed zone. In case of 3 zones, that means each zone should be able to host 50% more apps (memory capacity wise). We can calculate maximum memory usable by all orgs by doing sum of their quotas. However, in practice much less memory is consumed. This is because\\n1. Org quotas come in T-shirt sizes and have considerable size jumps (e.g. 2, 10, 60 100G). You need to reserve next quota if previous one is too small for your needs, yet it doesn't mean you will be using all the capacity of the larger quota.\\n1. App instance memory limits are set as upper memory consumption limit. Because of that, they tend to be set larger for safety. Actual app memory consumption is always lower, many times considerably.\\nPractical example - this is a snapsot of our prod deployment in Feb 2017:\\n```\\nMemory reserved by orgs: 368640 MB (360 GB)\\nMemory reserved by apps: 107108 MB (104 GB)\\nMemory actually used by apps: 32868 (32 GB)\\n```\\nThis is not unusual and CF v1 had default overprovisioning factor of 2 (that is, it advertised 2 times more capacity than actual).\\n","Decision":"## Decision\nWe will maintain at least 50% of total org reserved capacity available when a zone fails. That is, remaining zones will have to be able to cover 50% of total reserved capacity.\\n","Prediction":"## Decision\nWe will keep max app memory capacity on each AZ to 2x the default overprovisioning factor.\\nWe will keep app memory usage to 1\/3 of the overprovisioning factor.\\nWe will keep app memory usage to 1\/3 of the overprovisioning factor.\\nWe will keep app memory usage to 1\/3 of the overprovisioning factor.\\nWe will keep app memory usage to 1\/3 of the overprovisioning factor.\\nWe will keep app memory usage to 1\/3 of the overprovisioning factor.\\nWe will keep app memory usage to 1\/3 of the overprovisioning factor.\\nWe will keep app memory usage to 1\/3 of the overprovisioning factor.\\nWe will keep app memory usage to 1\/3 of the overprovisioning factor.\\nWe will keep app memory usage to 1\/3 of the overprovisioning factor.\\nWe will keep app memory usage to 1\/3 of the overprovisioning factor.\\nWe will keep app memory usage to 1\/3 of the overprovisioning factor.\\nWe will keep app memory usage to 1\/"}
{"Context":"## Context\\nIn computer science, declarative programming is a programming paradigm\u2014a style of building the structure and elements of computer programs\u2014that expresses the logic of a computation without describing its control flow.\\nMany languages that apply this style attempt to minimize or eliminate side effects by describing what the program must accomplish in terms of the problem domain, rather than describe how to accomplish it as a sequence of the programming language primitives (the how being left up to the language's implementation). This is in contrast with imperative programming, e.g. actions in Deployment Manager, which implements algorithms in explicit steps.\\nDeclarative specifications are more clear on the expected result, once you are familair with the syntax. To understand the result of imperative specifications, interpretation of the statements in order of execution is required. Declarative syntax more explicitly states the result. Next to that, declarative specifications are context-independant. They state what should be there, instead of continuing on things that happened (or should have happened) before.\\n","Decision":"## Decision\nWe will prefer declarative syntax over imperative syntax.\\n","Prediction":"## Decision\nUse declarative syntax to describe the expected result of a computation.\\nThis will make the specification easier to read and understand, and will prevent the need to repeat the same statements in the specification.\\nIt will also make the specification more context-independent, and will make it easier to support non-English languages.\\nThis decision is based on the following:\\n* It is more clear what should be there, instead of continuing on things that happened (or should have happened) before.\\n* It is more context-independent, and it will make it easier to support non-English languages.\\n* It will be more readable and understandable.\\n* It is more consistent with other declarative languages, e.g. Go, Kotlin, Python and Rust.\\n* It is more compatible with other declarative languages, e.g. Go, Kotlin, Python and Rust.\\n* It is more efficient, as it will eliminate the need to repeat statements, and will therefore be more efficient.\\n* It is more flexible, as it will allow specification of more complex control flows, e.g. with loops.\\n* It is more expressive, as it will make the specification easier to read and understand, and will prevent the need to repeat the same statements in the"}
{"Context":"## Context\\nHow we structure our support model has a large impact on the design of any\\nsystems we put in place or services we provide.\\nThe different models could be considered on a spectrum:\\n* A **Platform as a Service** model:\\n* A larger dedicated team of SREs provide a centralised full end-to-end\\nservice for building, deploying, running applications and required\\nservices for Service Teams to consume\\n* Usually implies centralised infrastructure and benefits from the economies\\nof scale that such an architecture brings\\n* Requires little to no ops knowledge from Service Teams\\n* Evolution of the system is dependent on the platform team\\n* A **shared responsibility** or **supported workflow** model:\\n* A smaller team of dedicated SREs provide the lower-level infrastructure\\nprimitives, and \u2013 by regularly rotating\/embedding\/consulting within teams\\n\u2013 work to identify common needs, and improve the re-usability and\\nreliability of shareable solutions and maintain best-practice\\ndocumentation\\n* Makes no assumptions about what is \"centralised\"\\n* Requires some knowledge of the ops side from service teams or a structure\\nfor support from the dedicated SRE team\\n* Evolution of the system is a shared responsibility\\n* A **Paved Path** model:\\n* Teams have their own dedicated SREs focused solely on their team's needs\\nand knowledge sharing is achieved by regularly contributing to\\norganisation best-practices, guidance documents or reusable code where and\\nwhen possible\\n* Usually implies de-centralised infrastructure with each team running their\\nown\\n* Requires strong knowledge of ops side within Service Team\\n* Evolution of the system is dependent on the Service Team\\nThe table below aims to illustrate where responsibility for the maintenance,\\nevolution and iteration of various parts of such a system lie in each of\\nthese models.\\n| Responsibility  | PaaS Model | Supported Workflow Model | Paved Path Model |\\n|---|---|---|---|\\n|  | Give us your code, we'll keep it running for you | Use our tools and follow our guidance, and we'll support you | Here's some examples and guidance, do it yourself |\\n| Application lifecycle (HA, Capacity) | RE | RE\/ServiceTeam | ServiceTeam |\\n| Observability etc | RE | RE\/ServiceTeam | ServiceTeam |\\n| Backing services | RE | RE\/ServiceTeam | ServiceTeam |\\n| Iterating new features | RE | RE\/ServiceTeam | ServiceTeam |\\n| Workflow\/Process | RE | RE | ServiceTeam |\\n| Productionising common features | RE | RE | ServiceTeam |\\n| Infrastructure Responsibility | RE | RE | ServiceTeam |\\nTraditionally GDS teams were operating closest to a \"paved path\" model where teams took complete control over their own infrastructure. GDS also operates the [GOV.UK PaaS](https:\/\/cloud.service.gov.uk), a platform targeting other government departments with limited operations capability.\\nMore recently GDS has created a Reliability Engineering team (RE) to provide support for common needs of Service Teams and reduce duplication of effort around areas like infrastructure, build tooling and observability.\\nWith more of the operations skills centralised in Reliability Engineering, models on the \"paved path\" end of the spectrum become more difficult to sustain. This leaves us with two choices:\\n* provide a full-stack PaaS model, growing the size of the PaaS team as required to keep up with demand for new features.\\n* provide only the lower-level building blocks, guidance and let service teams contribute to the development of new features\\n","Decision":"## Decision\nWe will design a system around a shared responsibility between Reliability Engineering and Service Teams.\\nWe believe that the middle ground where service teams have a high level of responsibility over their deployments but Reliability Engineering takes most of the responsibility for the lower-level deployment primitives as well as providing solutions to the common needs of service teams (such as deployment workflows, monitoring, deployment patterns) will maintain the flexibility service teams need when iterating new features as well as providing the framework for getting support from Reliability Engineering when such new features could benefit from hardening or sharing with the organisation as a whole.\\n* We should provide, promote and support the use of a common language for declaratively describing infrastructure and deployments\\n* We should provide, promote and support low-level deployment primitives suitable for service teams to build upon\\n* We should work closely and continuously with service teams to reduce duplication of effort by identify common needs and providing reliable, reusable solutions\\n","Prediction":"## Decision\nWe will use a **Platform as a Service** model for all new services and services\\nintended for consumption by the wider GDS community.\\nWe will provide a **Shared Responsibility** model for the maintenance and\\nevolution of existing services and services that are subject to\\noperationally significant change.\\nWe will provide a **Paved Path** model for the maintenance and evolution\\nof existing services and services that are subject to\\noperational change. This model is intended to provide a\\ncommon framework for the maintenance and evolution of services\\noperated by the wider GDS community. This model is not intended\\nto be used for the maintenance and evolution of services that are\\nsubject to operational change.\\nWe will provide a **Paved Path** model for the maintenance and\\nevolution of existing services and services that are subject to\\noperational change. This model is intended to provide a\\ncommon framework for the maintenance and evolution of services\\noperated by the wider GDS community. This model is not intended\\nto be used for the maintenance and evolution of services that are\\nsubject to operational change.\\nWe will provide a **Paved Path** model for the maintenance and\\nevolution of existing services and services that are subject to"}
{"Context":"## Context\\nWe're seeing some things happen with harnes actions.\\n1. The Observe state starts by sending a message that waits for any existing stack to complete and\\nthen runs an animation frame if necessary. This was done to make a test pass that triggered a\\n`Browser.Navigation.load` via a port command. The test asserts that the we see the external navigation\\nscreen and that wasn't loading until we waited for an animation frame update.\\n2. It turns out that `Browser.Navigation.load` needs to send a port message back into the app and this\\ninvolves a `setTimeout` call. AND since the `send` function on a port is just a regular function, there's\\nno wait to wait for things to complete (unlike the harness functions, which we can wait on). So once the\\nnavigation load happens, we send a message back to the program (which will update the view with the external\\nnavigation screen). But that message is processed via `setTimeout(0)` which allows the next function to be\\nprocessed, which turns out to be the observation of the screen, which fails since the update hasn't happened yet.\\nThen the update happens and we can see that the screen has been updated as expected.\\n3. Another test that shows animation frame tasks being reset between setups failed because it subscribed\\nto animation frames and *another* animation frame occurs before we observe (because of 1 above). This means\\nthat the behavior for tests witten with the harness differs from the behavior for regular specs, which is\\ndescribed in [this adr](001_animation_frames.md).\\n4. We can generalize the problematic case described in (2) I think. Any incoming port message that results in\\ncommands being processed should show the same problem, where subsequent function calls in the test will be processed\\neven though the activity resulting from the port message has not yet completed. Note that some usages of the\\n`Spec.File` functions follow a pattern similar to `Browser.Navigation.load` where a message is sent back into\\nthe spec program to signal that something should be recorded and processed.\\n5. Note that we just noticed this because the test in (2) asserts that after a navigation, the view updates\\nto show that we've navigated beyond the bounds of the elm program. This isn't really important to assert\\nupon, I suppose. But because of (4) there are cases where we would expect the view to have updated. So just\\nignoring this doesn't seem like a great option. Although maybe we need a better test that demonstrates why\\nthis problem is worth solving.\\nThere are a few options:\\n1. Don't run an animation frame before the Observe step ... and ignore the problem with the `Navigation.load` test.\\nBut see (5) -- not a good idea.\\n2. Expose a way to run a function (synchronously) on the program runner. That way, after a `Navigation.load` occurs,\\nwe call this function to immediately (synchronously) run any animation tasks. This will actually make the test pass.\\nAnd has the benefit of isolating this change to cases where a `Navigation.load` occurs. But due to (4) above, this\\nproblem is not just distinctive to loading a url ...\\n3. Really the problem follows from the fact that actions on the Elm program can be triggered by sending a\\nport message directly during the test -- something that can't happen during a normal Spec and is one of the\\nmain reasons we have the harness in the first place: so that we can describe the interaction between the JS and Elm\\nparts of a system in one test. So, we could follow the pattern in (2) and run any animation tasks after\\neach port message is sent from JS. Since we return a proxy for the app, we have a way to run a function before or\\nafter a port message is sent. It turns out that this will make the test pass. But we are limited to only\\ndoing something synchronous here. We can run any queued animation tasks, but we can't wait on subsequent tasks.\\nSo this doesn't feel like a sufficiently general solution. But we need a better test to prove that.\\n4. Add a `wait` function to the harness object. This will be an async function that either returns immediately or\\nwaits for the stack to complete and then runs any remaining tasks before signaling that it's ok to continue -- just\\nas happens at the end of every step. This probably would be a general solution -- assuming we're able to call this\\nfunction at the right time during a test. But it depends on the test writer realizing what's happening and knowing\\nto wait on this function to fix things.\\n","Decision":"## Decision\nWe will add a `wait` function to the harness object so that a test writer can wait for any actions\\ntriggered by the JS code (by sending a port message into the program) to complete. This seems like the most\\ngeneral solution.\\nWe will add some information to the docs to try and remind folks to use this function when necessary.\\n","Prediction":"## Decision\nOption 1: expose a way to run a function (synchronously) on the program runner. That way, after a `Navigation.load`\\noccurs, we call this function to immediately (synchronously) run any animation tasks. This will actually make the test pass.\\nAnd has the benefit of isolating this change to cases where a `Navigation.load` occurs. But due to (4) above, this\\nproblem is not just distinctive to loading a url ...\\nOption 2: use a `wait` function. This will be an async function that either returns immediately or\\nwaits for the stack to complete and then runs any remaining tasks before signaling that it's ok to continue -- just\\nas happens at the end of every step. This probably would be a general solution -- assuming we're able to call this\\nfunction at the right time during a test. But it depends on the test writer realizing what's happening and knowing\\nto wait on this function to fix things.\\nOption 3: use a `timeout` function. This will be an async function that either returns immediately or\\nwaits for the stack to complete and then runs any remaining tasks before signaling that it's ok to continue -- just\\nas happens at"}
{"Context":"## Context\\nThe form input functionality is required. At datapunt there are 2 different form packages used for dcatd_admin (react-jsonschema-form) and signals-frontend (react-reactive-form) but the experience learnd that they are difficult to be used for the requirements that we have.\\nTherefore there is a new research done and the new package [react-hook-form](https:\/\/react-hook-form.com) that was this year released seems to be closer to our project needs of this moment. This package is also suitable to be used with the [Amsterdam Styled Components](https:\/\/github.com\/Amsterdam\/amsterdam-styled-components)\\n","Decision":"## Decision\nFor this project the form functionality will be implemented with the react-hook-form package.\\n","Prediction":"## Decision\nThe decision is to use [react-hook-form](https:\/\/react-hook-form.com)\\nThis decision is based on the following:\\n* It is the only package that is used in the form package that is used in the datapunt form package and it is used a lot by other developers.\\n* It is the only package that is used in the form package that has the possibility to be used with the [Amsterdam Styled Components](https:\/\/github.com\/Amsterdam\/amsterdam-styled-components)\\n* It is the only package that has the possibility to be used with the [react-axios](https:\/\/github.com\/axios\/react-axios) package.\\n* It is the only package that has the possibility to be used with the [react-redux](https:\/\/redux.js.org) package.\\n* It is the only package that has the possibility to be used with the [redux-thunk](https:\/\/redux.js.org\/guide\/thunk).\\n* It is the only package that has the possibility to be used with the [redux-saga](https:\/\/redux.js.org\/guide\/saga).\\n* It is the only package that has the possibility to be used with the [redux-localstorage](https:\/\/redux.js.org\/guide\/local"}
{"Context":"## Context and Problem Statement\\nOpen Data Hub has release v0.8.0, some of the Elyra features required by Thoth Station experiments are\\nnot part of this ODH release. This would require to update certain components to the HEAD of main branch of ODH upstream\\nproject.\\n## Decision Drivers\\n* Opertational complexity of an environment diverging from an upstream release\\n* User needs of more current software components\\n","Decision":"## Decision\n* Opertational complexity of an environment diverging from an upstream release\\n* User needs of more current software components\\nChosen option: \"deploy specific versions of components\", because this will give the most efficient deployment to\\nOperate First operators and users.\\n### Positive Consequences\\n* operators can gain a maximum of experience, enabling feedback on component versions that might have not been tested\\nby the upstream project\\n* users get the feature set they need to get the most out of an Operate First deployment\\n### Negative Consequences\\n* additional deployment\/manifest customizations that are not valuable to upstream project, as they are out of scope\\nfor them\\n<!-- markdownlint-disable-file MD013 -->\\n","Prediction":"## Decision\n* Opertational complexity of an environment diverging from an upstream release\\n* User needs of more current software components\\nChosen option: \"Upgrade to Elyra 0.8.0\", because it provides: \"Elyra is the most current version of Elyra, which is supported by the\\nElyra engineering team and is the current release of Elyra. There are no significant changes between Elyra 0.7.0 and Elyra 0.8.0, and\\ntherefore the upgrade is not a breaking change. The Elyra 0.7.0 release is also the last Elyra release that will be supported.\\nAdditionally, the Elyra 0.8.0 release will be the first Elyra release that will be maintained by the Elyra engineering team.\\nThis is a significant change and the Elyra engineering team will be the first to provide support for it. The Elyra 0.7.0 release is\\nalso the last Elyra release to be supported by Elyra.\\n### Positive Consequences\\n* Upgrade is supported by Elyra engineering team and is the current release of Elyra.\\"}
{"Context":"## Context\\nForm Builder sends one email per file uploaded for each application. For those forms that allow multiple files to be uploaded this generates a lot of email noise for both form owners and the applicants. This can result in additional work to sift these emails or even create unforced errors in workflows that are manual in nature.\\nThe use of AWS SES as the email service means that there is a 10MB upper limit on the size of each email that is sent.\\nOptions discussed:\\n1. Attach as many files as possible to a single email up to the 10MB limit.\\n2. Hard cap limit on 10MB in total accross _all_ the files uploaded per form.\\n3. Admin interface for form owners to log in and retrieve file uploads per submission.\\n4. Email a link to the form owner allowing the ability to download all attachments in one go.\\n","Decision":"## Decision\nWe feel that it is reasonable to reduce the number of emails that recipients of each submission get to the absolute minimum.\\nOption 1 is the one which is the most reasonable solution at this time. It requires the least amount of code change and does not change the architecture or increase the threat surface of Form Builder.\\n","Prediction":"## Decision\nWe will use AWS SES as the email service for the project.\\nThis will allow us to:\\n* Hard cap the size of each email sent.\\n* Send a single link to the form owner to download all attachments.\\n* Provide a simple admin interface for form owners to collect and download their submissions.\\n* Allow form owners to log in to the application and retrieve their submissions.\\n* Allow form owners to mark a file as downloaded.\\n* Allow form owners to mark a file as uploaded.\\n* Allow form owners to mark a file as incomplete.\\n* Allow form owners to mark a file as rejected.\\n* Allow form owners to mark a file as failed.\\n* Allow form owners to mark a file as invalid.\\n* Allow form owners to mark a file as a duplicate.\\n* Allow form owners to mark a file as a blank.\\n* Allow form owners to mark a file as blank and invalid.\\n* Allow form owners to mark a file as blank and failed.\\n* Allow form owners to mark a file as blank and invalid and failed.\\n* Allow form owners to mark a file as a duplicate and failed.\\n* Allow form owners to mark a file as a duplicate and failed and failed.\\"}
{"Context":"## Context\\nEach commercial agreement will ask the buyer questions in order to ascertain what outcomes and supplier offers are applicable to the buyer. Some of these questions will be the same across agreements. Many will have common patterns and data, even if they differ in detail.\\nIn the future we will want to more automatically match buyers\u2019 articulations of need so as to work out which products CCS can offer to meet their need.\\nThe buyer needs data needs to be stored during a buying journey, but final agreement details, when the buyer agrees a call-off for instance, will need to be stored in the agreement records.\\n","Decision":"## Decision\nUse a shared component to store user tasks in a question and answer format during buying journeys\\nThe Q&A definitions should be defined from a common repository defining the agreements\\nAgreed outcomes must be stored in the agreement records\\n","Prediction":"## Decision\nWe will use [OpenAPI](https:\/\/www.w3.org\/TR\/openapi\/) to store the data that we need about the buyer\u2019s needs as they are articulated.\\nWe will use [OpenAPI2](https:\/\/www.w3.org\/TR\/2017\/WD-openapi-spec-20170524\/) to store the data that we need about the buyer\u2019s products.\\nWe will use [OpenAPI3](https:\/\/www.w3.org\/TR\/2017\/WD-openapi-3-0\/) to store the data that we need about the buyer\u2019s services.\\nWe will use [OpenAPI3](https:\/\/www.w3.org\/TR\/2017\/WD-openapi-3-0\/) to store the data that we need about the buyer\u2019s calls to action.\\nWe will use [Swagger](https:\/\/github.com\/swagger-api\/swagger-ui) to validate the OpenAPI files that are generated by the OpenAPI documentation tool.\\nWe will use [Swagger](https:\/\/github.com\/swagger-api\/swagger-ui) to validate the OpenAPI files that are generated by the OpenAPI documentation tool.\\nWe will use [Swagger](https:\/\/github.com\/swagger-api\/swagger-ui) to validate the OpenAPI files that are generated by the OpenAPI"}
{"Context":"## Context\\nBy switching the task manager to a distributed implementation, we need to be able to `list` all `Task`s running on the cluster.\\n","Decision":"## Decision\n* Read a Cassandra projection to get all `Task`s and their `Status`\\n","Prediction":"## Decision\nWe will use the [Couchbase Lite](https:\/\/couchbase.com\/) server to store the task list. The list will be stored as a document in the `tasks` collection.\\nWe will use the `Task` class to represent a single task.\\nWe will use the `Task` class to represent a single task.\\nWe will use the `Task` class to represent a single task.\\nWe will use the `Task` class to represent a single task.\\nWe will use the `Task` class to represent a single task.\\nWe will use the `Task` class to represent a single task.\\nWe will use the `Task` class to represent a single task.\\nWe will use the `Task` class to represent a single task.\\nWe will use the `Task` class to represent a single task.\\nWe will use the `Task` class to represent a single task.\\nWe will use the `Task` class to represent a single task.\\nWe will use the `Task` class to represent a single task.\\nWe will use the `Task` class to represent a single task.\\nWe will use the `Task` class to represent a single task.\\nWe will use"}
{"Context":"## Context\\nWe need to be able to query the GXA (Gene Expression Atlas) data set to attach expression data to genes. The most appropriate interface to this database is a Solr instance available from within HX.\\n","Decision":"## Decision\nWe have implemented the following endpoints:\\n* `SolrSearch` - a generic Search implementation using `SolrSearchBuilder`. This is not currenty tested completely as there is no easy way to set up an embedded Solr server (EmbeddedSolrServer doesn't work in my hands and appears to be discouraged anyway)\\n* `ExpressionSearch` - a join\/merge implementation using `SolrSearch`\\n* Expression endpoints (query, fetch, info, object) using `ExpressionSearch`\\nSolr is a good fit with ES (natch) so relatively few changes exist. Facets and `select` are not currently supported.\\n","Prediction":"## Decision\nWe will use the [Solr API](https:\/\/www.solr.com\/) to query the GXA data set. The Solr query language is implemented in Java and is available as part of the [SolrGXA](https:\/\/github.com\/solrgxa\/solrgxa) project. We will use the [SolrGXA Solr API](https:\/\/github.com\/solrgxa\/solrgxa-solrapi-solr) to query the Solr database.\\n### SolrGXA Solr API\\nThe SolrGXA Solr API is a Java library that provides a high-level API to access Solr. It can be used to query and retrieve Solr data from HX. It supports Solr version 7.0 and later.\\n### SolrGXA Solr API Query Language\\nThe SolrGXA Solr API Query Language is a SQL-like language that allows users to specify a Solr query in the following formats:\\n- `query`: This is the standard SQL query format. It can be used to query data stored in a Solr database.\\n- `create`: This is the SQL query format that is used to create a Solr index. It can be"}
{"Context":"## Context & Problem Statement\\nJames is written in Java for a very long time. In recent years, Java modernized a lot after a decade of slow progress.\\nHowever, in the meantime, most software relying on the JVM started supporting alternative JVM languages to keep being relevant.\\nIt includes Groovy, Clojure, Scala and more recently Kotlin, to name a few.\\nNot being open to those alternative languages can be a problem for James adoption.\\n","Decision":"## Decision\nNowadays, libraries and framework targeting the JVM are expected to support usage of one or several of these alternative languages.\\nJames being not only a mail server but also a development framework needs to reach those expectations.\\nAt the same time, more and more developers and languages adopt Function Programming (FP) idioms to solve their problems.\\nWe decide for options 4, 5 and IV.\\nThat means we need to write some mailets in Scala and demonstrate how it's done and then used in a running server.\\nIt also means writing and\/or refactoring some server components in Scala, starting where it's the most relevant.\\n###\u00a0Positive Consequences\\n* Modernize parts of James code\\n* Leverage Scala richer FP ecosystem and language to overcome Java limitations on that topic\\n* Should attract people that would not like Java\\n###\u00a0Negative Consequences\\n* Adds even more knowledge requirements to contribute to James\\n* Scala build time is longer than Java build time\\n","Prediction":"## Decision\nWe will use Kotlin as the JVM language for the James project.\\nThis will allow us to keep the project written in Java and support Kotlin as a JVM language.\\nKotlin will also provide a lot of the benefits of Java, namely type safety and static typing.\\nThis will help James to grow in the long term.\\nKotlin will also provide a lot of the benefits of Java, namely type safety and static typing.\\nThis will help James to grow in the long term.\\nKotlin will also provide a lot of the benefits of Java, namely type safety and static typing.\\nThis will help James to grow in the long term.\\nKotlin will also provide a lot of the benefits of Java, namely type safety and static typing.\\nThis will help James to grow in the long term.\\nKotlin will also provide a lot of the benefits of Java, namely type safety and static typing.\\nThis will help James to grow in the long term.\\nKotlin will also provide a lot of the benefits of Java, namely type safety and static typing.\\nThis will help James to grow in the long term.\\nKotlin will also provide a lot of the benefits of Java, namely type safety and static typing.\\n"}
{"Context":"## Context\\nWe discussed the relative merits of different JS package managers.\\n","Decision":"## Decision\nUse `yarn` instead of `npm` in all dev, test, and build environments.\\n","Prediction":"## Decision\nWe will use [Yarn](https:\/\/yarnpkg.com\/).\\nYarn is a package manager for JavaScript built on top of Bower and NPM.\\nYarn is maintained by a community of contributors.\\nYarn is designed to work with other package managers and services.\\nThis is useful when using a package manager that is less well supported.\\nFor example, it is possible to use NPM to install a package.\\n[This is not possible when using Yarn](https:\/\/github.com\/npm\/npm\/issues\/16969).\\nYarn is also designed to be a dependency manager for other packages.\\nThis means that Yarn can be used to install dependencies for other packages.\\nThis is useful if you want to use a package manager that has a less well supported dependency.\\nFor example, it is possible to use NPM to install a dependency for other packages.\\nThis is not possible when using Yarn.\\nYarn is also designed to be a dependency manager for other packages.\\nThis means that Yarn is able to install dependencies for other packages.\\nThis is useful if you want to use a package manager that has a less well supported dependency.\\nFor example, it is possible to use NPM"}
{"Context":"## Context\\nSeveral different types of user will need to use the Transfer Digital Records\\nfrontend. For example, a user from a transferring body (such as another\\ngovernment department) will need to log in to upload records. Some users, such\\nas departmental record officers or digital archivists, may need wider access to\\nview in-progress transfers started by other users.\\nOne of the main features of TDR is the upload of large volumes of files to an S3\\nbucket, so the authentication system should also let us authenticate requests to\\nS3, ideally directly rather than proxying the uploads through another service.\\nTDR data also needs to be read and updated by background processes, such as the\\nfile checks that happen when a file is uploaded. These machine clients will also\\nneed to authenticate themselves.\\nIdeally, we would use an existing user management system as a federated identity\\nprovider so that we don't have to support and administrate our own system. It\\nwould also be easier for users, because they would not have to manage another\\nuser account just for TDR. But we're not aware of a cross-government identity\\nprovider (other than [GOV.UK signon][signon], which is only for\\nhttps:\/\/www.gov.uk), so our plan is to build our own for TDR.\\nAlthough we're focusing on TDR at the moment, we might want to use the same user\\nmanagement system in future systems, such as an access system to let government\\nusers see their own records. We expect that it will be simpler for users if they\\nhave one account for both systems.\\nSee the Alpha notes on [user management in cognito][alpha-cognito] for some more\\ncontext about what we expect a user management system to support, both for TDR\\nuser and admin users. It was written early in the Alpha before we looked into\\nalternatives to AWS Cognito, but a lot of the context is still relevant.\\n[signon]: https:\/\/github.com\/alphagov\/signon\\n[alpha-cognito]: https:\/\/github.com\/nationalarchives\/tdr-dev-documentation\/blob\/master\/technology-considerations\/user-management-in-cognito.md\\n","Decision":"## Decision\nUse Keycloak, and host it in AWS with the other TDR services.\\nWe chose an OAuth2 service because it will allow us to authenticate different\\nTDR services, and allow us to reuse the user system in any future projects that\\nhave the same user base. Building our own service is not worth the time or the\\nsecurity risk.\\nWe chose Keycloak over the SaaS options mainly based on cost, despite the extra\\nmaintenance burden of hosting the service. If we had chosen a SaaS option, we\\nwould have had choose one of the more expensive tiers to take advantage of extra\\nsecurity features, and this wasn't worth it for the small user base on TDR.\\nWe chose Keycloak over AWS Cognito for a few reasons:\\n- Completely customisable login page, so we can use the same styling on the\\nlogin page as on the rest of TDR.\\n- Keycloak provides [service accounts], which are accounts that can be used by\\nmachine clients like the TDR backend tasks which the API can authenticate and\\nauthorize in exactly the same way as user tokens. There is no equivalent in\\nCognito, so we would have had to find an alternative solution.\\n- Keycloak has a built-in admin user interface. We may have to build our own\\ninterface for non-technical TNA users eventually, but with Cognito we would\\nhave had to have built it much earlier.\\n- Cognito does not support [silent authentication], which is an OAuth2 flow\\nthat removes the need to store refresh tokens in the browser. This is riskier\\nthan storing temporary access tokens. See these the GitHub issues in\\n[amazon-cognito-auth-js] and [amplify-js] for more.\\nSee the [Alpha notes on authentication][alpha-considerations] for a detailed\\ncomparison of all the options.\\nAs mentioned above, we will use Keycloak's built-in admin UI, and we will use\\n[service accounts] to generate tokens for backend tasks to authenticate with the\\nAPI.\\nWe will authenticate file uploads by connecting Keycloak to an AWS Cognito\\nidentity pool which has permissions to save files in the S3 upload bucket. We\\nhave already prototyped this in the Alpha phase.\\n[service accounts]: https:\/\/github.com\/keycloak\/keycloak-documentation\/blob\/master\/server_admin\/topics\/clients\/oidc\/service-accounts.adoc\\n[silent authentication]: https:\/\/auth0.com\/docs\/api-auth\/tutorials\/silent-authentication\\n[amazon-cognito-auth-js]: https:\/\/github.com\/aws\/amazon-cognito-auth-js\/issues\/92\\n[amplify-js]: https:\/\/github.com\/aws-amplify\/amplify-js\/issues\/1218\\n[alpha-considerations]: ..\/technology-considerations\/authentication_authorisation_considerations.md\\n","Prediction":"## Decision\nWe will use AWS Cognito as the identity provider for TDR. We will use a federated\\nidentity provider for the other systems that we build.\\nWe will use AWS Cognito as the federated identity provider for TDR because it\\nallows us to use the same user management system for both TDR and Cognito.\\nWe will use AWS Cognito as the federated identity provider for TDR because it is\\neasier for users to manage multiple accounts than one.\\nWe will use AWS Cognito as the federated identity provider for TDR because it is\\neasier for us to support multiple TDR systems than one.\\nWe will use AWS Cognito as the federated identity provider for TDR because it is\\neasier for us to support multiple TDR systems than one.\\nWe will use AWS Cognito as the federated identity provider for TDR because it is\\neasier for us to support multiple TDR systems than one.\\nWe will use AWS Cognito as the federated identity provider for TDR because it is\\neasier for us to support multiple TDR systems than one.\\nWe will use AWS Cognito as the federated identity provider for TDR because it is\\neasier for"}
{"Context":"## Context\\nApplications often need to drop in binaries or scripts, either downloaded or picked up from other images such as `proofreader-php`.\\nThe binaries needed do not need `root` permissions or system-wide installation.\\nThe binaries may need to modify the application files.\\nA container image may be an alien environment, as it makes it difficult to find out which of the files were provided by the Dockerfile build process.\\n","Decision":"## Decision\nEvery image should provide a standard `\/srv\/bin` folder, on the PATH, containing utilities owned by `elife`.\\n","Prediction":"## Decision\nWe will provide an image that includes the binary or script that will be dropped in the application.\\nThis will be a `Dockerfile` that will be used to build the container.\\nThis will allow us to drop the binary or script into the application and have it run.\\nThis will also allow us to drop the binary or script into the container and have it run.\\nA container image will not include the Dockerfile, because the Dockerfile is also built by the container.\\nWe will use the container image to build the container and deploy it to the target environment.\\nWe will provide a `Dockerfile` for the application, that will be used to build the application and deploy it to the target environment.\\nThis will include the `Dockerfile` for the application and the `Dockerfile` for the container.\\nThis will allow us to drop the application and the container into the application and have them run.\\nThis will also allow us to drop the application and the container into the container and have them run.\\nThis will allow us to drop the application and the container into the container and have them run.\\nA container image will not include the Dockerfile, because the Dockerfile is also built by the"}
{"Context":"## Context\\nInvestigation into stronger alignment of technologies, practices and processes across the Libero Suite of products has resulted in time-boxed discovery sessions to find the advantages and consequences of various different aspects.\\nOne such area was front-end technologies. Other areas looked at aligning around JavaScript technologies and removing PHP from Libero Publisher.\\nLibero Reviewer uses React and JavaScript so the hypothesis was that aligning Libero Publisher around those technologies would improve the developer experience and encourage more reuse between the products. However, Libero Publisher needs a read-optimised, highly performant, progressively enhanced front end. This need has been deprioritised in Producer and Reviewer, but Publisher has a strong need for this.\\n","Decision":"## Decision\nWe will not use the React ecosystem for Libero Publisher.\\nThe full decision and more context is given in [this community-accessible document](https:\/\/docs.google.com\/document\/d\/1FkdvBjSb1BH1fhgujtppunvL_NX5U3xVuxv7L0fV010\/edit#heading=h.oip2sudw7pju).\\nFurther decisions on where the front-end developer experience can be improved through sharing of tools and styles will be documented in subsequent ADRs.\\n","Prediction":"## Decision\nWe will use React and JavaScript for front-end development.\\nWe will use [React Router](https:\/\/facebook.github.io\/react-router\/) for routing.\\nWe will use [React Router Links](https:\/\/facebook.github.io\/react-router\/docs\/links) for links.\\nWe will use [React Native](https:\/\/facebook.github.io\/react-native\/) for Native development.\\nWe will use [React Native Test Lab](https:\/\/facebook.github.io\/react-native\/docs\/testing) for testing.\\nWe will use [React Native Testing Library](https:\/\/facebook.github.io\/react-native\/docs\/testing) for testing.\\nWe will use [React Test Utils](https:\/\/facebook.github.io\/react-test-utils) for testing.\\nWe will use [React Test Utils - Pro](https:\/\/facebook.github.io\/react-test-utils-pro) for testing.\\nWe will use [React Testing Library - Pro](https:\/\/facebook.github.io\/react-testing-library-pro) for testing.\\nWe will use [React Test Utils - Free](https:\/\/facebook.github.io\/react-testing-library-free) for testing.\\nWe will use [React Testing Library - Free](https:\/\/facebook.github.io\/react-testing-library-free) for testing.\\nWe will use [React-Leaflet-Maps](https:\/\/facebook.github.io"}
{"Context":"## Context\\n1. \u6211\u4eec\u7684\u652f\u4ed8\u53ca\u8d22\u52a1\u4e1a\u52a1\uff0c\u9700\u8981\u5411\u7b2c\u4e09\u65b9\u91d1\u878d\u673a\u6784\u53d1\u8d77\u8bf7\u6c42\uff0c\u7b2c\u4e09\u65b9\u673a\u6784\u51fa\u4e8e\u5b89\u5168\u6027\u8003\u8651\uff0c\u9700\u8981\u5c06\u6211\u4eec\u7684\u670d\u52a1\u5668 IP \u5730\u5740\u8fdb\u884c\u62a5\u5907\uff1b\\n2. \u7b2c\u4e09\u65b9\u673a\u6784\u8f83\u591a\uff0c\u9488\u5bf9\u670d\u52a1\u5668\u6269\u5bb9\uff0c\u66f4\u6362\uff0c\u62a5\u5907\u4e00\u6b21\u6bd4\u8f83\u9ebb\u70e6\uff1b\\n3. \u7b2c\u4e09\u65b9\u673a\u6784\u5ba1\u6838\u65f6\u95f4\u4e0d\u5b9a\uff0c1 \u5929\u5230\u591a\u5929\u4e0d\u5b9a\uff0c\u5173\u952e\u65f6\u523b\u5f71\u54cd\u4e1a\u52a1\u3002\\n","Decision":"## Decision\n\u6b63\u5411\u4ee3\u7406\u662f\u4e00\u4e2a\u4f4d\u4e8e\u5ba2\u6237\u7aef\u548c\u76ee\u6807\u670d\u52a1\u5668\u4e4b\u95f4\u7684\u4ee3\u7406\u670d\u52a1\u5668(\u4e2d\u95f4\u670d\u52a1\u5668)\u3002\u4e3a\u4e86\u4ece\u539f\u59cb\u670d\u52a1\u5668\u53d6\u5f97\u5185\u5bb9\uff0c\u5ba2\u6237\u7aef\u5411\u4ee3\u7406\u670d\u52a1\u5668\u53d1\u9001\u4e00\u4e2a\u8bf7\u6c42\uff0c\u5e76\u4e14\u6307\u5b9a\u76ee\u6807\u670d\u52a1\u5668\uff0c\u4e4b\u540e\u4ee3\u7406\u5411\u76ee\u6807\u670d\u52a1\u5668\u8f6c\u4ea4\u5e76\u4e14\u5c06\u83b7\u5f97\u7684\u5185\u5bb9\u8fd4\u56de\u7ed9\u5ba2\u6237\u7aef\u3002\u6b63\u5411\u4ee3\u7406\u7684\u60c5\u51b5\u4e0b\u5ba2\u6237\u7aef\u5fc5\u987b\u8981\u8fdb\u884c\u4e00\u4e9b\u7279\u522b\u7684\u8bbe\u7f6e\u624d\u80fd\u4f7f\u7528\u3002\\n\u5e38\u89c1\u573a\u666f\uff1a\\n![][image-1]\\n\u53cd\u5411\u4ee3\u7406\u6b63\u597d\u76f8\u53cd\u3002\u5bf9\u4e8e\u5ba2\u6237\u7aef\u6765\u8bf4\uff0c\u53cd\u5411\u4ee3\u7406\u5c31\u597d\u50cf\u76ee\u6807\u670d\u52a1\u5668\u3002\u5e76\u4e14\u5ba2\u6237\u7aef\u4e0d\u9700\u8981\u8fdb\u884c\u4efb\u4f55\u8bbe\u7f6e\u3002\u5ba2\u6237\u7aef\u5411\u53cd\u5411\u4ee3\u7406\u53d1\u9001\u8bf7\u6c42\uff0c\u63a5\u7740\u53cd\u5411\u4ee3\u7406\u5224\u65ad\u8bf7\u6c42\u8d70\u5411\u4f55\u5904\uff0c\u5e76\u5c06\u8bf7\u6c42\u8f6c\u4ea4\u7ed9\u5ba2\u6237\u7aef\uff0c\u4f7f\u5f97\u8fd9\u4e9b\u5185\u5bb9\u5c31\u597d\u4f3c\u4ed6\u81ea\u5df1\u4e00\u6837\uff0c\u4e00\u6b21\u5ba2\u6237\u7aef\u5e76\u4e0d\u4f1a\u611f\u77e5\u5230\u53cd\u5411\u4ee3\u7406\u540e\u9762\u7684\u670d\u52a1\uff0c\u4e5f\u56e0\u6b64\u4e0d\u9700\u8981\u5ba2\u6237\u7aef\u505a\u4efb\u4f55\u8bbe\u7f6e\uff0c\u53ea\u9700\u8981\u628a\u53cd\u5411\u4ee3\u7406\u670d\u52a1\u5668\u5f53\u6210\u771f\u6b63\u7684\u670d\u52a1\u5668\u5c31\u597d\u4e86\u3002\\n\u5e38\u89c1\u573a\u666f\uff1a\\n![][image-2]\\n\u65b9\u6848\uff1a\\n1. \u4f7f\u7528\u6b63\u5411\u4ee3\u7406\uff1b\\n* \u8fd9\u4e2a\u573a\u666f\u6700\u5bb9\u6613\u60f3\u5230\u7684\u65b9\u6848\uff0c\u4f7f\u7528\u8d77\u6765\u76f4\u89c2\uff0c\u6613\u61c2\uff1b\\n* \u9700\u8981\u6bcf\u4e2a\u5ba2\u6237\u7aef\u8fdb\u884c\u914d\u7f6e\uff0c\u6216\u662f\u5728\u5e94\u7528\u4e2d\u5bf9\u5355\u4e2a\u8bf7\u6c42\u505a\u4ee3\u7406\u914d\u7f6e\uff1b\\n* \u9700\u8981\u7ef4\u62a4\u4e00\u4e2a\u9ad8\u53ef\u7528\u7684\u4ee3\u7406\u670d\u52a1\uff0c\u5e76\u5907\u6848\u6b64\u4ee3\u7406\u670d\u52a1\u5668\u3002\\n2. \u4f7f\u7528\u53cd\u5411\u4ee3\u7406\u3002\\n* \u5728\u6211\u4eec\u7684\u670d\u52a1\u5668\u4e0a\u505a\u5bf9\u65b9\u670d\u52a1\u7684\u53cd\u5411\u4ee3\u7406\uff08\u542c\u8d77\u6765\u6709\u70b9\u7ed5\uff0c\u4e0d\u76f4\u89c2\uff09\\n* \u7ef4\u62a4\u7b80\u5355\uff0c\u5c31\u50cf\u662f\u6211\u4eec\u7528 nginx\/slb \u4e3a\u5bf9\u65b9\u505a\u4e86\u4e2a\u8d1f\u8f7d\u5747\u8861\uff0c\u4f46\u914d\u7f6e\u7a0d\u6709\u4e0d\u540c\uff1b\\n```json\\nserver {\\nserver_name {{ proxy_info.server_name }};\\nlisten {{ proxy_info.ssl_listen }};\\nlocation \/ {\\nproxy_pass_header Server;\\nproxy_pass {{ proxy_info.proxy_url }};\\nproxy_redirect off;\\nproxy_set_header X-Real-IP $remote_addr;\\nproxy_set_header X-Scheme $scheme;\\nproxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\\n}\\n}\\n```\\n* \u4f7f\u7528\u7b80\u5355\uff0c\u4f7f\u7528\u65b9\u914d\u7f6e\u672c\u673a hosts \u5373\u53ef\uff0c\u4e3a\u5bf9\u65b9\u57df\u540d\u548c\u4ee3\u7406 IP \u6620\u5c04\u3002\\n3. iptables\\n* \u7c7b\u4f3c\u5c40\u57df\u7f51\u5171\u4eab\u4e0a\u7f51\uff1b\\n* \u5bf9 iptables \u914d\u7f6e\u6709\u8981\u6c42\uff1b\\n* \u76ee\u6807\u57df\u540d\u5bf9\u5e94\u7684 ip \u5730\u5740\u6539\u53d8\uff0c\u9700\u8981\u66f4\u65b0\u914d\u7f6e\u3002\\n\u6700\u7ec8\u6211\u4eec\u901a\u8fc7 aliyun slb \u76844\u5c42\u8d1f\u8f7d\u63a5\u4e24\u53f0\u90e8\u7f72\u4e86 ss5 \u7684\u673a\u5668\u63d0\u4f9b\u9ad8\u53ef\u7528\u7684\u4ee3\u7406\u670d\u52a1\\n* 4\u5c42\uff08TCP\u534f\u8bae\uff09\u670d\u52a1\u4e2d\uff0c\u5f53\u524d\u4e0d\u652f\u6301\u6dfb\u52a0\u8fdb\u540e\u7aef\u4e91\u670d\u52a1\u5668\u6c60\u7684ECS\u65e2\u4f5c\u4e3aReal Server\uff0c\u53c8\u4f5c\u4e3a\u5ba2\u6237\u7aef\u5411\u6240\u5728\u7684\u8d1f\u8f7d\u5747\u8861\u5b9e\u4f8b\u53d1\u9001\u8bf7\u6c42\u3002\\n* ss5 \u542f\u52a8\u4e8e\u5185\u7f51\u5730\u5740\u5373\u53ef\\n* ss5 \u914d\u7f6e\u9700\u5173\u6ce8 AUTHENTICATION \u548c AUTHORIZATION\\n","Prediction":"## Decision\n1. \u670d\u52a1\u5668IP \u5730\u5740\u8fdb\u884c\u62a5\u5907\uff1b\\n2. \u670d\u52a1\u5668\u6269\u5bb9\u3001\u4f18\u5316\uff0c\u66f4\u6362\u3001\u62a5\u5907\u4e00\u6b21\u6bd4\u8f83\u9ebb\u70e6\uff1b\\n3. \u670d\u52a1\u5668\u5ba1\u6838\u65f6\u95f4\u4e0d\u5b9a\uff0c\u5173\u952e\u65f6\u523b\u5f71\u54cd\u4e1a\u52a1\u3002\\n### 1. \u670d\u52a1\u5668IP\u62a5\u5907\\n1. \u670d\u52a1\u5668IP\u5730\u5740\u7684\u67e5\u8be2\u64cd\u4f5c\u9700\u8981\u4f7f\u7528 DNS \u6d3b\u52a8\u53cd\u9988\u529f\u80fd\uff0c\u901a\u8fc7 DNS \u670d\u52a1\u5668\u8fdb\u884c\u67e5\u8be2\uff0c\u8fd4\u56de\u67e5\u8be2\u7ed3\u679c\u65f6\u4f1a\u81ea\u52a8\u8fd4\u56de \u670d\u52a1\u5668IP \u5730\u5740\uff0c\u67e5\u8be2\u7ed3\u679c\u7684\u67e5\u8be2\u7ed3\u679c\u662f\u8f93\u51fa\u7684\u5185\u5bb9\uff0c\u67e5\u8be2\u7ed3\u679c\u7684\u5185\u5bb9\u53ef\u4ee5\u76f4\u63a5\u4f7f\u7528\u4efb\u610fSQL\u67e5\u8be2\u64cd\u4f5c\uff1b\\n2. \u670d\u52a1\u5668IP\u7684\u67e5\u8be2\u64cd\u4f5c\u9700\u8981\u4f7f\u7528 DNS \u670d\u52a1\u5668\u8fdb\u884c\u67e5\u8be2\uff0c\u901a\u8fc7 DNS \u670d\u52a1\u5668\u8fdb\u884c\u67e5\u8be2\uff0c\u8fd4\u56de\u67e5\u8be2\u7ed3\u679c\u65f6\u4f1a\u81ea\u52a8\u8fd4\u56de \u670d\u52a1\u5668IP \u5730\u5740\uff0c\u67e5\u8be2\u7ed3\u679c\u7684\u67e5\u8be2\u7ed3\u679c\u53ef\u4ee5\u76f4\u63a5\u4f7f\u7528\u4efb\u610fSQL\u67e5\u8be2\u64cd\u4f5c\uff1b\\n3. \u670d\u52a1\u5668IP\u7684\u67e5\u8be2\u64cd\u4f5c\u9700\u8981\u4f7f\u7528 DNS \u670d\u52a1\u5668\u8fdb\u884c\u67e5\u8be2\uff0c\u901a\u8fc7 DNS \u670d\u52a1\u5668\u8fdb\u884c\u67e5\u8be2\uff0c\u8fd4\u56de\u67e5\u8be2\u7ed3\u679c\u65f6\u4f1a\u81ea\u52a8\u8fd4\u56de \u670d\u52a1\u5668IP \u5730\u5740\uff0c\u67e5\u8be2\u7ed3\u679c\u7684\u67e5\u8be2\u7ed3\u679c\u53ef\u4ee5\u76f4\u63a5\u4f7f\u7528\u4efb\u610fSQL\u67e5\u8be2\u64cd\u4f5c\uff1b\\"}
{"Context":"## Context\\nIn [ADR #9](0009-we-choose-a-cosmosdb-api.md) we decided to use the MongoDB API\\nfor CosmosDB but as soon as we implemented the first version of the DAL logic\\nwe realized that the MongoDB support in CosmosDB was flawed: a serious [bug](https:\/\/feedback.azure.com\/forums\/263030-azure-cosmos-db\/suggestions\/19361521-fix-bug-that-destroys-nodejs-mongodb-connection-po)\\nthat destroyed the connection pool every ~10s made the MongoDB API impossible\\nto use in a production environment.\\n","Decision":"## Decision\nSince at the time we were still in the early phase of development (around\\nAugust 2017), we decided to implement the DAL on top of the more mature\\nDocumentDB API (see [this commit](https:\/\/github.com\/teamdigitale\/digital-citizenship-functions\/commit\/c72b95ebb5ed038cdf62f43dc1adacbde9668d4e)).\\nNote that the bug has been recently fixed (October 2017), so it may be worth to\\nplan a migration of the DAL to the MongoDB API.\\n","Prediction":"## Decision\nWe'll use the [Docker-native](https:\/\/github.com\/Azure\/azure-cosmos-db-docker-native) MongoDB\\nAPI instead of the [MongoDB](https:\/\/www.mongodb.com\/) one.\\nThis will allow us to use a single MongoDB instance in the Azure Cosmos DB\\narchitecture without the need to manage a replica set.\\nWe'll use the [Azure Cosmos DB](https:\/\/docs.microsoft.com\/en-us\/azure\/cosmos-db) project\\nto deploy the MongoDB API.\\nWe'll use the [Azure Cosmos DB CLI](https:\/\/github.com\/Azure\/azure-cosmos-db-cli) to install\\nthe MongoDB API in the Azure Cosmos DB project.\\nWe'll use the [Azure Cosmos DB API](https:\/\/docs.microsoft.com\/en-us\/azure\/azure-cosmos-db\/api) to create\\nan Azure Cosmos DB resource.\\nThis will allow us to easily create a new Cosmos DB resource in the Azure Cosmos DB\\nproject.\\nWe'll use the [Azure Cosmos DB CLI](https:\/\/github.com\/Azure\/azure-cosmos-db-cli) to create a new\\napp setting to enable the MongoDB API in the Azure Cosmos DB project.\\nThis will allow us to easily enable the MongoDB"}
{"Context":"## Context\\nOckam wishes to be usable (and useful) in resource-constrained environments, including on embedded microcontrollers without a typical OS, and\/or where dynamic memory allocation is not possible (either because resource limitations which make it impractical, or because of industrial standards which forbid it). As a result, our Rust crates must function in the following configurations:\\n1. `no_std`: Requires that memory be allocated statically at compile time, and no global allocator is available. Some features might be enabled in this mode for stack based collection usage like `heapless`. This is stack only mode.\\n1. `no_std+alloc`: Standard collections provided by `alloc` can be used but they do not use the standard allocator. This allows the use of heap allocation via custom allocator(s).\\n1. `std`: normal Rust. This is the default mode.\\n### Background: The Rust Standard Libraries\\nThe public API of Rust's standard library is split into 3 parts.\\n1. `libcore` (e.g. `core::*`): This is the base of the Rust standard library, and cannot generally be disabled. It is limited to functionality which:\\n- Does not perform any dynamic memory allocation, or require the existence of a global memory allocator.\\n- Has no dependencies on external libraries or functionality, such as that provided by the system C standard library.\\n- Makes no assumptions about the environment where it runs, beyond what is guaranteed by the target architecture and instruction set.\\n- For example, it can perform atomic operations, but only if the target instruction set can do so natively.\\n- Concretely, it can't contain `#[cfg(target_os = \"...\")]` (`target_family`, `unix`, `windows`, and a few others are also among the `cfg`s considered off-limits) but is allowed to use ones such as `#[cfg(target_arch = \"...\")]`.\\n- More broadly, it does not require the existence of an OS, and has no access to things like files or threads.\\n2. `liballoc` (e.g. `alloc::*`): This is a superset of `libcore` which is allowed to perform dynamic memory allocation from a global memory allocator.\\n- The allocator may be user-provided (via the `#[global_allocator]` attribute), rather than the one provided by an OS or `libc`.\\n- This mostly contains collection types, such as `Vec`, `String`, `BTreeMap`, etc.\\n- Beyond the ability to perform global memory allocation, it has the same restrictions as `libcore`:\\n- No dependencies on system libraries.\\n- No assumptions about target environment beyond what is guaranteed by the target architecture.\\n3. `libstd`, (e.g. `std::*`): This depends on (and is a superset of) both `liballoc` and `libcore`. It is what is used by default.\\n- Broadly speaking, it requires an OS that supports files and threads.\\n- There are exceptions here, in that a few targets exist which have `libstd` support but just return errors when this functionality is used.\\n- It can assume whatever it needs to about the target environment.\\n- It's allowed to contain `#[cfg(target_os = \"...\")]` statements internally.\\n- It's allowed to link against system-specific libraries on the OS (`libc`, `libm`, `libSystem.dylib`, `kernel32.dll`, ...).\\n- It can even place requirements on the OS version in use (to forbid Windows XP, for example)\\n- As it depends on `liballoc`, `libstd` is allowed to perform memory allocation anywhere it wants.\\n- If a `#[global_allocator]` is configured, it will do so out of that allocator\\n- If no `#[global_allocator]` is configured, then `libstd` will provide one based on the OS's default allocator.\\n- In the rare cases where `libstd` supports a target without a system-provided allocator, `libstd` will provide a default written in Rust (currently, it uses a Rust [port of `dlmalloc`](https:\/\/crates.io\/crates\/dlmalloc) for these situations).\\nAs you may note, these are analagous to the three configurations we wish to support, but there is an important difference:\\nWe should do not adopt the restriction that `libcore` and `liballoc` has around target-specific checks and `cfg`s and such. We favor practicality over purity, and without this, we would be completely unable to provide useful functionality for embedded environments.\\n","Decision":"## Decision\nTo reflect these three scenarios, we will use three cargo features:\\n1. `feature = \"std\"`, which allows full use of `libstd`. Mirroring the Rust stdlib, it implies `feature = \"alloc\"`.\\n2. `feature = \"alloc\"`, which allows use of an allocator. As  `feature = \"std\"`.\\n3. `feature = \"no_std\"` is used to add dependencies which are needed only in `no_std` configurations.\\n- Generally, the user is going to have to provide either `feature = \"std\"` or `feature = \"no_std\"`.\\nCertain crates are not even possible in alloc and stack only modes like transport. These crates should still follow these guidelines to the best of their extent possible.\\nThere are three files to apply the guidelines to:\\nCargo.toml\\nlib.rs\\nREADME.md\\n### Cargo.toml\\nIn the Cargo.toml file, the following 4 lines should be directly under the `[features]` section\\n```toml\\ndefault = [\"std\"]\\nstd = [\"ockam_core\/std\", \"alloc\"]\\nalloc = [\"ockam_core\/alloc\"]\\nno_std = [\"ockam_core\/no_std\"]\\n```\\nThe last two lines do not apply to crates that only function in standard mode.\\n### lib.rs\\nIn the lib.rs file, add the following lines:\\n```rust\\n#![cfg_attr(not(feature = \"std\"), no_std)]\\n#[cfg(feature = \"alloc\")]\\n#[macro_use]\\nextern crate alloc;\\n```\\n### README.md\\nIn the usage section of the README.md add the following\\nstd:\\n```\\n[dependencies]\\nockam = \"0.1.0\"\\n```\\nno\\_std with alloc:\\n```\\n[dependencies]\\nockam = { version = \"0.1.0\", default-features = false, features = [\"no_std\", \"alloc\"] }\\n```\\nno\\_std without alloc:\\n```\\n[dependencies]\\nockam = { version = \"0.1.0\", default-features = false, features = [\"no_std\"] }\\n```\\nTo effect conditional compilation on `std`, `no_std+alloc` and `no_std` builds the following attributes can be used:\\n```rust\\n\/\/ only compile for std\\n#[cfg(feature = \"std\")]\\n\/\/ compile for any environment with an allocator,\\n\/\/ either `no_std` or `no_std+alloc`.\\n#[cfg(feature = \"alloc\")]\\n\/\/ compile for any `no_std` build,\\n\/\/ either `no_std` or `no_std+alloc`.\\n#[cfg(not(feature = \"std\"))]\\n\/\/ compile for `no_std` (without `alloc`) only.\\n#[cfg(not(feature = \"alloc\"))]\\n\/\/ Compile for no_std+alloc only.\\n#[cfg(all(feature = \"alloc\", not(feature = \"std\")))]\\n```\\nTo avoid compromising the readability of code through the over-use of conditional compilation attributes we decided to create the `ockam_core::compat` module.\\nIt's function is primarily to act as a wrapper around items in the `std::` library that are not available in the `core::` namespace.\\nThe rules of use are as follows:\\n1. always prefer `core::<mod>` over `std::<mod>` where available. (e.g. `std::fmt::Result` -> `core::fmt::Result`)\\n2. otherwise, use `ockam_core::compat::<mod>` equivalents where available. (e.g. `std::sync::Arc -> ockam_core::compat::sync::Arc`)\\n3. if you need to add new items to compat, follow the originating namespace. (e.g. `compat::vec::Vec` and not `compat::Vec`)\\n4. if none of the above apply use the conditional compilation attributes as documented above\\nIn the future, we may move `ockam_core::compat` into a dedicated crate in order to reduce feature fragility, however prior to general availability, this is probably fine).\\n","Prediction":"## Decision\nWe will use the Rust standard library, which is split into 3 distinct parts:\\n1. `libcore`: This is the base of the Rust standard library, and cannot generally be disabled. It is limited to functionality which:\\n- Does not perform any dynamic memory allocation, or require the existence of a global allocator.\\n- Has no dependencies on external libraries or functionality, such as that provided by the system C standard library.\\n- Makes no assumptions about the environment where it runs, beyond what is guaranteed by the target architecture and instruction set.\\n- For example, it can perform atomic operations, but only if the target instruction set can do so natively.\\n- Concretely, it can't contain `#[cfg(target_os = \"...\")]` (`target_family`, `unix`, `windows`, and a few others are also among the `cfg`s considered off-limits) but is allowed to use ones such as `#[cfg(target_arch = \"...\")]`.\\n- More broadly, it does not require the existence of an OS, and has no access to things like files or threads.\\n2. `liballoc`: This is a superset of `libcore` which is allowed to perform dynamic memory allocation from a global"}
{"Context":"## Context\\nDocker has a daemon that oversees running containers. It does not however have much functionality to it, can't automatically restart the containers, can't set up containers that must be run automatically and has a separate logging system. It's possible to write a \"unit template\" for systemd to run containers as system services.\\n","Decision":"## Decision\nWe will use systemd for running the containers. We will write a template unit that can run an arbitrary dApp with systemd templating mechanism. We will use systemd `enable\/disable` symlinking functionality to allow the user to configure which services must auto-start. We will advise dApp developpers to use standard output for logging.\\n","Prediction":"## Decision\nWe will use the `systemd` unit template to provide the Docker container daemon. We will use the `systemd` unit template to provide a \"systemd unit\" to run as a Docker container.\\nWe will use `systemd unit` to provide a way to run containerized services. Containers can be run as a service using this unit template. The service can be started manually, or automatically through the `systemd unit`.\\nWe will use the `systemd` unit template to provide a way to run a Docker image that is a service. Containers can be started as a service using this unit template. The service can be started manually, or automatically through the `systemd unit`.\\nWe will use the `systemd` unit template to provide a way to run a Docker container image that is a service. Containers can be started as a service using this unit template. The service can be started manually, or automatically through the `systemd unit`.\\nWe will use the `systemd` unit template to provide a way to run a Docker container image that is a service. Containers can be started as a service using this unit template. The service can be started manually, or automatically through the `systemd unit`"}
{"Context":"## Context and Problem Statement\\nTerraform recommends not nesting modules into a deep hierarchy. We felt that varying from this recommendation made sense. We've started feeling some pain due to that decision. Do we follow this recommendation, or continue trying to organize modules for reusability and abstraction?\\n## Decision Drivers <!-- optional -->\\n* Terraform best practice\\n* Simplicity (maintenance, use, enhancement, fixes)\\n* Abstraction\\n* Reuse\\n* The way Terraform handles perceived changes to state\\n","Decision":"## Decision\n* Terraform best practice\\n* Simplicity (maintenance, use, enhancement, fixes)\\n* Abstraction\\n* Reuse\\n* The way Terraform handles perceived changes to state\\nChosen option: \"Discard component modules\", the danger of breaking products and the pain of enhancement is too high with component modules.\\n<!-- ### Positive Consequences optional -->\\n<!-- * [e.g., improvement of quality attribute satisfaction, follow-up decisions required, \u2026] -->\\n<!-- ### Negative Consequences optional -->\\n<!-- * [e.g., compromising quality attribute, follow-up decisions required, \u2026] -->\\n","Prediction":"## Decision\n* Terraform best practice\\n* Simplicity (maintenance, use, enhancement, fixes)\\n* Abstraction\\n* Reuse\\n* The way Terraform handles perceived changes to state\\nChosen option: \"Use modules\", because\\n* Modules are more extensible than a single module\\n* Modules are easier to maintain and extend\\n* Modules are more easily re-used, even in different projects\\n* Modules can be nested inside modules\\n* Modules can be nested inside modules\\n* Modules are more robust, as they are a single source of truth\\n* Modules can be nested inside modules\\n* Modules are more robust, as they are a single source of truth\\n* Modules can be nested inside modules\\n* Modules are more robust, as they are a single source of truth\\n* Modules can be nested inside modules\\n* Modules are more robust, as they are a single source of truth\\n* Modules can be nested inside modules\\n* Modules are more robust, as they are a single source of truth\\n* Modules can be nested inside modules\\n* Modules are more robust, as they are a single source of truth\\n* Modules can be nested inside modules\\n* Modules are more robust, as they are a single source of truth\\n* Modules"}
{"Context":"## Context\\nTelemetry is paramount into debugging and understanding what the application is doing and how it is\\nperforming. We aim to expose metrics from modules and other core parts of the Cosmos SDK.\\nIn addition, we should aim to support multiple configurable sinks that an operator may choose from.\\nBy default, when telemetry is enabled, the application should track and expose metrics that are\\nstored in-memory. The operator may choose to enable additional sinks, where we support only\\n[Prometheus](https:\/\/prometheus.io\/) for now, as it's battle-tested, simple to setup, open source,\\nand is rich with ecosystem tooling.\\nWe must also aim to integrate metrics into the Cosmos SDK in the most seamless way possible such that\\nmetrics may be added or removed at will and without much friction. To do this, we will use the\\n[go-metrics](https:\/\/github.com\/hashicorp\/go-metrics) library.\\nFinally, operators may enable telemetry along with specific configuration options. If enabled, metrics\\nwill be exposed via `\/metrics?format={text|prometheus}` via the API server.\\n","Decision":"## Decision\nWe will add an additional configuration block to `app.toml` that defines telemetry settings:\\n```toml\\n###############################################################################\\n###                         Telemetry Configuration                         ###\\n###############################################################################\\n[telemetry]\\n# Prefixed with keys to separate services\\nservice-name = {{ .Telemetry.ServiceName }}\\n# Enabled enables the application telemetry functionality. When enabled,\\n# an in-memory sink is also enabled by default. Operators may also enabled\\n# other sinks such as Prometheus.\\nenabled = {{ .Telemetry.Enabled }}\\n# Enable prefixing gauge values with hostname\\nenable-hostname = {{ .Telemetry.EnableHostname }}\\n# Enable adding hostname to labels\\nenable-hostname-label = {{ .Telemetry.EnableHostnameLabel }}\\n# Enable adding service to labels\\nenable-service-label = {{ .Telemetry.EnableServiceLabel }}\\n# PrometheusRetentionTime, when positive, enables a Prometheus metrics sink.\\nprometheus-retention-time = {{ .Telemetry.PrometheusRetentionTime }}\\n```\\nThe given configuration allows for two sinks -- in-memory and Prometheus. We create a `Metrics`\\ntype that performs all the bootstrapping for the operator, so capturing metrics becomes seamless.\\n```go\\n\/\/ Metrics defines a wrapper around application telemetry functionality. It allows\\n\/\/ metrics to be gathered at any point in time. When creating a Metrics object,\\n\/\/ internally, a global metrics is registered with a set of sinks as configured\\n\/\/ by the operator. In addition to the sinks, when a process gets a SIGUSR1, a\\n\/\/ dump of formatted recent metrics will be sent to STDERR.\\ntype Metrics struct {\\nmemSink           *metrics.InmemSink\\nprometheusEnabled bool\\n}\\n\/\/ Gather collects all registered metrics and returns a GatherResponse where the\\n\/\/ metrics are encoded depending on the type. Metrics are either encoded via\\n\/\/ Prometheus or JSON if in-memory.\\nfunc (m *Metrics) Gather(format string) (GatherResponse, error) {\\nswitch format {\\ncase FormatPrometheus:\\nreturn m.gatherPrometheus()\\ncase FormatText:\\nreturn m.gatherGeneric()\\ncase FormatDefault:\\nreturn m.gatherGeneric()\\ndefault:\\nreturn GatherResponse{}, fmt.Errorf(\"unsupported metrics format: %s\", format)\\n}\\n}\\n```\\nIn addition, `Metrics` allows us to gather the current set of metrics at any given point in time. An\\noperator may also choose to send a signal, SIGUSR1, to dump and print formatted metrics to STDERR.\\nDuring an application's bootstrapping and construction phase, if `Telemetry.Enabled` is `true`, the\\nAPI server will create an instance of a reference to `Metrics` object and will register a metrics\\nhandler accordingly.\\n```go\\nfunc (s *Server) Start(cfg config.Config) error {\\n\/\/ ...\\nif cfg.Telemetry.Enabled {\\nm, err := telemetry.New(cfg.Telemetry)\\nif err != nil {\\nreturn err\\n}\\ns.metrics = m\\ns.registerMetrics()\\n}\\n\/\/ ...\\n}\\nfunc (s *Server) registerMetrics() {\\nmetricsHandler := func(w http.ResponseWriter, r *http.Request) {\\nformat := strings.TrimSpace(r.FormValue(\"format\"))\\ngr, err := s.metrics.Gather(format)\\nif err != nil {\\nrest.WriteErrorResponse(w, http.StatusBadRequest, fmt.Sprintf(\"failed to gather metrics: %s\", err))\\nreturn\\n}\\nw.Header().Set(\"Content-Type\", gr.ContentType)\\n_, _ = w.Write(gr.Metrics)\\n}\\ns.Router.HandleFunc(\"\/metrics\", metricsHandler).Methods(\"GET\")\\n}\\n```\\nApplication developers may track counters, gauges, summaries, and key\/value metrics. There is no\\nadditional lifting required by modules to leverage profiling metrics. To do so, it's as simple as:\\n```go\\nfunc (k BaseKeeper) MintCoins(ctx sdk.Context, moduleName string, amt sdk.Coins) error {\\ndefer metrics.MeasureSince(time.Now(), \"MintCoins\")\\n\/\/ ...\\n}\\n```\\n","Prediction":"## Decision\nWe will use the go-metrics library to expose telemetry to the application. Metrics will be\\nexposed via `\/metrics?format={text|prometheus}`.\\nWe will use the go-metrics library as a dependency to the application. This will allow us to leverage\\nthe library's ecosystem and tooling. This will allow us to easily add metrics to the application if\\nrequired.\\nWe will use the go-metrics library to expose telemetry to the application. Metrics will be\\nexposed via `\/metrics?format={text|prometheus}` via the API server.\\nWe will use the go-metrics library as a dependency to the application. This will allow us to leverage\\nthe library's ecosystem and tooling. This will allow us to easily add metrics to the application if\\nrequired.\\nWe will use the go-metrics library to expose telemetry to the application. Metrics will be\\nexposed via `\/metrics?format={text|prometheus}` via the API server.\\nWe will use the go-metrics library as a dependency to the application. This will allow us to leverage\\nthe library's ecosystem and tooling. This will allow us to easily add metrics to the application if\\nrequired.\\nWe will use the go"}
{"Context":"## Context\\nUse an LPA will be made up of two components - those for use by LPA _actors_, and those used by third\\nparty groups who are the _viewers_ of the LPA.\\nAt present it is expected that these two components will be hosted on two different domains.\\n","Decision":"## Decision\nThat both `Viewer` and `Actor` will both be separate modules of the same Zend application.\\nNote: it is still expected that they will be deployed separately into two containers.\\n","Prediction":"## Decision\nWe will use two different domains for the two components.\\nFor LPA _actors_, we will use the following:\\n* lpa-actors.lpa\\n* lpa-actors.dev\\nFor the LPA _viewers_, we will use the following:\\n* lpa-views.lpa\\n* lpa-views.dev\\nWe will use the following subdomains for the two components:\\n* lpa-actors.dev\\n* lpa-actors\\n* lpa-views.dev\\nThis is a simple way of splitting the domain into subdomains and ensuring the LPA _actors_ and _viewers_ are\\nseparate.\\n### Example\\n### LPA _actors_ and _viewers_ on lpa-actors.dev\\n* lpa-actors.dev\\n* lpa-actors\\n* lpa-views.dev\\n### LPA _viewers_ on lpa-views.dev\\n* lpa-views.dev\\nThis is a simple way of splitting the domain into subdomains and ensuring the LPA _actors_ and _viewers_ are\\nseparate.\\n### Example\\n### LPA _viewers_ on lpa-views.dev\\n* lpa-views.dev\\n* l"}
{"Context":"## Context\\nIt's a long story of relations between Celestia, Tendermint, and consensus block gossiping. Celestia's team discussed\\nmultiple ideas, several ADRs were made, and nothing yet was finalized. This ADR is another attempt to bring valuable\\nchanges into block gossiping and hopefully successful.\\nCurrently, we inherit the following from Tendermint. Our codebase relies on the blocks Parts notion. Each Part is a\\npiece of an entire serialized block. Those Parts are gossiped between nodes in consensus and committed with\\n`PartSetHeader` containing a Merkle Root of the Parts. However, Parts gossiping wasn't designed for Celestia blocks.\\nCelestia comes with a different block representation from Tendermint. It lays out Blocks as a table of data shares,\\nwhere Rows or Columns can be and should be gossiped instead of Parts, keeping only one system-wide commitment to data.\\n","Decision":"## Decision\nThe decision is to still treat Tendermint's consensus as a black box, but with few amendments to gossiping mechanism:\\n* Introduce `RowSet` that mimics `PartSet`.\\n`RowSet` is a helper structure that wraps DAHeader and tracks received Rows with their integrity against DAHeader and\\ntells its user when the block is complete and\/or can be recovered. Mostly it is a helper and is not a high-level\\nconcept.\\n* Replace `PartSet` with `RowSet` within consensus.\\n* Keep `DAHeader` in `Proposal`\\n* Remove `PartSetHeader` from `Proposal`\\nThe changes above are required to implement the decision. At later point, other changes listed below are\\nlikely to be implemented as a clean-up:\\n* Entirely removing `PartSetHeader`, as redundant data commitment\\n* Removing `PartSet`\\n* Relying on `DAHeader` instead of `PartSetHeader`\\n","Prediction":"## Decision\nWe will instead use a different block representation to gossip parts between nodes. This can be done via\\n`PartSetHeader` and `PartSetData` in the block. The headers and data are not gossiped. The headers are both\\ndeserialized from the part, the data is serialized from the part, and the data is a Merkle Root of the parts.\\n`PartSetHeader` is a list of `Part` objects and each `Part` object has a `PartSetHeader` field. `PartSetHeader`\\nis a list of `PartSetHeader` objects. `PartSetHeader` is a list of `Part` objects and each `Part` object has a `PartSet`\\nHeader field. Each `Part` object has a `PartSetData` field. Each `PartSetHeader` object has a `PartSetHeader`\\nfield. Each `PartSetData` object has a `PartSetData` field. Each `PartSetHeader` object has a `PartSetData`\\nfield. Each `PartSetData` object has a `PartSetData`\\nfield. Each `PartSetData` object has a `PartSetData`\\nfield. Each `Part"}
{"Context":"## Context\\nIn EdgeX today, sensor\/device data collected can be \"filtered\" by [application services](..\/..\/..\/microservices\/application\/ApplicationServices.md) before being exported or sent to some [north side](..\/..\/..\/general\/Definitions.md#south-and-north-side) application or system. Built-in application service functions (available through the app services SDK) allow EdgeX event\/reading objects to be filtered by device name or by device ResourceName.  That is, event\/readings can be filtered by:\\n- which device sent the event\/reading (as determined by the Event device property).\\n- the classification or origin (such as temperature or humidity) of data produced by the device as determined by the Reading's name property (which used to be the value descriptor and now refers to the device ResourceName).\\n### Two Levels of Device Service Filtering\\nThere are potentially two places where \"filtering\" in a device service could be useful.\\n- One (Sensor Data Filter) - after the device service has communicated with the sensor or device to get sensor values (but before the service creates `Event\/Reading` objects and pushes those to core data).  A sensor data filter would allow the device service to essentially ignore some of the raw sensed data.  This would allow for some device service optimization in that the device service would not have perform type transformations and creation of event\/reading objects if the data can be eliminated at this early stage.  This first level filtering would, **if put in place**, likely occur in code associated with the read command gets done by the `ProtocolDriver`.\\n- Two (Reading Filter) - after the sensor data has been collected and read and put into `Event\/Reading` objects, there is a desire to filter some of the `Readings` based on the `Reading` values or `Reading` name (which is the device ResourceName) or some combination of value and name.\\nAt this time, **this design only addresses the need for the second filter (Reading Filter)**.  At the time of this writing, no applicable use case has yet to be defined to warrant the Sensor Data Filter.\\n### Reading Filters\\nReading filters will allow, not unlike application service filter functions today, to have `Readings` in an `Event` to be removed if:\\n- the value was outside or inside some range, or the value was greater than, less than or equal to some value\\n- based on the `Reading` value (numeric) of a `Reading` outside a specified range (min\/max) described in the service configuration.  Thus avoiding sending in outlier or jittery data `Readings` that could negatively effect analytics.\\n- Future scope:  based on the `Reading` value (numeric) equal to or near (with in some specified range) the last reading.  This allows a device service to reduce sending in `Event\/Readings` that do not represent any significant change.  This differs from the already implemented onChangeOnly in that it is filtering `Readings` within a specified degree of change.  **Note:** this feature would require caching of readings which has not fully been implemented in the SDK.  The existing mechanism for `autoevents` provides a partial cache.  Added for future reference, but this feature would not be accomplished in the initial implementation; requiring extra design work on caching to be implemented.\\n- the value was the same as some or not the same as some specified value or values (for strings, boolean and other non-numeric values)\\n- the value matches a pattern (glob and\/or regex) when the value is a string.\\n- the name (the device ResourceName) matched a particular value; in other words match `temperature` or `humidity` as example device resources.\\nUnlike application services, there is not a need to filter on a device name (or identifier).  Simply disable the device in the device service if all `Event\/Readings` are to be stopped for the device.\\nIn the case that all `Readings` of an `Event` are filtered, it is assumed the entire `Event` is deemed to be worthless and not sent to core data by the device service.  If only some `Readings` from and `Event` are filtered, the `Event` minus the filtered `Readings` would be sent to core data.\\nThe filter behaves the same whether the collection of `Readings` and `Events` is triggered by a scheduled collection of data from the underlying sensor\/device or triggered by a command request (as from the command service).  Therefore, the call for a command request still results in a successful status code and a return of no results (or partial results) if the filter causes all or some of the readings to be removed.\\n### Design \/ Architecture\\nA new function interface shall be defined that, when implemented, performs a Reading Filter operation.  A ReadingFilter function would take a parameter (an `Event` containing readings), check whether the `Readings` of the `Event` match on the filtering configuration (see below) and if they do then remove them from the `Event`.  The ReadingFilter function would return the `Event` object (minus filtered `Readings`) or `nil` if the `Event` held no more `Readings`.  Pseudo code for the generic function is provided below.  The results returned will include a boolean to indicate whether any `Reading` objects were removed from the `Event` (allowing the receiver to know if some were filtered from the original list).\\n``` go\\nfunc (f Filter) ReadingFilter(lc logger.LoggingClient, event *models.Event) (*models.Event, error, boolean) {\\n\/\/ depending on impl; filtering for values in\/out of a range, >, <, =, same, not same, from a particular name (device resource), etc.\\n\/\/ The boolean will indicate whether any Readings were filtered from the Event.\\nif (len(event.Reading )) > 0)\\nif (len filteredReadings > 0)\\nreturn event, true\\nelse\\nreturn event, false\\nelse\\nreturn nil, true\\n}\\n```\\nBased on current needs\/use cases, implementations of the function interface could include the following filter functions:\\n``` go\\nfunc (f Filter) FilterByValue (lc logger.LoggingClient, event *models.Event) (*models.Event, error, boolean) {}\\nfunc (f Filter) FilterByResourceNamesMatch (lc logger.LoggingClient, event *models.Event) (*models.Event, error, boolean) {}\\n```\\n!!! Note\\nThe app functions SDK comes with `FilterByDeviceName` and `FilterByResourceName` functions today. The FilterByResourceName would behave similarly to FilterByResourceNameMatch.\\nThe Filter structure houses the configuration parameters for which the filter functions work and filter on.\\n!!! Note\\nThe app functions SDK uses a fairly simple Filter structure.\\n``` go\\ntype Filter struct {\\nFilterValues []string\\nFilterOut    bool\\n}\\n```\\nGiven the collection of filter operations (in range, out of range, equal or not equal), the following structure is proposed:\\n``` go\\ntype Filter struct {\\nFilterValues []string\\nTargetResourceName string\\nFilterOp string  \/\/ enum of in (in range inclusive), out (outside a range exclusive), eq (equal) or ne (not equal)\\n}\\n```\\nExamples use of the Filter structure to specify filtering:\\n``` go\\nFilter {FilterValues: {10, 20}, \"Int64\", FilterOp: \"in\"} \/\/ filter for those Int64 readings with values between 10-20 inclusive\\nFilter {FilterValues: {10, 20}, \"Int64\", FilterOp: \"out\"} \/\/ filter for those Int64 readings with values outside of 10-20.\\nFilter {FilterValues: {8, 10, 12}, \"Int64\", FilterOp: \"eq\"} \/\/filter for those Int64 readings with values of 8, 10, or 12.\\nFilter {FilterValues: {8, 10}, \"Int64\", FilterOp: \"ne\"}  \/\/filter for those Int64 readings with values not equal to 8 or 10\\nFilter {FilterValues: {\"Int32\", \"Int64\"}, nil, FilterOp: \"eq\"} \/\/filter to be used with FilterByResourceNameMatch.  Filter for resource names of Int32 or Int64.\\nFilter {FilterValues: {\"Int32\"}, nil, FilterOp: \"ne\"} \/\/filter to be used with FilterByResourceNameMatch.  Filter for resource names not equal to (excluding) Int32.\\n```\\nA NewFilter function creates, initializes and returns a new instance of the filter based on the configuration provided.\\n``` go\\nfunc NewReadingNameFilter(filterValues []string, filterOp string) Filter {\\nreturn Filter{FilterValues: filterValues, TargetResourceName string, FilterOp: filterOp}\\n}\\n```\\n### Sharing filter functions\\nIf one were to explore the filtering functions in the app functions SDK [filter.go](https:\/\/github.com\/edgexfoundry\/app-functions-sdk-go\/blob\/master\/pkg\/transforms\/filter.go) (both `FilterByDeviceName` and `FilterByValueDescriptor`), the filters operate on the `Event` model object and return the same objects (`Event` or nil).  Ideally, since both app services and device services generally share the same interface model (from `go-mod-core-contracts`), it would be the desire to share the same filter functions functions between SDKs and associated services.\\nDecisions on how to do this in Go - whether by shared module for example - is left as a future release design and implementation task - and as the need for common filter functions across device services and application services are identified in use cases.  C needs are likely to be handled in the SDK directly.\\n#### Additional Design Considerations\\nAs Device Services do not have the concept of a functions pipeline like application services do, consideration must be given as to how and where to:\\n- provide configuration to specify which filter functions to invoke\\n- create the filter\\n- invoke the filtering functions\\nAt this time, custom filters will not be supported as the custom filters would not be known by the SDK and therefore could not be specified in configuration.  This is consistent with the app functions SDK and filtering.\\n#### Function Inflection Point\\nIt is precisely after the convert to `Event\/Reading` objects (after the async readings are assembled into events) and before returning that result in `common.SendEvent` (in utils.go) function that the device service should invoke the required filter functions.  In the existing V1 implementation of the device-sdk-go, commands, async readings, and auto-events all call the function `common.SendEvent()`.  *Note: V2 implementation will require some re-evaluation of this inflection point.*  Where possible, the implementation should locate a single point of inflection if possible.  In the C SDK, it is likely that the filters will be called before conversion to Event\/Reading objects - they will operate on commandresult objects (equivalent to CommandValues).\\nThe order in which functions are called is important when more than one filter is provided.  The order that functions are called should be reflected in the order listed in the configuration of the filters.\\nEvents containing binary values (event.HasBinaryValue), will not be filtered.  Future releases may include binary value filters.\\n#### Setting Filter Function and Configuration\\nWhen filter functions are shared (or appear to be doing the same type of work) between SDKs, the configuration of the similar filter functions should also look similar.  The app functions SDK configuration model for filters should therefore be followed.\\nWhile device services do not have pipelines, the inclusion and configuration of filters for device services should have a similar look (to provide symmetry with app services). The configuration has to provide the functions required and parameters to make the functions work - even though the association to a pipeline is not required.  Below is the common app service configuration as it relates to filters:\\n``` toml\\n[Writable.Pipeline]\\nExecutionOrder = \"FilterByDeviceName, TransformToXML, SetOutputData\"\\n[Writable.Pipeline.Functions.FilterByDeviceName]\\n[Writable.Pipeline.Functions.FilterByDeviceName.Parameters]\\nDeviceNames = \"Random-Float-Device,Random-Integer-Device\"\\nFilterOut = \"false\"\\n```\\nSuggested and hypothetical configuration for the device service reading filters should look something like that below.\\n``` toml\\n[Writable.Filters]\\n# filter readings where resource name equals Int32\\nExecutionOrder = \"FilterByResourceNamesMatch, FilterByValue\"\\n[Writable.Filter.Functions.FilterByResourceNamesMatch]\\n[Writable.Filter.Functions.FilterByResourceNamesMatch.Parameters]\\nFilterValues = \"Int32\"\\nFilterOps =\"eq\"\\n# filter readings where the Int64 readings (resource name) is Int64 and the values are between 10 and 20\\n[Writable.Filter.Functions.FilterByValue]\\n[Writable.Filter.Functions.FilterByValue.Parameters]\\nTargetResourceName = \"Int64\"\\nFilterValues = {10,20}\\nFilterOp = \"in\"\\n```\\n","Decision":"## Decision\n#### Additional Design Considerations\\nAs Device Services do not have the concept of a functions pipeline like application services do, consideration must be given as to how and where to:\\n- provide configuration to specify which filter functions to invoke\\n- create the filter\\n- invoke the filtering functions\\nAt this time, custom filters will not be supported as the custom filters would not be known by the SDK and therefore could not be specified in configuration.  This is consistent with the app functions SDK and filtering.\\n#### Function Inflection Point\\nIt is precisely after the convert to `Event\/Reading` objects (after the async readings are assembled into events) and before returning that result in `common.SendEvent` (in utils.go) function that the device service should invoke the required filter functions.  In the existing V1 implementation of the device-sdk-go, commands, async readings, and auto-events all call the function `common.SendEvent()`.  *Note: V2 implementation will require some re-evaluation of this inflection point.*  Where possible, the implementation should locate a single point of inflection if possible.  In the C SDK, it is likely that the filters will be called before conversion to Event\/Reading objects - they will operate on commandresult objects (equivalent to CommandValues).\\nThe order in which functions are called is important when more than one filter is provided.  The order that functions are called should be reflected in the order listed in the configuration of the filters.\\nEvents containing binary values (event.HasBinaryValue), will not be filtered.  Future releases may include binary value filters.\\n#### Setting Filter Function and Configuration\\nWhen filter functions are shared (or appear to be doing the same type of work) between SDKs, the configuration of the similar filter functions should also look similar.  The app functions SDK configuration model for filters should therefore be followed.\\nWhile device services do not have pipelines, the inclusion and configuration of filters for device services should have a similar look (to provide symmetry with app services). The configuration has to provide the functions required and parameters to make the functions work - even though the association to a pipeline is not required.  Below is the common app service configuration as it relates to filters:\\n``` toml\\n[Writable.Pipeline]\\nExecutionOrder = \"FilterByDeviceName, TransformToXML, SetOutputData\"\\n[Writable.Pipeline.Functions.FilterByDeviceName]\\n[Writable.Pipeline.Functions.FilterByDeviceName.Parameters]\\nDeviceNames = \"Random-Float-Device,Random-Integer-Device\"\\nFilterOut = \"false\"\\n```\\nSuggested and hypothetical configuration for the device service reading filters should look something like that below.\\n``` toml\\n[Writable.Filters]\\n# filter readings where resource name equals Int32\\nExecutionOrder = \"FilterByResourceNamesMatch, FilterByValue\"\\n[Writable.Filter.Functions.FilterByResourceNamesMatch]\\n[Writable.Filter.Functions.FilterByResourceNamesMatch.Parameters]\\nFilterValues = \"Int32\"\\nFilterOps =\"eq\"\\n# filter readings where the Int64 readings (resource name) is Int64 and the values are between 10 and 20\\n[Writable.Filter.Functions.FilterByValue]\\n[Writable.Filter.Functions.FilterByValue.Parameters]\\nTargetResourceName = \"Int64\"\\nFilterValues = {10,20}\\nFilterOp = \"in\"\\n```\\n*To be determined*\\n","Prediction":"## Decision\nAll reading filters will be provided as part of the event\/reading object.  The event\/reading object will be converted to a `Reading` object and then the filter function will be invoked.  The `Reading` object will be an instance of the `Event` model object.  The filter function will be invoked with the `Event` as the parameter and will return the `Reading` object.\\n!!! Note\\n- this design is similar to the existing app functions SDK and filtering.\\n- the design is similar to and inspired by the existing app filtering (filtering by device name).  However, the filtering by device name must be implemented in the SDK (e.g. `FilteringByDeviceName`) and the filtering by device resource name must be implemented in the device service (e.g. `FilteringByResourceName`).  This is a design decision that will be revisited and may be modified as the need arises.\\n- the filtering by device name is not implemented in the SDK as the SDK does not have the concept of a device resource name.\\n- the filtering by device name is implemented in the device service - the SDK does not need to know the device resource name.\\n- the filtering by device name is implemented in"}
{"Context":"## Context\\nJMAP core RFC8620 requires that the server responds only properties requested by the client.\\nJames currently computes all of the properties regardless of their cost, and if it had been asked by the client.\\nClearly we can save some latencies and resources by avoiding reading\/computing expensive properties that had not been explicitly requested by the client.\\n","Decision":"## Decision\nIntroduce two new datastructures representing JMAP messages:\\n- One with only metadata\\n- One with metadata + headers\\nGiven the properties requested by the client, the most appropriate message datastructure will be computed, on top of\\nexisting message storage APIs that should remain unchanged.\\nSome performance tests will be run in order to evaluate the improvements.\\n","Prediction":"## Decision\nFor now we will not compute any properties, and instead will rely on the client to explicitly request them.\\nWe will implement the client with a `get` method on the `Properties` class.\\nThis will allow the client to retrieve properties with the `get` method.\\nThis will also allow the client to retrieve a list of properties with the `get` method.\\nThis will also allow the client to retrieve a single property with the `get` method.\\nThis will also allow the client to retrieve a single property with the `get` method.\\nThis will also allow the client to retrieve a single property with the `get` method.\\nThis will also allow the client to retrieve a single property with the `get` method.\\nThis will also allow the client to retrieve a single property with the `get` method.\\nThis will also allow the client to retrieve a single property with the `get` method.\\nThis will also allow the client to retrieve a single property with the `get` method.\\nThis will also allow the client to retrieve a single property with the `get` method.\\nThis will also allow the client to retrieve a single property with the `get` method.\\nThis will also allow the"}
